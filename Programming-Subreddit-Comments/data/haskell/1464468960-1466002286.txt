Thanks! Personally I've been using this with Linux on a BeagleBone Black, but the advantage of doing everything through sysfs (which this package does, on Linux, at least) is that it will work on any Linux GPIO-capable system. One disadvantage of using sysfs is that it's pretty slow. HPi is probably capable of a much faster duty cycle. Of course, another goal of hpio is to abstract the GPIO API from the back end, and I'm sure it would be possible to write an HPi interpreter for it which would get most of the performance benefits of HPi. Then, when you're on an RPi you could use the HPi interpreter, and the sysfs interpreter for any other Linux system.
I can half-heartedly recommend the Edison platform. The hardware is really nice. Unfortunately Intel is awful at software so the development environment is terrible. If you're used to dealing with crappy embedded environments, maybe it's ok. Me, I'm not, so I quickly get frustrated if I can't install Debian or another standard distro. Edison uses Yocto and, short of an old port of Debian Wheezy that's no longer maintained, as far as I know there is no other supported distro. The hardware is so appealing that I'm tempted to try porting NixOS to it, but that means getting to know U-boot, and I'm not quite that desperate yet. 
I found a quick and dirty workaround, using the `Alternative` instance of `Parse` by defining my own`.:` function so that it can try different column names instead of just one. r .: colnames = asum (map (r Csv..:) colnames) I can then define a `FromNamedRecord` instance like this instance FromNamedRecord Invoice where parseNamedRecordr = Invoice &lt;$&gt; r .: ["date", "invoice_date"] &lt;*&gt; r .: ["quantity", "qty"] etc ... The problem obviously, is it's a bit verbose and not really efficient (but in my particular case, it's efficient enough). 
Both cabal and stack unpack have special logic to grab the cabal file from the index, so it gets the latest revision.
I think Avro is easier to understand if you consider its central niche: * A binary- and schema-based alternative to JSON; * For long-term storage of large data files in Hadoop clusters. The key distinguishing feature of Avro is that it has not only a data serialization system, but it's built around its own container file format, which requires Avro files to have a copy of the schema that was used to write them. This means that: * Avro files are "self-describing"; a program that reads an Avro file of unknown provenance is able to discern its structure. This eliminates the risk of binary data becoming unparseable if its idiosyncratic schema is lost. * Datums can be serialized more compactly: * Several serialization systems represent records as a sequence of key-value pairs, to support clients that may send a subset of the fields or send fields in different order than the decoder expects. * In Avro, on the other hand, a record is encoded by encoding the values of its fields in the order that they are declared in the file's schema. * Schema evolution, correspondingly, works different. Readers compare the schema they expect (the *reader schema*) with the schema embedded in the file (the *writer schema*), resolve differences according to a set of predefined rules, and then transparently convert records from the writer schema to the reader schema. Where Avro falls down, IMO, is when used in contexts where there is no container file, or where there's a very large number of small files. For example, using Avro to encode messages in a queue is a pain, because there's no obvious way to associate individual messages with their schema, and thus no native facility for reprocessing older records with newer schemas. There are some [projects to alleviate this](https://github.com/confluentinc/schema-registry) but they're still young, not at all commonplace, and not built into Avro.
The basic ideas go back much further than this paper, for example see Burge's 1975 book on recursive programming techniques :-)
Many packages bumped their upper bounds to support aeson-0.10, independently. Upper bounds are not mystical magic which protect us from these things. Stackage even uses them! Sure, I tend to eschew unnecessary upper bounds, but you seem to think that Stackage entirely eschews them. Nonsense! So I have no idea what point you are trying to make. The "lie of missing upper bounds" seems entirely unrelated to the problem.
Do you have no justification for the "massive human synchronization required by stackage." comment? Or do you agree that it is equivalent and more efficient than the synchronization that would have happened anyway? (or worse, never happened at all, and left hackage in a state where you simply cannot use some things together) It is true that the overhead of synchronization will grow with stackage size, but the direct benefits of synchronization will grow as well. The size of modern day projects already seem unsustainable for a dependency solver to handle. However, the effect Stackage has on curation actually makes that approach more viable, by keeping constraints curated to at least allow these snapshots to be valid build plans. I try to make life easy on the solver an maintainer by leaving off unimportant constraints, but I guess that rubs some folks the wrong way.
&gt; Authors are often familiar enough with their dependencies to know when to be suspicious that a change they are making might be using a newer API feature of the dependency. So I don't think this should come up all that often to begin with. The line of reasoning of "humans are sufficiently smart" is the line of reasoning of dynamic checks. We should do better in Haskell, because we know it's easy to miss little unchecked details. It is true that if you are familiar with your dependencies and which API subset you are targeting, then you can predict these things. This is why I previously left off so many version bounds on store. The APIs used are usually so fundamental to the package that I know cabal won't use old enough versions that these APIs mismatch. &gt; Authors should be encouraged to include "Since version x.x" messages in their haddocks when they make API changes. Some package authors are already quite consistent about that. Agreed, but do you really want me to be tweaking version bounds for every code change? So tedious! Sure, I'll pay attention to these when they are present, and consider the effect on the package's installation flexibility. But when it comes to every single import I use, I've got much better things to do. (like comment on reddit, lol)
Isn't the composition example the other way around? The second version can start with `k` immediately since that's the first function in the chain.
How about generating all the permutations and then filtering the results to only keep the ones which satisfy your 1-bit difference criteria? Would that be too slow for your purpose?
You don't need the full power of `pipes` to do this. The `turtle` library is slicker for this sort of use case: {-# LANGUAGE OverloadedStrings #-} import Data.Map (Map) import Turtle import qualified Data.Map.Strict as Map import qualified Data.Text as Text toMap :: Fold Text (Map Text Integer) toMap = Fold step Map.empty id where step m key = Map.insertWith (+) key 1 m stream :: Shell Text stream = do file &lt;- find (prefix "tweets_") "../tmp/tweets" line &lt;- input file guard (Text.isInfixOf "knicks" line) return (Text.takeWhile (/= '\t') (Text.dropWhile (/= '\t') line)) main :: IO () main = do m &lt;- fold stream toMap mapM_ print (Map.assocs m) I'd be interested to see how that performs. I can't test myself because I don't have the data you used.
Should work for N=3 if the checking is fast enough (40320 permutations to check). Would not work for N=4 (20922789888000 permutations to check).
Yes, a newtype wrapper over a deque is probably the way to go. Digging through GHC release notes, I found the answer to my question: Data.Queue was added to base in ghc 6.4 [1] and deprecated in ghc 6.6 [2] when it was subsumed by Data.Sequence. From the 6.6 release notes: &gt; There is a new module Data.Sequence for finite sequences. The Data.Queue module is now deprecated in favour of this faster, more featureful replacement. [1] https://downloads.haskell.org/~ghc/6.4/docs/html/users_guide/release-6-4.html [2] https://downloads.haskell.org/~ghc/6.6/docs/html/users_guide/release-6-6.html
I agree and disagree with you in a same time. From first glance, usage of `head/tail/fromJust` could really lead to problems. But if you think, they add a lot of evaluation speed where you don't need to unwrap `Maybe`-is along you program. I discovered I don't use `head/tail` much, falling back to pattern matching when I use lists. And you always need to think what are you doing, aren't you?
It saves ~20 seconds, and it also returns `("",7341)` I'm assuming it has the same problem with the previous implementation. total time = 30.98 secs (30977 ticks @ 1000 us, 1 processor) total alloc = 18,086,389,512 bytes (excludes profiling overheads) COST CENTRE MODULE %time %alloc stream MapReduce 80.6 94.2 readTextDevice Data.Text.Internal.IO 19.1 2.0 unstream/resize Data.Text.Internal.Fusion 0.3 3.8 The intent of this was to make a fast implementation of this ETL: https://github.com/dimroc/etl-language-comparison#the-task 
Thanks for your work so far! While there are development board with reasonably fast CPUs and storage, virtually all of them have 1GB / 2GB RAM. The server type machines seem absurdly expensive compared to what similar performance in the x86 world would cost, more of a novelty product these days, I'd say. Many interesting boards are from tiny manufacturers with questionable support. Who knows what parts of Linux actually work and if you'll ever get an updated kernel etc. for them.
&gt; The size of modern day projects already seem unsustainable for a dependency solver to handle. I think you're confusing cause and effect here... &gt; I try to make life easy on the solver an maintainer by leaving off unimportant constraints ...because you're actually making life harder on the solver when you do that! That's why packages like `yesod` overtax the solver, because they leave off version bounds on their direct dependencies, thereby requiring the solver to traverse a much larger search-space until dead-ends/conflicts are found, resulting in expensive backtracking, and finally hitting the computation budget. This problem gets compounded the more dependencies exhibit inaccurate version bounds themselves. In the current matrix build-report at http://matrix.hackage.haskell.org/package/yesod you see alot of `FAIL (BJ 2000)` boxes. On the other hand, other packages such as `stack` which have similarly complicated dependency trees but are more diligent with specifying bounds don't lead the solver astray, and find solutions with much less effort. See http://matrix.hackage.haskell.org/package/stack (the red boxes can be ignored for the argument at hand). That being said, Andres has [recently improved the solver a bit more](https://github.com/haskell/cabal/pull/3208), and early experiments show it's helping mitigating the penalty incurred by under-specified constraints as in the case of `yesod` to some degree. However, the solver is not a magic oracle, and the problem it has to solve is *in general* a hard one, and that's why we have guidelines such as the PVP to bring the complexity down to a manageable level. Making the problem the solver has to solve extra hard to then take that as evidence for the solver-centric approach being unsustainable is an unfair argument. Moreover, you create the very problem for which you then claim Stackage to be the only sustainable solution. PS: There are other perfectly good reasons to (sometimes) use curated package-collections such as Stackage. Finally, I do *not* claim that the solver-based way is a perfect solution either! Both approaches, global-curation vs local-curation are differently flawed, otherwise we would have already agreed on the best single approach, which there obviously isn't -- so [we need both approaches, neither subsumes the other one](http://www.well-typed.com/blog/2014/09/how-we-might-abolish-cabal-hell-part-1/). And that's also why Hackage/Cabal *will* add first-class support for package-collections!
ı didnt understand reorder?can u explain?
It can, npm delivers https://www.npmjs.com/package/jvminstall
&gt; using Avro to encode messages in a queue is a pain, because there's no obvious way to associate individual messages. It depends on what you call 'obvious'. One option would be to use Schema Registry (http://docs.confluent.io/3.0.0/schema-registry/docs/intro.html), but in general the idea is the same: if you don't want to embed schema to each message it is always possible to embed some id which uniquely identifies the schema and then get the schema separately by this ID... 
&gt; True! Is this optimization worth the maintenance burden? Without adequate tooling, It seems to be a tradeoff between efficiency of package maintenance and correctness * broadness of version constraints. Yes. Because there aren't necessarily any maintenance burden. - My packages are on Stackage - once in a while `nightly` build say bounds are too restrictive - (I check changelog for what is changed) - I make a PR to my own repository, so Travis-CI runs the tests - If package passes tests on CI, I make a revision relaxing upper bounds Also deciding whether you have low-enough lower bounds is IMHO quite easy: Decide which GHC versions you want to support: have bounds so [the bundled packages don't need to be bumped](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Libraries/VersionHistory), or if the lowest interesting GHC is 7.8, start with Stackage LTS-2 (lts-2.22 is released in August 2015, lts-2.0 in April 2015 - over a year ago) and check that your package works with it.
Can Anyone help me ?
There is [TQueue](https://hackage.haskell.org/package/stm-2.4.4.1/docs/Control-Concurrent-STM-TBQueue.html) and [TBQueue](https://hackage.haskell.org/package/stm-2.4.4.1/docs/Control-Concurrent-STM-TQueue.html), worth looking at.
Travis for the lower bounds and stackage for the upper bounds is a reasonable approach for automating this such that the maintenance isn't so bad. This isn't enough to proactively bump historical upper bounds, but perhaps that's ok.
Thank you for the reference. The list of types of morphisms is very helpful and understandable. Unfortunately, I don't have any formal education in category theory (I'm in computational physics). So, from chapter three on things become a bit blurry to me. My understanding is that there seems to be a generic way (*adjoint folds*) to express all these different types of morphisms in the same sense as a lot of recursive functions can be expressed as folds. Are you aware of an explicit implementation of e.g. `even`, `odd` using these adjoint folds in, say, Haskell? I think that would help me a lot to understand these things better. The motivation for this project was mostly curiosity. Final encoding looks like an intriguing way to define EDSLs and I was wondering how one could define mutually recursive functions (*mutumorphisms* as it seems) in a finally encoded EDSL. In the end I implement mutually recursive let-bindings (if that is the right term) for the EDSL similar to those that Haskell offers. Are you saying that if instead of what I did, I would find a way to express *adjoint folds* in the finally encoded EDSL then this would be a more elegant, or general way? Or are these adjoint folds more of a theoretical tool but less intended for actual implementation?
https://www.scaleway.com/pricing/ Basically an Odroid C2 for 3$ a month
Yeah, they're ARMv7, though. I'd be a bit afraid that they at some point move away from ARM or suddenly go to ARMv8 etc., I'd prefer to have a local machine for consistency and faster data transfers.
Duh of course they're ARMv7 I'm tired sorry
[removed]
Well it's a bit unexpected, isn't it? ARMv7 has no future with server machines, so if I'd rely on one of their offerings for 32bit ARM builds I'd be a bit afraid they just suddenly switch to 64bit before GHC is ready. Then again, GHC might work fine on a 64bit distro, depends on if they ship 32bit libraries. I should probably give it a try.
Sounds great - would love to get involved. We're using Haskell at StackHut (www.stackhut.com), and can probably host at our office in Holborn every once in a while.
Indentation works as it used to, it just doesn't show the different guides visually. You have to cycle through them to see where they are.
I would actually argue that it is more useful to think of `f` as the *first* function in the sequence rather than the last.
Debian has arm64 builds of ghc. 8.0.1 is available in experimental.
I found this a good explanation of what is meant by "shallow" and "deep" embeddings in Haskell. Also, the code for this paper may be found at: https://github.com/robstewart57/END-dsl-implementations 
That machine probably isn't good enough to deal with lots of GHC work, I'm afraid (sure, 4 or 8 cores, but only 2GB of RAM and 16gb mSD card is super limiting and very slow).
96boards has both the [Cello](http://www.96boards.org/products/ee/cello/) and AMD [HuskyBoard](http://www.96boards.org/products/ee/huskyboard/) in flight, which I hope will close the gap on affordable server boards. The Cello will be in the ~$500 USD range fully configured from what I understand, and will be shipping relatively soonish.
I see two problems with this approach. Typically, you have integer as your seed, that's only 4 bytes. Can you show a good package for pure generation of random numbers that can use byte string of arbitrary length as seed? I can imagine this can be worked around without much hassle though. Another thing is that ideally we want one-to-one correspondence between hashes and resulting images. If we use hash as input for random generator, how to make sure that every byte of hash is used? Imagine we have 16 bytes in hash and all these bytes should be relevant. Then without control you may get a collection of layers that only consumes 8 bytes. This is “loss of information” that will unavoidably lead to duplicate images for different hashes. 
So here's my stab at implementing the full challenge, grouping tweets into separate files and using concurrency: {-# LANGUAGE OverloadedStrings #-} import Data.Foldable (traverse_) import System.FilePath ((&lt;/&gt;)) import qualified Control.Concurrent as Concurrent import qualified Control.Concurrent.Async as Async import qualified Control.Concurrent.Chan.Unagi as Unagi import qualified Data.ByteString as ByteString import qualified Data.ByteString.Lazy as ByteString.Lazy import qualified Data.ByteString.Lazy.Char8 as ByteString.Lazy.Char8 import qualified Data.ByteString.Lex.Integral as Lex import qualified Data.Discrimination as Discrimination import qualified System.Directory as Directory parseLine :: ByteString.ByteString -&gt; Maybe (Int, ByteString.ByteString) parseLine bytes0 = do (k, bytes1) &lt;- Lex.readDecimal bytes0 let (_ , bytes2) = ByteString.span (/= 9) (ByteString.drop 1 bytes1) let (_ , bytes3) = ByteString.span (/= 9) (ByteString.drop 1 bytes2) let v = ByteString.drop 1 bytes3 if ByteString.isInfixOf "knicks" (ByteString.map lower v) then Just (k, v) else Nothing where lower w8 = if 65 &lt;= w8 &amp;&amp; w8 &lt; 91 then w8 + 32 else w8 parseFile :: ByteString.Lazy.ByteString -&gt; [(Int, ByteString.ByteString)] parseFile bytes = [ (k, v) | line &lt;- ByteString.Lazy.Char8.lines bytes , Just (k, v) &lt;- [parseLine (ByteString.Lazy.toStrict line)] ] main :: IO () main = do let dir = "/home/gabriel/Desktop/tweets" files &lt;- Directory.getDirectoryContents dir let hiddenFile file = file == "." || file == ".." let files' = map (dir &lt;/&gt;) (filter (not . hiddenFile) files) (inChan, outChan) &lt;- Unagi.newChan let processFile file = Concurrent.forkIO (do bytes &lt;- ByteString.Lazy.readFile file mapM_ (Unagi.writeChan inChan) (parseFile bytes) ) traverse_ processFile files' kvs &lt;- Unagi.getChanContents outChan let processGroup kvs@((k, _):_) = Async.Concurrently (do let toLine (k, v) = ByteString.Lazy.Char8.fromStrict v let txt = ByteString.Lazy.Char8.unlines (map toLine kvs) ByteString.Lazy.writeFile (show k ++ ".txt") txt ) Async.runConcurrently (do traverse_ processGroup (Discrimination.groupWith fst kvs) ) This runs in 7 to 8 seconds on my machine when using 4 cores: $ cat stack.yaml resolver: lts-6.0 packages: [] extra-deps: - promises-0.3 $ stack build async unagi-chan bytestring-lexing discrimination directory $ stack ghc -- -rtsopts -threaded -O2 etl.hs $ mkdir foo $ cd foo $ time ../etl +RTS -N4 real 0m7.259s user 0m24.220s sys 0m1.756s $ ls 100.txt 120.txt 141.txt 161.txt 181.txt 24.txt 44.txt 64.txt 84.txt 101.txt 121.txt 142.txt 162.txt 182.txt 25.txt 45.txt 65.txt 85.txt ... $ cat 84.txt #Knicks @mitsuirutts lol, oh how far we've fallen. The Knicks should be ashamed of themselves...much like our guys @PhDingIt From what I understand about the current win lose ratio, we might be on the verge of more $10 Knicks tickets I haven't really profiled or tuned it yet, though Edit: Oops! I realized you only have to count them, not separate them into files. I'll update this to reflect that soon.
&gt; Or are these adjoint folds more of a theoretical tool but less intended for actual implementation? My personal experience with the constructive algorithmics stuff is that once you get past `cata`, `ana`, `hylo` it is mostly an exercise in mental masturbation. It is an interesting theoretical exercise, but not a very interesting practical exercise. That said, I never did add `mutu` to my `recursion-schemes` package. Maybe I should.
&gt; However, there is a point at which unconstrained deps yield very direct solutions. If there are no constraints, then the latest packages are . To me it seems intuitive that less constraints in my constraint satisfaction system == less work. The search space is narrower but hairier with narrower constraints. By hairier I mean intuitively I would think that the solver would have to do a lot more work lining up packages that use narrow constraints, as it plays whack-a-mole tweaking one version just to cause another to need to be some unviable intersection of ranges. The problem is a search problem with a very large space of potential solutions. If there are n transitive dependencies and v different versions of each dependency, then there are v^n possible solutions. It is simply not the case that less constraints == less work, because each constraint eliminates huge numbers of solutions from needing to be considered. You seem to be focusing on only one solution: the one that uses the most recent versions of everything. But that solution is not guaranteed to succeed. Two seconds ago someone could have just uploaded a version of a package that breaks things. "Prefer-the-more-recent" is just a heuristic that can be applied whether version bounds exist or not. The instant you discover that this solution doesn't work you have to ask yourself "which solution should I try next?" In a world without version bounds, you have nothing to go on. You're stuck doing an expensive compile that can take minutes for each solution until you find one that works. This is clearly untenable. When hackage was first created there were no version bounds. It was very difficult to get things to work. That's the whole reason version bounds were created. **The solver needs accurate version bounds to work well.** Intuitive logical reasoning supports this. The collective community historical experience supports this. Data from the matrix builder supports this. I really don't think this assertion can be disputed. &gt; Agreed, this is why stack has supported using the results from cabal-install's solver since 0.1.0.0. I don't need it very often, but when I do it's extremely convenient! If you agree the solver is important, then I think you have to also agree that version bounds (especially upper bounds since that is the direction that time travels in) are important. Maintaining them properly does require effort. But we shouldn't neglect to maintain them just because the tools aren't as good as they could be. The burden of maintaining them doesn't go away. It gets taken up by the trustees, people who's time could be far better spent on more productive things.
Oooooo, nice. I like the Unagi chans! Particularly succinct is the: traverse_ processFile files' kvs &lt;- Unagi.getChanContents outChan Did you pick ByteString for speed? I initially picked Text on the offcase I'd like to filter by something other than "knicks", but maybe I'm misunderstanding the use-cases of ByteString and Text
Yep, that sounds about right. Although, I might say `realm` is a type variable that *is* inhabited by *all* types of kind `Symbol`.
Alright, here's a version that just does counting, and I've got the time down to 3 seconds using 4 cores: {-# LANGUAGE OverloadedStrings #-} import Data.Foldable (traverse_) import System.FilePath ((&lt;/&gt;)) import qualified Control.Concurrent.Async as Async import qualified Control.Concurrent.STM as STM import qualified Data.ByteString as ByteString import qualified Data.ByteString.Lazy as ByteString.Lazy import qualified Data.ByteString.Lazy.Char8 as ByteString.Lazy.Char8 import qualified Data.ByteString.Search as Search import qualified ListT import qualified System.Directory as Directory import qualified STMContainers.Map as Map parseFile :: ByteString.Lazy.ByteString -&gt; [ByteString.Lazy.ByteString] parseFile bytes0 = [ neighborhood line | line &lt;- ByteString.Lazy.Char8.lines bytes0 , match line ] where lower w8 = if 65 &lt;= w8 &amp;&amp; w8 &lt; 91 then w8 + 32 else w8 match = not . null . Search.indices "knicks" . ByteString.map lower . ByteString.Lazy.toStrict neighborhood = ByteString.Lazy.takeWhile (/= 9) . ByteString.Lazy.drop 1 . ByteString.Lazy.dropWhile (/= 9) main :: IO () main = do let dir = "/home/gabriel/Desktop/tweets" files &lt;- Directory.getDirectoryContents dir let hiddenFile file = file == "." || file == ".." let files' = map (dir &lt;/&gt;) (filter (not . hiddenFile) files) m &lt;- Map.newIO let increment key = STM.atomically (do x &lt;- Map.lookup key m case x of Nothing -&gt; Map.insert 1 key m Just n -&gt; n' `seq` Map.insert n' key m where n' = n + 1 ) let processFile file = Async.Concurrently (do bytes &lt;- ByteString.Lazy.readFile file traverse_ increment (parseFile bytes) ) Async.runConcurrently (traverse_ processFile files') kvs &lt;- STM.atomically (ListT.toList (Map.stream m)) traverse_ print kvs The timings I get on my machine are: $ bench './etl2 +RTS -N4' benchmarking ./etl2 +RTS -N4 time 3.044 s (2.862 s .. 3.143 s) 1.000 R² (0.999 R² .. 1.000 R²) mean 3.098 s (3.087 s .. 3.104 s) std dev 10.20 ms (0.0 s .. 10.90 ms) variance introduced by outliers: 19% (moderately inflated) The bottleneck is basically searching the string for `"knicks"` (90% of the time spent is just on that). Anything that speeds that up will accelerate things further: $ cat etl2.prof Sun May 29 13:50 2016 Time and Allocation Profiling Report (Final) etl2 +RTS -N4 -p -RTS total time = 30.32 secs (121295 ticks @ 1000 us, 4 processors) total alloc = 15,300,772,888 bytes (excludes profiling overheads) COST CENTRE MODULE %time %alloc parseFile.match Main 90.8 72.2 parseFile.lower Main 5.5 8.2 parseFile Main 2.1 10.8 main.processFile Main 1.0 8.5 ...
Haven't looked at GHC ARM for a while, but do you really need the runtime linked? Can't you use dynamic libraries instead?
I changed a couple things to write the results to a file: let glue r (k, v) = return (ByteString.concat [ r , intToBs v , " - " , ByteString.Lazy.Char8.toStrict k , "\n" ]) let intToBs = T.encodeUtf8 . T.pack . show Async.runConcurrently (traverse_ processFile files) fileContents &lt;- STM.atomically (ListT.fold glue "" (Map.stream m)) ByteString.writeFile "../tmp/haskell_results.txt" fileContents I really like this implementation, actually. I'd like to benchmark a Text version as well, and maybe submit this. Would you be cool with that Tekmo (since it's all your code)? I'd like haskell to be represented in that ETL comparison since it's obviously quite good at this sort of thing 
Yes, feel free to reuse or modify my code and submit it!
I think the point was to have a unified theory of iterative/recursive functions, wasn't it? That's not a bad goal in itself.
I guess we've come full circle now :)
By the way, why do functional languages not use reference counting as in Swift? Weak references seem to solve the cycle problem, what else do I miss?
In swift you can also create cycles, but it is the programmers responsibility to mark one reference as weak such that the cycle is broken as far as reference counting is concerned.
why? I googled but did not find a reason for this. Wikipedia even has an [example](https://en.wikipedia.org/wiki/Reference_counting#COM) where it works in a big system.
Sure. As a theory it is fine, as a practice it hasn't paid out much. This is as much a function of presentation as anything. The whole thing has been dressed up in pseudo-Latin/Greek terminology in a way that makes people run screaming saying "I don't know category theory", when almost none of the terms are category theoretic and are just as alien to a category theorist. There was a whole "lets use hylo fusion for optimization" initiative that spun out of it for a while that petered out. Intuitively, it seems to me that basically once you are using `hylo` you're Turing complete anyways, so the normal form is "too big" and converting to/from it isn't nicely algorithmic. For a while there was a bunch of stuff about using distributive laws to make new recursion schemes from different monads and comonads, but that is basically just using a distributive law to make a more interesting algebra/coalgebra in a systemic way -- and it doesn't really compose well. It is like the monad transformer story where arbitrary monads don't compose, in fact the distributive laws compose exactly like monad transformers. The fact that you can construct novel recursion schemes using such a distributive law is interesting. Most of the individual cases, however, are not. The adjoint fold machinery subsumed that work and put it in a better framework at least. There is a subset of the community that doesn't like using recursive definitions and prefers to write all their code using `foldr`, the catamorphism for lists. My concern is when you push this view too far and say you should use say, zygomorphisms and generalized apomorphisms, you throw away a very large chunk of your audience, and at the same time put yourself in a very brittle state where small changes in algorithm lead to huge changes in code. It requires an excessive amount of cleverness to encode your algorithm in this way, and small changes require you to be excessively clever all over again. That "brittleness" of code written in this style seems to damn efforts to use it in practice, much like early lens library designs.
Well they were the first libraries to do general type serialization, so in that sense they were a great success. But people have been complaining lately about performance issues resulting from some problematic default serializations of primitive types. In the use case of binary format specification, those issues do not occur.
A fun exercise is to implement the instances for the classes in the `parsers` package. http://hackage.haskell.org/package/parsers The bulk (all?) of the parsec style combinators have been built atop that framework, so you get a lot for very little effort that way, and then they work even if you apply monad transformers to your parsing monad.
What a cutie!
How do print anything? You don't keep track of how much of the tape you have touched, and the tape is infinite in both directions.
Does anyone know what the following means (quoted from the article): &gt; As a meta-comment, one possible explanation for why this design choice was made might be that a lot of effort was invested in the Haskell’s GC to support concurrent mutators (a multi-core runtime). The additional complexity imposed by this extremely challenging and useful requirement may have encouraged runtime authors to keep the general GC architecture as simple as reasonably possible, which could explain this choice of using the same collection strategy in all generational spaces.
Hard to say - we've only began porting out Python prototype to Haskell over the past month or so so is pretty early. A few issues have cropped up with FFI support, static linking, and general IDE/build tool support but nothing major at present. Feel free to PM or email using my reddit username at company dot com to chat further.
That's true. I've wanted some way of telling the RTS to collect all roots to this closure, and spawn a new GC region for future allocations since forever. As you say, having multiple heaps in one process (erlang-style) is a natural option.
Ultimately I don't think it could ever reach the latency that separate heaps or processes can reach. You'd have to have a real-time GC system, but those are typically inefficient.
I'm fairly certain that the recent discussion[0] spurred from the post by James Fisher[1] arrived at the conclusion that there simply are no best practices or practices at all that will lower your worst-case GC latency sufficiently for soft-real time usage. Some of the advices were to move the large data set out of reach of Haskell GC, which seems quite a crappy way to fix an issue that appears to be fixable (as per Ocaml's implementation). Essentially you'd have to use FFI and move the long-lived data into C or something like that. Even Simon Marlow[2] confirmed that there is no real way of working around this currently in GHC. Note that I'm certainly not saying that this is something trivial, but it will be something that potentially has a *huge* benefit for all programs. I definitely think we aren't done with optimising the GC in Haskell/GHC. This might not be something new, but perhaps with Haskell getting accepted more and more, new use cases arise, and this particular one hits a wall with the GC. [0] https://www.reddit.com/r/haskell/comments/4j0imi/ghc_cannot_achieve_low_latency_with_a_large/? [1] https://blog.pusher.com/latency-working-set-ghc-gc-pick-two/ [2] http://stackoverflow.com/questions/36772017/reducing-garbage-collection-pause-time-in-a-haskell-program/36779227#36779227
Interesting!
&gt; Isn't Ref-counted GC also the one, and only, way to mess up your memory in rust? You can create memory leaks (you can do that without RC as well) but not undefined behavior.
Thanks for sharing!
This is exactly the route we went for to scale a scientific application to more than 10 cores (and to multi machine), I did a small writeup describing this work: https://www.fpcomplete.com/experience-scaling-computation
If it leaks, it isn't, unless you use a different definition of "safe". Which, yes, rust does. "Safety" is more than "doesn't segfault", "no uninitialised memory" etc.
Is there a minimum age for attendees? I'm 17 and meetup.com won't let me sign up until I'm 18.
 let increment key = STM.atomically (do x &lt;- Map.lookup key m case x of Nothing -&gt; Map.insert 1 key m Just n -&gt; n' `seq` Map.insert n' key m where n' = n + 1 ) ^ ^ ^ You can improve this using the "focus" machinery, to only pay for traversing to the key once: let increment key = STM.atomically (do Map.focus (Focus.alterM (\case Nothing -&gt; pure (Just 1) Just n -&gt; n' `seq` pure (Just n') where n' = n + 1) key m)
Try reading until you see a blank!
&gt; Your F# version is faster than your Haskell version because F# is better than Haskell. God, Harrop is a dink.
Ah, I see. :) data Binary = Blank | Zero | One
Oh, interesting. Should be all good now. I don't have a picture myself, but it probably doesn't apply to the organizer :)
One thing you shouldn't do is sign up with a fake birth date. Otherwise drop me a p m with your email address and I'll keep you updated until we setup a webpage or a mailing list somewhere.
And Erlang/OTP 19.0 gets even better: https://www.erlang-solutions.com/blog/erlang-19-0-garbage-collector.html
Is the library object based? And if it is does that make using the FFI impossible?
Nobody actually answers the OP's question. Funny.
I recenly wrote about Electron + Haskell[0], which might be close to what you want. If you're unfamiliar with Electron[1], it's basically chromium wrapped up as an app, and access to node.js API (i.e. file access etc). IIRC Atom, Slack and a lot of other apps use it. It does mean that you structure your game a bit as a webserver (in haskell) and client (in JS). In my example, I actually use jQuery, so it should get you going pretty quickly :) [0] https://codetalk.io/posts/2016-05-11-using-electron-with-haskell.html [1] http://electron.atom.io
&gt; let increment key = STM.atomically (do Map.focus (Focus.alterM (\case Nothing -&gt; pure (Just 1) Just n -&gt; n' `seq` pure (Just n') where n' = n + 1) key m) ah, you're right. Not really sure what you meant with the lambda passed to Focus.alterM tho. I'll try some stuff out and see if it effects the benchmarks
Oh, that's the LambdaCase language extension.
I've opened this issue about more comprehensive benchmarking: https://github.com/fpco/store/issues/39
There's [this library] (https://github.com/ajtulloch/dnngraph), which already provides a haskelly api for deep learning and abstracts out the backend. Seems kinda abandoned, but it might be worth cleaning it up and writing a tensorflow backend vs starting from scratch. 
Yes, but not necessarly. It's a very good question
If you need to support all libc features, how will the final product be much different than using a small general purpose unix kernel and replacing init with your program ?
Its for GHCi.
I'm in now - thanks!
Could you comment on where seL4 fits into this? The initial goal was a trusted kernel, and seL4 is verified. Wouldn't that make it a reasonable fit?
I didn't know about classy lenses, this is really cool. I always used the `zoom` combinator for this, but classy lenses of course compose better if you need multiple components...
There's also classy prisms, which are great for error handling (amongst other things).
Looks good! I've tried [a] -&gt; (a -&gt; b) -&gt; [b] and it doesn't return transform_convert as one would expect. I suggest you have a look at Neil Mitchell's [slides](http://ndmitchell.com/downloads/slides-hoogle_finding_functions_from_types-16_may_2011.pdf) about how matching works in hoogle.
Thank you. That is a good remark. I have to think about how to implement it. In my current ADT for the parsed types the currying is not flat but right-associatively nested (Elm code): type Signature = Arrow Signature Signature | ListType Signature | Tuple (List Signature) | TypeConstructor String | TypeApplication Signature Signature | VariableType String Perhaps I should change it to `Arrow (List Signature)` to make checking these permutation equalities easier. I already skimmed Neil's slides, but only understood parts of it. I will have a second look.
Just watched the linked talk (https://www.youtube.com/watch?v=GZPup5Iuaqw) which explains all this, there's no end of what I don't know about `lens` ;-)
&gt; Low-level functions end up with smaller sets of constraints. Larger ones accumulate the combined constraints. Such the ripple effect is the first sign of too high coupling. &gt; it is nice to look at a function and see exactly what capabilities are required to implement it If you think about it a bit more, you'll find exactly the opposite. You don't need the capabilities a function doesn't use directly. Instead of accumulating constraints, try to decouple different layers of your application by e.g. introducing more specific type class (or, better, by scraping unnecessary transformers, e.g. passing AppState explicitly as an argument.)
Has anybody been able to find the code for this? I would love to take a look at it.
It's interesting to see this post, since I (as a fellow Enterprise Haskeller :)) moved in the opposite direction: I mostly just use the application newtype in signatures everywhere. I've found that this makes updates to the code less painful (no need to propagate contexts everywhere), and I hardly ever need the polymorphism to use these kinds of functions in different contexts. When I do, I usually define my own type class like you mentioned and stop there. For me this gives me a nice balance between maintainability and reuse, but that probably depends on how/why you write your Haskell code, since for you the balance worked out differently.
There's another side to the polymorphism though, rather than just being about reuse in different contexts. By only allowing access via the classy lenses, you're reducing the number of valid programs with that type (mostly the ones that would mistakenly access things that they shouldn't). It's an interesting bunch of tradeoffs :) 
say you are in community X ... is there a community Y (probably doing almost the same but a bit different) hating X? ... the answer is always yes - where community is usually only a subset - there are probably many around who enjoy both F# and Haskell (well I do) but well there are always those in search of the *one* silver bullet (usually younger people, fresh in love with there technology of choice) If you want real hate try JavaScript :devil:
I dunno, a kernel which is running on somewhere between several hundred million and over a billion devices world wide sounds pretty mature to me.* *to be fair, it's OKL4 that's running on those devices, but it's all Data61 (formerly NICTA) work.
According to the paper it’s implemented in Haskell and uses parsec for lexing and parsing.
This is a really great technique! I've showed the use of this approach for identifying needed language features to classify fragments of EDSL code in a couple talks from a year or two ago, which I think drives home the value of this approach from a slightly different angle. One way I like to think of it is a progressive refinement of the perfectly polymorphic code we'd like to write. That is, writing `id` is great because you get some strong guarantees from the polymorphic type, and, hey, you can test the heck out of it! The reason you can test the heck out of it is because you the function author don't need anything from it. But as you start shaving facets off the fully polymorphic monolith to grant yourself purchase, you quickly lose track of what you've lost in terms of beautiful smooth monolith. Demanding constraints of the supplied structure without specifying its full form is a great way to keep track of exactly what you are doing without accidentally over-specifying which, surprisingly, tends to admit unintended behavior.
I wrote a dumb preliminary answer to (part of) your csv question, fwiw. I don't really care to answer SO questions any more, unless I have special knowledge, since in the last couple of years it has happened that, by the time you are finished with a complex answer, the various ops and notables have defaced the question with all kinds of arguments that the question was illegitimate. (Tomas P. points this out in his response to the above; I can't tell if he's thinking of Haskell-SO in particular, or the whole operation.) After that hits you three or four times, you give up. 
The subreddit you linked looks very small (~1000 subscribers) so I'm not too surprised that wasn't helpful. I find SO to be hit or miss. Personally, I don't really like answering questions there. I'm sure others feel similarly. Anyway, if an SO question is not getting the attention you want try adding a bounty. As for cassava, I've never used it so I doubt I would be much help. The reply on SO makes a lot of sense to me. It seems like Michael is on the right track. The representation in the question is costly and also not taking advantage of laziness (it appears to be suffering from it). Often times in Haskell, pure code can be aggressively transformed by the compiler to something nice. So while a nice STRef solution might exist a nice pure solution with the same performance may also be possible. I think the real issue with the linked question is that barely anyone took an interest in it. Sometimes that just happens. 
Despite the fact that Solo5 might eventually be integrated in Mirage (or not), the totally different portability and driver story in Rump make me think that rump should be prioritized as the next port. The resources are already scarce so I'd concentrate on one until there are more steady contributors.
Thanks! You can check out my proposal here: https://gist.github.com/rahulmutt/9bcab8c9b8bc3d99be26c11ae42d2795 If you're not so much interested in the implementation details, I particularly recommend going straight to the FFI section where you can get a taste of how you can write Java inside of Haskell and use Java methods from Haskell.
Thanks and impressive. I hope you succeed!
I don't think there is any question. It's just a link on a question, not a question itself.
I'm real glad you like it! If you have any suggestions for other interfaces we can do induction on like that I would love to hear them. Monoid is already done in GHC.Base, that's the only one I really though of.. Not quite sure if I'd want to do the Monad, Applicative, Functor hierarchy..
I'd be interested to see some example code of what you mean. I think once a project reaches a sort of equilibrium, managing constraints is pretty mechanical and straightforward. The compiler tells me exactly where the constraints are missing (and soon will warn me when they are unnecessary), and I do think there's a value in what the constraints of any given function tell me. Its much the same as what the signature of an explicit argument would tell me but has different ergonomics in practice.
&gt; d no-one on either /r/haskellquestions or stackoverflow could give them a decent answer. It seems that the place to ask question about Haskell is THIS subreddit. Nowadays, on SO, any non-trivial question is closed for being too broad or opinion based ... Also, question on SO which tend to get good answers are usually more theoretical (how do I use this or that extension?) than practical (how do I find a workaround for this bug in this library). &gt; Haskell's type-system and immutability are no longer noteworthy I guess it depends what you do. For fun, for example, nothing beat - IMO - Haskell type system (except of course the Agda Idris and friends), and immutability has some advantages. However, I agree that real world things can be done in other language than Haskell (and it might even be quicker to write and faster to execute than in Haskell) but that's probably true for every language. 
&gt; F# is just as readable as Haskell Yes and no. You get used to operators so comparing them doesn't really mean anything. However, immutability and no side effect, really helps readability, because everything you need to know is under you eyes. That's a really important point for me.
I programmed in APL in my youth. Matrix multiplication was denoted +.*, and one could swap in, say max and min for different problems. Deja vu, the above is very much in that spirit.
Wrapping Python sounds suboptimal. I suspect bridging two language barriers and having three runtimes in a single process will make for a grim debugging experience (not to mention wrangling all the build systems)
&gt; "classy" lenses are nice in that they give you a natural extension point to use an alternative or more granular reader state if necessary and encoruage a more constraint-based approach. Can you give an example how this works in practice? Is your main application state a nested record of substates? Or do you define local state types for different parts of your program and functions `AppState -&gt; LocalState`?
If you need multiple components, `zoom`. Otherwise the classy approach lets you let the compiler write most of the plumbing for you.
For example we need a function that increments a value in database. Lets use redis for that. We have functions to read and write a value: readValue :: MonadRedis m =&gt; Key -&gt; m Int writeValue :: MonadRedis m =&gt; Key -&gt; Int -&gt; m () If we accumulate constraints, our function will look like the next: incrementValue :: MonadRedis m =&gt; Key -&gt; m () incrementValue key = do v &lt;- readValue key writeValue key (succ v) First of all, now we expose the fact that we are using redis to store values. If you want to switch to memcache, you have to update all the code that uses `incrementValue`. Even worse, the type of `incrementValue` is not precise -- the function may perform any operations in redis (e.g. subscribe to a pubsub channel.) To fix that, you can create a specific interface, `MonadDB`: class MonadDB m where read :: Key -&gt; m Int write :: Key -&gt; Int -&gt; m () instance MonadRedis m =&gt; MonadDB m where read = readValue write = writeValue incrementValue' :: MonadDB m =&gt; Key -&gt; m () incrementValue' key = do v &lt;- read key write key (succ v) Now `incrementValue'` doesn't depends on which database you are using. And it can perform only `read` and `write`, there is no way for it to use any other redis API.
[removed]
I intend to offer a talk. If anyone has any preferences on what do let me know. My current best guess would be finding and fixing space leaks. 
Use pipes-csv if you want to process csv data incrementally. https://github.com/bitemyapp/csvtest/blob/master/src/Csvtest.hs#L73-L99 The pipes version uses 600kb of heap, compared to Cassava's 27MB. Please do not delete your comment, I will be using it the next time someone suggests creating yet-another-Haskell-subreddit-no-one-is-subscribed-to. Nota bene: my example here uses pipes-csv, but I think the Conduit CSV library is more correct and flexible in how it processes CSV data. I would expect memory usage of the two (pipes-csv and csv-conduit) to be similar.
I would think using the [embedding API](https://docs.python.org/3.5/extending/embedding.html) would be easiest. That's how [Julia does it](https://github.com/stevengj/PyCall.jl/blob/4d1426b54daf96b2cd53cf3aeede7097e7d9746a/src/pyinit.jl#L28). There is [a haskell library called cpython](https://hackage.haskell.org/package/cpython) ([documentation](https://john-millikin.com/software/haskell-python/reference/haskell-cpython/latest/)) that claims to do this.
Why does it matter that much if I may ask? Anyone will know what you are referring to regardless, given the context. 
Probably doesn't matter, but my instinct tells me to use 'type class' as a noun and 'typeclass' as a verb.
Straight from the horses mouth: "type classes". http://homepages.inf.ed.ac.uk/wadler/topics/type-classes.html 
The code speaks for itself in this case. I don't think I'd ever use it in anything other than a project Euler problem, though. It reminds me of [this](https://github.com/mikeizbicki/subhask#comparison-hierarchy) part of SubHask.
I posted this on behalf of a friend, so I know little about it. But thought the job description sounded interesting for Haskellers.
Curiosity and a niggling sensation to sort this detail out in my mind.
page 3 &gt; Furthermore, while we chose to implement the system in Haskell, our approach is not limited to this language. 
And on the same page &gt; We use the Haskell Parsec library to make lexing easy
I added a little demo of pipes-csv to my answer. The main problem here is that he really would like to accumulate the whole parsed file into e.g. a matrix, though he considers other things, so the relation to streaming libraries is only part of the answer.
Hey, most of the code is just instances, but there is one class in the published version and two in my current working version, so I'll work on adding more documentation. As /u/rpgglover said, though, the code speaks for itself and supplies the user with quite succinct definitions.
Yeah, only so much you can do, but it might still save space if the matrix representation is smaller (probably is) than the csv data rows.
This sounds like the classic case of "It can be re-written better".
I was thinking about it but I feel like so many people use it and it really does change your intuitions about type checking so I dunno if it'd be worth it, you know? What do you think?
No links?
It's honestly looking like this library could be something of a game changer for building these types of libraries. Sure, it doesn't provide the solution, but before we were fairly ignorant to the problem itself. Look forward to seeing more applications of `weigh`!
I think ghc 8 has some new pattern matching capabilities but I haven't learned them yet. edit: you could do something like that with type operators though. If + was a data type so for example {-# LANGUAGE TypeOperators #-} data (:+:) a = a :+: a d :: Num b =&gt; (a -&gt; b) -&gt; ((:+:) a) -&gt; b d f (x :+: y) = f x + f y That's not quite what you wanted but it's similar.
The OP's question on SO was 'why is this code slower?' And all the answers are 'here's some other code that isn't slower'.
What you really want to do is not pattern match on the argument of @d@, but instead make @Double -&gt; Double@ an instance of @Floating a@ and an instance of @Show a@. The floating instance is easy, but the show instance will take some work. Once you have said instances, @d@ can be defined using automatic differentiation (https://en.m.wikipedia.org/wiki/Automatic_differentiation)
I have those instances actually :P https://github.com/SamuelSchlesinger/Academic/tree/master/delude
But of course there's something to be said for having more code in Haskell and less reliance on stuff like TCP/IP written in C as part of Rump. Safety, correctness, more manageable code base. So maybe Solo5 is better suited for that.
Lisp macros might, if you abuse them enough.
That's a really good idea, thank you for that.
Yeah I've made things like your example, thank you for showing that though. I'm gonna read into automatic differentiation I feel like it might be fun to learn about.
&gt; I'm not sure it contains enough detail to reproduce anything. Ah, systems conferences!
[aeson-better-errors](http://harry.garrood.me/blog/aeson-better-errors/) looks relevant.
I'd just leave it off, and use the name as a noun: `getWith` takes an `Options`.
I didn't. It makes for a nice validation - here is the criterion output for mapMonotonic: benchmarking map/monotonic time 61.29 μs (60.39 μs .. 62.15 μs) 0.998 R² (0.997 R² .. 0.999 R²) mean 61.30 μs (60.35 μs .. 62.40 μs) std dev 3.539 μs (2.929 μs .. 4.282 μs) allocated: 1.000 R² (1.000 R² .. 1.000 R²) iters 47508.543 (47467.873 .. 47550.429) y -3340.557 (-47625.164 .. 43572.772) variance introduced by outliers: 62% (severely inflated) Good - the *iters* value very closely matches the result from `weigh`.
Off the top of my head: You may want to start with Forward mode, it is by far the easiest, and there is the most written on the technique. http://augustss.blogspot.com/2007/04/overloading-haskell-numbers-part-2.html There is also material written by Conal Elliott and a host of others. From there, http://www.bcl.hamilton.ie/~barak/papers/toplas-reverse.pdf is a paper by Barak Pearlmutter and Jeffrey Mark Siskind that dives into how taking derivatives of functions works in theory for reverse mode, etc. They went off and implemented this in scheme as a language called "stalin∇". This was the first functional reverse mode AD implementation I ever saw. The use of 'region parameters' to tag infinitesimals with a type argument like we do `ST s` is borrowed from an article by Chung-Chieh Shan: http://conway.rutgers.edu/~ccshan/wiki/blog/posts/Differentiation/ That was adapted by Björn Buckwalter for Pearlmutter and Siskind's `fad` library which is a precursor to my `ad` library. What is now `Kahn` mode is based on a technique that was used around the same time in Kansas Lava called 'observable sharing' to extract the graph with sharing information. Andy Gill did a much better job packaging things up than I did, so I switched to his implementation from `data-reify`. The paper on the technique is here: http://www.ittc.ku.edu/~andygill/papers/reifyGraph.pdf From there once I had the graph in hand I'd do a topological sort on the [computation graph](https://en.wikipedia.org/wiki/Automatic_differentiation#/media/File:ReverseaccumulationAD.png), and propagate sensitivities through that. The current `Reverse` mode on the other hand, accumulates something called a "Wengert list" as it goes using a technique from my `reflection` package, which is based on an paper by Oleg Kiselyov and Chung-Chieh Shan, but with an implementation that abuses GHC internals to run about 3 orders of magnitude faster. The original paper espousing the idea can be found here: http://okmij.org/ftp/Haskell/tr-15-04.pdf From there we have a bunch of other modes. The notion of a forward tower mode I first saw in some writings by Conal Elliot. http://conal.net/papers/beautiful-differentiation/beautiful-differentiation-long.pdf is a good sample from that work. The sparse and dense forward mode code was based on a quick jab taken at an earlier version of my code by Martin Berz, author of COSY Infinity, when he complained about how it'd be dreadfully inefficient at computing all of the nth derivatives of a k-ary function when both n and k are large, so I went back and built a special mode for computing distinct nth derivatives. In `ad` 4, we refactored some things so that we could support monomorphic AD modes, taylored to particular concrete base types. This was based on feedback from Lennart Augustsson and others who'd tried to use `ad` at scale. The infinity-`Jet` construction based on cofree comonads, etc. is all fairly novel and not very well written up, but followed as a natural consequence of generalizing the API over all traversable functors. I hope that gives some help in navigating the maze that is the `ad` codebase. If you need more help, I'd recommend popping into the #haskell-lens channel on irc.freenode.net. Someone there can probably help you. In particular I'm usually around and `alang` has done a lot of work on `ad` as well.
I think that would be a fantastic talk to have.
I'll do it
Argonaut was written in Brisbane. Are you nearby?
What about writing the parser using Trifecta? It will tell you exactly what it expected when the parser failed and you can customise the error messages per parser. (Not to mention the highlighted output) I have written a few non-trivial parsers using Trifecta. I will see if I can write something short tonight. (edit, I am a student)
Just for reference, I am hoping for a small reusable chunk of code. The route you are discussing would not cover the "library independent" part, which I think is what makes this project interesting in the first place. For a relatively small investment, everyone can have the *option* of nicer messages. It sounds like you are saying: for a very high investment, no existing code will see any benefits.
So it's a new compoundnoun?
This is a nice idea. By the way, your liftf1 is simply function composition (.) -- specialized to (b -&gt; b) -&gt; (a -&gt; b) -&gt; (a -&gt; b) I am mentioning this, since you have added the comment "I really don't quite [k]now how to describe this in words yet." to that function. 
Nice. Reminds me that so much of the potential of haskell seems to lie in the potential for better dev tools. Types could potentially allow such amazing guides at your fingertips, like truly type aware code completion, but it seems that the tooling is not quite there yet?
I'm certainly keen to help collaborate on this, I do all my work in torch (lua DNN framework) currently. I didn't realize it had a low level C api - this is definitely the way to go. Question is, what is the abstraction for building a graph going to look like? Expressions with observable sharing? 
Use `-ddump-simpl`: https://downloads.haskell.org/~ghc/7.6.2/docs/html/users_guide/options-debugging.html#id539390
Does anyone know why there isn't a Rational ToField/FromField instance? I use a lot of price and ratio data which I'd like to avoid pushing into and out of Float values.
[reddit post of Boston Haskell talk](https://m.reddit.com/r/haskell/comments/313evx/video_framing_the_discussion_with_edsls/)
How are immutable heterogeneous arrays different from tuples? Or are they basically tuples that still allow some kind of inductive definitions on them?
you can use InDict constraint to write generic functions accept all Dicts which has certain key. in another words, it accept extensible types.
Yeah I realized that was silly for that one, but I meant the more general notion of liftN :P
Is this a job for a category-syntax? (i.e. Arrow without arr, as obviously we can't lift Haskell functions into TensorFlow functions...) 
Less bug issues per github repository.
Oh, I didn't see the "and"
Same comment as before; add type signatures to your explanations of the primitives in the API.
There are rewrite rules but they only work at compile time. See https://downloads.haskell.org/~ghc/7.0.1/docs/html/users_guide/rewrite-rules.html
I had done http://oleg.fi/relaxed-json/ in the past, it should be trivial to translate into Haskell (with attoparsec e.g.), if people would find it valuable. Yet, don't write JSON by hand :( 
That blog falls under "I don't even" category.
ide-haskell with Atom. Powered by ghc-mod. ghci with -fno-code reloads instantly when I make a change in the module which is great, but since my editor isn't integrated with ghci I don't get red error markers on my source code. I've opened an issue about this and apparently it might be due to ide-haskell not using ghc-modi but ghc-mod. Never got a confirmation on that one.
`criterion` only shows you the speed something ran, but doesn't provide much of an indicator as to *why* it ran that fast. At least with `weigh` we can see if we're slowing ourselves down by introducing bloat. It doesn't replace `criterion`, and tbh it's probably not particularly interesting without `criterion` benchmarks too.
Sorry - "(Int ~ Bool) =&gt; Int" *isn't a type*? Why not? What qualifies as a type? Surely you just mean that (Int ~ Bool) is never satisfied?
And things that can't be satisfied are type errors. What's the problem?
Yeah, acronyms can be trying at times.
You can have a `run` function that pattern matches on the string and calls the appropriate function, much as in any other language. In the case of a shutdown function, you may want to use an `if` statement in `main`, since you would want to quit from `main`.
So an example of when this kind of thing might be useful: a library could define f :: Monoid Int =&gt; Int -&gt; Int which would require the user to write their own instance for Monoid Int before using the function. It would be incorrect to reject this library code, just because such an instance does not exist yet. I'm not at all this is very good practice for a library, but I think it should be possible, if only because it feels like it ought to. Also, I understand that this case is subtly different because the constraint has not yet been satisfied, rather than will never be satisfied.
Right, which is why the data does not support the blog post's conclusions
It's not unprincipled, it's simple. If I try to define: y :: Int y = x + 2 then I must satisfy (Int ~ Bool) in order to make it disappear from the type of y. I could obviously define: z :: (Int ~ Bool) =&gt; Int z = x + 2 and then use x however I liked, but I would never be able to make it affect the executable generated, because `Int ~ Bool` is not a constraint in the type of `main`, so I would need to satisfy it before using x in the definition of `main`. Could you please be more clear on why you think this is unprincipled?
The API looks like a specialized `vinyl`. type Store = Rec ElField Is `hetero-dict` faster? https://hackage.haskell.org/package/vinyl-0.5.2/docs/Data-Vinyl-Derived.html#t:ElField EDIT: `vinyl` seems as fast http://pastebin.com/CcLLMtcE https://github.com/winterland1989/hetero-dict/compare/master...sboosali:master
gif is pronounced with a hard-g. We're not changing it.
Interestingly, this constraint *is* allowed: {-# LANGUAGE FlexibleContexts #-} -- typechecks! x :: Monoid Int =&gt; Int x = 42 I guess equality constraints are treated differently than typeclass constraints. Maybe because an instance of `Monoid Int` could be defined in a separate module, but we can't make it so that `Bool ~ Int` in a separate module? *edit*: wait, that's exactly what you said.
OK! Sorry, I should have checked that first. FlexibleContexts doesn't help with `Int ~ Bool` though. I realise that this would be essentially useless, but it seems much more consistent and principled. All constraints would be treated the same way. Type equalities would not be treated differently, even if they would never be satisfied.
It's depend on your usage, but vinyl is linked-list based just like HList, some operations will be certainly faster.
I think `3` is a value of type `(Int ~ Bool) =&gt; Int`, just as `3 :: Monoid Int =&gt; Int`. This type does have values, but they are not usable in `main`, so they can't affect your program. See my reply to yitz's EDIT.
 // The API leans towards simplicity and uniformity instead of convenience // since most usage will be by language specific wrappers. It does!
You might have a better time trying to argue for your actual use case. Indefinitely propagating an obvious type error doesn't seem like a good idea. As far as the `Monoid Int` goes, I'm guessing that since people don't like orphan instances they wouldn't like this either. If you don't allow orphans then `Monoid Int` becomes just as unsatisfyable as `Int ~ Bool`.
I think of constraints differently. `(Int ~ Bool) =&gt; Int` is not *providing* evidence that `Int ~ Bool`: it is *requiring* such evidence. Similarly, `Num a =&gt; a` is not *providing* evidence that any type `a` satisfies `Num a` (that is obviously absurd): it is *requiring* that the type you instantiate `a` with satisfies `Num a`. It just so happens that I will never be able to satisfy the requirement that (Int ~ Bool), so I should get a warning that my code is inaccessible. But the code itself is not stating the (Int ~ Bool), so an error seems unjustified.
Let's wait until visual type application become a standard ; )
And you should be able to write `Bool`s intead of `Int`s, even without `unsafeCoerce`, given that `Int ~ Bool`! This would not affect runtime safety because things that assume `Int ~ Bool` can never be used in `main`, since `main` does not have the `Int ~ Bool` constraint. I am not saying that anyone should write this code, and I am saying that they should be warned heavily about doing so by the compiler. But there is nothing wrong with the types, so there should not a be a type error (or any kind of error)!
But errors are there to prevent us from writing code that's wrong. If even you agree that it would be a bad idea why do you think the language should allow this? Say we had a mechanism that turned an expression like "1/0" into an error because it magically knows that you're dividing by zero even though the information is not available at the type level. Would you also argue that this shouldn't be an error?
I guess it boils down to the role of an error message. I think that compilation should fail with an error when the executable that would be generated if compilation did happen would be unsafe in some way: when run it could cause a segfault or something. That is not the case here. Inaccessible code implies a bad design, but it would not affect the executable. As a programmer, I would expect a warning in this case: the compiler can tell that something is a bit off, but my executable won't crash if I run it. As it happens, no: I don't think that "1/0" should result in a compilation error. A warning, sure, but throwing an exception is perfectly normal behaviour for an expression like that. I think it is fundamentally different from a type error.
But something isn't "a bit off" here. It's 100% certainly an error. I appreciate your point though. It's a difference of perspectives. To me, and I assume a majority of Haskell programmers, an error is there to save the programmer from his mistakes (among other things). A warning is something that the programmer might have a potential excuse for.
Is there a reason why people prefer Persistent and Esqueleto over more type safe solutions like relational record and opaleye?
Well put. Not deleting this just in case it helps anyone else.
Anybody working on a Haskell API for Tensorflow would have their work cut out for them. I've read that just the Python wrapper for Tensorflow has around 60k lines of code. I haven't verified this figure myself, but it seems likely.
I think if someone writes something like `(SomeTypeFamily a ~ Bool) =&gt; a -&gt; Bool`, they're *expecting* a type error if `SomeTypeFamily a` reduces to something other than `Bool`. It's a constraint on `a`, not something they want to just get warned about.
You've already got a few answers, but let me be a little more picky about what you are asking. Most people answering so far are assuming that `shutdown` is not actually a function at all, but rather an `IO` action. If that's what you meant, then /u/BoteboTsebo has written a very good answer. Your options are to either embed GHC as a library (which is what the hint package does for you), or write your own mini-interpreter. If `shutdown` is actually a function, then you need to ask what it means to you to "run" a function. This concept doesn't exist in Haskell. A function is simply an association from input values to output values. Here are some things you might mean: * Evaluate the function with specific arguments, and print the result. * Evaluate the function with specific arguments, expect the result to be an `IO` action, and run it. * Evaluate the function with specific arguments, expect the result to be an `IO` action, run it, and print its result. * Evaluate the function with specific arguments, and if the result is an `IO` action, run it and then print its result; otherwise, just print the result. The last one is what GHCi does. It's nontrivial logic, really, and I don't see anything in hint that does it for you automatically. (In general, from your question, you don't seem to understand the distinction between a function and an IO action. This is worth understanding, as in many ways it's the *most* important way that Haskell differs from most other programming languages. A function is something that depends on parameters; i.e., that has a `-&gt;` type. An IO action is something that does things. IO actions are never functions, and functions are never IO actions; they are simply different things. For example, in Haskell, `main` is not a function. There are functions that evaluate to IO actions, though. For example, `putStrLn :: String -&gt; IO ()` is a function, but when evaluated with a specific parameter, its result is an IO action.)
[removed]
That is putting it kindly. For example, I saw the [why Haskell sucks](https://www.reddit.com/r/haskell/comments/4kdxf5/why_haskell_sucks_presentation_a_summary_of/) thread a while back and the author mentioned the lack of IDE only briefly. This shows a lack of understanding of the issue. The problem is not the IDE, but the compiler services underneath that power them. I intend to make some contributions to a GPU programming language written in Haskell in the future and it is unfortunate that currently opening it in Atom causes the ghc-mod to eat up more [than 5Gb of memory.](https://github.com/DanielG/ghc-mod/issues/797) But even without that, the feedback provided by `ghc-mod` in the Atom editor is far inferior to Intellisense in VS (for F#.) This makes me wonder whether such compiler's services are maybe difficult to do in Haskell due to the complexity of the language? Haskell's Linter is quite good though, kudos to that.
I think most of what you're looking for here has already been done, with LMDB. E.g. vcache https://hackage.haskell.org/package/vcache built on top of https://hackage.haskell.org/package/lmdb Blog on vcache vs acid-state https://awelonblue.wordpress.com/2014/12/19/vcache-an-acid-state-killer/
Another example could be an IoT sensor example built with HalVM. IoT is a field where safety is of of utmost importance and HalVM can be a good fit. Thinking about it, targeting seL4 as the kernel could make a whole lot of sense. seL4 is verified and if the only bits that run atop are Haskell and tiny bits of verified extra C, then it's a nice platform to build systems that are deployed in electrical outlets everywhere, with zero consideration to a safe environment. I don't know how the TLS stack in HalVM was built but if it's done with Cryptol, it might be possible to advertize the advantage of not just being built in Haskell but known to follow the verifiable model. This would be leg up compared to other non-C TLS stacks. I believe there's a non-Mirage TLS stack which is just for research, written in F# or OCaml that would be comparable.
Did you know about the allocation counter? https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.9.0.0/GHC-Conc.html#v:setAllocationCounter It's a much more accurate (and simpler) method for allocation tracking than GCStats.
&gt; throwing an exception is perfectly normal behaviour for an expression like that. I think it is fundamentally different from a type error. I totally disagree. Throwing exceptions is not proper behavior for pure code. Instead it should be treated as the effect that it is, and tracked at the type level. (Similar to `ST` tracking reference cell mutation.)
Can you try with deferred type errors? That's almost certainly what an unsatisfiable type equality constraint is. That option will cause them to be delayed until run time.
[removed]
[removed]
I used to use `(() ~ Bool)` as `_|_` for the category of constraints in the `constraints` package. http://hackage.haskell.org/package/constraints-0.6/docs/Data-Constraint.html#v:bottom Then you could at least require a proof `(() ~ Bool)` by requiring the user pass you a `Dict (() ~ Bool)`. I've since gotten sneakier. http://hackage.haskell.org/package/constraints-0.8/docs/src/Data-Constraint.html#Bottom is truly unsatisfiable, so http://hackage.haskell.org/package/constraints-0.8/docs/src/Data-Constraint.html#bottom is even safer. 
&gt; If we want to reason about impossible constraints, we can just use Dict from Data.Constraint instead. This is my usual recommendation. If you need the user to satisfy an impossible condition, just make them pass it as a `Dict`, then you can be careful about how you open that dictionary to blow up the world as needed at the time and place of your own choosing.
I am releasing binaries, and people are sensitive to size. For example, the fact that "the Haskell Platform" unpacks to 1GB for some folks convinces them to not use it. It would be a shame to add tons of MB to my binaries without a very strong reason. Ultimately I'm not sure how many MB this issue is worth to me, but 10MB seems like too much. As an aside, parsec is at least used in *some* of my binaries, though not in the particular one that needs the JSON parsing.
I came to Haskell after reading The Free Lunch Is Over, seen FP as part of the solution. Since then I've oscillated between learning Haskell and [Return Infinity's BareMetal OS](https://github.com/ReturnInfinity). Now my goal is clearer, in order to write very fast and less buggy software, I need to be able to: * develop as long as possible at high-level Haskell * choose my modules' implementation (a Haskell common library vs my special-purpose one, or a C one) * define and install low-level callback procedures written in Haskell (e.g. an interrupt handler) * reason about and choose my memory access pattern (hopefully from Haskell or seamlessly integrated to my Haskell program) and plugin the memory manager to use * choose my runtime paradigm, i.e. use my own work dispatcher (scheduler) instead of threads My whole point is about having a development platform that lets me use the power of current hardware. To which extent does HaLVM suport these needs? More precisely because Haskell (and libc) seems to be tightly bound to a runtime model that depends on threads, and on a specific memory organization. Both seems to be the problem to tackle in order to get better performance (while keeping some of the language benefits).
Isn't it annoying that `Sub Dict :: (Int ~ Bool) :- ()` fails? I really like that you have managed to make `bottom` work, but any unsatisfiable constraint should be an initial object. Clearly GHC should never have (Int ~ Bool) be true, but it would be nice if it could reason about what would happen with it as an assumption, rather than just throw up an error. By the way, I got the idea for this post from watching Typeclasses Against the World. =)
Are you stable in Gbg? me too! do you know Got Lambda?
it is a little annoying, but when you follow the consequences of letting `(Int ~ Bool)` come into scope unchallenged through, you basically get no program without type signatures that fails to compile for lack of type checking until you go to look at main and get a massive pile of constraints you can't remove. This kinda defeats most of the point of having a type system at all, as you'd get no errors until the end when they'd show up in their least useful form and all information about the provenance of those impossible constraints would be erased.
I tested criterion for this use-case but it doesn't output accurate bytes. In weigh for `count 1`, import Weigh main = mainWith (func "integers count 1" count 1) where count :: Integer -&gt; () count 0 = () count a = count (a - 1) The proper bytes of `32` are outputted: $ stack test --test-arguments='--case "integers count 1" +RTS -T' weigh-0.0.1: test (suite: weigh-test, args: --case "integers count 1" +RTS -T) Weight {weightLabel = "integers count 1", weightAllocatedBytes = 32, weightGCs = 0} This is consistent with what is reported by `-p`. Whereas in criterion, import Criterion.Main main = defaultMain [bgroup "count" [bench "1" (nf count 1)]] where count :: Integer -&gt; () count 0 = () count a = count (a - 1) it says between 63 and 64: $ stack test --test-arguments='--regress allocated:iters +RTS -T -RTS' benchmarking count 1 time 28.26 ns (27.99 ns .. 28.51 ns) 0.999 R² (0.999 R² .. 1.000 R²) mean 28.36 ns (28.08 ns .. 28.62 ns) std dev 980.9 ps (793.8 ps .. 1.249 ns) allocated: 1.000 R² (1.000 R² .. 1.000 R²) iters 63.999 (63.985 .. 64.013) Likewise, if I measure an IO action, simply `action "integers count IO CAF 1" (return (count 1))`: Weight {weightLabel = "integers count IO CAF 1", weightAllocatedBytes = 48, weightGCs = 0} (16 byte overhead for calling IO) criterion outputs: allocated: NaN R² (NaN R² .. NaN R²) iters 0.000 (0.000 .. 0.000) y 0.000 (0.000 .. 0.000) So they don't actually return the same information. 
Have you considered building with `-split-sections` or `-split-objs` to keep binary sizes down? You might find that this makes quite a difference (at the expense of a bit of compilation time).
Do you absolutely *need* jQuery? What for? 
#\^PDF Warning ^^^I'm ^^^a ^^^bot ^^^for ^^^the ^^^convenience ^^^of ^^^mobile ^^^users ^^^[Code](https://gist.github.com/braeden123/2f44ba61ba425b1c53e443c1744058f2) ^^^| ^^^[Contact](https://www.reddit.com/message/compose/?to=bsmith0)
Or something like actions :: Map String (IO ()) actions = Map.fromList [("shutdown", shutdown), ("thing2", thing2)......]
As you've shown, `vinyl` can perform very, very well in typical use cases. Your benchmark should optimize to something basically equivalent to nested tuples, i.e. the main cost is pattern matching on the nested constructors, which is quite fast. I think vinyl falls down mainly in circumstances where the lenses can't be specialized (and therefore inlined) at their call site, e.g. when they're used in a polymorphic function that isn't itself specialized. But it's been a few years since I worked on the subject.
Snap's dynamic reloader does not work with stack because it was written long before stack existed and stack puts things in different places than it expects. If you switch from stack to cabal-install it should work just fine. Also, the problem may actually be that [hint](http://hackage.haskell.org/package/hint) (the underlying library snap uses for reloading) is not compatible with stack. It's been a long time since I've looked at the code and I'm also not the original author so I could be wrong, but I wouldn't be surprised if this is really a hint issue.
For vinyl: https://github.com/sboosali/eight/blob/faff0c89aa420b0f9a027c718ef44747a3a88fae/sources/Eight.hs There are still issues with inference and generality (e.g. not getter, but lens?). But this works: p3 = Field @"a" "a" :&amp; Field @"b" True -- :&amp; Field @"c" 0 -- numeric literals need annotation :&amp; RNil main = do print $ p3 -- {a: "a", b: True} print $ #a p3 -- "a" print $ #b p3 -- True 
Hehe I wondered that too. :)
&gt; The problem is that in this case I don't need (Int ~ Bool) at compile time But, you *do*. Let's avoid polymorphic literals and look at this: x :: (String ~ Bool) =&gt; Bool x = "3" Clearly, x only type checks if String and Bool are the same type. In fact, the constraint is somewhat redundant. OutsideIn will generate the exact same constraint for this code: x :: Bool x = "3"
GHC extension. It's a type-level operator indicating definitional equality. I believe it wasn't originally syntax and only showed up in descriptions / error messages when using GADTs.
&gt; I think this is related to the fact that type checking/inference is based on generating/propagating type equality constraints in addition to the ones written by the programmer; I don't believe there's a difference inside the OutsideIn algorithm between a type equality generated by the algorithm, one explicitly written as a constraint, and one indicated by matching a GADT constructor. I am almost certain this is the case. Type equality constraints are aggressively processed by the type checker, presumably because if it can't get rid of one, there is a type error in the program. I was convinced by this ghci session: GHCi, version 7.6.3: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Prelude&gt; :k Int ~ Bool Int ~ Bool :: Constraint Prelude&gt; :set -XTypeFamilies Prelude&gt; :t let safeCoerce = id :: (a ~ b) =&gt; a -&gt; b in safeCoerce let safeCoerce = id :: (a ~ b) =&gt; a -&gt; b in safeCoerce :: b -&gt; b Prelude&gt; let { safeCoerce :: (a ~ b) =&gt; a -&gt; b; safeCoerce = id } Prelude&gt; :t safeCoerce safeCoerce :: b -&gt; b Prelude&gt; let { safeCoerce :: (a ~ b) =&gt; a -&gt; b; safeCoerce = undefined } Prelude&gt; :t safeCoerce safeCoerce :: b -&gt; b Prelude&gt; let { safeCoerce :: (a ~ b) =&gt; a -&gt; b; safeCoerce = undefined :: a -&gt; b } Prelude&gt; :t safeCoerce safeCoerce :: b -&gt; b
But how else will it become standard practice? 
With Backpack, you could shadow `Prelude.(+)`, replacing the typeclass with a signature. https://www.reddit.com/r/haskell/comments/4i40t2/demonstration_of_how_to_backpackify/ https://www.reddit.com/r/haskell/comments/2eoett/a_taste_of_cabalized_backpack_inside_206105/?ref=search_posts
We do a sort of split. All migrations are checked into the sql subdir of the monorepo as standalone migrations, but the SQL itself is generated by Persistent's auto-migration kit.
Could someone explain to me what `(Int ~ Bool)` means? especially the `~` (operator?) in this scenario?
I have actually wanted inaccessible code to compile one time: it was when we were doing conditional compilation with `#if`. It was rather annoying: I feel like GHC should allow anything to typecheck *ex falso*.
I haven't thought that much, let me check if i can use it.
Oh, OK. I only knew the [default hoogle](https://www.haskell.org/hoogle/). Thanks for the remark. It looks like it still needs to talk to a backend, something I would like to avoid.
Anything [PureScript](https://github.com/purescript/purescript) [related](https://github.com/purescript/pursuit) :)
Servant can always use a helping hand. I'd consider myself an expert-beginner (never used haskell seriously professionally) and I was able to get in some contributions even though my knowledge on the technologies they used were lacking at the time. When I started I didn't know what TypeFamilies or GADTs were. Now afterwards I do. Which is a great success in my book. We have a few newcomer-friendly tagged issues available. But you're welcome to tackle others as well! https://github.com/haskell-servant/servant/issues I think the overall conclusion is: Just choose something you really like and start helping! Open a PR, even if it is shit. People in the Haskell community are very helpful and all have a really 'mentory' vibe. So they will try their best to guide you through and help you improve. 
Are you trying to exclusively do derivatives? In which case Conal Elliot has some great papers on the topic, check out Beautiful Differentiation.
I have a feeling that when you're at that level of understanding, you can contribute to almost any library you want, or start writing your own. Just start small, don't expect to understand everything immediately, but you have all the tools to get there, either by yourself or, even better, with help from a maintainer.
Going beyond that, you could do something like this instance (Num a) =&gt; Num [a] where (a:as) + (b:bs) = (a+b):(as+bs) (a:as) * (b:bs) = (a*b):((b:bs)*as+(a:as)*bs)) And so forth, now the derivative becomes d (_:b:_) = b and the second derivative d2 (_:_:d:_) = d
Sorry to be grumpy but what’s your point? I don’t think anybody claims that the editor situation is good, some people say they don’t need tooling support but everybody else agrees with your statement. Editor tooling is simply hard work, not particularly attractive (“not fancy enough”) and you can’t write papers about it. Everytime a new tooling effort comes around everyone is exited for a few weeks/months but the amount of people contributing is pretty small. The fact that any tooling now needs to support at least cabal and stack, maybe even cabal new-build requires a significant amount of work before getting anything of the ground that’s actually usable. Adding to that the fact that the surface language of Haskell is growing bigger with each release, making it more complicated to write editor tooling since you really need to work on the original source in most cases, I doubt anything will change soon unless more people start contributing.
Try Atom. It's far easier to set up than emacs, albeit not so fully featured. And there's hope that haskell-ide-engine will bear fruit eventually.
Try using spacemacs + stack. I had a lot of brittleness issues with emacs too and they all seemed to disappear when I switched to spacemacs. Stack makes the installation of helper stuff like ghc-mod trivial as well. If you aren't a vi keys person, you can run spacemacs in "holy mode". Everything I run spacemacs auto update Haskell mode seems to get a little better. Still has room for improvement to be sure, most notably for interactive repl usage I've written a few of my own elisp macros to do things like evaluate lines and blocks. But it is getting there. Good luck!
Well I am using `spacemacs + stack`. I have just disabled `ghc-mod` and that makes the experience much more sane. But it is a bit of a shame because I so loved the `expand show type functionality`of it.
&gt; Try Atom Last time I have tried it, it was slow and could not handle my keyboard. I might give it a shot again as the [internationalization keyboard issue](https://github.com/atom/atom-keymap/issues/35) is about to be solved. 
**EDIT**: Made a little page demo'ing the features and setup instructions: http://commercialhaskell.github.io/intero/ --- We have a WIP Elisp mode [intero.el](https://github.com/commercialhaskell/intero/blob/master/elisp/intero.el) that we've been using at FP Complete internally and so far everyone is happy with it. It works on any stack project, it's fast, automatically installs the right `intero` executable within your stack environment (with the necessary [dependencies](https://github.com/commercialhaskell/intero#requirements) on some platforms), flycheck, completion, go to definition, type of selection, simple REPL, and some upcoming features of find uses and list all types in the project. You can trivially switch the targets you want to work with with `M-x intero-targets` and specify stack targets you would normally pass to `stack build` or `stack ghci`. It already supports GHC 8. It's not ready to package up (or announce) with instructions yet, but if you know how to load a `.el` file in your Emacs then you just need `(add-hook 'haskell-mode-hook 'intero-mode)` and it automatically enables and configures flycheck and company for you. Keybindings [here](https://github.com/commercialhaskell/intero/blob/master/elisp/intero.el#L68-L71). Make sure to disable `interactive-haskell-mode` (and remove the hook) if it's enabled. Tim Dysinger made a spacemacs script too: https://gist.github.com/dysinger/63b7cd03f77ac0125356051b23f47a37 
&gt; what's your point I was just expressing some feelings and frustrations. It might be relevant because many developers might not invest in Haskell given the situation. Somehow they would be right doing so (productivity matters) which is a shame because I feel the Haskell community needs them. That's said, I am aware the problem is difficult. Truly all these shiny IDE comes from either big companies or big communities.
If I want to compute partial sum, i.e. `partial_sum [0, 1, 2, 3] = [0, 1, 3, 6]`, what's the best way? In haskell, I could do this: partial_sum :: Num a =&gt; [a] -&gt; [a] partial_sum = reverse . foldl' helper [] where helper [] y = [y] helper xs@(x:_) y = (y + x) : xs I couldn't find a FunctionalPlus function for list concatenation, but I could find fold_left and fold_right.
I'm quite happy with the emacs approach (but I removed ghc-mod for now - it's just not worth the constant fiddling for me) - in case you have not seen it: if you use `helm` you can find your way around the commands in emacs quite easily, even if you don't remember or setup many shortcuts of course I'm still at the point (don't think I ever leave it) where even with the worst possible editor/ide my fingers are much faster then my brain when dealing with Haskell. Alas no PR will fix that issue :(
For `Enumerable`, you might want to take a look at (my) https://github.com/sboosali/enumerate 
&gt; That's said, I am aware the problem is difficult. Truly all these shiny IDE &gt; comes from either big companies or big communities. It's harder in some languages than others. Haskell syntax is crazy complex when comared with e.g. Java or Go. Add TemplateHaskell and CPP to the mix and I doubt you can get a good result unless you spend as much effort as spent on Visual Studio or something at that scale. I believe a more productive choice is to learn working with an editor + the usual command line tools. I'm personally very productive in Haskell using vim, ag, fast-tags (in Hackage), and a bunch of key bindings + plugins.
If writing parsers are your thing, I could really do with help with [rdf4h](https://github.com/robstewart57/rdf4h), a simple RDF library for semantic web programming. Parsing 3 RDF formats is supported, the parsec library is used. The test suite uses the[ upstream W3C RDF parser tests](https://github.com/w3c/rdf-tests) for the 3 RDF serialisations, and rdf4h [currently fails](https://travis-ci.org/robstewart57/rdf4h/jobs/123542541) 105 out of 784 tests (105 rarely used corner cases, but failures nevertheless). Passing all tests would be considered by the W3C RDF working group as being a complete RDF implementation for Haskell.
It sounds like you'd be especially well-positioned to write tutorial documentation because you'd know enough to work through libraries but remember what it was like to learn. Hopefully this means you can fill in the potholes on the on-ramp.
Or use quotes for more "name safety" $(doit ['length, 'minimum, 'maximum])) and `show` the `Name`
Yeah, it does seem rather huge! Such is the reality of this type of project I suppose. Maybe it can be partially automated - though by the looks of it to make a really good Haskell wrapper you'd need more information, static dimensions etc. for all operations.
Very cool. I'll look you up after I graduate.
&gt; instance MonadRedis m =&gt; MonadDB m where You can't write another instance for `MonadDB`. 
Yeah class Semigroup a where (&lt;&gt;) :: a -&gt; a -&gt; a default (&lt;&gt;) :: Monoid a =&gt; a -&gt; a -&gt; a (&lt;&gt;) = mappend http://hackage.haskell.org/package/base-4.9.0.0/docs/src/Data.Semigroup.html#Semigroup
I wrote something that reads JSON into Aeson's Value type. It uses Trifecta so I get really clear errors. I wrote [easyJSON](https://github.com/Skyfold/easyJSON) in two afternoons, but I am not sure what I should expand the library to.
Right. But it was just a sketch, in real code you'll have a data type that implements necessary capabilities. I think it is clear what is wrong with accumulating constraints and how to fix it. As I wrote already in the top level comment, I'd personally prefer not to use custom monads here at all. I believe `MonadRedis`, `MonadDB`, etc are bad abstractions (e.g. they don't allow your to have two redis connections to different redis instances.) Examples: readValue :: Redis -&gt; Key -&gt; IO () writeValue :: Redis -&gt; Key -&gt; Int -&gt; IO () data DB m = DB { read :: Key -&gt; m Int , write :: Key -&gt; Int -&gt; m () } redisDB :: Redis -&gt; DB IO redisDB redis = DB { read = readValue redis , write = writeValue redis } incrementValue1 :: Monad m =&gt; DB m -&gt; Key -&gt; m () incrementValue1 db key = do v &lt;- read db key write db key (succ v) (I'd even specialize `m` to `IO` everywhere, but it is a different question.) 
Just to clarify , an "expert beginner" is someone who passed the beginner stage but thinks is an expert because it doesn't know better (but think he knows everything). You mean "intermediate".
Me too. I'm quite optimistic about progress
I like "explicit dictionaries" too. With `-XRecordWildCards`: incrementValue1 :: Monad m =&gt; DB m -&gt; Key -&gt; m () incrementValue1 DB{..} key = do v &lt;- read key write key (succ v) But I also like `(MonadFree f m, MonadIO m)` ;) As a library author, I want to expose the functionally "at all levels", with convenience. e.g. the clipboard: low-level: foreign import ccall safe "Clipboard.h getClipboard" c_getClipboard :: IO CString getClipboard :: IO String getClipboard = c_getClipboard &gt;&gt;= peekCString medium-level: data ClipboardD m = ClipboardD { _getClipboard :: m String , _setClipboard :: String -&gt; m () } osxClipboardD :: (MonadIO m) =&gt; ClipboardD m osxClipboardD = ClipboardD{..} where _getClipboard = liftIO $ Cocoa.getClipboard _setClipboard = liftIO $ Cocoa.setClipboard -- platform-specific reverseClipboardOSX :: IO () reverseClipboardOSX = runClipboardT osxClipboardD $ do c &lt;- _getClipboard _setClipboard (reverse s) high-level: data ClipboardF k | GetClipboard (Clipboard -&gt; k) | SetClipboard Clipboard k makeFree ''ClipboardF -- platform-independent reverseClipboard :: (MonadFree ClipboardF m) =&gt; m () reverseClipboard = do c &lt;- getClipboard setClipboard (reverse s) I can't use raw `IO`, as that may hide linking to platform-specific bindings. http://www.haskellforall.com/2012/05/scrap-your-type-classes.html 
Thank you. However the name appears qualified using `show`, using `Language.Haskell.TH.nameBase` works in this case.
Hey /u/snoyberg thanks for the tip. And thanks /u/mgsloan for jumping on this. Doing stack build in the local project does not seem to help though. I even tried force-dirty but nothing seems to happen. I there a way that I can remove everything and start from scratch? Would that be recommended? 
I used to use ST3 but it was very very fragile at the time and I had to switch to Atom. The last time (not so long ago), I tried to get ST3 to work again but haven't succeeded. Are the main showstoppers sorted out now?
This is exactly what I was looking for. This is going to teach me about deriving my own generics, so thank you.
Noo I wasn't trying to do derivatives really, I didn't have anything in mind actually I was just curious if there were any interesting ways to leverage thunks 
Amen - the answer is avoiding GC. At least for now. Our compact Normal Form work is continuing and we now have mutable data structures inside compacts. They serve as separate mini-heaps with internal free-lists, but without using FFI and while retaining existing GHC in-memory representations. We'll post on this as soon as we've completed our implementation of this benchmark.
fpcomplete plz make good ide 4 us vim users 2 \- vim user
Welcome :-) You can also find instances for comparing and enumerating functions, in https://github.com/sboosali/enumerate-function The functions themselves (e.g. `enumerateFunction`) are exported too, so you don't need to import the orphans. docs are here http://sboosali.github.io/documentation/enumerate-function-0.0.0/Enumerate-Function.html
Our Vim devs seem to be switching to Emacs + evil-mode!
If you're going to go down this path, `Options` would be better named as `OptionSet`, then you get &gt; `getWith` takes an `OptionSet` ... which actually reads properly. "`getWith` takes an `Options`" causes a bit of cognitive dissonance for me - my brain immediately says "an Options what?", which is exactly this question!
There is my `digestive-functors-aeson` too. I think it's a bit higher level than this, but is designed to do "form validation" over a JSON structure. The idea is you get back a response object that is the same "shape" as the submission, but the values are replaced with errors, if any errors occured.
[hledger](http://hledger.org)! Hardly a lens or free monad in sight, and it's really fun to work on something that brings practical value every day. In this project you can get experience with any of: parsers, pure reporting/calculation code, command line interfaces, curses-style interfaces, yesod web apps, servant web apis, client-side web UIs, packaging, docs management, Shake, doctest, shelltestrunner, benchmarking etc. Drop by the #hledger channel on Freenode for support.
Please write this post!
Yeah that uses pretty much the same things internally, but it’s nice to have it packaged up.
Listen to this man. Please make this program even better than it already is.
Tim is one of my colleagues and he made the spacemacs integration: https://gist.github.com/dysinger/63b7cd03f77ac0125356051b23f47a37 A bunch of our devs are using spacemacs. Basically none of them use ghc-mod because it never works. 
I'd add the note, in case others haven't seen it, that CPP is usually used in Haskell for simple optional inclusions/exclusions guarded by GHC version flags and the like. Usually to keep the project warnings clean or compatible with different library/GHC versions.
&gt;Truly all these shiny IDE comes from big companies FTFY. What shiny IDE comes from a big community?
Thanks for this reply! These are great points in favor of TH. Although I must admit it never seemed as though TH had more accurate line numbers. Additionally I guess I'd be more amenable to TH if it could generate modules. Appreciate your take on this. It gives me a lot to think about.
Still, a hairsplitting remark and not really helpful for the OP
My preference is also "typeclass" for similar reasons as yours.
Just a note, for me `hdevtools` is much faster and more reliable than `ghc-mod`. I am a vim user though ...
Just a shot in the dark, but: &gt; .stack-work/install/x86_64-linux/lts-6.1/7.10.3/bin/myproject means that the current GHC used by Stack won't be on your PATH, which may cause problems (though I'm not certain, I don't know how Snap's reloader works). I'd try running `stack exec myproject` instead.
I'm very happy to take patches to almost anything I work on, especially documentation. I mean, how else better to show you understand something than to document it and load it up with doctest examples? =) https://github.com/ekmett?tab=repositories
You can work on anything. The more important question is what projects are you interested in working on. Motivation is a really important factor. Find something you're interested in and offer to help.
I would probably recommend making it a separate package. If your public package needs to depend on your private package, then you can try to abstract the interface somehow (maybe with a type class) and put that into the public package. Then you can write make your private code depend on that package. Even if there's an inversion of control going on such as with a web framework where your public app might be the one defining the main function, there's almost always some way to set up another inversion of control so that the public code doesn't need to depend on the private code. Instead of putting `main :: IO ()` in your public app, you would use something like `mkMain :: Foo -&gt; IO ()` instead and the private code could call it supplying the `Foo` (which could be a function, instance of the type class, or whatever is needed) to impart the private functionality.
What's the difference between this and Haskell mode?
Yes the situation right now is quite painful. If you are lucky and have the right kind of project, an IDE will work for you, and you'll feel happy for a short time until it breaks. The tools are very fragile due to the wide range of conditions existing in the wild (cabal, stack, multi-package projects, different platforms, how you start the IDE, etc.) And, there's huge potential. FYI there's an IRC channel where users of all IDEs/editors can support each other: [#haskell-ide](http://webchat.freenode.net/?channels=haskell-ide). Though it's small, questions here will not be lost in the noise. In the topic we keep links to the current best setup guides for each IDE.
I'll probably go with package way, but the problem I can see is how to get the private package depends on the public, when the private depends also on the publick (even though not on the same bits. Only main depend on private, but main and lib are in the same package). The second problem is how to do the dependency injection (or how to actually make the app use the correct private package).
The public package should not depend on the private package at all. If it does, then you need to pull out an abstracted form of the private package and into the public package so it can eliminate that dependency. Once you have done that, you make the private package have its own main that calls the "mkMain" function in the public package supplying arguments from the private package.
I'm looking for help to work on [sql-fragment](https://github.com/maxigit/sql-fragment) a type safe sql combinator (without monad) and with auto-joins. I haven't touch for months so the project look a bit dead. I've been busy and I was waiting for injective type families. (Now the problem is to migrate from GHC 7.6 to GHC 8.0) 
It's not a full IDE, but ghcid is quite nice. I used it while developing a small Servant app. Relaunching the server automatically on each save is very convenient.
Well, that and (reading a few of the other posts), it would be *very* weird if a compiler allowed you to compile a library with a function whose definition obviously[1] precludes *any possible* definition. (Yeah, yeah, warning vs. error, but personally I think such a situation qualifies as "error".) [1] As in: it can be proven trivially.
GHCJS?
Ok, so, show us all how that can be done. Everyone in the Haskell ecosystem will be forever be grateful! The real answer is that it *cannot*... unless a) your language is incredibly simple (LISP), or b) you have huge amounts resources (see: Java because popularity), or c) you engineer this into your compiler (see: TypeScript)
Where has this been all my life??
This is great! i like cmus but will try this one :)
I found that [Pandoc](https://github.com/jgm/pandoc) is a great place to start. E.g. it would be awesome if someone wrote an asciidoc reader. It's nice to get started because a lot of code can be taken from readers for other formats, yet things still need to be understood, adopted and changed to suit the new format. I wrote the Org-mode reader and learned a lot about Haskell development that way.
I find it difficult to answer without knowing what you are actually doing. I want to say that you should stick with template haskell and maybe a custom quasi quoter but maybe you're doing something I haven't anticipated.
&gt; I need GHC to assume (String ~ Bool) when type checking x I'm not an OutsideIn expert by any means, but I'm almost certain that there isn't a "stack" structure to the generated constraints that would allow you to discharge one temporarily.
You could drop the article, I guess.
More on why it necessarily violates referential transparency. The reason it can be done in lisp macros is that they operate (if you squint) on the level of (logical) references, not referents, so there's a distinct language at expansion time versus runtime. They're similar, and in fact lisps usually let you manipulate expressions and symbols at runtime, but they have strikingly different operational semantics. I'm not super familiar with template haskell, but the symbols are the operands in the kind of matcher you're looking for, and maybe that'd work (although it's still inherently referential opaque, since it's a function over references). Now, what you're trying to do is actually fine as long as your tool accounts for the relevant symbols. Happy hacking!
Doesn't run on my Debian system: haskell-player: afinfo: createProcess: runInteractiveProcess: exec: does not exist (No such file or directory) Maybe put system library dependencies in the `.cabal` file for `afplay` and `afinfo`.
Good to see the market expanding! If you want a Haskell job in Singapore and can deal with working in a bank, Standard Chartered is also worth looking into. (Don Stewart's team is hiring. One of the best bosses I ever had.) Best of luck with your startup. Singapore is a great place to live, work and pay taxes.
Existential quantifiers let you type all kinds of weird stuff. (In any case what cdsmith wrote applies.)
Hmm, the code doesn't seem very idiomatic. But that might be because the meat of the code is actually trying to do something rather imperative (and thus can't look like most Haskell code), not because there's anything wrong with the code itself. See https://gist.github.com/matthiasgoergens/6ec7f88825cb766b36c4cd7715687e0b for some suggestions. (Beware I haven't tried compiling that code.) Also, why don't you just stick the three cases of 'loop' into their own looping threads and rely on STM?
Ok, I'll quote https://www.reddit.com/r/haskell/comments/4m1in9/pragmatic_haskell_lecture_2_databases/d3s5mwb &gt; &gt; Automatic database migrations &gt; &gt; I really don't understand how breaking your database on you is a feature. To be clear, this is the incorrect comment that sparked this discussion.
We did plenty of CRUD apps at Standard Chartered with a Haskell-like language. Works well---and keeps them as 'boring' as they should be. No 'exciting' bugs.
Money made per hour?
Or with foldr: partialSum l = foldr (\a rest acc -&gt; a + acc : rest (a + acc)) (const []) l 0 scanl and scanl1 are more specialised than foldr, hence faster to grasp when reading. But foldr still beats foldl and foldl' if you want to work on infinite lists (or otherwise need to preserve laziness.) Would be interesting to know how well the foldr approach would translate to C++.
If you want that kind of IDE I bet the maintainer of [Leksah](http://leksah.org/) wouldn't mind some help updating/maintaining/funding.
&gt; If you want that kind of IDE I bet the maintainer of Leksah wouldn't mind some help updating/maintaining/funding. Sounds like a very safe bet :-)
Have you tried Leksah. What was top on your list of things we need to add or fix?
Could this be an outdated version of stack?
I wish ORMs spent more time talking about the schema they produce. When they don't it leads me to believe the outcome isn't great. I'd love if more ORMs were configurable at that layer (it seems persistent has some support for this). Then you could support multiple consumers of one database without having all but one be subjected to the garbage that the ORM outputs. I guess this is why I prefer a more verbose data mapper model to something sugary like active record.
You can install the Haskell Platform onto Android?
Also, I have TH deriving of storable here - https://github.com/fpco/th-utilities/blob/master/src/TH/Derive/Storable.hs . NOTE that this ignores alignment concerns. Plans to handle alignment here https://github.com/fpco/store/issues/37 These are exciting projects! Perhaps I do not understand the scope and goals of the Storable / Prim project, but if I can write a TH prototype for it in a day perhaps a summer long project should include more scope?
I see the page says in big letters **Designed for Stack**. Does this mean I'm out of luck trying to get this to work without Stack?
One thing that Leksah will never be able to fix is the whole issue of having to learn a whole new tool for a single language. It shares that downside with most other IDEs though.
Why not make those lifting instances `{-# OVERLAPPABLE #-}`? 
The executable `intero` itself works with `cabal repl --with-ghc intero`. The Emacs support just assumes `stack` projects. I'm afraid I have ever-shrinking finger bandwidth, I couldn't make and maintain a seamless user experience without all the battle-testing and sensible defaults that went into `stack`.
Thanks for the report, I think it was probably the `--no-build` flag I'm passing. 
So it seems that protobuf V3 is supported, at least partly. Am I correct ? Alain
It's likely I'll implement that too. And dependency management. Life is too short to manage imports and dependencies manually!
Yeah, I'd like it if Haskeller Vimers switched to Emacs+Evil or Spacemacs, not because I want to win them over (Spacemacs is nothing like my regular Emacs anyway, it's more alien than Vim), but just because there's more work done on Haskell productivity on the "platform" of Emacs.
Haskell-mode provides basic syntax highlighting, indentation, editor features. Intero provides the checking, "intellisense" type of stuff.
Hah, I just realised it sounded like testimonial indeed. But it's really the case, feel free to!
For the record, &gt; This is not an official Google product.
I will try out stack once I figure out how I can tell it to use my local GHC installs, because the builds Stack downloads for Linux don't work everywhere (here). If Stack used statically linked musl builds of GHC, then the GHC download would run on any Linux distro.
&gt; I will try out stack once I figure out how I can tell it to use my local GHC installs Stack will use your locally installed GHC if its in the PATH. It's only `stack setup` which downloads and configures a GHC appropriate for the resolver. Obviously, it expects the correct version for the right package set, which is why it might reject your local GHC.
Would you consider providing musl-static builds of GHC? Those will run on any Linux. If not, what's the blocker?
Looks interesting, it would be nice to publish it to Hackage.
Your "not that insane" editing features aren't all that easy to actually write, and effort is spread around a number of projects which slows all of them down (I don't think we'll ever agree about editors!) I find Spacemacs to be pretty solid, but I don't use enough Haskell to really pine for the kind of features ReSharper gives me at work for C# (but have you seen how much ReSharper costs…)
Yes! The Intero flycheck checker requires haskell-mode in the first place, so it's intended to work with the basic haskell-mode mode. The conflicting part is `interactive-haskell-mode`, but that's a minor mode that you have to manually enable anyway, so you just disable that and there's no conflict. **EDIT:** I added some code so that when `intero-mode` starts, it disables `interactive-haskell-mode` if it's active in the current buffer. Saves time.
TL;DR: we wouldn't turn this down if someone provides us a bindist and we can demonstrate that it works well. Feel free to [open an issue](https://github.com/commercialhaskell/stack/issues/new) to discuss. I general, we (the Stack team) prefer to use [official GHC bindists](https://www.haskell.org/ghc/download), since we can be sure that they're well tested and will be maintained far into the future. However, we are willing to accept alternate bindists from the community when there is a good reason to do so and have some assurance that they'll be maintained, and this sounds like a good reason. It's just a matter of someone putting in the effort to make a bindist that installs the normal way (`./configure --prefix=/path/to/ghc &amp;&amp; make install`) and demonstrating that it works with most Stackage packages. We'd also prefer that people then work toward getting their changes into upstream GHC so that there can be official GHC bindists in the future. I haven't had a chance to try out a musl-compiled GHC, but are you certain they'll work on all distros and will be able to generate regular dynamically linked glibc+gmp binaries as well? My vague understanding is that there may be problems with musl-linked programs using glibc-linked shared libraries, which then means that any shared library installed by a non-musl distro wouldn't be available. So if the musl GHC can only link with musl, that wouldn't be viable as a default. However, it would make a lot of sense as an easy option to use for people who want to build fully static binaries (there's some discussion of this in [here](https://github.com/commercialhaskell/stack/issues/1032)). 
I'm really in favor of the tooling being more liberal while also correct. Firstly, `-fdefer-type-errors` presents errors as warnings instead of proper errors, this leads to misleading results in the current error/warning machinery. Fortunately, on recent GHC 8, the warning is accompanied by "[-Wdeferred-type-errors]", which lets us identify the message as a true error and not a benign warning.
Maybe a bit off topic but SICP has a chapter on turning lisp into register machines; might give you some hints about the relationships between function and state.
The [straight-forward way of translation your code](https://gist.github.com/Dobiasd/3f62b4958b6b579a5d7ccf6614973805) fails as expected. But then I have to admit I do not understand what you are doing there. Would you mind explaining? :D
It responds to every input by complaining that it couldn't solve the dependencies. Sorry, I had to. I think parent's comment is supposed to be a joke.
Doesn't look too performant, judging from the liberal use of [attoparsec](https://github.com/google/proto-lens/blob/master/proto-lens/src/Data/ProtoLens/Encoding/Bytes.hs#L74), [lists, `Map`s and `String`](https://github.com/google/proto-lens/blob/master/proto-lens/src/Data/ProtoLens/Encoding.hs) (tags are `Int`s so that's a prime `HashMap` use-case), and (kinda weird) double allocations [like this](https://github.com/google/proto-lens/blob/master/proto-lens/src/Data/ProtoLens/Encoding/Bytes.hs#L74) -- why cast a word to a double instead of just parsing into a double directly? On the other hand it's not on Hackage and there's no benchmark suite, so the author's probably still at the "Make it work" phase, and I expect there's a factor of just talking to some of Google's internal streams--which I'm sure are quite interesting--from Haskell, regardless of performance. Author seems to be our very own Judah Jacobson of Haskeline. :-)
This is one of the better resources I've come across in the subject from the opposite way round: http://blog.moertel.com/posts/2013-05-11-recursive-to-iterative.html Not exactly what you asked, but it can sometimes be useful to look at things the other way up :-)
Hmmm this is an interesting issue. I'm not sure what the workaround is in the meantime though.
What a great way to fund the work! On the other hand, I always supposed that Haskell would be a natural fit for a Nix rewrite.
&gt; I'm locally assuming that I have discharged Monoid Int, right? No. There are two options for the compiler to solve this. First, static instance resolution. The `Monod Int` constraint completely disappears from the runtime representation, and the `&lt;&gt;` is "inlined" one layer. Second, dynamic instance resolution. The `Monad Int` constraint turns into a `MonadD Int` argument, and the `&lt;&gt;` is looked up from the record passed in. data MonadD a = MonadD { mempty :: a , mappend :: a -&gt; a -&gt; a , mconcat :: [a] -&gt; a } (Okay, `&lt;&gt;` is always inlined, and not an field in the record. Instead I was talking about the `mappend` field.) In both cases the constraint is globally (not locally) discharged via the transform. I haven't looked into the GHC implementation at all. But, I'm actually fairly sure that the second option is always used initially, and sometimes it gets converted into the first option by the optimization passes. The first option is only valid because of instance coherency. The first option is still available in the case of type equality constraints. The second option is not, because types are always[1] erased from the run time. [1] No guarantees.
&gt; http://www.thecrunchyfrogcollective.com/impromap.php Probably the wrong URL?
No, sadly not. It's not really so easy to make a library which seamlessly uses a GPU for such things, mainly because GPU memory is separate from the main memory - if you're doing some computation on the GPU you need to keep your intermediate values in GPU memory too.
For parsing text, it's pretty good comparatively speaking. For deserializing a pretty-structured stream of binary data? It's merely okay.
OK, what about `F a ~ b` for type family F and variables a and b? Surely that's locally assumed?
Um, you're right!
I feel like I've seen a lot of these for your group. Are you having trouble filling them or are you just growing that quickly?
&gt; Are you having trouble filling them These are some of the best Haskell jobs on earth, working with some of the best Haskellers on earth. Seems unlikely that there's any trouble filling them ...
Step 1: Look up all words starting with "co" and learn what they mean.
Each post is for new and different roles. 
Do you really need the `DrawWindow`, or are you asking how to draw to a custom widget? For the latter, I've had success by just listening to the `draw` signal, and from there you can retrieve the Cairo context and do whatever you want: http://hub.darcs.net/ocharles/brush-level-editor/browse/Editor.hs#435. For the former, I seem to be getting the draw window by just using `widgetGetWindow`: http://hub.darcs.net/ocharles/brush-level-editor/browse/Editor.hs#162
What would you use instead?
This is a pretty good paper on inductive graphs: https://web.engr.oregonstate.edu/~erwig/papers/InductiveGraphs_JFP01.ps.gz I mention them because it gives you a really good idea of how to a implement data structure that can seem difficult for an imperative programmer to wrap their head around in a purely functional context. edit: I might add that anything you can create in haskell using function composition rather than data structures is usually conceptually superior. So if you are moving from OOP to haskell your first inclination is to write a bunch of data declarations or typeclasses. The more haskell way of thinking is to write a bunch of type definitions for functions that compose well, using data declarations extremely sparingly. If you explain more about how you are approaching your particular problem I can show you more about this.
Unless standards are high and would-be applicants value living in places with sunlight :)
There's a very simple workaround: mkdir foo cd foo snap init cabal sandbox init cabal install -fdevelopment .cabal-sandbox/bin/foo ...and reloading works perfectly.
&gt; Dynamic reloading / ghc api things in general seem somewhat busted when linking in code that ties into specific external c code. I don't believe it's a GHC issue. The reloader works just fine for me when I build with cabal as we can see from the workaround I mentioned in my other comment in this sub-thread. Stack is doing something to cause the problem here.
It is an effort of multiple people, lead by Judah Jacobson, and it is used by couple of unofficial small projects internally.
The Atom setup is especially good at auto completion. With stack, vim + ghc-mod takes a fairly long amount of time to get the completions, freezing up the editor meanwhile. Atom + ghc-mod doesn't take as long, and doesn't freeze at all.
Gotcha. I was confused by the talk of usage of `hint` and ghci being involved.
What's odd about them?
I am the person to talk to on the snap side. I certainly would like it if we could get dynamic reloading working with stack. However, I don't personally use dynamic reloading and most of my projects don't use stack; and since using cabal-install is a very straightforward solution to this problem that works today, this issue will probably not be very high on my already very full todo list. That being said, if you or anyone else can provide a pull request that fixes this issue without regressions, I'll do my best to see that it gets merged in a timely manner.
Oh, I don't know. Anyway, thanks for the discussion. I'm now happyish with an error.
I guess Python's killer-feature for scientists is SciPy or SageMath. Heck, even Julia may be more convenient for numerical science than Haskell currently is which is rather sad
Well, the submitted implementation uses very minimal backtracking, seemingly out of convenience, and from the Encoding document, it's not clear that the `protobuf` needs it. Given that, I wouldn't necessarily choose attoparsec which pays a performance penalty for [always backtracking](http://hackage.haskell.org/package/attoparsec-0.13.0.2/docs/Data-Attoparsec-ByteString.html#v:try), and generally [carries around error information](http://hackage.haskell.org/package/attoparsec-0.13.0.2/docs/Data-Attoparsec-Internal-Types.html#t:Parser) which you probably don't need in a binary protocol. But maybe performance isn't a priority in the project.
Hmm, that could be the case. I just tried it on a linux machine and reloading works fine there too. But it has gas version 2.23.1 and I didn't see an easy way to get &gt;= 2.26 on my distro. So I suppose that could still be the source of the problem.
3Mloc is a lot. But I don't understand what's weird about that. People who program have been on the trading floor for many years now, maybe 30. 
Did you read Don's ad? It says about 16 experienced Haskell developers. And probably 100 who use Haskell now and then. 
I [just changed](https://github.com/Dobiasd/FunctionalPlus/commit/641dfcb18d457da6a002182dadfb260bf37d3f82) the signatures to the uncurried versions to better reflect the real C++ type and to make it more clear when the function really is curried for usage convenience, like e.g. [cyclic_value](https://github.com/Dobiasd/FunctionalPlus/blob/64f5ef6e13841fcb3120b11b4f94716fe77fb205/include/fplus/numeric.h#L250). This made it easy to allow tuple element permutations in the compatibility check. I.e. you will now find `fplus::transform` no matter if you search for `((a -&gt; b), [a]) -&gt; [b]` or `([a], (a -&gt; b)) -&gt; [b]`. Thanks again for your feedback. It helped me improve the search UX.
Biased, since I worked on it and it's new, but I'd use [store](https://github.com/fpco/store). Built from the ground up for performance. We didn't think to compare it to attoparsec, but it is quite tough (if not impossible) to be as fast as store and also support laziness + backtracking. That said, if you use `Store`'s internal APIs, you could certainly implement your own backtracking. Opened a ticket: https://github.com/google/proto-lens/issues/5
Thanks, for the useful feedback! Yes, I didn't test the case of files you can't play. There's a TODO to filter the list of file based on their extension, but that's not a very robust approach. What do you mean by "show duration, position and progress in a nicer way"? Can you make an example? Would you like to open some issues in the repo to track progress on these tasks?
[binary-serialize-cbor](https://github.com/well-typed/binary-serialise-cbor) takes a [remarkably novel approach](https://www.youtube.com/watch?v=Mj2cXQXgyWE) of creating an AST for serializing and deserializing. I think this approach could work for protobuf as well.
&gt; What do you mean by "show duration, position and progress in a nicer way"? Can you make an example? for example, `duration: 161.750167 - position: 54.0 - progress: 0.33384818` will look better as: `00:54 / 02:42 ~ 33%` I don't mind opening these as issues.
And some of us occasionally stand - trading floors now have standing desks. It's not like in the 80s movies anymore. 
Uh so Intel hasn't released any OpenCL drivers for OS X in a long time.. which is quite sad to be honest. If you want, there are linux drivers for Intel Iris however it is only tested on CentOS. If you want to install that, drivers are [here](https://software.intel.com/en-us/articles/opencl-drivers#latest_linux_driver). You can also instal windows and there are drivers for that. Unfortunately there is no way to native run OpenCL on OS X right now.
[Done](https://www.reddit.com/r/haskell/comments/4mfamu/sublimetext_3_haskell_in_9_steps/)! 
/u/mightybyte I tried the solution you posted above and replied to it. Sadly using cabal doesn't make any difference it's still the same error. It's related to the bug that /u/sclv posted.
[removed]
It's interesting that Okasaki uses ML as a primary language in the book but then has to reinvent lazy evaluation for the actual algorithms. That being said, it's enlightening and full of smart solutions.
Ooh, thank you for this information. I was not aware of failures on Linux until now. If you use IRC, come to #snapframework and we can investigate this problem in more detail.
&gt; They have three million lines of Haskell code. And Don read every single one of them. (At least he read through _all_ the commits when I was there. I wonder if he ever sleeps. Probably vegan superpowers or something.)
Nice to see someone else toy with this kind of thing! Have you thought about using a typeclass for the stack and/or cpu? I implemented parts of a Lua VM a while ago and for most things I used a similar approach. (Stack as part of the cpu, bad data structure choices). In my case using typeclasses for the CPU/Stack structures would have made it trivial to switch to better underlying structures later on.
does it have stack support?
I really wish I could remember the name of it, but a month or two ago someone made a parser combinator library with no backtracking whatsoever. Turns out, that makes parsing structured data like JSON way way faster.
The point was that once you move on to the next language you have to start from scratch again, unlike e.g. when using an editor like vim or Emacs where you can transfer 80-90% of your editing tool knowledge from one language to another (e.g. basic editing, snippets, indentation, completion, compile error displays in the editor,...).
&gt; We have around 3 million lines of Haskell, and our own Haskell compiler.
Only in London. In SG I've yet to see a standing desk on the trading floor, and ~3 years ago two people we both know actively got denied standing desks.
Maybe they like chewing gum?
It look like they install stack but never use it or mention it again. Anyways, I have heard that stack does work with sublime.
Yes it works with stack out of the box
It uses stack by default. That's what gives the compilation errors 
It's https://hackage.haskell.org/package/scanner . 
Or at least rust 
I would still love to see this released on Hackage though!
If we can already use proto-lens with Stack so easily, what difference does it make whether this is uploaded to Hackage?
Often a direct conversion is not what you want, and doesn't result in the most idiomatic interface to the converted code. Functional programming does have room for imperative algorithms. A direct conversion is always possible, if a little inelegant. Lennart Augustsson has written a handfuld of blogposts on the topic which are readily Google-able, but I am on mobile so I won't :) However, there is a very enlightening series of posts found [here] that describes a different technique; perhaps a more principled approach to imperative algorithms in a purely functional setting. Concretely the posts implements a Union-Find interface, which is Monadic (which is almost imperative, as is ;) and then proceeds to give both pure and mutating implementations. [here]:http://tel.github.io/posts/mutable_algorithms_in_immutable_languges_part_1/
Well as far as I understand it, they basically don't support compute shaders at all unless you want to use Metal, which doesn't have much appeal. IIRC the latest OpenGL you can get on OS X is 4.1.
It's not too stressful, but it can be noisy and sometimes requires fixing things quickly. Generally though we write stuff that just works which makes things less stressful!
Yeah, right. But I have been thinking about having Stackage nightlies be able to refer to git commits directly. That way Stackage could benefit from fixes and bound-relaxations as soon as those hit their git upstream repos. It often takes quite a while from the time a fix is merged upstream to the time it ends up on Hackage
Will this work on a network drive? I write on my windows host but the files are kept inside a VM where they are compiled. We also use Stack... these 2 things seem to have prevented me from using other solutions in the past.
Because GPUs aren't everything? Macs come with a lot of desirable stuff. I wouldn't trade mine for anything. Graphics are a bit underpowered, I guess. But that's never been a problem for me. Everything runs perfectly, except for some lower quality gaming graphics.
Thanks for your reply! I'll definitely give it a go on HackerRank and Project Euler problems. I'll also try to answer StackOverflow questions since that's mentioned in Don's post. A mock interview would be great, I'll surely get in touch once I have been practicing for a while and I feel more confident about my skills. Since you used to work there, I have a few more questions. Is there any difference between the roles in London and in Singapore? What's the relationship between the two teams? Is it possible for people to move between the two? Thanks again for your answer, it was very appreciated.
Do you mean run-time dependencies? Actually Haskell produces native executables with very few dependencies. See [Haskell Web Server in a 5MB Docker Image](https://www.fpcomplete.com/blog/2015/05/haskell-web-server-in-5mb).
Let's figure it out! So first of all, how are `(||)` and `(==)` even working in GHC 7.10? Value-level functions don't get promoted to the type level, at least not in a form in which they evaluate their arguments. Ah! It looks like those are both type families defined in `Data.Type.Bool` and `Data.Type.Equality`. Okay, I can imagine how I'd implement `(||)` using a type family, and also `(==)` on booleans, but `(==)` on Strings? There are way too many Chars, no way they are all enumerated in the type family's definition. Maybe it's predefined by the compiler? Taking a closer look at `Data.Type.Equality`, I see that `(==)` is not even defined for Strings! It is defined for Symbols however. And indeed, if I add the type annotation `(x :: String)` in the definition of `Contains`, I get the following error: ‘String’ of kind ‘*’ is not promotable If I annotate it with `Symbol`, it complains that `Symbol` is not in scope. Where is `Symbol` defined? In [`GHC.TypeLits`](https://hackage.haskell.org/package/base-4.9.0.0/docs/GHC-TypeLits.html). After importing that, the annotation is accepted. Okay, let's remove the kind annotation and switch to GHC 8.0.1: is `"x"` a Symbol or a String in 8.0.1? Installing GHC 8.0.1... Er, the code works just fine on 8.0.1! What are you complaining about? Correction: *my* code works just fine! Yours does indeed fail. And now that I've removed the kind annotation, the *only* remaining difference between my code and yours is that I've imported `GHC.TypeLits`! Why would that make a difference? Let's add the kind annotations back. String is still not accepted, and Symbol still is. And if I ask for a clearly-incorrect kind like `Int`? Expected kind ‘Int’, but ‘"x"’ has kind ‘Symbol’ Okay, so this confirms that type-level string literals like `"x"` are indeed Symbols, not Strings. Hmm, I wonder, is this still the case if I remove the `GHC.TypeLits` import? Expected kind ‘Int’, but ‘"x"’ has kind ‘ghc-prim-0.5.0.0:GHC.Types.Symbol’ Okay, so the import did not have an effect on the type. What else does the import bring into scope... Oh, did it perhaps have an effect on whether the `(==)` implementation for Symbol can be found? Let's see what kind of error message we get when we use a type for which `(==)` is not defined: data XY = X | Y bar :: (Contains X xs ~ 'True) =&gt; MyType xs -&gt; () bar = const () -- Couldn't match type ‘('X == 'X) || ('X == 'Y)’ with ‘'True’ -- arising from a use of ‘bar’ main :: IO () main = do let _ = bar (MyType :: MyType '[X, Y]) return () Bingo! That's the same error message you're getting. I think I now have enough information to reproduce the problem using my own custom type family, to make sure none of the builtin stuff for Symbol or `(==)` is involved: {-# LANGUAGE TypeFamilies #-} type family ConstUnit a type instance ConstUnit Bool = () -- type instance ConstUnit () = () foo :: ConstUnit a ~ () =&gt; a -&gt; () foo _ = () main :: IO () main = do print (foo True) -- compiles fine print (foo ()) -- Couldn't match type ‘ConstUnit ()’ with ‘()’ See? When a type instance like `ConstUnit ()` is not in scope, the compiler doesn't complain that `ConstUnit` does not apply to `()`, instead it simply doesn't evaluate `ConstUnit ()` further and complains that this unevaluated form is not syntactically-identical to `()`. The same thing happened with `(==)`: without the import, GHC did not further evaluate `"x" == "x"`, which caused it to not evaluate `("x" == "x") || ("x" == "y")` either, which is why it complains that this expression is not syntactically identical to `'True`. So it seems that GHC 8.0.1 no longer implicitly imports `GHC.TypeLits`. I don't know if that's an intentional change or if that counts as a regression.
An odd combo - what compiles? The VM or Windows?
The VM, we build and deploy to Linux. Makes sense to build and run in dev to Linux too, I just prefer my Windows environment.
Maxwell has 1:32 DP:SP ratio, Maxwell is a gamers architecture. The $200 R9 380 has more DP performance than a Titan X. So no, Nvidia isn't straight better.
when you have ghc-mod bailing out, sometimes M-x ghc-kill-process makes it work again ..
&gt; The whole thing has been dressed up in pseudo-Latin/Greek terminology in a way that makes people run screaming saying "I don't know category theory", when almost none of the terms are category theoretic and are just as alien to a category theorist. Guilty as charged. I've spent a bit of time reading up on the topic and found the following resources to be a fairly beginner friendly introduction: * http://blog.sumtypeofway.com/an-introduction-to-recursion-schemes/ * http://blog.sumtypeofway.com/recursion-schemes-part-2/ * http://eprints.eemcs.utwente.nl/7281/01/db-utwente-40501F46.pdf * http://stackoverflow.com/a/36911924/841562
What are the efforts to make contribution easier ? A lot of people contribute to, say, Stack to which all the points you raised apply. What's made differently with IDE's tools ? 
When F# finally had compiler services, which was a question best done by upstream experts both for technical and for trust reason, it enabled a lot of extensions to come to life. Haskell has lots of potential in that space, but it requires some industrial, coordinated, engineering vision, to deliver it and rassemble the scarce manpower. As you spoted, Haskell has many use, one of which is a meta-algorithm to produce research papers, and that is a very distinct matter from producing fine, usable tools with appropriate defaults and sensible tradeoffs.
how is it possible, that nobody has mentioned Haskforce so far? it is a plugin for IntelliJ IDEA https://github.com/carymrobbins/intellij-haskforce 
it's amazing it worked liked a charm. it just worked. first try, and voila, it's working in spacemacs. quite fast too !
yup, i helped patch adding back "i want to use cabal instead of stack" support as an option :) i've a slightly customized sublimehaskell install, but i've been really really happy with how responsive the current maintainer is. its a lovely tool and environment. 
My employer paid for it. I will never buy any Apple product out of my own pocket. Also, I have thousands of grid nodes to run computations on. I like being able to compile and run stuff on my laptop because I can do it anywhere.
I updated the format of progress bar's title following your suggestion. The update has been released in haskell-player version 0.1.3.2.
Nice one!
My GPU is fine for computation. It's not top notch, but it's fine. The benefits of the rest of the machine outweigh the desire to get anything else for a better GPU
If you're investing millions in CUDA and running it on a 750M I can't but help think you're doing something very wrong, if you need mobile compute, then I assume SSHing into your "home" box with a massive kepler based Quadro would fair a little better in productivity.
Thanks for your excellent work on this wonderful library!
that's good to hear! keep up the good work
Does it save keystrokes vs vim? I find Haskell development requires too many (unlike, C where Vim's understanding of the structure saves greatly). I would love to try anything that promises to savings in effort per edit.
Thank you! That certainly makes sense. 
I have vim emulation in ST3. At the very least having instant compilation with markers for the errors , plus the ability to jump to definition, is in my opinion a saving.
I'm jealous if go's imports. I will say that.
In that case, a Stack Machine responds to every input by printing a message about how Cabal sucks.
&gt; 425 i think converting these to hexadecimal/binary number notation will help. (think about what the lower order bits of a number thats a 4th power). Andrew and I only took over as of the work leading up to the most recent release, and theres a lot of stuff thats accumulated over time :) 
By more arrows I meant the following: The Haskell helper function has type: Integer -&gt; (Integer -&gt; [Integer]) -&gt; Integer -&gt; [Integer] The normal C++ rendering of that function would be equivalent to: (Integer, (Integer -&gt; [Integer]), Integer) -&gt; [Integer] But in this case you need at least: (Integer, (Integer -&gt; [Integer])) -&gt; (Integer -&gt; [Integer]) (I think. There might be mistakes. I'm a bit tired.)
I deal with CSV a lot and it's rarely convenient to lock yourself to specific column names and types. Data usually looks like Maybe Int, Maybe Double or Maybe String and you have to figure out which it is at runtime. For this reason I don't get much milage out of libraries like Cassava or DataFrames. I like strong typing as much as the next haskeller but this seems to be the way the world works. You need a couple different ways to type data and also a way to decide which one to use at runtime.
Thanks. Glad there's someone who does use the Macbook GPU. I wasn't looking for quantum computer speed. I was just looking to make use of what hardware was available.
One example: consider [Nilsson &amp; Goldberger](http://www.eng.biu.ac.il/~goldbej/papers/ijcai01.pdf)'s anytime n-best algorithm for first-order HMMs. Don't worry too much about what it's doing or why it works, just look at the algorithm (section 4.4). In the paper they use "steps" to mean the big chunks and "phases" to mean the small pieces of those chunks; but I'm going to swap the words because I think they make more sense that way. Their presentation has three phases, the first of which has two steps, the other two have three steps each; for a total of 3 phases and 8 steps. By using a proper induction principle instead (which involves using the proper inductive definition of HMMs, instead of the definition they use) you can completely eliminate the second phase (which is a spurious basis case) leaving only two phases: basis and induction. In addition to their bad definition of HMMs, the other problem is that they put the partitioning step in the wrong place: it should be the final step in each phase, rather than the first step. When you try to force the algorithm to be a proper induction, this mistake becomes apparent. And once you see the mistake it's obvious why their second and third phases look almost identical— they are, they just unrolled the induction loop by one step! Correcting these two issues you end up with a vastly simpler version of the same algorithm; moreover, you get an algorithm which can easily be extended to support higher-order HMMs, whereas with N&amp;G's presentation it's not at all clear how to do that extension.
The code is very low-level and hard to understand. Here is a high-level version: isPossibleFourthPower :: Integer -&gt; Bool isPossibleFourthPower n = (n `rem` 256) `elem` (residues4 256) &amp;&amp; (n `rem` 377) `elem` (residues4 377) &amp;&amp; (n `rem` 425) `elem` (residues4 425) residues4 :: Integer -&gt; [Integer] residues4 m = nub [i ^ 4 `rem` m | i &lt;- [0 .. m-1]] Numbers `[256, 377, 425]` are coprime and such that the ratio `length (residues4 n) / n` is very small, so the sieving is efficient. Exactly, ``` 1 - product [ length (residues4 n) / n | n &lt;- [256, 377, 425] ] = 0.999578717428616 ``` which justifies the elimination of 99.958% of numbers.
$15 recommended donation
From the post's "tier 0" example: backback :: Monad m =&gt; T x' x b' b m a' -&gt; (b -&gt; T x' x c' c m b') -&gt; T x' x c' c m a' I'm sorry but I feel offended. This is the type signature of `(//&gt;)` from `Pipes.Core`. The pipes library has some of the best documentation in the entire haskell ecosystem. Combined with the author's disclaimer that "[a]ny similarity to real library documentation, living or dead, is purely coincidental", this feels totally dishonest to me. You've taken a well-documented function from a well-documented library, strips away ALL documentation, and use that as an example? The author's second example `runE :: Monad m =&gt; E m r -&gt; m r` is probably just `runEffect` from pipes. I'm really sorry; I think the author is going to raise some good points, but I'm not going to respect them if the author is this dishonest.
It's interesting that you see this as a hierarchy. In some communities (hrrm Python...) they only have the "ensemble demonstrations" and not the "necessary base layer". That is equally frustrating! A good mix is the key! Edit: and oh, by the way, lose the left column with the cute mascot on smaller screen widths. It gives the text way too little space. You can disable it on small screens with a simple CSS media query.
Yes, using JavaScript or Emacs Lisp is very frustrating due to lacking a simple signature. What does this thing return? What is the type of the OUTPUT argument? Enjoy reading five paragraphs of pros to find what could be in one line.
It tells you that the author didn't think there was anything surprising to note. Leaving this comment blank may either indicate nothing of note, OR that it was forgotten. Leave some documentation, even just restating the name, lets people know that documentation wasn't omitted accidentally.
Do you think you could add a few more details about what kind of candidate you're looking for? Experience level? What kind of work they'll be doing?
Do you consider that good, or bad?
Another useful layer to consider: doctests for as many functions as makes sense
Who's the author?
Just reporting. 
UX note: the platform download page lists the sha256 sums, but only the first few bytes are visible. Since the html source contains the full hash, I assume that this is an unintended consequence of some CSS trying the make it "look nice"... 
&gt; see this as a hierarchy. In some communities (hrrm Python...) they only have the "ensemble demonstrations" and not the "necessary base layer". In the post: &gt;Take-aways &gt;You really need every tier. The hierarchy is because the higher tier stuff usually takes more effort to get the same coverage.
I'm the author of `pipes` and I don't feel offended. In this case the post is right in the sense that the documentation for `(//&gt;)` and `runEffect` does not make any sense without the context of the tutorial and surrounding module documentation. The point is that the haddock comment in isolation is not enough.
What is this used for?
For someone who's seen the pattern before, it's noise; for someone who's new at this, it's really helpful. It's more like: /** Concrete representation of IO actions that the client can perform. * */ class IOCommand { ... } If you're familiar with the [command](http://gameprogrammingpatterns.com/command.html) [pattern](https://sourcemaking.com/design_patterns/command), the comment is superfluous; if not, it helps.
This is a great idea (I mean this sincerely) if the maintainer has the patience to puzzle through the biannual, "so why are the doctests broken again?" However, as documentation, they're only so-so. They don't work great for the same reasons test coverage alone doesn't, but they can help. Sort of an off-to-the-side improvement which is why isn't part of the, uh, hierarchy I made.
Thanks for taking the effort. This is amusing. Implementing 'reverse' with C++'s foldr equivalent will be just as bad.
Possession and use of chewing gum is not forbidden. Just commerce and trafficking. You can also get chewing gum for medical reasons. (And I am sure they allow you to grow your own gum at home.)
Thanks! I am going to check out Debian Chioot right now.
Is this automated? Has it ever been actually done? Last I checked, the code generated by Accelerate at runtime was a pretty close fit with its host-level code.
Yeah, I agree working (and tested) examples are a good choice, with comparatively small maintenance overhead relative to prose.
Or that the author has established a habit of starting with filler comments and then maybe improving them.
Thank you OP. If I work through the whole book I can give you some feedback from an absolute haskell noob (which seems like the target audience of that book) 😊
Wow, are there really some vector implementations where emptiness cannot be determined in constant time? ;)
What happens if `operA` is always valid? How does `simulate` terminate in that case? There's also a bias towards `operA` here. If your simulation could handle two simulations - one for `operA` and one for `operB`, you could interleave steps of the simulation so that as soon as one operation stops being valid you commit to the other operation. It might also be worth having a look at [LogicT](https://hackage.haskell.org/package/logict) if you haven't come across it before (partly for ideas, partly for other potential pitfalls). 
Interesting. Your examples works, but as soon as I add another export depending on vs, fusion/inlining breaks. module Demo (s,ss) where vs :: [Int] vs = [1..10] s, ss :: Int s = foldl (+) 0 vs ss = foldr (+) 0 vs breaks it out into --vs :: [Int] --vs = eftInt 1# 10# --s :: Int --s = case $wgo vs 0# of ww { __DEFAULT -&gt; I# ww} etc. This makes sense as it's trying not to duplicate work. I guess I want to use something like `CONLIKE` but not sure if it's possible for parameter-less thunks. Of course, it works if I eta-expand `vs` and add an `INLINE` pragma, but that's only slightly better module Demo (s,ss) where vs :: () -&gt; [Int] {-# INLINE vs #-} vs () = [1,2,3,4,5,6,7,8,9,10] s, ss :: Int s = foldl (+) 0 (vs ()) ss = foldr (+) 0 (vs ()) gives --s :: Int -- s = I# 55# -- ss :: Int -- ss = s
Yeah, I've had similar experiences with doctests. I've contributed a little to vinyl, which uses doctests, and doctest did catch actual mistakes in examples I wrote up, which was cool to see. However, it is sort of a pain to get working sometimes. Making sure imports and extensions are right is tricky. Additionally, you have to use an ugly mutiline ghci syntax to get larger examples checked by doctest. Here's an [example in vinyl](https://hackage.haskell.org/package/vinyl-0.5.2/docs/Data-Vinyl-Tutorial-Overview.html) of this problem.
Your example is why many people want the dup function, which stop sharing and force compile to re-evaluate the same expression, but it's not a easy thing to implement dup AFAICT.
/u/bitemyapp: You asked for more examples of Tier 4. Not sure, would you think these count? [example-servant-minimal](https://github.com/haskell-servant/example-servant-minimal) [example-servant-elm](https://github.com/haskell-servant/example-servant-elm) Thanks for the post! Btw, it would be nice to have some contact information on the blog.
Sure, but now you are not just in the boot libraries. (Other benefit: if you use MonadIO you need transformers; w/o it you don't.)
Ah, I see. I assumed OP was only talking about IO in a covariant position.
We are currently looking for a moderately experienced developer for this job. However, if you think you can learn quickly enough, feel free to apply.
Got it! Thanks very much for your kind reply!
Why do you need transformers? MonadIO is in base
That's pretty recent though. Wasn't that just introduced in base 4.9? So you have to keep a fairly recent dependency, which may or may not be a good thing
You can reduce the repetition using a default signature. For example: {-# Language DefaultSignatures, FunctionalDependencies, FlexibleInstances, UndecidableInstances #-} module Demo where import Control.Monad.Trans.Cont import Control.Monad.Trans.Maybe import Control.Monad.Trans.Class import Data.IORef class Monad m =&gt; MonadRef r m | m -&gt; r where newRef :: a -&gt; m (r a) default newRef :: (MonadRef r m, MonadTrans t) =&gt; a -&gt; t m (r a) newRef = lift . newRef instance MonadRef IORef IO where newRef = newIORef instance MonadRef r m =&gt; MonadRef r (ContT r' m) instance MonadRef r m =&gt; MonadRef r (MaybeT m) 
I (öartly) disagree with this advice. Yes, functions should be the focus, but data (and newtype) declarations are great for two things: 1. Abstraction 2. Type checking Having every function be foo :: Population -&gt; Years -&gt; ... Instead of foo :: Int -&gt; Int -&gt; ::: Saves a lot of time debugging code that's only wrong becausebyou accidentaly got the order of the arguments wrong
&gt; Is there any decent practice to help avoid the repetition? Basically, no. Define a few lifting helper functions, boil your plate, and be on your way to the interesting bits.
Wow that does go a long way to making it better.
I agree, I just think that many programmers try to treat data declarations like objects, and that's what I'm trying to avoid.
&gt; esp. since MonadIO may not be strong enough for IO code that needs to catch/recover exceptions That seems like a good thing! That way, when you see functions like **MonadIO m =&gt; m a -&gt; m a**, you can be sure it doesn't perform any bracket-like operation. The types are more expressive.
With GHC 8 you don’t :)
I always appreciate pre-lifted functions. AFAIK GHC is (almost) always smart enough to remove the performance penalty, and the dependency on transformers is almost free (and it is even in base in the newest GHC). As others have mentioned it is not possible to lift functions that have `IO` in contravariant positions. In those cases you might find the dependency too heavy. If you want to be extra nice, you could provide a separate package with the lifted versions using [monad-control](http://hackage.haskell.org/package/monad-control).
This looks great!
[Intero for Spacemacs](https://gist.github.com/dysinger/63b7cd03f77ac0125356051b23f47a37)
A couple of days ago /u/chrisdoner [mentioned this project in what grew out to be a long thread](https://www.reddit.com/r/haskell/comments/4m68zp/the_infamous_editoride_situation/d3swux0) on a post by the title "The infamous editor/IDE situation". Chris started with: &gt; We have a WIP Elisp mode intero.el that we've been using at FP Complete internally and so far everyone is happy with it. [...] Then proceeded with: &gt; It's not ready to package up (or announce) with instructions yet [...] Which seems to be a little at odds with the overwhelmingly positive reactions that can be found in the thread as well. A [comparison with `haskell-ide-engine`](https://www.reddit.com/r/haskell/comments/4m68zp/the_infamous_editoride_situation/d3vnldm), and a [feature request for automatic management of imports](https://www.reddit.com/r/haskell/comments/4m68zp/the_infamous_editoride_situation/d3tqr8x) are also found among the comments.
That's why you usually wait for the person in charge to announce it themselves.
Cabal, no. Literate Haskell, done: https://github.com/commercialhaskell/intero/issues/73
I had [`[TESTING INCOMPLETE, DO NOT SUBMIT TO REDDIT]`](http://i.imgur.com/UnMqubE.png) on the home page big red text. Cheeky Rob [submitted it to Twitter](https://twitter.com/robstewartUK/status/739085202023141376). A technicality! No big deal. :-)
[Fixed here](https://github.com/commercialhaskell/intero/issues/77)! It's intended to work on files without a stack.yaml or x.cabal file in the hierarchy. Can you retry it?
&gt; does this use ghc-mod and how is the performance on non-smallish projects? It uses Intero, which is GHCi plus extra stuff, and is configured and installed by stack.
How does on-the-fly type checking work? Are you parsing the file on every modification, and loading it into the REPL when it's syntactically valid?
&gt; But there's a whole host of reasons why this is completely unacceptable. Elaborate? It works just fine for me in Ether.
wow ... you guys are insane - it works yes - ** thank you ** ---- ### update really strange but I retried in a new folder and get the error again - in short I have to create the `.cabal`, `stack init` and `stack build` - it will only work after this went through Also I had to install `hlint` via global-stack manually and get this warning in the repl: &gt; WARNING: /tmp is owned by someone else, IGNORING! (which is true - `/tmp` is owned by root on my system) ---- aside from this it seems to work out of the box with my existing projects - so I guess I go with it and see what the performance is like (one thing I mentioned: the indentation is a bit laggy - it seems you run fly-check(?) or hlint everytime I hit tab - maybe you could insert some throttle-delay there - of course it's probably fly-check and maybe there is already a way to set this up - never used flycheck before - so if someone can help)
Hopefully someone will integrate this with vim :)
It just runs `:load` in the intero process whenever flycheck asks for it. That means re-parsing and re-checking. It usually returns in a few milliseconds when there's an error, when there's no error it takes a bit longer because it collects new information for all the successfully loaded modules.
As painful as this is for me to say... it's worth just biting the bullet and learning emacs+evil mode. I felt uncomfortable for about 3 days, and now hacking inside emacs is fairly natural.
And then when you introduce the actual transformer that implements your class you have to write all the other class instances for it... Luckily you can derive most of them if it's just a simple newtype.
I don't see why it would not work. That's what I have. Stack + GHC system install does. Spacemacs too. And now intero. Go step by step and post pb you encounter but much of it has been tried many times so there's no reason it would not work..
How do I disable hlint warnings?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/ruhaskell] [On-site GHC-related Haskell\/C opportunity at Positive Technologies (Moscow)](https://np.reddit.com/r/ruhaskell/comments/4mssri/onsite_ghcrelated_haskellc_opportunity_at/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
 stack exec -- emacs might help, if the issue was stack's ghc being shadowed.
Installed debian and good ol ghc 7.6 on there with apt-get. Compiling a project of mine went ok - four threads of compiling with about 1500mb of memory usage. Nice to have enough memory to allow all cores to compile at once. Haven't tried to get a more modern ghc running yet. 
It's actually for complete programming noobs... ideally we'd have a different volume, or at least an addendum for people who have programmed in other languages before, because there are quite a few gotchas for those people. However, it's often not the same problems that people who haven't programmed at all have. Feel free to give feedback no matter how far you make it - even if it's just on the first page. Feedback makes it better! :)
Is this for Emacs 24.5 only? On 24.3, trying to install it via ELPA gives me &gt; package-compute-transaction: Package `emacs-24.5' is unavailable 
Providing an instance on any transformer means that no transformer may provide a custom instance without overlapping. Also, I believe that is an undecidable instance, which is usually undesirable.
Thanks! There have been several readers that have given us quite positive feedback. One allowed us to feature it on our leanpub. There have been a couple of negative feedback people, too, but mostly on design we initially had, typesetting and layout of the code. We got one person who mentioned that it doesn't deal with Monads... but really this is a super beginner book... in fact, one of the positive comments was that we don't mention Monads *at all* in our first volume. Our second volume does... (we will release it soon, but it's in beta, so still being written), and deals with more meaty programs. We're quite interested to know if more people like this style of book because we haven't seen many books like it... with bigger example code that drives out the topics covered and exercises the reader over many examples rather than going through topic by topic.
What does this offer that cannot be replicated with the right vim plugins? Skimming the feature list, I don't see anything that I don't already have, but everyone seems really excited. Is it just more polish?
I think GP's point is that it can't perform a bracket-like operation on the provided action, which is true.
I've got `(setq show-trailing-whitespace t)` in my `haskell-mode-hook` (actually, in a whole bunch of `-mode-hook`s like Idris or C++ as well) for that.
Either [disable it in Emacs](http://stackoverflow.com/questions/28520965/flycheck-disabling-clang-as-a-checker-permanently), or my preferred method, just delete hlint the executable.
Thank you, I found that just after writing this comment :)
as another emacs+evil convertee, I would like to support the advise of not using spacemacs and using vanilla emacs instead. [this guide](http://y.tsutsumi.io/emacs-from-scratch-part-2-package-management.html) help me get started with package management in vanilla emacs.
Many of us have never been able to successfully get Vim (or Emacs) plugins for Haskell up and running successfully. The big advantage I see in intero is that it's easy to set up and works reliably.
Alright interesting. So do you recommend that in the general case, libraries take this approach, using `OVERLAPPABLE` and `UndecidableInstances` for generalized default instances? It seems very nice, and I might want to implement it in some stuff.
Thanks! I had wanted to avoid "not found" errors, but $ stack path $ rm ~/.local/bin/hlint worked. (I had tried `flycheck-disabled-checkers)
Hm. Reading through that little debate, I think I have to side with Edward Kmett on this one. As much as I would love to be optimistic and pretend orphans don't happen, I'd rather not be the guy that causes problems when they do happen. I think I prefer the `DefaultSignatures` approach. I'd be interested in hearing /u/edwardkmett's opinion on that solution.
What decisions were made for reliability? e.g. the GHC API needs the same version between the interpreter and the interpreted files, so `intero` chooses the appropriate executable from a `stack.yaml`. (Right?) And it did work for me! Which is rare for these dev tools, in my experience.
Ah yes, I see what was meant now. 
I think one thing that's holding IDE's back in Haskell is that everyone who tries to write one ends up trying to support every tool in the Haskell echo system. They can't depend on what GHC version they have, whether they will use cabal or stack, etc. If I look at the very best IDE's out there, everything is integrated. Visual Studio doesn't have some server running in the background passing text around. It does provide a command line version of the compiler and build system, but the IDE itself does most everything itself. I think making an IDE in Haskell would be much easier if it just built from GHC.API, potentially had its own package management system (which could use e.g. cabal the library under the hood if that didn't make things too complex). Thoughts?
GHC is not smart enough to understand that `forall a b . a ⊆ (a ++ b)`. You'll need a type checker plugin to convince it.
A "row polymorphism" typechecker plugin would be awesome. GHC doesn't even understand what `⊆` is, like I said it comes from `vinyl`. For now, I think I have to change the representation. Like, define a GADT that builds the proof. I still want to be able to define `rcastLeft`, which may or may not use the above internally. 
Russian citizen required?
Just a quick info, I will write some more feedback when I am finished, but chapter 3 and 5 are the same in the epub version. You may want to fix this fast(-ish?)
The initial announcement can be viewed [here][announcement]. [announcement]: https://mail.haskell.org/pipermail/ghc-devs/2016-June/012197.html
Chris is the real expert here, and it's his brainchild, but from our conversations about this: intero is able to leverage the existing GHCi codebase (plus a few extra features) instead of needing to create a new GHC API wrapper, which simplifies things a lot. Then, it's able to leverage the fact that Stack has very reliable ways to initialize a GHCi session (via the `stack ghci` command) to get the intero process up and running in a way that it can reliably compile your project. Then it leverages Stack to be able to easily build Haskell projects. In other words, by relying on Stack to give a consistent environment, it's able to cut out a lot of the common failure cases for tooling.
Now if you have a new data type that just happens to have kind that subsumes `(* -&gt; *) -&gt; * -&gt; *`, but isn't a monad transformer it can never be an instance of `MonadMine` without overlap. In this era of the PolyKinds, having something of kind `k -&gt; * -&gt; *` is very possible. Give it your own instance CrazyEffectTag f =&gt; MonadMine (Foo f) Now, until that kind `k` gets refined to be `(* -&gt; *)` or something that definitively doesn't unify with `(* -&gt; *)` then the instance in question won't get picked. Neither one subsumes the other. Your `Foo f` now has a conspicuous `(* -&gt; *)` shaped hole in it its support for all kinds and can only be used pointwise at individual concrete kinds, never polymorphically, not if you want to use the `MonadMine` instance. It just has one kind where it acts crazy. Note: This doesn't use an orphan at all.
So, I tried this (disclaimer: complete spacemacs noob), and all kinds of stuff is broken. For one, spacemacs binds `M-.` to `evil-repeat-pop-next` and I'm not sure how to figure out what the name of the default jump-to-definition function is. Second, `C-c C-i` gives `&lt;interactive&gt;:1:1: Not in scope: thing` no matter what "thing" is. Although weirdly, `C-c C-t` seems to work fine.
Default signatures help a lot!
I don't understand this comment. What part of GHC 8 means that you don't need transformers to use MonadIO? It's a typeclass, and almost always one "plate" in a "stack" of monad transformers that comprise the architecture of your app. Edit: I just realized... you mean that the typeclass MonadIO has moved from the 'transformers' library to base. Yea, I was talking about transformers as a concept, not the 'transformers' library. My bad.
I think the gtk2hs project keeps its demos up to date, and they have reasonable coverage of gtk. Have you seen this https://github.com/gtk2hs/gtk2hs/tree/master/gtk/demo ?
This sounds like a fun project. It makes the mind swirl. One question I have is, why Haskell on bare-metal instead of something like Ada? Or perhaps even a marriage of the two? That could be one good looking baby. *edit: a word
I suppose it might be a fair assumption that orphan instances of MTL style classes should never happen. But I'm not totally sure on that.
Sorry, I don't think my statement was very clear. I prefer the `IO` version to be exported rather than the `MonadIO m` version. So, I think we probably disagree. But I do think that your position is the majority.
Great to see the next incarnation of `ghci-ng`! Naturally I was curious to see what additional magic sauce `intero` has over GHC's `ghci`, so I went looking at [TOOLING.md](https://github.com/commercialhaskell/intero/blob/master/TOOLING.md) which says "It's basically GHCi plus extra features. Those are...". However, the features listed there (`:all-types`, `:uses`, `:loc-at`, `:type-at`) seem to be the very ones that got [merged back from `ghci-ng` into GHC 8.0's `ghci`](https://ghc.haskell.org/trac/ghc/ticket/10874). Is there anything extra in `intero` not yet available in GHC 8.0? 
Why?
I'm having a bit of trouble fully understanding this. `Foo` has kind `k -&gt; * -&gt; *`, from what I can tell. `CrazyEffectTag` has kind `k -&gt; Constraint`. If `Foo` *isn't* an instance of `MonadTrans`, how does it conflict with this? instance {-# OVERLAPPABLE #-} (Monad (t m), MonadTrans t, MonadMine m) =&gt; MonadMine (t m) where
I assume you prefer to have a visible indicator that some IO is happening, like a `liftIO` in front of all things actually using the IO base of your stack?
It's not that I prefer `{-# OVERLAPPING #-}` because it's less code. I prefer it because it actually prevents unnecessary orphan instances and module dependencies.
Instances don't look at the body, just the head. Overlap occurs off the head. By the time you're checking for `MonadTrans` it is far too late.
I think it would be useful to make this question more specific (or even broaden the scope a bit). Why are you having `IO` in your types? If it's just for exception handling, probably better to use the `exception` typeclasses like `MonadThrow` etc. There's also the issue of taking another `IO` function, `bracket/withXYZ` style. Those are often only `IO` because it is the broadest possible type, i.e. you can get anywhere from IO. There it might be nicer to you the 'classy' approach as well. Especially since otherwise you might impose using something like `monad-control` on your users to get their stack across the `IO`. The least is offensive is of course what you probably have in mind, an actual IO action. Here I'd also prefer the lifting to be in the library than at every call site.
Yea I understand that, but I'm unclear on why it's too late at that point.
I guess for me, it comes down to these reasons: - I find it easier to read the type signature. - I don't mind using `liftIO` in code that calls `IO` functions. In fact, I prefer it since it reminds me that the lifting is happening. - I better understand type errors when I'm working in a more monomorphic context. - There is no loss of expressiveness. Plain `IO` can do everything that `MonadIO m` can. Naturally, all of these are just my opinions, except for the fourth point (which is objectively true). I don't expect that everyone will feel the same way.
Why is that a problem? A dependency on transformers/mtl/exceptions is likely free as they are such widely used libraries anyway. I can only imagine this to be an issue for special cases, like if you want your code to be merged into `base`, are building for a platform / compiler with limited library availability, boostrapping cabal-install itself etc.
That makes sense! 
That's really interesting. I always knew about that warning, but didn't consider it in this context. This and stuff like ADT records are just a unnecessary source of runtime errors. I wish stuff like that would just be removed from the language and made an error by default.
Is this manifesting like this? File mode specification error: (void-function flycheck-define-generic-checker) If so, I'm getting the same thing. Edit: Looks like I had a very old version of flycheck being loaded. 
Ah sorry about the missunderstanding. I guess calling a transformer library transformers is calling for confusion.
From https://downloads.haskell.org/~ghc/7.6.3/docs/html/users_guide/options-sanity.html &gt; This option isn't enabled by default because it can be a bit noisy, and it doesn't always indicate a bug in the program. However, it's generally considered good practice to cover all the cases in your functions, and it is switched on by -W. Although I gotta say, I definitely feel it should be the other way around, and have to be explicitly disabled, instead of explicitly enabled.
And thus was born the new future warning: [TESTING INCOMPLETE, DO NOT SUBMIT TO ANY SOCIAL SITE, THIS MEANS YOU ROB] &gt;.&lt;
I tweaked the layer to add some keybinding. https://gist.github.com/joehillen/72e0ee604355877d6f92f3a5cf398358/revisions
That sounds reasonable. So how about this: 1. Submit a GHC feature request to add a new flag that makes it an error, not just a warning. 2. If possible, help get it implemented. Should be a simple enough change. 3. Submit a request to Haskell Prime to make this the default in the future. Don't wait for it to be implemented if it looks like it will take more than a month or two; put in the request soon.
This looks like another example of why Haskell records sucks. Thanks for sharing, I'll be sure to avoid those from now on.
Now if one library defines a class `MonadFoo` and another defines a transformer `BarT` and I want to use them together, either one of the libraries should know about the other and define the boilerplate instance (unnecessary dependency) or I have to write an orphan instance (thanks, no).
You're welcome. I'll have to joy to implement that soon too :-)
I suppose you are right, but it's too late to change the title now. Probably it's more of a complaint about GHC, which happens to subvert the "if it compiles it works" mentality of Stackage snapshots. And, as I understand it, people who tend to stick with Stackage snapshots exclusively (such as myself) tend not to care if a package has upper bounds or not. Apologies!
Lovely article. Saving it. Thanks for writing it!
~~I tried to go to your dotspacemacs but chrome's giving me a 'Your connection is not private' error and not letting me load the page. Any chance you could throw it up in a pastebin or something? T'would be really appreciated. Thanks.~~ ~~Edit: Got bored and went in anyway. NVM!~~ Edit: Wow. That worked suspiciously easily... I wasn't even using stack before but, it just figured it out. Thanks a lot.
Grief. That's all I have. I'd much rather eat our just deserts now than have several minutely-diverging versions of append... It's append for crying out loud. Haskell will be the laughing stock of PL: "Learn Haskell, chapter 6: choosing the append for you."
Coincidentally, even though you can [drop `*` to a type](http://pointless-haskell.tumblr.com/post/144576116447/language-typeintype-idstar), you cannot [drop type variables to values](http://pointless-haskell.tumblr.com/post/145494986897/typevalue-forall-k-a-k-proxy-k).
Wow, that is chock full of valuable and little-known information and good ideas. You have my vote for Haskell Docs Czar! How about it ?
See https://ghc.haskell.org/trac/ghc/ticket/11219 and https://ghc.haskell.org/trac/ghc/wiki/Design/Warnings &gt; Introduce variant of `-Werror` (c.f. GCC's `-Werror=*`) which allows to specify the individual warnings to be promoted to errors, e.g. &gt; `-Wall -Werror=orphans` would only promote `-Worphans` warnings into errors &gt; `-Wall -Werror -Wno-error=missing-methods` would promote all warnings except `-Wmissing-methods` into errors 
Note that if you make the fields strict (usually what you want anyway), you get a compile time error instead of a warning.
Changes in sharing? 
&gt; GHC is going to support explicit type application, if the latest version doesn't already. It does.
The constraint would bubble up.
Hear hear. Partial records should be compile errors every time. If you want partiality, you can put `undefined` in yourself.
The last time I checked, which I admit was awhile ago, most the existing network stacks were not parametrizable on the IO subsystems they used. Specifically, they assume things like the `unix` and `network` packages, which are very tied to the underlying system, including `libc`. Some also require deep integration with an underlying event systems, as well. With the HaLVM, none of these exist, because we're not running on a POSIX system. There are basically three ways around this: Option #1: Add back all these capabilities, somehow. This is the rumpkernel approach, basically. It tries to regenerate the POSIX infrastructure around code so that you don't have to change anything. Option #2: Do some fancy dancing in the HaLVM and recreate POSIX via Haskell. In this case, you would initialize your hard disk and file system (for example) via Haskell. Then you'd call `setFileSystemImplemenation` to inform the runtime of your particular implementation. At that point, any calls to low-level file system operations would vector to your code, rather than C. Option #3: Change one or more of the Haskell web stacks to take in a NetworkStack and FileSystem type.
Well, the HaLVM lets you stay high-level, in Haskell, from the drivers up. If you're willing to write that code, that is. So that's a positive. On the other hand, it will define a threading model for you, choose your memory layout, etc., for garbage-collected Haskell objects. But you can define your own wire formats via Binary, Cereal, or Storable, if your goal is interaction with other systems. Finally, if the Backpack stuff for GHC ever gets merged, then that would help a lot of us in terms of swapping out different implementations of underlying libraries much more easily. I'm not sure that answers your question, but there's some more information. In some sense, it sounds like you want something more like Rust than like Haskell; in that case, you might look around to see if there are any Rust unikernel projects. But if you want to keep with Haskell, the HaLVM might be the project for you.
Any requests for talks? 
Doctests are extremely helpful. To see a great example of doctests: https://github.com/atnnn/haskell-rethinkdb It would have been very difficult to learn that library without them.
I don't think [`-fwarn-incomplete-record-updates`](https://downloads.haskell.org/~ghc/8.0.1/docs/html/users_guide/using-warnings.html#ghc-flag--Wincomplete-record-updates) would help anything in this case. It's [`-fwarn-missing-fields`](https://downloads.haskell.org/~ghc/8.0.1/docs/html/users_guide/using-warnings.html#ghc-flag--Wmissing-fields) that will catch OP's case and that's enabled by default.
/u/edwardkmett gave [a great talk at Boston Haskell, a year ago, about the tradeoffs between implicits and typeclasses.](https://www.reddit.com/r/haskell/comments/2w4ctt/boston_haskell_edward_kmett_type_classes_vs_the/)
[Haddock link for the lazy](http://hackage.haskell.org/package/rethinkdb-2.2.0.5/docs/Database-RethinkDB.html).
How I read this proposal, am I missing something? **Reasoned User:** I don't understand why a `Semigroup`has two operators `(&lt;&gt;)` and `(&gt;&lt;)` that do the same thing defined for it. I thought a `Semigroup` had a single associative operation defined over a ~~set~~ type? **Devil's Advocate:** No Reasoned User, they are different! The operator `(&lt;&gt;)` is right associative and the operator `(&gt;&lt;)` is left associative! Hence there are two operators. **Reasoned User:** But right and left associativity don't matter, by the definition of a `Semigroup` the operator `(&lt;&gt;)` must be associative! We can have a single operator like algebra demands and choose `infixr` or `infixl` arbitrarily. **Devil's Advocate:** Oh Reasoned User, if only you understood the folly of such thought! Suppose a library author defined a type `T` which was a proper `Monoid` with the standard associative operator `(&lt;&gt;)`, and an identity element `mempty`. But then for this same type `T`, the authors discovered a second `Monoid` instance with a second associative operator `(??)` and the same identity element `mempty`. However, the operators `(&lt;&gt;)` and `(??)` were not associative with respect to each other! How could one compose such operations? **Reasoned User:** I still don't understand, surely they used parentheses to disambiguate any order of operations, correct? **Devil's Advocate:** Quite to the contrary Reasoned User! Some library authors thought the two operations on the `Monoid` should be arbitrarily left biased and others thought that it should be arbitrarily right biased. All authors (apparently) decided to be clever and use both operators in a single line, relying on the right or left biasing to disambiguate the order of operations between `(&lt;&gt;)` and `(??)` as Haskell's parser constructed the abstract syntax tree! A left associative `(&gt;&lt;)` aliased operator allows these authors the freedom to keep the order of operations encoded in the opaque parsing process. We dare not disturb such black magic and added a left associative alias to ease maintenance pains. **Reasoned User:** Surely you can't be serious? Why would one introduce a ridiculous and confusing operator alias when parentheses would suffice? Why would you want to hide order of operations in the abstract parsing phase rather then be explicit with parentheses? **Devil's Advocate:** Haskeller's *really* hate parentheses! *PS:* I understand the need to standardize the fixity rank to 6 or 7, but not the necessity of a left associative alias when parenthesis exist... 
Good idea.
recently，i have problems with thrift package on hackage due to the missing upper bound on quickcheck and unordered containers, I fill an issue on their tracker, but no one respond, I really wish someone can edit the cable file to make it buildable!
Yes I see typeof() used a lot in my C# career. It often lets people hack in a new feature or perform bugfix and get the task off their list quickly. No refactoring or changing unit tests etc. to worry about. Boss is happy because problem was fixed quickly :-). 
Honest question why everybody loves so much turing machines rather than the λ-calculus for those things? It is a pain
Not directly related, but likely to be of interest if this is: Scott Aaronson [discusses](http://www.scottaaronson.com/blog/?p=2725) his student's recent work finding the smallest Turing machine whose halting is not provable in ZFC.
Umm... honest follow-up: which of those two have you tried implementing yourself?
Code golf instincts kicked in: `f x|x&lt;2=1&lt;2|mod x 2&lt;1=f$div x 2|1&lt;2=f$3*x+1` (Collatz conjecture)
Speaking of fields, I actually had a question about implementing a proper abstract algebra hierarchy - a field has two groups that interact properly within it, but I don't see how you could design an API that does something along the lines of Field a -&gt; additiveGroup a Field a -&gt; multiplicativeGroup a I guess I'm looking for hints on how to design structures that have multiple type classes and multiple instances of the same typeclass on them
These sound like great guidelines for Haskell packages, too! (1) Are there any recommendations in there that wouldn't also apply to a Haskell package? (2) Any recommendations for Haskell packages that should be added?
What's the right place for debugging and troubleshooting? Pasting the error message below. I have a global GHC &amp; cabal set-up alongside stack. Is that the problem? Do I have to nuke my regular setup? The following GHC options are incompatible with GHCi and have not been passed to it: -threaded -O0 Configuring GHCi with the following packages: Nightwatch Intero 0.1.13 (GHC 7.10.3) Type :intro and press enter for an introduction of the standard commands. &lt;command line&gt;: cannot satisfy -package aeson-0.10.0.0 (use -v for more information) --- This is the buffer where Emacs talks to intero. It's normally hidden, but a problem occcured. It may be obvious if there is some text above this message indicating a problem. The process ended. Here is the reason that Emacs gives us: exited abnormally with code 1 For troubleshooting purposes, here are the arguments used to launch intero: stack ghci --with-ghc intero "--docker-run-args=--interactive=true --tty=false" --no-load --no-build --ghci-options -odir=/var/folders/j7/fgt692sj0wzg0lvrk86ctnxc0000gn/T/intero1855H2s --ghci-options -hidir=/var/folders/j7/fgt692sj0wzg0lvrk86ctnxc0000gn/T/intero1855H2s Nightwatch After fixing this problem, you could switch back to your code and run M-x intero-restart to try again. You can kill this buffer when you're done reading it.
Paradoxically, Americans use Turing machines while Europeans use λ-calculus.
Because the Turing machine runs on the jvm. Edit: the lambda calculus is the Betamax of computation models.
Hang on, trying that now...
I've implemented far more lambda calculi than Turing machines. May be a product of where you are educated. I was constantly told at university how the lambda calculus is more elegant and easier to work with than Turing machines. 
I use `isInstanceOf` to simulate tagged unions (sum types) in object oriented languages that don't have them.
Just like with Typeable... The point was that since `seq` breaks a lot of the rules of the category theory constructs, it should have to be explicit as far up as possible.
In most languages, `isInstanceOf` is a value level mess that lets you do ad hoc `typecase`. In `ocaml` you have structural types so the story is a bit changed, since these aren't nominal types, but I suppose it'd be analogous to being able to reflect to see whether or not a type matches some signature / has some fields. In a staged setting like metaocaml, you might be able to impart part of that more safely and do something better, for some of the usecases for `isInstanceOf`, but I'm not familiar with what they actually do enough to speak knowledgeably.
Give me the pitch! Why would I use `sbv` over other SMT solvers? What would I even use an SMT solver for?
if you're actually selecting from the reals, that terminates almost nowhere :-) (although if you have a program that generates an honest to goodness real drawn with uniform distribution that would be a feat in itself i suppose)
I'm curious about it, could you point me at least Oleg's famous "make Haskell strict" thread? I Googled it but had no luck...
Are you sure you used the right issue tracker? I found [this one](https://github.com/haskell-infra/hackage-trustees/). It seems to be very active, and I don't see your issue there.
everything seems to be working fine after stack build. So excited to see symbol auto-completion. Hopefully coding in Haskell will be an order of magnitude easier now :)
There is nothing yet. Here is the ticket, https://ghc.haskell.org/trac/ghc/ticket/8779. 
If you write an interpreter for a language that uses symbolic values (eg. SWord) instead of regular values (Word), you can very easily prove stuff about programs or see what inputs cause a program to fail, automatically. If you write an interpreter for regexes, you can use it to do stuff like find all matching inputs for a regex and solve crazy regex crosswords: https://gist.github.com/LeventErkok/4942496
The ePub version is actually incredibly out of date. We left it there because one of our readers asked me not to remove it. We basically rewrote the book rendering engine after releasing it, and haven't built the ePub builder part of it yet. I really should do that soon, but we've been focussing on Volume 2. Do you think it'd be better to remove the ePub version entirely until we've finished the ePub engine? Personally that would be my preference.
It's not a solver. It's an interface to solvers.
Well I admit I have only implemented the λ-calculus but that was a just a question, in what sense are turing machines more suited to the problem?
Come to think of it... How about something like class Orderer o k | o -&gt; k where compare :: k -&gt; k -&gt; Int data Asc instance Orderer Asc Int where ... data Desc instance Orderer Desc Int where compare a b = compare @Asc b a empty :: Orderer o k =&gt; Map o k v empty = ... sortedAsc = insert 5 "hi" (insert 2 "hey" (empty @Asc)) sortedAsc = insert 5 "hi" (insert 2 "hey" (empty @Desc)) (Except the type application probably won't work as is) That gives us the best of both worlds: Instance coherency and no noisy newtype wrapping. At least for the particular example.
That makes sense. Thanks
&gt; noisy newtype wrapping sortedDesc = insert (Down 5) "hi" (insert (Down 2) "hey" empty) It's not *that* bad ...
I'm pretty sure that the analogue to the halting problem for a probabilistic turing machine would be computing a halting probability, so this one has the known halting status of "infinitesimally likely to halt".
Arbitrary [reasons](http://www.scottaaronson.com/blog/?p=2725#comment-1085117).
Never mind, it was a faulty `company` installation.
`-fwarn-incomplete-uni-patterns` warns against `\(Just x) -&gt; x`
What about interaction combinators? They are basically the the λ-calculus except with very clear computational and cost semantics.
I like providing an `.Example` module: * put the source of definitions in the haddocks, not in doctests (still has doctests too) * import only the "one re-export module" from my package, and import every other symbol qualified or explicitly Much quicker than doctests only, as the API is checked on every `cabal/stack build`. 
Agreed, we should rip the bandaid off now, rather than add a wart `(&gt;&lt;)` to be removed later...
Technically, there is no backtracking here since the simulation does not backtrack, and the main monad is not executed when the simulation fails. In the other side, if the operation is irreversible it may mean that it produces side effects instead of pure internal state changes. In the first case, it is at least thinkable to use compensations. For example: launchMissiles/destroyMissiles. In the second case -pure state changes- some use of some kind of continuations can continue with the other alternative using the unchanged state. [I did some backtracking with and without irreversible actions using compensations and continuations here](https://www.schoolofhaskell.com/user/agocorona/the-hardworking-programmer-ii-practical-backtracking-to-undo-actions). For an irreversible (to a certain extend) action: do launchMissiles `onUndo` (liftIO $ do time &lt;- flighTime if time &gt; 20 minutes then print "no luck. Wait for the end of the world" &gt;&gt; stop else destroyMissiles &gt;&gt; stop) peaceAgreement &lt;- redPhoneTalk when peaceAgreement undo For a pure internal state, the undo action just execute the alternative: do operA `onUndo` operB r &lt;- valid when (not r) undo operB will reexecute with the same state than operA since by hypothesis, operA perform pure internal state changes. It should be more complicated if operB would backtrack to operA too. then the second operando of `onUndo`, instead of invokink operB as such, it should invoke (`operB ``onUndo`` operA`)
I'll just note that your argument assumes that: 1. the developer of `keter` would correctly apply the rules in the first place, and then 2. reject a PR to soften the upper bounds because 3. the author immediately would remember that those additional fields could break third party code. If I was the maintainer, that would never happen.
Well, there are a lot of people who would say that the quicksort presented in LYAH is not really quicksort. But continue along your journey - there's a lot more coolness in store for you down the road.
Thanks for the link! I needed a big *shared* library with static inclusion of all basic ghc libraries (rts and ffi) and dependent custom libraries. With ffi I had a problem: libffi/ghc.mk assigned CFLAGS without any hooks where I could have *-fPIC* put. Not sure if alpine-ghc works it somehow around, but if so I'll probably try it.
It should do the trick for you as ghc is compiled with +fPIC in alpine-ghc as you can see [**there**](https://github.com/mitchty/alpine-linux-ghc-bootstrap/blob/master/7.10/ghc/APKBUILD) P.S: when i get back to home, i will post you a step by step of how i build a static binary. I think it will not be too hard to adapt it for a static lib
It looks like we should deprecate all the older versions too then, to encourage people to use &gt;= 1.3.12
What do you mean by sharing?
You're right — you do want services engineered for tool support, as in C#. https://www.reddit.com/r/haskell/comments/4m68zp/the_infamous_editoride_situation/d3ug3z0 Having said that, requiring correct code can be acceptable for refactorings. It's maybe a restriction for some workflow, but I'm not sure — when I program Scala refactorings only semi-work on broken code. Also, it's hard to say something is meaning-preserving if the code has no meaning. Finally, at least on nontrivial projects you want to separate refactorings that preserve behavior from actual behavioral changes that might introduce bugs, and preserve that split into commits. However, that doesn't make sense if you refactor on "young" code, which hasn't been debugged yet — that's a scenario where refactorings on incomplete code would make sense.
I think Intero is doing exactly that (parse + fallback), but not sure—can /u/chrisdoner confirm/deny?
Intero parses and type checks, keeps hold of all the info (locations, types, provenance, uses). Puts that in a database of all modules, with all spans in the modules, with all the info GHC generated while simplifying, type checking and compiling. Then when you query later it attempts an exact match on the thing you tried to query, otherwise it falls back to a more general global datum, finally it asks if the interactive context knows anything. I've been using this for a year (under a different name, ghci-ng) and it's pretty reliable. Go to definition pretty much always works. Type info regularly works (in broken code situations). There're other techniques to "backtrack" and figure out the changes that happened since, but IME it's not worth it yet. Structured-haskell-mode parses and then creates Emacs markers at all expression/pattern/decl boundaries. If you insert some text in Emacs, markers automatically adjust their position. It doesn't matter if the changes you make cause the current text to be invalid, the boundaries are still there. So while not perfect, there's still the ability to manipulate the AST as if it were correct. This is important for intermediate steps. For both cases, one learns a few habits (which is also the case in things like Visual Studio), to "help along" the IDE. In Haskell, this means e.g. inserting an `undefined` or `_` hole in key places, so that it can perform a new parse/type check. SHM helps the user to help it by automatically filling "if undefined then undefined else undefined" when the user types `if &lt;space&gt;`, same for `let`, `case`, `do`. In my experience both the GHC API or haskell-src-exts, or a combination of these and other tokenizers (like hasktags which works on invalid code), are all useful; not hopeless. And actually, the requirement that "your code is valid at key intervals" is practical. 
Sure, I replied to /u/sinyesdo.
I've wanted this for so long. Not all warnings are equal.
In my universe /dev/urandom is fine. It's as good as using a stream cipher seeded with a random seed. If you can attack it then I guess you can attack most team ciphers as well.
You should check this out: http://www.2uo.de/myths-about-urandom/
You can translate almost any λ-calculus term to interaction combinators, it is a straight, simple a 1x1 mapping (i.e., one lambda = one node, one application = one node, one variable = one node). You can then reduce the net to its normal form and translate back. This is the optimal way to reduce λ-calculus terms and is much simpler and cleaner than the usual string based algorithms. You can implement it in a very small number of lines of code. And those nets reduce in a very predictable amount of O(1) rewrites. For example, [here](https://github.com/MaiaVictor/LPU/blob/master/stats.txt) is a stats of computing `x^x` using church numbers on the λ-calculus. Evaluating `50^50` always takes exactly `12350` rewrites. So, basically, I think it is unfair to say that the λ-calculus doesn't have a trivial notion of time and space when in truth only the algorithms we used to reduce them don't. Not only interaction combinators have a very clear notion of time and space cost, but they also have a very clear and natural notion of parallelism: active ports can be reduced in any order and in parallel. Any inherent parallelism of any algorithm is visible on the graph.
:D
[this](https://mail.haskell.org/pipermail/haskell-cafe/2011-April/090664.html) perhaps? 
Process has also added fields to CreateProcess in 1.2 and 1.3. It would be better if it only exported the proc smart constructor and CreateProcess modification functions.
`seq` is there to control low-level thunk evaluation behavior. You need it to work for everything, or else you can't direct the compiler sufficiently. You would also have to make bang patterns based on a typeclass, which would be even sillier. If `seq` breaks category theoretical constructs, just don't use it for category theoretical programming. It should only be used for low-level performance optimization anyway.
Cool stuff, indeed sets itself up nicely :) Is there any way to stop intero from saving my file repeatedly? It interacts really badly with things like git operations modifying the file, which is then overwritten by intero :( Also it seems to disable automatic revert of the file.
As someone who started learning Haskell about 18 months ago I agree it's awesome but there's also a lot of frustration ahead of you! I'd treat learning Haskell as a marathon, not a sprint :)
Yeah, sorry about that. Accidentally thought "fibonaccis" again right after I wrote it, started it with 0 and 1 instead of 1 and 1.
It's quick sort, but it's not very quick. 
That's great! Remember this enthusiasm when things are tough, because that feeling will come again and again as you get more adept at Haskell. It's the gift that keeps on giving!
[This issue](https://github.com/aslatter/uuid/issues/16) seems to imply that it was actually using time and not urandom as a seed.
Since there are arguments both for and against strict upper bounds, the issue won't be solved by some principled argument - it will come down to something quantitative. So I'm glad to see these experience reports. The PVP debate is helped by having a quantitative sense of whether people's real-world code is breaking, how long it took to find the error etc, under both experimental conditions (with and without bounds).
RNGs generate bits, not reals. A real would be a countably infinite sequence of bits, and comparing that to K, if they are equal, would take forever.
Please have some reddit gold for sheer bad-assery! Fellow readers: the related blog post [nginx module to enable haskell binding to nginx configuration files](http://lin-techdet.blogspot.com/2015/12/nginx-module-to-enable-haskell-binding.html) is an easier starting point.
Bubblesort ist fastest in wall-time-clock for lists with less than 100 elements (because of caching and mem-alignment-effects). For bigger Datasets you want something like Counting-Sort or American-Flag-Sort (O(n)), if you can give a viable hash-function For lazy stuff you probably want the default-haskell lazy merge-sort which runs in O(n*log n) but can short-circuit if you only need the first few elements. Tl;dr: There is a lot to learn my young padawan :p
So you have to know docker a bit, but even if you don't know it yet you can catch it pretty easily. * docker run --rm --name alpine_ghc -v /path/to/my/project:/mnt -ti mitchty/alpine-ghc:latest Once you have downloaded everything you will endup in a terminal "bash-4.3" as root. You are inside the alpine os * apk update &amp;&amp; apk upgrade //maj the system * apk musl-dev gmp-dev zlib-dev //install necessary library * cp /usr/lib/gcc/x86_64-alpine-linux-musl/5.3.0/crtbeginT.o /usr/lib/gcc/x86_64-alpine-linux-musl/5.3.0/crtbeginT.o.old &amp;&amp; cp /usr/lib/gcc/x86_64-alpine-linux-musl/5.3.0/crtbeginS.o /usr/lib/gcc/x86_64-alpine-linux-musl/5.3.0/crtbeginT.o //to avoid a long standing gcc bug Those previous are necessary only the first time. If you reuse the container later on, you do not need those * cd /mnt //were you project was mounted * stack build --ghc-options='-optl-static -optl-pthread -fPIC' With those step I end up with a static binary of my program 
Perhaps the [iterative monad transformer](http://hackage.haskell.org/package/free-4.12.4/docs/Control-Monad-Trans-Iter.html) could be useful here. [Example](https://gist.github.com/danidiaz/e9382127d1c4025b9845).
Bookmark this version of quicksort for future reference : http://augustss.blogspot.se/2007/08/quicksort-in-haskell-quicksort-is.html ; it uses a few advanced aspects of the language (in-place mutation), so it's quite instructive when compared to the usual two-line lazy quicksort. And welcome to Haskell! 
&gt; option to construct some safe instances leveraging first class modules Okay, so maybe let's agree to put off explicit application of type class constraints until we have first class modules? (I'm not sure this option is very valuable, nor do I think I'll see it in Haskell or GHC this decade.)
Thank you! 
thanks for the explanation, even though I came late to the comment party. Btw, you guys are my heroes -- OS/system development with a modern language like Haskell is some seriously cool stuff. Being able to read about the history and design of HalVM is really great, so thanks for that. I knew that HalVM existed, but now I'm definitely keeping a closer eye on it.
The point seems to have a strict separation between static and runtime choice. Since type class are resolved only at static time you are forced to pin down your choice for families of function indexed by `Typeable a` at that time. The operation you apply can't truly depend on the value being received. How ever, before runtime, there is a whole logic which is safe to run at compile time. And it is perfectly kosher to pick a function based on static information, as long as everything is finished statically. For instance `Typeable a =&gt; Dynamic =&gt; Maybe a` is a family of function indexed by `a` but the constraint forces you to make up your mind statically. Once it's written in the marble statically, values can't change the function and make spooky action at a distance, they are real function with all the nice properties mentioned by Edward Kmett (parametricity etc..)
I haven't looked at the sort implementation in base, but if it's the traditional merge sort, this short-circuit can only happen in the last iteration, when you're merging the top 2 lists, so you only win on the last merge operation, which isn't much. EDIT: actually, since the sorting of your sub-lists are also not forced, you don't have to fully merge them to extract the first few elements, only up to that point. I'd sooner use a heap-sort if I wanted to do partial sorting (not on lists though)
I succeeded inside alpine-ghc to have a static lib like this stack init stack build --ghc-options='-fPIC' cd /tmp/nginx-haskell-module-master/haskell/ngx-export/.stack-work/install/x86_64-linux/lts-6.2/7.10.3/lib/x86_64-linux-ghc-7.10.3/ngx-export-0.1.0.0-3Z47AtIQBmyF4D4j04ohjy DEPS=$((ldd libHSngx*.so | grep '=&gt;') 2&gt; /dev/null | cut -d ' ' -f 3 | sed -E 's/(-ghc.*|.so.*)$/.a/' | grep .a) echo 'int __tls_get_new() { return 0; }' &gt; stub.c &amp;&amp; gcc -c stub.c &amp;&amp; ar crs libstub.a stub.o ld --whole-archive $DEPS libHSngx-export-*.a /usr/lib/ghc-7.10.3/rts/libCffi.a /usr/lib/ghc-7.10.3/rts/libHSrts.a /usr/lib/libc.a /usr/lib/gcc/x86_64-alpine-linux-musl/5.3.0/libgcc.a /usr/lib/gcc/x86_64-alpine-linux-musl/5.3.0/libgcc_eh.a libstub.a --no-whole-archive -o libHSngx-export.a ld -shared --whole-archive libHSngx-export.a --no-whole-archive -o libHSngx-export.so Well without saying that I am not sure if it will be working. But I will be pleased if you could tell me
Cool! Tomorrow I'll try your approach! 
As a note, I have ghc 8.0 ported as well and submitted upstream so hopefully that port can go away. Same image but just tack :8.0 on the end instead of :latest aka docker run mitchty/alpine-ghc:8.0 I've been lazy on keeping it up to date this past month. BUT, I got it working on arm, so you can do the same thing for arm binaries/libraries with ghc 8.0.1 now too.
Neat!
Cabal won't pick them as versions unless you explicitly request one of them.
As someone trying to migrate to an Evil workflow, how did you deal with the lack of tabs as workspaces? I find I'll frequently have something like: Tab 1: - something.cc - something.h Tab 2: - something_else.cc - something_else.h and I'll switch between the tabs to flip through my workspaces. Is there something similar in Emacs?
I suspect that ARM may allow natural static linkage without -fPIC, or not? 
Anyway your idea of how to collect dependencies (variable DEPS) looks very interesting. 
Pardon, been a busy day, getting my intern setup. 
Tf random isn't actually good statistically. It fails big crush. It's better than current random for most purposes. But there's better algs Good randomness engineering is a richer topic than most folks realize, and doing a good job updating our tooling in Haskell isn't a trivial project. I actually have work support to make this and a few other ecosystem tooling needs part of what I prioritize this summer, in part because I can justify it as being aligned with some needs at work. Good engineering takes time and resources. And humans. 
It should yeah, but I just was happy it worked and built things. I haven't tested it a whole bunch yet. The -fPIC is there on alpine linux more for abi reasons on x86_64, that and ghc bug https://ghc.haskell.org/trac/ghc/ticket/9007 So most of the hackyness of that port eventually feed back and down to that bug and hardening choices used on alpine. I think gentoo linux with musl wouldn't have the issue, maybe void linux is fine too but I've not looked into it. Glad it worked for you though! I did the whole thing more because I wanted a static pandoc binary, its come in handy in other ways though. Hopefully now that 3.4 is out I can pester people to get this hacky port into at least th testing channel.
That's interesting, I didn't know TF random did so poorly - thanks for that note. Still, it doesn't actually take much time or resources to do better than `random`. Indeed, SP800-90 has three really simple generators that are just sadly really slow (short of hardware support) and/or slow to split. Similarly we had many naive direct uses of stream ciphers in the likes of cryptocipher, cryptonite, and intel-aes (interesting just because it was available particularly early). All that along with tf-random being much better than `random` makes me take issue with "randomness is _too_ hard" when measurable improvement with real world impact are as simple as using a different package that already has API compatibility. To be clear: I'm thrilled we have a set of people who are making a huge improvement. I'm just trying to understand why the significant incremental improvements were never fully exploited.
SBV is much better at finding quality obsidian than Z3, CVC4, or even yices. More seriously, SBV can interface with many SMT solvers including Z3, CVC4, abc, yices, boolector, and MathSAT. Using SBV you get the benefits of a high level language (Haskell) in which you can construct your problem and all the power of the (fastest of) the set of solvers you choose to use by running them in parallel. For example, SBV is the work-horse behind Cryptol. I've used SBV to model protocol interactions. More recently, SBV was an extremely quick hammer for me to model and empirically evaluate the security of property preserving encryption. The SBV library itself has examples where you can prove things like encryption composed with decryption is idempotent - properties like this are valuable in showing an API or algebraic system is consistent.
Have you tried [Perspective](https://github.com/nex3/perspective-el)? [eyebrowse](https://github.com/wasamasa/eyebrowse) is another good one.
I sort of hate it when people say that... Look here: https://en.wikipedia.org/wiki/Quicksort#Algorithm , the partition -&gt; recursion. The Haskell version just a *perfect* way of describing this algorithm. Oh wait, you gotta check this [partition](http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List.html#v:partition) function out.
Great idea. I look forward to trying it out. Thanks for your work! edit: How cool would a smooth [Sparkle](https://github.com/tweag/sparkle) integration be?
I'm not a Vim user at all (Emacs all the way!), but in the past I basically left that up to XMonad. Not a solution for *everyone*, but it worked really well and let me have other programs (ie Firefox) as part of my workspaces. I was running an Emacs server, which meant that all my independent Emacs windows ("frames" in Emacs parlance) were connected—they shared buffers, internal state... etc. Switching between workspaces was really easy.
As far as I understand it, it's not so much "How fast does the final binary run compared to C?", but more "How much clarity and control does the language lend to the resulting runtime behavior?" People use C and C++ because it gives them control over their application's performance at the end of the day, such that it can be understood consistently. Sure you can optimize Haskell quite a bit, but if you run into a severe performance bug, how fast and reliably will you be able to understand and fix that bug? Could you do so on a tight deadline? And does your awesome optimization survive future versions of GHC? Is it portable in any sense of the word? How fragile is the fact that your Haskell project is currently meeting its performance requirements? How dependent is your code on GHC *just happening* to optimize it well? There's also a community culture to take into consideration here, too. If I have a performance-related question for C, there's plenty of C programmers who will be able to give valuable advice. But Haskell's community is pretty consistently in the "don't prematurely optimize" camp and doesn't seem as obsessed with that side of the language. Beyond general tips like "Don't use foldl", performance advice in Haskell quickly descends into a black art that only a few Haskellers are well-versed in. Not all of us are working on projects that push Haskell to its limits, first of all. And then there's just the overall high-level language dilemma with performance in regards to general abstractions - either you give up on it and commit to just being slow in the name of better developer productivity or you bring out a massive battery of optimizations that will hopefully fire at compile time that not only bloat compilation times dramatically, but also make the final speed of the program highly non-intuitive. Unboxing your data structures can be incredibly underwhelming in its results. Sometimes just not trying to help GHC at all turns out to be the best strategy, etc.
the course is highly *opinionated* - while IMO you can learn FP (and for this there are better MOOCs around) you probably will learn less Haskell than you would by going through https://www.seas.upenn.edu/~cis194/lectures.html on your own. For example last time I checked they used Hugs but if you are interested in Haskell as a language you probably should use GHC (the differences are the exact reason this course used Hugs) 
Wow! That TemplateAlgorithm (!). Looks like there is a type error on every line! Plus why a type class and not just a data structure? And why bring across OO design patterns. What are the functional ones?!
&gt; Haskell will give you the same performance In my experience, idiomatic Haskell will usually give you OK-ish performance compared to idiomatic C or C++ code. Sometimes, just a few tweaks give you C-like performance (and might be your own idiomatic style), but oftentimes you will need to work a lot more on it!
Oddly enough, I agree with your first three points, but I disagree with the fourth one. Expanding on ezyang's point from below, I've encountered libraries that export functions like the toy example below: foo :: IO Bool -&gt; IO () foo test = do print "Fix the problem, user" result &lt;- test if result then return () else foo test If my test function is wrapped inside a monad transformer, there's no way that I can use it in foo. However, if foo uses lifted types, then we could have foo :: (MonadIO m) =&gt; m Bool -&gt; m () foo test = do liftIO $ print "Fix the problem, user" result &lt;- test if result then return () else foo test
There's also elscreen which gives you something very like screen/tmux inside emacs
Did you read the article? The post is basically saying that writing general, high level Haskell with only obvious "optimizations" of very high time sync areas (e.g. calling Unicode functions on text guaranteed to be ASCII) will get you code that is, in general, faster than what you'd get using a similar strategy in C. Of course there are issues with the article: * The choice of benchmark (always something people can complain about no matter what benchmark was chosen) * the fact that no one in the comments seem to get the same benchmark results the author was getting * the fact that no nearly no C programmer actually writes this way. C programmers (of which I used to be one) tend to start out optimizing right from the start, at least to an extent.
tl;dr the comments on this are brutal. I thought I was in a safe zone for article comments but nope. for example: &gt;Mos Fring said... Instead of "TL;DR" where you mean "Summary" or "Abstract", why not consider "Summary" or "Abstract" in future? It will enhance the impression of competence and make your readers feel less like you're talking down to them. 
&gt; if you don't understand something, try your best but don't sweat it, try to move &gt; forward a bit, and then come back to that detail again and you might have a &gt; better intuition this time around. So true. I can never find the original quote, but someone once said something like "to learn Haskell, solve a real problem using only the techniques that you know; then repeat". This is how you get to and through the hard stuff.
Paging /u/rule
Does something like this exist for regular Vim? NeoVim is doing some amazing stuff, but I'm skeptical about switching before a stable release... EDIT: Apparently, NeoVim is stable enough to be your daily driver. Will give it a try.
I don't think this is completely true. Cabal will prefer other versions but there are other kinds of constraints that have higher precedence, such as reusing installed packages. Deprecation can help, but don't count on it. 
Message received :-) We have been very busy the last few weeks. The OpenCV binding has been moved out of our repository into its own public repository: https://github.com/LumiGuide/haskell-opencv The repository contains a working binding to OpenCV 3.1. It binds a subset of the original API. Eventually we want to have complete coverage of all the core modules. There is some [documentation](http://lumiguide.github.io/haskell-opencv/doc/index.html), mainly examples. The documentation is actually part of the test suite, so all examples are guaranteed to work. Building the library is a bit involved. We trick GHC into thinking the C++ compiler is a C compiler (see `cabal.config` inside the project). If you use [nix](http://nixos.org/) you can enter a `nix-shell` and simply `cabal build`. We plan on working on the library and making a proper announcement and initial release during the coming [ZuriHac](https://wiki.haskell.org/ZuriHac2016). (See the relevant [project](https://wiki.haskell.org/ZuriHac2016/Projects#OpenCV_3.1_Haskell_binding)). Everyone is welcome to help! *edit: documentation link
Great! A small pity is that `sbv` lacks support for dealing with inductive definitions, but that problem is inherent with any SMT solver anyway.
Just upgrading isn't enough, it seems, since point 2 still holds if you're using `randomIO` to generate UUIDs. So after upgrading, you should also switch to using `V4.nextRandom`. Can anyone confirm that that is correct?
I think this article is a bit misleading. First and for most each language is designed to tackle specific problems. Haskell makes development faster and is a well suited language for data processing but it's terrible for real-time/resource management. C is a language for optimizing and writting things closer to the metal without using assembly. The code example of 'C' provided is a very naive example normaly taught to beginners. Also the problem he's tackling is a lot better suited for haskell than C. There's also some flaws on his statement about cache mismatches and branch predictions. In Haskell Memory Management is indeterministic while C's is completely deterministic. This allows you to design your memory usage and optimize things which Haskell simply can't handle easily. On branch prediction: It depends on the metal. If you use x86-64 then haskell will definitely make very good use of branch prediction but on ARM there's no branch predictions. Haskell code, due to laziness, design and other factors tends to branch more often than it's imperative counterparts. Haskell can indeed be fast sometimes but one needs to know it's purpose and when to use it. Same case for C. My old Languages and Compilers teacher used to say "There's something worst than a bad programmer: one who only dominates one language"
I would be mostly interested in using it as another form of documentation. Generally we want to use the lowest version that provides the features we want - for some people that might just be V4 UUID generation (which goes all the way back to the first releases!), but they should be guided to still prefer the latest version.
You're right, that is correct.
I got this working last night. I'm concerned that the RCP server won't be merged upstream and this'll be out of date very quickly, as the forked intero repo is already 90 commits behind. It also requires you to be in visual mode and is extremely picky about the selection you send to be evaluated. It's lightning fast though, which is pretty amazing. *gets back to studying the neovim haskell plugin situation*
I've been using Neovim for quite a while, and honestly I haven't met anything that didn't work in Neovim if it worked in normal Vim. It is definitely ready to switch to!
Thanks!
Oh, sorry, I see from the context how my question was ambiguous. I was asking for an iterate function of type `iterateN :: Int -&gt; (Vector a -&gt; Vector a) -&gt; Vector a -&gt; Vector a`.
[removed]
Mos Fring's comment is a non sequitur.
I entirely agree with Mos Fring's first sentence.
Here for the reader's amusement are the results of [a benchmark of long compositions](https://gist.github.com/michaelt/7dc7b5328d30cab71d485cb6c0f154ed#file-compose-md) rather than single functions -- basically filter (\x -&gt; x `mod` 2 == 0) . map (+1) . drop 1000 . map (+1) . filter even translated into `streaming`, `iostreams`, `pipes` and `machines`. `vector` means the `...Fusion.Streaming.Monadic` streams following Twan v L's post. I used the github versions of pipes and machines. (The benchmark itself follows an example of [jweigley](https://github.com/jwiegley/streaming-tests/) )
Huh, cool, one more step towards CL style interactive development!
There's nothing stopping you from having both Vim and Neovim installed. You can even link Neovim's `init.vim` to your `vimrc`.
People keep saying this, but I'm never clear on what's out of date. Are there particular sections that are now dated, or is it just general rot across the whole book?
&gt; `type ShowS = TB.Builder -&gt; TB.Builder` Isn't this redundant? I thought: - the reason the original `ShowS` is a difference list is for efficient appending - `Builder`s _already_ support efficient appending. Am I missing something? --- Also, there is a bright side to GHC not inserting instances for `()`, I suppose: "My QuickCheck property passes! Oh wait, no it doesn't, it only works if all values are identical."
This is cool! foreign-store was made for this kind of thing, but there's some manual work involved. It's nice to see a wrapper library providing something more convenient! This also works with intero, FWIW. `M-x intero-devel-reload`.
Thanks for putting this out there. I definitely plan on taking a closer look when I get done with a couple of other tasks. Any other lessons learned in comparison to your peers who used other languages? It is intentionally open-ended as I am curious as to how other teams who presumably chose the language they were most comfortable in compared (in any way that measured or perceived) in accomplishing the same task.
source https://github.com/sboosali/vinyl-effects/blob/master/sources/Vinyl/Effects/Example.hs docs http://sboosali.github.io/documentation/vinyl-effects-0.0.0/Vinyl-Effects-Example.html
It seems to me like the proposed `Show` has the downside of requiring `showsPrec` to be implemented if the generic instance is to be over-ridden (rather than also allowing `show` to be implemented), since otherwise `showsPrec` and `show` may not match. I guess this is something that requires the built-in deriving.
Why wouldn't it be practical? Seems just as capable as anything else, with quite a lot of benefit to it
What would it take to get `text` into `base`? That seems like step -1 of replacing `String` with `Text`. GHC devs have done an incredible job (imho) with transitions and big changes in the last few years. What would a full plan for this transition look like?
This is the first I'm hearing of interaction combinators. \*frantic googling\*
It isn't really that old and it uses more advanced type system features. That was really all I was concerned about. Given jkarni's comment it sounds like there is nothing to worry about!
Ah. Yea that type level stuff requires some language extensions, which can be annoying. But I don't know of any breaking problems with it, aside from the cumbersome authentication.
I guess, what you mean is to have String support as PrimOps. I too have wondered "why not?" before. 
Neovim was a drop in for me as well. The only tweak was fixing tmux's esc delay time.
Yeah, I feed familiar with your advices because I have some Erlang background!
While I'm fond of the idea of replacing `String`, I'm not convinced that `Text`, in its current state, is the correct replacement. I only have one concern about `Text`, but to me it's a show stopper. It is that `Text` uses `UTF-16` for its internal representation of data (I think this is tied to the fact that ICU prefers `UTF-16` because Java Strings are encoded like that, but if someone with actual knowledge jumps in I'd be glad). This is a heavy bias to Windows, the only modern OS I know of that still uses `UTF-16`. Every Linux or BSD input or output needs to be converted from `UTF-8` and back to it. Also most of the world wide web (according to [Wikipedia](https://en.wikipedia.org/wiki/UTF-8) 87% of all websites as of June 2016 and rising) uses `UTF-8`. Here is some more [propaganda](http://utf8everywhere.org/). Then again, maybe this can be changed internally later without modifying the interface. I just think the replacement of `String` should not have its own baggage.
This looks quite nice! Hopefully a Haskell binding can fix the most irritating parts of OpenCV, namely it's API is very overloaded in a not typesafe way. I get endlessly annoyed by this type "InputArray" which is given as a template parameter, which can be often a cv::Mat, a std::vector or even a vector of Mat, you won't know which types it really accepts until you try it and it crashes! 
`ExplicitForAll` is enough to write `forall a. a -&gt; a` like signatures, or `ScopedTypeVariables` (which obviously requires it) which is probably a good idea anyway.
In this case flexible instances aren't requires, you can get away with two classes: class IsCalendar cal where next' :: Int -&gt; CalDate cal -&gt; CalDate cal class IsCalDate d where next :: Int -&gt; d -&gt; d instance IsCalendar cal =&gt; IsCalDate (CalDate cal) where next = next' instance IsCalendar cal =&gt; IsCalDate (CalDateTime cal) where next (CalDateTime cd h m s) = CalDateTime (next i cd) h m s Also using data kinds (like yours) or empty data types (like coopercm's) is dependent on whether you want to be able to add additional calendars outside your package.
&gt; Text uses UTF-16 for its internal representation of data Good to know. UTF-16 is a truly horrid format. Would the solution then be ByteString?
This. All the unnecessary conversions in web code due to Text being UTF-16 is really ugly.
Out of curiosity, can you expand on that? We've been waiting for negative feedback on it to figure out what should be changed/added/improved but didn't get any so far, which makes your comment interesting and worth expanding on =)
That http://utf8everywhere.org/ stuff is fascinating! There was a project 5 years ago to port text to utf-8. Here's the post about it: https://jaspervdj.be/posts/2011-08-19-text-utf8-the-aftermath.html I say we should revive the project! I don't care if some things got a little slower and the stream fusion doesn't work as well. The point of text should not be stream fusion benchmark results, it should be working well on real world data. Then again, these are smart guys and I don't have the full picture.
Awesome! I agree that this would be a great thing to address. One place that we unfortunately cannot easily escape `String! is exception handling. This is from two sources - the `Show` is a superclass and the `displayException` method. `displayException` is used by the default exception handler, and that's wired in. For `stack`, I recently wrote a bunch of utilities for `annotated-wl-pprint`. In particular, functions for displaying as `Text` and annotating the `Doc` with ansi coloring info, such as in `ansi-wl-pprint` . My code will actually be quite a bit less noisy with the control codes than `ansi-wl-pprint`, instead only outputting what's necessary. Here's the relevant code: https://github.com/commercialhaskell/stack/pull/2236/files#diff-5b5b1b4487cb1f108c64335c1b99acbdR206 Near the top, there's this comment: &gt; TODO: consider smashing together the code for wl-annotated-pprint and wl-pprint-text. The code here already handles doing the ansi-wl-pprint stuff (better!) atop wl-annotated-pprint. So the result would be a package unifying 3 different wl inspired packages. &gt; &gt; Perhaps it can still have native string support, by adding a type parameter to Doc?
Were you able to get it working with `ghc-mod`? Or did you just go on without using `ghc-mod`?
Sounds like you need [`async`](https://www.stackage.org/package/async). There is also a [lifted version](https://www.stackage.org/package/lifted-async) which does not need manually calling `liftIO`. Using it is pretty straightforward: call `async` to fork an `IO` action into another thread, and get an `Async` handle back. When the result of that action is needed, use `wait` or `waitCatch` to retrieve the result, and handle possible exceptions. EDIT: For anyone interested, I wrote a tiny implementation of a thread pool which you can dispatch `IO` actions to the pool for execution: &lt;https://gist.github.com/TerrorJack/f8edcb578d32fd21431e89eeb7580833&gt;
Every call to `async` spawns a new haskell-thread. This way every request would result in a new haskell-thread created. Haskell-threads are cheap but even they have their limits. Resource wise it would be better to write in some queue type which has a bounded number of workers reading from. 
Why does the internal representation matter? Out of curiosity. Aren't you supposed to explicitly encode/decode at the application boundaries anyway?
&gt; What would it take to get `text` into `base`? Since there are [ongoing discussions about splitting base into smaller packages](https://groups.google.com/forum/#!topic/haskell-core-libraries/TV0yGnfBTf4), I doubt moving the `text` package into the `base` package would be likely to happen. Probably the more likely approach would be to leave `text` as-is and simply make it be one of the packages that is shipped with GHC as is currently done with `base`, `template-haskell`, `integer-gmp`, `containers`, etc.
Don't be sold so easily on all the pro-UTF-8 propaganda. UTF-8 is great for western languages, but a poor choice for languages spoken by at least half of the people in the world. At work we process content - huge amounts of content - produced by enterprises that do worldwide localization, and most of that content is provided to us in UTF-16. I believe that the data in that Wikipedia article is skewed with a western bias. Much of the UTF-8 propaganda was initiated - and funded - by Google a few years ago, because at at that time it would have saved them money. But I'm not so sure it's true even for Google anymore, now that they operate much more in Asia than they did then.
I believe the benchmarks in that project were skewed towards western languages. And even then it was not clear that a switch to UTF-8 would be worth it. Run the benchmarks again with content that more accurately matches the proportion of people in the world that speak various languages, and I think it would be clear that a switch to UTF-8 would not make sense.
It seems that this is the [conclusion](https://github.com/yi-editor/yi/wiki/Future-work) that the authors of the yi-editor have come to (see: Should we dump Strings and ByteStrings in favor of Text where possible?). However then we are at a point where we treat Strings as bytearrays (hello `C` my old friend). I'd much prefer a fixed `Text` library. However, as /u/mgsloan pointed out, it seems there was an effort already and it resulted in no change. So honestly I don't know.
It's even worse than that. The set of real numbers whose sequence of bits is computable is a set of measure zero. So for essentially all real numbers you can't even write the program to compare them.
In typical workloads, thread-per-request shouldn't be an issue. But if you are willing to do some extra work, you can either: * Use a semaphore to control maximum thread count * Implement your own thread pool and dispatch your requests to the pool using a bounded channel
Even my bargain bin laptop can spawn 33,000 threads per second from a random GHCi statement (I feel like this is a regression from 7.10 to 8, but I don't have numbers I can quote on that). I feel pretty comfortable saying that they'll never be the bottleneck on a web server unless you're spawning like dozens per request.
Good catch! I've updated the code snippet.
The math behind Haskell is lambda calculus and the type theory to go with it with a touch of category theory (monads are the most category theory used) and denotational semantics. Learn those, and you will have learned the math behind the Haskell language.
Do they die off quickly? Spawning is one thing, but if you accumulate too many it would be a problem.
Ah, looking into it more, there doesn't seem to be a OAuth2 client implemented?
It depends on what you mean. Calculus is almost certainly the wrong answer. Calculus may be used for certain problems which you can solve in Haskell. But the same can be said for basically any field of human knowledge. There's very little in calculus that will specifically come up in the programming language itself. Elementary algebra (the kind that you typically learn around 14 years old) is critical to understanding Haskell. Evaluating expressions and function applications is the basic language by which you express yourself in a purely functional language, so being fluent in expression of ideas in that way will be a huge help (and vice versa). If you're looking at abstract algebra, the connections are less pronounced, but still there. Ideas like monoids, which come up all the time in Haskell, are rooted in algebra. The notion of identifying a concept by its structure and laws, which is fundamentally an algebraic style of reasoning, is used pervasively in the Haskell community for type classes. So overall, whether you meant elementary or abstract algebra, that's not a bad choice. Discrete math is a catch-all term for a lot of different things, so it's difficult to know what you mean here. But a lot of discrete math classes cover ideas in set theory and formal logic, which would definitely apply to programming in Haskell. Someone else mentioned category theory, which is another choice; but if you aren't ready for that yet, at least thoroughly understanding constructions with sets - including functions, cartesian products, and disjoint unions - is a good first step in that direction. (Category theory generalizes all of those ideas.) Logic is intimately related to types in a very deep way, and practice manipulating formal systems will be immensely helpful. So at least these parts of discrete mathematics have a significant synergy with Haskell, as well.
None currently available for servant right now, as far as I know.
Something to keep in mind though: diving directly into Category Theory is like diving into the deep end without knowing how to swim. I'm sure some people have, but they would've inadvertently gained a good understanding of Abstract Algebra in the process. My recommendation: if you're going to start diving into the mathematics, start with Monoids and Groups and *then* start diving into Functors, Natural Transformations and Monads. EDIT: I wrote big thing in another comment with my perspective.
This is likely to happen eventually anyway for two reasons: `text` is a dependency of `binary-serialise-cbor`, which I'm planning on working into GHC, so it'll come along with that. Independently of that, Duncan has plans to overhaul the .cabal parser, and to do that, he'll need parsec. So he'll also need to bring `text` along for the ride, too.
That's what I meant in my original post (I typed liftIO) and wondered if that'd be sufficient. Is it the same as what the asynchronous package would do?
In truth, it's very off. In practicality, most people really only need knowledge up to Monads, which I think was his point.
How important are thread pools in Haskell? I was under the impression that Haskell's threads were insanely cheap.
I wonder if compatibility could be addressed by generalizing from `String` to `IsString s =&gt; s`, similar to how `[]` has been generalized to either `Foldable f =&gt; f` or `Traversable t =&gt; t` with the foldable-traversable proposal. A more clever version of `IsString` may be necessary.
You can certainly use those fields to do some Haskell examples and thereby learn Haskell, but probably the best thing to take would be a **Real Analysis** course, or maybe Linear Algebra. The point of this is not any actual Real Analysis stuff, it's that usually this is also a university math department's first course in **mathematical foundations**. (If you find a low-level course that works that way, that'll be even better!) So your first few lectures will look like this: &gt; [Sets](https://en.wikipedia.org/wiki/Set_%28mathematics%29) are defined by what they contain: we write x ∈ S to say little-x is in the set big-S. We say two sets are *equal* when they both contain the same elements, that means that A = B means that if x ∈ A then x ∈ B, and also if x ∈ B then x ∈ A. &gt; &gt; There are some operations on sets, like A ∪ B, "A union B", which contains every element which is in either A or B or both; A ∩ B, "A intersect B", which contains every element which is in both A and B, and the set difference A − B which contains every element which is in A but not in B. &gt; &gt; The empty set written ∅ or {} is a valid set which has nothing inside of it, and from that we could build, say, the set that just contains the empty set and nothing else, {∅}, or the set that contains both the empty set and the latter set, {∅, {∅}}, which has 2 elements. Recursively we can actually build up the counting numbers this way, where the next set is always S ∪ {S} and always has one more element, so that we define 0 = ∅, 1 = {0}, 2 = {0, 1}, 3 = {0, 1, 2}, and so on. This is kind of cool because it unites two ideas which turn out to be deeply related in mathematics, the "is a subset of (or equal to)" relation ⊆ , and the "is less than (or equal to)" relation ≤. The set of all counting numbers {0, 1, 2, 3, ...} is called **N**. And we can define addition and multiplication and prove axioms about them in some obvious ways. &gt; &gt; When we start talking about the empty set we also have to mention trivial truth; to fit the pattern that "when you take an element away from a set S to make a new set S', for any other set X, if S is a subset of X, then S' is also a subset of X," we define that the empty set is a subset of every other set. This means that pretty much anything that you want to say about the empty set turns out to be "trivially true." It also means that if you want to say something interesting in mathematics, you first need to prove "existence": prove that the set of things you're talking about is not empty, so that you're not just talking about stupid trivial truths but you've actually got something to say. So you have this lecture on sets and basic set theory, and then you can come back to Haskell. Control question: go to GHCi and type `:m +Data.Set` to load the Set package, then type `let next n = union n (singleton n)` as above, is this expression well-typed? What is it about the polymorphic type of `Data.Set.empty` that allows the above definition to work in mathematics? Let's remove this polymorphism: because we never plan on containing any actual elements of any non-set type here! So we just need to write: Prelude Data.Set&gt; newtype N = MakeN (Set N) deriving (Eq, Ord, Show) Prelude Data.Set&gt; let zero = MakeN (Set Prelude Data.Set&gt; let next n@(MakeN s) = MakeN (s `union` singleton n) Now use `Data.Set.deleteMax` and `Data.Set.null` to recursively define `add (MakeN a) (MakeN b)` with the usual recursive definition (in English, "a plus b equals b, if a == 0; otherwise it equals (a minus one) plus (b plus one)." Use deleteMax to do the minus-one and use null to test if a == 0, with next doing the plus-one.) Then you get to another lecture where we start talking about more interesting things, like... &gt; [Ordered pairs](https://en.wikipedia.org/wiki/Ordered_pair) are a sort of glue to bind two things together, so (a, b) is the pair that lives in a space A × B called the "Cartesian product" of A and B, it's the set of pairs (a, b) such that a ∈ A and b ∈ B. Note that (a, b) is not the same as (b, a) unless b = a, and you can certainly take a Cartesian product of a set with itself if you want. &gt; &gt; [Equivalence classes](https://en.wikipedia.org/wiki/Equivalence_class) let us group things together according to an equivalence relation, where we say that two things are "equivalent." In order to say things are equivalent we should abide by certain rules so that we don't confuse anyone else: first that everything is equivalent to itself; second that if x is equivalent to y then y must be equivalent to x; third that if x is equivalent to y and y is equivalent to z, then x must be equivalent to z. Any definition of "equivalent" which satisfies these three axioms is called an "equivalence relation." The equivalence classes of a set under some equivalence relation are the subsets whose elements are all equivalent. &gt; &gt; With these two ideas we can get negative numbers, too. We define the elements of **Z** to be the equivalence classes of **N** × **N** under the equivalence relation that (a, b) is equivalent to (c, d) if and only if a + d = c + b. This makes b, d act like negative numbers relative to a, c, without inventing a separate version of the number -1 for (0, 1) and for (1, 2) and for (2, 3) and so on, instead -1 is this equivalence class {(0, 1), (1, 2), (2, 3), (3, 4), ...}. Haskell has, built-in, the pair constructor `(N, N)`, but you will need another `newtype` if you want to program this, because the default definition of `Eq` for a `Set` will check every single element of set A against every single element of set B, which for these infinite sets will take infinite time... either define `newtype Z = MakeZ (N, N)` or `newtype Z = MakeZ (Set (N, N))` and write your own implementations of `Eq` and `Ord` and `Show` for that, maybe. At some point someone will finally define functions in the Real Analysis course, in addition to all of this stuff about constructing a mathematical model for how numbers work: &gt; First off, some names for the things around the function: a function f lives in this set A → B (pronounced "A to B"), of all functions from the "domain" A to the "codomain" B. The function is a set of pairs (a, b) [hence it is a subset of A × B], obeying a couple of axioms. First, for every little-a in A, there must be some little-b in B such that (a, b) is in f. This means that the function must "cover" the entire domain: it doesn't have to cover the entire codomain in any similar way; the subset of the codomain that it *does* cover is sometimes called its "image" or some other such name. Second, for every little-a in a, if (a, b) is in f, and also (a, b2) is in f, then b = b2. So if you ever find that two pairs are in this function, and the first elements of the pairs are the same, you know that the second elements of the pairs are the same: which means that the pairs are the same, which means with the other axiom that there's exactly one (a, b) for every a. So if the first axiom said that every "input" in the domain must map to 1 or more "outputs", this says that every "input" must map to 1 or fewer "outputs", and that leaves? Exactly one output. Mathematicians write "y = f(x)" as an alternate syntax for (x, y) ∈ f, to really give the idea that the `x` is an input that the `f` is mapping to some output `y`. The key feature here is that this mathematical purity of the idea of a function is what makes Haskell a **functional** programming language. For other languages, a function is some *subroutine* that the program can call, it can do stuff in the real world, it can depend on a random number generator and return different answers when it is called with the same inputs, and so forth. Haskell intentionally doesn't have any of these superpowers. When you want to write something which interacts with the world, Haskell provides a high-level type of "a program which does something in the world and then produces a value of type x", which for shorthand we call an `IO x`. Haskell by its very nature tries not to actually *run* these programs (though there is a way to break this abstraction and do that, with `unsafePerformIO` and `Debug.Trace` and such). In fact Haskell just wants you to use pure functions to build up a program named `main`, which will be output as the executable created when you run `ghc my-source-file.hs`. That executable can then do all of the impure and dirty stuff of actually interacting with the world. Haskell's world is (hopefully) clean and pure and mathematically abstract: everything is just a function which maps its inputs into outputs and does nothing else on the side. If you can get those foundations, you'll understand Haskell a lot better.
Unless, as in the package I linked, that manipulation is done using UTF-8. You *could* do the wrong thing, but it is quite clearly false that any manipulation on it will, because utf8-string is a package which uses it and nevertheless does the right thing. Anyway, String and Text don't represent lists of characters either, but lists of Unicode codepoints, which are not the same thing. So, in your words, "any manipulation on it will do the wrong thing."
&gt; largely suited for solving constructed problems provided by a professor. They're also really good for quickly computing exact solutions to the kinds of problems computers are great at computing accurate approximations for... Which means, 50 or so years ago, they were extremely important branches of mathematics if you wanted to do anything within the STEM fields. These days, I'd argue sets, probability, propositional logic, proof systems and those parts of algebra are far more useful to the average student. Not because they'll use them formally, but because they *actually* teach you to think rationally and gives you tools to abstract over specific examples.
Ooh, you're correct that these branches of mathematics would've been far more useful before technology than they are now, I hadn't thought of that. Thanks for the perspective. EDIT: Parent comment changed, so I'm updating mine.
Yeah, I meant that Monads require more category theory than anything else in Haskell. In fact, learning Monads will likely teach you everything you need to know about category theory in Haskell, simply because all the other concepts are prerequisite to Monads (Monads are a type of Functor, which are something that exist in categories).
No I strongly disagree with that. Category theory encompasses a *lot* of concepts that are present in Haskell, many of which are out of the scope of monads.
Sure, a `ByteString` can represent a UTF8-encoded piece of text. It could also represent a UTF16-encoded piece of text, or a very large number, or an image. This (without a proper newtype) seems like a very error-prone representation for anything other than generic bytes on their way to somewhere else. Definitely not something to use as the default Haskell text type. And yes, `Text` and `String` represent lists of unicode code points. I didn't want to make the issue more complicated for someone who didn't seem to know the difference between `Text` and `ByteString`. So you can still do things wrong, but the amount of things you can do wrong is vastly reduced.
Currently using with SpaceMacs, this is awesome. Thanks!
Wow never thought I'd see this on here. I worked on this a years ago with, R Krawczyk, who created the idea. At the time we were relatively new the Haskell and this codebase got a little crazy with features we were trying to add in. It was originally on Google Code and I know he moved it over to bitbucket, but I've not looked it it in a long time so it could be a mess. It would have been cool to get working how we wanted, but he doesn't look at Haskell too much anymore and I've not picked hexena up in a long time. The original intent for me was to be able to analyze a PE file and be able to move in and out different sections of code to alter the structure of that PE file and have it still run. I learned a ton from this project even though it was never finished. I learned a lot about specs and implementation. The PE loader implementation does not conform to the specification. It'll practically run anything, and that becomes frustrating.
Excellent attitude! I'm a huge fan of the work you guys are doing (just got into the alpha and have been in touch with your man Alex). You guys have the right stuff, thats for sure!
Functions like f :: (Monad m, Monad m') =&gt; m (m' a) -&gt; m' (m a) are called [distributive laws](https://en.wikipedia.org/wiki/Distributive_law_between_monads) for monads. Your notion is stronger since swapM has to work for all monads, though.
In `base`, you're mostly right. But in other libraries that get used often in actual code, there's several examples. Kan extensions are a thing that get used occasionally to give functorality to types. Free monads are adjunctions. Comonads can be used for a lot of things, often to represent OOP patterns.
[Indeed](https://hackage.haskell.org/package/distributive-0.5.0.2/docs/Data-Distributive.html)
Oh, I like this. It actually captures what I'm doing better than my attempt because the IsCalDate class is just there to group common functions for CalDate and CalDateTime and the IsCalendar is for how to operate on a CalDate for a specific calendar (where my attempt was combining these two concerns into one). Thanks, this is what I was missing.
I didn't mean to be patronizing, and I'm sorry if I was. Giving your initial misunderstanding of my comment, I made that assumption which apparently I shouldn't have. But I still don't quite understand your position. You'd rather want to use a new library newtyping `ByteString`, most of which isn't written yet (compare the API of text and text-icu with utf8-string), instead of using text, which exists. What's the motivation?
It is painful to sprinkle String throughout code that could otherwise be free of it. You never need to touch String? That must be some unusual code. As much as we default to Text, there's still lots of usage of String due to `fail`, `show`, `displayException`, and filepaths. There are certainly libraries that use better representations for filepaths, but it is really difficult to avoid stringey filepaths in haskell, if you're doing a lot of filesystem stuff. It is actually questionable whether even `Text` is appropriate for filepaths. On one hand, the filepaths are probably human readable. On the other hand, in *nix and git, filepaths are just bytes. Modern linux distros default to treating them as UTF-8 for display purposes - http://unix.stackexchange.com/questions/2089/what-charset-encoding-is-used-for-filenames-and-paths-on-linux
Why should `distribute` operate on a `Functor`? If the goal is to distribute monads, wouldn't it be useful to have access to the `Monad` interface of the other type?
Good post, but I'd argue the Category Theory is a *generalization* of Algebra rather than a 'specialization' of it. This is evidenced by the fact that categorical constructions are useful even outside the field of algebra.
It's a terrible perspective, though. &gt;sets, probability, propositional logic, proof systems and those parts of algebra are far more useful to the average student [than calculus and the “other” parts of algebra]. The average student *of what?* While in industrial practice a civil engineer, let's say, will use finite element and finite difference techniques to analyse a particular design using big iron number crunching those techniques use formulae developed using (infinitesimal) calculus and algebra of the not-really-category-theory kind. Calculus—infinitesimal calculus—is the study of uncountably infinite sets and continuous functions between them and that's something that Computer Science—quite properly—has nothing to say about. But it is still the premier toolkit for analysing and understanding a vast range of physical phenomena.
That is a good point. I guess there are just generally fewer people who need to work out the *how* compared to the *what*.
How are you planning to do probability without calculus?
Denotational semantics then brings in topology, domain theory, and category theory.
Sorry about that, the parent comment edited and added some stuff that makes my comment seem out of place. I'm only really agreeing that the change in time period may not have been reflected in curriculum. I've updated my comment to reflect this.
There's no need for the `Monad` interface, just as there is no need for it with `Traversable` for which `Applicative` suffices. It _is_ a bit weird that you don't need some kind of `Coapplicative`. This has to do with the fact that Hask is a very simple category much like Set and all endofunctors have the necessary structure to be dual to applicatives.
From a technical perspective, you're absolutely correct. From a scholarly perspective, I think it's important to note that Category Theory is certainly a *derivative* field where you're required learn "bottom-up" rather than "top-down" simply because of how insanely abstract Category Theory can be. I think that "specialization" was the wrong word, maybe "extension" would've been better?
Those aren't Haskell specific. You could write a Comonad library in Python if you were so inclined. I was talking about the Haskell language.
Yeah, it is kind of weird in that respect. In a certain sense, category theory is completely elementary; one could, in principle, learn it without knowing *any* other maths. Practically speaking, though, you aren't going to get very far without a solid background in several fields that you can use to motivate the categorical construnctions. I think 'extension' is probably a better descriptor than either 'specialization' or 'generalization'.
Clicked knowing exactly what it would be &lt;3 
What are the prospects politically for this sort of unification?
Oh. Well then I don't really understand the importance of the point. Haskell as a platform deals with a lot of category theory, which is far more relevant than the arbitrary boundary of where the base language / library ends.
For Traversable, the logic behind Applicative is that Applicative is monoidal, which is all you need for the effect of traversing something. It's more intuition than knowable fact, as far as I know. What's the logic behind why Distributive only needs a Functor?
[Category](https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Category.html) and [Arrow](https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Arrow.html) come to mind.
Be careful when doing this that the data you are passing to the new thread is thread-safe. It's common to pull database connections out of pools, and if you do that and then pass the connection to the new thread, the connection might get put back in the pool in the first thread, and get garbage collected (causing bizarre errors).
This is known as a distributive law for monads. You might find this video useful although it's oriented towards pure math: https://www.youtube.com/watch?v=FZeoHPRoBVk
That's sone very important missing context.
What is the difference between intero and "ghc-mod legacy-interactive"?
Hmmm, I'm not quite sure what sort of answer will satisfy you. Would it help to say a comonoid _is_ a monoid, just in the opposite category? There's a whole lot of directions to go and connections to make in this and no one intuition is the _right_ one. `Applicative`s are monoids in the category of copresheaves with Day convolution so it's probably the case that coapplicative endofunctors are monoids in the category of presheaves. These concepts also relate to representable and corepresentable functors and so called "logarithms" of types. One neat application of `Distributive` is in Ed 's `linear` library where `distribute` acts to transpose matrices. That might be a good way to gain intuition for it. Fixed size containers (e.g. n-dimensional vectors) are prototypical examples of `Distributive`s so you can think of a special case: distribute :: V3 (V2 x) -&gt; V2 (V3 x) Maybe if I invoke /u/edwardkmett, he'll come along and give a better explanation than I can :-)
You can do something like this: {-# Language RankNTypes, TemplateHaskell #-} module Demo where import Data.Array.Unboxed import Data.Word import Control.Lens data Memory = Memory { _lowAddr, _midAddr, _highAddr :: UArray Word32 Word8 } makeLenses ''Memory addrLens :: Word32 -&gt; Traversal' Memory Word8 addrLens i | i &lt; midIx = lowAddr . ix i | i &lt; hiIx = midAddr . ix (i - midIx) | otherwise = highAddr . ix (i - hiIx) where midIx = undefined hiIx = undefined 
I'm extremely weak on this area of theory, so I'm sure this is either obvious or wrong, but based on a common simplified explanation of what Gödel's incompleteness means, the halting problem is incomplete - there are deterministic programs (that don't take arguments or inputs, so every run behaves identically) that cannot be proven to halt or not to halt, and such programs must be non-halting, and you cannot know that a particular program is in this set since that itself would be a proof of non-halting and therefore non-membership. If you can't know any particular program that has unknowable halting status, you certainly can't know the smallest. Of course unknowable isn't the same as unknown. 
I think my question is hard to answer because I'm not asking it very clearly =P With `Traversable`, the goal is to monoidally combine things. Thus, `Applicative` makes sense. With `Distributive`, what goal is there that requires comonoidal computation?
There was a lot of work on these kinds on things in the 1980s, but they never worked very well in practice. There are two well known problems you can run into, too little or too much parallelism. Too little: many programs are quite sequential (maybe because of the problem, maybe because of how they are written), and then reduction will only happen in one place at a time. Too much: Other problems have so much parallelism that doing everything in parallel will overwhelm the machine (like running out of memory).
I would say that it's because making a good choice between parallel or sequential evaluation requires knowledge about "meta" data -- in the sense that I not only know that I have a `Particle` data type, but that I somehow know I will be pushing enough particles to merit parallelism. This information is rarely encoded in a place where the compiler is capable of reasoning about it, so it is unable to make good decisions about parallelism. In principle, though, I think it would be great to encode this information somehow, since given access to the right information, it seems like something an algorithm should solve much better than I would.
&gt; The Haskell version just a perfect way of describing this algorithm Of course it isn't. The real algorithm uses destructive assignment to sort in-place. [A faithful Haskell version](http://augustss.blogspot.co.uk/2007/08/quicksort-in-haskell-quicksort-is.html).
[Lance Williams]( https://www.cs.unm.edu/~williams/pubs.html ) has been working on something right along those lines. * Williams, L.R., Programs as Polypeptides, European Conf. on Artificial Life (ECAL '15), York, UK, 2015. * Williams, L.R., Self-replicating Distributed Virtual Machines, 14th Intl. Conf. on the Synthesis and Simulation of Living Systems (ALIFE '14), New York, NY, 2014. * Williams, L.R., Evolution of Tail-Call Optimization in a Population of Self-Hosting Compilers, European Conf. on Artificial Life (ECAL '13), Taormina, Sicily, 2013. * Williams, L.R., Robust Evaluation of Expressions by Distributed Virtual Machines, Unconventional and Natural Computation (UCNC '12), Orleans, France, 2012.
Algebra, including category theory, and also one you don't mention: mathematical logic including in particular lambda calculus and type theory.
I would suggest using this type of Traversal (or something similar) to implement https://hackage.haskell.org/package/lens-4.14/docs/Control-Lens-At.html#t:Ixed.
Sorry, there is no magic.
I'd love for someone more knowledgeable to comment on this. This brings to mind the complex algorithms that are used in CPU ultra-low level optimization. Basically, I think something like this may exists, but probably not in a purely functional way. 
If we're going to fill out more of what we could do with this type with lens we can have {-# Language RankNTypes #-} -- addressSpace's type signature {-# Language TypeFamilies #-} -- Index and IxValue for Ixed {-# Language TemplateHaskell #-} -- makeLenses {-# Language FlexibleContexts #-} -- addressSpace's type signature {-# Language MultiParamTypeClasses #-} -- Each module AddressSpace where import Data.Array.Unboxed (UArray) import Data.Word (Word8, Word32) import Control.Lens data Memory = Memory { _lowAddr, _midAddr, _highAddr :: UArray Word32 Word8 } makeLenses ''Memory lowStart, midStart, highStart :: Word32 (lowStart, midStart, highStart) = (0, 1000, 2000) memoryIx :: Word32 -&gt; Traversal' Memory Word8 memoryIx i | i &lt; midStart = lowAddr . ix (i - lowStart) | i &lt; highStart = midAddr . ix (i - midStart) | otherwise = highAddr . ix (i - highStart) addressSpace :: IndexedTraversal' Word32 Memory Word8 addressSpace f x = Memory &lt;$&gt; aux lowStart _lowAddr &lt;*&gt; aux midStart _midAddr &lt;*&gt; aux highStart _highAddr where aux off sel = reindexed (\i -&gt; off + fromIntegral i) (indexing each) f (sel x) instance Each Memory Memory Word8 Word8 where each = addressSpace type instance Index Memory = Word32 type instance IxValue Memory = Word8 instance Ixed Memory where ix = memoryIx This integrates with Each, Ixed, and provides an indexed traversals so we can visit all the elements of memory and know what address they're at.
So, does this mean if I update my channel for say a security patch to some c lib then library changes, ghc version, etc come with it?
**EDIT** my n00b error was evidently 25.1.50.1: brew install emacs-mac ... fixed everything. Leaving this comment in case anyone else goes down my false path. * * * Trying a clean install, I get numerous gpg verify failures (not to mention a failure to retrieve packages via https, hence `--insecure`): $ emacs --version GNU Emacs 25.1.50.1 $ git clone &lt;spacemacs&gt; emacs.d; emacs --insecure # add `intero` to dotspacemacs-configuration-layers $ git clone &lt;spacemacs-intero&gt; emacs.d/private/intero; emacs --insecure Results in: --&gt; installing package: cmm-mode@haskell... [1/6] An error occurred while installing cmm-mode (error: (bad-signature cmm-mode-20150224.2346.el.sig)) ... and similarly for `haskell-mode`, `yasnippet`, `hindent`, `flycheck`, `shm`... what n00b error am I making?
Have a look at the justification, it seems that the move will actually be *better* for security patches. 
What's the relationship between transformers and distributive monads? I believe that all distributive monads can be made into transformers like this: instance (Distributive d, Monad d, Monad m) =&gt; Monad (Compose d m) a &gt;&gt;= k = ... instance (Distributive d, Monad d) =&gt; MonadTrans (Compose d) where lift a = Compose $ return a But obviously not all transformers make distributive monads. So I guess my main question is, what sorts of monads can you turn into a transformer that can't be distributive?
It turns out that every legal `Distributive` functor is (co)representable. That is to say `f a` is isomorphic to `(e -&gt; a)` for some `e`. This isomorphism may be non-trivial, but it is there. The distributive law for function spaces like that only needs functoriality, not a monad. The constraint would actually be `Comonad` for `Distributive` to be dual to `Traversable`, but preservation of limits for right adjoints is enough that there isn't any examples that need the `Comonad` rather than just a `Functor`. Hence why this winds up being useful for monads as well. Well, that isn't quite right. In reality you don't need a full `Monad` to traverse a `Traversable`, you just need `Applicative`. And every Functor has enough structure to be co-`Applicative`, in the same boring way that every comonoid in Hask is trivial. So the 'extra' constraint you need is trivially satisfied by every `Functor`. Because of that sneaky co-`Applicative` thing, it would have been much trickier to discover `Traversable` knowing `Distributive` than it was discovering `Distributive` knowing `Traversable`!
Oh that actually makes a ton of sense. In my post, I made `MonadSwap` like so class Monad f =&gt; MonadSwap f where swapM :: Monad m =&gt; f (m a) -&gt; m (f a) I guess this is actually `CoDistributive`? Since the arrow is just reversed. I put the `Monad m` constraint out of assumption, but if `Distributive` uses the `Functor` constraint as effectively a `CoApplicative` constraint (as someone else said in another comment), then I suppose the constraint should be `Applicative`. Anyway, this seems fairly different from `Distributive`, as it can be instantiated on lists, for example (or any `Traversable`, as I originally showed). What's your take on this? *(Also, sorry for my near-endless stream of questions about monads / category theory / etc.. I've taken a lot of your time in the past few months querying your knowledge =P)*
My dreams of upgrade lock in can finally be realized.
Yep. The Jones and Duponcheel paper I linked up above somewhere gives a few other ways distributive/absorption laws can be used to build monads. IIRC, they used distributive laws for monad w.r.t a pointed functor, though.
[Using Circular Programs for Higher-Order Syntax](http://www.cse.chalmers.se/~emax/documents/axelsson2013using.pdf) is great, you present a **surface** representation where users can construct terms with regular Haskell functions (higher-order): lam (\a -&gt; a + a + 10) but it evaluates — purely ­­— to the following (first-order) language Lam 1 (Add (Add (Var 1) (Var 1)) (Lit 10)) also ghci&gt; lam id Lam 1 (Var 1) ghci&gt; lam id · lam id App (Lam 1 (Var 1)) (Lam 1 (Var 1)) ---- Code taken almost directly from the paper: data Exp = Var Int | Lam Int Exp | Lit Int | Add Exp Exp | App Exp Exp deriving (Show) instance Num Exp where (+) = Add fromInteger = Lit . fromInteger lam :: (Exp -&gt; Exp) -&gt; Exp lam f = Lam n body where body = f (Var n) n = 1 + maxBV body (·) = App maxBV :: Exp -&gt; Int maxBV = \case Var{} -&gt; 0 App f a -&gt; maxBV f `max` maxBV a Add a b -&gt; maxBV a `max` maxBV b Lit i -&gt; 0 Lam n _ -&gt; n
The LTS window is something like 3 months. By Haskell ecosystem standards that _is_ long term support. ;)
I guess it makes sense. If they'll be only loosely following LTS releases and diverge here and there to include security fixes and stuff, it's no longer genuine Stackage™ and therefore should be called differently to avoid muddying the waters. They could call it Nixage instead.
circular programming is also known as "tying the knot" https://wiki.haskell.org/Tying_the_Knot The repmin example is definitely the classic: https://en.wikibooks.org/wiki/Haskell/Laziness#Tying_the_knot
Either I'm still *super* a novice at programming, or haskell has hit some really deep vein of math/ideas/CS/magic, and ppl keep mining away at it. Or both. That's certainly the case with me+haskell. Thanks for sharing, I've got such a huge queue of stuff to study.
It's an important perspective to have, though I wouldn't completely discard the old perspective. The issue is, these days CS is such a broad field that no one perspective can encompass everything. These old specializations like group theory and convex optimization are still very much alive and well— in very specific contexts, where people still need to get the most out of big iron. While a lot of the current problems being tackled are intractable (or impossible) to solve exactly —so we can't rely exclusively on old mathematics—, it's still crucial to know when various subproblems *can* be solved exactly, so we can try to break things up into more manageable chunks and only resort to approximations when computationally necessary. Moreover, knowing the *techniques* which render various things solvable in closed form is still crucial to knowing how to go about approaching things that can't be solved in closed form. It's just that "most" of CS isn't that kind of CS anymore; for the majority of people, grabbing an off-the-shelf tool is sufficient to get adequate performance. So here the crucial thing is knowing about the semantics of such libraries so you can glue them together in an efficacious way. This is where tools like logic and abstract algebra become crucial: the focus is on constructing composable interfaces which are easy to reason about, rather than focusing so much on absolute performance of a particular implementation. I do think that the way maths/CS is taught should be updated to reflect this split, but that doesn't mean the old mathematical subjects are obsolete, just that they should be taught within a larger context.
This seems closer to [A-Normal Form](https://en.wikipedia.org/wiki/A-normal_form) than imperative so I would *hope* that `bound` supports it.
ahhh, I was wondering if there was a language extension i needed. thanks!
And "support" merely/for-now means something like: only new versions that do not break api (as per pvp) are allowed.
&gt; LTS Haskell 5.x receives no updates either after LTS 6.x has comes out, so by the same logic there is no point in distributing LTS 5.x after LTS 6.x has become available. I don't understand this logic. Even though 5.x isn't receiving updates, there may still be people using it, right? Do I misunderstand the purpose of this particular set of packages? If I'm using Nix packages, would I be forced to update as soon as a new LTS version comes out, or find somewhere else that hosts the 5.x versions?
It's a matter of limited resources. LTS is still a young concept and some experimentation is going on. Maybe once it's settled down...
I remembered meeting you and talking about the example, but without the PR to show the OP how it was done I hesitated to recommend the approach. If you wanted to throw together an example of using `bound` 'side-ways' in this manner, it'd be a good contribution to the project, and could go a long way towards showing off the technique.
It isn't really geared for things in ANF either. ;) You can use something like gelisam's trick for both imperative languages and various normal forms, but it isn't really what the library was designed for.
indeed, off the shelf ... theres some pretty heavy lifting to get ANF working right. I'm actually trying to write up a paper and some code that goes through a collection of Bound inspired/applicable tricks to make ANF / CPS a bit saner to support with Bound / similar style libraries
nice set of notes. I like the stuff you cover at the top. One really good /nice way of making that blocking work a bit more composable is to use `atomically :: STM a -&gt; IO a` and have your stm code call `retry` after it goes through the list of RX (receive) sources and they're all empty. This has the really nice property that the associated green thread then sleeps until at least one of those STM variables gets a change :) it takes a little while to adjust to that style of thinking, but its kinda nice and avoids a lot of other locking issues :) and it lines up with the "epoll/kqueue" friendly pattern you allude to (as does... by construction, the GHC.Event networking code) how did you model your state machines? I've been playing around with coinductive modelling stuff lately
Are you sure this applies to interaction combinators? It is entirely different from combinators/supercombinators, it is just a similar name. It has very different properties.
It's coincidental. We're most likely to see a previous-major-version LTS release only around something extreme either in terms of security or difficulty upgrading. Maybe the GHC 7 to 8 transition will involve multiple active LTS releases while everyone slowly migrates to 8.
Isn't the inclusion of non-breaking upgrades what you want for "support"?
The only logic to the plan is if you think erasing all version information from hackage is a good move. What you can then do is find a good point to snapshot everything and work with that state of hackage for a while. At some point in the future, you take a new snapshot and break everyone's code. To avoid breakage, you can snapshot the nixpkgs that has the hackage snapshot you like. You now have security vulnerabilities in all your software.
thanks, I love this stuff, and your examples help a ton for me to understand what is going on. This concept of observable sharing means passing around references to data, right? This has been on my mind recently, and from the limited amount I understand of reification, is that what the big plus is? That you can pass around references? I've been trying to think of a way of accomplishing the same with the Free Monad patterns, do you have any ideas on that could work?
&gt; I asked about how one stays with a previous LTS after nixpkgs upgrades and was told to pay a NixOS dev to cherry-pick patches for me. No, don't. Just depend on an old version of nixpkgs. My dev environments do *not* update their nixpkgs at the same speed as my system or user account (there, it's stable vs. unstable), I only do it manually: with import (fetchTarball https://nixos.org/releases/nixpkgs/nixpkgs-&lt;version&gt;/nixexprs.tar.xz) {}; ...which means that they don't bitrot (of course, you can also `fetchGit` for greater granularity). If you want to bump a version without changing the rest, you can override individual packages, no need to mess around with nixpkgs files. 
Which only proves that context is as important as data.
&gt; The need for this is certainly a lot less than it used to be now that Stack exists, the only thing that this really buys you over just using Stack is that downloading it might be a little simpler. 
by Polya, right? Brilliant stuff
Actually, by Velleman - although Polya is great as well. I normally recommend Velleman because you only need high school maths going in, there's _tonnes_ of exercises, and it gives you everything you need for TAPL (although most of the Amazon review also say it gives you everything you need for a real analysis course).
The AWS S3 for 8.0 (next/8.0) repository does not exist. I cannt upgrade to newer version of stack
Doesn't the STM approach fail to handle the case where the network dies and nothing ever comes down the wire? You can't sleep until one of the vars changes in that case.
I see, it will break the first law.
I agree. It feels professional. How does one generate an installer? Is it through Xcode? Or a package like https://hackage.haskell.org/package/nsis-0.3/docs/Development-NSIS.html ?
So you keep track of all non-Haskell dependencies you use and build those updates into the nixpkgs version you've pinned? Or do you just run with old, eg, `OpenSSL` and hope for the best?
I was trying to understand your use of scare quotes around "support". It seems like what is offered for today's LTS6 *is* support, contrary to what is said by peti.
I quoted it to show that it is the word we're trying to define. 
If I was doing or using anything security-critical then yes I'd keep an eye on the openssl version. Generally, though, it's mostly internal tools that do things with standard input, output, and files. What I care most about there is that when I need to extend them or fix a bug I don't have to fix random bitrot, too: It might've been a year since I last touched that code. Not that it's a good idea to let things stew in isolation for too long, but divorcing fixing bitrot from making functional changes makes for a much, much, smoother development flow.
Thanks for clarifying, and sorry for misunderstanding! I got stuck in a rut of defending Stackage updates.
I believe that is the primitive that Cloud Haskell ends up using at some point, but I could see several benefits to using STM directly (as opposed to e.g. `match`). I don't understand your question. In what sense of "model"? Do you mean in terms of formal modeling? 
Clearly. Basic notions are necessary. Monoids, groups, notion of equivalence class, canonical surjection, the isomorphism theorems, etc that's something to know. As for the added value, the more you know about math the easier it is to see it as unifying all those similar constructs instead of proving once again product topology, product measure, product...Or cps, yoneda, church ..
oh yes, I got mixed up with the titles :)
[What are Types for, or are they only Against? (*18:51*)](https://youtu.be/3U3lV5VPmOU?t=1131) &gt; Let me just, yes apologize for one of my favourite perversities is defining infix operators with 3 arguments. &gt; &gt; I just can't stop, I'm addicted to it. &gt; &gt; *—Conor McBride* Stop channeling my brain Conor. The definition of composition is beautiful (f · g) x = f (g x)
You can create a table with non-integer id (in Mongo) like this: FooDB sql=foo Id String sql=_id
[PR sent](https://github.com/ekmett/bound/pull/26)!
You should probably also describe what the issue you're having is, in as much detail as possible.
Have you looked at Groundhog? It lets you disable the default primary key and mark another field(s) as primary. Updating tables that do not any unique keys works too, even though they can not be referenced. https://github.com/lykahb/groundhog/blob/master/examples/keys.hs
Have you tried the language extension that delays raising type errors until evaluation?
...monads unchained...Downvote me, I regret nothing! 
I haven't yet but I'm considering using it. However, I have a few questions - Does Groundhog generate models (data type) like Persistent do ? - Is there a tool to generate everything from the DB schema ? I don't mind having to tweak things manually if necessary. - How easy is to switch from Persistent to Groundhog ? - Does sum types works ? (I'm really really interested in this feature). - Is there a way to change dynamically the table name ? The database I'm using prefixes all tables by 0_ 1_ etc ... Thanks.
I'm using MySQL and it also works. Brilliant ! 
You might like [Unembedding Domain-Specific Languages](http://bentnib.org/unembedding.pdf) by myself, Sam Lindley and Jeremy Yallop. It describes a way to use a higher-order surface representation (called HIgher-Order Abstract Syntax (HOAS)): lam (\x -&gt; lam (\y -&gt; app x y)) and how to convert it to a first-order representation for analysis, interpretation, or compilation. We cover conversion from HOAS to untyped first order syntax, and also conversion from HOAS to GADTs that ensure that all terms are well-scoped, or well-typed. Section 3 discusses how to build specifications of languages modularly, and Section 5 has nice little examples of using the system for mobile code and relational database queries. The paper is from 2009, so some of the things in the paper could be done more nicely in modern Haskell. For example, the representations of types using a type class could probably be done much more neatly using DataKinds. Unlike the approach in the Axelsson-Claessen paper posted by /u/Iceland_jack, our approach excludes the construction of terms that are meaningless, such as: lam (\x -&gt; case x of Var _ -&gt; x | App _ _ -&gt; lam (\x -&gt; x)) Also, their description of HOAS on page 5 is wrong, though it is one of those persistent sorts of wrong that is on its way to becoming one of those "oh, you know what I mean" things. Properly, HOAS isn't defined as a recursive datatype, it is a collection of operations like `lam`, `app`, `var` that allow building of terms.
Have you tried the new rapid package?
&gt; In practice, most of my compiler errors are type errors. This is true at least for experienced programmers, but I'd love to see more deferred errors in the future. There's really no reason we shouldn't have a `-fdefer-syntax-errors` as well!
[Using Circular Programs to Eliminate Multiple Traversals of Data](http://ocean.sci-hub.cc/6c12216f103bd2ca3e5c3af4f901c4b1/bird1984.pdf) by Richard Bird is a great paper that expains the concept of circular programs.
query conn "sql stuff here" :: IO [Only String] Also, don't use '*' in your sql. Enumerate the fields.
* Groundhog does not generate data types. This design decision makes it more of an lightweight library than a framework. So the type declaration is independent of the database layer. The data types are analyzed to infer the defaults so the mappings are often more concise than in Persistent * https://github.com/lykahb/groundhog/tree/master/groundhog-inspector generates data types and mappings from the schema. If you have a large schema tweaking the naming style may be faster than editing the output * Groundhog and Persistent APIs are similar for many operations which makes switching easier * Yes, there is solid support for the sum types. Storing them as an entity with a table per constructor https://github.com/lykahb/groundhog/blob/master/examples/sumTypes.hs Storing them as a single column https://github.com/lykahb/groundhog/blob/master/examples/primitive.hs * Data type name does not have to match the table name if this is your question See more at https://www.schoolofhaskell.com/user/lykahb/groundhog and https://github.com/lykahb/groundhog/blob/master/examples
Good luck and thank you for the effort :) I will have a better look on the text later, just a quick remark now: have you considered using the minted package to render the source code examples?
Woops! I read the big red text "DO NOT SUBMIT TO REDDIT" as: do not submit bug reports to reddit, though in hindsight I don't know why I would have thought this. Honest! I hope I did not call FP Complete out on a more official announcement.
I'd like that too, but I bet it's tricky to get the parser to resume intelligently after an error. I could be wrong on that though.
... and merged =) Thanks!
... and merged =) Thanks!
Merged!
Thank you for that! I was hoping something like this existed.
Surely some type error :) Jokes aside. http://www.snoyman.com/ is down as well. Curl: ❯ curl -v http://www.yesodweb.com/ * Trying 104.197.79.86... * Connected to www.yesodweb.com (104.197.79.86) port 80 (#0) &gt; GET / HTTP/1.1 &gt; Host: www.yesodweb.com &gt; User-Agent: curl/7.43.0 &gt; Accept: */* &gt; * Recv failure: Connection reset by peer * Closing connection 0 curl: (56) Recv failure: Connection reset by peer ❯ # same for snoyman.com . Exact same IP as well So a wild guess: a faulty load-balancer or rev-proxy. And since it's saturday (in western europe: ~midnight, depending on exact location), they'll fix it tomorrow.
Not sure I understand. Do you mean you want to keep variable bindings when you load or reload a Haskell file? 
Maybe we need an LTS LTS and a nightly LTS until then.
Thanks, very interesting! I wonder if there is a pattern lurking here, maybe a weaker notion of abs/inst is the key: interestingly `abstract` can be written with a Functor constraint Bound.abstract :: Monad f =&gt; (a -&gt; Maybe b) -&gt; f a -&gt; Scope b f a Bound.Scope.Simple.abstract :: Functor f =&gt; (a -&gt; Maybe b) -&gt; f a -&gt; Scope b f a `instantiate` can't **Edit:** Works for functions from PR pAbstract1' :: (Functor op, Functor id, Eq a) =&gt; a -&gt; Prog' op id a -&gt; Prog' (Scope () op) (Scope () id) a
I really enjoyed this video. This, alongside Conor McBride's recent ["What are Types for, or are they only Against?](https://www.youtube.com/watch?v=3U3lV5VPmOU) video has opened my eyes to the idea of types _writing code for you_, not just verifying the correctness of the thing you did write. I get the sense I don't actually fully understand the implications of this, but it feels profound.
Thank you for your answers! With regard to the unsafe FFI how soon is "very soon" ?
That's depend on your application, some people even mark sql query function as unsafe to skip the OS thread switching overhead, you have to understand that: 1. once enter a foreign function, ghc rts lost the control of the stack/register/memory map...what so ever, so we can't do context switch during the function running. 2. a unsafe FFI will running on the same thread as your haskell code which make the call, if the call never return, the worker thread is stalled, and the capability it's running on is blocked. So make sure it will return, and the waiting is worthy. 3. use safe FFI to attack this problem is kind of overkill IMHO, a lot of people may recommend you using safe, but the truth is that now your function call will be running on a separated(maybe new created) OS thread, OS will do context switch instead of ghc rts to make sure it won't block other worker thread. but a OS thread context switching is expensive(save/restore the register/stack...). I always have a problem with the safe and unsafe naming, if you think a c function is safe, mark it with unsafe!
It's not even a true statement. We've released minor bumps of old LTS versions in the past.
Since your SQL code has to match your Haskell code exactly, using '*' will cause a runtime error if you ever add a field to whatever you're selecting. Although schema changes are rare, they do happen from time to time.
so rarely do I see performance profiled so diligently on the web. as a newbie would love to know more.
Here's the full function: findLink :: String -&gt; String -&gt; IO (Maybe Link) findLink dbConStr hash = do conn &lt;- connectPostgreSQL (C.pack dbConStr) xs &lt;- query conn "select url from url where identifier = ?" (Only hash) :: IO [Only String] -- identifier is a unique token to identify a URL in the db, outside its surroage PK -- have also tried Data.Text.Text as the [Only T.Text] explicit type declaration above, but same error return $ if null xs then Nothing else Just $ Link $ head xs Thank you for your help.
I'm glad to hear that! That was my goal with the talk, but when I finished it I wasn't entirely convinced I'd conveyed the message I wanted (but was generally happy with whatever message I did end up delivering :)).
Learn about the difference between initial and final style for typed DSLs.
&gt; consult an RNG to generate the bits That's where it breaks down. If you use a PRNG, you can only generate computable sequences of bits. If you use a physical RNG, like one based on a Geiger counter or radio noise or whatever, then at any given point the future output is undetermined so you have not yet picked a real number. I don't enough about "quantum real number generators" to know if they can generate a fully determined real number out of an uncountably infinite space.
First things first: Don't optimize but until the end. Haskell code is easy to refactor, so think about optimizations at the end. 1. Don't worry too much about using immutable state, the compiler and laziness will guarantee all the needed optimizations to avoid copying big chunks. Also as someone else mentioned, the State Monad and Writter monad are lazy, they don't really modify the state strictly, unless it's needed. In case you really need mutable state, you can use ST Monad or IORefs. I suggest using those only at the external layer and when you need interaction/IO. 2. Read http://research.microsoft.com/en-us/um/people/simonpj/papers/ndp/haskell-beats-C.pdf It will show you one very popular optimization called stream fusion. Normaly, you want to use this kind of packages. 3. Look at: http://dev.stephendiehl.com/hask/#pure-functions. It brings a lot of examples on using Haskell's FFI. There are also tools like c2hs that help building such calls.
Fair enough, but you'd think abstracts could be shared by now.
Usually someone creates a github repo with links to papers and abstracts. You could make it and post a link. Yes, this is something the ACM should do, but basically there's no hope in expecting it to do anything useful (note the very slow progress on open access) so we have to route around the damage as best we can. 
Did you define Link as 'newtype Link = Link (Only String)'? It would be really odd if you did. But your code wouldn't compile if you didn't... Something is fishy here.
It really looks like you're doing something like this: &gt; query conn "select 'hello' :: text" () :: IO [Only String] [Only {fromOnly = "hello"}] &gt; query conn "select 'hello' :: text" () :: IO [String] *** Exception: Incompatible {errSQLType = "text", errSQLTableOid = Nothing, errSQLField = "text", errHaskellType = "Char", errMessage = ""} You need to tell the library that you're expecting a single String instead of a list of Chars.
It's not difficult to make something that works quite well. You just save all the bindings the user has made, and after a module is (re)loaded you just do all the bindings again. And you can just ignore the ones that fail. That's what we do in our interactive Haskell. 
I do think free monads are overused though. I think the mtl approach is a bit misunderstood. I think people often don't realize that it's not limited to transformers.
doctest is awful compared to what they've got in racket.
Yes, the original function in my OP was using a function that would cast to TEXT in the SQL, but I simplified it out to use a straight query to remove such complications from the equation (so to speak). I even added casts in the SQL to VARCHAR to try and stop it treating it as TEXT. To answer your other question, `Link` is defined as `data Link = Link String`. It must be something stupid as none of this makes sense.
No, it's defined as `data Link = Link String`.
I agree with most things said in that post, actually. Especially the String thing is infuriating! However, while documentation is frequently pretty bad, it's usually not quite as bad as that post would make you believe. (In my opinion, anyway) Sure, the lens documentation is awful, but not every library is lens. Many libraries are comparatively small and come with usage examples. I'm not saying documentation of Haskell libraries is generally good. It is not. But it's not *that* bad, either. The other thing I noticed was DataKinds. While, as it is right now, it may seem weird to reuse the data keyword for kinds and you might wonder what you do when you need to promote even further, keep in mind that GHC 8 already adds TypeInType, which fixes the latter problem. The reuse of the data construct makes sense, if you consider DependentHaskell, which is a planned extension. A great many extensions will end up being enabled with DependentHaskell and they need to all work together. It may seem odd right now, but it will all make sense once dependent types are fully implemented.
The post author provides [this link](http://docs.racket-lang.org/lens/lens-guide.html).
I def enjoyed the data fun paper, though I'm wondering if proving montonicity via a richer proof theory would result in any fun leaps in expressivity over the finite bounds approach ;) There's a lot of cool papers in this years that I look forward to read. Suprisingly large number apply to work projects I'm doing!
I think the point of the author is that to create such documentation, one would have to use tools that are not haddock. Not sure what you mean by hyperlinked, it looks more like literate Racket + "sidenotes" to me. I clearly wouldn't want to do that with Haddock, and stuff like [this](https://hackage.haskell.org/package/pipes-4.2.0/docs/Pipes-Tutorial.html) is the best we have. It just doesn't look as friendly, and is brittle against code changes. Note how [results are computed](http://docs.racket-lang.org/lens/lens-reference.html#%28part._streams-reference%29) based on the [documentation](https://github.com/jackfirth/lens/blob/master/lens/private/stream/stream.scrbl). I think this is better than having a syntax for emulating GHCi output ...
https://github.com/gasche/icfp2016-papers
Recently discovered `stack haddock --open` and it's a pleasure to use offline. Unfortunately it doesn't list the dependencies (with version) as the generated pages on hackage do.
&gt; So if I change my API but I forget to update the examples in my documentation, I imagine that Racket will happily generate beautifully laid out, friendly, incorrect documentation Well the documentation would be correct, if irrelevant ...
&gt; I like doctests, because like types, they are a machine-checked form of documentation! Got it, I like doctests too then :) 
I am a bit surprised by the prominence of phantom types in this (excellent and enjoyable) summary. Don't GADTs generally provide the same benefits, while being more expressive?
&gt; Recently discovered stack haddock --open and it's a pleasure to use offline. Thanks! :) &gt; Unfortunately it doesn't list the dependencies (with version) as the generated pages on hackage do. Well, `stack haddock` is only a wrapper around haddock and haddock doesn't generate these package description pages. AFAIK that functionality is separately implemented by hackage and stackage. What exactly are you interested in? The links to the dependencies' haddocks? The version bounds? As a workaround you can currently `stack unpack &lt;package-name&gt;` and then look at the cabal file.
Putting it up here because it looks like it should be very practical
Have you tried [stack](http://haskellstack.org)? I'd expect it to be more robust against system changes, but if that's not enough it has [docker integration](http://docs.haskellstack.org/en/stable/docker_integration/) too! I don't know about Sublime but chances are it already supports stack.
Very well written post. Thank you. Exceptions are my pet peeve with Haskell, too. The other day I discovered that Wreq throws an exception for any non-200 response! And to make matters worse, the exception object does not have the standard `responseBody` field. The response-body is tucked away in some ad-hoc header's value! I found the following point **very** interesting. &gt; Create custom monadic typeclasses that provide interfaces to the functionality you want to perform, then create instances, one of which is an instance over IO. How does this relate to monad transformers? Is the difference that monad transformers give you **all** the effects at once (even if your function needs only a few effects), whereas this allows you to "pick &amp; choose effects"? For example: `ReaderT StateT WriterT IO` vs `(MyReader m, MyState m)` -- completely contrived example, btw! Also, in your approach the code is not "infected" by `IO`, but it's now infected by a more general `m`, wouldn't you say?
You can even generalize this: getIOError :: (a -&gt; b) -&gt; a -&gt; Either IOError b The problem is that unsafePerformIO has some implications on lazyness and execution ordering that, well, I at least was never able to fully understand. Personally, I do dismiss it because I don't like my programs to become unpredictable.
Random thoughts. `summarise` can be found under the more bureaucratic name [`mconcat`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Monoid.html#v:mconcat). ---- The `Monoid (a, b)` instance at *14:14* (a, b) `mappend` (c, d) = (mappend a mappend c, mappend b mappend d) should be (a, b) `mappend` (c, d) = (a `mappend` c, b `mappend` d) ---- Variations of &gt;&gt;&gt; summarise (map (\x -&gt; (Min x, Max x)) [1..10::Int]) (Min 1, Max 10) list comprehension &gt;&gt;&gt; summarise [ (Min x, Max x) | x &lt;- [1..10::Int] ] (Min 1,Max 10) with [`foldMap`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Foldable.html#v:foldMap) and [`Min &amp;&amp;&amp; Max :: a -&gt; (Min a, Max a)`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Arrow.html#v:-38--38--38-) &gt;&gt;&gt; foldMap (Min &amp;&amp;&amp; Max) [1..10::Int] (Min 1,Max 10)
Interestingly one needs `TypeSynonymInstances` to write `NFDataSig (*) f = f -&gt; ()` as `*` is a type synonym defined in `Data.Kind`. I'd say that other extensions in the first extensions block are common in type-level heavy Haskell nowadays (not sure if all are actually needed after all iterations).
&gt; seems to come up less often than the other approaches when discussing how to turn impure code into pure code. I think that's because it's always been the dominant method. Now there are *new*, less-used, methods so they're getting the article space.
Note: Our "Refinement Through Restraint" paper is not already in its final form. The paper available here is what we submitted to the reviewers. The final paper will be available from NICTA's open access policy presumably after ICFP. 
First of all, entry-level programming tutorial is never a bad idea, so good luck with your initiative! That said, my impression is that in its current form your tutorial struggles with an identity crisis: it can't really decide whether it wants to be a tutorial or it wants to be a reference manual ;-) In a tutorial I would expect to see a process of solving a small-yet-practical problem, and not just a list of fancy features that Haskell has (and no "Literals" section for chrissake). But a reference manual I'd expect to be more complete and formal and without attempts to look funny (e.g. "might scare some of you in the following way" etc).
Hahaha thank you I dunno the literals section can't hurt though, it's real short. I guess I just wanted to be complete with the basics.
Nice to read, but i have to say the `String` part is not true, and it seems bashing `String` is becoming a phenomenon among haskeller, but `String` has a very different asymptotic complexity: 1. (++) has time complexity O(m) only when you construct the intermediate structure, but most consumers use recursively pattern match, so they'll seldom strict on the whole input. Laziness often turns (++) into a O(1) thunking creation, OTOH, text's concat is always O(m+n). 2. At last, you want get the whole `String` result(evaluate to NF), but `String` is exceptional easy to fuse because the laziness and simple recursive data type, so you often only pay the price once. 3. `String` can be used as a simple streamming data type, try to switch a `String` parser into a `Text` one, you will find `String` is very good. 4. The core library has several places using `String`, `Read/Show` are mostly debugging tools, `Exception` use it as error report, `FilePath` are very small, do you really need gain performance in these places? Of course `String` has a higher memory consumption, so avoid it in large text/data processing, and that's text/bytestring exactly designed for. But's that's the same story between `[]` and vector: they're not designed to replace each other.
On one hand, I would prefer making the best possible version easy-to-access. If you have a more advanced preprint that takes some of the reviewer feedback into account, now or after the camera-ready deadline, I would be glad to update the link -- or accept a pull-request to that effect. On the other hand, I don't think it makes sense to way until after ICFP (or even, as ACM does now, just at the time of the conference) to have a link out. People are excited about the list of accepted paper *now* and they are curious and want to have a peek at the work. They will ask you good questions about it at the conference *if* they have had the time to eye-ball the preprint before. Edit: I changed the link to say "draft" instead of "preprint", to emphasize that it is not the final version.
Could you check out the colored version I just posted on the github? I removed the literals section and made it a little prettier, I'd love to see what other parts you think are manualesque or boring
Look, I don't have time for thorough editing right now. Maybe later. Just as a general suggestion: try to look at your writing from reader's standpoint, i.e. "what have I learned after reading this section?"
I wonder if an RWH-style book is worth writing, given the fact that both the language and the ecosystem moves really fast.
Thanks, yeah I'll look through it more myself sometime soon.
Emacs's new interactive Haskell mode does this when you load a file into a GHCi session that needs an extension. It's pretty handy!
First define the helper function 'querySingle :: (ToRow q, FromRow r) =&gt; Connection -&gt; Query -&gt; q -&gt; IO (Maybe r)' Now you can define 'findLink' as: findLink conn hash = fmap fromOnly &lt;$&gt; querySingle conn "your sql here" (Only hash) Otherwise you can also just use a case expression: case xs of [Only url] -&gt; ...; _ -&gt; ...
Thanks for doing this. Unfortunately the paper I'm waiting for (concurrent gc) is still not available ;-( ;-(
It would be great to see a comparison of the time and memory consumption of a few "real-worldish" programs, each implemented using `String`, strict `Text` and lazy `Text` respectively. Is anyone aware of such a study?
Have you thought of contacting the authors to ask them to make a preprint available online? It's a nice thing to do (they get to know that people are interested in their work, which is also nice), but you could probably wait for just after the camera-ready deadline on June 30 (camera-ready is the final version, possibly changed by the authors to improve the paper based on the reviews and feedback they got).
&lt;sigh&gt; I think the issue is that OSX (now macOS) is just plain unstable (using the definition implied by your question). It seems like almost every time Apple puts out a major new OS release, they break compatibility somehow. There are a number of people who work to get GHC, the Haskell Platform, etc working for every new release, but that is always going to take some time. I've worked at Haskell companies where there was a policy of **not** upgrading to the new OSX for at least 3-6 months after release. So I'd say either wait to upgrade, or perhaps build/develop on a different OS.
Funny enough, certain forms of halting probabilities for deterministic machines are computable. They just take superexponential time in the size of the input program.
Or just use [spoon](https://hackage.haskell.org/package/spoon). It's like fork, but with less pointy bits. :) Combining the *WithHandles versions with a handler that gives an Either should work.
Thanks for doing this every year! Much appreciated.
Same that sounds very interesting
You're welcome, but to give credit where it's due, plenty of people contribute to the lists, and other people than myself chair/maintain the lists for other conferences or other years. For ICFP'15 for example, Aziem Chawdhary, Adrien Guatto, Max New and Alix Trieu in particular were super-helpful in collecting many links. If you look at the base versions for other conferences and years you will see that it has been organized by Mikhail Glushenkov, Matthew Pickering, technogeeky and Jeremy Yallop (several times).
Is there completion inside the intero-repl ? Seems no. A deal breaker for me.
If efficient concatenation is what you're after, you can use ropes or Okasaki's catenable lists.
No one _needs_ MTL either, since you can always write a custom monad or typeclass there as well :) This could just be me, but I like both MTL and free as tools of thought - mostly to help me iterate on my interfaces so that I can remove any junk / noise / accidental assumptions I might have made. If I start with free, and write a few interpreters for different contexts, I'm usually a bit more confident that I've threshed all of that out. I once played with an interface and then worked out that what I actually had was equivalent to `FreeT` over `State`. After noticing that and separating it out, I had a better understanding of what was going on, and it felt like the interface was more minimal / correct. That seems to be a common pattern for me when I play around with these things. Maybe afterwards I'll go back and replace my MTL stack with a big custom monad (potentially with MTL instances written for it). Maybe the free monad will be part of that stack, or will become monad of it's own. Everyone's got their own way of tackling things, I guess.
I actually think it is always true that such a static natural transformation exists, even when you have multiple interpreters. They aren't always easy to find though, which is the only reason free monads are part of my vocabulary at all. 
&gt; the assumptions of GHC's rewrite rules Well, seeing as those assumptions are basically "laziness", this commutes.
I discovered hpack recently which takes care of this (has a couple other nice timesavers as well): https://github.com/sol/hpack It appears to be integrated into stack as well. Just running "stack build" rewrites my cabal file before it builds.
The real problem with the String data type is terrible constant factors for performance and memory
Stack won't fix things like Xcode 7.0 breaking how nm behaves for example.
[Fraxl](https://github.com/ElvishJerricco/Fraxl) is a much clearer representation of these ideas (I'm giving a small presentation on it tomorrow; I'll put the video on YouTube). Fraxl is a stack of a few different components. First, there's a special `FreeT` monad transformer that's designed to take an `Applicative` rather than a `Functor`, in order to get `Applicative` optimization in the `FreeT` monad through that `Applicative`. Next is the `Ap` free applicative, which works on any `* -&gt; *` (not just a functors). Those two components can be ignored, since the only thing that matters for this discussion is their composition. We're only really concerned with this: type FreerT f = FreeT (Ap f) So this is effectively a freer monad (that will allow you to optimize applicative computations via `Ap`, but again, that's irrelevant here). The next layer is a union type. You can think of union as a nested `Either`, and the one used in Fraxl works on `* -&gt; *`s, not `*`s. Union :: [* -&gt; *] -&gt; * -&gt; * With this, Fraxl builds the following: type Fraxl r = FreerT (Union r) So for an arbitrary list of data sources `r`, Fraxl is a monad made of those data sources. Fraxl's purpose is to give you tools for working with this monad. For example, there's a transformation that caches requests (requiring the data source to implement some type classes for comparison). There's a combinator for automatically parallelizing requests. The point is, for these combinators to exist for arbitrary `r`, the free monad is necessary. Now, in a sense, you're still right. From the perspective of the user, wherever they use a concrete `r`, the free monad and those combinators can be factored out. But it's possible to have an `r` that is determined at runtime. In these cases, Fraxl is necessary.
As a user of vim and sublime text any recommendations? I haven't really looked to closely at what's out there recently, but last time I checked most everything was considered highly experimental and unstable.
Looks cool! I might join you. 
Exactly. Hence, for example, the pipes and streaming libraries including replacements for lazy ByteString.
Unfortunately it clashes with the ICFP contest dates :( (to be fair it's probably impossible to find a summer date which doesn't clashes with anything important)
linker errors! There should be a warning. e.g. When a file with an appropriate extension (`.hs`, `.lhs`, `.hsc`, etc) is in one of the `hs-source-dirs`, but isn't listed in `exposed-modules` or `other-modules`. 
Please! I love your cofun series. I wrote `vinyl-effects`, in an (entirely failed) attempt to satisfy your challenge the components are guaranteed to be unique and the order doesn’t matter (effectively a Set) this should also deal with the current problem of asymmetry if the Sum components are a subset of the Product components we can automatically create a Pairing from the Pairings between the components. but `CoRec` (/ `Rec`) is at least an associative type-level sum (/ product) http://dlaing.org/cofun/posts/free_and_cofree.html http://sboosali.github.io/documentation/vinyl-effects-0.0.0/Vinyl-Effects-Example.html https://github.com/sboosali/vinyl-effects 
Umm... Source? Sounds cool!
It's a lot less cool when you actually get how long the superexponential time actually is. [Here's the paper](https://researchspace.auckland.ac.nz/handle/2292/23906).
This event is open to any experience level, from beginners to gurus.
Maybe this would help? http://www.stephendiehl.com/posts/vim_2016.html We don't have intero or an equivalent (yet) but I'm happy hacking with this setup.
I've just tried Zeal, and it's really nice. What do you do for 3rd-party packages like Cassava or MTL?
Try to pass `-v3` to `ghc` and search for `*** Linker:` in the output. It will contain the full linking command.
The benefit of `Free` here is mostly as a mechanism to hide lifetime-managed handles behind an interpreter, and for stack safety, which is tricky in Scala. There are a lot of ways to do it, but this turned out to be the cleanest of many representations I tried.
It's always some gcc library. For instance right now my cabal is broken: dyld: Library not loaded: /usr/local/Cellar/gcc49/4.9-20140223/lib/gcc/x86_64-apple-darwin13.1.0/4.9.0/libgcc_s.1.dylib Referenced from: /Users/misja/Library/Haskell/bin/cabal Reason: image not found
Great, thanks for the link. :) That's much better than having to remove files after using a template.
Thanks! I hadn't been aware of [all these nice `should` combinators](https://s3.amazonaws.com/haddock.stackage.org/lts-6.3/hspec-expectations-0.7.2/Test-Hspec-Expectations.html)!
Isn't this a bit like saying, 'Use Haskell lists until you realize they just `foldMap` onto some existing monoid, then just use that monoid...' The idea seems to be that the only reason someone could use `Free` or `FreeT` is in anticipation of writing an 'interpreter' and applying `iterT` `foldFree` `iterTM` and other kin of `foldMap` . But in fact we engage in all sorts of manipulation on lists, e.g. mapping, breaking, etc. that can't be reduced to 'applying an interpreter'/'applying foldMap' and it is the same with `Free(T) XYZ` 
There are some benchmark results of CSV parsing performance through different data types on page 7 of https://github.com/blamario/monoid-subclasses/wiki/Files/HaskellSymposium2013.pdf 
I enjoyed this bit in particular: &gt; There were some people at IMVU who had dabbled with Haskell, but it always seemed too idealistic and not very practical. In fact, one weekend, Andy went home with the explicit goal of proving that Haskell was stupid and not good for writing production software. He came back on Monday glowing. “Haskell is perfect for web services! ..."
I enjoyed this bit in particular: &gt; There were some people at IMVU who had dabbled with Haskell, but it always seemed too idealistic and not very practical. In fact, one weekend, Andy went home with the explicit goal of proving that Haskell was stupid and not good for writing production software. He came back on Monday glowing. “Haskell is perfect for web services! ..."
I enjoyed this bit in particular: &gt; There were some people at IMVU who had dabbled with Haskell, but it always seemed too idealistic and not very practical. In fact, one weekend, Andy went home with the explicit goal of proving that Haskell was stupid and not good for writing production software. He came back on Monday glowing. “Haskell is perfect for web services! ..."
I enjoyed this bit in particular: &gt; There were some people at IMVU who had dabbled with Haskell, but it always seemed too idealistic and not very practical. In fact, one weekend, Andy went home with the explicit goal of proving that Haskell was stupid and not good for writing production software. He came back on Monday glowing. “Haskell is perfect for web services! ..."
That should also work. I was thinking more general "GUI suppport".
For a completely different view of the matter see http://hackage.haskell.org/package/monad-coroutine-0.9.0.2 the original implementation of `FreeT` and the once much discussed tutorial about it, 'coroutine pipelines' in https://themonadreader.files.wordpress.com/2011/10/issue19.pdf It is essential to the discussion that it is the very same type that is elsewhere called `FreeT` The going literature on 'free monads' tends to emphasize a very particular view of these types. 
I think [this](https://github.com/dalaing/cofun/blob/master/code/cofun-coproduct/src/Util/Coproduct/List2.hs) is the latest version - it's been a while. I've been meaning to revisit it since coming across [this](https://www.cs.ox.ac.uk/projects/utgp/school/andres.pdf), which was pretty eye-opening after clawing things together from random blog posts and stack overflow answers.
I've thought for a while that the solution to "too much parallelism" is gathering information via something like JIT/PDO. "Is it worth it to parallelize this" can be answered by trying it and recording the result. Finding out at compile time is super hard and undecidable in the general case. 
Very interesting to read! Thank you for sharing. I have definitely witnessed the "I just want to use what I've mastered already" mindset. I understand it, but I am glad it isn't universal.
Interestingly, nothing about pains of building and stack, nothing about compiler performance, nothing about garbage collection stop the world problem, nothing about AMP nor BBP, etc. It seems that the #1 pain point with Haskell is the string situation. 
Great post, I'm glad you're enjoying Haskell! I think the pain points described are surmountable, "just" a matter of making it happen. The [jump](https://github.com/commercialhaskell/jump) project is working towards improving the documentation situation. Regarding exceptions, I agree they can be quite unpleasant to work with. This is particularly the case when you have a lot of stuff like async exceptions, various resources to clean up, connections etc. For me, this unpleasantness comes from a few sources: 1) The point focused on in this article, which is that it's hard to be sure if you're handling the things you ought to. Lack of this has indeed caused an unintentional behavior change in `stack` on at least one case that I can think of. 2) Not having stacktraces built in to exceptions. 3) Not having causation info in exceptions. For example, if you throw an exception during resource cleanup. If this cleanup was happening due to another exception then the original exception is lost, as far as the caller is concerned. So, I agree that exceptions can be a pain to work with. I've been considering this problem for quite a while, hoping to do something about it one of these days! I'm familiar with our style at FPCO, I think we tend towards a very pragmatic style. There are good reasons for using exceptions even for non-catastrophic things. Here are a few of them. 1) Oftentimes resource de-allocation / cleanup happens due to exceptions. If this logic was done based on exceptions yielded by "Either Err a", then it wouldn't work when other exceptions occurred. This isn't a problem when you have considered that case, but it makes it easier to write something that handles resources incorrectly on exceptions. 2) With explicit values for exceptions, you will need to plumb them around. Machinery like `EitherT` helps, but it can be inconvenient to use unless it's already baked into your monad. With normal exceptions, no plumbing is necessary. 3) Exceptions only impact performance when they are thrown. Casing on "Either" values adds overhead in the common path. Certainly some exceptional cases do belong as return values, it would perhaps be good to have a rule of thumb perhaps. "Will many of my users want to have a handler for this case" seems like a pretty good one. This does not mean that in the future something more explicit won't be used, but I haven't yet seen a library that seems like "yes! this! this is the right thing!" for making exceptions more checked. Such a library would just be the initial step to making exceptions more pleasant, as it would need to get adopted. A few things we can do now to make this better: 1) As observed in the post, the names of pure functions which throw exceptions should have a suffix or prefix indicating their partiality. I like the suffix `Ex` 2) Use the rule of thumb of "will many users want to have special handling of this case?" to indicate whether it should be an explicit result value. I haven't seen a library that nicely allows you to do this while being encouraged to handle resources correctly (reason to use exceptions (1) above). 3) For existing functions which throw exceptions, add versions with an explicit value. Perhaps deprecate the old function if particularly egregious.
Because it's true, as long as it's not using your system ghc, it's insulated from the state of how the haskell parts of your environment are set up. So your global haskell environment can be totally broken, and stack will work just fine. As mentioned elsewhere, stack also has builtin support for docker and nix, allowing you to leverage those tools to control all of those other aspects of the build environment.
Cool transformer, I've never seen that one. Unfortunately I don't think any transformer could do quite what I'm after in this case. If you're curious what I ended up doing, check out the edit I made.
Thanks for the reply. Your undo project is really really cool. I suppose I'm not strictly doing backtracking. Are you? If you are, could you, for example, undo a hash?
I don't think I ever read something that proposed a typeclass approach as an _alternative_ for monad transformers, only as a refinement. The main advantage seems to be that the concrete stack is an implementation detail instead of hardwired into the type. I think this makes refactoring and adding behaviour a lot more pleasant.
With a monad stack whose behaviour depends on the order it seems like a dangerous idea to use the typeclass approach. Although one could be crazy and call it a feature, a weird kind of overloading. I think I agree with /u/ezyang: &gt; when you are using the type class approach, you usually use only monads that commute with one another (from http://blog.ezyang.com/2013/09/if-youre-using-lift-youre-doing-it-wrong-probably/)
Which makes sense but also creates a lot of incentive against changes and progress. In a lot of cases that's a perfectly reasonable tradeoff (although I think people tend to be significantly too conservative in these things), but it does mean that you can't judge the quality of a tool based on its popular adoption and staying power (in either direction, really).
The two are very much compatible, given that Tasty can use Hspec: https://hackage.haskell.org/package/tasty-hspec
Apparently not (as far as I can tell), which is a shame. But it's a lot better than nothing, so I'm still glad I heard about it.
I still don't get how insulating ghc makes any difference when upgrading your operating system breaks your system ghc. The way I understand such a breakage would affect both ghcs equally
Nix on OS X has had some intermittent pain points for the last little while. If you can get it working and you're comfortable with rolling back when things go wrong, then it's still worth a shot. 
This is the reason why people think Haskell is too academic and unusable for the real world. The vast majority of lessons, answers and discussion around haskell are academic, so even if it's used a lot in "real world" you can't see the evidence of that
There is also link to the [same demo in Fay](http://darcs.nomeata.de/circle-packing/fay/fay-demo.html) and the [same demo in haste](http://darcs.nomeata.de/circle-packing/haste/haste-demo.html). It's a great demo and a great comparison! The source code link for all three demos links only to the [Fay source code](http://darcs.nomeata.de/circle-packing/Optimisation/CirclePacking.hs). However, it is easy enough to find the source code for all three demos from the [root of the darcs repo](http://darcs.nomeata.de/circle-packing/). You can also find a [similar demo for diagrams](http://darcs.nomeata.de/circle-packing/diagrams-demo.hs) there.
Here's an interactive version: http://files.luite.com/hs15/#/2/12
Cool, this is the Dockerfile we are using for our servant / ghc8 / stack API: https://gist.github.com/maerten/64fe17e019d7090c96788b5775c3efe8