Great use of continuations, IMO. Oleg's post is cool too, but I have to say I appreciate the demonstration in the original article more.
Ill see you all on the battle field. I am currently ranked 22.
What language are you using?
Your code doesn't work (here) because it's not setting the buffering to lined like in the official template.
Thanks for the shout-outs, apfelmus! I hope to see more people using MonadPrompt, although I am still not 100% satisfied with it. My switch from GADTs for return and bind, into the continuation-based approach you mention at the end of your article, was to solve exactly the problem you brought up: left-associated &gt;&gt;= has quadratic behavior in the GADT approach but becomes linear with the continuation based approach. In effect, ContT forces all calls to &gt;&gt;= to be right-associative, in the same way that difference lists force all calls to ++ to be right-associative, eliminating this performance penalty. (ContT m) has some other interesting properties; it is a monad even when "m" is not! It is only to implement "lift" that we need "m" to be a monad. I believe this has been used to solve the problem you mention with the probability-distribution monad. If you restrict mplus and mzero to objects that implement Eq (or Ord) you can change the interpreter to eliminate the duplicates (or combine them into an efficient structure like a map), without requiring full "restricted monad" support.
I suppose it says something when the response is more popular than the original. :-)
The Olegophiliacs are out in force.
Im using python so far. Got started before haskell was an option.
I think "coming of age" means "is woefully misunderstood and spends all day in its bedroom listening to heavy metal".
&gt; Thanks for the shout-outs, apfelmus! I hope to see more people using MonadPrompt, although I am still not 100% satisfied with it. And there I thought I was pitching my competing `operational` package... :-D But yes, I think (agree?) that the underlying principle is the "best" way to implement monads, or at least to think about them. &gt; My switch from GADTs for return and bind, into the continuation-based approach you mention at the end of your article, was to solve exactly the problem you brought up: left-associated &gt;&gt;= has quadratic behavior in the GADT approach but becomes linear with the continuation based approach. I originally wanted to present the approach from Unimo that avoids the left-associativity altogether, but then stumbled upon the very neat analogy to "lists of instructions". &gt; If you restrict mplus and mzero to objects that implement Eq (or Ord) you can change the interpreter to eliminate the duplicates (or combine them into an efficient structure like a map), without requiring full "restricted monad" support. I don't think that works, consider for instance m :: Prob A m = (choice [1..10] &gt;&gt;= \_ -&gt; return 'a') &gt;&gt;= foo With "restricted monads", this expression could be optimized to m = foo 'a' but I think it's impossible to do that by just restricting the type of `choice`. In general, the thing is that if `mplus` has a restricted type, it longer distributes over `&gt;&gt;=`: (a `mplus` b) &gt;&gt;= k ≠ (a &gt;&gt;= k) `mplus` (b &gt;&gt;= k) simply because the right hand side is not well-typed.
Okay, [fixed](http://jaspervdj.be/posts/2010-02-08-tron-ai-arrays.html#fn2).
Is this still scheduled for the 11th at 2?
kewl.
Yes it is.
Per the suggestions of Andres Löh and Sean Leather, we've already started looking at how we can improve the library by using type families. I guess that means I now have to start _really_ learning how those work instead of just copying examples from the wiki.
I'd like to see a version that can cope with the different number of type arguments indexed monads have in contrast to vanilla monads
Thanks!
Okay, I need to give this a good look for ideas for alt-stdlib.
Can someone explain why this "heterogeneous propositional equality" isn't evil? As a categorically-minded person I don't understand what it could mean to ask whether s : S and t : T are equal without at least some kind of relation given a priori between S and T. Also, this &lt;-&gt; equality of functions seems to not even be transitive.
May the best bot win (as long as it's written in Haskell).
[here is the full scoreboard](http://csclub.uwaterloo.ca/contest/rankings.php) It does not look like everyone had the same number of games played. 
Is Heath Ledger the patron saint of finances?
Right. That's why I was careful to say that the intended interpretation of (s:S)==(t:T) is "if S is T then s is t". This is vacuously satisfied for unequal sets. We could harden that up by separating out set equality and requiring it as a precondition for the formation of value equations, but that just means you have to do more paperwork to get the same reward. Rather, we just work in the situation where there's no way to exploit a heterogeneous equation unless you know why the sets involved are equal. A sensible question at this point is "what does it mean for sets to be equal?". My answer is "two sets are equal if they are related by an isomorphism *which preserves canonical forms*". The eliminators for equality are coerce : (S:Set)(T:Set) -&gt; :- (S:Set)==(T:Set) -&gt; S -&gt; T coherence : :- (S:Set)(T:Set) =&gt; (S:Set)==(T:Set) =&gt; (s:S) == (coe S T Q s:T) which effectively says that equal sets have a coercion from one to the other, but it doesn't actually do anything. At run-time, when we compute with closed values only, we replace all appeals to coerce by the identity function. You're correct to observe that heterogeneous equality of functions is not necessarily transitive. That accords with the hypothetical nature of this heterogeneous equality. Given "if A is B then a is b" and "if B is C then b is C", one would not expect to conclude that "if A is C then a is c". There's no cause for alarm. Quite the reverse, in fact: if this formulation were transitive, we'd provide a dangerous eliminator for heterogeneous equations. Observe, "if Bool is Nat then true is 3" and "if Nat is Bool then 3 is false" are both reasonable (but vacuous): "if Bool is Bool then true is false" is more troubling. But in the inevitable Jagger/Richards fashion, you do get what you need: trans :- (A:Set)==(B:Set) =&gt; (B:Set)==(C:Set) =&gt; (a:A)==(b:B) =&gt; (b:B)==(c:C) =&gt; (a:A)==(c:C) When you unpack == in terms of &lt;-&gt; and compute the special case for functions, you find that you know (x0:S0) =&gt; (x1:S1) =&gt; (x0:S0)==(x1:S1) =&gt; (f0 x0:T0)==(f1 x1:T1) (x1:S1) =&gt; (x2:S2) =&gt; (x1:S1)==(x2:S2) =&gt; (f1 x1:T1)==(f2 x2:T2) and you're given x0:S0, x2:S2, (x0:S0)==(x2:S2) so what you really need is an island in the middle x1:S1 equal to both. Fortunately, when you're working with equal functions *in equal function spaces*, you also have that Q10:-(S1:Set)==(S0:Set) and Q21:-(S2:Set)==(S1:Set). You can then construct the island by coercion, from either x2 or x0 x1:S1 = coerce S2 S1 Q21 x2 Coherence on domains gives (x2:S2)==(x1:S1), so symmetry gives (x1:S1)==(x2:S2) and transitivity on domains (inductive hyp) gives (x0:S1)==(x1:S1). With the island in the middle, we get both arches (f0 x0:T0)==(f1 x1:T1) and (f1 x1:T1)==(f2 x2:T2), so transitivity on codomains (inductive hyp) completes the bridge (f0 x0:T0)==(f2 x2:T2) as required. Please note that the implementation does not require transitivity to compute this way. In fact, we don't need to compute proofs *at all*. But it's good to know why transitivity is admissible. (Coherence and transitivity can be shown admissible simultaneously by induction on the structure of sets.) All that's really going on is the careful stewarding of evidence to keep terms checkable. If hypotheses of propositional equality were used magically by the typechecker, interpreting (s:S)==(t:T) as (S ==_Set T) =&gt; (s ==_S t) would be perfectly homogeneous. This is exactly what happens in extensional type theory (as implemented in NuPRL) but it loses decidability, strong normalization, and lots of other good properties (with consistency reassuringly a notable exception). I don't know if this comes anywhere close to convincing you that this stuff isn't evil. Equality is always contentious, and I wouldn't claim we've done more than jostle our way in at an already overcrowded pissoir. WE may not even have managed that.
I don't know how well it gels with the categorical methodology. However, it's quite useful in practice. (s:S)==(t:T) is not, as I recall, something you can use alone. To extract the fact that `s` and `t` are equal, you must provide a proof that `S` and `T` are equal. So while it looks heterogeneous when unrefined, it in practice is only provable amongst elements of a single type, like homogeneous equality. And that's important because it's easy to get into a situation where homogeneous equality is ill-typed, due to expressions being intensionally but not definitionally equal, for instance: (zero : Fin n)==(zero : Fin (n + 0)) `n + 0` is (intensionally) provably equal to `n`, and so `Fin n` is (intensionally) provably equal to `Fin (n + 0)`, but a homogeneous equality will not be able to take on the above type, because the type checker cannot recognize that fact. This sort of problem comes up especially with functions in a dependent type theory. For instance, we might think a sensible rule for congruence would be: cong : (A : Set) (T : A -&gt; Set) (F : (x : A) -&gt; T x) (x y : A) -&gt; x == y -&gt; F x == F y But, this cannot be typed with a homogeneous equality (without involving other substitution/congruence combinators in the type), because we don't know there that `F x` has the same type as `F y`, since we haven't unified `x` with `y` yet. Heterogeneous equality allows this to be typed, and puts off the unification of `T x` with `T y` to when `x` can be unified with `y`. As for equality of functions, I think that falls out from the conditional aspect. It goes something like: (x : S) -&gt; T == (x' : S') -&gt; T' (where T may be dependent on x, and T' on x') when (x : S)==(x' : S') -&gt; T == T' And the premise is predicated on `S == S'` as well. So the two function types are equal if their domains are equal and equal values in that domain yield equal codomain types. And then equality of functions of two different types is predicated upon this equality of function types. So proofs only work between functions that actually are the same type, and you get transitivity. So, it may not be in line with the typical categorical/structural perspective on equality, in that you can write: (5:Int)==('c':Char) (which is a non-starter, because `Int==Char` is false, and so the type above is essentially the elimination of false into types), but it makes it much easier to deal with the realities of decidable type checking.
I am going to rename the "restricted mplus" to "group". I would use the (admittedly weaker) convention that `(group a b) &gt;&gt;= k = group (a &gt;&gt;= k) (b &gt;&gt;= k)` whenever the latter is well typed. Alternatively, if you split the operators into "mplus" and "compact", with these signatures and laws: mplus :: M a -&gt; M a -&gt; M a compact :: Ord a =&gt; M a -&gt; M a group :: Ord a =&gt; M a -&gt; M a -&gt; M a group m1 m2 =observable compact (mplus m1 m2) compact =observable id then we automatically get that "group" distributes in the way I suggested above. Since compact has no observable effect on the results of the computation (only on the potential efficiency of the computation being done), and the laws for mplus hold, we get that the laws for mplus also hold for group whenever the results typecheck.
I was surprised to see so many C++ entries, as I don't often associate that language with AI. Here are some numbers as of the time of my posting, so that you can wonder about how overall representation might translate to top-ranked representation. * 1 Lua entry * 8 Common Lisp entries * 19 Haskell entries * 26 Ruby entries * 31 C# entries * 99 Python entries * 118 Java entries * 154 C++ entries
I'm also confused as to what their ranking algorithm is. It looks mostly like the win/loss ratio, however, I notice that #19 is currently 22/0/8, while #20 is 24/0/4. Also, 39/4/3 beats 35/1/10.
As a new years resolution I've been tracking all my expenses since the beginning of the year using hledger. So far the novelty hasn't worn off.
Part of the challenge is to explore the state space faster than your opponent (or, at least, more of it within a fixed time interval). I intend to use C++ (well, maybe just C) for that reason; my Haskell-fu isn't nearly good enough to guarantee that I'm writing fast code.
A 1-second window, if I recall. Can you keep state between moves, or is each move fresh? On a large enough board you could store a lot of your state such that it *might* not be a big deal how fast you explore the space.
I wonder what this does to efficiency, though. Would compiler support help? A (good!) side-effect of this post was that I joined the Dutch Haskell User Group mailing list.
In this context I read it as "Fantasy Role Playing".
You can keep whatever you want. You actually get three seconds to make your first move as well.
I've just released 0.8. See [hledger.org](http://hledger.org), and if you're interested in this sort of thing join us on freenode's #ledger where hledger and original ledger users hang out. I've been using it to track finances and time since 2007. 
Their FAQ says your rank is determined by (wins + draws * 0.5) / (wins + draws + losses) It doesn't seem correct in some cases though.
Um - is there more to the game than the "one player game / two player game / quit" screen? I don't see a way past it, and the code doesn't tell me immediately.
I have been around the block looking for alternatives and I am back at ledger. Time to try hledger I wager. 
This sounds very interesting. I assume Galios has been using this, and if that's right, for what? What other applications would fit it well? Any more information? 
I know it's quite old, but it's still functional. I wanted to remind y'all of it so the poor kid may be adopted.
links to demo sources don't work
I'm a bit tempted to whip up a simple Monte Carlo Tree Searcher algorithm like UCT in Haskell or Lua using only random play-out win percentages to see how it competes with more sophisticated evaluation functions. Since they are any-time functions you could fine tune the move timing relentlessly.
Yep someone deleted the old yhc repo and replaced it with http://darcs.haskell.org/york-compiler98/ , which doesn't seem to include any js stuff, at all http://github.com/michaelcdever/YHC/tree/master still has the sources (and demos), though, and the docs on the wiki are still acurate.
right ctrl
This is nice. I've made something similar to parse Blender's file format (although not with a DSL as the file structure is described in meta data contained in the file). ([hblend](http://github.com/noteed/hblend)) Binary format (for file or network protocol) are cool things to abstract; if someone knows of more generally applicable DSL for the task, links would be appreciated. Maybe this should go to haskell_proposals.
It's written in the readme included in the package, but I agree that it should be more obvious where to look. I'll just add it to the description in the next release.
Is that it? No video or explanation? I've been trying to put the codensity monad to use since just yesterday and I was excited to see this submission because it would have been relevant.
Oops, it's indeed mentioned in the Readme. Kudos for the package!
Please, go on, we're listening...
Oh no, now all the compass-and-straightedge conspiracy theorists will find this and will become interested in Haskell! What, you didn't know there was a compass-and-straightedge conspiracy theory? Yes, there is! Mathematicians conspiring to deny the truth about squaring the circle and such things, apparently. Somehow it has never gotten the same popularity as the ones about evolution or climate science.
That's great, but debian testing still has [ghc 6.8.2](http://packages.debian.org/source/testing/ghc6).
Fun project! I think it is a serious mistake to uniformly represent intersection results by `r Points`. It would be better to have individualized return types `r IntersectionLL`, etc. Then `r IntersectionLL` would be observationally isomorphic to `Maybe (Either (r Point) (r Line))`. The advantage of using these narrower types is that it lets the Haskell compiler prove that your case analysis of an intersection result is complete. You could of course allow these types to be embedded into a more general set-like type `r Points`. But frankly the concept of an `r Points` type is alien to synthetic geometry. Lines and circles are not sets of points in synthetic geometry but primitive notions. Yes, they come with incidence relations, but those don't correspond to the membership relation of any non-trivial set theory.
I don't know, I'd think that those sorts of cranks would have to stay away from computer verification, because it makes clear that they can't actually do what they're claiming to do (because they'll never get the computer to accept that they've proved what they think they've proved). For instance, Good Math, Bad Math has has some articles and ensuing vigorous discussions on people claiming to enumerate the real numbers, or otherwise defeat Cantor's diagonal argument. I've posted a computer formalization of the diagonal construction twice, and neither time did it seem to convince anyone. Maybe that's because it's impossible to understand by someone who doesn't know the language (Agda) I'm using for the formalization, but the idea that I've successfully had a machine check my proof doesn't seem to impress at all, really.
That was a great read! Thanks. 
play with happstack for 6c / minute without having to install and configure nothin'. bonus, haskell platform for free.
isn't this supposed to be handled by cabal/hackage?
Native packages are the stable/accessible binary set to support end users of software written in Haskell. Hackage is for developers.
Is there any intro to how this kind of service works? What other things could we host this way?
No, you see, they'll just claim to have found a flaw in Haskell, or Agda, or whatever ;)
can you use it to construct a regular heptagon?
Makes only $2592 per month, must be a real bargain.
While the weaker law certainly holds, I think it doesn't help with optimization. The problem is that it cannot be used as a rewrite rule from left to right compact m &gt;&gt;= k =&gt; compact (m &gt;&gt;= k) because any function doing that would have to pattern match on whether the type of `k` has an `Eq` constraint or not. And the thing is that we want to do exactly that because it's usually the `k` that introduces redundancies that can be optimized away, like for example in uniform [1..10] &gt;&gt; return 'a' = compact (uniform [1..10]) &gt;&gt; return 'a' = compact (uniform [1..10] &gt;&gt; return 'a') = return 'a' In this case, we could supply a special implementation of `&gt;&gt;`, but that doesn't solve the general case like uniform [1..10] &gt;&gt;= return . odd 
The demos are in [http://github.com/michaelcdever/YHC/tree/master/web/jsdemos/](http://github.com/michaelcdever/YHC/tree/master/web/jsdemos/). Keep the wiki page open in another tab or window, though, so you'll know what to click. *Edit:* Oh, but that's not useful. You just get a fancy github page with the source code, and a link to download the whole package as a zip. Is there any easy way to see these pages in a browser?
I saw one of those GMBM threads recently. Your proof did not convince me, but luckily I had long since been convinced by Cantor. :-) But it was the first Agda proof I spent much time looking at, and a goodly chunk of it made sense (thankfully it was well commented). I even showed it to my friends and gave them my approximate understanding of how it worked. So even if you did not enlighten any cranks, you intrigued some fellow geeks.
But you **can** curry in Python: from functools import partial def curry(n, func): '''Curry a function of n arguments''' if n &lt; 2: return func def func_(x): return curry(n - 1, partial(func, x)) return func_ def currify(n): '''Decorator to curry a function of n arguments''' return partial(curry, n) # Example @currify(2) def map_str(f, xs): return "".join(f(x) for x in xs) rot13 = map_str(rot13_char) 
The verbosity kind of defeats the purpose though :-)
Does anyone have links to examples?
Just download the whole directory and look at them locally. The html files are little stubs that don't have to do more than find the .js files.
How possible would it be to have a DSL for this written in terms of template haskell, so it didn't need a separate preprocessor?
Whenever you release a Haskell library that uses String instead of ByteString or Text, a narwhal loses its horn :(
one hundred up-votes for you!
patches welcome :-)
Hi dons! Thanks for the post. I've been meaning to post an announcement about it, honest! In any case, core changes from the last version are generally pretty minor -- a few corner cases handled better, and support for exception constructs (try, catch, finally). JExprs are now also monoids, which should have been quite obvious in retrospect. This lets large javascript blocks be built up in separate bits, but be combined before hygienic variable names are generated. The big new thing is that there's a library, but also an executable. The executable takes a file written in jmacro syntax and writes out one written in standard javascript -- this way, rather than just using jmacro's more functional syntax for generating quasiquoted javascript, it can be used to generate javascript directly. This means, in theory and with popularization, it could be used as a more general javascript preprocessor/syntax checker. As far as syntax checking goes, of course, it just gives you the bare bones and nothing along the lines of a genuine lint type program. I'll try to post some examples here, or maybe even in a real release announcement, in the next day or so.
No, sadly.
True. Well, the idea is that the decorator is in a library that you import. Even so, a bolted-on solution can never compare to the real thing. :)
unstable has 6.12.1-8 now
I believe that the current "right" way on Mac OS X is to use *launchd*. It works (or at least used to work, I still have Tiger) like this: You create an XML file called "wyvern.plist" in your *~/Library/LaunchAgents* directory, and add an entry to *~/.launchd.conf* so that the *launchctl* command will be run to add your agent to *launchd* every time *launchd* starts up. See the *launchd.plist*, *launchd.conf*, and *launchctl* man pages for more details. There's an example of this kind of plist file somewhere down the page in a [macosxhints.com post](http://www.macosxhints.com/article.php?story=20050430015530348) from about 4.5 years ago. But if you are more comfortable with cron, that also still works, last I heard.
Until gbaz1 posts more, the long comment at the top of the file *Language/Javascript/JMacro.hs* is very useful. Having read that, it looks like running *jmacro --help* ought to be useful as well.
I feel the same. We should help people by explaining what should be used when and why.
This is a good idea, might make the dragon go server as nice to play on as online-go.com. I was hoping there would also be an interface to make moves with, so it might be a full dragon go server wrapper... Maybe later?
The post says: "In order to trace the execution, we need a logging framework. Several exist for Haskell, but none of these really suits our purpose." I'd like to hear some comparisons, or articles about these other logging frameworks. 
So the following is a simple example of some jmacro syntax, which is pretty much javascript extended to allow some simple MLisms where possible. On top of that, the utility really comes from antiquotation, which isn't demonstrated here, and hygiene and code composition, which only shows benefits with larger level projects, and compile time guarantees of syntactic correctness (including that regexp literals are valid regexps!). someFuns = [$jmacro| fun foldl f v xs { var acc = v; for( var i = 0; i &lt; xs.length; i++) {acc = f acc xs[i];}; return acc; }; fun max x y -&gt; x &gt; y ? x : y; fun min x y -&gt; x &lt; y ? x : y; fun sum x -&gt; foldl (\\x y -&gt; x + y) 0 x; fun head xs -&gt; xs[0]; fun tail xs -&gt; xs.slice(1); fun cons x xs { xs.unshift(x); return xs; }; fun map f xs -&gt; foldl (\\acc x {acc.push(f x); return acc}) [] xs; fun filter p xs -&gt; foldl (\\acc x {if (p x) {acc.push(x)}; return acc}) [] xs |] (excuse the weird spaces after the lambdas. they don't matter either way, but it's hard to get rid of them in reddit markdown).
Esp. since [hslogger](http://hackage.haskell.org/package/hslogger) is very popular.
See the "interactive mode" section. =)
Great, thanks! I've added this information to the Quick Start guide.
pretty pictures
Very interesting, I'm impressed. I'd like to see the code itself here.
Taken from examples/smvm/ in the [data parallel](http://darcs.haskell.org/packages/dph) package. smvm' :: [:[: (Int, Double) :]:] -&gt; [:Double:] -&gt; [:Double:] smvm' m v = [: D.sumP [: x D.* (v !: i) | (i,x) &lt;- row :] | row &lt;- m :] 
GF is nice, I used it to study Swedish. I mixed a Swedish grammar with cgi and generated random questions for myself.
hslogger would be really powerful as a logger backend for what the article presents. The point is that each process will need to log and I need some way to discriminate them. Also, when debugging, it is nice to turn process logging on and off depending on what you are trying to understand in the application. What the current framework does not give, and what hslogger gives, is the ability to log to different kinds of outputs, logfile, console, and so on. In hindsight, it is clear that the logging frameworks provide different services, so it would definitely be beneficial to employ hslogger in haskell-torrent.
See also http://www.ccs.neu.edu/home/pete/research/enumeration.html 
JMacro has the sexiest antiquotation support I've seen. It reifies javascript values into haskell terms using HOAS, just long enough that you get to have hygienic macros and not have to worry about your names colliding.
Potential Haskell slogan? Programmer tested, Dijkstra approved.
At least we have an answer to WWEWDD? w.r.t. Haskell.
One thing that I find hard about finding monads is that occasionally the type you are abstracting into a monad is actually ends up being `m ()`. Since it isn't even a type constructor it is very hard to see. One example of this was my [assembly code monad](http://www.haskell.org/sitewiki/images/1/14/TMR-Issue6.pdf) where my `AssemblyCode` ends up being defined as `AssemblyCodeMonad ()`. More abstractly `m ()` is a syntax tree terminated with leaves with no information (or `m Void` is a syntax tree with no free variables) which is often the final result that one is thinking of at first. The monad abstraction is a convenient way of building this final result.
Can someone tell me if I have this strait: The difference between currying (as explained here or otherwise) and partial function application is a matter of timing? Currying implies/means that when a function of arity n is defined or curried it actually defines n functions with one argument. Partial application is the act using an n-arity function with m arguments and receiving a function of n-m arity. Sort of compile time or 'currytime' with the first, and runtime with the second? Even if the timing bit is wrong, is the logic behind my explanation correct?
Reading a lot of Dijkstra's papers, I got the impression that either Haskell is inspired by Dijkstra or that great minds think alike. In either case, I really wanted to know what Dijkstra thought of Haskell, knowing that he was very disappointed by functional languages a few decades ago..
Here is [another paper with the same title](http://www.cs.utexas.edu/users/misra/Notes.dir/RegExp.pdf), by Jayadev Misra. The acknowledgment section at the end states: &gt; The Tuesday Afternoon Club, under the guidance &gt; of Edsger W. Dijkstra, read and commented on an &gt; earlier draft of this manuscript. Pete Manolios &gt; developed a program in ACL-2 based on the &gt; earlier draft. The current version is helped by &gt; several features of Haskell, the type definition &gt; mechanism and lazy evaluation, in particular. I &gt; am indebted to Doug McIlroy who provided me with &gt; the merge and prod functions, and explored a &gt; number of other solutions with me. Ham Richards &gt; has helped me with the intricacies of Haskell. Manolios mentions this paper as a "preliminary version" in his own acknowledgments.
That is good news.
Well then, why not just use a lazier version of *inits*, like: inits' xs = [] : zipWith const (zipWith take [1..] $ repeat xs) xs Or in this case, where all we care about is non-empty inits of infinite lists, we could even use: inits1 = zipWith take [1..] . repeat Then your original *nest* function works fine. For example: &gt; let f [x] = x; f xs = sum . last . init . init . tails $ xs &gt; take 10 $ nest f 1 [1,1,2,3,5,8,13,21,34,55]
I just realized: &gt; prefixLength xs = length . takeWhile (uncurry (==)) . zip xs It seems that Haskell code matures like good Camembert `;-)`
I'm apparently noobing out. Most of my Haskell hacking hasn't involved Hackage... when I try to install dow, I get this: $ runhaskell Setup configure Configuring dow-0.1.0... Setup: At least the following dependencies are missing: GLFW &gt;=0.4, OpenGL &gt;=2.4, elerea &gt;=1.2.3 $ cabal install glfw opengl elerea Resolving dependencies... No packages to be installed. All the requested packages are already installed. If you want to reinstall anyway then use the --reinstall flag. $ runhaskell Setup configure Configuring dow-0.1.0... Setup: At least the following dependencies are missing: GLFW &gt;=0.4, OpenGL &gt;=2.4, elerea &gt;=1.2.3 Thoughts?
Cool solution!
It's used on the [Haskell web site](http://www.haskell.org).
Thank you for flying Lambda Airlines. Please come again sometime.
When people (such as myself) are set upon perfection, I am often reminded of this image (from a café in Nijmegen) http://www.kroegenweb.nl/kroegfotos/17579-3.jpg whose caption, suitably punctuated, is a reminder to question one's delusional behaviour.
Have you run **cabal update**? If all goes well, dow should install from Hackage afterwards. At least it did for me.
I did. Oh, from Hackage itself? I was trying to build from source. `cabal install dow` just installed it for me. Thanks!
Is there some text somewhere that spells out a relationship between vector and uvector?
Likely this will now supercede uvector. We need to check off a few things, but that's the plan. uvector was picked as a more stable point in the design space, while vector/dph were being developed, however, vector looks mature enough now.
Are there any porting notes needed, or is it pretty much 1-to-1? I've got quite a lot of uvector code at this point, so I'd like to make the switchover as clean as possible.
There are *no* porting notes yet. And there's no need to switch immediately. Let's just get the tutorials and porting notes together over the coming months. -------------- Edit: *there are now [notes on how to port your code](http://donsbot.wordpress.com/2010/02/15/migrating-from-uvector-to-vector/)*.
Restored for you. If someone would like to do a prettier version, I'd welcome it.
Translation for non-Dutch people: he eats he drinks he thinks he is mister Dijkstra Good point, though, w.r.t. the motto "If it passes the type checker, it must be correct". edit: formatting
How does this compare to the Data.Array lib in terms of performance? No doubt it is a lot easier to use, but I'm curious to see which is the way to go for the most performance demanding applications.
Just saw this in lib documentation: Data.Vector.Primitive Unboxed vectors of primitive types as defined by the primitive package. Data.Vector.Unboxed is more flexible at no performance cost. Looks great!
Think a little more [down to earth.](http://en.wikipedia.org/wiki/File:AmtrakLogo.png)
[`instance Prim a =&gt; Vector Vector a`](http://hackage.haskell.org/packages/archive/vector/0.5/doc/html/Data-Vector-Primitive.html#t%3AVector) I know whats going on there, but it looks really strange, like `Vector` is a type with bottom _kind_. 
Someone really should get around to adding qualified names to haddock, even if it would allow the proliferation of the `C`/`T` naming schemes that certain people seem so fond of.
Massively, massively faster, thanks to fusion. Plus, it is incomparably more flexible in its interface.
I assume there's a good reason for it, but why does anybody use the `Int` type? It seems to me that the problems of over- and under-flow make using `Int` correctly almost impossible. I can understand using `Word` variants, since sometimes the maximum required range is known, but `Int` doesn't even *have* a defined range. All we know is that it can represent numbers in the range [-(2^29), (2^29)-1], but behavior outside of that range is undefined. For example, it seems that using `Int` makes the following properties of `Vector` platform-dependent; is that true? prop_last x y = last y == last (x ++ y) prop_drop x = null (drop (length x) x)
It looks like (at least for GHC) `Int` is large enough for any vector or combination of vectors that will fit in your address space. (Wait, maybe not `Vector Word8`...) But don't take that as a counterargument.
Well, part of the appeal behind Haskell (as opposed to C) is that whether some particular code works or not doesn't / shouldn't depend on the compiler or architecture. Containers sized with `Int` can't contain more than (2^29)-1 elements without running into undefined behavior. Assuming unboxed vectors are implemented sanely and don't have a per-element overhead, you could hit that limit on a 32-bit system using many common unboxed types (anything 7 bytes and smaller, like `Word32`). On a 64-bit system, vectors of pretty much anything could exceed the `Int` limit without running out of address space.
The only implementation of Haskell that supports the vector library -- GHC -- has Int at 64 bits on a 64 bit machine, however.
It would be nice if there were a vector equivalent to Data.Map.mapWithKey [1] or ocaml's Array.mapi function [2]. Or perhaps there is and I just missed it. [1] http://www.haskell.org/ghc/docs/latest/html/libraries/containers-0.3.0.0/Data-Map.html#v%3AmapWithKey [2]http://caml.inria.fr/pub/docs/manual-ocaml/libref/Array.html
Yep, thats great, as long as he still has time to work on [DDC](http://www.haskell.org/haskellwiki/DDC).
I find it awkward that Haskell lacks a typeclass Sequence (or some such). Any plans for such a thing? 
http://hackage.haskell.org/packages/archive/vector/0.5/doc/html/Data-Vector.html#v%3Aimap ?
Not quite. Currying is a concept of having only 1-ary functions and producing n-ary functions with 1-ary functions via closures. Partial application is a completely different concept where it is possible to delay application until all arguments are saturated, which may not happen all at once. Curried functions obviously gives way to easy partial application, so it is a semantically elegant way to allow it.
s/minute/hour/... oops.
The Ruby algorithm here is a different one, and probably less efficient.. One of the comments suggested the Haskell algorithm in imperative form, and he should benchmark that.
I've benchmarked it (I think you meant the more optimized ruby version) and it's much closer to the haskell one's performance, but see my reply on that comment, I wasn't heading for speed/performance.
We don't need more fibonacci benchmarks. They always go into stupid land -- not measuring the same thing, and reaching odd conclusions about performance in general. This is why "nofib" is a 20 year old Haskell testsuite.
We have it in the form of, * Functor * Foldable * Traversable but that's not ideal.
Thanks, that's what I was looking for.
not only that, but I don't like seeing language comparisons that involve stuff from standard libraries :/
That's because [Haskell is grrrrrreat!](http://imgur.com/vv9OO.png)
Sorry guys I probably missed to emphasize my point enough in the post - I didn't want to compare/do benchmarks... my primary goal was to write about a naive Haskell newbie's thoughts about his two favourite languages and compare the two ways (of many) to solve a simple maths problem. My apologies for not being clear :(
"Deathmatch" might have been a bit misleading :-) 
Hall of shame / catchy titles, I get it :)
Anyway, points taken, thanks for the comments and suggestions, I will watch out for these mistakes in my forthcoming posts.
I'm only saying, because I did this myself ... http://donsbot.wordpress.com/2007/11/29/holy-shmoly-ghc-haskell-68-smokes-python-and-ruby-away-and-you-can-parallelise-it-for-free/ And I learned my lesson well -- no one should ever ever use fibonacci for any kind of comparison work -- there are just too many ways to compute it in entirely different ways, that all objectivity quickly gets lost.
Yes, you're right, the same goes for almost all mathematical constructs. I only wanted to show/sugges the ease and naturalness of the Haskell representation of the Fibonacci series - which is also the basic difference (for me at this point) of the imperative and functional approaches.
why not just use the Haskell support in RubyInline ? http://github.com/ujihisa/rubyinline-haskell
What about [this?](http://imgur.com/vv9OO.png)
I should really release my bytestring high perf mbox made months ago...
Does it support loading compiled Haskell code from packages via .so archives?
Wow, I took his NLP class 5 years ago. Good to see that the GF is still alive and kicking!
No, it just translates a small subset of Haskell into Ruby.
author here - I've talked to ujihisa, and he's got some patches in Hubris. RubyInline-haskell is wishful thinking, basically - purely syntactic munging.
I think it's a lot harder for people from a Ruby background to learn Haskell than to learn C, so point #1 doesn't really stand. Other than that: a little interesting.
I'm not sure that's really true. The idiomatic use of blocks as in Ruby is standard in Haskell, writing DSLs is a standard and accepted development practice, and you'll never have to write malloc/free code unless you're developing an interface to a C library.
I'm wondering how useful this is in practice at the moment. According to [this](http://lists.debian.org/debian-haskell/2009/11/msg00005.html) post, the shared libraries produced by GHC do not have any ABI guarantees, so if any Haskell package in a dependency chain is updated, then everything that depends on it has to be recompiled, which makes shared libraries still impractical. Unless the people on debian-haskell are mistaken and ABI guarantees are provided after all, of course. Still, you'd have to get distributions to actually bundle the shared libraries instead of just static ones, or you get to compile them all yourself, which you won't like unless you are a Gentoo user (and maybe not [even then](http://osdir.com/ml/xmonad@haskell.org/2009-12/msg00063.html)).. So I see the point of someone who commented on the blog that something like [Thrift](http://incubator.apache.org/thrift/), which does not require shared library support, may be more useful.
I think the ABI guarantees should hold for foreign exports - according to the FFI spec - which I think is how Hubris should (?) work. As I said on the blog, Thrift is fine for distributed RPC, but I don't want my inner loops involved.
yeah, I'm not pulling anything desperately funny. It's all going out through the FFI.
This article explains why ruby programmers should use Haskell, but it doesn't explain why I would want to combine haskell with Ruby. Ruby offers few benefits in most circumstances compared to Haskell.
Yeah, there's no problem with the C&lt;-&gt;Haskell interface. What I mean is that the shared library which is built depends on other shared libraries, i.e. the Haskell packages it depends on. When those get updated, the Hubris-compiled library better not be cached somewhere or it will break. But that can be fixed by ignoring cache entries older than the time Hubris was first used in a Ruby session. The real problem is having those shared Haskell libraries in the first place. The static ones are easy to get, e.g. in Debian you can just apt-get install those (and as we all know, almost all sysadmins run woefully outdated Debian out of spite). If the shared libraries cause problems in general, they won't find their way into distributions, so you get to compile them yourself, provided your sysadmin approves (we're talking about web development here, right? what with Ruby and all..). EDIT: I'm not trying to be dismissive. The idea is nice, and any interoperability increases between languages are good. They increase the value of both languages. In particular, interoperability between Haskell and languages that have a lot of people publishing code is a very good thing. It's just that things that are difficult to install or have magical dependency problems ("oops, the server broke, I wonder why? oh right, some library was updated..") leave something to be desired, and at least such caveats ought to be mentioned to avoid unnecessary user frustration.
Ruby's widely installed, widely known, has some useful libraries, and handles dynamism more naturally than Haskell in some circumstances. That said, I expect the appeal to be mostly to Ruby hackers looking for something more friendly than writing C extensions.
Either way... it's kinda silly given that there's a closed form soln for fibonacci...
good god, working with two slow languages instead of one 
C, by being a straightforward imperative language, is much closer to Ruby. The only thing a Ruby programmer will have to learn to use C is explicit pointers and manual memory management, which really aren't so hard. Especially since a lot of Universities will have taught C at some point. I don't see what this is about DSLs - surely when you use an FFI for speed, you're not also using a DSL?
anything like this for python? there are plenty of useful libs written in python, can't say that about ruby.
It's also LGPL, which could create licensing issues for some projects.
Any link to article or thesis?
Ruby: more dynamic than Haskell.
Understatement of the year.
This difference is a little more subtle, though, than "hard" versus "easy". I think the point the author is getting at there is that someone using Haskell without fully understanding the language is far less likely to have things go catastrophically wrong than someone using C. This is especially true if you're sticking to pure computations. Sure, you could have a space leak... but you are quite unlikely to have an exploitable security hole, or a random crash seven levels up the call stack.
Ah. Beginning to make a little sense :)
The algorithm used is described in my masters thesis http://jonas.duregard.se/AGATA.pdf /Jonas
thanks
The direction is actually the other way - while it'd be possible to call Ruby from Haskell, I haven't bothered putting the code in yet because no-one seems to be clamoring for it. As to a python port: I don't think it'd be terribly hard. You could use the same technique, just introducing a Pythonable typeclass and a set of instances for the common Python data types.
You mean I shouldn't be switching from UArray to IntMap IntSet in my lousy Tron bot rewrite and be using vector instead ? ;-)
Oh, thank you.
I'm not using Wyvern, but on the other hand, I *have* started playing on DGS. So I suppose dmwit's work was not entirely wasted.
yes, exactly. on a tangent: DSLs are not necessarily slow in Haskell, they're compiled just like normal code and have the full array of GHC's optimisations applied.
So is this just is a way of deriving "instance Arbitrary X" for most X? Or am I missing something? Does it generate tests as well?
Matrix exponentiation, while a very fast way to calculate an arbitrary fibonacci number, is much slower than simple addition when all you want to do is get the next fibonacci number. (or subtraction if you want to get the previous fibonacci number) However, you get *two adjacent* fibonacci numbers from one matrix exponentiation, which you can then use to get the next (or previous) number. So you might consider some kind of search (e.g. binary, although there would be better choices given the nature of the problem), combined with a linear scan once you get close enough. But really, that level of algorithmic sophistication is overkill for this particular problem... 
Still, we might as well avoid having to check for negative Ints in all function calls. It adds minor overhead and there's no real reason for it, is there? Or does GHC still generate slower code for the other machine types (Word, WordX) than Int? I remember seeing a ticket about that a while ago.
I don't think there's any good reason not to use Word.
this is a good point, and an area where Hubris could use some work: even getting it installed can be tricky at the moment. I would assume that it'd usually be replacing C in extensions through the rubygem system, and in that environment it seems more or less the done thing to ship source and compile on the host, rather than shipping binaries that need to be upgraded.
Oneliners abuse! From the blog code example: let mimicrouteCost (0,0) _ = 0; mimicrouteCost (0,_) _ = 1; mimicrouteCost (_,0) _ = 1; mimicrouteCost (line,column) cache = cache!((line-1), column) + cache!(line, (column-1)); To the clearest form: mimicrouteCost (0,0) _ = 0 mimicrouteCost (0,_) _ = 1 mimicrouteCost (_,0) _ = 1 mimicrouteCost (line,column) cache = cache!((line-1), column) + cache!(line, (column-1)) 
Before trying to compute A(4,4) one should consider its size, e.g., by reading the wikipedia entry. The number is far bigger than can be represented by ordinary binary representation in any computer. 
How useful is this in Haskell in practice? Most of these primitives sound useful at protecting mutable shared state, but Haskell threads usually have very little mutable shared state, and usually that state is already guarded by a lock (MVar).
Will this replace ByteStrings in the future (Vector Word8)?
The reason is that the author was coding in GHCi.
Right, thanks for the clarification. Someone also posted a specific faster solution for this problem using simple addition - which I avoided by decision. But again, probably that's even more imperative... :)
Good opportunity here for someone to chime in about how to do multiliners in ghci. OK since you asked: Enter :{ to start and :} to close. 
However, I've never been able to get layout to work there. So you can split things across multiple lines... but only if you use explicit braces. It would be really nice if layout were possible in GHCi multiline expressions, but it doesn't appear to be.
Good point. You prompted me to check the the manual whereupon I found this odd point: &gt;the lines between :{ and :} are simply merged into a single line for interpretation. That implies that each such group must form a single valid command when merged, and that no layout rule is used. The main purpose of multiline commands is not to replace module loading but to make definitions in .ghci-files ... more readable and maintainable. One-liners here to stay? 
No tests, just the generators. As with many things, there is more to this than one realizes at a glance though (I for one intended this piece of software as a small component of another project, but ended up writing my masters thesis on the subject!).
Can someone simply describe what it does exactly?
Yo Dawg, we heard you like Haskell's compiler, so we put a JIT in your Haskell so you can compile while you run your compiled code.
Great news. I would hope that the patch to LLVM will somehow be integrated into the regular LLVM distribution. 
Just FYI, it's not to be used as a JIT backend.
Those guys are pretty reasonable. I don't see why not.
Hmmm, this architecture should be reconsidered as the following is less funny: Yo Dawg, we heard you like Haskell's compiler, so we put a VM byte code compiler in your compiler so you can compile while you compile.
So performance seems a little slower than the native code generator in most cases. What are the pros of this project, then? The email doesn't really give an outline of the justification.
This is *extremely* cool. (I hope you enjoyed this insightful comment.)
Surely a bunch of the Haskell guys are students with loads of free time :-) Time to work on this thing :-)
It was never funny to begin with.
Check the thesis. Tight loops (e.g. Data Parallel Haskell or vector) are ~25% faster. And a bit of work should have it beating the GCC backend shortly. http://www.cse.unsw.edu.au/~pls/thesis/davidt-thesis.pdf Edit: listen to chak :-)
Full details are in David's thesis of course. Here the executive summary: (1) The LLVM backend is simpler than both the C and the NCG backends — we outsource backend work from busy GHC devs to LLVM devs. (2) The LLVM code is faster for DPH code, that suggests it will also be faster for bytestring and (u)vector code — this hypothesis remains to be tested. (3) LLVM supports architectures that GHC doesn't at the moment — LLVM should make porting easier for those. (4) This is just the first cut implemented by one person in effectively 3 months. I believe there is plenty of scope to improve on the current implementation and to move beyond the performance of the current NCG. (This answer is authoritative. I supervised the thesis ;)
It would be interesting to migrate GHCi's bytecode to LLVM too, so we get JIT for free there. Maybe once it's more established it'll be possible :)
Would this make it possible to cross-compile? I'm a bit uninitiated to LLVM, and the first few hits on Google only confused me more.
Perhaps a silly question, but why not? Not yet, or not ever?
It might make it somewhat easier, but GHC's build system isn't really set up for cross-compiling (last time I checked).
This is great progress but why is the entire logo of llvm? It should at least give some sort of hint that this is haskell with llvm not an llvm subreddit.
&gt; why not Perhaps because it breaks every other language (ex:clang) currently supported by LLVM. The patch needs reworked before it could even be submitted let alone accepted.
needs more lambda
I did! I was thinking at first it's kinda cool, but now I know.
Just deal with it for 24 hours. :-)
Can this be used separately from GHC as a Cmm to LLVM compiler?
&gt; What we really need is an "Advanced Haskell" book which assumes a knowledge of Level 1 Haskell and then lays out the CT bits in an orderly fashion. Will someone write this book for us, the noobs in the Haskell land? Please?
GHC doesn't really implement the entire C-- language. So, you can use it to compile GHC's Cmm (and that's what happens for part of the runtime system), but not as a genuine C-- compiler.
&gt; Of course, we can use a «Tying the Knot» technique, but, unfortunately, this doesn’t really help: if we change only one single value in our graph we will have to build the entire structure if we only use what’s presented there. Moreover, we are forced to know some things about our structure when we are writting our code. Wouldn’t it be more useful to have a general network of nodes which can be updated very fast? ... &gt; Thus, the update value function can be easily written: update :: (Eq p) =&gt; (Network p a) -&gt; (p, a) -&gt; (Network p a) update u@(Network v n) (p, nv) = Network (\pp -&gt; if (pp == p) then nv else v pp) n &gt; We could have used functions instead of values in the if..then..else part but I wasn’t sure if using id there would be a no-op with no performance penalty or not. The performance penalty of `id` is not the performance penalty I would be concerned with here.
The key word in Lennart's statement is 'somehow'. The patch at least shows that what we need can be done. Integration is always a matter of balancing concerns.
Perhaps the better question is, if a set of Wiki pages were created for the purpose, could such a book be community-authored? Or would it then lose the single vision that would make it useful or readable. 
The third level is advanced usage of the type system, like, say, Control.Monad.Exception or session types, as well as mind bending use of level 1+2 features like say uu-parsinglib, and, in general, continuations. Level four would then be apprenticeship to the title of "Oleg".
I don't think the single-path-to rome approach works well for advanced topics, and even less for texts that are in flux. Just add a section "What you should have understood before reading this" and "What you might want to read now you've read this" to each topic along with appropriate links, and give sensible indices to those topics. If those references loop, then it might be a good idea to write something introductory about that mutual dependency, to ease bootstrapping.
so the best solution would be for Oleg to write a book. I'vebeen advocating it for a long time.
What penalty do you have in mind?
What does this patch do more exactly?
Sure, but its more information for those reading the comments that the LLVM patch is not yet ready. Good to know if you are one of the many looking forward to an LLVM backend. Sorry if it sounded overly negative.
I wish my BSc was so cool with such cool teachers. My teachers didn't know what Haskell was, or even Python. We spent the first year learning HTML and ASP Classic, C, Pascal and learning about structured programming and pseudo code and Nassi–Shneiderman as a means of documentation, entity relationships and normalisation (thankful for that). I spent most of my time writing a Tetris game in C++ and working on Project Euler. Oh god it's all coming back. I dropped out after completing the first year. Second and third later I know from my friends who stayed on that they learned Java and object oriented programming (half of them didn't understand or know the meaning of inheritance, encapsulation). Anyway, **do not** take an ND or BSc in "software development" (aka. Computing &amp; IT) at Wigan &amp; Leigh College or Sunderland University. You will regret it. We hired someone who had a degree in "Computing &amp; IT" and of course he couldn't program; I thought these crappy courses were supposed to pump out engineers? Whoops this post turned into a rant. *Shakes fist at education system*
Why did you capitalise "mathematicians"? Am I missing an obscure joke?
Type constructors need to start with an uppercase letter no?
Very nice packaging. However I'm worried about the proliferation of such accessors/labels/fields packages.
ಠ_ಠ
Cool! I always write my own "result", "arg", etc in each module, cause I never wanted to package it up. Now I can use this :-)
There is also the very nice fact that having a major functional programming language implementation actively developing with LLVM gives more impetus to the LLVM developers to improve things that functional language developers need, e.g. tail call optimization on ARM, code-table location, etc. Not a justification, but the opportunity for synergy shouldn't go unremarked.
if you ever wonder what (.) (.) (.) does you should read this post. (Personally, I would like to call this the "alien boobs" function.)
 index = length w where w = takeWhile (&lt; limit) fibonacciNumbers ``w where w =`` is a very long way to spell ``$`` in this case: index = length $ takeWhile (&lt; limit) fibonacciNumbers 
The O(n) cost of looking up a binding in the network.
...and Haskell becomes NSFW? :D
It is in O(1). You just give a point and valueAt gives the value at that point in O(1) or neighboursOf give the neighbours in O(1). It is only function application. Or do you mean something else by binding?
I wrote my own type-safe embedding of LLVM assembly language into C++. It handles features of the LLVM type system I still don't see reflected here, e.g. recursive types using nameless up-references. They were definitely the trickiest part of the type system to implement; the interaction with `getelementptr` was a fun puzzle. I'm curious if Lennart has an elegant Haskell embedding in mind? Here's a simple example if anyone's curious: Module m("test6"); typedef Struct&lt;Pointer&lt;Up&lt;1&gt;&gt;, Int32, Int64&gt; MyStruct; Int32 named(x), named(y); Pointer&lt;MyStruct&gt; named(s1), s3; Pointer&lt;Pointer&lt;MyStruct&gt;&gt; s2; m.function&lt;Pointer&lt;MyStruct&gt;&gt;("main", x, y, s1). getelementptr&lt;Int32, 0, Int32, 0&gt;(s2, s1). load(s3, s2). ret(s3); m.module()-&gt;dump(); Output: define { \\2, i32, i64 }* @main(i32 %x, i32 %y, { \\2, i32, i64 }* %s1) { %1 = getelementptr { \\2, i32, i64 }* %s1, i32 0, i32 0 ; &lt;{ \\2, i32, i64 }**&gt; [#uses=1] %2 = load { \\ 2, i32, i64 }** %1 ; &lt;{ \\2, i32, i64 }*&gt; [#uses=1] ret { \\2, i32, i64 }* %2 } Everything here is statically type safe, so `getelementptr` needs to figure out at C++ compile time that the first field of `MyStruct` is `Pointer&lt;MyStruct&gt;`. My solution is a lot like a simple interpreter for lambda calculus using de Bruijn indices. It builds up a context (a type-level list) as it recursively traverses types. Atomic types are left untouched and compound types are mapped over functorially. But when it encounters a type up-reference `Up&lt;N&gt;`, it replaces it by the type in the context at position `N`. Mind you, this all has to happen with type-level programming.
I very much like this idea, and I've been thinking about a "community textbook" for a while now. Aside from links to "prerequisites" and "suggested future reading", I also want user comments and, to entice contribution, a sense of authorship. All this would require a custom web backend. Unfortunately, I currently don't have time to write such a thing, but I'd be happy to collaborate.
It sounds like the same trick could be used in Haskell, but I'm not sure I will bother. There are other things that cause actual problems that I would like to see solved, like handling basic blocks properly. 
You don't think being unable to specify something like a linked list data type qualifies as an actual problem? What's the current problem with basic blocks? One annoying little thing I ran into with my own implementation is that LLVM makes forward references easy for everything except phi nodes. I have to do my own backpatching. An unbound LLVM value handle maintains a list of referencing phi nodes which it patches up whenever it is eventually bound. For forward branches and recursive types, LLVM has more built-in support. You deal with forward branches by having unbound blocks be created as orphaned basic blocks which are later reparented off of a function. Forward references to types can be done by initially creating an opaque type and referring to that. Once you know the type definition you can use refineAbstractTypeTo() to tie the knot in the object graph.
You must know that you're begging for some APL junkie to one-up you with a few slanted lines, followed by something that looks like a carrot. 
Yeah, it isn't too hard to compress the same algorithm into about one third the J code: (&lt;:2x^y)&amp;|@(_2+*:)^:(y-2) 4x' " 0 
Full source for instant perusal: http://gist.github.com/309262
OK, I have to give you extra bonus props for being the Haskell submitter in the first place. :) That said, without knowing Clojure it seems like it ought to be able to directly represent the same basic solution as Haskell, right? I don't see anything terribly Haskell-specific about that. Java people are not know for trying to jam as much onto one line as possible.
Not being able to do linked lists in a programming language would be a problem, not being able to do it in a target language doesn't bother me. You can always do pointer casting. When compiling a high level language the type system will not fit with the LLVM type system anyway, so a few more casts don't bother me. The problem with basic blocks is that the type system doesn't force them to end with a terminating instruction. Also, you currently have to specify basic blocks when doing phi nodes (but that has a pretty easy fix). 
I need to figure out how to get Haddock on my mac, but it's complaining during the cabal installation process about some deps on 6.12.1. As a result I've not been able to preview the docs, so they're a real mess.
Hey, nice :) While we're golfing, I took a few characters out of your version: [n | p &lt;- [2..], let (n + 1) = 2^p, iterate (\ s -&gt; (s^2 - 2) `mod` n) 4 !! (p - 2) == 0] Anybody else?
What's wrong with n=2^p-1 ?
If you can get out of the limits presented by Haskell 98 and use GADTs (possibly the H98 GADT-emulation will do too), then you can use type-indexed data structures to ensure the terminator at the end of basic blocks. I have done that [here](http://code.google.com/p/omega/source/browse/branches/llvm-gen/src/GenLLVM.hs) with my humble Haskell skills. A terminated instruction [thrist](http://www.opendylan.org/~gabor/Thrist-draft-2008-07-15.pdf) then will receive the type Thrist Instr Cabl Term which becomes a basic block by tacking a name on it.
Your post helped me to figure out why an OpenGL app I'm working on wasn't doing it's depth test properly. My problem turned out to not be one of the things you mention, but it was useful to have working code to compare to. In short, it appears that calls to depthFunc, depthMask and similar don't take affect if you invoke them before creating a window. I have another question, though. Does anyone by chance know how to turn on anti-aliasing? "multisample $= Enabled" just isn't working. I assume there's some hidden knob somewhere I need to turn on.
You can also take out the brackets around `p - 2` without affecting the parsing. But to me that is much less readable.
It worked for me, thanks.. using OpenGL should provide a fun way to learn more Haskell.
With GLUT, basically nothing works before creating a window (the exceptions are the stuff in `Graphics.UI.GLUT.Initialization'). For anti-aliasing, you probably need a "multisampled" buffer: displayMode $= [ RGBAMode, DoubleBuffered, MultiSampling ] and maybe some other settings, too; but I'm just guessing here. You obviously also need proper hardware for that. I'm not totally convinced that multisampling support in GLUT really works, maybe try GLFW too (if you are an OSX user, be aware that GLFW needs the executable to be in an application bundle to work).
Unless I misunderstand something, this functionality is already part of the much more complete [combinat](http://hackage.haskell.org/package/combinat) library: * [permutations of multisets](http://hackage.haskell.org/packages/archive/combinat/0.2.4/doc/html/Math-Combinat-Permutations.html#6) * [partitions of multisets](http://hackage.haskell.org/packages/archive/combinat/0.2.4/doc/html/Math-Combinat-Partitions.html#3) 
Sorry for sending to reddit and not to haskell-cafe, but for some reason I can not post to haskell-cafe (despite being an enlisted member...)
I wanted to keep the current layout of the code. If I abandon that, I know how to ensure terminator instructions. Maybe it's worth doing.
No problem, reddit probably has better exposure anyway. Highlighting top-level functions! Spellchecking only comments and string literals! Thank for this!
&gt; You don't have permission to access /~conor/pub/she/fun/talk.pdf on this server.
We appear to be having a little local difficulty with our servers. Normal service will (had better, grr) resume shortly.
really? link to twitter?
and already 2 new bugfixes i've just encountered while eating my own dogfood: - fixed import definition to allow empty symbol list (for example to import Prelude() ) - bugfix when importing or exporting symbolic operators (which need parenthesis, for example (*) )
Well, fortunately I do also read reddit. I'll run a diff and see if the improvements can make it upstream into vim proper (where the current .hs and .lhs syntax file is hosted).
Some other bugs: a `intersect` b = "foo" -- a is incorrectly identified as the identifier being defined a .+ b = "foo" -- same here foo' a b = foo a b 0 -- the quote mark isn't included as part of the identifier name Also, `hsModuleLabel`, `hsImportLabel`, and `hsImportMod` should probably be linked to `Include` rather than `Label`. I also miss my `TODO`, `XXX`, and `FIXME` words in comments being linked to `Todo`. Otherwise, looks nice! I like being able to turn `spell` back on.
Very interesting one (why haven't I seen it before?), also because it touches my pet-peeve, the module system—more precisely, they assign a further meaning to the dot. The class-object terminology is a little confusing at first (no data keyword?), but there is some elegance to the concepts.
Aaand reddit just became usenet. We should write redhack!
thanks for telling me. Just pushed some fixes: - added support for TODO FIXME XXX TBD - linked hsModuleLabel, hsImportLabel, hsImportMod and hsInfix to more suitable classes - initial support for infix toplevel functions the one not addressed yet is when using symbolic infix operators in toplevel definitions like: a .+ b = "foo" i will fix this when i have more time 
Nice! I know how tricky these things are (I maintained a syntax file for vim for a little while too), so please forgive me for pointing out some more funny cases: foo = a `bar` b a `foo` b = c `bar` d
ouch... seems like vim used the longest match while i assumed it to be the shortest one... but easy to fix and already up 
This is pretty neat. Value-based dispatch is definitely a blind spot for Haskell, so it's pretty cool to be able to use a class as if it was a type.
Meanwhile, try http://strictlypositive.org/she/fun/talk.pdf for the slides, http://strictlypositive.org/she/fun/FileDemo.lhs for the (slow) free monad code, and http://strictlypositive.org/she/fun/FileDemo2.lhs for its codensity-inspired more CPS-ish improvement. But I can't imagine that the department will take too long fixing its web server, come Monday.
[PDF version](http://ropas.snu.ac.kr/~bruno/papers/Objects.pdf) [slides](http://sneezy.cs.nott.ac.uk/fun/feb-08/bruno-slides.pdf)
Pretty cool, though I'm not sure I like how verbose overriding by explicit dictionary passing is. Some syntactic sugar here would be nice, so you could write something like: sortExplicit :: [Int] sortExplicit = sort [2, 6, 5] with Ord.(&gt;) = primIntLT Instead of: object OrdReverse :: Ord Int object OrdReverse where x &gt; y = ¬ (primIntGT x y) sortExplicit :: [Int ] sortExplicit = OrdReverse.sort [2, 6, 5] Edit: fixed typo.
Presumably you could define an object that's an instance of Ord that takes a compare function, and you'd be able to do sortBy like so: sortBy cmp = (CustomOrd cmp).sort Though there's probably no need to define it when it's that short, but all the regular helper functions used with sortBy and friends would still work (comparing, 'on' etc.) which should alleviate the problem.
That depthFunc and depthMask don't work before opening a window makes sense, because you don't have a GL context yet and thus calling GL functions is a noop. It's just the way imperative languages tend to work, especially if it's actually two libraries, on top of many GL things depending upon extensions (that you should be querying before using) I can only recommend reading the red book and other GL-related material, it's definitely _very_ hard to figure everything out based on the API documentation. One other way is to forget about traditional GL and use, e.g., [pipe](http://www.haskell.org/haskellwiki/GPipe), which offers a purely functional EDSL that's compiled down to GLSL.
Looks simpler on basic inspection.
Has anyone tried this?
I agree that the *drawBox* function is the most interesting part. You can write it a bit more simply and clearly like this: drawBox = zipWithM_ renderFace n faces where renderFace norm face = renderPrimitive Quads $ do normal norm mapM_ vertex face You can find *zipWithM_* in *Control.Monad*.
Could you give more information about this? It would be really nice to be able to cross-compile for arm.
Borrowing a famous trick from 99-bottles-of-beer.net, I have just invented a new programming language called "Mersenne" in which the empty program prints the list of all Mersenne primes.
There have been efforts to cross-compile to ARM, but they are all separate from the main distribution, see for example http://www.alpheccar.org/en/posts/show/94
I've only skimmed the paper, but it looks like somthing I've been playing around with in my head for some time, but never gotten past the wild-musing stage with. I basically wanted to turn classes into explicit parameterized records, instances into default values and class-constraints into implicit function parameters (optionally provided by the programmer, or else the default value of the correct type). Seems I'll have to do a more thorough reading.
I have... but I've been using it for a little bit as a pet project, and I believe the original author Tim Newsham, has done several 9P implementations, including a really good one for Python that I've used before. This was actually his code that I dropped in over my project, with permission from Tim, because I hadn't gotten as far along as he had with the implementation of 9P encoding/decoding in Haskell. Anyway, I have a long way to go on the docs, but I've been having a little trouble with getting Haddock installed properly on my Mac. I also want to see if I can make this a little more useful for writing 9P servers via a nice EDSL. It's pretty easy to take Network bytestring data and run it through the get/put functions it uses from binary to decode and encode 9P messages, and I've run it against an unauthenticated Inferno share.
Awesome!
The option is actually "Multisampling", with a small s. That by itself doesn't seem to be working for me either. According to the GLUT-2.2.2.0 docs, there's also a "WithSamplesPerPixel Int" option, but my installed glut was GLUT-2.1.1.2, which doesn't recognize it. I upgraded GLUT (and its various dependencies), but the "WithSamplesPerPixel" option appears to be ignored. For what it's worth, my graphics card is an ATI Radeon 3200 HD.
This is wonderful. Now can we use the LLVM code to eventually produce iPhone code? (This is as opposed to the current iPhone patch). Can we also get the code really tight?
just pushed a rewrite of toplevel function definition syntax matching supporting symbolic infix functions and proper highlighting of Constructors in patterns.
Does LLVM have a possibility of becoming the default backend sometime in the future?
I'd be interesting in this sort of thing for providing APIs into and out of other Haskell programs. I'm really starting to get tired of everybody talking to each other (or, more accurately, _not_ talking to each other) over C calling conventions. Something needs to replace this.
so just like pycuda
Once it takes up a full screen it stops working properly, keeps scrolling up to the top when I type.
This is great. I'd like to see a way to ask for the type of something, though. Say, ":type fmap" or so.
That's an interesting bug. I've tested for that on Chromium, Opera 10, IE6 and Firefox 3.5.8. What browser and version are you running? Cheers!
That's planned!
 &gt; let f x = x * 2 not an expression: `let f x = x * 2' &gt; f x = x * 2 &lt;no location info&gt;: parse error on input `=' &gt; f x :: Int -&gt; Int Not in scope: `f' Not in scope: `x' :&lt;
Chromium 4 on Linux
This is really great! I'd love to see an interactive tutorial and perhaps a small spinner to show that its working. The lag between hitting enter and recieving the answer is too long. Otherwise this is great for when I want to play around with Haskwll and I'm away from my laptop! Edit: noticed a bug. Once the output lines get to the bottom it scrolls up to the top, even though the cursor is off the box. You can hit enter and have it eval but it brings you back up.
This is excellent stuff! Is it at all possible to support the ':t' like in GHCI? It would be great to be able to display types like that.
I'm curious what kind of security restrictions this employs?
Works pretty well. A nice feature would be allowing the user to use `let`, so they can define and play with their own functions easily.
I'm assuming that's handled by [mueval](http://hackage.haskell.org/package/mueval)
Try let f x = x * 2 in f 16
I see the same bug in chromium 5.
Here's a fun one: type any expression, then press enter repeatedly as fast as you can.
I reposted this to Hacker News: http://news.ycombinator.com/item?id=1144418 The people there seem to be giving some worthwhile feedback. Either way, great work, and hopefully you will be able to continue improving this cool project!
Try you a Haskell for great good!
Very cool, good work! If you want other Javascript apps to have access to this from other domains, you might want to consider JSON-P. It's pretty easy to implement.
I can't repeat this locally because it's too fast so I'll have to step through the code. Cheers! **EDIT:** I tried it at work, lets you send the expression twice, haha! Naughty!
OK I'm reproducing this bug, thanks!
freshhawk's correct, all handled by mueval.
Thanks, I can reproduce this. Simple bug.
Yeah, a spinner is a good idea. I considered it as being important for when it's slow. The bug you mention is Chrome I'm guessing? Cheers!
Thanks, that's exactly what I was after. I'll look into that!
That's planned.
Nice application.. Great work.
Just replace `{}` with `foo({})`, that's all it takes.
I think amdpox referred to using let as it's used in GHCi where you can write. let x = 5&lt;return&gt; x + 6&lt;return&gt; 
English isn't my mother tongue, so I had to look up the verb "snerged". These were the two first definitions I [found](http://www.urbandictionary.com/define.php?term=snerge): &gt; * to gain sexual gratification through the act of sniffing chairs/bar stools etcetera. &gt; and &gt; * The act of funneling creamed corn into the rectum using a length of tubing. From the context, I understand of course that you mean something like "Made/Programmed by".
English is my mother tongue, and I would generally say that "snerg" would be the root you need to look up, rather than "snerge". Obviously this doesn't help you at all: &gt; Sniffing a woman’s bicycle seat when she has recently dismounted. So much for urban dictionary! :-)
Another option to consider would be having, separate from the REPL, an editbox for definitions, à la DrScheme.
It would be really cool if you could integrate it with vacuum to show beginners and experienced users how the inner structure works. They have a couple of binding (Cairo, Dot, Opengl) so at least one should fit the bill. 
I wonder if making exercises for writing functions which are then run against QuickCheck would work out well. That's an interesting idea, by the way. Maybe an expandable box.
That is a fantastic idea. One could write a canvas tag/Raphael JavaScript library for it, perhaps!
[ltu discussion](http://lambda-the-ultimate.org/node/3837)
With Konqueror Version 4.3.2 (KDE 4.3.2) Kubuntu 9.10, I can type closing paren, double quote, coma, arobase ... but I can't type 1. opening paren : ( 2. quote : ' 3. dot : . 4. dollar : $ 
If you can install gtk, you can do your final output with e.g. the chart library from hackage and not worry about the gui stuff. You can also output via gnuplot, but the quality isn't as nice. Better to start with the mathier, more core stuff and worry about output and ui later, in fact. So you want to think about how you want to represent functions. Do you want them to be actual haskell functions, or some syntax tree representation of functions? Then you need to think about how you're going to read them in. You need to write a parser to turn them into your internal representation -- take a look at parsec. Then you need to write an evaluator that takes any given point in your domain, and your representation of a function, and turns it into a point in your range. If you're representing functions actual haskell functions, then that evaluator is just function application. Otherwise, it might be slightly trickier. So if you can read in functions and evaluate them, then using chart to plot them should be a snap. Then on top of that you can, if you want, start to explore what it means to create a real gui.
bheklilr, http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours This wikibook taught me how to use parsec, if you need a good tutorial for that so you can parse your functions to plot. That will give you an idea of how you can parse your functions into an internal representation. It is about making a scheme interpreter, but it was actually nice to see parsec used in an example, and I've since took that and made other parsers. And this article: http://twoguysarguing.wordpress.com/2010/02/20/opengl-and-haskell/ about openGL and Haskell was just post to /r/haskell lately if you didn't see that.
The most important one is built into Haskell: IO operations are quarantined within the IO type. So, just by handling functionally pure expressions and not dealing with the IO monad, you get a lot of the needed security "for free."
Not sure what to make of your post. Most of the Haskell community is open source. Do you know about the plethora of projects on [hackage.haskell.org](http://hackage.haskell.org)? Do you know that the main compiler, [GHC](http://www.haskell.org/ghc), is open source? I personally feel [c2hs](http://www.cse.unsw.edu.au/~chak/haskell/c2hs/) could use some love, but don't expect most undergraduates to be interested. There are a host of [proposals](http://www.reddit.com/r/haskell_proposals/) for new or improved Haskell projects. Also, [GSoC](http://code.google.com/soc/) tends to fund some Haskell projects (though I don't know where our proposal list is this year). What are your interests? Do you like or hate graphics and GUIs? How about networking? Cryptography? Perhaps music and sounds are your cup of tea?
There are a bunch of Haskell projects on [github](http://github.com).
Nice work and a great UI. 1. I would suggest hiding type signatures until you want to reveal them as part of a tutorial. Typing "1+2" and getting: &gt; 1+2 =&gt; 3 :: (Num t) =&gt; t Would have made my brain explode before I learned Haskell's type system. Alternatively, follow GHCi with the "+t" option (so types are shown): &gt; 1+2 3 :: (Num t) =&gt; t &gt; let f x = const 1 f :: (Num t1) =&gt; t -&gt; b -&gt; t1 2. Command history in teh console with up/down arrows. 3. Hayoo/Hoogle integrated documentation/lookup. 4. Real-time syntax checking (e.g., illegal expressions are flagged right away).
 &gt; @vixen Is that you lambdabot? &lt;no location info&gt;: parse error on input `@' Darn!
Cheers! Yeah when I start the tutorial I'm gonna explain the type idea as soon as possible, with them being so important in Haskell. Hayoogle integration is a good idea! Real-time syntax checking.. could do it with a JavaScript syntax highlighter. Command history is in the works.
Darcs is pretty interesting and always looking for help http://wiki.darcs.net/Development/GettingStarted
&gt; Uses some libraries from hackage: &gt; [...] &gt; Chart + gtk2hs Unfortunately gtk2hs is not a hackage library, so installing the Haskell platform then typing "cabal install criterion" is only part of the process. On Windows it isn't a lot of fun - I found a mostly untested [unofficial installer](http://www.mail-archive.com/gtk2hs-devel@lists.sourceforge.net/msg00340.html) for GHC 6.10.4 which may just work, but otherwise you basically need to set up a linux environment and several versions of GHC. I gave up, to be honest, but I'll try again once the platform and gtk2hs finish their move to 6.12.1. I don't see any news on the gtk2hs bug tracker, but are people working on cabalizing it right now? It would be a nice step forward for the "batteries included" Haskell.
Criterion is still quite nice without chart, but it is frustrating to have such a large dependency for graph rendering. FWIW, I'm in the same boat as you as I gave up on gtk2hs on any Mac or Windows development machines.
Large deps for rich graph rendering is fine, imo. I *want* png, svg, ... output, and to be pretty, so gtk2hs is fine for that. The problem is that gtk2hs isn't perfectly portable yet.
Look over to the right of the page: * [New Haskell Project Ideas](http://www.reddit.com/r/haskell_proposals) We've been collecting ideas for people -- just for this moment!
Gtk2hs is a portability disaster.
Done command history, though still experimenting, debugging.
The Haskell solution also wins in LOC if you don't write it as verbosely as he did: fib_numbers = 0:1:(zipWith (+) fib_numbers (tail fib_numbers)) main = print $ length $ let limit = 10^999 in takeWhile (&lt; limit) fibonacci_numbers
Does anybody know where to find a vector version of that spiffy haskell logo dons uses in his slides? Here's another raster version; the metadata says it was made in Inkscape. The account of the uploader has since been deleted. http://commons.wikimedia.org/wiki/File:Haskell-Logo-Variation.png
Sorry, but: Floor[(1000Log[10] + Log[5]/2)/Log[GoldenRatio]] 4786 and voilà: StringLength[ToString[Fibonacci[4786]]] 1000 
There are Haskell versions of most of the examples in the [Red Book](http://www.opengl.org/documentation/red_book/), the [Orange Book](http://www.3dshaders.com/home/), and more packaged with [GLUT bindings](http://hackage.haskell.org/cgi-bin/hackage-scripts/package/GLUT). You can browse the source tree directly [here](http://darcs.haskell.org/ghc-6.8/packages/GLUT/examples/).
Thanks to all of you for you help, I will certainly look into these as soon as I get the chance. to sclv, I was wanting to worry more about getting to the point where I would actually need the parser before I got to writing that. But thanks for helping me out with ideas on how to do that as well to safiire, thanks for recommending that wikibook, after a quick look it seems like just what I need. Same to noahmedling =P
We've known for decades that [using fibonacci as a benchmark is a bad idea](http://www.dcs.gla.ac.uk/fp/software/ghc/nofib.html). fibs = 0 : 1 : zipWith (+) fibs (tail fibs) main = print . length . takeWhile (&lt; limit) $ fibs where limit = 10^999
Here are some tutorials I have found useful. The last one is kind of old, so it might not compile cleanly against up-to-date HOpenGL. http://blog.mikael.johanssons.org/archive/2006/09/opengl-programming-in-haskell-a-tutorial-part-1/ http://blog.mikael.johanssons.org/archive/2006/09/opengl-programming-in-haskell-a-tutorial-part-2/ http://public.beuth-hochschule.de/~panitz/hopengl/skript.html The Haskell wiki also has links to the API documentation: http://www.haskell.org/haskellwiki/Opengl
Can we stop with the monad tutorials! I understand monads already, no thanks to these tutorials but I have resisted all temptation at releasing my own monad tutorial to the wild so far.
Sure I'd like those things, too, but it's more than should be strictly required, and that extra weight is limiting a significant number of users to having no graphical output at all. 
Not until I stop hearing "monads are so complicated, I don't understand them, they make Haskell so hard to learn" from otherwise competent programmers. If /r/Haskell doesn't want them, then I don't bother you with them, but there are still many people for whom the word "monad" holds an entirely undeserved sense of dread.
I think the main point is that the prevalence of monad tutorials attaches more value/hype to them than they warrant. They're pretty cool structures but they aren't essential and shouldn't be studied early in a Haskell student's career. In that sense, I disagree with &gt; This is a problem, because monads are as fundamental to modern Haskell as procedures are to C, or classes to Ruby. and in fact wrote a rather long comment on it [here](http://www.reddit.com/r/haskell/comments/ag6bv/should_i_give_haskell_another_try/c0hg3f2).
Where are the Mac OSX maintainers??! We have huge Linux distro teams, but barely anyone working on Mac infrastructure. Apple isn't going to do it for us!
An extra beef is that the Karmic packaged gtk2hs cairo package doesn't have profiling enabled, so I can't reuse my criterion test programs to also generate profiling data. It would be nice to see criterion split up into chart and no-chart portions.
I played around with Criterion; it's very nice, although dons's slides are a bit sparse so what you'll probably end up doing is mucking around the Haddock documentation and type signatures. Some of the docs are a smidge out of date; I'll file reports when I get around to it.
Cool! I'll definitely be trying it out.
The first fibonacci number with 10^1000 digits has index: 47849719667816659713581897523767369103156029829682320941041882785231598 96935981390793900887322904182788884885348675479255502673221641175789925 11917895886139594266617314743967557576930803929815590455298124204046381 72031086668518709036485138118794903322420549152338402651558857966786690 56225291831103983771626617483754646238550743266142168726786787396960114 55174203784607299540288112501046153014972839033961219159935021281847415 32384412788320478931279457506480801636710528553755350502913429822962882 62263151694732839815131664277409022385268489350769666913596654437246805 21534741726934359604694631621902038298542219407755591228280948919789329 02236807701844525775774951068481444743707927468729967181250287889414617 84978033064316029932390739371618892239323669637469225618892625581867243 55874279753260725862339356157534507852982112578387927886202260519003724 79847977016721793305820403329620695526061541002728544841347932722224415 62036643594442771196457521636022955010458743524963810823991243257028590 1099690 real 0m0.059s user 0m0.022s sys 0m0.010s I think this clearly establishes my internet penis as being the biggest. I'm going to bed. Yes, the result is exact. There are no rounding errors. Feel free to double check my work. I will probably post source code at some point but I don't blog. 
Monad is not difficult. Concentrate your mind. Mutter the code into your haskell interpreter tenderly. Aim at an inherently stateful problem you don't like. Now, unleash your annihilation of Love!
Don't worry about the CAF. The following is just as fast. You guys and your premature... optimizations. fibs = 0 : 1 : zipWith (+) fibs (tail fibs) main = print $ length $ takeWhile (&lt; 10^999) fibs
fibs[4782] is the smallest fib with 1000 digits. fibs[4786] is the largest fib with 1000 digits. Sorry.
&gt;Last time I benchmarked Ruby it also came in… a little late, and some commentators suggested trying out other Ruby compilers — The truth is however, that small benchmarks like this really show very little and if you are working in a performance critical zone, you’ll want to test something which mimics the actual project and the actual environment. No mention of which version of Ruby he used, and then he says the whole idea of benching with fib is silly, so I think I may just vote this article down I suppose.
How embarrassing! I was off by one: `digits[b_,x_] := Floor[Log[b,x]]+1` so that I should have taken `Floor[(999Log[10] + Log[5]/2)/Log[GoldenRatio]]`, except that this gives `4781`, but I expected to be off by one because I'm approximating F(n) as phi^n/sqrt(5) to be able to take logarithms. In any case, thank you for correcting my mistake!
&gt; Are we supposed to use the new (beta) Haskell Platform instead of the regular GHC install to get this sort of tool support? Yes.
&gt; Given that installing Haskell packages is painless with Cabal Unless you a Snow Leopard mac, in which case... Resolving dependencies... Configuring mmap-0.4.1... /var/folders/u3/u3JfEOrdGAWC636N0MgTDU+++TI/-Tmp-/11673.c:1:0: error: CPU you selected does not support x86-64 instruction set Configuring terminfo-0.3.1.1... /var/folders/u3/u3JfEOrdGAWC636N0MgTDU+++TI/-Tmp-/11673.c:1:0: error: CPU you selected does not support x86-64 instruction set Configuring utf8-string-0.3.6... /var/folders/u3/u3JfEOrdGAWC636N0MgTDU+++TI/-Tmp-/11673.c:1:0: error: CPU you selected does not support x86-64 instruction set Configuring zlib-0.5.2.0... /var/folders/u3/u3JfEOrdGAWC636N0MgTDU+++TI/-Tmp-/11673.c:1:0: error: CPU you selected does not support x86-64 instruction set cabal: Error: some packages failed to install: darcs-2.3.1 depends on zlib-0.5.2.0 which failed to install. hashed-storage-0.3.9 depends on zlib-0.5.2.0 which failed to install. haskeline-0.6.2.2 depends on utf8-string-0.3.6 which failed to install. mmap-0.4.1 failed during the configure step. The exception was: exit: ExitFailure 1 terminfo-0.3.1.1 failed during the configure step. The exception was: exit: ExitFailure 1 utf8-string-0.3.6 failed during the configure step. The exception was: exit: ExitFailure 1 zlib-0.5.2.0 failed during the configure step. The exception was: exit: ExitFailure 1
Cabal install on Windows is a lottery too if the packages have native dependencies.
Of course there should be a cabal binary included with ghc. This seems to be obvious to everyone except the people promoting the Haskell Platform. As it is, creating the cabal binary is a major pain. Having that pain felt by one person putting together the ghc distribution is better than having it felt by thousands installing ghc. In my opinion, a missing cabal binary seriously detracts from a good Haskell experience. If the Haskell Platform was released at the same time as ghc this would not be a problem, but I don't see that happening anytime soon. (It's now been 3 months since the ghc release and there are still crucial packages that are part of the Haskell Platform that don't work.) 
I don't know or like Ruby that much, but I bet it can be also.
You need to [patch](http://www.haskell.org/haskellwiki/Mac_OS_X) the script that invokes ghc and have universal versions of the foreign C libraries available. Those can be installed with sudo port install zlib +universal for example.
Why isn't the patch applied to the haskell platform?
I was wondering the exact same thing two days ago. However, perhaps it's time to consider something to replace cabal with something a little more capable and include *that* into GHC. And you can just steal/convert/wrap the existing Hackage packages and host them some other way (ie, git, mercurial) with versioning information intact
Do you think this become less of a problem as GHC matures and fewer people need/want to upgrade to the latest version as soon at it comes out?
No, I think more people will want to upgrade to the latest version as Haskell gets more users.
Certainly this isn't just going to happen on its own. There will continue to be fancy new features in the latest version of GHC. It's not as if using GHC 6.10, which is nicely packaged with the Haskell Platform, is any kind of a burden; people just want to try the latest version. There has been some (it looks to me) conscious effort to talk about the release of a new GHC as an *internal* step toward the release of a new Haskell Platform. That might relieve the pressure a bit, if it's successful. And I'm convinced it's the right way to go. Asking compiler people to handle library compatibility issues prior to releasing a new compiler just makes it harder to release a new compiler... and there's a whole group of people excited about making the Haskell Platform work nicely.
&gt; with something a little more capable now you're scaring me. do you know how clever cabal is, considering what it does?
Because we haven't done a release since Snow Leopard came out. One is scheduled for March 20-ish, if we can get it together in time.
My main objection is conceptual, but as for practical problems, if we bundle it with ghc then it also has to be included in the ghc development and build process. It means the intermediate and beta ghc releases also need a working cabal program. It would mean that like haddock, it'd be sucked into the ghc build system and people would expect that every time ghc makes some change that Cabal would stay in sync. This just isn't realistic. During the last ghc development round we had a couple months where ghc-pkg was doing new things that the cabal-install program did not yet understand. If we require these sorts of changes to be synchronised then they're more likely simply not to happen. We could say "just chuck a pre-compiled cabal program into the final release ghc installers" without properly integrating cabal into the ghc build system. I'm not sure that'd satisfy everyone since it means it'd not be there in all the betas and pre-releases and it makes more work for the people preparing installers for various platforms. Perhaps the solution is to downplay early ghc releases even further (eg make them source only) and have more of a "live" permanent beta version of the Haskell Platform so that testers and early adopters can have more of a whole system to play with, even if not every aspect of it works immediately. We're open to suggestions but making ghc HQ responsible for integrating and releasing more stuff is going in the wrong direction.
Is it the case that to get cabal running you download an appropriate cabal-install tarball from hackagedb, unzip it and run bootstrap.sh? How about this: Put together a standalone haskell exe that can * contact hackagedb and ask what cabal install tarball to download * download it * unzip it * run the equivalent of the commands in bootstrap.sh so it doesn't rely on external programs or c libraries being installed. Then, to get cabal running on a naked ghc install, include a script for each platform with ghc which does ghc --make xxx &amp;&amp; xxx on this code. Then getting cabal running on any ghc install is a one liner. 
Personally, I want to upgrade to the latest version every time new, tangible performance gains are made, especially with regard to concurrency. Of course, I haven't found the Haskell Platform to be compelling at all and wind up installing everything directly with cabal (which is, as you say, a pain to build).
I couldn't get the website to load, and google search for cached results wasn't helpful. Anyone have the link?
As far as I'm concerned "just chuck a pre-compiled cabal program into the final release ghc installers" is fine. People who actually download beta versions of the compiler can be expected to go through some extra trouble. I don't dispute that it is conceptually ugly, but people trying to install Haskell first experience the ugliness of the current practice, rather than its theoretical beauty. I'm quite tempted to set up my own ghc binary distribution site, which just has cabal added to the bin directory.
i found an error in comment highlighting. in http://hpaste.org/fastcgi/hpaste.fcgi/view?id=22959 (line 4), the comment is not highlighted as such.
cabal-install is a sophisticated constraint resolving system for packaging, more expressive than pretty much any other tool out there, in how it resolves versioning. cabal's build system isn't terribly sophisticated. Remember that Cabal / cabal / cabal.exe / cabal-install is a huge set of tools for supporting distribution and construction of software.
I'll bite: because GHC is a compiler and you don't need the command-line Cabal tool in order to compile Haskell? Why isn't apt-get installed with gcc on Debian?
Because Windows doesn't have a package system.
As dons says it's not the build system part. That bit is rather embarrassing frankly. It turns out that selecting the right versions of all the dependencies of a package is rather hard. You'd expect that it's just a matter of looking at a package then recursively looking at all of its dependencies. It's actually not that simple because there is the cross-cutting restriction that each dependency on a package A must refer to the same version of A. Suddenly that makes the problem NP-complete. So that's the job that cabal-install does. It has a (not very clever) constraint solver which in practice, most of the time, is able to consistently install a bunch of packages and their dependencies.
Because apt-get isn't a C library management tool, it's a system-wide package management tool. Oddly enough, it *is* installed as part of Debian, whether or not you install gcc. Cabal, on the other hand, is strictly a Haskell package management tool. It doesn't do anything for you if you don't have a Haskell compiler and, if you have a Haskell compiler, you'll probably want to install some extra libraries at some point.
But if GHC included everything you "probably" wanted, it would include a lot of those extra libraries to begin with (most people will *probably* want Haddock, bytestring, the unix or Win32 libraries, and parsec at some point, just to pick some examples). I thought the whole point of the Haskell Platform was to bundle only the essentials with GHC and make everything else optional. If Haskell programmers didn't want the option of downloading a toolkit that included just what's absolutely necessary to write code rather than everything they might conceivably ever want for development, they'd be Java programmers.
http://rapidshare.com/files/355328555/10.1.1.53.4124.pdf.html
Uh, you just replaced one link I couldn't download from... with another link I couldn't download from. :-( (gave you an upvote anyway for trying to be helpful though)
OK, so show me the Haskell programmers that don't want cabal.
I'm just jaded because I failed to get wx or Gtk2Hs working on my XP work PC. The amount of cygwin mysys hoop jumping is just depressing compared to apt-get/cabal install on Linux :( 
I think I can find five or six just in my office alone.
Why isn't gem installed with ruby? Oh, wait.
I definitely think we should emulate the Ruby community. The next ICFP will feature a talk on how to parameterize your types like a porn star.
Ok, dl works now. Tossed in my public for those of you who hate Rapidshare. http://web.mit.edu/~ezyang/Public/10.1.1.53.4124.pdf
It _is_ possible to take the good and ignore the bad.
thanks a lot. It's a really great example for my test file. It was kinda fun to debug and revealed 2 interdependent bugs. It's fixed and changes are already pushed to the repository.
I admit it was a bit unfair to bring that up, but it *is* all I think of whenever someone mentions Ruby. Anyway, maybe I'm just a dinosaur for thinking this, but GHC is useful without cabal-install. I honestly have no idea whether Ruby is useful without gem. But I know I want to be able to download a compiler for a language I program in without filling up my hard drive with stuff I don't use. There's a reason flashlights come without batteries included: maybe I got a really good deal on batteries at Costco and I don't need any more.
Five or six that have never used cabal? Then I think they are seriously missing out. In my opinion, hackage and cabal or some of the best things about Haskell these days.
On the other hand asking compiler people to handle library compatibility issues might make them less likely to break compatibility all the time which is one of the main reasons Haskell is a joke for any serious development by people not involved full time in the community at the moment.
If it is so clever why is Haskell the only language on my Linux system where packages consistently fail to build right in the middle of the build process due to library incompatibilities (instead of some configure step at the start) and where I have to manually reinstall packages in a certain order just to get anything working again (luckily xmonad is built with static libs or that fact would have forced me to switch to another WM long ago)? And I am not even talking exotic packages, just basic stuff like haddock, bytestring,...
We really need to work on that avoiding success thing.
That and it doesn't have default locations for any libraries or includes which makes any kind of development on Windows a huge pita.
&gt; where packages consistently fail to build right in the middle of the build process due to library incompatibilities what are your tools for other languages doing? nothing? no automated dependency resolution? you download and install and track deps by hand? This is an NP hard problem, remember. That said, if you're installing 1000s of packages, and upgrading bits and pieces of that dependency graph on the fly, in any language other than Haskell, I'd love to hear about how they handle versioning.
&gt; Haskell is a joke for any serious development by people not involved full time in the community There are plenty of happy engineers at Amgen, Galois, Credit Suisse, BarCap not involved full time in the community. Lighten up, and make some concrete contribution to the debate.
No, I download and install everything via Portage. (though I am no stranger to compiling stuff manually either, usually stuff that is not in the portage tree or is a version directly from version control or the stuff I develop myself). For some reason Haskell has a lot more problems integrating properly with Portage than other languages. On the other hand, unlike e.g. Ruby Haskell doesn't work properly when I install all libraries via cabal install either. Most compile failures I have seen so far are related to the bytestring package, which, looking at the documentation, doesn't make much sense at all with its several versions all implementing the same functionality and apparently major changes (judging by how narrow most other packages define the bytestring dependency version range) in the API from version to version. A lot of other failures also result from missing packages that would be installed later in the same run. Either a lof of authors don't list their dependencies properly or that part of the tool needs some work.
&gt; Most compile failures I have seen so far are related to the bytestring package Are you running an old GHC? bytestring versions have been stable for almost 2 years now. &gt; Haskell has a lot more problems integrating properly with Portage Can you talk to the (largish) gentoo team about this.
I am just frustrated because I do like Haskell as a language a lot. I want to use it at work but I can't with good conscience recommend it to my coworkers and that is mostly for the reasons that half of the time I am trying to learn about Haskell I spend fixing something in the toolchain or libraries I am trying to use. And that is on Linux, I don't even want to think about platforms like Windows where even mainstream languages like C++ suck a lot.
The Haskell Platform bundles: * GHC * A set of libraries * A package installer * A set of dev tools It *is* the install bundle you're looking for.
How can you use Hackage if you don't have full dep resolution? Or don't they use 3rd part packages?
Every language has packages with version numbers. Very few try to support multiple versions simultaneously. We do -- we can build more things -- but it is also harder.
I've used cabal. I don't need it for my daily work. The best things about Haskell for me are the things that existed when I started using Haskell ten years ago.
Here's an OpenGL [fractal renderer](http://hackage.haskell.org/package/hfractal). It's not perfect, but uses some of the 2D OpenGL capabilities, so may useful code to read and borrow from. 
Most of the time, what comes with GHC is sufficient for the work I do (with a few exceptions like parsec and mtl, but people managed to install those long before Cabal existed). What's useful about Haskell to me is that it's a concise language that lets me say more with less code. 99% of what's on Hackage has little if any relevance to anything I do on even a yearly basis. It's good that all those things exist, and I'm not arguing for the abolition of Cabal or Hackage, of course; just that we shouldn't drink so much of the applications kool-aid that we forget that our language is worth using in and of itself.
I don't agree that Haskell is a joke for serious development. I do think that someone who tells you Haskell is a joke is expressing genuine frustration and it might be worth your while to ask why. You and I both know that there are plenty of people happily using Haskell for serious development, but telling that to someone who has tried it and *isn't* happy is not constructive feedback for them.
&gt; No, I download and install everything via Portage. It is not fair to compare portage with hackage. Portage has a large team doing package QA. They make sure all the current versions of packages work together. Hackage is more like sourceforge, it's the original upstream distribution point. Each package author releases whenever they are ready. There is not a great deal of QA infrastructure to work out which packages work with what other packages (though we have plans for it).
Ghc upwards compatibility is a joke. Every major release breaks existing code in some way. But this is a price I am willing to pay, but I can see how other people might find upgrading the compiler too onerous. 
It can be very frustrating. 
I've been using bauerbill with the aur repository on arch, trying to avoid using cabal as well, but this may cause problems...
I love this paper. I wonder how many people have actually abided by the reporting rules mentioned in Section 3, though. I also love that nofib still includes fib :-)
I agree they're expressing a serious concern, but whinging about cabal isn't detail enough to act on. Be precise in what you need, and there's a chance tools will help. Stupid comments about "Haskell is a joke" when *no freaking other language toolset is doing what we do* don't help at all.
For practical purposes, cabal should do exactly what cpan does for perl. Not including it with ghc and the platform is a huge mistake.
It's included in the platform.
woops, still though... doesn't perl come with cpan?
&gt; A combinator is the "...use of a [Higher Order Function] as an infix &gt; operator in a function-definition..." &gt; [(Parser Combinator - Wikipedia)](http://en.wikipedia.org/wiki/Parser_combinator) Infix? Higher-order functions only? That's not how I ever understood the term "combinator". In the context of everyday use in programming, I would define it as "a function that can be combined with other functions using composition to form more complex functions". True, in combinatory logic they happen to be all higher-order functions, and even in programming many of the most interesting and useful ones are higher-order. But many functions that I would definitely consider as combinators are not higher-order. And anyway, the distinction between higher-order and not higher-order has little significance in the context of curried notation. Does anyone agree with me that the Parser Combinator page on Wikipedia needs editing?
A part of my mind just died a little.
Given a man a fish and he'll eat for one day, give a man a fishing rod and he'll eat for life. Give a programmer a library and he'll have a library, give a programmer cabal and he won't ask for any more libraries.
Care to expand on this a little?
Space suits! Radio active waste! Bears! Oh My!
I write ocaml at work, and I blame it regularly for not being strict enough. I like purely functional programming, and I don't rely on references, exceptions and so on for algorithmic purposes. But having an escape is nice, for tracing, or when you need arrays. I like Haskell because it is pure and its type system is more expressive than the ocaml one (type classes, forall-quantification). However I still don't get why laziness is a benefit (given that complexity gets very hard to evaluate). To me Haskell with destructive updates for _all_ objects is somehow less "strict" than ocaml. It shares syntax but I can't see a use case : why remove the features that make Haskell Haskell ? tl;dr : haskell is pure and beautiful, if you want holes in it ocaml is already there.
It doesn't really have destructive updates for all objects though. If the type says there aren't any effects, then there aren't. All objects *can* be destructively updated, given the appropriate effect type, but there's no danger of mixing effectful code in where pure code is required.
I have some sympathy for this position, but it is a problem that the platform was meant to solve. Shouldn't we be putting our effort (i.e. the community's effort) into really slick packaging of the platform, and providing early alpha/beta platform releases for people who want to try out a recent GHC? We ought to have nightly builds of the platform just like we do for GHC, completely automated with new snapshot distributions popping out each morning for the eager punters to get their hands on. Honestly we have enough trouble just keeping the GHC build working without worrying about packaging cabal-install and its dependencies too. It's never just a case of "chucking in" a cabal binary: the binary has to come from somewhere, and then you end up wanting to automate that process.
Laziness by default gives a form of modularity. Which makes the composition of function cheaper than in a strict language. Morever laziness is "just" the default behavior, strictness can be gained by using seq, but is better achieved using strict fields in some data types and strict arguments. In Ocaml to use laziness you have to use lazy and Lazy.force and this is a lot more explicit than changing a data type.
&gt; I write ocaml at work Where do you work? I don't know of many commercial organisations using FP...
I think you are using a different definition of strict....
I like how Clojure seems to be doing things. The language is strict but it has, what appears to be, great support for lazy sequences. In my experience I generally only really want lazyness when I'm dealing with streams of data. The way you manipulate streams in Clojure is also unified with how you manipulate lists, which gives it a big win over Python's generator stuff in my opinion.
indeed. I used "strict" in the sense that it enforces more constraints on computation performed (so this is not related to laziness).
From the article: "This is a problem, because monads are as fundamental to modern Haskell as procedures are to C, or classes to Ruby." That's simply not true. I've written substantial amounts of Haskell code without using any monad operations at all. I've never written a substantial amount of C code without using a procedure. In fact, I tend to find that the better I am at expressing ideas in Haskell, the less often I end up writing code using monads in substantial, structural ways. (I'm not counting incidental uses of lists, and list comprehensions, the ((-&gt;) a) monads, etc. just to structure a local piece of code... largely because in this case, it's often difficult to tell the difference between what should count as "using a monad" and what's just using the basic type) I used a lot of monads, stacks of monad transformers, and the like when I didn't know Haskell very well, and when I was taking college courses that explained algorithms in imperative terms and then assigned us to implement them. That's not to say that the monadic style is always bad... but I do wonder if we too often give people a chainsaw when they ought to be working with a whittling knife.
So to be clear, you're saying you don't particularly care for Cabal or Hackage at all, which is why GHC shouldn't include Cabal? That's a valid point of view, but lots of people really need 100's of packages to do stuff, and those people are the ones who want Cabal in GHC.
With GHC 6.10.4, which has known critical bugs (if you are doing certain things)
Consider to help with Leksah? http://www.leksah.org 
Yep. Then take it one step further, and put it in the GHC installer (or the GHC-Cabal installer). I just want to access a new compiler, with a copy of Cabal, faster than the platform even intends to deliver.
Yes, there should be nightly builds of the platform, but there isn't. I volonteer to generate the cabal binary once a year for inclusion in the ghc distributions if that's all that's missing.
A great example of this is the fact that, in a lazy language, you don't need any "special forms", that is, ways to make code evaluate or not evaluate syntactically. It's all done for you!
No, that's not what I'm saying. I'm saying that GHC is useful without Cabal (and citing the use habits of myself and some of my friends as evidence for that). It makes no sense to me to strip most libraries out of GHC and require them to be downloaded separately (as has been the trend) and then bundle it with something else that you also don't, strictly, need in order to use GHC.
Isn't that basically what lisp is, minus the static typing?
in a lazy language you need special forms to do things strictly.
yes, and haskell is like java but without objects.
The whole point is to statically type and separate the effects.
It's also the last stable release of the 6.10 branch -- GHC's best effort to build a stable compiler. Jumping to 6.12 prematurely breaks a lot more things, in more unusual ways. Living on a stable toolchain isn't for everyone, though, of course. But it works for many many thousands of users, going by the downloads.
Don't wait for agreement. Be bold.
For a real understanding of what Disciple/DDC is trying to achieve I suggest you read the first chapter of Ben Lippmeier's PhD thesis titled [Type Inference and Optimisation for an Impure World](http://cs.anu.edu.au/~Ben.Lippmeier/project/thesis/thesis-lippmeier-sub.pdf). The first chapter is highly readable and details the problems of effects, purity and mutability, the various solutions (Haskell's monads, Clean uniqueness types) and the problems with those solutions. 
It *is* necessary for other systems. apt-get has some kind of "best first" search that's apparently powerful enough to play sudoku while the SUSE package manager has a SAT solver.
Lovely.
I've been learning to write ocaml lately and I find it really hard to figure out how to do things without laziness. Today I had a map of identifiers to expressions that I needed to process. Expressions can refer to identifiers, but the references are known to be acyclic. I need to process this map to create a new map from identifiers to semantics. To compute the semantics of an expression will involve looking up identifiers and their semantics. In Haskell I would simply do computeSemantics exprMap = semantics where semantics = map expr process process expr = ... (lookup semantics identifer) ... In particular when processing semantics I would feel free to look-up identifiers in the semantics map that I'm in the middle of building. In Haskell I would only compute as much of the semantics as I need to use, and it would automatically chase references and cache the results (essentially Haskell is doing dynamic programming for me). But I'm not sure how to do it best in ocaml. One way is to compute a topological sort and process the map in that order, but then I end up processing the entire map even if I don't need to use it all. The other way, which would be more similar to Haskell code, is to put the semantics into some mutable map that starts out mapping everything to None, and then fill it in with Some result as things are demanded. But then I have mutable state and all the problems that come with it. Not that my code is multi-threaded at the moment, but I'd need to add locks to make it thread safe. Maybe Haskell has made me too wary of mutable state. I'm an ocaml newbie, so maybe I'm missing something obvious. :)
to get the code: git clone git://github.com/elbrujohalcon/wxhnotepad.git
http://hackage.haskell.org/packages/archive/pkg-list.html
Sure. But in this case, the annotations are for strictly performance changes, whereas laziness annotations change the semantics of code.
yes, in ocaml you would essentially have to apply the conversion to the entire map, or just not convert the map at all and apply the conversion manually when you need it. But you exactly now what is going on when you apply a function ; there is no hidden cost. My favorite (counter-)example is how to take the two minimal elements of a list : in a non-lazy world there is a particular algorithm that is cheaper than sort. In Haskell you just say "take 2 . sort" and it is optimal. Black magic :) And apart from that, yes I don't like mutable state very much...
bookmarked, thanks !
You don't, really, unless you count case, which is necessary in the strict languages, too. With case, you can implement `seq` on every datatype. The only thing missing will be function types, but having `seq` work on those is arguably a mistake anyway.
Interesting but terrible name.
Me. I think cabal's build system is a horrible botch -- letting arbitrary code be executed at configure and run time is a *bad* idea. I want both of these tasks to be completely declarative and trivially auditable, and no, having Cabal provide default hooks so that most programs use the default code, doesn't cut it -- it's still *code* rather than data. Make is not great, but it does at least provide the "-n" option where I can see what will be done.
What do you call the backend that the "uninstall programs" applet uses?
Is there some particular reason as to why the computation is delayed until runtime? Combining accumulate from the C++ standard library with Boost's counting iterator: std::accumulate(make_counting_iterator(1), make_counting_iterator(100000001), 0LL); and the whole thing is computed at compile-time. 
That would be one optimization, using Gauss' formula would be another possibility that would be faster. (I do realize that he was making a general point of course but it might be worth checking for sums of lists of consecutive numbers in the optimizer if cases like this happen regularly). Of course it is great when compilers get smarter but programmers should still be able to get better results by manual optimization at bottlenecks.
The LLVM doesn't do the loop constant folding. Though there's probably a way to trigger it. I'll explore that this weekend. But that isn't the point of the post. I was torn: * I needed an example where pretty much anyone could understand the assembly * It needed to show fusion If anyone has a nicer example that doesn't, e.g. have a closed form...
Perhaps the hailstone sequence?
I suspect the primary purpose of that optimization is to optimize small scale benchmarks, and nothing else. I can't think of any time in my entire life that I have simply added the numbers for 1 to a constant large X in code with no other intervening logic for any real program. Or even with intervening logic; generally if you've got something being enumerated like that the numbers are unique IDs and it's not actually meaningful to add them. (Forstalling one likely objection, recall that adding together numbers in a set of data so you can average them is not the point; adding numbers from runtime data can't be pre-optimized.) The Internet being what it is, one or two people reading this may have had some obscure use once, but I seriously doubt it's a generally useful thing or worth even the small price of adding it as an optimization (where it inevitably incurs risk of bugs and future risk of maintenance).
Maybe you don't need the latest GHC for that, why wouldn't you use or recommend The haskell platform ?
Yeah, I suspect you are correct. The general principle should work though: Try to recognize naive implementations of well known and common problems and substitute the best known for their algorithm.
You could make the upper a bound a run-time parameter to rule out compile-time evaluation as optimization. But this doesn't really add anything to the point of your post; I like it the way it is.
You suspect wrong. Fusion, also known as deforestation, is about generically merging any number of passes over a data structure into a single tight loop, easily supports that intervening logic you mention and has been used in production for ages now. An implementation of a statically-typed functional language that doesn't come with _some_ kind of fusion is a toy. [Stream fusion](http://www.cse.unsw.edu.au/~dons/papers/CLS07.html), the newish scheme dons is talking about (and invented) has the property that, unlike older schemes, it can fuse literally anything under the sun.
1+2 
WAI?
I'll be publishing my PhD thesis on this topic "real soon now"TM, though there will be less assembly and more fixpoint induction. :-)
&gt; Stream fusion, the newish scheme dons is talking about (and invented) co-invented ;-) along with Roman (of DPH fame) and myself. &gt; unlike older schemes, it can fuse literally anything under the sun. Sadly it's not quite that good. There are places it's better than build/foldr but there are also cases where build/foldr works better. If it were always better then we'd have switched it over already in ghc.
Because we don't have a Haskell partial evaluator or termination analysis.
Have they fixed the case that can trigger exponential running time yet?
I like how the headline isn't biased or anything.
Yes, for the most part.
I'm not talking about fusion being useless. I'm talking about writing an optimization that optimizes a straight up summation of a for loop. Yeah, it works, it's all perfectly sensible even in gcc, but when's the last you _did that_? When did you _actually_ add the numbers from 1 to one billion in your program, hard-coded in your source code? The closest thing I've ever done, now that I think about it the next day, is 20 for i = 1 to 2000 30 next in Commodore 64 basic; looping from 1 to 1000 took roughly one second, good enough for government work. Still no summation. _That's_ what I'm talking about. Stream fusion is great. This particular optimization is for the trivial benchmark that straight-up summing in a loop is.
As a daily darcs user, let me say great work darcs devs. In 2.4 I especially like the systematic benchmarking that's coming together, hunk editing (now I can record adjacent changes separately), the long list of resolved issues, the number of committers, and the attention to cleanups and consistency. Thanks! 
We've been using darcs for years for the Tahoe-LAFS project (http://tahoe-lafs.org), and I'm pretty happy with it. It has very good usability. It has some performance problems for a project as large as ours, so I'm glad to see that the darcs hackers have started doing automated benchmarking and graphing the performance results. Also I've noticed that a few Haskell hackers have been focussing on benchmarking, profiling, and optimizing Haskell data structures, so I anticipate that future versions of darcs will benefit from improvements in those tools.
Web Application Interface
I actually switched from darcs to Mercurial; the Brian O'Sullivan connection with Haskell (http://hgbook.red-bean.com/) helped me get over my feelings of disloyalty. For my light personal use, Mercurial is a pair of shoes that fit. I don't feel like my data fell into a "black hole with assurances". Of course, the controlled experiment would be for me to switch back to darcs and see if I now know how to use it effectively. But life is short; we'll see.
`sed` is annoyingly strict. I accidentally tried to print an infinite list in this colored GHCi, but didn't realize it, because `sed` wasn't producing any output.
Meh, you're right. There's actually a few problems. Sometimes it eats the first character of a result. I have no idea how to fix this. 
Oh, I couldn't agree more. If such things aren't caught by supercompilation or other general measures, just don't care about them. In ghc's case, users can write their own optimization rules, so even the unlikely cases can be dealt with, _if you really want_.
I sometimes work in LabView which is a domain-specific dataflow programming language. LabView takes a similar approach to concurrency to strict pure functional programming languages like Standard ML. The result is natural concurrency that doesn't require the user to interact with concurrency primitives or even higher level traditional concurrency objects. The entire system is highly composable, with entire programs that have complex user interfaces fitting into larger systems without any re-working. The price of this is a lot of rigidity on the UI side, which is not necessarily a bad thing in some circumstances. It is difficult to create custom user interfaces unfortunately. Anyway, I think this is the sort of thing that the author is talking about in his post. Most of the people commenting on his blog haven't really understood what he's talking about. I find myself disagreeing with the author on several points about OO languages. I don't necessarily agree that the actor model is always bad. I have used a more abstract, more natural concurrency system, and it does improve program composability while permitting fine-grained multi-threading. It does improve my output to not worry about concurrency, while still achieving concurrency; much like not worrying about garbage in Java improves my output over C.
This is neat. I thought of something similar the other day, different colours for input and output, possibly with a differently coloured prompt as well, but never got around to look at how I'd do it.
&gt; sometimes sed delays the colored parts a bit, causing your prompt to be printed before the success or error message. This doesn't make sense to me. sed doesn't change the order of its output...
Well some part of LLVM does some kind of constant folding: #include &lt;stdint.h&gt; int64_t foo () { int64_t s=0, i; for ( i=1; i&lt;=10000000LL; ++i ) s+=i; return s; } via http://llvm.org/demo/index.cgi becomes define i64 @foo() nounwind readnone { entry: ret i64 50000005000000 } 
Just a matter of getting the code into the right shape, and triggering the phase llvm is using.
Does ghci use both stdout and stderr?
Yes, it does, and it does so in a strange way, but I don't remember the details. I was playing with some wrappers around ghci once, and never did manage to get the stdout and stderr in the right order 100% of the time.
Great work! I'd don't think "Haskell died" is the right message we want to send for e.g. "fix error". Something like "Terminated." is what we used in lambdbot. Have you looked at the lambdabot sandboxing design? There is some useful code there for reinterpreting messages to be friendlier, and more.
rather awesome, without a doubt!
I was just thinking: http://tryhaskell.org + http://moonpatio.com/vacuum/ + http://thejit.org/ 
Huh, it's weird that the the command history is cyclic. I'm not totally convinced i don't like it, it's just unconventional.
This is great! Looking forward to more stuff in the tutorial.
A good example of why not to enable blog comments for programming-related posts.
I have extensive experience with C, C++, python and (a long time ago) 6809 assembly and Forth. Also done a fair bit of emacs lisp. step7 makes no sense to me. "let villain = (28,"chirs­") in fst villain" WTF is this "in fst villain" bit? What causes it to return the first element of the tuple? I guess "fst" means "first", but the rest of it is Greek to me. You are going to get a lot of beginners asking about this.
You're right! "Haskell died" is really bad for this tutorial. :) I'll look through lambdabot's handling of error messages. Cheers!
Also, I know you're in alpha, but the fact that it stops at step11 feels like a bit of a gyp. I don't feel like I've learned anything. But perhaps at this stage you are merely requesting feedback regarding the interface?
I think that's a good point. It's really just trying to step around the fact I haven't implemented a command to make variables yet. It's next on my list. Though if, beforehand, I introduced `let x = 5 in 2 + x`, perhaps the above `let` might make perfect sense? **EDIT** fixed typo'd "command"
Try this: http://learnyouahaskell.com/syntax-in-functions#let-it-be
I copied that behaviour from Emacs's shell-mode. Not sure if newbies will even think to use command history, I might have to explain that in the tutorial?
This is very cool. But I'm not sure how newbie friendly it would be? I might just implement it on tryhaskell anyway and then we can decide how best to present them and with what functions. Do you fancy implementing an interface from vacuum to the JIT? :P
That's right, the tutorial is WIP. It may be restructured if people don't like it. I'm interested to see if I'm on the right track.
This is really neat! Is there anything to can be done to improve the display on netbook widescreen monitors, like, display the tutorial instructions besides the interpreter frame and not below them?
Ahh. Do you have to scroll to see all the guide section? Maybe we could have a toggle "Horizontal/Vertical" button? I definitely can sort this out, though I'd have to rework the layout because it doesn't expand horizontally right now! **EDIT:** Another option is enabling the user to vertically resizing the console.
I'm not totallly an Haskel n00b, since I can read some of it, but I never wrote anything in it. I think it's a good idea to start with simple things like you do here, and crank up the complexity gradually. It makes people feel in control, which is primordial for this kind of friendly, feelgood tutorial. 
It may come in handy to explain visually how some functions work (foldl, foldr, map...)
I get some weird encoding issue on Chrome 5.0.307.5. All the characters on the page look like a box with a question mark..
Showing a general optimization with a special case will always look funny. However, a real-world case would seem unnecessarily complex and would have uninteresting details.
Ah, yes, that makes much more sense to me. Thank you.
Definitely think this is a promising idea, and I appreciate your work on it.
I cannot paste text into the form. Is this supposed to work or my browser (Galeon, gecko based) is culprit?
I can't find xmonad-extras in the AUR neither can I find the included packages in xmonad-contrib(-darcs). Am I looking somewhere wrong? Thanks.
Yeah, I have to scroll to see all the guide section. This is mostly problematic because typing into the console scrolls up again, so carefully copying what the tutorial wants me to type necessitates a lot of scrolling down again. :)
Because you might get corrected?
It's not supposed to work although I have considered trying to do it. Hopefully by making code snippets clickable (so that it inserts it into the console) I can avoid copy/paste.
Ah. That's unworkable, I'll have to come up with a horizontal display. Don't want people having to scroll.
I think Haskell lends itself better to a dual setup. On one side should be an editable input box containing your definitions, and on the other side should be the normal interaction console, with the ability to push a button to reload the console with current definitions, sort of like how DrScheme works. EDIT: Whoops, that's one of your ideas. Didn't notice.
How about providing a classic textarea for pasting (perhaps hidden until a "paste button" in the form is pressed) and a button to confirm and place the text from textarea to the console component? That could be useful for pasting from other internet sources.
this may be the time I keep going after "hello world" in my attempt to learn haskell, looking forward to the whole course, thanks :D
This looks magical to me: ~% ghci &gt;/dev/null 2&gt;/dev/null Prelude&gt; What fd is it actually printing on?
The [Haskell wiki](http://haskell.org/haskellwiki/Combinator) disagrees with both Wikipedia and you: &gt; Cobinator: a function or definition with no free variables.
"Service Unavailable" - Thank you, C#!
I hate to come into the conversation without much substantive stuff to say, but *holy smoke that's cool stuff.*
 sumsq g = U.sum (U.map (\\x -&gt; x * x) vector) where vector = R.randoms g 10000000 :: U.Vector Int is this the bug (?), that rewrite rules fail to fire for some expressions with ($) but work ok with explicit parens/application?
Interesting. I wonder what improvements are possible for Haskell programs which crunch (algebraic) data rather than numbers. And what about interaction between high-level optimisations made by GHC and low-level LLVM optimisations? Are they independent or can LLVM maybe even mimic some high-level optimisations (make them redundant and help simplify GHC's optimiser)? GA's look like a good tool to answer such questions.
What bug? I was just trying to write it in a clear manner -- calling out the vector being generated.
&gt; Darcs 2.4: the slowest in easy version control FTFY
the usual haskell for that would be: sumsq g = U.sum $ U.map (\ x -&gt; x * x) vector where vector = R.randoms g 10000000 :: U.Vector Int but, http://hackage.haskell.org/trac/ghc/wiki/RewriteRules at the bottom.
The $ doesn't make any different here (try it) -- I use it in other examples. GHC has no problem seeing through $.
Just last week, I was toying with porting several type classes to C# interfaces. In particular, I wanted to see what extra tricks were needed to get higher kinded type classes to work (since .NET doesn't support higher kinded parameters in generics). I got the use of `fmap` down to just one dynamic cast (which is runtime checked) in the [`Functor`](http://gist.github.com/311058) example. In the [`Read`](http://gist.github.com/312509) and [`Monad`](http://gist.github.com/312713) examples, I needed a separate dictionary to be passed around for the `return` function.
Has the "can not find comctl.dll" bug been fixed?
&gt; I got the use of `fmap` down to just one dynamic cast (which is runtime checked) in the `Functor` example. A shame C# [_still doesn't have_](http://connect.microsoft.com/VisualStudio/feedback/details/90909/need-covariant-return-types-in-c-all-net-langage) covariant return types...
How about a log rotation scheme? Every once in a while, begin a new log file and compress the old one.
Wow, this makes arbtt-stats usable again (still takes a lot of time, but at least it doesn't eat 1 GiB of memory) :-)
I'm wondering if some of GHC's C-- optimizations might interfere with what LLVM can do. Possibly even some core transformations, though that seems less likely.
The right way to do this is to build a custom GHCi using the GHC API. Not only would that eliminate the hack of piping everything through *sed*, it would also open up more possibilities. Like colorizing the Haskell code in error messages and debugger output. And even more - working that way, after playing around with it and getting some combination that works well, colorization could easily be incorporated into the standard GHCi with a flag to enable it.
I expect it'll go both ways. Some improvements will remove the need for heavy lifting by LLVM, others will expose bigger/better/more loops and straight line code segments to the LLVM / NCG layer giving it more opportunity to do it stuff. For example, Ben reported that the current backend doesn't generally create big enough basic blocks for the graph colouring register allocator to actually do anything interesting. I expect LLVM is being similarly limited at the moment. Higher level optimisations like evaluating complex constants would remove the need/opportunity to do the same at the LLVM layer.
http://hpaste.org/fastcgi/hpaste.fcgi/view?id=23137 ghc 6.12.1, main with ($) fails to fuse and there's still list of Ints.
Oh, please switch to the vector library, and use the -Odph flag. I'll update the list fusion library to build on top of vector at some point.
Is it just me or is this getting frighteningly close to the mythical _sufficiently smart compiler_?
The tty that stdin is connected to, I guess? echo 2+2 | ghci &gt;/dev/null 2&gt;/dev/null produces no output.
Does that still occur? Sounds like a proper weird error.
What I meant was that perhaps some optimizations at the C-- level may prevent certain LLVM optimizations from firing, resulting in a net loss. It would be interesting to see what happens if the genetic algorithm is allowed to turn on and off individual C-- optimizations as well as LLVM optimizations.
Unfortunately, it looks like this broke support for functions that use the semicolon list constructor in their argument patterns: foo (x:xs) = undefined
&gt; some optimizations at the C-- level may prevent certain LLVM optimizations I think that might be true. I think the form of the C-- we generate is also detrimental. The more we can look like C, the more optimizations will fire.
I didn't want vectors, I needed fusible lists. and isn't it about building it on top of Stream module from vector? maybe that module should be extracted to its own package?
Yes, it is about building it on top of the stream module from vector. There shouldn't be any harm switching to vectors (they stream in and out of lists). And you'll get far more robust optimizations.
no, thank YOU for being so awesome and hard working.
We need one of these for Perl and Python as well. Hell, why not one for every language? :P
Yes, still happens, and both on Linux at work and OS-X at home, so it looks like it is not some configuration issue on my end. [I took a screenshot](http://imgur.com/dQToC)
If you optimize enough, you don't even need to run the target program. 
Is it realistic to work on skipping the C-- step all together, and go straight from core to LLVM?
Don't worry, this is a common misunderstanding when you start using ghc rules. Since rules match on an actual named function then that function had better not be inlined away immediately or the rule will have nothing to match against. If you look at your code you'll see you're telling ghc to inline `myEnumFromTo` but you're also telling it to rewrite it via a rule. All you need to do is to delay the inlining to a later phase using the syntax: {-# INLINE [0] myEnumFromTo #-} With that change your code fuses perfectly using the old list stream fusion library with ghc-6.12 -O2. BTW: the gory details of phase ordering of inlining and rules is covered in the GHC user guide.
thanks. I've fixed that and a few other glitches i found with symbolic infix function (these are really tricky). Update is pushed and ready for download
very interesting article. I wish you used larger data sets though. I can't help but wonder how much time is spent initializing when we're talking about a 10% speedup from .315s to .296s ( &lt; .019s ) Acovea spends hours, any chance you could spend a couple extra minutes crunching larger data sets?
Awesome, thanks.
I'm not sure it is worthwhile to rip out C-- entirely -- it's a useful imperative representation, and has shown its worth: we generate C, asm and LLVM bc from it.
My next step is to use criterion to measure the results in a robust way -- resampling until the measurements are meaningful.
[Ulf Wiger's comment](http://pchiusano.blogspot.com/2010/01/actors-are-not-good-concurrency-model.html?showComment=1267445070356#c7220839714103549438) on that post really nails the point of Erlang processes (with reason; he's probably written more Erlang code than the whole community of reddit put together). Erlang actors are made to create loosely coupled components, isolation of code and generally aim for *decomposition* rather than composition. The author kind of missed the point if he was indeed targeting Erlang with this post. EDIT: to make myself clear, I don't disagree that actors (and side effects) make for bad composition in parallelism, but rather that in the case of Erlang, parallelism wasn't their objective to begin with.
Specialized decision-making bots are not AI. They are, however, nifty.
well, that's nice, but it still doesn't explain what's wrong with ($). I'll pay more attention to copying&amp;pasting code next time:)
You're welcome! It's a trivial point, and thanks for being gracious.
Congratulations! Thanks for your hard work representing Haskell at this event. It sounds like it was fun.
Thanks. And yeah, it was a lot of fun :-)
&gt; Distinguished elements 0, 1 in R So you are not considering trivial rings?
Congratulations and thanks for posting. In the end my bot was similar to yours, I'll try to post it this week if time permits. Got 6th place at the Haskell leaderboard and I blame it on my lousy board evaluation function ;-)
Well, considering that almost every top scorer used minimax, we can say it scratches traditional AI, but I fully agree with you.
I think "distinguished" refers to `0` and `1` being special compared to other ring elements, as opposed to `0` and `1` being distinct elements. In short: distinct ≠ distinguished.
Ah right, my bad.
Maybe I've missed something but I don't see any problem with ($). Do you have a specific example in mind?
Is there any programming utility to this discussion? I'm thinking not particularly, but I'm not sure. Please note I'm trying to check my understanding, not criticize. I have no problem using Haskell to explore theory. Demonstrations of how I'm wrong not merely welcome but desired.
I [described such a mostly safe mechanical translation](http://higherlogics.blogspot.com/2009/10/abstracting-over-type-constructors.html) awhile ago. It requires a runtime cast, but it's statically type safe.
Another thing to do, although a bit more advanced: take that system state using keys, and wrap it up in a state monad. Then, instead of threading the system state through each call, you can write helper functions on top like "modifyCrew :: (MonadState SystemState m, Monad m) =&gt; CrewKey -&gt; (Crew -&gt; Crew) -&gt; m ()" and soforth. then you can write a nice sequence of transformations in a do block and you've got something as convenient as global state, but much more well behaved, as it really just scopes over a given runState call. Throw that state behind an MVar (or even a TVar) to enforce effectively linear access, and then you can safely multithread.
You don't. You manage functions. You model your data in a way that works well for the functions you want to write. There are many ways to model the data that you are describing. The way you presented is one of them. Whether or not that's the best way depends on what functions you want to write. The main problem here seems to be: given that model, how do you type one of those things in at the GHCi prompt? The easiest answer is: all in one let statement.
Well, the discussion of initial algebras (small notes down near the bottom) is the basis of fusion techniques for removing intermediate datastructures. Just to pick one relevance (though, of course, fusion applies to many languages beyond Haskell).
See [here](http://pgraycode.wordpress.com/2010/01/25/a-general-network-module/) a short example on how to handle different graphs in Haskell. That module can be updated to something more efficient if the author would have much more feedback from the community
That's a good start, but you need more operations, in particular you likely need some sort of topological traversal. This can be done, but the code is a bit tricky.
Oh my. I certainly will have questions. EDIT: Ah, now I get to the bottom of the article and it says, "oh, basic category theory knowledge expected." :o)
&gt; M should contain only elements that are required to exist by i and the operations of the structure. Doesn't that make i a surjection (and thus a bijection) as well?
&gt; the free structure over a set is given by the monoid of lists of elements of that set, with concatenation as multiplication. Now, is this free structure unique, or is this just one of many possible free structures? Also, what is an example of a non-free structure of the set; is that a useful thing to think about?
&gt; For instance, the empty set is initial in the category of sets, and any one-element set is final. This doesn't seem intuitive to me. The Wikipedia article enlightens this a little more by saying "The empty set serves as the initial object in Set with empty functions as morphisms. Every singleton is a terminal object, with the functions mapping all elements of the source sets to the single target element as morphisms. There are thus no zero objects in Set." although this seems to imply that there are many morphisms to a single target element for the terminal object.
I intend to do them in a free weekend. Thing is, I am involved in several projects and my school homeworks will start to appear. But I'll manage it. Thanks for the suggestion.
Utterly tangential to the issue, but the ghci session is a good real-life example of why let-bindings are *not* imperative assignments. I had a hard time explaining this once. 
I had the same problem with a HappStack application I wrote [1], and I came up with about the same solution as yatima2975 in the first comment after your post on stackoverflow. My web app had users, posts, and comments, and these all referred to each other in a circular way. Updating state correctly was difficult if not impossible, so I broke the cycles in the data structure by giving each type of thing (whether a user, a post, or a comment) a numeric ID, and then each of these things could refer to each other by ID rather than a direct reference. This allowed me to modify things without also changing the things that point to it. At the top level, my webapp state had a handful of Data.Maps that resolve IDs to their actual object. (Some entities, like "user" had multiple maps; one that would resolve users by name, another that would resolve users by id.) Doing things this way does remove some degree of type safety; for instance, it's possible to have an active reference to an ID that has been deleted from it's associated map, or to accidentally re-use an ID. Another consideration that may be important for HappStack is that it's probably a bad idea to have any large object that's pointed to directly from more than one data structure. Normally, the memory would be shared, but I suspect you'd lose that sharing if HappStack is shut down and then restored from it's disk image. I expect this is a common problem, and a well-known solution, but this is the first time I've actually seen it discussed anywhere. [1] http://lists.bootlegether.net/
Indeed. The article claims to be "relatively informal". The only thing informal I can tell about it is that it lacks the diagrams, theorems, and definitions, i.e. the esoteric notational formality. But the not-so-esoteric notational formality, i.e. the use of "initial"/"terminal", even the mention of "injection", remains a formidable obstacle to the intended audience. G.-C. Rota: "Let the student work with unrigorous concepts that lead as quickly as possible to a half-baked understanding of the main results and their applications." Vast is the spectrum of unrigor: how unhelpful would a tutorial titled "a free functor is like a [insert fave texmex food]" be? 
My plan to deal with this problem was to use [accessors](http://hackage.haskell.org/package/data-accessor) to "point" to objects (from the global state). I would put the "pointers" into fields of the other objects. The implementation I imagined might be the similar to your and yatima2975's solution, but the interface would be more abstract because there would be no Key types (or at least no exposed Key types). A "key" for an object of type `Item` would simply be a value of type `Accessor GlobalState Item`. Once you go this far, depending on your application, it might be suitable to use non global accessors of type `Accessor LocalState Item` in some instances. That being said I never go around to writing out this idea in code so I have no idea if it works out, nor what details need to be filled in. In particular, I don't immediately see how to allow for creation and deletion of objects (though creation may not be as bad as deletion). Perhaps partial accessors of type `Accessor a (Maybe b)` are needed. Of course dangling obsolete references is a problem in any language. Anyhow, if your task doesn't involve creation and deletion of objects, this might work very well. Edit: One observation is that you cannot compare accessors for equality to determine if the "identity" of two objects are the same or not. I'm not sure if this is a good property or a bad property.
Given any set S, there is exactly one function from the empty set to S and exactly one function from S to a singleton set. Empty -&gt; S -&gt; Singleton. Empty is in front, hence initial; singleton is at the end, hence final. Why do we care? Because it makes precise the notion of "simplest" in the haskellwiki article.
Suppose {X} is the set you're trying to monoidify. A possible monoid is {X} with trivial multiplication. That monoid is obviously not the free monoid. 
The injection guarantees |S| &lt;= |M|, cardinality-wise. Why would the operations of M generally reverse the inequality? 
&gt; You don't. You manage functions. You model your data in a way that works well for the functions you want to write. So the real meaning behind data is not sua per se, but the arrows/morphisms on them? Go Yoneda! 
This is a common FP perspective, but it won't really be persuasive for anyone coming from an OOP background. OOP gives you more freedom to code to a generalized domain and then specialize its behavior as requirements become more apparent. I'm probably dumb and wrong, but I'd really find Haskell far more useful if it had more sugary ways of expressing the state of heterogeneous, relational domains. Haskell looks great when attacking lots of problems, but Haskell games and business apps all look pretty messy and *prematurely corrected*. I'd be thrilled to be referred to code that dispels this observation.
I actually didn't intend for "injection" to refer to the formal term. Just a way to turn elements of S into elements of M. A way to 'inject' elements of the former into the latter set. (Edit: indeed, for a general 'monoid over S', if people talk about such things, it might not be one-to-one.) The fact that it is (formally) an injection could presumably be gleaned from the idea that 'no extraneous equations should hold' for the free structure. So i x = i y should only hold in M if x = y holds in S. Anyhow, I'm not sure what to do. Understanding what free things are requires a fair amount of mathematical background. Explaining every term from set theory up would probably result in a (small) book, not an article.
It would be nice to have a simple introduction to category theory. I've searched for one and haven't found it yet.
I'm curious, why no mechanical parser generators ? It requires another tool but doesn't it offer potentially more static assurance and performance ?
&gt; I'd be thrilled to be referred to code that dispels this observation. I would call this an assertion, not an observation.
I'll add to this by saying use: Monad transformers to compose various monads. Then you can do things like separate out bits of global information/state like read only "varibles" ones which only be initialized inside a monad can be in a Reader monad transformer. Use type classes to work on parts and keep the code generic without referring to a particular monad transformer stack. By the way you need flexible contexts haskell extension for your example to work I believe.
Dear lord that looks horrible... Doesn't anyone read Tufte anymore?
&gt; The only equational laws that should hold for the generated structure are those that are required to hold by the equational laws for the structure. I don't see how one is supposed to prove that.
I must admit I had to use it once and it was painful indeed.
IMO, the use of a mechanical code generator means that our language has failed the promise of code reuse and abstraction. We shouldn't have to resort to such a blunt tool.
I'm not really sure how to _state_ it, except in English like that. Except, of course, via a universal property in category theory, which is the point of introducing it. It turns out the (an) appropriate way to make that condition rigorous is to say that all objects of that kind have a unique factorization through the free object, and then, it's probably less difficult to imagine proving that such a unique factorization exists (if we're considering, say, proving that lists are a free monoid).
In finance, time is money, so date handling routines are usually given careful thought. I'm wondering if Barclays Quantitative Analytics group, who are known to include haskellers, would be willing to weigh in on the topic. 
I've always thought an API that is a superset of something similar to this would be nice: -- | Time without any reference point data Duration a -- | Duration since some specific point in UTC time (in other words, absolute time) data Time a -- Ways to construct a duration seconds :: a -&gt; Duration a minutes :: a -&gt; Duration a hours :: a -&gt; Duration a days :: a -&gt; Duration a years :: a -&gt; Duration a -- Ways to construct a time now :: IO (Time a) -- Relationship between Duration and Time instance VectorSpace a =&gt; VectorSpace (Duration a) instance AffineSpace a =&gt; AffineSpace (Time a) where type Diff (Time a) = Duration a -- Various ways to format Times (including such things as time zones, etc.) foo :: Num a =&gt; Time a -&gt; String bar :: Num a =&gt; Time a -&gt; String baz :: Num a =&gt; Time a -&gt; String The `VectorSpace` and `AffineSpace` type classes are in the vector-space package. With the above API, your `addSeconds` function would be something like this: addSeconds :: AffineSpace a =&gt; a -&gt; Time a -&gt; Time a addSeconds s t = t .+^ seconds s
&gt; A way to 'inject' elements of the former into the latter set. Isn't "embed" or "embedding" usually used to describe what you just wrote? Embeddings are usually but not always injective, since the idea is to perceive the original elements in the context of an enriched ambient space.
To avoid duplication of effort, I delegate the graphing to gnuplot. So either I need to better configure gnuplot when plotting, or effort needs to be put into gnuplot instead. Suggestions for better plotting options are welcome.
Ah, yeah, that's probably a better word. Thanks.
That's basically what .NET does: DateTime is a point in time, TimeSpan is a range.
Yes, you're right. I changed the end of the sentence to read "structures of that type," which is hopefully clearer. I guess I should go back through and see if I use the words consistently. And you're right, of course. The free group over a set S is `List (S + S)`, quotiented such that the same element with opposite tags annihilate one another. So for instance: [0 x, 0 y, 1 y, 1 z] == [0 x, 1 z] And it is also a monoid over S, but it is not a free monoid over S.
To be fair, there's no language in the world with a good datetime library. Java's, Python's, C++'s, Erlang's, they're all terrible
for the most part?
We have difftimes in haskell too. The semantics are just somewhat bizarre and the range of operations is confusing/limited. Edit: which reminds me, the read/show qualities of all the basic time types (and difftime in particular) are nonuniform and borked.
But this is Haskell! We make tricky things simple and insanely hard things possible :-)
http://hackage.haskell.org/package/Chart
Correct me if I'm wrong, but isn't a http-parser a full HTTP parser, with chunked support, etc., and the Attoparsec parser just deals with the HTTP header? The difference is not moot because if you need to recognize Content-Transfer-Encoding, Content-Length etc., you introduce additional data-dependent branches (or increase the size of the parser tables), which will affect performance.
Part of the problem is time is really messy in the real world. There are a tons of formats most of which are ambiguous. Toss timezone weirdness. Then toss in weirdness like leap seconds. Add to that things like "business" days. It's just really really messy. This is part of why in the Perl world there are a million and one date time libs.
I notice you exclude months, and I agree, because it is not well defined. (months 3 = 28 + 31 + 30 or numerous other answers) The same actually holds for years. (7 years can be 7\*365, or 6\*365+366, or 5\*365+2\*366) Shame to have to give up on years though.
But we like DSLs :) However, I agree -- an EDSL is easier since you don't have to learn a new language.
Since when has Haskell ever been bothered by this "real world" you speak of? ;)
That's right. It's still there, but it's extremely rare to hit that corner case in practice. At least that's what I took away from the many discussions the darcs developers had at the official mailing list.
True, the useful notion of time is always application dependent. Bankers and astronomers won't care about the same thing. One might start out assuming that astronomers are simply more precise, but that simply wouldn't be right. Maybe best to port a few of the better Perl libraries. 
You're raising an important point here -- a usable API needs to have addYears and addMonths functions, even if years and months don't have determined durations. That messiness is precisely what a good API should help us to deal with.
I wondered what Attoparsec was, so I [googled](http://www.google.com/search?q=attoparsec) it. Apparently, it's about 3cm.
Years are well defined, at least as accurately as we can measure, (365.24219878 days), just not in common usage (365 or 365.25 days). I agree with you on principle though, because sticking to just one or the other definition is not going to be technically or intuitively correct in all cases. The problem, of course, is that we actually have two absolute time keeping systems (one based on rotation of the Earth, and one based on the Earth's orbit around the sun) which we try to track with flawed calendar systems. Perhaps what we actually need is an API that keeps these units distinct in the types with explicit conversions. The Gregorian calendar stuff is going to be inherently complex, perhaps, but if we can keep it separate from days and years then perhaps we can at least keep those parts simple. I will probably comment here again with another API proposal that is brewing in my head to address this, but I shouldn't take any more time from work for it.
This wikipedia page gives the basic sorts of date arithmetic that banks concern themselves with: http://en.wikipedia.org/wiki/Day_count_convention One test of a good API should be that it lets this stuff be done with minimal fuss -- and in fact, the plain date component of Haskell's datetime libs is pretty much up to snuff in this regard. Mainly that's because toGregorian and diffDays do what they're supposed to. EDIT: the tricky bit comes when dealing with holidays, which are different from context to context and country to country. Holiday aware date arithmetic is too much to ask of a core lib, but again, a good core lib should let such a wrapper be written easily.
I disagree. Haskell's date time library is one of the best I've seen and it is **needfully** painful because proper handling of time is messy. The other "simple" libraries out there all break down because they try to make things simpler than they really are (example, see the old Windows daylights savings fiasco). That being said, your code can be simplified to addSeconds s = utcToLocalTime utc . addUTCTime (realToFrac s) . localTimeToUTC utc And that being said, yes, perhaps the library could contain a few more helper functions or possibly some adhoc polymorphism. **Edit:** You could remove the call to `realToFrac` and push the burden to the caller (and rename the function `addLocalTime`). The code `addLocalTime 1` will continue to work because litereals such as `1` can be coerced to `NominalDiffTime` Prelude Data.Time&gt; 1 :: NominalDiffTime 1s **Edit2:** Damn it, I just realized I fell for the old "this library sucks, see you can't even write this code nicely" trick for eliciting a response to a programming language question. Next time I will try to be on guard for this and call it out.
Oh dear -- adhoc polymorphism. the point is to make it accessible! Don't get me wrong, I appreciate the care and scrupulousness in what the library actually does -- I'm not for scrapping the good work in it. But as the example shows, adding seconds to a date (thanks for the cleanup, by the way) should not take four function calls, and it should be obvious what the right way to go about it is.
You left out the entire concept of local time which is at the heart of the issue raised by the original post. You also don't handle leap seconds and leap years. And finally all your concepts are a subset of the existing Data.Time library: Time is UTCTime and Duration is DiffTime (or perhaps it is NominalDiffTime; it is hard to tell because your interface doesn't handle leap seconds and thus is conflating the two types).
&gt; Oh dear -- adhoc polymorphism. the point is to make it accessible! Question: does overloading `addSeconds` to work on `UTCTime`, `LocalTime`, and `ZonedTime` make the library more accessible or less accessible? I really want to know, because I have similar issues with my Colour library.
&gt; Error establishing a database connection `:-(`
&gt; You left out the entire concept of local time which is at the heart of the issue raised by the original post. No, I didn't. Local time is a formatting issue. &gt; You also don't handle leap seconds and leap years. The Gregorian calendar is another beast. The superset of funcationality including my proposed API would include way to convert to and from it. &gt; And finally all your concepts are a subset of the existing Data.Time library By design. I didn't have the time to address all corner cases. It's just a start. My main point was that we can work with absolute and relative time without having to constantly convert back and forth if we use the proper abstractions.
&gt; &gt; You left out the entire concept of local time which is at the heart of the issue raised by the original post. &gt; No, I didn't. Local time is a formatting issue. Okay, but had the orginal poster used UTCTime instead of LocalTime then he would have simply written `addUTCTime (realToFrac s) t` and wouldn't be complaining. No need to reinvent the entire library.
Hmm... a few more overloaded functions might do the trick -- my frustration could just be in running up against the limitedness of LocalTime as opposed to UTCTime. Or even having the ability to add NominalDiffTime directly to LocalTime (at which point, one realToFrac call hardly feels like a burden). By the way, I really wasn't trying to troll to elicit code advice -- nicer code is nicer and all, but I feel like the steep learning curve of the Data.Time library has been a consistent source of frustration to myself and plenty of other folks over the years, and as a community we need to hash out why and how to fix this.
http://74.125.95.132/search?q=cache:www.serpentine.com/blog/2010/03/03/whats-in-a-parser-attoparsec-rewired-2/
&gt; I feel like the steep learning curve of the Data.Time library has been a consistent source of frustration to myself and plenty of other folks over the years, and as a community we need to hash out why and how to fix this. This I agree with.
You should have [Wolfram Alpha'd](http://www.wolframalpha.com/input/?i=1+attoparsec) it for extra cool points ;) Apparently, it's "0.61 x length of a AA battery (~~ 51 mm )". Thank you Wolfram!
Aww! Time to go hug a warm, fuzzy monad ;)
&gt; I must admit I had to use it once How shameful.
We have a third time keeping system based on a uniform time scale (the earth doesn't spin at a uniform rate, so the length of time it takes to do (1/86 400) of a full solar referenced rotation (or 1/86 164.0905 of a sidereal rotation) varies. This difference between the two time keeping systems you noted is what causes leap days, This difference I'm noting is what causes leap seconds.
I was not aware of this. I mean, I knew the Earth was on an irregular rotation, but I didn't realize we didn't just average it out. Thanks for clearing that up.
The Royal Astronomical Society of Canada publishes an annual Observer's Handbook. This book has a very nice chapter explaining the intricacies of time measurement. I haven't even started in on the various choices of relativistic reference frames to choose from (do we measure time at sea level, at the center of the earth, at the center of the solar system?) since you'd probably consider that going too far. :D
I actually wrote one of those using attoparsec. It's fast enough that connection overhead absolutely dwarfs it - Bryan was getting ~100000 parses per second and I bet I'm not far off of that.
that's a very pretty theme you've got there!
It's Spring!
I guess just friendly image for people using the 'haskell' tag?
I try darcs every now and then but I started out on Mercurial and it hasn't won me yet, though I do admire the theory and don't really care about the performance.
Awful lot of .NET on that screen for it to be the Haskell tag.
Stack Overflow fills unsold ad inventory by "featuring useful open-source programming projects". This ad [was created](http://meta.stackoverflow.com/questions/31913/open-source-advertising-sidebar-1h-2010/38029#38029) by [gbacon](http://stackoverflow.com/users/123109/gbacon) 
That's pretty amazing. What makes the Attoparsec library so much faster?
http://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/
I seem to recall that the one that finally made me grok monads was [You could have invented Monads...](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) 
I remade the list data type, functor, applicative functor, monoid, and monad from scratch, and then I understood most of the concepts a lot more. And then I wrote it out as a tutorial for my brother, [here it is](http://irkenkitties.com/tutorial.lhs)
I didn't even know how to *use* monads for a long time (edit: except IO). I always felt there are not enough "Hello World" code snippets to introduce a specific monad. Eventually i really wanted to know what the fuzz is all about and made a "Hello World" program with the easiest monad i could think of (the Writer monad). After using a monad, understanding was straight forward, using the link tirpen posted here.
http://ertes.de/articles/monads.html seems pretty good.
Well, you could try a [recent post I wrote here on reddit](http://www.reddit.com/r/programming/comments/b20oq/a_monad_nontutorial_or_why_you_shouldnt_ask_what/c0km0cr). I think it's pretty accessible to imperative programmers, particularly ones who are familiar with assembly and/or virtual machines. See also: http://higherlogics.blogspot.com/2007/10/on-importance-of-purity.html
The key to gett'in Monads is to concentrate on the type signatures of the required operations, don't look at the implementation. Focus and notice the fact that your dealing with type constructors. I suggest doing this first with Functors. Read [this](http://learnyouahaskell.com/making-our-own-types-and-typeclasses#the-functor-typeclass) and [this](http://learnyouahaskell.com/functors-applicative-functors-and-monoids). Then go back to monads.
&gt;&gt; You left out the entire concept of local time which is at the heart of the issue raised by the original post. &gt; No, I didn't. Local time is a formatting issue. Yes, you did. Local time is a non-continuous function of time, due to daylight savings time. Handling local time correctly is not so simple.
&gt; But as the example shows, adding seconds to a date (thanks for the cleanup, by the way) should not take four function calls, and it should be obvious what the right way to go about it is. I disagree. It is not clear what you mean by "adding seconds" to a local time. What happens if that interval of seconds happens to span one or more changes to or from daylight savings time? There are several different things to do. Your specification of those functions chooses one. I think it is a clear and elegant way to express your chooice. It accurately reflects the level of complexity of what you are trying to do.
I used to think that, too. I tried using one of the many libraries that claims to "simplify" Data.Time. I soon learned what a mistake that was. Trust Data.Time, it is extremely well written. If something looks harder than you think it should be, don't assume it's the fault of the library. Instead, think a little more carefully about what you are trying to do. Guaranteed - you didn't specify your task correctly, and Data.Time is trying to show you the point that you missed. As other posters have repeatedly said: time is not simple.
No, I didn't. There exists a function from absolute time to local time, even in the presence of daylight savings. If there was no such function then a particular time zone would occasionally have multiple times or no time at all, which would mean our time system has even more serious flaws than I already thought. The function isn't continuous, sure, but that has nothing to do with this.
Hey, that looks great, I'll make sure to read it!!
I loved that one. I found it really helpful to work the exercises without peeking ahead.
why do you mind?
You're right that there are various choices, and you're right that once you get a handle on what's going on, you can (mainly) specify these choices with simple function composition. But there's got to be a better, cleaner way to expose and specify common functionality and sets of choices so that dealing with the datetime libs doesn't become the most painful part of otherwise straightforward programming projects. Even barring that, there's an open market, at the least, for a well written guide to the range of ways to go about things in the time libs, and the standard/simple ways to do basic things. I didn't realize, for example, that I could add a NominalDiffTime directly to a UTCTime, and so went through the extra step of POSIX. Sure, shame on me. But my point is that what to do in any given case is far from obvious, and even though individual functions are well documented, there's no overall guide to where to start looking. I'm convinced that we can do better. EDIT: I think LocalTime is particularly the culprit here, of course, because it's the messiest for any number of reasons. But people mainly want to be able to think and enter data in LocalTime. So, for example, its weird to me that LocalTime is a day and a time of day, rather than directly a UTCTime and a locality specification, along with appropriate projections.
Monads are not meant to be understood. They are meant to be used. Just learn individual monads as needed and the abstract stuff will gradually sink in. But like all Haskellers, I will readily tell you what I think sucks about most other peoples' explanations. ;) I am not a fan of the choice of functions in the Monad type class. Here's what makes it nicer for me: class Functor f where fmap :: (a -&gt; b) -&gt; (f a -&gt; f b) class Functor f =&gt; Pointed f where point :: a -&gt; f a class Pointed f =&gt; Applicative f where (&lt;*&gt;) :: f (a -&gt; b) -&gt; (f a -&gt; f b) class Applicative m =&gt; Monad m where (=&lt;&lt;) :: (a -&gt; f b) -&gt; (f a -&gt; f b) Let me line some of these type signatures up, for clarity (contexts left out): fmap :: (a -&gt; b) -&gt; (f a -&gt; f b) (&lt;*&gt;) :: f (a -&gt; b) -&gt; (f a -&gt; f b) (=&lt;&lt;) :: (a -&gt; f b) -&gt; (f a -&gt; f b) These all just modify functions! Functor, Applicative, and Monad do nothing but give you a way to lift various kinds of functions to work in some functor. If anybody says that monads are burritos, they are just making it complicated.
A few years ago, when I realized that a rather complicated set of compositions could be reduced simply to `mconcat` using the Reader instance. That was when I really started to get that asking what your code *means* can be an advance over asking what it *does*.
java version works different than haskell one (it's line by line). but java version still has much nicer implementation than haskell - all those unsafeInterleaveIOs...
w00t!
Click the pretty picture and note where you land!
I only recently really began to grasp what a Functor "is" in the kind of Zen, "what it means" sense. Particularly, it was through studying Combinatorial Species that it really dawned on me that Functors _are_ structures on types, even in a more powerful/general sense than the obvious data structure notion. That at the core -- data structures are kind of approximations of functors, and functors approximations of data structures. It may seem trivial, but I liken it to knowing how to use monads and knowing how monads work. It's a imperceptible step which changes "being able to use" into "being able to create" into "being able to generalize/extend/change" 
Sure monads are supposed to be understood! At first you just use them, then slowly you begin to understand that monads are just things which build and tear down structures on a particular type, then you understand what that means. Monads are (actually) not so hard, it's just the language and the "fluff" get in the way of really understanding them.
I think my greatest moment was when ghc asked for -XUndecidableInstances (yet again), and I could prove in the back of my head that enabling it would be safe. It made me realize how much magic is going on in the background to keep the typechecker strongly normalizing.
If I wasn't lazy, I'd do 99 Questions in the type system.
Well done!
Care to elaborate for people who don't know?
 fix
This should be made into sticky. Can't upvote hard enough.
Sometimes local time isn't just a formatting issue. For example for something I'm working on at the moment, a report on customer behaviour grouped by 'hour of the day' is required, where the hour is in local time. Because customer behaviour patterns are expected to correlate to hours in local time.
Oh, good example! I guess you could have a different representation for local times. This is getting very near to what the existing library offers already. Perhaps the core of my proposal can remain, however: I just want some genericity and nice operators for it.
You could put it in the thunk until you're ready.
Thank you!
It's hard to imagine a [nicer implementation](http://haskell.org/ghc/docs/latest/html/libraries/base-4.2.0.0/src/System-IO.html#interact). Without `do` notation, it's interact f = putStr . f =&lt;&lt; getContents
I really would have liked to see this post about... oh... twelve hours ago. Galois is like twenty minutes from where I live.
I tend not to think on the level of thunks, usually preferring to stick to a mental model of expression reduction for pure code. However, for IO, we can mimic quite directly the implementation of thunks in GHC, which are pointers to *code* that update themselves after being entered the first time to point at a smaller piece of code which returns the result directly. We want to write a function once :: IO a -&gt; IO (IO a) so that the result of running once x is an IO action which will execute x only the first time it's run, obtaining a result v, and will simply be equivalent to return v thereafter. What we'll do is to create an IORef r for holding the next action to take when the produced action is run. This way, we can initialise r to the action which will: 1) run x getting the result v 2) rewrite r to return v 3) return v itself import Data.IORef import Control.Monad once x = do r &lt;- newIORef undefined writeIORef r $ do v &lt;- x writeIORef r (return v) return v return . join . readIORef $ r The only ugly part here is that we had to initialise r first to undefined, because we needed to use r in what we really wanted to initialise it to. There's a perhaps conceptually prettier way to do this called mfix (reddit's formatting code is doing something weird, so forgive the spacing around the lambda): import Data.IORef import Control.Monad import Control.Monad.Fix once x = return . join . readIORef &lt;=&lt; mfix $ \ r -&gt; newIORef $ do v &lt;- x writeIORef r (return v) return v 
Somebody on #haskell a few days ago suggested the following elegant solution to this: once :: IO a -&gt; IO (IO a) once = liftM return
I should add that there is a difference between this version and what can be done with `IORef`/`MVar`, and that difference may be exactly what you're after. In particular, `once` will execute the IO immediately, though it looks (just by reading) like `iothunk` will execute the IO some time later. So maybe you should ignore me: it's possible I missed the point altogether.
For me it was `mfix`. God, it was like learning recursion all over again.
Has anyone found a tutorial or book that they have found particularly helpful in learning either Darcs or version control? I've been looking but haven't found anything good yet.
Perhaps it's Stack Overflow's version of Clippy: "You appear to be using an awful lot of .NET. Would you like assistance with writing sane code? If so, try Haskell!"
Is it better than uu-parsinglib?
Thanks, did you read any of the tutorial? Do you find it helpful?
Wrong Haskell.
I believe the "Galois Developer Symposi[a]" are the super sekrit talks for Galois employees. The "Galois Tech Talks" are the ones that are open to the public.
You spotted it :-) Sometimes the content is so good though, it just wants to be on the interwebs.
This is sort of what I did for my master's thesis, but for Coq, not for Haskell, and I only did it by hand, not by implementing any language extension or preprocessor.
So you're saying that it is very difficult for a Haskell programmer to get dates, right?
My version of the idea is here: http://hackage.haskell.org/packages/archive/xhb/0.3.2009.6.28/doc/html/src/Graphics-XHB-Shared.html#Receipt The idea is that the 'thunk' is supplied in two pieces, the second piece being an MVar which another thread uses to supply the value. Well, the other thread supplies a ByteString and the creator of the 'thunk' supplies a function (ByteString -&gt; a).
Though this explaining is quite useful to get you using them it isn't right at all. Monads don't really have anything to do with order of evaluation. When I realized **this** is the moment when I **really** started to be able to use them.
I actually used the Peano numeral example in the notes at the bottom, about initiality, since datatypes are initial algebras, and Peano is about the simplest interesting example. In fact, one probably can characterize it as the free such algebra over the empty set/type. And this even seems to work in Haskell, where the 'empty' type is not actually empty, because in Haskell, Peano actually contains: _|_, Successor _|_, Successor (Successor _|_) ... which is what you get via applying the relevant algebra operations to `i _|_`. And of course, it's not surprising that "free(/initial) algebras" are the special case "free algebras over an empty collection". Of course, the above example doesn't parlay well into talking about free monads (which was what originally motivated the whole thing).
this should be extended and added to haskell platform. it should generate a project with additional bells, like testsuite setup (test target for cabal, dependencies, perhaps some scripts like DocTest to search source files for test cases/properties)
The successor to mkcabal was recently added into the cabal-install tool (I think). Brent Yorgey and someone whose name I can't recall were working on it at the Edinburgh Hackathon.
Be sure to read [this](http://eprints.nottingham.ac.uk/archive/00000223/01/pearl.pdf) short paper; it makes beautiful use of monads (which is probably what you need after reading tons of abstract explanations over and over), and is what helped me develop some intuition for them. And if youv'e ever had the pleasure of writing a parser in an imperative language, you'll appreciate it even more.
You could always use `unsafeInterleaveIO` and ACTUALLY put it in a thunk. I usually don't like to do this though because running I/O outside of IO puts you in "leaking asynchronous exceptions" territory.
Err... no. I just don't know where to start. But you might want to work through (or start to work through) [Software Foundations](http://www.seas.upenn.edu/~cis500/current/sf/html/toc.html), which tells you much about what's necessary to prove something total (that is, for finite input it takes finite time), followed by e.g. [Type checking with open type functions](http://www.cse.unsw.edu.au/~chak/papers/SPCS08.html), where the reasons of disallowing nested type function application by default are explained (grokking the gist is quite sufficient, no need to scrutinize every greek character, which I neither did nor am qualified to do). As example, GHC accepts this: data Zero = Z data Succ n = S n type family Add n m type instance Add Zero m = m type instance Add (Succ n) m = Succ (Add n m) as well as data TT data FF type family Even n type instance Even Zero = TT type instance Even (Succ n) = (Odd n) type family Odd n type instance Odd Zero = FF type instance Odd (Succ n) = (Even n) but not type family Mul n m type instance Mul n Zero = Zero type instance Mul n (Succ m) = n `Add` (n `Mul` m) because a) the implementation disallows nested applications to ensure to be both consistent, terminating and type interfering all without you having to give proofs for any of it, and b) isn't smart enough to figure out that that every application is structurally decreasing, and therefore, by [induction](http://www.seas.upenn.edu/~cis500/current/sf/html/Basics.html#lab29) sooner or later hits the bottom cases of both Mul and Add, and therefore either terminates or errors out with the type-family equivalent of a pattern match failure (say, Add (Succ Apple) Zero). I believe closed type functions and a more complete kind system (to disallow (Succ Apple), note that the type level is vastly untyped) could fix that, but even without that ghc does an awesome job figuring out that the first three examples terminate while consistently disallowing _every_ non-terminating type function.
Ya, I read it. And from there I finally understand what that funky "fmap.fmap" thingie mean. Also, this tutorial is very well written. In about 700 lines it explains a great deal of stuff. Thanks for sharing.
Cool. Am I right in thinking this is "like Hack, but more scalable ?"
Essentially. There are other minor differences, like Hack providing a cache record in the Env and WAI not, but I don't consider those major features. It really boils down to "Hack needs lazy I/O for space efficiency, WAI doesn't."
is it ok if I post myself, or do I have to sign a contract with dons?
Nothing wrong with original content, especially with so much of it. Very nice job.
Now I just want a stable way to use it for the iPhone.
What boilerplate does it require when using ZipList and Applicative-style? The getZipList/ZipList wrap/unwrap?
idiom brackets or explicit use of (&lt;*&gt;) operators. edit: fibs = 0 : 1 : getZipList (($) &lt;$&gt; pure (+) &lt;*&gt; ZipList fibs &lt;*&gt; ZipList (tail fibs)) but: fibs = 0 : 1 : zipWithN (+) fibs (tail fibs) 50% of characters in ziplist version is boilerplate.
My type-checker in my head says the type of liftM return should be: Monad m =&gt; m a -&gt; m (m a) Or specialized to: IO a -&gt; IO (IO a) 
Yes, of course. Fixed, thanks.
using ZipList over and over again can be very frustrating... but using 'fmap' and 'zap' as described by Paczesiowa it is in typical applicative style very much like ZipList (remember &lt;$&gt; is fmap). Or better use symbolic operators instead of `fmap` and `zap`: import Control.Applicative (&lt;$&gt;) infixl 1 &gt;&lt; (&gt;&lt;) :: [a -&gt; b] -&gt; [a] -&gt; [b] (&gt;&lt;) = zipWith ($) test = (,,) &lt;$&gt; [1,2,3] &gt;&lt; "hello" &gt;&lt; "world" the &gt;&lt; operator is the 'zap' function. But still having a generalized zipWithN function is so much cooler
Thanks for posting that! Good read; great for understanding things a bit more!
Never needed them.
As I said there, I find my version is extremely clean, and quite a bit less involved: http://community.haskell.org/~wren/wren-extras/src/Data/List/ZipWithN.hs There's no boilerplate, and the only caveat is that the final type needs to be annotated if it can't be inferred from context (which it usually can be).
This does a straight string substitution; even in this scenario that's a pretty dangerous approach to take. At a bare minimum some sort of support for various kinds of escaping or processing-before-insert would be necessary to make this truly safely useful.
That's an odd comment.
It's a bit backwards, I prefer the version, where knowing the zipping function drives the whole process and substitutes other types, because it is usually used in such a way. But in every case where your version needs type annotation (which IS boilerplate!), my version not only doesn't need it, it also helps disambiguate it, should the caller need it. As for "less involved" argument: in that hpaste link, there's much "less involved" version. It's hard to understand (just like yours) and thus ugly. The "involvedness" of my current code is the best part - noticing that it's just a curried fold over the list of arguments makes the code much more easier to understand, if we assume the other pieces are given.
To be fair, the canonical version would be: fibs = 0 : 1 : getZipList ((+) &lt;$&gt; ZipList fibs &lt;*&gt; ZipList (tail fibs)) Or in Conal's style: fibs = 0 : 1 : (inZipList2 . liftA2) (+) fibs (tail fibs) Indeed, it would be nicer to just write &lt;$&gt; and &lt;*&gt; for ZipLists as operators: fibs = 0 : 1 : zW (+) &lt;&gt; fibs &lt;&gt; tail fibs vs: fibs = 0 : 1 : zipWithN (+) fibs (tail fibs) May not be that bad?
He's scared of the Haskell pimp.
If you want to delay, you can use: once = liftM return . unsafeInterleaveIO
I love you.
Ditto. It's perfection.
Why does hack need it it and WAI not?
Perhaps I will format it into a proper web page and clean it up a bit, if people are finding it helpful.
Hack specifies that the request and response bodies are lazy bytestrings. WAI uses a "Source" for the request body and an enumerator for the response body; both of these are amenable to chunked reading with lazy I/O. As far as a Source, the final design was given to me by Felipe Lessa in a comment on [a blog post](http://www.snoyman.com/blog/entry/simpler-is-better/). The definition is: newtype Source = Source (IO (Maybe (B.ByteString, Source))) Essentially, "give me a chunk of data and a function to get the next chunk of data, if available." Enumerators have become a hot topic recently, and you've probably seen at least *some* definition of them, but WAI uses: newtype Enumerator = Enumerator { runEnumerator :: forall a. (a -&gt; B.ByteString -&gt; IO (Either a a)) -&gt; a -&gt; IO (Either a a) }
You'll have to admit that you sound like the mafia head of this board... "Hey don, can I post this on /r/haskell? It'll be good for the Family, I swear!" "- I need a vector library! - don's gonna make you an offer you can't refuse..."
You have my blessing
I realized that you *really* can get any dynamic/metaprogramming behavior you want in a dependently typed program when I wrote a function to calculate induction principles for the natural numbers. E.g, the usual induction goes like P 0 /\ (forall n . P n -&gt; P (n+1)) -&gt; (forall n, P n) if you want to prove something about fibbonacci numbers you might like P 0 /\ P 1 /\ (forall n . P n /\ P (n+1) -&gt; P (n+2)) -&gt; (forall n, P n) and so on. Well, n_step_induction 1 :: P 0 /\ (forall n . P n -&gt; P (n+1)) -&gt; (forall n, P n) n_step_induction 2 :: P 0 /\ P 1 /\ (forall n . P n /\ P (n+1) -&gt; P (n+2)) -&gt; (forall n, P n) and so on. (One important trick was writing a function to compute what the result type should be).
I was thinking that too, but buried in the details - he's also plugging in remote Amazon EC2 nodes. &gt; one process per core on the local machine, and one process on each of a range of instances running on Amazon EC2
I'd quite like to actually do this properly -- the program the article describes is deliberately as simple as possible, and I'm planning some follow ups that improve on various aspects. For me, the interesting part is not in getting your code to run on an EC2 instance in the first place, but getting the maximum computation done efficiently.
I use Leksah and I like it. Admittedly, without the "Getting Started" pdf no one would ever know how to get the damned thing to work, lol. Haven't tried the new version, but they could certainly benefit from a more intuitive way to start coding with it.
Sounds like it needs a tutorial.
RWH took me from “How can one possibly do anything with functional programming?” to off and running with Haskell in a matter of days. It's a beautiful book, at what is, to me, just the right level.
Congrats! We thought Turbinado will do better, but now there is, at least, some alternative to Happs. :-D 
I'm trying to get this installed on the latest Ubuntu. I've had an older version installed at one point before switching to EclipseFP, but I'd like to see how Leksah is doing. Trying to cabal install, I'm getting an error that I need an updated gtksourceview2. I presume this is supplied by one of the gtk2hs packages, but which one? Hackage shows many.
amtrak is piece of metal junk
Last week I studied what a comonoid was. It is really stupid (in a Cartesian category). The short of it was that a comonoid on `X` is an operation `op :: X -&gt; (X,X)` such that `fst . op = id` and `snd . op = id`; there is only one. Edit: there is also a `cozero :: X -&gt; ()` which as you can see is pretty irrelevant.
You see, there are these arrows... and when you flip them around you get the dual... ah, I'll let someone else explain it :-)
stream fusion?
Judging solely from the screen shots, it's starting to look like a real application. Way to go. I like the tabs on the side idea; hope it works out well. Also does the client/server aspect mean I could eventually use Emacs as the client instead of the GUI front end?
Try searching for Awodey's book. I haven't started reading it but someone recommended it for me.
No, but you may use Yi as editor component, which is a more powerful text editor then gtksourcview2, which is currently used. But this needs work and hope we find volunteer for this.
Actually there's more to it than that. There are multiple different kinds of "arrows" and there's only one of them that gets flipped. And there's more to duality than just reversing the morphisms: why are conjunction and disjunction dual? *Edit:* For example, if we munge up the different kinds of "arrows" then how can we see that `curry : (A*B-&gt;C) -&gt; (A-&gt;(B-&gt;C))` and `cocurry : (C-&gt;A+B) -&gt; ((C-&gt;B)-&gt;A)` are dual? How come that top-level arrow doesn't switch as well, yielding `uncocurry : ((C-&gt;B)-&gt;A) -&gt; (C-&gt;A+B)` ? Well, it's because the actual forms of those definitions are defined in the metalogic not in the object logic: f : A*B -&gt; C -------------- f^* : A -&gt; C^B and f : C -&gt; A+B -------------- f_* : C_B -&gt; A Of course the top-level arrow isn't always the deduction bar in the metalogic. For example, Filinski has the natural transformations `phi : A*(B_C) -&gt; (A*B)_C` and `theta : (A+B)^C -&gt; A+(B^C)` which do reverse the top-level arrow, since it's actually a morphism (or collection thereof). *Edit 2:* Bonus points: why is injectivity (as opposed to, say, totality) the dual of surjectivity? ;)
Andrzej Filinski has some good works for getting some of it under your belt. In particular [his master's thesis](http://www.diku.dk/hjemmesider/ansatte/andrzej/papers/DCaCD.ps.gz) or the paywalled [short version](http://www.springerlink.com/content/m2105282ru426654/) cover the basics of co/exponential and co/product dualities in a straightforward manner. In addition to the straightforward explanation he also provides a categorical treatment, which should help you get into the ideas of category theory where much of this is dealt with. *Edit:* In a similar vein, the classic [Haskell is Not Not ML](http://research.microsoft.com/en-us/um/people/simonpj/papers/not-not-ml/not-not-ml.pdf) also explores negation and continuations, which are related to the issue of duality for CCCs.
Please do!
Indeed, the 'init' command is now in cabal-install 1.8. The person whose name you can't recall was Benedikt Huber.
:-( that's what I get for posting behind don's back...
Cheers! :-)
did I miss something?
You didn't pay your protection money and he burned down your blog post :-( We all enjoyed it though! :-)
I totally missed something, didn't I...
Seems pretty nice from the examples. I'll be checking it out.
It also helps to think about functions as values themselves. A point free definition is just a value. It clears away some bad thinking if you look at it this way and makes it easier to see the fact that a function is nothing special.
I don't like violence, ithika. I'm a businessman. Blood is a big expense.
May I suggest that the somewhat grumpy/critical tone of this article isn't really necessary and doesn't send the best message to Haskell newbies. If newbies want to be snarkily beaten over the head with their own stupidity in between useless flamewars, there's already comp.lang.lisp for that, and that's not a model to emulate.
As a Haskell newbie, I find that I don't even need other people to "be snarkily beaten over the head with my own stupidity" - I just write code and wait for the compiler to tell me I'm stupid for even asking it to do that. :(
This would be a lot more successful if marketed as "OVER 9000 concurrent connections." If posted to /r/programming, it'll convince everyone that we're cool and not geeky academics who don't know internet memes, and everyone will embrace haskell and the world will be a better place.
I agree. While he has some valid points about the subject, the condescending tone and mud-throwing makes it hard to take the post seriously. Making bold generalisations and calling people who put a "Conclusion" section in their blog posts "bad people" is just plain silly.
It's always been a pity to see generic operations whose behaviour is wholly type-directed implemented by dynamic despatch on run-time type representatives. Generic programming should be compile-time, when the structure it relies on is known at compile-time. This is work that shows the way.
No - generic programming should be runtime, since then you get far better composability and can put things in libraries etc. Compilers should optimise away structure that is known a compile time to get optimisation. These are separate points, that when combined, offer several advantages over compile time generic programming.
Uniplate is also faster than SYB, but still a library and often gives you more concise generics than either Hinze- style or SYB. Hinze-style is sometimes useful, but tends to be rarely.
Why do you get more composability? I'd expect the opposite, just like typed programming (if you include type-classes) yields more composability than untyped programming. e.g: Can write functions like filterM with type-classes, but not in a dynamic language. Without filterM, you have less code re-use (less composability).
I think we're in danger of violent agreement. I was only trying to draw a contrast between generic programming by run-time type analysis and generic programming by compile-time type analysis. By rather different mechanisms, both Hinze-style generics and Uniplate are on the sensible side of that divide. I'm not especially an advocate of Hinze-style generics versus other presentations which deliver specialized code: it was an early step in a good direction, and thus deserves appreciation, even if better technology is now available. This structure-driven presentation of generics was around when the SYB feeding frenzy kicked off, but it somehow got buried in all the excitement about type-safe casts. I agree that generic programming should be "ordinary programming" as far as possible (indeed, that's a central design principle of Epigram 2). Hinze-style generics is too hard-wired in that respect. Fluidity and uniformity between phases of execution (not to mention side-of-the-colon) sound good to me. I don't think of "ordinary programming" as necessarily "runtime", because in my world, it really gets everywhere. From a language design perspective, though, I would rather send a slightly stronger signal than a compiler pragma or command-line option that I expect certain kinds of specialization to be performed statically. Whilst it's great that compilers fiddle about in the back of the room and make it all go faster, writing programs which invoke the voodoo appropriately remains a dark art. Often, the people who are good at it are also the people who made the voodoo and can adjust it if it doesn't quite work the way they want. So I agree with you about the ends (generic programs being "first class" but yielding specific programs at compile-time for statically known types) and about the means (ordinary programs, specialization by compiler). There are big wins ahead, as we get better at programming with the structure of types. We'll get much more powerful abstractions and much more compact high-level code, but by the same token, we'll also be much more dependent on specialization for performance. We need clearer ways to ensure that it happens.
Yes, we do seem to be agreeing. I think for optimisations what you are really after is predictability. You want to either annotate structure as being compile time only, or know that the compiler guarantees certain types of structure will disappear. With the current state of the art that requires voodoo, with an appropriate supercompiler (hopefully coming soon!) hits magic just happens (mostly).
In Haskell functions are first-class, and can be put together in lots of ways. Whenever you switch to either types or classes (in Haskell) you loose some of the first-class-ness. They certainly do compose in some ways, but I think they probably aren't as flexible a normal function. Just at a really trivial level of syntax, you can't nest them under a where.
Did you not spot that the blog poster uses a section titled _Conclusion_ at the end of the post? Does this not hint that they were being less-than-100%-sincere? That maybe the condescending/mud-throwing which you see is in fact in jest?
I did notice it. But I interpret it as sarcasm.
The problem is that it's a tiny joke that gets buried by the overall tone of the article. The conclusion itself makes statements like "the blog poster is dumb" and that you can tell that the HN poster "is an immature retard." The final sentence is a masterpiece of condescension that, even if intended sarcastically or jokingly, is simply off-putting. You can see the effect of all this in the first comment, apparently from the one of the subjects of the post, which doesn't appear to be in jest. 
I've looked at this a number of times, but never encountered it "in the wild." Are there any examples of this feature being put to good use? Edit: Dear lazywebs, is it possible to write a nice version of compos with this? 
We used it on a project at Galois involving generic traversals, and it improved runtime dramatically over SYB. The experience was very positive.
It's still far from the performance of hand-written traversals, but it's usually good enough.
&gt; From a language design perspective, though, I would rather send a slightly stronger signal than a compiler pragma or command-line option that I expect certain kinds of specialization to be performed statically. Can you give us an example of what you think it should look like?
I read the post and 3 of 3 comments at the time and the first commenter comes off as a complete twat. Look at this bit: &gt; _This shouldn’t be surprising, because blog posters who put a "Conclusion" section in their posts tend to be bad people._ &gt; &gt; I applaud you. You have succeeded in your efforts to come across as an idiot. Especially given that this very blog post contains a conclusion. They read the post, quoted the joke, wrote a huffy response to the joke and didn't once reflect that it was done on purpose. That takes a staggering lack of introspection.
There's also the [recent type families-based generics](http://www.cse.unsw.edu.au/~chak/papers/CDL09.html), which is within a few percent of hand-coded in the benchmarks in the paper.
If you want to provoke a "staggering lack of introspection" in someone, a good start is to call a total stranger a "bad person" and "dumb". That's the problem here: insulting and calling people names is only all in good fun among people who know each other - and even then, it often masks underlying hostility. Doing it with strangers over the internet just doesn't work, and can only cause problems. Besides, it's not actually amusing, so there's really no point. 
Are there any open-source examples that use this feature?
I'm really not sure. There are lots of possibilities. There's been a lot of interesting stuff about staging which is worth a look. See Tim Sheard's notes from the Spring School on Datatype Generic Programming, 2006, for example. A good start would just be a notation to demand that a recursive definition be specialized. I know there are pragmas to this effect, but it would be good if there were a way to declare an *expectation* of optimization resulting in a static error if the optimization is inapplicable. One might hope that something of the order of specialize map f = foldr ((:) . f) [] would result in the generation of map f [] = [] map f (x : xs) = f x : map f xs or fail in the attempt.
Move along now, there's nothing interesting here to see.
Meh. The event library can handle hundreds of thousands of concurrent connections without jumping through any peculiar hoops.
Yes, looks like the author either a) already had this, or b) missed the memo.
...except that it's nice to know they're using Haskell at Facebook. Hi guys!
Something that amuses me to no end is the fact that PHP was picked as the language to start doing source transforms on, mostly pioneered by Facebook. (It especially speaks to my lineage; I consider PHP to have been my first language and Haskell to be my latest language.)
You got learned a Haskell!
Maybe need to get away from the Array library. Hmm, bos, drop by for a SF FP meetup? :)
For great good!
I think the breaking of the Data.Set invariants (see [this post](http://www.haskell.org/pipermail/haskell-cafe/2010-March/074325.html) for example) is more of a problem with Set than with generalized newtype deriving. If you replace the standard Data.Set implementation with one that takes an explicit comparator, like this: data Set a = Set (a -&gt; a -&gt; Ordering) (Set' a) data Set' a = Tip | Bin {-# UNPACK #-} !Size a !(Set' a) !(Set' a) ,and then go through the source of Data.Set and make the necessary changes (yes, I did this, and yes, I should release this!), the comparator also gets 'flipped' when using downSet/conv. The show of the results is still the same, but now Data.Set.valid returns true for both cases.
Many congrats, that looks really impressing, especially the RESTful nature. Looking forward to see some AJAXian support. This definitely is the most advanced haskell web framework where the effort in creating one is not evaporated in mastering the language ...
How did you install gtk2hs and what version of ghc are you using? Maybe you try leksah google group for quicker answer? 
When presented with the difficulty of building a *description* of a side-effecting program, the first thing that might come to mind is: Just use a list of side effects, and "compose" bigger side effects by concatting the list: data SideEffect = &lt;abstract data type&gt; type Program = [SideEffect] Then, it seems like you can just compose programs like: a ++ b However, then you realize that your programs can only have "output" effects (that don't yield any information to the program). You want your effect to be able to read from the user/etc. You also want to be able to choose the next elements in the list based on previous inputs, so instead of: type SideEffect = ... You need something like: data SideEffect a = .... So that it can yield an input of type a. But then, [SideEffect a] is also not enough, because the list has all the effects in it before any of them were executed, so you can't have next elements in the list depend on previous effect results, so you have to replace: (++) :: Program -- first program -&gt; Program -- next program -&gt; Program -- composed program with something like: chooseAndComposeNextEffect :: SideEffect a -- first program -&gt; (a -- result of first program -&gt; SideEffect b) -- next program (allowed to depend on result of first program!) -&gt; SideEffect b -- composed program (the composed program's result is simply the second program's) chooseAndComposeNextEffect is actually called (&gt;&gt;=). When you actually do not care about the previous result, you're back where you just want (++) between programs, and that's called (&gt;&gt;): (&gt;&gt;) :: SideEffect a -- first program -&gt; SideEffect b -- second program -&gt; SideEffect b -- composed program And SideEffect is actually called IO. This is a really useful structure when you want a sequence that looks like: * Some process that produces an "a" * Pure computation that converts an "a" to: * Some process that produces a "b" * Pure computation that converts an "b" to: * Some process that produces a "c" ... In the case of IO "some process" means a "program" that is allowed to have side-effects. But this interlaced sequence is very useful in many other kinds of processes. For example, "some process" might be a computation that can have multiple results. Instead of (a -&gt; b), it is (a -&gt; [] b). Then, our "pure computation" stage would simply be executed for each possible result, and then each continuation process would be run and all the results combined. The common structure here, the ability to implement both: (&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b and: (&gt;&gt;=) :: [] a -&gt; (a -&gt; [] b) -&gt; [] b already suggests that these might fit into a type-class. If you add: return :: a -&gt; IO a return :: a -&gt; [] a then you get the whole Monad class: class Monad m where return :: a -&gt; m a (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b where "m" just takes the place of "IO" or "[]" or any other type that can implement (&gt;&gt;=) sensefully.
A while back, I wrote an (incomplete) tool called `hpath`. The idea was to extract Haskell definitions for use in LaTeX, instead of embedding the LaTeX in one's code. Wouldn't it be preferable to separate annotations from definitions? Since code is a tree structure, we could very sensibly have an `xpath` like language for finding definitions and annotating them with specializations. Maybe for one particular program, I want a specialization of `map` as you specify; in another, I might not. If we separate annotation from definition this is easy to do.
I'm posting this here, because I think it is a fairly good overview of the issues with STM. Many of the issues mentioned are specific to trying to retrofit STM onto an existing imperative language design, but some of the policy concerns apply equally well to STM in Haskell.
Yes congrats and thanks for contributing this. The package(s) and web presence and usability look good and the getting started example really is easy. FWIW I was able to go from 0 (well, from ghc 6.10.4 and Haskell Platform) to 1000 req/s in 10 minutes on a macbook. (Testing the uncompiled code with ab -n 1000 -c 100. Compiled was a bit slower. With -c 200, something gets flaky.)
I appreciate Daniel Franke's comments to the article.
Great news! So LLVM beat GHC with patch integration?
Hey, I managed to get it working. I had to install libgtksourceview2 through apt-get.
&gt; A simpler way to think about monads is as a hack to turn function composition into a way of forcing the sequencing of function calls, or the functional-programming equivalent of a shell pipeline. And having said that provocative thing, I’m not going to go into the gory technical details required to make that actually happen. That's not true -- monads are not really about sequencing. A simple f . g . h . i is like a shell pipe-line and already "sequenced". Functor -- Allowing a computation to carry some context with it Applicative -- Allowing composition of multiple contexts Monad -- Allowing context-based results to determine which context to compose with
Type classes aren't modules. The "Ord t" constraint doesn't just say that there may be some implementation that matches the signatures of the class methods, it says there is a single such implementation. Giving Data.Set an explicit comparator isn't just changing the implementation, it's also changing the meaning of all the Data.Set functions.
&gt; The single most delightfully weird detail of Haskell I’ve run into so far is this: you can have type-valued variables, and write type-valued expressions that are recursive! What language *doesn't* have recursive types? struct Tree { struct Tree *left, *right; void *data; } Or is he talking about something more complex here? &gt; Local color so you can sound like you know what you’re talking about: in Haskell-land, a closure is called a “thunk”. His definition of "closure" is... strange. I get what he's going for (think of every argument as a function that returns a value instead of just a value), but I've never heard "closure" used to describe that function itself.
`f . g . h . i $ x === runIdentity (return x &gt;&gt;= (return . i) &gt;&gt;= (return . h) &gt;&gt;= (return . g) &gt;&gt;= (return . f))`
if a well-performing llvm backend can made, why not a jvm backend?
Okay, I see your point. I think however (not an argument against you, btw) that the title of the reddit post and the post on haskell-cafe are overly alarmist. I mean mapMonotonic can also break the Set invariants and you don't see anyone complain about that! It's a tricky point, the interaction between type-classes and invariants in datastructures. I wonder how GADTs change the story; would a Set type like data Set a where Nil :: Ord a =&gt; Set a Bin :: Ord a =&gt; !Size -&gt; a -&gt; !(Set a) -&gt; !(Set a) -&gt; Set a carry the dictionary for Ord around on all the constructors? Time for a little more experimentation...
because jvm assembly is java without syntactic sugar and mapping apples onto oranges just doesn't work well.
If you haven't done it already then go through and solve the problems on [project euler](http://projecteuler.net/) other than that I'd say find a problem that needs solving or a project that needs help.
Now you integrate it in all of your work. Similarly, I'm doing that with F# :).
Nothing about the combination of words that makes up that title can possibly be good.
You're closest to having learned it if you know you haven't learned it. :)
Use it.
&gt; What language doesn't have recursive types? The example you gave isn't recursive since you need a parameter to recurse over for there to be recursion at all. You could write it in Java though: class Tree&lt;A&gt; { Tree&lt;A&gt; left, right; A data; } This still isn't really a recursion though, it's the result of one (compare with how 120 isn't recursive, but factorial(5) may well be.) In Haskell the recursion would look like: data TreeF a b = Leaf | Branch a b b newtype Mu f = Mu (f (Mu f)) type Tree a = Mu (TreeF a) Which, minus newtype noise, gives us our familiar tree type: data Tree a = Leaf | Branch a (Tree a) (Tree a) I've probably made some blunders, but at the very least I'm sure not many languages can express a type-level fix.
...?
You'd have to rewrite ghc/rts/*.c first. 
Teach it.
nice idea but I don't really know anyone who would want to, and if I did -- I'd just be passing on the problem.
The Haskell is Not Not ML paper looks very cool.
You can do lots... any interesting project ideas? Try it in Haskell. Can't think of a good project? I've got a few-- I can definitely find bugs or features to add to one of my projects, I've got some other ideas for new projects too. Good at game programming? Try a game in haskell, maybe (as a by-product) you'll come up with a nice game engine or graphics engine in Haskell. Basically, just do something! 
Um, prove it? If you had said you learned Pascal, or BASIC, or even C++, I'd probably be satisfied to take your word for it. But Haskell doesn't "top out" quite so easily; it just supports higher levels of abstraction and harder challenges. So, what have you learned, exactly?
Actually I find Haskell much easier than C++.
Well, I would have to agree with that. But... seriously... got some code to share?
Move on to harder drugs, like Agda.
I already know agda ;(
The only surviving code I have is this rubbish http://code.google.com/p/janus-haskell/downloads/list http://rascal-haskell.googlecode.com/svn/trunk/ -- it's the kind of experimental stuff we all write when learning. (from ~2 years ago)
Well, it's more than I've written in Haskell. I'm admittedly a beginner myself. I was writing a series of blog posts in which I explored some toy programs, starting with this one: http://praisecurseandrecurse.blogspot.com/2006/12/division-bell-tolls-for-me.html Around mid 2007, though, things started to get crazy, and I kind of dropped out shortly after _Real World Haskell_ came out. It's my fondest hope to get back to Haskell. 
I like pink floyd.
I think you need to give mores details other then "I've learned Haskell" there are so many lines of research in Haskell both in terms of formally defined (and implemenated) high level abstractions and interesting/useful language extensions to Haskell in GHC. You might have learned Haskell the base language but that does not mean you're at an advance level of Haskell and functional programming in general. I think you need to define your level and your experience. Lots of relatively new research ideas involving Haskell, I really doubt you have learned them all. 
Llvm and jvm are very different beast.
I'd enjoy reading ESR explanation of monads and making practical use of them.
Since the best Haskellers I know say they have still many stuff to learn, I'm pretty sure you're not done yet with Haskell. Most remaining knowledge will come with experience, I think. Contribute libraries, applications, bug fixes, whatever :)
I'm not esr, but... The problem re "learning monads" is that there are two problems. One is the problem of creating/understanding a general abstraction which covers a large variety of coding patterns; and this is hard to explain without invoking at least a little category theory (though you can do so in the subtext, rather than using the jargon). The other is the problem of sequencing I/O, which is where most Haskell learners first run into monads. The sequencing problem is easy to explain without CT, provided you're familiar with functional programming idioms. The short explanation for the IO monad is that it does a continuation-passing transformation to make ordering of evaluations explicit. People do CPS transformations to enforce sequentiality of pure code too; the only difference here is that there are side effects.
&gt; His definition of "closure" is... strange. Indeed. Closures are for cleaning up function definitions so that they don't use (local) variables bound by the environment outside the original function definition, enabling local function definitions to be hoisted to the top-level. Closure conversion doesn't really have much to do with laziness...
&gt; A simple f . g . h . i is like a shell pipe-line and already "sequenced". Only if all those functions are strict. If they're lazy then we can't guarantee the order in which they are evaluated without looking at the actual definitions (i.e., without doing strictness analysis).
It's been tried: http://portal.acm.org/citation.cfm?id=693097&amp;dl=GUIDE&amp;coll=GUIDE http://www.cs.yale.edu/~tullsen/haskell-to-java.ps But as others said, there's no relationship between the reasons you'd want a JVM back-end and the reasons you'd want an LLVM back-end.
We can't really guarantee the order in which things in m &gt;&gt;= f &gt;&gt;= g are evaluated, either, without knowing what the monad in question is, and possibly knowing what `m`, `f` and `g` are.
These are duplication and deletion morphisms. As it turns out, they are very useful in symmetric monoidal categories, which generalize cartesian categories to account for linearity. See Baez's [Rosetta Stone](http://math.ucr.edu/home/baez/rosetta.pdf) paper.
I've read a few tutorials from haskell.org and the RWH chapters. I should have been more specific, though. I do get how it's just formalizing a general abstraction. I do get how to write a simple monad, with and without do syntax. I do get how to use some of the basic monads like Writer, Reader, State, Maybe, list ... Where things always come unraveled for me is how to use them in a monad transformer stack for some practical use. I just can't keep things straight in my mind at this point and it almost becomes like a liftM spaghetti mess. And it often seems like you need to stack monads up to make good practical use of them, because each one is so specialized.
Ahh... the [Advanced Functional Programming](http://www.cse.chalmers.se/edu/year/2010/course/afp/) exam I reconn. Personal favourite from that course: [the *dynamical programming-function*](http://www.cse.chalmers.se/edu/year/2010/course/afp/lectures/lecture11/Memo.hs.html).
&gt; The example you gave isn't recursive since you need a parameter to recurse over for there to be recursion at all. Nonsense. Something like `data Tree = Leaf | Branch Tree Tree` is a perfectly fine recursive data type. What munificent wrote _is_ a recursive data type. ESR is being stupid and ignorant. As usual.
I'd benefit from some teachings. I have yet to have anything click in about two weeks of reading different books. Very frustrating!
&gt; Where things always come unraveled for me is how to use them in a monad transformer stack Yeah, that part is harder. If you want to get rid of the `liftM` mess there's a standard idiom, provided your stack doesn't contain multiple layers from the same transformer and provided you treat it holistically (i.e., don't push and pop layers of the stack). Namely, use a newtype to wrap your whole stack, and then give instances of Functor, Applicative, Monad, MonadReader, MonadWriter, MonadState,... The instances will be a whole bunch of `liftM`, but afterwards you can just use the methods for those classes directly. This approach also helps maintenance if you end up needing to change the stack (since you don't need to adjust the `liftM` scattered throughout your program). If you do have duplicate layers, you might be able to use additional newtype wrappers with MonadFoo instances to "cast" between the layers you want to access (a la the many Monoid instances for Integers). That only sometimes works out though. And if you're changing the stack throughout the program (e.g., to restrict who has access to what state) then the idiom won't help.
I considered putting some AJAX features into this release, but- like a model- decided it would be best to leave it out for the moment. Do you have an specific ideas in mind for AJAX support?
Cool. What code were you running with exactly?
I'm in the process of taking another stab at Haskell, so I'll try this out and see if it helps. It sounds great. Thanks for the help.
I've been learning Haskell for 19 years now, and I don't feel like I know it all.
can you give me some examples?
For instance, people keep coming up with new and clever ways to use the type system, even the Haskell 98 type system is very powerful. (Which is no surprise since the ordinary Hindley-Milner type system has the computational power of a Turing machine with a finite tape.) 
Contribute. For all the code on Hackage, there are a lot of areas in the Haskell ecosystem that could use some love. The hogre library currently binds to a mere subset of Ogre3d -- build it out so it supports the whole library. Frag currently has no enemy AI, and no netcode for online play. Either improve it or create your own game. Get involved in Leksah and turn it into a world-class IDE. I'm still learning Haskell, but these are the projects I wish I had the time, and the chops, to help out with. It's still such a young community, and there are a ton of things to do. 
Oh, and a good way to check if you know a language reasonably well is to write a compiler for it. You could always do that.
I've grown to like and appreciate it a lot. Any word on if and when they'll be a 2nd edition?
More digging with the profiler suggests the large number of threads has nothing to do with it. Rather, we suffer from bad I/O handling and suboptimal piece management. Both are fixable.
I'd also be somewhat wary of strictness with cml. Are the underlying MVars being updated in a strict fashion? (You risk moving work to the wrong thread, building up thunks in locks and so on).
No, they are not. So your points are completely valid. There are definitely a lot of places where the "wrong thread" evaluates the thunk.
The hello world app created by "yesod". 
It's no wonder the profile doesn't tell much with all those functions inside where-clauses and no SCC annotations. I'd say move the functions to the top level and use a ReaderT or similar to pass those arguments that are currently lexically bound.
Some httperf output: http://gregorycollins.net/posts/2010/03/12/attoparsec-iteratee
so many submission suggestions...here's what i *don't* want in haskell SoC project: - something that does not get finished. if that means a smaller project, so be it. "finished" means it has docs and i can get it from hackage - something that does not show off the value of haskell. for example, a binary that sews together a bunch of system libraries with ffi (e.g. the webkit-based browser suggestion) - something that has mostly academic value. there are plenty of academics playing with haskell who have budgets and time - "rewrite foo in haskell". the community will rewrite software as it sees fit, i don't see the need for SoC work to bother with this kind of task. SoC projects should have some novel value
Those are pretty good points you make. Proposals should articule the value to the Haskell community their project would bring.
You might want to have a look at http://moonpatio.com/fastcgi/hpaste.fcgi/view?id=8537#a8537 , which uses O(1) (or, rather, O(log . f n) where f n is the number of choices at the current input position) time. In the general case this can lead to exponential parser construction time and polynominal space usage, but for short bursts of non-recursive grammars it looks like a fast thing to include as one parsing strategy among others. The code could be a bit smarter and avoid some exponentiality, but the problem is inherent to parsing: If you have the grammar many (option (tokens "abcde") *&gt; tokens "xyz") , the next repetition of all possible matches to that grammar is a huge number of matches in the future, so even if recursion would be dealt with more smartly things would tend to blow up EDIT: added those MapOf instances
I just don't understand the complaints. Environment2 can just wrap Environment1. If monads are not a good solution for a particular problem, just don't use them for that particular problem. Haskell is IO friendly. The fact that it encourages pure functional programming doesn't prevent it to be used as an imperative language. The post makes me feel like if the author tried to explain his application design, expressed what's wrong, he could find a good solution to his problems.
I just added two proposals which I tried to motivate: * http://hackage.haskell.org/trac/summer-of-code/ticket/1580 * http://hackage.haskell.org/trac/summer-of-code/ticket/1581 Does anyone feel they should be motivated further?
&gt; The post makes me feel like if the author tried to explain his application design, expressed what's wrong, he could find a good solution to his problems. Yup. On the other hand, there is this lure of perfectionism in Haskell. In other languages, you just put up with the clunkiness, but in Haskell, you always think that you're just missing this really elegant solution... ;-) 
[FreshLib](http://homepages.inf.ed.ac.uk/jcheney/programs/freshlib/), a library by James Cheney for fresh name generation and binding, uses derivable type classes.
Thanks, I'll look at that.
Can someone explain why I should care that someone doesn't want to use Haskell?
(Why the downvotes? It's a serious question. If there's something interesting in this article to justify spending 5 minutes reading it that could have been spent working on some cool Haskell code, I'd like to know. Otherwise, from the title, I'm not sure why those of us who aren't language evangelists should be interested.)
I personally don't understand how the quest to find ideal language abstractions could be uninteresting, especially to a Haskeller of all people. This article seems perfectly relevant to me.
Okay, that was enough for me to read the article. But I don't see anything in there about the quest to find ideal language abstractions. The author's criticisms of Haskell seem to be: * No OO-style subclassing. Okay, if you want to use an OO language, then program in Smalltalk. Sounds like the usual disease of wanting to write [your pet language] code in any new language you try, rather than learning new ways to think about programming. * Monads are hard, let's go shopping. Sure, things you haven't learned yet are hard; dismissing them as such isn't particularly educational for anyone. Both of these are criticisms that get levelled against Haskell time and again, and both of them seem to reduce to nothing more than "Haskell isn't Java" or "Haskell isn't C". Am I missing something?
OO offers the ability to create kinds of abstractions that Haskell doesn't. I would consider this a valid weakness of Haskell. The author's use case seems weak in my opinion, to be fair. I don't think the argument was that monads are hard as much as that monads don't cure cancer. We are using monads as a crutch for our weaknesses rather than finding declarative ways to describe IO.
And I don't think anyone you'd want to listen to has ever claimed that monads cure cancer. There's an interesting argument to be made about declarative ways to describe IO, but this post read more like "the IO monad is different from what I know and therefore bad" to me.
Kudos to the authors and others like Oleg and Norman Ramsey for all their interesting work. It would be nice if someone wrote a tutorial to condense and summarize the prominent ideas about embedded DSLs. 
It takes more time to ask Reddit why you should read the article, and read their responses, than it does to just read the article. And reading the article is surely more educational than reading the Reddit comments. ;-) Writing the Haskell code would be more educational still, but the fact that you're asking on Reddit indicates you don't want to work on Haskell code at the moment. Side point: I've found that on a lot of mailing lists, both in my job and in the open source world, it's easier to implement a suggestion than to waste time arguing over whether it's a bad idea or impossible. If it's a bad idea, that will almost always become apparent once it's been implemented, and if it's impossible, that will also become quickly apparent. Otherwise, you can easily spend *months* debating a change that you could prototype in a week or two. I've heard that the first version of AdSense was developed this way - there was significant debate within Google whether content-specific ads were a good idea or even possible, but while everyone was arguing over it, Paul Buchheit just whipped up a prototype in a day and *showed* it was possible and would probably be a good idea.
I did end up reading the article, and regretted spending the 5 minutes.
+1 As a person with tendencies toward omphaloskepsis, Haskell is a lot of fun for me, but I spend a lot more time refactoring and making better abstractions than I do making something useful. The same goes in C++. For me, increased productivity comes from more boring languages like Python, Java, and Go, where there isn't much fancy for me to tinker with.
Ooh.
Video should be appearing soon, I hope.
You might have a [problem with it](http://hackage.haskell.org/trac/ghc/ticket/2824) if you run GHC 6.10. I don't have 6.12, yet, and I saw this. I still had to fix the imports and things. I've got a working version that works in GHCi but does not compile with GHC 6.10. Let me know if you want it.
&gt; OO offers the ability to create kinds of abstractions that Haskell doesn't. Given that the author showed no such thing - can you give an example? 
&gt; For me, increased productivity comes from more boring languages like Python, Java, and Go, where there isn't much fancy for me to tinker with. I think of these as my practical languages. They're the sort of things I solve problems with, rather than invent problems in. Which is not to say that impractical, for lack of a better term, aren't wonders for their own sake.
Any inheritance relationship is an example. (Yes, I know it's technically possible to work them in, but I mean as a nice, boring old language feature)
Interfacing with OO APIs. Tried writing what essentially amounted to a Java code generator in Haskell. The end result was a bit on the ugly side mainly due to inheritance and polymorphism.
I read it more like "the IO monad is exactly the same thing as what I already know instead of functional and therefore bad." Maybe it's still not great logic, but a little better.
Inheritance is rarely a solution to anything. Do you mean subtyping?
I'm not saying it's a good solution to anything, but it is something OO languages can do that Haskell can't.
What's the purpose of HaskellTorrent? Maybe the devs are trying to prove haskell's worthyness but what features make it stand out from other torrent clients for the end user's benefit? Could they not come up with a better name because if every program was named after the language, we would just end up with C++ torrent, java torrent, python torrent etc.
Well, FFI difficulty doesn't count as an inability to abstract something :-)
Is inheritance an abstraction? What is being abstracted away?
Too bad I am still too unfamiliar with Haskell.
Do all O'Reilly books look that good on the reader?
Well it's hardly a good thing to be able to express something that's a bad idea.
Seeing as you never responded and appear to only talk about games on reddit, I'm not sure if you are following this thread. On the small chance you are then you might be interested in the [GSoC announcement](http://www.haskell.org/pipermail/haskell-cafe/2010-March/074496.html).
PDF file: http://www.haskell.org/~simonmar/multicore-haskell-marlow-qcon2010.pdf
I'm sorry if I came off as though I'm not serious about Haskell or programming. The reason I tend to comment mostly on the gaming subreddit is more because gaming has been my hobby for a long time, while I'm relatively new to Haskell, and therefore have less knowledge to comment with. I've read the comments on this thread and have looked through the pages you and others have suggested, and have looked at the Haskell GSoC ideas as well. I am indeed very interested in the GSoC, but the level of experience and skill required in Haskell, and programming in general, is probably beyond where I am currently. In fact, the main reason I posted here was because I knew I probably would not be able to participate in projects like the GSoC, but was still interested in spending the summer coding for open-source projects. In addition, my CS class is currently working with Python, C, and a few other small languages, so I spend most of my free time working with those, rather than Haskell. I made this post primarily because when I do have free time, which would probably be in the next few weeks, I would like to have some places to start. I assure you I appreciate the suggestions you have left, and am sorry that I gave you the wrong impression.
This appears to be a parallel, underground project to the bos/tibbe/JaffaCake event work. Hopefully Snap can be used to test out the event lib before it goes into GHC...
Which is linked in the abstract on the scribd page, and also if you click the big "download" link. I was also careful to put the .pdf link in the abstract. Really though, this is best on Simon M's blog. The fact is, scribd/flash slides consistently get more readers than .pdf links. So they're more accessible, bizarrely enough, which is the goal of putting content out there.
Which is not clickable there, and you have to sign in to download the pdf. Edit : I'm not criticizing the use of scribd, I'm only explaining why ehamberg's comment is useful: for some readon, the pdf link you put in the abstract is not clickable, so I'd have to select, copy and past it to the address bar. And the download link requires me to sign in, I could using OpenId but then it asks for the URL, which I don't have ready. So it was easier to read it to click on ehamberg's link. 
For the downloadlink you have to register, which is anoying.
Chances are, if you're not specifically choosing projects so as to use Haskell, then you won't use Haskell "in the large" on any of your work in physics or math. That said, I'm a graduate student in mathematics, and I use Haskell all the time just to check conjectures, find examples, and just do exploratory computations. I'd also urge you to learn Haskell just for the reason that it will, to be a bit cliche, make you a better mathematician. It will give you a concrete area in which build nice abstract formalisms is helpful and commonplace. Haskell is really the only widely used programming language where this advice is reasonable; in any other language, the messy environment in which your programs run tends to get in the way of being able to reason about them, so that writing code is a bit too isolated from mathematical thinking.
Yep, that's definitely in the cards. The libev event model is (obviously) pretty similar to what the event lib will give you. Bryan is writing an http server also, I can see us taking them all out to the salt flats to race :) P.S. dons you're a *machine*.
They're sprouting like fungus. Go haskell!
Do you like to broaden your mind? Do you like to get a glimpse of the fundamental structures underlying computing? As a math and physics major, I'm sure you enjoy learning about fundamental principles that make our universe tick. In Java and C, it is hard to see the forest for the trees, but in Haskell, you have a chance of getting an idea of what computing is really about.
Check out "Fun with Type Functions" http://research.microsoft.com/en-us/um/people/simonpj/papers/assoc-types/ (first paper in the list).
&gt; With the current version of GHC in Debian and a large and growing collection of Haskell libraries, writing Haskell code on Debian using nothing but Debian packages is now a pleasure. I'm willing to believe that, _iff_ the steady influx of users with debian/ubuntu-problems to #haskell now subsides. On another hand, I don't think propagating using pre-packeged versions for development work is a good idea, at all: Too often have I cabal fetched packages, messed with stuff, bumped the version, and then installed it locally.
Unless you plan on going the agda/coq route instead, definitely yes.
Does it implement full HTTP 1.1, or some arbitrary approximation?
I haven't run it through a model checker to ensure it follows the specification or anything, but it understands pipelining, keepalive, chunked transfer-encoding (on input and output), etc. "Full HTTP/1.1" is a funny word choice though, HTTP is a simple protocol, there isn't much to it.
Pre-packaged versions of GHC are a good idea :-) And users of applications don't want to use cabal-install, so we certainly want the apps packaged (gitit, darcs, xmonad, hpodder, ...), so those libs have to be in there as well.
yes
After learning Haskell, I suggest you also learn Agda.
Thanks for the positive reinforcement. I guess I'll start reading and trying to write some code!
This is great news on the heels of announcement about abandoning Turbinado. What especially gives me high hopes is that authors have a field experience with happstack and see its shortcomings. So hopefully the project is not gonna end up like another overengineering contest. I was poking common lisp for more than a two years until i found that hunchentoot web server was ready for production use. And i am still poking haskell web ecosystem so far without success. I really hope that Snap is going to be a breakthrough (into haskell) for me personally and a killer app for haskell community. 
both of those seem great. and, if its relevant, it would be nice for the html library to be html5-aware
I've seen projects on various topics (AI, networks, web servers, blog posts, OS kernels, etc) written in Haskell. You only have to choose a topic and start writing. Learning a functional programming will help you in many ways. At the very least, it will make your thinking less do-this-and-then-do-that oriented (no for (i...) when you can have foreach; no locks/barriers when you can write your functions without side effects)
I answered your question http://cdsmith.wordpress.com/2010/03/15/type-level-programming-example/ I wasn't able to figure out one bit... namely, I can tell if a number is prime, and if a number is not a fibonacci number; but I wasn't able to build a type class to tell if a number is a fibonacci number. I still suspect there's something obvious I'm missing.
Is there a video? Slides alone offer very little.
If one uses [] as the monoid in the writer monad, then one has only oneself to blame for poor complexity. Use something sensible, like Data.Sequence.
I manged to get it to install on freebsd 8 amd64. Took me a while to realize I need a workspace to open a package but trying to actually use it for a minute without it completely crashing out is crazy. This is before I even have a source file open.
As a physicist, you might need to calculate some eigenvalues every now and then, which is a pain in Fortran/LAPACK. The [hmatrix](http://sites.google.com/site/wwwhmatrix/) library is much more pleasant to use!
I created an account on the Haskell GSoC Trac wiki this morning but can't seem to be able to use it to log on. Does anyone know what I'm doing wrong? Thanks.
Most people I have asked just give a hand-wavey reply so thanks for actually taking the time to answer! It seems that a fair amount of the code could be removed if Haskell had built-in support for compile-time arithmetics?
Would Template Haskell be cheating?
 class isFibH x y instance isFibH Zero One instance (isFibH x y) =&gt; isFibH y (x:+:y) class isFib x instance isFibH x y =&gt; isFib x Does that work?
So from what I understood for concurrency the best results will be obtained with immutable data structures inside a mutable reference (either with locking or STM TVar)? What about NDP, what's the latest with that?
You should try Agda, and specifically Relation.Binary.PreOrderReasoning.
Alas, no. Illegal type synonym family application in instance: x :+: y In the instance declaration for `IsFibH y (x :+: y)' The type checker wants to reason backward from the right-hand side to infer requirements on the left-hand side of each implication. For that reason, the right-hand side of the instance declaration has to be an actualy structural type that can be matched against for type checking.
Yes. Or, if I had used one of the existing libraries for type-level arithmetic. I chose not to, because I figured that building it from scratch would be more fun, and more enlightening.
We're now at 80+ hackers!
This is guaranteed to work for numbers up to six (EDIT: and greater than zero, but you don't need that). After that, you're on your own: data HTrue data HFalse type family Or a b type instance Or HFalse HFalse = HFalse type instance Or HFalse HTrue = HTrue type instance Or HTrue HFalse = HTrue type instance Or HTrue HTrue = HTrue type family NatEq a b type instance NatEq Zero Zero = HTrue type instance NatEq Zero (Succ n) = HFalse type instance NatEq (Succ n) Zero = HFalse type instance NatEq (Succ n) (Succ m) = NatEq n m data Cons a b data Nil type family Member elem list type instance Member x Nil = HFalse type instance Member x (Cons a b) = Or (NatEq a x) (Member x b) type family NFibs count a b type instance NFibs Zero a b = Nil type instance NFibs (Succ n) a b = Cons a (NFibs n b (a :+: b)) type family IsFib n type instance IsFib n = Member n (NFibs n One Two) class FibClass n instance (IsFib n ~ HTrue) =&gt; FibClass n 
I get Illegal type synonym family application in instance: x :+: y In the instance declaration for `IsFibH y (x :+: y)' although I may be missing a pragma or two.
I didn't try it myself, nor do I really know too much about type level programming. How about instance (isFibH x y, x:+:y ~ z) =&gt; isFibH y z 
Thank you! That's the "obvious" bit I was missing: there's no such thing as an "or" in type class contexts, but by reifying true and false as types, we can define arbitrary functions on them. Excellent! I'm rewriting more of the code that way.
Please stop saying "The IO Monad", "The Maybe Monad". Just say "The IO type", "The Maybe type". Maybe has instances for Functor, Applicative, Monad, MonadPlus, Monoid, ... Why do you say "The Maybe Monad"?
I think I instinctively talk about the "Maybe monad" because you can easily make a "Maybe type" but its the instances of Functor/Applicative/Monad/etc that are the important bit, and it's a bit more concise than saying all of those other things. If it bothers other people too I can start being careful about this distinction.
I get Type constructor `:+:' used as a class In the instance declaration for `IsFibH y z' Even if it did accept it, the approach still wouldn't work. The exact error is *IsFib&gt; testFibH (undefined :: Zero) &lt;interactive&gt;:1:0: No instance for (IsFibH Zero y) arising from a use of `testFibH' at &lt;interactive&gt;:1:0-27 Possible fix: add an instance declaration for (IsFibH Zero y) In the expression: testFibH (undefined :: Zero) In the definition of `it': it = testFibH (undefined :: Zero) The type class mechanism is meant for dictionary passing polymorphism, so it isn't enough for there to be _an_ instance of IsFibH; GHC has to know exactly which instance to take the dictionary from or that the instance is the same for all types. The former is why functional dependencies and type families were invented. The latter is why things like flexible instances exist.
Ah of course.
Thanks. I've looked at various small HTTP implementations lately (most of them not written in Haskell, admittedly), and came across all kinds of odd omissions, even in the HTTP parser itself.
I think this drives a wrong point across to a lot of people that Haskell is "all about monads". I think that in this particular example used, too, the monad instance of Maybe is not really being used even... 
It's true; we're only using the Functor instance (with syntax from Control.Applicative; what a mess!)
Now updated with a working version.
So using that general technique of pure type functions with reified Boolean values for predicates throughout made the code a whole lot cleaner. There is now only one actual type class, which is the one used to build the final function type. http://cdsmith.wordpress.com/2010/03/15/type-level-programming-example/
Haskell's stdlibs could really use a cleanup of the Functor/Pointed/Applicative/Monad classes and division into modules...
It's [broken](http://www.haskell.org/pipermail/haskell-cafe/2010-March/074497.html). I emailed edwardk and he added my name to the list, but I still can't log into trac.
Because often times the interesting part is in the composition. You're right in questioning the use of "Monad" everywhere, but replacing it by "type" would be worse because it doesn't suggest anything about composition strategies. Many examples of List and Maybe use do syntax, so talking about their Monad instances is often quite relevant. 
In this case he was talking about their Functor instance
Yes, and I agree that he should have said that. But I don't think replacing "Monad" with "type," as you suggested, would have been an improvement. 
Can you elaborate your comments about functor/applicative/monad? I didn't understood a word, and it seem there's an insteresting perspective behind them. 
For the record, I've updated the post to be more careful with terminology.
Agreed. I'd really like to see a video of this talk.
Sure: &gt; Functor -- Allowing a computation to carry some context with it If a type constructor "f" is a Functor, then the type: "f a" can be seen as a computation that has a result of type "a", and a "context" of type "f". The "context" really means different things depending on the "f" in question. For example, if f = [] then f a is [a]. This can be seen as a computation that results in type a, and has a "non-determinism" context. That is, the result can be one of several things. The Functor class cannot be used to modify the context (the "f" part of "f a") because it has just one operation: fmap :: (a -&gt; b) -&gt; f a -&gt; f b And since one of the functor laws says that: fmap id = id that means that fmap only applies to the "a" in "f a" and cannot modify the "f". In our list example, fmap applies to the values in the list, but the non-determinsm context is the same. If we choose f = IO, then our "context" is side-effects. "IO a" can be seen as a computation that performs some side-effects before yielding the result "a", and again, fmap cannot affect the context (the side effects), only the value that will be yielded. &gt; Applicative -- Allowing composition of multiple contexts Applicative adds: pure :: a -&gt; f a (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b I'll ignore pure for now. (&lt;\*&gt;) is similar to fmap, except (a -&gt; b) is also wrapped in "f". This means that while fmap mustn't touch the "f", (&lt;\*&gt;) takes in 2 "f"s and returns just 1 "f". So it must do some sort of composition of contexts. For example, if f=[], then it takes [(a -&gt; b)] and [a] and results in [b]. It has to somehow convert 2 lists into 1. In the list context, it tries all possible combinations (a cartesian product). If f=IO, Applicative must compose two IO actions into one, so it just sequences the side-effects into a single action that has the effects of both. But note that with Applicative, we must know ahead of time all of the "f"s that we're going to be composing with (&lt;\*&gt;). We cannot look at the "a" within an "f" and decide which "f" to compose using (&lt;\*&gt;). So (&lt;\*&gt;) is limited to "static" composition of contexts. &gt; Monad -- Allowing context-based results to determine which context to compose with Monad adds to Applicative another interesting operation: join :: m (m a) -&gt; m a This operation lets you take a computation that has a context "m" and results in another context, and compose these two "m"s into a single "m". So, while both (&lt;\*&gt;) and join compose two contexts into one, join does so on a context that happens to be a result of a computation within that context. So what Monad is adding here, is dynamism. We can compose "runtime" or "dynamic" contexts (results of context-ful computations) with static contexts. This dynamism, in the context of lists, means that we can have different kinds of non-determinism based on previous results. If you look at a list comprehension as an example, while this would be possible with just Applicative: [(a, b) | a &lt;- [1..5], b &lt;- [5..10] This would require Monad too: [b | a &lt;- [1..10], b &lt;- [1..a]] Because there's a different context (different list) based on previous "results" here (the "a" is a previous result). In the context of IO, with Applicative, you have to pre-choose all of the side effects you're going to compose together, and you cannot use the results of getLine, for example, to choose which effect to compose next. Monad's dynamism means that with IO, you are allowed to look at previous context-ful results (that depend on side-effects) to choose which side effects to compose next.
yes
I got way too excited when GHC 6.12 showed up in my list of packages in my dist-upgrade the other day. Thanks to all involved.
Well, Data.Sequence did come to mind, though I mentioned difference lists instead. This post certainly isn't my most interesting or profound, but hopefully it's a bit more approachable for novices than some of my others. Also, I wanted to: 1. Clarify the error I made. 2. Offer another solution to Data.Sequence or diff lists 3. It seemed like a nice introduction to "Asymptotic Improvements of Computations over Free Monads." 4. And, in retrospect, I should have made clear that this implementation of the writer monad is a key aspect of the corecursive queue monad. (Which probably is why I made the error in the first place.) Regarding #4, neither the regular writer monad nor the continuation monad is strictly more applicable than the other; there are problems where one applies and the other doesn't.
That might look something like this: http://www.galois.com/~emertens/prod/prod.html (I rather rushed this so please excuse a lack of "polish")
This is one of the best comment I've ever read. It gave a new perspective and a lot to study. Thank you very much.
Your first point seems utterly wrong to me. People use lists as defaults generally with small amounts of data where performance is not at issue. Haskell has great syntax for adts and data types of all sorts, although, granted, with view patterns it could be even better. Your second point is sort of silly. If you want a different behavior, you can just use a different type or even, e.g. Flip [a], and then reverse it yourself (or not). Your third point is maybe useful, for certain types of transformations, but unless the compiler "knows" what laziness behavior you want, there's strong limits on what you can do. And replacing a type that is simple and fast for small things (like list) by a type which is asymptotically better for larger things (like Sequence) is only sometimes a win. 
Sorry, but that is a crappy analogy. Also, I don't think you'll win many converts telling people to restrict themselves. The type system is a [tool](http://kerneltrap.org/node/5591 ) to help you do things you couldn't/wouldn't have done otherwise. But don't take my word for it, listen to John Hughes: "[It is a logical impossibility to make a language more powerful by omitting features, no matter how bad they may be.](http://www.cs.chalmers.se/~rjmh/Papers/whyfp.html)"
Personally I don't subscribe to to the Haskell == Enlightenment dogma. Nonetheless I find learning Haskell a very good thing, not only because the "mathematical niceness" but because it is a very interesting practical language, constantly getting new libraries and more and more users with very different backgrounds! Don't other redditors think that Haskell has found out of the "academic" niche? (or at least is currently finding its way out?)
It's been a while since I've read that paper, but no, it's not a logical impossibility. There are a number of cases where adding more features causes the formal power of a system to collapse. E.g., if you take a language like [the symmetric lambda calculus](http://www.diku.dk/hjemmesider/ansatte/andrzej/papers/DCaCD.ps.gz) but give it a true terminal object, then it becomes a mere boolean algebra nowhere near as powerful as the plain lambda calculus. Or if you take [CCG](http://groups.inf.ed.ac.uk/ccg/) and add various collections of combinators then it collapses to only be able to recognize sets, moving from stronger than CFGs to weaker than regexes! But those are quite a bit different than adding a type system to a language :)
It probably is a pretty crappy analogy. Yay experimentation.
Another example of this applies not quite to a language, but to the "metalanguage". All sorts of problems are decidable for finite automatons. But as soon as we add the ability to read/write memory (Turing machines) it becomes impossible to decide anything that you could have decided for the less powerful version. Acceptance. Equivalence. Totality. Nullity. All decidable for DFAs, and undecidable (and in some cases, unrecognizable) for Turing Machines! (Of course, the problem with this sort of argument is that in practice, we can get pretty darn close...)
The essential nature of a (static) type system **is** that it restricts the class of programs you can write. A typed program is a program with a proof of something, (e.g. that if a value (program) normalizes (terminates) then the result will begin with a constructor of the data type). Being a proof system it is typically incomplete (or inconsistent). It is well known that there are untyped "haskell" programs that are proper programs (meaning it normalizes to a constructor of the data type) but cannot be given types that the type system will accept. This will be the case for any reasonable decidable type system. Now, Haskell's type system is hooked to do things you couldn't/wouldn't do otherwise because the class system allows a sort of dispatched based on type. But this is a secondary use of the type system and not the point of the article. You are right that adding a type system doesn't make a langauge more powerful (by itself). And you may be right that one won't win many converts telling people to restrict themselves (to correct programs). That is their loss.