Does it? Do you know where? That‚Äôd be interesting. (Regardless, this is a very minor issue.)
Nice to see Julie join the club of is-at-every-conference :-D
Sure thing. If you have any questions, feel free to ask me and I'll try to answer. 
&gt; I am using the "Haskell for Mac" IDE That's your problem, Haskell for Mac cannot access things like `pkg-config`, see the [sandboxing](http://haskellformac.com/haskell-for-mac-technical-specification.html) section
So I'm pretty much screwed trying to do anything GUI related unless I started from scratch?
I just downloaded the standalone `ghc` from the GHC websiet to check what it ships with and it comes with `bytestring` installed. It's a pretty fast library for `ASCII` text. Unicode is not supported though, for that you will need `text` which does not come pre-installed with GHC On linux, you can check what libs ship with ghc by: ``` ls /usr/lib/ghc-&lt;version&gt; ``` On windows: ``` dir &lt;path_to_ghc&gt;/libraries ```
Do you mean literally only `base` or also the other built-in packages that come with GHC? You could cobble together a vector-like type using FFI, or use `array` and ST perhaps, if that package is available (it should be bundled with GHC).
Another way to see which libraries ship with ghc is to use `ghc-pkg list`, whose which ship with ghc are listed in the same section as `base`.
Since it's a school assignment, I would just use pure bytestring and write a comment complaining about the schools terrible Haskell library support and decrying the use of your code in "real world" and then write the magic words "I assume these bytes are magically utf8 and, in doing so, stoop so low as to almost be as careless as the average C programmer" No point in reinventing the universe to etch out a little bit of performance at the cost of tons of your time. (Alternatively, write the while thing out on a personal computer, pull in every library manually, and compile all 20+ libraries on the department computer? Maybe? Doesn't seem worth it.)
Will it run as your user? If so, it's possible to install Stack locally rather than system wide, and just pull in whatever packages/deps are required all in your own home directory.
In regards to your example, human readable text like that should be handled by external templates, not hard coded. Edit text, restart binary, bam! 1 minute change.
In haskell, you do this without doing type gymnastics. Or, really, you try your best to do this so that you don't have to use complicated gymnastics. Instead of building weird interfaces to add abstractions, you use abstraction to avoid needing weird interfaces. 
&gt; The actual program behavior when an error occurs is up to the implementation. -- [3.1 Errors](https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-230003.1) --- Basically ‚ä• is the Haskell Report's "undefined / implementation-defined behavior". So, for a expression whose semantics are ‚ä•, GHC is allowed to do anything. (The report does have several suggestions, not limited to specific instructions for the ‚ä•s produced by `error` and `undefined`.) And, IIRC, even before "postfix functions" were allowed, we had a number of optimizations that resulted in *more defined* programs than necessary.
The right language for learning FP from scratch is Lisp. Haskell is pretty awesome for the intermediate / advanced stuff though.
Nice! I would not have read it this way, but it absolutely makes sense. Thanks a lot! And I thought I knew the Report! :-)
Slight background: while compiling a class constraint like test :: Eq a =&gt; a -&gt; a -&gt; Bool Is translated into data Eq a = Eq { (==) :: a -&gt; a -&gt; Bool } test :: Eq a -&gt; a -&gt; a -&gt; Bool Thus means that a top level declaration like foo :: Eq a =&gt; a Looks like a constant expression but will be optimized like a function. To circumvent nasty surprises this is a compile time error instead. Usually you would add signatures for top level functions that take a class constraint but no arguments to avoid the error. You could also disable the monomorphization restriction, in ghci that is the default. 
So, this is kind of an ugly solution, but I had fun seeing if I could try this without all of the equal comparisons. howManyEqual :: Int -&gt; Int -&gt; Int -&gt; Int howManyEqual x y z = mod 18 $ 6 - (length $ foldr (\a b -&gt; if elem a b then b else a : b) [] [x, y, z]) Basically, you first fold over the list of inputs, discarding an input if it is already in the accumulator list. Then you get the length of your final list. If the length is 1, then you know all of the inputs were the same. If it's 2, then you know 2 out of the 3 inputs were the same. If the length is 3, then there were 0 equal numbers. The tricky part was finding a function that could convert length (1, 2, or 3) to number of same numbers (3, 2, or 0, respectively). I figured out that the function `mod 18 (6 - length)` results in a correct mapping of length to number the same (1 -&gt; 3, 2 -&gt; 2, and 3 -&gt; 0). I'm sure no one will care haha, but I had fun just messing around with it. 
There is no real concept of a haskell tuple as a collection. You can think of pairs as being a totally separate type from triples, quadruples, etc. So all the "dude use a list" answers you're getting are because there is no consistent interface for iterating tuple types... Because there essentially can't be, Cause (a -&gt;b) would only work on the fist element of tuple (a,c). You could specialize a specific sort of function or class for homogenous tuples, but that's generally not what tuples are for and you'd have to reimplement ir for each possible length of tuple because they're all different types. So, that's why the weird answers. (uncurry f) is probably your best bet in most situations - it'll only work for pair tuples, but generally I find using anything more than 2 slots in a tuple probably means I'm doing it wrong anyway.
for a beginner, sqlite + sqlite-simple. Typesafe DB access is a good thing eventually, but I think you have enough concepts to learn as a Haskell beginner.
We started lifting Num, etc. for most of the newtype like data types in base, but Applicative Num isn't always uniquely determined. There are two conventions you might follow for, say, (e, a) instance (Monoid e, Num a) =&gt; Num (e, a) would be the version you'd get for following the writer applicative. instance (Num e, Num a) =&gt; Num (e, a) would be the version. So as much as I'd like to be able to point to a perfectly consistent rule to apply, it becomes a matter of taste.
What kind of text-manipulations do you need to perform (besides concatenation)? What kind of "text" do you need to support (ASCII, ISO 8859-1, or Unicode; and what encoding?)?
On the other hand, http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers1/ makes a pretty good case for list mappend being a + by supplying a compatible * and proceeding to beat the hell out of it. Once you accept that seminearrings have a place in the numerical hierarchy, + ceases to be necessarily commutative.
2 years ago I said to myself, "hm, clojure or haskell?" Clojure was easier to get going with, so I became proficient quickly and now clojure is my day job. Everyday I get to solve problems like: "hm, what kind of map is this argument?" "geez, gotta get the repl running and connected to aws for me to figure out what my code does" "let me look at who _consumes_ this endpoint for me to figure out what it does" "why is my state funky, oh right, side effects" "Hey bayesmind, here's a new batch of runtime errors to solve!" Then I come home sit down, and have a nice evening with haskell (or coq, nowadays) and get to _build cool shit_.
Facebook may have dropped the `PATENTS` poison pill from react, but it's still present in this project and others (e.g., duckling). Disappointing, because it's fairly interesting work.
If you modify your expression type to use observable sharing to recover let binding and the like you can get this down to a near optimal encoding. This is a large portion of how I handle generating samplers for BRDFs with derivatives.
I wouldn't be so sure. Have you installed the command line tools as in: https://community.hfm.io/hc/en-us/articles/213702017-Can-I-add-extra-Haskell-library-packages-to-Haskell-for-Mac- ? And the used them as in: https://community.hfm.io/hc/en-us/articles/216666257-Usage-of-the-command-line-tools ? 
I'm still confused about this. Is there or is there not an implicit patent license when you use BSD or MIT? Because if there isn't then I don't see the issue, but if there is then it is kinda sketchy. 
I suppose it depends if you come from a "rings first" or "monoids first" view of the world -- the latter is more natural from a CS standpoint.
Depending on the setting, something as simple as [difference lists](https://wiki.haskell.org/Difference_list) might be enough.
`length . map f` is equivalent to `length`, so you can make that: howManyEqual :: Int -&gt; Int -&gt; Int -&gt; Int howManyEqual x y z = mod 18 (((6-) . length . group . sort) [x, y, z])
People think there is. I'm not a lawyer so I can't tell either way, but that's definitely where the concern comes from.
Don't know how much it would be usable in the "real world" (whatever that is), but Hackett http://docs.racket-lang.org/hackett/index.html sounds interesting.
&gt; `length . map f` is equivalent to `length`, so you can make that: haha wow, that's so obvious now. Note to self: don't attempt to code after a long day of teaching. 
I do both. Both are good. I think BayesMind has a similar experience to me. You end up doing a lot of work just to validate things conform to your expectations. It matters a lot more when you do a lot of network programming (in particular parsing edn or json a lot). 
Don't we always choose the Applicative that is consistent with Monads when we can? In that case, we'd need the Monoid constraint.
&gt; QA notices a spelling mistake in the subject-line of an email triggered by the system &gt; Fair enough, but this does not speak well for the underlying architecture. 
 &gt;ghc-hotswap: Core library for swapping shared objects safely &gt; ghc-hotswap-demo: Demo program that shows up swapping &gt; ghc-hotswap-so: Sample cabal project for building a shared object OMG yes! Been waiting for this for a year. My version of hotswapping directly with the ghc api segfaults on some types, and crtl c was broken. Thank you for releasing it, with documentation and examples too. 
Until the courts show for sure either way I'm just going to continue using all this stuff, but I'll remain skeptical.
Enterprise software can be surprisingly gigantic. 
This article is talking about clean architecture / onion architecture, I think. http://five.agency/wp-content/uploads/2016/11/Graph-1.png I tried to implement this architecture in Haskell in [this GitHub repo]( https://github.com/eckyputrady/haskell-scotty-realworld-example-app) Since you are asking for "how", let me walk you through the code to show how clean architecture is applied there: Core: * Struct.hs - define the types &amp; typeclasses * RealWorld.hs - define the "operations" allowed on the domains External Interfaces: * PG.hs - Repository implementation with PostgreSQL * JWT.hs - Token implementation using JWT * Web.hs - Interfacing with the core via HTTP Main: * Lib.hs - Tying up everything together The main idea here is to NOT allow `core` depends on implementation details (PostgreSQL / JWT / etc). If you see you are importing those in your `core` files, then you depend on those details and it violates the principle. "But I need to interact with database from `core`, how can I NOT depend on it?" The solution I used here is to create a typeclass representing it. For example, [UserRepo](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/master/src/Struct.hs#L77). Now, the core doesn't need to know about the database details. All that `core` knows is there is a function to do that. I use those typeclasses in [RealWorld.hs](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/master/src/RealWorld.hs) to do the action I want, like storing and getting user without knowing the database details. Next, we implement the details using our DB of choice (Postgres in this case). I implemented the operation in [PG.hs](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/master/src/PG.hs). Notice that this still aligns with Clean Architecture principle: "external layers may depend on the core, but not vice versa". After that, we tie it all up together in [Lib.hs](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/master/src/Lib.hs). You can see on line 36 onwards how I "tie" the implementation of `UserRepo` to use PostgreSQL using functions defined in PG.hs Here's a snippet from the article: &gt; Blessed is the team whose architects have provided the means by which all these decisions can be deferred until there is enough information to make them. So, what if we want to change PostgreSQL to something else, let's say, MongoDB? What needs to be done is to create a new `MongoDB.hs` module and implement the necessary operations ... then tie them all up in `Lib.h`. I hope my explanation is clear enough and could give you idea :) Please note that this is just one of the approach to implementing Clean Architecture. There are other approaches, like using `Free` monad.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [eckyputrady/haskell-scotty-realworld-example-app/.../**Struct.hs#L77** (master ‚Üí 05c6de7)](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/05c6de7ef8de4e6b736488428c38699af22c53f4/src/Struct.hs#L77) * [eckyputrady/haskell-scotty-realworld-example-app/.../**Lib.hs** (master ‚Üí 05c6de7)](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/05c6de7ef8de4e6b736488428c38699af22c53f4/src/Lib.hs) * [eckyputrady/haskell-scotty-realworld-example-app/.../**PG.hs** (master ‚Üí 05c6de7)](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/05c6de7ef8de4e6b736488428c38699af22c53f4/src/PG.hs) * [eckyputrady/haskell-scotty-realworld-example-app/.../**RealWorld.hs** (master ‚Üí 05c6de7)](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/05c6de7ef8de4e6b736488428c38699af22c53f4/src/RealWorld.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Apologies beforehand, for snapping back at this. I'm sick of this "purer than thou" attitude in the community. Every time a negative aspect about Haskell tooling/libs/ecosystem is being discussed someone comes and points fingers at how the original code itself, is not worthy enough. Examples: * You shouldn't have used TH. This is the reason I stay away from TH * you shouldn't have used Generics. This is the reason I stay away from Generics * it's a bad idea to have such large monolithic apps. Break it up into smaller modules. * this is not the true FP way of doing things. * stop bringing Java/Rails/Python examples to the discussion. Those languages suck. I don't want to have a meaningful discussion. * don't use ORMs anyway. They are the scum of the earth. I handwrite my SQL in all. My projects. Please sir, show me **any** project that you have architected and I can bet real money that I can find pedantic examples of why the "architecture is bad" What was your point exactly? That the Haskell compiler isn't fast? Everything is fine? It's the user's that don't know how to use Haskell properly? 
Thanks for the tip. Will give it a shot and report back. 
How any modules and LOC in your code?
Please read my response to this at https://www.reddit.com/r/haskell/comments/76zljl/comment/doiwamu
Please read my response to this at https://www.reddit.com/r/haskell/comments/76zljl/comment/doiwamu
Would be kind enough to share the GHC Trac issue for this? Also any way to quickly find out if my project is hitting these corner cases in GHC 8.0.2?
Would be kind enough to share the GHC Trac issue for this? Also any way to quickly find out if my project is hitting these corner cases in GHC 8.0.2?
Please read my response to this at https://www.reddit.com/r/haskell/comments/76zljl/comment/doiwamu
you are a lucky dev (scientist?)
I was simply pointing out that your example is easily fixable. If you are spending 20-30 minutes compiling to fix a _text misspelling_ then it's worth reconsidering an alternative method to handling that if it is actually a problem chewing up deployment times. If your point was to show an example of long compile times - then pick a better example. If you want to misconstrue some basic advice as "holier than thou" that is up to you. Keep in mind it's a two way street though - I think people get a little snippy with you both here and in IRC because you're carrying around a lot of pre-Haskell baggage and in the posts you've made over the last year you come across as someone assuming that the Haskell community is doing it wrong more often than not.
&gt; If you are spending 20-30 minutes compiling to fix a text misspelling then it's worth reconsidering an alternative method to handling that if it is actually a problem chewing up deployment times. If your point was to show an example of long compile times - then pick a better example. I'm sorry, but I strongly disagree. Changing email subject lines is not a regular task in my dev workflow that I need to invest time in figuring out infra/libraries for that specifically. But, when it was required, I could not stand that the state-of-the-art currently takes 30mins to do it. And it is as good an example to demonstrate slow GHC compile times as any. And there is another in my comments -- changing a migration file. And there was another that I faced, changing how static assets are served in dev vs production. Each of them hit the same soul-crushing compile times. And if you disagree with me particularly, please search this subreddit about the number of times this has been brought up by other people. Please let me know what you think would be a good example to demonstrate the slowness of the compiler, and I will give those to you, from many real-life (and possible open-source projects). But, please realise that the slowness of the compiler is the topic of discussion, not changing subject lines of an email. &gt; I think people get a little snippy with you both here and in IRC because you're carrying around a lot of pre-Haskell baggage and in the posts you've made over the last year you come across as someone assuming that the Haskell community is doing it wrong more often than not. No, I am not. I have a hold technology to high standards, in general. And I will not apologize for that. And yes, I do find that a number of people in the Haskell community refuse to acknowledge its weaknesses, using "purer than thou" comments most of the time. You can see elsewhere in the thread that the tone of my engagement is very different where the problem is acknowledged and possible solutions are being discussed. I will desist from commenting any further on this thread.
Well, this is in the context of "Installing stack". If you just want to install stack, there is no advantage to haskell platform, and you will waste space by having two ghc installs. So, this is a different topic entirely, of the particular things that you'd like to see improved in stack. I'm confused why this is ending up on reddit, and not on github. I guess since we didn't solve the github issues you feel this is a better place? In the future, feel free to re-ping the github issues. There's a lot of discussion of stack, so it's easy to forget about these things when I and other stack developers do not encounter the issue ourselves. The best way to get this sort of thing fixed is proposing a good solution and implementing it. Cabal's automatic flag resolution is a very surprising feature. It would make way more sense if automatic flags had explicit conditions. This is a feature we could support. However, the automagic flags are way too much magic when you want reliability and predictability. Yes, some convenience is sacrificed. Unfortunately, cabal_macros.h lacks pkgconfig dependency version info. If it had it, the solution would be clear - just use CPP and no flag automagic. So, we have two clear options options: 1. Add pkgconfig dependency info to cabal_macros.h . I like this approach. 2. Add explicit conditionals to cabal flags &gt; To build a Gtk+ app like Leksah with stack we currently need a big stack.yaml and another one for macOS. Plus the Gtk version needs to be specified manually. All of the flags and extra-deps can disappear if these packages get added to stackage. Simple! You only need the `stack.osx.yaml`. It looks like it just adds an extra-dep, and if an extra-dep isn't required by the build plan, then it isn't built / isn't required to be buildable. So, can delete your `stack.yaml` and rename `stack.osx.yaml`. With these changes, all you will need is: ```yamle resolver: lts-10.0 packages: - '.' - 'vendor/leksah-server' - 'vendor/ltk' - 'vendor/haskellVCSGUI/vcsgui' #- 'vendor/yi' - 'vendor/haskellVCSWrapper/vcswrapper' ``` Which is what typical stack configurations look like. &gt; Also cabal new-build works really nicely with nix in a way that I don't think stack can currently (correct me if I am wrong). I believe that nix pkgs stuff for haskell is built off cabal, so it is not surprising that it works natively cabal. A popular way to use nix is to generate the config from a stack config ( https://github.com/input-output-hk/stack2nix ). You can use stack just fine in a nix shell. Sure, you cannot use packages out of the global database, but that is intentional. It protects users from broken package databases left behind by cabal-install. In nix you don't have this problem as much because there's a quite sane deterministic build system atop. It may make sense to have an option to use packages from the global DB in the particular context of nix. Once again, contributions are appreciated. We do not have unlimited resources to make stack work great for every usecase.. Just the usecases faced by most users, particularly users in industry and newcomers.
&gt;Clojure seems like a productive language that's easier to get started with than Haskell. A better way to gauge productivity is to look at what has been accomplished in clojure and how many lines of code it took. I suspect "productivity" is dependent partly on domain. Picking a language because it's "easier" is probably a mistake. Many people seem to mistake Haskell being "tall" (that is, there's always more to learn) for Haskell requiring that you learn a lot to be productive. &gt;It sounds like monads, for example, are actually possible in Clojure as long as you install a library for it. This may be, but one of the great things about static types is that the community organizes around them. Thus, if a library returns an `Int`, you can be sure it won't have side effects. &gt;I get the impression though, that by learning a lisp you are only "sort of" learning FP, comparing to what you would be learning with something more like HS. Lisps and MLs are the dominant FP paradigms. They're pretty different. I don't know of any libraries for recursion schemes for any Lisp. &gt;But I was wondering if I could be helping myself more with FP than other Clojurists would if I learned how to use features in it that's more associated with haskell. If your goal is productivity, trying to learn Haskell paradigms while writing Clojure would not be the way to go. &gt; Another idea I've had is to jump right into Yesod... there is an entire book dedicated to learning that, and based on the first 2-3 chapters that I remember reading, it sounds like maybe I could even be productive with that pretty shortly. That might be a good idea, particularly if you're into doing web development in the future. I used the Yesod book before I understood one third of it, and it served me well!
&gt; But, please realise that the slowness of the compiler is the topic of discussion, not changing subject lines of an email. I was bikeshedding a trivial point, which is really easy to do on a Reddit forum. You are correct though, the topic is about GHC compile times. Google ran into this compile time problem with their massive C++ code base and they created Go. A middle of the pack boring language without generics, which consequently is one of the reasons it can compile so fast. GHC will never reach those compile time speeds, because that's the nature of a language with an advanced type system, generics, and the like. If you want a more state of the art GHC then you need to start finding companies willing to back research and development of it, much like Google has thrown millions into Go, or the enterprise world billions into the JVM. Until then we're probably going to have to deal with really long and crippling compile times, or least hope the work Simon's team at Facebook ends up with improvements getting backported upstream. &gt; I do find that a number of people in the Haskell community refuse to acknowledge its weaknesses, using "purer than thou" comments most of the time. While that does tend to happen, and we can both agree is unfortunate, there's also a lot of times when people are talking right past each other. Like making strong assumptions that the other party understands their point of view right out the gate, or the that the non-Haskell technology they are referring to as a reference is understood by the other party, or that the other party actually has non-Haskell software engineering experience, or that the other party has some magical insight into the author's complex code base. Sometimes the information is just too asymmetrical, right?
I was talking about pulling/building the Agda master branch, which is around ~100k lines of Haskell. I just used it as an example because it's a fairly large, freely-available project that I sync/build frequently. My own projects are all small enough that I've never had any issues with build times (largest are slightly over 10k loc).
Perhaps you're right. That comment I wrote was bad because it didn't offer anything actionable. I could delete it but the damage is done, so let me offer you an apology. I'm sorry, my comment was bad, and I feel bad for offering it to you as it stands. But, this has nothing to do with ANY preferences or proclivities of type, FP, generics, template haskell, rails, ruby, python, ORMs, or anything. If you're genuinely inserting non-developer written copy into code by hand then you've doomed yourself to compiling a lot. Your compile times will be greater than everyone else's just due to the fact that you've disregarded a really really important axiom for programs that face users. This is built into most UI/UX and even notification frameworks. You have a dictionary of localized string objects not only because it makes it natural and easy to localize by a string, but also because otherwise your life is a perpetual compilation and redistribution timesink. So yeah, in this case this is a genuinely bad decision. And you're right: it's nothing to be ashamed of. But really, for your own sake find time to externalize user-facing data so that it's a data objects if you haven't already. If you don't have a sidecar to store it in (some file distribution mechanism, a database, a key value cache, w/e) then it's painful. But your story makes it very clear you're well past the point where you need it. 
you have made a simple and eloquent explanation. Thanks
Does anyone know what the registration fee will be (or what it used to be in the past years)?
I think exposing APIs for consumption by web frameworks would be a nice non-critical thing to use it for. There's also Turtle for scripting, although examples are a little sparse.
&gt;exposing APIs for consumption by web frameworks Can you explain what you mean by that?
From the email: &gt; The Zurich Haskell Hackathon is a free (as in beer), international, grassroots collaborative coding festival [‚Ä¶]
Let's say you have a webshop. The frontend framework (React, Angular, whatever) wants to display items, the shopping cart contents, etc. The backend is a database or something. Via something like Servant, Haskell is used to respond to API calls for data from the database, as well as some business logic.
I see. Good point!
Great advice! Semi-off topic: While it is slightly jumping into "Avoid big-design-up-front", are there some good resources on how to structure larger Haskell programs? 
Uncle Bob is full of great ideas, but the way he presents them suffers from a bit of an OOP tunnel vision. If you skip over the overly concrete examples, and instead look at the gist of it, there is a lot of wisdom there that applies regardless of programming paradigm, especially the last bit. You want to defer the unimportant (technical) decisions, so that you can focus on the important (business) decisions first, and in order to do that, you need to decouple the "business rules" code from the more technical code. And the proposed way to do that is to use **narrow interfaces**, a concept that appeared in early Unix a good while before OOP went mainstream. So how do we do these things in Haskell, if not using classes and OOP "interfaces"? Well, the main thing is to manage dependencies between parts of our codebase, and Haskell has perfectly fine tools for that. One building block is *modules*: this is where we control who gets to see what, through imports and export lists. Particularly, we can make our data types *abstract*, that is, we can export only the fact that a type exists, and a handful of restricted accessor functions, but not its constructor, effectively hiding the implementation. This is equivalent to the `public`/`private` visibility specifiers in an OOP class, except we're doing it at the module level. (We can also do it at the Cabal package level, by putting modules into `exposed-modules` vs. `other-modules`). Another building block is *typeclasses*: by accessing concrete types through typeclasses, we can provide unified and narrow interfaces to them, and enforce the decopling that way - a function `doStuff :: PersistenceBackend a =&gt; a -&gt; IO ()` does not rely on anything other than the `PersistenceBackend` typeclass, which, if we design it in a backend-agnostic way, prevents our `doStuff` function to do anything backend-specific, and this allows us to defer the technical decision, much like programming against an `interface` in OOP would. Yet another building block is plain old functions, particularly, closures. We can provide an abstract interface by closing over a concrete implementation; this is particularly useful when the implementation choice must be made at runtime. For some insight into what this would look like in a real-world codebase, I recommend reading the sources of Pandoc (isolating readers and writers from the core) and HDBC (isolating individual database drivers from the public API through a common typeclass and a wrapper type in case you need to defer the backend choice to runtime).
Haskell is pretty powerful when it comes to practical things like Http APIs (via Servant) and any form of nontrivial parsing (Parsec). And having something like Persistent between your application and the database is a significant win over your typical prepared statement and/or string interpolation. Also, something having lots of IO doesn't make it better suited for imperative. If anything, it makes it all the more imperative to have something like the IO monad to keep track of all of it. Sure, printing to the terminal takes a bit more work, but that's more than compensated for by having a logging monad to actually give *structure* to how and where things get logged. For what it's worth, I would start with Elm before bringing out the heavyweight. It's easier to teach, and there are always opportunities to create simple web interfaces to your application data.
The event log is just a transaction log to deal with crashes (part of acid is that crashes don't corrupt your data after all!) after checkpointing it only stores the current value an resets the log. So it doesn't always rebuild from the log. 
But the thread is about discussing compile time for haskell code being an issue, why do you have to feel the need to comment on the other guys architecture etc. ? 
Relevant: https://davecturner.github.io/2017/08/21/haskell-experience.html
&gt; Get familiar with one, and the other will seem hard to read. Or get familiar with both and they will both be easy to read. 
are you using servant in production? IMO there are some parts missing (or not in the official package) like a happy redirect story, cookies/token/header-values, maybe session handling So I'm wondering if you did write those yourself or what you are using in place for those (wai-middleware?) Thanks and sorry if this is a bit OT here
*gloss* is limited in its scope. You could use bindings to `sdl2`, `sdl2-image` and `sdl2-ttf`. As far as I know there is no multi-line rendering (which the C version does have).
No answer from me since I have the same question. I've used it from the shell as well so far. üòÖ
Content-Type support in Servant (when used on the server side) is also pretty bad. It assumes if you specify multiple content types that you can always provide all of them which does not work for use cases like images that might be png, jpeg,...
\&lt;insert [Virding's Law](http://rvirding.blogspot.fr/2008/01/virdings-first-rule-of-programming.html) here\&gt;
Yes, and I have such a successful story at my work (telecom area). We are using [nginx-haskell-module](https://github.com/lyokha/nginx-haskell-module) in production on heavy loaded HTTP routers based on Nginx. Originally, I was experimenting with Haskell code in Nginx at my home spare time, and the module was able to make only synchronous tasks, but finally it appeared that Haskell is extremely useful in spawning async tasks, so I equipped the module with ability to run custom request-based asynchronous tasks and worker-based services. They appeared to be robust and fast (working in the same address space with Nginx worker process, and being optimized when using ByteStrings). So I just proposed to use this to my team-lead, then I wrote in Haskell 200 loc custom service we needed, then we went to our higher boss, talked about advantages of this approach and received OK. Next month we stress-tested it and finally put in production (it 's there more than half a year already). By the way testing on the corporate level helped to make the module grow faster and become more robust.
&gt; language without generics, which consequently is one of the reasons it can compile so fast Nah. There are a number of fast-running compilers for languages with parametric polymorphism, and not as a result of industry money. OCaml, D, Oberon... I don't really want to enumerate them all. GHC may face performance challenges that these languages do not, but parametric polymorphism is not among them.
Thanks a lot for that! 
Yesssssssss
&gt; **GHCi-as-a-service** &gt; ... This approach severely hinders performance, however. Running interpreted code is strictly slower than optimized compiled code ... I don't get it. ghci supports compiling to object code instead of binary code right?
Thank your for your answer, let me address the second and third building blog for better understanding. Please correct me when I get wrong &gt; typeclasses *what is gained from this is* dependency inversion. the module of the business logic is decoupled from the module of the database. for example: lets say there is `article` to be processed in the logic. the module that contains the logic does not now anything about the database all it knows is that there an interface that have some function to perform over articles, then the implementation of those function will be on the DB module. That way we can easily change the type class implementation without affecting the business logic. &gt; plain old function. I am not sure I did understand this point. 
/u/JonCoens I also have a question about this. Even if you build an `.so` shared object, this must be the outcome of a full GHC run (typecheck + codegen + link) that probably has lots of modules of input it recursively depends on, so it seems to me that the _Haskell_ side of building an .so should take roughly the same time as building a full executable. How comes you still benefit from building `.so`s? You mention "package and distribute", this sounds like the time advantage comes from something outside of the Haskell side of things, e.g. the Facebook deployment infrastructure favouring a 1 MB `.so` deploy over a 200 MB executable deploy?
&gt; ghci supports compiling to object code instead of binary code right? Right, but when you turn that on, then ghci does almost the same things that ghc does, and makes it tak as long as an incremental `ghc --make` does.
We (Futurice) use Haskell for internal stuff, and once in a while there are discussions about it. (In big enough company there's always someone who doesn't know that we use Haskell :). Few quotes from the latest one: &gt; "I guess the important bit is if you're able to find people to maintain the &gt; software written on it‚Äù - this is often misused rationalizing that ‚Äúthere‚Äôs &gt; millions of people in world who know this tech‚Äù forgetting that only 3 of &gt; them wants to work with that tech in future. &gt; About learning Haskell in general: I worked on our internal tools this &gt; summer, and did stuff with Haskell the first time then. My prior knowledge &gt; consisted primarily of Java and Python and a little Clojure. To be honest, &gt; for the first month Haskell felt really overwhelming. After I got hang of &gt; it, I enjoyed it. Now I am missing some of the Haskell's properties while &gt; working with other tools. &gt; Wow. Cool to join a company that's not hostile to Haskell &gt; - Do we really need the guarantees complete purity and type soundness provides on every project? &gt; - Good question, I'll answer with a tweet: &gt; &gt; ""I want the conciseness and simplicity of Python with the type safety of Haskell." &gt; Good news: You can! &gt; It's called Haskell." by /u/taylorfausak https://twitter.com/taylorfausak/status/919660141292720128 I suspect that older people haven't followed the development of GHC / Haskell For example first version of `text` on Hackage is from 2009, [RWH](http://book.realworldhaskell.org/) is from 2008 (e.g. it uses [`HTTP`](http://hackage.haskell.org/package/HTTP) to do HTTP-requests. State of the ecosystem is way better nowadays. If a person have dated experience, e.g. as they played with Hugs or just by going through few chapters of RWH... well.. you can get quite skewed overview. More specific comments. The things you list: - reading files (do you have huge files, streaming *is* easy in Haskell) - calling REST apis (raw `http-client` or thru `servant-client`, nice1) - parse JSON blobs (`aeson`, downside: you'll find all the quirks in the formats) - printing stuff to the terminal (use `prettyprinter` for an additional wow) - extra: doing `bash` stuff is nicer (in the long run) with `shelly` or `turtle`. are where Haskell has good library support. I disagree that Haskell code doing these things would be any harder to understand than Python. You don't need to use `singletons`, `free`, `recursion-schemes` etc there without a reason. The lack of a very specific libraries is a downside. https://kubernetes.io/docs/reference/client-libraries/ lists: Clojure, Go, Java, Node.js, Perl, PHP, Python, Ruby and Scala, which we could summarise as: JVM languages, language the service is written in (Go), and a script collective. If you cannot or don't want to invest into improving library situation then you are out-of-luck. But if you can spend few cycles on making the bindings, then e.g. kubernetes API has `swagger.json`, so with a little of extra information (e.g. swagger 2.0 doesn't tell which fields are nullable) you can autogenerate the library. Soundcloud's version is made that way (and `amazonka` and `gogol`, using different tools though).
If potholes == nulls then I agree completely!
Do your company use Java? Then you can use [eta](http://eta-lang.org/), which has a super-easy interoperability with all the JVM languages, so you can use all their libraries/frameworks
&gt; what is gained from this is dependency inversion. I don't like the term "dependency inversion", but yes. Rather than having the business logic depend upon the database implementation, it now depends on an implementation-agnostic interface (the typeclass), and the implementation *also* depends on that interface. I prefer to think of this as *decoupling*, not "inverting the dependency" - if we were completely inverting the dependency, we would now have a database implementation that depends upon the business rules, but we don't really want that either. We want to decouple the two, having each of them only depend on the interface between them. &gt; &gt; plain old function &gt; I am not sure I did understand this point. The idea is the same: decouple two parts of your codebase by providing an interface between them such that either side has to depend only on the interface. Plain old functions can be such an interface. For example, suppose you have a function that needs to log something; but you don't want that function to know anything about the logging that it doesn't strictly need to know. In good old dependency injection fashion, this means we will have to pass the logger in as context, which in practice boils down to adding a `ReaderT` to our monad stack, or simply passing the logger as an additional argument. So our original function `myAwesomeAlgorithm :: Inputs -&gt; Output` gets an additional parameter: `myAwesomeAlgorithm :: Logger -&gt; Inputs -&gt; Outputs`. But what is a logger, really? Well, we can easily express it as a function that takes a string to log, and returns an action in a suitable Monad, e.g. `IO`. So: `type Logger = String -&gt; IO ()`. So we could write `myAwesomeAlgorithm :: (String -&gt; IO ()) -&gt; Inputs -&gt; Outputs`, except that this means we cannot call our logger inside the function, because we don't have an IO context. So we rewrite our function implementation to be monadic, which isn't usually a big deal, so then we get `myAwesomeAlgorithm :: (String -&gt; IO ()) -&gt; Inputs -&gt; IO Outputs`. But wait, our monadic rewrite doesn't actually use anything that requires `IO`, except for the logger; it really only uses the `Monad` interface. So we can actually change the type signature to `myAwesomeAlgorithm :: Monad m =&gt; (String -&gt; m ()) -&gt; Inputs -&gt; m Outputs`; and it still compiles! What we're saying here is that 1) the function works for all `Monad`s, and doesn't use anything other than the `Monad` interface; 2) you can pass any logger, as long as it runs in the same Monad as the return value you're asking for. So if we use this function in `IO`, we have to pass a logger that matches `IO ()`; if we use it in `Writer String`, we have to pass a logger that matches that; etc. If we pass a logger that works for *all* Monads, e.g. a "black hole" logger like this: `blackHoleLogger = const (return ())`, then we can call our function in any given monad, including `Identity`. The key thing here is that `myAwesomeAlgorithm` doesn't know anything about the logger, and doesn't depend on any specific one; it merely depends on the shared interface `Monad m =&gt; String -&gt; m ()`. And likewise, the logger implementation doesn't depend on the algorithm implementation in any way; we can even write loggers that have internal state, e.g.: mkIORefLogger :: IO ((String -&gt; IO ()), IORef [String]) mkIORefLogger = do ref &lt;- newIORef [] let logger msg = modifyIORef (++ [msg]) ref return (logger, ref) This returns a logger function and the IORef that it will log to. (In a real application, you would probably use `Chan` or `TChan` or even a size-limited Chan implementation instead of `IORef` though). And then you could go: main = do (logger, ref) &lt;- mkIORefLogger output &lt;- myAwesomeAlgorithm logger myInput print output print =&lt;&lt; readIORef ref Following Uncle Bob's argument here, we are deferring the decision "which technology are we going to use for logging" by decoupling it from the code that depends on it; now we can start with an overly simple logger (e.g. just `putStrLn`), but when we decide we need something better (e.g. log to a database, log in a separate thread, use some sort of logging framework, ...), we can casually swap it out, simply by passing a different logger function.
&gt; So, I made do, but I'm finding that my program ends up manipulating and concatenating into large strings, which is causing slowness. Using a difference list might at least help you avoid pain from repeated concatenation.
We started to use it internally as a mock for external depencies. We have a system of tests where we test the system as a black box (just make http requests and make assertion). Ofc we don't want to hit our external dependencies every time a developer runs these tests in their laptop so we mock them. At first we were using nginx + perl scripts to get the mocking behavior but that simply didn't scale to more complex test scenario. Additionally we had all sorts of issues with concurrency and performance. Maybe if we knew perl and nginx better we might have found the issue but no one was an expert in the field. We did, however, have some people who knew Haskell and Scotty. One of our developers also uses xmonad and realized that "you can just feed a Haskell file to docker container, have the file use Scotty and have the container launch it like it would xmonad". And it worked! We managed to mock all our dependencies with ever more complex testing scenario. As to how we convicted the leadership? We just converted one of the examples to using Haskell and showed that the performance of the mock went up (functionality was the same) which meant that our performance tests would not be dampened by our mocks anymore. That was enough to get the go ahead. Another great advantage was that now anyone can just compile the mock before deploying it and testing it, since that was a major productivity hit (you never knew whether the perl script, running nginx in, was correct until you ran it). After that first mock was rewritten the others follow. It fixed all sorts of issues in the other ones (each specific to whatever they were doing) and turned our mocks from horrible, unreadable perl to nice, small Scotty apps with all the power of Haskell behind them. Idk if we will ever use it in production though since that may require corporate cooperation.
As an aside, I gave a talk to #ClojureX last year about recursion schemes. I'm not sure anyone really processed it. https://github.com/JulianBirch/beyond-reduce-presentation 
Monads are possible in any language. But they're useful and convenient in Haskell and just kind of a weird and non-idiomatic thing to do in Clojure.
And that's the point isn't it? We want fast compiled code.
Servant is lovely for this, plus you get the added benefit of being able to auto-generate your client code. I'm using elm on the front and JSON parsing is a bit of a pig to get right but this is now just auto-generated from the Servant API. As a newb this made me very happy!
How compatible is eta with Haskell?
My impression is that the article suggests waiting the time it takes to do a full `ghc --make` on the project would be forbiddingly slow for their deployment. But I'm also not super clear on it, e.g. my [question above](https://www.reddit.com/r/haskell/comments/76zljl/hotswapping_haskell_at_facebook_jon_coens/doj7561/).
This is cool! Would you consider using something like [hs-git](https://github.com/vincenthz/hs-git/) or my [duffer](https://github.com/vaibhavsagar/duffer/blob/master/notebooks/demo.ipynb) to remove the dependency on Git?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [vaibhavsagar/duffer/.../**demo.ipynb** (master ‚Üí 582fd1c)](https://github.com/vaibhavsagar/duffer/blob/582fd1c8bfbbb37e3d260e33fb837fca4f80bda6/notebooks/demo.ipynb) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
&gt; my program ends up manipulating and concatenating into large strings, which is causing slowness Additionally, if you have a somewhat recent version of ByteString, you can use [Data.ByteString.Builder](https://hackage.haskell.org/package/bytestring-0.10.6.0/docs/Data-ByteString-Builder.html) to get _really_ fast concatenation.
Ah, thanks for those links! I first used `gitlib-libgit2` which was faster and nicer (although the API is slightly weird somehow) but it broke my setup for static compilation and I didn't want to deal with that so I rewrote it to use just `git` commands. Of course a pure Haskell solution would be the best! (Here's the diff if anyone's curious: https://github.com/lessrest/restless-git/commit/90a6158d0aebed303ea60f934d8da3d9ec8da8f7) I was pretty time pressured (the whole library is a piece of yak shaving for another project) so I decided to just use `git` commands instead of investigating other libraries... I'm confused about the role of "packfiles" and whether the Haskell reimplementations would be fully compatible... Your duffer repo says that hs-git is more ready for production, so I'm inclined to give porting to hs-git a go, and because my library is really simple it shouldn't be more than a day of work... But I don't really have time until next month probably!
I can't even introduce it to my friends at *University*. 
What are your experiences with Elm? 
Nice proposal, but until something like that gets implemented, the `IsList` instance is just a show stopper for NonEmpty.
It compiles most of hackage with a few exceptions
Is it only for Helsinki or is Berlin fine too?
yikes my team would probably tell me to fuck off if i tried to push technology like that
&gt; Well, this is in the context of "Installing stack". Sorry my mistake. I skimmed the thread and thought you were suggesting no one needs cabal-install.
i have similar experience with clojure, except at night I am not as strong as you and just go cry in my pillow
&gt;Another question was what should we use Haskell for initially. It seems logical to use Haskell first for non-critical, self-contained, small-scale stuff, which for us means automation tools used by developers in their workflows. However I feel it's not an area where the strengths of Haskell lie. These things are kinda imperative in nature. Reading files, calling REST APIs, parse JSON blobs, printing stuff to the terminal. The Haskell code will probably be harder to understand than the same code written in Python, without providing commensurate benefits. Like, it may crash less, but generally it's not a huge deal if it crashes; And there will not be large-scale refactors. I use Haskell and turtle for these types of things. Can you provide a python example which you believe the haskell version would not provide commensurate benefits over? 
It's true, `duffer` is a learning exercise first and a usable library second, but I think the API might be friendlier than `hs-git` (which also doesn't seem to have much documentation). I also think it's smaller and easier to read through than `hs-git` for that reason; if you want to know how e.g. packfiles are implemented you can read the source! Packfiles are a space/network transfer optimisation that compresses many loose objects into a single file and also uses delta compression to further reduce space. Both libraries are able to read indexed packfiles but I don't think either library can write them. If you need that you might be able to periodically do a `git repack -ad`. I can help with implementing support for my library if you'd like, let me know!
I like readable libraries, so that's cool! I'll definitely check it out. Good to know about `git repack`, I was hoping that's how it worked (that you can write unpacked normal stuff and have `git` do whatever packing stuff it wants).
If you have an Arch (I suppose Arch linux, no ?), ghc / stack / everything are available with pacman.
For every incremental update, `ghc --make` is not going to build the whole project, only the relevant script. This requires shipping the codebase and compile on production server on the first deploy, which might be troublesome for their workflow though
We are looking for a person to join our team (which is completely) in Helsinki. We are open to the idea that after incubation-period (few months?) we can geo-split. Please apply, and we'll figure this out.
Ooh, you can express [typing rules](https://github.com/pietervdvn/ALGT2/blob/5d9a648c8b9c9d7358ea17a7ec0a5706b4b9f18b/src/Assets/TestLanguages/STFL.language#L127) and [small step operational semantics](https://github.com/pietervdvn/ALGT2/blob/ee2d2535296657c00dce54cdb9fa1d28cdc95a8c/src/Assets/TestLanguage.language#L47)! You should probably spell that out somewhere more prominently, it really wasn't clear from your post nor your readme how your tool differs from a parser generator like [bison](https://en.wikipedia.org/wiki/GNU_bison). I assume it's possible to execute those rules, so you can typecheck a term in the defined language according to the typing rules, and you can evaluate it using the small step semantics? What is the "functions" section for? Are those terms in the language? Ways to transform terms into other terms, used e.g. to perform a substitution when giving the small steps rules for a beta reduction?
While I don't disagree with what you say, I would still be willing to make a bet that an implementation in Haskell (or OCaml) could prove easier to play/experiment with. Of course that probably won't happen, so it's all speculation.
I just have always used binaries. A matter of preference. It does make me more prone to some careless or rushed errors like downloading the wrong file, haha.
&gt; It can also be as simple as having the crucial functions in App.CrucialLogic accept some functions as arguments which it can use to interact with the database without depending on the database library. Isn't this what typeclasses are for?
Hi! I updated the readme and post with some more info, based on your questions. These are exactly the reason I'm letting 'strangers on the internet' test it! The functions are there for multiple reasons: - Some reductions are expressed more succintly as a function, such as `and` of two booleans. Turning this in a relation involves declaring the relation and adding at least 2 rules, thus involving 7 big lines of code, vs only three now - They serve to inject builtin functions on numbers, such as `plus`, `div`, ... Yes, executing these relations is possible. See the updated instructions in the readme. 
Thank you :).Indeed it really gave me a very good idea. I have checked the code for the PG and it was clear for me with the explanation above. however, it is still a bit ambiguous how the same work for the web. 
me too :( 
Don't you just run `pacman install ghc` and it all just works? That's pretty much how it works on Debian (apart from the `pacman` bit!).
Yeah that happens sometimes.
Did some benchmarking: * The max I could extract on my local machine (4-core, 10 GB RAM) was a ~25% speed-up with `-O1 -j +RTS -A32m -RTS` (12m 18s) vs the base-line of `-O1` (16m) * On a "monster" EC2 instance (16-core, 64 GB RAM) things looked a lot better. A speed-up of ~68% with `-O1 -j +RTS -A128m -n2m -RTS` (4m 23s) vs the base-line of `-O1` (13m 15s) It may be noted that `-O0` is **consistently 3-5x faster** than `-O1` in all the variations of compiler flags that I tried. /u/ElvishJerricco: This doesn't point to the linker being a problem, does it? And there's also the following data: * [**757 sec spent building the library**](https://gist.github.com/saurabhnanda/c8f8654a7f29c1adb753b357b897b5f3#file-building-library-01-txt-L28) * [**08 sec spent building the executable**](https://gist.github.com/saurabhnanda/c8f8654a7f29c1adb753b357b897b5f3#file-building-exe-txt-L25) * [**16 sec spent linking the executable**](https://gist.github.com/saurabhnanda/c8f8654a7f29c1adb753b357b897b5f3#file-linking-executable-hs-L25) -- can someone please confirm that this is correct? I don't understand why GHC would be involving in the linking process resulting in another set of stats from `-sstderr` RTS flag.
Could you please look at https://www.reddit.com/r/haskell/comments/76zljl/hotswapping_haskell_at_facebook_jon_coens/dojghuf/ and share your comments?
Great, another position in Finland :). I just told my wife that the position in relex solutions was a once in a lifetime opportunity.
I don't think so. Basically, what you want to do is pass some behavior from one context to another. If that behavior is just one function, then you'll just pass the function‚Äîthat's the beauty of functional programming, right? If you have some more, you'll want to bundle them up somehow, so that the behaviors form some coherent combined concept instead of just being a bunch of different things... So you can do that with a record very easily. You can also do it with a typeclass. What's the difference? I don't actually have an articulated view on the difference, so let me try to think about it, and see if you agree. Is it that with typeclasses you avoid passing the parameter, so that it's more nicely implicit? Well, you still have to mention it in all your type signatures, so it's kind of like the implicit parameters extension, or like a more convenient reader monad... How about polymorphism? We have polymorphic records, and it's even better than typeclasses because you get into trouble with type inference when you have more than one type parameter to a typeclass, and you have to use a language extension... I actually find it hard to explain what characterizes the situation of wanting to make a typeclass. They seem to be mostly useful when you have a multiplicity of types that all behave in similar ways enough to deserve an overloaded set of functions that work with all of them‚Äîand the pinnacle examples are truly abstract things like `Functor`, but you also have convenient things like `ToJSON`. Consider that you only have at most one typeclass instance per type, and so if you want to have several behaviors with the same underlying type, you make `newtype` wrappers (like the `First` and `Last` newtypes that let you use `Maybe` as a `Monoid`)... Well, I'm just kind of rambling, so someone please contribute a concise explanation of why typeclasses exist and what to use them for!
If the only manipulation you need to speed up is concatenation, then you can probably just use a `DList` type of solution, which only takes a few lines to implement. `String` concatenation is slow because `String` is a linked list, so the idea of `DList` is to bake concatenating into the data type. newtype DList a = DList { unDList :: [a] -&gt; [a] } instance Monoid (DList a) where mempty = DList id DList f `mappend` DList g = DList (f . g) toList :: DList a -&gt; [a] toList (DList f) = f [] singleton :: a -&gt; DList a singleton x = DList (x:) fromList :: [a] -&gt; DList a fromList xs = DList $ \z -&gt; foldr (:) z xs {-# INLINE fromList #-} The difference between a `DList` and a regular list is in the terminator. In regular lists, it's just `[]`. A `DList` represents a list in the same way, except it is a function that uses its argument in place of the `[]` terminator. So if you want to convert a `DList` to a list, just pass `[]` as the argument to get it to use `[]` as the terminator. The advantage of this is in the `mappend` instance. To concatenate two lists, all we have to do is compose these two functions. When the left hand side evaluates its list, it will take the result of the right and one and place that at the end where it would normally put `[]`, thus resulting in a concatenated string. This solves the problem with `++` being `O(n)` time complexity. But it does introduce a new problem: `fromList` is now the part that is `O(n)`. It has to fold down the entire list to rebuild it with a different terminator. So if literally every concatenation you do reduces to the form `fromList xs `mappend` ys`, then you may have the same problem. But if you're concatenating things that were themselves a result of concatenation, then you'll start to see performance improvements. Now, the thing that makes all of this tricky is fusion. I won't get too deep into it, as it's definitely not a beginner topic. But `fromList "constant string"` is extremely likely to not have that `O(n)` time complexity. Basically, if you compile with optimizations turned on, GHC can find certain patterns in list functions and simplify them to a constant time cost. For example, `map f $ map g $ map h xs` is simplified to `map (f . g . h) xs`, eliminating the `O(n)` traversal and allocation for each intermediate `map`. In fact, even `++` is often simplifiable. But recursion often gets in the way of this analysis, so it's not hard to break it and prevent `++` from being simplified away. But `fromList` should literally always be simplified if called on constant lists/strings, so that cost is gone. The problem then remains with lists that aren't constants that you get from other libraries, like `getLine`. Ultimately, if you want to get around this cost, you're just going to have to write a new version of `getLine` that concatenates a bunch of `singleton`s.
Does `-j` require a number argument to be passed to it? I always give it one, indicating the number of threads to use. Anyway, it certainly would not seem to be a linker issue, if the timing you're reporting is correct. Not sure how to verify. So that is indeed a very surprising amount of time building the library. What happens if you do everything yourself with `ghc --make`? I wonder if building it all as one component, rather than building an intermediate library component, would help at all (it really shouldn't but it's a good experiment). And just to confirm, all these tests are just with changing a single module that's only near `Main` in the import graph, right? So you're only rebuilding a few modules? These are just some very unusual numbers. If you're not already using GHC 8.2, could you give that a try, considering Simon Marlow's other comments in this thread?
I did this a few years ago. https://engineering.imvu.com/2014/03/24/what-its-like-to-use-haskell/ I started the conversation by putting out a full replacement of a live production service. The service we picked was not mission critical (it could fail with minimal impact to customers) but it drove quite a lot of load. We turned a few heads when we showed that we could serve 10x as much traffic on a tenth of the hardware. :) Longterm, I think one of the reasons why it worked out was because we weren't afraid to write own network protocol parsers or FFI bindings. We rolled a bunch of stuff from scratch that we wouldn't have needed to if we'd played it safe. I agree that build tooling isn't the best place for Haskell. Haskell binaries are really fat and startup time isn't so hot.
I find there are 2 types of "External Interfaces": * The ones that are being called from Core. (e.g. Databases, Caches, WebService) * The ones that call the Core. (e.g. Web, CLI, Desktop GUI) For the former, we decouple the Core by defining typeclasses that will later be implemented by the actual things, like you have seen in the parent comment. For the latter, nothing special needs to be done for decoupling the Core because the Core never needs to know about them in the first place! From [Clean Architecture by Uncle Bob](https://8thlight.com/blog/uncle-bob/2012/08/13/the-clean-architecture.html): &gt; The overriding rule that makes this architecture work is The Dependency Rule. *This rule says that source code dependencies can only point inwards. Nothing in an inner circle can know anything at all about something in an outer circle.* In particular, the name of something declared in an outer circle must not be mentioned by the code in an inner circle. That includes, functions, classes. variables, or any other named software entity. As you can see, we still adhere to the dependency rule as the Core / "inner circle" never use anything from the external interface / "outer circle".
Sorry, meant to reply to you. See my other comment.
We choose the Applicative to be consistent with the Monad by law. Num has no law relating it to either one.
Yeah, I was thinking about it inside-out.
I don't necessarily disagree either- my point is just that there are other factors in play as well. Incidentally, the first implementation of rustc was written in OCaml. :)
Have you tried to force the type of `foo` to something like `Int`. That's stupid, but `Num` have no associativity guarantee. Force a type such as `Int` and perhaps you'll get something better. Note: `Float` is not associative by default if we follow the IEEE standard. That's why there is `fast-math` in GCC.
Well type classes are for ad hoc polymorphism. So from what I understand they allow functions to operate over multiple different types of data. That is indeed not what we want. Like you say we can't define multiple typeclass instances on the same type. So instead of having functions that operate over multiple different types of data we want functions that operate over different implementations. So my conclusion is that we should use functions or records of functions as arguments instead of typeclasses to make functions that can operate over different implementations.
why does nginx-haskell-module take the method of also being a haskell compiler? Can it not simply call out to pre-build haskell binaries?
Actually I tried, and you are right, even with `Int#`, ghc is not able to fold the constant. GCC can do it for C, but only for int by default, and for float by enabling `-ffast-math`.
Finishing implementing this is on my todo list. If someone wants to pick it up where I left it last year, it is all here: https://phabricator.haskell.org/D2858
It can both run compiler and load compiled library via dlopen(). The first method was implemented on the earlier phase of the project, and it can be used for very simple tasks when fast editing of Haskell code in configuration is enough. Loading libraries is, of course, must be preferred for more complex tasks.
&gt; Note to self: don't attempt to code after a long day of teaching. Or do, that way the rest of us can learn something ;)
thanks!
Using the terms from my example, the demo program and .so are separate projects, so the full rebuild of the .so doesn't touch the build of the program. If the demo program is small relative to the .so then you're correct that it wouldn't make much of a difference. If the demo program is an order of magnitude larger than the .so, then just rebuilding the .so is significantly shorter. When shipping things around the world, total bandwidth used is correlated with how long it takes to do an update. We get more wins from speeding up our build, but still see noticeable effects by shipping smaller things.
I've never really fooled with pacman, I'd say that's how it works. For a while, i was really into Slackware, I've just always installed binaries.
I've been organising this weekly FP practice, which is language agnostic, but been mostly using Haskell, as I think it makes it easier to focus on FP rather than the language syntax. This being said, there's a handful of people that rotate in attending to these sessions every week. The goal, at least for now, is not to introduce Haskell in commercial projects, but to get people started both on Haskell and FP, including myself.
Underdeveloped language. It should really have typeclasses, but it seems to be stagnating there. PureScript seems to have better direction. Lovely community though. I watch Elm-targeted talks sometimes. 
From the GHC docs, -jN means "When compiling with --make, compile ‚ü®N‚ü© modules in parallel." Setting N to 1 means you remove all parallelism from builds.
Why it isn't possible to just use rewrite rule like `map` and `foldr` have?
when I was still committed to making my relationship with clojure work, I tried real hard to get monads to be convenient in it. The lack of a type system made it worse than useless :(
As far as I know, rewrite rules doesn't know about the term they rewrite. So `1 + 2`, `x + 2` or `x + y` are seen as the same pattern `x + y`. Constant folding rules need to know which part is constant.
What would that rewrite rule look like? Rewrite rules really only do syntactic substitutions.
A rewrite rule doesn't know enough to deal with things like (1 + a) + (2 + b) = 3 + a + b 
I think there's a version where ghci could work too. When first looking at this we found overall perf to be a problem and steered toward straight compiled code. Being able to update cleanly while our server handles many requests in-flight at the same time didn't play out. I've forgotten most of the details on this track at this point, though.
You're probably right about being explicit wrt. to identifier searchability, but by god this hurts to look at. Must I sacrifice my beautiful code to such callous disregard for aesthetics!? The engineer in me says yes but it makes me sad :(
Nice work. Did you observe a significant improvement on performances on a real code base ? I have the feeling that, except in case of a lot of inlining, constant folding does not change a lot, so I'm not sure that constant folding coming from associativity will result in a significant performance boost.
Right, but what happens when you do `-j` with no number, as it would seem /u/saurabhnanda did in the comment above? Does that infer a large `N`, or does it default to 1?
My main problem with manually writing explicit imports is that it's another interruption in my process. It's just yet another thing I have to do to write anything. I've found it much more convenient to just have a local hoogle server, and to have a key combo for project wide grep. I have to search much less often than I have to import. But having a tool that does all the import management for you would go a long way toward making that the more convenient way. PureScript's thing is very nice, which automatically adds things to your import list *as you're typing*, using the local dev environment's info. I think this is the bare minimum to keeping this sort of thing out of the way enough to prefer it.
I haven't tested it on a real code base yet.
In theory you could use something like (fromInteger x + fromInteger y) = fromInteger (x + y) :: Int for some of the cases. The problem is making it handle a sufficiently large number of patterns. Without an over-arching theory it basically just becomes a game of whackamole.
I keep trying to get them to use Haskell2010 but my coworkers, the firebrands, keep using MPTC and other heresy that isn't in the report.
Many Thanks :)
I mean without typeclasses you have zero ad boc polymorphism and so anything of the form `type -&gt; value` is impossible. So no `print` or `&gt;&gt;=` or `+` or anything. 
In other languages that have explicit import lists, we actually look pretty dumb having to type all this stuff out. Java people are laughing at us, as they regularly have 50+ lines of imports and don't even think about it - the editor just does it all for them. Yes, I'm actually tipping my hat towards the Java community.
&gt; Is it that with typeclasses you avoid passing the parameter, so that it's more nicely implicit? Well, you still have to mention it in all your type signatures, so it's kind of like the implicit parameters extension, or like a more convenient reader monad... In practice, if you are to use record, it will involve more typing. Let me examples. Here's the original implementation using typeclass approach: login :: (UserRepo m, TokenRepo m) -&gt; Auth -&gt; ExceptT UserError m User -- 1 login auth = do (uId, user) &lt;- (lift $ findUserByAuth auth) `orThrow` UserErrorBadAuth auth token &lt;- lift $ generateToken tokenRepo uId return $ user { userToken = token } Notice that we need to write the typeclass once: at type signature. And, here's what you would write if you pass the record as function parameter: ``` login :: (Monad m) =&gt; UserRepo -&gt; TokenRepo -&gt; Auth -&gt; ExceptT UserError m User -- 1 login userRepo tokenRepo auth = do -- 2 (uId, user) &lt;- (lift $ findUserByAuth userRepo auth) `orThrow` UserErrorBadAuth auth -- 3 (for userRepo) token &lt;- lift $ generateToken tokenRepo uId -- 3 (for tokenRepo) return $ user { userToken = token } ``` Notice that we need to write each repo 3 times: type signature, function declaration, and caller. Other downsides: * if this function is to call another function that requires the same dependency, you need to explicitly pass it into that function. * now what if I need to talk to 5 repos? Parameter explosion + making sure the parameters are in the correct location becomes more difficult * now what if I need to add another repo? Update every call site. Ok, so what if we are to use Reader? We would come up with something like this: ``` type HasR r r' m = (Has (r m) r', MonadReader r' m) login :: (HasR UserRepo r m, HasR TokenRepo r m) =&gt; Auth -&gt; ExceptT UserError m User -- 1 login auth = do userRepo &lt;- asks getter -- 2 (for userRepo) tokenRepo &lt;- asks getter -- 2 (for tokenRepo) (uId, user) &lt;- (lift $ findUserByAuth userRepo auth) `orThrow` UserErrorBadAuth auth -- 3 (for userRepo) token &lt;- lift $ generateToken tokenRepo uId -- 3 (for tokenRepo) return $ user { userToken = token } ``` We still repeat each repo 3 times, but this approach results in more code to write. This approach, however, fixes some downsides of the former approach. Ok, this might be a small boilerplate addition, but if you have lots of functions like that, it would get very repetitive soon. And as you can see, the typeclass approach gives you the least problem. With that, I think typeclass approach is best for this task.
Isn't handling such cases NP complete in the general case due to commutative pattern matching being NP complete?
I really love the feature in Intellij IDEA that adds and removes imports on-the-fly. I'd like to implement this feature for `importify` some day..
Thanks for the explanation!
Strictness analyzer should unbox `Int` and produce a normal set of addition instructions. Seems like the LLVM backend should be able to optimize that stuff away.
How does it compare to [Ott](https://www.cl.cam.ac.uk/~pes20/ott/) ?
Well, consider: mathy :: Num a -&gt; a -&gt; a -&gt; a mathy Num {..} x y = x + y * 2 data Num a = { (+) :: a -&gt; a -&gt; a , (*) :: a -&gt; a -&gt; a } intNum :: Num Int intNum = Num { a + b = _, a * b = _ } x :: Int x = mathy intNum 1 2 It's just that you have to pass the "dictionary" around instead of it having been automatically resolved from the types. I guess the most wonderful benefit of typeclasses is that they can autoresolve even complicated types, like `Show (Maybe [Int])`, which with a `data` scheme like above would need to be composed manually from different named implementations. And the most fun to have with typeclasses is writing inferred instances like `instance (Monoid a, Monoid b) =&gt; Monoid (a, b)`. It seems like when your typeclass is amenable to that type of thing, then it really deserves to be a typeclass! As it happens, I guess most consider the `Num` class to be fairly bad, because it's not a well-defined mathematical object (like `Ring` or whatever). Still, there are some inferred instances for `Num`, like `RealFloat a =&gt; Num (Complex a)` which is pretty nice... Hmm, I think I have to read this article to more fully understand all this: https://www.schoolofhaskell.com/user/thoughtpolice/using-reflection
Reminds of this tool implemented by one student in his master thesis which allows you create complete type inference algorithm for dependent types by specifying only reduction rules: https://github.com/esengie/fpl-exploration-tool
I thiiiiink I would write it as login :: Monad m =&gt; UserRepo m -&gt; TokenRepo m -&gt; ... login UserRepo{..} TokenRepo{..} auth = do (uid, user) &lt;- lift $ findUserByAuth auth ... ... So I still use the fact that the `Monad m` will carry the necessary implicit state for the user repo and token repo. I would then have e.g. openUserRepo :: HostName -&gt; Port -&gt; UserRepo (StateT AppState IO) openUserRepo host port = UserRepo { findUserByAuth auth = get &gt;&gt;= ... , ... } One bigger advantage I can see with typeclasses there is that the `UserRepo` and `TokenRepo` can be polymorphic by just requiring e.g. `MonadState UserRepoSession m` ... or could we maybe do something like this? openUserRepo :: HostName -&gt; Port -&gt; (forall m. MonadState UserSession =&gt; UserRepo (StateT AppState IO)) openUserRepo host port = UserRepo { findUserByAuth auth = get &gt;&gt;= ... , ... } 
Instead of translating to Coq/Isabelle/..., the declaration is interpreted. This makes it easier for new users to get started, as no such language is needed. And no other boilerplate is needed (e.g. this will be translated to Int in Coq, to Int32 in Isabelle, ...) A pretty printer to Latex is missing in ALGT though
Cool! The first version of ALGT was for my master thesis as well. However, that code became quite a mess, so I decided to create a new, clean edition.
We only need a very simple form of pattern where you run through and grab all the variables and constants. It is more complicated than what RULES can support, but not full blown AC pattern matching
What about cases like: (1 + a) + (d + (c + (6 + b) * 3) * 5) Being replaced with 91 + a + (d + (c + b * 3) * 5) Isn't it possible to come up with cases that will run into NP complete behavior / just not be detected.
We don't use JVM a lot. Yes I have been following eta. My understanding is that it's still pre-release? Do you have experience with it? Are there many rough edges?
That's an interesting use case! We do run nginx and occasionally we need to extend it. The default is to write some lua code. I'll definitely look into nginx-haskell-module. Thanks!
Is there a way I could be notified about when that gets added? E.g. subscribe to a github issue? I'd like to try that once it works.
Yeah I've been trying to generate the haskell bindings for Kubernetes in my spare time. The swagger codegen, which SoundCloud used, doesn't generate very usable code. The models generated requires all the fields in a JSON object to be set or else it fails to parse, whereas the reality is that a lot of these fields are optional. Gogol is better in that sense, where all the record fields are wrapped in Maybe. I'll need to study how it does it.
For example simple manipulation of JSON-encoded data. In Python you can decode the data into a dict and manipulate the dict without ceremony. Sure it's less safe, but in many cases it's good enough. Anyone with three months of programming experience can understand it. With Haskell, I would need to explain to people lenses, parser combinators, applicative functors, monads, etc.
Haskell is great for REST APIs, much better than other languages including python (see servant / servant-client). Only thing is small stuff is pretty easy in any language, so it's not going to "wow" anyone. Still... baby steps.
This was just with regards to addition. Multiplication adds a whole other story. =) Finding a minimal result in a larger calculus in a 'work-safe' manner where it never increases the amount of work done is quite difficult. I'd be quite surprised if it was _only_ NP, as I'd expect Œ£_2^p at least.
&gt; The file we care about gets placed in ghc-hotswap-so/dist/build/ with a filename prefix like HSghc-hotswap-so and extension .o. The path to this file is important (or copy it to a nicer location for yourself) as you'll need it later. is there a `cabal` option or something that returns the object file path from a module name or file path? like cabal build --dry-run Project.Types ... ghc-hotswap-so/dist/build/&lt;etc&gt; ... to avoid manual configuration. 
This is cool! Tried on my small projects, works fine for me. Thanks for this tool!
There's no such open issue. But you can create one :) _on-the-fly_ refactoring feature won't be implemented quickly because it's not so trivial. And first version probably will only remove redundant imports on-the-fly. But don't worry, once it's implemented the post on Reddit will appear shortly after that.
&gt; Would it be a good idea to automatically run "stack test" on my library for every nightly Stack resolver? Sure that's a decent idea. I usually just update my stackage snapshots once in a while and I go to the stackage website to see what changed. For example: https://www.stackage.org/diff/nightly-2017-10-17/nightly-2017-10-18 &gt; Can products like CircleCI help me do such automatic scheduling? My understanding is that they can't - that they can only run a pre-defined script once for each new commit. So maybe a custom Jenkins or cronjob is the way to go? I'm not sure I understand the trouble here. Yes, CircleCI runs scripts. Won't Jenkins or a cron job run a script too? You can make a script that gets the latest stackage nightly resolver and uses that as the `--resolver` argument to `stack`. You would probably need to remove upper version bounds from your `.cabal` file using something like `sed` though. &gt; My goal is to write a library that "just works" for almost everyone, especially people using Stack, and such that nobody will ever have to ask me to bump a version for some dependency. It should be automated as much as possible. One fairly common thing folks do is leave upper version bounds off of dependencies. This can be a controversial thing to do, and I won't rehash all the arguments for or against it here, but just know that it is an option. For what it's worth I generally leave upper version bounds off of dependencies in my packages.
Have you seen Steele recent (re) talk about language specification language ? (CMS)
You are welcome! Please report bugs on github if you'll try this.
Thanks for your feedback! I really glad that you find this project useful :)
Added: https://github.com/serokell/importify/issues/64
Yes you are writing terrible code. Overlapping instances are almost always a design error. Anyway the error is easy to explain. You have two instances `B a` and `B [b]`. Lists of b are just another normal data type so you can substitute that into the earlier instance with the type variable `a` being `[b]`. At this stage contexts are ignored. Overlapping instances will resolve this issue because one of them is more specific than the other. 
There's already a library for this (I think) https://hackage.haskell.org/package/atom
As you are `stack` user, the most easy way for you is to add your package to Stackage. Then you'll be pinged whenever your package bounds are too restrictive. If you want to be proactive, you can add [`packdeps`](http://hackage.haskell.org/package/packdeps) task to your CI. Travis supports periodic jobs automatically, Circle CI has https://circleci.com/docs/1.0/nightly-builds/ Or you can subscribe to the feed on the website: e.g. http://packdeps.haskellers.com/feed?needle=vector And the last: how to automate bound handling / testing. If your CI uses `cabal new-build`, you could add `cabal new-build --allow-newer yourpkg:*` which will try to build your package with its (but only its!) dependencies' upper bounds omitted. I guess you can do that with `stack`, but it will be more complicated (you'll probably rely on `cabal` to construct your a snapshot). The missing part is how to automate "there's new version of a dependency", "my CI says it's ok to use", "alter the source", "make a revision on Hackage". With `Cabal-2.2` we will have working parse-print round trip (though ruining the formatting) of `.cabal` files, so you could hack something together (something more robust then fancy regexes, it is).
I would say `aeson` for simplicity. Almost every language on the earth has a JSON parser now and even if your app falls out of favor / becomes unmaingained your data is still safe. You can even edit it inside a normal text editor. 
no scientist, just, haskell handed me a thread and I keep pulling :)
Thanks for the reply. I'm definitely not a Haskell beginner, but I'm would never claim to be an expert either. Don't worry, this code is never going anywhere, I was just throwing together a quick test for something and was hoping this would work, but I can easily refactor around it. Once I started looking at it in more depth I was just curious as to why I got the error; thanks for pointing at that the instance lookup happens before context checking, which I didn't realize. (Is there a reason for that? Would checking contexts first ever cause type checking to fail where it doesn't now? I assume that as soon as GHC picks the instance it wants to use for a given type, it has to immediately check the context on that instance anyways to prevent errors down the line.)
As a Hackage Trustee I have to kindly point out that old versions of your packages bitrot: - https://matrix.hackage.haskell.org/package/stratosphere - https://matrix.hackage.haskell.org/package/eventful-core - https://matrix.hackage.haskell.org/package/oanda-rest-api Please make revisions to correct bounds on older releases.
Sure. Play with features in your spare time and have fun :) I apologize if I was too harsh. I‚Äôm actually not entirely sure why GHC doesn‚Äôt look at the instance contexts. But at least this is documented behavior: https://downloads.haskell.org/~ghc/8.2.1/docs/html/users_guide/glasgow_exts.html#instance-resolution
Yes, with custom generator you can save a lot of trouble from tweaking output by hand. I haven't looked at Kubernetes `swagger.json`. It might be that all `required` are listed, and the rest can be wrapped in `Maybe`. As for Gogol, it has custom generator, see https://github.com/brendanhay/gogol/tree/develop/gen
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [brendanhay/gogol/.../**gen** (develop ‚Üí 896ef34)](https://github.com/brendanhay/gogol/tree/896ef3472ce09116f13d0cda3b8134b28e833a19/gen) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dok00uo.)^.
TBH it does not even bother to replace `map id` with `mapId`.
&gt; I don't get it. ghci supports compiling to object code instead of binary code right? By "GHCi-as-a-service" here we're referring to interpreting source code. You're quite right that GHCi can load object code, and in fact that's more or less exactly what we're doing - but if you're only loading object code, you don't actually need the compiler, so we bypass GHCi and use the linker directly. What we don't want to do is compile the code on every machine though (which was your second suggestion, if I'm understanding correctly). We did actually consider doing that, but the tradeoffs come out worse. Yes you'd save a bit of network bandwidth because shipping source code changes is cheaper than shipping compiled object code, but that would be more than offset by the CPU cost of compiling the same code on thousands of machines. Also these machines are pretty busy, we can't afford to tie a few cores up for a while compiling stuff, not to mention the fact that you'd have to have a full compiler toolchain and source tree checkout available on every production box...
This is not really fusion anymore. `map` uses fusion under the hood so it becomes some combination of `foldr/build`. Can‚Äôt remember what the exact rule is but I *think* looking at the rules you should be able to see it isn‚Äôt able to eliminate rules. Now why doesn‚Äôt it replace `map id` with `id` I‚Äôm not sure. I suppose perhaps no one wrote this rule?
Do you have a link? My google-fu is failing me on that one.
But why do we need hard coded rule for such a trivial transformation. I think it should be trivial to automatically infer it. Obviously beta reduction and eta reduction are not enough but there should be some *reduction* that can do it.
Yes I had already downloaded the command line tools and used them accordingly. In fairness it does say at the bottom that "some packages depend on first installing specific non-Haskell libraries", I just wish that they mentioned that the IDE can't access them. It seems the sandboxing aspect really limits what you can do as far as IO is concerned. So much potential but I guess I might as well get to know the command line. Maybe the IDE was a crutch anyways. 
Even if we stick to things with nice normal forms like foldr/build fusion, it works well because the tail continuation is invoked exactly once. When you go looking for a more general fusion form, you find, it, but its not as powerful and effective as foldr/build (or unfoldr/destroy) (or stream fusion) all of which benefit from the 'stream'-like structure of the fold. cata-fusion and the like rarely apply. Now moving beyond that and looking for general purpose transformations gets even harder. There was a bunch of work on hylo fusion back in the day where the compiler would try to auto-optimize into a form suitable for fusion, but it never really went anywhere. If I had to arm-chair quarterback why, I'd say `hylo` is already Turing complete so as a 'normal form' it hasn't normalized much.
Sure no problem, here's a slightly shortened version of the original talk https://www.youtube.com/watch?v=dCuZkaaou0Q (this is at the last clojure conj) He did a first version somewhere, can't find it, maybe on vimeo, anyway it's still dense.
While specialization of polymorphic functions potentially enables a lot of optimizations, there are also drawbacks, e.g. 1. longer compile times and sometimes larger binaries as each specialization needs to be compiled separately, which potentially creates exponentially more code (allthough this happens rarely in practice); 2. in some cases worse performance, as the additional code needs to be stored in the cache (not sure if this is a problem in practice); 3. polymorphic recursion would not be possible, as it would lead to creating an infinite amount of specializations.
Not that I know of. Might be a useful feature to send upstream.
No worries, I didn't think it was harsh. I realized that my original question was framed without any context (no pun intended), so I thought I would say that I was doing it purely out of curiosity :P. I was digging through all of the OverlappingInstances documentation and I never even thought to check plain old instance resolution, which clearly gives the reason this fails. Thanks for the insight!
the link to the lambda calculus example (from the main github page, not the one in the post) seems to be broken
Hey! I'm a confused by you urging me to create an issue and then closing said issue right away once one is created. Care to elaborate why you closed it?
So, you wouldn't compare it to haskell. If that's the case, you're on the same level as some of my of my professors and colleagues - if they compare haskell to Java, in their opinion haskell is inferior. 
You'd need some kind of inductive reasoning: The compiler must infer that `map id = mapId` for every possible input. That's kind of hard to do, especially for a compiler with compile time constraints like GHC. Now, as a compiler dev you could try to detect some of those special cases. Here, the key insight is that `mapId [] = map id []` and that `mapId (x:xs) = map id (x:xs)`, e.g. the simplest inductive case split possible. But where do you stop? There's always some 'obvious' identity you want the compiler to recognize and after a while you end up with an SMT solver baked into your compiler. Which actually sounds quite interesting, given that there are already uses in type systems.
&gt;For example simple manipulation of JSON-encoded data. In Python you can decode the data into a dict and manipulate the dict without ceremony. Sure it's less safe, but in many cases it's good enough. Anyone with three months of programming experience can understand it. With Haskell, I would need to explain to people lenses, parser combinators, applicative functors, monads, etc. Amount of things you need to learn is a different metric isn't it? In regards to simple json manipulation, what do you think of: https://github.com/codygman/concise-json-parsing-in-haskell You'll likely correctly( I think) point out that adding lenses to the equation makes it harder to teach. 
What do you mean? Or rather, what exact problem cannot be solved (or not easily) with servant? The image story in particular is in my opinion pretty nice (see [this post](https://haskell-servant.github.io/posts/2015-08-05-content-types.html)). 
The versions that are failing are definitely old versions that I no longer support. Also, what's wrong with `eventful-core`? The only problem I see is versions older than GHC 8.2 don't compile with GHC 8.2. To be blunt, if all I would have gained from adding version bounds is that some of the more than 1 year old versions of my packages would compile with the bleeding edge versions of dependencies from today, I don't think that's worth the extra effort of dealing with raising the upper bounds and having to manually futz around with them every time a dependency makes a major release. I'd rather my older, unsupported versions bitrot than the alternative option where people can't use my newer versions with up-to-date dependencies because I put an overly-cautious upper bound. There is a distinct trade-off here between older versions of my packages working with new dependencies, and newer versions of my packages working with new dependencies, and I don't see the benefit the upper bounds would have brought me. I'd love to hear your thoughts, maybe I'm missing something big here.
ZuriHac is and always has been free, but depending on where you're from Switzerland itself can appear quite pricy. My hotel was comparatively cheap with about 130 franks per night, we paid 50 franks for parking during the conference, and food under 10 franks per meal is hard to get by. 
Well the goal is to introduce Haskell gently. For a lot of folks this would be their first encounter with Haskell. I don't want their first experience to be overwhelming, thus reinforcing the idea that haskell is not "practical."
How does it compare to [K Framework](http://www.kframework.org/index.php/Main_Page)?
This is part of Haskell's design, not just a ghc choice. That design decision was made because the type class system is open. The fact that you haven't found a particular instance yet doesn't imply that it won't exist in the next bit of code you compile. So you can never rely on the current absence of an instance to decide that there is no such instance. Well, that's not quite true. Whole-program compilation could work that way. But Haskell's specification was chosen to not require whole-program compilation. 
&gt; Isn't this what typeclasses are for? Maybe. I think you use a typeclass when instances are unique, or when infix operators provide a lot of clarity. Otherwise just pass around a record.
Fair enough, I like the idea that the state is there to be edited by hand, I‚Äôm not so keen on non-transparently managing files on disk.
At my company we also prefer explicit imports. We write/update them by hand so far (looking forward to checking out Importify) but we wrote and use [Longboye](https://github.com/SuperpowersCorp/longboye) to to format them in a more pleasing (at least to us) way. You can see an example of it in Longboye itself which of course [eats its own dogfood](https://github.com/SuperpowersCorp/longboye/blob/master/src/Longboye/Imports.hs). It's not to everyone's taste at first but it's quite nice once you get used to it (IMHO anyway).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [SuperpowersCorp/longboye/.../**Imports.hs** (master ‚Üí 3fb2faf)](https://github.com/SuperpowersCorp/longboye/blob/3fb2fafc2f08ad7db36185bc157a92739e0e8a14/src/Longboye/Imports.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dokd20h.)^.
&gt; My main problem with manually writing explicit imports is that it's another interruption in my process. My way of dealing with this interruption is to write the code as if everything I need is already imported and then fix up the imports as I'm fixing type errors (repeatedly typing `:r` in `cabal new-repl` and fix things up). Used this way, I feel like it's manageable enough to be worth it.
No. It turns out not only were they already familiar with it, but they were using it. So I failed to introduce it, for it had already been introduced :)
packdeps.haskellers.com has RSS feeds to notify you when your package needs testing with new version. For example, this is mine: https://packdeps.haskellers.com/feed?needle=bss%40iguanasuicide.net (My only package is crypsy-api and it's not maintained because Crypsy shutdown. If anyone knows how to remove my email from the package, I'd love to know how.)
Hey since you are experimenting and learning things, why not abstract out the logic of dealing with persistence of JSON-enabled structures and put together a small library? Maybe even release on Hackage? It would probably take one hundred lines max to implement a small JSON persistence library that: * deals with reading and writing files * auto-save to disk whenever your app changes data * auto-reload from disk whenever file changed from outside app (I‚Äôve never done this actually but I think with `hinotify` it shouldn‚Äôt be hard at all) or maybe reload upon SIGHUP/SIGUSR1 * offer easy undo/rollback With maybe another hundred lines, you can even implement versioning and migration. 
Lucky you!
Check this out: https://packdeps.haskellers.com e.g.: https://packdeps.haskellers.com/feed?needle=stack Not sure if there's a way to use it on non-published packages.
A friendly reminder: don't attempt to code after a long day of teaching.
One thing to consider is that optimizations like this while seemingly easy are also quite low reward. How often do people put `map id` or similar in their code, and remember it's not worth converting something like `map (+ functionArg)` to `if functionArg == 0 then id else map (+ functionArg)`. So it really has to be a statically known `map id`.
I don't like navigating the politics. I try to take the advice others gave me, find a place that Haskell has already landed at, and where the porting is over with.
To me its just about what ends up being more useful. I would personally not require `+` be commutative (although a subclass that requires commutativity / invertibility etc. sounds great). Because then you get: -- [a] (+) = (++) (*) = liftA2 (+) -- Parser a (+) = (&lt;|&gt;) (*) = liftA2 (+) And basically any similar idea (concatenation / biased choice) where `+` can't be commutative. And you still get the huge array of instances where you do have commutative: -- Any non-floating-point number type -- Matrices -- Fixed sized containers (a, b) + (c, d) = (a + c, b + d) (a, b) * (c, d) = (a * c, b * d) -- Functions / ZipList / similar applicatives (+) = liftA2 (+) (*) = liftA2 (*) -- Set (+) = union (*) = intersection and so on. Honestly such a ring-like hierarchy in base would make me so happy. So many operations on common data structures fit into a group and ring-like hierarchy. So something like: (+) :: Semigroup a =&gt; a -&gt; a -&gt; a (*) :: Seminearring a =&gt; a -&gt; a -&gt; a zero :: Monoid a =&gt; a one :: Semiring a =&gt; a negate :: Group a =&gt; a -&gt; a inverse :: DivisionRing a =&gt; a -&gt; a Or perhaps it would be worth separating the multiplicative hierarchy from the additive one, and then providing ring-like classes that join them, depends if there are many situations where we want `*` but don't have an appropriate `+`. Many other useful functions can be derived directly from above such as alternative multiplication operators that represent repeated self addition, so `2 *. [a, b] = [a, b, a, b]`, `(3 :: Int) *. (2.5 :: Float) = 5.0`. Extending this to exponentiation would allow you to model the regex `(a|b){10}` as exponentiation: `("a" + "b") ^ 10`, which is pretty cool IMO. Then of course there are the usual sum and product and `-` and `/` and so on.
I have around 135k lines across 700 modules, and builds are fast for me... on a laptop, 0.5s for a no op, 4s for a top level module, 38s to add a function to a low level module, and I just timed 226s for a complete debug build. The opt build is 660s. My usual rebuild retry cycle is basically 0 though, because I use ghci. When I can't use ghci, 10s-20s is about usual for editing something mid-level. Of course who knows why exactly there is such a difference, but one reason might be that I use shake, not ghc --make. Also interactive development can remind me when dependencies seem to be too much, and motivate me to fix it... though to be honest my discipline is not good here. It would be nice to have a tool to find split candidates in the import graph, such as a single edge that drags in a forest. Shake gives me a visualization of the dependencies, but the few times I've tried it it's just too big and messy. I tried one that turns source into dot files too, but it's too hard to read without some simplification step in there, or some better backend than SVG + browser. Shake also has tools to analyze which files are dragging down the build, or rank the most expensive modules, visualize parallelism over time, etc. I haven't used it much, but it's pretty fancy.
Hmm... But GHC is LESS defined no? ``(1 %) `seq` 2 = 2`` according to the report but ``(1 %) `seq` 2 = _|_`` according to GHC.
I have updated my patch. I'll try to get this merged (in 8.6 probably)
This example is easily handled: we can replace (6+b)*3 with (18+3*b) without changing the count of operations. Then we can propagate constants to the outer level with (+) associativity, etc. My patch handles it correctly. More complex cases like (6+b)*(3+c) are not as easy to handle because we distributing (18+3b+6c+bc) adds 3 operations. In some cases it may be worth doing it if it allows further simplifications but we need more than a small-step rewrite rule to support this.
 &gt;&gt;&gt; null ('a':'b':error "oh no") False &gt;&gt;&gt; length ('a':'b':error "oh no") /= 0 *** Exception: oh no CallStack (from HasCallStack): error, called at &lt;interactive&gt;:5:17 in interactive:Ghci3 
Any idea why length evaluates the items?
It doesn't: length ('a' : 'b' : error "oh no" : []) 3 It's just that in the original example `error` is the list itself, including the structure and length and so on.
I don't think it does: ``` Prelude&gt; length ('a':'b':error "oh no") /= 0 *** Exception: oh no CallStack (from HasCallStack): error, called at &lt;interactive&gt;:1:17 in interactive:Ghci1 Prelude&gt; length (['a','b',error "oh no"]) /= 0 True ```
I want to write a function that maps from Word8 to String. The project is for ID3v1 tags which has a genre byte with the first 148 values corresponding to a standard Genre string. Is there an efficient way to write this that doesn't look like genreMap :: Word8 -&gt; String genreMap 0 = "Blues" genreMap 1 = "Classic Rock" ... genreMap 147 = "Synthpop"
What's an opt build? 
Interestingly, this won't happen if `length` returned proper (non-strict) peano numerals instead. Then the identity would hold, afaics.
Have a look [here](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#controlling-what-s-going-on-in-rewrite-rules). It has some compiler flags to check which expressions are rewritten. &gt; I can't think of anything that would stop it from working? If you compile with `-O0` they might not fire, but that is probably somewhat obvious.
That is incorrect. (I am the author.) The section that you are referring to is about interactively evaluated Haskell code in the playground. All installation of extra packages happens outside of the sandbox ‚Äî in fact, it would be impossible inside. (In case you are wondering, this is perfectly within the scope set by Apple's Mac App Store rules, as they provide for an officially sanctioned escape hatch in the form of separately installed "user scripts".)
Sorry, I meant "optimized", which just means add -O. As an aside, at one point I did some benchmarks, and -O2 was slower to compile but no faster to run.
Oh of course! I read it wrong. Thanks
Yup. Correct thanks guys. 
Hi, I am the author of Haskell for Mac. I am sorry that you encountered these problems. Let me try to explain what's happening and see whether we can find a way forward. There are a few separate issues involved in this. Firstly, I don't think sandboxing has anything to do with your issue here. The main problem is that 'haskell-gi' depends on C libraries whose configuration is provided to the build system via `pkg-config`. This is a pretty Linux-y way of doing things that tends to get developers in trouble on macOS. The reason for the error message from cabal is simply that it cannot find the `pkg-config`executable in any of the locations where it is looking. This is a common problem regardless of whether you use an IDE or the command line. The best way to fix this ‚Äîalbeit annoying‚Äî is to invoke cabal install in an environment, where you explicitly set the the PKG_CONFIG_PATH environment variable. Unfortunately, Haskell for Mac can't do that automatically for you, as it cannot know where you installed pkg-config. There is also currently no way to tell it where that executable is ‚Äî however, that is something I should add! Hence, you will really have to drop down to the command line to issue the install command. Try with env PKG_CONFIG_PATH=/PATH_TO_PKG_CONFIG/pkgconfig cabal install haskell-gi where you need to fix the path to the install location of pkg-config. (Please make sure the Haskell for Mac command line tools are installed and marked as the default Haskell tools in the Command Line Preferences tab in the Preferences dialog box.) Once you installed the package like that. It will automatically also be available in the IDE. Feel free to email me at support@haskellformac.com if you encounter any further problems.
Would you be able to see if O1 is fast to run for you? OR takes 3-5x to compile but I'm unable to perceive a speed-up. 
Better yet, make it format agnostic. :)
Specifically I am talking about clients which do not know the content type they will get and so will not be able to set the Accept header accordingly but instead I want to return a content type that is stored along side the image data on the server (e.g. in a file or database).
-O1 is the same as -O, and it definitely makes a huge runtime difference for me. Same as you I see 3x compile time slowdown, but my app gets more than 2x faster. Sometimes even 10x faster. Garbage and GC time also decreases. I can tell not just from wall clock times, but also when I do profiling, different things wind up in the list if I omit -O. I do a lot with Data.Vector and parsing with Text and attoparsec, and -O makes a difference there. I think it helps the vector use get de-polymorphized, then inlined and unboxed. If you profile without -O and the same stuff shows up on the list, then that's a bit surprising, but you might as well omit -O. Or use -O*, I've never tried it, but it says it's for compile time.
You can use a static `array` or `vector` and simply index into it; both have constant-time indexing.
Wild guess: Isn't there a `lambda calculus (+ case) -&gt; basic algebra (+, *, ...)` mapping? My idea is, we would obtain equation, which would would solve for `mapId` and that could give us some interesting insights for any recursive function.
``` Œª: :seti -XDataKinds Œª: :seti -XPolyKinds Œª: :seti -XKindSignatures Œª: :seti -XTypeInType Œª: data D (x :: (a, b)) = D (a -&gt; b) Œª: :t D even D even :: forall a (x :: (a, Bool)). Integral a =&gt; D x ``` It is possible..kinda ‚ò∫Ô∏è
Become the leader, choose colleagues. That is my story.
Very interesting, especially since there's nothing `x` can actually be specialized to.
Cabal runs a solver. It may be that the latest version of your package (call it P) has a restrictive lower bound on some dep, call it A. So you depend on A &gt; 1.5. Now someone else pulls in your package, and something else (call it Q) that depends on A &lt; 1.5. Enter the solver. It says "hmm, maybe an earlier version of P will work" so it tries an earlier version. But that earlier version is now bitrotted because it has inaccurate deps! It could be the solver could find another solution, maybe by picking a different version of Q. Or it could be that it could just report a failure about mismatched bounds that makes sense. But instead, now the end user has a broken build and doesn't understand why. Moral of the story: the solver doesn't always use the latest version, so it is important to keep bounds correct on earlier versions too, and certainly when you _know_ there are errors.
Actually you are right. If I'm correct, the assembly output shows the addition reduced. And it happen with the llvm backend as well as the default one.
 Prelude&gt; data D' a b = D (a -&gt; b) Prelude&gt; :set -XTypeFamilies Prelude&gt; type family D x where {D (a, b) = D' a b} Prelude&gt; :kind! D(Int, Bool) D(Int, Bool) :: * = D' Int Bool 
Just to extend on this, if you're not much of an RSS user, you can easily get IFTTT to send you an email whenever this RSS feed changes, or push an item into your Remember the Milk list, or flash your Hue smart light bulbs, or publically shame yourself on Twitter. The possibilities are endless!
I cannot imagine any real world code, that would depend on this kind of behavior. I would go so far to completely forget about the existence of bottom during optimization. It breaks even the most trivial ones like `0*x = 0` with no apparent gain.
Oh this indeed requires something a little fancier that what's supported out of the box. It would probably require a custom combinator with -server/-client/etc instances, a somewhat "untyped" alternative to `ReqBody`/`Verb`. Feel free to drop by the issue tracker and open an issue about this. The very first motivation for designing servant the way it is designed is that if the library doesn't have a good enough solution (or just a solution at all) for your problem, you can provide one yourself in a way that will play as nicely with the ecosystem as all the existing combinators. So if someone needs a solution to this problem badly enough to tackle it, then there will be a solution available. But servant contributors usually don't solve problems they have not faced or at the very least heard about, hence the offer to open an issue. In situations such as the one you describe, whatever you would implement would not give you nicely typed clients or handlers and you would probably be forced to just deal with (MediaType, ByteString) directly or something along those lines, which would not be _much_ different from writing a `wai/warp` application directly (or using any other library). You could even implement this as an `Application` (from the wai library) and stick under a `Raw` endpoint for the time being. I do agree that it would be nice to have a solution to this that doesn't force you to write a raw `Application` and allows you to keep everything all statically typed and taken care of for you except for this variable content-type response thing. I'm however curious how a (non-browser) client could reasonably handle "any content type" as a response, without some kind of list of decoders for reading the data back into a structured form and using it in any meaningful way. A list of decoders (or encoders) is precisely what, say, `[JSON, HTML]` consists in. The use case about images does suggest that you might rely on some external program or library's list of decoders to do that part for you, I guess? 
&gt; I do believe that, overall, a static type system can help more than hinder the understability of monads. I actually came to full understanding of monads, after I implemented one in dynamic language.
You mean `not . null`, right? Seems to me this rewrite rule would be more confusing than helpful though. It would only work if you write `0` literally; if `0` is computed by an expression, you'd be back to the normal behavior. What's so hard about writing `not . null` instead of `(/=0) . length`?
Yep it is still pre-release but the Eta team is ver responsive and it could be suitable for non critical projects and as an strategy for breaking the complaints about interoperability. Haskell now is perceived as an all-or-nothing option. If, in the medium term, the company could use Haskell side by side with other JVM languages in the same project, then that reservation could be removed.
&gt; You mean not . null, right? Hah, yeah, edited. Guess I was a bit too hasty. It's not really that it's hard or anything, just that it's a common pitfall to check if a length is empty by checking the length, where that would actually be bad performance-wise since length traverses the list, so you made O(1) into O(n) all of the sudden.
Agreed. Anyone care to propose a counterargument?
While that breaks it, I would make the case that you would probably never intend for that behaviour to happen in the first place. I feel like it would be a nice minor optimization, unless someone is specifically relying on the check to be O(n), which would be....weird. Would this sort of thing warrant a GHC proposal?
I want to calculate large fibonacci numbers, modulo another number: Fib(100) mod 33 say. I implemented this in haskell, using the pisaro sequence. However, my solution is not fast enough and I'm struggling to understand why. So, can you help me speed this up? Also, any other coding feedback is appreciated. fibs = 0 : 1 : zipWith (+) fibs (tail fibs) findPisaroPeriod :: Integer -&gt; Int findPisaroPeriod m = period + 1 where tmods = tail $ map (`mod` m) fibs period = length . takeWhile (\s -&gt; s /= (0,1)) $ zip (tmods) (tail tmods) main = do [n, m] &lt;- fmap words getLine let newm = read m :: Integer period = findPisaroPeriod newm index = (read n :: Int) `mod` period res = (fibs !! index) `mod` newm putStrLn $ show res
This is what GADTs are for. ``` data D ab where D :: (a -&gt; b) -&gt; D '(a, b) ```
With that scheme, what happens if you remove the import Language.Haskell.Exts.Extension line in the file you link to? The diff now touches all the other 40+ import lines?
Or if you don't care about `cabal` users, it's problematic for `stack` workflow too. Let's say older package of yours is in the current LTS snapshot, then newer `aeson` is released. People care about newer `aeson`, so even they use LTS Snapshot, they add `extra-deps: aeson-1.3`. If older version of your package doesn't have an upper-bound on `aeson` and (unfortunately) doesn't compile with it, people will get compilation errors, instead of a kind and early error about mismatching bounds.
Running `stack ghc -- -ddump-rule-rewrites -O2 LengthTest.hs` on the following code, ``` module LengthTest where main :: IO () main = print $ length [1,2,3] /= 0 ``` Gave me the core dump below. Not the best at reading core, so if anyone is a hawk at that, I would appreciate the help :3 ``` Œª .testing/Haskell stack ghc -- -ddump-rule-rewrites -O2 LengthTest.hs [1 of 1] Compiling LengthTest ( LengthTest.hs, LengthTest.o ) Rule fired Rule: Class op length Before: Data.Foldable.length TyArg [] ValArg Data.Foldable.$fFoldable[] TyArg GHC.Integer.Type.Integer ValArg GHC.Base.build @ GHC.Integer.Type.Integer (\ (@ a_d2iw) (c_d2ix [OS=OneShot] :: GHC.Integer.Type.Integer -&gt; a_d2iw -&gt; a_d2iw) (n_d2iy [OS=OneShot] :: a_d2iw) -&gt; c_d2ix 1 (c_d2ix 2 (c_d2ix 3 n_d2iy))) After: GHC.List.length Cont: Stop[RuleArgCtxt] GHC.Types.Int Rule fired Rule: Class op /= Before: GHC.Classes./= TyArg GHC.Types.Int ValArg GHC.Classes.$fEqInt ValArg GHC.List.length @ GHC.Integer.Type.Integer (GHC.Base.build @ GHC.Integer.Type.Integer (\ (@ a_d2iw) (c_d2ix [OS=OneShot] :: GHC.Integer.Type.Integer -&gt; a_d2iw -&gt; a_d2iw) (n_d2iy [OS=OneShot] :: a_d2iw) -&gt; c_d2ix 1 (c_d2ix 2 (c_d2ix 3 n_d2iy)))) ValArg GHC.Types.I# 0# After: GHC.Classes.neInt Cont: Stop[BoringCtxt] GHC.Types.Bool Rule fired Rule: length Before: GHC.List.length TyArg GHC.Integer.Type.Integer ValArg GHC.Base.build @ GHC.Integer.Type.Integer (\ (@ a_d2iw) (c_d2ix [OS=OneShot] :: GHC.Integer.Type.Integer -&gt; a_d2iw -&gt; a_d2iw) (n_d2iy [OS=OneShot] :: a_d2iw) -&gt; c_d2ix 1 (c_d2ix 2 (c_d2ix 3 n_d2iy))) After: (\ (@ x_a2nt) (xs_a2nu [Occ=Once] :: [x_a2nt]) -&gt; GHC.Base.foldr @ x_a2nt @ (GHC.Types.Int -&gt; GHC.Types.Int) (GHC.List.lengthFB @ x_a2nt) GHC.List.idLength xs_a2nu (GHC.Types.I# 0#)) @ GHC.Integer.Type.Integer (GHC.Base.build @ GHC.Integer.Type.Integer (\ (@ a_d2iw) (c_d2ix [OS=OneShot] :: GHC.Integer.Type.Integer -&gt; a_d2iw -&gt; a_d2iw) (n_d2iy [OS=OneShot] :: a_d2iw) -&gt; c_d2ix 1 (c_d2ix 2 (c_d2ix 3 n_d2iy)))) Cont: StrictArg GHC.Classes.neInt ApplyToVal nodup (GHC.Types.I# 0#) Stop[BoringCtxt] GHC.Types.Bool Rule fired Rule: fold/build Before: GHC.Base.foldr TyArg GHC.Integer.Type.Integer TyArg GHC.Types.Int -&gt; GHC.Types.Int ValArg GHC.List.lengthFB @ GHC.Integer.Type.Integer ValArg GHC.List.idLength ValArg GHC.Base.build @ GHC.Integer.Type.Integer (\ (@ a_X2iH) (c_X2iJ [OS=OneShot] :: GHC.Integer.Type.Integer -&gt; a_X2iH -&gt; a_X2iH) (n_X2iL [OS=OneShot] :: a_X2iH) -&gt; c_X2iJ 1 (c_X2iJ 2 (c_X2iJ 3 n_X2iL))) ValArg GHC.Types.I# 0# After: (\ (@ a_a2nU) (@ b_a2nV) (k_a2nW [Occ=Once] :: a_a2nU -&gt; b_a2nV -&gt; b_a2nV) (z_a2nX [Occ=Once] :: b_a2nV) (g_a2nY [Occ=Once!] :: forall b1_a2nZ. (a_a2nU -&gt; b1_a2nZ -&gt; b1_a2nZ) -&gt; b1_a2nZ -&gt; b1_a2nZ) -&gt; g_a2nY @ b_a2nV k_a2nW z_a2nX) @ GHC.Integer.Type.Integer @ (GHC.Types.Int -&gt; GHC.Types.Int) (GHC.List.lengthFB @ GHC.Integer.Type.Integer) GHC.List.idLength (\ (@ a_X2iH) (c_X2iJ [OS=OneShot] :: GHC.Integer.Type.Integer -&gt; a_X2iH -&gt; a_X2iH) (n_X2iL [OS=OneShot] :: a_X2iH) -&gt; c_X2iJ 1 (c_X2iJ 2 (c_X2iJ 3 n_X2iL))) Cont: StrictArg GHC.Classes.neInt ApplyToVal nodup (GHC.Types.I# 0#) Stop[BoringCtxt] GHC.Types.Bool Rule fired Rule: Class op show Before: GHC.Show.show TyArg GHC.Types.Bool ValArg GHC.Show.$fShowBool ValArg GHC.Classes.neInt (GHC.List.lengthFB @ GHC.Integer.Type.Integer 1 (GHC.List.lengthFB @ GHC.Integer.Type.Integer 2 (GHC.List.lengthFB @ GHC.Integer.Type.Integer 3 GHC.List.idLength)) (GHC.Types.I# 0#)) (GHC.Types.I# 0#) After: GHC.Show.$fShowBool_$cshow Cont: Stop[BoringCtxt] GHC.Base.String Rule fired Rule: /=# Before: GHC.Prim./=# ValArg x_a2nn ValArg 0# After: case x_a2nn of _ [Occ=Dead] { __DEFAULT -&gt; 1#; 0# -&gt; 0# } Cont: StrictArg GHC.Prim.tagToEnum# Select nodup wild_a2Ld Stop[BoringCtxt] GHC.Base.String Rule fired Rule: tagToEnum# Before: GHC.Prim.tagToEnum# TyArg GHC.Types.Bool ValArg 1# After: GHC.Types.True Cont: Select ok wild_a2Ld Stop[BoringCtxt] GHC.Base.String Rule fired Rule: tagToEnum# Before: GHC.Prim.tagToEnum# TyArg GHC.Types.Bool ValArg 0# After: GHC.Types.False Cont: Select ok wild_a2Ld Stop[BoringCtxt] GHC.Base.String Rule fired Rule: +# Before: GHC.Prim.+# ValArg 0# ValArg 1# After: 1# Cont: StrictArg GHC.Types.I# Select nodup a1_a2nQ Select nodup wild_a2nl Stop[BoringCtxt] GHC.Base.String Rule fired Rule: +# Before: GHC.Prim.+# ValArg 1# ValArg 1# After: 2# Cont: StrictArg GHC.Types.I# Select nodup a1_a2nQ Select nodup wild_a2nl Stop[BoringCtxt] GHC.Base.String Rule fired Rule: +# Before: GHC.Prim.+# ValArg 2# ValArg 1# After: 3# Cont: StrictArg GHC.Types.I# Select nodup wild_a2nl Stop[BoringCtxt] GHC.Base.String ```
Not completely on topic but I actually found out that GHC can generate an explicit import list with the `-ddump-minimal-imports` flag. https://stackoverflow.com/a/26744192/1663462
Not completely on topic, but I recently found out that GHC can generate explicit import lists for each module, using the `-ddump-minimal-imports` flag. https://stackoverflow.com/a/26744192/1663462k
 &gt; If you want to be proactive, you can add `packdeps` task to your CI. To add to this; you don't even need `packdeps` anymore; Cabal now supports this natively via its new `cabal outdated` command which was designed with CI in mind. From the [documentation](http://cabal.readthedocs.io/en/latest/developing-packages.html#listing-outdated-dependency-version-bounds): ---- ## 3.3.2.4.3. Listing outdated dependency version bounds Manually updating dependency version bounds in a `.cabal` file or a freeze file can be tedious, especially when there‚Äôs a lot of dependencies. The `cabal outdated` command is designed to help with that. It will print a list of packages for which there is a new version on Hackage that is outside the version bound specified in the `build-depends` field. The `outdated` command can also be configured to act on the freeze file (both old- and new-style) and ignore major (or all) version bumps on Hackage for a subset of dependencies. The following flags are supported by the outdated command: - `--freeze-file` Read dependency version bounds from the freeze file (`cabal.config`) instead of the package description file (`$PACKAGENAME.cabal`). - `--new-freeze-file` Read dependency version bounds from the new-style freeze file (cabal.project.freeze) instead of the package description file. - `--simple-output` Print only the names of outdated dependencies, one per line. - `--exit-code` Exit with a non-zero exit code when there are outdated dependencies. - `-q`, `--quiet` Don‚Äôt print any output. Implies `-v0` and `--exit-code`. - `--ignore` *PACKAGENAMES* Don‚Äôt warn about outdated dependency version bounds for the packages in this list. - `--minor` *[PACKAGENAMES]* Ignore major version bumps for these packages. E.g. if there‚Äôs a version 2.0 of a package `pkg` on Hackage and the freeze file specifies the constraint `pkg == 1.9`, `cabal outdated --freeze --minor=pkg` will only consider `pkg` outdated when there‚Äôs a version of `pkg` on Hackage satisfying `pkg &gt; 1.9 &amp;&amp; &lt; 2.0`. `--minor` can also be used without arguments, in that case major version bumps are ignored for all packages. ### Examples: $ cd /some/package $ cabal outdated Outdated dependencies: haskell-src-exts &lt;1.17 (latest: 1.19.1) language-javascript &lt;0.6 (latest: 0.6.0.9) unix ==2.7.2.0 (latest: 2.7.2.1) $ cabal outdated --simple-output haskell-src-exts language-javascript unix $ cabal outdated --ignore=haskell-src-exts Outdated dependencies: language-javascript &lt;0.6 (latest: 0.6.0.9) unix ==2.7.2.0 (latest: 2.7.2.1) $ cabal outdated --ignore=haskell-src-exts,language-javascript,unix All dependencies are up to date. $ cabal outdated --ignore=haskell-src-exts,language-javascript,unix -q $ echo $? 0 $ cd /some/other/package $ cabal outdated --freeze-file Outdated dependencies: HTTP ==4000.3.3 (latest: 4000.3.4) HUnit ==1.3.1.1 (latest: 1.5.0.0) $ cabal outdated --freeze-file --ignore=HTTP --minor=HUnit Outdated dependencies: HUnit ==1.3.1.1 (latest: 1.3.1.2) ---- 
Lazy IO with `[Char]` (i.e., `System.IO`) should be able to produce almost exactly `'a':'b':error "oh no"` (but more `IOException`-y) 
Sure, but in the case of `length xs == 0` vs `null xs`, one would always prefer the lazier `null xs` even in your scenario, no?
Ah, that's great, thanks!
They may behave differently for infinite lists: length [0..] doesn't halt, but null [0..] works fine.
Could you give an example how one might be able to specify intent if haskell had an (almost-)supercompiler?
I began to love Haskell thanks to @__gilligan and his motivation towards this learning group. I was afraid of math for so long, and now I know it was just stupid. Haskell is fun and I looking forward to learn more and more of it!
Yep. But, the question is over "real world code" ‚Äî which is not necessarily any good.
This gives core dump ghc -ddump-simpl -O1 LengthTest.hs -dsuppress-all And the output looks like this main1 = hPutStr2 stdout $fShowBool1 True -- main1 = print True ... main = main1 `cast` &lt;Co:3&gt; -- main = main1
I'm not sure I follow what you mean here. I'm interested in 'not necessarily good' (real world) code examples too, that depend on, e.g., the simpler `0*x == 0` throwing where `x = _|_` instead of returning True. I'd wager, agreeing with /u/Ford_O that we could safely make the optimization that `0*x = 0` always. I'd also like someone to refute this with a counterexample, if possible.
The point is that we do not encode anything, we just deliver existing data that was uploaded before (e.g. a logo for a category or a profile pic). Essentially the use case is that some parts of the application allow the upload of images in an arbitrary image format and these images are displayed again with no encoding or decoding whatsoever.
I'm pretty sure it's a bad idea to allow `f x = 0*(x/0)` to optimize to `f x = 0`.
Ignoring bottoms like that makes it harder to reason about performance, and sometimes makes it worse. How would you know whether `length xs` forces the spine of the list? Adding explicit `seq` with special optimization rules may cause a double traversal if `length xs` isn't optimized away. Respecting the identity `0 * x = 0` would require every multiplication `y * x` to have a runtime check of whether `y = 0`, which generally will not be optimized away.
To quote a well known underappreciated Haskeller who couldn't have said it any better: ["I am so sick and tired of the PVP sniping going on. This **** is what has prevented Haskell from really taking off."](https://twitter.com/snoyberg/status/768719160171954176)
In the simplest case: `newtype`. For example, you differentiate between the additive and multiplicative monoid via the `Sum` and `Product` wrappers. More generally, you write a DSL of some kind, as a data-type -- potentially using GADTs w/ phantom and lifted types, if you want to get fancy. But the tags of your type often communicate additional information about how that information is to be used.
You should have mentioned the name of the tool in the title. ALGT will soon be a common household word, you know.
&gt; To quote a well known underappreciated Haskeller who couldn't have said it any better: "I am so sick and tired of the PVP sniping going on. This **** is what has prevented Haskell from really taking off." Ok, I'll bite at the risk of feeding a troll. You seem to confuse matters. This is *not* PVP specific and would equally apply in SemVer land, or any other versioning scheme for that matter. The dependency specification in in the meta-data is supposed to accurately describe which dependencies your package is known to be *compatible* with. In the case at hand, you have hard evidence that certain combinations don't even compile (which implies non-compatibility), and so the meta-data needs to be corrected to reflect that evident knowledge. Ensuring accurate meta-data on Hackage at all times for the overall package collection in order to provide a good user-experience as well as helping/assisting package maintainers to help us in achieving that objective is the primary mission goal of the Hackage Trustees initiative.
As /u/zvxr and /u/JohnnyPopcorn pointed out that is not a valid transformation for infinite lists/lists with exceptions in the spine. So as long as we want to adhere to Haskell as defined we can't rewrite it. As far as I'm aware Haskell currently doesn't allow any Exception/Bottom to disappear so I wouldn't expect that to be changed either. But it's a nice example for situations where undefined behavior would allow additional performance optimizations.
This difference in behaviour shows that it would not be a valid optimization to convert `null xs` into `length xs == 0`, but going in the other direction should be allowed! My understanding is that [optimizations are not allowed to make expressions less defined, but they are allowed to make them more defined](https://www.reddit.com/r/haskell/comments/4edx9n/fun_fact_hask_isnt_a_category/d1zdd26/).
`length` evaluates the argument's spine, whereas `null` doesn't. So there is risk in leaving things unevaluated. Similarly, the whole `seq` primitive would be pointless by that reasoning, as it is always correct to have `seq _ = id`.
Oh, then a `Raw` application makes more sense in my opinion. There's no point in trying to be smarter than that for images that are just going to be stored and served as-is. I have in fact done something similar with encrypted audio files in different formats.
Not if you diff with `--ignore-all-space` or `--word-diff`.
What is "not fast enough"? How are you compiling it? How are you timing it? How fast should it be?
True. Could something similar make the (very real) added potential for merge conflicts go away easily?
I don't see why one should give up parameter parsing and validation in servant-server by using a Raw application just because servant hard-codes the mime type selection in a rather convoluted way that can not be overridden.
Neat. &gt; Lowering the barrier to entry Hope this means relocating main development to GitHub or something.
Damn that's a lot of talks
A auto-merge-up-to-whitespace operation seems quite plausible and would only have to be a tiny bit semantically aware.
I would like to add that the [PVP](https://pvp.haskell.org/) recommends explicit imports or qualified imports: &gt; However, note that this approach is slightly risky: when a package exports more things than before, there is a chance that your code will fail to compile due to new name-clash errors. The risk from new name clashes may be small, but you are on the safe side if you import identifiers explicitly or using qualification.
With `genericLength` and a lazy unary representation of natural numbers, it can be made to work. But of course that's not the representation you get for `Int`, which is what the vanilla `length` uses.
Another thing about this is having to load and write the whole structure to reflect user changes.
First let me thank you for your time and answer. &gt; The presentation layer operates on the domain layer and returns some state what is exactly the role of the presentation layer, it seems a bit vague. as for the domain layer it is clear as well as for the interface, regarding the other external layer its a bit vague. could you be more specific please?
well explained. Thank you so much
as article says &gt; This may well involve moving from Phabricator to GitHub.
&gt; we could safely make the optimization that 0*x = 0 always. &gt; I'd also like someone to refute this with a counterexample IEEE floating point numbers. 0*(1.0/0) = 0*+Inf. = NaN 0*(-1.0/0) = 0*-Inf. = NaN 0*(0/0) = 0*0 = 0
There seems to be many videos twice in the playlist.
According to the [LambdaConf program](http://lambdaconf.us/events/2017/lcusc.html), the Haskell ones are: 1. [Adjunctions in Everyday Life](https://www.youtube.com/watch?v=FuOZjYVb7g0) * [Building a Blogging System with Haskell and Yesod](https://www.youtube.com/watch?v=GPOnrvsUrjY) * [Corecursion, Codata, and Just a Little Combinatorial Game Theory](https://www.youtube.com/watch?v=WksM9GJ4q40) * [Deforestation and Program Fusion](https://www.youtube.com/watch?v=MVuj8yVUrho) * [Enhancing your Haskell with Dependently Typed Programming](https://www.youtube.com/watch?v=_UIpI0dOWAo) * [Froid: Functional Programming on Android](https://www.youtube.com/watch?v=_Hwcjreq_XU) * [Functional Infrastructure With Nix](https://www.youtube.com/watch?v=R5BSWic4_Uc) * [Graphics Programming with LambdaCube 3D](https://www.youtube.com/watch?v=pA8CdW56CbI) * [Greek Classics: Lambda Calculus For Beginners](https://www.youtube.com/watch?v=-DPlj75YJR4) * [Higher order Abstractions](https://www.youtube.com/watch?v=3D2ezqTwcWM) * [How to Program like a Five Year Old in Haskell](https://www.youtube.com/watch?v=YqVTCZFPoyQ) * [I Command You to be Free!](https://www.youtube.com/watch?v=Ej5FQtEgTBw) * [Less is More with MonadReader](https://www.youtube.com/watch?v=zLa9wQRhY-8) * [Let's Lens](https://www.youtube.com/watch?v=inyfR3Gb6GE) * [Mighty Morphin Data Types](https://www.youtube.com/watch?v=HnR3zjXUtac) * [Neural Networks are Programs, Too](https://www.youtube.com/watch?v=BgoZESn1jNo) * [One Year In: A Beginner's Take on the Difficulties of Learning Haskell](https://www.youtube.com/watch?v=TTsYpIkIuIg) * [Refactoring Recursion](https://www.youtube.com/watch?v=TEvDaKg4XXA) * [Singletons and You in Haskell](https://www.youtube.com/watch?v=ZoygbBiLo-w) * [Some1 Like You Dependent Pairs in Haskell](https://www.youtube.com/watch?v=PNkoUv74JQU) * [Statically Typed Interpreters](https://www.youtube.com/watch?v=Ci2KF5hVuEs) * [The Power of Optics](https://www.youtube.com/watch?v=pTyqF67y7Yk) * [Transient: Unrestricted Algebraic and Monadic Composability](https://www.youtube.com/watch?v=L-iqhEpKUkk) * [Variables in Haskell](https://www.youtube.com/watch?v=UF0D2bL6pMw) * [Web Programming and Streaming Data in Haskell](https://www.youtube.com/watch?v=9957qVltU00)
Indeed, it was. Thanks for letting me know, I've fixed it
Yes, unfortunately.
&gt; As far as I'm aware Haskell currently doesn't allow any Exception/Bottom to disappear so I wouldn't expect that to be changed either. I heard [the opposite](https://www.reddit.com/r/haskell/comments/4edx9n/fun_fact_hask_isnt_a_category/d1zdd26/)! We should try to get a definitive answer.
No idea, I've never heard of them before. I'll take a look at it when I have a bit more time (prob tomorrow)...
I love your enthusiasm! However, I fear that quite some other similar tools already exists and quite some more testing and development is needed to improve it. Its hardly usable now...
No, not yet... I'll do it soon‚Ñ¢, hopefully tomorrow, or Monday. It's on my to do list now now
We'll clean that up. There's also another 15 or more unlisted ones we're trying to locate.
much better than the to list list
also issue tracker (trac -&gt; github issue)?
Thanks, really interesting. Are there still efficiency benefits then in using typeclasses vs passing a function dictionary via a parameter? Ex: `foo :: MyFunctions -&gt; IO () vs foo :: MyFunctionsMonad =&gt; m ()`
If you were to catch the error from IO, you get two different results, when you use the rewrite rule or not.
`(\f g x -&gt; ...)` is lambda function with three arguments. Arguments are separated by white space so `f` is substituted by `(\y -&gt; y * 2)`, `g` by `(\z -&gt; z + 1)` and `x` by `(1)`.
Yes, and my point is that not crashing and succeeding is superior.
Now *that* is less likely, at least in the short term. Would need a good way to migrate huge number of existing tickets. And there are so many places in the wild pointing to existing tickets on Trac. Not the mention the custom fields that are pretty useful, and which have no equivalent yet in GitHub.
I don‚Äôt understand your question. Your expression can be more succinctly expressed as (*2) . (+1) Does that help?
Wow I was completely shocked when I saw who had written this post. Thank you for taking the time to respond to my question. I tried what you've suggested and it definitely seems to be in the right direction but at the bottom I still get the following issue:cabal: Error: some packages failed to install: 'conduit-extra-1.1.17-8UdIyVXaXah42tBKaJQFHh depends on conduit-extra-1.1.17 which failed to install. haskell-gi-0.20.3-2LrBRPCxQfJ4RtYRLOvbel depends on haskell-gi-0.20.3 which failed to install. regex-tdfa-1.2.2-Lka063aLhc5F5bsI7OP67E failed during the building phase. The exception was: ExitFailure 1 streaming-commons-0.1.18-Ij4BOI603DY3tMZTRb3GAS failed during the building phase. The exception was: ExitFailure 1 xml-conduit-1.4.0.4-JKtaXba47mF3Qx08CbixXj depends on xml-conduit-1.4.0.4 which failed to install.' 
It's not about superior or inferior, it's about having consistent semantics, and being able to reason about them. That's why people use haskell.
First of all thanks for replying, I was shocked to see who had written the post when I first read it. I've tried what you suggested and it seems so close to working but I got the following error at the bottom: `Configuring haskell-gi-0.20.1... cabal: '/usr/local/bin/pkg-config' exited with an error: Package libffi was not found in the pkg-config search path. Perhaps you should add the directory containing `libffi.pc' to the PKG_CONFIG_PATH environment variable Package 'libffi', required by 'gobject-introspection-1.0', not found cabal: Leaving directory '/var/folders/kz/8j4kvhnx1xg1w1y7f9vtl1140000gn/T/cabal-tmp-39610/haskell-gi-0.20.1' cabal: Error: some packages failed to install: haskell-gi-0.20.1 failed during the configure step. The exception was: ExitFailure 1` Any thoughts? 
I hope not and that a custom GitLab instance is used, while utilizing the built-in functionality to bi-directionally sync everything with GitHub.com and GitLab.com. I might get downvoted for this, but I have to be honest and voice my opinion that we have to resist centralizing FOSS development Facebook-style when we have interoperability by running GitLab.
Dare I suggest JIRA+BitBucket? I know the hate against JIRA, but it's good at these kind of custom workflows. 
Well I know what the next month of silicon valley traffic commutes is going to be an opportunity for. Damn even restricting it to the haskell talks this is a hell of a lot of content.
[Video](https://www.youtube.com/watch?v=Txf7swrcLYs ) from my previously linked Scala World talk on the topic: The main new material mentioned here is a bit more detail about how to deal with handling layout monoidally, and I had a much longer time slot to work through the material with the audience. Code: https://github.com/ekmett/coda
;-)
Amazing story! May I ask what your experience level was when you started pushing for Haskell at work? Had you done any big project in Haskell by that point? Were you well versed in topics relevant to performance, such as inlining, fusion, unboxing, lazy vs strict, profiling and debugging, etc.? I would sheepishly say I'm past the beginner phase, but I don't have any real project experience in Haskell. I fear that I'm not well prepared for the unknown unknowns.
Well there are [imprecise exceptions](https://www.microsoft.com/en-us/research/publication/a-semantics-for-imprecise-exceptions/) which I based my statement on. Since this is not part of the report there is also this in the Report: &gt; When evaluated, errors cause immediate program termination and cannot be caught by the user. Which is very clear. 
I like how the image for the entire playlist ended up being Chris Allen going ¬Ø\\\_(„ÉÑ)_/¬Ø .
Could you explain the downside of GitHub over a custom GitLab? It‚Äôs really not obvious to me. GitHub has always been a much better platform to me (faster, better UI, more reliable, etc.). As long as the code isn‚Äôt literally only on GitHub, I don‚Äôt really see how being centralized on it is a bad thing.
I've used Bugzilla and JIRA, and over the years I've never come across an instance that is as responsive as Mantis or GitHub/GitLab tickets. Granted JIRA and Bugzilla are much more sophisticated, but I want to ask this: can any of the Bugzilla or JIRA users/proponents point me at a publicly accessible instance I can check for responsiveness? By responsiveness I mean how quick the main page loads, how long modifications take to be processed, and how fast search is. This is something where Bugzilla and JIRA, despite usually being run on a dedicated host, are much slower than GitHub/GitLab tickets or Mantis.
Monad transformers are great, if only they didn't have runtime costs :( I thought it was all in the types and that lifts are just something waiting for syntax sugar to appear.
&gt;Well the goal is to introduce Haskell gently. For a lot of folks this would be their first encounter with Haskell. I don't want their first experience to be overwhelming, thus reinforcing the idea that haskell is not "practical." So to answer the original question with those constraints we have to nail down what qualifies as gentle haskell and what doesn't. With that information I can write a haskell example and compare it to python and deduce whether there are any benefits. Is Aeson with deriving generic simple enough? Do we need to avoid typeclasses entirely? I'm interested in doing this exercise if you don't mind nailing those details down with me. 
But why? Trac serves our purposes just fine, is no more annoying than Jira would be, and, perhaps most importantly, works today with no further effort. What would we be gaining in such a move?
Had used this when I was messing around with HIVE (thanks to the BigData hype) -- https://issues.apache.org/jira/projects/HIVE And there's the bug I fixed as well -- https://issues.apache.org/jira/browse/HIVE-794 :)
Actually, I was surprised to see Trac **and** Phabricator. I'd understand only Trac, or only Phabricator. But, why two tools?
The DevOps Group is going to have a major emacs/vim (or stack/haskell-platform) moment when it comes to the issue-tracker and source-code hosting. Best of luck... you're going to need it :)
Thanks, this is one of the more responsive instances, although admittedly it's (for me personally) still not as quick as the simple trackers. Took 10s to load front page and its dynamic activity box. Then clicking around tabs in the linked bug of yours also took more than 2 seconds. This is fine if you use it once a day for 10 minutes, but if you have to work with many tickets multiple times during a work day, at least I would start to dread and look for offline replication of the database with a local client to operate on it and push the changes back to the shared JIRA database. Guess I should mention I tend to agree with Fossil-SCM's design to integrate tickets within the project's repository, just like code commits, and the related git-appraisal model of code review, but that's another aspect for another discussion :).
The Trac instance existed way before Phabricator was used and I think the tickets were never migrated over to Phabricator.
I also have found gitlab to be better than github+all the tools you have to hook into github to actually make it work for large projects. Most of the github CI tools seem to be extremely focused on a single "happy path" and if you want to do anything slightly off that path you're screwed.
it needs to be approximately 30% faster for an input of '100 100000' And yes, I'm compiling it with the -02 flag. Do you have any suggestions as how I can improve things? For 
1. By demoting the GitHub instance to a meeting place for inter-operability, we promote the use of a fully controlled GitLab instance and keeping more of the functionality under the influence of the project. No need to write blog posts signed by project leaders to beg GitHub for some feature. There are some feature GitHub for commercial reasons never has and apparently won't allow to disable in project settings, while in GitLab you can, and they don't take away your control that way. 2. GitHub and GitLab or any other central host isn't always available for everyone or few, and by promoting and making use of federation when it's already available, we are able to do the right thing which incidentally doesn't take any effort. 3. I resist the urge to be pulled into GitHub just for social/network effects and give up control of my projects' life cycle to a for-profit company I have no influence over. 4. I highly value extending the D in DVCS to other aspects of project life cycle, and federating by using GitLab as the inter-operable open source GitHub replacement, while allowing users who insist on doing so to use GitHub as well. 5. I like to see the D be applied for code reviews (git-appraise) and tickets (Fossil-SCM) as well just because I much prefer working locally and syncing with a shared or p2p network when applicable. 6. I don't live in a world where my team's workflow is stalled because GitHub is down and don't want to. This is why I strongly advise against SPOF designs in language package managers. 7. I much prefer using local tools like my Emacs instance to work on stuff (just like people use all kinds of fancy Eclipse project plugins) as much as I can while operating a browser on GitHub only where necessary. 8. Lastly, I prefer for stuff to be available, reliable and just work and not be the victim for DDoS attacks, which is why I'm one of those proponents who wants to have more P2P networking in tools, not less.
This is a pretty standard optimization in other contexts, though. That depends on whether you've defined semantics for division by zero. If it's defined to throw an exception, then you can't allow that transformation. If division by zero has undefined behavior, then no possible behavior for `f` is forbidden, including `f = const 0` or even `f = id`.
Seems right to me -- I see a lot of benefits and no costs. I'm wondering if anyone disagrees.
0/0 is also NaN.
I didn't have any professional experience with Haskell at the time, but I had a lot of experience with general systems programming. If you want to go this road, you should know enough Haskell to confidently drop into and out of IO, and you should be sufficiently confident with systems programming to be able to build the thing in C or Python or whatever. It's not enough of a foundation to make this kind of effort easy, but when you get hit by "unknown unknowns," you'll have what you need to bust out the FFI and sledgehammer the problem.
&gt; No need to write blog posts signed by project leaders to beg GitHub for some feature. I'll grant you this. But frankly, if that's what it would come to with GitHub, then it would likely come to writing a plugin or something for GitLab, and no one in GHC is going to spend the time to do that. Plus, I don't think issues of this sort are likely to come up for GHC. &gt; GitHub and GitLab or any other central host isn't always available for everyone or few, and by promoting and making use of federation when it's already available, we are able to do the right thing which incidentally doesn't take any effort. I don't see how a custom GitLab is any more likely to be available? &gt; I resist the urge to be pulled into GitHub just for social/network effects and give up control of my projects' life cycle to a for-profit company I have no influence over. You absolutely do have influence. Once they stop meeting our needs, we can up and leave. That's what's nice about Git. &gt; I highly value extending the D in DVCS to other aspects of project life cycle, and federating by using GitLab as the inter-operable open source GitHub replacement, while allowing users who insist on doing so to use GitHub as well. This doesn't make sense. GitHub does not prevent users from keeping their clones / forks in other places. &gt; I don't live in a world where my team's workflow is stalled because GitHub is down and don't want to. This is why I strongly advise against SPOF designs in language package managers. Again, how is a custom GitLab any better about this? &gt; I much prefer using local tools like my Emacs instance to work on stuff ... How does GitHub have any affect on this? &gt; Lastly, I prefer for stuff to be available, reliable and just work and not be the victim of DDoS attacks, which is why I'm one of those proponents who wants to have more P2P networking in tools, not less. *A custom GitLab is no more distributed.* --- Are you suggesting GitLab, or some completely new distributed system?
When the specializer / inliner is doing its job, the runtime cost is extremely minimal. The safe way to make it very likely to do well is to add `INLINABLE` pragmas all over the place.
Eerily similar to something I wrote just two days ago: [Object-Oriented Haskell](http://programming.tobiasdammers.nl/blog/2017-10-17-object-oriented-haskell) - the core idea, encoding OOP interfaces as Haskell records, is exactly the same.
So this kinda reminds me of a better form of 2 level grammars (aka vw grammars aka affix grammars). Kinda like [cdl/cdl2](http://www.cs.ru.nl/cdl3/cdl3.pdf)
This was a fantastic talk. It's fascinating to see how much you can accomplish with a principled design framework from the ground up and that often seemingly innocuous design decisions like multi-line comments can have such unforeseen hamstrings in the future. Is there a reason the code has its own Finger tree? It looks the same as the one from Edison Core, and it's definitely a non-trivial amount of work! Also, intuitively, it seems like this sort of approach to parsing would rule out (or at least be greatly complicated by) things like user-defined syntax rules - even fairly simple ones like defining the operator precedence of an infix operator would cause subsequent parsing to be stalled, no? Although maybe this is a win-draw scenario: win if you already know the operators, and draw if you have to wait on previous blocks to parse?
As far as I remember ghc transforms the latter into the former, but has optimizations may get rid of the dictionaries. Unfortunately, I don't know if this is a special type class optimization applied before the translation, or a general optimization, which is applied on the encoding afterwards.
Hilmi this is the haskell subreddit, not stackoverflow
The "dark" talks. 
GitHub is pretty much useless for large projects. For example, there is no way to exclude when searching.
Hey, I'll respond fully when I am in front of a computer. In brief: Presentation layer is for presentation logic. Your ViewModels or presenters would normally live here. In my brain, it would take messages from domain layer, manipulate them, and provide the view with some state to display, for instance. The external layer are things outside of your control. Like database library for instance. Stuff you really don't need to chose concretely in order to still be able to implement your core business logic. It should be noted that all these layers are optional. You don't need to make things this abstract, and in a lot of cases you won't want to.
GitLab is not perfect or finished, but it is here, can be federated, and has built-in project lifecycle tools. It's very likely that one of the webhooks you'd use with GitHub will be unavailable/shutdown at some point, and the OAuth permissions model is risky given what kind of permissions they need to ask for with GitHub's missing granularity. &gt; I'll grant you this. But frankly, if that's what it would come to with GitHub, then it would likely come to writing a plugin or something for GitLab, and no one in GHC is going to spend the time to do that. Plus, I don't think issues of this sort are likely to come up for GHC. I have to disagree with the extension/plugin dev aspect. Haskell.org already patches Phabricator and major open source projects use GitLab and therefore there's a more lively and larger pool of development/admin teams with needs and bandwidth to improve GitLab. &gt; I don't see how a custom GitLab is any more likely to be available? We would use federation and have synchronized instances at least on gitlab.com and gitlab.haskell.org, making it less likely that they're all unavailable at the same time. &gt; You absolutely do have influence. Once they stop meeting our needs, we can up and leave. That's what's nice about Git. Move where? A GitLab instance maybe? The commits are already distributed, nothing GitHub or GitLab adds on top. The lifecycle of patches, tickets, builds, releases is what GitLab has a suite of tools for which are integrated. &gt; This doesn't make sense. GitHub does not prevent users from keeping their clones / forks in other places. Please see above. &gt; Again, how is a custom GitLab any better about this? It's a tangential comment by me trying to stress how other aspects of our development workflows suffer from the same underlying issue. &gt; How does GitHub have any affect on this? With GitHub we are at the whim of GitHub Inc for any tool integration to be support at their remote network endpoint. With GitLab or a fully distribute system like Fossil-SCM we can maintain a fork if needed or get our needs upstreamed into a codebase we can contribute while that's impossible with GitHub plus list-of-random-webhooks. &gt; A custom GitLab is no more distributed. You're right in your observation that GitLab doesn't get us the D for the rest of a project's lifecycle. My point, which I may have been ineffective at formulating, is that GitLab is a cheap way to at least promote and encourage federation/inter-operability. That is half the way and doesn't require a move away from git or developing new tools.
Can you explain this? I love vim but don't know what you mean. Thanks
I think they mean that one toolchain will be supported more actively, making life harder for those who cannot/won't use it.
I get your point, though to me Stack is like what Spacemacs is to Emacs. You don't have to use Spacemacs if you already use Emacs and are happy with your setup, but new users getting onboarded with Spacemacs is the equivalent of suggesting Stack to new Haskellers. So I'm not sure there is a real problem here since it's similarly easy to share functionality like Stack already does by being a layer on top of Cabal (the library). For instance, Stack could use the new-* functions like they already do with other Cabal APIs.
Please don't, GitHub is a huge single point of failure and is frequently actively targeted by [–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä](https://github.com/github/gov-takedowns/tree/master/Russia/2017) and as well as [other regimes](https://en.wikipedia.org/wiki/Censorship_of_GitHub). Moreover, just earlier this year they had some controversial ToS change that got everyone worried due to fuzzy legalese.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [github/gov-takedowns/.../**2017** (master ‚Üí 133db12)](https://github.com/github/gov-takedowns/tree/133db12c593e9cbfd55f55f3b009957e37aff086/Russia/2017) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I've come to appreciate GitLab-CI with the whole lifecycle tooling plus per-project Docker registries very much. It's also great that you can run your own runner/agent, which is something that would be great for GHC, since we already are using a custom builder tool for bindists created by the wider community with Solaris or FreeBSD interest.
Why why why? People keep bringing this up. We just finished discussing this a few weeks ago. Github's code review process is just atrocious. Github is just not designed for the scale of patches we have. I routinely put my patches on it to get a top level overview. And often it just gives up and says the diff is too big to render, and when you force it to expand it a lot of the time hangs while phab has no issues. Github's file browser also has a fairly low limit of files it'll display in a folder. Just take a look at the gcc mirror, half the folders don't render all the files. Github forces you to use a branch based approach to managing patches. This is just not scalable with large patches. A common workflow is to have all the patches in the same branch. Phabricator is flexible enough to handle both patch and branch based approach to managing split patches. Github will require a host of bots to replace things like herald. CircleCI, Travis etc all introduce a great barrier for debugging. You can't get system traces or performance counters to figure out why a particular test may be failing. We often have cases where only the CI fails. Now we'll have to submit and pray. I can keep going all day. Github is pretty poor for any substantial project. No matter how web 2.0 it is.
[removed]
Thanks for sharing your perspective. That's good to know. I'm confident in my system programming abilities. I'll need to learn more about FFI.
Nothing is decided, we're looking for feedback at this stage, and feedback from regular contributors (such as yourself) is really important - thanks.
Thanks for the correction. Floating point numbers are a travesty. ;P
&gt; If it's defined to throw an exception, then you can't allow that transformation. In Haskell we can, because while `throwIO ex` has non-bottom semantics, `throw ex` does have bottom semantics, so we get to do whatever.
How do you implement multiple interfaces in the same type with this method. With type classes it's simple, you write the instances for each interface that you are implementing.
The one from fingertree in Data.FingerTree doesn't quite fit the style of the surrounding code, so I adapted it to fit better. Fundeps can't really be erased to type families, but if I start with a type family I can go the other way. An Edison-like approach for the relative stuff is a possibility, but it probably won't use Edison itself. Having my own fingertree in there is a stepping stone towards adding a "relative" finger tree annotated with a relative monoid.
As for precedence, it turns out it basically doesn't stall the stuff I care about here: name capture and figuring it scoping. That stuff is respected regardless of fixities.
This method requires an explicit "upcast" call, so it's easy to support multiple interfaces.
Someone tell me about the must watch videos, please!
Did you try the obvious things? Reboot. Uninstall and re-install GHC. FCU only started rolling out a few days ago. So it's conceivable there is an issue. I suggest you contact the GHC team on the [GHC users mailing list](https://mail.haskell.org/cgi-bin/mailman/listinfo/glasgow-haskell-users).
That's really nice. The Is type class and cast function are a clever way to solve inheritance problems. Cheers
Alternatively, to avoid the explicit upcast: data BicycleF a = BicycleF { changeCadence :: Int -&gt; IO () , changeGear :: Int -&gt; IO () , speedUp :: Int -&gt; IO () , applyBakes :: Int -&gt; IO () , implData :: a } type Bicycle = BicycleF () data AcmeBicycleD = AcmeBicycleD { cadence :: IORef Int , speed :: IORef Int , gear :: IORef Int } type AcmeBicycle = BicycleF AcmeBicycleD implAcmeBicycleBicycle :: AcmeBicycleD -&gt; BicycleF AcmeBicycleD implAcmeBicycleBicycle data = BicycleF { changeCadence x = writeIORef (cadence data) x , changeGear x = writeIORef (gear data) x , speedUp x = modifyIORef (speed data) (+x) , applyBrakes x = modifyIORef (speed data) (subtract x) , implData = data } newAcmeBicycle :: IO AcmeBicycle newAcmeBicycle = do c &lt;- newIORef 0 s &lt;- newIORef 0 g &lt;- newIORef 1 return $ implAcmeBicycleBicycle AcmeBicycleD { cadence = c , speed = s , gear = g } Use functors for non-final classes, use non-functors for final classes. Works fairly well until you want multiple inheritance. You can even do checked downcasts via `Typeable` and `Dyn`.
Yesterday I was reading about how you can use [ExistentialQuantification](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#record-constructors) for OO-style programming
Yes I tried the usual things. I don't think it's related to the FCU, as this problem was happening before as well. 
They mean that if the GHC DevOps group switches to (say) GitHub for source code hosting, we'll have a situation on our hands similar to Emacs-versus-Vim or Stack-versus-Cabal. Look no further than this very thread for proof of that. 
Why didn't you post it?
Hello, I'm a programmer moving from a language which doesn't have type currying and was wondering how one would define a `Functor` instance for the first element of a `(,)`?
There was an effort by /u/mpickering to migrate from [Trac to Phabricator](https://mail.haskell.org/pipermail/ghc-devs/2017-January/013500.html). I don't know what happened with that. I assume it was abandoned for a reason.
How will this affect the rate at which research ideas (eg, contributions of graduate students working with SPJ) are accepted into GHC releases? Part of what I like about Haskell is that it gives the academic community a playground to test new ideas on a bunch of willing code monkeys (the rest of us). I'd hate to give that up in the name of stability. In fact, it seems like if stability is a goal, an industrial group should commit to backporting bugfixes to older maintenance releases of GHC and libraries. (Which, I thought, was the point of lts stackage relases).
Iirc a lot of the runtime cost is because it breaks the constructed product result optimization. As in, if you calculate an Int in your function and return it it's split into an inner function that returns the machine Int# and an outer function that allocates the heap Int which is always inlined and easy to optimize away. But if you return Identity Int this breaks because it can't do this for nested constructors because it might screw up laziness.
I think you're right, but your post didn't talk about using an Is / Isa type class to enable proper polymorphism. It would be great to see examples with multiple inheritance and composition in Haskell. I do generally use type classes for these problems, but I can see now that I can further parameterize my classes to reduce the need for so many.
We are 100% committed to keep GHC the same playground as it has always been. Being at the bleeding edge is one of the things that makes Haskell so powerful and we wouldn't want to give that up. However, this is actually much more of a topic for the GHC Steering Committee, which decides on new features for GHC. What we mean by stability here is a compiler that does not crash; i.e., being stable in the software equality sense (which has also always been a goal of Haskell).
Thanks for clarifying! I misunderstood the timed releases portion of the announcement, I think.
Your problem appears to be that you are forcing LOTS of `fibs` to be evaluated and remain in memory (because you share that value). What you want is for your `findPisaroPeriod` function to not construct a huge value in memory and cause all sorts of swapping. Using the below I get the result about 100x faster: findPisaroPeriod :: Int64 -&gt; Int findPisaroPeriod m = (period 1 1 0) + 1 where period !k !p !acc | (k `rem` m) == 0 &amp;&amp; (p `rem` m) == 1 = acc | otherwise = period p (k+p) (acc + 1)
Gotcha. Forgot how seriously people take that. Sure ones always going to be better than the other but it might not be obvious which for a long long time. I like that we can have both.
My guess would be that you need to figure out where your `libffi` installation put its `pkg-config` and add multiple paths to `PKG_CONFIG_PATH` by using something like env PKG_CONFIG_PATH=PATH1:PATH2 cabal install ... How did you install `libffi` and the GTK libraries (I guess, `haskell-gi` uses the `gobject` from GTK)? With Homebrew? If so the `brew info` command may be useful. It lists the location of `pkgconfig` at the end of the output.
The timed releases will, at the end of the day, help us to get changes out more quickly and predictable, because releases will be more frequent and the release schedule more predictable for contributors.
Please please please don't use Jenkins. It's flexibility will tempt you but it is never a good long term investment. Since this whole project seems to be looking long term then the CI server needs to be thought about carefully. I've been thinking about this a lot recently having decided to use Jenkins and then regretting it 6 months later for the tenth time. Jenkins doesn't actually give you much, it's not much more than a distributed scheduler. It provides a difficult and badly documented plugin system that has a lot of plugins however they are not very stable and many are of dubious quality. My experiences tells me that it would be well worth the investment, building something custom, perhaps on top of nomad or something similar, if the alternative is Jenkins.
&gt; Why why why? People keep bringing this up. Maybe because those people would prefer to use GitHub.
`instanceof` tests are not proper polymorphism. They are an antipattern. The polymorphism in OO languages should come from method dispatch (single or multiple), not through manual type tests. If the built-in dispatch is on the "wrong" argument, the Visitor pattern can be used. If you really what `instanceof`, I don't think new type classes are quite the way to go. You can't test for instance existence at runtime. Instead you make everything `Typeable` and safely coerce values with identical type representations. If you do mutliple inheritance via linearization of the parent tree (IIRC, Perl, Python, and Ruby do this), then this technique can continue to work. "interfaces" are also functors in the chain. Construction can get a bit "hairy" though. If you support virtual base classes (like C++), then you'll *probably* want to use type class instance(s) to carry around not only the the virtual base object pointers, but *also* the normal vtable entries, which will remove at least a few layers from your "functor stack"; you'll only need record fields for non-virtual members.
Please avoid the [Existential Typeclass Antipattern](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/), when possible. It's related and bad. Existential quantification in general is cool.
Does anyone know where OOHaskell went? http://lambda-the-ultimate.org/node/979
The prototype worked exceptionally well from my perspective. There is still an instance hosted [here](http://104.130.13.28/). Ultimately I ran out of energy trying to convince people it was a good idea as no one was willing to have the decisive authority about whether to do the switch. 
The order in which types appear when defining the constructor is actually significant. `data (,) a b = (a,b)` does actually care about the position of a and b in the data declaration. So, `instance Functor ((,) x) where ` defines a functor instance for the second type. If, instead, you defined : `data (,) b a = (a,b)`, then you'd be defining it for the left hand side of the tuple instead. Probably the easiest way to deal with this conundrum is to use newtypes. Newtypes allow you to wrap types in a semantically different but representationally equivalent context so that classes and functions can treat the wrapped data differently without altering characteristics like performance. Œª&gt;newtype OtherPair a b = OtherPair (b,a) deriving (Show) Œª&gt;toTup (OtherPair (x,y)) = (x,y)` Œª&gt;instance Functor (OtherPair n) where fmap f (OtherPair (b,a)) = OtherPair (f b,a) Œª&gt;toTup ((+1) &lt;$&gt; OtherPair (1,2)) (2,2) `toTup` lets us extract our regular tuple from the `OtherPair` newtype constructor, whereas `OtherPair` has allowed us to reverse the left and right positions of the tuple in it's declaration. This is kind of a silly dance for this particular use-case, but it's actually a very useful technique with some pretty far reaching implications - See the `Sum` and `Product` newtypes from `Data.Monoid` for another neat example of gaining extra behaviors from wrapping existing types.
The barriers to contributing to GHC are not that you have to submit patches on Phabricator. To even get to that point you have to 1. Work out how to browse tickets on trac 2. Work out how to build GHC 3. Work out how to begin to fix the problem The time it takes to build GHC on its first run is ample time to [follow the instructions](https://ghc.haskell.org/trac/ghc/wiki/Phabricator) about how to setup arcanist which is very straightforward if you have managed the other steps. 
the sidebar here seems rather lackluster. Anyway, I'm looking for a text/book (even a textbook would do) that uses Haskell as a theorem prover / proof checker or some other cool math-y text like that. Thanks
No worries mate.
Thank you for the feedback. Can you say why you are regretting your choice? Is it the bad documentation and poor stability of the plugins, or have you encountered other problems as well?
Theory sounds reasonable but without code I'm struggling to pin down what you actually would recommend doing. Could you write this up?
Well, the question is *why* they would prefer to use GitHub. Is it because of some specific GitHub features, because they're already familiar and experienced with GitHub or simply because they already have a GitHub account and used it for all their other open source work?
I really liked [Som1 Like You](https://www.youtube.com/watch?v=PNkoUv74JQU). Nice talk on practical application of dependent types.
How is it bad? That post doesn't precisely explain why it's bad.
The section following the one I posted points out some problems. I wasn't advocating for or against it either way. I just thought it was neat.
&gt;I believe it has to be understood as quotienting by the order, ie sorting in any order. Hence the type x better be comparable... But I have no proof of that, it merely makes sense from a combinatorial point of view. You might like the following paper, which provides semantics for fractional types: https://www.cs.indiana.edu/~sabry/papers/rational.pdf Alternately, you can see in the [paper](http://strictlypositive.org/diff.pdf) by Conor McBride that one can view a list's type somewhat like a geometric series.
I would assume, all of the above. Different people will have different reasons. Don't underestimate the appeal of using a single tool for a specific type of job. Many of us are seriously time-constrained. Learning a new tool and even switching between two different tools on different projects creates an overhead many are not willing to pay.
Your GitHub account and password can be reused on many such sites, including GitLab.com and the hypothetical gitlab.haskell.org. So I don't think that's a strong argument. GitLab.com, the shared instance used by many projects struggles under its load, but GHC doesn't have to use that as the canonical place, only as one of multiple. This is why people will say GitHub is faster, which isn't wrong but constantly improving as gitlab.com is worked on. Developers also fear the loss of things they've come to love on GitHub. Which is a valid concern, but I'm at least equally concerned by the many changes on GitHub I disagree with which I cannot undo or patch back in. If you're fully happy with GitHub, it's cool, but if you're not, you can blog about it and send a request to github support which is noted but seldom influences a future change. Yes, I've had multiple such support tickets with GitHub about behavior changes and weird bugs on the website. What I find more interesting is that technical folks seem to have more resistance with using a 2nd github or 2nd messaging-app compared to non-technical folks. I know many smart phone owners who are non-technical and have multiple messaging apps in active use. Therefore, I get the impression that tech folks have this desire to make everyone use one messenger or one GitHub or one $EDITOR. What's unfortunate is that this disregards the technical and future-proofing arguments of those that argue for using a self-hosted and federated FOSS solution. I mean, Hadrian is improving the build system, and using GitLab would improve the rest of the onboarding and project lifecycle. Or so I think, at least.
No.
I suppose the tools you wrote are or can be made public and be modified to support other targets (say, gitlab tickets, jira tickets, etc.), right?
If I understand correctly, you can think of it as a set, instead of a list sorted in any order. The former (at least mathematically) doesn't require any ordering. You're defining an equivalence relation on the type List(X); two elements are equivalent iff there is a permutation that takes one to the other. You can still do this even if the type X doesn't have a linear order defined on it...
I have built GHC more often than probably almost everybody reading this. I still don't want to use Phabricator.
&gt; Took 10s to load front page and its dynamic activity box. Then clicking around tabs in the linked bug of yours also took more than 2 seconds. omg that's slow 
This is very true for things we've invested time to learn and master, be it Haskell, init.el, your FVWM/XMonad config. It's a valid concern to have, though in the GitHub vs GitLab discussion, it should be noted that GitLab is similar enough and a superset of GitHub to make people feel at home with little friction if any. I have a hard time moving away from XMonad because I failed to make anything else work the same way. It's the little details of your personal workspace that fits you like a glove and gives you a config file that's unlikely to be comfortable for anyone else. If GHC uses GitHub, they cannot make such customizations, but GitLab provides more project settings and is patchable if some behavior is not upstream-worthy but vital for GHC. I know Phabricator is patched on haskell.org. Those who actually signed up on Phabricator to contribute and went through the process only for GHC are few and are those with a vested interest in improving GHC, even if it's a tiny patch here and there. This implies being passionate enough to make time for it, compared to a drive-by patch on github.com/haskell. We've already started accepting documentation patches on github, which helped resolve the problem with trivial diffs. If we can heave this to general patches by using GitLab, I think that this will turn into more users who will go down the rabbit hole of contributing substantial patches.
I didn't mind the web interface Phabricator and have used it, but I never got comfortable with arcanist and the default to collapse a carefully crafted topic branch into a single mega commit. Breaks bisecting and following the development of a larger feature. When you go read that commit later, it's one big commit and all the commit messages from the separate stages of development are missing. I have no idea how this works for development at Facebook, but I've painfully learned that Linus is right and prefer to have traceable changes when I get a large change to review. Seldom such a feature is written in an afternoon, so being able to follow the change process makes review easier.
But then again people seem to love infinite scrolling and all kinds of asynchronous replacement of DOM elements after the page supposedly loaded. I find that very irritating outside of notifications or visualization of an event I actively triggered. Try reading Youtube comments or browsing a Twitter or Instagram page. It's inexplicably user unfriendly.
Pardon my ignorance. But I'm "not allowed to post to this mailing list". So how do I contact them?
We can always add GitLab as an option to be considered. It ought to have the benefit of not requiring any local tool installation, but it will still be unfamiliar to most (I assume, maybe I am wrong?) It also, from what I know, will require more maintenance work from us (= less time spent on GHC itself). But I have one request. Please don't put down people who haven't signed up for Phabricator and demean GitHub contributions as "drive-by". Frankly, this is insulting to people like me who have been contributing to GHC for a very long time, have made a commitment to contribute to the project (e.g., I am on the committee evaluating GHC proposals), and whose livelihood literally depends on GHC (as professional Haskell developers). I'd really appreciate if we could keep this technical.
Yes, that is a good point.
yeah why are even the first page of YouTube comments lazily loaded? the video it's already streaming is so much more data than text. 
Well Haskell isn't really usable as a theorem prover, due to the fact that the type system isn't sound. `a = a :: False` and all that.
I like the LambdaCase extension in this kind of scenario: {-# LANGUAGE LambdaCase #-} genreMap :: Word8 -&gt; String genreMap = \case 0 -&gt; "Blues" 1 -&gt; "Classic Rock" ... 147 -&gt; "Synthpop"
Maybe they don't mean to leverage the type system but the values.
If the tool is not fit for purpose then what is the point exactly? We gain a lot of things by the use of Phabricator.haskell.org. Such as a sane cover letter for each commit and a link back to the review, and an audit trail of reverts and issues raised against a commit. In 6 months when your simple change is causing a segfault or a heap corruption and you have "moved on" or are "seriously time-constrained" it's others who have to fix it. How about instead of trying to shoehorn a project into an ill fitting tool, we use the right tool for the job.
Ah I see, so like programming a theorem prover in Haskell? Because yes that sounds much more doable and probably quite fun, I thought they were talking more about leverage Haskell itself directly.
I'll provide the opposite opinion: Of all CI servers I've tried, Jenkins is the one I like best so far. It has benefits including * being super easy to install and run (run a jar, that's it) * a high quality release process (very stable weekly version, even more stable 3-month LTS version) * a reliable security process with CVE numbers you can subscribe to, even in plugins * the commonly used plugins being very reliable (I haven't encountered one bug in the last two years of heavy usage) * a fully declarative build you can check into your git repo (with the new Pipelines plugin) * a new UI that doesn't suck ("Blue Ocean") * reliable incremental builds because it makes one working directory per branch, so stack/cabal/make builds can just continue if you want that to happen * full control over everything I switched some Haskell projects from TravisCI/CircleCI/... to a Jenkins server on a 50 EUR/month machine, because it doesn't consume my time with their weird particularities especially when it comes to caching and submodules, it builds 20x faster for the same price point and for rare cases when a build fails weirdly I can just SSH in and start working manually in the workspace where the build failed, which allows for a fast iteration to fix the problem. I think that building something custom is also a good idea, a Haskell based CI tool would be awesome. But I'd first get something working and _then_ build my own custom tool. I can set up a useful Jenkins instance in 5 minutes but I'd plan for at least half a year to implement basic useful CI server.
One doesn't have to host and manage GitLab if not desired, and it will be a more featureful GitHub in that case, more like Phabricator. I'm very sorry about my choice of words which you interpreted in a way I certainly didn't intend. Hopefully I can explain what I meant less ambiguously. Please, if you can, let me know if I managed to clear things up, as I feel bad about the ambiguity. drive-by patches: zero barrier to submission of trivial changes. Since GitHub is limited to trivial changes, and those are done while logged into github and editing in the browser, I used the word drive-by, but not in a negative way. One drives by, so to say, while reading documentation or the code, and decides "oh let me edit and submit a pull request". Takes minutes and no learning involved. It's a good thing and I've submitted numerous documentation fixes to projects after taking notes during reading the docs. Please excuse my word "drive-by", since you interpreted it in a way like I was making such diffs seem silly or unimportant. It's the opposite. If everyone who notices a fault in docs would be submitting a fix ala Wikipedia, we would have less errors. So it's a good thing GitHub is used for that right now. Phabricator signup: I absolutely didn't try to say more people must get comfortable with arcanist. To the contrary, I meant that only those who have a strong interest and passion to do so do it and that it's different enough to make us lose contributors. The only thing I meant is negative is that potential contributors are off-put by Phabricator. It's for many a new tool to learn and I don't mean those using it are any less developers. In fact, I have signed up and used Phabricator in another project, because I really wanted some change upstream. I like the code review feature of Differential and I have defended it and Gerrit in the past, so I wouldn't be me to bad mouth Phabricator. My whole point was that we can still have a more featureful and FOSS tool ala Phabricator but in a more familiar package, namely GitLab, making the whole barrier lower and process more inclusive. I apologize for expressing this in a way that seemed to demean developers.
I don't understand current web development practices.
No, I was asking how bottom is defined in Haskell. I wasn't sure if bottom was required to be distinct from all values, or could non-deterministically choose any possible value. In the former case, replacing 0*x with x changes the meaning of the program when x is bottom. In the latter case, you can't reliably determine whether certain error conditions occurred after the fact. The report says "The actual program behavior when an error occurs is up to the implementation", which looks like bottom can be replaced nondeterministically.
Thank you very much for the clarification! I am glad that we are having the same intentions.
On the other hand, "The actual program behavior when an error occurs is up to the implementation."
When `(&gt;&gt;=)` is properly inlined, this is very rarely a problem. But it does absolutely have to be properly inlined.
While the situation with emacs/vim is indeed real, I do not get this rejection of Stack. Stack IS a haskell enterprise development toolchain. There's simply no other option. 
I am sorry, but I don't buy the argument that GitHub is not fit for the job. The [Rust compiler](https://github.com/rust-lang/rust) team and [Swift compiler](https://github.com/apple/swift) team seem to do just fine. I don't understand your remark about simple changes.
For one thing, it can often be insanely slow, since you're requiring the use of runtime dictionary passing / dynamic dispatch (with no JVM-style JIT to optimize it). But mostly, it's just the exact same thing as passing a record of stuff around, except you're needlessly abstracting the type so that you can't reify it to something more concrete if you ever want to. We have parametric polymorphism for keeping it abstract where we want to.
/u/chak, another consideration: If using GitHub.com, there's no amount of money or clout you can utilize to convince them about a certain desired change. With GitLab and its adoption by other big FOSS projects, not only is it likely someone will share the same grievance and work on a patch, but it is also more likely that a change is accepted in the first place, regardless of the developer. Tickets I've submitted are in the open and have almost all been tagged and equipped with release targets. When I asked GitHub support about changing some feature or restoring another, it is noted friendly and vanishes in a black hole, because it's a proprietary web page operated by a team we have no influence over besides complaining in blog posts. So if the only options are Phabricator or GitHub, as the canonical place to work on the project, improving Phabricator would get my vote. I see GitLab as the FOSS GitHub (users will be familiar) with built-in lifecycle tools and the option to self-host if desired.
&gt; I think that building something custom is also a good idea, a Haskell based CI tool would be awesome. A fully developed version of Neil's bake?
I'm a simple fellow. I see Edward Kmett, I upvote.
&gt; being super easy to install and run (run a jar, that's it) The problem is source-based configuration/getting builds to work. I gave up with Jenkins last time I tried and I'm not really the quittin' type. Something like Drone CI is a better alternative if you don't want to use TravisCI or CircleCI. Cf. http://bitemyapp.com/posts/2016-03-28-speeding-up-builds.html I've heard good things about https://buildkite.com/ lately. Most of the Haskell I work on that have CI at all, OSS or private, are usually Travis. CircleCI probably in second place in commercial projects.
You have to subscribe to the list first.
On the other hand, GitHub hasn't lost customer data recently and has a much more mature deployment that has been battle-tested by attacks from state-sponsored hackers.
&gt; But, why two tools? Because different tools have different strengths; I have yet to see a ticket tracking tool that is as lightweight yet powerful as Trac. Likewise, I have yet to see a code review tool that makes reviewing large patches as effortless as Phabricator. From what I've seen, GitHub, even with its recent improvements, is no match. Admittedly, both Trac and Phabricator have their shortcomings but on the whole I think they serve their respective purposes quite well.
Nix is pretty good. Honestly I wouldn't embark on any large project without it. The advantages Stack has over Nix are all superficial compared to the advantages Nix has over Stack. I like Stack a lot and frequently use it to do simpler things. But Nix just does so much more than any other build system out there, regardless of language. Cabal new-build is still getting close to very good though. It's for people who don't want to depend on an LTS and prefer a more traditional path to dependency updating. Freeze files new-build do a pretty good stack impression these days. But the real problem is the logistical divide it creates. The Haskell committee literally cannot publish third party software as the official platform. They just can't be held accountable / responsible for software they can't control. This is the part that really causes the divide. Even if everyone on the committee loved Stack and Stackage, unless they controlled maintainership over them, they just can't make it the official platform. It really sucks.
Nixt cannot even be installed on many popular platforms. See here: https://github.com/NixOS/nix/issues/879 When nix is installed as simply and easily as stack let me know. 
Although I think you‚Äôre being a bit dismissive, that wasn‚Äôt really my point to begin with.
Considering that I've spent several hours of my life trying to install nix unsuccessfully, I would not say I am dismissive. Nix is simply not an option for me and probably for a lot of other people. 
Again, that wasn‚Äôt really my point. The point was that there are legitimate reasons not to use stack. It is not The One True Way. There‚Äôs never One True Way.
Hmm, I went back and reread your comment and I could not find a single reason NOT to use stack. You gave some reason t use other tool. But not a singlke one not to use stack. &gt;The Haskell committee literally cannot publish third party software as the official platform. How does this point not apply to Nix? 
Preferring another tool is a reason not to use stack, since you won't use them simultaneously. Just invert the statements to the perspective of Stack. Stack doesn't give you traditional dependency updates; Stack doesn't give you Nix-like levels of control over both your Haskell and system dependencies; Stack doesn't have quite as good caching as Cabal new-build. &gt; &gt; The Haskell committee literally cannot publish third party software as the official platform. &gt; How does this point not apply to Nix? You're right, it does. I am not claiming that Nix has trumped Stack in this regard. I'm merely saying that there is a *necessary* divide, in that the committee can't make Stack (or Nix) the official platform.
Don't feel like your code needs to be pure. It's totally fine to write code that has a ton of IO, especially at first. We require that impure code use the `IO` type, but we don't require that you only use pure code. For "batch" programs, it's really easy to avoid `IO` for the inner logic. You have some function: getInputData :: SomeArgument -&gt; IO SomeData And your `main` function calls that, processes the data, and then prints it out. main :: IO () main = do (fileName : _) &lt;- getArgs someData &lt;- getInputData fileName let someResult = process someData print someResult Your pure code is then process :: SomeData -&gt; SomeResult --- The story does get a little trickier for interactive programs. There are a wide variety of techniques for this sort of thing. I'm curious what you mean by "constantly reads the directory": is this a callback on an OS event listener? is it supposed to poll the directory and report changes?
&gt; If I have understood the desired design correctly I ideally have an IO main function which then progressively has less or ideally none IO as I 'go down' the hierarchy of other functions. I wouldn't say this. Rather, your functions should be separated between IO and non-IO. What you should do in this case is anticipate the data that you're going to get out of your files. As a simple example, let's say your files store a bunch of numbers. What you should have are some processing functions that don't use IO at all, but just take a list of numbers and returns whatever you get after processing. In your terminal implementation, it should have some call that gets the file, and then call your pure processing functions, something like proccess :: [Int] -&gt; SomeType proccess l = ... terminal :: String -&gt; IO SomeType terminal path = do filedata &lt;- readfile path return $ proccess filedata Your IO functions should do very little to no processing of your actual data. Instead, all data processing should just be done purely, assuming that you have some data already without assuming you get it via some IO.
&gt;How do I deal with the enforced purity of Haskell and IO. If I have understood the desired design correctly I ideally have an IO main function which then progressively has less or ideally none IO as I 'go down' the hierarchy of other functions. Well, first off, that's a big question in general :) The way Haskell words (with monadic IO) is not that you progressively peel back layers of IO, but rather that you use monads to make mixing pure and impure code effortless. As such, you should strive to write pure functions *when possible* and then worry about how to integrate them with the rest of your code later. &gt;I would love to have a separate function that just gets the IO Filepath and returns me a non-IO list, but that clearly defeats the haskell design as the IO taints everything. Here's what I would do: write functions that manipulate a normal list, like you wanted before. Then, you can do something like this: -- | This function takes a file path pointing to a directory and returns the items in it getDirectoryContents :: FilePath -&gt; IO [FilePath] getDirectoryContents = ..... -- | This takes a list of files and formats them in some pretty way. manipulateDirectoryContents :: [FilePath] -&gt; String manipulateDirectoryContents = ... main :: IO () main = putStrLn =&lt;&lt; fmap manipulateDirectoryContents (getDirectoryContents "my-directory") You could also write `main` using do-notation like so: main :: IO () main = do files &lt;- getDirectoryContents "my-directory" putStrLn $ manipulateDirectoryContents files
What I mean is that for example the terminal library has a keyListener, and as soon as say the "enter" key is pressed I want to go up or down the directory list and show the user the current content of that directory For example here is the example demo code from the library https://github.com/jtdaugherty/brick/blob/master/programs/ListDemo.hs which in the `appEvent` function on keypress adds a new item to the list (in case of the demo only a dummy value) here I would like to call a function like for example listContents = do n &lt;- getCurrentDirectory df &lt;- listDirectory n return df which of course then involves IO
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [jtdaugherty/brick/.../**ListDemo.hs** (master ‚Üí ec1693f)](https://github.com/jtdaugherty/brick/blob/ec1693f8e1b969c5124199eb872202dfcf2820a7/programs/ListDemo.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I see a lot of negativity or criticism in this thread, but I for one support and totally understand the decisions and considerations/concerns. You can‚Äôt make everyone happy. I get everyone has their preferences, but not being on GitHub is the equivalent of wanting to promote your business but not using Facebook. GitHub is open source made social, and (virtually) everyone is on there. If we want more contributions, expansion of the community, inclusion, etc, it makes sense. Complaints about views in the browser, etc, are overblown, IMO. Personally, the only time I ever use the browser for interacting with GitHub is commenting on, approving, and merging pull requests.
I've only briefly used `brick` before and I can't remember it at all. So, I notice that the last thing in the type signature of `appEvent` is `T.EventM`. I scroll up, see it's imported from `Brick.Types`, and go to the hackage page to view the docs for that module. [`EventM`](https://hackage.haskell.org/package/brick-0.28/docs/Brick-Types.html#t:EventM) is documented here. It's a monad (hinted by the `M` at the end of the name?), and it has a few useful instances: one of which is `MonadIO`. That means, we can call `liftIO :: IO a -&gt; EventM s a`. So we *can* use `IO` in this! It looks like all those functions call `M.continue` with the new data they want to do. So I want to look that up. `M` refers to `Brick.Main`, so I look at that module and grab [`continue`'s documentation](https://hackage.haskell.org/package/brick-0.28/docs/Brick-Main.html#v:continue). Looks like it takes a "next" state and goes to the next bit of the event loop. So, given `listContents :: IO [String]`, it looks like we can do `liftIO listContents` to give us an action with type `:: MonadIO m =&gt; m [String]`. Since `EventM` is an instance of `MonadIO`, then that can become `EventM s [String]`. I'm not sure how you'd wire that into `brick`.
thanks I appreciate the practical way to go about this especially and this is a good starting point. Monads are still very opaque to me though, I still have trouble following the types when monadic operations are involved. Coming from a very 'non-haskellish' way to reason about these things, is there a good way to get more easily familiar with these structures or is it just a lot of practice?
Similarly, I was hoping to develop a `Category` instance for `Lens`, with all four type variables. I was disappointed to find this can't work. I think it has something to do with GHC being unwilling to unwrap the type level tuples or something. {-# LANGUAGE KindSignatures #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE RankNTypes #-} module Test where import Control.Category type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f b data CLens :: (*, *) -&gt; (*, *) -&gt; * where CLens :: Lens s t a b -&gt; CLens '(a, b) '(s, t) instance Category CLens where id = CLens $ \k s -&gt; k s CLens f . CLens g = CLens $ \k s -&gt; f (g k) s --- [1 of 1] Compiling Test ( Test.hs, interpreted ) Test.hs:16:8: error: ‚Ä¢ Couldn't match type ‚Äòa‚Äô with ‚Äò'(s0, t0)‚Äô ‚Äòa‚Äô is a rigid type variable bound by the type signature for: Control.Category.id :: forall (a :: (*, *)). CLens a a at Test.hs:16:3-4 Expected type: CLens a a Actual type: CLens '(s0, t0) '(s0, t0) ‚Ä¢ In the expression: CLens $ \ k s -&gt; k s In an equation for ‚ÄòControl.Category.id‚Äô: Control.Category.id = CLens $ \ k s -&gt; k s In the instance declaration for ‚ÄòCategory CLens‚Äô ‚Ä¢ Relevant bindings include id :: CLens a a (bound at Test.hs:16:3) | 16 | id = CLens $ \k s -&gt; k s | ^^^^^^^^^^^^^^^^^^^ Test.hs:17:42: error: ‚Ä¢ Could not deduce: b2 ~ t1 from the context: (b ~ '(a1, b1), c ~ '(s, t)) bound by a pattern with constructor: CLens :: forall s t a b. Lens s t a b -&gt; CLens '(a, b) '(s, t), in an equation for ‚Äò.‚Äô at Test.hs:17:3-9 or from: (a ~ '(a2, b2), b ~ '(s1, t1)) bound by a pattern with constructor: CLens :: forall s t a b. Lens s t a b -&gt; CLens '(a, b) '(s, t), in an equation for ‚Äò.‚Äô at Test.hs:17:13-19 ‚Äòb2‚Äô is a rigid type variable bound by a pattern with constructor: CLens :: forall s t a b. Lens s t a b -&gt; CLens '(a, b) '(s, t), in an equation for ‚Äò.‚Äô at Test.hs:17:13-19 ‚Äòt1‚Äô is a rigid type variable bound by a pattern with constructor: CLens :: forall s t a b. Lens s t a b -&gt; CLens '(a, b) '(s, t), in an equation for ‚Äò.‚Äô at Test.hs:17:13-19 Expected type: a1 -&gt; f b1 Actual type: s1 -&gt; f b2 ‚Ä¢ In the first argument of ‚Äòf‚Äô, namely ‚Äò(g k)‚Äô In the expression: f (g k) s In the second argument of ‚Äò($)‚Äô, namely ‚Äò\ k s -&gt; f (g k) s‚Äô ‚Ä¢ Relevant bindings include k :: a2 -&gt; f b2 (bound at Test.hs:17:32) g :: Lens s1 t1 a2 b2 (bound at Test.hs:17:19) | 17 | CLens f . CLens g = CLens $ \k s -&gt; f (g k) s | ^^^ Failed, 0 modules loaded.
Practice makes perfect :) It can be very difficult to understand monads if you haven't worked through the underlying concepts (types, type classes, higher kinded types, etc.). The best book I've seen for learning Haskell is the http://www.haskellbook.com and it has a fantastic lead up to monad.
Broadly speaking, you don't. We have better tools for that sort of thing (eg type classes) in Haskell.
Exactly. And elsewhere in the comments there's a nix vs stack moment in progress. 
So, they're not like interfaces then...
`mapM`, `sequence`, and the functions in Control.Monad can be really nice for composing IO operations and for dealing with the : You did a `[IO n]` when you meant to do a `IO [n]` problem. `sequence` solves that problem when it happens, `mapM` and his friends in Control.Monad help avoid that problem happening in the first place. Also, as others have stated - Sometimes you are just legitimately composing a bunch of IO actions. It's totally fine to do that. 
Sort of. You just need a different upcast function. Suppose the Java class also `AcmeBicycle implements Bicycle, Vehicle`. In the Haskell, we'd have a type data Vehicle = Vehicle { the vehicle methods } and another upcast function acmeBicycleToVehicle :: AcmeBicycle -&gt; Vehicle and, for any Java function like: public void ride(Vehicle v) ... we'd write: ride (acmyBicycleToVehicle someBike)
I am not a Jenkins pro but I've managed a handful of projects through it over the last few years and I always get frustrated with it and wonder if I wouldn't be better off just writing something custom that does only the thing things I want it to do. I also agree with the comment that many Jenkins plugins are of dubious quality. We've had to modify some widely used Jenkins plugins in order to get behavior we expected. On the other hand, there's an enormous convenience to Jenkins and typically once I've set-up a project, I rarely have to modify it. However, I do groan whenever thinking about setting up a new Jenkins project: I'd rather pass that task off to someone else who has a chance of enjoying it. I offer this as a small opinion from someone whose primary role is not setting up CI servers.
What??? I wasn't aware that phabricator.haskell.org lost data recently or had even been the target of state-sponsored hackers. Where can I read up on that?
But then you don't have polymorphism, you have a function that only works for bicycles. If you then try to make it work for any kind of vehicle, you'll have to make a type class (e.g. a `Is` Vehicle or Vehicle a). Right?
Yeah I noticed stuff was being "lifted" to a relative version like `List`. It seems like there could be some free structure there? But as I understand, the issue is [that the intuitive approach puts a delta at each cons](https://github.com/ekmett/coda/blob/bf0d23fb3c0367fcefb289db84200b436a2034fd/src/Coda/Relative/Class.hs#L109), while ["RelativeList" factors this out](https://github.com/ekmett/coda/blob/b6d95c7a01c87ff8a3390e351d477887345cf624/src/Coda/Relative/List.hs#L36). Is there a way to get the factored result for free? It seems like it would save a lot of work and help separate some of these independent ideas.
hey.,Thanks. let me think in loud voice. assuming that the presentation layer is external like DB so it can be for example some react view that bind data form the model layer in order to respond and present it to the user. however, I cannot be sure that you means that since I think you are considering it not external and outside the control then indeed it is crucial for the business logic.
Java's interfaces rely on subtype polymorphism, which Haskell doesn't have. In my limited experience with Java, it is very rare to see something like this: public void &lt;A extends Vehicle&gt; ride(A vehicle) which does map to the type class style: ride :: HasVehicle a =&gt; a -&gt; IO () I have *generally* found it more pleasant to just accept a tiny amount of boiler plate and write manual conversions.
&gt; that clearly defeats the haskell design as the IO taints everything Don't fall into the trap of thinking that IO is _bad_! The program you've chosen to right is bound to be IO-heavy because of its very nature, and that's fine! Haskell just helps you (forces you, haha) to be honest about what you're doing. And it gives you the tools to express yourself clearly. Some of your functions will be pure, and you'll see that in the type signature. Some will require IO, and you'll see that, too.
Surely you've seen a lot of code that was something like: void ride(Vehicle vech){...} Vehicle in this case can be an interface, no need for generics. This is (in my experience) one of the best uses of interfaces.
Of course! And that matches the Haskell concrete Vehicle better, as well, for most of the semantics. Consider: void foo(Vehicle a, Vehicle b) vs foo :: Vehicle a =&gt; a -&gt; a -&gt; IO () The former guarantees only that both parameters are subtypes of `Vehicle`. The latter guarantees that both parameters have the same type which is an instance of Vehicle. You could alternatively do foo :: (Vehicle a, Vehicle b) =&gt; a -&gt; b -&gt; IO () But that's pretty uncommon to see in Haskell.
Its not that uncommon. The Eq, Ord, Show, and Bounded instances for (a, b) are a good examples of how useful that pattern is. Could the post be updates to talk about objects rather than interfaces? At least at my University and work places they've been considered very different things and it is worth being clear about the difference.
On the fly code removal without reverse has potential to get annoying pretty fast in my opinion (deleting function and wanting to add it somewhere else after couple of minutes "staring at code"‚Ñ¢ only to find out I need to import it again...).
&gt; Practice makes perfect : No, practice only makes better. I've been doing Haskell for nearly 10 years, 5 of them professionally, and I'm still far from perfect.
The last Haskell snippet I posted is extremely uncommon and would certainly fail code review in the places I've worked. You almost always want your types to communicate more than what this is communicating, and you almost always get better error messages and usability by skipping the subtype classes and converting directly to the intended types. The post does a good job of discussing interfaces vs objects IMO given how they're commonly used in Java. Thanks to subtyping, if I say: public foo(SomeThing a) then `a` might be a `SomeThing` class, or it might be something that `extends SomeThing`. Interfaces are abstract classes which permit multiple inheritance. Haskell's type variables correspond to Java generics, so if you really want to get into type classes, then you're going to need to be talking about generics in the Java code.
 newtype Practice a = Practice a type Perfect = Cofree Practice It never ends!
"just fine" is a relative term. They have an army of bots just to do the simplest things. Also have a spaghetti web of labels and tags. So yes it works, with a lot of invested infrastructure behind to make it more like fab. LLVM uses phabricator and GCC is giving it a trial run soon. I can name others as well. 
You can think about monads as an interface: interface Monad: : Monad (Monad a) -&gt; Monad a map : (a -&gt; b) -&gt; Monad a -&gt; Monad b singleton : a -&gt; Monad a Lets try to implement it for `List`: List implements Monad: -- flatten [[1,2],[3,4]] == [1,2,3,4] flatten lists = foldr (++) [] lists -- map (+1) [1,2,3,4] == [2,3,4,5] map f [] = [] map f (a:as) = f a : map f as -- singleton 1 == [1] singleton a = [a] Pretty obvious wasn't it? What about `Either` though? (Try to do it yourself before checking my solution ;) Either e implements Monad: -- flatten (Right (Left "error")) == Left "error" flatten (Right (Right a)) = Right a flatten (Right (Left e)) = Left e flatten (Left e) = Left e -- map (+1) (Right 10) == Right 11 map f (Right a) = Right (f a) map f (Left e) = Left e -- singleton 1 == Right 1 singleton a = Right a See, this was a little more tricky, because you might have tried to do something like map f (Left e) = Left (f e) but `Monad` is defined only for `a` in `Either er a` so we might safely ignore `Left` every time it pops up. Do you feel confident you could implement `flatten`, `map` and `singleton` for `Maybe a`? Go and try it. ------- Now haskell interface for monads uses different names than `flatten` `map` and `singleton`: interface HaskellMonad : &gt;&gt;= :: HaskellMonad a -&gt; (a -&gt; HaskellMonad b) -&gt; HaskellMonad b return :: a -&gt; HaskellMonad a Now you might easily guess that `return` is actually `singleton`, but what is `&gt;&gt;=`?? Hmm, lets see the type signature of `flatMap` flatMap f monad = flatten (map f monad) Any idea? ... ... ... Maybe you should try it with `List a` first: flatMap f list = flatten (map f list) maybe an example will help flatMap singleton [1,2,3] == flatten (map singleton [1,2,3]) == flatten [[1],[2],[3]] == [1,2,3] soo `list :: List a`, `f :: a -&gt; List b` and therefore flatMap :: (a -&gt; List b) -&gt; List a -&gt; List b and how does it relate to `&gt;&gt;=`? Well (&gt;&gt;=) = flip flatMap To sum it up, we have found the three functions `flatten`, `map`, `singleton` so useful and common, that we created interface for them and called it `Monad`. Why are they so useful is a different topic though. 
I would say that is about the fine details. Does error "Foo" show Foo or a generic Error? Does it include line numbers? That sort of thing. Given the interpretations where the report contradicts itself within a few sentences or the one where it is about the fine details of how errors are reported I think the second one is more reasonable. 
While this may sound snarky; but I'm genuinely curious. But have you ever done in depth code reviews on GitHub?
Is Category even polykinded?
Perhaps run memcheck? Sometimes ram issues only affect certain access patterns, and programs compiled with GHC often have abnormal access patterns compared to other languages. This has only happened to me with some pretty compute heavy stuff, though.
Alright, I'll end up giving my two cents as well. As a contributor to GHC, I did have to learn to use phabricator and arc. Yes they are non-trivial, but once you stop trying to treat arc as git, and treat it as a diff uploader to a review board, I stopped having any issues with arc. Obviously there is going to be a pushback from existing contributors, who are happy with arc. And we do not know the number of people who would have contributed if we had a GitHub first approach. As such the current contributors might be biased, as they made it over the phab/arc hump already. I do agree that GitHub has a very low level to entry. Yet I do prefer phab's diff centric (as opposed to commit centric) code review. My main issue is that we are spread out too much across tools. And I'm very sad to see that /u/mpickering s Trac to Phab migration didn't go anywhere. However I feel like I should have little say in this. And those who do and have to actively use the issue tracking should have a say in this. Maybe I would use the issue tracker more if it was not trac, maybe not. I just feel it's always working against me, logging me out, awkward "cc me on this issue" handling, and never really finding what I'm looking for. Of course this could totally be me being just incapable of operating it. Let me pose a final question: who would have contributed to GHC, and if so what would you have contributed, if GHC wasn't using phabricator and arc?
That's a multiset, not a set.
As an exercise you should try to implement `flatten`, `map` and `singleton` for data Foo = Foo data Bar a = Bar String a (Bar a) data Baz a = BazA (a, a) | BazB Bool a | BacC Int don't worry, you don't need type classes (interfaces). Just use different name for the functions: flattenFoo, mapFoo, singletonFoo flattenBar, mapBar, singletonBar ... 
Obviously my mileage may be different but: 1. It's reasonably easy to run but I wouldn't say super easy. It requires a specific version of Java. I wouldn't trust that it will run on every platform that has a JVM but it's not bad. 2. In my experience the release process is not reliable at all, things often break that have been fixed previously. Really this is one of the worst parts. Upgrades are also very dangerous, settings can just randomly dissapear/change. 4. I have experienced many plugins being unreliable, some which are presumably quite common. 5. The pipelines stuff is simply awful. "Fake Groovy" as we like to call it is so annoying and the security model is really weird. Also, the Jenkins settings themselves aren't realistically possible to use via settings files, they consist of lots of complex xml files, often not organised consistently. It always requires manual config via the web ui. 6. I find Blue Ocean a bit rubbish :-) This is just one experience I've had, this is many, many experiences, always with the same end result. I'm sorry to sound so negative, I just have many emotional scars caused by Jenkins (although less than windows) :D
bake is based around a particular branching model (also happens to be one that I think is bad). I'm not sure how easy it would be to extend but it would certainly be nice to have an all-Haskell implementation.
Assuming you don't upgrade Jenkins, any of the plugins or any of the nodes you are running it on :O
I had a look at nomad in more detail again last night (I have used it a little but never in prod). It runs on many platforms including QEMU, it looks like it would be a good platform to run many builds on many different platforms at the same time. What would be required to turn it into a CI system would be an app that stores state (maybe in a decent DB), provides an endpoint for web hooks, and creates and triggers jobs. As nh2_ points out this would be a bit of a pain to install as it would require installing nomad (and possibly a database) separately. However the cool stuff you could get out of using say nomad, postgres and elasticsearch in combination would be great. Maybe I'm just dreaming.
What does it mean the type system is not *sound*? What are the implications?
You can still parse the parameters: `type API = Capture "imgid" ImageId :&gt; QueryParam "size" ImageSize :&gt; Raw`, `server imgid imgsize = ...`.
If you need a simpler kickstart, try "Learn you a Haskell", I like it very much. It has no exercises, so it is better to follow it with some project in mind.
Well with a sound type system having a value of type `Foo` is a proof of `Foo`. In Coq this is fine, but in Haskell the fact that you can do `undefined :: False` kind of ruins that. The coercions system is sound so don't worry, I'm not implying you can get any runtime type errors or segfaults.
There is no free lunch. There is e.g. http://www.cs.nott.ac.uk/~pszgmh/pih.html but it's still 318 pages.
My first pass is making sure I can do everything as efficiently as possible. Later on, once I have a sense of how slow some bits and pieces I may be willing to slow things down and accept the extra boxes that, say, working generically are likely to strew in my path. =)
RelativeList is one way of pulling a delta out front, but its not universally the best way. I wind up having to put them in each constructor in the Map for instance to deal with union.
You might try the Yesod book (it's more of a cookbook-based approach), but based on your question I think you might need to adjust your applications. I'm not really sure why you want to build a scalable application in a language you don't know. You're going to need to understand monad transformers at some point down the line. For what it's worth, refactoring Haskell is much easier than refactoring in non-functional languages.
I'd be quite happy to move forwards with this. One of the biggest sources of friction for me is the different kinds of markup we have in our different tools, and moving Trac to Phabricator would eliminate that. Feel free to bring it up again on the ghc-devs list. (although I guess we should wait for the outcome of the current discussion about moving away from Phabricator)
With Phabricator you upload each commit separately, and Phabricator tracks the dependencies between them, showing you a tree of commits in the UI. Here's an example: https://phabricator.haskell.org/D3913 Scroll down to "Revision Contents" and click on the "Stack" tab Each commit is reviewed, revised, and accepted separately. Having used this workflow for several years I like it - there's some overhead to maintaining the stack of diffs and rebasing when necessary, but being able to revise the diffs in the middle of the stack without having to add more commits on the top (thereby destroying the nice history) is really useful. In GitHub you would have to force-push the whole PR, and then you lose all the existing comments in the code review.
As an active contributor you *do* have a strong say in this. Thanks for the feedback. I'm not sure that the Trac to Phab migration is necessarily dead. The main bottleneck was the effort needed to properly evaluate it, it's always easier to just stick with what we have than acquire the activation energy to make a change. But I for one would be happy to ditch Trac.
Yes: `Category :: (k -&gt; k -&gt; Type) -&gt; Constraint`
Relevant ticket https://ghc.haskell.org/trac/ghc/ticket/7259
Read it 8 hours a day...
Yes, the points about bottom, exceptions, and infinite lists are all well and good. However, there is also somewhat more practical issues of space usage, namely your optimization, while in most cases is desirable, can also introduce memory leaks. (Though going from null to length can also cause space usage problems, one isn't inherently better than the other on that count.)
If you want a platform that allows you to fake your way through it, correctness and certainty be damned, then Haskell is not your friend. Also, building scalable applications is hard regardless of the technology you use; scalability is a matter of architecture more than individual technology choices - scalable applications can be built in Haskell, but it is not significantly easier or harder than in most other languages. That said, you don't need to learn everything in the book in full depth before you can be productive. You may also want to complement the book with other material, such as the typeclassopedia and Stephen Diehl's "What I Wish I Knew...", and I highly recommend joining at the very least the #haskell IRC channel on freenode - one of the largest and most helpful communities on all of freenode. If you are already versed in scalability issues, I recommend preferring simpler Haskell libraries, and use scalability strategies you are already familiar with; e.g. Scotty rather than Yesod, simply because it is more straightforward, more idiomatic Haskell, and lighter on the magic.
Why do you need to be logged in to github and use the browser? Why can't you just fork and sumbit a pull request?
&gt; it has to be understood as quotienting by the order, ie sorting in any order. Hence the type x better be comparable Your logical misstep is here. Quotienting by the order in which the elements does not require a order*ing* on the elements themselves. [/u/Solonarv has it right](https://www.reddit.com/r/haskell/comments/77he9y/e_is_the_type_of_sorted_list_but_in_which_language/dommi99/).
Why are patches used? I've never understood this, how does a branch based approach make things not scalable (other than the github UI being broken)? Why not use a local tool to view the pull request diff anyway, why do you need to use the github UI? Currently I think you may need to use it to add a comment but it is probably easy to write a tool to use the http api to make the comment. I don't see why you need to use the web ui if you don't want to.
[Originally for C++ but I think it could help your predicament.](https://www.geek.com/wp-content/uploads/2010/03/program21days.PNG) But seriously, it's going to take time. You're going to have to find it if you need it quick unfortunately. You can't concentrate knowledge.
It's not about the github.com UI when it comes to work flow. Github.com supports only the github.com development model. Which is not the same as the git model. As an example. If I have a feature, which is split over 15 different components for easy of review. Github forces me to have 15 different branches instead of what I currently have which is 15 commits. Where the patch you submit is just based on HEAD~1. Why is this desirable? Because It makes rebases a lot simpler. It also allows for other bookkeeping operations such as reordering of patches, splitting or easy merging of patches. I then submit only the chunk I want reviewed, where as Github would need a branch with a history, so instead of seeing the changes only introduced in that change, you see the previous ones too. Phabricator allows you to submit an arbitrary diff, which does not need to apply to the current master. In fact it's parent can be non-existent. You can then tell it about the dependency between patches in the same series. So arc knows if you want to apply patch 5 out of 14, it needs to apply patches 1-4 as well. Also knows that even if patch 5 is accepted, you need 1-4 to be accepted before continuing. And it tells you this! Github lacks support for these operations, it's only mechanism is essentially a mention. But it can't enforce these constraints. 
The wikibook is nice, if you have prior programming experience.
You could, like me, buy a 300 page Python book, then spend 8-10 years slowly buying about 3500 more pages as you master a more off-the-bat-easier language, and then eventually pick up a 1000 page Haskell book because of your longstanding frustration with not being able to reliably **know** what the Hell is happening in your program once it grows beyond a certain size.
**But why?** Do you need to write a complete program for work? Do you have a deadline? Is there any other reason you have time constraints, so that you are willing to sacrifice correct understanding? You need to provide more context and more justification for the desire to rush things true. Have you programmed extensively in the functional paradigm before? If not, then trying to write a real-world Haskell program in a week is like trying to write a real-world Java program in a week without any knowledge in programming. Haskell is so radically different from any other programming languages, that it requires you to unlearn what you've already learned about imperative programming and rewire your brain to think differently. It's still radically different even from what is usually sold as ‚Äúfunctional programming‚Äù. For example, I used map/filter/reduce, partial application, and composition in Python and Emacs Lisp long before learning Haskell, and I still had to reconstruct the way I see programming. Haskell is not just map/filter/reduce, or algebraic data types, or currying. It's also monoids, and how `mappend` must be associative. It's also functors and applicative functors, and that typeclasses are about laws, not interfaces. It's about the fact that you can't ‚Äòextract‚Äô a value from the IO type. No, Haskell is not as hard, as people paint it. It doesn't require a PhD or any maths knowledge beyond high-school. It's not just for the elite. Knowing it doesn't make you morally superior. That's not what I am saying. I am saying a very simple thing: Haskell is *different*. To learn it, you need to learn it properly. Properly doesn't mean slow. It doesn't mean you have to suffer. It doesn't mean you have to study category theory or even Haskell's own advanced features. It just means you have to learn from simple to complex, without skipping prerequisites. With Haskell, you can't rely on your past knowledge and intuitions. It's a completely different way of doing programming. There are no for-loops, variables, increments. You can't just dive into it, thinking that it's just Java with different syntax. In the long run you won't win anything by skipping the basics. You'll struggle to write even a simple hello-world-like application, you'll spend hours debugging and Googling without any clue what's going on, you'll keep spending days reinventing the wheel, where you could spend an hour learning the wheel. See also [*Why we don't chuck our readers into web apps*](http://bitemyapp.com/posts/2015-08-23-why-we-dont-chuck-readers-into-web-apps.html).
If you are a very good programmer, I might suggest getting a taste from [cs240h - Basics](http://www.scs.stanford.edu/16wi-cs240h/slides/basics.html) ([and here are the rest of the lecture notes](http://www.scs.stanford.edu/16wi-cs240h/slides/)) and supplement with the [Haskell Book](https://en.wikibooks.org/wiki/Haskell). But remember that sometimes rushing things just means it's going to take you more time in the long run.
[removed]
I think it's really have to know more details about the system I was designing to make any concrete recommendations. My main post in the thread is just meant to provide alternative, not claim it is better. I'm not a fan of "one-size-fits-all" solutions. Partially, this is due to being a bid guy with short legs and a long torso and never having any one-size-fits-all clothing fit, at all. Partially this is to avoid programming pattern dogma.
Information loss. Many OO designs have a lot of operations that preserve subtype. It's difficult to model that when you are throwing existentials around.
I thought something along those lines a while ago. I made some mistakes in my analysis, which some people corrected. See https://www.reddit.com/r/haskell/comments/6vn968/compose_conference_choose_your_own_derivative/dm21tbp/
&gt; Why are patches used? Try asking [Darcs developers](https://en.wikibooks.org/wiki/Understanding_Darcs/Patch_theory)...
Related: You could instead use `first` from `Bifunctor`. (I.e. mapping over the values of types of type constructors applied to 2 type arguments.)
Is seems like we did this better [13 years ago](http://lambda-the-ultimate.org/node/319), anyone still have the OOHskell paper? I can't find it on Oleg's site, and it looks like it was originalyl elsewhere. Several of the papers articles still on Oleg's site do refer to OOHaskell, so I don't think it's a fever dream.
It should generate / show the type signature, too! ;)
Sounds like your event loop needs to be in `IO`, because on each iteration the polling it needs to do ivolves actually hitting the file system. But, I'm not sure the internal parts need to be in `IO`. They could be `[FilePath] -&gt; a` for some a, that might include a command object, a new directory to list, or whatever.
"Normal pregnancy" is a time which is 40 weeks, but I need fastest way to produce a generic son, any helpful suggestions please?
As /u/simonmar points out, you can have fine grande differentials. that are logically separate from each other but build ontop of each other. This is usually done by specifically instructing arc to diff only the range the patches affect to diff. largish diffs may come up with git as well, when the author just ends up putting everything into a single commit. I feel this is more of the personal choice of the author, while I do agree that phabricator favors a squash-commit style work flow whereas many other project do not squash commits for PRs. This is just to illustrate that there are potentially two different approaches. And I certainly do not want to get into any kind of my-way/your-way discussion here. For your carefully crafted topic branch, I would suggest to split it up into multiple diffs for review. Maybe it helps to think of diffs, as independent reviewable chunks of changes that make up the final piece of work. Instead of reviewing the final piece of work in one go. Does that make sense?
I have, and they worked fine. It's possible they weren't in depth enough or big enough to meet the bar set in this thread, but I've had no problem. 
I gound both [paper](https://arxiv.org/pdf/cs/0509027.pdf) and [code](https://github.com/nkaretnikov/OOHaskell). Looks like someone tried to polish it up last year.
Maybe [pay someone to train you](https://www.fpcomplete.com/training)? A teacher would be able to adapt to your particular situation, going quickly over the bits you understand quickly and going slowly over the bits you struggle with. This way your time would be better spent, on the things you want to learn and on the things your teacher knows you need to learn before that.
Oh, yes that's correct! Missed the subtlety somehow.
The point is that in the first case foo :: Vehicle a =&gt; a -&gt; a -&gt; IO () both parameters must have the same implementation of a Vehicle. For example they must both be bikes or both be cars, if you interpret typeclasses as interfaces and instances as implementations. That's because this coherence property is the entire point of type-classes. However, because of their implicitness that they also allow for a limited form of overloading, which superficially resembles sub-typing. Type-classes are not interfaces, but the exact opposite of interfaces. They are constraints on interfaces (= types).
In my opinion Haskell is best learned with some time investment to get the fundamentals right. It's like flying a large, modern air plane: You can get lots of stuff done in short time when flying it, but you better understand what those knobs and instruments do or you might be flying in circles. That said, if you are willing to throw money at the problem (and "building scalable application"s sounds like you might be doing this for work), then you might take a short cut by getting in-person Haskell training from one of the Haskell consultancies. For example, I've been giving Haskell training with FP Complete (training page [here](https://www.fpcomplete.com/training)) recently. The trainees were employees of a company that started a new project in Haskell. Within 10 days of training (around 2 hours per day) we got to a stage where they could work on the core task (webserver programming, data processing, database access) and learn more on-the-job. In-person training doesn't magically reduce the time to become a proficient Haskeller to 20 hours; you still have to learn stuff by yourself. But it does provide a big speed boost to have a trainer who, from the big pile of stuff to learn, picks exactly those topics you need to work on *your* project, who is dedicated to answering *your* questions, and who can eliminate road blocks in case you hit any. (In the air plane analogy, an in-person copilot trainer can show you within a few flights how to fly this specific air plane on this specific route, and which instruments matter for that, so that you can do it confidently for your day job as quickly as possible.)
Maybe we should just blanket allow PRs on GitHub, and then, once they outgrow what GitHub can handle, or the reviews are willing to handle on GitHub, to move those over to Phabricator? One thing I'm a bit opposed to is, losing phabricator, as I very much prefer phabricators workflow. There are supposedly some technicalities with PRs on GitHub and how the mirroring works that would need to be ironed out.
I may have made half of those up lol. Not that I'd mind knowing what a patamorphism is, of course :)
We had agreed on this plan (allowing GitHub PRs and converting them to Phabricator diffs) before, it just never got implemented.
I'm aware. My point is that the above signature for foo is not necessarily the only reasonable one. E.g. simulating two vehicles colliding should not require them to be the same type of vehicle. I don't know why this kind of function is considered so rare by the responses to my comments. Its a pretty normal thing to need to do (namely, combine things of different types that have the same interface).
I often wish that Haskell would have a standard typeclass for this, something like: class Upcast a b | a -&gt; b where upcast :: a -&gt; b Then you could just write `ride (upcast bike)`. It would be even better if there were a standard unary operator for this, e.g. `map ride [?bike, ?car, ?motorboat]`.
&gt; To be really fair, is not a problem to [reimplement crypto when you don't know what you're doing]....only if it is applied in deployed applications. seems to be in contradiction with the fact that the library is available on hackage. This is dangerous.
I think we actually do allow GitHub PRs. And there were a few small ones, so the need to migrate them to Phabricator never came up. It did however say that we allow "small" PRs. I'm just wondering if we should simply drop the "small" restriction? /u/bgamari would know better though I assume.
Hey, Not quite. Let's say you're writing a yesod app. Your view is the function that produces the html component that renders a section of your app. A view model would produce the state that the component should be in. Think of it this way: Let's assume you're to write an app. Now, this app will have a native desktop GUI front end and a web front end, but you don't know what technologies these will be. All you know is that they'll have the same set of screen components, and the designer might rearrange them so they're more natural for whichever platform they're on. Your frontend will probably be built with one or more frameworks, be it shakespeare templates or whatever. You know what set of components you need but not necessarily how they'll be built or what they'll look like. What you do know is how they'll behave. Your View can be thought as your code that builds out the UI of the components themselves using the external framework as a tool. Your View Model dictates how that view behaves. It will hand the view it's initial state and notify it of any updates. It'll also take events from the view and handle them properly. Events can be full fledged data types, or they could simply be function calls. Doesn't matter here. The point is, while your view would possibly change between your desktop ui toolkit and your web ui toolkit, the behaviour is the same. The states you produce are the same, and the way you interact with the rest of the system remains the same. So, we can abstract that out into this view model, which is a module of functions, a record type, or whatever, and defer figuring out exactly what the view will be rendered with until a later time. Your comment about "bind data from the model layer" is correct. Your domain layer IS your model layer. It is a collection of functions that "get shit done" and they're there, separate from your view models, for any of your view models to use. Imagine the scenario when you have the same action being done by 3 different components! Instead of having to write out that logic in 3 different places, we write it once and use it everywhere. So our Model contains all of our abstract business logic, and talks to instances of type classes it defines to get work done, like reading from a database or whatever. The view model takes input from the view and output from the model to define view behaviour via some state object. The view then renders the state object. There's a relationship to remember here, a "dependency" structure: * View -&gt; ViewModel -&gt; Domain stuff -&gt; Entities * Data -&gt; Domain stuff Where -&gt; is representative of "depends on" * The view knows about the view model, and listens to it for state updates and propagates user events to it * The view model doesn't know about the view, just about the behaviour of a view, but does know about the domain stuff. * The domain stuff only knows about other domain stuff (like type classes for database interaction) and your entities * The entities only know about themselves * The data is dependant on the domain stuff because it's the layer where you'd have Persistent or whatever other lib You do all of this, because you don't want to have to deal with deciding what frameworks (Persistent, Yesod, Snap, etc) to use when you start, and you want to be able to swap out that framework with something else with absolute minimal impact on the core logic of your application.
Actually, we do allow GitHub PRs, and Ben merges from GitHub when they are small, or asks the contributors to move them to Phab (or does it himself, not sure) when they need serious review. Seems to be work nicely, and we got a bunch of useful minor fixes there.
We've had this kind of discussions the last couple years every now and then (which led e.g. to [WhyNotGitHub](https://ghc.haskell.org/trac/ghc/wiki/WhyNotGitHub)). However, we shouldn't spread code reviews across two different tools; moreover I'd argue we're optimizing for the wrong goals for the wrong reasons by allowing GitHub PRs (which were mostly allowed as a concession for trivial drive by contributions and as a kind of gateway drug to Phabricator; and anything actually needing actual code review and/or CI testing ought to be moved onto Phab anyway). There's also quite a bit of technical impedance mismatch opened up by allowing GitHub PRs, which is why I'm still very unhappy that we allowed them in the first place. But at this point I don't feel like rehashing the rationale voiced in past discussions about why it's a fallacy to think that lowering the barrier to entry via GitHub is important for GHC, and in fact ironically may harm GHC in the long run.
&gt; lowering the barrier to entry via GitHub [...] may end up harming GHC in the long run Whoa, this is a bold claim. I'm not familiar with the history of this discussion. Can you explain why this might be the case? 
We need to discuss what *different types of vehicles* even means. In OOP you would model this with classes. Vehicle is the base class, which can be compared to other Vehicle objects. The *right way* to model this in Haskell would be to define an interface (~ type/data type) data Vehicle = { bounds :: Polygon , ... } collides :: Vehicle -&gt; Vehicle -&gt; Bool which defines vehicles and collisions. Then you have your objects (~ implementations/classes/subclasses) bike :: ... -&gt; Vehicle car :: ... -&gt; Vehicle toyota :: Vehicle toyota = car ... which implement that particular type. Or you could use upcasting (see my [other post](https://www.reddit.com/r/haskell/comments/77gmwx/java_interfaces_map_to_haskell_records/domvezm/)) to connect arbitrary data types to the interface. instance Upcast Car Vehicle where upcast = ... With this approach you can have lists of vehicles, e.g. `vehicles = [bike ..., ..., toyota] :: [Vehicle]. With the typeclass approach, every vehicle in the list would need to implement the same "interface" (`Vehicle a =&gt; [a]`). Why? Because that behavior is exactly *the point* of type classes. Because *typeclasses are not interfaces*. They are great and have their use, but they are not the main mechanism of abstraction in Haskell. That would be plain old types.
Well I think I actually got it to work. I entered `brew info libffi` into the command line and it said at the bottom For pkg-config to find this software you may need to set: PKG_CONFIG_PATH: /usr/local/opt/libffi/lib/pkgconfig So I entered: env PKG_CONFIG_PATH=/PATH_TO_PKG_CONFIG/pkgconfig:/usr/local/opt/libffi/lib/pkgconfig cabal install Haskell-gi and then I got: Resolving dependencies... Configuring haskell-gi-0.20.1... Building haskell-gi-0.20.1... Installed haskell-gi-0.20.1 After which I checked the package management tool (after restarting the app) in the IDE and it says "installed" for both the `haskell-gi-0.20.1` and the `Haskell-gi-base-0.20.1` packages. So it definitely appears to work. I greatly appreciate your help in the matter and I'm far more optimistic about using the IDE. 
Read it 2 hours a day, work exercises 9 hours a day.
That's a really fun experiment, but how well is it useful in real life ? In my (hobbyist) experience, I observed that: - Most of the time you don't care about an open hierarchy. Hence virtual dispatch can be done using ADT. - Virtual dispatch with open hierarchy can be done using partial application. For example, if you want a list of `Shape` with an `intersect` method, in an OOP language (say C++ here), we'll do a `std::vector&lt;Shape*&gt;` with `Shape` an abstract class with a virtual `optional&lt;Intersection&gt; intersect(const Ray&amp;) const` method implemented in child class. In Haskell, we can simplify that doing a `Vector (Ray -&gt; Maybe Intersection)` and just fill it with preapplied intersection function, like `(intersectSphere :: Sphere -&gt; Ray -&gt; Maybe Intersection) aSphere`.However there is a memory / performance issue with this approach because for each interface you want, you need to store a pre applied function. This is usually not a problem for cases where you don't have huge collection of objects. And actually, in C++, when we have huge collection of object, we stores them in separate arrays without virtual dispatch to avoid the cost of pointer indirection, fragmented memory layout, code cache trashing, ... - For really rare cases (Well, for now, I just toyed with it, never had to use it in real), we can use existential or GADTs to get something which is really close to your article.
&gt; One of the biggest sources of friction for me is the different kinds of markup we have in our different tools, and moving Trac to Phabricator would eliminate that. I'm sorry to break this to you, but this will not *eliminate* our markup diversity... ;-) - Phabricator too uses its own custom markup called "reMarkup" which is *similar* to Markdown but differs in subtly annoying ways from actual Markdown as used everywhere else (which frequently requires me to fixup the markup as I always get it wrong when switching between GitHub/GitLab and Phabricator). Afaik, Phabricator does *not* support reStructuredText yet. - We use reStructuredText for our GHC User's Guide via Sphinx. - Trac supports does have its native default Wiki markup which happens to be "close enough" to [Wiki Creole](https://trac.edgewall.org/wiki/WikiCreole]. But it also [natively supports reStructuredText](https://trac.edgewall.org/wiki/WikiRestructuredText) which we considered as one (of multiple) reasons for switching the GHC user's guide switching to reStructuredText. - Last but not least, we use Haddock markup in Haskell code. And Haddock is *not* going to support Markdown any time soon; it may however optionally support reStructuredText at some point. 
I never force push or rebase in Git, ever I just add more commits to the end of my branch and when the PR is ready to merge I use the "squash merge" option to combine them into a single logical commit Not only does this clean up the history but this also makes it easier to revert the change and to bisect the master branch
This is like saying: "Don't use Linux because it is a single point of failure that attackers are trying to exploit"
The reason why is because the stated goal is to lower the barrier to entry and GitHub has the lowest barrier to entry I'm not arguing against your proposed GitLab solution to mirror to GitHub (I'm interested to learn how it compares to GitHub). However, I do want to point out that "more difficult to implement" is not a good reason to reject a solution, because the simplest solution to implement is to do nothing. Presumably the whole point of the GHC devops group is to implement difficult solutions if they will improve the user experience for contributors.
The main issues with Jenkins I've run into: * the Jenkins configuration for building a project does not live in version control alongside that project * Jenkins builds are not (easily) composable, meaning that you will run into a lot of issues if you try to create Jenkins build pipelines
&gt; Let me pose a final question: who would have contributed to GHC, and if so what would you have contributed, if GHC wasn't using phabricator and arc? I would have contributed drive-by fixes to core library documentation if it were as simple as editing a file and raising a PR on Github. (It doesn't need to be Github -- just needs to be as simple).
It's what happened to GLADOS in Portal 2. :)
Isn't that already possible *right now*?
Typo: data Catch2 a b = Catch 2 a b The second "2" is detached
&gt; How do I deal with the enforced purity of Haskell and IO. Two thoughts: * Write code with all the `IO` you need; if you later notice that a bit of code doesn't need `IO`, refactor it away. * Once you have a good sense of a pattern your inner code uses, define an interface, or an abstraction boundary for it. Type classes are common for this, but not necessary (see [Scrap your type classes](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) for an example).
I agree. That's why I'm saying that you shouldn't expect this feature soon... :)
It's a nice book, but it very much stops before any 'real world' skill is attained. Gentle introduction to basic Haskell concepts, but there are few useful programs you could write after reading it.
Surely ten women could do it in 4 weeks?
"There shouldn't be any variable declared in the body of a function. Only parameters and other functions should appear on the body." What about Local Definitions? How cam we het anything done without them?
Well, not if you need just one. If you need to sustain a production rate of 1 baby / 4 weeks, ten women is the minimum, and assumes a zero defect rate.
&gt;reflex 0.5 and reflex-dom 0.4 isn't available on Stackage/Hackage, which is a major pain You can define packages that are pinned to a git commit using stack. Have a look at the stack [docs](https://docs.haskellstack.org/en/stable/yaml_configuration/#packages) and you should find some examples you can use :)
I believe you're "different types" are just different constructors for the same type. You lose the information about the difference at the type level. That's a fine way of programming things but you can't then make functions that only take bikes (as an example). That may not be something you want to do, but if it is it's a lot easier with type classes. Why is your version better? You've said multiple times that it is, but not really expanded on why. I'm happy to accept that it is, but I don't see it yet.
Exactly how I was going to respond.
You "lambda lift" the local definition to the top level. This requires adding a new argument fir every value that your definition closes over For example, you would transform this: f x = x + foo where foo = x * 3 ... into this: f x = x + foo x foo x = x * 3
I would really like to know what "scalable" means in this context
Aha. Thank you. I wonder why I use "where" so much. This way makes sense.
HTAC. Haskell Tutorial And Cookbook, https://leanpub.com/haskell-cookbook
If it is, then I don't know how. Possibly a link called `fix this documentation` next to every API doc would close this gap.
that actually cleared up how to think about monads for me, thanks for the thorough explanation. 
I've been using bitbucket a lot recently rather than github and the PR review view can look at individual commits. I'd forgotten about not having it on github, it's really usefull. In your example you can have one branch with the feature and when reviewing you can look at and comment on the individual commits. But.. why do you need 15 branches, why does this simplify things, surely the 15 different components are in different folders? A pull request doesn't need to be based off master, what does having a phab/arc group commits give you over a branch? If you really want to split these 14 patches and have a linear hierarchy, why not branch off branches? The only advantage I can see this giving you is that you can have complex hierarchies and graphs of patches. This sounds pretty dangerous to me.
Wow, this is amazing. I am totally stunned :-) It's great stumbling on these areas of CS that you had no idea existed. However....the more you reduce the size and increase the frequency of releases the less relevant this type of thing becomes. This complexity is what continuous delivery tries (and often succeeds) to remove. 6 Months is a hell of a long time to not release, in all the places I've worked it would be crazy to only release this often. What's to stop extremely regular beta releases of GHC? The more you batch the worse things get.
[removed]
Make it ugly but working, then refactor as opportunities and techniques present themselves. Don't wait for a perfect design while you're learning the principles of the language.
The fundep is unnecessary, though, because you can totally have multiple things you can upcast to. `AcmeBicycle` implements both `Vehicle` and `Bicycle`, so you'd want to have two instances. Without the fundep, though, type inference gets pretty bad. For the cases where you want this, it almost always makes more sense to just describe the `b` type as a class, using something like classy lenses, and then accept that polymorphic type variable there. Or, if you're anticipating wanting lists of "heterogeneous" values, then a single class for the domain like `class HasVehicle a where toVehicle :: a -&gt; Vehicle`.
I just read https://secure.phabricator.com/phame/post/view/766/write_review_merge_publish_phabricator_review_workflow/ and I just don't get how this is any different from pull requests on github/gitlab/bitbucket? What does "publishing" mean? If a change is able to be reviewed publically then surely it's "published"? In PRs you are free to rebase and do whatever else you want to do, it's your request. You can set github and bitbucket (not sure about gitlab) to squash commits on merge and other strategies. I really don't understand the difference?
&gt; Without the fundep, though, type inference gets pretty bad. That's why I included the functional dependency. I prefer frictionless type inference over complex type casting. &gt; it almost always makes more sense to just describe the b type as a class I don't see how this would be better. That way you have to define a `HasXXX` class every time you want to do this, as opposed to just an instance `instance Cast Bicycle Vehicle where cast = ...`. There could even be a GHC extension to automate this by deriving.
Ok, you can squash instead of force-push, and then the workflow is basically identical to what we do in Phabricator. Instead of treating a PR as a set of logicaly-separate changes that you want to retain when merging to master, you're treating a PR as a single atomic commit, with a history that develops during code review but isn't retained in the repo once committed. I'm totally fine with this workflow (because it's the same as the one we use in Phabricator). when you squash the commit at the end, what happens to the history in GitHub? Can you still see it somewhere?
Well yes, what I really mean is that we don't advertise or document that you can do this, and the process of converting a PR is currently quite manual, so it would need some more effort to scale it up.
Type inference and subtyping generally don't work great together :) You pretty much have to either choose explicit casts or type annotations. For what it's worth, `Cast` exists as [`Coercible`](https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Coerce.html) and *is* automatically derived for `newtype`s. For more complex types, classy lenses (from either `makeClassy` or `makeFields`) work great to express subtyping on properties.
&gt; I believe you're "different types" are just different constructors for the same type. You lose the information about the difference at the type level. Yes, that's exactly how it should be if the type is the interface. The type is basically the lowest common denominator of all terms that describe vehicles. Data types are like interfaces in OOP languages, since they only contain the signatures of the functions, while classes carry the implementation, just like terms do in Haskell. It's just hard to see because OOP mixes so many things into the concept of a class: Inheritance (implicit conversion), accessibility (public vs. private), constructors (really just functions), virtual and non-virtual functions (static vs. part of the record), namespaces and many more things that are unrelated to the abstraction itself. &gt; but you can't then make functions that only take bikes [...] it's a lot easier with type classes. In that case you would have to create another type `Bike` together with a function to turn bikes into vehicles. I don't think that the type class approach is fundamentally easier. And I suspect that it has at least the same amount of boilerplate to it. Also it makes certain abstractions, like lists of different vehicles, impossible without existential types and the rise in complexity that always comes with them.
&gt; Type inference and subtyping generally don't work great together I know, but with the functional dependency you can only ever upcast to a single type (single inheritance), which can be uniquely determined if `a` can be determined. With careful use and some luck, this shouldn't interfere too badly with type inference. &gt; For what it's worth, Cast exists as Coercible and is automatically derived for newtypes. I'm thinking more along the lines of deriving the following instance: data A = A data B = B { bA :: A, bChar :: Char } deriving (A) -- instance Upcast B A where cast = bA I'll have a look at classy lenses, thanks.
Here's the stack file for reflex development: http://lpaste.net/1845479797061320704 To make intero work with reflex code i have 2 stack files. The one for reflex, i name it stack-js.yaml And the one for intero, I name it ismply stack.yaml Here it is: http://lpaste.net/3581887486596481024 Then you code in intero and in compiles and gives you all the hints and errors and type info as usual. But it does not create javascript files. SO when you need to compile your app to js, you just tell stack to use a different stack file. Like this: #!/bin/sh stack build --stack-yaml=stack-js.yaml cp -r .stack-work/install/x86_64-linux/lts-7.19/ghcjs-0.2.1.9007019_ghc-8.0.1/bin/clientadmin.jsexe/ ../static/ The last line is just to copy your js files to the web app static folder. 
Of course you can't get runtime type error; types are erased at runtime (though you can use `Typeable` to keep type information). Segfaults are perfectly possible, you just have to use pointers :&gt;
I use `dante` with reflex-platform. It's almost exactly the same thing as Intero, but supports anything that can open a GHCi session, so it works with reflex-platform just fine. I recommend using `jsaddle-warp` for GHC builds so that you can actually run and test the app in a real browser. It makes development way way faster since it runs faster, compiles faster, and lets you run from GHCi. With reflex in particular, I'd say it's worth it to just buy into Nix and use it fully. You'll get a *ton* of benefits for free from the expressions in reflex-platform. The `text` library is optimized for GHCJS, you get cross compilers for building mobiles apps for free, all the dependencies are cached so you don't have to build them, the list goes on.
You do not want to actually calculate the Fibonacci sequence before taking the modulo of them. Fibonacci numbers grow quite fast, and dividing them is quite slow. Instead, you directly build the Fibonacci sequence modulo `m`: fibs' = map (`mod` m) $ 0 : 1 : zipWith (+) fibs' (tail fibs') This way we keep all numbers small and reduce the time complexity from O( m^2 ) to O(m). If you really care about performance, we can even avoid division by replacing it with an if statement since we only add two integers (no multiplication), both smaller than m: mod' x m = if x &gt;= m then x - m else x
The history is still there since the original branch is never merged or deleted (although GitHub will prompt you after the squash merge if you want to delete the branch)
I thought it was pretty clear that what I meant was that you can't somehow trick the compiler into letting you concatenate two Booleans. I know that it wouldn't literally be a Python style type error. And unsafe* and Foreign.* are well known escape hatches and so I didn't think it was worth mentioning them.
The `makeFields` gives you something similar, but it *is* explicit: data A = A data B = B { bA :: A, bChar :: Char } makeFields ''B gives you class HasA a b | a -&gt; b where a :: Lens' a b instance HasA B A where a = ... class HasChar B Char where char = ... This lets you do `view a :: HasA a b =&gt; a -&gt; b`. One potential issue here is the multiple inheritance problem. SUppose: data C = C { cA :: A, cB :: B } makeFields ''C Now, if we want to upcast a `C` to an `A`, do we go through the direct attribute, or through `B`? The `makeFields` approach will require you to compose: c'sA :: (HasA c a) =&gt; c -&gt; a c'sA = view a b'sA :: (HasB c b, HasA b a) =&gt; c -&gt; a b'sA = view (b . a)
Please indent your yaml by four spaces so reddit displays it properly.
Indeed we do allow pull requests and it is advertised, although arguably not as well as it should be. The GitHub repository description includes the text "First time contributors are encouraged to get started by just sending a Pull Request". However, perhaps advertising this via the mailing list and reposting to Reddit would be a good idea. Regardless, by my count we have merged around 60 pull requests so far, so clearly someone noticed. &gt; I'm just wondering if we should simply drop the "small" restriction? The problem is then we start fragmenting review, which makes more work for everyone. Essentially the only things that I accept via GitHub currently are patches that require no real review. 
Your types say you want an object containing a single `action` field containing a list of Action objects: data Actions = Actions { action :: [Action] } deriving (Eq, Show) But your yaml is a list of objects, each of which has a single `action` field containing an Action object.
Thanks for sharing your experience. Indeed I did feel much of the pain that you report while bringing up our Jenkins infrastructure. While evaluating CI options I did briefly consider a custom solution, but frankly I don't think we want to support yet more custom code. Any effort that we spend on infrastructure maintenance is time that we can't spend making the compiler better/faster/lighter/stronger/water repellent/[insert desirable property here]. 
Until they do: - action: - description: some description - project: inbox - contexts: [ online, computer ] - action: - description: some other description description - project: inbox - contexts: [ online, computer ]
So this seems really great and I'm glad there's a team committed to this work. My only thought is that it would be very wise to heavily favor the opinions of existing GHC contributors, and to be very skeptical of the idea that "lowering barriers to entry" will result in more meaningful contributions or people willing to help maintain GHC. I mean for years bos was the sole maintainer of like the majority of the haskell library ecosystem. I don't think there's any evidence that those libraries being on github was much of a boon to him. In OSS almost no one is willing to go beyond reporting bugs, and this is already very easy to do for GHC, and I've always found it to be a very smooth and positive experience as a reporter. https://www.youtube.com/watch?v=VS6IpvTWwkQ
I'm doing the same thing and reaping the same benefits. The combination of jsaddle-warp and ghcid is also pretty awesome :) 
For using the browser to edit and submit a pull request without forking to your local disk first.
Thank you for your response! I have tried doing so, but the https://github.com/reflex-frp/reflex-dom no longer has a .cabal file in it's root directory, which makes Stack very unhappy and un-cooperative. Unfortunately it simply crashes on "stack build".
Doesn't this just mean it doesn't support all development models yet?
As someone who makes frequent use of rebasing while working on his own patches, I'm not sure how this model would scale to larger bits of work. For instance, consider my DWARF work: this consisted of [a couple of dozen](https://ghc.haskell.org/trac/ghc/wiki/DWARF/Status) independent patches. When working on a project like this, I generally keep my repository in a state where there is one well-structured commit per patch. Each patch is a logically coherent atomic change. `git commit --fixup` followed by a rebase makes it easy to amend these patches. This ensures that the work remains well-structured, easy to review, and rebase. How would you accomplish this under your model? It seems like you would either have an ever growing stack of patches (which is a nightmare to rebase) or entirely lose the ability to keep logically distinct changes distinct in the final commit history. 
Thank you for your response, and thank you for the heads up on dante! I will gladely give myself to Nix, as long as I can keep the quality of life (or at least some) that Intero brings. Hopefully dante will provide! Could you please describe your work flow when developing with Nix? I would really appreciate it!
Thanks for your response! That's pretty neat! I will look into Dante with Nix as ElvishJerricco suggested, and if that fails I will look into your solution. Thanks!
I can't answer the final question, but I would suggest caution, regardless: Surivorship Bias is a *huge* deal in this kind of thing. Right now the people contributing had to overcome a couple of big(?) hurdles to actually be able to even *start* contributing. That's to say -- they already had to be pretty motivated to start with, and so there's no "luring them in" effect, currently. It's ridiculously difficult to tell how big the "luring them in" effect could be, but I think I say someone from the O'Caml core team(?, [1]) mention that it's actually been a surprisingly effective recruitment tool. I should note also that IIRC the Linux kernel gets a *huge* amount of drive-by contributions because it's so ridiculously easy to just mail off a patch to a subsystem maintainer. (Greg KH has a talk about the contribution process where he mentioned some stats. I can't recall them at the moment, but it was a sizable proportion of contributors that just contributed a few patches at most.) [1] I'm not entirely sure it was O'Caml, but regardless is was a relatively "niche" FP project.
&gt; What's to stop extremely regular beta releases of GHC? If Linux can manage an average of 70 days per release, why can't GHC manage at least that? Well, do keep in mind that the Linux kernel has several orders of magnitude more labor at their disposal and yet only an order of magnitude more code to maintain. :) Regardless, I have described some of the reasons in a previous [blog post](https://ghc.haskell.org/trac/ghc/blog/2017-release-schedule).
react-hs also doesn't have a .cabal in the root directory, and to install it you put the following in your stack: packages: - location: git: https://github.com/liqula/react-hs commit: 8eb9d31fcfa0a79ef006565f299d47b210bdd690 # please check if that's still the most recent commit on master. extra-dep: true subdirs: - react-hs - react-hs-servant - react-hs-examples So the equivalent should work for reflex.
&gt; As /u/simonmar points out, you can have fine grande differentials. that are logically separate from each other but build ontop of each other. This is usually done by specifically instructing arc to diff only the range the patches affect to diff. Yeah, I know, it was just that the default made little sense to me and achieving the desired differentials was "cumbersome". Having to specify ranges, when you have topic branches, is a little odd. &gt; largish diffs may come up with git as well, when the author just ends up putting everything into a single commit. I feel this is more of the personal choice of the author, while I do agree that phabricator favors a squash-commit style work flow whereas many other project do not squash commits for PRs. If you're the sole developer of a piece of code, it can work with having less commits, each with higher diffstat. Though in my experience you need excellent memory or very extensive code comments to allow retracting things when you go to debug a mega commit. I've come to appreciate a reasonable balance of multiple commits combined with good commit descriptions aided by code comments. Commit comments and code comments are the result of 1 person or multi person code reviews and prior documentation of the development steps and considerations you had. Once you squash everything into one big diff, you lose most of that. &gt; This is just to illustrate that there are potentially two different approaches. And I certainly do not want to get into any kind of my-way/your-way discussion here. From experience I can say that the different approaches are defined by what kind of change we're talking of. A complicated change suffers more from collapsing it into a mega commit than smaller ones that aren't a combination of several changes leading to a bigger change. &gt; For your carefully crafted topic branch, I would suggest to split it up into multiple diffs for review. Maybe it helps to think of diffs, as independent reviewable chunks of changes that make up the final piece of work. Instead of reviewing the final piece of work in one go. Does that make sense? Not really. When I say I have a carefully crafted patch set (aka topic branch), what I mean is that the change is big enough that it warrants multiple commits that allows (1) easier review of how the big diff came to be developed and (2) improved bisect'ability. If I understand you correctly, you propose I would submit several differentials, one for each commit, and have multiple discussions for each commit. This isn't practical. Let assume an example: SimonMar writes a new GC that is still concurrent, combines the advantages of OCaml's GC and has support for advanced linear types. The diff as a whole may be just 2000 lines, but as a single unified diff that touches, say, 15 files it will be much harder to review than one where we follow the 3 or 5 development steps that happened during the refactoring. Each commit gets its own descriptive commit message on top of code comments, and review is eased by following the separate commits one by one. This is a contrived example, but I've experienced this myself in much smaller projects than the Linux kernel, and found it way easier to review a substantial change when it was split like that. On top of everything else, having multiple commits in a topic branch that is merged with --no-fwd, leaves the git merge history intact, allowing you to see the combined set of commits from the topic branch. It also makes it more likely that when you bisect you land on a commit that isn't one bug 2000 lines diff. If you have a mega commit, more than one bisect session will land you on that one big commit, whereas separate commits will land you nearer to where it happened and in smaller diffs to analyze.
I have, and while it's not *great*, it does get the job done. (No code review tools I have *ever* used have been great. Honestly, if everyone could agree on it, I'd go full Linux on it and do it by e-mailed patches + inline comments on diffs... but that's not really realistic.)
But when you squash it first, you lost the history in the main tree, unless you find ways to recreate with some git command. When you merge a topic branch like Simon's dwarf topic and avoid fast-forward, you have a tree in your history and you can see the commits involved in the merge commit. The merge commit can also be used to include extra metadata. It took me a while, but I learned to appreciate the usefulness of merge commits and came to dislike a flat stack of patches with zero branches/leafs in the history. Mercurial extends this process by keeping metadata by default in forcing you to use things like bookmarks for untracked history.
Memtest86 produced no errors.
I'd like to echo your sentiment that spreading code reviews across two different tools probably even worse than just choosing one and sticking to it. (I'm also... puzzled like https://www.reddit.com/user/taylorfausak )
&gt; With Phabricator you upload each commit separately, and Phabricator tracks the dependencies between them, showing you a tree of commits in the UI. &gt; &gt; Here's an example: https://phabricator.haskell.org/D3913 Scroll down to "Revision Contents" and click on the "Stack" tab &gt; &gt; Each commit is reviewed, revised, and accepted separately. Having used this workflow for several years I like it - there's some overhead to maintaining the stack of diffs and rebasing when necessary, but being able to revise the diffs in the middle of the stack without having to add more commits on the top (thereby destroying the nice history) is really useful. Agreed. This and your DWARF branch comment below make it seem like you've come to learn and appreciate the usefulness of merge commits, patch queues aka topic branches, rebasing to keep the patch queue useful (e.g. no fixup! commit left), just like I did. I also started off with Mercurial and Git, coming from Subversion, thinking a flat history ala what Phabricator pushes to GHC.git by default is a desirable state for the revision to be in. &gt; In GitHub you would have to force-push the whole PR, and then you lose all the existing comments in the code review. That's a limitation of their code review system and something Differential and Gerrit do not suffer from.
(Sorry to double-reply.) Could you point to a Phabricator review or two just as an example of what you're talking about? Maybe that'll give everybody a better idea of what "level of review" we're actually discussing :)
Better answer: &gt; We can always add GitLab as an option to be considered. Thanks. If it's either Phabricator or GitHub, then Phabricator would be my choice. But if GitHub is chosen, using GitLab to get a system with similar features and future-proofing like Phabricator but the familiarity of GitHub is a reasonable option. &gt; It ought to have the benefit of not requiring any local tool installation, Nothing needed, it's just like GitHub, has built-in CI and extended lifeycle tooling, etc. &gt; but it will still be unfamiliar to most (I assume, maybe I am wrong?) If you like and want to have something like GitHub, then it will be familiar but with more features and being customisable/open if required. &gt; It also, from what I know, will require more maintenance work from us (= less time spent on GHC itself). If you want to host an instance, then it's work but not any more than Phabricator. Potentially less work due to the larger and more active community around it.
Done, thanks. M
Is there a write up or a blog post how to setup nix, dante, jsaddle-warp together? 
You can see the source for the built in Functor instances by clicking o the "source" links on [the hackage docs for Functor](https://hackage.haskell.org/package/base-4.10.0.0/docs/Prelude.html#t:Functor) which covers a lot of them. Regarding "an example of a type of kind * -&gt; * which cannot be made an instance of Functor" [this recently posted 11:00 talk Stephan Boyer at LambdaConf](https://www.youtube.com/watch?v=3D2ezqTwcWM) gives a whirlwind tour of Functors, Bifunctors and Profunctors and should shed some light on that question. 
I don't quite understand this, they already use loads of third party software to build and publish GHC, how is Stack or Nix any different from make in this respect?
This is all very true however in my personal experience the effort maintaining Jenkins is more then the effort maintaining custom software written in Haskell.
wow this is awesome, marvelous. Cristal clear now. thanks for your answer. however, would you mind to share a GitHub implementation for such architecture if there is any open source. Thanks again
&gt; building scalable applications OP didn't talk about "scalable" though. They seem to just want to get started with a fun project.
Hey, thanks a lot for that, these kinds of answers are one of the main reasons for posting this here
Hmm, it doesn't seem to have worked. To be clear if your yaml is ABC DEF GHI JKL MNO Then you need to write I'm trying to parse this format of yaml: ABC DEF GHI JKL MNO I've looked through the examples and tests... The blank lines are also important.
I don't know of any examples of this in haskell. I'm actually tinkering with one in my spare time but I'm not really satisfied with it yet.
I agree that such a link would be nice. For what it's worth, GHC has been accepting pull requests through GitHub since January-ish: https://github.com/ghc/ghc/pull/16
Good, so we can do it. But when would one choose this pattern over a "classical" functional one? 
It's [complicated](https://web.archive.org/web/20170225054418/http://www.snoyman.com/blog/2016/08/haskell-org-evil-cabal)
GitLab, not Phabricator, lost data: https://about.gitlab.com/2017/02/01/gitlab-dot-com-database-incident/ &gt; We lost six hours of database data (issues, merge requests, users, comments, snippets, etc.) for GitLab.com. GitHub may be a single point of failure, but they rarely fail in part because they have been targeted by state-level attacks and know how to handle them: https://it.slashdot.org/story/15/03/27/1420250/github-under-js-based-greatfire-ddos-attack-allegedly-from-chinese-government &gt; The Chinese government's dislike of widespread VPN usage may have caused it to arrange the attack, where only people accessing Baidu's services from outside the firewall would contribute to the DDoS.
There are [562 merged pull requests](https://github.com/pulls?utf8=%E2%9C%93&amp;q=is%3Apr+is%3Amerged+user%3Abos+-author%3Abos) for bos's projects on GitHub. I obviously can't speak for him, but that seems like a boon to me. [60 pull requests](https://github.com/ghc/ghc/pulls?utf8=%E2%9C%93&amp;q=is%3Apr%20created%3A%3E2017-01-30) have been opened against GHC since they started accepting PRs on GitHub. 
Where's your (this person's?) Patreon, I wish to subscribe.
I thought we already did allow drive-by PRs on github? There's a number that have been accepted here: https://github.com/ghc/ghc/pulls?q=is%3Apr+is%3Aclosed
&gt; GitLab, not Phabricator, lost data: https://about.gitlab.com/2017/02/01/gitlab-dot-com-database-incident/ &gt;&gt; We lost six hours of database data (issues, merge requests, users, comments, snippets, etc.) for GitLab.com. You omitted the most important part: &gt; **self-hosted installations were not affected** 
As far as the Haskell platform goes, those are things that, when they break, only affect the GHC devs. The committee can't ship something as the recommended way to do things unless they have the ability to fix it when a critical issue is reported. For instance, Nix has had *numerous* issues on newer versions of MacOS. If they had to sit and wait for the strict code review of the `nixpkgs` maintainers to fix things, that's out of their hands, and that's not acceptable. They need to have control over the toolchain that they're asking users to trust them on. The platform does ship with [some libraries they don't control](https://www.haskell.org/platform/contents.html), but they go out of their way to label them as non-core libraries, to show that they can't consider them officially supported. Stack would go in this category, so they can't hinge the *entire platform* on it.
Don't think so, but that's on my todo list.
Thanks for the introduction, I didn't know about classy lenses.
I basically just run Cabal from within a nix-shell that has the Nix-defined GHC on the path. You *can* do `cabal build` from outside the nix-shell as long as you do `cabal configure` from inside it, but I'm not sure that this is really *supposed* to work =P It just happens to right now (for instance, this strategy does not work with `new-build`; that requires you to do it all from within the nix-shell). Anyway, you can either just get the `cabal` commands working outside the nix-shell, or you can configure `dante` to run its command in the nix-shell. For more details, I'm hoping to do a write up on this. Hopefully I'll be able to put it in `reflex-platform`.
This is a really interesting article, not least because it is so typical of what happens in industry. As you said in your article, it all comes down to 2 things, more automation and more regular releases. I really think rust seems like a model GHC should aim for. It feels like all the steps in your article could (with enough effort) be reduced to tasks that take hours rather than weeks or days. I also like the rustc idea of nightly, beta and stable, I see no reason why GHC can't follow this model. It's amazing how this is all exactly the same as what I'm doing in my current job :)
I'm not sure I agree. Thanks for explaining your point of view. 
Bake is based around testing multiple patches/features at the same time (as it happens, something I totally disagree with). That's really the main selling point of it and what makes it good. What we would still need is a system for co-ordinating all the jobs that have to go in to a run. This means distributed work and pipelines with dependencies. Bake is not the tool for this side of things.
The Newcomer page links to `How to contribute a patch to GHCHow to contribute a patch to GHC` which documents the Github option since [9 Months.](https://ghc.haskell.org/trac/ghc/wiki/WorkingConventions/FixingBugs?action=history) There are other places where it isn't mentioned though.
Ha, here's future me to say that it's a good thing this thread exists.
I just read the GHC Jenkinsfile and had to laugh, I can empathize exactly with the maintainers of that as I am sure they have been though many of the same experiences that I have :) How could I get involved and perhaps make use of some of this pain?
One neat way to think about it is that lazy Peano numerals are isomorphic to `[()]`: ``` data N = Z | P N from :: N -&gt; [()] from Z = [] from (P n) = () : from n to :: [()] -&gt; N to [] = Z to (():xs) = P (to xs) ``` With this view, the lazy version of `length` becomes: ``` length :: [a] -&gt; [()] length = map (const ()) ``` 
So basically taking in an `n` and returning a list of `n-tuples` involves having the return type dependent on the input value, which requires dependent types. So you can't do that. If you really wanted to you could take in a type level nat and then perhaps generate a list of statically sized vectors or a HLists of all possible combinations. I would suggest just using `replicateM` though: replicateM 2 [1, 2, 3] = [[1,1],[1,2],[1,3],[2,1],[2,2],[2,3],[3,1],[3,2],[3,3]] replicateM 3 [1, 2] = [[1,1,1],[1,1,2],[1,2,1],[1,2,2],[2,1,1],[2,1,2],[2,2,1],[2,2,2]]
What does this gain you above and beyond the declaring default method implementations in a regular haskell class? Seems like you could tag that ADT with a dialect type and basically get all this same behavior without the weird baggage of carting around records of functions at runtime.
just out of curiosity, why to create a type for `user` and another for `updateUser` , the same goes for `article` and `updateArticle`. is a data structure/type for every action needed?
Should have just submitted a link to scala.org
Well thanks for doing the math. I agree those are pretty convincing numbers! I think part of the trick is figuring out how to get contributors to become maintainers who can also help code review and so on, but it's hard to argue that more contributions aren't a net win.
I find most books can be finished much more quickly if you only read the prime numbered pages.
Thank to you I discovered `jsaddle-warp`. That's fast as hell compared to the ghcjs version reflex can result in. However I'm afraid of the model, am I right when I say that ALL client side event are sent to the backed to be interpreted by reflex, which in exchange send a json containing the dom modifications. Do you have any feedback of this in production? How does it scale?
`jsaddle-warp` is definitely a dev tool. I‚Äôve never even tried to deploy it to a server. It‚Äôs mostly just there so you can `:r` your app and use a faster compiler. It‚Äôs nearly a perfect emulation of running your app in GHCJS, so it‚Äôs just a really accurate way to speed up dev cycles. I‚Äôve used it this way on a fairly large app and it worked great.
&gt; there so you can :r your app and use a faster compiler You just replaced my 20s wait cycle by a 0.1s wait cycle... THANK YOU.
Why not split those independent changes into multiple smaller pull requests?
Ok, so Data.Yaml has an Array type, I guess I should drop my Actions type, use that Array type as a container for my Action type, I‚Äôll look for aeson examples of the usage of Array.
A set would have been 2^x : for every element of type x, either it is or it isn't in the set.
A record of functions may provide an abstraction, but it is not an "interface" in Java/C#. in C# one can create a record of functions just fine; these aren't confused with "interfaces". using System; namespace BicycleModule { public struct Bicycle { public Action&lt;int&gt; changeCadence; public Action&lt;int&gt; changeGear; public Action&lt;int&gt; speedUp; public Action&lt;int&gt; applyBrakes; } } 
A record of functions may provide an abstraction, but it is not an "interface" in Java/C#. in C# one can create a record of functions just fine; these aren't confused with "interfaces". `Action&lt;int&gt;` is a function that takes an int and returns () using System; namespace BicycleModule { public struct Bicycle { public Action&lt;int&gt; changeCadence; public Action&lt;int&gt; changeGear; public Action&lt;int&gt; speedUp; public Action&lt;int&gt; applyBrakes; } } 
If a record of functions is an "interface", then you'd have to also admit that SqlDialect below is an "interface" in C#, right? public struct SelectQuery { } public struct SqlDialect { public Func&lt;SqlDialect, SelectQuery, string&gt; renderSqlQuery; } 
Multiset is the proper type, sure. But there is no magic : we need an order to quotient... The well ordering principle comes to mind but it is allready too strong. Is there an ordering principle properly realized ?
I spent a number of years programming in Ocaml. I found that whenever I actually used Ocaml's OO features, I ended up regretting it and switching back to plain Ocaml structs.
You don't need an ordering. You just quotient by all permutations. For example the multiset of size n of elements of type T is T^n / sigma(n), where sigma(n) is the set of permutations of n items.
I have now :)
Yeah, though notably this is different from the set above, since it might have infinitely many elements. For a finitary set you want something like a set backed by a finite non-repeating list.
So, it hasn't actually been said yet, so I'll say it: Building a webapp is not a good way to start learning Haskell. I would suggest picking a smaller project. When people describe Haskell as a high level language, they do not mean high level like 'ruby' or high level like 'perl'. Those languages do a lot for you by providing tons of syntactic sugar and nice shortcuts. Haskell provides almost no sugar, and no shortcuts. You can build webapps with haskell, certainly, but there are no 'frameworks in a can' that will provide you with mystical super-sugar to get up and running with super speed. Chances are good that if you want to do anything other than the most basic of things, you'll need a strong working understanding of the type system. I would suggest doing something like a quiz program, a sudoku solver, or solving some puzzles on one of the numerous hacker challenge websites that seem to have sprung up all over the web. Once you've got a few simple projects under your belt, and you feel like you've got a solid understanding of the basics, then maybe build a simple webapp with servant or something, and start adding on concepts as you gain proficiency.
&gt; But I still think we should not confuse a record of functions for an interface Why not? They are in every way equivalent. On the machine level both are collections of pointers to functions. A record value itself is of course not an interface, but the records type is.
[Surely it's more like this](https://i.imgur.com/Xct1Mwd.jpg)
"Effectful" and "non-effectful" would be better words than "pure" and "impure". `writeFile :: FilePath -&gt; IO ()` is a pure function.
Thank you for mentioning `replicateM`. I've just looked at its source and its not quite what I imagined but remarkably concise.
Some of you are based in Munich, right? Let me know if you are stuck we might fix it over coffee :-) 
I recommend taking the time to understand the typeclass-based API. Haskell is not the kind of language you can use by searching for snippets and blindly combining them.
Because the legal inputs for the operations are different enough from the "model" type to the point I have to create different type for them. If they are exactly the same, I would reuse the type. One of the best practice for functional programming is "[makes illegal states unrepresentable](https://fsharpforfunandprofit.com/posts/designing-with-types-making-illegal-states-unrepresentable/)". Loosely translated: "Make your type as close as possible to represent the legal values". in `UpdateArticle`, the [API contract](https://github.com/gothinkster/realworld/blob/master/api/README.md#update-article) says "there are only 3 accepted fields: title, description, body. and they are all optional" which is quite different from [`Singe Article` contract](https://github.com/gothinkster/realworld/blob/master/api/README.md#single-article) that has tons of other fields and they are all required to be not null/missing.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [gothinkster/realworld/.../**README.md#update-article** (master ‚Üí 424d5eb)](https://github.com/gothinkster/realworld/blob/424d5eb2d36d29cf98a75cf8c7f1123150fbda6c/api/README.md#update-article) * [gothinkster/realworld/.../**README.md#single-article** (master ‚Üí 424d5eb)](https://github.com/gothinkster/realworld/blob/424d5eb2d36d29cf98a75cf8c7f1123150fbda6c/api/README.md#single-article) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply donvflr.)^.
They are equivalent on the machine level. if you're going to bring that up, types do not even exist at that level either. However, a record of functions do not do the same things as Java or C# interfaces at the Java or C# type level.
Not quite what I was looking for, but thanks. The fractional type paper is interesting, but I don't get how it could be applied to e^x here. For instance, they use negation (-) to relate inputs/outputs. When - is written in the context of datatypes, it's more intuitive to consider it as "removing a case". For instance, the type of non-empty list of type x can be defined as List(x) without nil, ie List(x) - 1. When - is used to denote inputs, -1 + list(x) should be understood as the type of list constructors, which I would have written 1-&gt;List(x), and taken to be isomorphical to List(x)^1. Confusing isn't it ? 
&gt; You just quotient by all permutations. It looks like a specification, not an algorithm or a construction. For instance, with natural number, you can sort them and keeps only the difference between each. For instance, {1,1,7} would be {+1,+0,+6}. And {1,2,2,3} would be {+1,+1,+0,+1}. Finite types are even easier : a multiset is merely the count of occurence. So that's bucket sort in disguise. But quotienting with arbitrary type ? I don't get it.
[removed]
[How to Program like a Five Year Old](https://www.youtube.com/watch?v=YqVTCZFPoyQ) was really enjoyable. Short and sweet talk that laid out a neat architecture.
Intero actually does work with reflex-platform. You just use stack on top of nix.
This would be an ideal opportunity to open a PR to add such a link, no? ;-)
Don't think you need msbuild to do that.
&gt; Yeah, I know, it was just that the default made little sense to me and achieving the desired differentials was "cumbersome". Having to specify ranges, when you have topic branches, is a little odd. This is pretty much what I meant by treating `arc` as `git`. I truly fear this is going down the vim/emacs kind of route. Where different styles and mindsets collide. Building proper topic branches is a skill as well. What you, I assume, would like is to have whole sets of commits preserved in the final tree. E.g. merging without squashing. Assuming we do this by default, the next person might not have the patience, experience or practice building clean topic branches, and you end up with lots of tiny :fire:, bump, fix, f*ck, ... commits. At that point we start arguing that people please clean up their PRs, by rebasing, squashing and building proper topic branches? Or are we going to decide on a PR by PR basis if we are going to squash or not? The current model with phabricator, doesn't impose any restriction on how you use git, as it decouples the patches from the underlying VCS. However, as you rightly say, turning topic branches into multiple diffs can be tedious (been there, done that), and the commits you would like to preserve might not feel like great coherent pieces to review. I've yet to see where we run into real issues reviewing too many small diffs instead of one gigantic one.
Thanks for the offer. I would like to avoid things like multi-parameter type classes, functional dependencies, type-level stuff, mtl, template Haskell, and lenses. I'll try to come up with a concrete example.
There are two ‚Äúlevels‚Äù to consider, I guess. Within (safe) Haskell, `writeFile` is a pure function that just builds an `IO` action. But while writing the code, you also reason about it in terms of the imperative effects that action will have when executed by the RTS, and in those terms `writeFile` is impure. One of the nice things about Haskell (and effect systems in general) is that these levels of reasoning can be done separately.
Hence "effectful" and "non-effectful". Purity already has a well defined meaning.
What I‚Äôm getting at is that ‚Äú`IO` is pure/non-effectful‚Äù and ‚Äú`IO` is impure/effectful‚Äù are all true, depending on which level you‚Äôre looking at.
I don't have one. Maybe I should set up one? :-) I guess I could provide a BitCoin address or Ada address? I also offer consultancy services ;-)
Anyone giving this a spin, **please** let me know how it goes. I'd be eager to know what does, and what does not work. Let's improve this to the point where it "just works" :-)
Not sure where you got the idea that I would be trying to copy paste snippets of code, but that‚Äôs not what I‚Äôm trying to do at all. What I‚Äôm trying to do, is, after having read LYaHFGG, a lot of articles online, and being more than halfway through Erik Meijer‚Äôs online course, is understand the language by using it to do something a bit more practical. Unfortunately documentation seems lacking in most libraries I‚Äôve looked at. The yaml library for example, has a couple of example for very flat yaml structures. After looking at them and reading their type signature, I‚Äôve been able to understand them, but finding documentation for anything a bit more advanced seems difficult. 
I tried playing around with synonyms a bit and while it can definitely reduce the amount of code (in some specific cases), it didn't make it much easier to reason about. I'm intrigued now by the OP's article and wonder what a type system with auto lifting of these functor operations could look like.
&gt; OK, MultiSet. But still, there is no magic, it has to be sorted... otherwise it's just a list. It depends. If you want a normal form then I don't see any alternative to sorting. But an unsorted list is a perfectly sensible implementation of a multiset. You just have to check that `==` doesn't take into account order.
&gt; Yes, a programmatic API. All queries are complete programs. ... That might sound scary. I recently talked at a local meetup and received plenty of related concerns from the audience. Their concerns have since been addressed with a reasonable solution. It's no longer scary. What that looks like pends as another post. The post is long so I didn't read it all in detail. I don't think you showed what your language looks like. I was wondering if you used Dhall. If not could you say why not? I've been thinking of writing an programmatic API and if I did my first stop would be Dhall.
Looks interesting! I don't think semantic versioning makes sense for interfaces or APIs. There is no abstraction in an interface or API which means that there are no patch releases. See for example https://snowplowanalytics.com/blog/2014/05/13/introducing-schemaver-for-semantic-versioning-of-schemas/ Data structures already has a great standard - JSON Schema which I think mostly subsumes thrift, protobuf etc. for web standards. The diff of JSON Schema elements is mostly missing, but that's mostly orthogonal to representing the data structures.
I've been wondering about the same thing (that is, building a query language with Dhall, not "why didn't you use Dhall here"). We should chat!
That's a very strange format. Where did you get this file from? The parser that you wrote would parse the following format: - description: some description project: inbox contexts: [ online, computer ] - description: some other description description project: inbox contexts: [ online, computer ]
That tends to bias you towards the introductory material, though. It'd be better to select that same number of pages more uniformly I'd think.
Here's a hack using lens (don't use this!). You need to give types explicitly: import Data.Function (fix) import Control.Lens all_tuples :: Each s s b1 b2 =&gt; [b2] -&gt; [s] all_tuples l = fix (each (const l) . head) all_tuples [1, 2] :: [(Int, Int, Int)] -- [(1,1,1),(1,1,2),(1,2,1),(1,2,2),(2,1,1),(2,1,2),(2,2,1),(2,2,2)] all_tuples [1, 2, 3] :: [(Int, Int)] -- [(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),(3,2),(3,3)] all_tuples [] :: [(Int, Int)] []
I didn't show it on purpose. I wanted to flesh API versioning first as it's my biggest gripe. Examples do exist in the repo. I know that's not the best answer, but I promise I'll get around to documenting that part. It doesn't use Dhall. I actually haven't heard of it until a some weeks ago like a day or two before the meetup. A few people brought it up at the meetup as well. I glanced through it once since. Turing completeness is more important to me than guaranteed termination. I have reasons that I will explain more detail in the future post. Zzzzz... But for now, 1) The query runs in a sandbox environment which the server controls. Using the meta as additional input, the server can support intricate policies. Currently, lambdas, expressions, variable count, and service calls are monitored. 2) I wanted to future proof the language so that the client can transpile code.
&gt; Turing completeness is more important to me than guaranteed termination Interesting. I look forward to seeing what this language can do.
You're right. Technically, I don't use PATCH because it doesn't make sense to expose. Referring to SemVer was a simplification and misappropriation. Thanks, for the link. I'll dig into JSON Schema and the other stuff mentioned in there.
Currently installing LLVM 5, which is taking ages. Will try to toy with it after this, but the usage instructions are a bit vague :/ I assume it's 1. Get the dependencies (LLVM 5 and Xcode on macOS) 2. Clone the toolchain wrapper and `./bootstrap` along with getting it on your path 3. Fetch a binary distribution from http://hackage.mobilehaskell.org (going for iOS here) 4. Run `./configure --something...` which was a bit unclear to be exactly how you investigate this, and then `make install` 5. Use the generated ghc binary to compile a Haskell program? Does that sound about right?
Did you mean to do `export PATH=$(PWD)/toolchain-wrapper` and completely overwrite `$PATH` or should it have been `export PATH="$(PWD)/toolchain-wrapper:$PATH"`?
Why data SqlDialect = SqlDialect { renderSqlQuery :: SqlDialect -&gt; SelectQuery -&gt; String } Can't you just use: data SqlDialect = SqlDialect { renderSqlQuery :: SelectQuery -&gt; String } and rewrite member as member :: cls `Is` inst =&gt; (inst -&gt; a) -&gt; cls -&gt; a member prop obj = prop (cast obj)
Hmm, that's odd, since it has not worked for me. I have tried puttng the following in my stack.yaml packages: - . - location: git: git@github.com:reflex-frp/reflex.git commit: d9ef8457dfd140fde32d15a6959a15ddcd7e15e4 extra-dep: true - location: git: git@github.com:reflex-frp/reflex-dom.git commit: 585d49e5b79914e699cc73401299631380e49e3b subdirs: - reflex-dom - reflex-dom-core extra-dep: true However when i run "stack build" I get the error Warning: /home/seba/Documents/code/haskell/reflex-starter/stack.yaml: Unrecognized field in PackageGitLocation: subdirs Stack looks for packages in the directories configured in the 'packages' variable defined in your stack.yaml The current entry points to /home/seba/Documents/code/haskell/reflex-starter/.stack-work/downloaded/SqhqAEWvRrra/ but no .cabal file could be found there. The "SqhqAEWvRrra" folder is simply the one where reflex-dom is downloaded.
I looked more into JSON Schema. And it doesn't align with Fluid's goals. The main reasons were already described in the post. I'll sum them up here: * JSON Schema does the optionalizing hack. * No way to overload type and functions. This is for event sourcing. * Just my opinion, it's hard to read. It's overly explicit due to supporting many features. Most aren't needed or flat out avoided by Fluid. Thanks for pointing it out though. It's good to be aware if a different use case arises. :)
I'm not super familiar with the relationships between Stack and Nix. Do you mean that a new Stack project should be instantiated inside the nix one like that? I _will_ read the docs, but I'm on my computer at the moment
The format was something I came up with, I guess it could make sense, in the yaml, to make an Array where each element is an action, under an actions Object. The Haskell type used for parsing was also something I came up with when trying to model the yaml format. I guess my problem at this point is that I still don‚Äôt fully understand the types provided by Data.Yaml, and can‚Äôt seem to find examples I could analyse and try to understand.
Thank you for your responses! I am going to try to get the Dante setup working now, starting with getting Emacs 25.1 it seems :) 
Right. it should have been `export PATH="$(PWD)/toolchain-wrapper:$PATH"`. I'll fix it.
Yes pretty much. However getting LLVM5 on macOS should be as simple as `brew info llvm` or just unpacking the binary distribution from the llvm project. Ideally configure should only need `--prefix=/path/to/your/preferred/install/location`, however, due to some rather annoying ghc triple logic, you **may** need to provide `--target=aarch64-apple-ios --build=x86_64-apple-darwin --host=x86_64-apple-darwin`. This issue might be visible on linux and for the android compilers only.
I think you need to dedent the subdirs field. It shouldn't be a sub part of the location block.
Thanks! Yeah I was installing through brew, but `make` was running forever, got it done now. Ah, that'll be helpful. It seems a nice wrapper around these options would be a good starting point as for figuring out the exact `--target`, `--build` and `--host` commands. Seems `--build` and `--host` are always the same? Then just querying the user for which target they want etc :) I'll look more into it as I progress, currently my internet speed is killing me and the DL for the binary is taking ages, lol.
Scalable mean application where you would have a holy flow people accessing it so it doesn't slow or crash when you have this flow ;)
Thanks do much:) very helpful. PS : I don't want to fake my way... Otherwise I wouldn't use haskell üòâ
Could you sell.me. some times guys üòÅ
You can implement such function with `TemplateHaskell`. It's easy to implement `fst` using `TemplateHaskell` which can get you first element of the any tuple (though you need to pass size of this tuple). And it's easy to refactor such `fst` into your `n-tuples` function.
Using msbuild spares you the trouble of debugging mysterious compiler errors when working with windows sdk.
I am not an imperative language skilled programmer .. I started very slowly reading/knowing/discovering about haskell a years ago and I discovered this treasure ;) Now, I want to train myself writing a proper art code for a project that I will start in 2 months. So I want to write complete (SMALL) program . I did a bit of functional programming in stratagem (very weird crap language) . I did already all the courses of Discrete mathematics, category theory and logic (+ some correctness stuff) in order to start functional programming. So it is more like I want something like the book of Julie &amp; Chris but shorter and faster so I could have a big image about functional ;) Thanks so much for your time writing, awesome reply and thanks for sharing the link ;) 
I know monad, I am really far away from using it üòÇ but there will be someone who could help.me, but my point from the questions is that I want to have a big image, learn fast for 2 months and the scalable application is more for long term project and of course not in the short term ;) thanks 
I've opened https://github.com/mobilehaskell/hackage-overlay/issues/1 (understood that you wanted the issues there)
True ;)
Thus something I need :D Thanks
The Dana Scott videos are really good. They start out slow, but ramp up quite a bit by the end.
Exact, I would rather use purscript for the web app but for some back end/algorithm etc.. I would like to do in haskell ;) 
I don't want to produce a genetic son ( I think my question wasn't clear enough) my plan for building scalable app is for a long term but what I am asking for is a material/resources where It helps me to write calculator, Sudoku, hello world :p , parser etc..
You will have anomalies haha I don't want that :p
I have a little startup and we decided to do it in haskell, it is a long term project but we want to learn the how to fly in a specific air plane/specific route. So yes, I think a trainer would be good start ;) Thanks :)
It is a matter of experience/time ;) We will see it ;)
I used this to learn purscript, it was awesome, so yes maybe good to give it a try for haskell üòâ
thank you!
These should actually **not** be required. Their requirement right now is due to some defect in the configure script. I'll try to get this sorted upstream ;-)
Haha, wow, of course it would be something that simple... Thank you! It ran the build correctly, but did unfortunately crash on a missing Semigroup instance in Reflex.Patch.DMap. Glad to know the subdir works for future reference though!
I agree with what @tdammers said &gt; scalability is a matter of architecture more than individual technology choices I think this is the key point to build scalable application in a sense that the application is decoupled so when you update some part of the application you won't affect the other part. I suggest that you can check some shorter book such `programming in Haskell` for graham Hutton.
You can always parse a valid yaml document as a `Value`. The example you provided get's parsed as: Array [ Object (fromList [("action", Array [ Object (fromList [("description", String "some description")]) , Object (fromList [("project", String "inbox")]) , Object (fromList [("contexts", Array [String "online", String "computer"])]) ])]) , Object (fromList [("action", Array [ Object (fromList [("description", String "some other description description")]) , Object (fromList [("project", String "inbox")]) , Object (fromList [("contexts", Array [String "online", String "computer"])])])]) ] From that, you can see your action object has one field named "action" and with array value. Each array element is a one field object. So, here's how I'd go about writing the instance: First, we need a way of combining the one field objects into a single object. I'll define a helper function for that, using combinators from foldl: import qualified Control.Foldl as L squashArray :: String -&gt; Array -&gt; Parser Object squashArray name = L.foldM $ L.sink $ withObject name pure Next, I'll define some with helper functions, imitating the style of `withObject`, `withArray`, etc. withLookup :: Text -&gt; (Value -&gt; Parser a) -&gt; Object -&gt; Parser a withLookup name f obj = (obj .: name) &gt;&gt;= f withSquashedArray :: String -&gt; (Object -&gt; Parser a) -&gt; Array -&gt; Parser a withSquashedArray name f arr = squashArray name arr &gt;&gt;= f Finally, the FromJSON instance itself: instance FromJSON Action where parseJSON = withObject "outer Action" $ withLookup "action" $ withArray "inner action" $ withSquashedArray "action subobject" $ \v -&gt; Action &lt;$&gt; v .: "description" &lt;*&gt; v .: "project" &lt;*&gt; v .: "contexts"
Any chances of having all these steps encoded as Nix expressions?
Yes, eventually. As I see nix as a nice way to manage this for those who use nix, I don't want to impose the requirement of nix, just for using the cross compilers. I think we want these to be easy to install without nix. They will then also be easy to install with nix.
Will do ;)
Another trick (which you probably shouldn't use): {-# LANGUAGE DataKinds, TypeFamilies, MultiParamTypeClasses, FlexibleInstances #-} data Nat = Z | S Nat type family Result (n :: Nat) arrows where Result 'Z a = a Result ('S n) (a -&gt; b) = Result n b type family CountArgs f where CountArgs (a -&gt; b) = 'S (CountArgs b) CountArgs result = 'Z class (CountArgs b ~ n) =&gt; Applyable a b n where apply :: [a -&gt; b] -&gt; [a] -&gt; [Result n b] instance (CountArgs b ~ 'Z) =&gt; Applyable a b 'Z where apply = (&lt;*&gt;) instance (Applyable a b n, a ~ a') =&gt; Applyable a (a' -&gt; b) ('S n) where apply f x = apply (f &lt;*&gt; x) x applyAll :: Applyable a b n =&gt; (a -&gt; b) -&gt; [a] -&gt; [Result n b] applyAll f x = apply [f] x applyAll (,) [1,2] -- [(1,1),(1,2),(2,1),(2,2)] applyAll (,,) [1,2] -- [(1,1,1),(1,1,2),(1,2,1),(1,2,2),(2,1,1),(2,1,2),(2,2,1),(2,2,2)] from the n-ary zip in [this](https://github.com/goldfirere/thesis/raw/master/built/thesis.pdf) paper.
&gt; Not sure where you got the idea that I would be trying to copy paste snippets of code, but that‚Äôs not what I‚Äôm trying to do at all. That's good to hear! The reason I got the impression you were copying bits of code is that you first said you were looking for examples and tests related to your format, and then you said it again about Array. This didn't seem right to me; if you understand the type of `decode`, then you shouldn't need to see separate examples for each type you want to parse. decode :: FromJSON a =&gt; ByteString -&gt; Maybe a The type says that it can decode any type which has a `FromJSON` instance. So I would look at [the `FromJSON` instances](http://hackage.haskell.org/package/yaml-0.8.23.3/docs/Data-Yaml.html#section.i:FromJSON) to see what types I can parse. `Int`, for example: &gt;&gt;&gt; import Data.Yaml &gt;&gt;&gt; :set -XOverloadedStrings &gt;&gt;&gt; decode "42" :: Maybe Int Just 42 Given that `Int` example, the `FromJSON a =&gt; FromJSON [a]` instance, and some knowledge about type classes and about the yaml syntax for lists of things, it should be straightforward to figure out to parse a list of `Int`s: &gt;&gt;&gt; decode "- 32\n- 42\n- 52\n" :: Maybe [Int] Just [32,42,52] Since you seem to understand yaml, I incorrectly guessed that you either didn't understand type classes yet, or that you were used to searching and combining examples instead of looking at the types and figuring things out.
My first Haskell project was a text game, but my second project was a web app, which I started 2 years ago and still enjoy maintaining and adding new features to today. I started building that app having read less than half of Learn You a Haskell plus random chapters of Real World Haskell, depending mostly on random tutorials on the internet. Use Scotty -- it has the easiest types and most tutorials. (You can always switch to another web framework later.) Don't worry about understanding everything right away. Make mistakes and learn from them. Cool thing about Haskell is when you realized you've screwed up, the types can help you fix things. I do agree that there's no shortcut -- this still takes time before you should feel comfortable accepting money from real customers. I'm sure a lot of people find this trial-and-error sort of a learning experience very frustrating. But personally if I *had* to read a long book or complete a course, I would just not use Haskell. Luckily Haskell is like any programming language and there's no wrong way to learn.
&gt; I truly fear this is going down the vim/emacs kind of route. Where different styles and mindsets collide. Building proper topic branches is a skill as well. What you, I assume, would like is to have whole sets of commits preserved in the final tree. It can be a skill, I can't really judge that since I come from the pre-Git world and were used to patch queues. What's most important is that it's one of those things, like Simon Marlow noted about his DWARF branch in a sibling comment, that once you learn and experience it, you appreciate it and do it by default. The reviewability is only one aspect and one that vanishes once merged. The more useful history retained by having a merge commit that has several commits dangling off it is another, and this also enabled more practical bisectability. Once development is done and merged, the bisectability is the most widely applicable advantage. I've had my fair share of bisecting where I landed in one of those mega commits, which is the culprit causing the bug, but is impractical to dissect. If it's split reasonably like mentioned by simonmar, not just for artificial constraints of how big diffs should be, then it's highly unlikely that the commit bisect lands you on will be just a black box of 1000s of lines changed. It's like Haskell's semantics. Once you learn and appreciate them, you don't want to go back and start trying to achieve them in other languages. Therefore, I don't think the comparison of vi/emacs is as fitting, since that is admittedly more about subjective styles of editing and extending your tool. &gt; Or are we going to decide on a PR by PR basis if we are going to squash or not? I haven't been in the position to merge a differential myself, so I don't know the actions available. But on github and gitlab you can select the type of merge you do when you're about to press merge. This would suit the situation you mention, where the maintainer just squashes them all into one when merging. &gt; I've yet to see where we run into real issues reviewing too many small diffs instead of one gigantic one. Wait, isn't the preference to have one big diff as created by arc? So don't you mean there has been no problem with mega commits, merged flatly into master, without merge commits?
How many requests per second or do you consider to be really large?
1.5 million user /request
That's only 18 requests per second, which any Haskell service combined with a Postgres database can handle without any issues whatsoever
Do you feel like learning Haskell before is good for improving programming in R ? How much of a functional language is R and what Haskell techniques apply there ? Ultimately I know R is not the most elegant language but in my domain it is the best tool.
How many requests (x= ? request per second) you should have to consider your application as *it has a big flow*? 
Nah, the importance of later stuff in almost all books ever written tends to fall off, astonishingly, by almost precisely 1/Log(N)
You used HTAC to learn purescript ?
In this case, if you were familiar enough with the language, one thing you could do is set up a pure state, and build a datatype to represent input events, then have your IO code initialize the pure state, and using a pure function defined elsewhere, create a new state based on the translated event and the old state. This is very much like the state monad, so you could use that here if you understand it, but if not, you essentially want: data WorldState = ... data Event = ... updateProgram :: Event -&gt; WorldState -&gt; WorldState ... initialState :: WorldState ... render :: WorldState -&gt; IO () ... -- or display :: WorldState -&gt; DrawCommand draw :: DrawCommand -&gt; IO () render2 = draw . display main = do world &lt;- newMVar initialState keyListener (\key -&gt; modifyMVar_ world (return . updateProgram (Key key))) ... forever $ (readMVar world &gt;&gt;= render &gt;&gt; threadDelay (someTime)) I haven't looked at the exact semantics for `keyListener` that you're talking about, but hopefully this should give you a pointer in the right direction of how you can just build a small impure wrapper around a large pure calculation.
I needed to write out some XML and used HXT which worked fairly well but left me with a nagging question. HXT defines a couple arrows but I am gonna pick IOLA as example: data IOLA a b = IOLA (a -&gt; IO [b]) IOLA is an instance of `ArrowApply` which means that it is a monad. However iirc `IO [a]` isn't a valid monad instance. So my question basically comes down to: Are some of the `Arrow*` typeclasses HXT uses unlawful or could it use a monadic interface?
Sorry, I meant this PureScript by Example by Phil Freeman ;) It is on the same website learnpub
Ok. HTAC, by Mark Watson, is well written and a much faster intro than HPFFP.
Basically almost never.
Will take a look on it, thanks :)
 IO [a] Has a valid monad instance. You just can't directly make it a monad, but with a newtype wrapper: newtype IOL a = IOL (IO [a]) instance Monad IOL where ... Is perfectly fine. Likewise for `IOLA`
Hum ok, I guess I see what was the problem with my yaml. If I removed the dashes from before each of the key, value pairs, then I‚Äôd get a single object with multiple fields, which would remove the need for squashArray and then just use withObject instead, did I understood it correctly? I‚Äôll have to re-read this a few times to completely understand everything. I haven‚Äôt yet completely got why the withLookup for the outer action, but my impression is that it will look for the ‚Äúdescription‚Äù, ‚Äúproject‚Äù and ‚Äúcontext‚Äù, and the reason it‚Äôs done this way is again because the keys are inside a yaml Array (which I will definitely fix). Thanks for the example above, it‚Äôs been really helpful for me to understand how it works.
Hi, I‚Äôve used yaml a few times, usually some format already defined, never actually had to define my own before (and apparently I screwed up because I made my Object into an array of objects accidentally). I did knew that decode would work in any type that as a FromJSON instance, I even had my own in the code. When I asked about Array was also because I knew there was an Array on Data.Yaml which had an instance of FromJSON, but I didn‚Äôt knew how to combine that with the part of my data structure that is custom, and all the examples I could find were either using simple yaml formats where you have just a list of ints or strings, and the only example that was a bit more complex (the Config.hs in the library‚Äôs examples), didn‚Äôt have nested structures like Objects inside Arrays, for example, that‚Äôs why I was looking for examples.
Personally I wouldn't go that far. I'd still use proper/custom types from the start. I just wouldn't go too crazy trying to find the perfect type class or higher order function. 
It doesn't follow a &gt;=&gt; (b &gt;=&gt; c) = (a &gt;=&gt; b) &gt;=&gt; c though, right?
I mean like during literally the first minute. 
Learning Haskell will improve your facility with functional design, and basic usage of higher order functions (eg mapping a function over the elements of a container). This is because it provides a very concise and elegant type system that shines a bright light on these issues. It will not help learning idiomatic R or any of the common libraries, and it will foster a deep sense of longing for a static type system during subsequent use of R. üòâ 
What would it take to be able to create a type that rejects certain literals, like all integers below 1? For example, a type `Positive a` where `Positive Int` and `Positive Word64` would fail at compile-time if passed 0 as a value.
Thank you. TIL about lens. However, I am very far from understanding how it works. It also seems to use things like profunctors and bifunctors that I don't understand yet either.
&gt; How is it bad? One answer: it's not, _per se_, but beginners will often reach for it due to their familiarity with OO, and it makes their lives harder. Haskell _can_ do OO; most of the time, though, it _shouldn't_.
**Compile Time Checks** Literals of certain types that are out of a range can cause a warning/error: Prelude&gt; :set -Werror Prelude&gt; (-1) :: Word &lt;interactive&gt;:3:3: warning: [-Woverflowed-literals] Literal -1 is out of the Word range 0..18446744073709551615 &lt;no location info&gt;: error: Failing due to -Werror. The question becomes how is this done and is this extensible? The answer appears to be no, a recent version of GHC has the type to check explicitly enumerated: warnAboutOverflowedLiterals :: DynFlags -&gt; HsOverLit Id -&gt; DsM () warnAboutOverflowedLiterals dflags lit | wopt Opt_WarnOverflowedLiterals dflags , Just (i, tc) &lt;- getIntegralLit lit = if tc == intTyConName then check i tc (undefined :: Int) else if tc == int8TyConName then check i tc (undefined :: Int8) else if tc == int16TyConName then check i tc (undefined :: Int16) else if tc == int32TyConName then check i tc (undefined :: Int32) else if tc == int64TyConName then check i tc (undefined :: Int64) else if tc == wordTyConName then check i tc (undefined :: Word) else if tc == word8TyConName then check i tc (undefined :: Word8) else if tc == word16TyConName then check i tc (undefined :: Word16) else if tc == word32TyConName then check i tc (undefined :: Word32) else if tc == word64TyConName then check i tc (undefined :: Word64) else return () **Run Time Checks** To force checking at runtime you'll want to look into **smart constructors**. That is, export a function of a type like `pos :: a -&gt; Maybe (Positive a)` so you can check the value before declaring it is positive. From then on you can lean on the type system to ensure all values of type `Positive a` are in the right domain.
It was an analogy. Some things just take time AND effort. For most people, learning Haskell is one of those things.
I think the magic number is still 10k req/sec.
Agree, no doubts ;)
I mean there is "`ListT` done right". Which does follow that. 
I mean I generally start out with writing some my types and the types of the initial functions, so I never have a point in time where I use string. They aren't very convenient to work with compared to ADTs and various "smarter" data structures. 
Yes, but as far as I know every ListT done right implementation works by interleaving effects and consing in some way as in ListT m a = ListT { uncons :: m (Step m a) } Step m a = Done | Cons a (List m a) While `IO [a]` very much does not.
Best book after LYAH? I am looking at getting the haskell cookbook
I have gotten most things working now! Although Dante does not seem to recognize my dependencies in my test files in subdirectories. For instance it cannot find Test.Hspec even though it is in my cabal dependencies. The tests do run and work, but I have constant red squiggly lines in my test imports. Did you experience this issue? 
Are you using Nix? If so, how are you entering a nix-shell? `work-on`? Or a custom Nix expression?
I want to try and setup a backend with Scotty to use with Elm. Yet I somehow can't even get past the first few steps. I'm using this [tutorial](https://maciek.io/rest-api-in-haskell/) and I'm pasting this into my project.cabal: executable projectname hs-source-dirs: ... #what you want main-is: Main.hs other-extensions: OverloadedStrings build-depends: base, scotty, aeson, postgresql-simple default-language: Haskell2010 and I get this error message: paul@ubuntu:~/projekte/scotty-test$ stack build scotty-test-0.1.0.0: configure (lib + exe) Configuring scotty-test-0.1.0.0... scotty-test-0.1.0.0: build (lib + exe) Preprocessing library scotty-test-0.1.0.0... [1 of 1] Compiling Lib ( src/Lib.hs, .stack-work/dist/x86_64-linux-nopie/Cabal-1.24.2.0/build/Lib.o ) Preprocessing executable 'scotty-test' for scotty-test-0.1.0.0... [1 of 1] Compiling Main ( app/Main.hs, .stack-work/dist/x86_64-linux-nopie/Cabal-1.24.2.0/build/scotty-test/scotty-test-tmp/Main.o ) /home/paul/projekte/scotty-test/app/Main.hs:3:1: error: Failed to load interface for ‚ÄòLib‚Äô It is a member of the hidden package ‚Äòscotty-test-0.1.0.0‚Äô. Perhaps you need to add ‚Äòscotty-test‚Äô to the build-depends in your .cabal file. Use -v to see a list of the files searched for. -- While building package scotty-test-0.1.0.0 using: /home/paul/.stack/setup-exe-cache/x86_64-linux-nopie/Cabal-simple_mPHDZzAJ_1.24.2.0_ghc-8.0.2 --builddir=.stack-work/dist/x86_64-linux-nopie/Cabal-1.24.2.0 build lib:scotty-test exe:scotty-test --ghc-options " -ddump-hi -ddump-to-file" Process exited with code: ExitFailure 1 If I add scotty-test to scotty-test.cabal under build-depends then I can build but not run with stack exec scotty-test-exe... Any ideas?
Getting back to OOP basics, we have messages and objects. Every OOP concept can be expressed in these terms -- inheritance is merely delegating a set of message to some other object. In Ruby, we can think of a "duck type" as "an object that responds to the set of messages I call on it." So a method like: def hello some_object some_object.foo(name: "Hello") some_object.bar(age: 28) end accepts a parameter `some_object` that has a "duck type" of "accepts a message `foo` with a single parameter of `name: String`". In a Haskell-like syntax, using type classes to dispatch on these attributes, this would be: hello :: (HasFoo object (Name -&gt; IO ()), HasBar object (Age -&gt; IO ())) =&gt; object -&gt; IO () hello object = do foo object (Name "hello") foo object (Age 28) Interfaces are used in nominal statically typed languages to say: "I can accept any object as a parameter that responds to these messages." In dynamically typed languages, you don't get types to help restrict your inputs. In structurally/statically typed languages, you get "ad hoc" interfaces where you can say something like (stealing PureScript syntax for a bit): foo :: forall r. { print :: String -&gt; IO () | r } -&gt; IO () foo object = object.print "lmaooo" Since Java, C#, and Haskell lack structural types, we have to give explicit names to these interfaces. class HasPrint o where print :: o -&gt; String -&gt; IO () public interface HasPrint { public void print(String str); } Java requires us to explicitly satisfy these interfaces in the definition of a class: public class Printer implements HasPrint { public void print(String str) { ... } } And now we can accept a `HasPrint` object: public void foo(HasPrint printer, String str) { printer.print(str); } Interfaces don't "do" anything -- they just provide a way to work around a lack of structural typing by providing a name to a common set of attributes/methods. If you *can* express structural types, either via structural subtyping or row polymorphism, then you don't need interfaces.
you can use foo n = sequenceA . replicate n instead. I find it much easier to understand when your original functions are written as `join $ liftA2 (,)`, `join.join $ liftA3(,,)`, `join.join.join $ liftM4(,,,)`, etc., BTW.
You can work through the book pretty fast if you can devote a lot of time to it and have someone that can help. Fortunately, #haskell-beginners on Freenode/IRC is full of those someones, as is the #haskell-beginners channel on [the FPChat slack channel](https://fpchat-invite.herokuapp.com). The page count is high, and it's mostly exercises that ensure you actually grok what you're doing. You can learn a ton if you're able to pair program on a project with someone that's already familiar with the language and ecosystem. You can probably hire consultants like [Well-Typed](http://www.well-typed.com/) or [FPComplete](https://www.fpcomplete.com/) to help out with the initial phases of training and writing the application. You might be able to find an individual who doesn't mind pairing/helping if it's a personal project.
I think this was the most exciting talk from Compose this year. The basic idea is that you annotate functions with apartness checks - `a` is apart from `b` if the compiler can tell that they will never unify. If the apartness check succeeds (i.e., the call to the function can never typecheck), then you emit a custom error message. This is particularly useful for DSL library authors, where you want to keep the programmer thinking in terms of the domain, maintaining the abstraction. Unfortunately it's all experimental extensions that aren't in mainline GHC but hopefully something like it will come in soon.
The haskell book. 
Avoid PAKT publishing.
Why so?
&gt; I looked more into JSON Schema. And it doesn't align with Fluid's goals. The main reasons were already described in the post. I'll sum them up here: &gt; &gt; JSON Schema does the optionalizing hack. I'm not sure I understand what the optionalizing hack is. You mention that word a few times in your article, but what does it exactly mean? &gt; No way to overload type and functions. This is for event sourcing. Can you give examples of this. Not sure what you mean here either. JSON Schema has `anyOf`, `oneOf` to specialize, generalize, create sym types and similar. For strings you have `format` which is user-extensible. Other than that, unspecified fields must be ignored which means you can embed your own standard keywords and be compliant with the standard afaik. &gt; Just my opinion, it's hard to read. It's overly explicit due to supporting many features. Most aren't needed or flat out avoided by Fluid. Given the constraint that it's parseable as json, I don't see how it's hard to read. An advantage is that it's a standard, and you see the same syntax and semantics appear in other systems such as in OpenAPI/Swagger. 
`Data.Coercible` can help with `boxs`.
Tried and true: The Craft of Functional Programming.
I am not claiming you see ki me ec me j I'm lmybcnj I'll j k.
I am claiming that interfaces are needed unconditionally. I am just saying that a record of functions doesn't do all the things that an interface or typeclass does in Java, C#, or Haskell (due to lack of structural types as you mention above), so it is a bit disingenuous to imply that a record of functions is equivalent to an interface, that's all
I really recommend this video for anyone who wants to learn about lenses:https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation
10,000 requests per second is the minimum I expect from a single server for basic CRUD operations (or about 1 billion requests per day)
It's possible. CompCert was written in Coq.
They tend to produce poorly edited, poorly composed, and otherwise low-quality books. At least the Haskell related books I've seen fit this bill.
I wrote a register allocator in Coq that was used in a compiler written in Haskell. 
Thanks man, saved me $30
Theory: Coq isn't Turing-complete, so there are some programs it can't express. Practice: Coq can express all the programs that matter. The difference between Coq and a compiler is like the difference between a trident and a pitchfork. They're pretty much the same thing, they just came from different places and with different purposes. Also practice: Coq doesn't have shit for libraries, compared to e.g. Java or Python. General-purpose programming might be difficult.
Their videos too, I made a Haskell video course for them and I refuse to link to it because I'm ashamed of the poorly-edited result :(
Idris seems to be the closest to "programming language and proof assistant". Agda is also viable in that vein I believe. 
Username checks out.
Ltns! :)
&gt; Emacs gets new maintainer as Richard Stallman signs off &gt; &gt; Long-time contributor John Wiegley steps into the chair" &gt; &gt; (Source: [The Register, Nov. 2015](https://www.theregister.co.uk/2015/11/05/wiegley_new_emacs_maintainer/)) Didn't realise. Kudos!
We rarely go through the trouble, but it's possible. The solution we usually use is smart constructors, and the compile-time solution builds on it, so let's start with that first. In a separate module, I'd define the `Positive` type and a "smart constructor", that is, a function which fails at runtime if its input doesn't satisfy the type's requirements: module Positive (Positive, unPositive, positive) where import Control.Monad (guard) data Positive = PrivatePositive { unPositive :: Int } deriving Show positive :: Int -&gt; Maybe Positive positive n = do guard (n &gt; 0) pure $ PrivatePositive n Since the export list does not include the `PrivatePositive` constructor, users of this library cannot construct values of type `Positive` except by going via the smart constructor, and so you are guaranteed that all `Positive` values hold a positive Int. Now, this check is performed at runtime, and that's useful when you're receiving an Int at runtime and you don't yet know whether it is positive or not. For literals, it's a bit annoying because we already know that `positive 42` will return `Just (PrivatePositive 42)`, so this `Just` is uninformative. There is a (partial) function which we can use to remove the `Just` when we are 100% certain that the `Nothing` case will never happen: `fromJust`. import Positive import Data.Maybe (fromJust) -- | -- &gt;&gt;&gt; realPositive -- PrivatePositive {unPositive = 42} realPositive :: Positive realPositive = fromJust . positive $ 42 -- | -- &gt;&gt;&gt; notPositive -- *** Exception: Maybe.fromJust: Nothing notPositive :: Positive notPositive = fromJust . positive $ 0 If you were 100% certain and yet were dead wrong, an exception will be thrown at runtime when `fromJust Nothing` gets forced. Let's turn this into a compile-time error: {-# LANGUAGE TemplateHaskell #-} import Language.Haskell.TH.Syntax (lift) realPositive :: Positive realPositive = $(lift . fromJust . positive $ 42) -- error: -- Exception when trying to run compile-time code: -- Maybe.fromJust: Nothing notPositive :: Positive notPositive = $(lift . fromJust . positive $ 0) What I did here is I used [TemplateHaskell](https://ocharles.org.uk/blog/guest-posts/2014-12-22-template-haskell.html) to run my smart constructor at compile-time, and then I used [`lift`](https://hackage.haskell.org/package/template-haskell-2.12.0.0/docs/Language-Haskell-TH-Syntax.html#v:lift) to convert the resulting compile-time `Positive` value into a runtime expression with the same value (I also had to add `deriving Lift` to my definition of `Positive`). If I give a non-positive input, this compile-time-to-runtime conversion will force the `fromJust Nothing`, resulting in a compile-time error, hurray! We can do even better by using a QuasiQuoted to define a custom syntax for our `Positive` literals: import Control.Monad ((&gt;=&gt;)) import Text.Read (readMaybe) import Language.Haskell.TH.Quote (QuasiQuoter(..)) import Language.Haskell.TH.Syntax (Lift(..)) positiveLiteral :: QuasiQuoter positiveLiteral = QuasiQuoter { quoteExp = \s -&gt; do case (readMaybe &gt;=&gt; positive) s of Nothing -&gt; fail $ show s ++ " is not a positive number" Just p -&gt; lift p } The input is a string `s`, and I took the opportunity to throw a `Positive`-specific error message instead of the generic "fromJust: Nothing" message, but otherwise this is basically the same code as before. Since I have defined it as a QuasiQuoter though, I get this nicer syntax: realPositive :: Positive realPositive = [positiveLiteral|42|] -- error: -- "0" is not a positive number notPositive :: Positive notPositive = [positiveLiteral|0|] 
It is quite feasable: Program in Coq (or Isabelle) and extract to the language of your choice (ML, Haskell, Scala, ‚Ä¶)
Could you explain in what way did you do it better? The paper linked below is quite long..
Perhaps if you wanted to do a port of existing code in a direct way.
Stack supports piggy backing off of a nix shell instead of managing its own dependencies. So you can write a custom `shell.nix` to get you into a dev environment (Reflex functions to help you do this easily, namely `workOn` in `default.nix`). Then you just configure your `stack.yaml` to use that nix file. At that point all `stack` commands open the nix shell underneath before running anything. With this technique you can use `intero` without a hitch.
&gt; I'm not sure I understand what the optionalizing hack is. You mention that word a few times in your article, but what does it exactly mean? The hack refers to non-required fields. With Fluid, every field is required without exception. A field may seem optional when constrained by a `Maybe`. But, the `Maybe` field is still required and expects an explicit null for `Nothing`. Not having a field versus not having a meaningful value in an existing field are different semantically. Furthermore, non-required fields are often used as a means of introducing or deprecating fields. Specs should be immutable when automating the version. Updates should made independently, such as to a copy. This is how changes are diff‚Äôed between the two. &gt; Can you give examples of this? Not sure what you mean here either. I tried to explain the motivation in the two sections, **Fluid's Implementation &gt; The spec &gt; Types** and **What's next &gt; Event sourcing**. Essentially, all service calls (functions) have type equivalents. &gt; JSON Schema has anyOf, oneOf to specialize, generalize, create sum types and similar. For strings you have format which is user-extensible. Yeah, the user extension would solve that and other things. I don't think it's a good idea though ... &gt; Other than that, unspecified fields must be ignored which means you can embed your own standard keywords and be compliant with the standard afaik. ... because Fluid's usage would end up being a largely customized use JSON Schema anyways. Why not let Fluid‚Äôs opinions be explicit. Even what you would think be similar, it still clashes with JSON Schema. For example, Fluid‚Äôs members/properties for structs/types are in an array containing singleton objects of `{name:type}`. It's not one object containing multiple pairs of `{name:type}`. Fluid: [{"a":"A"}, {"b":"B"}, {"c":"C"}] JSON Schema: { "a": "A", "b": "B", "c": "C"} &gt; Given the constraint that it's parseable as json, I don't see how it's hard to read. An advantage is that it's a standard, and you see the same syntax and semantics appear in other systems such as in OpenAPI, Swagger, RAML. To me it seems ubiquitous. So if your syntax improves on readability you have to weight that against that most users and tools know JSON Schema. Parsable as JSON, though crucial, isn‚Äôt the sole constraint. The Fluid spec format knows what it is and doesn't do anything else. I don't have any aspirations to generalize it into a grand standard like JSON Schema. This additional constraint enables Fluid's readability. A good design prevents mistakes by making them difficult to commit. This is how Fluid does sum types: { "n": "Color", "e": [ "Red", "Green", "Blue", { "tag": "Custom", "m": [{"red‚Äù:‚ÄùU8"}, {"green‚Äù:‚ÄùU8"}, {"blue‚Äù:‚ÄùU8"}] } ] } The first three are constructors without members. The last constructor, Custom, has three listed members of unsigned 8 bit integers. I failed to find any clean way to represent the equivalent sum type in JSON Schema. Sum types is one of my favorite language features in Haskell. It's unfortunate that JSON Schema doesn't natively support it. But perhaps you can show me otherwise? 
I've wrote an GUI app with Gtk2hs on macOS. How can I hide the terminal when start the app by double-clicking :) Also, what is the best practices for storing resources in stack project, like xml, images, icon...? Thanks.
I think the summary on LtU does it justice even if it is understated.
You can ask /u/HuwCampbell questions :).
Wow. Thanks for the TH and QQ solutions. I can you them for a couple of things in my project. Could you explain the `lift` part a bit? What is being lifted to what? 
This is the last of the Compose Melbourne talks! The full playlist is now online. I've was super excited to see this one, so it's great that it's able to be seen by a wider audience!
Yes. Also, you could ask on r/Coq.
It differs from normal programming languages in that, in order to be sure that it does what you want, you need to manually check the specification rather than the implementation.
Perhaps. Here are two other options, without changing the types: * Change the `FromJSON` instance of `Action` to parse an `Object` inside and `Object`. So the syntax for an `Action` is now action: description: ... project: ... contexts: ... * In the `FromJSON` instance for `Actions`, require the top-level `Value` to be an `Array`, not an `Object`, and parse each element of the `Array` as an `Object` (i.e., `traverse` an `Object` parser over the elements of the array).
I have not made the jump yet! Was mostly getting Dante/hlint to work with an already working stack setup. Baby steps. Hopefully the error is stack specific, though I noticed that if I add my deps to the library part of the cabal file Dante picks then up just fine 
To me this looks orthogonal to JSON Schema, or Swagger, or the like. I would use the versioning ideas described in this post to implement versioning of a schema in one of those languages.
I'll try to explain. I suppose you know `traverse`. It executes an action for every element of a list (Traversable), and returns an action of a list (Traversable) of results. So we can take an list, substitute each element for a list monad, then we get all combinations with a list of the same size: &gt; traverse (\_ -&gt; [1, 2]) [(), (), ()] [[1,1,1],[1,1,2],[1,2,1],[1,2,2],[2,1,1],[2,1,2],[2,2,1],[2,2,2]] `Each` does the same, except it works for a tuple with all elements of the same type: &gt; each (\_ -&gt; [1, 2]) ((), (), ()) [(1,1,1),(1,1,2),(1,2,1),(1,2,2),(2,1,1),(2,1,2),(2,2,1),(2,2,2)] Note that `each` uses a typeclass `Each`, which has instances for most tuples. Now the problem is that you don't know which instance is used, so you cannot provide it with a template tuple! The solution is to take a tuple from the output, which works because of lazyness (and because we don't inspect the element): &gt; all_tuples l = let t = each (\_ -&gt; l) (head t) in t rewrite it with `const` and `fix`: &gt; all_tuples l = fix (\t -&gt; each (const l) (head t)) eliminate the lambda: &gt; all_tuples l = fix (each (const l) . head) 
Idris is pretty slow unfortunately. It does produce pretty good javascript though, at least as far as I've heard. According to the devs the LLVM backend needs work.
`PrivatePositive 42`, of type `Positive`, is lifted to `[|PrivatePositive 42|]`, if type `Q Exp`.
Yes, you understood correctly. `withLookup` is just used to get the value of the "action" key. If you make your proposed change, you can still use `withLookup` if you want, but you might find it simpler to do this: instance FromJSON Action where parseJSON = withObject "Action" $ \outer -&gt; do v &lt;- outer .: "action" Action &lt;$&gt; v .: "description" &lt;*&gt; v .: "project" &lt;*&gt; v .: "contexts"
Semi-related, what's with trac's SEO? It seems like Google refuses to pick up trac tickets that almost have the exact wording as my search query. The number of times I've asked on IRC or even raised a new ticket only to be pointed to an already existing one...
We have an office in Munich, yes. You are welcome for a coffee at any time. 
Find the class on Hackage and look at all it's instances. Each instance is one thing it's good for.
I'm not sure whether my question was unclear, or whether your answer simply requires more knowledge than I have to answer my question: [Applicative Instances](https://hackage.haskell.org/package/base-4.10.0.0/docs/Control-Applicative.html) tells me less than the [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia#Instances_2) listing - at least in terms of how much practical application I can learn from a listing. I'm looking to enumerate the toolkits that people have in terms of using type classes to solve understandable problems, where "understandable" is defined in terms that programmers who do not understand functional programming might recognise.
The thing is, when an abstraction is "designed", then asking "what is it good for" make sense. These typeclasses describe very general interfaces and they were "discovered" much more than they were designed. You say "applicative solves the problem of joining async operations" but it doesn't. Not only is this just one thing that happens to be describable by an applicative interface, you could have the exact same expressive power without ever mentioning the `Applicative` class.
I'm away from `ghci` at the moment and can't try this out, but I think you can do something with `liftM` to get this to work.
For associated types, you might be able to get away with a "mother of all (associated) type families".
In the pragmas list, there's an unnecessary space in `MultiParamType Classes`. Also, for people looking for more on the same subject: http://okmij.org/ftp/Haskell/TypeClass.html#Haskell1
https://mail.haskell.org/pipermail/haskell/2007-March/019181.html 
By the way: what happened to the Munich Haskell Meetup? Any interest in reviving that? I think i could take care of hosting it in our offices.. what do you think?
Given a function `a -&gt; e`, you can lift it to a function on `ListF`. mapListF :: (a -&gt; e) -&gt; ListF a c -&gt; ListF e c mapListF _ Nil = Nil mapListF f (ConsF a c) = ConsF (f a) c Then there are some `hoist` functions to take care of traversals. The one for `Fix` can be defined with recursion-scheme's `cata`, and `Cofree` already defines one. hoistFix :: Functor f =&gt; (forall x. f x -&gt; g x) -&gt; Fix f -&gt; Fix g hoistFix n = cata (Fix . n) hoistCofree :: Functor f =&gt; (forall x. f x -&gt; g x) -&gt; Cofree f a -&gt; Cofree g a So mapping over `a` for `List` and `CFList` is given by `\f -&gt; hoistFix (mapListF f)` and `\f -&gt; hoistCofree (mapListF f)`. The more general pattern here is that both `Fix` and `Cofree` are functors, with the functor category on Hask as a common domain, but their codomains are the Hask category and the functor category respectively, which makes it difficult to define and use a single typeclass for them.
F* is a hybrid proof/verification langauge which can extract executable code to ocaml and F# www.fstar-lang.org
Not sure how helpful it will be without explanation and context, but here are a few for you: **Semigroup:** Combining bounding boxes on a 2D canvas. Concatenation of non-empty containers or data sources. **Monoid:** Concatenation of containers or data sources that might be empty. **Monad:** Algorithms that can naturally be described as a sequence of steps, such as: stateful calculations, calculations that can report an error, sequences of nested loops over a container, calculations that can have external side effects, non-context-free parsers. **Functor:** Anything where you can naturally apply a function inside. Just about everything mentioned here is an example. **Applicative:** Concurrent async actions. Any calculation in `Monad` where you can guarantee that no step depends on the result of a previous step. **Alternative:** Context-free parsers (Parsec, attoparsec, optparse-applicative,...). `Maybe` viewed as a calculation that provides a sequence of fallbacks if previous ones fail. **Arrow:** HXT
Even then it's almost never a good idea.
Since you really only care about the record, which always has kind `*`, I think your class can be even simpler: class Default a where def :: a For example: instance Default (ShowD Int) instance Default (ShowD [Char]) instance Default (FunctorD Maybe) instance Default (SetD ([Int], Int))
Predominantly applicative-ish things: - Concurrent actions. - Composable folds over streaming data. (You can give Monad instances to streaming fold datatypes, but AFAIK they must keep the whole history of the stream to work, so it kind of defeats the purpose.) - Validations that keep all errors instead of just the first one. - Nested bracketing for resource allocation (this is also monadic, but it's mostly used in an applicative-ish way). - Actions carrying some kind of annotation that can be inspected before being running the action. (Also a use-case for arrows.) - Also Applicatives are very composable, be it with products, sums, `Data.Functor.Compose` or `Data.Functor.Day`. Alternative-ish things: - Parsers - Concurrent actions that race each other. - Streams of data that can be interleaved. - Getting the first success out of a series of actions. Comonad-ish things: - Non-empty somethings, can-always-provide-a-value somethings. - Values that carry some kind of environment. - Composable folds over streaming data (again). 
Here are a few practical use-cases that I've seen and used first-hand: * Monoids: Just a fancy name for any data-structure that can be concatenated. And, it must have an "empty" value called, `mempty`. Eg. strings, lists, vectors, hashmaps, HTML, etc. * Arrow: Take a look at the QueryArr Opaleye library and various arrows in HXT. Although, TBH, I'm not sure how arrows are _really_ different from an unapplied function from `a -&gt; b`. * Functor: Is used by data-structures that want to allow the outside world to apply a function to their "contents", without bothering about **how** the contents are stored. The most astounding use-case for this is lenses. Which allows you to build uniform way to get/set data-structures with the help of such functors. * `Default Constant` -- discovered this via Opaleye. Use to define a standard/global way to convert a value of type `a` to a value of type `b`. Although using it is not very pretty, because it forces you to manually specify the type `b` via type-annotation. * ToHTML - via Lucid. Standard way to convert any data-structure to an HTML representation. * ToJSON - via Aeson. Standard way to convert any data-structure to a JSON representation. * FromJSON - via Aeson. Standard way to build any data-structure from its JSON representation. The most practical intuition that I've built for myself is this: * If you want to write a function that accepts different _values_ of the same _type_, write a regular function. * If you want to write a function that accepts values of different _types_, write a type-class along with instances. (hopefully the function behaves similarly across the different types)
Is partial application really a performance issue, given that (at least to my knowledge) partially applied functions are only evaluated when applied to all arguments? I am not being snarky, I'm genuinely asking. At any rate, I agree with you that I don't really see all that much utility in emulating the OOP paradigm, because I generally find that algebraic data types (at least in Haskell, I haven't really used them in other languages) are a much better substitute. I can't tell you the number of times I went back and forth on an OOP design in Java because I could never find a model of my problem that adequately represented all aspects of the problem. In Haskell this is rarely an issue, and when standard Haskell isn't enough, there are always language extensions for greater abstractive power.
Great article! Very similar to the Known typeclass in the [type-combinators][] library that I use often :) Known is also higher-kinded as well like this one is. [type-combinators]: http://hackage.haskell.org/package/type-combinators
Yeah. It has some ergonomics issues but I was genuinely surprised how Haskell-like it could be at times. 
Yea this is just due to cabal components. You could set a dir local for the test sources telling dante to use the test target to open the repl.
Thanks, I discussed this in a new section called "Other approaches". 
Thanks, I discussed this in a new section called "Other approaches".
&gt; Also, for people looking for more on the same subject: http://okmij.org/ftp/Haskell/TypeClass.html#Haskell1 Thanks, I discussed this in a new section called "Other approaches".
&gt; Very similar to the Known typeclass Interesting. For raising values known at compile time to the type level? &gt; Known is also higher-kinded I guess you mean poly-kinded?
These abstractions are a set of functions along with laws about how those functions relate. So they are "good for" anything that satisfies those constraints. These typeclasses don't "solve a problem". Rather, they provide a common interface for otherwise unrelated concepts. Having said that, here is what you asked for: *Monoid* * Lists * Integers (addition) * Integers (multiplication) * Maybe (pick the first Just) * Maybe (pick the last Just) * Booleans (conjunction) * Booleans (disjunction) * Configuration * Documents * Functions that return monoids * HTML *Functor* * Nondeterminism * Tuples * IO * Functions * Errors (no error info) * Errors (single error info) * Errors (accumulating errors) * State * Async computations * Environment passing * Phantom types * Transactions * Pure mutable computations * Functor sum * Functor product * Functor composition * Parsers * Backwards state * Time-varying values * Key-value maps * SQL queries * HTML *Applicative* * Nondeterminism * Tuples (with accumulator) * IO * Functions * Errors (no error info) * Errors (single error info) * Errors (accumulating errors) * State * Async computations * Environment passing * Transactions * Pure mutable computations * Functor sum * Functor product * Functor composition * Parsers * Backwards state * Time-varying values * SQL queries * HTML *Monad* * Nondeterminism * Tuples (with accumulator) * IO * Functions * Errors (no error info) * Errors (single error info) * State * Async computations * Environment passing * Transactions * Pure mutable computations * Parsers * Backwards state * SQL queries * HTML *Arrow* * Functions * Parsers * Validation * Monadic functions * SQL queries
"You couold have invented Monads" gives a few practical examples of Monads.
Although reflective elaboration is a real win in the tactics space, Idris is primarily about bringing the benefits of dependent types to practical programming. As a proof assistant, it is rather bare bones. Idris' JS output is often smaller and faster than GHCJS. But, the type checker can take a while.
We didn‚Äôt mean it to die, we‚Äôll be scheduling the next one soon. Without going into too much detail, I had a very stressful summer :-/
&gt; Is partial application really a performance issue, given that (at least to my knowledge) partially applied functions are only evaluated when applied to all arguments? Depending on how things inline, you may end up allocating closures (function + some parameters) on the heap. That not only increases memory footprint, but also causes more indirections resulting in more cache misses.
I see. If the impact was significant enough, one could control that using pragmas, no?
See for example the pet shop swagger description here: https://github.com/BigstickCarpet/swagger-server/blob/master/samples/sample1/PetStore.yaml#L14 These definitions are 100% the same as JSON Schema, the required fields, the enums, putting them under definitions, and using `$ref`. It's the same.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [BigstickCarpet/swagger-server/.../**PetStore.yaml#L14** (master ‚Üí ed3ec3d)](https://github.com/BigstickCarpet/swagger-server/blob/ed3ec3dfbaa7805965a64786169078594d461fda/samples/sample1/PetStore.yaml#L14) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
In some cases. Not in others.
Thank you so much! The hoist functions are exactly what I was looking for.
&gt; Is partial application really a performance issue I was talking in the context of an interface with multiples "methods". If you want to create a collection of items following one interface, for example, the `intersection :: Ray -&gt; Maybe Intersection` method, we can create a `[Ray -&gt; Maybe Intersection]` and fill it like: myListOfIntersectable = [intersectSphere aSphere, intersectCube aCube, intersectPlane aPlane] Then, if we want to compute the intersection on each element of the collection, you just need to `fmap (\f -&gt; f aRay) myList)`. This is memory efficient: if we ignore the gc overhead and stuffs, each partially applied function contains a pointer to its context (i.e: the shape) and a pointer to the unapplied function. That's no more than what we can get in, say, C++, using vtables. However this interface does not allow multiples methods. If we now want to implement the interface `pointInShape :: Point -&gt; Bool` which returns if a point is inside a shape, we need another list of preapplied functions: myListOfPointInShapable = [pointInShapeSphere aSphere, pointInShapeCube aCube, pointInShapePlane aPlane] We now have two lists, and in each list we have, for each item in the list, a pointer to the unapplied function and a pointer to the original shape. Actually, for N items, if we have M interfaces for O different implementations, we need M * 2 * N pointers. In the other hand, the classic approach to vtable in C++ (or the one depicted in the article, or using GADTs) will only introduces N * 2 + M * O pointers. And also, with this method, It is difficult to modify the inner object. For example, if I have a function `translate :: Direction -&gt; t -&gt; t` implemented for `Sphere`, `Cube` and `Plane`, I actually need a preapplied function with a recursive type: type Api = (Ray -&gt; Maybe Intersection, Point -&gt; Bool, Direction -&gt; Api) translate :: Direction -&gt; Api Where the preapplied `translate` function is able to generate the new entry point for each interface for the object. That's mostly painful to write, and leads to a lot of pointer duplication and a huge overhead. I give a detailled example here: https://gist.github.com/guibou/d1115c38b44155ebeb1cab933034b6f7 Of course, there is a lot of boilerplate which can be reduced using typeclass or template haskell.
higher, to contrast with the Default suggestion mentioned elsewhere in the comments :) and yes that is essentially the purpose. it might not mesh well with this here because of overlapping instances but it uses the same mechanics
`oneOf` should support sum types fine. I've encoded various ways of doing it with a tag field or with a nested object. Note that `allOf`, `anyOf` etc. allow much higher level of abstraction over sets of fields than what you can do in Haskell. The `oneOf` validator makes it an error to create two constructors that overlap. That's a practical invariant that's easily skipped otherwise. ``` { "Color1" : { "oneOf": [ { "const" : "Red" } , { "const": "Green" } , { "const": "Blue" } , { "properties" : { "tag": { "const" : "Custom"} , "red": { "$ref": "#/u8"} , "green": { "$ref": "#/u8"} , "blue": { "$ref": "#/u8"}}}]} , "Color2" : { "oneOf": [ { "enum" : [ "Red", "Green", "Blue" ] } , { "properties" : { "Custom" : { "$ref" "#/Custom" } } }] }} , "Color3" : { "oneOf": [ { "const" : "Red" } , { "const": "Green" } , { "const": "Blue" } , { "properties" : { "Custom" : { "$ref" "#/Custom" } } }] }} , "Color4" : { "oneOf": [ { "enum" : [ "Red", "Green", "Blue" ] } , { "properties" : { "allOf": [{"$ref": "#/Custom" ,{properties: { "tag": { "const": "Custom" }}}]}}]} , "Color5": { "properties" : { "tag" : { "enum": [ "Red, "Green", "Blue", "Custom"]} , "if" : { "properties" : { "tag" : { "const": "Custom"}}} , "then" : { "$ref": "#/Custom" }} , "Custom" : { "properties" : { "red": { "$ref": "#/u8"} , "green": { "$ref": "#/u8"} , "blue": { "$ref": "#/u8"}}} , "u8" : { "maximum": 256, "minimum": 0} } ```
How does it compare to persistent?
I like the contrast between your approach and the usual "build first, optimize later."
You should look at the (very good) Gtk2Hs tutorial : http://www.muitovar.com/gtk2hs/chap2.html. I don't have this library installed on my machine, so I can't really verify what I'm saying, but I think you should first create a window (with something like `windowNew`) and "attach" your dialog to it to have it rendered (and then call `widgetShowAll` to actually render it in your window). Anyway you should really look at the tutorial, I think it's great, and it shouldn't take you long before you can do what you are trying here :) 
Check out what the operator `&gt;&gt;=` does. It is central to the understanding of the Monad concept and of Haskell's `do` syntax. The latter is just syntactic sugar to write monadic code, especially of the `IO` monad, in a more imperative-looking manner. The operator `&gt;&gt;=` "binds" two values of a monad type together. What this actually means depends on the type of monad. For the `IO` monad this means that the first action is executed and the result is passed to the second operand. Regarding your example, your directory read function *must* return `IO [Filepath]` because there's no way to determine that list of filenames from the argument without doing I/O. On the other side, it *can* take an argument of type `IO Filepath` for the directory name, but it's not required because IMO this function should not care where this path comes from. To use the result in pure code, for example to sort the list, use something like the following: `do contents &lt;- readDirectory dirname; let sorted = sort contents; println (show sorted);` Haskell transforms this into this: `readDirectory dirname &gt;&gt;= (\contents -&gt; let sorted = sort contents in println (show sorted))`
Check out what the operator `&gt;&gt;=` does. It is central to the understanding of the Monad concept and of Haskell's `do` syntax. The latter is just syntactic sugar to write monadic code, especially of the `IO` monad, in a more imperative-looking manner. The operator `&gt;&gt;=` "binds" two values of a monad type together. What this actually means depends on the type of monad. For the `IO` monad this means that the first action is executed and the result is passed to the second operand. Regarding your example, your directory read function *must* return `IO [Filepath]` because there's no way to determine that list of filenames from the argument without doing I/O. On the other side, it *can* take an argument of type `IO Filepath` for the directory name, but it's not required because IMO this function should not care where this path comes from. To use the result in pure code, for example to sort the list, use something like the following: `do contents &lt;- readDirectory dirname; let sorted = sort contents; println (show sorted);` Haskell transforms this into this: `readDirectory dirname &gt;&gt;= (\contents -&gt; let sorted = sort contents in println (show sorted))`
To debug type errors, yow can instruct GHC to tell you what type an expression in your code has. Use a type cast and the type name `_`, called "hole", for that. Example: `let a = 'a' :: _` GHC will then generate a special kind of type error and tell you that it inferred `Char` for that hole.
That looks pretty solid to me.
I believe most of the GHC part is already done in nixpkgs. Look at https://github.com/reflex-frp/reflex-platform to see how Reflex uses it for building mobile apps. The Hackage package set still takes some tuning though, which is where these other Hackages cropping up would come in handy.
Slightly vague "how `IO` really works" question: So I get that when you start your program, `main` executes its pure computation, creating an elaborate set of nested lambdas and other values (mostly thunked) on the heap. Once it completes, this (the `IO ()`-typed value) all gets handed over to the runtime, and effectful stuff starts happening. Is there some way to "see" the state of your program at the point when `main` has completed, but the runtime hasn't yet started processing the `IO ()` value? eg are there RTS flags that tell you how long it took to get this far, how big the heap is at this moment, and how much GC has happened already? Clearly if the program is just searching for some massive prime and then does one `putStrLn` before exiting, then it's almost all before this point. But for something "normal" (eg a web app) where almost no code actually executes before `main` hands over to the effectful runtime, what's the right intuition for this moment? Does it take milliseconds, or what? And what's a good mental model for the overhead of having almost all the code referenced via lambdas on the heap? (eg I presume it doesn't copy all the instructions out from the text segment, for example, but just keeps some closure data and a pointer into those instructions?) 
 &gt; Semigroup: Combining bounding boxes on a 2D canvas. [God damn...](https://media.giphy.com/media/EldfH1VJdbrwY/giphy.gif)
I have a real job. I have to do enough of that day to day, so when I'm decompressing, I like to see what the other side is like. Building the "Maximum Viable Product" can be an incredibly freeing experience.
I find this account to be overly pedantic. Yes `main` has type `IO ()` and it's data and it can be put into a list or map or passed to functions and be manipulated, but his description that "[i]n most programming languages that we‚Äôve ever experienced, the world as we know it lives and dies in the confines of the opening and closing brace of the main function, which is the entry point to our application. Everything that happens in our program, the universe our program interacts with, happens between those two braces" is quite true in Haskell too. Bindings that aren't transitively mentioned from `main`? That's dead code. 
Ralf Laemmel and SPJ offered up class Sat a where dict :: a much along the same lines back in [Scrap Your Boilerplate with Class](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/gmap3.pdf)
I think your comment is more pedantic than the article. The difference isn't about what the program is doing, which is going to be sequential execution, but the mental model one applies in writing the program.
Good question. Look forward to seeing the answers.
Those are refinement types, and there's [a nifty little library](https://hackage.haskell.org/package/refined) for them. But as others have pointed out, they aren't very commonly used in practice. 
* At what stage does the crash occur (you can find that out by adding print statements between your lines) * What is the `return 0` for? iiuc it does nothing. Try to recreate the bug with a minimal example and that might help narrow the problem down. * What do you mean by GHC crashes? I understood the title as the compiler crashing during compilation time, but iiuc it is actually the resulting compiled program that crashes?
Here are some minor suggestions. A good golfing exercise to practice functional programming is to find ways of avoiding hand-written recursion, by composing more general-purpose functions (so you are using recursion indirectly). - `toPoints` (hint: `scanl`) - `validRec` `keyboardMouse` has some code duplication that you can avoid by adding a layer between raw inputs (`Char _`) and their handlers (`do ...`). 1. Map raw inputs to an intermediate type of commands (here, 4 directions) 2. Handle the commands
Thanks for the explanation.
Thanks!
Great article. I have the opposite issue. Most of my time studying Haskell, or computer science, or really anything at all, is spent trying to upend my own apple cart. I have a hard time understanding how someone can go 10 years without experiencing a fundamental shift in the way they view programming. It seems to happen to me fairly often. I guess the downside is that my productivity isn't as high as it should be because I spend so much time taking in new information.
I am very happy to hear that!
Great read. Believing you already know is the best way to guarantee both ignorance and boredom.
Yes, LLVM uses Phabricator. LLVM was Chris Lattner's first big OSS project started at a time, where there was no GitHub. Now, Chris Lattner's latest big open source project, the Swift compiler, uses GitHub.
&gt;Idris is primarily about bringing the benefits of dependent types to practical programming. As a proof assistant, it is rather bare bones. That's fair. I've still been able to do nontrivial things with it, which is more than I can say for Haskell. &gt; Idris' JS output is often smaller and faster than GHCJS. It's smaller because it GHCJS needs to bundle a runtime in order to run while targeting a strict language.
&gt;Monad Monads are good for [monadic parser combinators](http://www.cs.nott.ac.uk/~pszgmh/monparsing.pdf) and for [separating effectful/non-effectful code](http://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf). &gt;Monoid Monoids are basically "things that you can add together". &gt; Arrows I've only ever used arrows with tuples, honestly. &gt;Functor Too many to count! One of my favorite applications is that you can use functors to generalize syntax tree traversals and folds, though that is not easily accessible to beginners.
&gt; Just a fancy name for any data-structure that can be concatenated Not really, no. It's an *abstraction* and in particular it satisfies associativity. This also guarantees that things like `O(log n)` sums will work on a GPU. 
I'm guessing you are using `runhaskell` or `runghc`. This runs the code in an interpreter, and code run by the interpreter is not run in the main thread. One approach might be to have `main = runInBoundThread $ do`, to ensure it's on a bound thread. If that doesn't work, well, things should work if you compile it via `ghc` or by making a stack project and using `stack build`.
That is only "equivalent" when the DSL is a thin layer above IO.
To expand a bit : [here is an article](https://hbtvl.banquise.net/posts/2017-05-29-01-parametricmonad.html) where I wrote a standard interpreter for some IL, which could be done the way you wrote (except it is completely pure). By changing the underlying monad, I could turn it into a symbolic interpreter "for free". I don't think you can do that in C# with an embedded DSL.
If we're being pedantic, then IIUC, it doesn't _satisfy_ associativity, it _presumes_ it. Not clear about your GPU comment. Please elaborate. 
Watch this video : https://www.youtube.com/watch?v=ftcIcn8AmSY 
&gt;[**Four Solutions to a Trivial Problem - Guy Steele Jr. [61:47]**](http://youtu.be/ftcIcn8AmSY) &gt;&gt;Google Tech Talk, 12/1/2015, Presented by Guy L. Steele Jr. &gt; [*^GoogleTechTalks*](https://www.youtube.com/channel/UCtXKDgv1AVoG88PLl8nGXmw) ^in ^Science ^&amp; ^Technology &gt;*^21,083 ^views ^since ^Jan ^2016* [^bot ^info](/r/youtubefactsbot/wiki/index)
Oh wow! Thanks for liking the post :) I felt that I need to write it. Ever since starting my "journey" not too long ago, all I wanted to do is shout "CAN'T YOU SEE?!?" at people. It took me a while to realize just *why*, after so many attempts to give FP a try I was finally able to. I don't really know Haskell that well - I'm just starting out. I'm not even using it at work - we use Scala where I work. But now I have the foundations, the source of inspiration for all the functional libraries that are used in Scala, and I feel like a kid in a candy store! It's just so cool! I really think this is about conquering fear. Most people who dismiss Haskell as "academic", "math-y", "ivory-tower", "useless in the real-world". We know this is just undeserved slander but it's all true - Haskell *is* all of these things and *none* of them. Being this dismissive is human nature, a defense mechanism, very eloquently [described in a comic by The Oatmeal](http://theoatmeal.com/comics/believe). A fundamental shift is scary for most. I think Haskell (or, finally "letting go" of the fear) has cured my impostor syndrome. I am no longer afraid of not knowing everything. Whether or not that sounds crazy - I don't care :)
&gt; Instead, the program ‚Äúdescribes‚Äù in a declarative way what is going to have happen when your program executes, and this ‚Äúdescription‚Äù will be returned into the Haskell runtime, and exactly what was described will happen - with no possible side-effects or surprises. the part `with no possible side-effects or surprise` is confusing, let take an example, we describe in the main that we are reading from a file and doing some computation on the data in the file, however, we couldn't open the file for some reason. isn't this a runtime side effect or am i missing a point here?
There are things you can make that can inhabit your class that aren't function spaces, but they aren't satisfying. You really want to know that if you pick `x = ()` that this doesn't "change" f in any meaningful way, but you don't give the corresponding distributive law here. Otherwise campy cases like: instance Close [] where close _ = [] pass. Once you add the right law, there is a fancy high-falutin' reason why `Close` is `Distributive` -- function spaces and things isomorphic to them are the only things that preserve limits in Hask. Showing that is a bit non-trivial, but has to do with the fact that right adjoints preserve limits and there is only one covariant functor that is a right adjoint from Hask -&gt; Hask: the `(-&gt;) x` functor, all other such cases are isomorphic to it. Operationally you can look at it as if 'f' isn't just a function space you can't commute it past the (x -&gt;), without having to make up the other bits. If you have sums you have to pick a case blindly. If you have products with other stuff you have to fill in defaults. Neither one of those bits and pieces can pass the laws one would really want this class to have.
Hello, author here! Of course you're right -- but what I wrote served a different purpose. My post was aimed for people who don't give a shit about FP or Haskell (which are most people). I was deliberately vague (and probably even wrong), because I wanted to show a different "world view" (paradigm, by Kuhn's definition). This isn't (and almost never is) about syntax.
I‚Äôve re-read that several times, trying to make sense of that too. The way it is worded implies no side-effects during execution. But unless the Haskell ‚Äúdescription‚Äù deals with exceptions etc. there is still a possibility of side-effects... I think the author intended is to say that the description itself will not contain side-effects or surprises. The benefit being a huge reduction in potential bugs before even getting to the runtime. Nevertheless it is a great article and does an brilliant job at pointing out some of the reasons why Haskell is special.
Thanks! I was indeed trying to fit an elevator pitch about Haskell into a blog post, that was intended for people who generally dgaf about Haskell :) I'm pretty sure I'm wrong on so many technical levels, but this kind of reasoning helped me develop the intuition to "get it", why Haskell was so different (and all the benefits you get from that, mainly, referential transparency and equational reasoning. There's no way to get that in almost any "mainstream" programming language - and nobody knows about this!)
Hi! Thanks for writing this. I‚Äôm just a bit worried those kind of articles will confuse newcomers further because for newcomers syntax plays a much bigger role because the first thing you learn about a language is its syntax. An intuitive grasp of the semantics only come later. IMHO, it is probably better to first teach that the do notation inside `main` is just like the braces in C/C++/Java/C# and the program sequentially executes the statements inside it. This is also not exactly right but really IMHO it helps newcomers better because it maps a familiar concept to a new syntax. 
&gt; If we're being pedantic, then IIUC, it doesn't satisfy associativity, it presumes it. If we're being pedantic, that is absolutely wrong. 
&gt; I'm looking to enumerate the toolkits that people have in terms of using type classes to solve understandable problems, where "understandable" is defined in terms that programmers who do not understand functional programming might recognise. That will naturally result in some simplification! I could absolutely say concatenation is an example of a monoid but I think the other reply is absolutely correct here. There is no metonymy for monoids. Rather, in FP, we structure our programs to take advantage of these lawful classes. Though we often fail, monoids are a very nice of example of this sort of programming. 
Compiling different modules using independent ghc processes can save a bunch of time. For our modest codebase of 500 modules the difference is about 10-20%, with more opportunities for concurrency this will only be faster. Overhead on GC is big...
Can you still write in C#, though? Or do you, like me, feel nothing but frustration when you have to write your seventeenth for-loop or tree traversal where an fmap would suffice, or when you have to type five to ten lines of code each time you want to compose or curry something? When I write C# or Python, I feel like I'm the compiler and that I'm wasting time and effort. How do you guys get over that?
The examples in that comic are pretty much lost on me... I'd like a german version. Roe vs Wade? No idea what it's about. George Washington and slave teeh? Sure, why not.
Sorry, replied in haste earlier and misread your comment. Known is probably actually more accurately described as being used to bring types to the value level, not values to types. It's similar to the SingI typeclass in singletons and KnownNat/KnownSym in base
Any .net integration is much appreciated. The only functional language we have right now is F#.
I agree with you, it is inspiring
thanks for your help!
I don't think you understood his point. With Haskell, you build a data structure then put it into main so it gets executed. With imperative programming languages you start in main and it is calling other functions and routines during the life of the program.
Then again, I've learned how to use whole C# libraries just through the autocomplete dialog. Without even opening the docs. You trade one frustration for another.
Also related: class c =&gt; Static c where closureDict :: Closure (Dict c) See the [distributed-closure](http://hackage.haskell.org/package/distributed-closure-0.3.4.0/docs/Control-Distributed-Closure.html#t:Static) package or [my recent talk](https://skillsmatter.com/skillscasts/10632-static-pointers-closures-and-polymorphism) on the subject.
&gt; So I get that when you start your program, main executes its pure computation, creating an elaborate set of nested lambdas and other values (mostly thunked) on the heap. Once it completes, this (the IO ()-typed value) all gets handed over to the runtime, and effectful stuff starts happening. This is a good model in that it lets you imagine how effects can be separated from the pure evaluation. However, it's not an accurate model of what happens operationally. There are several reasons for that: 1. It would violate laziness. In your massive prime example, how would the program know it actually needs to compute the prime until it reaches putStrLn? 2. In the code produced by ghc, there is no distinction between the pure and impure code. Constructing some abstract representation of an IO action and then executing it would be inefficient; so your program simply executes the action when it "sees" it. So, the 0th approximation to how ghc-compiled program works is just executing all instructions, pure and impure, top-down and in the order dictated by laziness and the ordering between IO actions. A better approximation would be to think of IO as a lambda `RealWorld -&gt; (a, RealWorld)` (ie. the State monad). If you know how Haskell evaluates pure functions, you know how it evaluates IO actions. In particular, some optimization aside, no evaluation happens *inside* lambdas, before the argument is supplied. Applying this to the IO type, nothing will happen inside `main` until the `RealWorld` token is supplied ‚Äî i.e. until the impure part is given permission to start.
&gt; With Haskell, you build a data structure then put it into main so it gets executed. With imperative programming languages you start in main and it is calling other functions and routines during the life of the program. I think /u/kccqzy's point is that imperative languages can *also* be seen as "building a datastructure and then putting into main so it gets executed" and Haskell can *also* be seen as "starting in main and calling other functions and routines during the life of the program". It just so happens that the latter interpretation is easier to come to in imperative languages and the former is easier to come to in Haskell. But they're both equally valid. cf. http://conal.net/blog/posts/the-c-language-is-purely-functional 
That is true but you can do so much with a thin layer above IO. For example, you could make Interpreter a monoidal writer with a "reading list" passed in for the reader, which might be handy for a unit test scenario.
Care to elaborate what that defect is, or point to a Trac number?
I jumped ship and now write Scala for a living. I haven't touched C# (or the .NET ecosystem) in over a year :)
&gt;Good luck. Not only is your question vague, but it sounds like you're asking for help on a homework assignment and are too lazy to do any research. Make it clear that you're not just getting someone else to do your work, and are really deserving of assistance. I will also add: we're human and we like interesting problems! The more you can convince maintainers that your bug is the "real deal", the more interested they'll be. Xavier Leroy's post on [finding a bug in Skylake](http://gallium.inria.fr/blog/intel-skylake-bug/) is a particularly neat example of this I think. &gt;If I see people answering questions on a mailing list or Stack Overflow, I'm appreciative. I consider them a comrade-in-arms. And I consider them assets to the community. They've earned my respect, I'm indebted to them, and I want to entice them to continue. I'll also add: there's such a thing as being a good student! I'm good at Haskell; my software/computer knowledge has some pretty wide gaps. I try not to condescend and assume I know more about CPU registers than the person I'm helping. That's part of being a good helper. But it really grinds my gears when someone barges in asking for an analogy so that they can understand monads in 15 seconds and then dismisses anyone says that's impossible as pretentious (this is hyperbole). I *do* have expertise and experience with FP, which is supposedly why you asked me a question in the first place!
There's good psychological foundation for this: [intelligence directly correlates with long-term cooperative behavior](https://www2.warwick.ac.uk/fac/soc/economics/staff/eproto/workingpapers/rpd_iq.pdf). The more you contribute and cooperate with others, the more other intelligent players will cooperate with you. It's a nice bit of serendipity in the minefield of human cognition.
Sounds like a nice project -- I have thought about similar things in the past and I think it's an interesting design space to explore. One thing that is not clear to me is: how do you plan to have an efficient precise GC if your memory representation does not let you distinguish data from addresses? You could integrate the notion of addressiness to the runtime type representation, but I would be worried about losing a lot of the sharing that is currently possible if specializing.
Pinging /u/haskman -- does Concur alleviate this problem?
(Note, I'm not the one who made it, I found it on my twitter feed and thought the concept was interesting.)
Yes, I should have clarified it was an author-you rather than submitter-you; eventually they'll find the thread :-) (Can you ping them?)
I don't know the reddit account of the author, so I can't ping them. It doesn't seem like they use the same account name on github and on reddit.
Well, semigroups are basically everywhere where you can't find a proper identity element to the monoid you think you spotted. For bounding boxes, I used to have an identity element defined as `{ min: V3.max, max: V3.min }`, which shouldn't exist by the definition of 'bounding box' (e.g. where the invariant `bb.min &lt;= bb.max` holds). So, bounding boxes actually seem to be a sub semigroup of a more general min/max-lattice.
&gt; Most people who dismiss Haskell as "academic", "math-y", "ivory-tower", "useless in the real-world" Ironically, this is exactly what prompted me to investigate LYAH in the first place. "There's an Ivory Tower? Can I come in?"
You can see some uses of semigroup and monoid here http://tech.frontrowed.com/2017/09/22/aggregations/
Paging /u/ollepolle.
GHC's binary distribution scrip hasn't been designed with cross compilers (host != target) in mind. As such it tries to adapt to the system it's being configured and installed on as good as it can. This however also means that it starts recomputing build/host/target, which will usually not be a big issue as they are all the same and will be almost identical to the one the compiler has been built for. It will also try to recompute your linker, archiver, ... (e.g. adapt to the system it's being installed on). Now for cross compilers, we know pretty well the exact toolchain we compile for, and we want to retain this toolchain as good as we can. So this "defect" in the current configure script is what makes passing --target/--build/--host necessary. This should be resolved with the next set of binary distributions, later this week ;-)
Yes. This is kind of the situation I want to avoid, where you end up tying yourself to nix to get all the details and configuration knobs just right. I want these to *just* work, and then the nix expression would be trivial as well. However if someone wants to contribute a nix expression, I'd happily accept a PR against the package-overlay. &gt; these other Hackages These are not really *other* hackages. The overlays are essentially just hackage + a set of patches for certain packages. The idea is that those overlays will converge against empty sets, by having the patches upstreamed. Of course there might situations arise where upstreaming is not an option. I hope to write some more on this soon!
I am mostly ignorant of how all this works, but couldn't the `Ptr` type have some special interaction with the GC?
Yes! Thanks for the ping! That's exactly the problem [Concur](https://github.com/ajnsit/concur) aims to solve. I wanted to get away from framework-itis and towards what made functional programming fun for me. Concur is like a widget combinator library. Widgets are completely self contained units. There are widget combinators for siblings as well as parent-child relationships. Absolutely no micromanagement or boilerplate needed. The effect system is orthogonal to widget composition and does not require a contrived "architecture". Any widget can perform IO or STM actions at any point of time without boilerplate. Concur performs inversion of control, so "messaging" from child to parent is simple as handling the return value from the child. Messaging from parent to child is simply passing arguments to the child. This is how functional programming should be. You are free to use any FP tricks, like typeclasses or monad transformers, to simplify this argument passing. So if something needs to be accessible by all children, slap on a Reader/State monad. Most concur combinators are abstracted so you can use transformed widgets just like normal widgets. Also, if you really need it, it's easy to send messages to arbitrary parts of the page with IO/STM. So for example, widgets can push to an STMChannel, and a completely separate part of the page can wait for messages on that channel. But usually you won't need to do this, just like you work around the need for global variables in functional programming. 
Here's a [paper](https://www.cs.cmu.edu/~rwh/papers/til/pldi96.pdf) for a very similar system. We indeed need to integrate addressiness to type representations. 
Well the Nix in reflex-platform for cross compiling *is* remarkably simple. It‚Äôs not turning hardly any knobs. The hard part was nixpkgs, and it wasn‚Äôt even GHC. The stuff for GHC was also fairly straightforward IIRC. The hard part was just that nixpkgs was **not** originally designed with cross compilation in mind, so it took a lot of infrastructure work just to make it possible for *C* projects. Anyway the main point of my comment was just that the Nix already exists, not that GHC couldn‚Äôt use any more work to simplify things.
This is really nice! Thanks a lot for the link.
Yeah, I expect simplification; I think what I'm looking for is a breadth-first understanding such that I can think "I need to X so I should look for (or write) an instance of typeclass Y", even if my understanding isn't deep enough to do it right away, I have a way to search around more meaningfully than my current level of fumbling :)
The question is: how does the GC know what is of Ptr type? The GC runs at runtime, so you need to have enough data at runtime to distinguish pointers from non-pointers, either attached to values themselves (OCaml does this by tagging integers). Because a polymorphic variable may be instantiated with a `Ptr` type, you can't know statically where pointers are and include the data in the generated code, so it needs to be flown with the data itself, either as part of the data representation or as a separate argument.
Yes, I wanted to mention that this was fairly similar to the TIL work. However, note that addressiness is sensibly more costly than size to carry around, especially if you expect to have the information for the full recursive structure of words accessible from the value. I would be worried that any performance result in absence of addressiness support would not hold once you add this, and thus that evaluation of compilation strategies could be premature. (I guess this is the right moment to mention Rapha√´l Proust's PhD thesis that was going in sort of the same direction: [ASAP: As Static As Possible memory management](http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-908.html), Rapha√´l Proust, 2017.)
Examples are good, but take a page out of mathematicians' book and look for non-examples as well. I.e. where are there things that you *wish* were monoids but aren't. 
Thanks for pointing out the history! I certainly wasn't fully aware of it. I think we are on the same page here ;-) And as I said, if someone wants to provide the nix expression, I'd be happy to accept a PR.
What you describe sounds exactly like Reflex.
I cannot speak to Halogen, but Reflex certainly fits your description. In Reflex you have a lot more flexibility with how you architect your code. However, that comes at the cost of having to figure out exactly how you want to architect your code. You can talk about communication at a very granular level and you can close over state without leaking it to parent widgets at all. One nice thing that Elm arch gives you is an implicit "Writer" monad that collects up all the possible events in your template. Reflex actually has this as well, if/when you need it, in the form of `EventWriter` and `DynamicWriter`.
I have managed to get the following somewhat working... can someone confirm that this is indeed the right magic? ghc-options: "*": -j -rtsopts -with-rtsopts "-A128m" If it _is_ the correct incantentation - why is `-rtsopts -with-rtsopts` needed and why does ghc complain about `-n2m`?
My own intuition on specialization issues is that if we accept Rust monomorphization, then we should accept the equally-or-less pervasive TIL monomorphization as well. Also, mandatory runtime polymorphism is rare in production code and unlikely to be bottleneck in performance, based on what I subjectively perceive in Haskell. Maybe you could explain your intuition here? I personally really like the idea of dependent intensional polymorphism, in which setting rich runtime type codes can't be avoided even without GC, because type codes must include functions computing runtime type dependencies, e. g. for pi and sigma.
Could the project eventually feature a language with more interesting type-system features? Chiefly, I'm interested in linear/affine types or dependent types, (which are orthogonal to each other, I guess), either of which I'd find nice to have in a Haskell-like language. Also, I followed a link and ended up at the [Shimmer repo](https://github.com/DDCSF/shimmer): &gt; Bumped variables are used to avoid name capture, for example in the expression \x. \x. \x. x x^2 the variable occurrence x refers to the inner most parameter, while x^2 refers to the outermost one. This is interesting: a sort of de Bruijn hybrid, if you will.
Ahhh - thank you! Just what I needed explaining.
&gt;seventeenth for-loop or tree traversal where an fmap would suffice Often you can use Linq. It's not the same but at least you don't need for-loops as much.
I would love a keter-like reloader for web apps based on this.
An old coworker of mine, Eli Gottlieb, had a similar system https://code.google.com/archive/p/decac/wikis/SumTypes.wiki but it was a lot less finished. =) I've long wanted a variant on this where instead of Maybe A taking the greatest of the size of Nothing and Just, it still allocated the smaller size, but used the type information to reconstruct the size of 'A' to get past it to the next argument. For fixed sized things this could be worked out at compile time, for variable sized things you might have to look at it to figure out a tag, or should pass in the size outside of the structure. If you then combined tags until they filled a word you could get a rather dense encoding for Maybe (Maybe a), and a version of List could actually unroll internally in the data constructor until it was a flat array with a hideous tag in front of it, if you wanted to. The trick would be storing _enough_ size information to do the reconstruction of any position in the structure in O(1) to access the ith component of any constructor.
No, you are not alone with this. Before implementing them, I use pen and paper to write my thoughts and ideas down, in a mixture of mathematics and pseudo Haskell. On bad days I have to maintain Excel VBA code written by non-programmers and this is the only way to make it bearable.
Strict? or Lazy?
If I understand correctly, the variant you describe wouldn't be able to have pervasive laziness, right? Although now that I think about, I don't think pervasive laziness is possible in any language where everything is unpacked.
Well, one big difference is that Reflex is FRP based and Concur is not.
So `Succ $ Succ $ Succ $ Succ $ Zero` will be represented as `11110` in memory? The implementation of `Vector` seems pretty wasteful, carrying `n*Nat` around, where single `Nat` or `Int` would suffice. 
I also work in scala but I am constantly fighting it. It seems like a poor replacement for Haskell. Are you using `scalaz` or other functional libraries?
You could go for a mtl-style interface: class Monad m =&gt; PurchaseOrderMonad m where newOrderId :: m OrderId getPOAmount :: OrderId -&gt; m POAmount You then parametrise your functions with the constraint `PurchaseOrderMonad m =&gt; ...` and can provide different implementations for testing on your machine (a `State` monad for instance) vs. running it with a persistent DB.
I'm using Attoparsec to make a program to handle ID3 tags (used for identifying information in mp3 files basically). Two things would be helpful. 1. Would anyone mind taking a look at this code for ID3v1 and seeing if it looks good/idiomatic data ID3v1 = ID3v1 { title :: C.ByteString, artist :: C.ByteString, album :: C.ByteString, year :: C.ByteString, comment :: C.ByteString, track :: Maybe Word8, -- ID3v1.1 adds track field genre :: Word8 } deriving Eq --TODO add support for ID3v1.2 which adds extended tag id3v1 :: Parser ID3v1 id3v1 = do string "TAG" t &lt;- take 30 a &lt;- take 30 b &lt;- take 30 y &lt;- take 4 c &lt;- take 28 t1 &lt;- anyWord8 t2 &lt;- anyWord8 g &lt;- anyWord8 endOfInput let notrack = ID3v1 t a b y (B.append c (B.pack [t1, t2])) Nothing g let yestrack = ID3v1 t a b y c (Just t2) g return $ case (t1,t2) of (0,0) -&gt; notrack (0,_) -&gt; yestrack (_,_) -&gt; notrack 2. How do I implement the more complicated parsing for ID3v2 tags? The biggest issue is the need for some sort of state because there are flags in the ID3v2 header that change the parsing of the entire rest of the tag. Obviously the idea of a State Monad comes to mind, but how do I integrate that with attoparsec or do I need to use another parsing library?
&gt; it's far more useable than Coq or Agda, Could you elaborate on that?
&gt; Theory: Coq isn't Turing-complete, so there are some programs it can't express. General Recursion is an effect just like IO. You can very well use a monad to *express* all the programs you're interested in and use an escape-hatch to *run* them.
Most likely strict. I thought about how you could implement lazy fields. But it actually doesn't make much sense if you want to avoid pointers in the first place and therefore lack sharing. Without mutability you even have to add another primitive (e.g. `Lazy Int`) to implement proper sharing of already computed results.
Thanks! This is a good question that doesn't seem to have an obvious answer. Currently, Sixten uses a conservative GC, but I'm planning to switch over to a precise one. You've already touched on this in a sibling comment, but one way to do it would be to store pointer information in the type representations that get passed around. A problem with this approach is that operations on type representations (say, forming the product of a type A and a type B) become expensive compared to just tracking sizes. Indeed, I had exactly this problem when I tried to add alignment information to type representations. Calculating the type representation for `Vector n a` became linear in `n` instead of optimised (by LLVM) to the constant-time `n * a`. For this reason I resorted to using sizes that are multiples of the word-size, obviating the need for alignment tracking. For the same reason I'm hesitant to store pointer information in type representations. So, currently I'm looking at either pointer tagging or the use of a separate bitfield to keep track of pointers. But I'd also be open for new ideas! :)
Strict.
This is actually another point for monomorphisation, as you're then free to non-compositionally (or at least non-obviously-compositionally) choose type representations, e.g. to use `null` for `Nothing` in `Maybe (Ptr a)` and otherwise represent it like a `Ptr a`. This isn't exactly what you're proposing, but one feature that can save space for `Nothing` or `Nil` is to add a notion of boxed types: https://github.com/ollef/sixten/issues/38 Otherwise it appears to me that it's important that operations on type representations are cheap, at least if you're gonna do vectors like in the README (see the part in my reply to gasche about alignment).
For the frontend support the [language server protocol](https://langserver.org/) could be the best option.
No, it's not currently able to handle sub-wordsize-aligned data. `Nat`s are however special-cased and desugared to 64-bit ints. It should really be bigints but I don't currently have any. How do you figure it carries `n * Nat` around? `Vector n a` is represented as an `n-tuple` of `a`s, and since tuples happen to be flat, that means that it's represented as a contiguous chunk of memory of unpacked `a`s with no `Nat`s in sight.
Yep, that's [the plan](https://github.com/ollef/sixten/issues/13). :)
Not. At. All. 1. Reflex is not a widget combinator library. It does not provide a Widget centric view. You do not combine widgets to create new widgets, you combine events/behaviours to create widgets. To do something at the top level, based on the input from a widget deep in the stack, you sometimes have to do very hairy things to extract the corresponding event/behaviour/dynamic out. 2. That is a symptom of the core problem with Reflex - While it provides a Monad, it provides the wrong Monad. I don't need a Monad to lay out DOM elements on a page. DOM is inherently Monoidal, and composing DOM elements to sit side by side on a page should be a `mappend`, not a `bind`. But this Monad pervades Reflex, and necessitates a lot of the juggling with `MonadFix`. To put it another way, Reflex makes it easier for you to extract an event from a Widget which is rendered earlier in the page, and use it to control a widget which is rendered later in the page. But what we really really need in non-trivial apps is to extract an event from a Widget which is shown earlier **in time** and then use that to show the same or a different widget **later in time**. Concur provides that Monad. This actually makes Monadic bind useful again. In Reflex, to actually sequence Widgets in time, and for any application logic, you need to use FRP, which is completely unnatural (subjective, but it's never stopped feeling alien to me). FRP also means that you can't use any usual FP tricks like State Monad etc. Reflex relies on FRP completely, including effects. Want to make an ajax call? Create an event which describes all the times you would potentially want to make the ajax call. It's fascinating, but gets tiring after a while. Inversion of control, as provided by Concur, is a much better way to go about it. 3. Binding existing JS libs in Reflex is not easy, because you have to provide an FRP interface. 4. Reflex is supposed to allow you to precisely control changes you want to make to a page, (as opposed to a dom-diffing algorithm). This I believe is too much power, and too much bookkeeping. I once had to choose between keeping a modal around and mutating the contents, vs recreating it from scratch everytime it's shown. I **can** even choose to re-render the entire app on every change by using a top level dynamic widget and a pure view rendering function. It will make my life as a programmer easier (and bring me closer to Elm/React-ish architecture), but will make the end user's experience much worse. With Concur, the easiest way is the right way and the performant way. And it's not necessarily just with a virtual-dom backend. I am currently experimenting with a [backend](https://github.com/ajnsit/concur/blob/master/concur-doom) that will allow making explicit granular changes to the dom without dom-diffing. One day I will get around to finishing it. So while Reflex is also more of a library than a framework, I found it less than satisfying (having built large apps with it). I wanted something midway in power between Elm and Reflex which led to Concur. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ajnsit/concur/.../**concur-doom** (master ‚Üí b68f9cd)](https://github.com/ajnsit/concur/blob/b68f9cd414bdaf66cdb4a5abe352438f19b2030d/concur-doom) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I am sorry if this question sound redundant since the guys mentioned this before( in the last question) but it seems I got confused. now lets me explain what I have understood. in this case we assume that we have some function to get data from the database - lets call this wishful thinking as SICP does - then in the use cases or the application core I can use those function to continue the logic and the function in the use case will be parametrize by the `purchaseOrderMonad` later on the functions details could be implemented in for any specific database. but what about the state monad? could you elaborate more please 
Oh, I thought that `Pair` is a Constructor, not a type. It makes sense now.
My 2 cents. Invest this time (ie composing long replies on Reddit) in fixing concur docs and giving it a bit of marketing spin. The only reason I haven't bothered to look into Concur deeply, is because of the lack of motivating docs. 
If you want to run some tests without having to first pick a DB, initialise it, authenticate, etc. you could have an instance that goes something like this: newtype FakeDB = FakeDB { latestID :: OrderId , content :: Map OrderId PO } instance PurchaseOrderMonad (State FakeDB) where newOrderId = do oi &lt;- latesId &lt;$&gt; get modify (\ s -&gt; s { latestID = oi + 1 } return oi getPOAmount oi = poAmount . lookup oi . content &lt;$&gt; get (assuming `poAmount :: PO -&gt; POAmount`) You can now run `PurchaseOrderMonad m`-parametrised code in ghci by using this lightweight implementation which could be useful for debugging some of the interactive code.
It would almost-necessarily be a strict language.
&gt; After creating the PO I should create the orderid which should be unique so I have to check with the DB for the latest orderid before I render the new one. I'm not quite sure I agree. `AUTOINCREMENT` and `RETURNING` means I don't have to know anything about the current IDs before I create a new row. &gt; regarding the invoicing, I have to check that invoice amount is appropriate for the related PO so I have retrieve the PO amount from the database and check whether the invoice amount is valid. That makes some sense, although a `CHECK` might work here as well. --- In any case, depending on a persistence API or even a database API (HDBC, JDBC, etc.) is fine. Just don't depend on the specific database (MariaDB or PostgreSQL).
What's your take on Concur vs Reflex, /u/ryantrinkle?
Curious - could this be implemented in Purescript? Or does it rely on the lazyness provided by GHCJS?
I am interested in such a thing, and have a few suggestions / feature requests: 1. Please make it possible to interface with PulseAudio, which uses DBus in a very strange way where it has an endpoint on the session bus that returns a DBus socket address that you can connect to. The current `dbus` library has this property, but I've seen libraries that fail to expose an API capable of connecting to an arbitrary DBus socket address (IIRC Rust's dbus library has this problem). 2. Please expose a lensy (or lens-compatible) interface for this library. I was very frustrated with the amount of Haskell record nonsense I had to deal with when using `dbus`. I even [started](https://gist.github.com/taktoa/f57ee10f7150ab33d60ba848bb9a6b1d) on writing some lenses for it, but did not finish. Haskell records are not really compatible with abstract data types, since exposing a subset of the records will result in extremely confusing Haddock documentation, where the record fields just look like functions. 
This code looks good. Perhaps the last case expression could be split in two instead of three: if t1 == 0 &amp;&amp; t2 != 0 then yestrack else notrack I don't know anything about ID3v2 but I guess the monadic interface of attoparsec (that you are already using) is going to be sufficient, you might not even need `StateT`. It would be useful if the state evolves while parsing. id3v2 :: Parser ID3v2 id3v2 = do h &lt;- id3v2Header id3v2WithHeader h id3v2WithHeader :: ID3v2Header -&gt; Parser ID3v2 id3v2WithHeader h = do if someFlag h then parse it like this else parse it like that 
&gt; In fact, I find myself questioning the state of GHC itself nowadays :) What are the major issues with GHC? What's a good point of comparison on a language/compiler that does it better?
It could certainly be implemented in Purescript, I don't think the lack of laziness is an issue. In fact I have been meaning to port it over, as a way to get more familiar with Purescript, but the todo list is already quite long..
Yeah, I just fear that slick docs will signal to people that this is production ready when it's not. But I guess it's time to bite the bullet if I want people to use it and provide feedback.
While dependent and linear types _are_ orthogonal features, there are type theories in which they can interact (e.g.: in which there is a linear Œ†-type). See [this](https://bentnib.org/quantitative-type-theory.html) and [this](https://link.springer.com/chapter/10.1007%2F978-3-319-30936-1_12), which have apparently been added to Idris's type system.
This seems to be almost exactly the same advice as the classic ESR post that has been around forever: http://www.catb.org/esr/faqs/smart-questions.html