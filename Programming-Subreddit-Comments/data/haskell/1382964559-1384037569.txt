I've recently been trying to use Nix to install all my Haskell packages. So far, it has been working very well. While what vagif says is true, not many people have time to maintain both cabal and various packages for many distros. With Nix it is easy to make a local branch of nixpkgs and fix up any package problems I encounter (which have been none so far). I don't think that would work out so well with Debian :). I can even develop/install packages I make locally with a relatively simple Nix expression: let nixpkgs = import &lt;nixpkgs&gt; {}; hp = nixpkgs.haskellPackages; in hp.cabal.mkDerivation (self: { pname = "lens-family-core"; version = "1.0.0"; src = ./dist/lens-family-core-1.0.0.tar.gz; buildDepends = [ hp.transformers ]; }) 
Not as the author of the critique, but in general agreement with it, I've usually found that the Haskell community gains a lot by being more professional in the community behavior it encourages. I think that regardless of the social context, puns targeting "inappropriate" subjects are a the kind of cheap laugh that many of us strive to minimize so as to keep the focus on intelligent debate. /r/askscience has a similar policy enforced far more strictly and I felt it was the only way that subreddit was able to survive the brunt of being a default subreddit while maintaining its mission of high information value. On a subsequent note, "whore" being an insult with disparate impact on men and women may indeed move this community away from being as welcoming as we'd genuinely like it to be. I won't assume anything here, but instead just leave it as more justification for why your joke might have gotten downvoted.
Applicative notation? I seem to see 'applicative' and `&lt;$&gt;` appear together. Applicative functors, maybe?
Yeah, I will write it up into a post.
No question, cabal. I don't use system package managers at all for Haskell projects any more. And I don't even use the Haskell Platform. I just install GHC and use cabal. In my mind, the only reason to use the Haskell Platform is to bootstrap a cabal binary. I suppose system package managers might be good for that too. I don't use the Haskell Platform because it (on mac at least) installs more stuff as --global and I want to minimize that. If I'm having "cabal hell" build problems, then my solution of last resort is to remove all my --user packages and start over with just the stuff in --global. But if the Haskell Platform installs more packages in --global, then that will be less likely to work. I know this approach may not be "politically correct", but it is what seems to work best for me.
If you take the tupling approach, it's harder to write functions that don't care about the extra data and just work on the `Monster`. They'd either have to both accept a `Monster` and a `(Monster, a)`, or you'd have to tuple up the simpleMonster with a `()` before passing it in.
I don't think that having a polymorphic field is such a nice idea. In which kind of data structure you're gonna hold all monsters? For every 'a' a separate list? If a user adds a monster with a new 'a', where should he put it? 
I tend to put them wherever I define my datatype. Whilst this introduces a dependency on QuickCheck, I feel like it's worth it since it allows users of my library to use arbitrary values in their own testing, and e.g. when using `elements` to pick some constructor value, it's easier if the arbitrary definition is close to the datatype, so it's updated accordingly whenever the set of constructors is changed.
Package manager. I use gentoo; hackport is awesome
You just write functions that only accept the data they need. You would be responsible for plumbing the right fields to them.
I see, thanks!
Thanks for linking to the SHE site - not because I think I'll use it, but for the fantastic writing. "You'll find that its error support is fantastic, if you need help making errors."
I prefer to define them in the same module as the datatype, but I'm not sure that it is good for a library to be built against a specific version of QuickCheck. Has anyone had an experience that convinced them to keep their `Arbitrary` instances separate from the main source? Many libraries seem to define them separately.
As you know, whenever you find the cycles to improve the design I'm sure your proposals would be welcome :-)
isAnagram string1 string2 = (sort string1) == (sort string2)
&gt; Those of you who have done java before will groan: basically same principle. Maybe something in the air of the 90s. Namespaces will be placed in a directory structure that corresponds to the name. By the way, if this is something that actually does bother you in a meaningful way, it's something you can definitely help to change! That's not part of the spec, so I suspect introducing code to do something more principled will be welcomed by GHC HQ.
It's probably best if you come up with an informal algorithm description, and then we can help with the Haskell part.
And you gave none of the caveats that I think you should have given when citing that paper. I view this as irresponsible. It's an unfortunate habit the Haskell community seems to have inherited from academia, where people routinely make the most tangential, half-baked / half-relevant associations and references to work, and this is viewed as some sort of virtue. Think about it - the OP comes up with what he/she thinks might be an original idea. They're wondering if anyone's come up with the idea before, or if it's been explored, etc. They decide to 'ask the experts' on the Haskell reddit. You come along and refer them to Data Types a la Carte. You speak in an authoritative tone - "Have a look at Wouter Swierstra's paper...". The OP is going to be inclined to now go read this paper, and I can pretty much guarantee they're going to be very disappointed. IMO, you have wasted this person's time. I think you know that, and honestly, you just didn't think much about it. If you really want to cite Data Types a la Carte, consider your audience and what they're trying to achieve, and place the citation in its proper context. You could maybe say something like "There's a classic paper, Data Types a la Carte, which deals with some similar concerns. The details are quite different, and your approach is much lighter syntactically, so I don't think you'll find it directly useful, but you might find it interesting nonetheless."
You can do it very fast probabilistically. Map each letter to an int and add them up. Do it with more than one map to reduce the chance of a false positive match.
You can still get rid of the `let` and `if`. You want `map toUpper (sort str)`. What Haskell compiler are you using that gives you that format of error report? I know it's not GHC!
I was thinking of inserting the elements into a dictionary, which amounts to the same thing given the finite size of the key space. However your idea of radix sort will probably be much faster.
&gt; For end user applications like xmonad - package managers. The system installed xmonad will actually break if you're not using the same version of GHC it was built with. This happens fairly frequently if you're using a distro like ubuntu that often has an older version than you want to use.
&gt; if (sort str == alphabet) then True else False is the same as just &gt; (sort str == alphabet)
If you have an predefined lexicon, you can build a map which is deterministic.
It's wrong in the sense that all monads are functors and therefore there exists an fmap function for any monad. It's correct in the sense that, while all monads are functors, they are currently not required to have functor instances in GHC modules. We can't say for sure that any arbitrary monad has a functor instance. At best, we can say that there exists a function fmap, but we do not necessarily know what it is. Hypothetically, this will be changed in the next release or two (it's been a long time coming..). As of 7.6.3, GHC defines monad as: class Monad m where (&gt;&gt;=) :: forall a b. m a -&gt; (a -&gt; m b) -&gt; m b (&gt;&gt;) :: forall a b. m a -&gt; m b -&gt; m b return :: a -&gt; m a fail :: String -&gt; m a m &gt;&gt; k = m &gt;&gt;= \_ -&gt; k fail s = error s In the future, we should see a class declaration with an applicative constraint (thereby implying fmap), but that isn't the case at the moment.
Note that there is one disadvantage to using `cabal` to install libraries, which is if the library has a dependency on a non-Haskell package (i.e. `gtk2hs`) then `cabal` will not automatically install that dependency, whereas the equivalent distro package would. Even then I still prefer to use `cabal` to install these, but it requires understanding the error messages so that you know what system package to install.
Yaaaaaaaay! ^^\(I ^^really ^^like ^^this ^^podcast.\)
Indeed. Blogs (web logs) are inherently ephemeral, so the “log” and “my opinion” aspect belongs only in the blog. But useful instructional information would ideally be extracted and copied over to more central places. E.g. I could've written [this](http://hackage.haskell.org/package/aeson-0.6.2.1/docs/Data-Aeson.html#g:1) as a blog post but instead wrote it as documentation. Nowadays, whenever I see a blog post giving instructions on how to use a library, I think that could've been contributed as documentation, whereas now it will be instantly out of date/questionably up-to-date and bitrot.
no video? :(
Definitely don't go this route. Rather than type out an explanation on my phone I'll link http://www.vex.net/~trebla/haskell/sicp.xhtml#unsafeInterleave
I always wondered why some people do this. Thanks for explaining. 
Tell that to the people of Scunthorpe.
 import Safe (headMay) lookup :: Char -&gt; Int -&gt; [(Char, b)] -&gt; Maybe b lookup char offset = fmap snd . headMay . drop offset . dropWhile ((/= char) . fst) I am writing this on my phone so there might be mistakes.
It sounds like what you really want is to rotate your keys. rotateLetter :: Int -&gt; Char -&gt; Char rotateLetter n c = toEnum $ a + ((fromEnum c + n - a) `mod` 26) where a = fromEnum 'A' &gt; lookupCipher (rotateLetter 2 'A') cipher -- results in 'K' Note that this supports negative offsets, and `rotateLetter 1 'Z'` is equal to `'A'`.
Can anyone give a summary for the lay-haskeller?
No. That's not what I'm saying. The linked article mentions interleaving package manager with cabal. Whereas I only install the platform, and that's the point where I stop. But I get why people would go from that to a more "general" rule of thumb. Each to his own, I guess.
I had some trouble finding the audio link because of noscript. In case anyone has the same problem, here it is: https://soundcloud.com/haskellcast/003-simon-peyton-jones-on-ghc
Only if the intermediate variables have unbounded precision. If you're using a finite type there will always be collisions. Then again, it's always possible to design them to have properties like two colliding strings will always have at least a minimum edit distance.
Awesome. I could listen to spj bumble about nothing-in-particular for hours.
Really depends on your OS and your relationship with it. I use the system packages for libraries whenever possible and actively avoid installing from hackage unless I really really want a new version.
Your `PropertyList` is just an alias for `[(Char, Char)]`, as you used `type` to define it. Tekmo's function works with `[(Char, b)]` for any type `b`, including `Char`; thus it will work with your type. `headMay` is a function just like `head` (which gives the first element of a list), except that it results in a `Maybe b` value instead of crashing your program on an empty list. Loosely speaking, a `Maybe b` value is a value of type `b` which might not be there (e.g. because a lookup didn't find anything). It allows you to deal with failure gracefully, as in the following toy example: charMessage :: Maybe Char -&gt; String charMessage (Just c) = "The ciphered char is " ++ show c charMessage Nothing = "No ciphered char found" `Safe` is a module which defines the `readMay` function. It lives in a library package called `safe`. If you don't want to worry with installing packages right now, replace `import Safe (headMay)` with `import Data.Maybe (listToMaybe)`, and use `listToMaybe` instead of `headMay` (the name is more cumbersome, but they do the same thing).
it should have said "subclass" though, so your attention did get something fixed
`Char` is an instance of the class `Enum`, which defines `toEnum` and `fromEnum`. toEnum :: (Enum a) =&gt; Int -&gt; a fromEnum :: (Enum a) =&gt; a -&gt; Int `fromEnum` will convert a `Char` into an `Int`. For instance, `fromEnum 'A'` is equal to 65, `fromEnum 'B'` equals `66`, etc. The function `rotateLetter` converts its argument `Char` into an `Int` using `fromEnum`, adds a modular offset (that respects a 26-letter cycle), and then applies `toEnum` which converts the resulting `Int` back into a `Char`.
I've really enjoyed these. Keep up the good work! 
In the cabal section it might be nice to mention that it's a good idea to start out with a `cabal sandbox init`, since it will likely save you a lot of dependency-management pain down the road.
Simon's cam wasn't working :(
We're probably getting a bit *too* off-topic here...
In homotopy type theory, truncation is an operation that, given any type `A`, returns the proposition which is most "similar" to `A`. By "proposition" here I mean a type with at most one inhabitant. In the types-as-propositions interpretation, a proposition in the sense of HoTT corresponds to a "proof-irrelevant" proposition, i.e. one for which we only care about the truth value, not the proof. When you apply the truncation map to an provably inhabited type, you get a type which is equivalent to the unit type. What Nicolai has discovered is a rather surprising fact: for any inhabitant `a` of `A`, if you only know the projection of `a` into the truncation of its type `A`, you can recover `a` up to **definitional** equality. This is actually not true of all types `A`, but many types (including the natural numbers) do indeed have this property. The reason why this is surprising is that, operationally speaking, one would expect the information contained in `a` to be lost when passing to the truncation. In fact, this truncation is provably equivalent (thus equal) to the unit type, which has exactly one element. So this result effectively shows that in any implementation of HoTT, truncation cannot delete information, and in particular truncation is not a form of erasure.
&gt; 2 2 2$&amp;lt;/code&amp;gt;, \infty n. 2 2 2$, \infty n.
Essentially, `N` is the type of natural numbers, `|t|` is the "truncated" version of t, i.e. the type which has the elements of `t`, but where we consider every element equal. The function `|_|` (with the underscore being where you place the parameter) puts an element of `t` into `|t|`, essentially ignoring which element it was - or so one would think. This post uses some kind of magic to invert `|_|`. I'm not entirely sure how the typechecker is convinced, but it is easy to calculate, as you, for various reasons, will need to keep a `t` for each `|t|` in the actual implementation. Also, in case someone didn't realize, that's not Haskell.
^(2 2 2$&amp;amp;lt;/code&amp;amp;gt;, \infty n. 2 2 2$, \infty n.)
Thanks for letting us know. I'll see if we can make the experience better for noscript users. &lt;3
Would it even work if you throw it through the equality to unit and back again? Edit: intuitively, while the type of isomorphisms is contractible, is it correct that the type of isomorphisms acts [more like |N| than 1](http://www.reddit.com/r/dependent_types/comments/1il1h0/is_i_1_i/), in that it is the choice of isomorphism, not the choice of unit, that decides the resulting natural number?
Sorry, I'm not aware of a way of doing that. My guess is that those newlines are added by Pandoc by default to separate the blocks. The `pre` block is interpreted as a block separate from the surrounding text block.
One approach would be to put the equation inside `&lt;code&gt;` tags instead of `&lt;pre&gt;`. Then, tweak the script a bit to add newlines around the raw tex if it starts with "\b". (Here I'm assuming that anything that anything that starts this way is an environment and should get the newline treatment. If that's too rough and ready, you can make the test more exact.). (Warning: untested.) import Text.Pandoc.JSON main :: IO () main = toJSONFilter rawtex rawtex :: Inline -&gt; Inline rawtex (Code (_,["rawtex"],_) code) = RawInline (Format "latex") $ addNewlines code rawtexInline x = x -- add newlines around raw tex if it starts with "\b": addNewlines :: String -&gt; String addNewlines ('\\':'b':xs) = '\n':'\\':'b':xs ++ "\n" addNewlines xs = xs Another approach would be to use two different class names instead of "rawtex". For example, "texinline" and "texdisplay". Then pattern match on the class: rawtex :: Inline -&gt; Inline rawtex (Code (_,["texinline"], _) code) = RawInline (Format "latex") code rawtex (Code (_,["texdisplay"],_) code) = RawInline (Format "latex") ('\n' : code ++ "\n") rawtex x = x 
I would just like to mention that the type of `char board[3][3];` is in fact `char[][]` not `char[3][3]`, and does not include the size. Be wary of mistaking the array notation for some sense of a more rigid type constraint.
Well, if you fix the inhabitant, you can define an equivalence with unit that would preserve it when you to go to unit and then back (just send the only element of unit to it). But clearly, there is no one such equivalence that would work for any inhabitants. Not sure if that answers your question.
Sure. In a dependently-typed language (such as homotopy type theory), you can have types that correspond to mathematical propositions. You also have objects which correspond to proofs of those propositions. To show a proposition is true, it turns out you need to show a proof has the *type* of the proposition it proves. Just to give some examples, we can prove "1 + 1 = 2", or we can prove "5 is prime" or we can prove "the natural numbers form a monoid". But there's something neat (and arguably innovative) when you prove things in type theory as opposed to classical mathematics. Type theory is proof-relevant. That means that a proposition might have not just one proof, but many different ones. After all, proofs are just objects, and just as 0 and 1 are two naturals, I might have two proofs of some proposition. For instance, the last claim I made ("the natural numbers form a monoid") actually have several proofs because the set of natural numbers has several possible monoid structures on it. Perhaps I add the numbers (and my identity is 0) or perhaps I multiply them instead (and my identity is 1). Of course, this isn't true of *all* propositions. Some have just one proof, as we would expect. There's exactly one proof of "1 + 1 = 2" (we call this object `refl 2`). Similarly, there's only one proof of "5 is prime" (I believe. The definitions get pretty technical pretty quickly, but I'm fairly sure this is true here). Mathematical propositions which have exactly 0 or 1 proofs are called *mere* propositions in HoTT. They are precisely statements which are either false (0 proofs) or true in *exactly one way* (1 proof). The variety of different proofs for a non-mere proposition is what gives HoTT its flavor. But it can also get in the way. For instance, the law of excluded middle (P or not P) fails very very badly in HoTT for non-propositions. (In particular, it is inconsistent with univalence, which I won't discuss here). From a more practical standpoint, it's also obnoxious when programming in these languages because we often don't care about how things are done, just that they can be. (That is, we "merely" want to know that they are true, not necessarily why). But we can turn a proof-relevant proposition into a mere proposition using a trick called propositional truncation. Suppose `A` is a type. We are going to declare a "truncated" version of `A` called `|A|`. This new type will have all the same elements as the old one, but we decree that all elements equal. In a sense, they are glued together ([quotiented](https://en.wikipedia.org/wiki/Equivalence_class)), and as a consequence, no function in the type theory is allowed to distinguish between them. Furthermore, you can prove that for any type `A`, the truncated version `|A|` is isomorphic to the unit type. You would think, then, intuitively, every function which took `|A|` as its only parameter must be a constant. But, somewhat paradoxically, it's possible to write a function which "undoes" truncation! That is the discovery this blog post is talking about. You can write a function which takes an object of type `A`, glues it together with every other element, throwing away all of its identity in the process..... but then returns it to its original form. If you want a cute little picture that's totally not how it works, but at the same time, totally is, think of it like this: tl;dr, you take some object (maybe a number or a tree or some other data structure), you crush it down to a singularity, where you can't distinguish it from a spec of dust. Then, you can restore it to its original form. EDIT: Removed some slightly wrong information at the very end.
So one cannot postulate the definitional equality `x, y : Prop ⊢ x == y : Prop`? (`Prop` here is `Σ(X:Type).isProp(X)`, of course) If something similar were proved for Set truncation (which wouldn't be too surprising, I guess), would it prevent theorem provers to implement dependent pattern matching when they know the type is a set? (dependent pattern matching = like in Agda, which proves K, but that's not a problem for sets)
The take-home message I got (not necessarily the one Nicolai intended) is that the truncated version of a type has all the same values/points, we just add paths (aka "equality" proofs) between all of them. Thus, truncated types are distinct from singleton types. That is, the naive view that `|_| : T -&gt; ‖T‖` maps all the points of `T` into "the single point" of `‖T‖` is wrong. Since both types have all the same points, `|_|` is just one direction of a bijection between the types (considered as sets). N.B., this bijection is not an isomorphism since it doesn't preserve/reflect all paths. Methinks that's the reason behind the phrase "nearly invertible". (But all this is fairly OT for Haskell per se...)
^*nom*
Shut the hell up, Makes_Small_Text_Bot
I think you can. That's the point of truncation? But its just a weird definitional equality!
Right. Following this intuition, we have truncation doesn't smush information down, but just adds _more_ information, such that "under the brackets" we appear to have less.
&gt; So I don't think you need something 'extra-system' here. The metatheoretical statement is that the resulting function `A -&gt; A` is the identity. It turns out to be so, but you can't prove it within the type system.
Shut the hell up, Reads_Small_Text_Bot
Shut the hell up, Makes_Small_Text_Bot
I actually learned about liftM when I was writing monadic code and ran hlint on it. It kept spitting out things to the effect of: You wrote "getLine &gt;&gt;= return . process" why not "liftM process getLine" and I eventually got the message.
I recently made a parsec tutorial for very beginner: http://yannesposito.com/Scratch/en/blog/Parsec-Presentation/ You might then read this: https://www.fpcomplete.com/school/text-manipulation/attoparsec You will found a HTTP parser written by Brian O'Sullivan. Don't click if you don't want to get spoiled. https://bitbucket.org/bos/attoparsec/src/tip/examples/RFC2616.hs
There is [already something](http://www.gwern.net/haskell/Archiving%20GitHub) on the front page this subreddit on the topic, which might be helpful. If you're not at that level with Haskell yet, [this](http://www.cis.upenn.edu/~cis194/lectures.html) is a great place to start.
+1 an interesting question with no obvious right answer. Someone added a small test suite to `numbers`, which I maintain, and I refactored it so that the library could be installed without its testing dependencies. If you *don't* put the Arbitrary instances in the same module as the data types they are defined for, then you end up with orphan instances. But if you *do*, then you end up with a superfluous dependency. QuickCheck is in the Haskell Platform, so I wouldn't feel too bad about incurring that dependency. On the other hand, orphan instances aren't that bad as long as you clearly document them. It's a toss up. This dilemma could perhaps be solved with an enhancement to the Haskell module and package systems.
Shut the hell up, Eats_Small_Text_Bot
&gt; So one cannot postulate the definitional equality x, y : Prop ⊢ x == y : Prop? (Prop here is Σ(X:Type).isProp(X), of course) That's right, you can't. However, that judgement doesn't quite look right. I suppose you mean something like: P : Prop, x, y : P ⊢ proof-irr x y : x == y &gt; If something similar were proved for Set truncation (which wouldn't be too surprising, I guess), would it prevent theorem provers to implement dependent pattern matching when they know the type is a set? (dependent pattern matching = like in Agda, which proves K, but that's not a problem for sets) I still don't understand dependent pattern matching very well, but I don't think there's any problem there. In fact, you can prove that `K` holds for sets, including the correct computational rule (up to definitional equality).
this is, imo, **the** problem with current Haskell. I think the only way to fix it is a proper module system (your package gets parameterized by quickcheck modules), but that is a can of worms in itself.
Shut the hell up, Reads_Small_Text_Bot
You can use the Nix package manager with another distro. I'm also using Nix with Chrubuntu. That said, the support for Nix outside NixOS is a little lacking and things don't seem to just magically work like in NixOS.
That's actually not the case. The resulting function `A -&gt; A` can definitely be [proved to be identity](http://red.cs.nott.ac.uk/~ngk/html-trunc-inverse/trunc-inverse.html#3194) within the system. The tricky point, which initially confused me quite a lot, is that the "left inverse" `myst` of the projection `p : A -&gt; |A|` does not actually have type `|A| -&gt; A`. Its type is instead something like: myst : (x : |A|) -&gt; E(x) where `E(x)` is a carefully crafted type expression that evaluates to `A` when `x` is equal to `p a`. If you're curious about the details of the construction, the idea is actually relatively simple, and it's explained very well in the post and in the linked agda code. I'll try to summarise it here. Essentially, given a pointed type `(A , a)` (for a "homogeneous" type `A`), you can define a function from `A` that maps every element `b` to a path between `(A, a)` and `(A, b)`. Since the codomain is contractible (it's a singleton), you can lower this function to the truncation, and then `myst` is defined by extracting the `b` component out of the path. With this description, it should be intuitively clear that this function will return `b` when applied to `p b`, and in fact it follows from the computational rule of the truncation.
Shut the hell up, Makes_Small_Text_Bot
Indeed, if you incur technical debt to Satan, the price is...your soul. 
There's [another link](http://jabberwocky.eu/2013/10/24/how-to-start-a-new-haskell-project/) on the front page about starting Haskell projects, so if you're new to Cabal that might be helpful us well. I don't think you'd really need it for the homework in that lecture course, but it might help.
I'll check it out. Thanks!
Thank you for the correction. 
I think everyone could. He is adorable and brilliant at the same time.
I'm curious to learn more about "life with a proper module system". Would you say that OCaml's serves this purpose well?
yes. But, it does not have type classes so the interesting issues around the intersection don't show up. Coq has modules and typeclasses, but the typeclasses are in a rather different place in the design space from Haskell. Coq is the most "advanced" language around in including high end features from the PL community--but it doesn't really try to be elegant or usable for people who are not PhD students. Also, neither Coq or OCaml have generative functors, an interesting feature of SMLs module system. Dually, SML does not have "applicative functors" (these have nothing to do with applicative functors in Haskell). OCaml now has first class modules, but I haven't tried using them so can't comment. "Objects" in scala are a much like (first class) modules in other languages ML family languages (and orders of magnitude more powerful than the objects in languages like java). High end module style stuff in scala though is amazingly complex. So I say OCaml is as good a choice as any. IMO, Haskell remains the prettiest of the functional languages, but if you want to learn about or use modules OCaml is a good place to start.
I keep forgetting to use `hlint`! Thanks for the reminder.
Hm. I'd love to learn more Scala in the near term, so seeing the tricks one can play with their "objects" might be fun... except I feel always that all roads to Scala come from Java and get mired somewhere around recursion. In the long term, I'd rather spend my free time poking around OCaml, SML, Coq.
&gt; Think about it - the OP comes up with what he/she thinks might be an original idea. They're wondering if anyone's come up with the idea before, or if it's been explored, etc. They decide to 'ask the experts' on the Haskell reddit. You come along and refer them to Data Types a la Carte. Sure, which makes sense, seeing as it's a paper that, in addition to being interesting in its own right, presents a different solution to the same fundamental problem. About the respective merits of each solution, I make no judgement. But if the OP was looking for prior work, it certainly meets that criteria. &gt; The OP is going to be inclined to now go read this paper, and I can pretty much guarantee they're going to be very disappointed. Disappointed how? Why would the OP be disappointed? &gt; IMO, you have wasted this person's time. IMO, I haven't wasted anyone's time. It's a relevant paper and the respective benefits of each approach deserve discussion. &gt; There's a classic paper, Data Types a la Carte, which deals with some similar concerns. The details are quite different, and your approach is much lighter syntactically, so I don't think you'll find it directly useful, but you might find it interesting nonetheless. I didn't draw a comparison between the OP's approach and DTalC's approach, but I did mention that Swierstra's approach is different. If the OP is interested, the OP is free to make the comparison themselves. If not, no harm is done.
Your `liftM2` version has type `(Monad m) =&gt; m (IO ())` or `IO (IO ())` in this particular case, which is an IO computation that results an IO computation.
The do-notation equivalent of your first definition of `main` would actually be main = do n &lt;- name d &lt;- datum return $ B.writeFile n d which has type `IO (IO ())`. Since you probably want `main :: IO ()`, you could write main = join $ liftM2 B.writeFile name datum or, since this is pretty silly, just main = B.writeFile "aaaa" $ B.pack [54]
I actually encountered this problem in a slightly larger context, but simplified it to remove unnecessary details. This 'join' seems rather interesting. Is it safe to use? Doesn't it throw away information by removing a level?
It throws away the information that you have an `IO` action which will produce another `IO` action, but you presumably don't care about that; you just want to execute them both. If what you want is main = do n &lt;- name d &lt;- datum B.writeFile n d then `main = join $ liftM2 B.writeFile name datum` is equivalent to that.
I should have been more clear. What I am wondering is if the following property is true: a :: m (m b) a == return $ join a
 join . return == id but in general return . join /= id For example, (return . join) [[1,2],[3,4]] == [[1,2,3,4]]
All `Monad` instances are _supposed_ to follow the monad laws, but Haskell's type system is insufficiently powerful to actually enforce that. It's safe to assume (although it's a useful exercise to prove!) that any of the well-known `Monad` instances out there obey the laws.
That's really useful! Just tested it with a [script](https://github.com/wunki/vagrant-freebsd/blob/master/bin/vagrant-setup.sh) I made this week and it showed me some good tips. Thanks!
Oops, that's right, I meant P : Prop, x, y : P |- x == y : Prop (I use the notation x == y : A for "x and y are equal elements of A", with the type explicit) (you can't have a term witnessing it since definitional equality is a judgement, btw, though you can prove it for propositional equality) I don't quite get dependent pattern matching either, but proof unicity holds propositionally for Props, too, but that seems not allowed to hold definitionally, I'd expect the same for Sets and UIP. 
Looks interesting, perhaps add a warning about using variables in arithmetic expressions without a #10 suffix as they might be treated as octal if they ever have leading zeroes?
Oh, you're right, `==` was supposed to mean definitional equality, sorry. And sure, not all equalities in a set are going to be definitionally equal to `refl` (obvious, in a nonempty context), but I don't see why that would necessarily break pattern matching. One caveat, though, is that the K rule itself would depend on the particular chosen proof that something is a set. However, this is not observable internally.
using cabal from the start is great though (and its sandboxes!). you don't have to know the tedious details on how to compile stuff, etc. i am using cabal for everything i am not doing completely in the repl and even then, when it involves complex libraries.
Can you explain why you're doing that? It seems `writeFile` takes two pure arguments so why are you wrapping them? What's wrong with: main = let name = "aaaa" datum = B.pack [54] in B.writeFile name datum ?
I think I make or would make half of the mistakes this program shows. Not anymore!
This one was really good - a great variety of topics, a lot of room for the interviewee to answer questions and technically flawless. Great work guys, I can't wait for the next one!
Do you actually want to write an HTTP parser yourself, or do you want a library that will do it for you?
It's a good idea to always give main an explicit type signature of IO () to avoid this sort of issue.
Ok, trying to follow along. The latest Haskell Platform from MacPorts is installed. So... $ cabal install cabal-dev cabal: There is no package named 'cabal-dev'. You may need to run 'cabal update' to get the latest list of available packages. Oh - ok, that's easy. $ cabal update Downloading the latest package list from hackage.haskell.org Note: there is a new version of cabal-install available. To upgrade, run: cabal install cabal-install This is starting to remind me of Windows Update. Ok, try installing the latest cabal-install so it can install an update to cabal. $ cabal install cabal-install Resolving dependencies... Downloading Cabal-1.18.1.2... ... fatal error: sys/cdefs.h: No such file or directory ... Failed to install Cabal-1.18.1.2 Yikes!
Or just: main = writeFile "aaaa" (B.pack [54])
You don't do this in Haskell with a fold. You do it with a [`takeWhile`](http://hackage.haskell.org/package/base-4.6.0.1/docs/Prelude.html#v:takeWhile).
I think orphan instances are fine if they come from the same author/package. Alternatively, GHC could let a module say: "instance Arbitrary Foo from Other.Module" to seal the position of that instance -- without incurring a dependency (you have to explicitly import it to get the instance). This extension would solve any orphanhood issues while allowing making fine-grained dependencies.
It's *a* solution, but I doubt it's the only solution. In a [sibling comment](http://www.reddit.com/r/haskell/comments/1pdxcr/how_to_start_a_new_haskell_project/cd24huz) I offered an alternate solution, and there are probably more possible solutions. That said, a good module system could be nice, but the "right" way sounds like it would be the Agda way. Loosen the constraints from the type system enough that a module, a type-class, and a record can all be represented by the same thing (at least from a type-system perspective). I don't think Haskell can evolve in that direction, and I would not want to see modules done in a different way. So in that aspect, I think Haskell is a dead end.
One way is to use [foldM](http://hackage.haskell.org/package/base-4.6.0.1/docs/Control-Monad.html#v:foldM) in conjunction with an `Either`-like monad.
And sometimes scanl/scanr can be useful, if you need the last folded result that matches some condition, like the last factorial that's less than 150: &gt; last . takeWhile (&lt; 150) $ scanl (*) 1 [1..] 120
Not recommended.
Aww .. doesn't support tcsh.
Oh sure!
Isn't that what the Acme namespace is all about?
This is a slippery slope. I've been sending myself messages from the future using the backwards IO monad for a while now -- mostly just solutions to tricky code problems and what not -- but the risk of \_|\_'ing out the universe is high.
It might also be very confusing to receive answers to problems one does not yet know one has… ("will have had"? ;)
Does anyone have any experience using this along with the Acme.Year package? Especially around New Years.
Three practical Haskell applications announced on one day! This makes me very excited.
Might as well include `IOT` while you're at it.
You misspelled "Truly".
Obviously this is very dangerous, but is there any practical use case for it?
Then why do people use liftM? It's an extra import after all.
I've always wanted to model my life as a game tree and perform minimax on it to optimize my outcome. This lets me do it!
Jokes aside, this library demonstrates perfectly some of the problems with modeling `IO` as a state monad and why it is pretty weird to think about it this way.
I can recomend you this Cursor library. http://hackage.haskell.org/package/xml-conduit-0.7.0.1/docs/Text-XML-Cursor.html And here is small tutorial https://www.fpcomplete.com/school/starting-with-haskell/libraries-and-frameworks/text-manipulation/tagsoup
So if there is something which is an instance of Monad but not Functor you could not use fmap. That makes sense, thanks for clarification.
We mostly use `liftM` to define fmap 'for free' given a Monad defined in terms of `return` and `(&gt;&gt;=)`. Similarly you can use `liftA` to define fmap 'for free' given an Applicative defined in terms of pure and `(&lt;*&gt;)` and `liftW` given a `Comonad`. These help justify the existence of `Functor` as a (logical) superclass for all of these other classes -- although, it is also justified by the theory behind them all.
Further investigation reveals the Platform and ghc seem to have been broken by a recent upgrade (not sure if it was tge OS or compiler). I'll just reinstall ghc and try all this again.
&gt; It's mainly focused on handling typical beginner and intermediate level syntax errors and pitfalls where the shell just gives a cryptic error message or strange behavior My guess is that's the problem. You seems to understand your shell well enough to not need this. I think this ShellCheck is catering to a different audience.
If you overwrite the current universe with the universe from the future, isn't that effectively time travel (faster than +1 s/s)?
I'm getting much better at shutting up! Also Simon is a force of nature. Glad you enjoyed it!
I knew Haskell would do something truly dangerous in the end.
I use the system manager exclusively for my Haskell work. The system package manager in question is Nix from http://nixos.org. That said, I don't install things system-wide - I take advantage of Nix as much as I can to work in small sandboxes. I write something like this [default.nix expression](https://github.com/ocharles/netwire-classics/tree/master/default.nix) and then I type `nix-shell --pure` and I get dropped into a bash shell with all the Haskell libraries available. As you can see - I don't just have to depend on what's in Nixpkgs - the official package library - I can also add in my own dependencies. And once I start to have `default.nix` expressions everywhere, I can use `callPackage` to compile other projects that are interdependent - a lot like `cabal-meta`. Did I mention all of this work ties in transparently with [deploying binaries to servers](https://github.com/NixOS/nixops) and [configuring continuous integration](http://nixos.org/hydra/)?
I do all my hacking with Nix in exactly this way. It takes a minute to setup a default.nix and then I can drop into a dev environment with `nix-shell`. Even better, I can depend on `git` and `gvim` or whatever else I need to get a sane dev environment.
You have from Data.Map import M I think you mean import qualified Data.Map as M Anyway, you can make this even more idiomatic with a monoid instance, and using different map functions, and by using a simple tuple for Events: data Player = Player { playerMadeShot :: Int, playerMissedShot :: Int, playerRebound :: Int} deriving (Show) type Event = (String, Player) type Players = M.Map Name Player instance Monoid Player where mappend (Player a1 a2 a3) (Player b1 b2 b3) = Player (a1+b1) (a2+b2) (a3+b3) mempty = Player 0 0 0 madeShot = mempty {playerMadeShot = 1} missedShot = mempty {playerMissedShot = 1} rebound = mempty {playerRebound=1} aggregate :: [Event] -&gt; Players -&gt; Players aggregate events players = M.unionWith (&lt;&gt;) players (M.fromListWith (&lt;&gt;) events) output = [("playera", madeShot), ("playerb", missedShot), ("playera", madeShot)]
Set up the right probabilities, then make your major decisions based on single atomic events. Then in some universe you are doing the optimal thing. You can then hope that you are in the more interesting universe ;)
 import Control.Lens import Linear data Cell = E | X | O deriving (Read,Show,Eq,Ord,Enum,Bounded) type Board = V3 (V3 Cell) foo = V3 (V3 O E O) (V3 X X O) (V3 X E E)
Please, please don't use this package! Think of the untold billions of lives lost each time a `RealWorld` is reclaimed by the garbage collector!
What were the other ones? 
I'm new to haskell but finding these talks incredibly interesting. Keep it up! :)
In Haskell, variables don't change. So 'watch' isn't a thing?
Yes, just want something equally useful.
I don't think the syntax has been invented yet.
Every time I use the ghci debugger I can't help but think how much better things could be. All of the needed functionality is exposed, but the CLI interface simply isn't very efficient. I think a nice browser interface could make for a far more fluid user interaction (e.g. highlight the subexpression corresponding to each frame of the evaluation stack). With a proper interface (and perhaps a bit of a speed improvement) it could become a valuable tool. At the moment I find it's just too awkward to do anything serious with.
But Acme products work so well for Wiley Coyote! I bet he could find a useful way of catching that pesky roadrunner if he was able to undo reality.
Theres definitely ways the debugging tools can be improved, all it takes is someone taking ownership and making it happen! :) 
[Luckily I didn't launch any missiles... ](http://i.imgur.com/6mr4rZB.png)
Ah, okay. That did initially seem like the easy (read: only way in which I could figure out) way to do it but it also seemed like it was far more intensive than was necessary. Ah, poop.
I should have expected this - it happens with every major release. :/ I'm still rebuilding everything... Are these tweaks documented somewhere? A wiki perhaps? Trying to run a `port upgrade outdated` just didn't work, so I manually uninstalled Haskell (and a bunch of other stuff). When rebuilding ghc dependencies, I got a weird error with libgcc failing to run cc1 (preprocessor). According to the Macports, running: xcode-select --install to ensure all the command-line build tools are installed should fix the problem. 
Open a file in AppendMode?
Will this allow me to append only specific lines and not just tack things onto the very end of the file? Again I'm a total noob. This is my first big project written in this language and although I think I've got a lot of it down, there's still whole buckets of shit that confuses the hell out of me and probably will for quite a while. That being said, this language seems far, far easier for me to wrap my head around initially than pretty much any of the other ones I've been shown/taught and the project isn't due for another two months (which is why I started it now: if I can't get it to work in a month, I can start again in a language I actually know and turn in a working copy).
Corecursion can be formalized in a stateful way as a function `μ ν f = ∃a. a ∧ (a → f a)` This is stateful because it is a seed, and a generator that gives us new states for each new invocation of the generate (think streams of variable updates). There's a dual combinator for bounded recursion but I don't know how to interpret it statefully. I think a practical vision for a watch command would then be where one could watch a function, and that each new invocation of that function is thought of as a new update on the state. But that's just breakpoints but on anonymous functions as well. Can anyone think of a stateful interpretation of bounded recursion?
How would you do it in any other language? I can all but guarantee that the same strategy will work with Haskell. 
No, with Haskell that kind of time-checked typetravel is safe!
Maybe a kickstart?
This is the kind of project I'd love to do if I had a lot more free time (and no job). 
Some speculate that Joey Adams is not in fact human, but rather an advanced form of artificial intelligence from the future, sent back in time to squash bugs in his own design, forming a self-reinforcing time loop with unknown intentions on the fate of the universe.
And with a monoid instance also for the player map, we can use a writer monad (or monad transformer) to aggregate events. newtype Players = Players (M.Map Name Player) instance Monoid Players where mappend (Players a) (Players b) = Players (M.unionWith mappend a b) mempty = Players M.empty event :: MonadWriter Players w =&gt; Name -&gt; Player -&gt; w () event k a = tell $ Players (M.singleton k a) output :: Players output = execWriter $ do event "playerA" madeShot event "playerB" missedShot event "playerA" madeShot Which could be nice in some situations.
I wouldn't say it demonstrates problems with the IO monad, but it is a good example of why it is weird to think of it as a state monad. An action in the state monad takes an initial state, and then returns a new state along with some value. So conceptually, IO takes the state of the universe (the `RealWorld`) and returns a new state of the universe. But of course this isn't really true, and this demonstrates why: in the state monad it's perfectly possible to revert back to a previous state as if nothing ever happened, and perhaps try another route. Which (obviously) isn't the case with the `RealWorld`. But in GHC, this is actually how the IO monad is implemented; the `RealWorld` is just a myth that makes everything work.
... but... it... uses... `unsafeCoerce`...
It puts you in the same environment that is used to build binaries, but without actually running the build steps. So you get all the GHC libraries you want, and don't have to pollute your system with them. See http://nixos.org/wiki/Create_and_debug_nix_packages#Using_nix-shell_for_package_development
Shouldn't that be `map process` in `main` or `[String] -&gt; [String]` for `process`?
I don't use it either, but I think the main reasons for that are 1) I never learned how to use it 2) I'm quite used to work without a debugger That said, if there was a nice interactive graphical debugging environment, I would definitely use it, it would be much more convenient
I feel sure there should be a &lt;http://shouldioptimiseyet.com&gt; to sit alongside &lt;http://isitchristmas.net/&gt; but apparently not.
Eclipse FP mentions good interactive debugger: http://eclipsefp.github.io/features.html I've not used it yet, but looks promising?
The design of Haskell is often forcing one to do the right thing. Perhaps not having a good debugger is forcing one to write more modular and therefore easier to test code. ;)
A lanuage without a good debugger is not a super hero :)
Except this thing just unsafeCoerce#s one RealWorld into another. Of course IO fails when you flip the bird to the type system. That's the point of having a type system in the first place.
The uses of `unsafeCoerce#`don't really play a role in this observation about `IO`'s model, though. The interface is all you need to see what's weird about it.
The point is, that everything has multiple consequences. There're even some c developers disliking a debugger, because it might encourage the fast and thoughtless fixing of bugs, without the need of thinking of the real issues. I'm not saying, that this must happen if there's a debugger, but it's interesting how something might cause an unexpected effect, and how the summation of unexpected effects might result in bad software. 
Take a look at: http://justtesting.org/post/64947952690/the-glasgow-haskell-compiler-ghc-on-os-x-10-9
Aw. I'm disappointed. The acmeism is cute, but I actually thought someone made an undo-able IO monad, like transactional IO. If you were going to do that, you could restrict yourself to a UIO monad that can undelete files, unwrite files, unwrite IORefs, unlaunch threads, etc. either by doing any side-effects as a log that only take effect at the end or actually undoing real operations.
Sour grapes. ;-) &gt; To be honest, I don't even use the GHCi debugger at all. I usually just evaluate expressions in GHCi, and see if the results match my expectations. In really convoluted settings occasionally I'll use Debug.Trace.trace to spit out the real-world parameters to some function where something is going wrong, and then from there, I can play with things by hand, and see what part of the definition isn't working correctly. This really sounds like “I didn't want good debugging support anyway!” The age old approach of inserting printf statements is doing the work that the debugger can do for you. That's the point of it. You don't use the debugger because you don't have good integration, not because it's somehow superfluous. &gt; Often the first thing I'll do is to work on factoring things into smaller definitions to make sense of them. The extent to which you can push everything out into referentially transparent functions is the extent to which you don't need a debugger to make sense of the context in which your programs are running, because that context can't matter. This is the right approach, and it would be harder if you didn't have a REPL with which to test your smaller functions individually. But you could just use ghc and compile a `main` function for every expression you want to test. But you don't, because that's laborious, and you already have a well integrated REPL. You can get the types of sub-expressions, kinda, with implicit params or inserting a `()`, but ghc-mod and FPC's IDE are providing sub-expression info. Likewise, the debugger isn't a crutch that lamers with badly put together code use, it's another tool that tells you things about your code. Look at Agda's IDE. It's completely legitimate to expect decent support in this area, and beyond that. E.g. going further, why can I not write an expression somewhere, start writing a function which the expression uses, and see online, live output of the evaluation steps of that function. Why do I have to imagine it in my head and only see the end result? Why does the type system only tell me about failures, when it could show me what it's doing, I could step through it, see the unification environment, etc. Don't get me wrong, I'm happy that Haskell's purity and type system makes it possible to do production-ready code with (historically) barely any decent integration beyond “compile the text file, look at the errors, eval this expression”, but let's not sell Haskell's tooling potential short.
Or much more commonly, you have a Monad constraint on the type-variable in scope, but not a Functor constraint. The type *is* usually an instance of both, but you don't want to explicitly spell out: (Functor m, Monad m) =&gt; ... everywhere.
Instead of an array, you can get the contents, split it to a list of lines, then apply an addition to some index, then write it back out. I'll use the Lens library: import Control.Lens main = do getContents &lt;&amp;&gt; lines &lt;&amp;&gt; ix 5 %~ (++"BOO!") &lt;&amp;&gt; unlines &gt;&gt;= putStr Note: &lt;&amp;&gt; is flipped fmap, so lets me post-fix apply a pure function to the current action. &gt;&gt;= is flipped (=&lt;&lt;) so lets me post-fix apply an effectful function to the current action.
Let's leave aside the question of whether you were "right" or "wrong", I understand you don't feel you did anything wrong. But perhaps we can both agree it might have been beneficial to the OP to provide a bit more context along with your citation, something along the lines of what I suggested. And I hope you'll consider providing such context in the future when citing things that you think are relevant to a technical discussion. :)
Well, but programming as a whole is abstract in the same sense. When I play with a file in C, it isn't a literal file in the concrete sense, but it's beneficial to think of it as if it were one. Consider OOP. It's predicated on symbolic "objects" akin to RealWorld being useful and intuitive in helping to understand how a program functions. Even looking at primitives, an int isn't actually an integer; rather, it's a close-enough abstraction built on an imperfect physical model of a perfect, mathematically-pure concept. None of these are actual, concrete things, but that doesn't matter. They act as if are what they represent. So of course the illusion is broken when you start mucking around with the innards. You've exposed the man behind the curtain. That doesn't make an abstraction like RealWorld any less useful or intuitive. We may very well be taking the concrete RealWorld and having our way with it. From our perspective, we don't know the difference until we delve into the details of the construction of the abstraction. That said, this is just my opinion. I don't see the problem with a RealWorld state symbol. 
In your case the genRequest mentions (Parameters a) but not `a`. And since any number of types can have same Parameters type associated with it - ghc can't decide which instance to use. discogs is same is `show . read` here. You can make bidirectional type family, which works like bidirection FD. but it's ugly. &gt; type family FromParams a :: * &gt; class FromParams (Parameters a) ~ a =&gt; Request a where &gt; type instance FromParams Text :: User &gt; class Request User where &gt; type Parameters User :: Text EdIt: Another option is taggin your request/response with type of `a`, of course you''ll have to add a-lot of annotations &gt; {-# LANGUAGE ScopedTypeVariables #-} &gt; newtype Tag a b = Tag a &gt; data RequestFunctions req resp = RequestFunctions &gt; { genRequest :: req -&gt; Tag Int resp &gt; , parseResponse :: Tag Int resp -&gt; resp &gt; } &gt; class Request a where &gt; type Parameters a &gt; f :: RequestFunctions (Parameters a) a &gt; discogs :: forall a. Request a =&gt; Parameters a -&gt; a &gt; discogs params = parseResponse f req &gt; where &gt; req :: Tag Int a &gt; req = genRequest f params 
What does this model allow you to express that is impossible? Also, I would argue that the model says nothing about concurrency. We know that our function changes the world, nothing more, nothing less. Maybe that's your point, but in that case, I don't see the issue with building concurrency on top of a RealWorld token.
&gt; **hypothetically** ∷ `IO a → IO aSource` &gt;Perform an action and return its value, but undo any side effects to the universe. Thus, it appears to return instantly, regardless of how long the action would take to run. Handy! hypothetically launchMissiles 
Well, you can take my comment as a criticism of GHCi's debugger if you like. That said, while it'd obviously be nicer to have better ways to inspect what's going on in running Haskell code, I'm not sure the right sort of thing would look anything like gdb. (Though maybe for IO computations specifically that sort of thing would make sense.) It would be nice to have a way to inspect expression graphs at runtime and visualise the heap as it changes in that way. The main thing I think we need better visualisation for is understanding space usage, rather than semantic correctness problems. It becomes much clearer where space is going, what's causing retention, etc. when you can see the expression graphs.
&gt; What does this model allow you to express that is impossible? The interface provided by `Acme.RealWorld` is impossible, for example, yet perfectly expressible by the state monad model. &gt; I would argue that the model says nothing about concurrency. The model says concurrency is impossible. If everything is a state transition function then there can be nothing else operating on the state until the function has returned the new one. In the real world, no discrete update is atomic like that; it's a bit different to think about with a continuous model, but that is even further from the state monad model.
Every `RealWorld` is sacred! Every `RealWorld` is great! If a `RealWorld` gets wasted Simon gets quite irate
I've recently found this different lens/arrow operator useful in my code: (&lt;~~) :: (Arrow ar) =&gt; ASetter s t a b -&gt; ar s b -&gt; ar s t setter &lt;~~ arrval = proc x -&gt; do bval &lt;- arrval -&lt; x returnA -&lt; x &amp; setter .~ bval It's not doing the same thing as `overA`, but is meant for situations analogous to where you'd use `&lt;~` in State Monad code: [Example usage](https://gist.github.com/fizbin/7217274)
thank you for your answer sopvop. if i understand correctly, the bidirectional type family will make it essentially injective again. the second approach works and i'll use that. i think i will get rid of ScopedTypeVariables while at it (hinted by ghorn and donri).
Yes, you're right. I had a case of the brainfarts.
Nope. It's that there's no one owning that part of ghc proactively right now. Though there's a few pieces of ongoing work that may land in 7.10. There's quite a few areas of ghc dev which have people working on volunteer time, but it's quite hard to see how raising money that is less than a salary would shift the needle there
"Smack'em Pilot" sounds safe.
Summary: Martin Odersky outlines the main categories of static type systems as well as some new developments, and discuss the tradeoffs they make. Lots of comparisons with Haskell's type system and the choices it has made. He also discusses his work on DOT (Calculus for Dependent Object Types) and Dotty, a Scala-like language based on DOT which map functional type ideas (higher-kinded, existential) into object-oriented types (compound types, refined types). 
This is the best web talk UI I've ever seen.
Except you cannot check out just the slides, and also cannot fast-forward/backward in the video
&gt; This is the best web talk UI I've ever seen. I think it's more that it's the _only_ web talk UI. Everything else is "hey I uploaded my talk on Vimeo see link in description for slides.”
... if only the benefits Scala derived were proportional to the price they pay.
&gt; I have no problem with GHC using a made up RealWorld token under the hood. I have a problem with pretending that is IO's denotation. This model allows you to express things that are impossible and doesn't allow you to express some things that are possible (concurrency, for example). To what extent is it really a problem with the denotation vs. not having linear types? If we had `type IO a = RealWorld ⊸ (a, RealWorld)` (with the linear implication "lollipop" instead of the function arrow), wouldn't the impossible programs be inexpressible? (Though strictly speaking that wouldn't be our State monad, it would be something like a linear state monad...) (Not that that would address the concurrency issues...)
Sent PM recently...
Atomic read-and-update is impossible, but the model would still imply it even with linear types. There are actually *two* issues remaining because of this. Both could be construed as "concurrency," but they don't even require `forkIO`: * The state fed into the second argument of `(&gt;&gt;=)` is not necessarily the same as the one returned from the first argument. * The state transition function doesn't have exclusive access to the state it is operating on.
I'd love to see a system that somewhat reliably transcribes the text. Difficult of course but i'm sometimes a bit shocked that speech-to-text technology hasn't yet come far enough to make this common-place.
I love this answer. For extra credit, what if instead of 'ix 5' you needed to add to any line containing "HONEYBOO"? 
Perfectly expressible if you're willing to throw away the type system's guarantees and coerce results into one another with `unsafe`, you mean? As long as the IO system hides the ability from safe code, I don't see the problem - and in that case it also obscures the state monad model as well. &gt; The model says concurrency is impossible. No, just that ordering effects when using facilities provided by the OS to do so (threads, for example) is not deterministic. Which makes sense, because there are very few hardware systems that provide deterministic access to the real world, and it would make little sense for IO in Haskell to adopt a model for hardware almost no one has access to. Instead, they adopted a sequential model that requires the user to explicitly submit to non-deterministic ordering if they want to use concurrency. That seems like a very reasonable model to me.
Yeah, the vast majority of tech talks tend to be, "Hey, let's point the camera at the presenter and ignore the slides." (Especially see this in talks at local user groups and such.) I've struggled with this internally at my company's tech talks. Are there any tools (free or commercial) that make what InfoQ does reasonably easy to achieve? The ones I've tried (ie. Captivate) can do it but it's not simple for me, at least.
Then add this line: &lt;&amp;&gt; filtered ("HONEYBOO" `isInfixOf`) %~ (++ "BOO") There is a caveat, though, that filtered is not a valid traversal in such a use case. Though for the case of a setter like here, I think it's okay. Another alternative is to first tag those that match the condition, and only then use filtered on the independent tag.
I don't think you understand. I am talking about denotational semantics. I don't know what kind of model you are talking about. I suspect (if I may put words in your mouth for a moment) that you are actually talking about an *analogy*, which is conventionally accepted as only being able to "stretch up to a point." I'm talking about something much more precise than that.
What I want is Buddha or Hat. They let you evaluate a program, and then drill into subcomputations until you find the base case that is causing your wrong answer. Because Haskell programs are typically not about imperative, 'steps.' They are about recursive decomposition into subproblems. You need a debugger that lets you inspect that structure.
That talk was wonderful, thank you for the link. His notions about ASTs, languages, and their representations reflect some of my own thinking, and I really hope he is able to follow up on those ideas uninterrupted. 
+1 just to keep this above the comment about presentation web UIs… (Seriously, though, thanks for sharing.)
&gt; denotational semantics &gt; The interface provided by Acme.RealWorld is impossible, for example, yet perfectly expressible by the state monad model. Remove the `unsafe` code and it is not perfectly expressible because it doesn't check. No?
You can download the slides after you make a login. There is a link under the video on the right hand side.
Thanks, that answers both my questions.
The fact that I don't reveal the full power of a denotation in the interface is irrelevant. The denotation doesn't just define the interface but also how to think about it. Unlike an analogy, you should be able to reasonably derive laws from a denotation.
I installed the latest Haskell Platform on a brand new Macbook Pro with Mavericks. Installed XCode 5 first. No problems. Only compiled smaller projects yet, though. And I did not compile GHC itself.
Nice!
Note: pandoc is written in Haskell: http://hackage.haskell.org/package/pandoc
What would be very helpful is the ability to attach to a running process and start inspecting the internal state: what has or has not been evaluated, what's been memoized, etc. Add the ability to "reset" something to "unevaluated" and then step through it's evaluation to determine why it is ending up with an unexpected value.. that would be excellent.
Think of it as a movie. You don't fast forward a film to find out how it ends quicker. If it isn't good just turn it off. 
hehehe he goes through all this stuff about "freedom is slavery" and "having loads of options is too difficult to handle" and then wants people to be able to program in the same "language" using diverging syntaxes and grammars and whatnot. You thought the tabs vs spaces debate was bad? I got no idea why people hate on text so much. My knee-jerk suspicion is they're the sort of people who don't like to read books or even in-depth articles. Why communicate using words and sentences when we can use pheromones and gestures?
It's not a movie, it's a presentation. I get a lot of value from looking at slides. I wouldn't mind having the narration to a slide that I choose not to skip past, but there's typically a lot of "fluff" slides in any presentation that can be skipped through. This one is no different.
I understand his stance on trying to make a syntax independent language. I've often had the thought myself. A chance to interact with a ast using whatever syntax you want to. If it ever actually worked it would be great, not that I have any clue how that could be done. But even if it succeeded, what would probably happen is programmers would start arguing about the ast instead and we'd end up essentially where we are now, but deeper.
Off the top of my head: * If you take any two of the random extensions that have been thrown into scala and try to use them together, they typically don't play nice. e.g. Implicits and subtyping don't play nice together. * Type inference works right up until you write anything that needs it. If you go to write any sort of tricky recursive function, you know, where inference would be useful, then it stops working. * Due to type erasure, its easy to refine a type in a case expression / pattern match to get something that is a lie. * Free theorems aren't. * Since you can pass any dictionary anywhere to any implicit you can't rely on the canonicity of anything. If you make a Map or Set using an ordering, you can't be sure you'll get the same ordering back when you come to do a lookup later. This means you can't safely do hedge unions/merges in their containers. It also means that much of scalaz is lying to itself and hoping you'll pass back the same dictionary every time. * The container types they do have have weird ad hoc overloadings. e.g. Map is treated as an iterable container of pairs, but this means you can't write code that is parametric in the Traversable container type that can do anything sensible. It is one of those solutions that seems like it might be a nice idea unless you've had experience programming with more principled classes like `Foldable`/`Traversable`. * You wind up with code that looks like myMap.map(...).toMap all over the place due to `CanBuildFrom` inference woes. * Monads have to pay for an extra map at the end of any comprehension, because of the way the `for { }` sugar works. * You have type lambdas. Yay, right? But now you can't just talk about `Functor (StateT s IO)`. Its `Functor[({type F[X] = StateT[S,IO,X]})#F]`, and you have to hand plumb it to something like `return`, because it basically can't infer any of that, once you start dealing with transformers ever. The instance isn't directly in scope. `12.pure[({type F[X] = StateT[S,IO,X]})#F]` isn't terribly concise. It can't figure out it should use the inference rule to define the implicit for `StateT[S,M,_]` from the one for `M[_]` because of the increased flexibility that nobody uses. * In this mindset and in the same vein as the `CanBuildFrom` issue, things like `Either` don't have the biased `flatMap` you'd expect, somehow encouraging you to use other tools, just in case you wanted to bind on the `Left`. So you don't write generic monadic code over the `Either` monad, but rather are constantly chaining `foo.right.flatMap(... .right.flatMap(....))` ensuring you can't use the sugar without turning to something like `scalaz` to fill it in. Basically almost the entire original motivation for all the type lambda craziness came down to being able to write classes like Functor have have several instances for different arguments, but because they are so hard to use nobody does it, making the feature hardly pay its way, as it makes things like unification, and path dependent type checking harder and sometimes impossible, but the language specification requires them to do it! * You don't have any notion of a kind system and can only talk about fully saturated types, monad transformers are hell to write. It is easier for me to use the fact that every `Comonad` gives rise to a monad transformer to intuitively describe how to manually plumb a semimonoidal `Comonad` through my parser to carry extra state than to work with a monad transformer! * I've been able to get the compiler to build classes that it thinks are fully instantiated, but which still have abstract methods in them. * Tail-call optimization is only performed for self-tail calls, where you do not do polymorphic recursion. * Monads are toys due to the aforementioned restriction. `(&gt;&gt;=)` is called `flatMap`. Any chain of monadic binds is going to be a series of non-self tailcalls. A function calls flatMap which calls a function, which calls flatMap... This means that non-trivial operations in even the identity monad, like using a Haskell style `traverse` for a monad over an arbitrary container blows the stack after a few thousand entries. * We can fix this, and have in `scalaz` by adapting `apfelmus`' operational monad to get a trampoline that moves us off the stack to the heap, hiding the problem, but at a 50x slowdown, as the JIT no longer knows how to help. * We can also fix it by passing imperative state around, and maybe getting scala to pass the state for me using implicits and hoping I don't accidentally use a `lazy val`. Guess which one is the only viable solution I know at scale? The code winds up less than 1/2 the size and 3x faster than the identity monad version. If scala was the only language I had to think in, I'd think functional programming was a bad idea that didn't scale, too. * `for` `yield` sugar is a very simple expansion, but that means it has all sorts of rules about what you can't define locally inside of it, e.g. you can't stop and `def` a function, `lazy val`, etc. without nesting another `for` `yield` block. * You wind up with issues like [SI-3295](https://issues.scala-lang.org/browse/SI-3295) where out of a desire to not "confuse the computation model", it was decided that it was better to you know, just crash when someone folded a reasonably large list than fix the issue.. until it finally affected `scalac` itself. I've been told this has been relatively recently fixed. * No first-class universal quantification means that quantifier tricks like `ST s`, or automatic differentiation without infinitesimal confusion are basically impossible. def test = diff(new FF[Id,Id,Double] { def apply[S[_]](x: AD[S, Double])(implicit mode: Mode[S, Double]): AD[S, Double] = cos(x) }) is a poor substitute for test = diff cos ... but it runs on the JVM. 
I'm totally doing this.
I've been working on this for the past couple of months and would really appreciate any comments, code review, contributions, etc. There are still some core features missing, but it's just about at the point where I would call it useable for my particular use case, which is a pure functional core for a larger app written in TypeScript.
But if the 'pettiest' thing software engineers argue over is the layout of their ASTs rather than their brackets/etc., then (as you say) it would mean that the pettiest argument is at least at a slightly deeper level! And raising the level of common (or even the dumbest) discourse is never a bad thing.
I don't mean to be a downer, I'm just wondering what this presents that isn't offered by Faye or Elm? It looks awesome, I'm just worried the community will get fractured with too many functional Web languages. 
I see what you're saying. I started working on this for a couple of reasons - for one, I just thought it'd be a fun project. But also, I was looking for a language that I could actually use for my day job, and each one I considered had something that made me reconsider. They are relatively small concerns, but the biggest one is that the generated Javascript needs to be simple, small and readable. In fact, I really liked Roy for its generated Javascript.
I am not a Javascript expert, but I think it is not designed for the same niche as Fay or Elm. Fay emphasizes being as compatible with existing Haskell code and idioms as possible. Elm emphasizes structuring web programming using totally new design patterns and idioms (mainly FRP). PureScript, on the other hand, seems more like sugar for a useful subset of Javascript that emphasizes functional programming best practices (immutability by default + local ST windows + static types + sum types). In other words, it tries to be a better Javascript rather than a limited Haskell.
I guess I'd greatly disagree - all that matters is the interface and belief in the abstract power of denotation when the surface area is different seems to fly in the face of my understanding of Haskell and even what little category theory I know. For example, suppose the internal mechanism for IO was drastically different but had exactly the same semantics - what matters, the denotation or the semantics? The user can't discern the difference, and the theoretician *cannot* derive different theorems based on the denotation because we know that each implementation had the same semantics. So even if the internal mechanism for IO was not a state monad, even if the internal mechanism was just straight up compiler hackery - built-in keywords and mechanisms that override monadic operators to give IO the appearance of acting like a monad - it doesn't matter. It doesn't affect our ability to reason about IO once we know its semantics. Now, true, it might be harder to discern those semantics given such an implementation, but that doesn't change the argument. Just as two functions that have the same semantics may be equivalent and replace one another, that doesn't mean they are equivalent in ease of determining those semantics. One can make an arbitrarily complex function equivalent to `f x = x + 1`, but what matters for reasoning about it is the semantics, not the implementation.
Makes sense! 
add RDFa ?
As I say, Roy would have been my main choice. My (very minor) quibbles are as follows: 1. The FFI seems a bit liberal, 2. I would like to eventually add type classes. Both of these could be addressed by my forking Roy, but I'd personally enjoy implementing such things in Haskell more than in Javascript. As I say, minor quibbles. I was also selfishly motivated to just have a new Haskell project to work on.
Awesome. I've been looking for something like this. Can you expand the FFI documentation? The canonical FFI usage seems to be JQuery and dealing with foreign object method invocation etc. Also, is there a reason you don't compile functions to this: var fn = function() {...} instead of: function fn() {...} Wouldn't it be simpler to support high order functions with the former?
Only top-level functions get compiled as `function fn() { ... }`, to support self-recursion. When a function appears inside a term, in gets compiled as you said. I plan to work on wrappers for libraries for things like JQuery soon. I think it will require a little more work than just writing type signatures for existing methods. I'll work on expanding the documentation, but for now you might like to look at the examples folder. Edit: This example is probably the most useful https://github.com/paf31/purescript/blob/master/examples/passing/Console.ps https://github.com/paf31/purescript/blob/master/examples/shelltest/Console.js
I'm doing the same thing. Do you know how to make the VM resolution match the native screen res?
Very cool. I've always wished there was a haskell compiler for web. I am also writing an app in TypeScript. I think this is awesome. Would have to examine more closely on how clean the output is, but judging from example page - looks good. I checked out WebSharper and Dart (compiled to js), and decided not to pursue either because js they compile is messier than I am willing to read. Typescript translation is exceptional, hope yours is on par. 
This is really, really nice! I can't wait to try it out.
It won't be as tidy as TypeScript, since you can basically just erase types from a TypeScript program and get back valid Javascript (not exactly true obviously, but you get the point..) whereas PureScript has some constructs that Javascript does not. I hope it's fairly easy to read though.
It's not a story, it's a presentation. The difference is critical. I want the information, but not the lowest-common-denominator setup and other fluff. Why should I have to invest an hour when I can get the gist of it in 15 mins?
&gt; just crash when someone folded a reasonably large list than fix the issue.. until it finally affected scalac itself Since this is the only point I can find to take any issue with, I will: that's not quite how it happened. It did affect scalac, and I did attempt to use that fact as ammunition, but I never needed any convincing about fixing it and I don't think that fact had any bearing on the result one way or another, because I'm not sure martin was ever convinced at all. It was finally fixed because the locus of actual decision-making had moved far enough westward that it could be fixed, at least in that instance. [Edit: here's a thread. https://groups.google.com/forum/#!topic/scala-internals/RrmfYQpTTfc ] &gt; but it runs on the JVM. The taunt is justified. Still, let us not conflate the compromises scala has made with the compromises which must be made to run on the jvm. The set intersects, but not heavily.
[this one is even better: Programming and Scaling, by Alan Kay @ tele-TASK](http://www.tele-task.de/archive/video/flash/14029/)
LiveScript also has similar goals to this project: http://livescript.net/ Not trying to imply anything here, just thought you might be interested in seeing this if you haven't already. 
Re: The resolution of that issue, I was operating pretty much on hearsay by the end, as I'd basically given up on it ever being fixed and just kept helping close issues from our non-FP-savvy staff who encountered it by having them switch to using the version from scalaz. I also freely admit some of the things in that list are completely unfair. I wasn't originally planning on this having er.. quite so broad a distribution, but all of these things are pain points I've experienced at some point working in scala, and they are all things that I don't suffer in ermine, which also works on the JVM with a much heavier encoding. I'm not saying Ermine is the right solution, it is nowhere near baked enough, and its down to about 1/2 a person working on it in the short term. I'm merely saying that "we have to do this to work on the JVM" is a bit of a cop out argument, which I can back by the fact that we don't do those things, and run on the JVM. ;) The compromises that scala makes that I don't think pay their way but which are set in stone are the lack of first class rank-n types and the lack of a kind system forcing type lambdas on you everywhere which impacts implicit inference, and the use of implicits over typeclasses meaning you are forced to endure loss of coherence of instance resolution leading to insane ad hoc rules about which instance is "better" in any given context. When you add Martin's intent to take the parts of it that do work (type parameters) and possibly replacing them with the parts that more often don't (existential members), I'm left with little to love.
&gt; Free theorems aren't. I will be giving a talk on free theorems using scala next month. I will be answering the question, "to what extent are they not?"
I haven't looked in detail, but I will, thanks!
The project looks awesome, congratulations! I was going through the exact same thing. I need to use JS at work, but am constantly frustrated by it and can't really use something like Fay, which compiles to really non-standard JS code. :) Guess I'll have something to look through tonight. 
Great, please let me know how it goes or if you have any issues.
I have been thinking lately about row-polymorphism and a javascript-transpiling language. My interest is in seeing if this would provide sort-of first-class ML functors, as a module would just be an object, or a function from object to object (as is tradition in javascript). I think having polymorphism restricted to the top-level would make this not work so well. I have thought of just having the top-level be sugar for an object itself, however this could make it a pain for you to provide typeclasses in the future, as you really do want a priveleged sort of namespace for those to be declared in. How do you propose to support modules in a way that would be friendly for external javascript to interface with? EDIT: for clarity
Will you post slides/video of that talk?
This did inspire me to wonder, however, if e.g. the improved/yoneda'd free monad IO construction or the like might not be more _efficient_ in Haskell, or at least equally efficient without the occasional confusion introduced by -fno-state-hack. Also if that or a similar construction would still let us model ST, fork, etc. in similar ways.
Excellent work. I may send a couple of pull requests your way. You could use more unit tests (as opposed to integration &amp; regression tests, of which you have a good number) and I noticed a few things that HLint could help with. I’d like to pick your brain about rows as well, for my own language project.
You need to be able to export types in your records to have first-class ML functors. This exists in dependently typed languages.
Yes for sure.
what's the canonical location to get information on what the principles of ermine are? short duckduckgoing did not get me the right page, only your compilers.
Looks really awesome. I like the general design: very simple and elegant. Since you have extensible records, have you considered adding polymorphic variants à la OCaml? I think it would add a nice symmetry to the language: both sums *and* products would have the same sort of subtype polymorphism. Also, I found polymorphic variants in OCaml to be *extremely* useful, especially for web work. I built a little library for type safe CSS (using js_of_ocaml) where polymorphic variants made it very easy to specify what values any given CSS property can take. Also, as part of the same OCaml project, I designed some nice, idiomatic and typed interfaces for jQuery, SVGs and FRP. The aforementioned CSS types were very useful here: I could plug FRP behaviors directly into jQuery elements, ensuring they had the correct types for whatever CSS property I was using. It all came together pretty well. Unfortunately the code I wrote isn't open source (yet), but I'd love to talk with you about designing your modules for jQuery and the like. 
That's a good introduction to compiled heist. The only thing missing is how to reuse HeistState between multiple sessions (i.e. http requests). Because moving the whole chunk of code with initialization and template loading would miss the very point of having compiled templates, isn't it?
I actually added [license tracking to packdeps](http://packdeps.haskellers.com/licenses), which should hopefully help people make decisions about which libraries they can use.
That looks very useful! Is this also available in the command line tool? That would save us some time tracking this stuff ourselves.
&gt; If it ever actually worked it would be great Except if you want to share code through a pastebin or an article&amp;mdash;the diverging syntaxes would give any code a lower readability than Perl. And there *is* a language where you more or less build the AST by hand&amp;mdash;LISP. What bugs me isn't the AST thing, it's his opposition to text. Language is the standard human concept interchange format&amp;mdash;when we want to model reality, we describe it with a language, either natural, formal or programmatic. Text is just a visual representation of language. It was a *huge* step forward for humanity when we invented language and became able to discuss abstract concepts rather than being limited to pointing, facial expressions, etc. Inventing text was pretty huge too, now we can send messages through time. It's how we're communicating now! I think language and text is pretty awesome, to the point that I'd rather read a transcript than have to listen to someone giving a talk. So when these Bret Victor types say they want a less textual representation of code, I fear what they'll come up will be [worse](http://zone.ni.com/cms/images/devzone/pub/nrjsxmfm912163998723206173.jpg)&amp;mdash;at least for me. Discussions about brace positioning isn't even a real syntax discussion, it's pure bikeshed colour.
&gt; Your syntax is 90% Haskell. Have you considered making it 100% Haskell and reusing the haskell-src-exts parser library? It's very good, and make it very easy for people to move from Haskell to your library. You wouldn't have to use the Haskell semantics, so your while could look exactly like it is now, but perhaps with a do instead of a :. Most importantly, it would save you the work of writing a parser. More reasons: * I can re-use all my editor and tooling support that I already have for Haskell. * Data-type re-use. Sharing the data types between server and client is very important. Even if the runtime semantics of Haskell and PureScript differ, your data types can be shared and thus kept consistent.
But brace and spacing discussions are stylistic discussions&amp;mdash;the placements are syntactically equivalent. If the rest of the syntax becomes style as well, it'll just exacerbate the problem of people discussing how to present code in the prettiest manner. Probably it'll necessitate something like CCSS&amp;mdash;Cascading Code Style Sheets. :p
There isn't much out there so far. We got permission to open source what we were doing and open sourced it so fast management's heads spun. The scala version is sort of a technology preview of what the thing looks like assembled. We wrote it first, but we had growing pains. We found that, in the end, we were spending more time fighting with scala than productively improving the compiler, so I started a greenfield Haskell version of the compiler in my spare time, designed to target a small common core that we could run on multiple platforms. The former is useful in that you can get to a REPL and play around with a basic version of the language. The latter is useful in that it highlights many of the ways I like to build a compiler, but its not yet done. The main novel contribution in Ermine is that it is a language that has been extended with row types, permitting us to talk about records polymorphically in such a way that we can do things like strongly type the kinds of joins on relations you get in SQL and the like. I did a talk at CUFP this year that was recorded, but hasn't yet hit the internet. I'm doing a couple of variants on that talk over the couple weeks. I may be able to record the version I'm doing in Slovenia for http://www.meetup.com/vblatu/ on the 14th. We primarily use an embedded DSL inside Ermine to generate financial reports such as factor backtests and portfolio attributions in a way that can be customized easily in production by folks farther out in QA rather than core developers, and eventually more and more by the top 2-3% of "power users" among our end users.
see [my comment](http://www.reddit.com/r/haskell/comments/1pjjy5/odersky_the_trouble_with_types_strange_loop_2013/cd3gk9f)
Additionally; What are some good resources to learn about compilers and compilation? Either specifically GHC (an overview of its layers and abstractions), or perhaps the name of or a link to the best formulation you have seen of the mathematics involved in compilation and the problems that follow.
You can go rummage through my old toy http://comonad.com/haskell/security-policy/test/Policy.hs and replace `Classified` and `Secret` with `BSD` and `GPL` respectively, then try to figure out to convince the lawyers. ;)
This is awesome! Having two syntaxes for function calls proved suboptimal for scala, might want to just pick one. Also: why not use the haskell FFI syntax? It's almost the same as yours, but a tad more extensible. It's not clear to me if your do blocks are typed in a way that you can tell impure functions from the types... or are they impure mutation only? In which case, how do you structure DOM manipulation, etc? The syntax of do blocks seems to break from the rest of the language a lot... not sure about that. Pattern matching directly on arguments instead of needing case statements would be really nice sugar to add. RankNTypes would also be awesome, though I understand that's a lot of work.
No need to be condescending.
Good overview of GHC http://aosabook.org/en/ghc.html
Where do you live? What qualifications do you have? Don't just be an idiot, actually provide us with information if you want actually useful answers.
I just want to say I'm not advocating this whatsoever. I think it would fail hard and it is a waste of time to try, but... You wouldn't share code through pastebin, you'd share this ast language instead, then open it in your code formatter, and it would look the way you prefer it to look. If you like it in text, then it's in text. If some other means of interacting with it came along, like some sort of 3d or neural interface you might interact with it that way instead. Same code, different interface. Of course it would never work, but it is nice to think about.
Had to cut off the long hair to make room for the runway.
It is not. 1) Dynamic typing
Yes, those F-18s will play havoc with your coiffure! 
Sure, we'd use different tools for sharing code in general, but we often share just snippets and incomplete pieces of code. We even share it here on reddit, between backticks and indented with four spaces. Would it be as trivial to share trivial pieces of code if what he have is an AST, and no agreed-upon method of displaying that in a browser? We'd have to have something like `&lt;code src="/path/to/ast.ast"&gt;alt&lt;/code&gt;`.
Presumably you'd just paste the ast, or a link to it, and the person would just click on it and his browser would use his local dev tools to display it, or some default method like a text representation. For learning, people would just use the most popular syntax at that point in time, which might be a certain style of text if that was in fashion, or something else if it wasn't. Presumably you could even just display this language as some sort of easy to read pseudocode or summary view which is not enough info to compile down into anything useful, but could give a human enough info to infer meaning from it.
Regarding two function syntaxes, I wanted to make it as easy as possible to use existing Javascript libraries, so tried to keep the multiple argument notation, but I agree, it is a bit of a wart. The more I think about it, I see most libraries getting wrapped before getting used anyway, so they could be made compatible with whatever syntax/type system seems best. In which case, I'd be more than happy to ditch that particular syntax. Nothing's set in stone at this point. The same goes for the FFI syntax. To implement the do blocks, I keep a separate environment of mutable variable names. It's a pretty specific way of doing things, and I'd like to find a nice general way of tracking effects such that the generated Javascript stays pleasant. I had also thought about just annotating types with their effects, but not followed up on it. I would like top level pattern matching, and RankNTypes. In fact there are issues on GitHub for both of them. I think RankNTypes, or at the very least PolymorphicComponents would make a lot of things really nice, like type classes and modules. I'm more than happy to accept pull requests ;)
If you have free time, the best way to begin is to learn off-the-job. All the programming tools you need are free and (if you have a linux distribution with a package manager) really easy to obtain. The best thing about programming is that you don't need permission from anybody to design your own language. Just look at [paf31](http://www.reddit.com/r/haskell/comments/1pkzd0/show_reddit_my_weekend_project_purescript/) who wrote his own language on his own time.
I had considered adding polymorphic variants, but didn't have a good use case in mind. Your CSS example sounds really neat, and I think it would be nice to have the symmetry as you say. It should be pretty simple to use the existing machinery to add polymorphic variants, but I'm guessing.
&gt; before I can actually try it out, it would need to support jQuery out of the box What would you use jQuery for in pure code?
Why is Fay's output an issue for you? In case you want to write libraries to be used from JS, have you seen [Calling Fay from JavaScript](https://github.com/faylang/fay/wiki/Calling-Fay-from-JavaScript)? 
One of these things is not like the others: Simple, Compositional, Reification, Monadic :-)
&gt; Excellent work. I may send a couple of pull requests your way. By all means :) &gt; You could use more unit tests Right now, as you say, I have some basic cabal tests (does file X compile or not) and a very small set of tests using `shelltestrunner`, which use `node` to assert some console output from the compiled Javascript. These give me some confidence that things work as I expect, but unit tests would definitely increase that confidence. I also toyed with the idea of using QuickCheck, for things like `parse . prettyPrint == id`
haskell-src-exts is awesome. It's a big syntax tree, so I would suggest you use a generics library with it (http://skillsmatter.com/podcast/home/uniplate), but I have used it in loads of projects. I understand the temptation to write PureScript in PureScript, but my view would be that Haskell is the perfect language for everything, but PureScript is for when you want to run in a browser - hence a compiler still makes sense in Haskell. Entirely up to you where the project takes you, of course, and I'll be on the look out if you ever get to jQuery support.
As I say, Purescript-in-Purescript is becoming less of a goal for me, I've been really impressed by the ability to add compiler features quickly in Haskell. Regarding generics, right now I'm using `syb` which I've quite enjoyed, but I'll look into `uniplate` as well, thanks.
The benefits from no-generics to any-generics are massive. From one generics to another is a lot smaller, but uniplate is likely to be faster and simpler than syb for everything uniplate can do, and for the occasional thing that uniplate can't do you can always use syb.
Just to make the water more murky, if BSD code includes GPL code then it is not governed by the GPL until it actually changes hands. The person who did it is free to use GPL and BSD however they like but the rules change when they give it to someone else.
So `Program a` uniquely identifies an `a`? Is that the condition when this is possible.
"Simple". It's the only of the four you find in marketing materials and user guides.
There was a comment made at the ICFP presentation of this paper (IIRC by /u/edwardkmett) that this is "solving the problem" of using monads when you should be using applicatives. I'm not sure how much I agree with the comment, but I thought it would be good to mention it.
You could add it to Safe Haskell! Legal attacks like lawsuits are threats to security just as much as breaking type safety...
Monadic. It's the only one with a 'd' in it.
I don't understand why Cond :: Program BoolE -&gt; Program () -&gt; Program () While :: Program BoolE -&gt; Program () -&gt; Program () The following would fit more naturally into the monadic interface and are equally suited to be interpreted with `compile`. Cond :: BoolE -&gt; Program () -&gt; Program () While :: BoolE -&gt; Program () -&gt; Program () 
It's also the only one derived from Greek. The others are derived from Latin.
No. BSD code becomes GPL if and only if the copyright holder licenses it under GPL. 
Let me summarize why this works. ~~I think the treatment I'll give here is actually simpler than the one in the paper.~~ (actually at the end they do actually make it clear they know that this is just a free monad with interpreter, but don't say so explicitly). data Prg = PMove | PTurnRight | PTurnLeft | PSensor Name | PCond BoolE Prg Prg | PWhile Name Prg Prg | PSeq Prg Prg | PSkip | PAssign Name BoolE is isomorphic to `Prg' ()`, where data Prg' a = PMove | PTurnRight | PTurnLeft | PSensor Name | PCond BoolE (Prg' a) (Prg' a) | PWhile Name (Prg' a) (Prg' a) | PSeq (Prg' a) (Prg' a) -- PSkip not needed here. -- Its functionality is provided by Return | PAssign Name BoolE | Return a `Prg'` is naturally a monad with `return = Return` and `bind` is substitution. (In fact it is a free monad). All that the `Program` type does is hide the constructors. The only constructors we get benefit from hiding are PSensor, PWhile and PAssign, because they deal with variables. We'd like to be sure variables are in scope whenever they are looked up. I believe `Program` is then just a unique supply monad transformer over `Prg'`. It doesn't need to be a GADT. `Move`, `TurnLeft` and `TurnRight` are easy to write simply as values in the `Prg'` monad lifted into the transformer. The other three "constructors" of `Program` deal with variables and need to use facilities provided by the unique supply. All these operations will basically be doing the work that `compile` does in the paper. I await Tekmo's assessment of my summary :) 
That's precisely what the Heist snaplet takes care of for you. But even if you use interpreted templates instead of compiled templates, you still want to reuse the HeistState so you don't have to read all the templates from disk and parse them every time.
I assume there's still the issue of "blindness" to the licenses of wrapped C-libraries? (which is actually a problem of missing meta-data in the .cabal file) (e.g. there were some packages that wrap LGPL code, but the Haskell part is BSD3, so that's what's declared in the .cabal file)
He is referring to the combined work of GPL and BSD code, which can only be GPL licensed when distributed. Yes, that does not revoke the license of the original BSD parts, but any modifications to those parts are not necessarily governed by that original BSD license, so it effectively becomes a GPL licensed work, unless unmodified.
It is true that the combined work would be licensed as if every part of it was GPL. But there is a persistent myth on the internet that the GPL is somehow "viral" and can force non-GPL code to become GPL. Re-reading the comments i replied to more carefully i realise that they were not spreading this myth. My apologies.
The HTML CV looks fine, but the pdf version leaves quite a bit to be desired in my opinion. Is this remedied by learning better markdown?
&gt; Not true unless you cast explicitly yourself somewhere, and that is an open lie. It arises when you are working with GADT-like constructions on a fairly regular basis. &gt; Mind to give an example. That is a very broad claim. One of the properties type classes have is that you have coherence properties for them. e.g. if you have an instance of `Ord Int` it is the same dictionary no matter where you got it from. You can rely on this invariant in data types. Haskell makes a lot of use of this to move the dictionary out of the container and move it to the use site. This makes it much easier to make a data type, like say a monad transformer, that just improves the capability it offers when you improve the capabilities of what is contained in it. It also lets you work with things as instances of more general classes like Functor/Foldable, despite the fact that you usually intend to put things in them that belong to a more restricted class of objects. This is a very general pattern and in many way describes the difference in programming style between Haskell and ML. In Scala, because of the extra power you are granted by implicits you can pass anything you want to an implict argument. This is one of those things that sounds like on the surface it would be a good idea. You get more power right? But as Paul points out true freedom does come from restriction in this case. You can't know you'll always get the same dictionary everywhere. This means that container types like Haskell's `Set` which can use asymptotically faster operations to merge can't do those things in scala, but the phenomenon extends a lot further. All of scalaz is written in a form that assumes you will always receive the same implicit given the same type. Effectively the entirely library is written by pretending implicits give you coherence guarantees they do not. &gt; You only need CanBuildFrom when you essentially want to change the collection type. The `CanBuildFrom` issue arises when you write fairly polymorphic code on the result of the map. Given the tone, I'm not going to bother with the rest.
If you were working on a code base with both GPL and BSD code and wanted to limit the percentage of code under GPL when distributed this would be a boon.
One of the things the GHCi debugger gives you is a way to essentially insert Debug.Trace.trace on the fly without modifying and recompiling your code, or deriving Show instances.
&gt; For my own use case, I'm not sure if I will be using JQuery directly from PureScript, but I think it will still be the first library I try to write. This is going to involve figuring out the IO story for PureScript first, yes?
I would personally go for allowing variables to have side effects - effect systems are complex and take you further away from Javascript, unrestricted side-effects (+ discipline) are a much more Javascript way.
:-). It's not a word.
Out of curiosity, how does generics help with big syntax trees? Automating traversal? 
See the linked video in the post above. As /u/ndmitchell says, it helps to separate the dull code (boilerplate pattern matching) from the interesting code (special cases) in such a way that the dull code becomes much smaller.
I feel like it's easier to just install a real `gcc` and use that. How does the wrapper compare speedwise?
Compared to the speed of the compilation, it is lost in the noise.
Fair enough. I was compiling with a clang wrapper right after moving up to Mavericks and compile times were significantly slower. If that's not the case now, I'm all for it.
What if you just had one single IO monad? Fay only has one monad (Fay) and we get away with it.
Ah, if you're comparing clang+wrapper to gcc I can't tell you. I was referring to the extra time the wrapper adds. On my machines (various MacBooks) I see no worrisome difference for moderate sized projects (60+ haskell files, TH, cpphs) between clang+wrapper and gcc. But mind you, I haven't timed it, just built from scratch and didn't notice any issue.
I considered the idea of compiling things like `put` into efficient Javascript, but then those functions need a special status. What if, e.g. you pass `put` to a higher order function? Edit: Maybe a solution is to implement it as a rewriting step on the Javascript AST before emitting Javascript code? I think you're right about function args. I previously had object property and array element mutation (`o.foo = bar; a[1] = baz;` etc.), but then you need to copy the mutable input to stop mutation being visible elsewhere. For that reason I got rid of it, and now there's little reason not to let args be mutable.
That code has been through google closure and other optimizations, you might deploy it like that, but you should develop with the pretty printed version. The trickiest part in debugging is when things go wrong in type conversions (bad ffi declarations or people passing incorrect values from JS). We're working on improving that. Source maps support in browsers isn't top notch, I'm hoping that will be improved over time (but I haven't experimented that much with it). 
See also units of measurement. https://github.com/mcandre/safeunits/blob/master/safeunits.hs
Someone would write _unsafePerformGPL_ and then boom.
Are techniques that rely on laziness in order to design efficient amortized/worst-case data structures a strong argument for lazy-by-default structures even in the case of well-founded data?
Well, you get things like finger-trees that need to have a lazy spine or their cons/snoc slow down to O(log n).
Need to, or are most easily/intuitively implemented as such? The major argument against laziness seems to be that while it may (rarely) allow for a simpler implementation, it is never strictly (heh) needed.
Need to. There are problems for which the call-by-need version has better asymptotics than every possible call-by-value version written in a pure language. Fingertrees _are_ actually one of those things that cannot be implemented with their proper asymptotics in a pure strict language. You're relying on accesses that goes down into the structure later to force work that you deferred earlier during an `O(1)` `cons` or `snoc` operation. Call-by-need is providing you with a limited form of mutation there that is necessary to their proper function.
&gt; Some jealousy there from the Haskell folks, I read. Did you even read the rest of ekmett's point there?
It's a strong argument for having laziness available, but not so much for laziness as the default. Okasaki's book uses explicit thunking and forcing, which makes the analysis (though not necessarily the code) easier to follow.
You should look then at fay; the author is intending this to be used in purely functional libraries; fay as a trivial interface to jquery
Don Stewart made a pretty compelling case for data structures being at least spine lazy by default in his [Haskell Cast Episode](http://www.haskellcast.com/episode/002-don-stewart-on-real-world-haskell/).
Why not go aggro and not deal with IO at all? Having a Haskell that compiles to readable javascript but can't do IO, especially since Fay already exists, actually seems like a reasonable trade off.
Sure but that's doable in a strict language with optional laziness. It just might be somewhat more messy, syntactically. 
Well, you have to be careful there. If you have a language like scala where you get shackled to the java stack model, even adding a `lazy val` here and there won't help you. I used to think I wanted a strict by default language, too. My experience isn't that its somewhat more messy, but rather than its a slaughterhouse to get the level of laziness that makes it useful. YMMV but one of the biggest wins of haskell for me is that algorithms written by different people compose well. Those that are put together in a strict-by-default environment are often strict enough that they just don't compose without "time leaks", so you wind up reimplementing the whole thing over again, entangling the two implementations by hand, because its easier than dealing with the laziness annotations, defeating the whole purpose of the exercise to me. You "could" build a culture in a strict language around structures with lazy spines and get things that compose, but I can't point to a single language out there that _can_ do this that does. The 'quick and easy path' in that setting leads to mutation and folks tying the knot with mutation and nulls, not lazy values and bottoms.
Sure, by "that's doable in a strict language with optional laziness" I don't actually mean it's doable in *every* such strict language. I don't even know if there's *any* such strict language where it works! I really mean something *much* weaker, like "that's doable theoretically in a strict language with optional laziness whose implementors gave due care to the implementation of the lazy parts" :) I don't know whether such a language exists, but I know such a language *can* exist because it could simply be syntactic sugar over Haskell compiling to GHC. [EDIT: GHC rather than simply Haskell, because there's no *guarantee* any given Haskell implementation will support laziness well! On the other hand we already know that GHC does :) ] 
Yes I did but deliberately based my interpretation on the way he starts the argument. As I am anyway being downvoted against a Haskell semi-god who's word is the truth, I don't actually waste my time on this thread any further. Happy Halloween people. It would be interesting to do a linguistic analysis of these kind of posts.
I can't watch the episode now, but I find it hard to believe given that most of our data structure (e.g. all of the containers and unordered-containers) are spine-strict. At the very least spine-strictness give you worst case (instead of amortized) complexity bounds.
&gt; You "could" build a culture in a strict language around structures with lazy spines and get things that compose, but I can't point to a single language out there that can do this that does. Doesn't clojure use lazy streams by default in most APIs? 
I guess it comes down to whether you consider that sufficient that most algorithms compose well. To me that just means that you can often contort your thinking into streams / generator and those compose well. This is one of the reasons why generators are so popular in python. It is a nice start, but having used it, it doesn't get me what I'm looking for.
Thanks! Fixed.
From the compilers POV, laziness is just an implicit data structure transform with some implicit algorithm transforms. You don't need laziness as a compiler feature at all, you could apply the same transforms yourself. Obviously having the compiler do it is convenient when it's what you need, but e.g. when people claim that pure functional programming in Haskell is good for parallelism because there's no mutation so no need for cross-thread synchronization, they're forgetting that the thunks themselves mutate. And analysis of data structures depending on laziness even means you need to be aware of the data structure transform. The point is not to say Haskell-bad. The point is a question I've been wondering about recently. A persistent data structure is one that remembers past states of that data structure, so queries can be performed on the remembered past states. Obviously, pure functional is one kind of persistence, but more generally a persistent data structure doesn't preserve the representation of past states, only the meaning. A persistent data structure may (and often does) reorganize data that past states reference for time and space complexity reasons. There are also retroactive data structures - you can not only query the past but also change the past - but for what I'm thinking, that'd be a violation of referential transparency. What I'm thinking is that maybe there are special cases where library programmers should be able to modify the data structure and algorithm transforms applied by the compiler for laziness - to allow programmers to exploit persistent data structures to get better performance guarantees, preserving referential transparency semantically but not representationally, so that forcing one value can actually change the representation that another (past state) value will be computed from in a more general way than just forcing some thunks. Obviously that would be a very sharp-edged tool, but in expert hands it might offer significant performance benefits. For some information about persistent and other temporal data structures, the lecture videos and notes for the first few lectures of [MIT 6.851 Advanced Data Structures](http://courses.csail.mit.edu/6.851/spring12/lectures/) are a good source - and unfortunately the limit of my knowledge of the topic for now. 
Does Disciple maybe qualify for this?
&gt; You "could" build a culture in a strict language around structures with lazy spines and get things that compose, but I can't point to a single language out there that can do this that does. C# and Linq, although I'm not clear on whether it has exactly the structure you describe.
Personally I don't know. Although people say Disciple is a lot like a strict Haskell, when I read the introductory paper it seemed considerably different. 
&gt; I used to think I wanted a strict by default language, too. I'm still not 100% sure, but I'm definitely moving more towards laziness-good over time. &gt; YMMV but one of the biggest wins of haskell for me is that algorithms written by different people compose well. Compositionality is good, but it's also that laziness is something you rarely need to think about for correctness. Stack overflow means your code is broken, sure, but while avoiding that is a bit different than in a strict language, it's not a big problem. BTW - between you, your `free` library and its dependencies and [Andres Löh](http://skillsmatter.com/podcast/scala/monads-for-free), I've finally been forced to start learning category theory. I hope you're happy with yourself &lt;glare&gt;
&gt; if you have an instance of Ord Int it is the same dictionary no matter where you got it from. Not true. Even in Haskell98 you CAN have two different instances of Ord T (for some type T) in different parts of your program; moreover, this could lead to invalid Set's. That's exactly why we need "orphan instances" warning.
On the other hand, there is something to be said for all generic data structures to be value-strict by default, since you can always make them lazy again by wrapping values in a lazy box.
Interesting - I'll take a look soon. 
I still don't think laziness by default is the end of the story, though I think it's the best option at the moment. The important thing is being able to conveniently write code which is polymorphic in its strictness. When you write higher order code, this is crucially important (you don't want a combinatorial explosion of HOFs for every possible combination of strictness). Laziness by default gives you this sort of polymorphism, but I think of it as kind of a hack. I'd be interested in a language where evaluation were tracked (conveniently) as an explicit effect, and one could explicitly write HOFs and algorithms that were polymorphic in how they evaluate. Such functions could then be instantiated lazily (for composability) or strictly.
Linq is really more about sending the computation down by reflecting on its structure. It is a mishmash of the generator story and an EDSL describing a subset of the queries that can be executed remotely. Across Linq providers things don't compose very well. I say that as someone who used to sell such a provider, before moving on.
Indeed, and I've often wondered whether Haskell's strict and lazy `Map` are just the same strict datastructure with the values of the lazy one implicitly wrapped in such a lazy box. 
My experience is that in a strict language, pure or not, you wind up wanting to entangle the details of both algorithms you compose to get one with better asymptotics. This isn't a correctness issue at heart so much as a composability one. I have to be smarter to work efficiently in a strict setting. I have to know how to hold all the details of both algorithms in my head simultaneously to get the right asymptotic behavior, In a lazy language I can spend more time obsessing about getting the details of the library right once, so that users can just slot it in and not care about how all the sausage is made. This raises the abstraction ceiling in ways I find very pleasant. YMMV
The performance hit for the extra explicit box makes that option highly upsetting to me.
Can you give us an example of two such algorithms that are easier to compose in the lazy setting?
I am surprised that optimization wouldn't deal with that automatically.
If only you could convince the compiler to make the box implicit. You can still write containers that are generic in the strictness, though class Strictness s where seqBy :: Proxy s -&gt; a -&gt; b -&gt; b data Lazy data Strict instance Strictness Lazy where seqBy _ _ x = x instance Strictness Strict where seqBy _ = seq -- strict or lazy list data List s a = Nil a | Cons a (List s a) -- smart constructor, write everything in terms of this cons :: forall s. Strictness s =&gt; a -&gt; List s a -&gt; List s a cons x xs = seqBy (undefined :: Proxy s) x (Cons x xs) I wonder how this would perform in practice, and how inconvenient it is to use. And ideally the compiler would do all of this automatically, and allow you to just write `[!Int]`.
Slides are available here: http://yowconference.com.au/slides/yowlambdajam2013/McDonell-GPGPU.pdf and a lot more info on the talk can be found at: http://www.yowconference.com.au/lambdajam/Speakers.html and then click on "Trevor McDonell" (end of the page). From that page: **Accelerating Haskell Array Codes with Multicore GPUs** *Workshop* Current graphics cards are massively parallel multicore processors optimised for workloads with a large degree of SIMD parallelism. Peak performance of these devices is far greater than that of traditional CPUs, however this is difficult to realise because good performance requires highly idiomatic programs, whose development is work intensive and requires expert knowledge. To raise the level of abstraction we are developing a domain-specific high-level language in Haskell for programming these devices. Computations are expressed in the form of parameterised collective operations —such as maps, reductions, and permutations— over multi-dimensional arrays. These computations are online compiled and executed on the graphics processor. In the workshop, I will go into more detail about how to actually write programs using Accelerate (with some advice for making these programs efficient). This will work up from the basics; types in the language, arrays and operations on them such as map and fold, building towards development of a range of larger example programs such as a Mandelbrot fractal viewer and an N-body particle simulation. *Prerequisites:* None required. However, if your computer (nix/mac) has a NVIDIA graphics card, you can follow along with the examples in the workshop by installing the CUDA SDK: http://developer.nvidia.com/cuda-downloads as well as the Accelerate package from github: https://github.com/AccelerateHS/accelerate https://github.com/AccelerateHS/accelerate-cuda For those who wish to follow along but do not have the right hardware, you can use the interpreter from the base package to run Accelerate programs without using the GPU (much more slowly of course). The most recent version of GHC is recommended (currently 7.6.2). 
My old bloom-filtered version of the COLA actually aggressively used lazy indexing, so that if you didn't use a bloom filter you never paid for it. This made insert-mostly-then-read-mostly workloads much cheaper. In this very series I'm in the process of writing a series at the moment on how I can obtain optimal B-Tree-like asymptotics, while working in a purely functional lazy setting. The design of this algorithm makes heavy use of laziness to get the right worst-case asymptotics. The design of the deamortization scheme is based on a technique by Overmars and Van Leeuwen, that is more or less completely independent of the static data structure I'm building. I don't have to be "smart enough" to fully hold the details of the static structure in my head while I work on the deamortization scheme beyond knowings its gross asymptotics. In a strict setting I have to intimately mingle the two implementations.
Looks good to me! I'm not sure what's best to represent the effects either. Ideally it'd be something comfortable for haskellers to use as well as something that generates clean and simple to debug JS. 
As I'm going through how to make cache oblivious structures into purely functional ons more and more I've been focusing on how I can put elements next to each other in memory and still remain lazy, not just chase an extra pointer. I'm not yet there across the board, but I can start to see how more and more of these structures fit together. In that setting I can have an appropriate Array class that specifies if something is being stored boxed or unboxed and defer most of that reasoning to the instance that way. It is a rather different scheme, but I have to say, it is one that I've so far rather liked the effects of. Even the best traditonal "pointer model" style data structures will wind up with log factors they can't kill here and there. With the oblivious packed form I can start dividing more and more them out by the block size or the log of the block size.
* automagical [playlist for videos in this post](http://radd.it/r/haskell/comments/1pp66m?only=video) *^Downvote ^if ^unwanted. ^Comment ^will ^be ^removed ^if ^score ^is ^0.* [^Message ^/u/radd_it](/message/compose/?to=radd_it&amp;subject=Please Blacklist&amp;message=Good sir, I appreciate your efforts but do not require automagical playlists. Please add me to your bots blacklist.) ^to ^never ^receive ^comments ^from ^this ^bot.
I meant specifically Linq-to-objects, which is the functional API on IEnumerable&lt;T&gt;. I wasn't referring to Linq expressions.
Very cool! I would love to see more of these.
You might enjoy [this thread](https://groups.google.com/forum/#!topic/fa.haskell/crXl7KQAaSo) on the Haskell Cafe mailing list. Stefan demonstrated something that I think is very much like what you are talking about, though with different primitives. He introduced a typeclass `Fun`, which abstracts over function application and lambda creation, which can be strict or lazy. With nicer syntax for it, and it being built into the language, it might be quite usable!
It's different in many ways. I don't know / can't remember the details, but I think they have something where the language is default-strict, but you can explicitly suspend thunks, and in that case they won't be forced just by passing them as arguments to a function, only when their value is needed.
Their convex hull algorithm dominates search results. From Overmar's publications page, would http://www.cs.uu.nl/research/techreps/RUU-CS-80-10.html be the technique you have in mind?
Wow, that's a great thread! Thanks for the link. It's very interesting that Stefan introduced his lazy/strict distinction by through this function-like abstraction. It means that it can be directly implemented in Haskell as-is, unlike my idea. However it seems to have the drawback that it doesn't distinguish lazy and strict *datatypes*.
In general as with generators that side of linq only helps you fuse together a big traversal for the streaming parts of your problem. If your algorithm isn't about streaming things, but is instead just going in and looking at some part of say a big financial model, you get no benefit.
I have heard you talk about "quotient"ing several times now in contexts where I can't make heads or tails of it. Could you explain what you mean here by quotient, or point me in the direction of resources that could help me understand this.
That is the one! You can find a summary that is perhaps more readable to modern eyes [here](http://www.cs.uiuc.edu/~jeffe/teaching/datastructures/notes/01-statictodynamic.pdf) in the second section after he goes through Bentley-Saxe.
Cool. I didn't know that.
is it an open workshop? can anyone simply walk in? 
Thanks, that helped a lot. When dealing with a free monad, their chosen ~ is isomorphism under codensity, I think. Is that what you are getting at?
Codensity will reassociate all binds to the right, so yes that would be one way to describe it.
So basically, using a free monad proves that this quotenting is sane and you don't have to do any more work to make sure it holds.
With the free monad you don't need to quotient your you inspect the result, two 'equal' trees actually compare as equal structurally With `bound` you can do the same thing and compare for alpha equivalence with structural equality. Neither of these are quotienting, they are just picking data structures such that that are 'exactly the right size'.
Any good tutorials/articles with some non-trivial examples on how to effectively use the debugger in ghci? The interface seems... clunky to say the least. I can insert a trace and quickly recompile a single module faster than mucking about with trying to figure out the debugger and understand its output at the moment. However, as I'm moving on to larger projects in Haskell with many more moving parts I'm finding myself looking for a better solution than trace.
I don't know why it's bothering me so much but he really needs to keep his hands out of his pockets when he's presenting. He looks like he's in a straight jacket most of the time.
Not specifically Haskell related, but the keynote for the conference was given by Manuel Chakravarty who has made many contributions to the Haskell ecosystem. Title: Do Extraterrestrials Use Functional Programming? Video: https://www.youtube.com/watch?v=gUZYHo_nrVU Slides: http://yowconference.com.au/slides/yowlambdajam2013/Chakravarty-Extraterrestrials-Keynote.pdf Description: Is functional programming just the result of clever language design? Are there deeper reasons for the effectiveness of the paradigm? Why has functional programming not caught on earlier? In this talk, we will have a look at the roots of functional programming, at their contribution to the success of the paradigm, and at the lessons we can draw to maximise the benefit we derive from functional languages. I will argue that the core of functional programming is a principled approach to software design compatible with both rigorous and agile methods of software development. It stems from a desire for purity, composability, and elegant logical properties, and I will outline how to leverage these ideas to solve practical programming problems. 
It's going to be interesting to see him implement NFA to DFA and DFA minimisation in Haskell.
If you're interested in those things, the Alex source code is pretty good.
Very cool! I'm glad this method yielded the beginnings of some tests. I can't even estimate how good of coverage it might yield, but the method of intermediate representations is a clever technique.
I do not think it is easy at all. I do not know of a widely published software/hardware stack that automates it either. I think this is at least one reason why more online presentation videos are not like the ones at infoq.com.
I feel pretty contemplated in /u/paf31's response. I need to be able (at work) to share my code with my co-workers, and if I'm going to use a language that compiles to JS, it needs to compile to really readable and preferably simple code. 
OP here - fyi I am not the author of the videos. I just happened to run across them on youtube.
Do you plan on merging this into `containers`?
Everything was very nice to read except unionWithKey.
Ha, this reminds me of conducting meta-meta-meta analysis on stability of stability of stability estimates of statistical estimators. I think even without being able to talk about coverage, the intermediate representation generation is a good tool for collecting "challenges" to the abstraction. You can collect properties already, but when someone wonders if a property holds correctly "when X happens" it could be added in a canonical way to the intermediate language and garner evidence from the QC suite.
Yes, I know. I'm hoping to come up with some sort of a `mergeWithKey`-like function, so that I don't have to write that again. That function is a mess.
Great! The speedup is much bigger than I anticipated! I was hoping for 20%.
What does this particular line/notation mean? I'm having difficulty understanding it. &gt; So, anywhere along the list, one can grab the pair, and the tree falls out of the structure: &gt; &gt; (a, |-&gt;) -&gt; (b, |-&gt;) -&gt; nil 
Actually, when I originally implemented it, I got a slight slowdown. I think this slowdown was due to the fact that I was benchmarking with dense maps, where the more accurate `nomatch` doesn't help at all. The thing that sped it up so much was the fact that, with this representation, you can share some of the work of deciding which path to take down the tree (see the `xorCache` variable). I have yet to devise benchmarks that test how much the more accurate early failure helps.
The cache makes a lot of sense. I do something very similar in my `sparse` code, so its not surprising it helps here.
Have you checked coverage with hpc? In case you're missing something obvious?
I found this blog written in a very unreadable style, even though I understand most/all of the concepts being discussed. I'm not sure if the target audience is any larger than just the author.
Awesome. I particularly enjoyed the clever generation methodology.
First of all, negative numbers are put at the end, but not in reverse order. Secondly, I would much rather check if the root branches on the sign bit than biasing the tree, as the latter involves an extra cost on all functions, not just the traversal functions. However, it does have the advantage of not breaking into the WordMap abstraction. I'll have to think on this, but I think I will implement the sign bit checking. Edit: I implemented it.
This is probably a dumb question, but why are `member`, `notMember`, `lookup`, and `findWithDefault` all implemented separately? Is there a chance for optimization in the different strategies? Is the extra `isJust` expensive in some way?
The primary reason for splitting up the implementation was that I saw that stock `Data.IntMap` did it, so I figured that there must be some sort of advantage to it. However, `member` and `notMember` can actually terminate earlier than `lookup` and `findWithDefault`, as the keys are stored higher up in the tree than the values - because of this, `member` can terminate as soon as it finds the relevant key, while `lookup` has to continue down the tree to find the value.
I think this will work: cabal unpack wai-extra somewhere and edit RequestLogger.hs to remove the comments on both line 161 and 166. Then compile and install. The problem is those lines are comments and contain the sequence &lt;star&gt;&lt;slash&gt;&lt;star&gt; -- which I think combined with the CPP extension is causing the commenting out of the Haskell comments to get confused!
I'm guessing it's probably [this](https://github.com/connermcd/dotfiles/blob/github/.vimrc).
Very cool work, Gabriel! 
When you have data declarations I'm pretty sure you want the constructors listed in decreasing order of usage. You seem to have done the opposite. You should have Bin before Tip and NonEmpty before Empty. Apparently that can improve performance by up to 10%.
In extreme cases where neither source files are under the developer's control, this warning can be disabled with the `-fno-warn-orphan-instances` GHC flag. I had to do this once where the class and the type were both in an external library.
You can even contain this evil flag to one file with `{-# OPTIONS_GHC -fno-warn-orphan-instances #-}`.
It gains me two things: 1. As you mentioned, if I had a single type, I would have to avoid putting `Empty` nodes in the middle of the tree. By splitting the type, GHC checks that invariant for me. 2. More importantly, the root node, unlike all the other nodes, needs to have both a maximum and a minimum. The extra type allows me to make that distinction.
Thanks! :)
Thanks. I'd heard of this before, but didn't think of it. Also, do you know why this is true?
Joshua Dunfield and I had a paper on implementing higher-rank polymorphism in the most recent ICFP, *Complete and Easy Bidirectional Typechecking*. In addition to being very easy to implement, this algorithm also yields excellent error messages. Daan Leijen's recent work on Koka (see *Koka: Programming with Row-Polymorphic Effect Types*) describes how to repurpose row polymorphism to do effect inference. Since you've already got rows, this might be a good way to track effects with minimal additional machinery. If you're interested in pattern matching, then I have a POPL'09 paper (*Focusing on Pattern Matching*) which describes a simple coverage checking algorithm (including clausal definitions) along with a somewhat over-complicated pattern compilation algorithm. A much simpler algorithm (which does both coverage checking and compilation) can be found in [the following blog post](http://semantic-domain.blogspot.co.uk/2012/08/pattern-compilation-made-easy.html). 
If I recall correctly, Fay is being used in production by FPComplete. Please correct me if I'm mistaken.
You're not mistaken, we're using Fay in production.
Others have explained what's the part of the code that is triggering the warning message. However, I think the reasoning that continuational has given is not that correct. Actually the compiler can check for conflicting instances and in that case it issues a proper error during the compilation of the head of the diamond, not a warning. The whole orphan module business is a performance optimisation in GHC. It's not about runtime performance, but compilation performance. The more orphan modules are imported from a source file, the more slowly the compilation will be. More concretely, the interface files for orphan modules have to be loaded even if they're only used indirectly. So in your example, having orphan instance in the example is no big deal, because others won't import your example. Having orphan instances or orphan rules in a library is a bit worse, but actually you can find examples for that even in the base libraries. You can read more here: http://www.haskell.org/ghc/docs/7.6.3/html/users_guide/separate-compilation.html#orphan-modules If you have more questions after digesting 4.7.12 from the manual, I'm happy to help.
I never got around to, but I wanted to implement an Intmap where the node size increased by depth. The idea is that mutation frequency differs and thus memory*time. When memory*time is small, small objects and low fanout wins. When memory*time is large, large objects with large fanout is better.
Silk.co uses it in prod as well I've heard. 
&gt; Is there a way to leave classes open, encouraging users to define their own instances? Classes are always open in the sense that users are encouraged to define their own instances for their own data types. Classes are intentionally *not* open for data types *not* defined by the user. The result of that is orphan instances, which are considered bad.
The problem is in the line sqrtInt :: Int -&gt; Int sqrtInt = floor . sqrt . fromIntegral fromIntegral converts from an Int (in this case) to any type `a` with a Num instance. sqrt takes any type `a` with a Floating instance and gives you an `a`. floor takes any type `a` with a RealFrac instance and gives you (in this case) an Int. The warning you're getting means that `a` isn't uniquely determined here: there could be an arbitrary number of types with Num, Floating, and RealFrac instances. There are in fact at least 2: Double and Float. There are multiple ways to fix this, both of which involve telling GHC 'ok, I want to use Doubles internally': f :: Int -&gt; Int f x = floor . sqrt $ d where d = fromIntegral x :: Double g :: Int -&gt; Int g = floor . (sqrt :: Double -&gt; Double) . fromIntegral 
[See also][1] [1]: https://github.com/faylang/fay/wiki
In that case, I reckon the best option is wrapping `String` in a `newtype` and defining the instance for that. Note that, given that `hellogenetics.hs` is just example code, ignoring the warning would be of no consequence - other than, arguably, suggesting to your users that it is okay to define orphan instances. &gt;Is there a way to leave classes open, encouraging users to define their own instances? Classes are open; it is just that orphan instances can cause headaches, so you get the warning. Looking at it from another angle, either there is just one correct way to make `String` a `Gene` instance or there are multiple possibilities. In the former case, you can define the canonical instance at `genetics.hs`; in the latter, your users are likely better off using newtypes, for the sake of clarity and precision.
Instead of using `String` as the gene, your example could use a `newtype` wrapper for String as the gene. Then you would avoid the problem of an orphan instance.
Thank you! Nice color scheme title :)
 sqrtInt x = stable . iterate (newtonNext x) $ 1 + x `div` 2 where stable (x:xs@(x':_)) | x == x' = x | otherwise = stable xs newtonNext x y = (y * y + y + x) `div` (2 * y) EDIT: Well, that's the (approximate) equivalent of using `round` instead of `floor`. If you really need the `floor` version: sqrtIntFloor x | y * y &gt; x = y - 1 | otherwise = y where y = sqrtInt x
After a fair bit more implementation, I reran the benchmarks and found that this is a bit slower than `Data.IntMap`. Apparently, my previous benchmarks were discovering a bug that I myself hadn't discovered, and so ran extremely fast. I still think this technique has merit, though, so I'm working on profiling and improving it. EDIT 2: Current Progress 11/6: * `insert` is the only function that is less efficient than in stock `Data.Map`. Also, it is actually more efficient when the key already exists in the map. * I improved the representation yet again, making `lookup` more efficient and saving some memory. EDIT: Current Progress 11/3: * `insert` is 1.4x slower than stock `Data.IntMap`. * `lookup` ranges from ~7% faster to ~20% slower than stock `Data.IntMap`. * `member` is consistently ~15% faster than stock `Data.IntMap`. * `delete` is ~30% faster than `Data.IntMap` when there is stuff to delete, and ~6.4x faster than `Data.IntMap` when there isn't.
It turns out that the speedup didn't actually exist - I had a bug that my benchmarks triggered but my tests didn't.
np. Consider some operation I'll call `(%)` for brevity. It takes two automata and returns a new one. And suppose `a%b` is going to be huge, much bigger than `a` and `b` are. On the one hand, we could just build the whole `a%b` automaton. Given the new `c = a%b` we can perform operations like asking if `c` recognizes some string, or whatever. In this case, we're actually performing the operation. On the other hand, we could instead just pretend to generate `a%b`. That is, rather than actually building the automaton, we create some closure/thunk that says we did. With this closure we have enough info to be able to lazily build parts of `a%b` on demand to answer questions like whether `a%b` recognizes some string, or whatever. This isn't a call-by-need thunk however because, in the limit, call-by-need would still end up building the whole automaton. Instead it's more like a call-by-name thunk (i.e., we don't memoize the results) which allows partial computation. This is what I mean by simulating the operation: we return some `c'` which behaves the same as `c = a%b`, but where we're only simulating parts of `c` as needed, rather than actually building the whole thing. Turns out, for operations where `a%b` is going to be huge, you're better off simulating it rather than actually doing it. The downside of simulation, of course, is that you have to recompute things. The upside is that you don't get those exponential blowups, so you can reduce memory pressure which in turn reduces running time when dealing with large enough automata. Another benefit of simulation is that it can allow you to "undo" the operation, since you can still recognize how the automaton was built; thus you (i.e., the library) can perform certain kinds of symbolic manipulation before getting down to actually building things.
This was a big thing in [XFST](http://www.cis.upenn.edu/~cis639/docs/xfst.html). I believe they still do it in [OpenFST](http://www.openfst.org/), but it's been a long time since I've looked at OpenFST.
&gt; The primary reason for splitting up the implementation was that I saw that stock Data.IntMap did it, so I figured that there must be some sort of advantage to it. I guess that's an example of [cargo cult programming](http://en.wikipedia.org/wiki/Cargo_cult_programming) :-)
http://www.haskell.org/haskellwiki/The_JavaScript_Problem
The [Elm](http://elm-lang.org/) dev was [hired by Prezi](http://engineering.prezi.com/blog/2013/05/21/elm-at-prezi/). You could argue that Elm is production ready. It's a haskell-like language which provides you an FRP api to web programming that compiles to html/css/js.
Yuck. Oh well. [Reported](https://github.com/yesodweb/wai/issues/192) to the yesod team.
Well, [there's Haskell.](https://github.com/ghcjs/ghcjs) That's a pretty cool language, I hear.
&gt; List comonadic Nope, can't implement extract. The "doesn't happen. Really" better be a joke. The only thing right about this article is the blog's title.
I'm not familiar with newtypes. Could you provide an example?
Wow, that's more tokens than I anticipated. Know if there's a shorter way to get around the `Floating` issue?
&gt; Looking at it from another angle, either there is just one correct way to make String a Gene instance or there are multiple possibilities. In the former case, you can define the canonical instance at genetics.hs; in the latter, your users are likely better off using newtypes, for the sake of clarity and precision. Yeah, looks like newtypes are the way I'll go. Hmm, I must have incorrectly assumed that `Eq`, `Ord` `Show`, and friends were type classes.
`floor . (sqrt :: Double -&gt; Double) . fromIntegral` works for me, thanks!
[PureScript](http://functorial.com/purescript/)
&gt;Hmm, I must have incorrectly assumed that Eq, Ord Show, and friends were type classes. You weren't wrong, as they are type classes - and the same principles hold for them. In the case of `Eq`, `Ord` and `Show`, the assumption is that there will be a single canonical instance for each type; in the overwhelming majority of cases, it will be the instance automatically generated with `deriving`. In fact, for classes as elementary as these it becomes clear why One True Instance makes sense: beyond any technical issues, just imagine how confusing it would be if, say `(==)` on `String`s gave different results depending on the particular combination of imports you happened to be using. As for the second possibility, there are examples in the core libraries of classes for which multiple instances may make sense; such cases are handled with newtypes. One example is [`Monoid`](http://hackage.haskell.org/package/base-4.6.0.1/docs/Data-Monoid.html#t:All) - booleans under conjunction or disjunction, instances of `Num` under addition or multiplication, etc.
I agree, I wouldn't use a language in a project where no one else knows it. I don't think the readability of the output is the most important thing. People will have to fix bugs, and they can't do that by editing compilation output. At least some others have to know/learn how to use the language and the company needs to commit to keeping it that way (probably a hard sell for most). There's no silver bullet here, some people might find purescript too hard, maybe even typescript. We use Fay at Silk because we're all Haskell programmers. 
Monomorphic `_1` and `_2` lenses!
Related: * [Testing Monadic code with QuickCheck](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.9528&amp;rep=rep1&amp;type=pdf) by Claesson &amp; Hughes * [Beauty in the Beast: a functional semantics for the Awkward Squad](http://www.cs.nott.ac.uk/~txa/publ/beast.pdf) by Swierstra &amp; Altenkirch The approaches are vaguely similar: instead, think of testing things as generating random interactions between components (a script of things to do,) and link them to 'drivers' which perform one singular interaction a piece. Then you ensure the in the drivers (the I/O-y part) your invariants hold, or otherwise report an error. So QuickCheck generates interactions, and the drivers ensure nothing is messed up. This approach is used in part of Cloud Haskell (`network-transport-tcp`) and I used a similar approach at work recently. It works very well and `network-transport-tcp` has proven to be incredibly stable.
It's due to the way code is compiled. In GHC, we basically equip a pointer to a constructor with a tag, which determines which alternative it is. Pattern matching is sequential as you would expect (that is, it matches in order.) When we want to check if a constructor matches, we check the tag bits. So the code gets transformed into a sequential code that looks at the pointer tags, and dispatches to the appropriate case: data Foo = Bar | Baz f :: Foo -&gt; ... f (Bar x) = ... f (Baz x) = ... becomes something like: x = &lt;load constructor pointer&gt; b = get_tag_bits(x) if (b &amp; 2) // second case goto case_2 ... fallthrough ... ... deal with Bar ... ... case_2: ... deal with Baz If you order your matches such that common ones come first, the lack of a forward jump (like the forward jump for the case of `Baz` with `f` in the example) helps the processor a bit.
It sounds like you want a combination of, say, foldl and takeWhile which cuts off the list and then folds it. You can do it yourself using the composition operator, which is of type (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c So, for your existing folding function f and initial value x, and takeWhile condition g, you could use (foldl f x).(takeWhile g) $ list
It seems inefficient , because I have to go through the list twice. If it is in an imperative language like python, it just cost me one turn only.
Somewhat related: IIRC, the [BazQux](https://bazqux.com/) reader uses [Ur/Web](http://www.impredicative.com/ur/) 
It does *not* go through the list twice. Haskell's laziness means the takeWhile doesn't check any list element until the foldl requests it.
I see, another noob question: g and f in the above relpy is actually like: f = timecomsuming function args g = timecomsuming function args == Breakvalue Will the result of the timecomsuming function be cached?
No. Also that does not quite make sense the way you say it, because f and g are functions and don't even take the same number of arguments. If you want to understand Haskell, you *really* need to give up any habit of confusing a function with the value it returns when applied to its arguments. It's easy to get the results cached, but I cannot show you how without knowing slightly more precisely what you want to calculate.
args are not important， just ignore them or imagine a closure which both f and g share, like this test args = (foldl f acc0) . ( takeWhile g) $ list where f acc p = if func args p &gt; acc then func args p else acc g p = func args p &lt; Limit 
Well that is not well-typed - f is a function to *combine* list elements, not a function to apply to a single one. I am going to guess that what you actually want is to return the last func args result which is not == X. In which case you can do: test args = last . takeWhile (/= X) . map (func args) $ list 
Yeah I realized after I sent it that the 'reverse order' thought was off, but was stuck at a wedding sans internet and unable to follow up. =) 
Code part can be surrounded in blocks, &lt;- this is the only problem. I can benefit from this blog, so I voted up.
Tracked down the details: Indeed, c-pre-processing, by either gcc or by clang, treats the &lt;slash&gt;&lt;star&gt; on line 161 through the &lt;star&gt;&lt;slash&gt; on line 166 as a comment. Both compilers remove the comment. However, gcc places the remainder of line 166 at the end of 161, whereas clang leaves it on its own line. I think clang is actually slightly more correct here.... Nonetheless, there is a lesson here: When using -XCPP, if _any_ of your Haskell looks like c/c++ comments, you are in trouble!
Can you let me know the bug reference? I'd like to annotate it with my findings.
the second one, the code in the question is just a metaphor, the true code is like this: test args = (foldl f acc0) . ( takeWhile g) $ list where f acc p = if func args p &gt; acc then func args p else acc g p = func args p &lt; Limit
IntMap does it because it avoids allocation of a Just, which is more expensive than no allocation at all.
f actually takes a start value and an element of the list, I changed a little bit of the above reply, func args p is a time consuming function and appears a lot with the same arguments, I want it to be cached.
Strictly speaking, this doesn't require monads; `Applicative` will do. The `GPL` and `BSD` monads are isomorphic to `Identity`, and the `Identity` monad is definable in terms of its `Applicative` instance: import Control.Applicative newtype GPL a = GPL { runGPL :: a } instance Functor GPL where fmap f (GPL a) = GPL (f a) instance Applicative GPL where pure = GPL GPL f &lt;*&gt; GPL x = GPL (f x) instance Monad GPL where return = pure ma &gt;&gt;= f = runGPL (fmap f ma) where joinGPL :: GPL (GPL a) -&gt; GPL a joinGPL = runGPL
Its not really production ready yet :) plus, the language might change quite a bit before 1.0.
Do what MonadicTraversal (and you) said if you want to go via floating point. But there is something inelegant about dragging in IEEE floating point arithmetic, with all of its semantic ugliness, when really all you are dealing with is integers.
This function works for me on win8 x64. share your code and i will have a look.
I've never used Lens yet. Sounds like this release changes tons of names to ease transition from "lens-family" to "lens" libraries. Can I learn to use this library by reading "lens" documentation and tutorials?
Is it production ready?
[Code](https://gist.github.com/ZacharyKamerling/7288243)
Great reply, this is exactly what I am looking for.
Why not `maximum . takeWhile (&lt; limit) . map (func args) $ list`? Note that you should check if the list is empty before passing it to `maximum`, or use the non-partial versions like `maximumDef`. EDIT: I derped, just noticed that it is `(&lt;)` not `(&lt;=)`.
Keep at it. Haskell is hard to learn but once you get used to maps, folds, and zips you won't ever want to use a loop ever again! I try to think of operations as three categories. **These are maps.** * Transforming every object in a container with an operation `(a -&gt; b)` * The result is always the same length as the input. **These are zipWith's.** * Transforming a container of values where every operation takes a different value. * The resultant container is the same length as the input container. Eg I want to add the list [1..10] with [2..20] or run a function with the index of the current element (`zipWith myFunc myList [0..length myList -1]`). **These are folds.** * I want to accumulate a value (which can be a container itself) over every input and possibly throw out some of resulting computations on the input. * The output container doesn't have to have the same length as the input container. Hope that helps.
That's a suspicious number, 50^6 ... But are you sure you want *CPU* time for your game loop rather than *clock* time? CPU time will happen faster or slower depending on how much work your process is doing.
I heard it didn't satisfactorily support recursive types or parametric types, or something else very basic that was a deal breaker for me.
maximum seems to be better, but the return value of `func args` is a tuple `(a,p)`, I want the **tuple** which has the max a, so maximum doesn't work , and (maximum . fst) . takeWhile ( &lt; limit) . map (func args) $ list doesn't work too because I want a tuple `(maxa,p)` instead of just `maxa` , this is why I choose fold instead of maximum In python it would be like: max( tuples, compareFunc = lambda x y : if x[0] &lt; y[0] then x else y) Is there any function in haskell do this?
Great, it helps, I am thinking about this too, glad you point it out.
Then, use this? maximumBy (comparing fst) . takeWhile ((&lt; limit) . fst) . map (func args) $ list `maximumBy (comparing fst)` returns `(a, b)` in a list `[(a, b)]` where `a` is the largest: λ&gt; maximumBy (comparing fst) [(1, 2), (3, 1), (2, 5)] (3,1) (You need to import `Data.Ord` to use `comparing`.)
I am using [haste-compiler](http://hackage.haskell.org/package/haste-compiler) in my product which compiles haskell source to JavaScript.
&gt; The representation of a GeneString in memory is identical to that of a String, and the functions that convert between them are no-ops that produce no compiled code. I would add, though, that the compiler isn't always smart enough to recognize, say, that `map GeneString` should also be treated as a no-op, and in cases where it doesn't you unfortunately end up traversing the list at run-time even though it is essentially performing the identity transformation on each element. My understanding, though, is that their work on a "roles" system makes progress towards making the compiler a bit smarted about such things.
What is current state of compiler. Do you have any problems with it?
Thank you for the suggestion, it's working great!
Btw, i *highly* recommend you start using cabal to build your projects. Do a `cabal init` and anwer some questions (defaults are fine) then you can do `cabal build` without any manual Makefile management.
Haskell-like? [livescript](http://livescript.net/) take = (n, [x, ...xs]:list) --&gt; | n &lt;= 0 =&gt; [] | empty list =&gt; [] | otherwise =&gt; [x] ++ take n - 1, xs take-three = take 3 last-three = reverse &gt;&gt; take-three &gt;&gt; reverse Obviously Haskell *inspired*, actually a fork of coffeescript. looks really good though, has been around for a few years and is well battletested.
Great, maximumBy is what I was looking for.
Many of the simpler examples from the `lens` tutorials will work with `lens-family`.
I think comonadic structures preserve their shape throughout their (co-?)computations. So as long as you start with a non-empty list in the first place, you will indeed not encounter the case where extract is called on an empty list. The reason I think comonadic structures need to preserve their shape throughout is because of the [comonad laws](http://hackage.haskell.org/package/comonad-0.6.1.1/docs/Control-Comonad.html#g:1). Since `extend extract` must be the same as `id`, the `extend` implementation cannot modify the shape of the structure unless it can check that it argument is not `extract`. But it's not possible to examine this argument in order to check that this is not the case! The argument is a black box function of type `w a → a`, so it cannot be examined directly, and since `extend` is parametric in `a`, neither can its output.
For some reason he's converting the html-file to PDF instead of using the markdown directly. That's probably why it has a bitmap font which I think is what you're referring to. https://github.com/chmduquesne/resume/blob/master/Makefile
&gt; It can't infer types but the types are too complicated to figure out myself. If I don't want to figure a type out myself I load the module in GHCI and do `:t identifier`. If I agree with the inferred type I paste it into my code. If I don't agree with it then I have found a bug or need to deconfuse myself!
They're monomorphic? They look polymorphic to me?
This is a known Hackage bug. See [here](https://github.com/haskell/hackage-server/issues/103).
See https://github.com/haskell/hackage-server/issues/103 This is not a regression; the same seems to be true of old Hackage: http://old.hackage.haskell.org/packages/archive/http-conduit/1.9.5.1/doc/html/Network-HTTP-Conduit.html#v:simpleHttp
Oh yes! I've begun using cabal for https://github.com/mcandre/genetics. I've just haven't gotten around to package managering all my code, including nonhaskell code.
However, Elm is extremely domain specific and still lacks many necessary features (e.g., getting a stream of clicks on a given element, dynamic event switching (which actually is occasionally necessary in practice)).
I'd invite you to hack on [Fay](https://github.com/faylang/fay/wiki), a project to compile Haskell to JavaScript. It could use more contributors. It's not "production ready" though. ClojureScript is probably the most mature compiles-to-JavaScript, though it's a Lisp, not an ML. Also see Opal and CoffeeScript, though they're Ruby and better JavaScript respectively.
I prefer to remove intermediate names using a more terse wrinting, E.G: the main: main = getContents &gt;&gt;= execParse
There's [Forml](http://texodus.github.io/forml/). 
You might have a look at this forth implemented in Haskell: https://github.com/ezag/harrorth Especially have a look at the doc/ directory - it contains a lot of discussion on its design. 
If you want to ensure that the `timeconsuming` function only gets called once, then you just precompute it using `map` before feeding it to `takeWhile` and `foldl`. In other words: foldl' step nil . takeWhile (\(x, _) -&gt; x == breakValue) . map (\a -&gt; (timeconsuming a, a)) ... where `step` might also use the result of the `timeconsuming` computation.
So GHC does this ordering on a global basis, not on a function-by-function basis? I would have thought that f Bar = ... f Baz = ... goes to if (b &amp; 2) //second case goto case_2 while g Baz = ... g Bar = ... goes to if (b &amp; 1) // first case goto case_1
It's sort of going through the list twice, in that two things are going through the list. It just happens to be that they go one on the tail of the other so it's not twice the time of a single list traversal.
AFAIK it's the easiest thing to use at the moment, because of compiling to whole webpages rather than just the JS. It still is not as rock solid as GHC(but that's obvious) and some features are still missing, but it's really interesting.
Thanks a lot! I'm going to need to look into Applicative more to understand Parsec. Didn't know `type`s could be point free, either. That's interesting. Xor was just silly. &gt; still clumsy, but I don't know anything better for the moment Nah, it's a hack in the real Forth, too. They probably do it with bit operation magic but I don't want to go there.
Thanks. Design wasn't really a concern because the project was to remain small but now I might have an excuse to extend it.
Hmm, the problem seems to be limited to `^?` indeed. No love for a `^=?` or something like that? (I've named it `.?` in my code but that's not in line with the rest.) I feel like retrieving elements from a list by index is something that needs to be done a lot, so there should be support to mix it with the other state operators. Or am I approaching `lens` in the wrong way? 
I don't actually know what you want to do there, and what these are for. What do you want it to do? To do GA, you need individuals that mutate, and some way to select which ones are "better". How will you do that? Actually, what are the individuals here? Note that the only thing you should need to do yourself is the genome, how to mutate it, and how to select: genprog should do everything else. Your `lookupSequenz` doesn't work because it shouldn't work: it doesn't make any sense. To me it seems that, for your `evalGenom` to make sense, you should just take the `map lookupSequenz` out and it would typecheck. As I say, without knowing what you want to do I can't help you. Also, using a `String` when you want four binary digits is overkill and unidiomatic in Haskell: make it use four `Bool`s instead.
Use `preuse` and `preuses` (for any `MonadState`).
What I meant was that, without `NoMonomorphismRestriction`, it won't even compile Parsec code until you've used `parse` somewhere. And if you leave it on the types inferred by `:t` are way too general. It just feels wrong. 
If you add explicit type signatures you don't need `NoMonomorphismRestriction`, and you can make them less general if you want.
&gt; And if you leave it on the types inferred by :t are way too general. Could you give an example? Sometimes types that constrain to a lot of typeclasses can be very confusing, but often more general types are better. They can require a bit of hand-tuning though.
But does it occupy less memory? That alone could make it worthwhile.
Yes, it does occupy less memory: `Data.IntMap` stores 2 `Int`s in every branching node and one in each tip node, while I store 1 `Word` in every branching node and no `Word`s in each tip node.
I don't get it and the type errors are exploding. I want a function `Getter s [a] -&gt; Int -&gt; m (Maybe a)` for any `MonadState` or something like that. Could be generalized to include Either. Just a safe getter in a state monad. `flip preuse ` isn't it...
It looks like there is a typo in your foldr. The first line should be foldr f z [] = z
 preuse (element 4) :: State [a] (Maybe a)
I put this together quickly so that non-Haskellers can try out the compiler without having to install the Haskell Platform. Code is [here](https://github.com/paf31/trypurescript) for anyone interested.
Actually there is [doctest](http://hackage.haskell.org/package/doctest) for that.
You should put a link to a page which explains what PureScript is. This looks cool. I'm kind of working on a similar project.
I heard it needs more enterprise.
You're welcome!
Very cool stuff! Looking at the array example I get a bit scared that it will blow the stack though. 
Yeah, some sort of TCO would be nice, but you can always fall back to a loop if you like: sum = \arr -&gt; do var total = 0 foreach x in arr: total = total + arr return total Not as nice as the recursive version though.
Yeah, that's where 10^5 comes from, but doesn't explain 5^6 * 10. OTOH, [Tim_M's link](http://www.reddit.com/r/haskell/comments/1prrd8/cpu_timing_problems_windows_8_64/cd5nu04), http://ghc.haskell.org/trac/ghc/ticket/2140 : &gt; getCPUTime always returns a multiple of [...] 15.625 mS Which reminded me, that's exactly the (default) length of a timeslice on multiprocessor NT. It's quite possible that the counters that GetProcessTimes looks at are only updated on task switches. If that's true, and your process never gives up partial slices, you would expect to only get multiples of a whole timeslice. It's also possible that it only has timeslice granularity, even though it has 100 ns precision.
D'oh! Fixed...
Your `foldP` is easily written in terms of `foldr`: foldP :: (a -&gt; b -&gt; b) -&gt; (a -&gt; Bool) -&gt; b -&gt; [a] -&gt; b foldP f p acc = foldr go acc where go x r | p x = f x r | otherwise = r
You can clean up `eval (DoLoop...)` even more by defining a custom `loop` op. eval (DoLoop xs plusLoop) = op loop &lt;|&gt; return () where loop idx lim = do pushLoopStack lim pushLoopStack idx mapM_ eval xs _ &lt;- popLoopStack _ &lt;- popLoopStack inc &lt;- if plusLoop then popStack else return 1 let idx' = idx + inc guard $ if inc &gt;= 0 then (lim &lt; idx) == (lim &lt;= idx') else (lim &lt;= idx) == (lim &lt; idx') loop lim idx' I'd still shy away from that guard statement, since it takes a bit too much examination to see what's going on. All you're really checking is that both `idx` and `idx'` are on the same side of `lim`. idx' &lt;- (idx +) &lt;$&gt; if plusLoop then popStack else return 1 guard $ if idx &lt;= idx' then lim &lt; idx || idx' &lt; lim else lim &lt; idx' || idx &lt; lim I might be tempted to revert to using `not`, since I'm used to seeing bounds checking like: guard . not $ if idx &lt; idx' then idx &lt;= lim &amp;&amp; lim &lt;= idx' else idx' &lt;= lim &amp;&amp; lim &lt;= idx But really, I think this is one of those situations that merits a comment, for the sake of future devs.
agree, the array + indexed "boxed vs no" approach gets you pretty far! Its also one of my favorite api tricks
`preuse (loopStack . (ix 0))` does it! Neat. Thanks man. 
You're welcome!
With [this](https://github.com/chrisdone/purescript/commit/84f83b4cfaa6d9c843acceb1ce73e90a89ac7573) you can write sum = \acc, arr -&gt; case arr of [x:xs] -&gt; sum (acc + x,xs) [] -&gt; acc and it will be translated to sum = \acc, arr -&gt; do var _tco_acc = acc; var _tco_arr = arr; var _tco_repeat = false; var _tco_result = false; while true: _tco_repeat = false; _tco_result = case arr of [x;xs] -&gt; do _tco_repeat = true; _tco_acc = acc + x; _tco_arr = xs [] -&gt; acc if _tco_repeat: acc = _tco_acc; arr = _tco_arr else: return _tco_result Not the most trivial of output but the AST has limited expressivity. Get proper name resolution in there and less artifically verbose output and this is probably not far off. Implement tail recursion modulo ~~cons~~ any-known-constructor, and you can write your original version: sum = \arr -&gt; case arr of [x:xs] -&gt; x + sum xs [] -&gt; 0 ‘Cause `x + sum xs` is really tail-recursive modulo the known-non-recursive function `+`.
just so you'd know, Stack Overflow is more suited to these sort of questions
Wow, that's great :) thanks.. Would it be ok if I pulled this into my master copy and included it as a command line option? 
It was just a demonstration, really. It's missing the fact that it should lookup “sum” taking scoping into account, otherwise it might see a sub-`sum` and think it's a tail-call of a parent function. I leave that detail to you! I plan to play with rewriting some JS things I have in PureScript some other time to take the language for a spin. :-)
SO is a good resource, but I prefer Reddit, a much more helpful community in my experience.
OK the reason I wanted to see the args is that my trick as shown only works if they were as I guessed from what you had written. Otherwise you need to use a more complicated trick, which Tekmo provided. Except for one mistake that seems to repeat in this thread: the argument to takeWhile should be a function that checks whether you should *continue*, rather than one which checks whether it should break. 
It's once in the sense that often matters in Haskell: You can stream through it only once and let the GC discard values as soon as they are used, which means it can handle e.g. a list produced by lazy IO with constant memory overhead. In theory it could even be fused to a single loop computation, although I don't see foldl and takeWhile in the ghc manual's list of functions that the list fusion rules handle automatically. 
Glad you found it relevant! Is a strict datatype different than just making the constructor functions strict? (I'm genuinely asking) Because that would be pretty easy to achieve..
&gt; So as long as you start with a non-empty list in the first place, you will indeed not encounter the case where extract is called on an empty list. The type of extract is `forall a. Comonad w =&gt; w a -&gt; a`, specialized to `[]` this is `forall a. [a] -&gt; a`, which does not exist for all `a`, hence there is no extract, hence lists aren't comonadic. If you want a list-y comonad, use non-empty ones a la `data NonEmpty a = Tip a | a :| NonEmpty a`.
A "how to switch to Haskell from *X*" series could be very interesting. I don't know any Ruby, but if you do the same for Python I'll pay close attention!
By strict datatype I indeed mean one with strict constructors. Although this by itself is achieveable in Haskell, it doesn't seem possible to indicate strictness in the type system. If we were working in a strict language (or in a version of Haskell where every function argument and constructor argument implicitly had `!` applied to it) then we could introduce a `Thunk` datatype, whereby `Thunk a` contains an unevaluated `a`. I don't believe we can do the same thing for a lazy language. One can create a `Strict a` datatype that always contains an already evaluated `a`, but this flounders on the sea of laziness. When you see let a :: Strict Foo a = f b c the `a` is actually a thunk, despite your best efforts to make it strict!
Ok, yes, there are semantic differences, but the ideas are similar. First class functions, pattern matching, function combinators (bind). Sure, its js and theres no type system to speak of but its better than vanilla js to be sure.
Note: I was comparing a _pure_ strict and a _pure_ lazy language. You can't implement that compiler transformation by hand there, as you don't have the power to evaluate a thunk and write in the answer. I used to think things like `DiffArray` and persistent but not functional data structures were a good idea, but I've since reconsidered my view based on the abysmal performance of `DiffArray`, the headaches of the MVCC model in the stratified B-Tree, the ugly internals of Daan Leijen's version of the revision control monad, etc. Now, I've decided I'm going to try to "wear the hair shirt" and just keep digging deeper on the pure FP side of data structures, with the ST hack mentioned here as a new tool and see if I can't just find number systems that let me deamortize everything I need to deamortize to get the right overall persistent asymptotics for the data structures I want to implement.
&gt; You can't implement that compiler transformation by hand there Absurdly pedantically, you can - you just need an abstraction that models working with a mutable memory. That's not meant to be a useful answer, of course. It's just on my mind because I'm using IntMap as essentially a way to model pointers in a pure union-find in another learning exercise. I'm a bit disappointed that persistent non-functional data structures didn't work out, but still - some people have achieved great things while wearing those hair shirts (and even some barbed wire shirts) - good luck with yours! 
You should also start with a Hello World already in the box. Compare with the front page for the [Go Playground](http://play.golang.org/), for instance. I'd shoot for something about the size of that Go sample, too; if Hello World is about the size of `print "Hello World"`, give us something else, too.
I'm sorry, but I'd rather not have the Ruby crowd ported over. Surely there are exceptions, but most of what I've interacted with are not that advanced in the field (yet they have a [knowledge arrogance around them](http://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect)), and far less immature that they ought to be.
Yeah, I get that vibe from the ruby crowd too. They are very certain they are doing things right and you are doing them wrong. It seems like a great community if you drink the koolaid. p.s. This could probably be said of much of the haskell community, but haskell is often backed by research not fuzzy feelings.
(Puts on best Mr. T voice.) I pity the fool who could apply for this internship, but doesn't!
Aren't you the creator of Fay ? Can I ask what niche do you think this language fills that Fay doesn't ? Thanks.
I'd rather the rubyists be humbled and better themselves, the current state of things in web dev culture is a bit raw.
Why then should we not liberate them? There's enough koolaid to go around
You are right about the Ruby community in general, but a lot of the anti-intellectual wannabe-rockstar-types left for node.js already and I don't expect the remaining bad eggs to leave for Haskell as things stand right now. But I doubt they are a good target for Haskell anyway. They seem to shun theory even when it would be useful in their day-to-day work. E.g. learning relational algebra vs sweeping it under the rug with ActiveRecord. Or doing weird manual CPS transforms in their code without realising it because somebody told them "tell don't ask" is an iron law of programming.
&gt; An elegant, reliable, easy-to-maintain, high-level, parallel-friendly, native language. What's not to like? Lack of backtraces in not-built-for-profiling programs, no "small" binaries (maybe I haven't investigated enough), memory problems due to laziness. Otherwise, I'm loving it and continuing using it everywhere I can. // edit: s/exceptions/backtraces/
Yeah, I would love to do something like that, too bad it's only for Phds.
My introduction to Haskell was Conal Elliott's article introducing FRP in Dr Dobb's Journal. I was literally amazed at how the code could be so short and yet expressive. The best advocacy for languages is surely *showing* what they're good at rather than arguing what you believe they're good at.
I think you mean "lack of backtraces".
Check out the [monad-loops](http://hackage.haskell.org/package/monad-loops) package. It's got [iterateWhile](http://hackage.haskell.org/package/monad-loops-0.4.2/docs/Control-Monad-Loops.html#v:iterateWhile) which sounds a little like what you're looking for.
I have no experience developing mobile application with Haskell, so I'll leave that point to someone else. But on planning a program in Haskell, the thing I can say is: iterate. That doesn't sounds different at all from something you'd do in an imperative language, but in functional programming that constitutes quite a large impact. Due to the fact that in functional languages you will build from the bottom -&gt; up, it is not a strange practice (this is just my case) to build programs for each feature you want and then merge functionality. The high isolation of state and code helps you tremendously in that regard. Feature by feature, in different applications, which then you just group/refine into modules that hook up and interleave on the data they work on. 
So the main difference between your stated view of Ruby community and your stated view of the Haskell community is: the Ruby community thinks that it is better than other communities and is sure that it is right BUT the Haskell community thinks that it is better than other communities *and is sure that it is right*. Less elitism, more inclusion!
This *is* helpful. Thanks.
Fay isn't native-JS-fast. It's a layer above JS, with thunks and currying and such. It's perfectly sufficient to write applications in, but anything that needs to be done thousands of times per second (for example), you shouldn't be using Fay for that, you should use plain JS and call into it via the FFI from Fay. PureScript is a rewording of JavaScript with a type system. It's barely even a layer above JS. So if you need very fast code (e.g. some WebGL game or w/e), e.g. that Fay might call into, but don't want to write it in unsafe, anything-goes JS, then PureScript seems like a decent option. Similar to TypeScript, but TypeScript's type system is sucky.
This is a *long* article?
Hear hear! Anyone who thinks that a certain community only contains a certain type of person has a very naive, over-simplistic world view. Frankly, one of Haskell's biggest problems is the prevalence of this type of elitism. It's not at all the case that Haskell is the One True Language or that FP is the One True Paradigm. This sort of exclusionary elitism only serves to reduce the diversity our gene pool and leads to exactly the kinds of fucked up that you so often see in any inbred aristocracy. I would like to point out that I have never heard this sentiment from the real "elites" of the Haskell community. The Simon Peyton Joneses and Simon Marlows of the community are all, in my experience, very welcoming of newcomers and very willing to acknowledge the langauge's faults and shortcomings. SPJ after all used to call Haskell a "useless" langauge!
Agreed; what got me hooked was SPJ's paper on modeling contracts, with the identical reaction: amazement at how the code could be so short, expressive, and general.
you'll need to compile with the `TemplateHaskell` extension, either in a language pragma at the top of your file, or in a command line flag.
Thanks. ghc AcidState.hs -XTemplateHaskell -XTypeFamilies -XDeriveDataTypeable edit: I just noticed I failed to cut-n-paste the following lines from the original source. {-# LANGUAGE DeriveDataTypeable, TypeFamilies, TemplateHaskell #-} module Main (main) where 
This reminds me a bit of [an old experiment of mine](https://github.com/jgm/HeX). My approach was to use Haskell files to define macros which could then be used to process "hex" files with LaTeX-like syntax, converting them to HTML, LaTeX, or some other format. The idea was to get the flexibility of LaTeX, but with macros written in Haskell and supporting arbitrary output formats. 
A better place to ask this question (and ones like this) would be StackOverflow. The Haskell tag there is quite active.
This is a task and a half... Is hiring a Rubyist that is also a Haskeller enough? The mindset of the Ruby community, just like that of the Haskell community or any other community for that matter, is that they would like to stay inside Ruby for as much development as humanly possible. Is there really an attack route that will result in a wider acceptance/adoption of hybrid Ruby/Haskell development shops? For comparison, surely there is less of a hurdle to overcome in the adoption of Hybrid F#/C# development environments yet, from what I gather, F# is still having a bit of a rough time finding a place among the C#/VB developers who dominate the .Net community. Are there any parallels that can be drawn? That said, this task faces difficulties all its own. To introduce Haskell to the common Rubyist is even harder than F# to C#'er since many Rubyist drink the "types are bad" and the "Everything needs to be super easy" Koolaid. Also, Rubyists are accustom to reading tutorials that help them get something useful up and running so that they can be productive. Haskell is a very productive language but a lot of the material out there is of the kind that encourages one to study for a few hours just to get a game of life up and running in a clever way -- this is unacceptable to many in the Ruby community. The only way show Rubyist the benfit of Haskell is to present Haskell to the Rubyist in such a way that it "encourages" her values as a developer. To have a chance, a very clever person, not necessarily a Ruby grandmaster Haskeller, who is familiar with Ruby culture and has a very strong grasp of the fundamental simplicity that makes Haskell great will have to cater to the values of the Ruby community by: * Showing how Haskell can be very simple and easy to work with * Showing how Haskell benefits pair programming (yes, very many Rubyists value this technique) * Showing how Haskell aids the "Agile" process * Showing how Haskell is easily testable and deployable In general showing Haskell to be a "Ruby" for the "sensitive stuff". Even still it will be hard to get Rubyists to eat the cookie.
The primary reason my boss won't let me write on-line services in Haskell :(
Excited to see where this leads for the FP Complete team. I mainly work in Ruby and JavaScript, and I've been dying to learn some Haskell, although my only progress (as of yet) has been buying a copy of "Learn you ..." I'll keep on chuggin'. But, a quick aside: Ruby and JavaScript have been great for me -- and for other kinesthetic learners too, I'd imagine -- because "it is known" how to go from their fundamentals to concrete applications. You don't have to scratch your head very long to see what things you can build with either of them. Even just copying and pasting can get you pretty far. The most popular beginner's tutorial for Rails (an incentive to learn Ruby) essentially has the user build a Twitter-like website. To be able to go from "I can write a `.each` loop" to "I made my own Twitter clone" *that* quickly is pretty damn intoxicating. I would love to see a Haskell project or framework or "from zero to deploy"-style tutorial that gives folks that immediate inspiration, that "Wow, if I learned that, I could do anything!" feeling.
Nice. The approach of haskintex is to use the HaTeX library to program those macros as functions. This library includes a LaTeX parser that can be used to translate to other formats if desired. Also, haskintex is able to actually run the embedded Haskell code, and optionally show the Haskell code you have written in the LaTeX output so you don't need to duplicate your code. Furthermore, if you add the -lhs2tex flag you can feed the output of haskintex to lhs2TeX to get a prettier output.
I think that you are actually choosing the take what criticalbone said in the worst possible way. The fact of the matter is that Ruby has a low barrier to entry because its a nice and simple language that focuses on programmer productivity. As such, it is not unreasonable to believe that the community would attract many newer developers who repeat a lot of demagogy or boldly express many naive opinions. If new programmers do that then 9 years ago I was guilty as sin -- I'm smarter now (kind of). Now while I wish criticalbone would have expressed himself a bit better I don't think it fair to attack him so strongly. 
&gt; But on planning a program in Haskell, the thing I can say is: ~~iterate~~ recurse. FTFY
I like the integration with TikZ. Writing plain TikZ is always a pain and takes ages. Generating the TikZ code from Haskell is potential a huge timesaver for me. +1 from me.
&gt; Secondly, there’s no inch specified! So I had to use the relative standard for inches being 72pts per inch. Someone is doing their CSS very, very wrong.
Those damn double negatives. I didn't say they where inferior, and I don't now if immature implies that. But take it as you will.
Possibly, though defining a width for a document that can be printed makes sense to me. 
Interesting project! We have a different technique for including haskell diagrams in LaTeX documents described [here](https://github.com/diagrams/diagrams-doc/blob/master/doc/latex.rst). It would be nice to add support for diagrams though haskintex too.
Someone has built a simple Android app using ajhc: https://www.youtube.com/watch?v=n6cepTfnFoo There are links to the github repo on the video page. 
Yes, it would be great. Although I am not a diagrams expert yet. Anyway, the TikZ code is generated by the HaTeX library, and it has nothing to do with haskintex itself. What haskintex does is to evaluate and render an expression of the LaTeX type, which is defined in the HaTeX library. This library contains all the methods for matrix rendering, TikZ, etc. Therefore, to include diagrams using haskintex, the only thing needed is a function "fromDiagram :: Diagram -&gt; TikZ", where "TikZ" is the type defined in the HaTeX library to represent TikZ scripts.
There is a TikZ backend for diagrams in the works. I'm not sure if it is using HaTeX or not. Our current methods call out to diagrams-builder to build a file then include the graphic in the LaTeX.
Well, I know of this: http://hackage.haskell.org/package/diagrams-tikz. But I think that is hard to integrate here.
This looks cool. If I was super comfortable with this, I wouldn't even go to wolfram alpha. I'd just write out computations in haskell. 
Very cool. A sort of pie in the sky dream of mine is to make a LaTeX replacement in Haskell, it's just that building all that underlying machinery sounds like a total nightmare. This looks awesome, and that TikZ cleverness is so cool!
Lots of criticism, mostly fair. Clay is still pretty much a prototype and hasn't really been battle testen in the real world. At least not by me. Most downsides he explains are not really design choices, but the result of quick initial implementation. Patched are welcome ;-)
Glad you saw it! I might add some stuff similar to the color functions in SASS, but for another time :)
It's pretty easy to avoid getting into a situation where your Haskell program can actually crash. 1. Avoid the use (and creation) of partial functions. That is, functions like `head` and `!`. 2. Whenever you use pattern matching, make sure you handle every case. This goes for both function definitions and case statements. You can turn on compiler warnings in GHC at least which will tell you when there are any unhandled patterns. 3. Catch exceptions in IO when you call IO functions that can throw them. If you follow those rules, especially #1 and #2 then your program will not crash and you can detect when your program state diverges from expectations programatically and emit whatever diagnostics are suitable with no built in backtraces required. Static typing catches most errors anyway. (And if you already know all this but couldn't convince your boss: my condolences!)
How about: {-# LANGUAGE OverloadedStrings #-} import Shelly main = shelly $ verbosely $ do run_ "sudo" ["ls", "-l"] Note that if you prime your sudo creds before running this program you won't be prompted for a password. Also have a look at the "-A" option to sudo and the SUDO_ASKPASS environment variable. 
The fact that Clay is Haskell gives it significantly more power than most (all) other tools that compile to CSS. Significantly!
Yes, I agree that it is only non-empty lists which are comonadic, I wasn't trying to argue otherwise. Just attempting to understand what geophf might have meant by "doesn't happen"! I like your `NonEmpty a` representation, as it is a lot less error-prone than calling `error` when the programmer incorrectly attempts to use an empty list comonadically. I just wish I didn't have to abandon the wealth of helper functions and special notations reserved for the native list datatype. How about this representation instead? type NonEmpty a = (a, [a])
The word "Reported" is a link to the issue. Sorry, I should have made it more visible. Anyway, thanks so much for your contribution to this interesting issue.
So, I have two comments: 1) This seems like one of those excellent opportunities to sponsor open-forum style tutorials, not so unlike [your competition](https://www.fpcomplete.com/business/resources/competition-overview/) for code, based upon rewards. I know Ruby quite well, and I enjoy Haskell quite a bit, but my interests my not perfectly align with your desires. (I prefer low-level systems programming and building UNIX-style tooling, not websites, for instance.) I'm sure that there are others who, properly motivated, would be more than willing to contribute their perspective. I happen to be a fan of the growing "micro-task" movement that rewards individual contributions rather than subsuming a project within a singular framework or ideology. That being said, what are the general goals of this project? There aren't a lot of details in your post. It might be easier to get people excited if they understood your direction a bit better. 2) Selling Haskell to Rubyists has been, for me, quite difficult. There is a general sentiment that "types don't play well with Ruby code" (near-perfect quote). Being a student of Type Theory and programming languages in general, I understand how strange this comment probably sounds to a Haskeller, yet this general notion seems to pervade the Ruby community. Overcoming people's pride is a singular difficulty in getting people to adopt something radically different. Taking pride in not using types is one of the current hallmarks of "dynamic language" enthusiasts, these days. To what extent, then, will these tutorials focus on basic, pragmatic education to break people from these ideological frames of reference? To what extent are you expecting people to adopt Haskell not simply based upon "easy projects" but because it helps them understand computation more easily?
Also there is [Frege](https://github.com/Frege/frege) which is Haskell 2010 (with some differences) on JVM. I don't have much experience with it, but that is something you can investigate. (IIRC someone managed to make an example app with it.) If you want to be pragmatic, Scala would probably be the best option right now. It's not as extreme as Haskell, but many ideas translate fairly well over there. I've actually developed an Android app using Scala, you can see [Scaloid](https://github.com/pocorall/scaloid) to see how Scala can abstract many common patterns in Android development.
Is it a bad title? Or is it just me who will instantly think about, you know, \def\main#1{ \let\re@lworld… 
i second that. Were I still a student, i'd totally want to apply for this. Everyone who find this internship tempting: you should still get involved in GHC dev! :) 
Came here to say this :)
Is that this paper? http://research.microsoft.com/en-us/um/people/simonpj/Papers/financial-contracts/contracts-icfp.htm
&gt; The only way show Rubyist the benfit of Haskell is to present Haskell to the Rubyist in such a way that it "encourages" her values as a developer. I wonder if there's some way of presenting types as unit tests.
There have been a few Haskell advocacy links and discussions recently. I came across this, was amazed, and though it was worth sharing. Does anyone know of similar experiments that might have been conducted since?
Perhaps I'm misinterpreting what you mean, but printing documents can be annoying when they're designed specifically for certain paper sizes. The most common issue I have is trying to print Letter documents on A4 paper.
&gt; inRegion :: Point -&gt; Region -&gt; Bool &gt; p `inRegion` r = r p &gt; &gt; This function was strictly unnecessary, and resulted in more lines of code, but was considered to be an elegant form of self-documentation. Even if I never used it, I thought of something like that once. In Objective-C function call with multiple parameter mostly read like sentence. It is clearly far more verbose, but in the same often clearer. Here is an example for the `map` function: data Sep = onElementsOf -- Sep should be some meaningful names apply :: (a -&gt; b) -&gt; Sep -&gt; [a] -&gt; [b] apply f onElementsOf l = map f l It might be a silly idea. But it is somewhere in the corner of my head.
You might find something interesting here. http://www.reddit.com/r/haskell/comments/y6i7d/do_we_have_real_world_examples_of_where_static/
 module Sed where will export everything, which will make auto-detection of dead/unused code not work.
~ ~ ## *Please consider the environment before printing this email* ~ ~ 
First of all, the idiomatic way to write your function is with `foldr1`. Or to factor it into `minimum . takeUntilIncluding (&lt;=bound)`, where `takeUntilIncluding` is a function I just made up that takes elements until one satisfies a predicate, and then also returns that one. takeUntilIncluding pred = foldr (\x ys -&gt; if pred x then [x] else x:ys) [] Worker functions are often declared in a where clause, and a common name for them is `go`. I would call a list `xs` even when not taking its head. `sofar` is an accumulating argument for a tail recursive function, these are often called `acc`. But more specific names are fine also, I would maybe call it `smallest` or something in this case. I would expect the order of the parameters to be the other way around. Things that don't vary, like `bound` in this case, usually come first. Because of lazyness, tail recursion is not going to be any better than non-tail recursion in this case. The `sofar` argument will become a large thunk, `min (min (min (min a b) c) d) e`.
Haven't had any problems so far. It compiles to very fast and compact JavaScript code.
This reply uses 100% recycled electrons
On 2): I usually put the worker function inside a `where` clause, and call it either `worker` or `go`. The `where` binding has the additional advantage that you don't have to pass around the invariants like `bound` is in this example. I don't think it's harder to read at all, and you can definitely put a type information inside `where`. And I don't think `where` has anything to do with tail recursion.
i don't get your example, but when i coded with objective c (on a next!) i really liked the smalltalk-y syntax as well. unfortunately it does not play nicely with automatic currying or at least i don't know how it might work.
Very useful, thanks.
Please link them in that case :-)
In what way is it not a story about refactoring? It occurred to me that in Haskell one shouldn't have to change e.g. every distribution every time code is refactored but it would be better to have some evidence.
The biggest challenge for me as a Ruby programmer who also enjoys Haskell has been finding intermediate level learning materials. You've just finished Learn You a Haskell and you want to build something... now what? I think more tutorials on how to build something -- a server, a chat client -- whatever, but complete with how to structure code, cabal sandbox, writing tests, etc. So much effort goes into monad and lens tutorials, but I just want to build things. 
For worker functions, I use `go`. For your minimum, I’d use a `foldr1` like : lowMinimum :: Ord a =&gt; a -&gt; [a] -&gt; a lowMinimum _ [] = error "empty list" lowMinimum l = foldr1 (\e a -&gt; if e &lt;= l then a else min e a)
From my recent experience, this isn't enough. First of all, I've got "stack overflow" exception rencently (which you can't fight with types), secondly, it's quite tedious to catch/rethrow-with-additional-info exception every time you do IO. You usually want just a single top-level exception-handler for IO code that would log it into the right place. So the problem of locating the exact place (backtrace path) of an error still exists today, for me at least.
I didn't say it wasn't about refactoring, and I didn't say it wasn't about Haskell. I said it **wasn't not** about Haskell, i.e. the story applies equally well to Haskell as it does to any language :) 
Well, for example, my code for dealing with the same kinds of distributions doesn't have to be refactored to provide all of the higher-order derivatives, since `ad` handles that case natively. ;) My experience with Haskell is that as long as I implement things that satsify universal properties wherever possible, and build up from the bottom up rather than engineer to fit a top down goal, I don't refactor anywhere near as much as I do in other languages, and when I'm forced to refactor the strength of the type system makes it so almost invariably after the refactored code typechecks it is still correct. Another trick, not unique to Haskell, is to factor things so that instead of O(n*m) feature interactions you factor into interfaces in such a way that you can get O(n+m) growth. What does work particularly well in Haskell is that you can abstract over more things, enabling this pattern to work in more situations.
The TikZ backend does not currently use HaTeX, but I doubt it would be hard to port (and it would probably be a good idea anyway).
| * Free theorems aren't. Well, to be fair, [few if any](http://www.iai.uni-bonn.de/~jv/papers/FreeTheoremsInThePresenceOfSeq.pdf) ever get this right.
I don't know anything about your level of experience/skill, so I hope that my response will not seem patronizing. That isn't my intent at all. &gt; First of all, I've got "stack overflow" exception rencently (which you can't fight with types), That is true, but usually it is fairly clear when you're doing something that will eat up stack. It's good to get in the habit of using strictness annotations unless you explicitly want something to be lazy. &gt; secondly, it's quite tedious to catch/rethrow-with-additional-info exception every time you do IO. I would think you'd just create one function which you would use instead of the normal exception catch/throw stuff. You could use that function to catch and throw a new exception with the extra information, perhaps while supplying a label. For example: myCatch "what_this_code_is" $ do launchPuppies Instead of a string, you could also use a custom exception that can embed the caught exception as well as additional information. &gt; You usually want just a single top-level exception-handler for IO code that would log it into the right place. Yes, you can do that as a failsafe to handle "impossible" situations but it's probably not good practice to rely on it. I'd also point out that generally you want a small percentage of your Haskell program to live in IO. If you have a lot of IO code, that is probably an indication that some refactoring is necessary. Also, there are various Haskell packages for debugging and tracing, including generating stack traces such as this one: http://hackage.haskell.org/package/control-monad-exception So if you want a stack trace, you certainly can have it. Here's a Hackage search on the word "trace" which has some other likely prospects: http://hackage.haskell.org/packages/search?terms=trace I haven't used them. My approach is generally to use verbose logging since I am typically more interested in the program state that lead to an exception/malfunction rather than where it occurred. 
That's the one!
I believe that last line should return `e` instead of `a`, lowMinimum l = foldr1 (\e a -&gt; if e &lt;= l then e else min e a) By the way, since we are talking about naming, I would have called that variable `x`.
&gt; instead of O(n*m) feature interactions you factor into interfaces in such a way that you can get O(n+m) growth. What does work particularly well in Haskell is that you can abstract over more things, enabling this pattern to work in more situations. This is a nice observation.
&gt; and build up from the bottom up rather than engineer to fit a top down goal You often need a combination of both, as well as some refactoring. Ideally you could start from the bottom down with nice abstractions. But to know where to start you (or at least I) often have to start top down with large clunky things. Only after you try to write them does it become clear how to break them apart into smaller and more composable bits. You can see this story in several haskell libraries, where a new version suddenly became simpler because a nice abstraction was discovered.
awesome news for me, I applied 5 minutes after seeing haskell-cafe mail.
I think people who are hiring for Haskell (and in general) are missing a trick with their "no remote" requirement. Why do people feel they need to limit themselves to only those who can physically reach their office?
&gt; but as far as I'm aware, the optimizer doesn't treat expressions with unsafePerformIO in them specially. Of course, there was a time for all of us when object oriented design and multiple inheritance were difficult... honestly even though I feel like I get the concepts it seems much more complicated than the functional approach though.
There are a number of legitimate circumstances that could preclude remote working. Also notice that Soostone is headquartered in NYC.
What would be the advantage of this over just throwing the two separate exceptions?
The point is that I've got a value of type `Either ErrorClass1 ErrorClass2` already, and this would save doing: case err of Left x -&gt; throw x Right x -&gt; throw x Though, to me, the catch semantics are even nicer.
This is tricky in that you have to express how types act as unit test without mistakenly pitting types against unit tests. Presenting types as unit tests is also a hard sell since given an option between the two most Rubyist will surely just choose unit tests. Types have to be shown to offer a benefit all their own and one that fits nicely into the whole iterative process theme that is so valued within the Ruby community. How about augmenting "Test Driven Development" with types yielding "Typed Test Driven Development": Unit tests serve as a rapid rapid feedback mechanism. Types offer an even tighter feedback loop as types are faster and machine checked. The goal would be to present types as the lowest level in the "Red Green Reactor" cycle. First your types have to check, then your test has to pass, finally refactoring segues right back into the type checking portion. As you can see I'm using a lot of terminology found withing the Ruby community. Haskellers are not usually down with that kind of jazz but I think that is what its gonna take if one intends to go in and try to integrate and grab some share.
Yeah, looking through these, about half have serious errors and half of the remaining ones have minor errors or infelicities. I wouldn't take these as good introductory Haskell material.
I think a good integration point would be to show off Haskells concurrent abilities. Right now long running and concurrent tasks in Rails apps use something like BackgroundRB or Delayed Job. These solutions are both heavyweight (usually piggybacks on some kind of DB) and inflexible (It gets rough when you try to get two jobs to pass data to each other -- last time I checked). Haskell on the other hand is light weight, fast, type-safe, maintainable, and very flexible when it comes to this sort of thing. Cooking up a small restful example where Haskell is used as a component to communicate with Rails in efficiently handling long running tasks might be a good thing.
What, not one based on `foldMap` yet??
If you care about portability, be careful about using sudo in your code. A lot of *nix systems don't have it installed.
I'm a mere employee, but I would hate not to work in the same room with at least part of the team. Perhaps some employers feel the same.
I think the ability to have a remote worker speaks volumes for having a fantastic communication situation, but it's often survivable without it.
Because I don't use dynamically typed exceptions in my libraries, but I need to be able to interface with others who want to in theirs or in their apps.
Whoops - I didn't initially pick up that bound and sofar are singletons not lists, so I'd just call them a and b or work out a way to golf them out of the picture. They do add distracting noise to the function definition.
If I had the experience, I would apply right away. I'm going to ask about internships.
Since we are clearly on our way to a long an unproductive argument, let's just pretend I agree with all three points.
While this seems to be an oddly unpopular opinion online, I think talking face-to-face is very important. You should certainly be able to work from home, but you should also come into the office fairly often. You're far more likely to have conversations with random people, get to know everyone well and come up with new ideas if you're all at the office rather than talking remotely.
To be fair, the description on GitHub makes it sound like the author did this as a beginner, and it may have been a good learning exercise for that one person.
You should check out implicitcad, which uses this idea for 3d modeling
I don't think it says "only for," although "ideal" has me wondering. There are plenty of people in the Haskell community that I think could do excellent research and work on DSLs without Ph.D. work. It seems short-sighted to restrict it to only those in academia, and I don't think they're doing that (and hope they aren't!)
String is not a good string type for this. Maybe use Text?
Thanks for the feedback! Is this because of encoding issues?
`String` is basically `[Char]` while `Text` has a more compact representation allowing fast operations. Both `String` and `Text` can deal with encoding, but the fastest libraries are those using `ByteString` / `Text`.
Okay, I see. I'll fix that when I get a chance. Thank you!
Starting from the bottom down *is* an interesting approach.
A rather significant hazard of this it is very hard to both throw and catch this exception. You have to explicitly name the pair of types explicitly when throwing it and you need to explicitly name the pair of types explicitly when catching. `either throw throw` has the benefit that you don't have to catch both together. Throwing and catching `Left Foo` is different than `Foo` and also different for `Left Foo` for a different _Right_ exception type, which is a recipe for confusion.
Very readable! Wish I had something more constructive to say. Do you plan on using this for anything in particular?
My only suggestion is to use something like xpath for grabbing data out XML/HTML instead of regexs. Other than that, looks easy to use! edit: Oh, and maybe change the name of the project to something more descriptive.
Overall, the code seems well-documented and clean. Great job on that. A couple of things I came across: * You don't seem to be doing any sort of error handling. The `wikiRequest` function just returns an empty string in case of errors ([WebService:65](https://github.com/guoguo12/aether/blob/master/src/Aether/WebService.hs#L65)). Maybe the functions should return `Either` or `Maybe` values in case of errors? Or is error handling not in the scope of the project (as you mention it's meant to be quick and easy to use and not full-fledged)? In that case, you may want to `fail` with the error message instead of having a silent failure with the empty string. * The `strip` function converts to `Text` merely to strip whitespace ([Parser:29](https://github.com/guoguo12/aether/blob/master/src/Aether/Parser.hs#L29)). You can achieve the same effect by using [`dropWhile`](http://www.haskell.org/ghc/docs/7.4-latest/html/libraries/base-4.5.1.0/Data-List.html#v:dropWhile) ` ` [`isSpace`](http://www.haskell.org/ghc/docs/7.4-latest/html/libraries/base-4.5.1.0/Data-Char.html#v:isSpace) ` . ` [`dropWhileEnd`](http://www.haskell.org/ghc/docs/7.4-latest/html/libraries/base-4.5.1.0/Data-List.html#v:dropWhileEnd) ` isSpace`. Of course, the issue is moot if you switch the library to use `Text` as suggested by a different comment.
masallah masallah, pegzel pegzel ozguncugum.
good luck with the application.
I kinda like gibbon xs = foldr (\x y -&gt; (+) 1 y) 0 xs but gibbon' = foldr (const (+1)) 0 is way nicer. EDIT: Or the leftward gibbon'' = foldl (const . (+1)) 0
I agree with the other comments. And in the `WikipediaPage` type, `lastEdit` should be a `UTCTime`, and `queryURI` should probably be a `URI`. Stringly-typed data is an anti-pattern, parse the data into something more structured!
Obligatory [Mitchell &amp; Webb](http://www.youtube.com/watch?v=co_DNpTMKXk)
take a look at http://www.haskell.org/tutorial/arrays.html
:)
Fair question. I don't know the answer and there are many who are more qualified to give it anyway. I just want to take this opportunity to chime in a remind folks that the choice only matters in code that actually runs often or in a time sensitive process. For example, I see too many people suggesting that authors covert all their code to a more efficient data structure when it uses `String` with little to no clue whether the change will make an ounce of difference or not in an actual run of a program that uses the code. As we all know well the author has to profile her code to determine what changes need to be made or (as I should say) are worth making.
the reason is IMO patternmatching and the fact that lists are datastructures you can easily reason about (basically the most simple recursive-datastructure you can think of). As MasGui wrote: if you really need performance/random access then there are other - more suitable - datastructures avaiable in haskell (arrays, maps, etc.)
Lists are for random access like C++ singly linked lists are: not. If you need random access, have a look at the `vector` and `array` packages. If you need bidirectional access there are *zippers*.
`&lt;$&gt;` isn't Applicative but `fmap`, and in this case `fmap = (.)`.
You will find that it's quite rare to actually store things in lists in Haskell. The closest analogue in C++ is the iterator, not the array. You wouldn't expect iterators to provide random access, lists don't either. For displaying a 'list' of things to users, it looks like you'd be better off with Data.Sequence.
I've used them. In fact, any time I want to improve the performance of a piece of Haskell, the first thing I do is convert all the lists to one of the two, and they are pretty much drop in replacements with an import or two. Even when I'm not doing any random access at all this typically reduces the run time by two thirds. My question is why can't all Lists be vectors under the hood?
I would find this confusing given the common usage of `Either` as an error monad with only the first component being thrown. If you feel a need for this, why not use a different name?
I also asked myself that question many times and i don't have an answer. I think, lazy evaluation might have something to do with that. I mean, you can build infinite data structure with lits easily. And i guess, that might be complicated with array's? 
That's true. I forgot the `fmap = (.)` bit. I only mention applicative because it's so common in applicative style code. That's a bad habit. :D Edited to reflect.
In addition to what others have already said, the important operation that immutable, singly-linked lists support is that creating a new list by prepending an item to an existing list is O(1). The same is true for splitting an existing list to a (head, tail) pair. However, any kind of update you do to in terms of an existing, immutable array is O(n). TL;DR: Arrays are a sensible default for mutable data, but singly linked lists usually make more sense for immutable sequences.
Your use case appears to involve some premature optimisation. If you are displaying a list to the user, your list is probably not that long. Even if accessing the chosen item is O(n), it really won't take that long. The key is architecting the code so this happens only once. Not every line of code has to be optimised. Only the ones that make up the bottleneck do. I don't agree with all this "Lists and Strings are rarely used in production". We use them all the time. If you aren't using lists and strings in production in non-performance-critical code, then you are wasting you development time. 
Haskell uses lists as a control flow structure. Lists are used instead of for / while loops. You don't store things in lists. You process things. A list value is merely a computation step. They are rarely fullly evaluated. More frequently, laziness transform their usage in iterations. As such, you don't need random access in lists. You only need to iterate over them once. Iterating twice is often bad, because it may mean that your program stores a reference to the fully evaluated spine of the list (ie you have a ref to all of its elements). This means more memory usage, and more work for the garbage collector. Arrays are not very useful because immutability implies that any modification requires a new copy each time. Mutable arrays are sometime used when needed (ex: in-place quicksort). But they are contained in a ST monad. Trees are used to implement dictionnaries, sets and heaps. Those are usually very good replacement for c++ arrays. They have better semantics (ie more specialized) and allow very good immutable implementations.
That's usually not the first thing you should try for improving performance. If that's what you do and it works, it probably means that you are trying to use lists as if they were C arrays. When lists are used idiomatically - such as for iteration with automatic memoization and de-memoization - you'll rarely want to replace them with something else. Once you get the hang of picking a simple and natural data representation for an algorithm, you'll find that optimization usually starts with thinking about the algorithm, not the data structure.
I use the `container` package far more than `vector` and `array`. I find that those make it easier to keep a natural pure functional feel to the program. A lot of work has gone into optimizing them, so it's very rare that you really need C-like arrays. It's usually a premature optimization.
To help myself keep that in mind, I make it a habit always to import `&lt;$&gt;` from its home in `Data.Functor` and not its re-export in `Control.Applicative`.
Lists are a very natural algebraic datatype, and one of the simplest recursive datatypes you can imagine. Lists are not suitable for every usecase, but simplicity brings benefits all of its own, so is not to be underestimated.
O(log(n)) is O(1) for all practical values of n ;-) I'm using `containers` as the go-to library as well, but `vector` has a list-like API (and fusion) as well. It's really more a matter of taste sometimes.
Thanks! Not really...
Okay, thank you! That makes sense.
(I say json because the MediaWiki API allows users choose between several data formats, including XML and JSON.)
It's a nice study, a nice introduction to some ideas of the Haskell language and the functional programming, it introduces the young readers the often useful idea of functions that define areas, and higher order functions that operate on them. It also shows the culture clash between people used to Ada mindframes and functional programming. But I don't know how much scalable (both in number of areas and in attributes to attach to areas) that solution with functional areas is compared to more traditional solutions in Ada. This is probably a common concern for a programmer used to OOP. This Haskell version is similar to the code shown in the article: http://www.fantascienza.net/leonardo/naval_original.hs Here I have done small changes in the Haskell version, added types for the functions, etc: http://www.fantascienza.net/leonardo/naval_modified.hs And this is a D language translation, a system language that can also be used as a functional language: http://www.fantascienza.net/leonardo/naval.d The idea of functional areas is visible in some 3D programs and algorithms (like: http://en.wikipedia.org/wiki/Constructive_solid_geometry), you can also see it used in this little PicoLisp solution of a Rosettacode problem, it shows how much simpler this code becomes compared to most other solutions: http://rosettacode.org/wiki/Yin_and_yang#PicoLisp
&gt; So, anything that is text should use `Text`. So do most Haskell developers not use `String`s very much at all?
&gt; `O(log(n))` is `O(1)` for all practical values of `n` ;-) Right :) Well, for *most* practical values of `n`. I didn't say that I *never* user them.
tl;dr
&gt; Coming from a C++ background, I don't understand why Haskell uses lists everywhere instead of arrays TLDR answer: Nope. If want random access arrays, we use arrays (Data.{Array,Vector} etc). Haskell lists are not used as arrays, nor as `std::list`. Think “stream processing” or some such.
The catch is that you can't add compiled splices to `HeistState` after you had ran the `initHeist`. Even in snap.
Sure if you want to catch exactly this either again, but the idea is you can catch either side separately, or any N together.
Time and space usage are factors which directly impact the composability of code. Poor performance is "leaky" in the same way that a bad abstraction is. And steering users to inferior types/libraries when there are more correct and performant replacements doesn't help anyone. The problem is that every project is also a learning experience. Telling everyone to keep their Haskell training wheels on until they've proven that they're not working on kid stuff is often missing the point. Ideally, you master techniques for achieving high performance *before* you commit to projects that require them, not after.
Persistent data structures are definitely possible in C++, but memory management is a problem. Persistent structures don't have a clear owner, so you wind up dealing with shared pointers or some other method of garbage collection.
&gt; either in a language pragma at the top of your file, or in a command line flag Or in the `extensions` field in your cabal file.
Of course almost everything is possible in any language, the difference is the typical usage and convenience (and safety)
Have you read your [SICP](http://mitpress.mit.edu/sicp/) today?
String is good for short-lived configuration values and being the default type represented by `""` they make a good external API type. In general, I think String can be OK for representing metadata. For actual data you want to decide whether it's semantically better to think of it as a binary stream where non-UTF8 characters will occur with reasonable probability—then use ByteString—or whether your data should be thought of as encoded text of some kind where Text is better.
That's unavoidable fro compiled splices. However, you can work around it by binding new splices inside another splice.
Is the condescension necessary?
That's been my experience too but interestingly the project which this story is about ([stan](http://mc-stan.org)) uses AD which I think they wrote themselves as they couldn't find a C++package that did everything they needed. So in theory they shouldn't have needed such wholesale refactoring of the distributions.
If you're interested in a data structure with better access-element and "set"-element complexity, you should check out [Okasaki's Random Access List](http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=DF0F650462880E779E51380281DB05D6?doi=10.1.1.55.5156&amp;rep=rep1&amp;type=pdf). It supports *O(1)* head and tail operations, while also supporting *O(log(n))* retrieve element, set element, and length. The data structure is purely functional, which makes it compatible with a Haskell coding style. Here's a [package](http://hackage.haskell.org/package/random-access-list-0.2/docs/Data-RandomAccessList.html) that implements them, but they're easy enough to RYO.
As you can see from the URL above, the Hackage frontend server now has functioning SSL support. There is also SSL support enabled on the main wiki: https://www.haskell.org/haskellwiki/Haskell - note that http://haskell.org just 301s to the WWW domain I believe, but it should probably be fixed to properly route HTTPS as well. This is why it behaves awkwardly. The configurations will still see some tweaking I'm sure. I'm sure the next question on everyones mind is when we get SSL support in `cabal-install`. Well, that's more complicated and I don't know. The primary holdup is we don't have a comprehensive SSL solution in the Haskell Platform yet. When this will happen remains to be seen. In the mean time, we can do something else (that has been in the plans for a while,) which is to cryptographically sign the package index, containing all the package hashes. This would also mean we could have 3rd party untrusted mirrors, ensuring they don't compromise package integrity. This is a bit easier from a package standpoint, since there are many free and robust signature schemes we could include right in `cabal-install`. To this end, I've written up a portable [ed25519](https://github.com/thoughtpolice/hs-ed25519) package that I think could be a candidate to be included in `cabal-install`. I'd like to look into doing this soon(ish.) I'll also be rolling out to http://ghc.haskell.org soon, but it may require some special attention for the `git` HTTP transport, etc. Thanks to John Wiegley for starting the rollout last night. Bonus points: the Hackage instance supports TLSv1.2 with forward secrecy ciphers (tested Win8/Chrome 30) and SPDY. I'll get around to configuring the remaining servers to support this as well. There is no SSLv3 support, so IE6 users may be out of luck.
By `ad`, I meant the eponymous Haskell package, not the technique in general. =)
Chrome warns that there is insecure content on the (front) page. The logo from http://industry.haskell.org/ihg-logo.png is not available over https, so that's a bit disappointing.
This is technically a different server which I don't have access to anyway. I have already brought it up on IRC. We should just copy it over to the actual server, and I could, but I don't know how to deploy the *new* server myself with the updated index page. All the other pages come up clear however from my browsing (the Hackage server is pretty standalone, it just happens to have that 1 image file on the index page.)
&gt; lists support that arrays don't is O(1) insertion in the middle Did you mean "in head"? Since in the middle is O(n).
Maybe it is just a matter of vocabulary, but I didn't write many haskell applications when I used a list to store a collection of stuffs. I tend to focus more on data flow than on data collections. When I think about a value as a resource for the application, I usually need to query those resources to perform operations, and then linked list are not good enough anymore. "A linked list isn't some inferior data structure that's only good for loops." The only sensible thing to do on a list is a standard operation like filter, map or fold which are all used to replace loops in imperative languages. I am not saying that the List is inferior. But using it for something like a dictionary is a bad idea. 
I'll be a bit more kind than chrisdoner, and call that the "party line", especially as you go farther back in time. I'd add to some of the other answers that for a long time, the thought was that these lists _would_ be adequate for many more things than they actually are. Consequently lists ended up everywhere for what I would call _legacy_ reasons, including as a special case in the syntax (where the type `[] a` can be expressed as `[a]`, which is a special case I'm not really in love with), and as an at-times excessively-privileged type in Prelude and such. As it has become obvious that they could not actually be beaten into sufficient submission (especially with regard to performance at the highest levels), additional structures have been added, most notably vector, bytestring, and Data.Text, and further optimizations have been made by taking advantage of various type classes like "monoid" to create [builders](http://www.haskell.org/ghc/docs/7.0.3/html/libraries/ghc-binary-0.5.0.2/Data-Binary-Builder.html) of these things, which exhibit both performance and composition abilities that _can_ theoretically be obtained in a mutable language, but are significantly more challenging. There's been a lot of refocusing in the community to correctly determine what typeclasses a particular algorithm requires and specifying things in terms of those, instead of calling for concrete lists directly (which unnecessarily limiting in many cases), along with continuing work on special-purpose structures for certain specialized needs (like [repa](http://repa.ouroborus.net/)). I think we're already a long way down the road of not over-privileging lists, and that you'll be seeing them settle into roles where they are actually the correct solution, instead of being the default as they were for so long. All that said... in Haskell, a vector or array should still not be your "default" data structure for "normal" programming (i.e., not numerical, or graphics manipulation, or something where it really is the optimal structure). If you're reaching for them all the time, that's a _serious_ code smell, and a good sign that you may be programming C++-in-Haskell.
Thanks for the detailed response. I can see why silent failure would be unhelpful. I'll go with using `Maybe`/`Either`.
Oh, that's a good idea. I think I'll adopt that habit.
If `cabal-install` downloads the packages and the signature over http, there's not much point to signing, is there? Or am I missing something?
[`xml-conduit`](http://hackage.haskell.org/package/xml-conduit)'s cursors might be what you're looking for. It adopts concepts from Xpath. Example: cursor $// laxElement "tagName" &amp;/ laxAttribute "attrName" Will return a list of values of attributes `attrName` under tags `tagName`. ^Disclaimer: Untested. 
The master hackage server generates a tarball containing the index, and md5sums of every package. It also signs this with a secret key. You distribute a public key with `cabal-install` itself so it can verify that any downloaded index has a legitimate signature from the real server. When you `cabal update`, it checks that the downloaded index and signature are legitimate w.r.t the servers public key. When you install something, you can then check the hash of the download (since you already got a legitimate hash from the server before.) There is always the chance of course someone intercepts your very first download of `cabal-install` (when you first download it, or update or whatever) and replaces it with one with nefarious keys and then intercepts the rest of your traffic, but you're realistically no worse off here than you were before. And if you are that worried, you can download the source tarball (with the public keys) over an SSL webpage now, I guess.
This is if you already have a pointer to the middle element. Arrays can also insert at head if they over allocate like c++ vectors do.
Ah, cool. I never thought about being able to change a list but still have an exact representation of the old list available. That does seem useful.
Your second point makes a lot of sense. Not sure about your third point though, lists in Haskell don't support random update at all so O(n) random update for vectors doesn't seem like a problem
From a theoretical perspective, lists are an extremely simple and natural data structure to consider. They are, in an abstract sense, a generalization of the natural numbers. The natural numbers are the least fixedpoint of the polynomial functor `1 + x`. In English, that is the Maybe functor. This gives us something like an encoding for natural numbers: `0` is `Nothing`, `1` is `Just Nothing`, `2` is `Just (Just Nothing)`, etc. (Important technical details omitted). To get the type `List a`, we jiggle the `x` a bit to get `1 + a * x`. Here, the `*` means a pair (or product) type. In Haskell, this functor would be something like: data ListF a x = Nil | Cons a x (Do you see how similar this is to the definition of Maybe?) Now, we can construct lists: `[]` is `Nil, [a1]` is `Cons a1 Nil`, `[a1, a2]` is `Cons a1 (Cons a2 Nil)`, etc. The important technical detail I mentioned a second ago is that we actually need to take least fixedpoints of these functors. That is, if we want to treat `Nothing` (standing for `0`) and `Just Nothing` (standing for `1`) as the same type, we need them to be the same type. Least fixedpoints are a theoretical tool that lets us do that in a sane way. Haskell, however, fudges them a little bit, because there's nothing wrong with an expression that's `Just (Just (Just (...)))` ad infinititus. There is a related notion of greatest fixedpoints that gives us infinite objects, but the rules are slightly different. The rules, however, are just in place when we want to make sure that our programming language is consistent as a logic -- which, thanks to general recursion, is not the case in Haskell, or most languages.
Lists do support O(n) updates, updateAt n new [] = [] updateAt 0 new (x:xs) = new : xs updateAt n new (x:xs) = x : updateAt (n-1) new xs but that won't fuse very well (even if you rewrite it as a fold, I think). Also note that reading lists is still O(n) for random access, while vectors are O(1).
That will just create a new list if your list is immutable - which it is in Haskell. 
I don't disagree with the substance of what you're saying, but he actually might expect [random access from iterators](http://en.cppreference.com/w/cpp/concept/RandomAccessIterator).
Well, it's necessary to have access to the old one because of referential transparency/purity/etc. So this is more a consequence of the properties of the language, not a "feature". 
Are we talking about what you do and personally would prefer, or _actual_ Haskell code, as written by the Haskell community in general? This has piqued my interest enough that I'd be interested in doing analysis of all of Hackage, but lists are so ubiquitous you only have to look as far as [the GHC API](http://www.haskell.org/ghc/docs/7.6-latest/html/libraries/ghc-7.6.3/) to see them. Just click on a random module and you'll see list types in almost every one. I'm genuinely baffled by your claims that people don't use lists as data structures when simple experience of looking at a few codebases will show you that clearly they _do_. Are you simply towing the party line, as Jerf claims? If so, fair enough—many newbies use lists prematurely when a vector or a set would do better, so it's a line worth towing. But be clear that's what you're doing. Otherwise it's simply false.
Sure, it's a consequence of the properties of the language. But the properties of the language *are* the feature.
To be clear String is exactly [Char]. You can use the names interchangeably. It's called a type synonym.
&gt; Time and space usage are factors which directly impact the composability of code. That's not true. You are trying to convince me that I must keep performance in mind while coding. You are very wrong. Concentrate on what you are trying to build, performance is an after thought (unless speed is your goal which is not the case most of the time). If you just happened to use String then sure you can go an blindly change all your code to "A more efficient data structure" but you may be wasting your time. Correctness is a different concern please don't conflate them to further your already imaginary concerns. &gt; steering users to inferior types/libraries when there are more correct and performant replacements doesn't help anyone. Are you making this stuff up. Who is doing that? At what point did I say use X over Y. Oh, that's right I didn't. &gt; Telling everyone to keep their Haskell training wheels on until they've proven that they're not working on kid stuff is often missing the point. Who is saying this stuff. Certainly not me. Please, all your points jump to grand conclusions that never actually appear in what I posted so, again, please just stop. Telling people to not worry themselves with performance concerns before necessary is actually asking them to "Take the training wheels off". So please just stop. You know you're right though. Every time I spot a piece of code that can be parallelized I'll recommend that the author do so because it is a &gt; more correct and performant replacement You are kind of just going off on me for no reason.
Not Haskell specific, but Okasaki's "Purely Functional Data Structures" has a lot of fascinating discussion of efficient persistent data structures.
Just to share my cents, which are a bit similar: If I want detailed error handling (note that I don't always want), I don't use exceptions, but return Either e1 a, where e1 is an ADT specific for that function. A function calling two other functions with error types e1 and e2 lifts the left side of their return value to an (again, specific) e3 ADT.
I would really appreciate something on the `time` package.
Count me in if you want some help. I loved the series last year.
I have never heard of Frege, but Haskell on the JVM sounds amazing. I will check it out.
Nice. I will definitely check out their work.
or better thyme
If you can parse JSON, you should consider checking out [Aeson](http://hackage.haskell.org/package/aeson). It's awesome! 
vinyl, layers, diagrams, folds, linear
... which is probably why OP wrote after "O(1) insertion in the middle" that "lists are immutable so Haskell doesn't even take advantage of that". :-) 
Another suggestion is that for the state updating, you really should consider the `Control.Monad.State` monad since this is the exact intended use case. There is a good chapter in LYAH which teaches it.
Off the top of my head: repa, async, gloss, unordered-containers, really basic ghc-api (maybe a bit rough for the 24 days of hackage format though), hlearn (already mentionned this one on twitter), hflags, vector, diagrams (as dtellerulam suggests) Good luck and let me know if you want help on some of those. 
Congrats! One question - why a PDF instead of a post on your blog, etc? It seems Haskell has a larger number of paper writers than other languages.
Doh!
I'm suspecting that this might have been made public by accident. Likely Google's default is for a video to be public. Almost feels like I'm intruding on a private conversation... Anyone else feel that way? Apologies if I'm mistaken.
I'm really looking forward to these. =) Ollie did a wonderful job last year. No pressure. ;)
Self promotion here, but I would love you to take a look at jmacro! I'm not volunteering to write about my own library, of course, but I'd be happy to answer any questions or help you (or someone else) out with it.
Self promotional: cassava
&gt; Many people find blogging an intimidating experience, and commiting to a full blog can feel like a big undertaking. I think 24 Days of Hackage can be a great way to get a taste for what it’s like to share your thoughts with the masses, and experience the tremendous support of the Haskell community. An excellent idea.
Three reasons off the top of my head: * We initially wrote it for a class. * Neither of us have blogs. * I don't know any blog software that uses LaTeX.
Repeating here what I tweeted for more commentary: LVish/Lattices, linear, Hashtables, HLearn, threepenny-gui, unordered-containers, persistent/groundhog/esqueleto, repa/accelerate
&gt; Neither of us have blogs. You can set up a blog using static pages on github, it's pretty simple. &gt; I don't know blog software that uses LaTeX. You can usually compile LaTeX directly to HTML.
Oh, I got an intuition now. But I think we'd better not express ourselves like this :)
More seriously, I'd be more than happy to help out answering questions on anything I've written especially if it'll help you along in this series.
Of course. I never use exceptions unless I'm forced to by some library I'm using.
A linked list *is* an inferior data structure. It has terrible locality unless you allocate all the nodes contiguously, traversals involve a linear number of indirections, you have linear space overhead to store all the pointers, you can’t use SIMD on it directly...
Another option that unifies Maybe/Either is the [failure](http://hackage.haskell.org/package/failure) package. See ezyang's [blog post](http://blog.ezyang.com/2011/08/8-ways-to-report-errors-in-haskell-revisited/) on the matter
The choice of function for tarball hashes is, ultimately, arbitrary. We could use BLAKE or Keccak or just SHA512 or something. MD5 is just an example and people know what it is.
I think dons covers this in an old presentation: http://donsbot.wordpress.com/2010/08/17/practical-haskell/
That sounds like a good idea, though I can't seem to get any more performance out of `insert`.
Wouldn't that need to be an *indexed* State monad, since you have different state types `inputState` and `outputState` before and after executing the action?
Hmm yeah, I was trying to be descriptive. Edited, thanks.
Have you tried `pandoc`? You can convert pretty much anything to anything using the `pandoc` utility. Just do something like: pandoc -i tutorial.tex -o tutorial.html It will probably generate something halfway decent, and then you can play with `pandoc`'s other features to get it prettier.
`safe`, `chart`, `cereal`, `layers`, `hxt`, `data-(t)reify`, `liquidhaskell`, `sbv`, `GHC.Generics` (?) (not even mentioned in last year's post on base)
According to the paper, yes.
Gloss! Repa! Accelerate!
Combining the replies by Hrothen and Tekmo: For a `pandoc`-backed static blog, you will definitely want to consider [`hakyll`](http://jaspervdj.be/hakyll/index.html).
I'm no game programmer, but you really should mention stuff like the [ST monad](http://www.haskell.org/haskellwiki/Monad/ST). State doesn't give you true mutable state, where ST allows you truly mutate stuff in place. This has some performance implications for games that are more demanding than the one you made.
A paper like this certainly doesn't need to delve far into the intricacies of monads, but I feel that the continual insistence that they're difficult to explain and "involve too many long words" does more to drive people away than any actual problems with monads.
I wrote the first version of this a few years ago in JavaScript. [Look at the uglies](https://github.com/bergmark/Hangeul/blob/master/lib/Hangeul.js). Fixed several bugs by porting it to Haskell :-) 
More self promotion: fay!? and/or an overview of haskell or written in haskell to-JS compilers, we are approaching critical mass :) 
I'd be happy to help out with persistent/esqueleto. I'm no expert, but I've spent a fair bit of time with it. Other postgres compatible libraries are also interesting, as long as they support joins! 
why am i getting downvoted? is thyme not the better, more performant "successor" of time?
Doesn't matter. At lot of use cases build a huge memory structure for repeated use. Insert performance is not important for these use cases, but memory use and member test is.
Looks like others agree with you - I've added them all to the list! `layers` has always interested me and I haven't had chance to use it yet. Perhaps this will give me an excuse :)
I think `ghc-api` is a bit rough, but I might look into `hint` instead - which I find can be pretty cool. Thanks for re-iterating your Twitter requests - looks like I've got these written down.
I'm sure we will venture into That Part of Hackage at some point this year - so I'll be sure to pick your brains! Thanks for the support :)
I don't talk for the haskell community in general. I don't have any authority about that. I talk about the way I see things, and how I believe things should be. I am not saying that lists are not often used in haskell. I know they are used. I use them myself. I just don't use them to model a collection of stuff. I was merely answering to Dooey's first question which was "Why people use lists instead of arrays in haskell. Lists have bad random access performances.".
Great list - I've popped them all on my list, and it seems like you aren't alone :)
Why do you think it needs to be wrapped into a return?
The time library is very standard and widely used. I have never found it to be the performance bottleneck. I know that some of the finance people do need the thyme library, but for most people is the huge effort of moving to a new time library worth it? In fact, if thyme is API-compatible or can be made so, why not just make it a future release of the time library? EDIT: btw no I'm not one of the downvoters :)
i don't care about the downvotes, i wanted to know whether i am wrong.
Yes I second that. The time library is one of the greatest examples of the semantic power of good type system, without going into any fancy type trickery. Yet it is much aligned by people coming from the buggy time libraries in other languages not understanding our time library. So we need a lot more nice documentation about the time library. [roconnor's wiki post](http://www.haskell.org/haskellwiki/Time) is great; another post in the style of ocharles, and more visibility by being part of the 24 days series, would be also be great. EDIT: I'd be happy to help out with anything related to my timezone libraries. What roconnor wrote in his wiki post is a good summary.
I see a lot of you have comments on the code itself, so I just wanted to mention that nothing gets your point more across than patches (to &lt;bweakfwu@plaimi.net&gt; for instance). While I appreciate that we should have learned the State monad by now (and Lens, and using record, and a lot of other things I'm sure), it's a lot to take in. And after all, we're supposed to make a *game* as well, so we need to implement actual gameplay stuff too sometime. Patches would take precedence and force us to dive into it. And also, we'd just love to collaborate with people and learn from them. :-) P.S. Keep the coding suggestions coming, by the way. I am taking notes! Just don't expect them to be implemented right away. ;-)
Eh, I'll try and do this if I get the time. But I'm also somewhat in the same position. 
&gt; putStrLn stdout $ "..." putStrLn $ "..." 
Well I feel silly, that was pretty obvious... lol
Well... mathematicians will tell you that counting a collection involves creating a bijection between its elements and a subset of 1, 2, 3..., so length [] = 0 length xs = fst $ last (zip [1..] xs) Sorry about the special case for [].
You can have mutable state with State, either by using modify or explicitly putting and then getting the updated state. &gt; flip execState 0 $ do { modify (+1); modify (+1); modify (+1) } 3
Thanks, that seems so obvious. It's actually hard for me to recall why/how I made the mistake.
bound =D
I recently made a small project using gloss + repa to make a game of life renderer. Message me if you think that would be useful for you.
You should get comfortable with this feeling, because in Haskell land you will feel that way quite often. ;) Haskell is quite a different beast of programming language than most other ones, but it's also very satisfying if you've wrestled the beast.
I hope this one's better than the Swedish translator from [Why Haskell is Great - in 10 minutes](http://www.youtube.com/watch?v=RqvCNb7fKsg).
The linked list provides pretty much the fastest and simplest possible lazy immutable data structure for the `cons` and `uncons` operations. Those two operations are the bread and butter of recursion in a purely functional language. It is better to think of list as a building block for control flow rather than as a structure for storing data. We have lots of good data structures implemented as libraries, such as those in the packages `vector`, `containers`, `unordered-containers` and `hashtables`.
https://npmjs.org/package/unidecode 
Interesting and I've used it in a recent project. That would should be easy to fit in!
Hey, author of `layers` here! I'm currently working on a new release that changes/renames a lot of stuff. The only thing holding it up is the documentation. The problem is just that as I try to explain what things do in the documentation, I struggle, and when I finally come up with something, I realise it should have been named something else in the first place. So then I rename and refactor, and then the documentation I've already done is out of date. I'd love to just release, but the first release had really good (or at least ambitious) documentation and I'd like to maintain that standard. However, I don't want to discourage you from doing a writeup about it. If you do, I volunteer to amend it to work on the new version when it's released. I'd like to say that it will be real soon now, but my life is really chaotic at the moment so if something comes up (e.g., it seemed like I was getting evicted two weeks ago) that could set it back a few weeks. EDIT: I didn't actually read your post until now. I see you're asking for people to help do the writeups for you. In that case, I'm happy to do the `layers` one. Do you have some kind of deadline?
After reading some outstanding haskell libraries, I've taken to he following conventions * Inner "worker functions" == "go" * loop variables named after what they store * [is == "ins", os == "outs", c == "count", xs == "some inner list"] * If it's an accumulator, it's acc, Here's some examples: everyNth :: Int -&gt; Int -&gt; [a] -&gt; [a] everyNth n s is = go s is [] where go _ [] os = reverse os go 1 (x:xs) os = go n xs (x:os) go c (x:xs) os = go (c-1) xs os scoreCandidate :: Candidate a -&gt; Float scoreCandidate = BS.foldl accum 0 . _result where accum acc c = acc + scoreValue c &gt; I am also worried about losing tail-recursion in where clauses. At least in the below code it is unambiguously the last statement to call itself. GHC should perhaps have a "Warning: not tail recursive" for recursive functions. I can only assume you're talking about [this bug](http://ghc.haskell.org/trac/ghc/ticket/5997), which has been fixed since 7.4.2. in the case of everyNth, GHC may wrap the inner function with a thunk, but the inner function will be tail-recursive.
To expand on that, Strings are implemented as singly linked lists of boxed characters (which even unboxed are 31 bits wide). Bytestring and Text are souped up pointers to byte arrays.
Great job! Although I'd love to see everyone simply learn 한글 because it's such a beautiful writing system :) Maybe this will, in fact, help with that!
I think it's definitely a popularity thing and a complexity thing. It's better to cover `time` before `thyme` as `thyme` essentially just extends `time` with nanosecond precision, lenses, and `vector-space` instances.
That only does one direction, right? This library goes both directions.
We'll likely be incorporating a couple of the polymorphic name manipulation tricks from this into `bound`.
Be sure to check out the [differences between Frege and Haskell](https://github.com/Frege/frege/wiki/Differences-between-Frege-and-Haskell)
That's a pretty high bar to set
Have you run into any issues with type erasure with Frege? One of the problems that plagued Scala (don't know if it is still an issue) is that a lot of pattern matching was crippled because type information was removed when compiled to the JVM.
Yes, type erasure is done, mainly because it is not possible to represent higher kinded types correctly. Laziness does not make thigs easier either, as any value can ayppear as X or Lazy&lt;X&gt; in Java. But to my knowledge, pattern matching it is not crippled in Frege.
You don't ask for much, do you? I kid, thanks for the list!
Well, we can dream, can't we?
If you're not making mistakes, you're not trying hard enough! :)
(Indent by four spaces to retain your original formatting.)
upvoted for truth
I think that's a little narrow-minded, especially with the growing popularity of alternative languages built on it. What's the problem with the VM itself (as opposed to the original language built for it)?
Still won't have proper tail calls for at least another 3 or 4 years.
It's a purely emotive expression; it isn't *truth-functional*, much less true.
Deadline is the 24th Dec ;) But yes, I'd accept a post on that!
I think your definition for `sternBrocot` is wrong: sternBrocot :: Tree Rational sternBrocot = unfoldTree (\(Pair a b) -&gt; Just (Pair (a+1) b, a % b, Pair a (b+1))) (Pair 1 1) Just by inspection: Pair 1 1 -&gt; Just (Pair 2 1, 1 % 1, Pair 1 2) Pair 2 1 -&gt; Just (Pair 3 1, 2 % 1, Pair 2 2) Pair 1 2 -&gt; Just (Pair 2 2, 1 % 2, Pair 1 3) So you've got the subtree generated from `unfoldTree (\(Pair a b) -&gt; Just (Pair (a+1) b, a % b, Pair a (b+1))) (Pair 2 2)` repeated as both a child of `1 % 2` and `2 % 1`. This also doesn't jive with the diagram just below it at all, which shows the children of `1 % 2` as `1 % 3` and `2 % 3` and the children of `2 % 1` as `3 % 2` and `3 % 1`. To get that diagram, you want something more like: sternBrocot :: Tree Rational sternBrocot = unfoldTree (\(Q a b la lb ra rb) -&gt; Just (Q (a+la) (b+lb) la lb (ra+la) (rb +lb), a % b, Q (a+ra) (b+rb) (la+ra) (lb+rb) ra rb)) (Q 1 1 0 1 1 0) Which generates Q 1 1 0 1 1 0 -&gt; Just (Q 1 2 0 1 1 1, 1 % 1, Q 2 1 1 1 1 0) Q 1 2 0 1 1 1 -&gt; Just (Q 1 3 0 1 1 2, 1 % 2, Q 2 3 1 2 1 1) Q 2 1 1 1 1 0 -&gt; Just (Q 3 2 1 1 2 1, 2 % 1, Q 3 1 2 1 1 0) Q 1 3 0 1 1 2 -&gt; Just (Q 1 4 0 1 1 3, 1 % 3, Q 2 5 1 3 1 2) Q 2 3 1 2 1 1 -&gt; Just (Q 3 5 1 2 2 3, 2 % 3, Q 3 4 2 3 1 1) Q 3 2 1 1 2 1 -&gt; Just (Q 4 3 1 1 3 2, 3 % 2, Q 5 3 3 2 2 1) Q 3 1 2 1 1 0 -&gt; Just (Q 5 2 2 1 3 1, 3 % 1, Q 4 1 3 1 1 0) Q 1 4 0 1 1 3 -&gt; Just (Q 1 5 0 1 1 4, 1 % 4, Q 2 7 1 4 1 3) Q 2 5 1 3 1 2 -&gt; Just (Q 3 8 1 3 2 5, 2 % 5, Q 3 7 2 5 1 2) Q 3 5 1 2 2 3 -&gt; Just (Q 4 7 1 2 3 5, 3 % 5, Q 5 8 3 5 2 3) Q 3 4 2 3 1 1 -&gt; Just (Q 5 7 2 3 3 4, 3 % 4, Q 4 5 3 4 1 1) Q 4 3 1 1 3 2 -&gt; Just (Q 5 4 1 1 4 3, 4 % 3, Q 7 5 4 3 3 2) Q 5 3 3 2 2 1 -&gt; Just (Q 8 5 3 2 5 3, 5 % 3, Q 7 4 5 3 2 1) Q 5 2 2 1 3 1 -&gt; Just (Q 7 3 2 1 5 2, 5 % 2, Q 8 3 5 2 3 1) Q 4 1 3 1 1 0 -&gt; Just (Q 7 2 3 1 4 1, 4 % 1, Q 5 1 4 1 1 0) I haven't proved that this does generate all positive rationals, but it does match up with the rationals in the given diagram. 
A general solution to lack of tail call optimization is definitely a problem, but I agree support for it will probably come not too far in the future. Even now, some of the JVM languages are able to compensate for its lacking by using trampolining in some cases (there are problems doing it in (albeit common) cases like non-final methods). I think both Groovy and Scala have some support for this; if I understand correctly, Clojure does not.
I rather doubt that will happen. I imagine the extent of functional programming in most Java for quite some time will be, 'hey, these first-class functions make some collection code nicer.' And that doesn't typically cause you to run into stack issues. Lack of proper tail calls is something that can even be seen in current and past Java, but people are used to just falling back on loops, so they don't notice it. On a completely different note, Java's garbage collector is not set up to do certain things that lazy languages want, and probably never will be. So Frege and other such languages will probably always be second-class citizens in that respect.
It's good to know that feeling that way is normal. I definitely get the "elusive obvious" feeling quite a lot with haskell.
It's not truly mutable state though. It's more like sugar for an extra hidden parameter. You can't truly destructively update an array with it like you can with ST.
Freges appproach to make up for the tail call problem is described here: http://stackoverflow.com/questions/10008673/does-frege-perform-tail-call-optimization
Can you summarize what's new here? I skimmed the paper but so far it just looks like the usual polymorphic recursive DeBruijn indices stuff..
You're being downvoted because everyone here knows about LYAH. It's in the sidebar, even.
That isn't terribly surprising. They did most of the development of this on [a branch of `bound`](https://github.com/np/bound/tree/injections). ;) Many of the parts of the paper, like closed = traverse (const Nothing) the use and name of `(&gt;&gt;&gt;=)`, etc. come directly from `bound`, names and all. The main addition is to lean on a Data Types á la Carte approach to make it so you can talk about inclusion of an environment in another. `bound` didn't bother with that vocabulary, because it was originally concerned with demonstrating you didn't need anything outside of Haskell 98 to work with scopes effectively. They do have [a cute hack](https://github.com/np/bound/commit/4d3fe5a083a85de10e83d0fb7aad43a976df490e) that works for a single variable `Scope` that lets you ensure you deal properly with the extension of the environment, though.
If your dataset is small (&lt; 1 GB), you might like to try [acid-state](http://hackage.haskell.org/package/acid-state). Really fast and easy to use.
Named after Gottlob Frege?
Sure thing: &gt; The Frege programming language is named after and in honor of Gottlob Frege.
That's interesting. I would have expected a runtime error if you deserialized it to the wrong type. Surprisingly it parsed correctly (although it was still invalid, as the author noted). Perhaps serialized values should contain type information so that this kind of thing could be avoided.
My guess is that people thought you were making a random drive-by pun, rather than referring to an existing package which was named as a pun. I have no idea how to defend against that sort of downvoting. I kind of hope that's the reason and the someone who thought that there was some issue with thyme versus time voted you down rather than commenting with their opinions on the matter.
한글 (Or Hangeul) is the name of the Korean writing system :p I wish I still saw it as a bunch of funny shapes because that's what intrigued me enough to start learning it http://en.wikipedia.org/wiki/Hangul
I've prototyped polymorphic variants [here](https://github.com/paf31/purescript/tree/variants). If you had any feedback, I'd be really interested to hear it. The tricky bit is that I had to get rid of constructors with zero arguments, like *Nothing* in data Maybe a = Nothing | Just a 
It might help if you described what sort of stuff you're trying to do. For example, if you are trying to extract certain parts of an XML document without worrying about validation and such then a tagsoup type library would likely be simpler to use. I probably can't help you directly since I don't do much with XML, but with more information hopefully others will!
I think that the behavior you observe is due to the fact that internally, `Data.Map` stores all the keys in sorted order. Thus, when it is serialized, it outputs a sorted list of keys and values. Because the deserialization function uses the usually true assumption that its input was the result of serializing something of the same type, it can assume that the serialized list is already sorted, thereby deserializing asymptotically faster than would be possible otherwise.
While that's weird behavior, it's also kind of weird code. Serialization is a major system boundary and thus needs to be done really carefully. Any parser function should be treated with a lot of scrutiny. As seen here, `decode . encode` is not unlike a wonky `cast`.
&gt; takeUntilIncluding pred = foldr (\x ys -&gt; if pred x then [x] else x:ys) [] Isn't that just: takeUntil p = takeWhile (not . p) ?
That is what Cloud Haskell does. But even then somebody may spoof the type descriptor and end up with a "mind bending" experience. There is no way this can be made SafeHaskell unless you come up with a typeful physical layer (i.e. electrons, photons, etc).
I think this is a nice tutorial: http://www.cs.york.ac.uk/fp/HaXml/icfp99.html Otherwise I agree, I think the XML libraries are difficult to understand too.
I haven't had time to read the code yet, but why did you have to get rid of nullary constructors like that?
I saw you post about this yesterday on Twitter, and just wanted to say I would be happy to take over coverage of one of the packages this year! I haven't written a blog post in a while and have been looking for something to write about. My blog is located at http://5outh.blogspot.com if you want to check out my writing; let me know if you want some help / what with! I'd love to be a part of this, and thanks for doing it again!
It seems to me that inferring the type of a constructor would become impossible. I'm not sure what magic Caml works in these cases but based on what I've seen it seems to make an arbitrary choice. 
Ah yes, missed that.
Notice the the type decode . encode :: (Binary c, Binary a) =&gt; a -&gt; c does not encourage us to believe that decode . encode is the identity. It's more indicative of, as tel said, a wonky cast.
maybe you should use ghc + cabal-install, rather than haskell platform? :)
Has it worked in the past? You could also try configuring old-hackage just to compare if it makes a difference.
Probably late now, but I would have looked at what the core outputs.
I'm trying to make a better one now. I'm distracted for a bit, but I'll get back to it soon and write about it here.
The bad behaviour, if it is anyone's, is Data.Map's. Should it be possible to deserialize to an invalid Map? I don't know. It depends where on the efficiency/safety tradeoff curve you like to live. 
I'm not sure--I'd have to think about it. It certainly does work though: # `Foo;; - : [&gt; `Foo ] = `Foo Maybe the trick is in the `&gt;`? This just means that `` `Foo`` can have any type that includes `` `Foo``. Another thing to note is that in OCaml constructors are not functions. So if you have type foo = Foo of int and you try to just use `Foo`, you get an error: # Foo;; Characters 0-3: Foo;; ^^^ Error: The constructor Foo expects 1 argument(s), but is applied here to 0 argument(s) On the other hand, if you did this with a normal function, you'd just get a function: # let foo x = x + 1;; val foo : int -&gt; int = &lt;fun&gt; # foo;; - : int -&gt; int = &lt;fun&gt; So in the ```Foo`` case, Ocaml just knows that it's a nullary constructor. Also, in OCaml, you can overload the same polymorphic constructor name. It's fine to use `` `Foo`` and `` `Foo 10`` in the same code: # let foo = function | `Foo -&gt; "foo" let foo2 = function | `Foo i -&gt; Printf.printf "%d" i;; val foo : [&lt; `Foo ] -&gt; string = &lt;fun&gt; val foo2 : [&lt; `Foo of int ] -&gt; unit = &lt;fun&gt; However, you can't have two different versions of `` `Foo`` in the same function. I'm sure you got this right, but it's also important to note the variance--``[&gt; `Foo]`` vs ``[&lt; `Foo]``.
Because Data.Map uses a list to serialize/deserialize. The same behaviour would be seen if in the code it would deserialize to a list, and then apply fromAscList to get the Map.
 instance Lepton Electron where ...
I don't think converting to binary and serializing are the same tasks. I would advise to leave Binary alone, and to start thinking about the requirements of a serialization library. Do we support user types? Do we need to support past versions of a user type? What if the type of the serialized value doesn't exist in the current program, because it's from a completely different program? ** edit ** I figured out why the binary parsed correctly. At least for such a small Map, the binary representation of a Map is the same as the binary representation of the (sorted) list of tuples it contains. The OP was very unlucky because he happened to encode a value whose binary representation looked exactly like a Map, except for the fact that the keys were not in order ("three" &lt; "two").
[xml](http://hackage.haskell.org/package/xml) is very easy. Also slow and not very expressive. There's also [xml-arrow](https://github.com/silkapp/xml-arrow) based on xml.
This isn't Mind-bending behavior; this is the beginnings of a security exploit! This needs to be patched and every package depending on binary and cereal needs to be updated immediately. edit: I hammered out a [pull request for cereal](https://github.com/GaloisInc/cereal/pull/20).
Are you insane?! &gt; The only possible issue is that the deserializer doesn't enforce the Data.Map invariant on the resultant value. Yes. Yes that is the issue! You know, the one that is a security exploit waiting to happen.
Sml has this, as does Elm. I'm not totally sure why Haskell doesn't. 
It is probably better to just use fromList rather than fromDistinctAscList and some complicated 2-pass decoding. In the case of `containers`, we've optimized it to gracefully upgrade `fromList` as it can by producing content in ascending runs and merging. 
I'd recommend renaming the project to transliterate-korean if you're planning on uploading to Hackage :)
Dependent types are more powerful than type-safe physics.
Indeed. It is documented that fromDistinctAscList and other functions may produce invalid Maps. Perhaps such functions ought to be moved to a "Data.Map.Unsafe" module.
Make sure to checkout Helm(1) if you are interested in this. It's Elm in Haskell basically, with some uniqe features. 1. http://helm-engine.org/
I think this is now fixed in HEAD and will be included in ghc 7.8 and HP 2013.4... late November or December: [http://ghc.haskell.org/trac/ghc/ticket/8266](http://ghc.haskell.org/trac/ghc/ticket/8266) [http://permalink.gmane.org/gmane.comp.lang.haskell.ghc.devel/2569] (http://permalink.gmane.org/gmane.comp.lang.haskell.ghc.devel/2569) 
Because it doesn't consistently contain anything! Maps would be more application specific, but think of numbers and buffers. Suppose deserializing an int could somehow give you a number X so that X &lt; 0 and X &gt; 4096 are both false, but also 0 &lt; X, 1 &lt; X, ..., 5000 &lt; X, and it hits code like char buf[4096] size = deserialize() if (size &lt; 0 || size &gt; 4096) { error("bogus size"); } else { for (int i = 0; i &lt; size; ++i) { buf[i] = get(); } } The point is, code is probably at best secure against arbitrary possible values, not arbitrary things that violate invariants of their types.
I don't think 7.8 will make it into the next HP release, but it's good to know progress has been made! With that dynamic linking patch, will the dylibs 'just work', at least on the development system? I can build just like above and it'll pick up the libraries just fine?
I followed the instructions [here](http://www.haskell.org/haskellwiki/Mac_OS_X) without any problems. :)
Check out [xml-conduit](http://hackage.haskell.org/package/xml-conduit). It is fast(no *String*s!) and relatively easy to get started: import Text.XML import Text.XML.Cursor import Data.Text.Lazy as TL let doc = parseText_ def "&lt;root&gt;&lt;el&gt;foo&lt;/el&gt;&lt;/root&gt;" let cur = fromDocument doc let "foo" = TL.fromChunks $ cur $| element "root" &amp;/ element "el" &amp;/ content
&gt; Perhaps serialized values should contain type information It's a slippery slope to TAL. The idea was always that different tagging/encoding strategies should be layered on top of the primitives. The basic binary instances are just one such example.
Being the author of the patch I'm inclined to say that dylibs will 'just work' on OS X in GHC 7.8.1, at least in terms of dylib locations. I've only tested it with OS X 10.8 and Xcode 4.6 though. I also can't make any claims as to all the non-dylib-location-related problems that might haunt dynamic linking on OS X.
Check out the script mentioned in this reddit post: http://www.reddit.com/r/haskell/comments/1pn6bk/wrapper_script_for_xcode_5_os_x_109/ 
Any reason why you're using macports over homebrew?
No, no good reasons :( I had ports first, heard brew was better, could not get some python libraries I needed to work, stuck with ports, and now it is a couple of years later...
Thanks, will check it out.
I totally missed this post, thx!
Cool, didn't know about that one! 
I'll probably add some more alphabets to it, but either way transliterate may be too generic. What I couldn't figure out is what module hierarchy to put it under. `Language` doesn't really refer to natural languages... 
&gt; instance Lepton Electron where I hope you are not joking. Elementary (and also compound) particles have wonderful invariants that can be modelled with dependent(-alike) types. This connection between type systems and physics is beginning to unfold right now. See e.g. the Atkey paper on [parametricity and the Noetherian theorem](http://bentnib.org/conservation-laws.html). 
I don't think that it should use `fromList` - instead, it should just use a checked form of `fromDistinctAscList`. Unless this type of code is to be an expected, supported use case, which I think it most definitely should not be, the best result here should be an error. EDIT: Also, even though it does gracefully upgrade, there is a reason the `fromDistinctAscList` function exists: it is faster, even if not asymptotically.
Homebrew 4 lyfe
If your goal is to demonstrate why this is a serious problem in Haskell, then it is not particularly convincing of you to use an example from *another, less safe language* to prove your point...
Looking at your [github avatar](https://github.com/bergmark), it's not difficult to see why my transliteration would appeal to you. 한 I'll get cracking as soon as I unbreak my Fay installation.
Getting data with corrupted structure is usually a more serious problem than simply getting the wrong data with correct structure. So by virtue of being "more of a bug" it's also "more of a security bug", but indeed I don't think it is *specifically* more of a *security* bug.
TAL? What if there were `safeEncode` and `safeDecode` functions which required a `Typeable` instance and wrote that information before the data's serialization? Would that be reasonable?
As a beginner trying to learn haskell using leksah, what do I have to do to make this work on mavericks?
What invariants do you expect to hold of serialization? The only one I expect is: decode . encode = id (Of course, only when it has the right type. Otherwise I expect nothing in particular.) I do not expect that serializing a value of one type and deserializing a value of another type is *ever* a sane thing to do. In production code I often go so far as to add phantom types to serialized data, if I intend to hold on to it in memory for some reason, to avoid doing this. You apparently have stronger expectations of serialization than I do, but I don't know what.
Wow, thanks very much for addressing this so quickly, Edward! I added this information at the top of the the blog post. Being new to the Haskell community, it's great to see that folks are so proactive about improving libraries like this.
You probably wouldn't be capable of deserializing an invalid map... But that's weaker than what you asked.
I suddenly have a desperate need for an ELI5 about this.
No one expects an invalid Map. This is what happens. Someone crafts a parsable invalid map with some evil hidden payload in it. Then after the map is parsed you check that the map you parsed satisfied whatever fancy invariants you want hold and it passes. But it is already too late. You are no longer in control of your own program, the attacker is. Your invariant checks are useless because the map is actually invalid and so all your "safety checks" are just doing undefined things, and they are passing because the attacker has crafted it that way. Later, as you process more of the attacker's input, you start munging with the tree; inserting and deleting elements. This causes some rebalancing tree rotations to happen and suddenly your unlookupable data that sneaked unseen through your previous checks is now rotated into a lookupable position. Now the attacker completes his goal by making the system lookup that unchecked data from your tree and do god-knows what with it. You are now 0wned.
YES! Please, please do this.
You're right, I should have given more information. I basically need to extract data from simple RSS/Atom files. There's maybe even a specialized library?
Interresting! Do you plan to had collisions?
I don't think type erasure has an impact if your language cannot dispatch on types at runtime.
I'm trying to say that I think the preconditions are expected to hold because the input is supposed to have only been produced by `encode :: Map K V -&gt; ByteString`, and I think this is reasonable because it is consistent with the property I gave earlier. It's totally nonsensical to mix `Binary` instances.
Can you be more specific about the exploit that one could trigger using this behavior?
`fromList` is as fast as `fromDistinctAscList` now, as long as the list is actually distinct and ascending.
Data.Binary doesn't contain types. For many usecases this is precisely what you want. Otherwise you can serialize the type first, but nothing prevents someone from maliciously constructing a bytestream and its not currently engineered with safety against attack in mind.
This can only work if encodings of all possible types are distinct on the wire.
It seems like the problem for me is having constructors which are functions alongside polymorphic variants. One thing that I thought might work would be to give vectors of function arguments their own kind, and allow quantification over them, so that a lone constructor `Foo would have type forall v. (args :: v) -&gt; &lt;Foo: t0 | r0&gt; where v could be unified later by a pattern to determine Foo's arity. Maybe this could also be a neat way of handling varargs functions. For now, I think I'm just going to keep this code in a separate branch until I figure out the best thing to do with it.
I like this one, but can't help bikeshedding. length = last . (0:) . zipWith const [1..]
Cross posted from /r/fsharp. My main reason for sharing this with the Haskell community is that I find it really interesting that F# computation expressions end up using different syntax for *MonadPlus* and *MonadOr* computations. This is not a problem for the Haskell *do* notation (which only uses monadic part of the computation), but it might be interesting if someone wanted to extend the notation in the future. Also, this is my +1 to refactoring *MonadPlus* according to the [MonadPlus reform proposal](http://www.haskell.org/haskellwiki/MonadPlus_reform_proposal).
Because Elm and Frege are esoteric and have one author/benevolent dictator and no users to tick off/incite bikesheddery. They can just implement what they feel is right without considering the dozens of other ways to do it that people would lobby for. Sml probably had it from the start, not sure.
It wouldn't be perfect, but I just imagine that a dependently typed Map type would likely only be able to be built while maintaining its invariants, so it'd still be a proper map even if the data were bad. That's just a big assumption about how deserialization would work in a dependently typed language, though.
So, a notation that privileges "return" (as opposed to do notation, which does not) seems useful in general - it allows us to have do notation for applicatives more easily, multiple returns for additive monads, and perhaps more that I haven't considered. Interesting..
Thanks for the xpost. I'm really stoked by the feedback. I did not expect this. :)
Wow, that looks cool. I didn't know is yet. Thanks. :)
I already suggested this through twitter, but I figure it's better to be annoying than forgotten: alex and happy (lexer and parser) would be very interesting to cover, I'd really like to know more about repa and/or accelerate and the Scotty framework seems like a nice project for people wanting to do simple web apps.
BTW: The computation expression notation also supports applicatives, but only in a research extension. You can see [some samples here](http://tryjoinads.org/index.html?computations/applicative.html). It is taking a complementary approach to "idiom brackets" and looks like ordinary computation expression (in the style of *do* notation), but restricts the binding to what can be captured by applicatives. Privileging "return" is an interesting thought - although this looks syntactically similar for plain monads, the fact that "return" is just a function called inside "do" notation but a keyword in the F# world is definitely an important difference. But perhaps it would make sense to have a "yield" function for MonadOr (as opposed to "return" for MonadPlus)?
Would you mind posting the source code? I'm learning Haskell and this would be really useful
I think you're exaggerating the link times unless you have a really slow storage device. As for why dynamic libraries are not shipped with HP, that's probably because it would increase HP size almost twofold. HP would need to ship two versions of all modules.
I'm slightly in favor of changing `MonadPlus`, but it seems to be an issue that flies remarkably far below the radar. It takes me a half hour to convince any individual person there is even an issue, and we've figured out how to live with the status quo as uncomfortable as it is, since in general `mplus` "does what they mean".
I'm the "David" the article talks about. The backstory is this: I spent a year abroad, but we were regularly talking over XMPP. One day, Dobi gave me a link to an online book that has a sun saying "holy shit" on the front page. It was [learnyouahaskell.com](http://learnyouahaskell.com/), and it spoke of some obscure language I had never heard of. But I found the pictures funny so I started reading the first chapter. A month later, Dobi had long lost interest in Haskell himself, but I was hooked. Every "wow FP is awesome moment" was forcibly shared with him. I soon began to talk about ApplicativeMonoidMonadFunctor and how neat these abstractions are. Since that one day when I received the LYAH link, it's marked "Dobi shows David Haskell day" in my calendar, this has continued: I'm learning Haskell, and everything cool is thrown at Dobi. He even made a second attempt at learning Haskell, but due to its inaccessibility (relative to Elm), he lost interest again. Then, a couple of weeks ago, he asks me whether I knew Elm. I said yes, but as someone who now uses Haskell as his main language, I thought of Elm as a toy. Today I see how this was wrong: Elm is an excellent language, maybe not to write a scalable webserver, but it gets a few things right that Haskell doesn't: - Elm is very accessible. - Even simple and small programs can have cool output, and by that I don't mean conceptually cool but you-can-show-your-friends cool. Getting positive feedback is really motivating in the beginning when you're not quite sure whether the language is useful yet. - Elm has Haskell-like syntax, and is of similar semantic structure. It has recursive lists, fold, map, Maybe and all those other Haskell Prelude things. - No category-like abstractions (as a result of not having typeclasses, mostly). Monads are pretty abstract, and are pretty infamous among people that don't know them. This significantly lowers the barrier of entry. The crucial thing is the similarity to Haskell: Elm is a much better gateway drug to Haskell-like languages than say XMonad (which is pretty geeky) or other nice little things. Once you can write Elm, a Monad is just another addition to a language you already know how to move in, and you can learn to include it in your programming instead of requiring it in the first place. I think instead of having a "beginner-friendly monomorphic Prelude" we should have generalized the crap out of Prelude (Foldable, Traversable, all those things go in) and recommended Elm to newcomers if they have problems with Haskell.
♡
I do not think the distinction between "return" vs. "return!" and "let" vs. "let!" has anything to do with the strictness. You're right that strictness means that some additional handling is needed - see the discussion about the "delay" and "run" operations in the paper. However the distinction between "let" and "let!" is simply a distinction between an ordinary binding (binding value of type T to a variable of type T) and monadic binding (binding the result of computation T M to a variable of type T). The "return" vs. "return!" case is similar, but when returning result (return value vs. return the result of a monadic computation).
I think it should also ship profile versions of the libraries. It is not obvious to newcomers how to profile their program now. Once you use external libs (even those shipped with HP) you can' easily profile.
I'm not exaggerating, a small project like I described takes 18 seconds to link on my machine. And that's just the linking (!), not even counting compiling on a single core. GHC often produces very poorly performing code without lots of INLINE and SPECIALIZE pragmas, which obviously then end up generating tons of code, slowing down compilation/linking further. If you then happen to use Template Haskell and the LLVM backend, build times exceeding minutes for the smallest possible change to your program seem to be common. At least for me, this is a real and very important issue with Haskell. Any improvements would make a real difference to me. This is also not related to storage speed at all. You can build on a RAM disk and it doesn't make much of a difference. People will of course have differing opinions and needs regarding the size of the HP, but just for me personally, whether the HP is 100MB or 100GB is completely irrelevant compared to the productivity drain of lengthy link times.
Hmm, there are p_ versions of all the libraries in my HP library directory, and I'm pretty sure my default Cabal config had 'library-profiling: True' set. I think shipping the shared / profile versions of libraries might not even be necessary if Cabal is just improved a bit. Currently it does not seem to reinstall differently compiled versions of already existing libraries into a sandbox, making it very tedious to work around issues like missing shared / profile objects.
Those interested in a pure 2D physics engine are welcome to check out the [Sloth2D](https://github.com/cobbpg/sloth2d) library. It’s not a serious project, more like a fun little experiment.
And lens-aeson for easier access to nested structures (where each level may fail).
I don't agree, for a couple reasons. I think it would be an obvious improvement to make the format self describing. You then write out the format, followed by the data. On reading, you check the format is what you expect before blindly deserializing. Esp if your format isn't context sensitive, the checks can be done entirely up front and can be quite efficient. Furthermore, deserialization should _never_* return an invalid instance of a data type, it should either return a valid instance, or an error / `Nothing` / etc. Think about it, if it returns some half valid object, and you are writing higher order code, how the heck are you even going to know you've encountered a problem? There is no general purpose way to check for 'validity'! That the `Map` deserializer doesn't guarantee a valid Map on deserialization is an absolutely gruesome abomination, a bizarre C-ism that does not belong in any Haskell library, and a bug waiting to happen! *: Only in very restricted circumstances (like if you tightly control both the producer and consumer side, and serialization/deserialization speed is your bottleneck) is it appropriate to do anything else. It is entirely inappropriate for a generic type like `Map` to have a serialization format that can result in a corrupt Map. Incidentally, Java's built in serialization has gotten this correct since like the mid 90s. Deserializing a map does reinsertions.
What you describe is a valid design for a library. It is exactly _not_ the type of library that `Binary` was written to be! There are such self-describing formats -- Cloud Haskell, written using binary as an underlying layer, is one. There's also a place for "safe" formats. But as edwardk has pointed out elsewhere in this thread, that's also a much more complicated thing. For example, you want to be sure you're not given something too "large" that just breaks your deserialization, etc. Binary is a deliberately simple, low level library, that pervasively assumes you can "trust" the data you're deserializing. If you're in a situation where you can't trust that data, there are a _host_ of reasons you shouldn't be using Binary directly.
Cool. I wonder if you can generalize that cute hack to n params, if you're willing to do some unsafe stuff in the implementation.
Okay, let's ignore the self describing thing a minute. I agree that is a different library than `Binary` (and such a library could use `Binary` as its implementation). But implementing deserialization for `Map` via `fromList`? To me that is like a no-brainer. Of COURSE you should do that. For the cost of (at most) a log factor, you get a deserializer which won't return a silently corrupt result. Are we seriously debating what the right thing is to do here? (Not to mention the fact that edwardkmett got the performance of `fromList` to match the less safe versions anyway!!!)
Mhh, the [guildelines](http://www.reddit.com/r/gamedev/comments/1jtdo7/please_read_the_subreddits_guidelines_before/) there seem quite repelling. ;-)
We do not yet have a proof that Alternative's laws entails the MonadPlus laws. Either set.
What I am referring to is that the new `fromList` is all but as fast as the new `fromDistinctAscList`, both of which are faster than the old implementations by 10x and almost 2x respectively, so there is really no reason not to use `fromList` any more. The speed difference is 5% for `Map` and within the margin of error for `Set`.
Interesting! For `Alternative` my (intuitive) expectation would be that it behaves more like `MonadOr` rather than `MonadPlus` (which is also a reason why I never liked the idea for using `MonadPlus` as an abstraction for multiple clauses in pattern matching in Joinads: http://tomasp.net/academic/papers/docase/). 
Currently, `Alternative` is overloaded to have the same mixed semantics as `MonadPlus`, so any such intuition is... optimistic. ;) This is gaining more teeth in 7.10 as it'll transition to a type error to have a `MonadPlus` that isn't also Alternative.
For python with homebrew, I don't use brew to install/manage python packages. I instead set up a virtualenv and use pip to install the packages I need.
MonadPlus has laws? ;)
&gt; I don't talk for the haskell community in general. I don't have any authority about that. But that is precisely what you did: &gt; Haskell uses lists as a control flow structure.
`let` vs `let!` is exactly the same as `let` vs `&lt;-` in Haskell. I'll concede `return!` though -- it corresponds to a simple expression in final position in a Haskell `do` block, but it is needed in some cases because of the strictness.
Erk, I always forget their guidelines are fairly strict. Maybe /r/devblogs would be more appropriate.
The term you are looking for is "church-encoding" or "scott-encoding" (scott encoding is closer to pattern matching), meaning encoding everything using functions. This works even for pattern matching, and here's an example using `Maybe`. {-# LANGUAGE RankNTypes #-} import Prelude hiding (Maybe(..)) type Maybe a = forall r . (a -&gt; r) -&gt; r -&gt; r -- ^ ^ -- | | -- Just continuation --+ +-- Nothing continuation -- Behaves like the `Just` constructor just :: a -&gt; Maybe a just a = \just_ nothing_ -&gt; just_ a -- Behaves like the `Nothing` constructor nothing :: Maybe a nothing = \just_ nothing_ -&gt; nothing_ -- Example of how you "pattern match" on a value of type `Maybe` (&lt;|&gt;) :: Maybe a -&gt; Maybe a -&gt; Maybe a -- Compare to: m1 &lt;|&gt; m2 = m1 -- m1 &lt;|&gt; m2 = case m1 of (\a -&gt; just a) -- Just a -&gt; Just a m2 -- Nothing -&gt; m2 -- Compare to: main = -- main = (nothing &lt;|&gt; just 1) -- case (Nothing &lt;|&gt; Just 1) of (\a -&gt; print a) -- Just a -&gt; print a (return ()) -- Notihng -&gt; return () The idea is that the church-encoded version of `Maybe` comes "built-in" with the ability to pattern match. You just pass it continuations corresponding to what you would do for each branch of the case statement. Edit: Note that this is *not* how Haskell implements pattern matching under the hood, but I just wanted to show that you can in theory desugar pattern matching to functions. In practice, constructors and pattern matching are primitives in Haskell.
Better still it has two largely unrelated sets of them. ;)
I confess I misspoke on `let` vs `let!`.
&gt; I'm trying to say that I think the preconditions are expected to hold because the input is supposed to have only been produced by encode :: Map K V -&gt; ByteString, If you think this is a reasonable assumption to make to avoid executing undefined-behaviour then we are all totally screwed.
Interesting. Yes, Church encoding is definitely what I was thinking of. I kind of figured Haskell was smart enough to handle patterns at a high level. Looking at your translation of Maybe, it's not hard to see why patterns are primitive.
xml-conduit is well maintained and actively used... I am making myself switch from HXT which had a lot of properties I liked but seems to be dying. 
&gt; If you are using Binary in a situation where an attacker has control of the file you are already doomed. I can make malicious files that say that you should read terabytes of data next into an array and bottom you out, etc. If the only security exploits I ever had to worry about it DoS attacks, I would be so happy. But there is a world of difference between making a crafted input that predictably attempts to read terabytes of data and crashes my program and a making crafted input that invokes undefined-behavior and turns a Haskell service into a weird-machine that does whatever the attacker can program it to do.
See [my other comment](http://www.reddit.com/r/haskell/comments/1q4r3b/mindbending_behavior_for_deserialization_in/cd9mutn) for details, but the details don't matter. Once you go from user-crafted input to undefined-behaviour, then you have a potential security exploit and you should simply assume that it is exploitable.
It is also now fixed in cereal-0.4.0.1. Thank you so much for pointing this out. In my mind, although apparently not many people agree with me, this is the most serious bug I've seen in the Haskell ecosystem since the great [`Ix` segfaulting bug of 2006](http://ghc.haskell.org/trac/ghc/ticket/1046). Though this may be more of a reflection of me not paying enough attention to the Haskell ecosystem over the years.
Great read. What does it look like when there's an error in the computation expression?
In type theory, the definition of every type is associated with a set of rules: * *Formation rules*, which describe how to construct the type itself. These are called *type constructors* in Haskell lingo. * *Introduction rules*, which describes how to construct an element of the type, i.e. *data constructors*. * *Elimination rules*, which describe how to use elements of the type. * *Computation rules*, which describe the how elimination rules act on introduction rules. The combination of elimination and computation rules give rise to *pattern matching*. Strictly speaking though, pattern matching (without constraints) is actually more powerful -- enough to form non-terminating or ill-defined programs, which is generally undesirable in type theory. Equivalently, one may assume the existence of a *recursion principle* associated with each type. The recursion principle of a type is a generic function that allows you to define any other function out of the said type. Of course, in Haskell, pattern matching is a primitive feature so recursion principles are technically unnecessary. In contrast, in type theory, pattern matching is merely a syntactic convenience for using the recursion principle. *Example 1.* For the type `Maybe a`, the recursion principle is defined as: recMaybe :: b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b recMaybe x _ Nothing = x recMaybe _ f (Just a) = f a Once `recMaybe` is available, you won't really need pattern matching to define functions out of `Maybe a`. *Example 2.* For the product type (the type of tuples), the recursion principle is actually just `uncurry`: uncurry :: (a -&gt; b -&gt; c) -&gt; (a, b) -&gt; c uncurry f (a, b) = f a b Source + more info: http://homotopytypetheory.org/book/ (Chapter 1)
Technically this is Boehm-Berarducci encoding, not Church encoding. Church encoding targets the untyped lambda calculus. The importance is that you don't get the eta rules unless you target a language with parametricity. consider case p of {(x,y) -&gt; (x,y)} = p is church encoded to p (\x -&gt; \y -&gt; \s -&gt; s x y) = p which does not hold in the untyped setting, or even at p :: X -&gt; Y -&gt; Z but only at p :: forall z. X -&gt; Y -&gt; z Anyway, just an interesting observation. In practice we use the term "church encoding" for the typed case all the time.
&gt; As far as I'm aware, there's no way to unwrap that x without pattern matching. When you make a definition like `data MaybeInt = Nothing | Just Int`, you are saying the following: * `Nothing` is a value of the type `MaybeInt` * `Just` is a function from `Int` to `MaybeInt` * All of the values of type `MaybeInt` are of the form `Nothing` or `Just x` for some Int value x. So if have a value of type `MaybeInt` and you want to "extract the x", you first have to make sure that it was constructed using the `Just` function - otherwise there is no x to extract. 
Currently, if there is a typing error related to the definition of the computation operations (the worst case), then you get an error in terms of the underlying operations... However, the compiler, at least, nicely handles locations of the error. In the paper (http://tomasp.net/academic/papers/computation-zoo) we actually try to improve this and there is a type system for computation expressions that captures all common uses, but would allow nicer error reporting...
Tells its private
Clarification: this cannot be used to encode lists in a normally typed language because you get a recursive type. In an untyped or uni-typed situation (e.g. untyped lambda calculus) or one where arbitrary recursive types are allowed, then it could be used. In Haskell you would have to use either a newtype wrapper to allow the recursion, or use a slightly different encoding. Using Pair with newtype for recursion: type Pair a b = (forall r. (a -&gt; b -&gt; r) -&gt; r) newtype List a = List { asPair :: Pair a (List a) } Or, a different encoding (I believe this is actually the direct Church encoding of a list - I could be wrong, but this is what we used when we studied PLC at university): type List a = (forall r. (a -&gt; r -&gt; r) -&gt; r -&gt; r) EDIT: There's actually a further problem with the newtype wrapper here which means you cannot really use this to represent arbitrary length lists in Haskell - the non-recursed version results in lists of a fixed length (nested tuples), while the recursive one results in lists of only infinite length (there is no base case). So if you want to encode lists in a CPS-ed way I would suggest the foldr-style one.
I'd personally love to see reactive-banana, pipes and repa.
Please, don't use Church encoding. Use Scott encoding, it reflects reality better. (They coincide for Maybe.)
http://nehe.gamedev.net/tutorial/introduction_to_physical_simulations/18005/ Introduction to Physical Simulations 
This is both a Church encoding and a Scott encoding. They do not differ on non-recursive types, acting as pattern matching in both. But they crucially are different on recursive types: Scott encodings are still pattern matches, while Church encodings are folds. The basic idea of both is that if you look at the types of the constructors for a type, you can make a small change, and then derive a case/fold function using that type. Consider: Nothing :: Maybe a Just :: a -&gt; Maybe a One thing we can do, to get Scott encodings, is to replace all of the final `Maybe a`s with `r`, and define `maybeCase` to have the derived type maybeCase :: Maybe a -&gt; r -&gt; (a -&gt; r) -&gt; r Another way is to replace all occurrences of `Maybe a` with `r`, and define `maybeFold`. This has the same effect, however, because `Maybe`'s never uses `Maybe` in recursive positions, only as returned types. Pairs and `Either` are similar. But look at lists: [] :: [a] (:) :: a -&gt; [a] -&gt; [a] If we replace the final `[a]`s we get listCase :: [a] -&gt; r -&gt; (a -&gt; [a] -&gt; r) -&gt; r but if we replace all `[a]`s: listFold :: [a] -&gt; r -&gt; (a -&gt; r -&gt; r) -&gt; r You can extract all of this sort of thing automatically (both type and definition), and in a dependently typed setting, no less. ~~Goguen, McBride, and McKinna wrote a paper ([here](http://www.strictlypositive.org/goguen.pdf)) on replacing all case and built in recursion with this sort of thing.~~ Actually, McBride's [Elimination with a Motive](http://www.strictlypositive.org/elim.ps.gz) is probably better.
Take a look at my comment in reply to Tekmo. It clarifies an important distinction he wasn't making.
For all non-recursive types, indeed!
Forget homotopy type theory. That won't help you understand this stuff, it'll probably confuse the hell out of you. Read Frank Pfenning's lecture notes from his constructive logic seminar on his site, and/or watch [his lectures at OPLSS2012](http://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html).
No. What may happen?
As someone who has uses macports (and has only tried macports and nix), why should I use homebrew? 
&gt; type Maybe a = forall r . (a -&gt; r) -&gt; r -&gt; r NB `Prelude.maybe` does one direction of this equivalence and the other is given by `\f -&gt; f Just Nothing`.
You have to call `actuate` on the `network`. It won't react to input events while paused. ... source &lt;- newAddHandler network &lt;- setupNetwork img source actuate network --&lt;-- this eventLoop source 
Added it (updated main post). Still doesn't work. :/
Just seems like there's a lot of overhead from Operational.
Nice things: refinements, algebraic data types, nice records. Negative things: no static checking (and no, static checking is not an impediment to pedagogy)
THIS, this is what Dart should have been and what Mozilla should target. 
Y U NO TYPE INFERENCE
GHC doesn't support refinements (without LiquidHaskell), and it has static checking. It'd be easy to compile this language to Haskell, but I don't think it's just Haskell with different syntax.
I updated my comment to mention this and to say that Scott encoding is closer to shallow pattern matching.
TAL: http://www.cs.cornell.edu/talc/ Because yr closures are mobile, ftw!
Are you sure that it works without reactive-banana? What happens if you replace `snd esMouse` with `render img` (and adjust the signature to pass `img` instead of an event source)?
I always wanted to ask this question: does Haskell support data types created/checked/whatever while program is running (not known at compilation time)? For example I want to download JSON with data structure descibing another data structure and parse some data attached to it using that(decoded) structure. Is it possible to do it in a simple way?
Atop this, the team is the Racket team so they are of course biased in their implementation language.
It is an impediment to refinement types though, I'd think. Though I suppose you could just delete the refinements before static checks it means the two systems are somewhat disjoint.
LiquidHaskell manages to check refinements statically.
Types are by definition a syntactic (compile-time) notion. Anything else people sometimes call types are not types. It doesn't solve your problem, but understanding this will help you avoid confusion.
As a regular user of OCaml, I have no idea what you are talking about. Could you explain?
Polymorphic variant types?
Polymorphic variants are not dynamically generated data types.
I would expect to get either a `Word8` or an error. But in any case, would not expect to get an invalid `Word8`. Fortunately, there are no invalid `Word8`'s, so there is no issue of undefined-behaviour here. In the case of cereal, I expect (decode :: Either String Word8) to always either return `Left` or `Right` and never an error (unless, of course, the ByteString itself is an error). `Right` should never hold an invalid structure, even when decoding an encoded object of the wrong type.
Ah, I misunderstood the parent post.
It does not, at least not using this code: main = do SDL.init [SDL.InitEverything] SDL.rawSetCaption (Just "foobar") Nothing surf &lt;- SDL.setVideoMode 640 480 32 [] img &lt;- Image.load "foo.png" gameLoop surf img gameLoop surf img = do (x, y, _) &lt;- SDL.getMouseState SDL.blitSurface img (Just (SDL.Rect (x-25) (y-25) 50 50)) surf Nothing gameLoop surf img But the framework code *is* correct? I'm mostly looking for understanding this not getting something useful out of it. 
&gt; But the framework code is correct? Probably. But that's difficult to test if your SDL game loop doesn't work in the first place.
It does, in a few forms. The Data.Dynamic package let's you stuff most anything in an opaque Dynamic value and get it back out again. When you actually want duck types in Haskell, this is how you get it. There's also existential quantification, which lets you store a value of a type that belongs to one or more type classes and to forget the exact type. You can use this to effect certain OO patterns. Edit: derp. Didn't read closely enough. Haskell types aren't runtime values like they are in languages like Python. You could use template Haskell to generate data structures from json, but that json has to be available at compile time. (Now, you could ship the compiler and generate code at runtime, but that's a big can of worms)
Okay, so I think your expectations might make sense if you are not in control of the data. I guess it's just that I don't think of `binary` and `cereal` as being for such situations. For externally provided inputs I would expect to just use `attoparsec` or something.
Only thing that you know is how to read this JSON and parse it. The resulting structures created from processing it are not known. 
How so? The very goal of parsing is to build a KNOWN structures :) You cannot build something you do not anticipate. Either you know the structure you are going to build upfront, at the time of writing the program, or your program fails with error at runtime. 
Neat. So it seems the main difference between the two encoding methods occurs in recursively-defined types? Much of the second paper is beyond me, but thanks for the clarification!
For example my server receives data from different sources and combines them by given data structure. This structure is not known for a program before it receives it. Imagine it like a restaurant worker who gets a sandwich recipe from a client and basket full of some exotic fruits. Your program just crashed, but this man will make it anyway. 
I think you're arguing past each other a bit, in that you're both right from different points of view. It's possible to build software that can do meaningful work with JSON data, regardless of the semantic content of the data. Structural editors, for instance. From the point of view of semantic content of the data, your software has no idea what structure it will find. But from a purely syntactic viewpoint, the structure is well-known. It's JSON data. It is a tree with a couple internal node types and a few leaf types. That's a perfectly well-defined structure that the data must conform to. I'm more sympathetic to the latter view, because I like isolating things when possible - first do the syntactic processing, then the semantic processing. If your application's semantic processing is the identity operation, that doesn't change the fact that the syntactic processing resulted in a well-known structure.
&gt; I like isolating things when possible - first do the syntactic processing, then the semantic processing. At which point you WILL BE aware about semantic structure at the time of writing the program. You will be anticipating certain structure in your code. As i said, i've yet to see an example of code that builds truly unknown structures at runtime, without having no knowledge of their structure coded in the source.
The papers are a bit complex, indeed. I'd be happy to help with EwaM tho, if you're interested in understanding it! It's probably the easiest of the two.
&gt; For example I want to download JSON with data structure descibing another data structure and parse some data attached to it using that(decoded) structure. Is it possible to do it in a simple way? There are two cases: 1. You know in advance the possible shapes of these data structures, and so you are aware of all possible records these JSON responses will take. Reading the JSON is then a deserialization operation into one of the known structures. 2. You do not know in advance all possible data structures, in which case what you are trying to do is interpret a data description language (reading JSON = parsing a program). Your Haskell program is then an interpreter, which Haskell let's you do quite easily, and with a certain degree of safety. For instance, you can define a JSON record with a type-level natural describing the number of fields in that record, and ensure functions you write have the correct numbers of fields, etc. There are always limitations though, unless you go full dependent typing (and even then, the limits are your brain).
Personally, I'm really interested in Fay, but I wouldn't use it without having strictness semantics. // Built-in &amp;&amp;. function Fay$$and(x){ return function(y){ return new Fay$$$(function(){ return Fay$$_(x) &amp;&amp; Fay$$_(y); }); }; } This kind of code is unappealing to me from a performance standpoint and as far as consuming fay outputted code from js.