If a *running* program can migrate to a different system with a different environment, then simply putting the system info in `IO` is woefully inadequate. You'd need to be able to ensure that the information doesn't change between the time you collect it and the time you use it. Fortunately, you can't actually migrate a running program to a wildly different machine.
The only good application I've personally seen for implicit parameters thus far is `HasCallStack`, which uses them under the hood. I think most of the time it's better to be explicit or use `reflection`.
Awesome! Thanks for the tip, `dim` looks promising.
That's a good question. You'd have to check. That said, `Data.Map` doesn't have terribly many, or terribly complex, rewrite rules for the most part, so you could easily copy them if necessary.
Maybe I just don't remember what information is included. It's been a while and I don't currently have access to a proper computer. 
Well, migrations can happen at opt-in times, the data still isn't pure.
 $ tee main.hs &lt;&lt;HASKELL module Main where import System.Environment (getArgs) main = getArgs &gt;&gt;= mapM_ putStrLn HASKELL $ $ ghci main.hs [1 of 1] Compiling Main ( main.hs, interpreted ) Ok, modules loaded: Main. *Main&gt; :set args one two three *Main&gt; main one two three *Main&gt; :set args red green blue *Main&gt; main red green blue 
There's a data science project with decent performance written in OCaml, but that's unsurprising because it uses OpenBLAS. This is perfectly fine because all of HPC links math libraries written in C or Fortran that are basically written and set in stone. https://github.com/ryanrhymes/owl/wiki/Tutorial:-Scripting-and-Zoo-System https://github.com/ryanrhymes/owl/wiki/Plot-Gallery https://github.com/ryanrhymes/owl/wiki/Evaluation:-Performance-Test
This is a very good question, and I'd be very keen to learn more about this as well.
&gt; It's like an FP version of an Object from OOP. This is comonad, not "coinductive record approach", right? (even that seems wrong though, as last comment in that blog post proves) I'm still clueless. Why do people even link to comonad posts when I ask about "coinductive record approach"? 
Hey, one of my packages, `foldl-statistics`, is currently not in the nightly, with the reason being the `foldl` version, but the bounds include a higher version than what's in the nightly. I'm curious why it's still not included, and also where the right place to ask these questions is :)
It does work fine, just not with standard GHC 8 pre-built binaries for now. GHC 7 pre-built binaries work fine out of the box. GHC 8 works fine if you compile it with `./configure --disable-large-address-space`, which you can do either on a different Ubuntu machine, or on WSL itself using GHC 7. There is an unofficial pre-built GHC 8.0.2 binary [here](https://github.com/sgraf812/ghc-dlas) that works fine on WSL. MS has reportedly already fixed this issue, although I haven't tried it because it is only available on Windows Insider which I don't want. If the reports are true, it will be fixed for stock Windows 10 when the Fall Creators Update comes out in a few months.
I do wish it was on the JS problem page, not really too sure how to go about getting it on there. Unfortunately putting it on the README of react-flux is impossible since the repo is completely dead.
I don't experience the same thing - the clojure developers I work with seem very opposed to learning anything from other languages, waiting until Rich enshrines it with his blessing. But likely that's unique to my group or clojure. 
I think [this post by Bartosz](https://bartoszmilewski.com/2017/01/02/comonads/ ) is well written. The section on "Programming with Comonads" might explain why IO is rarely mentioned with comonads. I think the answer is to pair them with the monads - [Phil Freeman](http://blog.functorial.com/posts/2016-08-07-Comonads-As-Spaces.html) has a good blog post on which monads go with which comonads. And the blog series ["Cofun with cofree comonads"](http://dlaing.org/cofun/) is a pretty thorough description of the 'design pattern' of using a free monad for the DSL and a cofree comonad for the interpreter of a language. 
I guess you need to start with [coinduction](https://en.wikipedia.org/wiki/Coinduction) and then combine that with records. Comonads are just a convenient way to implement that in practice as you need to store your record *somewhere*, and comonads are an easy way to do so. (If you're thinking 'why not keep my record in the state monad, read the Bartosz post that I linked in the other comment). Now, since you're storing your record, you'll need some way to modify it and get the values out? And you might be thinking of using lenses for that? Well, turns out [lenses are coalgebras for the store comonad] (https://patternsinfp.wordpress.com/2011/01/31/lenses-are-the-coalgebras-for-the-costate-comonad/). So if you put your record into a comonad and then use lenses to modify it, you're working with coinductive records. But there's probably other ways of doing coinductive records too. 
indeed, agda lets you do that, and i've some basic examples of doing that same encoding in normal haskell. i feel that linear logic really shines when doing coinduction https://www.reddit.com/r/haskell/comments/4aju8f/simple_example_of_emulating_copattern_matching_in/ 
Yes, that'd be more accurate. I don't think it does anything to support it, but it certainly doesn't do anything to prevent it. lt's definitely tailored for RPCs over HTTP, however. Now, if only we had a Monad that let us make remote function calls as if they were local, so we didn't have to bother with this nonsense...
Author here: I've updated the post to say more clearly what the problem is. Apart from passing the right arguments to GHC to make it faster, the problem is also that Stack/cabal-install usually build *too much*. If you're only interested in building a single file, `ghc --make` is usually better. As a case in point, I took the [monad-par](http://hackage.haskell.org/package/monad-par) library and put `ghc-options: -O0 -no-link -dynamic` in the Cabal file. Here are the results for ghcc (see the blog post), cabal-install and Stack: $ rm -rf .ghc-temp &amp;&amp; time cabal exec -- ghcc tests/AllTests.hs &gt; /dev/null 2&gt; /dev/null real 0m2,047s user 0m1,832s sys 0m0,136s $ cabal clean &amp;&amp; cabal configure --enable-test &amp;&amp; time cabal build &gt; /dev/null 2&gt; /dev/null real 0m4,163s user 0m6,548s sys 0m1,960s $ stack clean &amp;&amp; time stack build --test --no-run-tests &gt; /dev/null 2&gt; /dev/null real 0m5,807s user 0m5,308s sys 0m0,440s (Passing `--fast` to Stack doesn't make a difference, presumably because I've already changed the GHC options in the Cabal file.) In this case the main win seems to be that `ghcc` doesn't have to build the whole library before building the tests.
I don't see why hidden modules would be a problem. `cabal-cargs` will give the path to all source directories to GHC, so it should be able to find all modules in a package, no? Regarding `stack build --fast`, see the reply to /u/ElvishJerricco. And you're right that targeting a single file is a main benefit. The other one is not having to compile the library and the test suites, etc. separately. I've updated the post to clarify this.
I find them quite convenient as an alternative to using a reader monad. Having to rewrite an entire group of functions to monadic style just so you can pass around a shared config value can be quite irritating. But I suppose this would be a case where you'd recommend `reflection` over implicit params?
I could get behind file syncing and one remote spot instance instead of a collection of remote instances working together, if we came up with a way to hack around the start up time (possibly a pool of reserved instances that get allocated to a set of developers at the start of each day). I know this is creeping into CI, but I am still very focused on the synchronous case. I think CI systems handle the asynchronous build and test running case well enough. Also, I usually compile a few times between commits. I also don't know if intero is "good enough" for the developer feedback, making compiling not as important, as I haven't used it yet.
I think older versions didn't and I shied away from it as well for this reason.
What language extension is author using?
Does this branch support the non-unicode lollipop syntax `f :: a -o b`, or is Tweag seriously gonna force us all to use `f :: a ⊸ b`? Similar for the omega multiplicity.
&gt; In the next post in the series, we'll take a look at how linear types can pave the way towards safe zero-copy packed data exchange across clusters. Yes please! Looking forward to reading that ;) Exciting times lying ahead for GHC and the Haskell ecosystem it seems, can't wait!
Don't worry, we don't intend to force unicode on anybody. The branch currently only has unicode syntax, because an ASCII syntax hasn't been settled yet (`-o` for instance, is likely to break parsing of existing libraries). Choosing and ASCII syntax needs to be done with a little care, and it hasn't been the priority yet.
Think about everything with a neighborhood.. Image filters, graph algorithms, etc..
And it's not in mainline GHC yet. We are developing a branch at: https://github.com/tweag/ghc/tree/linear-types . Using linear types with this branch doesn't require an extension (yet). See the README there for more.
I would love to, but I'm not sure I have a lot to say about it. Having linearity in the language is syntactically more convenient, we also have more control on error messages. It makes it possible to integrate with lower strata of the compiler (we envision inlining all linear functions, for instance). We also have more polymorphism. My understanding is that the Linearity Monad approach is more useful for DSLs, where linear function may be compiled/represented differently than unrestricted function such as [Benton and Krishnaswami's _A Semantic Model for Graphical User Interfaces_](http://www.cs.cmu.edu/~neelk/icfp11-krishnaswami-benton.pdf). But I may be wrong. What I would like to understand is whether linearity in the language helps make the Linearity Monad easier to define and use. But I don't know that yet.
I mean, how is he able to use data constructors in type signature? `data Socket (s :: State)` does not type check for me, as well as `x :: Socket Unbound` Is it DataKinds and KindSignatures as my GHCi hints?
`DataKinds`, which is also implied by `TypeInType`. Part of /u/goldfirere's effort to finally implement `DependentTypes` (which would allow you to use data constructors in function type declarations _and_ function bodies, roughly). This is a wonderfully accessible introduction to `DataKinds`: [An introduction to DataKinds and GHC.TypeLits](http://ponies.io/posts/2014-07-30-typelits.html)
Yep. According to [the GHC Wiki page](https://ghc.haskell.org/trac/ghc/wiki/LinearTypes) the non-unicode syntax is up in the air. Of course, this is an engineering challenge, specific to the GHC compiler, and not a fundamental theoretical issue. We're already working on the assumption that GHC will have linear types, and the paper linked in the blog post has usage examples. It just seems like it would also make some sense to _assume_ an ASCII syntax, even if we won't be able to implement it cleanly in the GHC codebase as it now stands.
*wink* `(-.)`
servant is agnostic to whether you're doing "good old RPC" or actual REST. Nothing prevents you from doing proper REST. We even had ideas back some time ago for helping people support HATEOAS easily without having to write too much of the boilerplate themselves, but that never materialiased as usable code. Anyway, the goal of servant is to be able to have an inspectable API structure, so that you can reason about it, analyse it, perform checks, or transform it. It seems to me that servant-client by itself is superfluous when you're talking to a service that'll tell you where to go and what to do as you progress through some workflow. Generating clients is however not the only point of having an "almost first class" API. I'm suspecting there are some ideas out there that would allow us to do proper REST, with HATEOAS, without doing too much of the wiring up ourselves. Anyway, in short, servant lets you deal with the HTTP side of the app once and not have to worry about it anymore (where should this and that value come from -- query param, request body, some header in the request, etc). Whether the said app is basically RPC over HTTP or a proper REST application is irrelevant to servant. servant-client however indeed doesn't know about REST and HATEOAS, and therefore won't be able to navigate around a web app's endpoints by following hypermedia links, for example. You just get functions that can hit the endpoints that you've declared, period. I really hope that someone will explore HATEOAS support again, I'm sure there are things to be done on the server and client side to make it easy to define proper REST applications _and_ clients. We've got the entire structure of the API at our disposal.
&gt;The top layer is where you glue everything together. I was wondering how widget that have shared state are glued together, if most of the state is local it would be hard to keep track of it especially when having several widget that depends on each other.
The ICFP programming contest is a 72 hours programming contest associated to the [International Conference on Functional Programming](http://icfpconference.org/). This year it is organized by the University of Edinburgh. And it's fun as in functional!
&gt; And it's fun as in functional! It's also wOOhOO what a great competition as in OO.
Seems like a good one.
Groan.
&gt; Every example I've seen of comonads so far has been a wrapper around a catastrophic space leak waiting to happen. Really? Can you provide an example?
Warning to those who venture here: Complicated yet completely unenlightening thread sibling to this one!
It's pretty depressing that this article uses `DataKinds`. A mere data SocketUnbound = ... data SocketBound = ... data SocketListening = ... is perfectly sufficient. `DataKinds` serves absolutely no purpose here. Most of the applications that people cause people reach for high powered extensions can be done very easily in plain old Haskell98.
Ah man. I always want to do this, but never get organised. Have fun everyone!
Good suggestion. Done: https://github.com/ghc-proposals/ghc-proposals/pull/71
I'm not organized either, apart from the 72 hours being freed up for this purpose. And it seems I will be alone this year, again. Anyway you can just look at the task and have some light fun, it doesn't have to be serious!
So can't you just ssh into your shared big machine which runs docker containers for developers? Your compilation script does something like: # commit and get it on a branch somehow - probably some faster git magic than this. git commit -a -m 'just compiling' COMMIT=$(git rev-parse HEAD) git reset HEAD^ git branch -f temp-compilation $COMMIT # Push the commit and start compilation through some hook? git push docker temp-compilation:refs/temp-compilation # something like stack build -jlots | socat - UNIX-CONNECT:/tmp/stack.output ssh docker 'socat UNIX-CONNECT:/tmp/stack.output' # or just run ssh docker "stack \"$@\"" 
That's cool. How did you deal with flowing text? Does cairo do that automatically?
What is the status of cabal new-build(Not just in HSoC. In general)? Is anyone using it? Can it replace stack in future?
Hspec allows to combine QuickCheck as well. I don't know if there are any other advantages to Tasty.
Cairo is capable of flowing text (it calls that wrapping), but in this case each inline element can have a different font, italics, bold, size, etc. (or later, embedded images or padding) so I did flowing manually word-by-word by asking Cairo for the text metrics and font metrics ([code here](https://github.com/chrisdone/wish/blob/master/app/Main.hs#L346-L372)). 
That was four whole minutes ago and still no problem specification! I demand my money back!
Cool!
Pretty disappointing that this happens in 2017.
Mirror of the task: http://f.nn.lv/ou/5t/s6/task.pdf
`cabal new-build` already works. I use it. If you combine it with a stackage snapshot's cabal constraints, then it's nearly the same thing as stack.
Chris, this is cool. I mean, thanks for sharing it! Did you know that `wish` has been a common name in UNIX `PATH`? https://en.wikipedia.org/wiki/Wish_(Unix_shell) https://linux.die.net/man/1/wish
I'd really like to hear more about haskey since I recently implemented a mutable b-tree in haskell myself. I'd love to see the different allocators the student has worked on.
Is this just an experiment or do you have aspirations for it to become a Dillo alternative?
This seems to be very similar to what I do in my [`prefolds`](https://github.com/effectfully/prefolds) package. The encoding there is as follows: data Drive a = Stop !a | More !a newtype DriveT m a = DriveT { getDriveT :: m (Drive a) } data Fold a m b = forall acc. Fold (acc -&gt; m b) (acc -&gt; a -&gt; DriveT m acc) (DriveT m acc) There are multiple ways to compose `Fold`s, most notably there is a monad instance for `Fold a m` (when `m` is a monad). I also have stuff like `chunksOf`, but my version is defined in terms of more generic `chunks` like this: chunks :: Monad m =&gt; Fold a m b -&gt; Fold b m c -&gt; Fold a m c chunksOf :: Monad m =&gt; Int -&gt; Fold a m b -&gt; Fold b m c -&gt; Fold a m c chunksOf = chunks .* take `chunksOf 5 f g` reads like "fold each consecutive 5 elements of a list using `f` and then fold the resulting list using `g`". In the same way you can define a pretty cool version of `inits`: inits :: Monad m =&gt; Fold a m b -&gt; Fold b m c -&gt; Fold a m c which is two orders of magnitude faster than the prelude version.
It's a demonstration only.
`close :: Socket state -&gt; IO ()` and other functions that does not care about the state of the socket will be a bit smoother with `DataKinds`, since you can be polymorphic over it. Each to their own! :)
As a newer haskeller, why would you advise against using wreq as a dependency in a library?
This is true. The `DataKind`s style would be ideal for that. `close` does not, however, exist in that blog post!
that's less awkward than `-&lt;&gt;` I wonder if they're planning to support `-&gt;_p` for `p` other than 1 and omega.
This overview is great, and the whole Summer of Haskell operation seems very clear and effective. Impressive!
* \#4 has no purpose. Codensity and CPSing are both kinda doing the same thing, forcing the result to be constructed once you tell it the meanings of the parts and deferring work until then, so applying Codensity to a CPS'd representation doesn't help. * \#6 is pretty close to the "freer" monad stuff that Oleg and company like nowadays. That is basically a Reflection without remorse 'free' built over `Codensity f` rather than over f. This plays the same role as the "operational" monad does with respect to the original free monad. If you are only ever going to consume the result once, then something CPS'd or RwoR is probably a good idea as it makes left associated binds less. If you are going to do a mixture of binding and the interpreting outer layers of the monad then reflection without remorse is your friend. Unfortunately reflection without remorse can't handle certain kinds of effects like Cont gracefully. The CPS'd reps here would have to pay for the entire size of the structure to convert back/forth to a normal encoding for each inspection. If you need to inspect the result lots of times and constants matter to you, then the traditional ADT encoding is probably your best bet. RwoR handles this case but the constants are way way worse.
If you view a free monad as building up a tree out of nodes with the shapes given by the base functor then something like data Bin a = Bin a a deriving Functor lets you build binary trees with Free Bin a. When you do a left associated bind you graft new binary trees at the leaves, then construct the whole tree, then you walk that whole new tree and go down to the leaves again and do another substitution. You walk the 'prefix' of the tree twice. A right associated bind substitutes once at the leaves with a bigger tree that it constructs on the spot. An simpler, but analogous case is to consider the cost of: (a ++ b) ++ c vs a ++ (b ++ c) The cost to finish constructing the former in the presence of laziness is 2x the length of a + the length of b, where the cost of the latter is the length of a + the length of b. 
Feel free to open an issue on the stackage github repo, or send an email to the Stackage mailing list about these sorts of things. Something weird is going on with your package. Have the bounds on the latest version been revised? http://hackage.haskell.org/package/foldl-statistics-0.1.4.6 I can't see any revisions, and yet, when I attempt to re-enable it, I get several bounds issues. base-4.10.0.0 is out of bounds for: - [ ] foldl-statistics-0.1.4.6 (&gt;=4.9.1.0 &amp;&amp; &lt; 4.10).@Axman6. @Data61. Used by: library, test-suite, benchmark criterion-1.2.1.0 is out of bounds for: - [ ] foldl-statistics-0.1.4.6 (==1.1.*). @Axman6. @Data61. Used by: benchmark foldl-1.3.0 is out of bounds for: - [ ] foldl-statistics-0.1.4.6 (&gt;=1.2.5 &amp;&amp; &lt; 1.3). @Axman6. @Data61. Used by: library, test-suite, benchmark statistics-0.14.0.2 is out of bounds for: - [ ] foldl-statistics-0.1.4.6 (&gt;=0.13.3.0 &amp;&amp; &lt; 0.14). @Axman6. @Data61. Used by: test-suite, benchmark Somehow all-cabal-hashes has the wrong version bounds for this package version, and I'm not quite sure how that happened. I'll check with the other curators and see if we can figure out what's up.
Once you removed the solver and singular global package db, there never was any meaningful difference between Stack and Cabal. It was just that the solver and singular global package db were the default in Cabal, and sandboxes were a mediocre fix to that problem. Nix-style local builds solve the package db problem better than Stack IMO. But you still have to manually decide to use a Stackage snapshot instead of the solver, and you still have to manually manage GHC versions. The only remaining differentiator was multi-package projects, but `new-build` does that just fine too with `cabal.project` files. The biggest problem is probably that Cabal still has all the old features so it's easy for beginners to confuse themselves and do a bunch of the wrong things. Stack still makes the better default for most people, I think. Since it uses Stackage by default and handles your GHCs. But at this point, there's not any *critical* reasons that it's better than Cabal.
I wonder if [infix](https://ghc.haskell.org/trac/ghc/ticket/12363) [kind application](https://ghc.haskell.org/trac/ghc/ticket/12045) could be used, something like a -. @p b
ummm... I dont know why anyones thinking about comonads... those aren't what i'm talking about ..
&gt; https://www.reddit.com/r/haskell/comments/4aju8f/simple_example_of_emulating_copattern_matching_in/ see my post from a year or so ago 
Nevermind my naming conflict comment, then.
Chris, how do you get so much Done?
Figured it out. It's because the `test-suite` and the `benchmark` build targets have the restrictive bounds. These are enabled by default. You can either fix the bounds for those, or request for the benchmarks and test-suite to be skipped for this package.
At least for the purpose of polymorphism, yes. (I'm not quite there yet, though)
http://imgur.com/a/4HmO4
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/tWeTkf4.gifv ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dl5xfii) 
It took me quite some time to realize that the main reasons why `stack` made Haskell development so much more enjoyable for me are not having it manage GHC versions, stackage integration or even caching. It’s two things 1. multi-package projects 2. not having to mess with `cabal configure` (to this day I still don’t know what exactly that does and when I need to call it) `cabal new-build` solves both of these just as well (there is `cabal new-configure` but it behaves differently than `cabal configure`) so these days I’m using both `cabal new-build` and `stack` depending on what makes more sense for a particular project or which tool I managed to not break by doing something weird :)
I’m using it quite heavily. First of all, if you want to use it, I highly recommend that you build `cabal-install` from the master branch. in `1.24` it’s not really usable imho and even the upcoming `2.0` release will be missing things that many would consider crucial. However, `master` is actually quite usable and there are not that many things missing so depending on your workflow you might not miss anything at all. 
Wow, I didn't know about cabal-cargs, hdevtools, etc. before. How do they deal with things like ghc-pkg databases? My current workflow is to run `cabal configure` in a Nix shell; since `cabal configure` saves the database path to `dist/`, subsequent commands like `cabal build`, `cabal run`, etc. will use the right DB without having to use Nix.
Thanks for the link, this is really cool! 
Brick author here - very nice app! I've added it to the Featured Projects list in the brick README: https://github.com/jtdaugherty/brick#featured-projects
I'm totally in the same spot. I switch between the two on a per-project basis.
Yeah, if you're fine with using `cabal build`, etc. then you don't need need `ghc --make` and `cabal-cargs`. But if you want to speed up compilation of individual files it can be worth it. I'm not sure if Nix does anything special wrt. package databases, but it sounds like you're not using Cabal sandboxes, so you just get the default behavior where you have a global DB and a user DB. `cabal-cargs` won't change that.
Hspec's big advantage is test autodiscovery. The autodiscovery mechanisms in Tasty weren't great when I tried it out last (maybe 6 months ago?)
Yeah but if I program that in an imperative and strict language, I can keep the smallest subset needed for calculation in memory. In a functional language, it seems I must keep every potential state possible and then exploit laziness to prevent the program from potentially needing more space than the known universe? Is there any way to have slightly more guarantee that the program won't set your computer on fire than crossing your fingers and hoping for the best?
Well, the code speaks for itself, but I can try to explain it in english. Say you are using `auto-update` to call `getCurrentTime` time once per second: getTime &lt;- mkAutoUpdate defaultUpdateSettings { updateAction = getCurrentTime } - Until `getTime` is called, `getCurrentTime` is never called. - Say at `t = 5` some thread calls `getTime`. The worker thread wakes up and calls `getCurrentTime`, then sleeps for one second. All threads that call `getTime` between `t = 5` and `t = 6` will share the result of this one `getCurrentTime` call. - At `t = 6`, the worker thread wakes up and clears out the cached value, and we the logic loops (`getCurrentTime` is not called until `getTime` is called, etc...). &gt; My point is, yes it will be performed in a dedicated thread, but if the thread wait to be kicked in when needed, then you don't save any time my having another thread (as you have to wait anyway). You don't save time in the _first_ thread that calls the IO action, but you do save time and work across all the threads accessing the same value!
:D
There are more stuff stack gets right: - It's pretty well documented and got pretty decent guides for many levels in visible places - Getting started with a new project i cloned from github only requires `stack build` and waiting a lot - --fast, --file-watch, --profile, exec, script, install, hoogle, etc. all these commands and flags pretty much just work and are awesome - I can have a global ghc version with some package and also work on specific per project ghc+packages (though you already kinda mentioned that) I think these were the main stuff that made me use stack. Most of the time it just does what I want it to do.
Oh sure stack gets a lot of stuff right. But personally, I don’t rely on most of those too heavily so not having them/having something different (e.g. I actually prefer the way `cabal new-build` handles things like noopt and profiling builds since it doesn’t overwrite build products) is not a problem for me. The last point you mentioned, having a global project that I can use too quickly try things out, is actually something I miss a lot and the thing that `cabal new-build` just can’t replace at this point for me.
FWIW this is integrated into haskell-ide-engine, so available in any client supporting the Language Server Protocol.
Some things never change... ... but not hardware. "5 megs free!"
&gt; The problem is making a reasonable syntax for all this stuff Why not just use Idris' or Agda's?
I understand that, but you don't need to spawn another thread to get this behaviour. That's just normal caching with expiry date. Each you need the value , you check if it's expired and either the value or recompute it. My question is (I'll try gain) is this behaviour the intended behaviour ? I'm asking because in that [post](https://www.fpcomplete.com/blog) the author describe something which auto update a value every n secondes, but suggest to use auto-update which behaves sleep slightly differently ( update no more that every n secondes).
So this is fundamentally different from other formatters.... ...but how?
Cool! Does `brittany` solve the infix operator fixity problem that hindent suffers from?
&gt; based on ghc-exactprint &gt; Brittany aims to nicely layout the code and retain empty lines and comments as they appear in the input Is that not different enough?
I'm going to diverge a bit from the conversation. I'm looking for a Haskell source formatter myself, but with a unique constraint: I want to be able to omit the white space in a function application, when possible (and in particular, when the right-hand side is a parenthesized expression or a tuple). Am I doomed to write my own, or is it possible there's some option configurable enough to serve this need? I cannot simply remove them after the fact, as this breaks other alignment (both for prettiness, and even layout) that happens afterward.
https://hackage.haskell.org/package/hbro
glad I can help. As some other comments on this thread mention (to which i emphatically agree), coinduction + copattern matching very much is exactly what you'd use to describe/model/do "pure / immutable OO". Linearity then lets you recover "no backtracking" in modelling stateful systems thusly. 
I don't know. Doesn't `hindent` do that?
One difference there is that hbro uses the WebKit rendering engine.
It doesn't. It moves comments to the top of functions and reduces multiple spaces into one. You can't align things vertically.
Just noticed your post. Please open an issue on the project and we'll figure it out. I wonder if you're running into a memory limitation on the install machine...
[Found](http://ku-fpg.github.io/practice/remotemonad/) what I was looking for. And there's already support for HTTP with [JSON-RPC](http://ku-fpg.github.io/2016/02/09/remote-json/). So in theory, this could be implemented as just another event type in Miso.
Did you have prior knowledge of how to build a simple browser?
If you want HTTP-RPC, why not use an implementation of JSON-RPC in the remote monad design pattern? E.g., [remote-json](http://ku-fpg.github.io/2016/02/09/remote-json/). That particular implementation doesn't take care of the transport layer, which is annoying. But in theory, this seems like the better approach.
This could be useful as a lightweight companion for GHCi in the form of a documentation viewer without the huge overhead of a real browser. But then again, it'd probably be better to have a custom non-HTML representation for documentation and be even more efficient at it.
You can also disable vertical alignment in Brittany, which I consider positive.
&gt; He doesn't give a shit. "Everyone here knows Python. So we use Python. That's how we move fast." This can't be 'fixed'. The only feasible way is, as you said, get a team of Haskellers or Haskeller-to-be's and get shit done. Whether it's new hires or converted existing members, that's going to be much harder than, say, PHP -&gt; Python, due to the large gap between Haskell and 'mainstream' languages. &gt; but they will always see it as an "FP toy" :( That's an interesting perspective. It does seem like FP is either 'useless toy' or 'rocket science' to most people. Have you tried this: instead of showing them Haskell, show them how Haskell would have solved/prevented the problems they have. A bit like the discussion [in this thread](https://www.reddit.com/r/haskell/comments/6q2qax/ideas_and_help_to_convince_a_manager_to_move/)
Idris is relevant for Haskell as it seems like a test bed for ideas that could eventually be implemented in Haskell. It's also written in Haskell.
I started with http-conduit ([`Network.HTTP.Simple`](https://hackage.haskell.org/package/http-conduit-2.2.3.2/docs/Network-HTTP-Simple.html)) when I was faced with the same question recently. I haven't tried anything else yet but it was definitely OK. Being in IO and doing the exception handling was kind of a pain. For my next project I would settle on a better model for effects first of all and then pick the HTTP client based on that. 
&gt; "..Everyone here knows Python. So we use Python. That's how we move fast..." I disagree with this sentiment, unless he means moving fast as in spinning your wheels by dealing with bugs all the time. When dynamic language projects get large it becomes exceedingly expensive and risky to add new features. The team has to become obsessed with testing and review in order to push out even the smallest change out of fear of what it will break. It's sad. &gt; He's probably right. For the business, throwing everyone into Haskell is probably a bad idea Don't be so sure. I am thoroughly convinced that the cost of training is well worth it. But the more disappointingly common resistance I see is that people are shockingly unwilling to learn something new once they've started their career. I'm not sure I can continue working in a industry where this mentality is so pervasive. It's embarrassing.
Something I learned doing sales is the importance of finding the most receptive person to your product in the room. If you can get someone to say yes to what you are proposing first in front of a group, you stand a better chance of convincing everyone to say yes. The same works for no too. Even if "no" has gained sway initially, just getting one person to say yes may lead the way to another. Try to find the most receptive person and get to know what problems they work with, what interests them and try to angle Haskell to appeal to those specific things. If you can get them playing with the language in their spare time, you have succeeded. Just be patient and find ways to introduce Haskell that are appealing. If you get resistance though, just back off for a bit and try again later. From the sounds of things the only way to convince your boss to start doing work in Haskell is to convince your coworkers to want to use Haskell. For him it would be an uphill battle to impose a new language on them, especially when the language is from a completely different paradigm than what they are used to.
This is a talk about getting your organization to use Rust but the tips discussed are really universal: https://youtu.be/GCsxYAxw3JQ Maybe it'll help you. Good luck. Remember there are lots of Haskell jobs out there too. :)
I'm a fulltime Python engineer, and I'm often kinda surprised when people say they find it hard to get it correct. I have very little trouble writing Python code that works. But I also lean heavily on the REPL during development, and I don't think a lot of people do that (this may be changing with Jupyter, but oddly I think the most REPL-driven Python development is happening in the least engineery area: data science). It's hard for me to imagine learning a language without a REPL now. 100% agree on refactoring, and woe betide you if you have poor test coverage.
Unfortunately choice of AGPL makes it impossible to use britanny in some companies, where lawyers consider AGPL to be too risky to use :(
I'd recommend that you try to use Coconut for Python. It supposedly solves many of the issues that make Python unsuitable for functional coding and it transpiles down to pure Python 2 or the latest 3.
I recently developed a small web application with Haskell and Elm, but now I have to get back to Javascript... I really understand how you feel. Thankfully, I work at a startup and we have plans to slowly start transitioning our codebase (currently Ruby and Javascript).
If you want to make the Python a little more palatable, check out the `typing` module. It's standard in the newer versions of the interpreter and, together with type hints, offers a way to make the language behave in a little more strongly-typed fashion. I use it extensively within PyCharm, which has a type checking engine that uses the hints to locate type errors.
Same here. People will ask me questions and I'll call them over and have them watch me figure out the answers in seconds just by testing things out on the command prompt. They walk away happy, then turn right around and ask the same sorts of questions again the next day. I'm a "guru" to them, somehow.
A few job ago I was working at a place that did mostly C++, and I was told that I could implement a little throwaway script in whatever language I wanted, to collect some data from a device and do some stats . At the time I had years of Python experience and was maybe a year or two into playing with Haskell in the evenings. I did the task in about half a day in Haskell, including some QuickCheck properties that helped me fix some mistakes I'd made - one with a file format, and on with translating from the spec to code. It seemed really solid when I hooked it up to the equipment, and all was well. The next day I was told that "whatever language I wanted" meant C++, Java or Python, so I decided to do it with python. It took about half a day to do it in python, with a fair few unit tests in the place of property based tests. It took another day or so to get it to the point where I was mostly sure it was mostly correct. I spent quite a bit of time in the REPL in both cases, and I had more Python experience than Haskell experience and a working Haskell implementation to crib from, and it still took more time to get to a point where I was less confident in the output. I was still up on my Python at the time, and while I figured / had heard that Haskell had some good things to offer with respect to correctness, I hadn't really properly _felt_ it until then. I'm not sure I would have noticed the differences between the experiences if I hadn't had to complete the same task in a small period of time. I've only done bits and pieces of Python since then, but when I do I now tend to notice the class of little things I have to discover and fix up along the way, that I was previously dealing with but not noticing. I'm now a bit surprised when people say that don't have correctness issues with Python. Maybe different groups mean different things by "correct", or there are different perspectives in play, or something. I mostly like to share the anecdote because it had a big effect on me, rather than having any particular Python vs Haskell axe to grind (although I clearly have a strong preference these days).
Issue with REPL driven development is it gets a unit of code there fast - to an implementation of said unit that works on an example or a few number of examples. The hard part is that real problems are rarely single units of code on single examples, they require putting units together in ways that work, and that's where a REPL workflow in a dynamically typed language breaks down. I say this as someone who leans heavily on REPLs.
The upsides of Haskell are pretty great. But they can't compete with a team and business that doesn't care. You can either sell your team/business on Haskell, leave for greener pastures, or accept the idioms of dynamically typed languages and live with them as well as possible. "When in Rome," etc. 
Would that work? Haskell has syntactic distinction between types and values in the form of initial capitalization. 
I sketched something here for the generics case, tell me if that works. Everything is done so that you can have the last line. https://gist.github.com/guaraqe/2774f1c479d29a039a41ccda3735cf40
This is really nice to see for me because I've been idly wondering about the best way to write a simple graphical app that's still mostly text-based, and somehow "SDL with Cairo in Haskell" didn't come to mind.
Even a python only shop should recognise there's good Python and bad. Your Haskell journey should mean you write (and recognise) good Python. 
Where are the actual validations? `instance PostForm Text`? Wouldn't that force the same validation for all text field everywhere?
`lens` is a **_huge_** library. We tend to think you [shouldn't impose it upon your users unnecessarily](https://www.reddit.com/r/haskell/comments/23uzpg/lens_is_unidiomatic_haskell/) if you're writing a library. Admittedly this was more of a problem a couple years ago, when the Haskell world was still in the grips of `cabal` hell. The more libraries you installed, the closer you were to breaking your entire Haskell environment and having to delete everything and start from scratch. `lens` pulls in a metric bucketload of dependencies and transitive dependencies, since it tries to do so much. Nowadays, with `cabal sandbox`, `stack`, and `cabal new-build`, `cabal` hell is not a problem, but `lens` is still a very heavy library. By using `wreq`, you're forcing your end user to wait for `lens` and all its dependencies to build when they build your library. That ain't a good look.
Yes, that's the problem I see. That can be handled with newtypes or type tags, but otherwise I'm not so sure how that can be handled in a way that is not explicit...
What is the advantage of asking for 1TB of virtual memory? Instead of (a bit more) then you need
You can do Haskell at work, but it takes a lot of commitment. You'd probably have to do a lot of personal projects to manufacture your own experience as probably no regular company will let you play with Haskell on the job, and no Haskell start-up is generally rich enough to hire you with a plan of training you unless you have some other immediately applicable skill. I think most Haskellers using it at work today did so by using it a lot at home and then either introducing it at work or changing jobs to a company that used it, or by making a startup/becoming self-employed. Like others said, you can't win against a team that doesn't care, so either learn to love Python, or work hard to build your Haskell portfolio and move to somewhere that loves Haskell.
Haskell enforces more discipline on you than Python does. It sounds like your Python code could use some more discipline. Don't abuse the type system ("Some functions return one kind of type when used in one fashion... Some inputs to functions even work the same way."). Here you're blaming the language for your (team's) own abuse of it. Carefully limit side-effects. When I program in Python, even my objects are mostly functional. If refactoring/correctness is hard, you need to lean more on tests since you don't have Haskell's compiler checking on you. You're enjoying the puzzle solving "really hard lego" that you're conflating with "real programming". These are just tools with different pros and cons. Best thing to do is apply the useful lessons you've learned from Haskell to improve your Python code as I've described a little. Monads and Typeclasses on their own aren't going to impress anybody unless they're better for solving a real problem. The best case you can make at work is to take a real problem that was hard to solve in your Python codebase and show how much better it is in Haskell, and if you can't make that case then you don't actually *have* a case. I also want to point out that your attitude seems extremely poor. Your manager "doesn't give a fuck/shit", referring to Python "code monkeys", Haskell is "real programming" while Python isn't, etc. Be aware that your attitude *will* (and has) come across at work and your manager will notice your disdain for the work you're doing and for him and the rest of your "python code monkey" team.
Haskell does have REPL support. 
And I need to add the integration was done by Zubin Duggal as part of the [Haskell Summer of Code](https://github.com/haskell/haskell-ide-engine/blob/master/docs/Report-2017-07.md) work this year 
&gt; The next day I was told that "whatever language I wanted" meant C++, Java or Python, so I decided to do it with python. ...That was just *too* convenient :)
&gt; Does anybody else feel this way? Yes. I share that exact feeling. Especially the part about Haskell programming being like Lego. This is what made programming fun again for me. I only did professional Python programming after I was familiar with Haskell, and then Python was a more than underwhelming experience. It's feels so much harder being productive in Python, even if you fully commit to the idea of dynamic typing. I think the reason for this is that static types are essentially approximations of your programs and give you information about the code without running it, even if you're just looking at fragments of your code. Pythons programs can't be really understood without reading the entire thing and running it in your head, especially if there are side effects.
I assume you have had a look at [the linked comparison between `brittany` and others](https://github.com/lspitzner/brittany/blob/master/doc/showcases/BrittanyComparison.md) that already highlights how the quality is consistently better with `brittany`. But I concur that "fundamental" still needs justification. When deciding where to insert newlines, the searchspace has exponential size. Brittany uses a specialized algorithm that explores only a linear sub-space but that still manages to find many "good" solutions. The trick is to calculate and cache some estimated size information of nodes in the syntax tree in a first pass, and use that info in a second pass to only inserted newlines when necessary. I have seen this used in general-purpose `Doc` pretty-printers, but I think `brittany` is the only source formatter using this strategy. The following code highlights the results: Let's first consider the `hindent` output: mybinding = RH.performEvent_ $ postBuild &lt;&amp;&gt; \() -&gt; liftIO $ do runMaybeT postCliInit &gt;&gt;= \case Nothing -&gt; return () Just () -&gt; foo Note that everything from `RH.performEvent` to `do` fits in a single (80 column) line, but `hindent` uses the conservative approach to insert newlines in order to prevent potentially overflowing the first/other lines. And without a clever algorithm, this is the right choice - in general we rather want newlines than potential overflowing. Still, `brittany` manages to explore the single-line choice and finds and produces: mybinding = RH.performEvent_ $ postBuild &lt;&amp;&gt; \() -&gt; liftIO $ do runMaybeT postCliInit &gt;&gt;= \case Nothing -&gt; return () Just () -&gt; foo `brittany` does not just have more special-cased instructions to handle specific cases in some better way; this is really the difference of the underlying algorithm. And I don't think that `hindent` could be modified to have the same properties without adopting this central idea (and if you did that, you'd end up with a re-write anyways). 
&gt; It does seem like FP is either 'useless toy' or 'rocket science' to most people It's cognitive dissonance: I'm a pretty smart person and I don't use Haskell, therefore there must be lots of good reasons for my choice. It doesn't really matter if "toy language" and "rocket science beyond what we do here" contradict: the important thing is that either way justifies my behavior and means I don't have to go through the pain of investigating and potentially having to confront that I may have made a less-than-informed choice in the first place.
I'm really happy to see my little library (sdl2-cairo) being actually used! :) I am open for any suggestions or feedback about the Canvas API!
I assume you refer to [#238](https://github.com/commercialhaskell/hindent/issues/238)? Well, let's see: brittany produces func = do do pgsqlExecStmt conn x proceed savepoint `catch` handleSqlError savepoint `catch` handleQueryError savepoint which.. is exactly the input, and certainly valid haskell. To be fair, I have a vague notion that this example breaks an invariant in `brittany`'s internal intermediate representation - but the end user doesn't have to know about that as long as it works :p If you find any related example that breaks, please report it.
Haskell is the opposite of Python and also has drawbacks. Some or many Haskell libraries are unnecesarily complex, because they are done with a research mindset. The learning curve is thus, unnecessarily long. Python has a well established base of libraries that everyone knows. Haskell publish a dozen libraries for the same purpose each year, and everyone is supposed that must adopt to be in the crest of the wave. Some of them are from the same author. This is good for hobbyists, but not at all for production. Most of this is being addressed by FPcomplete but the overhearing noise produced by hobbyists and academia has given Haskell the reputation that it has and this is very difficult to change. It does not matter if mtl is well maintained if in a few months a new effect library gain traction and makes mtl code obsolete and a large proportion of the Haskell ecosystem with it. Technical obsolescence in the haskell community is worst than bitrooting. In the haskell community, nothing is fixed. Not even the most basic things. Easy refactoring mitigates this problem, at the cost of wasting more time in the internals rather than in the domain problem. I like the ETA initiative. Names are important. Giving Haskell a different name under the JVM would change the perception from outside and inside. I hope so.
For dependencies I might see this, but for a tool that transforms **other** "works"? Or do those lawyers see the possibility/risk that the transformation (and output) itself becomes a "covered work", even when the input is your own?
This is an example. but I actually meant these: [#392](https://github.com/commercialhaskell/hindent/issues/392) [#230](https://github.com/commercialhaskell/hindent/issues/230) The fixity of operators will have to play a role in formatting. Thank you /u/ocharles for bringing this to my attention.
The only way to change it is to offer something that they can not refuse. They do not appreciate type safety, factorization, compile-&gt; works or any other feature derived from type safety. This killer feature is algebraic composability and reusability without compromises. [An effect library that can compose well](https://www.reddit.com/r/haskell/comments/6mt8i6/do_you_have_to_choose_between_composability_and/) That's what the software industry really need. 
Well, it is certainly possible to add a config option in `brittany` that enables this behaviour.. how much work this would be is a bit hard to see. The main issue seems that whether or not you want the whitespace depends on the next element inserted, and this dependency is not regularly present. I see the possibility of an evil hack relatively late in the internal process, although that might come close to what you described as "removing after the fact" and might suffer the same problems. Feel free to open an issue with examples and potential issues that you foresee and I'll do some estimate whether it can be done in `brittany`.
&gt; and I'm often kinda surprised when people say they find it hard to get it correct. I have very little trouble writing Python code that works tbh this is a bit embarrasing to admit but I can't seem to write any python that works. I tried writing a very small test that calls a process, pass data through stdin, gets back data through stdout/err and decides whether it terminated correctly or not. I couldn't get it to even call the process. And the error messages were completely unhelpful. a general "syntax error" or a general "error" with no information beyond that. I just copied like two lines from the main docs that seems really straightforward but it didn't work and i didn't get any feedback I could use to understand what's wrong. This has happened on more than one occasion. With everyone having such a huge success with python the only thing I can conclude is that I'm too dumb for python.
But actually, if your goal is to introduce static types then Rust might be a more palatable language to introduce. Either way, introducing small, easy to understand bits of code in places that show real value is the way to go.
`brittany` does not look at fixities atm, no, and I am aware of this general issue because I'd like CNTrackNotes $ CNNoteTrack M.empty $ Foldable.toList ns &lt;&amp;&gt; \(p, e, expre, artics) -&gt; CNNote p e expre artics () over CNTrackNotes $ CNNoteTrack M.empty $ Foldable.toList ns &lt;&amp;&gt; \(p, e, expre, artics) -&gt; CNNote p e expre artics () the latter being currently produced by `brittany`. I have considered hardcoding fixities for some special cases but haven't implemented it yet. (still, #392 is type alias which `brittany` does not transform at all yet, and #230 seems to properly handled by brittany (apart from perhaps the custom indentation added. #230 does not seem to benefit from fixity-awareness, regardless.)
It's simple then. You don't like your job. Find another one. Don't adapt to the bullshit you have a distaste for. The world is full of opportunities. Start going to Haskell meetups and conferences and build up your connections. I've met a lot of recruiters at such events. That would also be the way that I would hire people myself.
&gt; Haskell publish a dozen libraries for the same purpose each year [...] This is good for hobbyists, but not at all for production. I've dealt with a lot of this in Haskell production code and it has never been a problem. For example going from enumerators to conduits was very easy. The AMP proposal (back in GHC 7.10 I think) made all my monad instances invalid, which took at most five minutes to refactor a 10k code base. And when all compiler errors are gone, it just works. Ripping part of you code out and replacing it by something else is so much easier than in any other language I know. This is where all of Haskells strengths come together. And it makes maintaining the code quite pleasant, actually.
Compilation should be much faster. Perhaps binary caches for haskell libraries would be a welcome part of that.
Last time I have engaged in the discussion of details there were two concerns: * wording which is used by AGPL has not yet been tested in courts and, if interpreted wider than common sense would, it is very easy to "contaminate" other software being worked on or being worked with if AGPL tools are used; * development environment we are working with is very much based on tools which perform various kinds of network operations (e.g. distributed builds and tests, code linting and formatting via a central service, distributed source control along with distributed workspaces, etc.), which makes AGPL wording very dangerous since it suggests that any networking operations are "AGPL-contagious". I believe these were two main reasons why lawyers banned AGPL tooling in our development environments.
It seems like intero is the best way to go but at the moment Atom's use of ghc-mod seems like the most usable. For my project this is excruciatingly slow (to bring up information on a type by hovering for example). However, finding the use of Atom with ghc-mod, hslint and hindent somewhat useful at the moment.
I love it! Seems like others also weren't aware of it and how easy it makes text rendering on SDL. Actually I did add a couple functions for text/font stuff in a fork which I've linked to in the stack.yaml. I was considering cleaning them up for a PR. Traveling at the moment but you could take a look!
Nah, but I'm familiar enough with web dev to figure out the basics.
You can point out that discord runs on elixir (a functional language). I also believe fb uses Haskell and I know there are some unofficial Google projects in it.
Seconded. Outstanding effort by everyone involved!
Maybe suggest getting started on a similar panfish that has a lot barrier to entry (like Python down). Like Elm, or something. Your boss sounds like he has a rotten attitude about things, even if what it says makes sense. I would let my employees at least work in internal tools in whatever language they want. (So long as others aren't forced to maintain them.) That being said, I still like Python quite a bit despite getting more and more into FP. Some things are more concise and more easily generalizable in Python.
See [the `stack` docs](https://docs.haskellstack.org/en/stable/travis_ci/), in particular the ['complex'](https://raw.githubusercontent.com/commercialhaskell/stack/master/doc/travis-complex.yml) script is what you are looking for.
I haven't used `cabal new-build`, but I have a project which does CI with multiple versions of `cabal` and `stack` LTSes here: https://github.com/tebello-thejane/bitx.hs The pattern should be simple to discern. 
And you also can use it on python 2 defining the types using comments. Take a look at http://mypy-lang.org
Thanks, that looks like a good starting point. Adding new-build hopefully is not much more than another case statement.
Not really. It's a 4GB i3 Chromebook, with crouton installed. I will open a ticket there. Thanks
To avoid frameworky stuff - I've used wai/warp which is the networking/http level part of yesod. You can see a basic example, with routing here, https://hackage.haskell.org/package/wai
&gt; "..Everyone here knows Python. So we use Python. That's how we move fast..." &gt; I disagree with this sentiment, unless he means moving fast as in spinning your wheels by dealing with bugs all the time. Agreed, but IMO it goes beyond that. Personally, I loath most "fast-paced environments", including daily standups. All of these things were IMO created to milk as much short-term productivity as possible out of cogs in the startup machine. With Haskell, you don't really need all that. Just by using Haskell, you can switch from daily standups back to ad-hoc biweekly or weekly short meetings over cups of tea. Everyone can enjoy a more relaxed and serene atmosphere and be productive for years and decades without burning out.
I would really like to have a `--inplace FILE...` option, so I can do `brittany --inplace **/*.hs` in my source code repos ;-) (And I think `-i`= `--inplace` is much more useful than `-i` = `--input`...)
Haskell is most certainly my favorite language. That's said, I _love_ python. And my knowledge of Haskell has made me a much better Python programmer. I've learned to use functional concepts in Python whenever I can, I refrain from writing functions which have side effects, and I think about my functions in terms of what their input and output types are. The point is that even though Python doesn't explicitly enforce the rules that Haskell does, you can write excellent, working code in Python. And, if used sparingly, the dynamic nature of Python can come in handy in a pinch.
/u/jkarni once experimented with a servant-y approach to validation. Not sure where the project stands these days, but here it is: https://github.com/jkarni/verdict
So you mean these two are very different, because they both amount to 'it's none of my business'?
&gt; That's what the software industry really need. Well, I guess the only fair way to see is to wait 
Ah that's exactly what I was wondering, thanks for the clarification!
Get a Haskell job. 
It's probably not about Haskell, per se. It's about staying in one's comfort zone and not having to do any thinking or make any tough decisions.
I agree, I use python for my day job and FP has just helped me think about how to write better code. Either shift to a company that will let you use Haskell or just know that it will give you another way to think about how to solve problems, regardless of language.
I'm a Java developer. My Haskell experience is some puzzles/toy stuff in a college course on programming languages and working halfway through haskellbook.com. If your complaint is the language you have to use instead of the quality of code you might be in a pretty fortunate situation. I recently worked on a recent green-field project written in modern Java on one day, and some legacy Java stuff on the next. The contrast was startling and I really lost a lot of my motivation when I had to work on the legacy stuff. I'm also a bit bemused (and confused) when people that come from a dynamic language praise Haskell for its type system. (Side note: I have not experience with dynamic languages). I'm sure the Haskell type system is way cooler than Java's. I know for a fact that it is much more advanced then the types in C. But even C has types and that language is an old hat. I never know in such situations if the praise for Haskell is really warranted when compared with e.g. Java. Retraining all employees to learn Haskell and rewriting all your code seems unreasonable to me. My older colleague already has problems with Java 8 features (mostly lambdas). Learning a whole new paradigm is not nothing. I would suggest looking into *static typing in Python* (/u/hosford42 and /u/peterjoel mentioned rellevant projects) and other *static analysis tools*. Use them in your own code first and try to convince your colleges to use them. Also look into whether or not the automated tests could be improved. Try to have your *tests* run and analyses checked by a *continuous integration* tool. If you really hate dynamic languages (which might be reasonable, I don't know much about them) there are plenty of languages besides Haskell that support static typing but might be more employable. Java, C#, TypeScript, Kotlin, Swift, Scala, ...
I've been in this exact situation several times. Here's what I've found: selling Haskell won't work. *Very* precious few shops switch their language at all, let alone switch to a language that's in an entirely different paradigm quadrant (dynamic OOP vs static FP). Whether this is good or not for the company is largely irrelevant. The only successful cases I've seen are when the management actually make this transition themselves and initiate. I even had the chance once to convince the VP of Tech at a company that static FP was the future. He believed me, but his managers wouldn't budge. You have some options: 1. Try to do tiny little side projects in Haskell. This only sometimes works and it rarely has long-term impact on your company as a whole. It's mildly satisfying, but it might only make you more unhappy. 2. Try to slowly push your company into the other quadrant. In your case, start trying out Mypy for the type system. See if you can get traction there. Or possibly see if you can write some Coconut to get more FP flavor but still work with Python. Depending on your management, one of these may be more feasible than the other. 3. Try taking advantage of nascent interest that may be in your favor. For example, I've noticed a lot of Pythonistas getting excited about Rust. Rust is not Haskell, but it's a heckofalot closer. Maybe you could learn Rust and get traction there. 4. Look for a new job. 5. Start your own job. (Consultant, startup, whatever.)
It'd be cool if you could have Brittany take a user supplied list of fixities, with Brittany providing a default for common operators. This way `haskell-ide-engine` would be able to pass the actual fixities that it knows from GHC.
hvr has a template for using cabal new-build: https://github.com/hvr/multi-ghc-travis
Also note these related articles: - [The Reader Monad](https://hackernoon.com/the-reader-monad-part-1-1e4d947983a8) - [The Reader Monad Part 2](https://medium.com/@jonathangfischoff/monad-reader-part-2-d812dda1d03e) 
I'm interested in this as well. Currently my server is just trusting the client's validation. This is ok-ish when your API is used exclusively by your front-end and you have proper authentication/authorization/CSRF-prevention in place. But it's still not ideal.
HIndent prefers regularity over special cases. The dangling lambdas and do's are special cases that were contributed. I don't think long leading lines are a good idea, they don't read well. So I don't think Brittany's output improves on the original HIndent output. 
I doubt anyone other than me will be interested in this, but I've got [this really bad Nix based CI](https://github.com/ElvishJerricco/nix-simple-ci) that you could use. You start up a build server somewhere and point a GitHub webhook at it. Every time you push code to the repo, it essentially just does `nix-build ci.nix`, and posts those little commit status bubbles to indicate success or failure. Since you an write whatever Nix expression you want, the build can be arbitrarily complex, including running tests, benchmarks, or whatever else. The typical Nix build for Haskell code is probably close enough to a Cabal build, but you could definitely setup an actual `cabal-install` based build if you needed to. And there's already a `buildStackProject` to test Stack builds. You can see an example `ci.nix` in the repo itself, and if you look at the commits, you'll see some of those status indicators. The reason I say it's bad is that it currently doesn't post the build log anywhere for you to see, and it doesn't build pull requests. It has a log on the server you can read to see the build log, but they're not separated or anything. So it's definitely not ready for any kind of prime time. But it's worked nicely for me. One of the benefits of it is that if your Nix setup is somewhat complicated, on top of the fact that most CIs don't let you have as much control as Nix does, all your dependencies (and I guess the build too) will be cached on your build server. So any teammates who use that as a remote cache on their system won't have to rebuild those dependencies. This has proven really valuable for WebGHC, since our clang builds [on `wasm-cross`](https://github.com/WebGHC/wasm-cross) are cached.
But they aren't special cases for brittany. It is the same "check how much space is left in current line + in all next lines" everywhere. 
Interesting; I'll have a look, but it will be mostly out of curiosity about how much work this route takes; I haven't dove into nix stuff really so I'll first try the other suggestions here. It does look like a much more beautiful approach in general though. The amount of cycles wasted due to lack of proper caching is a pity, in many simple CI setups..
&gt; It does not matter if mtl is well maintained if in a few months a new effect library gain traction and makes mtl code obsolete and a large proportion of the Haskell ecosystem with it. This is clearly not true, is it? Why would you say this?
If you're a Stack user, you can get started with Nix very easily! `nixpkgs` comes with a really easy way to make a Nix build that basically just does `stack build` and `stack test`. You could try this `ci.nix`: with import &lt;nixpkgs&gt; {}; haskell.lib.buildStackProject { name = "foo"; src = ./.; inherit ghc; } But `buildStackProject` has a bunch of problems, in that the Nix build it creates doesn't cache the dependencies that Stack will build. So you lose the advantages of *both* Nix and Stack, and each `nix-build` will have to recompile ALL of the Haskell dependencies. I guess it works for CI, but not much else. If you're willing to eat the fact that `&lt;nixpkgs&gt;` doesn't necessarily have the same package set as your `stack.yaml`, you can use: with import &lt;nixpkgs&gt; {}; haskellPackages.callCabal2nix "foo" ./. {} But if you do want to use your Stack package set, you can use [stack2nix](https://github.com/input-output-hk/stack2nix). Just call it on your project, save the result in `ci.nix` and check that into git each time you change the `stack.yaml`. It's kind of heavy weight but it is easy, and gives you all the caching.
Have you found the type system in C/C++/Java or C# to get in your way much?
&gt; A nearly useless type system is worse than no type system. I don't agree with that. Recently I have started working on a project in TypeScript after working in JavaScript. And let's be hones, the type system of TypeScript is not beautiful or greatly powerful, it's incomparable to Haskell or Scala (which I would recommend to the OP - a lot of FP, but still used quite a lot in "real" world). But if one actually uses types (TS doesn't enforce them), it is still a tremendous help. Even not-so-great type system of TypeScript managed to catch dozens of errors which in JS would be really hard to find (like various crashes in asynchronous code in which stack traces are useless; or not clearly defined where null/undefined can be followed by "random" crashes at runtime). Also TS allowed to skip almost entire class of tests - no need to write some type checking at runtime in tests. (In my case I don't have allowed to test a lot of functionality [cheap client], so I am really grateful that at least TypeScript compiler does some checking which quite frequently ends with complains = saved time and nerves on debugging).
wow, impressive! But too much type-hackery for my taste. There's got to be an easier way to solve this problem
This is really easy to detect in people. You can try proposing using a more complex library on an internal tool, and see how someone reacts. Some people engage you. Some people shut down before even considering it.
According to [release notes](https://downloads.haskell.org/~ghc/master/users-guide/8.0.1-notes.html#runtime-system), it simplifies memory allocation. I don't know beyond that. 
I use both Haskell and Python in my day job. And honestly, Python isn't that bad once you wrap your brain fully around it — a thing few people bother to do. If you care to dig into the innards of Python and figure out what makes it tick as a language, I can heartily recommend the book *Fluent Python* by Luciano Ramalho.
Small, self-contained projects can help to introduce "weird" tech, especially if it at least starts out as a tool that you alone are using. I've done that in the past with Common Lisp; basically, there was a need I saw to improve on some tooling, and wrote the code to make my job easier. I also offered to re-write the cludgy old tool whose limitations I was trying to work around, and was denied. Being a billable, client-facing resource meant that they didn't want to spend my time, nor did anyone feel the need to commit dev time to an internal tool. So anyway, my tooling left with me, since nobody else knew or cared. Not the ideal case, but it worked well enough for me.
Could you elaborate on how you think `cabal new-build` fails in these ways? The scripting thing is definitely unique to Stack, but the HSoC project for `new-build` has covered many of the other points, and I think your second bullet is covered just by what's already been released.
Are there any plans for implementing more advanced quantum computing tools? Right now you have what's basically the first twenty minutes of an undergrad QC course--a great start, but a very simple one.
I haven't used cabal since I started using stack about two years ago so I can't really. edit: but: - I'm not aware if cabal handles ghc installation so handling multiple version of ghc is still your responsibility I think? this is also relevant to my second bullet. - I found cabal's documentation lacking and I would not refer a beginner toward it, I also rather not explore it when I run into trouble. I already had ok experiences looking up docs and guides for stack when I needed.
My Python code has a clear maximum size beyond what it can not grow. It changes by how much coupled is the problem, but every time I keep adding stuff to some code there is a point it will break and I can not get confident I have fixed it anymore. It will mostly work, but I can never be certain. I have seen a similar problem with C (not C++), Perl, PHP (with really small sizes), and Bash. From those, C and Python scale the best, but I have never seen such hard limitat on the large OOP languages, and Haskell seems to scale even better.
One thing that I really enjoyed with Haskell was how pure it is, how mathematical it is, and all that. What sold me about all of those things is how you can compose Monads together and different type-classes together. What *floored* me about it was GHC being able to derive tons of code all over the place with newtypes and other deriving methods; combined with everything else, it lets you write more correct and performant code with less work. Write less; reuse what you wrote in more places; have your code be more correct, easier to debug, and often faster. That's probably how I'd sell it in the industry.
&gt; I never know in such situations if the praise for Haskell is really warranted when compared with e.g. Java. The Java type system is mostly restricting. I mean, it helps you organize your code by telling you you can not do things, and nothing more. In contrast, the Haskell type system is almost as empowering as it is restricting. It creates new ways for you to express yourself about as much as it says "Nope, that shall not pass".
In fact, text rendering was not one of the selling points I thought of, I could not even dream that someone would use it mainly for text rendering! I've looked at the functions you added, they make perfect sense for that purpose, but I'm thinking about exposing all the information in FontExtents and TextExtents in the most general and useful way. Probably passing through FontExtents directly and wrapping TextExtents in some fashion that simplifies alignment and removes the need for seperate functions textR, textC and textBaseline. Maybe with a parameter like data Anchor = N | NE | E | SE | S | SW | W | NW | Center | Baseline and generalize that in a way that harmonizes with the Dim type that is used for rectangles, ellipses and images.
It's not that type systems in C-family languages get in the way, for me, it's more that I don't feel like they actively help. Can this value potentially be null? Or is it guaranteed? In Haskell I use a type that tells me, in Java I have to use @nullable and hope that I used it *everywhere* religiously and that anyone else's code that I ever touch has also used it, in C/C++ I just test *everything* for nullity. You also tend to not notice the weaker type system in C-family languages because functions aren't really first class citizens at all, so you're not returning the results of a function. As such, there's a lot less things you have to return; usually some sort of generic success value and that's it. (Except for Java which is awfully fond of foo.bar().baz() style). With always passing functions into each other, it makes a type system exponentially more useful because each step of a pipe-line has its types and you check all of that with the type system. What I might write in one function in Java, I might write in 5 functions in Haskell--each of those functions being one "block/step" in Java. But then, since each one is typed, it's like having a test for each block in every function verifying that the output is what I expect it to be, but I don't need to spend any time writing those tests. Contrast that with a C-family language in which I'd just return 1 or 0 or maybe True or False or maybe even return the result if happens to be something very simple.
It looks like he just turned on the kitchen sink to get something close enough to dependent types; I'd imagine this kind of library would be much easier (even elegant) to write in a fully dependently typed language. Refinement types also might help a lot with that (separate from dependent types). In fact, verdict seems to be a library that exists only because we don't have a type system strong enough to impose arbitrary constraints on types like we can with dependent types.
Everyone learns differently, but IMHO this is going about learning things backwards from what seems to be your desired goals. No amount of learning about cool concepts and abstractions will make you more able to pick up a web framework. What you need is a primer in the basic syntax and then a tutorial on the framework you're interested in playing with. If you feel like you "don't understand what's going on" for awhile -- that's fine! The more you build, the more you'll be able to see patterns between what you've built. Or, if you really want a more thorough understanding, read through blog posts by the framework author or source code to pieces of the framework itself. These give you concrete handles on the specific thing you care about understanding. I love the abstract theory stuff that floats around the Haskell community, but those things are not Haskell and they are not programming and if you wait to understand them all before you really code you may be waiting forever. New stuff being thought up all the time :)
IMO you can't be ready without just diving in and actually using this stuff. All the book learning in the world isn't a substitute for writing real code. My first few months of Haskell consisted of spending 15+ minutes thinking for every line of code I wrote. It was slow and frustrating, but it felt good to actually get done! In terms of actionable advice, I'd suggest looking at Servant; it lets you define a type describing what your API should look like, and then you just write regular Haskell functions for each route. There's very little (visible) magic going on, so I suspect you won't have too many issues with it. Source: a professional Haskell programmer who still hasn't figured out how or why to yesod.
This is very nice. Great job.
Yes, I have some plans, in the next period I want to focus on implementing some algorithms, define quantum register and associated operations and right now I'm working a module that will be used to measure the qubits. I plan to support this, now small, library for long time and I'm open to new ideas. 
I'm not expecting the types to represent validations based on values. I think it's overkill and is clearly in the research domain, as far as I'm concerned. I'm just expecting to define similarly shaped records. One which has unvalidated values, and the other which has a function to validate each individual field, and a way to "zip" these two on a per-field basis.
&gt;My manager doesn't give a fuck. I try to sell him on the upsides of Haskell. The powerful abstractions. The relative comfort of refactoring. Typeclasses. Monads. The whole nine yards. &gt;He doesn't give a shit. "Everyone here knows Python. So we use Python. That's how we move fast. And hey, somebody used a lambda recently in one of our scripts!" &gt;He's probably right. For the business, throwing everyone into Haskell is probably a bad idea. We'd need to hire people who know Haskell, and that's not going to be as cheap as hiring some python code monkeys of the street. I'm not sure that's right. All the things you mention as pain points in Python have a concrete cost in terms of developer time. I personally find Haskell to be more expressive. Admittedly this varies by domain, but there are places where Haskell is the most cost-effective. &gt;I try to show coworkers a little bit of Haskell, and it's "nice", but they will always see it as an "FP toy". :( Times are a-changin'. The 'm' word is becoming cool. Friends are even asking me why the `(.)` or `($)` functions work they way they do. &gt;I had to get all that off my chest. Does anybody else feel this way? Is my thinking misguided? &gt;I welcome any comments you all might have. In terms of personal advice: there are a bunch of people hiring Haskellers now. Your manager sounds... not the best.
Like the others, I would also recommend that you start with a small coding project. Nothing teaches you more than getting your hands wet. To get started, I recommend [Scotty][1]. Put the example in a source file, compile it, and go from there! You can add the conceptually more intricate stuff later. (That's how I usually do my projects: Start with an absolutely minimal working example, and then rewrite and extend as needed. The advantage of minimal is that, it doesn't take much time to completely rewrite it.) [1]: http://hackage.haskell.org/package/scotty
&gt; It moves comments to the top of functions That's not true. But it does completely disregard whatever whitespace the user started with.
Exactly. You could propose Rust or Ruby and the reaction would be *the exact same thing* for a chunk of these people.
&gt;The only way to change it is to offer something that they can not refuse. The thing is, there is a large chunk of programmers that will *always* refuse, because they don't understand it. OP's manager sounds like one of them, although I'm sure OP can judge that far better than I can.
&gt;Well, I guess the only fair way to see is to wait I think this is a fairly defeatist attitude. The thing about Haskell is it *did* have design goals that were very theoretical, and that was borne out in the GHC we have today. I agree that "Haskell is better because it makes refactoring easier" depends on having a version of Haskell that's usable enough to compare to languages like Python, Ruby, or Rust, but there's nothing wrong with *trying* to design a good language and telling people why your design is good ahead of time.
&gt;Here you're blaming the language for your (team's) own abuse of it. That is absolutely not the case. Languages are tools, and they shape the hand that guides them. This sort of nihilism with respect to language design would imply that it's as easy to write good COBOL as it is to write good Python. &gt;Monads and Typeclasses on their own aren't going to impress anybody unless they're better for solving a real problem. Monads *do* solve a real problem, namely separation of pure and impure code. &gt;I also want to point out that your attitude seems extremely poor. Your manager "doesn't give a fuck/shit", referring to Python "code monkeys", Haskell is "real programming" while Python isn't, etc. Be aware that your attitude will (and has) come across at work and your manager will notice your disdain for the work you're doing and for him and the rest of your "python code monkey" team. Not really. Programming is full of anti-intellectual, recondite people that are completely uninterested in improving. I don't know what OP's manager is like in real life, but it would not shock me if OP's manager is in that category.
I agree, but I still like monadic IO in Haskell.
&gt; But the more disappointingly common resistance I see is that people are shockingly unwilling to learn something new once they've started their career. &gt; I'm not sure I can continue working in a industry where this mentality is so pervasive. It's embarrassing. Same here. It's honestly puzzling the extent to which people insist on using unwieldy and buggy tools. Why would you *not* want to spend less time debugging? Why do you insist on calling everyone who can't write bug-free Python "stupid" rather than just deferring your opinion until/when you know Haskell?
&gt; The point is that even though Python doesn't explicitly enforce the rules that Haskell does, you can write excellent, working code in Python. But why would you want to write code following the same rules in a language that doesn't enforce them? That's just imposing more overhead. I'd guess you know more Python than me, but I can confidently say: * It's harder to refactor Python, consequently * It's harder to write good tests in Python * It's impossible to get anything like Monadic IO in Python. 
&gt; Don't abuse the type system ("Some functions return one kind of type when used in one fashion... Some inputs to functions even work the same way."). Seriously. I saw this and cringed. For the last couple years no I've been programming Python full time, and *no* one I work with would write a function that returns two different types (with the exception of functions that return a value or `None`, which I think of as returning a `Maybe`). This is just bad programming. The problem is not the language.
&gt; Haskell is the opposite of Python and also has drawbacks. Some or many Haskell libraries are unnecesarily complex, because they are done with a research mindset. I actually found the opposite. The only place I've seen Python work is science labs. Writing anything big or writing a library in Python is painful. Writing something that works once on your computer (in order to get a paper published) is surprisingly pleasant. &gt;Haskell publish a dozen libraries for the same purpose each year, and everyone is supposed that must adopt to be in the crest of the wave. Some of them are from the same author. This is good for hobbyists, but not at all for production. I think this is a sign of a vibrant, fast-moving ecosystem. Which has its upsides. &gt;Most of this is being addressed by FPcomplete but the overhearing noise produced by hobbyists and academia has given Haskell the reputation that it has and this is very difficult to change. This is most definitely not true. Our codebase uses both `lens` and `recursion-schemes`. Both are certainly academic. &gt;In the haskell community, nothing is fixed. Not even the most basic things. This is simply false.
If we stopped inventing better things, none of our code would ever be obsolete! I like this plan.
&gt; I'm often kinda surprised when people say they find it hard to get it correct. I have very little trouble writing Python code that works. I don't think it's as easy to say as you imply. When the language/tooling don't find the errors automatically, some invariably slip through the cracks. It's kind of meaningless to compare Haskell that probably has fewer bugs to Python that probably has more bugs, because they likely aren't the "same" program.
I feel as if this post really misses /u/Keith's points. He's not arguing language doesn't matter, or that Monads are useless, or that OP's going to lose his job for having a bad attitude. Frankly, I have trouble seeing how you got that out of his post. He's arguing that the core problem OP is facing can't be fixed by a change in programming language. OPs problem is clearly caused by bad culture and expectations at his work, and a bad attitude on his own part. Haskell will not solve those problems.
&gt; The hard part is that real problems are rarely single units of code on single examples, they require putting units together in ways that work In fact, I'd venture most problems in fact arise in the interaction *between* components in any large project.
&gt; I never know in such situations if the praise for Haskell is really warranted when compared with e.g. Java. Haskell has monadic IO, which is definitely different from Java. As well as functors. &gt;support static typing but might be more employable. Java, C#, TypeScript, Kotlin, Swift, Scala, Things like algebraic data types are not present in all the languages you mention.
&gt; Your Haskell journey should mean you write (and recognise) good Python. Saying "it's possible to write good code in any language!!!" is kind of nihilistic. It *is* easier to write bug-free Haskell, and, moreover, the tools you use as a programmer matter. There's no reason to insist on using bad tools.
&gt; And honestly, Python isn't that bad once you wrap your brain fully around it I would not say Python is bad, but writing *correct* Python is hard once your project grows to a certain size.
I would suggest building a small application with Yesod. Do a `stack new my-project yesod-postgres` to make the starter scaffold, [read the excellent book](https://www.yesodweb.com/book), and have fun. If you're comfy with monads, `do` notation, basic syntax, etc. then this is fine. Ultimately, you want to get as much code written as possible even if it sucks or isn't very Haskelly. Get a feel for what tools solve which problems, and what tools to reach for when you're feeling a specific pain point.
Also, if you find Scotty too big a step, then consider using Aeson to work with json in isolation inside a command line program, or try using whatever db library you are interested in (maybe sqlite-simple or direct-sqlite). If all of those are too frustrating still, write command line programs to do anything with any library that does make sense to you. Alternatively, if you are frequently doing exercises just from Haskell books, that should move you forward. Just don't let yourself off easy and only passively read.
What would be great would be a way to not only make a quantum register, but a quantum circuit and then a quantum "Oracle-like cpu" so that one could implement directly the quantum algorithms as they are presented (QFT, Simon's algorithm, shors factorization, etc) in literature. Perhaps that would enable some semi automatic analysis of complexity? It'd be interesting either way. It would also be interesting if one was also able to measure qubits at certain timepoints along the circuit. Thanks for working on the library! It's a fun exercise. I might tinker around on it sometime soon and see if I can implement some of my QC notes in it.
Yeah it's the intended behavior
https://github.com/ghc-proposals/ghc-proposals/pull/62
&gt; It sounds like your Python code could use some more discipline. [...] Here you're blaming the language for your (team's) own abuse of it. While I agree with you on OP's tone, this is really a lot easier said than done. I've been running a Javascript project for the last several years and we've made an effort to keep it purely functional. Even in Javascript, which probably has more functionally-inspired libraries than most other non-functional languages, you're always fighting against the rest of the ecosystem, and the libraries and abstractions you want just aren't there. More importantly, though, without a compiler checking that the code is pure, if other people that need to work on the project aren't 100% on board with keeping code relatively pure, null free, well abstracted, etc., they won't. Not even maliciously - experienced developers who haven't done pure FP sometimes don't immediately grasp what's discouraged, or the priority of it, or just slip into old habits. In the worst case they can be less-than receptive to what seem to them like arbitrary constraints to little practical end. Unless you're in a position to be the person to review every commit, and you're consistently very thorough, there's always a risk that one day something at the bottom of your call stack will suddenly be mutating its input, making synchronous IO calls, or just calling a concrete method in a function that should only be abstracted over an interface, often just accidentally. Even being in that position, it's at best a daunting time sink to be playing the part of the tools you don't have, and at worst it has a social cost. I've ultimately given in to just picking my battles and moving to better tooling down the road. Discipline is good, but it's not a substitute for not needing it. Tools serve a social purpose - they move the conversation from "do _you_ really need to be blocking _my_ commit" to "are _we_ happy with this tool". Blaming the person advocating for that discipline as not being disciplined enough seems a bit off the mark.
I think maybe you're mistaking the cause and symptom here. Daily "standups" can be valuable in lots of circumstances. In particular: - If multiple people are working on projects that take substantially different sums of time, but have interdependencies. - if requirements are changing based on the results of ongoing experiments. - if the team has a substantial "remote" component, people working out of office. In the same sense that you can't always apply social fixes to technical problems, you cannot assume better technical capabilities will remove the need for social coordination.
Yeah. They're big words, but small concepts. Write some code, get mad at the compiler, read a couple blog posts, get confused, write more code, have it work (but you still don't know why), poke at it a bit, and eventually it'll come together. I say start small and choose something with solid docs, like servant. The less the library does for you, the easier it will be to pick up.
In some ways I prefer using WAI directly to using Yesod.
To reinforce what others have said, let me para-rephrase what you said in terms of learning to drive a car instead of learning to use a web framework: &gt; I want to learn to drive a car, so I've been reading about tires, brakes, transmissions, AM/FM stereo decks, power windows, windshield washers, physics, meterology (since I will mostly be driving outside), the art of persuasion (for dealing with car salesman), criminology (in case I get a ticket) etc. &gt; When will I be able to drive a car?
I'd be opposed to something like this, as it appears to increase the ambiguity of the `=` operator. `let ... =` gives a clear signal to both humans _and_ machines (e.g. parsers for tooling) that you have a pure binding vs a monadic binding. What you're proposing appears to increase the complexity involved in quickly parsing out the intent of a do-block for _extremely_ little practical gain. I just can't imagine that the amount of work and breaking change this would introduce being with the minor and - in my opinion - dubious syntactic improvement it represents.
Well, that's taking what I'm saying to a kind of extreme. But of course tools are important. I guess I'm suggesting he could make peace with working with Python and his Haskell life doesn't have to be an entirely separate, self contained thing. But the opposite extreme would be suggesting he quit and I'm definitely not doing that.
ooh, haskell-ide-engine is already a usable LSP server? nice
&gt; Blaming the person advocating for that discipline as not being disciplined enough seems a bit off the mark. Fair points throughout, and I appreciate your hard-fought experience trying to be disciplined with Javascript of all languages, but this last bit is unfair. Point is, people are fully capable of writing good code in any reasonable language, and there's a middle ground of "let's improve our coding practices in our existing language" before "let's all switch to Haskell" (which of course will come with its own share of difficulties).
How does it increase the ambiguity of the `=` operator? There is exactly one way for an `=` operator to be parsed, and that is as a pure binding, and there is exactly one way for an `&lt;-` operator to be parsed, and that is as a monadic binding. I feel like I'm missing something, as what you are saying seems totally wrong to me. Is it the fact that I used `pure $` when describing the desugaring? In hindsight I should have said `foo = bar` desugars to `let foo = bar` as the end result is the same. Modulo deciding whether or not `foo = 1 : foo` should bind recursively or look above for a `foo`. The only difference between this `=` and the typical `=` you see around is that it wouldn't have things below it in scope, but that is pretty intuitive seeing as non monadfix do blocks generally represent a linear sequence of actions. And increasing readability of many code bases is not "extremely little practical gain", I would definitely find noticeable benefit from this change. Readability is crucial to making Haskell successful. Also this is definitely not at all a breaking change, ok I feel like you definitely don't understand the proposal. You can still use `let` if you want.
What do you mean by caching an ASync? Caching in memory? Or persisting on disk? - If it's the former a simple `Data.Map.Map Key (ASync a)` might do, wouldn't it? - If it is the latter, would you want to be able to resume execution of an in-flight `ASync` in case of premature program shutdown? Or only really store the values for the fully completed `ASync`s?
Could you write down the code of the boiler plate you would like to reduce ? I might have some TH code which helps with it (which I use myself to complex CSV validation)
I mean caching in Memory, with expiry date and so on (purging if too many etc ..) Map Key (Asinc a) would do in it, but it needs to be thread safe, accessible from different call of the same function (ie mutable etc ...), so unfortunately it's not just a Map. I probably can do it myself, but I don't want to reinvent the wheel.
I don't think that I like the idea. I'd be happy to have it behind a language extension and see how it plays out though. It seems to me like this makes it much harder to provide reasonable type and syntax errors. Haskell already has a pretty bad problem there and I'd hate to make it worse to avoid an indent. 
I'm in favor of the proposal, and I, too, would prefer if it was behind a language extension flag. It has been suggested that it could be called `-XNoRequireLetInDo` in [an earlier discussion](https://www.reddit.com/r/haskellquestions/comments/6jdgn1/why_do_we_need_let_inside_do_xallownolet_continued/djdirzb/?utm_content=permalink&amp;utm_medium=front&amp;utm_source=reddit&amp;utm_name=haskellquestions).
I very much agree. [Here one may find further discussion on the subject](https://www.reddit.com/r/haskellquestions/comments/6jdgn1/why_do_we_need_let_inside_do_xallownolet_continued/).
I watched this ages ago.. you might wanna look around for another position. https://youtu.be/ZR3Jirqk6W8
One solution would be to introduce a new operator, `:=` for example which works inside a do block (and even list comprehension)
I might be wrong but it seems that `&lt;- return $` is actually different from =, in the way it deals with pattern matching failure: the layer will raise and error whilst the former will call fail.
You want the same syntax, but without the word "let"? ....
Agreed, Yesod is probably closest to what you've done before. In theory, if you know MVC, you can put up a simple web app knowing almost nothing about Haskell. If you've already read up on Haskell, you'll be pleasantly surprised about what Haskell type safety can add. But whatever you do, the most important thing is: just start writing code.
Ok sorry, `foo = bar` should desugar to `let foo = bar` now that I think about it.
Can you give an example of a situation where it would be harder to provide a good type / syntax error. Because I definitely disagree with your assumption.
I don't think this increases readability, and I think it introduces ambiguity in what exactly the `=` operator is doing. As far as I am aware, the point of having `let ... =` is that it indicates that something gets desugared to `let ... = ... in`, versus a monadic bind. It's a very clear way of indicating to the user _and_ any tooling that is used by editors/debuggers what is going on.
Yes. Or rather make the word `let` optional. It would have two benefits. One is simply removing a few unnecessary characters. Two is avoiding the whole "needing two indents" issue that currently comes up when using case or guards inside those assignments. 
I'd be disinclined towards `:=` since Haskell's already seen as a bit of an operator zoo and `let ... =` seems to be perfectly adequate. That being said, I'd probably prefer a whole new operator to adding additional behavior to `=`.
Actually, you can easily get past that particular case with `enumFrom 1`. But you're right, there have always been different philosophies about templates, and the hamlet philosophy definitely leans differently than the philosophy you described. I don't think it's unreasonable to call associating numbers to the entries of a list as "programming logic". It's simple programming logic, but programming logic. So template purists would say you should do that in your program. Hamlet isn't religious about it - you definitely can put some logic into your templates well beyond "$if boolean_var" and "$foreach list_var". But that's not the primary style that hamlet is designed for.
Why isn't it just `TVar (Map Key (MVar a))`?
Your comment seems so off-base as to have intentionally misrepresented my comment. &gt; Languages are tools, and they shape the hand that guides them. This sort of nihilism with respect to language design... Where did I espouse "language nihilism"? Literally from my comment: "[Languages] are just tools with different pros and cons" and I pointed out that learning things from Haskell can improve your Python code. &gt; Programming is full of anti-intellectual, recondite people that are completely uninterested in improving. And so is it full of bikeshedders who'd rather argue about programming languages than improve at their craft in a way independent of a particular language.
&gt;Haskell enforces more discipline on you than Python does. It sounds like your Python code could use some more discipline. Don't abuse the type system ("Some functions return one kind of type when used in one fashion... Some inputs to functions even work the same way."). While I agree with you and **my own** python code would never be like this, you can't control other people's code. Python's ecosystem ecosystem is riddled with type abuse. It's even considered a good thing by some people. The python community doesn't seem to cringe about code like: def get_metric(metric=None): if not metric or metric == 2: return euclidean if metric == 1: return manhattan elif isinstance(metric, int): return minkowsky(metric) elif hasattr(metric, "__call__"): return metric There's a function in scikit-learn's code base that, while not exactly like this, is very close to it in spirit. And this is not an isolated case, the whole library is riddled with stuff like that. You read all the time in the the documentation things like (actual excerpt): &gt; **max_features**: int, float, string or None &gt; &gt; The number of features to consider when looking for the best split: &gt;• If int, then consider max_features features at each split &gt;• If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split. &gt;• If “auto”, then max_features=sqrt(n_features). &gt; ... Some parameters expect an int, a string or a callable (with unspecified argument types)!!! Pandas is another library that has a lot of functions and methods where the same argument can receive different types that share no supertype — for example `GroupBy.agg` can be passed a string, a function, a list of strings or functions, and a dict whose keys are the column names (which could be any hashable thing) ans whose values an be functions, strings, lists or other dictionaries. And this is a very popular library, well praised by the community. You could say that I'm pointing to niche libraries, all related with numerical calculations and coded by academics that do applied numerical calculations all the time (a community that's not famous for their code standards), but I only gave those examples because they're fresh in my mind. You can find the same behavior in a lot of different libraries. 
if people don't give a shit then no language will fix that
&gt;* It's impossible to get anything like Monadic IO in Python. I wouldn't say impossible. I would say cumbersome, slow and with a lot more code than would be reasonable. 
I happen to like making the distinction between monadic and pure bindings in a `do` block more clear by using an explicit `let`. But that is just a personal style preference. I don't see any reason not to permit pure bindings without an explicit `let` if people want that, as long as we limit it to syntax that doesn't clash with existing permitted syntax. EDIT: I added the following explicit proposal to the [ghc-proposals PR](https://github.com/ghc-proposals/ghc-proposals/pull/62): &gt; Every consecutive sequence of one or more = bindings in a do block is treated as if it were wrapped in a single let.
 -XImplicitLet
&gt; Point is, people are fully capable of writing good code in any reasonable language, and there's a middle ground of "let's improve our coding practices in our existing language" before "let's all switch to Haskell" precisely! and .. no language will make a team magically give a shit, if the python is poo, odds are the haskell will be too
You removed 3 letters (let), introduced behind the scene magic (&lt;- pure ..), lost recursive bindings, and it is somehow more readable? There are a lot of things that can be fixed in haskell. "Let in do" is not even in the first thousand. 
Thanks for the examples. Because Python doesn't do function overloading based on type, there is often some type checking done within functions. It's somewhat "necessary" in Python as the alternative is to just have an explosion of function names for different argument types (which is the approach languages like Eiffel, Smalltalk, and Obj C take). Pandas is actually a great example here, because [it'll give you a `DataFrame` back if you give it a choice of a whole bunch of valid things](https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe). It's a choice between saying `pd.DataFrame` or `pd.DataFrame_from_list_of_dicts` and `pd.DataFrame_from_dict_of_series`, and so on. (Actually looking at the docs they do have some of those, `pd.DataFrame.from_records`, `pd.DataFrame.from_items`, etc.) Relevant point here is that it'll always give you a `DataFrame` back. I'd argue that a consistent return type is the more important thing. For example (with the caveat of not being sure what `minkowsky(metric)` returns), `get_metric` is also consistent in its return value (a callable metric). (Obviously there's a problem there with magic constants...) So, this type of "overloading" with manual type checking, for better or for worse, falls out of the ergonomics of the language. Of course it can be used better or worse, hence the need for some discipline :)
All instances of Life would be part of the world state so IO should be fine.
Ya gotta come over and see muh sin-bin sometime. 
It might work Indeed. Is it safe to mix TVar and MVar ? Also you still need and extra lock to avoid 2 threads to computes the same value at the same time. Basically checking if the key exists and updating it should be atomic : exactly the kind of trap you don't fall into when using a battle tested package ...
There's nothing wrong, but I don't believe that anything other than experience can prove: &gt; That's what the software industry *really* need.
I have since adjusted my stance and think `foo = bar` should simply desugar to `let foo = bar`. No ambiguity there, `=` always means `=`.
I changed my stance on the `pure` thing. I would simply have `foo = bar` desugar to `let foo = bar`. Which is definitely not magical. And its less the 3 letters, and more the avoiding the double indentation all over the place.
Fantastic, do you think the proposal will be accepted?
unsafePerformIO
That's why it's an `MVar` inside. You use the atomicity of `STM` to make sure that only one person at a time has access to change the map, so that if the key already exists, you know someone has started the work. The `MVar` is placed when the work is started, and filled when the work is done.
Is there more to programming than life?
Awesome. I'm going to be implementing a unification algorithm in Haskell in the near future, and this is exactly what I've been looking for. Much appreciated. :)
On the meta, it's nice to see someone expressing their frustration with the stupidity of people, as a diversion from the typical stream of intriguing albeit emotionally detached and borderline academic topics. It's... humanizing. Haskell needs more of that. And yes, your manager IS an idiot, so don't fucking doubt your frustrations with his stubbornness, or you're just as much of an idiot for buying into his BS.
I personally really do not like having to repeatedly explain to beginners the difference between using `let` in a `do` block and using `let..in` expression generally. I really wish the concepts had no syntactic overlap, so I would not have to have that conversation again.
 Is this the RealWorld? Is this just fantasy? Caught in a monad, No escape to reality Open your eyes Look at the call stack and see I'm just a program, I need no sympathy Because I'm easy start, easy end A little fast, little slow Any bit of data, doesn't really matter to me, to me OS, an empty list, passed to Prelude head, pattern matched it, now I'm dead OS, exec had just been run, But now I've gone and thrown it all away OS, ooo Didn't mean to make you cry If it's not cleaned up by this time tomorrow Carry on, carry on, as if nothing really matters Too late, my time has come Shivers down my list spine Leaf's aching all the time Goodbye everybody I've got to go Gotta leave you all behind and eval my exit code OS, ooo (anyway the wind blows) I don't want to TERM I sometimes wish I'd never been exec'd at all I see a little type, describing a prototype, Scaramouch, scaramouch will you pass the static type check Imperative design, very very frightening me Haskell Curry, Haskell Curry, Haskell Curry, Haskell Curry, Haskell Curry Figaro - magnifico But I'm just a program and nobody loves me He's just a program from a poor philosophy Spare him his life from OO monstrosity Easy come easy Go will you let me Go Bismillah! No we will not let you Go - let him Go Bismillah! We will not let you Go - let him Go Bismillah! We will not let you Go let me Go Will not let you Go let me Go (never) Never let you Go let me Go Never let me Go ooo No, no, no, no, no, no, no Oh mama mia, mama mia, mama mia let me Go Ed Kmett has a lens type put aside for me, For me For me So you think you can TERM me and spit in my eye So you think you can run me and let CPUs fry, Oh baby can't do this to me baby Just gotta get out just gotta get right outta here Ooh yeah, ooh yeah Nothing really matters (Maybe u) can see Nothing really matters; nothing really matters to me Programmers, I beg you to think twice before putting your precious, precious values in `IO`.
I have been in a similar position many times and I have never felt like I understand what libraries were doing. However, after finishing the Haskell book (OK i'm actually going through IO.. not quite finished yet) I have finally been in a position to learn by doing practical coding in Haskell. It's still painful as documentation tends not be great in the Haskell ecosystem but doable. And yes it is greatly satisfying as you do this and get confident. I would strongly recommend finishing the book, at least all the heavily theory focused chapters. After reader that would be State, Composing Types, Monad Transformers, Non strictness and IO. I've been skipping most of the more practical exercises and I'm doing real stuff instead. Monad transformers are really common in libraries and it's better to learn the theory from the book rather than wing it whilst writing real code IMO.
I proposed this kind of setup on a bytestring PR, but unfortunately it never happened. Besides requiring a long time to build, it sounds like a good idea to me. https://github.com/haskell/bytestring/pull/73
Agreed on transformers. Maybe someone should write a thin wrapper over wai that allows very simple web programming, that doesn't use a transformer stack, purely for people very new to Haskell to use.
We could give this test a name, the "quickcheck test" or "parsec test". Take a tech lead or dev and present them a very low risk, isolated proposal for using the quickcheck or parsec clone in language X. Observe reaction.
I can confirm this experience. I've been developing in TypeScript for the last few months and in the most recent code base that is TypeScript only, I've seen run-time errors twice.
Quite likely. Implicits are ... pretty weird. I don't personally think they really fit well into the overall design of Haskell. They live in the constraint system, but they're not coherent. In order to prevent their incoherence from infecting other parts of the system, their use is restricted in ways that feel unnatural. And the fact that they're based on *names* seems quite anti-modular. The `reflection` package, on the other hand (as long as you stay away from `give` and view `reifyTypeable` with a healthy dose of suspicion) is extremely well-behaved and fits nicely into the rest of the language.
That works when you update a key but not when you are creating it. If two thread s try to create the same key at the same time there is no MVar yet.
I have thought really hard about this and my experiences trying some of these options in the past. Consider (4) and (5), in particular. (5) needs a ton of marketing, or you need to have to have a strong existing network of people, or you need to know how to pitch and land capital. Unless you go the capital route, you will be alone a lot as well, so you will not be learning Haskell very fast. You will not have a budget to hire a senior Haskell person for quite a while. You may have enough money for Haskell consultants or training to fill the knowledge gap. (4) Another Python job sounds reasonable. Many people might wonder if they should go for the Haskell job. If someone has spent some time studying Haskell at night, they might have a shot at some shop that is interested in taking on someone who is enthusiastic and has shown some initiative learning Haskell basics. The question is whether a developer should leave the mainstream and take such a position. At this point in time, I think one should consider the following when deciding whether to chase after a Haskell job in 2017: 1. Most software shops are managed poorly, often more focused on time estimates than building an environment amenable to high throughput and few bottlenecks. Haskell shops might be better than average in management, but they still will be subject to the general pattern. What happens if you pick a dud? Would you be willing to ride it out to stay in Haskell and build enough experience to be able to land another Haskell job? You most likely will lose most of the mobility you are used to with mainstream languages for the first few years. If you aren't willing to ride out a poorly managed Haskell shop, you should stay sharp in one other language ecosystem and try to use it at your Haskell job. 2. Where do you stand with respect to other developers you know? Do you find yourself particularly fast or bright or middle of the pack? This will also affect how you can ride out picking a Haskell dud. If you are very bright, you can probably find another Haskell job quickly, even with light experience. Otherwise, you will meet some difficulty switching Haskell jobs, possibly having to compete hard for remote positions aimed at the best or senior devs, even after you have gained a few years experience. 3. How important is it that you work on very pressing or big impact software? There aren't many Uber's implemented in Haskell (for now). There are tons of awesome things about working in Haskell as well, but it is good to keep some balance in mind when considering a bigger move like taking a Haskell leap.
&gt; I actually found the opposite. The only place I've seen Python work is science labs science and computer science define two different sets &gt; I think this is a sign of a vibrant, fast-moving ecosystem. Which has its upsides. And its downsides. Upsides are constantly mentioned. For the subject of the post, the downsides also count. &gt; This is most definitely not true. Our codebase uses both lens and recursion-schemes. Both are certainly academic. What is not true? That haskell has not a certain reputation? ;) &gt; This is simply false. I bet that for the average programmer the sequencing of statements is something basic and the discussion among haskellers about either to concatenate statements with transformers or a dozen effect libraries might be a evident sign that not even the most basic is fixed.
I don't understand this "two indents" business. In your examples, both the `let` version and the non-`let` have the following block indented to the `=` sign. That's seems reasonable. Are you calling the `let` one "double" because it took more spaces to reach the `=` sign in that case? It doesn't seem "double" to me: for an identifier of length n you'd need n+2 spaces without the `let` and n+5 with it.
not saying those languages are as good as Haskell, just maybe better than python.
No, when you're creating a key, you use an `STM` transaction to insert an empty `MVar`. That way anyone else trying to create the same key will just end up with the same `MVar` that you've inserted. Then all threads waiting for that key just wait for the `MVar` to be filled.
Maybe I misunderstand STM but if two threads are try to create a new key at the same time, I understand that the Mao will be updated only once, however there is nothing to stop the second thread to waste time computing the value (which I believe put some load and the system and therefore reduce the overall performance which defeats the object of having a cache)
All IO is sandboxed within a pure state, since the space-time equations that describe the universe are pure. (it should be pure since it describes everything and there is nothing outside) 
`:=` is more appropriate as a substitute of "&lt;-"
I mean the reason I am calling it double because you need literally twice as many spaces, it goes from 4 to 8. And since I use 4 spaces for indentation it goes from 1 layer of indentation to 2 layers of indentation. This is particularly notable if you need a `do` block within either of the case statements, especially if said do block also has a similar `let` + `case` deal going on.
`:=` was just the first thing which came in mind. There is probably better alternative.
Try creating a list of increasingly complex programs and go through them one by one. For example, create websites that: - returns the text "hello world" - returns "hello world" from `/hello`, "hi world" from `/hi`, and "not found" from anything else - has the routes `/&lt;op&gt;/&lt;n&gt;/&lt;m&gt;` where &lt;op&gt; is one of +,-,*,/, and &lt;n&gt; and &lt;m&gt; are integers, return proper errors when encountering problems - use lucid or something similar to create html pages and return them instead of returning text - serve markdown files, converting them to html first (use a library for this!) - serve files And so on and so on. This can be done with any framework, but scotty is fairly simplistic so i'd suggest atarting with that. You can use [this tutorial](http://adit.io/posts/2013-04-15-making-a-website-with-haskell.html) for scotty to get started. Also use stack instead of cabal when you are getting started. glhf.
Oh, so you don't line up the next lines with the `=`? You always use 4 spaces no matter how long the identifier is? I guess there are different indentation styles.
#yolo
Yeah, otherwise indentation stacks up pretty darn quickly with longer identifiers. 
&gt; there is nothing outside That's what the programmers of our universe want you to know...
This is really cool, and definitely going on my code reading list in case I need it in a compiler project. I think it’s worth pointing out that you don’t generally *need* higher-order unification if your type system has a few reasonable restrictions, like forbidding type-level lambdas. That’s one of the reasons type synonyms in Haskell have to be fully applied, so we can abstract over type constructors (higher-kinded types) with simple, decidable unification.
I'm guessing not, but does [unsafeUseAsCString](https://hackage.haskell.org/package/bytestring-0.10.8.2/docs/Data-ByteString-Unsafe.html#v:unsafeUseAsCString) do what you want by any chance?
Yes, I am using *unsafeUseAsCString()* to get addresses of the Bytestring data, but this function itself does not guarantee anything (and it has *unsafe* name).
Can we please have this yesterday?
[Alas, I am but a poor college student.](https://i.imgur.com/f0Iu0xE_d.jpg?maxwidth=640&amp;shape=thumb&amp;fidelity=high)
That figures, I'm not surprised about the reluctance of a certain group of individuals to accept Stack (NIH or something else?) and support its users.
&gt; Nix-style local builds solve the package db problem better than Stack IMO. I'll bite... can you explain in what way nix-style builds are supposedly better than Stack? I'm sure whatever minor benefit they may have can be easily accomplished in Stack as well (paging /u/snoyberg).
In my opinion, as soon as you have type abstraction like in System F, pattern unification with contextual metavars (used in Agda, Coq) is both the fastest and simplest solution. The reason is that you need to track scoping of type variables, and contextual metas are far better than Skolem variables (used in GHC, Purescript) for this purpose.
Well, without copying, you necessarily have to pass a pointer into your heap to the ~~dark~~ C side, which can do absolutely anything. So it has no chance of being safe. Also, if your C code is expecting a null-terminating C string and the ByteString is not, things will break, hence the *unsafe* name. There is no free meal, but if you know what you're doing, have strong performance constraints and can trust the C part, it's a reasonable choice.
The C part uses passed references for read only, it does not somehow change them or pass further. And I use *CStringLen* and special size-aware kinds of strings on the C side. So it must be safe from this perspective. But during these references life-time on the C side, the Haskell RTS may *garbage collect* the underlying ByteString. It means that 1. In simplest case, the ByteString will be just deleted because nothing on the Haskell part references it, and so the C-side references will get invalid. 2. I could pass *StablePtr* to the ByteString in the C code, to guarantee that it won' be *garbage collected*. However, in this case the garbage collector may *move* the ByteString to other memory addresses, and the C-side references will get invalid again, though the original ByteString is still alive but relocated.
Are you by any chance coming from a language which used parenthesis for function calls?
If you're not past the monad hurdle yet, I think you'll have a hard time. But don't believe you're done after monads because after that, it's monad transformers (even the simplest scotty app uses monad transformers). Getting past these was hard for me. I'm glad I didn't give up. Keep on fighting. Your efforts will be rewarded.
Indeed, I did not think about GC. Using `StablePtr` looks like the way to go ; it will neither be collected /nor/ moved &gt; A stable pointer is a reference to a Haskell expression that is guaranteed not to be affected by garbage collection, i.e., it will neither be deallocated nor will the value of the stable pointer itself change during garbage collection. All you have to do is to call `freeStablePtr` on it once the C call is done. That should address all your concerns.
Though StablePtr will not be moved *itself*, but the data it points out shall be *definitely* moved after GC. &gt; (ordinary references may be relocated during garbage collection) So I can use *StablePtr* only as technical mean to keep the underlying ByteString alive, but it does not guarantee keeping addresses inside its contents.
No, I think you're reading this wrong. `StablePtr` is not an ordinary reference - the documentation is emphasizing their difference. EDIT: Actually, coming back to the original problem, `unsafeUseAsCString` already solves the problem for you by using `withForeignPtr` internally. &gt; The memory may freed at any point after the subcomputation terminates, so the pointer to the storage must *not* be used after this. So, you just have to make sure that the C code does not keep any reference to the string after returning (if it's doing things asynchronously, you might consider making the call synchronous and handling asynchronicity on the Haskell side).
If you can get away with first-order unification without unduly restricting the type system this is undoubtedly preferable. As a little motivating example I've added a [type checking algorithm](https://github.com/jozefg/higher-order-unification/blob/master/src/Client.hs) for a language with `Type : Type` which does rudimentary type inference. This was the original application I had in mind and adding the restrictions you described would make the system rather restrictive in an unintuitive way.
Perhaps, I am wrong. But what I learned from how Haskell's GC works, it always move *alive* root objects from the old heap to a new heap (see a good article [here](http://blog.ezyang.com/2011/04/how-the-grinch-stole-the-haskell-heap/)). Do *StablePtr* change this standard behavior? In my seeing, StablePtr is just a dynamic pointer that points to the root of the object, that changes its value after changing the GC heap. And that's why casting between simple *Ptr* and *StablePtr* is actually necessary.
Comment for EDIT: I cannot wrap the C code inside *unsafeUseAsCstring* because it's intrinsically asynchronous.
In the particular case of ByteString, the root of the object is all you need, I'd say. The ByteString buffer is not managed by the Haskell GC. data ByteString = PS {-# UNPACK #-} !(ForeignPtr Word8) -- payload {-# UNPACK #-} !Int -- offset {-# UNPACK #-} !Int -- length deriving (Typeable) 
Yeah! Bare metal WAI forever!
Hmm, ByteString is using pinned memory arrays without finalizers. Function [create](https://hackage.haskell.org/package/bytestring-0.10.8.2/docs/src/Data.ByteString.Internal.html#create) uses [mallocPlainForeignPtrBytes](http://hackage.haskell.org/package/base-4.10.0.0/docs/src/GHC.ForeignPtr.html#mallocPlainForeignPtrBytes). This sort of foreign ptr does not have finalizers and uses *newPinnedByteArray*, and so I am afraid that the memory used there is bound to the GC heap and will as well relocate after change of the heap. If it would use simple *ForeignPtr* with *malloc'ed* data, then I could use it in conjunction with *StablePtr* to guarantee the ByteString aliveness, and this would be the best solution!
&gt; newPinnedByteArray &gt; Create a pinned byte array of the specified size. *The garbage collector is guaranteed not to move it.* I think you're safe :) 
That's cool! Where did you read this?
Straight from [the docs](https://hackage.haskell.org/package/primitive-0.6.2.0/docs/Data-Primitive-ByteArray.html).
Thanks! I missed that. So the solution is: 1. Pass original references to pinned ByteString data via *unsafeUseAsCString s return* along with a *newStablePtr* to original ByteString to the C side. 2. Using these references so long as they needed on the C side. Meanwhile the Haskell's GC may relocate the ByteString itself, but the referenced pinned memory will not move. 3. When the references is no longer needed, I just call a Haskell function foreign export ccall ngxExportReleaseHeldData :: Ptr () -&gt; IO () ngxExportReleaseHeldData :: Ptr () -&gt; IO () ngxExportReleaseHeldData = freeStablePtr . castPtrToStablePtr This will remove the stable ptr that holds the ByteString alive and the next GC phase can finally delete it.
Sounds great ! I think you can use directly: foreign export ccall ngxExportReleaseHeldData :: StablePtr ByteString -&gt; IO () ngxExportReleaseHeldData = freeStablePtr 
I'm too lazy. I'll put this poem in an IORef for when I have time to read it later.
Yes, sure, I used *poke* to write it to the C pointer as value, but as soon as *StablePtr* is *Storable* too, I can *poke* it directly, and pass back to the Haskell cleanup function directly too.
My suggestion is that "computing the value" ought to be reduced to "finding or inserting an `MVar` in the map." Once you've used `STM` to agree upon an `MVar` to represent the actual computation, only the thread that actually inserted it begins doing the hard work to compute the value to fill the `MVar` with.
I'm (ab)using GHC together with a custom prelude, to teach a blend of functional programming and mathematics to children. It's explicitly a goal to use standard mathematical notation, which means uncurried functions, and parentheses for function application.
Well, I'm not on the committee :). The committee process is described in the [ghc-proposals readme](https://github.com/ghc-proposals/ghc-proposals). My proposal is a significant change to the original PR of /u/Iceland_jack. So it's up to /u/Iceland_jack whether to modify the PR that way or stick with the original. 
I’m not entirely sure I understand your requirements, but I maintain [lrucaching](https://hackage.haskell.org/package/lrucaching) which is relatively simple at the cost of requiring you to be in `IO` but it seems like in your case that’s not a problem.
If I may put here a tiny small criticism at the community, sometimes I think a portion of the blame for this feeling rests on the style of documentation people make for their libraries. Sometimes there's only a hackage page, with types and the barest of descriptions, if any. And this is true even for some popular libraries. The user is tasked with finding out which functions are actually the most important, what are the "entry points", or what does the names of the functions mean. I realize documentation is a boring, slow, time consuming and expensive job. Also, it seems to me that there's no standard place to put documentation on (when we are talking about docs that are not only hackage style API docs). But it goes a long way on helping people use a library or framework when you have an actual manual, explaining the concepts,what are expected functionalities, some basic examples and patterns to scale from there. Python's community have places like [readthedocs.org](https://readthedocs.org) where people share docs on python libraries. One main difference with respect to hackage is that the API docs are NOT the most important feature of each docs, but the actual manuals for the libraries. I think something like that would be very helpful. Of course for Haskell, the documentation of the types and API is a lot more critical than for python. But it is not all. There should be some more Textual description of the concepts that lead to that design and how to combine the pieces and use them. As an example, the [Flask](http://flask.pocoo.org/docs/0.12/) web framework has one of the best documentations I know of. I realize that a nicely type API would reduce this docs to probably a third of its current length + simple API docs. But it wouldn't be *just* API docs. A community effort to build better docs to commonly used libraries would be very nice, and I would gladly help with that if there's traction.
Lazy ByteStrings are represented essentially as a lazy list of strict bytestring chunks, so its direct representation in C is not what you'd expect. Almost always you instead want strict Bytestrings or explicitly streamed (ex. using `pipes` or `conduit`). In that case, you can allocate your bytestring in C then use [`unsafePackMallocCStringLen`](https://hackage.haskell.org/package/bytestring-0.10.8.2/docs/Data-ByteString-Unsafe.html#v:unsafePackMallocCStringLen) which is constant time and will add a `free` finalizer but won't move it around in the GC. Alternatively, you can allocate the `ByteString` in Haskell using an unboxed null-terminated string literal, like `"hello"# :: Addr#`. An `Addr#` is just a pointer outside the GC and can be passed directly to C here as a `char[]`, or converted to a `ByteString` in constant time with [`unsafePackAddressLen`](https://hackage.haskell.org/package/bytestring-0.10.8.2/docs/Data-ByteString-Unsafe.html#v:unsafePackAddressLen) These require that the C doesn't modify the underlying memory to remain safe.
Yes, I am aware about differences between lazy and strict bytestrings. I am using lazy bytestrings as a list of buffers after *foldlChunks* on *unsafeUseAsCStringLen*. And yes, I do not modify them in C code. I only want that buffers stay valid so long as I need them. As far as I use StablePtr to the original ByteString and the original buffers are not relocated after GC (because they use pinned memory arrays, as it was [noted](https://www.reddit.com/r/haskell/comments/6rxc4i/is_there_any_reliable_way_to_pass_bytestring/dl8imuu/)), it seems to be Ok.
Wouldn't it be enough that there aren't causal relations between possible worlds to retain purity?
Cool, but I could not imagine writing directly in that style :/ If it could transpire hamlet or something to it, then I would be on board though!
&gt; It describes everything Quantum mechanics? That can't be pure, you've got some very nice random number generators in there :)
&gt; When the language/tooling don't find the errors automatically, some invariably slip through the cracks It's not inevitable, though. With proper test coverage you can be very certain that your code is bug free. I'm not saying it's not more work than writing the same functionality in Haskell, but it's doable. Indeed, there's a plenty of Python code in the world that works very reliably.
I'm open for suggestions for the syntax. Obviously, there could be a TH based syntax shim in another package.
FWIW, I've seen two downsides to the nix-style local builds: * Lack of support for older GHCs, which would force two different codepaths depending on the GHC version being used * Last I checked, there was no clear answer to how to make the script/ghci case work correctly. In Stack's world, it's easy: your database stack always guarantees a single canonical installed package per name. With nix-style databases, this guarantee disappears There are certainly nice aspects to nix builds, especially from an elegance standpoint (for some subjective definition of elegance). But I don't see enough concrete benefits to outweigh these two costs, which are relatively steep IMO.
*accursedUnutterablePerformIO, then
It is, with the Everett interpretation. Randomness is in the eye of the beholder.
[HacFreiburg2017](https://wiki.haskell.org/HacFreiburg2017) Project. Follow-up to /u/Profpatsch_'s [request](https://www.reddit.com/r/haskell/comments/6b2krt/can_somebody_please_document_monadbasecontrol/) a few months ago :-)
Another, shorter tutorial: http://newartisans.com/2013/09/using-monad-control-with-monad-transformers/
[removed]
Just a fun weekend project I whipped up; probably lacking in efficiency and elegance, but I'm happy with how it turned out. About half the code just sets up data structures, another quarter of it is defining the game-loop and starting board, leaving the actual implementation around ~25 lines. You can define your own rules using Comonad `extend` to look at neighbours of your current cell; all made more elegant by using Representable's `index`. Indices 'wrap around' the edges. Ideally wouldn't hard-code board size, but w/e Here's a simple board description: start :: Grid Bool start = mkGrid $ glider `at` (0, 0) ++ blinker `at` (5, 10) ++ beacon `at` (15, 5) And here's the definition for `blinker`: blinker = [(0, 0), (1, 0), (2, 0)] 
It would be.
Sorry to be the beginner here, but what is the difference? :O I've been using it all along thinking it was the same, just scoped by the surrounding `do` block instead of `in`.
I wrote a short series of articles a few years back on the topic ot automata in Haskell using some code I banged out with ReinH to get started: https://www.schoolofhaskell.com/user/edwardk/cellular-automata It builds up the notion of automata via the store comonad, and then uses additional comonads to deal with things like the folds it has to use for producing CRCs when performing image generation as it produces its own pngs.
Consider this from the other side -- Haskell starts out with a huge syntax from the beginning and GHC language extensions make it even more complicated. This kills your editor tooling people. We don't need more syntax sugar extensions, instead we need to slow down, decide which ones that we have we want to keep, and fold them into the main language in the next report. After that's done we can consider entering a new experimental phase if we want.
Nice! I think that instead of defining Grid you could use the Store type from the adjunctions package. 
no... coinductive computations dont retain paths in memory, the semantics of a field selection is like function application
I see, however I'm not familiar with TVAR and STM. Could you show me how one would "finding or inserting an MVar" atomically using STM ?
I think this package would still be easier to understand without the type aliases.
 type Key = Request type Cache k v = TVar (Map k (MVar v)) fetchCached :: Key -&gt; Cache Key Response -&gt; IO Response fetchCached req cache = do var &lt;- newEmptyMVar join $ atomically $ do map &lt;- readTVar cache case Map.lookup req map of Just var' -&gt; return $ readMVar var' Nothing -&gt; do writeTVar cache (Map.insert req var map) return $ do res &lt;- actualFetch req putMVar var res return res This uses STM to make sure you don't do the work when someone else already started it, and it uses `MVar`s to represent started and completed states.
I've thought about inlining them, too, but decided to leave them. I think they have two points in favor: * The type signatures of `liftWith` and `liftBaseWith` would become even more difficult to read, and more importantly, * they give you a place where you can put documentation and explain what they correspond to ;-)
Why stop experimenting? I'm all for standardizing some existing extensions, but the two actions are not dependant.
That's interesting. Does it works with concurrent thread ? Does cached items have an expiry date ?
Concurrent access is safe but there is no expiration date on items. As the package name suggest they're evicted based on an LRU scheme.
Why does treating sum as pure break everything? Sure it allocates and deallocates, but only internally and the answer for a given input should never change. What am I missing?
I need expiration date
Ok, thank you very much !
This is wonderful documentation, Haskell needs more of this!
But I never call those functions, I only ever implement them - and then I have to constantly manually unwrap the type alias to work out wtf I'm meant to be writing.
Because we're way overstretched right now and it's a pain for the editor tooling people and for people learning Haskell. Too many extensions in common use at one time is bad for the ecosystem.
Ah, the list monad.
I remembered I should also provide the functor definition for SParse if anyone is trying to compile this: instance Functor (SParse a) where fmap = liftM And you also need the applicative (I'm bad at this!) instance Applicative (SParse a) where pure = return (&lt;*&gt;) = ap 
The list comfirms what I already knew, that Python is the lowest common denominator. As for Haskell for embedded, they are almost certainly talking about the [Atom EDSL](https://hackage.haskell.org/package/atom) which allows you to write Haskell and generate C.
I suspect it's due to laziness. Sum doesn't do any allocation of its own, that's handled by the Haskell code that calls sum. So, the pure sum might not be demanded until the list has been freed. 
You're precluding the existence of [beings who casually travel between different configurations of the universe](http://scifi.wikia.com/wiki/Xeelee). Which is rather sad, considering that Haskell is obviously the Xeelee programming language of choice.
This is too limited and quite frankly too boring of a model. Have you not ever contemplated another possible world, and then used information from that contemplation in this one? Never read any science fiction? Possible worlds *can* have effects on actual worlds. And [they often do](https://www.nasa.gov/topics/technology/features/star_trek.html).
I didn't think of `enumFrom` which solves indeed most of my problems with hamlet. 
Agree. I actually got a job at the host organization of the local Haskell meetup. 
In an ideal world this is a very interesting discussion to have. In the real world we have far too few brain cells in the Haskell world to spend them on something like this. Please spend them writing a data processing library or something.
Good idea! I tried it out and it cleaned up the code a lot, but it slows to a crawl almost immediately; have any ideas? Maybe a space leak somewhere in here? https://github.com/ChrisPenner/conway/blob/store/src/Conway.hs
There's a lot of things that are horrendously difficult and unwieldy about Haskell, and those things actually matter to developers.
I meant this store: https://hackage.haskell.org/package/adjunctions-4.3/docs/Control-Comonad-Representable-Store.html You’ll still need BoundedV, which will memoize the store operations. 
This seems to me like a better guide to understanding `monad-control`: https://www.fpcomplete.com/blog/2017/06/tale-of-two-brackets
Ahh! Great, I'll give that one a go! Still though, why would traditional Store get so slow? Is it continually building up bigger and bigger thunks?
Did you just say that modal realism is *too* mundane? Absolutely amazing.
No, I said that the idea that they have no causal relations to each other is boring. Many worlds is awesome, but not if they're all independent and cut off from each other.
But that essentially means that there's only one universe, even if it's connected in curious ways together. The ordinary universes are merely parts of the whole then. Having multiple possible worlds that are completely unreachable is the much more odd scenario.
&gt; The list comfrims what I already knew, that Python is the lowest common denominator. Lmao, this subreddit I tell ya.
Looking up `Monad` in [Prelude](http://hackage.haskell.org/package/base-4.10.0.0/docs/Prelude.html#t:Monad), and clicking 'Source', we see: m &gt;&gt; k = m &gt;&gt;= \_ -&gt; k (Your tutorial on monads should have told you this too.) Which means if you don't implement `&gt;&gt;` for your instance, it will get this implementation. Now things should be cleared up. --- Oh my god, this is the first time I've noticed that the docs did not say this...
I love Haskell, but I use python at my day job and it's great! Language bashing (rather than constructive criticism) is stupid and bad 
And it's not 'web'.
Right? We use it for web at work and I love it!
TIL about atom for embedded, cool!
I know this definition. I still have no idea how it applies.
Ahh, gotcha, that makes more sense; I was hoping I was misunderstanding something there. Thanks!
Wait, maybe I understand it now. I'll will try to write the calculation down again.
I think the answer would be: abParse = SParse (token 'a') &gt;&gt; SParse (token 'b') = SParse (token 'a') &gt;&gt;= \_ -&gt; SParse (token 'b') = SParse (\st -&gt; concat [ sparse ((\_ -&gt; SParse (token 'b')) x) rest | (x,rest) &lt;- (token 'a') st ]) Parsing "abc" the calculation would go like this sparse abParse "abc" = (\st -&gt; concat [ sparse ((\_ -&gt; SParse (token 'b')) x) rest | (x,rest) &lt;- (token 'a') st ]) "abc" token 'a' "abc" = [('a', "bc")] = concat [ sparse ((\_ -&gt; SParse (token 'b')) x) rest | (x,rest) &lt;- [('a', "bc")]]) sparse ((\_ -&gt; SParse (token 'b')) 'a') "bc" = sparse (SParse (token 'b')) "bc" = token 'b' "bc" = [('b', "c")] = concat [[('b', "c")]] = [('b', "c")]
its a totally reasonable / common confusion. laziness is a *good* optimization of call by name computations when they're pure. But the moment you want to model effects that are implicit, you tend to want an action to happen each time you do it. :) perhaps a better explanation is: induction is the path you can walk to you value, coinduction is the set of all possible paths you can choose to take! In haskell we have inductive types plus a bunch of coinductive stuff thats sensible in the presence of laziness (we can still model any coinductive type, but its going to use functions for the record selection modelling) see my copattern matching example in haskell post (linked in another comment), to maybe better see the distinction I'm not doing a good job of explaining this clearly, but I hope this is enough to get you started :) 
Switched it over without a hitch, thanks!
Seconded
I mean, it's listed as embedded but not web, so I can't really take this seriously. Clearly they are having problems understanding our ecosystem.
`ghc.cabal`?? :-/ Where can one find out more about this (and `cabal-2.0` in general)? 
Looks good to me :)
Expiration can be "easily" -- and crudely -- simulated, by storing each value with an expiration timestamp. If, after reading an existing key in the cache, you find that the value's expiration time has passed, you should invalidate that key, create the value again (bypass the cache), and insert the new value into the cache. I'm not totally sure if that will interact with the LRU semantics in any funny ways, though. In particular, since items are not kicked out when they expire, but rather when you try to access them after they have expired, the cache can still fill up and start kicking off items in an LRU fashion (ignoring expiration). But it might be okay for your needs. Yes. Would be nice to not have to implement these semantics yourself, though... 
 max(min 1 2) 4 That looks weird.
Sure, so don't use curried functions. `max(min(1, 2), 4)` looks pretty normal, and is perfectly good Haskell with the right prelude. But most formatters will rewrite it as `max (min (1, 2), 4)`, spoiling the game.
Thanks! I feel bad asking for it, since I want it for something that's sort of questionably real Haskell in the first place. But at the same time, I could definitely relieve you of the need to change whitespace depending on the next element. I think in the near future, CodeWorld will be using haskell-src-exts to pre-check the source, and will just generate an error if the right-hand side of a function application lacks parentheses. Then my request would just be to always omit the space.
That Store doesn't actually store anything, so to calculate a frame it needs to recalculate all previous frames again. 
I believe [managed](https://hackage.haskell.org/package/managed) also deserves mentioning.
Because its arguments don’t determine the result: The argument is a pointer but the result depends on the values found by dereferencing the pointer. Those values can change throughout the execution of your program while the pointer stays the same which means that the function can’t be pure.
Take a look at the `validate` function at http://lpaste.net/357415 
That's more of a haddock issue then. It should give you the option to unwrap type aliases (eg. on click) or unwrap them automatically (eg. write the binding twice, once with original types and once unwrapped). I agree with fmthoma that it's nice to have the type aliases to have something to attach documentation to. But that doesn't mean that haddock should be rendered exactly the same.
This is pretty incredible in terms of how far you can take type families and newfangled GHC features. There's so little actual (term-level) "code" here. It's wonderful. Soon GHC will need to compile types themselves and implement a type-level interpreter in order to have acceptable compilation times ... or reuse GHCi somehow ... there lies DependentHaskell. Completely shameless plug of a similar library I have been working on, also in its infancy: https://hackage.haskell.org/package/nice-html . Brief comparison: is also faster than blaze; but does markup compiling at runtime; might support 'dynamic' markup/templating a bit better (IMO it is suited to making a react-esque component library in, but I haven't had time to do this [yet?]); does absolutely nothing to validate html; has a monadic interface or two.
As you might have noticed, the term level functions are quite redundant for performance reasons. You could bring the Function.hs to about 100 LOC if you're aiming for elegance. It's kind of sad that the obvious and elegant solution (don't build a list and mappend everything) doesn't work well for all types. Perhaps I'll write an simple specification module with elegance in mind. This would as well increase quickcheckability.
I think you both may have misunderstood what "lowest common denominator" means.
No, it's not a haddock issue, the same happens when I try and implement the functions using holes.
I've been bitten by exactly the same issue a couple of days ago :-) On stackoverflow it was suggested to me to [use the MemoTrie package for memoization](https://stackoverflow.com/a/45507708/474311).
&gt; Daily "standups" can be valuable in lots of circumstances Just curious: do people actually stand up during the standup meetings you've attended?
No, no one is based in San Jose. London &amp; MA. Also we're hiring.
That's exactly the kind of stuff metamorphosis has been designed for. All the TH code is there to generate your boiler plate but as it's still experimental and might I manage this problem slightly differently I might need to add some convenient wrapper around the basic function.ilk give you a full example tonight if you are interested.
&gt; the discussion among haskellers about either to concatenate statements with transformers or a dozen effect libraries might be a evident sign that not even the most basic is fixed. Oh right, so what's the correct fix to this?
"Designed for applications where reliability is important." - at least this part is non-controversial :)
Is metamorphosis available on github?
If you're interested in applying, please fill out [this short form](https://goo.gl/forms/opAVCRy727FIuqvF3).
OK, the obvious question, why don't you use Hydra for that?
Same question here what is this Cabal 2.0 file format ? If there is some work done to improve stack config files, why not use the format of hpack ?
Others know more about this than I do, I'll fill in the details from what I know. Caveat emptor: I may be wrong on some details, at least outside of the Stack and Stackage codebases. There are in some senses _three_ different things that have the name "Cabal" and which get versioned: Cabal the library, cabal-install the build tool, and cabal the file format. It so happens that these things all synchronize on the same version, but that's more confusing than helpful in understanding this. The cabal file format itself is specified by the `cabal-version` field in your .cabal file. You can theoretically have things like "my library requires Cabal-the-library version 1.24, but uses cabal-the-file-format version 1.8." And so on. Anyway, Cabal the library 2.0 made a lot of changes, and a number of these are reflected in cabal-the-file-format. For example, there's a new syntax for version bounds, `^&gt;=`. Older versions of Cabal-the-library—and therefore any build tools that use that version, such as Stack &lt; 1.6 (not yet released) or cabal-install &lt; 2.0 (also not yet released)—wouldn't be able to parse those files. Other such tools include packdeps (both CLI and web versions). Stack has been doing something relatively dumb that no one noticed and didn't really affect anyone: parsing `.cabal` files for packages present in the global package database. This is dumb because it's not really needed: we're just going to use the version in the global database. But it didn't really affect anyone, since who cares if it spends a few extra milliseconds parsing a file? In any event, with the extensible snapshots work I just did, I happened to accidentally get rid of this dumb behavior. Alrighty. GHC itself ships with a library, `ghc`, aka ghc-the-library (funny naming pattern we've got here). This is what libraries like `hint` or tools like `intero` and `ghc-mod` use to get information out of GHC. AFAICT, there was never a release of ghc-the-library to Hackage before GHC 8.2.1. But this time, there _was_ such a release, and that release includes [a cabal file](http://hackage.haskell.org/package/ghc-8.2.1/ghc.cabal) with a `cabal-version: &gt;= 2.0` field, preventing it from being parsed with tools that use Cabal-the-library &lt; 2.0. (From what I can tell, this was used in order to allow the `^&gt;=` syntax.) As a result: Stack 1.5 would try to parse this file, even though it didn't need its info, and fail. The 1.5.1 release includes a simple workaround to just ignore the parse failure for ghc.cabal, and the next major release will (1) include Cabal 2.0 support and (2) use the better logic I mentioned from extensible snapshots. I considered making this request elsewhere, but I'll just put it in here as a wishlist: it would be nice if Hackage enforced a grace period of at least a few months before allowing the new Cabal file format to be uploaded. This would give authors of tooling (e.g., packdeps, Stack, and even cabal-install) time to upgrade to the newest Cabal library and test their changes before things start breaking. Finally: Stack will _immediately_ allow you to build packages using the new Cabal 2.0 library, since it does not use its compiled-in Cabal library at all, instead shelling out to `Setup.hs` in all cases. (AFAIK, this is an architecture difference from cabal-install, but I could be mistaken.) So (perhaps surprisingly), Stack 1.5.1 will allow you to build GHC 8.2.1 packages, using Cabal 2.0, but will _not_ allow you to put `cabal-version: &gt;= 2.0` in your `.cabal` file. Go figure :)
See my sibling comment for the (unfortunately wordy) explanation of what's happening here. Stack remains compatible with the Cabal ecosystem/build system by still relying on the Cabal file format and the Cabal library for performing builds. There's no way for us to make any modifications to what goes on there without breaking compatibility, which is all by design. hpack is a really great idea, and allows for a different syntax and some other niceties (like module globbing). But ultimately it has to convert into the official cabal file format in order for all of this compatibility to work.
So... The `ghc` library was uploaded to Hackage with a `.cabal` file that no current build tool is able to read? Ok. 
Right, in that case things might relatively simple (but no promises :p). For now I'll just point you in the right direction: Responsible for the whitespace are the `appSep` invocations in [this line of expression layouting](https://github.com/lspitzner/brittany/blob/052e9432217a6695e28ff3822aed86e5505a5b31/src/Language/Haskell/Brittany/Internal/Layouters/Expr.hs#L114) and again below (lines 114 and 153). A first test would be to simply remove the `appSep` and see where this gets you/whether/how much breaks, for which inputs. Just for context: The `docAlt` there lists a number of possible layouts for (function) application (with multiple arguments in the first case, a single argument in the second), and the rest of the brittany mechanism chooses appropriately from those choices. Feel free to ask if you run into any issues / if the code is too hard to comprehend.
It's not on hackage yet but it's on github [there](https://github.com/maxigit/Metamorphosis). You can also have a look at the examples (there is a link at the end of the README).
&gt; So... The ghc library was uploaded to Hackage with a .cabal file that no current build tool is able to read? ... and, in fairness, no current build tool needs to read.
What about the Rule of Two?
if you compile your build tool of choice against `Cabal-2.0` it will be able to read `cabal-version: 2.0` cabal files. --- &gt; It would be nice if Hackage enforced a grace period of at least a few months before allowing the new Cabal file format to be uploaded. This would give authors of tooling (e.g., packdeps, Stack, and even cabal-install) time to upgrade to the newest Cabal library and test their changes before things start breaking. No. Tooling should simply ignore `.cabal` files with `cabal-version: &gt;= not-known` (maybe warn/error if specifically asked to read that file). By the same argument we should disallow publishing packages with e.g. `DerivingStrategies` or `UnboxedSums` language extensions, because `hlint`, `ghc-mod`, `hindent` etc. will break (because `haskell-src-exts`). - Rather, GHC-8.0.2 errors with "I don't know", even it *could* compile the empty module. - `hlint` (I'm not sure if the choice is made already `haskell-src-exts`) takes another route and tries to parse the file even there is an extension it doesn't know about, and fails if it cannot: - being optimistic makes sense for tool like `hlint` though --- $ cat New.hs {-# LANGUAGE DerivingStrategies #-} $ ghci New.hs GHCi, version 8.0.2: http://www.haskell.org/ghc/ :? for help New.hs:1:14: error: Unsupported extension: DerivingStrategies Failed, modules loaded: none. ^D $ hlint New.hs No hints $ cat New2.hs {-# LANGUAGE DerivingStrategies #-} newtype T a = T a deriving Show deriving stock (Eq, Foldable) $ hlint New2.hs New2.hs:4:3: Error: Parse error Found: newtype T a = T a deriving Show &gt; deriving stock (Eq, Foldable) 1 hint --- As a `Cabal` contributor I admit, there is a bug in `Cabal` (and thus `cabal-install` and `stack`) that it tries to parse `.cabal` files with unknown `cabal-version`. It should fail (with distinguishable from "syntax error" -error), and not pretend to be forward-compatible. We learned a hard lesson here. One plan is to require `cabal-version: x.y` to be the first entry in `.cabal` files to allow parser adjustions. (a bit like `LANGUAGE` pragmas modify GHC behaviour starting from the parser).
Please note: I was not saying servant is bad, simply pointing out that if you're generating a client side API based on URIs, that has nothing what so ever to do with REST. I explicitly said later in the thread that if I were writing a REST service in Haskell I would, without a doubt, be using servant for that. I just wouldn't use any client code generation. Also please note that I didn't say there was anything wrong with HTTP-RPC (which is what the servant-client is) but I wish that people would make it clear which they mean because these two architectures are very, very different and have completely different trade offs.
Good book on applying category-theoretic ideas to program design that starts at an undergraduate level. The author wrote it to help his students who were struggling with the overload of pure category theory when studying the Algebra of Programming. 
I would feel differently if there was a way to distinguish "uses newer format" from "invalid parse" but, as you point out, that doesn't exist right now. It's been important historically to have tools error out aggressively on bad parsers because of things like byte-order markers being accepted by Hackage but rejected by Cabal's parser. I'd be opposed to setting a standard of silently (or even verbosely) ignoring invalid parses. Though to be fair, in the case of Stack, you may be correct. Nonetheless, I stand by my general sentiment. The case of new versions of GHC is not comparable: you can distinguish that if desired by looking at bounds, and we don't need something like packdeps to to run against all code available on Hackage. Side note: your points here agitate towards needing a strictly defined markup format for cabal files with an evolving definition of the contents of that format, instead of the current situation where both the markup itself and the contents can change in each version. Has this been considered?
Yea, it's very difficult because REST is such an abstract architecture compared to others. One is meant to define a new type with different semantics for the fields of the type. One could define some kind of "manager" that keeps track of relationships between types and inserts reference links in the final render for a given type, but this would only be valid for what I would call "generic REST" (i.e. completely generic types and generic links between them). For what I could call "Full REST", maybe your state machine idea is the most we could actually do for it but I think the methods wouldn't be "Get", "Put", etc. but rather the different things you can do with that type. Admittedly, what you can most often do with any type is exactly these operations, but the issue is that sometimes there are multiple operations that would be a "Get" or "Post" or whatever. But perhaps one could get quite far with such an abstraction.
&gt; Side note: your points here agitate towards needing a strictly defined markup format for cabal files with an evolving definition of the contents of that format, instead of the current situation where both the markup itself and the contents can change in each version. Has this been considered? I'm not sure I understand you. It's considered to require `cabal-version` to be the first field of `.cabal` files. Parsers can then detect the `cabal-version` faster, at least for newly uploaded package versions. Yet we are stuck to scanning of what's already on Hackage. --- Unrelated: Consider adding `stack-version` to `stack.yaml`. E.g. your extensible snapshot change is not-so compat change. At some point you'd like to drop supporting old syntax etc.
&gt; Name errors: If there's an undefined name, raise a runtime exception where the name is used. Has been previously suggested. `-fdefer-out-of-scope-variables` (which is included by `-fdefer-type-errors`) already exists.
Thank you very much! I am currently searching for material which deals with relationships between Haskell and CT for my bachelor thesis, so this surely will come handy.
My point is that, if the markup format itself were standard, you could use the standard markup format parser to get something like a `Map String Value`, and then do `lookup "cabal-version"` on that, regardless of which version of the Cabal file format is being used. For example, to borrow from the reference to stack.yaml, we're going to be able to continue parsing old and new versions of the file format to at least a `Value` since YAML is a standard format, regardless of changes we make inside the stack.yaml file itself. If cabal standardized its current markup, then it would be possible to have a two-step parse that would allow grabbing the cabal-version field. AFAICT, this is quite possible with the way cabal files are specified right now, though I don't know the details of the grammar enough to say with certainty.
Interesting. Seems like it wasn't in 8.0.1 https://downloads.haskell.org/~ghc/8.0.1/docs/html/users_guide/using-warnings.html?highlight=defer#ghc-flag--fdefer-typed-holes but it was in 8.0.2 https://downloads.haskell.org/~ghc/8.0.2/docs/html/users_guide/using-warnings.html?highlight=defer#ghc-flag--fdefer-out-of-scope-variables 
If you extract part of the store it just computes it so you don't get any sharing. If you calculate an iteration you extract each chunk of the previous iteration 9 times. That in turn extracts each chunk two iterations ago 81 times and so on. If you memoize you compute each chunk once and reuses the result.
While the GP definitely overstated the case, I think [s]he was probably reacting to experiences in the past where people are clearly using a terrible language and then claiming "you can write good/bad code in any language!" [1]. Well, yes, but some languages seem to make the worst forms of abuse the easiest way to program. And people who have only been exposed to that language are very unlikely to recognize good vs bad coding practices. [1] At least I view it this way because I've run into this sentiment myself many times in the past. Nonsense like "a good builder never blames his tools!"... well of course he doesn't because he doesn't have a pointy-haired boss who insists on picking the worst possible tool for every job.
So is this an [anti-pattern](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/) or not?
&gt; "Yeah, but those bugs could be caught by tests!" Also: "tests are just code. If code can have tests, bugs can have tests. The compiler's guarantee is stronger".
I don't understand why you would want either. `defer-type-errors` is useful when you don't understand the type error, and you want to debug a running program in order to learn more information about the circumstances under which the type error occurs. If you're using a name which is not in scope or if your code has a syntax error, running the program is not going to teach you any new information; you can only fix this kind of error by looking at the code, not a trace of its execution.
Obvious use case might be editor integration. Then you could get a ghci session and interact with other parts of the code without having to manually typehole or comment out the code you are currently working on.
Great tutorial, thanks for sharing this.
Any chance there would be a recording of this?
I'd be very happy to see more maintainers for the Stack project! Cool idea and hope to see a bunch of people get involved.
Enabling `-fdefer-out-of-scope-variables` allows the compiler to produce more error/warning messages, and would allow modules depending on it to compile. And I would occasionly remove a function because it makes no more sense. It would not only be useful to know where it could be used at runtime (obviously), but also to know that the parts where I got rid of the usages of that functions now work as expected. And `-fdefer-type-errors` does *not* do runtime type checking. It errors out *directly*, so the 'circumstances' are pretty static.
&gt; Because Python doesn't do function overloading based on type, there is often some type checking done within functions. It's somewhat "necessary" in Python as the alternative is to just have an explosion of function names for different argument types There's a nice function in the stdlib that helps in keeping this kind of code clean and separate dispatching from the actual logic https://docs.python.org/3/library/functools.html#functools.singledispatch But obviously, even if the team wholesale adopted it, it would still be of limited help
Thanks. That's the key question. And this is the proposal: [An effect library that composes well](https://www.reddit.com/r/haskell/comments/6mt8i6/do_you_have_to_choose_between_composability_and/)
Yeah, also with strictNullChecks and noImplicitAny, Typescript is pretty sane Just like Kotlin and Swift (and Scala to a lesser extent), all those languages have Algebraic Data Types and null-safety. That's pretty good in my book. Sure, they don't have HKT (except Scala) or a good way to track effects. I obviously prefer Haskell since it helps prevent so many problems, but I'm also working on a small TypeScript project at this time. I'll see if it'll disappoint me, but at the moment I'm optimistic
I can imagine that some want to use `defer-type-errors` for larger refactorings. It might enable the possibility to just fix and test one module at a time. 
&gt; I don't understand why you would want either. For two reasons. 1. As [/u/Tarmen says](https://www.reddit.com/r/haskell/comments/6s4rjz/suggestions_for_deferring_errors/dla63ec/), editor integration. I would like my editor to be able to tell me the types of various expressions, even if other parts of my program are broken. 2. It's a psychological trick. I've occasionally heard from people that Haskell "scares" them because the compiler "shouts at them" when they make an error (i.e. it refuses to run their program). Python, for example, on the other hand, is more forgiving, because it will happily run their program even if it contains name errors. This is somehow psychologically less intimidating for some people, and I'd like to help these people.
Don't worry, I didn't take anything badly, I was debating the connection between REST and servant, which in summary is somewhat there, but maybe not enough to justify mentionning "REST" right there on the homepage, indeed.
A warning is *not* going to help (2).
It's useful to have access to the rest of the module in GHCi, even if parts of it don't work. This way I can evaluate expressions and use `:t` or `:i` to figure out what's going on. In practice, I've found this just as useful with out-of-scope variables as I did with type errors, and I would like it for as many other errors as possible. Basically, any time I could comment out some code or replace an identifier with undefined, I would rather the compiler did it for me automatically and gave me a warning. This saves me a round-trip between my code and the REPL and meaningfully improves the cycle time for exploring and fixing problems in my code. I would like this enabled for as many classes of errors as possible, although I understand why it's incredibly difficult to do well for parse errors.
Whether there's a warning or not is irrelevant. It's the running program that's the important thing. (Disclaimer: I'm not a UX expert)
 Susan Eisenbach and Chris Sadler's introduction on page 181 is excellent and still current (mostly). The timeline of FP development on p. 182 and the glossary on p. 186 are worth keeping handy. An article by John Darlington on program transformation - we're still working with unfold/fold. Harrison &amp; Koshnevisan talk about how to program without objects... that's still very current! Also, Pournelle's column, Ciarcia on his Basic Stamp - that's still being sold, I think. The Amiga. And the ads!! Thanks 
I mean this should be fixed in haddock, or haddock changed to handle these cases better than it does currently. We should not have to adapt our code to haddock, it should be the other way around, haddock should render the documentation in the best possible way. If that means haddock has to be modified, so be it.
Type synonyms are also problematic when they show up (or not!) in compiler errors. Quite frequently, the GHC compiler error complains about the expanded type synonym while you you as the author of the code causing the compiler error, conceptually, are thinking in terms of the non-expanded type synonym. Or the other way around. Establishing this relationship between the type synonym and the expanded form can be quite exhausting. This is particularly frustrating when the type synonym is either a curried function or some monad transformer, since partially applying the function or `lift`ing only halfway through the monad transformer will result in terrible error messages that talk neither about the intended type synonym nor about the fully expanded form. 
&gt; Consider adding `stack-version` to `stack.yaml`. E.g. your extensible snapshot change is not-so compat change. At some point you'd like to drop supporting old syntax etc. One aspect GP may not be aware of is that with generic formats you have to take into account two format versions, the version of the data you're encoding but also the markup format itself may be versioned! In the case of YAML, there's the `%YAML 1.1`/`%YAML 1.2` directives which you can place right at the start of the YAML file to declare which spec-version the YAML syntax is following, and consequently which version the YAML parser needs to support. This is quite important if you care about correctness, as failure to ensure YAML parser and `.yaml` file agree on the spec-version can result in silently misinterpreting the data encoding. Just like YAML, we want to have the liberty for future `.cabal` versions to evolve, including changing the lexical structure if needed, and having the `cabal-version` be quickly detectable (e.g. by looking at the first 4096 bytes and with a simple logic that's also easy to implement in C code) w/o requiring a full parser is desirable for various reasons, and there's prior art for that, including YAML.
Basically, Hydra was far more complex than I was looking for. And I don't think it has any GitHub integration, does it?
... and, if you're curious, there's backstory about that: [reinstallable lib:ghc](https://mail.haskell.org/pipermail/ghc-devs/2017-July/014424.html).
Interesting article, but nothing to do with Haskell the language. Makes a good point that many Haskellers and associated hangers on fetishize impenetrable mathematical code. Nice description of building the algorithm. Nothing that couldn't be implemented equally well in Haskell using a mutable vector, say. No idea what the performance would be though. In general, rather too polemic for my tastes. &gt; her majesty Lazy Evaluation No need for the snide remark, thanks. &gt; won’t use your cores “for free” no matter what the FP propaganda keeps saying Not sure any FP propaganda is saying that FP parallelises "for free". In fact I think it's quite well-known now that implicit parallelism performs extremely poorly. &gt; “aggregation is bad, divide and conquer is good”, which basically means: scan and fold and zip etc. are bad for parallelism, map/reduce is good. Right, `scan` and `fold` and `zip` on a linear data structure, like a list, at bad for parallelism! That's because a list is built and consumed sequentially. How about a more balanced data structure that can be built and consumed in parallel?
I try to avoid negative words like "unfortunately", "risky", "dangerous" and "contagious" w.r.t. licensing. In the case of the AGPL: Affero, the FSF, SFLC, etc. chose their wording very carefully to pursue their goals. Likewise, Free Software authors are responsible and capable of choosing licenses for their own work. In this particular case, of (potentially) using an AGPL-licensed code-manipulating component in a networked code-manipulation system, I wouldn't describe AGPL's wording as "very dangerous"; rather I'd say it's "preventing exactly what it was designed to prevent" :)
I quite like the idea for recovering from syntax errors, seems like it ought to be doable at least at a coarse per-top-level definition granularity. If the parser detects an unexpected dedent to the top-level, just treat the current definition as unparseable and start parsing a fresh one. 
~~I think it's likely, but I don't have equipment myself, so I can't guarantee it.~~ Greg Hale will be recording it, and we'll be sure to make it available to the community. Edit: Geoff Moes will also be recording the talk, so there's a pretty good chance that at least one video will be usable!
Have you tried using `-fdefer-type-errors` when teaching Haskell to people? I wouldn’t have expected that this is really helpful for beginners much but I’ve never tried it so I’m definitely interested in learning about the experiences of people who have tried it.
The point of these sorts of functions as despised by the author is not to model a domain. The point is to quickly and simply model a naive implementation of an algorithm that will get decent performance so that you can spend time focusing on the problem you're actually trying to model. If you find a problem in performance, you can go back and replace the algorithm with something more finely tuned later. If the programmer failed to understand that this particular piece of the code needed to support a full scale simulation, that's a fundamental design and architecture issue of the sort that would cause trouble for any paradigm. I do agree that our community sometimes puts too much emphasis on 'elegant' functions, but it's not like OO is the only way to model a problem domain. 
I enjoyed this, and in particular the discussion of the domain-centred model, but I found a couple of things about it fairly bewildering: * one of my favourite things about Haskell, and also (according to my own impression) one of the central points of Haskell evangelism, is that Haskell gives you a really nice set of tools for accurately representing your domain. In particular I'd much rather work with `data Slice = [...]` than a C# `Slice` class. I'm not aware of any evidence that Haskellers favour code that avoids making use of these tools. * Of course the program which discards memory safety and makes sure that stuff gets inlined is faster; the fact that this is presented as novel, or something which Haskell evangelists haven't considered, borders on disingenuous, I think. This again doesn't match with my impression of Haskell evangelism, which says that, 99% of the time, it's fine to take the hit from 'dumb' (from a performance perspective) high-level code, because you now don't need to worry about a really huge source of program crashes, RCE vulnerabilities, and so on.
I haven't taught Haskell to people but I'm *guessing* that it would be helpful and I'm *guessing* deferring all the things would be even more helpful. Paging /u/bitemyapp
It does actually, I think it's a plugin of sorts. We're using hydra with our GitHub-centric development flow and it works well. Fetches the PR's and sends statuses to commits (those bubbles). If you're interested I can share the details of our setup (the NixOS configuration).
Yea I'd be interested to see how complex a simple Hydra setup with GitHub webhooks and statuses is.
&gt; It’s ok to model everything in terms of lists of lists of lists of integers, because then you can play with all those nifty library functions. One data structure with 1000 functions FTW! wat A lot of libraries have a similar (if not the same) interface but make different compromises in terms of the asymptotic complexity of the functions exposed. Does anyone ever argue that we should only ever use lists or is the author just pulling a strawman?
A typical example would be fixity declarations: if you have a block of fixity declarations and then a block of definitions, commenting out some of the definitions will lead to a syntax error because the corresponding fixities don't have an accompanying binding anymore. These non-fatal errors (we simply have "too much" information about non-existing bindings and we could merely forget about it) can be really annoying. It's the sort of pain points we have been trying to remove from Agda since we've put the warnings mechanism in place. It makes interactive development (especially refactoring!) a lot nicer.
Actually you don't because the implementation of Functor and Applicative has to be equivalent to the above, modulo possible optimization.
 The result that is ignored is the 'a'. The other part is handled by the monad machinery. 
It is sometimes helpful in larger chunks of code, but I find `undefined` and let expressions with refutable types (assert the type is `a`, see what the type checker says the type of the expression is) to be more useful. This essentially lets you ignore whatever part of a problem you aren't sure of. Don't know how to satisfy a particular type? `let v :: TypeYouWant; v = undefined`. Don't know what type should be ascribed to an expression that only works inside a particular chunk of code? (that is, can't be trivially expressed in the REPL for `:t`) `let v :: uhwhatisthis; v = someExpr`. Even in my day to day work, turning -fdefer-type-errors on and off isn't faster than using `let` and `undefined` 95% of the time. In particular for beginners, you're relying on hidden state to dictate REPL behavior. They often forget they have it on. `let` and `undefined` are in the code and you're able to make a choice about what bits to ignore/defer. I have [streams of myself programming](https://youtube.com/c/bitemyapp), including a [somewhat old workflow video](https://www.youtube.com/watch?v=Li6oaO8x2VY). You can see some of what I'm describing in the videos. I'll write down the time next time I do it in a stream so I can link to it directly. Overall, learners shouldn't ever be so over-extended as to need `-fdefer-type-errors`.
It's an FRP library that along with another `reflex-dom`, targets frontend browser app development. But the `reflex` lib itself is little more than an interface and type class morphisms, allowing many implementations, of which `reflex-dom` is one (and currently the only, afaik).
Yep, strictNullChecks, noImplicitAny and a bunch of TSLint rules and while it's more verbose code it also catches so many issues before even running it. I think Scala is getting ADTs in (some) next version (I am really looking forward to Dotty). I am still a Haskell noob (combating with monad transformers and [for me] insufficient docs of most libraries), but working for some time in JavaScript and then moving to TypeScript was really refreshing. Sure, it's no Haskell, or Scala, or Java (meh...), but it is way better than raw JavaScript crashing on undefined all the time in runtime, because me/someone else mixed up somewhere "types".
But isn't 'I listened to the advice of the compiler, now it compiles, and immediately it works' more satisfying? Personally, I would rather work with an engineer that says 'your plan has problems because this and this won't fit together', than with one that directly goes on to work and fails after a while, complaining 'Uh, now I can't fit this part in'. I don't think I believe that the latter is more forgiving. Edit: Forgiving /= Failing in more obscure ways. I have strong belief that those are just intimidated by either 1. error messages in general, or 2. the long messages generated by GHC, that tries to explain the context around the error *Not* the fact that the compiler won't let their program run
If the author aimed at making it impossible to use the library in an internal company partially-networked development environment, then this is fine. I used "unfortunately", since this is one more tool which we (Haskell fans at our corporation, which does not widely use it) can not use and will have to reimplement. So it's more work for us to bring our Haskell development experience up to par with other languages. Hence less chance of a big Haskell success story. I used "risky", since the license has not yet been (to best of my slightly outdated knowledge) tested in court, hence it is an actual legal risk for the company to make use of any software which has this license. I used "dangerous", since it is "contagious" even if any networked interaction has touched the AGPL-licensed code, even if not purposeful, the license is contagious. In our development environment, where thousands of engineers work on a huge monorepo and change things all the time, it is impossible to fully guard ourself from such mistake. And to prevent few probable questions: * no, no one would be making money directly off the library (e.g. no one would sell it as a service); * yes, this is important for a group of Haskell enthusiasts who want to use Haskell, but are not actively supported by management.
That's the old strawman people used to use against lisp, too. And the lisps have their problems, but lack of non-list data structures is seldom amongst them.
The racket people agree with that. Their experience teaching freshmen is that static types don't help rank beginners.
The needs of beginners and more advanced users of the language are different. Type errors can be quite abstract. Failing at runtime (on the same problem), can add a concrete example. To be really silly, compare adding "apple" + 3 vs a type error a out String not having a Num instance.
I hope so
I would love to see an implementation of this algorithm in Haskell to compare performance differences. I may give it a go, I rarely need to work with vectors and other mutable structures :D
It was an interesting read, but I don't tend to like this excessively scornful tone. I also tend to favor adequately modelling your domain in your code. That doesn't mean that abstractions can't coexist with that.
&gt; Syntax errors: With sufficient work could the parser have enough recoverability to defer syntax errors? If not all syntax errors, then some subset? Broadly speaking, that's been tried in a number of languages and it tends not to work anywhere near as well in practice as it works in your head. It's hard to think as stupid as a parser and not bring your own human understanding of the code into play when you try to imagine how it works. Generally speaking I'm much happier when the compiler bails at the first error than when it tries to "keep going", which often produces screenfulls of spurious errors. I'm speaking across a lot of languages of experience here, but I wouldn't expect Haskell to fare better than some of the other languages I've seen. If this _is_ going to work, you want more grammatical redundancy rather than less, and while Haskell is not entirely non-redundant, it's definitely on the low end of the scale.
For the psychological trick, I'd love to have `-freverse-errors`, so the first error is shown last. Then I'd never need to scroll. (There is `ghcid` etc, but this might be cool anyway :)
Uhm, but Scala already has ADT (via case classes) Maybe I'm misguided, but if I had to choose between Java and Typescript, I'll take the latter, since it has those 2 things I mentioned that Java lacks (ADT and null safety) 
You're going to have a tough time convincing me your solution is better if it takes you 20 pages to explain what I understood clearly from 6 lines of haskell.
I've found that implementing the original algorithm with an Unboxed Vector, and using a strict scan would improve the performance by 100 times, making the proposed algorithm only a fraction faster (if any) than the original haskell code, and with great more complexity: https://gist.github.com/kuribas/c61f16d902a6586cf488891007067921 For fun I implemented it in C and C using sse vectors (1 byte): https://gist.github.com/kuribas/ddd977968c3a56eac505901acb3e8eee I get (on average) 19ms for the haskell code, 11ms for the C code, and 3.5ms for the SSE optimized code, using a random list of 10 million integers. That's about 1 cycle/element for the SSE code, 4 for the C code, and 7 for the haskell code. I'd say that the haskell doesn't do bad at all. The SSE code, while fast, is not portable, and took a lot of time to write (but was a fun and educative exercise).
This might help: https://github.com/mikeizbicki/ifcxt
If you can make something like `defer-all-errors`, where your program just never errors at all! This would be great for production :D
&gt; But isn't 'I listened to the advice of the compiler, now it compiles, and immediately it works' more satisfying? I really don't know, but it seems possible to get the best of both worlds by having this as a flag.
You can pack an optional dictionary using a `Maybe (Dict (Show a))` using [`Dict`](http://haddock.stackage.org/lts-5.1/constraints-0.8/Data-Constraint.html) from the Constraints library.
Thanks. This requires enumerating all the Show instances. Or if you derive them using its mkIfCxtInstances it will only enumerate those in scope, missing any client Show instances.
Eg: data Foo where Foo :: Typeable a =&gt; Maybe (Dict (Show a)) -&gt; a -&gt; Foo If there's a `Show` instance in scope for the `a` type you define, then `Just Dict` will type check. If there's not a show instance in scope, then `Just Dict` will fail to compile. You can access `Show` like: mayShow :: Foo -&gt; String mayShow (Foo mdict x) = case mdict of Just Dict -&gt; show x -- the `Show` dictionary is in scope in this branch Nothing -&gt; "&lt;&lt;unshowable&gt;&gt;"
Thanks. This is close, but the user will have to specify Just Dict or Nothing himself instead of the compiler doing it for him. The compiler knows if the type is an instance of Show or not.
Not at runtime, though -- when you existentialize the type, you are telling GHC "I don't need to know anything about this value, other than this `Typeable` dictionary that I'm packing along with it." There's no way for GHC to know that it needs to *also* pack a `Show` dictionary along with it.
That's what I thought too. See my proposal and the discussion: https://github.com/ghc-proposals/ghc-proposals/pull/22 The main issue is with orphan instances. I still think it is manageable but you would have to convince other people that it is worth it and to prove that it is not unsound.
A similar issue arises when specifying [character encoding in HTML files][1]. [1]: https://en.wikipedia.org/wiki/Character_encodings_in_HTML#Specifying_the_document.27s_character_encoding
One of the best teachers I had.
Do I need to know things like monads. This looks like a good opportunitty to learn more about haskell. But im still learning haskell by the book of bitemyapp 
This article makes a good point that is sometimes obscured by the needless attacks on functional programming. The main point it makes is actually what I like about Haskell: that it allows you to focus on the domain. Using data types to model the domain objects and write functions that transform them according to the domain rules, just like described in this post. No need to fetishize folds and scans, just like the C# code doesn't fetishize dependency injection and factories. In short: good argument, could do without the needless hostility.
Maybe GHC should allow instance selection based on context. A simple selection rule would be: take the matching instance whose context is the syntactic superset of all others that match. So *(C1, C2) =&gt; X* would be selected over *(C1) =&gt; X* if both C1 and C2 are satisfied. If there is no single superset then error like today. Note, "match" here means including the context. So if C1 is satisfied but C2 is not then *(C1) =&gt; X* would be selected.
You could make two functions, `mkFoo :: (Typeable a, Show a) =&gt; a -&gt; Foo` and `mkFooNoShow :: Typeable a =&gt; a -&gt; Foo`. The compiler won't ever tell you when you're calling `mkFooNoShow` where you could be calling `mkFoo`, but it will catch the other case. Also, it's potentially brittle, but Template Haskell knows what instances are in scope if you want it really hands off.
I work in a 4 person app development team. Every member of the team sits in the same shared cubicle. It's a 15+ year old app that's been essentially end of life for about 5 years now. We still have daily team stand ups in which we all walk 10 feet to a 'collab' room and discuss what we're working on, which is usually some banal clerical task, while standing. Coporate culture is terrifying.
talk is cheap.
Clearly you need to know monads... but not for this gig! Here you just need to have a general understanding of Stack to be able to help out with the usual customer support questions. If things require more in-depth knowledge, you can simply hand it over to level 2 support.
Related http://chrisdone.com/posts/twitter-problem-loeb contains other Haskell implementations.
See example [there](https://gist.github.com/maxigit/506d2ce4a9d07c1aef59c59ab8902dcd)
This is a cool article so far, but I see a small mistake on page 9. In the function for `Leak`you have this equation for updated water: `srcWater’ = max(tgtWall + tgtWater - srcWater,0)`. However this seems incorrect. Imagine if the function was called like this `Leak( (6,3,0) , (4,3,2) )` then the result would be `(6,4,-1)`. So the amount of water would somehow increase, which can never happen in this model. Not to mention you would have negative air amount. I think what was supposed to be there was `srcWater’ = max(tgtWall + tgtWater - srcWall,0)`. **EDIT**: Seems like the author just forgot to edit that since it's correct later on. Also on page 14 we have Changes 1, 2 and 4. Did the 3rd one get taken out?
Can't you do it with overlapping instance ?
Definitely agree.
&gt; Unlike the game of life, this rule immediately converges to a fixed point after you apply it once. Um, that is only true if you make some assumptions about the algorithm (in particular, the order cells are visited). Which I thought we weren't doing. This particularly comes up when they claim the worst-case complexity is O(nm). It only works if you've already devised the original algorithm they are criticizing. &gt; Although I’ll use integers to represent those amounts, this is no longer a constraint for correctness, and I could have used floats as well. This suggests they don't really understand the original algorithm, since it could use floats just as easily. More broadly, they seem to have an odd model of how water flow works. I can see how it solves this problem, but when they talk about changing the code to show what happens as the rain falls, I find it hard to believe they could make those changes easily. Of course, first you'd have to define what the new problem even is. If it means filling to a given maximum depth, then that is fairly easily handled by the original algorithm. In that, there are three possibilities for the water height in any given column: the height to the left, the height to the right, or the height of the tower (if the column is dry). You determine which it actually is by comparing them. To account for a maximum depth, you just add a new possibility: the height of the column plus the maximum depth. You now have to worry about multiple passes, but that is the same in the cellular automaton approach. If they mean they want to add a certain volume of water and see where it goes, the cellular automaton solution is going to have fundamental correctness issues. If you put one unit of water in a cup two units wide, where does the water end up? The underlying idea here is not bad, but at least they have chosen a bad problem with which to demonstrate it.
That's great! Thanks! 
Clicking through your posting history, I wonder whether this is the latest account of StackLover / StackSucks. I don't quite know what your goal is though. Most of your recent comments have read to me like pretty immature fanboyism, which is probably leading to the downvotes you've been seeing and almost certainly isn't helping your cause. Do the mods have some kind of policy on these kind of novelty accounts, seeing that they seem to be aimed at creating / fueling drama?
See https://wiki.haskell.org/GHC/AdvancedOverlap data HasShow data NoShow class S flag x where {show' :: x -&gt; String} instance Show x =&gt; S HasShow x where { show' = show } instance Typeable x =&gt; S NoShow x where { show' = (...) } Then you just need to construct a type family so you can dispatch to the appropriate instance. I believe I have written something like the following before, but don't have GHC handy. Apologies if the implementation is a bit different type family Showable (c :: Constraint) x where Showable x = If (Show x ~ ()) (Show x, HasShow) (Typeable x, NoShow) Now a wrapper class class S' x where {show'' :: x -&gt; String} instance ((c,f) ~ Showable x, c, S f x) =&gt; S' x where {show'' = show'} This has all the pieces, although it's not nice to work with (GHC won't realize that S' x is trivial, you probably have to allow ambiguous types). I will get this to type/syntax check when I have a chance if anyone needs that 
&gt; This would be great for **development and tooling** FTFY.
See, that's the thing. I found the Haskell code (hell, even the problem description) borderline impenetrable. On the other hand, I immediately grasped the problem and the solution just by looking at the first picture.
This was actually just about removing stackage curator imposed upper bounds while trying to get the next version of GHC on the nightly build - not some ideological tirade against bounds!
How do you precisely determine if a lambda term is in the oracle-free fragment? It would be really great to have a typechecker for EAL terms; have you looked in to this at all? EDIT: I looked [StackExchange](https://cstheory.stackexchange.com/questions/32406/is-sort-typeable-on-elementary-affine-logic#) and found this comment (you were in this thread too by the way) &gt; what makes Lamping's algorithm work without brackets and croissants is a stratification property of terms, which is purely structural and independent of types: So you can determine if a lambda term is in the oracle-free fragment without using types, but how?
I mean how is that not true? Python is slow and doesn't have any kind of static guarantees, but its easy to learn and reasonably intuitive, that is like the definition of lowest common denominator language.
Interesting! [I got these benchmarks](https://i.imgur.com/UhskPjH.png), the list variant appears to have similar asymptotics but much higher overhead. Somehow, your unboxed vector code is running ten times faster than mine. When I implement [your C algorithm in ST](https://gist.github.com/parsonsmatt/1f83f8b8499af60c19a00d730d52f948), the ST only performs slightly better [(possibly due to unsafe reads?)](https://imgur.com/a/t6dnZ)
Yes, there are some papers that developed type checkers and even inferencer for EAL, but no implementation so far AFAIK. I'm under the impression a combination of EAL + CoC would be an amazing language, because it solves 3 problems at once: reducing terms efficiently, expressing induction (because we can use Fix!) and proving consistency (easily follows from linear types)! That is just my impression, though, I don't know much and I could be wrong, but I have plans to try it. &gt; So you can determine if a lambda term is in the oracle-free fragment without using types, but how? Using [this](https://www.reddit.com/r/haskell/comments/6phxvb/cleaned_up_implementation_of_lampings_abstract/dkq57yx/) untyped language. Tldr: allow only linear functions, but extend the language boxed terms `(!term)` that don't allow lambda variables inside them, plus a `let` expression which allows duplication of boxed terms.
You say curious, I say magnificent. And it's not a peephole for your average Joe to squeeze their face in whenever they so please. It's a privilege of celestial beings and celestial beings alone.
I found it hard to appreciate *why* the Haskell version works until I drew it out it. It was easier for me to think of as filling in gaps in stacks of blocks with something like sand, rather than flowing water. You start with an arrangement of blocks: blocks = [2, 5, 1, 2, 3, 4, 7, 7, 6] ········· ······▓▓· ······▓▓▓ ·▓····▓▓▓ ·▓···▓▓▓▓ ·▓··▓▓▓▓▓ ▓▓·▓▓▓▓▓▓ ▓▓▓▓▓▓▓▓▓ Then you dump a bunch of sand on. ░░░░░░░░░ ░░░░░░▓▓░ ░░░░░░▓▓▓ ░▓░░░░▓▓▓ ░▓░░░▓▓▓▓ ░▓░░▓▓▓▓▓ ▓▓░▓▓▓▓▓▓ ▓▓▓▓▓▓▓▓▓ Take a shovel, and see where you can scrape off the excess, lifting (but never lowering) your shovel when you hit a wall. You can start from the left and then from the right: scanl1 max blocks ·····┌────&gt; ·····│▓▓░ ┌────┘▓▓▓ │▓░░░░▓▓▓ │▓░░░▓▓▓▓ ─┘▓░░▓▓▓▓▓ ▓▓░▓▓▓▓▓▓ ▓▓▓▓▓▓▓▓▓ &lt;─────────┐ ······▓▓└─ ······▓▓▓ ·▓░░░░▓▓▓ ·▓░░░▓▓▓▓ ·▓░░▓▓▓▓▓ ▓▓░▓▓▓▓▓▓ ▓▓▓▓▓▓▓▓▓ Or from the right and then from the left: scanr1 max blocks &lt;─────────┐ ░░░░░░▓▓└─ ░░░░░░▓▓▓ ░▓░░░░▓▓▓ ░▓░░░▓▓▓▓ ░▓░░▓▓▓▓▓ ▓▓░▓▓▓▓▓▓ ▓▓▓▓▓▓▓▓▓ ·····┌────&gt; ·····│▓▓· ┌────┘▓▓▓ │▓░░░░▓▓▓ │▓░░░▓▓▓▓ ─┘▓░░▓▓▓▓▓ ▓▓░▓▓▓▓▓▓ ▓▓▓▓▓▓▓▓▓ Either way, when you’re done, you’ll have scraped off as much of the excess as possible in both directions: zipWith min (scanl1 max blocks) (scanr1 max blocks) &lt;------┌──┐-&gt; ·····│▓▓└─&gt; ┌────┘▓▓▓ │▓░░░░▓▓▓ │▓░░░▓▓▓▓ &lt;─┘▓░░▓▓▓▓▓ ▓▓░▓▓▓▓▓▓ ▓▓▓▓▓▓▓▓▓ Then you can dump out the sand to separate it from the blocks: zipWith subtract blocks $ zipWith min (scanl1 max blocks) (scanr1 max blocks) ········· ········· ········· ··░······ ··░░····· ··░░░···· ··░░░░··· And measure how much you needed to fill in all the holes: sum $ zipWith subtract blocks $ zipWith min (scanl1 max blocks) (scanr1 max blocks) ·········· ░░░░░░░░░░ 10 
I think the second one is more accepted (but only for long argument sets). My preference is to use records and lenses in functions that needs that many arguments.
If I need to indent, I prefer the second style. It's easier on the diffs and the editor integrations.
I **strongly** prefer the second one. I just find it much more consistent with the typical two space indentation found in the language. The only problem is that it makes it harder to grep for a type signature; I sometimes just search a file for a name followed by a space and two colons, which requires multi line matching in this style (something not all editors / browser search features support).
I personally don't hold myself to either one, depends which one looks more readable in that context. Depending on things like identifier length and number of arguments and so on.
A page or two in I realized that the OP could have achieved all his domainness (without his 10s of lines) by naming the haskell scan portion `leakLeft`, the `zipWith` could be `minimumRetained`, but tbh the original is better. It's easier to fit 3 lines in my head, even if I have to "decode" it, than read through a whole bunch of cruft It's already been pointed out that, had the original haskeller meant to optimize, similar performance could be achieved. And the OP mentioned maintainability; a few lines is a lot easier to modify than OPs behemoth.
&gt; 0.998 R² (NaN R² .. 1.000 R²) Spot the `criterion` bug :) Edit: actually, probably `statistics`
u/haskdev is correct: any skill level with Haskell is welcome to apply, and I do hope that it turns into a great opportunity for people to learn more about Haskell. Good luck with the learning!
Yeah, it's possible via sealed case classes. But the new ones are less verbose - http://dotty.epfl.ch/docs/reference/enums/adts.html. (Union types are also interesting to see in such language as Scala.) I think Java has some way of handling nulls, Optional class or something like that and also some annotations. Unfortunately I don't think it's used that much in libraries. TypeScript, when on its own, is nice. But after adding some libraries, or using framework like Angular it's getting a bit messy. Libraries doesn't have always proper typings (e.g. Ramda is missing a lot of overloaded versions of functions for objects, so either one has to write them themselves or just rely on &lt;any&gt;) or it's not yet possible to check type at compilation type (e.g. Angular and passing data across components - there are no type checks in templates - it happens to me all the time passing data of type A and binding them to an @Input expecting type B [I keep forgetting to convert them]). Java feels much more strict - it's not common that types are lost at some point (well, generic types are lost at runtime, but at compile type are fine).
No, no. I really mean in production. If there could be a compiler flag that means that neither the compiler nor the program at runtime ever errors, that means we have perfect software, right? That's what I want :p Joking aside, errors at compile-time are useful, errors at runtime are dangerous.
It isn't memoizing anything. It is just building up bigger and bigger functions. But it has to walk all the way out to the first generation for the raw data.
Stop using the word evangelists... It's embarrassing and makes Haskellers sound like a religious cult... Other than that, I agree.
There's definitely a difference in understanding WHAT is being done as opposed to WHY it works, Haskell is a tool which helps with building solutions that can actually execute, but it's still up the the programmer to decide what needs to be done. One programmer's approach will be unintuitive to some other programmers, that's OKAY! The solution is not necessarily to try and solve the problem in a way that best matches the domain (computers don't work that way usually) but rather to explain clearly with names and comments the WHY of how a program solves a given problem. Separating WHY from HOW gives us modularity.
I started to write a library for this some time ago, the front end parts are not finished yet: https://github.com/tomsmalley/quickform
I don't believe so. There is an active field of research called implicit complexity which deals with precisely this sort of question. It uses particular sub-exponential modalities (which are more than just linear types) to capture some complexity classes, but it's always an under-approximation: all the function of some type are in the corresponding complexity class, but many function of that complexity class don't have the appropriate types. I have never seen any claim of attempt at completeness. Though I've heard some claim of practicality (something like: it work for most practical function, up to trivial reformulation). But implicit complexity is not my academic cup of tea, so I can't really comment in depth. At any rate, this is about checking rather than inference. I don't know what would be a good approach for actual inference. All that being noted, I would like to raise the point that complexity is, in my opinion, of limited practical relevance for programming: you should probably know what approximate complexity your function have, but a high-complexity function may be the faster one for your program.
If the full function signature is less than about 90 characters I put if all on one line: toList :: Stuff a -&gt; [a] IMO, breaking it across multiple lines serves no benefit whatsoever. 
You may be interested in ["Dual Light Affine Logic"](http://www.sciencedirect.com/science/article/pii/S0890540108001119) which does let you infer time the exactly polynomial time complexity of any algorithm written in it, and which as I recall is complete for polynomial time algorithms.
&gt; touring completeness Should be "Turing" after Alan Turing, the mathematician. 
&gt; The only calls that could theoretically run completely in parallel in the code above are the scanl1 and scanr1. lolwut. zipWith (-) not paralleLizable? Or sum? 
@snoyberg you have a respons from me 
Why couldn't a zip be done in parallel just the same as a map could?
Awesome!
Beautiful ASCII art. I'm very impressed with the arrows.
Thank you! :) I actually had better arrowheads, but they didn’t render correctly in my browser. And it still doesn’t display right in the Reddit app because it doesn’t seem to support newlines in code blocks. And mobile Chrome renders the middle dots with the wrong width. *sigh*
Since you can't get the compiler to do it for you, I find this a better alternative to the Dict solution: data Foo where Foo :: Typeable x =&gt; x -&gt; Foo FooShow :: (Typeable x, Show x) =&gt; x -&gt; Foo
In general I do: function :: k a =&gt; a -&gt; b Except when I have to document each input, since it is Haddock incompatible...
&gt; Type errors can be quite abstract. Failing at runtime (on the same problem), can add a concrete example. It'd be nice to patch the compiler to produce [Dynamic Witnesses for Static Type Errors](https://arxiv.org/abs/1606.07557). :D
A list can only be walked sequentially. I guess if the computations that you run on each element take a lot longer than the time to walk the list then you can see a speedup. Presumably that's not the case here.
How did you draw it?
How would sum on a list run be parallelizable?
Except if you have to annotate an argument with a haddock comment
I saw that paper at last year's ICFP, too. Great stuff!
If scanl1 can be run in parallel, then surely sum can.
I've always used the first, but this second style seems strictly better, I might adopt it!
This is super interesting; I was musing about the possibility of carrying complexity certificates along with the type signature but didn't know the research terminology for it. I found this abstract on the subject : https://www.cs.rice.edu/~sc40/obt15/mazza.pdf
Always the first one. It makes it easy to find function definitions by grepping for `foo ::`.
I don't think that's what he meant. He meant `scanl1` and `scanr1` can run in parallel with each other.
You're right, sorry. I misread the sentence (and didn't read the whole thing at first). I thought he was talking about data-parallelism because the example looks extremely well-suited for that.
If you're interested in names to look up this line of research: Damiano Mazza, indeed, Patrick Baillot (cited by Edward Kmett below), Ugo dal Lago, Marco Gaboardi, are a few I know.
Wouldn't it be even easier than that? We wouldn't have to synthesise anything. Ultimately, with a deferred type error we get to an expression composed of two subexpressions whose types don't match properly. Couldn't we use `Show` instances, where available, to print them out and say "these things didn't match"? For example, if I write foo :: IO () foo = putStrLn 1234 the error could be "`putStrLn` takes a `String` argument but `1234` was of type `Int`". 
The incompatibility with Haddock kills this style for me. I'm curious though: How do you format long chains of operators? f $ -- trailing operator g -- or f $ g -- leading operator (maybe indented)
Grepping for `^foo` is much more reliable. (Or even `^\s*foo\b` if you want to go crazy with the regex.) Better yet, use [hasktags](https://hackage.haskell.org/package/hasktags)! 
So back to my initial question: Why would you hire Haskellers to write PHP if that's the technology you want to use and it's actually cheaper to hire those code monkeys? I mean, I don't go around asking at 3 star Michelin for executive chefs to work at McDonald or Burger King. I mean, this logic is even understandable for small kids ... Luckily Haskellers will have a future in Europe (or with Companies that will handle Europeans citizens data). Who would have thought that what would push Haskell (and similar pure languages) to the masses, would be the legal system: http://blog.stermon.com/assets/talks/2017-07-27_Elm_Meetup_July_Rocket_Labs_Data_protection_by_design_and_by_default.pdf
I also strongly prefer the second one. The first way causes you to reformat lines if you change the name of the function. And sure, you can automate the formatting, but you're making blaming, diffing, and merging more difficult on yourself. 
So, I've got some mixed feelings about this. What he's done is to essentially replace one of the core GHC subsystems with a C implementation because it provided a slightly better abstraction over system differences and slightly better performance. On the one hand I understand the motivation, and it does solve a problem now as opposed to later. On the other hand the numbers seem to indicate it's only a *very* minor performance improvement. On the gripping hand however I think the better option would be to work to address any issues with mio because then everyone benefits and GHC gets better.
Operators after. This makes indentation completely independent of operator size, which is not the case with other styles. For example: f $ a &lt;&gt; b instead of: f $ a &lt;&gt; b I also like the fact that something that is before `::` is a function name, something before `=&gt;` is a constraint, something before `-&gt;` is an argument, very consistently. It is not completely uncompatible with Haddock either, its just clumsy. You have to do: function :: x {- ^ Value -} -&gt; y
Or sometimes, it's nice to show an equivalence between arguments: intersectionWith :: (a -&gt; b -&gt; c) -&gt; Map k a -&gt; Map k b -&gt; Map k c 
&gt; `^\s*foo\b` This will net you some of the _calls_ of `foo` as well. An unfortunate side effect of Haskell's minimalistic syntax. I will check hasktags though, it looks interesting!
&gt; Here is the full source code, which generates a list of 10 million towers with random height between 0 and 200 and then measures the CPU time required to calculate the collected water (so the code generating the world is excluded from measurement). Except, it doesn't.
I have _no_ feelings on this yet, I'm still digesting it. But as a devil's advocate comment: most of the GHC implementation is in C anyway, so the fact that we're using a different C implementation isn't really a detractor. And the fact that it's an established library written and maintained by others, with IOCP support to boot, is nice. I'm also aware that there are good arguments _against_ using libuv in the GHC RTS, though I unfortunately don't have enough experience with the debate to know what they are.
We hire software engineers to make software. Part of that is Haskell, part is PHP, part is JavaScript. Most of the PHP I write these days is maintenance fixes for projects and integrations for new Haskell services. We use PHP because the owner of the company knows it, and he typically builds the prototypes and first versions of our software. We use Haskell because PHP is a slow, unmaintainable mess, and critical systems need to be fast, correct, and maintainable.
someone made a haskell version of the final solution https://gist.github.com/Teggy/1f525cf3027c92d9008d 
It doesn't?
&gt; Can't write instances for unboxed types, .. Does this not make sense class Destructible (a :: TYPE rep) where destroy :: a -. () instance Destructible (Int# :: TYPE IntRep) where destroy :: Int# -. () destroy = ...
One approach to `zip` might be to provide an alternate interface, either by using [These](https://hackage.haskell.org/package/these-0.7.4/docs/Data-These.html) and defining `azip :: [a] -&gt; [b] -&gt; [These a b]` (I'm ignoring linearity on arrows and substituting list for streams, but I think the concept holds), or (I think I like this one better) by returning the leftovers `lzip :: [a] -&gt; [b] -&gt; ([(a,b)], Maybe (Either [a] [b]))`.
Sure! I was leaning towards the second one as well, but there are a few variants. Don't forget you need both stream ends as well. The return type got a bit too hairy for me to feel like any particular was obvious enough for me to hammer down like it was the only truth, kinda. It gets a bit awkward when you want to use this guy in the middle of a pipeline too... :) 
Well look at that! Neat, haven't seen before. :) 
Doesn't `streaming` already have an answer for this? Something like zip :: Stream (Of a) m r -&gt; Stream (Of b) m r -&gt; Stream (Of (a, b)) m (Either (Stream (Of a) m r) (Stream (Of b) m r)) Edit: Oh wait, that's your second one.
That should not be possible, as it's the same code... Also I expect that after fusion the vector code should give code similar to the ST version.
Allowing your program to still be loaded into GHCi even when you have type or other compile time errors is incredibly useful though. 
Author here, thanks for the suggestion, i think i should give more background on why i'm doing this. First, I'm totally aware of Phyx's [effort](https://github.com/Mistuke/ghc-win-io-system/) to bring IOCP to ghc, but looking at his code, you will soon realize it can't be easily integrated into current mio's API, namely `threadWaitRead/Write`, the reason is that IOCP is completely different model for async I/O. (That's not to say it can't be integrated into base, we may find other ways.) Now let's take a step back, thinking about designing a whole new I/O subsystem. What do we want? an extensible `IOException`? a slimmer `Handle`? or just a new event I/O backend ? I'd said i want *ALL* of them, but soon i find to get any of these targets done, the I/O subsystem got to be rewritten from scratch. The problem becomes to rewrite mio with new extensible exception, then rewrite `Handle`. Holy crap, i can never do that alone. I gotta to try something different, so here comes this experiment: can libuv help me building a performant I/O system in haskell? I've been told that libuv is *slower* than directly using event system provided by OS, so i'm pretty skeptical at first if this experiment would sucess. I'm expecting to see some lower numbers from libuv branch. The more time i'm putting into the new I/O manager, the more libuv code i have read. The more i'm convinced that libuv is just a thin wrapper around different event system, with some encoding hacks built-in. That's a good thing because i need exactly this, nothing more, nothing less. After about two weeks, i finally put all my hypothesis, designs into code, and i got my evaluation on libuv: it's OK to use it as a solution. The point is not about the slightly better performance, it's about the giant saving on developing time. This post is a record on why my road branched to this direction. And i hope people can understand the motivation behind this. The code is still not mature of course, so take it with a grain of salt. My new I/O library will take lots more effort, and i'm still on it. Last, i'd really like to thank the libuv developers, i asked lots of questions under libuv/help repo, their responses are fast and helpful. Couldn't say more nice things about this great project, hack on!
There you go! https://github.com/ghc-proposals/ghc-proposals/pull/30
Nope. Haskell does lazy evaluation. To your import list, add: &gt; import Control.DeepSeq &gt; import Control.Exception (evaluate) and replace: &gt; let hs = take 10000000 (randomRs (0, 200) g :: [Int]) with: &gt; hs &lt;- evaluate $ force $ take 10000000 (randomRs (0, 200) g :: [Int])
Yeah, something is a little weird here.
It would be great to have a plug-in to search a function by its signature, a plug-in which should understand the language (not grep), like Hoogle but local. Maybe this feature already exists?
You can use hoogle locally.
I'll use whichever words I want to, thanks.
&gt; Nope. Haskell does lazy evaluation. Thanks, I'm aware of that. Do you realise the author addresses lazy evaluation in the paragraph directly above the one you quoted? 
The paragraph you refer to is justification for printing the result rather than merely evaluating it. It does not justify not evaluating the list of random numbers before starting the timer.
Thanks for the clarification. Just to be clear, I wasn't criticizing you really, more making an observation. In a purely architectural sense the best approach is to improve the GHC RTS, but as you pointed out that's a HUGE amount of work and not likely to be done anytime soon, so I can totally understand why you opted to go a different route and tackle a smaller problem. At the very least having an alternative approach can provide some good reference numbers to compare against while working on the RTS enhancements.
Ah, I see. You're pointing out that the Haskell list of randoms is not evaluated before the timer starts, whereas the C# list of randoms is.
I'm wondering if `vector` may be written to use SSE in a way or another: by hardcoding it in the `vector` code base or by letting the llvm backend find opportunities for vectorization. Perhaps a stupid question, but how do you guarantee (with your benchmark) that the lazy evaluation of the random numbers are not part of the runtime for `rainfall1`? I had always been afraid of that and I'm always using `deepseq` before calling by `bench`. I tried to extend your benchmark using `accelerate` (llvm native code generation), but with no interesting results, I'm close to 3 times slower than the vector approach with one thread, and 8 times slower with 4 threads (my computer have 4 natural core)). I'm usually deceived by accelerate on theses kind of stuff.
I get runtime of 8.8s without `force` and 5.8s with `force`. 
https://gist.github.com/guibou/8450d4c54f2353d4ac2c520352938c3f My gist with the accerelate code.
Stick with "evangelist". I've seen some hate for it on this subreddit recently, but it's the right word. Wikipedia defines a [technology evangelist](https://en.wikipedia.org/wiki/Technology_evangelist) as someone who "promotes the use of a particular product or technology through talks, articles, blogging, user demonstrations, recorded demonstrations, or the creation of sample projects". It also notes: &gt; The word evangelism is taken from the context of religious evangelism due to the similarity of relaying information about a particular set of beliefs with the intention of converting the recipient. It's been in use since at least 1990, so it's not exactly a new thing. 
**Technology evangelist** A technology evangelist is a person who builds a critical mass of support for a given technology, and then establishes it as a technical standard in a market that is subject to network effects. An evangelist promotes the use of a particular product or technology through talks, articles, blogging, user demonstrations, recorded demonstrations, or the creation of sample projects. The word evangelism is taken from the context of religious evangelism due to the similarity of relaying information about a particular set of beliefs with the intention of converting the recipient. There is some element of this although most would argue it's more of showcasing the potential of a technology to lead someone to want to adopt it for themselves. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
Non-Mobile link: https://en.wikipedia.org/wiki/Technology_evangelist *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^98923
Can it be used within a single project? Is it easy to configure? Why do all people just use grep instead?
Yes. Stack has built in support for it. I believe Cabal is able to do it, but i don't remember how. And Nix has pretty good support for it as well. EDIT: I use grep because it's right there in my editor =P
&gt; probably from all that recursion ? &gt; if the language serves no practical purpose other than intuitive code and amazing readability ? &gt; fire at will pew pew
Lecture 27 of [this linear logic course](http://www.cs.cmu.edu/~fp/courses/15816-s12/schedule.html) deals with Implicit Computational Complexity.
I think it's interesting, but there's a lot of power in programming to solve a general problem, and not a domain specific one (like not having to rewrite a function a bunch of times). I guess it would really depend on what you're programming. If it's something that will literally only be used once for a specific domain, then focusing on the domain for performance is great. 
I'm not sure I understand your comment.
&gt; Nothing that couldn't be implemented equally well in Haskell using a mutable vector, say I can confirm that with an *immutable* vector (and no other changes) my runtime goes down from 8.8s to 0.91s. This is a better relative performance that the naïve C# code. It's amazing what you can do by choosing a decent data structure! The rest of this paper no longer has any reason to exist [regarding the Haskell complaints -- the other bits are interesting]. EDIT: My code: http://lpaste.net/357525 And someone on Twitter has already done this very nicely: https://gist.github.com/Porges/9ca15a9ec01bf055edcd88394496dbe3
Your post is strange, and the answer to all your questions is just something that requires experience and the understanding that each domain of problem solving in programming has a different weighted set of requirements. If performance is weighted significantly above all else, then of course people will use fortran/c/c++. Equivalent Haskell representations of the same problem often perform within a factor of two of low level code. So, if that is acceptable, then you can take that hit in order to take advantage of all the benefits of Haskell. For certain domains, like finance, this performance hit is nothing compared to the code clarity/correctness gained from using Haskell.
1. Recursion's not expensive 2. "Intuitive code" and "amazing readability" would be killer features for *any* language and would be mic drop time. The best we can say about Haskell is that it has "sometimes intuitive code" and "quite good readability". 
I'll second that that video series is amazing! And there's a Chrome plugin that can convert .tex files to PDF, I think it will work with imports in tex if you choose to select multiple files at once. https://chrome.google.com/webstore/detail/tex-latex-viewer-and-edit/lmbknmfadpeadepgoblginkbiljjpcea
I agree with (2), but (1) is making me raise my eyebrow. Of course, I might be wrong, but you're telling me that recursive is not expensive, from which I can only infer, iterative loops are more expensive than recursion?
I believe the question marks imply that he's puzzled by your comments. Haskell is used in industry and serves a practical purpose for their requirements and needs: https://wiki.haskell.org/Haskell_in_industry &gt; I don't understand why anyone should even learn Haskell. You do understand insofar as you're learning Haskell. People learn programming languages for a multitude of reasons and some of them are exclusively used in academia. What's there not to understand?
Ah, I understand. I guess the mistake I made was think that the only programming problem out there is performance because of which basically, no language other than low level languages can be used. But thanks, your comment helped me realize exactly where I was wrong.
Actually i'm on the camp that advocate moving I/O system out of base ; ) Both for reducing maintenance cost, and for moving fast without bundling with GHC.
Personally, I see Haskell as the closest analog to *Python* in practice: expressiveness is *exactly* what matters most. Haskell gives us a high-level language with great facilities for abstraction, a type system to help organize and lightly verify our code and reasonable performance. Exactly the niche Python fills now, except Haskell does it in a totally different style, one I like *way* more. One particular distinction is that Haskell is great at domain-specific languages, which lets it extend to domains the base language can't handle. Computational physics is a great example—nothing short of C or Fortran works for the most demanding problems in that area because performance dominates other concerns. You can't use Haskell for that, but neither can you use Java or Python or the vast majority of other commonly used languages. But you *can* use a Haskell DSL that generates, for example, LLVM bytecode, and it can give you real advantages. A real-life example is the [Ivory](https://ivorylang.org/index.html) language designed by Galois for writing hard real-time control software for UAVs. This is another area where Haskell's performance doesn't cut it, but neither would most other languages. In fact, even C isn't good enough: you have to restrict yourself to a particular subset of C to ensure performance is consistent enough. But designing a DSL in Haskell lets us get around this: the DSL has exactly the semantics you need to write hard real-time code and compiles to efficient assembly, while still letting us have a lot of the expressive power of Haskell (including its type system) by, essentially, turning Haskell into our *meta-language* for the Ivory program. While you can adopt this technique in many other languages, I think Haskell is one of the best-suited languages for this approach, which makes it worthwhile all on its own.
To be entirely honest, I'm learning Haskell without really thinking about where I can use it in the future. I'm doing it because I'm bored and because it's interesting. But the link you've provided has given me great insight. Thanks for the link!
&gt; iterative loops are more expensive than recursion? What makes you think that recursion compiles to code any different that an iterative loop?
&gt; Haskell as the closest analog to Python in practice How so?
Well, I was thinking more on the lines of runtime, wherein with large recursive stacks more memory would be used up, when loops wouldn't do that. I think.
Thanks for the link! I've started to work through the book a couple of times. I'm very much looking forward to watching the videos. I typeset the notes for you here: https://github.com/jdolson/hott-notes/tree/pdfs/pdfs 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [jdolson/hott-notes/.../**c03a4072becf2f44740ddafb97f2e529d0cd02b5** (pdfs → c03a407)](https://github.com/jdolson/hott-notes/tree/c03a4072becf2f44740ddafb97f2e529d0cd02b5/c03a4072becf2f44740ddafb97f2e529d0cd02b5) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dlc4l6o.)^.
Have you heard of tail call optimization?
&gt; To be entirely honest, I'm learning Haskell without really thinking about where I can use it in the future. I'm doing it because I'm bored and because it's interesting. That's a pretty good reason to learn things in general!
Once you get good at Haskell, most programming tasks regardless of scale become deterministic and easy to do. This includes tasks that include effects like DB access. Learning Haskell takes effort, but the effort pays off with the amount of complexity it allows even a single developer to scalably manage. The exception to this is if you're using Haskell for a domain without good library support yet. Luckily it's rewarding to write Haskell libraries for such things! But it does take time, effort, and taste; which means lack of libraries can potentially make Haskell a nonstarter. 
&gt; then Haskell would be practically worthless because (I've heard that) Haskell is extremely resource hungry (probably from all that recursion). Not actually true. Haskell isn't extremely resource hungry compared to other high-level languages, and Haskell code can potentially perform really well (idiomatic code often ends up within "twice as slow as C", and with enough effort, you can usually get even closer). There is some significant overhead due to the fact that GHC, the only viable industry-strength compiler, requires a rather heavy runtime, which adds to the runtime memory footprint and distribution size; but this is a once-per-binary cost. Also, the garbage collections can introduce performance issues, which is mainly a matter of prioritizing throughput over latency (i.e., GHC's runtime accepts occasional stop-the-world pauses for GC in order to maintain high throughput on average). For most applications, this is absolutely no problem. Also note that recursion isn't expensive per se, and rarely is in practice in a language like Haskell. Recursion is problematic if implemented naively with strict semantics, i.e., strict and without Tail Call Optimization (TCO). Haskell isn't strict, though, so this is a non-issue, even without TCO. That said, Haskell is not a great choice for number crunching (yet), because it lacks high-performance numeric computing libraries like Python or R have. It is still used a lot in academia, just not for number crunching. Its main academic uses lie elsewhere, particularly programming language research, automated proof research, etc., as well as teaching / illustrating functional programming principles. &gt; But if the language serves no practical purpose other than intuitive code and amazing readability, I don't understand why anyone should even learn Haskell. It serves the same purpose as any other general-purpose language. I've written all sorts of software in it: web servers, content-management systems, website compliance checkers, systems utilities, music notation software, a toy web browser, template engines, games, you name it.
Not really.
Tailcall optimization resolves this. Anything you can write as a loop, you can write as a tail recursive function and have literally exactly the same performance.
FWIW, Haskell is also quite high performance. It's no C, but it's usually on par with the likes of Java, and far faster than the likes of Python.
Thanks!
&gt; A real-life example is the Ivory language designed by Galois for writing hard real-time control software for UAVs. Another shoutout for low-level Haskell DSLs is [Lava](http://hackage.haskell.org/package/kansas-lava) which allows you to generate VHDL to program FPGAs. I've never used it myself, but I've always wanted an excuse to.
I interpreted the article as focusing on the domain, not solely for performance, but so it can adapt to changing requirements.
&gt; In short: good argument, could do without the needless hostility. Honestly, I think it's a bad argument for a good thesis. Plus hostility.
Isn't that what he immediately goes on to talk about? Specifying what parts of that you're unclear on might be more fruitful...
Python does quite well in machine learning and simulations even though it's not a performant language on its own. The trick is that performance critical code can be written in a different low-level language, and then be wrapped to be used in a high-level language. The (lack of) ecosystem (libraries, tutorials, users to answer questions on SO...) is a much more serious limiting factor. Actually, if you do need performance, Haskell can be optimized quite well. Much better than Python for sure, and at least good enough to [filter spam at Facebook](https://code.facebook.com/posts/745068642270222/fighting-spam-with-haskell/). (Recursion is pretty cheap in fact. The real trouble comes from memory allocations, which result in the GC running more often.) But anyway for a lot of software, "intuitive code and amazing readability" trumps sheer performance. For example, these applications are written in Haskell, and do not need to be particularly efficient: - xmonad: a window manager - pandoc: a document converter (markdown, TeX, etc. to HTML, PDF, etc.) You can also use Haskell to build websites (e.g., with [servant](https://haskell-servant.readthedocs.io/en/stable/)). You can apparently compile Haskell to JS. I've also heard some good things about Purescript and Elm, which are Haskell-like languages for the web. FP languages such as Haskell, OCaml, F# are excellent to write compilers, program analyzers, and similar tools. Haskell itself serves as a testbed for programming language research, so it's quite popular in this part of academia.
&gt; I'm also aware that there are good arguments against using libuv in the GHC RTS, though I unfortunately don't have enough experience with the debate to know what they are. Relevant: https://ghc.haskell.org/trac/ghc/ticket/8400.
I currently work on a job with some Haskell in it, and I have previously worked in places where understanding the code was a critical component of the job. Intuitive code and amazing readability come second to none in my priorities. This is often the case once you realize that you spend most of your time reading code rather than writing it, and that code often outlasts the coder, and moreso the coder's memory. That being said, I wouldn't say Haskell is inherently a better language to write clear code. It merely offers some tools which may help you do so, at the cost of some features other languages have.
I think Haskell is frequently used in academia as a language for investigating comp sci principles / questions, as opposed to a bulk data analysis. However, it can be used for that role as well, and is, just not as frequently as Python / Julia / R. The resource hungry criticism, as explained by others here, is a bit disingenuous. Space leaks can be a design challenge, but once the author achieves proficiency in the language, this becomes less an issue and more just a design consideration. Haskell gives you some really powerful tool to avoid excessive resource consumption, it just takes some thought to identify which tools you should use. Computationally, it is certainly superior to Python, and just like with Python, delegating to C for heavy lifting if necessary. However, using the C FFI in haskell does incurr some performance overhead, whereas in Python delegating to C is basically always a net performance win. Due to this phenomenon, Haskell gets a reputation for being less suited to the task of large scale analysis, but really, C libs are doing all the lifting at runtime for Python anyway, so this is more an ecosystem challenge than a language challenge.
Also memory usage is typically much better than Java, at least from the benchmarks I have seen.
Almost! You need to fit another `r` in there somewhere though :) 
A huge part of understanding day-to-day Haskell for me is having editor support--i.e. autocompletion, type info on hover, jump to definition, etc. (but those are the big three). You really need this kind of support in a type-heavy, type-inferred language like Haskell. You can find plenty of tools nowadays which give you this: Emacs with Intero, VSCode or IntelliJ IDEA with an appropriate Haskell extension; there are also other possibilities.
&gt; I have spent eight hours (and more disk space than I can afford) over the past two days trying every way I can find to turn these files into PDFs. I keep getting weird error messages that I can't fix or understand. If someone can tell me the best place to find help (I use Linux Mint--but again, barely understood enough to manage to install it) that would be great I suspect you'll need the packages * `make` * `texlive` * `lmodern` * `latexmk` and then you can just type `make`.
You are not telling me nothing new. What I don't understand is the part where you say that there is some PHP which will not be re-written. Again, back to my analogy, you are a 3 star Michelin restaurant chef but you still need to make MacD fries ... I understand the mentality of lets hire Haskellers (they tend to be really clever people) and make them do shit at us. That way they will not be doing really cool stuff at our competitors ... But that kind of mentality doesn't work. The only reason you probably getting people is because they need the payroll, but as soon as they get a better gig (more money) they are gone.
I'm glad the solution is so simple and effective, but i don't think this article improves on [the complaint of snide remarks](http://reddit.com/r/haskell/comments/6s5uh0/programming_as_if_the_domain_mattered/dlac0u7) =P
&gt; i don't think this article improves on the complaint of snide remarks Oh it doesn't, I just have an uncanny ability to mimic style.
Do I? Where?
Oh, this is great! And so fast, too! I've been like a boy on Christmas Eve here. Look at those beautiful diagrams! Thank you for doing this. This Eric Finster [one-off](https://www.youtube.com/watch?v=z3IBvmrcObg) is not a bad Chapter 0, either--certainly the best of the YouTube videos I've found so far. But after the Harper CMU vids I do think you will find your next trip into the HoTT book will be a very different experience!
Thanks! This does seem great, and simple enough even for me to use. But so far I can't seem to see how it can draw on multiple files, either uploaded from my computer or from Drive, which appears to be what needs to happen for the notes files to render correctly. That seemed to be a real sticking point for some of the other methods I tried, too. (Others were even more baffling.)
To return the `r` from the shorter stream.
This looks very much like the README example from [rank2classes](https://hackage.haskell.org/package/rank2classes). 
Haha great little post. As an aside, what I find so ironic about this entire discussion is that the domain is basically never a silly, simple, interview question. In fact, the domain of most practical problems is rather huge and takes many thousands of lines of code to capture. Those domains are rarely simplified to mathematical jargon, even in Haskell. When it comes to silly little algorithms like this one, most often the point is to get it done and get it right as fast as freaking possible.
Is it possible to download this Panopto videos series for offline viewing?
Don't have a ton of time and on mobile but I love it for ease of refactoring and maintainability.
FWIW, after installing the following packages on my Fedora 26 system, 'make' ran successfully. I guess other distributions will have similar names, YMMV. latexmk texlive-fixcmex texlive-cm texlive-hyphen-base texlive-tikz-cd texlive-amscls texlive-dashrule texlive-mathtools texlive-fancyhdr texlive-comment texlive-glossaries texlive-textcase texlive-mfware-bin texlive-bibtex-bin texlive-microtype texlive-updmap-map texlive-palatino texlive-mathpazo texlive-dvips
Oh, interesting. I wonder where that fits.
My follow-up: https://www.reddit.com/r/haskell/comments/6sf3lj/programming_as_if_the_correct_data_structure/?ref=share&amp;ref_source=link
&gt; lmodern Do you also have `lmodern`? If not, where does `lmodern.sty` come from. `make` is also needed, of course!
Thanks for covering for my absence /u/nh2_. Kevin gives a good overview indeed. 
Hey /u/nish2575. Sorry for the late reply. Kevin's comment linked by Niklas gives a good overview. For further info I suggest delving into the user manual of Tamarin. The current maintainers did a really great job there. 
How do you compile / launch the code with `par`? I'm compiling with `-O2 -threaded` and running with `+RTS -Nx` (with x between 1 and 8) and the parallel code is always slower.
Did you try disabling the parallel garbage collector?
 # dnf whatprovides */lmodern.sty Last metadata expiration check: 2:18:51 ago on Tue 08 Aug 2017 06:55:28 PM CEST. texlive-lm-6:svn28119.2.004-33.fc26.2.noarch : Latin modern fonts in outline formats Repo : @System texlive-lm-6:svn28119.2.004-33.fc26.2.noarch : Latin modern fonts in outline formats Repo : fedora I kinda expect 'make' to be available on any *nix system by default ;)
I'm just reporting what's written here https://gist.github.com/Porges/9ca15a9ec01bf055edcd88394496dbe3
I would use the second style if it didn't break colorization of `toList` in all the editors I tried.
I'm excited by the massive decrease in allocations, honestly. Between that, decreasing complexity, and the huge decrease in developer investment time required, this seems like the right choice so far. External dependencies make me wary, however. Even something big and stable like llvm can be a pain in the ass. Perhaps it might be good to talk a bit with the developers behind libuv and see where they're taking things and if there might be a way to accommodate GHCs unique needs a little easier?
Well hello there! nice to see someone else other than me is taking the time to learn Haskell :D I have to say that (about 6-7 months ago, when I started learning) I had the same impression. A bit of background: I'm 19(so not the most experienced programmer you'll find around) and even though most of my programming experience is either from school (C, Java, SQL) or from my free time (C#, F#) I've worked in production environments as a JS developer before - once as a node.js(+mongo) developer on a platform that sadly(luckily?) didn't take off, and the other time was as a front-end js developer(with a bit of PHP and SQL here and there) as an internship for a Welsh design agency(they shall go unnamed, even though i have nothing but good things to say about them). Even though I still knew a bit about functional programming(the basic JS stuff. closures, promises, map/filter/reduce) it wasn't even close to what Haskell taught me. So, let's start with what people already mostly told you: Haskell is really good at EDSLs. Like, really good. It's mostly a product of the powerful abstractions you can create, together with the clean syntax. Sigma (facebook's spam filter) is a perfect example: they have this [Haxl](https://github.com/facebook/Haxl) monad(applicative?) which does batching and caching of request, and also lets you concurrently create requests from different data sources. As if this wasn't enough, it's all hidden behind do notation, and it's so easy to use that they trained people to use it really quickly(I think how quickly is said in [this talk](https://www.youtube.com/watch?v=mlTO510zO78) but i'm not sure). Another thing about haskell is that it's pretty fast, but it is in a very *counterintuitive way*. Mostly because of laziness, which I don't think I'm qualified enough to explain. But the thing that really makes Haskell worth it in my mind is something i will shorten to PX: **Programmer Experience**. Haskell turns programming into such a pleasant experience(once you start appreciating the type-checker). Yes, you can still write bad code and yes, logic errors can still be there. But the type-checker catches so much stuff at compile time. Especially coming from languages like Javascript (where at times I've had an application run flawlessly during testing for days, and then... just as I'm about to push to master, a strange screenshot gets sent to me... of a terminal. And there's written...`callback is not a function`) where even simple errors(like swapping the arguments to a function) can turn into undebuggable nightmares. Haskell is also really exciting to use, especially when you start getting the hang of it. All these abstractions, solving problems in an elegant and efficient way. All these concepts which, after a while, click and then you suddenly start seeing them everywhere(like the first time you realize that JS Promises are actually Monads). And even though some stuff can get hard(i don't understand lenses yet, but i hear they can get pretty tricky) other stuff(which would be really difficult in other languages) is super easy. An example? For my high school graduation, I built a website which let you differentiate functions and show their representation. It included a Parser(you don't really want to manually type ASTs) built using Parsec, and automatic differentiation (i think i could've used the ad library, but i built my own as an exercise) which was built thanks to [beautiful differentiation](http://conal.net/blog/posts/beautiful-differentiation) and Tikhon Jelvis, who helped me when I was in trouble(and made me discover automatic differentiation and symbolic numbers in the first place, thanks to a Quora answer). The site itself was a simple HTML(Hamlet?) template(with chart.js to help with the visualization) served from a Yesod back-end(which worked beautifully). And I did this in about a month or so. I would've never even dreamed of building such a thing in any other language, especially not in so few lines of code. So, for me, the answer to the question is that **haskell makes me love programming**(even more than I already do) `
&gt; with large recursive stacks more memory would be used up This is actually an implementation detail. Some implementations of programming languages do not need to blow up the stack when doing recursion, or don't use a stack at all. How Haskell does it is a bit more complicated since Haskell evaluation model is "non strict", but i'll link a lecture that tries to explain the different kind of recursions in a strict language (scheme). logically, you can definitely write recursive functions that do not need more than O(1) space and many compilers can detect these situations which are well defined and optimize these cases away. Try watching this for a bit: https://www.youtube.com/watch?v=dlbMuv-jix8#t=19m15s
If by "practical" you mean that people can build programs that satisfy their or their customers needs then Haskell is practical because people can do (and do) that. There is a many domains where building systems with Haskell can be a good choice because the language and library ecosystem are a good fit for those domains, and GHC is an industrial strength compiler that can compile Haskell programs that do not fall short than many other very popular programming languages in terms of performance. 
I have trouble telling if `stack bench` is doing the right thing when benchmarking parallel code. Not a shot at `stack` - I'm just not sure what to do.
Guy Steele also gave an excellent talk on this problem. He gives overview of the imperative and parallel solutions to this problem. The parallel solution is in his Fortress language, but uses the same algebraic properties a Haskeller is comfortable with (e.g. Monoids) Plus, anything Guy Steele does is worth a watch. https://www.youtube.com/watch?v=ftcIcn8AmSY someone made a haskell version of the final solution https://gist.github.com/Teggy/1f525cf3027c92d9008d 
I've simply copy pasted the code from this gist. I got coherent results, except for the parallel strict one which is slower than the non parallel strict one. Surprisingly, the "lazy scanl" version is, as expected, roughly two time quicker using the parallel version.
I did nothing special. Do you suggest I should try to disable it? (Which is in some extend a hack).
About the simple and effective part: I think the point would've been much more clearly made using the [~0.1s version here](https://gist.github.com/Porges/9ca15a9ec01bf055edcd88394496dbe3). You see an identical writing of the original code, just with a different data structure. Contrast that with the final function which has two "pars", a lambda, and strict functions; all that fiddling and monkeying around for an additional 2x or so speedup? It seems like, to me, that the message sort of loses its potency when "just use a different data structure" is demonstrated as "completely change all of the code into something that looks almost unrecognizable as the original code." I liked the post overall, but that part just rubbed me the wrong way a bit as I had no idea how the code went from version A to version B without looking at the github code linked at the bottom. Edit: I also would've enjoyed a snarky comment about how benchmarking the "right thing" (ie, not including the random number generator) drops the time to 3.3s, but I digress.
Are you sure? It seems like I can still write an algorithm that's O(n^2) on the surface but actually O(n) due to some nontrivial fact.
It's a fair comment. I just wanted to write something snarky and let Porges's code (which appears in full on his Gist) do the talking.
&gt; just scoped by the surrounding `do` block instead of `in` I mean that is the difference they are talking about.
What a ridiculous statement, people are free to spend their brain cells on whatever they want.
It's true. It's a ridiculous statement.
I'm confused, wouldn't replacing Haskell with C code totally contradict your recently expressed ["destroy imperative programming, stop all software bugs"](https://www.fpcomplete.com/blog/2017/08/stack-issue-triagers) philosophy? After all, isn't this what your [sales pitch about Haskell](https://youtu.be/Fqi0Xu2Enaw?t=1m1s) for the benefits of using Haskell over alternatives is based on as well? One of the examples contrasted imperative code final int LIMIT = 50; int[] a = new int[LIMIT]; int[] b = new int[LIMIT - 5]; for (int i = 0; i &lt; LIMIT; i++) {a[i] = (i+1) * 2; if (i &gt;= 5) b[i - 5] = a[i];} to functional code let a = [2,4..100] let b = drop 5 a and [literally claimed this would leave "no room for bugs"](https://youtu.be/Fqi0Xu2Enaw?t=3m05s). EDIT: clarifications
The two `par`s might trick one into thinking that two things are being done in separate threads while the main one proceeds. In point of fact, only one of those ends up being done in parallel, because the results are demanded immediately. So it might as well be written with one `par` and one `pseq`. But then thinking a bit more about the problem, I have the feeling that this sort of parallelization is the wrong approach. Stream fusion does a fantastic job; `rainfall'` only builds one intermediate vector! So my best guess is that GHC's code generator could be doing better. Using the LLVM backend might make a difference here. Edit: I was a little vague there. If you're going to actually build `maxl` and `maxr`, then it makes good sense to spark a thread to build one of them in parallel (it doesn't make sense to spark two threads, because one will be demanded immediately). But stream fusion means that you only actually need to build *one* of those vectors; the other will fuse with `zipWith3` and `sum`. The amount of computation per vector element is utterly trivial, so it's hard to imagine that it really makes sense to allocate a vector in the main thread to have something to do while the child thread is running; fusion just makes more sense.
With terrain of size 10^6 I get % ghc -O2 -rtsopts -threaded foo.hs &amp;&amp; ./foo +RTS -K1G -N2 -RTS [1 of 1] Compiling Main ( foo.hs, foo.o ) Linking foo ... benchmarking rainfall time 24.90 ms (23.93 ms .. 26.09 ms) 0.991 R² (0.981 R² .. 0.996 R²) mean 18.83 ms (16.43 ms .. 20.40 ms) std dev 3.755 ms (1.531 ms .. 5.551 ms) variance introduced by outliers: 75% (severely inflated) benchmarking par rainfall time 17.90 ms (13.78 ms .. 21.12 ms) 0.872 R² (0.703 R² .. 0.981 R²) mean 14.36 ms (12.03 ms .. 15.85 ms) std dev 3.666 ms (2.709 ms .. 4.982 ms) variance introduced by outliers: 82% (severely inflated) benchmarking rainfall' time 13.06 ms (12.11 ms .. 14.00 ms) 0.986 R² (0.977 R² .. 0.997 R²) mean 9.434 ms (8.606 ms .. 10.27 ms) std dev 1.701 ms (1.276 ms .. 2.230 ms) variance introduced by outliers: 75% (severely inflated) benchmarking par rainfall' time 16.31 ms (13.24 ms .. 18.83 ms) 0.863 R² (0.732 R² .. 0.950 R²) mean 11.41 ms (9.634 ms .. 13.42 ms) std dev 3.818 ms (2.975 ms .. 4.468 ms) variance introduced by outliers: 89% (severely inflated) benchmarking original time 656.3 ms (548.1 ms .. 773.8 ms) 0.996 R² (0.986 R² .. 1.000 R²) mean 607.0 ms (568.9 ms .. 629.8 ms) std dev 34.84 ms (0.0 s .. 39.44 ms) variance introduced by outliers: 19% (moderately inflated) I didn't do anything to do with a parallel garbage collector. Interestingly the strict non-parallel version is fastest. No idea why. I'll say "fusion?" because that sounds cool. With terrain of size 10^7 it all goes weird % ghc -O2 -rtsopts -threaded foo.hs &amp;&amp; ./foo +RTS -K1G -N2 -RTS [1 of 1] Compiling Main ( foo.hs, foo.o ) Linking foo ... benchmarking rainfall time 185.9 ms (124.4 ms .. 228.5 ms) 0.988 R² (NaN R² .. 1.000 R²) foo: ./Data/Vector/Generic.hs:245 ((!)): index out of bounds (-2147483648,1000) 
That's consistent with my observation that strict, non parallel is fasteset: https://www.reddit.com/r/haskell/comments/6sf3lj/programming_as_if_the_correct_data_structure/dlcmm73/ 
Copy and paste in (terminal) Emacs. Typically what I do for this sort of thing is grab the characters I want to work with using C-x 8 RET (unicode character name) RET, and maybe bind them to macros as my “palette”.
I just tried and youtube-dl managed to download it. Just use `youtube-dl URL`.
That last error just screams "integer overflow" to me and looks like an issue with unboxed vectors combined with Int datatype. (In fact, it might not even be the unboxed vector so much as the Int datatype). Edit, it appears that index has a type of `(!) :: Unbox a =&gt; Vector a -&gt; Int -&gt; a` so the Int parameter could certainly overflow. Or perhaps the unsafe index was used?
I admire your application!
That definitely makes sense to me. To me, that would also count as another huge win for Haskell; a compiler smart enough to fuse irrelevant constructions of containers together to create code efficient enough that you don't even really need to "think" about explicit parallelizing the code.
Thanks, that worked perfectly!
Yeah it does, though I don't see what could overflow on a vector of size 10^7. This is on GHC 7.6 though, so it's likely to no longer be an issue. Why am I using GHC 7.6, I hear you ask? Well, I'm so committed to stability that I use Debian oldstable.
Hah, I hear you on that. I'm testing the code out on my laptop right now using 8.0.2 and I'll let you know how that goes :) Edit: Yup, 10^7 has no issues on my computer at all running 8.0.2; it must be an older bug somewhere.
Great post and great video series!
You can encode all polynomial time algorithms in the language and the number of section symbols in the type will be based on the polynomial time that it executes with _in this encoding_. That isn't to say it will necessarily give the optimal power bound, but that isn't what being P-complete means. There is a P-time embedding of a P-complete language into DLAL. P is an interesting complexity class because you're still allowed to spend P-time to convert things to get it in/out of the right form, this is why it is sensible across a wide array of machines, from Turing machines to more sensible models.
Haskell is well-suited to any domain where: 1. Performance matters. Haskell tends to be around as fast as Java, sometimes as fast as C. It's almost always faster than Python or Ruby, unless you're doing something really silly (trying to index into a linked list...) 2. Correctness matters. Haskell is the *best* mainstream language for writing correct code. Haskell's type system is powerful enough to conveniently forbid many bugs. QuickCheck and similar testing frameworks make it easy to write tests where types aren't convenient. 3. Maintenance matters. Haskell's type system and laziness make it extremely easy to write modular code that's easy to refactor and rearchitect with a high confidence that you'll be fine. Now, this isn't all domains: sometimes it makes sense to do an iterative prototype in Ruby or Python. Sometimes you need to be even faster than Haskell allows, and you drop to Rust/C++. Sometimes the script really doesn't matter at all and you bang it out in Perl/Bash/PHP. Sometimes you need to develop for the frontend and JavaScript makes more sense. But Haskell is well-suited to a variety of problem domains.
Right, I just slapped them on as an "it's really easy to parallelize stuff in Haskell, without thinking too much about it". It gave a speedup for the non-strict scan (for me), so that was good enough. FWIW: My gist isn't meant as contra the original author's argument, but to give them a 'level playing field' comparison across languages. (That said, I do think the original is somewhat misguided, there's no reason you can't write a 'simulator' version in Haskell.)
The argument for using domain knowledge in the original post is interesting. The whole UML modeling and object orientation community revolves around this idea. And it is intriguing on first sight. A thorough and intuitive model should lead to intuitive code which is easily maintainable. The problem is that that it favors local solutions against global solutions. Local solutions iterate a differential equation for each time step to get to a solution. Global solutions allow to calculate the solution in a closed form (Bartosz Milewski makes this point way better [in his blog](https://bartoszmilewski.com/2015/04/15/category-theory-and-declarative-programming/)). I think that functional programming tweaks the thinking more towards global solutions which may look more difficult on first sight but are more maintainable on the long run.
I think we need to make it a community policy that all benchmarks have to be tested with the LLVM backend =P It would help a lot in determining where it helps in reality.
all (flip elem tours) ? Sorry. Had to.
I code in Haskell professionally. I am not speaking for my employer, but from my point of view the reasons we use Haskell are as follows: * People who know Haskell well are highly likely to be good coders and good all-rounders, not because Haskell teaches them these skill, but because people with these skill are attracted to Haskell. * Its easier to write correct code in Haskell than just about any other production-ready language. * Haskell code (once you are experienced) is easy to read, easy to modify, easy test and easy to modify and extend. * With a little effort and experience, Haskell code performs pretty well. Its usually possible to get within a factor of 2x to 4x slower than a C implementation. * The Haskell language (especially the clean separation of pure and impure code) provides more opportunity for code reuse than most other languages. * Haskell's compilers, tools and libraries are exceptional, and we have extra tooling on top of that. * We use property based testing (Quickcheck, our own library and Hedgehog) extensively to rigorously test everything we write. Property based testing originated in the Haskell world and the Haskell implementations are probably the best implementations. That may not actually be all the reasons, but its the ones that immediately pops into my head.
The thing that scares me about Haskell is the amount of times I've made a working program by accident and I couldn't tell you how it actually works. 
Why don't these OO languages model their domains with generics? One would think that the whole point of generics would be to supply a general structure for algorithms that implementers of class instances could use by filling in the blanks. Is this just not seen as 'modeling the real world' or is there some weird thing about , say, C# or Java that makes this really hard to do?
&gt; The problem is that that it favors local solutions against global solutions. Local solutions iterate a differential equation for each time step to get to a solution. Global solutions allow to calculate the solution in a closed form Over the years I've come to be more impressed with those sorts of local solutions than global solutions. Global solutions are clever but they often *don't work at all* if the domain decides to add a complication that ruins your algebraic trickery. Local solutions will generally always work, they're just more hairy (Bret Victor makes this point way better [over here](http://worrydream.com/SimulationAsAPracticalTool/)). The lucky thing is that a functional language like Haskell is perfectly capable of doing this sort of simulation-style modeling; people just don't do it often because of the culture.
That’s a bit disingenuous. I put operators at the beginning of the line, but I never use alignment that depends on the size of things: f $ g &lt;&gt; x =&lt;&lt; y 
FYI, libuv is managed on github like many other open-sourced software. You can report bugs on main repo, ask questions on help repo, or discuss new ideas on [Libuv Enhancement Proposals](https://github.com/libuv/leps) repo. There's also a mailing list and an IRC room. There's no big organizations behind it AFAIK, the core team is pretty much listed here: https://github.com/orgs/libuv/people . In fact the project size may not be as that large as many people would think: ~100k LOC consist by lots of `#ifdef`s .
GHC 7.6, like all versions of GHC prior to 8.2, suffers from [Trac #13615](https://ghc.haskell.org/trac/ghc/ticket/13615) and therefore can't really be used reliably with parallel code. The time has finally come to upgrade.
You can also run :ctags in ghci to generate a tags file which can be used to find functions from your editor. 
He talks about Steele's lecture, but only about the first, impractical parallel solution. He completely ignores the practical one, based on ... wait for it ... *scans* (in the form of "parallel prefixes"). This is not an honest article.
Jerking tip: you've got to be more subtle like you were last time! Spelling it "Haskal" here just isn't going to work. Some of your previous comments here have been in exactly the sweet spot for trolling: believable, but absolutely confusing. Honestly, I think your best bet to successfully troll here is to keep posting insightful comments under this username, it's confusing the hell out of everyone. Good luck!
Indeed, that works too. I have the impression that most people prefer aligning words, that's why I gave that example.
a lot of ground has been covered in other replies but wanted to add - recursion is _not_ used that much by even beginner-intermediate haskellers. a big part of the issue is stop reading "learn you a haskell" and read Haskell Programming From First Principles haskellbook.com As a physicist, hopefully a pedagogy that focuses on foundational principles will resonate with your other education. at this point 'learn you a haskell' is counterproductive compared to other options and seeing beginner after beginner fall into the same traps and blocks is getting kind of frustrating.
I have tried it. My experiences were pretty borderline. The situations in which it helped were sort of a niche; it helps students who have bitten off more than they can chew, and then backed off but still have relics of broken code in their program. The advantage is not at all in helping them to fix the broken code, and entirely just in letting them ignore the problem while they get a simpler problem solved. I ended up reverting the change, in my case, but the reason was that the tools weren't really up to it, and the duplication of errors was confusing. I think I'd re-enable it if I had a better development environment for my students, which would simply promote the warning to an error in the UI rather than repeating it.
Agreed. This would be amazing. In order to keep the problem tractable, I think you'd just have to specify that a parse error invalidates the entire statement at the containing layout context. It can then be ignored if it's in a definition (`let` or `where`), replaced with `_|_` in a `do`, or replaced with `_ -&gt; _|_` in a `case`?
It would make sense as GHC does far more optimizations than Idris. On the other hand, GHCJS does not seem to be an official backend. Luite's efforts are impressive but how to be sure GHCJS will be kept up to date. When will GHCJS support GHC 8.2.1
C++ is the only mainstream imperative language that comes to mind in which people​ regularly abstract over algorithms statically in such a way that the code is both reusable and efficient (using templates over iterators). I feel like it may be *possible* in other languages, give or take some readability or efficiency, but it’s generally modeled in OO-land using runtime dispatch with virtual functions, just because that’s more consistent with the culture.
At the time Bryan O'Sullivan and I wrote MIO I considered using libev or libevent (which are similar to libuv). At the time I believe I was worried about overheads caused by calling back into Haskell from C callbacks (the RTS has to do extra work to make such callbacks safe). Perhaps this problem doesn't apply to libuv.
I changed to the strict sequential version since that had the fastest numbers anyway.
&gt; Using the LLVM backend might make a difference here. Actually, no ;) At least on my laptop, llvm backend is slower λ manta ~ → nix-shell -p llvm_37 -p 'haskellPackages.ghcWithPackages (h: [ h.vector h.criterion h.random ])' \ --run 'ghc -O2 -threaded Tower2.hs -fllvm -fforce-recomp &amp;&amp; ./Tower2 +RTS -N2' [1 of 1] Compiling Main ( Tower2.hs, Tower2.o ) Linking Tower2 ... benchmarking rainfall time 135.6 ms (134.6 ms .. 136.4 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 135.2 ms (134.8 ms .. 135.5 ms) std dev 482.2 μs (317.5 μs .. 730.8 μs) variance introduced by outliers: 11% (moderately inflated) benchmarking par rainfall time 74.03 ms (73.60 ms .. 74.32 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 74.33 ms (74.10 ms .. 74.57 ms) std dev 411.6 μs (319.7 μs .. 539.2 μs) benchmarking rainfall' time 64.92 ms (64.45 ms .. 65.54 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 64.94 ms (64.71 ms .. 65.37 ms) std dev 506.3 μs (226.1 μs .. 749.5 μs) benchmarking par rainfall' time 72.12 ms (71.49 ms .. 72.75 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 71.82 ms (71.66 ms .. 72.08 ms) std dev 329.5 μs (205.6 μs .. 495.4 μs) λ manta ~ → nix-shell -p 'haskellPackages.ghcWithPackages (h: [ h.vector h.criterion h.random ])' \ --run 'ghc -O2 -threaded Tower2.hs -fforce-recomp &amp;&amp; ./Tower2 +RTS -N2' [1 of 1] Compiling Main ( Tower2.hs, Tower2.o ) Linking Tower2 ... benchmarking rainfall time 131.8 ms (129.4 ms .. 134.0 ms) 1.000 R² (0.999 R² .. 1.000 R²) mean 130.4 ms (129.7 ms .. 131.4 ms) std dev 1.192 ms (263.9 μs .. 1.525 ms) variance introduced by outliers: 11% (moderately inflated) benchmarking par rainfall time 76.58 ms (75.65 ms .. 77.45 ms) 1.000 R² (0.999 R² .. 1.000 R²) mean 76.41 ms (76.06 ms .. 76.77 ms) std dev 578.5 μs (399.4 μs .. 750.1 μs) benchmarking rainfall' time 60.56 ms (60.35 ms .. 60.82 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 60.62 ms (60.50 ms .. 60.91 ms) std dev 314.8 μs (139.2 μs .. 483.9 μs) benchmarking par rainfall' time 75.31 ms (74.24 ms .. 76.21 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 75.91 ms (75.42 ms .. 76.95 ms) std dev 1.131 ms (377.7 μs .. 1.663 ms) 
Yes, the difference is that libuv provides a stepping mode which behave like an epoll_wait with 0 timeout. In stdio I provide predefined C callbacks so there's no need to call back into Haskell. I only need to collect events after uv_run finished. So there is no safe ffi, thus no new os threads.
Well, C++ has traditionally had the advantage of specialization templates. That is, you can define a generic template and then say "except for types that look like this, then the template becomes this instead". C# couldn't do this when I was actively using it (don't know today) and Java doesn't really have generics [1]. This specialization allowed very efficient implementations. [1] Java Generics are basically just compiler support for the casting you always had to do in Java. It does provide some safety, including specifying what interface or base class parameters must have, etc., but I'm sure the final VM code is exactly the same as it was, which has major implications if you want to safely do any kind of reflection.
Firstly, you mean `fmap` not `map`. Regarding performance, I doubt there's any difference. But you should never try to optimize for performance without doing a benchmark first anyway. Regarding readability, these things are often highly context dependent. I could offer my personal opinion but I don't think it would help.
I respectfully disagree with the point "Haskell tools are exceptional". In my experience, the ecosystem is still lacking. * The dedicated IDE is immature at best, the plugins for editors/IDEs (ST3, VSC, IntelliJ) are in different state of readiness (depending, among other things, on OS). None is remotely close to what VS/CLion/Idea/Eclipse are capable of. Sometimes these plugins clash with other standard plugins (eg. SublimeHaskell fot ST3 does not work well with GitGutter) * Even a simple "rename a function" can become difficult * Cryptic error messages both from the compiler and the libraries * The debugging is not really different from debugging with gdb (unless there is a better way than `ghci :step` that I overlooked). * To my knowledge, the incremental compilation/incremental linking still have some issues * "cabal-hell", "lts snaphots", "dependency resolving" anyone?
You can do something like Stream (Of (a, b)) m (r, Either (Stream (Of a) m r) (Stream (Of b) m r)) or smash them together in the end with some `f :: r -o r -o r`. The main issue is that sometimes you only _want_ the zipped stream, but then you still have an annoyingly linear infinite stream of garbage you don't care about, but can't discard. :&lt;
[removed]
I have to ask about your level of experience with Haskell, because to me, the above complaints seems to be from someone who is very used to something like Java or C# and is expecting exact replacements. &gt; * The dedicated IDE is immature at best My colleagues seem to use Emac or Vim. I use Geany (as a text editor) and *Linux* as my IDE. &gt; * Even a simple "rename a function" can become difficult I code on Linux. If I need to rename a function across a project, I suspect I can do it just as fast as you with `find` and `sed`. &gt; * Cryptic error messages both from the compiler and the libraries Here again I question your level of experience. I think in general the error messages are good and I rarely need to read the whole error message to figure what is wrong. &gt; * The debugging is not really different from debugging with gdb (unless there is a better way than ghci :step that I overlooked). I debug in the REPL, by printing types and running little tests. I have done a lot of debugging of C and C++ code in GDB and I don't need that for Haskell. When people want debuggers in Haskell its usually because they are caught in the trap of thinking about their code in an imperative rather than declarative manner. &gt; * To my knowledge, the incremental compilation/incremental linking still have some issues Would like more info, because this is not something I see. &gt; * "cabal-hell", "lts snaphots", "dependency resolving" anyone? Cabal hell has been totally fixed by cabal sandboxes and tools like Stack and [Mafia](https://github.com/ambiata/mafia). If you are complaining about "cabal hell" then your probably aren't using cabal sandboxes/Stack/mafia where dependency resolution is far less of a problem. I'm not trying to say you're not having problem, just that experienced Haskellers have overcome all of these or find that the productivity and beauty of the language make those problems seem insignificant. Or maybe its Stockholm Syndrome. 
&gt; "cabal-hell" "dependency resolving" anyone? That's a problem long solved by Stack and Stackage. Stack made version constraints and dependency solving unnecessary and so cabal hell was finally defeated
Regarding performance, I would think avoiding `f &lt;$&gt; g &lt;$&gt; h` when possible is advisable simply because it's going to traverse a potentially large data structure twice.
Good point. It won't *traverse* lazy structures twice, but it will always apply `fmap` twice. In any case this is the kind of thing that will be optimized by RULES, and I think one should always benchmark. I would always write for readability over performancew in the first instance.
I'm not sure about "f &lt;$&gt; g &lt;$&gt; h" because people have to remember the functor instance for functions. Sure, it's simple (just composition), but I wonder how commonly this functor instance is used in practice.
&lt;$&gt; associates from the left, so won't the functions get composed before the list is traversed?
Ah you're right. OK that's not so nice, a bit too clever for my taste.
As /u/haskellgr8 pointed out, this is actually equivalent because of the associativity of `&lt;$&gt;`.
Let's revisit this: no problems until you need to use the good ol' unrestricted monad in the same module... `-XRebindableSyntax` steals the do-notation, so for one of them you'll need to use qualified infix monadic operators which is... eh... dirty. Upcoming blog post on this one :) 
That's hilarious. I've never noticed that before.
`f &lt;$&gt; g &lt;$&gt; h` is equivalent to `(f &lt;$&gt; g) &lt;$&gt; h` because `&lt;$&gt;` has left associativity. For functions, `&lt;$&gt;` is defined as `(.)`, thus, the whole expression is equivalent to `(f . g) &lt;$&gt; h`. Then, this goes through `h` at most once. Now, if we talk about `f &lt;$&gt; (g &lt;$&gt; h)`, by definition of `&lt;$&gt;` is equivalent to `fmap f (fmap g h)` which is equivalent to `(fmap f . fmap g) h`. By the second functor law we have that this is equivalent to `fmap (f . g) h`. I assume that GHC is smart enough to realize this and not go through `h` twice.
First application will simply be the composition and the second will traverse the list, right? So, basically, `f&lt;$&gt;g&lt;$&gt;h` desugares right away to `fmap (f . g) h`?
I think it's a bit optimistic to expect GHC to optimize this. Keep in mind that the functor "laws" are merely things that are expected to hold. They're not enforced and there's no explicit warning that your code may silently and unpredictably break! Many of the common libraries have REWRITE rules that instruct GHC to do just what you suggested but they must be added by the library author, with the understanding that they know better than GHC.
Oh, don't question my level of experience, I'm just a hobbyist=) * *I suspect I can do it just as fast as you with `find` and `sed`.* I don't doubt that. Anyway, my point stands. You are just using a glorified "replace all" provided by your OS/editor, not scope-aware dedicated refactoring tools. When renaming a java class in Idea is only a `shift+f6` away, "replaceAll" for Haskell looks like a step back to 90s (and don't forget to rename module names in .cabal! =) ). * *I think in general the error messages are good and I rarely need to read the whole error message to figure what is wrong.* Good for you. I find debugging parsec errors when it consumed way too much input quite tedious. * *I debug in the REPL, by printing types and running little tests* Again, my point stands. Even with your experience with tooling you are reduced to "println debugging", which is (from a java developer point of view) is rather limiting * *Cabal hell has been totally fixed by cabal sandboxes and tools like Stack and Mafia.* I'd say it was not fixed, it was delayed, but my opinion is quite uneducated. In my pet projects I regularly see that the only resolver that matches the project is lts-nightly or whatever it's called. * *Or maybe its Stockholm Syndrome* That's why we are all still here =)
I really expected more than one person to get the joke. OP is openly asking for people to join the SIT.
This is precisely what I was looking for. I come from the Python world. Scientific calculations based on arrays are all developed on top of NumPy. Haskell will probably never have a single, quasi-mandatory array library like NumPy, and so I can't wait for the next posts in this series to map out the alternatives.
Yeah exactly. It's a trick!
I have a similar issue with GHC 8.2 : stack --resolver nightly-2017-07-31 ghc --package 'random' --package criterion --package parallel -- -O2 -rtsopts -threaded Tower2.hs -fforce-recomp &amp;&amp; ./Tower2 +RTS -N2 -K1G .... ... WTF variance ? variance introduced by outliers: -9223372036854775808% (severely inflated) benchmarking par rainfall' time 151.6 ms (100.8 ms .. 217.7 ms) 0.920 R² (0.884 R² .. 1.000 R²) Tower2: ./Data/Vector/Generic.hs:245 ((!)): index out of bounds (-9223372036854775808,1000) CallStack (from HasCallStack): error, called at ./Data/Vector/Internal/Check.hs:87:5 in vector-0.12.0.1-CnPH69pDwM4A5esizlXfXi:Data.Vector.Internal.Check Tower2: thread blocked indefinitely in an MVar operation 
&gt; Even a simple "rename a function" can become difficult I'd say it differently: Java refactoring tools made renaming functions extremely simple. You cannot do much more than that. Try to change something else, try composing classes differently, change functions calls, change data structures - and you end up doing all the same both in haskell and Java. However, since Haskell has much richer type ecosystem, doing these 'complex' refactors is actually a breeze - when you resolve all the compiler errors, you very often end up with well functioning code.
apply types to burnt area
No promises, but would you care to elaborate on what kind of applications/algorithms you are looking at implementing? (It is always good to know what people actually want to know! :)
Many Linux distros with package managers often don't install build tools automatically—I guess including `make`—or label them as "development tools", suggesting they aren't useful for non-developers. They sure aren't *designed to be usable* by non-developers.
I think the more important part of their comment was not the part involving functor laws, but the part about left associativity. `f &lt;$&gt; g &lt;$&gt; x` doesn't need GHC to be smart about the functor laws, since the left associativity makes this *literally* `f . g &lt;$&gt; x`. Their point about right associativity isn't very relevant since that will not be a common use.
I agree that modern tools for Java are quite powerful, no denying that. Yet take a "non-complex" refactoring - "extract method/variable". To my knowledge, none of Haskell plugins for IDEs is capable of that. In my toy projects I miss this functionality a lot. 
Interesting, didn't know! I can think of *one* reason: Haskell supports unboxed data structures. In Java, instead, each data structure (apart from primitives) must be allocated on its own, on the heap (*not the stack*), with a few words of "prelude" for use by the runtime; including it in a bigger structure needs a pointer. So that some Java data structures get easily 10x bigger than the contained data (http://ftp.barfooze.de/pub/sabotage/tmp/oopsla2007-bloat.pdf); Unboxed structures allow smaller overheads, though I haven't seen an apple-to-apple comparison across languages. EDIT: both Java and Haskell use a GC, which introduce other overheads for both—but I'm aware of no reason why Haskell's GC would use less memory (while keeping other things constant). In fact, tuning high-performance GCs is said to cost millions of dollars, and common Java GCs have undergone much more optimization.
This is so weird. You're seeing a 64 bit wrap around! How could this happen?
It is worth noting that even though left associativity ensures that won't happen, it is easy to put oneself in a position where it does happen. foo = f &lt;$&gt; x bar = g &lt;$&gt; foo Simply having these things in different definitions is enough to cause double traversal. Some functors are nice enough to have rewrite rules to allow this to fuse (when things are `INLINABLE`). But you can *force* it to fuse by using `Yoneda` or `Coyoneda` around your functor. The main downside is that lowering either of these incurs the same cost as the original `fmap`, meaning that lowering intermediate results can cause you to do the same traversal twice. foo = f &lt;$&gt; liftYoneda x y = lowerYoneda foo -- Ouch! bar = lowerYoneda (g &lt;$&gt; foo) In this example, the fact that we had to lower `foo` twice means we had to do both `fmap f x` and `fmap (g . f) x`, which is an obvious duplication of work. So this really only helps if you know duplicating `f` is cheaper than traversing an extra time, or if you know the result will probably only be lowered once.
I'm happy with how it is. This sub looks much better than any of the others I visit. Stuff is actually aligned in a coherent way and the colors make sense. Text sizes are still all over the board, though.
Why wouldn't it traverse the lazy structure twice (in a right associative usage)? It should still produce an intermediate structure that needs to be traversed, it's just that the intermediate structure never has to be fully materialized in memory, which helps when the structure is large enough for the GC to intervene and free parts that have already been traversed.
And the used index / bound is equal to the variance introduce by outliers ! Actually I suspect something weird with criterion in a `-threaded` context because I cannot repeat the same thing using a `forever` loop. I misinterpreted the "Generic.hs" vector error as an error from `rainfall`, but it may be an error from criterion.
Would be nice to see this with all dependencies compiled with LLVM too
The second style seems to work in VS Code just fine.
What's your interpretation of "traverse"? `fmap f (fmap g x)` on a list will indeed scrutinise each `:` twice, but the second time it scrutinises the first `:` is directly after the first time it scrutinises it. With a strict list the second time it scrutinises the first `:` is directly after it scrutinises the `[]`. The latter sounds more like "traversing twice" to me.
Yeah I wonder if that's the case, but on the other hand the problem only seems to occur with large world vectors.
One kind of relatively common(?) not-linear-algebra array type programming problem are e.g. (2D) Ising or X-Y models. That is, each array element represents some "thing", e.g. a spin, and then for each simulation iteration, each element interacts in some way with its nearest neighbors (+ usually periodic boundary conditions) (and maybe next-nearest neighbors etc.). These are quite straightforward to do in a low level language like Fortran, C, C++, though with a heavy dose of "indexitis". But due to the indexitis I'm not aware of any really satisfactory approach in a "wholemeal" style (you can do something like creating shifted copies of the array, albeit at a cost in memory usage and allocation rate). I've never done anything like this in Haskell, and would be interested to see how Haskell gurus would approach it. Just as an idle curiosity though, I'm not working on anything like this for real at the moment..
This is awesome. I wish more companies did this. It is funny to see the Haskellers overreach for too complex an algorithm. They will have to regroup and go simpler next time!
Whether contrast is enough depends on your display and eyes and it's possible your experience is good. I find the contrast too low compared to /r/ocaml/ and playing with my screen settings doesn't help. /r/rust's contrast is also lower than /r/ocaml but not as much. If everyone is fine with the contrast, I'll have to find a way via userContent.css, though I would like to see a higher contrast theme out of the box. Maybe there's a stylesheet to override all subreddits and have the default style that /r/ocaml uses.
I mean if you use Functors and Applicatives much it should be thoroughly baked in that `&lt;$&gt;`, `&lt;*&gt;`, `&lt;$`, `&lt;*`, and `*&gt;` are all left associative, specifically all `infixl 4`, as that is the only way for applicative style to work.
I prefer `f . g &lt;$&gt; h`: 1. For readability I prefer to use the most specific function that makes sense, so that it gives a little extra type-hint for the reader, which helps a lot in complicated overloaded expressions 2. I also prefer to use infix operators when they make sense so that the flow of reading doesn't get disrupted. I usually only use the prefix version when partially applying or passing as a function argument (ex: `traverse (fmap f) x` rather than `traverse (f &lt;$&gt;) x`). Some would disagree about point 2 and dislike operators in general as a personal preference, and that's fine, but I can fully recommend point 1 in every case.
Which text in particular bothers you? The main texts (comment body, title, user names) are good for me, and I know from past experience that I'm not on the high end of the Bell curve when it comes to contrast perception. Some of the minor UI elements are a bit light I suppose (e.g., "10 minutes ago"), but it's intended to push user attention to more important content.
**TL;DR:** The number of branches and allocations is the same, which represents the actual work being done in a traversal. The only difference is the necessary space usage, and GC pause time to some degree. I don't think that's a meaningful definition for "traverse." The amount of work being done in both cases is the same. For each `:` in the input list, we have to do case analysis twice, and we have to allocate two new `:`s, which increases GC pressure. This is 100% of the actual work being done, and it's double the necessary work in the absence of rewrite rules. The only time the laziness effects this is if the list is large enough that the GC can kick in and free intermediate `:`s while we continue to traverse. But this doesn't affect total allocations or the running time of the program (minus GC pause times, which I believe do get smaller). If the list is small, chances are the GC will not kick in and the space usage will be the same. But when the list is large, the GC can begin to free those extraneous intermediate `:`s, reducing pressure (and I think reducing the total pause time, though I'm guessing this isn't huge).
I don't think it will change a lot, because AFAIK most of the source code generation happen during the compilation of the final module (due to loop fusion and inlined functions inside `vector`). However I'll try that once I find the way to rebuild `vector` using llvm ;)
In the topic overview, title, and the following two lines. In a post, all text. So I guess any text doesn't stand out enough from the background as /r/ocaml does.
That sounds like a stencil convolution. Would that sort of structure work?
I really want to know whether the large overhead of the theoretically faster algorithm or 'Haskell overhead' was the major reason of the slowdown. To be fair, I *would* expect a published *asymptotically* faster solution to be much slower than a greedy/backtracking.
I would recommend installing some user CSS for reddit. I personally recommend the Stylish extension, grabbing a theme that meets your needs, then tweaking it. Me, I rather like the /r/haskell theme as-is. While you're at it, grab Greasemonkey or Tampermonkey and add some user scripts as well, given reddit's laughably poor UI (this is 2017, hitting the back button shouldn't mean getting stale notifications). 
&gt; I don't think that's a meaningful definition for "traverse." It's certainly a meaningful definition. The question is whether it aligns with other accepted definitions :) If you're interested in further discussing this semantic point consider `x = Stream (Of a) IO r` (from `streaming`). Does `fmap f (fmap g x)` traverse it once or twice?
It wouldn't necessarily be linear, but yes, it has that sort of local-and-translation-invariant flavour to it.
I use Haskell Syntax Highlighting plugin for VS Code and it doesn't work with the second style: it colors `toList` like ordinary text. What plugin do you use?
Yeah, it looks like pretty much everything inlines.
 , rsUnresolved :: ![Unresolved] FWIW this strict field is redundant. And if you're going for speed, why would you use a list here?
This is indeed a stencil pattern, but not necessarily a convolution (where the image is processed with a constant window of coefficients). In the general definition of a stencil, update rules may include highly non-linear operations. Please note that the naive Fortran/C/C++ implementation, consisting of a simple loop nest, performs very poorly because of bad locality. On reasonably-sized domains, a form of tiling (in both space and time) or cache-oblivious algorithm is required if you want good performance. The same goes for parallelization, because element-wise execution on multiple processors will result in a poor compute/IO ratio, as each element requires several inputs. You can get impressive performance speedups with such optimizations.
I don't have any trouble, but that doesn't mean we can't improve the situation for you and others. Could you alter the CSS using your browser tools and then post a before and after picture? Then we would have a concrete proposal and a better idea of what you're suggesting.
&gt; To be fair, I would expect a published asymptotically faster solution to be much slower than a greedy/backtracking. What do you mean? It sounds like your saying "I would expect the faster solution to be much slower..."? Do you mean the graphs they were coloring were not big enough for the better asymptotic performance to kick in?
Have you seen the stencil stuff in Repa? Not sure if it can work for this case, but it might.
This post would be a lot more helpful if it explained *why* an unboxed vector is the right data structure in this case, as opposed to a regular list. It would also redeem the author’s snarkiness, which is hardly justified when he appears to just repeat what someone else told him.
&gt; This post would be a lot more helpful if it explained why an unboxed vector is the right data structure in this case It surely would be more helpful to the general population. However, the aim of the post was not to be educational. The aim was to quickly shoot down someone spreading the view that Haskell is a slow language, to do it in such a way that discourages him from spreading such views in the future without due care, and to give a bit of a laugh to Haskell fans. NB someone who claims to be doing a performance analysis of algorithms really ought to take care to know when he's using a linked list versus an array. &gt; It would also redeem the author’s snarkiness, which is hardly justified when he appears to just repeat what someone else told him. Do you mean me? I don't understand. What am I just repeating?
I've been working a lot with image stacks (i.e. video). Our data processing in inherently parallel (multiple image stacks can be processed separately, then combined together at the end). Since the datasets are pretty large (~100GB), the images have to be loaded from disk, aligned, normalized, ..., and then averaged. This kind of stream processing seems perfectly suited to Haskell's strengths. 
Interesting, I've only just noticed this. Yes, most of the text is too faint. I'm not going to be able to stop noticing this now :(
I'm definitely a novice, but I remember trying to write image warping code in haskell that needed to do bi- and trilinear interpolation and I could not figure out where to begin. So for example, take a MxNx2 array representing a 2D vector field, and a PxQ scalar image, and generate an MxN image by interpolating the image at the locations given by the vector field. Performance is important, using GPU is advantageous. Is accelerate the way to go for such things? How does one write an efficient bilerp that could be used in an `A.generate` expression? In python/cython and pycuda, this is relatively straightforward, though of course there is a lot of legacy code to lean on there.
Definitely twice. Again, it's about the amount of work being done. The overhead of `fmap` there is allocating twice as many constructors and doing twice as much case analysis. The fact that you have to traverse the entire structure several times in right associative fmaps is the reason the free monad is slow, and `Stream f m` is exactly equivalent to `Free (Sum f m)`
Interesting. I doubt you are in the majority there. I suspect most people would reserve "traverse twice" for the situation where you execute the `IO` actions all over again.
&gt; External dependencies make me wary, however. Isn’t this only a problem if it’s GHC that needs additional dependencies, as opposed to if we were to move this stuff out of GHC? If this stuff were in a library instead, you could decide for yourself whether the extra dependencies are worth it and, if not, choose one with fewer dependencies (and possibly slower execution).
I'm sorry.
Haha, that's OK! Ironically, I had just stopped noticing it :) Now I'm going to go back to the state of mind when it all seemed fine :) 
Yess I have been hoping for something like this, the effort to define all these `Verified*` instances is way too prohibitive.. Reusing `GHC.Generics`, being able to derive it is amazing
That's just a matter of context. When you're thinking about the overhead of `fmap`, "traverse" means something different than when you're thinking about stream consumption. Regardless, with data structures like lists, I think traversal is about observing the constructors, not about the time at which you observe them. So the fact that `fmap f (fmap g x)` only observes the `[]`s simultaneously doesn't change that there were multiple `[]`s observed.
What is that list used for? Or are you implying that lists are generally slow? If they're using it for constant time cons-ing, and only ever need to walk the list in full, linked lists are very good. If they need `++` or random access, yea it'll be pretty bad.
You're right. I jumped to the conclusion that it's not used merely as a stack, or to be walked linearly. Indeed the comment says "Stack of variables that have not been resolved". Furthermore, I'm not even sure this field is used! https://github.com/cheftako/LinkedInThreeColorability/search?utf8=%E2%9C%93&amp;q=rsUnresolved&amp;type= In any case, making the field strict is close to useless.
The foreground to background [contrast ratio](http://webaim.org/resources/contrastchecker/?fcolor=4d5763&amp;bcolor=f0f3fc) is 6.62:1. That website recommends a contrast ratio of at least 7:1, so /r/haskell's is borderline. For more information see [here](https://www.w3.org/TR/UNDERSTANDING-WCAG20/visual-audio-contrast-contrast.html).
&gt; [W]e have to do case analysis twice, and we have to allocate two new :s, which increases GC pressure. This is 100% of the actual work being done[...] A piece missing from your analysis is memory access / cache. If you're operating on data you've just generated, you don't need to read it from memory again.
What do you think about massiv (https://github.com/lehins/massiv)? 
&gt; I would expect the faster solution to be much slower Yes! &gt; Do you mean the graphs they were coloring were not big enough for the better asymptotic performance to kick in? That's one. I mainly expect typical run times of the 'faster' algorithm to be quite a bit closer to the upper bound, and the less smart ones can enjoy little optimisations that go everywhere and add up to significant amounts. I haven't really carefully studied the problem, but for example, some heuristical greedy cases can really help with the solution, *but* it is possible that they will never kick in, so they don't help with the bounds.
Speculation: Backtracking with heuristics? Very helpful! 'Davis-Putnam-style backtracking with more sophisticated matching and network flow based ideas.'? Meh.
Good point. How does the large amount of indirection caused by linked lists affect this?
fgl is quite slow for big graphs, might be one of the slowness &amp; resource hungry contributor.
Is there any documentation or an introduction somewhere? It's hard to see what it's about.
I would also say it's a bit fainter than I'd like. The textbox for writing new comments is even worse.
I really wonder how a translation to SAT and using a state-of-the-art SAT solver would have fared. That would have been my preferred "Haskell" solution. 
[This Post on Quora](https://www.quora.com/A-friend-is-raving-about-how-Haskell-has-helped-his-work-flow-productivity-What-does-Haskell-offer-that-differentiates-it-from-the-other-popular-languages-particularly-in-back-end-for-web-and-mobile-development-What-is-Haskell-notably-good-for) covers a good summary what areas are really good in Haskell
Well one of the benefits of this would be to reduce the effort required by GHC devs to keep developing GHC. Adding even more work by supporting two separate solutions would be even worse than just keeping GHCs own solution.
Zero documentation??
This was also written in 2011. I'd assume that factors into the code a bit.
Asymptotically faster doesn't mean faster, it just means that for some n it will be faster, but that n may be very large.
LYAH is using a non-transformer variant of the state monad for simplicity's sake. If you're up to it work on the instance for newtype StateT s m a = StateT { runStateT :: s -&gt; m (a, s) } instance Monad m =&gt; Monad (StateT s m) where ...
I also use Haskell Syntax Highlighting, v2.3.1. For me, `toList` is white, whichever of the two ways I format it. The `::` and `Stuff` are always blue. The arrow is blue the first way and white the second way. The `[a]` is white both ways. It's possible it does break it and I just didn't notice or maybe my highlighting is just broken and I never noticed. What would you expect to see?
I disagree. I like the existing `Monoid` instance for `-&gt;`. /u/tekmo gets some mileage out of this instance in [Equational Reasoning at Scale](http://www.haskellforall.com/2014/07/equational-reasoning-at-scale.html).
I mean I get where you are coming from. But OP is talking about web development. Scotty, Yesod, Servant and Snap all have fantastic documentation, both internally and in terms of tons of outside tutorials and examples.
It is really useful when writing custom Ord instances.
I use Flatland Monokai theme which colorizes function names in signatures in green. http://imgur.com/a/XCJzF 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/C0xj3W6.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dle8dp1) 
I'm ambivalent about the Right Choice, tabula rasa. But you can't get there from here.
I mean `instance Monoid (a -&gt; a)` isn't valid Haskell 98 or Haskell 2010. It requires `FlexibleInstances`. Even if it were people would not expect a non-standard (flexible instance requiring) instance for a built in data type on a built in class. Since there exists a non trivial compliant instance. I personally think "lifted" instances of various type classes for things like functions are cool. I'd love it if everything were implemented more like `SubHask` with lifted `Eq`, `Ord` and group and ring like typeclasses. It's like a form of lateral function composition. So `foo x = bar x &lt;&gt; baz x` becomes `foo = bar &lt;&gt; baz` and the same could be done for things like `pred x = x &gt; 5 &amp;&amp; x &lt; 100` becoming `pred = (&gt; 5) &amp;&amp; (&lt; 100)`. Although this does require language extensions, the difference with this being that it isn't possible to implement a standard instance of `Eq (a -&gt; b)` or similar at all, and non-standard &gt;&gt;&gt; nothing at all.
That is a pretty strong usage of the word "wrong." "Wrong" would tend to imply it violates the monoid laws or something. This is "inconvenient" at best. Anyway, the instance as you have it requires `FlexibleInstances`, or at least a type equality constraint. So either way it's going to have worse type inference and terrible error messages compared to the current instance. Not to mention, making a backward-incompatible change is far more annoying than sprinkling `Endo` and `appEndo` around. Having `WrappedApplicative` around somewhere sounds like a good idea though. You could give it a `Num` instance and the wrapped functor instance as well: instance (Num a, Applicative f) =&gt; Num (WrappedApplicative f a) where a + b = (+) &lt;$&gt; a &lt;*&gt; b ... instance Applicative f =&gt; Functor (WrappedApplicative f) where fmap f a = pure f &lt;*&gt; a
You realize there are functions in the python base library and in plenty of extremely well established python libraries such as pandas and numpy and django, that do exactly that all over the place. This isn't some easily avoidable problem.
Why would Rust be more palatable? Just because it's less functional and pure and such?
Both of the last two companies I worked at allowed me to do Haskell on the job, it's not impossible if you make a good argument for it and if it doesn't make the rest of your team's life more difficult.
Like what?
Functional programming in Python sounds like a recipe for ridiculously slow code. Also if you write Python like Haskell it seems like all you have given yourself is an extremely crappy version of Haskell. May as well just use Haskell instead unless you have a really good reason not to.
I am a Python dev first and foremost, with fluency in Ruby and a good grasp of several other languages. Here are the first three things that come to mind for me: Haskell has an excellent type system. Exactly *why* that's a good thing is very difficult to explain without knowing your level of understanding of how other languages do the things that Haskell's type system empower, but rest assured, it is one of the central benefits of the language. Haskell is a compiled language. That means that I can write a utility on my Mac, compile it to a binary, and send that file off to someone on the other side of the world and it will run. The only requirement is that the binary be compatible with their system's architecture. Compare that to Python, where that same utility would need a Python interpreter to be installed on the target system, along with all library dependencies - oh, and those dependencies must have the correct version, and multiple version cannot be installed in a given environment simultaneously. Haskell embraces [tacit/"point-free" programming](https://en.wikipedia.org/wiki/Tacit_programming). This allows you to write utilities that operate in much the same way as most *nix utilities, which can operate on an input stream line-by-line that is "piped" to it from the output of another utility.
**Tacit programming** Tacit programming, also called point-free style, is a programming paradigm in which function definitions do not identify the arguments (or "points") on which they operate. Instead the definitions merely compose other functions, among which are combinators that manipulate the arguments. Tacit programming is of theoretical interest, because the strict use of composition results in programs that are well adapted for equational reasoning. It is also the natural style of certain programming languages, including APL and its derivatives, and concatenative languages such as Forth. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
From experience I can say that "easiness" does NOT scale, not to mention the performance and refactorability are just all around garbage.
I'd be curious to see the examples from `django`. `pandas` and `numpy` suffer from having a lot of code that I think of as 'scientist' code rather than 'engineer' code.
I have spent a lot more time coding Python and understand it fairly completely, I still think it doesn't hold a candle to Haskell.
the relatively unknown (but extremely fast) `massiv` library got some support for this as well: https://github.com/lehins/massiv/tree/master/massiv/src/Data/Array/Massiv/Stencil Also it's the library powering the `hip` Haskell image processing library: https://github.com/lehins/hip
I do both Haskell and Rust in my own time, and I talk about them with people I work with. In my experience, people have far less fear of Rust than they do of Haskell.
That wasn't what you said though. You said that no one you work with would return a function that returns two different types, and that doing so was bad programming, all those libraries have exactly such functions. So by your definition those libraries (and oh so many many many more) are badly programmed, thus the whole Python ecosystem is composed of bad libraries, thus Python is not good.
It's still a work in progress, with quite a lot of experimentation going on. There's plenty of haddock, but a README (to start with) is on the TODO list...
Python is generally a slow language but there is no reason to conclude functional programming in Python should be "ridiculously slow". Most functional paradigms already exist in Python (list comprehensions, map, reduce, functions are first class objects, etc.) it's just up to the programmwer whether they should use them. Additionally, writing functions which have no side effects is trivial (don't make use of variables that are not particular to the scope of that function and don't mutate data within the function). Obviously, in an ideal world, we'd all write Haskell all the time. But, with the constraints that are put on us by our working environments, sometimes we have to write code in other languages. Taking what you've learned in Haskell and applying it to other languages can help you to right better, safer code in that language.
That's strange, Rust is significantly more strict and limiting about how you code with things like the borrow checker. It's probably just because of `Monad` and friends scaring people off unnecessarily, not 100% sure though.
Well it's just that recursion in Python in general is slow as shit, and TCO / guarded recursion both do not exist. So you HAVE to use some sort of imperative code (for loops and similar) under the covers to get semi-decent performance. Basically everything in Haskell is built up from recursion. It's just you don't have to deal with it explicitly since the higher order function or whatever does it for you. That would never work in Python.
&gt; So by your definition those libraries (and oh so many many many more) are badly programmed Well, they're not great. The scientific python community as a whole is heavily skewed towards scientists who may not be the best software engineers. &gt; thus the whole Python ecosystem is composed of bad libraries That's a bit of a leap. I can find scores of libraries in any language that aren't really fantastic. I hope you're not going to argue that all of the libraries on Hackage are of a consistent and high quality! Like for any language code quality varies a good bit from library to library. Many Python libraries are excellent. Some are less good. The scientific libraries in particular do fine for what they are, but aren't the best written. &gt; thus Python is not good I get to this and I wonder if you're just trolling me. To formalize your argument a little: Some well known libraries aren't great therefore the ecosystem is composed of bad libraries therefore the language is bad. Each step is obviously not a reasonable inference. The overall argument is just absurd.
If you are used to object-oriented or procedural programming, Rust looks a lot more familiar.
Yeah that makes sense. I wish FP was taught more in college / high school.
Working with mutable graph data structures, determining the runtime complexity of certain algorithms, working with strings in a single standardized way, and grokking some of the impenetrable documentation quickly.
I mean Haskell libraries at least almost always don't have side effects outside of `IO` and don't change the result type based on input values, and so on and so forth. &gt; Some well known libraries aren't great &gt; therefore the ecosystem is composed of bad libraries &gt; therefore the language is bad. Not quite: By your definition the vast majority of libraries are badly programmed (you did say that changing input values -&gt; changing output types = bad programming no?) Therefore the ecosystem and language as a whole is bad. It's an aggressive statement but it follows logically.
It is still in the experimental/development stage, so documentation is coming. :)
&gt; Again, my point stands. Even with your experience with tooling you are reduced to "println debugging", which is (from a java developer point of view) is rather limiting Sorry, working in the REPL is nothing like println debugging. 
That's the goal that it will powering `hip`, but for now `hip` uses `vector` and `repa` for computation. Once it's done though `hip` will get a serious performance boost and a better interface :)
&gt; It's an aggressive statement but it follows logically. So I'm thinking logic may not be your strong suit. The necessary assumptions to make your argument follow logically are The vast majority of python libraries are badly programmed The vast majority of libraries being badly programmed -&gt; the ecosystem is bad The ecosystem is bad -&gt; the language as a whole is bad. Each assumption is questionable. Let's go through them: The vast majority of python libraries are badly programmed First, I think functions that return multiple types are much rarer than you seem to think. That's obviously a factual question, but it's certainly my impression. If you want to test it with your proposed example of `django` we could start going through functions and see how many return multiple types. I wouldn't be shocked if there were an example or two in the large (and old) code base, but if they exist they'll be few and far between. I work with the `django` source code daily, and have read large fractions of it I feel pretty confident that you're not going to be able to produce a ton of examples there. &gt; (you did say that changing input values -&gt; changing output types = bad programming no?) Sure. You're making another unwarranted assumption here though: any bad programming in a library -&gt; the library is bad I'm generally willing to forgive a few imperfections before I declare a library bad. So I don't accept that the vast majority of libraries are badly programmed. Actually, I feel that the code quality is pretty good outside the scientific python community. Since we're talking logic though, lets hit the other assumptions: The vast majority of libraries being badly programmed -&gt; the ecosystem is bad It's easy to imagine a language that has a million terrible libraries, and a few thousand great libraries where the ecosystem is still fantastic (assuming a good way to find the good libraries exists). So this is obviously not necessarily true. It's probably true that most languages with mostly bad libraries are bad, but it definitely doesn't follow logically. Last assumption: The ecosystem is bad -&gt; the language as a whole is bad. This one may be true, but at least for the sake of argument, it's possible to imagine a language which is so good on other fronts that the lack of ecosystem doesn't render the language bad. So... yeah...
Think about code that you have written that has needed to handle a large variety of possible input, or where you never felt like you could write enough test cases to have deep confidence in it. That's when you need Haskell. There are a lot of other great use cases, but that one stands out to me.
Read this: https://www.reddit.com/r/haskell/comments/6segl8/why_should_anyone_use_haskell/ ... and then make up your own mind.
&gt; Working with mutable graph data structures Fgl is one of several approaches that make this quite nice. &gt; determining the runtime complexity of certain algorithms I never really found that particularly hard. It also depends a lot on how you design your algorithm, some are easier to analyze than others. &gt; working with strings in a single standardized way `IsString`, `Monoid`, `string-conv` &gt; and grokking some of the impenetrable documentation quickly. Generally haven't found that to be much of a problem.
Here's a [short video about what's so great about Haskell and why people like it so much](https://youtu.be/Fqi0Xu2Enaw).
I think instances like that `Num` instance are best avoided, as in general they won't be able to preserve many of the arithmetical laws we expect to hold. For example, we would expect `x + y - y` to equal `x`, but consider: xs (.+) ys = (+) &lt;$&gt; xs &lt;*&gt; ys xs (.-) ys = (-) &lt;$&gt; xs &lt;*&gt; ys [1,2] .+ [3,4] .- [3,4] -- [1,0,2,1,2,1,3,2]
As a Brazilian, I dislike this ratio. /joke
Alright fine I'll admit it. I was being hyperbolic to support my view. I do think there really are a large number of functions that do that. The main situation I can think of where that stuff happens is flags like whether or not to flatten the result or which field(s) to keep, or which axis to operate on. Also something as simple as a getting function on a dict falls under the above if the dict isn't homogenous, which oh so many ducts aren't in Python. That's just what I could think of in a few minutes on my phone without googling anything. And I don't use Python all that much these days. 
i do apologize for missing this post - as a secondary question - are the older haskell books still useful or have there been too many changes to the compiler and toolchain in general that makes them definitely obsolete?
Real World Haskell is decent, but you should be going through [The Haskell Book](http://haskellbook.com/) instead.
Depends on what you want to learn. I'm sure you'll encounter some minor problems with older books, like module imports being out of date, or similar things. I think RWH may be especially prone to "decay" since it's quite a bit on the practical/optimization side (AIUI). Other than that I can't really comment -- didn't learn from any books. I'm not sure exactly how I did it, but I somehow moved from O'Caml to Haskell. FWIW, I hear good things about the book /u/fosskers mentions.
Interesting, but missing commentary on the relation of this to r/haskell?
I see that LYAH is claiming that `Control.Monad.State` defines a data type called `State`, and then LYAH gives you the definition of this data type. Well, that's a lie! The true definition is much more complicated, so I can see why they gave this simplified definition, but the consequence is that their example code doesn't compile when you import the real `Control.Monad.State`. That's quite confusing, it would have probably been easier if LYAH didn't mention `Control.Monad.State` at all! &gt; Control.Monad.State does not expose its value constructor The real `Control.Monad.State` does not expose a value constructor `State`, but that's not because its authors chose to keep this value constructor private, it's because `Control.Monad.State` uses a fancier implementation called a "monad transformer" which doesn't even have a value constructor called `State`. &gt; And shouldn't (State h) be (state h) ... `State h` only makes sense with the LYAH definition, while `state h` only makes sense with the `Control.Monad.State` definition! For short, in the following I will use the `C.` prefix for the definitions you get when you import `Control.Monad.State`, and the `L.` prefix for the definitions you would use with the definition of `State` which is given in LYAH. So, there is a function C.state :: (s -&gt; (a, s)) -&gt; C.State s a which you can use to construct a `C.State` value in the same way in which you would use the `L.State` value constructor to construct an `L.State` value. For example, `C.get` is implemented as `C.state (\s -&gt; (s, s))` while `L.get` is implemented as `L.State (\s -&gt; (s, s))`. In this case, however, `(L.State h) &gt;&gt;= f = ...` is pattern-matching on the `L.State` value constructor. Replacing it with `(C.state h) &gt;&gt;= f = ...` would not make sense, since we cannot pattern match on a function application to reconstruct the original function and its argument! For example, `succ 42` is 43, but I can't write `let f x = 43 in ...` to obtain `f = succ` and `x = 42`; after all, 43 could just as well have been constructed via `pred 44`, or in a number of other ways. &gt; ...as we (sic) it seems as though we are warping (sic) a stateful computation h. Hmm, I think this might indicate a major misunderstanding. Could you please explain, in your own words, what is a stateful computation and why `state h` would make more sense to you than `State h` if `h` was a stateful computation? &gt; Finally, the 's' in (State s) is a type variable, but the 's' in the lambda is a value. Correct? Correct. And the `h` in `State h` is a function whose type has the form `s -&gt; (a, s)`.
it's type scale
As a german, I appreciate (Actually, I've no opinion I this, but I got the joke)
For what it's worth, State and StateT actually used to be different types. I think they were unified about the same time as LYAH was written. There's a good chance that actually was the definition of State for at least the first draft of that chapter. 
This code seems to imply that DPH is a dead dream. But SPJ seems to intimate in a few different talks that DPH is syntax (but not performance) ready. When trying to get DPH on my GHC, I have failed (new vector library internals broke the DPH examples package the haskell wiki points to). Is DPH dead? Could a clever student resurrect it? That list comprehension syntax seems so damn attractive, it'd be a shame if it were never realized
DPH is basically dead, in that the main people that once worked on it now work on Accelerate instead.
A theoretically faster algorithm might never shortcut. In this case, they mention the C++ algorithm searching for chrome wheels. A trivial example might be `product`. Contrast: productAsymptotic = product -- the normal product productFast xs = if any (==0) xs then 0 else product xs This fast algorithm would be worse, clearly, but may do well on 5000 random `Word8`s. 
That's a good example, thanks. Repa stencils ought to be an option here as well.
How does it differ from, say, Repa?
Good bot
Thank you dramforever for voting on WikiTextBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). ^Even ^if ^I ^don't ^reply ^to ^your ^comment ^, ^I'm ^still ^listening ^for ^votes. ^Check ^the ^webpage ^to ^see ^if ^your ^vote ^registered!
It's definitely recommended to get a new book, but most old books' code samples would still work, but some changes can be non-obvious. Join a community (like #haskell on freenode) and ask, and you would be fine.
Another contrast from python (which I spend half my time in) is your ability to update code. If you want to rename some key data point in your financial analysis from `coupon-rate` to `interest-rate`, it is very difficult to do this in Python. You might hope you would notice with some runtime failure when you execute your script, but what if one of your functions is `def f(x): return x.get("coupon-rate",0)`. You might feel like this is not the correct Python, or that you could grep your code on update, but this is fairly idiomatic for being"analyst level" Python, and it honestly feels simply impossible to reliably update your code. Contrasting our Python and Haskell codebases at work, we have probably 50 percent of python scripts in deep disrepair, compared to maybe 4 of our 100+ Haskell libraries. When I went to add authentication to our Mongo accessing libraries, I changed the signature of `withMongoConnection`, and GHC held my hand as I updated our libraries. This sort of continuously integrated code is the selling point of Haskell to me. Haskell is about exposing as much information to the compiler as possible. 
Yes, there is a range of options for stream processing. However, this makes this a significantly more complex problem than "just" array programming à la NumPy. (That's of course perfectly fine, but just makes it harder to cover in a blog post ;) Does Python have standard options for this type of streaming? BTW, here is a nice recent article on one approach to streaming in Haskell, in case you haven't seen that yet: http://www.haskellforall.com/2013/09/perfect-streaming-using-pipes-bytestring.html
&gt; Haskell supports unboxed data structures ...do they?
Yes. I mean https://downloads.haskell.org/~ghc/8.2.1/docs/html/users_guide/glasgow_exts.html#unboxed-types-and-primitive-operations. Not sure I can parse your question.
I didn't know about `hip`. It looks great! Looking forward to the improvements.
It looks interesting, but there is no write up about it yet that I could find, at least.
DPH essentially ran out of money. Its main funding came from Microsoft Research and the Australian government, for which we are very grateful. As a research project, I like to think, it was very successful. It gave us things like type families, vector, Repa, and Accelerate. However, we definitely underestimated the amount of effort needed to realise full-scale nested data parallelism in Haskell. The syntax and similar is not the problem, that is fine. The issue is **reliably** getting high performance for very dynamic algorithms. I still think, flattening is the right way to go, but the problem with going for a very expressive language in high-performance with one big leap is that you either win or you don't get anything. That was a strategic mistake. As @kamatsu indicated, Accelerate is very much the successor project for us. There we started with a language of rather limited expressiveness, but where we can much easier control performance and work our way up. In fact, our Haskell Symposium paper this year, takes a first step towards re-introducing nesting and the flattening transform: http://www.cse.unsw.edu.au/~chak/papers/CMCK17.html (We still like to win ;)
Safety is the definite king of reasons. The type system in this language is beyond first class. It's also phenomenally good at writing parsers. Ease of authorship is pretty subjective, I personally find it much easier to think in terms of plugging inputs to outputs in a functional fashion, so after the initial learning curve, I find the day to day authorship experience to be very pleasant and rewarding. But it's a hell of a learning curve for some folks, and opinions will vary. I find, generally, that code I write in haskell has a drastically higher performance ceiling than code I write in other languages. That's somewhat subjective, and is probably influenced strongly by my aptitudes as a programmer. In general, code I write lazily on a first pass just to get the behavior down probably isn't as performant as (insert popular OO languages), but that code gets written and re-factored pretty quickly. Relatedly, on a less subjective note, the maintenance experience in this language is incredible. It exceeds my capacity for description, there is simply nothing I have found that comes close to matching it. Refactoring or rearchitecting existing code, or adding entirely new functionality, is incredibly painless compared to any language I've ever worked with. Reportedly, tooling and IDEs are pretty sparse. I find spacemacs to be a perfectly functional Dev experience out of the box, but I've never much cared for IDEs anyway, so take that as you will. Cross platform development is a pain in the ass. Avoid it if possible. 
&gt; worse, clearly But, not asymptotically.
the first thing i'd reach for in python would be generators: https://wiki.python.org/moin/Generators seems like a distant cousin to streaming libraries in haskell but since there's no formalities for IO in python generators just capture the lazy on-demand aspect of streaming.
I think that too. VCS is a really important reason to support 2nd option.
I am unsure, but has the community moved towards avoiding lawless typeclasses? If so, what is the preferred alternative recommendation? Create a record type with fields that correspond to the functions in the typeclass?
Oh a metabot!
A yogi finally releasing the secrets of levitation ;-)
Doesn't -- | work? Oh it doesn't. How inconsistent. Fortunately I use operators before so I don't have that problem, but if I were married to operators after I might see about getting haddock to support -- | in type signatures! And while I'm here, I do: function :: A -&gt; B -- until column 80 -&gt; C -&gt; D -&gt; etc. Works with both grep and haddock, though I usually use tags.
&gt; has the community moved towards avoiding lawless typeclasses I do not really know why this is asked for. Sure, `Show` has a law (`read . show` identity), but how does that make it more suitable to having a typeclass than `Pretty` ?
Another is [TidalCycles](http://tidalcycles.org) used for livecoding music performances.
For me the article resonates with what I have observed in the haskell ecosystem, where there has been significant growth, and there is now the inevitable tension between the needs of the now broader base of users, and the earlier users. It is good to see that this is a fairly universal experience, and not necessarily bad, so long as it does not get out of hand. I do not believe it has got out of hand for haskell.
Perhaps a naive question, but: you call out how it deals with character encoding. I'm assuming this means that (for example) on a Linux system using a non-UTF-8 character encoding, it translates UTF-8 filepaths in your app into locale-specific ones, or something like that. Is that correct? Secondly, how does it deal with the possibility of filepaths that are simply a byte sequence which does not decode correctly in the current locale?
I was just wondering aloud similarly. I haven't formed a preference myself yet.
Sounds like a compiler bug, but what do I know?
I think that, like many things, it is a matter of taste. For example, I like `Pretty` (which has no laws), but not `Arbitrary`[1] (which has laws). Between them lies `Binary`, which is very useful for some protocols (working derived instances are awesome), and useless for others (in that case I just write the `Get` and `Put` functions, not the instance). EDIT [1]: actually it doesn't, I thought the shrink part had!
Everyday a different 7x1. Edit: [everyday.](http://i.imgur.com/eVcfRzT.png)
Libuv will NOT translate your path on unix, you pass a `char*` and libuv pass it to unix kernel as it is. It is node.js that assume your unicode string should be encoded with utf-8 before passing to libuv. Node.js later add support for raw buffer filepath for dealing with non-unicode filepath, which doesn't have any encoding at all. I believe this is the right way to go since inteprete filepath based on some environment variable is NOT in any unix specs, it's worse than a utf-8 everywhere convention.
That makes sense. The situation with filepath encoding is a really ugly part of POSIX, just lobbing `char*`s around is the only really sensible approach.
I have to disagree on the parsing front, at least from my experience: With `parsec`, I got abysmally slow parsers that took many seconds to parse just a few hundred thousands lines. With `attoparsec` the errors suck. Tokenizing prior to parsec while retaining positions isn't easy. Happy/alex have a terrible UX.
The `generic-lens` package might be part of a solution but I think you will have to extend and modify it to perfectly suit your needs. https://hackage.haskell.org/package/generic-lens-0.3.0.1/docs/Data-Generics-Product-Subtype.html
I'm pretty sure you want a way to create a record or update fields of a record by referencing field names of another record. A bit of template Haskell can do that.
Wow, this is brilliant. Do you happen to know how this example uses same field-names across two record types? https://github.com/kcsongor/generic-lens#structural-subtyping Will this still work with the following types: data Human = Human {hName :: String, hAge :: Int, hAddress :: String} data Animal = Animal {aName :: String, aAge :: Int} 
Yes. In a typical web-app API, that's 60-80% of the work. Shunting data back &amp; forth between the JSON record-type and the DB-record type. Is there anything which makes this faster (from a development standpoint) in Haskell?
&gt; Operation not permitted Do you have write access to /media/sf_hs/... directory ?
Personally I would probably stick to what you're doing now. It honestly doesn't seem like that much more work (certainly not a difference between an hour and a day). It takes maybe 20 keystrokes in my editor to go from your Ruby code to the Haskell variant. That being said, one "solution" would look something like this: Use `makeFields` from `lens` to generate lenses for your types AND the corresponding `HasX` classes. Then you need some wrapper type to keep your lenses polymorphic while you work with them. Something like `newtype Field c a = Field (forall s f. (c s a, Functor f) =&gt; (a -&gt; f a) -&gt; s -&gt; f s)`. Now let's make a specialized heterogeneous list for our fields. data FieldList (cs :: [* -&gt; * -&gt; Constraint]) (as :: [*]) where FN :: FieldList '[] '[] FC :: Field c a -&gt; FieldList cs as -&gt; FieldList (c ': cs) (a ': as) Now we need a function like `setFields :: FieldList lens fields -&gt; a -&gt; b -&gt; b` with some appropriate constraints. Since we'll be doing type directed recursion it will actually be a type class. class SetFields cs as s t where setFields :: FieldList cs as -&gt; s -&gt; t -&gt; t instance SetFields '[] '[] s t where setFields _ _ t = t instance (SetFields cs as s t, c s a, c t a) =&gt; SetFields (c ': cs) (a ': as) s t where setFields (FC (Field l) fs) s t = setFields fs s (t &amp; l .~ (s ^. l)) Or something like that anyways... Then you can use it like setFields (Field @HasName @Text name ˙FC˙ Field @HasEmail @Text email `FC` FN) json order where `order` is something you're already filled with undefined values or something.
&gt; It honestly doesn't seem like that much more work (certainly not a difference between an hour and a day). It adds up very quickly. We are used to defining our DB schema and then spending *very little* time in writing the Rails code to expose that DB schema via a domain-aware JSON API (most of the time is spent in writing domain logic, not DB&lt;&gt;JSON glue-code). In the same time that it takes to wrap-up the entire API in Rails, we find ourselves barely having completed the DB&lt;&gt;JSON glue-code in Haskell.
I guess you could always whip up some Template Haskell to do this for you.
DuplicateRecordFields lets you do that.
Thank you enobayram for detecting a metabot. This bot wants to find the best and worst metabots on Reddit. [You can view the results here](https://en.wikipedia.org/wiki/List_of_Medabots_episodes). 
The standard response is "have you tried `megaparsec`"?
Not yet - but it was announced as parsec-like in terms of performance? And what about tokenization passes? Dealing with whitespace in parsec (without tokenization) is error-prone and time-consuming...
I think they'll need a different operation that isn't provided there, because they need a type like `sup -&gt; sub` i.e. to go from the general type to the detailed type.
What you're trying to do seems sound to me too but for quite subtle reasons the compiler can't recognize that. I haven't looked at GHC's code but from my observations, I believe the typechecker works as follows, roughly. When we pattern match on the constructor `SZero :: SNatural n`, we get the equality `n ~ Zero` only after matching. We can't assume it before matching succeeds. Here, matching on the `At` pattern has to happen before matching `SZero`, and that also requires to instantiate `n` in the type of `At` to resolve the `KnownNatural` and `Index` constraints, but for the reason I just mentioned, the constraint `n ~ Zero` is not visible at that point, so `n` remains ambiguous. `S0 :: SNatural 'Zero` does not have that issue, its type is known before even matching on it. In fact, this reveals a situation where the opposite happens (typechecks with GADT constructors, doesn't with synonyms): -- OK fromSNatural :: SNatural n -&gt; Int fromSNatural SZero = 0 fromSNatural (SSucc SZero) = 1 -- Won't typecheck fromSNatural :: SNatural n -&gt; Int fromSNatural S0 = 0 -- the type of S0 reveals the value of n before we even match on it. fromSNatural S1 = 1 To make that distinction, the type of the constructor `SZero` is in fact not `SNatural 'Zero`, but `() =&gt; n ~ 'Zero =&gt; SNatural n` (the GADT declaration syntax is made of sugar, in a way), where the second constraint is *provided* by the pattern. (c.f. [the GHC user guide about typing of pattern synonyms](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#typing-of-pattern-synonyms)). In contrast, patterns such as `At` and numerical constants *require* constraints. There's a good reason why my second `fromSNatural` example should not typecheck: it breaks type safety if we redefine `pattern S0 = _ :: SNatural 'Zero`, so that, after erasing the types, the first clause is always taken even if `n` was actually not equal to `SZero`. However, in your case, I couldn't come up with a way to break type safety, but I also don't see a general way to improve typechecking of GADTs to allow that. In any case, I also think this is less of a practical limitation of the typechecker, and more of a consequence of the questionable use of a pattern synonym (`At`) with required constraints on a type variable (`n`) that does not occur in the result type (`Sum ts`). Here are other ways to work around the issue: 1. With a data family: data family SNatural (n :: Natural) data instance SNatural 'Zero = SZero data instance SNatural ('Succ n) = SSucc (SNatural n) Now `SZero` actually has type `SNatural 'Zero`. 2. With type annotations (`ScopedTypeVariables`): fromTrit :: Trit -&gt; Int fromTrit (At (_ :: SNatural 'Zero) ()) = 0 fromTrit (At (_ :: SNatural ('Succ 'Zero)) ()) = 1 fromTrit (At (_ :: SNatural ('Succ ('Succ 'Zero))) ()) = 2 The singleton is in fact redundant. The fact that each instance of `Index` only matches on one constructor of `SNatural` is evidence that, in some way, a `Index n ts` constraint already carries some runtime information about `n`. We can actually replace `SNatural` with `Proxy`. We can almost remove it altogether using `AllowAmbiguousTypes+TypeApplications`, but I can't see how to use an ambiguous pattern synonym. 
The opening paragraph from William Gibson's _Johnny Mnemonic_: &gt; I put the shotgun in an Adidas bag and padded it out with four pairs of tennis socks, not my style at all, but that was what I was aiming for. If they think your crude, go technical; if they think you’re technical, go crude. I’m a very technical boy. So I decided to get as crude as possible. Haskellers, go crude!
Can I ask what you don't like about `Arbitrary`? One annoyance I have are that it's very common to want a more specific generator (e.g. using `forAll myGen myProp`); QuickCheck provides a whole bunch of newtypes, but if we're going to unwrap those constructors (e.g. `myProp (Positive n) = ...`) then we've saved little boilerplate compared to applying a function (e.g. `genPositive`). The other one is that the straightforward implementation of `arbitrary` has an unbounded amount of recursion, e.g. generating a binary tree by tossing a coin for `Leaf`/`Node` and calling `arbitrary` for the left and right sub-trees results in a call to `arbitrary` making, on average, `0.5 * 0 + 0.5 * 2 = 1` recursive calls. For types with higher branching factors, we can expect each recursive call to make *more* calls. This is mentioned in the original paper, and there are solutions to this; the easiest I've found is passing in a "fuel" argument and making recursive calls conserve fuel (e.g. given `n` fuel, give choose a random amount `l`, between `1` and `n`, to the left sub-tree, and give `n - l` to the right sub-tree. This doesn't fit with the existing `Arbitrary` interface though; we must define fuel-accepting versions of `arbitrary` for each type. I've found the built-in "sized" mechanism inadequate for this (since it doesn't "conserve size" when making multiple recursive calls).
No, but you might be able to modify the library to make it work.
&gt; I can't handle them in one function, i have to pass some of them, others not, but there is only one type, so i have a lot of partial functions. I understood that `MemberDecl` is a big sum, and you have individual functions to work on each constructor that you'd rather keep separate. But beyond that, I don't quite see the problem. What does a "traversal" do? fField (FieldDecl ...) = ... fMethod (MethodDecl ...) = ... ... What about this way of collecting all the functions together: f d = case d of FieldDecl{} -&gt; fField d MethodDecl{} -&gt; fMethod d ... 
&gt; it's very common to want a more specific generator That is my complaint. I *very often* write single use generators in the `Gen` monad.
It's typically faster than Parsec. Also see https://markkarpov.com/post/megaparsec-more-speed-more-power.html for how to get additional speed from version 6. For dealing with whitespace you can start by reading docs of this module: https://hackage.haskell.org/package/megaparsec-6.1.0/docs/Text-Megaparsec-Char-Lexer.html, it's known to be more flexible than Parsec's approach.
I don't know if I'm just using it wrong or what, but `DuplicateRecordFields` has been pretty disappointing for me. It does at least allow declaration of the fields, yes, but as soon as I try to actually use them, I get all these annoying errors about not being able to resolve which `someField` I'm talking about - even when this should be completely resolvable through unification with the surrounding context. It's pretty frustrating.
Explicit records for each sum case are a bit of extra work - but they're worth it to avoid the partiality. If Haskell had structural records it'd be much less of an issue.
Would you mind to elaborate about the problem you mention with size in QC? I also find it somewhat unsatisfactory but I can't find a concrete way to criticize it. Although it allows the solution you described for trees, I guess that for tree-like structures, it more naturally describes a limit of "depth" rather than a "size" proportional to the memory usage. 
How do you define lawless here? Is it lawless if there aren't any equations relating its methods? If so, what about one-method type classes?
i have functions and datatypes that only need some constructors of the sums, for example functions that handle every node that has a type. FieldDecl has a type, but MethodDecl not. There are other nodes (import-statements etc). But i can't separate them from the other constructors of the type. i guess explicit records are the only solution, but this is some serious amount of work. I would like to avoid that.
Very good question. It is indeed similar to Repa, but v3, not the new v4 one. Apologize ahead of time, I know you one of the maintainers of Repa, but besides the fact that it isn't very well maintained at this point, there are quite a few differences. Some major ones of top of my head: * Better scheduler, that is capable of handling nested parallel computation. * still shape polymorphic, but with improved indexing datatype (not being very objective here though) * Safe stencils for arbitrary dimensions, not only convolution. Stencils are composable through an instance of Applicative. * Improved performance on almost all operations. (I might be wrong here, but so far it looks promising and rigorous benchmarks are coming to prove the claim) * Structural parallel folds (i.e. left/right - direction is preserved) * Super easy slicing. * Delayed arrays aren't indexable, only Manifest are (saving user from common pitfall in repa of trying to read elements of delayed array) In any case, I wanted to hold of until it's finished to do a proper comparison with Repa and other libraries, but "somehow" the cat got out of the box. :) So, I'll have to deal with it quicker then I expected. Once I'll get that comparison, I'll post a link here.
So a metametabot is a human being, this'll be a breakthrough in AI research! (Loved the metabots link BTW)
&gt; it was announced as parsec-like in terms of performance? Not sure where you read that. I don't think there would have been much point writing it if it had parsec-like performance. "Despite being quite flexible, Megaparsec is also faster than Parsec. The repository includes benchmarks that can be easily used to compare Megaparsec and Parsec. In most cases Megaparsec is faster, sometimes dramatically faster" https://hackage.haskell.org/package/megaparsec &gt; And what about tokenization passes? I've no idea about that, sorry.
Would love to see a future ruled by robots where they're desperately trying to earn electricity credits from each other by starting NI-based startups.
That sounds really cool. Looking forward to seeing it in action (:
I think we need to know a *lot* more about how your pipeline works to give you a good answer. This looks a lot like an [XY Problem](http://xyproblem.info/). The most obvious question I have reading this is "why is `json` not just a field of your record"? What really happens here? You get some JSON over the wire? As part of a REST API? Then you want to insert or modify a database record based on this JSON, plus some other stuff like the current time? And you have tens of such REST API calls with different types? Is that a fair summary? Do you need anything else?
One-method type classes still can have laws, for example Idempotency. sort . sort = sort
&gt;&gt; Using the LLVM backend might make a difference here. &gt; Actually, no ;) At least on my laptop, llvm backend is slower So it made a difference! :)
As someone who is not a GHC dev, the way I would handle this is to identity a minimal reproducible example and open an issue on the [GHC Issue Trac](https://ghc.haskell.org/trac/ghc/). The people who wrote GHC are good at fixing it, and they are very responsive to issues that provide minimal examples.
I don't think you need the typeclass, because the GADT constructors should "direct" the types on their own. The typeclass will let you inline the steps of the recursion though, whatever benefit that gives you. (probably very little)
As you say, it's the exponential size of tree-like structures, and the fact we're memory constrained. With the standard setup, we get an exponential distribution of sizes: this gives us a few *huge* values, and *loads* of small ones. We need to tweak the parameters to avoid the big ones running out of memory (naive QC generators easily fill up as many GB as you give them); but this increases the proportion of tiny values, which aren't very useful (generating `Leaf` 50% of the time isn't exercising many code paths). With the "fuel" approach we get more direct control over memory (since we're counting nodes rather than levels), we avoid tiny values (they're only produced when a tiny fuel value is chosen, rather than exponentially-more-often) and we can probe further into the realm of large values, where counterexamples are more often found (in my experience, since there are more sub-expressions and combinations of sub-expressions, it's easier for invariants to break). This is explicitly talked about in [the paper](http://www.cs.tufts.edu/~nr/cs257/archive/john-hughes/quick.pdf‎) in section 3.2: &gt; We do *not* attempt to 'divide the size bound among the generators'... The reason is that we wish to avoid correlations between the sizes of different parts of the test data, which might distort the test results. I think this is a reasonable point, but I also think that the default approach distorts the test data into being very small. The [generic-random package](https://byorgey.wordpress.com/2016/09/20/the-generic-random-library-part-1-simple-generic-arbitrary-instances/) looks like a nice way to solve this.
The GADT doesn't know about `s` and `t`, and they need to appear in the constraint. I think you still need the class.
No, you're right. They suck (at the moment?). My only use case right now is to be able to define those types and generate instances that care about field names like (To/From)JSON and lens.
That's my use case too. The other thing that annoys me is that GHC warns about unused fields even if these fields are indeed used by the To/From JSON instances via derive `Generic`. I have to write verbose pattern matches explicitly naming each field to make it happy.
`Functor` has two laws for just `fmap` as well as the "it works like the default" law for `&lt;$`. Dually, that goes for `Contravariant`. Similarly for `Profunctor`. `Semigroup` and `Semigroupoid` have one law each for just the operator. Relatedly, I recently documented laws for the single-method `Apply` class. That might be considered cheating, since it has a superclass, but you could easily turn it into a (reasonable) self-contained, single-method class with similar laws. Related to /u/chshersh's example, the total ordering laws for `Ord` can be expressed in terms of just `compare` without reference to the `Eq` superclass.
&gt; Create a record type with fields that correspond to the functions in the typeclass? This is certainly what I do. Though sometimes it sends up being a GADT.
&gt; The other thing that annoys me is that GHC warns about unused fields even if these fields are indeed used by the To/From JSON instances via derive `Generic`. That's a GHC bug, which has been [fixed](https://ghc.haskell.org/trac/ghc/ticket/13919) in GHC HEAD.
That's great news! I'm tempted to do a clone and fresh build to get this early.
Have you considered using [vinyl's CoRec](http://hackage.haskell.org/package/vinyl-0.6.0/docs/Data-Vinyl-CoRec.html)? It would allow each function to list the constructors they expect and the constructors they return, without having to define a datatype for each subset of constructors you want to name.
Some ideas for how you can extend `generic-lens`: if using GHC 8.2 is an option, you can have types like data Human = Human {hName :: String, hAge :: Int, hAddress :: String} data Animal = Animal {aName :: String, aAge :: Int} and manually write instances of the `HasField` class ([link]). This allows you to write code like `getField @"name" fooHuman` without `DuplicateRecordFields` as in `generic-lens`, and you can then leverage this to write your own lenses. (The idea is somewhat similar to classy lenses.) Now, for subtyping, you can probably get away with something like type Humanish a = (HasField "hName" String a, HasField "hAge" Int a, ...) and just leverage constraint implications (if something satisfies a constraint C like the example above, it satisfies weaker versions that don't ask for all the fields C wants). You can write a one-instance typeclass to prevent the type signature spilling out into annoying GHCi errors. If you would prefer to not write the lenses yourself, you *could* use classy lenses, passing around constraints like `Humanish`. Another alternative I thought of if you'd prefer not to use TH is to use GHC 8.2's `AppendSymbol` class: type family DatatypePrefix a :: Symbol type instance DatatypePrefix Human = "h" type FieldName (fld :: Symbol) (ty :: Type) = AppendSymbol (DatatypePrefix ty) fld and then modify `generic-lens` to write functions that pull out `hName` when you call them with, e.g. `getField @name fooHuman`. This would also let you write types like `Humanish` in a datatype-agnostic way, replacing the `hName` and so on with calls to `FieldName`. [link]: https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.10.0.0/GHC-Records.html --- (Disclaimer: I'm not sure if this approach will lead to anything useful, since I haven't had the time to play around with the new release yet.)
&gt; I think we need to know a lot more about how your pipeline works to give you a good answer. Actually, I have been struggling for quite some time to come up with a sensible answer for doing web API development with Haskell, but haven't been able to get satisfactory answers which are type-safe **and** fast (from a development standpoint). &gt; The most obvious question I have reading this is "why is json not just a field of your record"? Most common case is when you have a DB row with many fields, all of which, are not required at the time of record/row creation. However, they will be required as the row/record progresses in its lifecycle. Few recurring examples: * At the time of creation, the incoming JSON won't have the PK, createdAt, and udpatedAt fields. However, once the same record is fetched from the DB, they are non-nullable fields. How does one handle it? Create two separate record types called `NewOrder` and `ExistingOrder` and write boilerplate to convert a `NewOrder -&gt; ExistingOrder` (which is what is happening in my original example). * And this is not always about book-keeping fields, like PK, createdAt, and updatedAt. Say you are creating an invoice that can be paid online. At the time of creation, it won't have the online payment link. It will be generated just before the record is created in the DB. However, once the record has been created, an invoice will *always* have an online payment link. How do you model this in Haskell in a type-safe manner without making a mess at the web-API layer? Another example which makes working with records a nightmare -- UI sends a `PATCH` request containing only those keys that it wants to change. How do you update them in an existing record without writing tons of boilerplate?
`isStrictPattern` is used for `ApplicativeDo` desugaring. Maybe this helps in narrowing it down a bit. edit: You should really open a ticket on https://ghc.haskell.org/trac/ghc for this.
&gt; Actually, I have been struggling for quite some time to come up with a sensible answer for doing web API development with Haskell, but haven't been able to get satisfactory answers which are type-safe and fast (from a development standpoint). Others can answer your actual question much better than I can, but I just want to point out that you're not alone here. However, given that easy, type-safe database interaction doesn't even *exist* in most ecosystems, I'm not really surprised that there are some growing pains here. Traditional statically typed web programming involves loads of boilerplate and it's quite inflexible. Traditional dynamically typed web programming is very easy but isn't at all type safe! So having low boilerplate, flexible, *and* type safe all in one solution really is a challenge to achieve. 
If gaining type safety is costing you too much time, just drop it. Use plain old `Map` and it will feel a lot more like Ruby at that point. Does the extra cost of the Haskell approach *include* time you'd have to spend writing tests in Ruby?
Ah, that explains it. Annoying problem.
ah... Yeah
Another option is to just import them into http://sharelatex.com
You nailed it. This has nothing to do with how "smart" GHC's typechecker is, but is instead a consequence of a design choice that GHC makes when typechecking GADT patterns. A simpler way to describe the issue is to consider the functions `f` and `g` in the code below (taken from [here](https://ghc.haskell.org/trac/ghc/ticket/12018)): data T a where TBool :: T Bool f :: a -&gt; T a -&gt; Int f True TBool = 3 g :: T a -&gt; a -&gt; Int g TBool True = 3 Somewhat surprisingly, `g` typechecks, whereas `f` does not. The reason is simply because of ordering! As /u/Syrak explained above, the return type of `TBool` (in GHC Core) is closer to `T a` under the *given* constraint (or *provided* constraint) that `a ~ Bool`). Because `a ~ Bool` is given, GHC only learns this after it has finished pattern-matching on `TBool`. Additionally, GHC typechecks patterns in a left-to-right, inside-out order. As a result, if you match on `TBool` *before* matching on `True` (as in `g`), it'll typecheck. However, if you match on `TBool` *after matching on `True` (as in `f`), it'll fail to typecheck, since GHC hasn't learned yet that `a ~ Bool`. This ordering makes typechecking GADT patterns compositional and predictable, but it can lead to some surprising type errors, and there are several GHC bug reports (e.g., [this one](https://ghc.haskell.org/trac/ghc/ticket/12018)) mistakenly claiming this behavior is a bug!
While it does seem fishy (and someone else even bothered to submit a [Trac ticket](https://ghc.haskell.org/trac/ghc/ticket/14104) about it), I disagree that it's a bug. See [my comment](https://ghc.haskell.org/trac/ghc/ticket/14104#comment:1) on the Trac issue, as well as the comment chain starting [here](https://www.reddit.com/r/haskell/comments/6so95h/why_do_pattern_synonyms_work_here_when/dlf50c9/).
The file that implements the example is [here](https://github.com/kcsongor/generic-lens/blob/master/examples/Examples.hs).
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [kcsongor/generic-lens/.../**Examples.hs** (master → bfcd00a)](https://github.com/kcsongor/generic-lens/blob/bfcd00a83d9284ead43de9e6481d34c4677d772e/examples/Examples.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dlff9ub.)^.
I tried fixing this but my solution was rejected: https://github.com/nick8325/quickcheck/pull/116
https://gist.github.com/d258b88a0e0b3be2c0b3711fdd833045
I'm hoping that with some prodding, /u/Iceland_jack will write this up as a GHC proposal. :)
For those who are remote? No. In most circumstances, I try to. My goal is to put a timer on them; we may need a meeting but we seldom need an all-hands meeting. I try to push stuff "offline" as quickly as possible.
The "not corporate" technical response: Slack and all its various IRC incarnations, is also very frustrating. At any moment you're subject to distraction and without other outlets, you can't control when those requests come in or how their priority is assessed.