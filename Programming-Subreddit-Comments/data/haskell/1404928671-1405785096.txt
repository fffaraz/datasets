I'm a big advocate for things like splint, findbugs, hlint, sonarcube, etc. That shouldn't stop us from improving our languages. A dependent type system (without these escape hatches) allows you write additional static analysis rules using the same language as your program and have the compiler check them. But, if you have a bad `believe_me` (from Idris) in there, &lt;undefined behavior&gt;. Most of the time, that's just a crash and you'll be able to find the bad call fairly quickly. But, perhaps aggressive erasure gets rid of the term that causes the crash and instead, the security / safety guarantees you thought were encoded in the types are invalid but pass right through the compiler and are not (further) checked at runtime.
Just use evil-mode with emacs. I haven't looked back
I pasted the example code for the primes and it said &gt; parse error on input `='
http://i.imgur.com/IG43hEh.jpg
&gt; That shouldn't stop us from improving our languages. But there is your core problem, you believe removing the functionality which enables very powerful and easy to use results will improve the language. I don't know if I agree with that. Kind of like operator overloading. F# did away with it because it was misused, but Haskell has shown it is a very useful tool when used correctly.
One reason is simplicity. This way the code using the function does not need to know anything about the structure of the function. As far as 'main' is concerned, it simply gets a function Int -&gt; Int. 
Yup, the possibility of data corruption is a serious drawback which never entered my mind. I'll look into the paper you linked to.
I use guards fairly frequently, but I wouldn't miss them if they were gone. The real challenge is when your total case breakout is a complex mixture of patterns and guards. These are sometimes useful in types which maintain complex invariants (memories of `containers` code comes to mind) but man do they scare me...
I don't think we recurse into ghc782 yet, because that would cause everything to build on [Hydra](http://hyrda.fynder.io). You can trust me that all of those ghc763 packages are available to install via `nix-env -i haskellPackages_ghc782.&lt;foo&gt;` though, but you will have to compile them. When 7.8.3 is out, we're going to switch from 7.6 to 7.8 by default, which means we'll start building binaries again.
If you do try NixOS again, please rant as much and openly as you want on our issue tracker (well, within reason). It's entirely possible all of us using it day-to-day have fallen into the known-work-around habit and avoided solving real problems.
Re 2. I use `cabal build` all the time within `nix-shell` and it works fine. I don't `nix-env -i` Haskell packages though, maybe that's where we differ?
I really like it. The only thing that seemed off was that the spacing between the left and right columns made it seem like they were meant to be unrelated, so it took me a while to realize the examples were associated with the interpreter. It was also not clear from the UI that the interpreter was interactive, unless you read the little bit of text above it.
Nice post. Even better, you can eliminate the list entirely: given an arbitrary rational, you can directly compute its Calkin–Wilf successor and Calkin–Wilf predecessor! This is demonstrated in [prelude-safeenum](http://hackage.haskell.org/package/prelude-safeenum-0.1.1.1/docs/Data-Number-CalkinWilf.html)
Depends on what you consider a "functionality". I don't consider `unsafePerformIO` or `head` a functionality. I consider referential transparency a functionality but we don't have that in Haskell since it is violated by `unsafePerformIO` or certain FFI imports -- removing those things would add referential transparency. I consider totality / productivity guarantees a functionality, but with `head` in the language we don't have a totality / productivity guarantee -- making non-total (for data) or non-productive (for codata) function definitions an error would add totality / productivity guarantees. "It seems that perfection is attained not when there is nothing more to add, but when there is nothing more to remove." --Antoine de Saint Exupéry It's actually a middle ground, but just because we have a certain function(ality) right now, doesn't mean the language wouldn't be better without it.
&gt; They must recommend the platform installers, as installing GHC and Cabal by hand is error prone and a poor user experience. I think you're simply wrong. The amount of newer Haskellers with problems in #haskell that are resolved by simply "install GHC and Cabal instead" is fairly huge. Haskell Platform pretty quickly becomes error prone and a poor user experience.
Installing binary distributions of GHC and switching between them with cabal is very straightforward (on linux), `cabal install cabal-install` has never given me problems either. Are you using the newest cabal-install and building in sandboxes? That has made things much better for me. I feel your pain and yes lots of people care about continuing to improve the situation. I don't think it's particularly clear at this point how dependency management is going to be "solved"; it's a tricky problem. If you want to post details of any particular issues you're having, I'd be happy to try to help.
&gt; I spent hours trying to download and compile yesod and hours with ghcjs. Tbh, you seem to have chosen two of the more complicated (dependency-wise) packages to get started with... :-/
Just this. I don't want lawyers on my back.
NP-Complete problem in general + Incomplete data + Largely volunteer infrastructure + various underlying platform interactions. There's no magic pixie dust to make the problem easy to solve in a general and principled way. It takes a lot of effort to keep complex systems of constantly moving pieces in sync. Part of the "problem" here is that modularity and code reuse actually *works* in Haskell, and people take advantage of it. But pervasive modularity and re-use in a world of separate maintenance and compilation of those modules gives rise to another set of problems, for which the solutions are as much social as technical.
&gt; Why is upgrading ghc difficult? Why is upgrading cabal anything more than &gt; `cabal install cabal-install` Because it installs the upgrade somewhere off the PATH for a huge number of users. This is a well known issue with cabal, it's not some weird corner case only this guy is having.
Only people who make the mistake of trying to use a system-wide GHC/cabal installation.
Like anyone who installs the Haskell Platform, aka basically every new user.
I've given myself a fair bit of frustration with cabal in the past (depending on how I'd installed it) by not having ~/.cabal/bin at the front of my $PATH - the result of which is having the command run successfully, but seemingly to no effect - [others have clearly had this issue as well](http://stackoverflow.com/questions/5380888/cabal-install-and-debian). It's kind of an "understanding how your OS works" issue, but many other projects, like nvm and npm, seem to have managed to sidestep or minimize these issues in one way or another.
I think the somewhere you mean is ~/.cabal/bin which you should put on your path if you are using cabal. 
In the specific instance I encountered this bug that was not possible, because by default installing the Haskell Platform on Windows puts cabal-install in a portion of the PATH you can't move cabal/bin ahead of. (The solution is to rename your old version of cabal-install) For people encountering the problem on other OSes the issue is that they *don't know that they should put ~/.cabal/bin on the PATH* and it's pretty weird that the default behavior isn't to do that already.
Right. I think we can focus more on clear and accessible advocacy without being nearly as sales-pitchy. If anything, we want to worry less about making haskell seem "awesome" than making it seem _welcoming_ and not scary. Lots of the typical things we'd say about haskell that are "low key" are really just technical words about features that _we_ understand are important. The challenge, I think, is describing _how_ the feature is important. As long as we do that, in a way that doesn't send new users running for the hills, I'm fine with low key :-)
That looks intriguing. Fantastic example with the mouse coordinates! Am I to understand that's basically FRP in action? I'd suggest posting this to /r/javascript as well.
There is absolutely no reason why dependency executables need to be on your $PATH. I always found that requirement strange. 
Very nice tutorial! But there's a manual step at the beginning that could get expensive. You seem to look at the schema, and then enter the corresponding QuasiQuotation by hand. If the schema is very large or complex, this can become tedious and error-prone. Do you know of an approach to automate this?
Can confirm, I have a system wide ghc instalation because the wiki recommended it. I'm not too concerned about it right now because my haskell learning stage is still in the very beginning stages of reading books and understanding the code snippets. 
NP-complete problem? I understand this is definitely not an easy problem. I am basing my opinion off of my experience with other pkg management systems. aptitude has always worked extremely well for me. most of the conflicts I've had with pip are because of distutils + setuptools + distribute + easy_install upgrade chaos but for the most part is very easy to use. homebrew is excellent. etc, etc. And also other language installs. I guess I'm missing one key point though, cabal requires compiling of packages and their dependencies while the others don't run those checks (right?). But can't different versions of packages live side-by-side and removed when they are no longer needed? Is this already happening? I feel we need an official tutorial of how to best get up and running outside of the (somewhat aging) Haskell Platform since manual setups seem to vary widely in how and where everything is installed and often conflict with the popular Haskell Platform. I don't have any experience architecting programming-language/library installs nor building pkg mgmt software. I understand it's hard but I hope the entire process of installing ghc, cabal, and downloading and managing dependencies can be smoothed out a bit.
My knowledge of linux administration is very small. Is there a standard method of extending the PATH programmatically, while making sure that multiple installs modifying path do not step on each others toes? My current impression was that each extension of PATH was ad-hoc and therefore would occasionally cause problems. I imagine not having cabal modify the PATH it ensures that cabal does not mess anything at the system level. I would also imagine that it avoids necessitating custom scripts for various distro flavors. edit: grammer
Exactly. $PATH is a global variable. Every novice programmer gets indoctrinated how bad they are, yet on a os level people seem to accept them without a second thought. I look at everything that wants to change my global system state with extreme suspicion... 
&gt; I am basing my opinion off of my experience with other pkg management systems. I have little knowledge of most pkg management systems, but my understanding was that most pkg systems like aptitude work by making all released packages work in lock step. So no new package is released until it works with everything else. This requires a high degree of official coordination between package maintainers. On hackage however the maintainers of packages are not required to have that kind of official coordination. There is for the Haskell platform, there is for stackage, there is also the libraries committee(which do not remember the exact responsibilities are right now.). If we wanted cabal and hackage to work like aptitude we would need to limit access to uploads to only those who can be depended on keep their packages updated in lockstep with everything else. Hackage and the ability to upload easily has been a big boon to Haskell. So restricting hackage seems like it might do more harm then good. Using something like stackage might be what you are looking for. At least until there better technical solutions are brought to bare. 
Debian (like other distros of similar scale) has an army of volunteers and a pretty strict policy about how everything should work together. Don't imagine that it all just works simply because they've got the right tools. Part of the reason distro-level packaging works reasonably well is that the boundaries between packages are better-defined. Shared C libraries work at a much rougher granularity than Haskell modules, which makes dependency management in C way easier to tackle. Plus, the entire OS is built around the C runtime, dynamic library loading, and that sort of thing. C code re-use tends to focus around big monolithic frameworks like glib or medium-sized task-specific libraries which don't typically have many dependencies themselves. Languages that are JIT-compiled or bytecode-interpreted have it somewhat easier as well, as there's no separate compilation pass to manage dependencies for. Source libraries are lighter-weight and it's easier to install tons of duplicate versions, though this can sometimes lead to subtle run-time issues, some of which would show up as compile-time issues in Haskell. GHC's package management, cabal, and cabal-install could all be improved, but again there needs to be someone who actually does the work. Although there are some very prolific and helpful members, the Haskell community is not exactly anywhere near the scale of Python or Debian. Regarding the Haskell Platform, a new release is apparently just about to be made, so hopefully that will make the problem a bit easier for people who mainly need a basic usable set of libraries. Meanwhile, make heavy use of cabal sandboxes. That's the only way I know of right now to have multiple separate versions of libraries live together in a reasonable way.
Go ahead. xposting is not a crime.
We already can do combinators for extracting lenses using the existing GHC.Generics. http://lpaste.net/107186
Well you could, but usually people just keep prepending new stuff to it in ~/.bashrc so it's the same in all sessions. Setting up different environment variables for different sessions would be more work to manage.
I'll be the one fully agreeing with you. There isn't much infrastructure ensuring that packages work with the latest version of their dependencies (automatic builds, nag mails, golden builds etc). There is a belief that having upper version bounds on dependencies is a solution to an important packaging problem (building obsolete software without having to reconstruct Hackage at an earlier state). The result is that developers don't know when their stuff doesn't work, developers obstruct usage of new versions of dependencies to be on the safe side (upper bounds), and in general progress is ensured by individuals doing the missing integration work that developers don't do (Haskell platform, stackage, NixOS, etc). To get ahead Hackage must absorb some of the ideas from stackage, NixOS, Travis or similar in the workflow. Just testing locally and pushing to Hackage can never work. A continuous process must replace the Hackage/cabal process.
Right. But the bad idea is not adding things to the $PATH. That much is fine, and it has nothing really to do with the OS people. Shells provide you with all the tools to build easy and custom scopes, and there are fairly easy to use scripts to use them, so if you feel that your $PATH or whatever needs to be "locally different" you can easily do so.
That's a beautiful post, thanks for writing that! I knew about the Stern-Brocot tree, but didn't know about Calkin-Wilf. It's a very natural idea in retrospect, it's just a tree of all possible executions of Euclid's algorithm, going from m/n at the leaf to 1/1 at the root. Another beautiful picture in the same vein is [Ford circles](http://en.wikipedia.org/wiki/Ford_circle), when I first saw it I almost couldn't believe that the mathematical universe could be so nice.
Because there is no stable core repository that other libraries will adapt to. There is no sane testing system that will tell which library will break with the newest compiler release. This mess will not be fixed soon because Haskell would be at risk of escaping from academic world. 
yesod seems like one of the most important packages for Haskell adoption. I don't see why this is an excuse.
&gt; aptitude has always worked extremely well for me Aptitude relies on a package repository that is curated. Not only manually curated, but with large build farms in behind it which perform integration testing on every build. Hackage is entirely community run and managed, and anyone can upload packages. Packages can be updated at any time, often in ways that break someone else's build. And there's no way to know until that person reports the build breakage to you. Haskell library writers walk a tightrope between making dependency version bounds tight enough to reject packages that would break the build, and open enough to prevent dependency resolution from failing when installed alongside other packages. Haskell Platform is one solution to these problems. It is a curated set of core packages that all work together, suitable for building small to medium sized programs. Unfortunately it does not include cutting-edge fast-moving projects such as Yesod or Snap, and trying to install these sometimes leads to the afore-mentioned build failures, **especially** if you've also installed other packages beforehand which add their own version constraints. Stackage is another solution. It is a curated package set that includes recent versions of cutting-edge packages, and FP Complete do a very good job of making sure that all the packages play nice together. You won't get the very bleeding-edge packages that you would from Hackage, but you're less likely to be cut, too.
This isn't going to happen I'm afraid (with almost 100% certainty), unless there is a large outcry, and honestly - there isn't. Analytics provides a *lot* of insight into how your pages are utilized and what problems users might have, and how to optimize your pages for them - seeing how users move through your site provides a lot of feedback on what might be obvious or non-obvious, and how you can provide users with the right information based on what they might have been doing. Just look at all the attention we've gotten in these threads here and on places like hacker news - there are *already* hundreds of suggestions of what to tweak at this point, and frankly, that kind of input just isn't manageable *at all*. Having actual user navigation data provides far deeper insights into what we might want to change to make the site better for newcomers and existing visitors. I understand the concern, and I say this as a NoScript and Adblock user and someone who's fairly committed to my online privacy and security. But at the end of the day, I want my sites to work *better* and I want Haskell.org to be well designed for newcomers and provide them with the right information. Taking endless input from bystanders - not all of which we can act on, or much of which might conflict - and shooting in the dark isn't going to work out as well here - especially considering this is all pro-bono work. We're not a team that can devote our lives to this 8 hours a day or pay oodles of money for other solutions. If we were, then yeah - we probably could do something just as good without analytics (or with another solution) to help inform us. If you have alternative suggestions, I'm all for it. But the use of these tools for us is actually quite valuable, so you're going to either need to make a very good case for it, or just use NoScript. Sorry.
I think it would help if the informational notice to upgrade cabal also included something like "Did you remember to add ~/.cabal/bin/ to your $PATH"?
FWIW I fixed this earlier today. 
I used to have a similar opinion to you, but I really don't have all that much trouble these days. Also, you cherry picked possibly the two packages in the whole ecosystem that are most susceptible to this problem. Yesod has more (probably significantly more) dependencies than the vast majority of hackage packages. Its authors also chose not to put version bounds which magnifies the dependency problem. ghcjs isn't even on hackage, so I would say that's not really in the domain of valid criticism. I'm currently working on a project that directly depends on 91 packages. Also, 14 of those dependencies are not on hackage, which tends to make things more difficult. It tends to work pretty well most of the time. When we do get build problems, it's usually not too hard to track things down because we have version bounds on just about everything. But I attribute most of those problems to the fact that it's just a hard problem. This isn't unique to Haskell. Any language with a sufficiently large ecosystem will have the same issues. The term "DLL hell" was around long before people started complaining about hackage build problems.
You'll need to motivate that. In my experience it works great unless you depend on libraries from abusive maintainers that don't target the latest platform. In my mind we need to press-gang maintainers that don't keep up platform compat into either labeling their libraries properly or changing their evil ways. Oh hey, that's an idea for new-hackage! A badge for libraries that build with the current platform, and a gasface for those that don't.
You have few options: use system wide package manager (yum, apt...), try to install all packages at once, use sandboxes (Cabal version &gt;= 1.18). I prefer sandboxes. (There are perhaps other options like NixOS, but that is quite hardcore) Hackage is not like Linux distribution repository of tested packages that all work with each other, it is more like higher level interface to Github, Darcs... I think that was original intention of Hackage - it is for developers and package maintainers that should handle these problems for end users of executables and libraries.
Go look at a package manager from a bigger language like bundler. There's no notion of "hard to install" packages there, and there shouldn't be in cabal either. 
To be fair, Bundler is a pretty fucking awesome package manager (and more), while cabal is just a build tool. While I wish there was a hBundler, it's really unfair to compare the two.
Could you explain that to me a little more? I always thought that both cabal and bundler did an equivalent sort of 'install packages' thing. What is the difference that makes comparisons unfair?
Bundler is a superset of Rubygems, which is an actual package management system. Bundler adds some nice features to the pack, such as sandboxing and fetching packages straight from git. Bundler itself does not build gems. It just manages them. Cabal builds and packages Haskell programs. It is not meant to do tasks such as upgrading or removing packages. In a sense, and please don't slap me for saying this, you can see Bundler being an analogue to apt, while cabal would be an analogue to make.
One man's global is another man's local.
Ever tried to install numpy or matplotlib on Windows using pip? There's a reason I just go grab the exe installers instead.
what do you mean no version bounds?
Sorry if I'm being obtuse, but what on earth is "cabal install" if not a package installer? How is it different then apt? (I didn't remember the difference between bundler and rubygems (haven't used rails in a while), but let's just say we're talking about rubygems then. 
This is probably more fair comparison to the problems that Haskell faces as a lot of numeric Python libs have to be compiled and linked and generally this goes very badly. That community seems to have just embraced shipping prebuilt [binary environments](https://store.continuum.io/cshop/anaconda/) for the common platforms beacuse the problem of having users build from source was so intractable.
GHCJS is currently in a state of flux since there were some major changes in the default setup, related to a change in how Template Haskell is handled [1]. The default setup now does not generate native code anymore, Template Haskell and Cabal Setup.hs scripts are now compiled to JavaScript, and run with node.js. It's the biggest change in the build system in well over a year. This required changes in the Cabal patch and also uncovered a some bugs in our bindings for the `base`, `directory` and `process` packages (all of which use the node.js api for their functionality: reading/writing files and directories, running and reading from processes). Most of these have been fixed now, but there are some remaining issues that might take a few more days to fix. Obviously this is one of the reasons that GHCJS is not on hackage yet. Please join the `#ghcjs` channel on freenode and ask there if you want to work with a development version, or better, idle in there to see what's going on ( public transcript here: http://ircbrowse.net/day/ghcjs/today/recent ). We're getting closer to a release but are still not quite there yet. [1] http://www.haskell.org/pipermail/ghc-devs/2014-July/005415.html
http://ivanmiljenovic.wordpress.com/2010/03/15/repeat-after-me-cabal-is-not-a-package-manager/ I had the same question a couple of days ago and found the above helpful.
Bundler isn't really a package manager though. It is an application dependency manager. The package manager is called RubyGems. 
Very informative article with terrible advice at the end. Relying on the system's package manager is definitely not a sustainable solution. 
I was highly surprised to see that paper not mention Cloud Haskell in its related work. The authors did some really impressive stuff, though. 
Thank you for taking the time to reply. &gt; If you have alternative suggestions, I'm all for it. But the use of these tools for us is actually quite valuable, so you're going to either need to make a very good case for it, or just use NoScript. I do understand the need for analytics. My biggest concern is w.r.t. the use of *Google* analytics. I am not an expert in this matter but searching for "Google analytics alternatives" yields plenty of results, e.g. http://www.searchenginejournal.com/9-google-analytics-alternatives/92071/ Edit: How about something open-source such as [Piwik](http://piwik.org/) (http://en.wikipedia.org/wiki/Piwik) which you can host yourself? They also offer a hosted solution. 
I disagree. What you wrote needs to go in the "more information" section, it requires way to much context to understand.
I had to add (add-hook 'evil-insert-state-entry-hook (lambda () (when (eq 'agda2-mode (buffer-local-value 'major-mode (current-buffer))) (set-input-method "Agda")))) (add-hook 'evil-insert-state-exit-hook (lambda () (set-input-method nil))) to my .emacs to make evil and the agda input mode to play nicely
Yes, cabal installed executables should take precedence when cabal installing. In fact, I would look at any package using whatever is available in your $PATH with suspicion. 
Depends. If it's something sensible like Nix then it should be fine. Anything other than a purely functional package manager like Nix is probably going to be a pain.
Could someone give a concrete example why things are different in bundler? It seem to address the same problem, namely figure out a working set of constraints given a specification (http://guides.rubygems.org/patterns/#pessimistic_version_constraint).
it does not follow the [package versioning policy](http://www.haskell.org/haskellwiki/Package_versioning_policy).
No, I'm just using nix-shell with a default.nix file for the project. No packages installed through nix-env -i. I just tested on another project and cabal did work, but I was getting a cabal error on a different project... 'cabal list --installed' would show that the packages were available but 'cabal build' would not find them...
I posted this as a comment: What trouble are you having with static arrows? The isomorphism is displayed on the first page of the paper. It says that applicatives are equivalent to arrows `arr` equipped with an isomorphism between `arr a b` and `arr () (a -&gt; b)`. This should be all you need to know to answer your question. Feel free to ask for clarification.
This looks really useful!
It's also impossible for lots of projects built in Haskell. Is there a better way of getting remotely up-to-date Agda or Idris executables than through Hackage and cabal-install? Both Debian and Fedora have old versions of Agda in their repositories, for instance, and both projects recommend installing the respective executables through cabal-install from Hackage. His remedy seems to be that people should change their entire operating system and use Gentoo instead as a workaround for the lack of up-to-date executables in .deb or .rpm repositories. That isn't going to happen.
It's not that hard anymore. We've been using Haskell for web-dev (though not Yesod) for about 5 years now, and while we do have the occasional problem, usually wiping the sandbox is enough to fix it.
[Direct link to frame-jacked article](http://blog.sqreamtech.com/2014/05/why-sqream-technologies-loves-haskell-code/)
To the contrary, it's the *only* sustainable solution... you can work on haskell package management all you want, but you will *never* succeed in pulling in dependencies that aren't written in haskell.
You need to build your own debs using the cabal database as an upstream. It sucks, but it is definitely the "right" solution. Ideally, somebody out there would provide an apt repo where this stuff was just available, and then others would just use that.
&gt; Aptitude relies on a package repository that is curated. Not only manually curated, but with large build farms in behind it which perform integration testing on every build. Quite true. But: &gt; Hackage is entirely community run and managed So is Debian.
&gt; Go look at a package manager from a bigger language like bundler. There's no notion of "hard to install" packages there, and there shouldn't be in cabal either. I've had much worse times with Ruby, bundler, rubygems and rvm than I've ever had with Haskell and Cabal. The grass isn't greener on the other side, they have the same dependency hell problems as Haskell has.
&gt; Unfortunately it does not include cutting-edge fast-moving projects such as Yesod or Snap ..so are you suggesting to have Yesod or Snap included into the Platform? And if not, why not?
&gt; Upgrading ghc is extremely difficult. Upgrading cabal is extremely difficult. I'd change "extremely difficult" to "unnecessarily complicated". After all, it's mostly just about manually updating symlinks. But I agree that this task should be automated.
Thanks, I tried replying to that on Stackoverflow before seeing this here, and the SO comment box is just awful for non-oneliners. If we had a typeclass class (Arrow arr) =&gt; ArrowStatic arr where iso :: arr a b :&lt;-&gt;: arr () (a -&gt; b) then does this statement mean I can turn an applicative into something that is an instance of `ArrowStatic`, and thus, of `Arrow`? Isn't this what `Applicarrow` is doing in my example? And if yes, how does that make `Arrow` a **more** expressive interface than `Applicative`? Doesn't this mean the opposite, that `Applicative` is more expressive since given an instance of it, I can also use `iso` on `Applicarrow` in addition to the `Arrow` methods?
There's no good way to modify a user's path, and I imagine it would be very poorly received by most users if you even attempted it. The best solution is probably to create a symlink in a directory that's already in the path (/usr/local/bin most likely, but if you're installing from a package manager, /usr/bin might be appropriate as well). For a few programs, this may not work due to the need to launch it from a particular working directory or with a bunch of environment variables set up. In those cases, the best option is probably to bundle a shell script that sets everything up properly and launches the real executable, and put that shell script in /usr/local/bin or wherever. Alternately, you can just clearly document to the user that they need to take some action like adding ~/.cabal/bin to their path.
What i did there may be retarded, but e.g. SDL2, SMFL and many other libraries I would like to use do not even compile... Sorry, but this is a fact not some abstract monad annotated not to be so lazy.
The site is great, but I'm of the opinion that I don't like the new repl. The old tryhaskell that worked like ghci was much easier than the new one to show to people. Especially because the new one generates so many lines of stuff not particularly relevant to what you are trying to show, too. I tried to show something to someone the other day that would be a sinch in ghci, found myself on the new tryhaskell struggling to break down the types of things in expressions that actually make sense rather than just confuse.
This actually looks nicely done, congratulations! You've done a good job using monad transformers &amp; parsec, so the end result is clear and concise. My first Haskell project was also a brainfuck interpreter and it was *way worse* -- I got some help here as well and it ended up looking [more like yours](https://github.com/5outh/fhck). There's some experimentation with Free Monads in my project that I never got back around to, but ended up using them in other projects later on. A couple of minor improvements you could make would be to use `attoparsec` for faster, text-based parsing (though this likely won't make much of a difference in such a small program) and, like I started to do, refactor the interpreter into a [Free Monad](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html) for fun. One more feature could be to include a [REPL](http://en.wikipedia.org/wiki/Read–eval–print_loop), either built from scratch or using [Haskeline](https://hackage.haskell.org/package/haskeline).
Awesome! I will investigate the sandboxes. Do you use the sandbox when deploying to production? Or, do you just statically link everything and create a package (deb, rpm, etc.)?
the same can be said about Haskell (or almost any lang.)
We build on our continuous integration server (Jenkins) in (cabal-dev) sandboxes. The resulting binaries are packaged with some other resources, and those zips can be deployed to dev machines, staging/testing and production. The reason we still use cabal-dev is that the built-in cabal sandboxes still have some issues for larger multi-package projects. But if we can get away with it (smaller subprojects and such) we use cabal's built-in sandboxes.
Awesome work! Keep it up :D
There's one key technical issue that leads to most of the version conflicts. Version conflicts are almost entirely avoided by working in cabal sanboxes (the compile time and maybe disk space to populate a sandbox on the first build is annoying, but it does avoid dependency problems). The problem is that being compiled, and additionally depending on cross-module inlining for performance, compiling the exact same version of one package against two different versions of some of its dependencies can product incompatible binaries. GHC is perfectly happy to have different *versions* of a package around, but can't keep different builds straight. GHC is perfectly fine to have a `foo-1.0` and a `foo-1.1` compiled and ready to use at the same time. What it can't handle is having a `foo-1.1` that was built against `text-1.0` and a different copy of `foo-1.1` that was built against `text-1.1`. This is bad news if you installed one package that needs any `foo` and explicitly needs `text &lt; 1.1`, and then later try to install something that needs any `foo` but also `text &gt;= 1.1`, and it tries to again use `foo-1.1`. Cabal does a decent job figuring out a compatible set of packages to satisfy a single installation request. Dependency problems usually come up when you try to install multiple things with the same package database - like just "cabal install" without sandboxing, tossing everything into your user package database. Sandboxes isolate different builds, which is how they avoid this. The convenient solution, of course, is to make ghc/cabal better at dealing with multiple builds of packages, perhaps like Nix. I don't know if anyone is working on that. Could you say a little more about the other problems? I haven't used yesod and it sounds like there are maybe some issues with that library being packaged badly, but I haven't seen the other problems. cabal install cabal-install works fine, I had .cabal/bin on my path from installing other binaries with cabal (like happy or agda). Upgrading ghc means compiling libraries against the new ghc, and sometimes packages need to be updated for changes in the standard libraries, but I didn't think of either as "extreme difficulty", or really all that surprising.
I put a larger answer at the root, but in short one big problem is that "different versions" can't always live side by side well enough - actual different version numbers can, but builds of the same version number against different versions of dependencies can't (yet, should be possible), and GHC does (and depends on) enough inlining that two builds like that are not interchangeable. This is largely why there is interest in Nix around here - keeping around and maybe GCing multiple package versions is exactly what it's supposed to be good at. Something like apt is different, they only have a single version of a package available at a time (or explicitly make distinct packages out of different upstream versions, if they want to provide both), and have build farms and things testing that they are compatible. I have heard of a "Stackage" for Haskell that is supposedly equivalent, but haven't tried it myself. 
The additional complexity is that ruby is not usually compiled - `foo-1.0` is just `foo-1.0`. In Haskell, and in particular with inlining for decent performance, `foo-1.0` compiled against `bar-1.0` isn't interchangeable with `foo-1.0` compiled against `bar-1.1` (Ruby interprets stuff at runtime so it all works out). Unfortunately GHC can't handle multiple versions like that (it can keep one build each of `foo-0.9` and `foo-1.0`, but not multiple builds). This means you can't just cabal install everything into the same GHC package database and expect everything to work. cabal probably *could* cache all the separate builds somewhere under .cabal and paste them together to initialize a sandbox rather than rebuilding.
It would be quite a bit more presentable if the HP page wasn't constantly promising a "next release" many months in the past.
Do you know of any guides to using Nix with Haskell, without switching to NixOS? I've found a few step-by-step guides for setting up nix itself, but they seemed to be assuming I wanted to install *everything* through nix so I stopped following them.
&gt; In case anyone's wondering, the "well known bug" in waitForProcess may actually not be very well known yet. There's a race condition documented in the source code, but it's not nearly as narrow a window as implied there. The code in question says: -- don't hold the MVar while we call c_waitForProcess... -- (XXX but there's a small race window here during which another -- thread could close the handle or call waitForProcess) &gt; Thus the motivation for fixing the problem in Data.Conduit.Process. Temporary workarounds are ok, but let's also fix it properly. It doesn't look that hard. Do we have a ticket open on it? If one looks at the code we can see that we have a couple occurences of `withProcessHandle` which uses the MVar, and a couple of `modifyProcessHandle`. The point is we don't want to have a thread that's blocked in `waitForProcess` to stop other threads from trying to kill the process. So we don't want total exclusion on reading the process handle. Here's a suggestion, lets arrange so that only one thread calls the `c_waitForProcess` at once. We can add another process handle state for being in a "closing" state. That state will have an MVar for the ExitCode that other concurrent threads calling `waitForProcess` can wait on. The thread that first calls `waitForProcess` will create the MVar and put the handle into that state and go and call `c_waitForProcess`. When that returns it can fill the MVar for any other waiting threads.
I like it! One question: Does your Parsec piece do anything that a `filter (flip elem "&gt;&lt;+-.,[]")` couldn't do less efficiently? Because all that Parsec code seems to be to heavyweight for what it's accomplishing if not.
Does Data.Conduit.Process do anything to aid or hinder the use of other file descriptors (like the ones created by pipe(2))? When I read the intro, that there are four ways to communicate with a child, my first thought was that it's actually really common to open secondary pipes and communicate through those as well. Is this possible with Data.Conduit.Process?
What about [Backpack](https://ghc.haskell.org/trac/ghc/wiki/Backpack)?
That's because it predates Cloud Haskell. Here is another paper on it, published in 2004, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.510 That paper gives some end user examples and stuff. It is pretty neat. Alas, it never got adopted. 
Welcome to the real world. There be sharks and all.
Well, my guide doesn't really assume much buy in, and certainly doesn't assume you'll be running NixOS. Just having `nix-shell` should be enough.
As usual with core libraries like process, we *have* to have a temporary workaround, since we'll be dealing with older versions for the next 2-3 years. But my feeling from looking at the code was that the only robust solution we'd up at is the one of having a separate thread dedicated to grabbing the exit code and updating a TMVar (or MVar). Under those circumstances, it actually makes sense to me to *leave* this problematic behavior in process, as it can be an efficient-but-dangerous approach that people can use when they know there will only ever be one thread waiting. If there's a way to have our cake and eat it too, however, I'm very much interested.
While I don't think it was meant to be an exhaustive list, the four ways to interact are not complete by any means. Off the top of my head you can also: * Send signals to it (and get them back). I don't imagine anyone has done this, but using the SIGUSR1 and SIGUSR2 you could set up bidirectional communication between one parent and one child. (To avoid other processes interferring just use hamming code / checksums / MAC.) * Read/write to any shared file descriptor, depending on mode. I can't remember the reason, but there's at least one low-level utility out there that you pass an fd (int) as an option-argument and it will read data from that fd. O_CLOEXEC isn't the default.
I could have been clearer. What I meant is that the System.Process API directly exposes four means of communication, and Data.Conduit.Process exposes those. If you open up other file handles (such as via pipe), Data.Conduit.Process won't touch them, and you're free to read from/write to them as you normally would. I think I'll add a section to the docs demonstrating this, and update the description accordingly. Thanks for the input!
You're right, see my [clarification](http://www.reddit.com/r/haskell/comments/2abmwu/rfc_new_dataconduitprocess/citj9fx): &gt; I could have been clearer. What I meant is that the System.Process API directly exposes four means of communication
Now I want to write more Haskell and put it on github :P
To clarify, they are hiring these people to program in Haskell, not PHP.
I agree. Go ahead and respond to it. Notice that they assumed that you must be a senior level developer because of your work in Haskell :) What Haskell informs these recruiters is that you are not afraid of trying something known to be hard. 
Thanks! That is very helpful information, and I'm not surprised to see heavy CI usage; especially with cabal.
It was motivated: the experience of a number of people offering regular help on IRC, twitter, etc. is that the HP has been the first step in getting a new user stuck and frustrated. The advice given to newcomers all too often starts out the same way: remove the HP and start again with GHC and cabal-install. It's not proof that the platform is the wrong choice, but it's frustrating when people who don't support new users insist on offering the opposite advice to those who do support those users. If the people offering help to get people up and running want to support dealing with the HP, then that's the way to go. If they find it easier to get people going without the HP, then why push it? I don't really think there's a right answer beyond what gets people up and running most reliably, so I'd settle for a poll of the most active folks on IRC, reddit, SO, twitter, etc.
I guess there's nothing wrong with them sending out this kind of email. It might even catch a lot of talented people with low self who would never imagine themselves as facebook material. But I don't want people who get these emails to imediately go through a week of excitement and heartbreak when it was really just an algorithm that chose them.
Cabal isn't a package manager, cabal-install is. It does all the usual package management stuff like recursive dependency resolution and downloading, cabal (usually) does the building and installing, ghc-pkg does the real low-level stuff like registering. It's also not supposed to be a distribution-scale package manager, though: It e.g. won't ever install C libraries for you. Given that distributions already have their own managers, it'd be insanity to replace them, and at the same time the info in the `.cabal` files allows distributions to generate distribution packages for haskell packages with no to minimum manual work (like, say, specifying which distro package contains a certain PKGINFO dependency).
I actually didn't know Haskell was supposed to be hard when I learned it. Actually, I started programming in Haskell about 5 years ago when I met dons on IRC. He told me Haskell was easy!
The sysadmin in you shouldn't care, because the sysadmin is going to push statically linked binaries to the server in any case, a server that probably shouldn't even have ghc on it. The developer, now that's a different topic.
Thank you so much for your time. I'll try implementing the REPL first.
No it doesn't. I wanted to get better at Parsec and so I only thought in terms of Parsec. But I will remember to look for simpler solutions next time.Thanks. 
Franky, I'd say Parsec is a bit overkill for a Brainfuck interpreter. Just remove all non-command characters, group the brackets(granted, you have to use explicit recursion), and interpret.
They mention 'on... Haskellers' -- clearly they're looking for people with Haskell skills. You know they have a bunch of top Haskell people working for them already, right?
I don't understand from the example what the waitForProcess race is supposed to be. The example, when built without -threaded, succeeds twice, which seems a little weird. (1,ExitSuccess) (2,ExitSuccess) With -threaded, one thread successfully waits for the process and the other fails because the process no longer exists, which is the behavior I expected to see. (1,ExitSuccess) foo: waitForProcess: does not exist (No child processes) Note that the program succeeds either way. Neither behavior seems like a bad race to me. -- don't hold the MVar while we call c_waitForProcess... -- (XXX but there's a small race window here during which another -- thread could close the handle or call waitForProcess) alloca $ \pret -&gt; do throwErrnoIfMinus1Retry_ "waitForProcess" (c_waitForProcess h pret) withProcessHandle ph $ \p_' -&gt; To see an actual race behavior, I added a threadDelay 100000 between the last 2 lines above. Now the example code demonstrates actual broken behavior: foo: waitForProcess: does not exist (No child processes) - exit 1 snoyman says this race doesn't have a narrow window, but I don't see how that is the case. Certainly seems worth fixing regardless..
If you decide to pursue this, can you please let us know how it turned out? I'm curious to see if employers are doing this for bait-n-switch reasons. It could be one of those "oh hey, ya, we love FP and haskell!" and you get there and the hiring manager tells you that what they're actually looking for is a legacy Java maintenance dev (this has happened to me once already for a small ISV, and once before at a megacorp). Thanks.
+1000 the majority of the IRC support load I have to deal with wrt OS X is solely haskell platform induced. Let me repeat myself: the majority of OS X support issues on IRC that newcomers have (and even some experts!) are HASKELL PLATFORM related. I will do a statistical analysis of the darn IRC logs if need be (though I lack the time to do it properly mind you), but I promise you this, the majority of Mac related support issues have been platform related. on OSX now that the 64bit is the default for everything, GHC works great without any special babysitting.
Your bug and my bug are the same bug: import System.Process import System.Exit import Control.Concurrent.Async import Control.Exception main :: IO () main = do (_, _, _, ph) &lt;- createProcess $ shell "sleep 1" let helper i = do ec &lt;- try $ waitForProcess ph print (i :: Int, ec :: Either IOException ExitCode) ((), ()) &lt;- concurrently (helper 1) (helper 2) return () with your 100000 delay ends up with the following on my system: (2,Left waitForProcess: does not exist (No child processes)) (1,Right ExitSuccess) In both cases, one of the threads throws an exception, the other doesn't. The abstraction ProcessHandle is supposed to provide is that you can wait for the exit code as many times as you like.
I am VERY happy that platform isn't listed for Mac. Why? Because the majority of Mac Related problems people have are due SPECIFICALLY to haskell platform related matters. people NOT using platform reduces the NET support load I and others have for helping newcomers on OS X. Unless someone is willing to pay for volunteer time to cover the increased support load, please don't suggest haskell platform on Mac. Please. PLEASE. Note I'm basing my experience upon helping provide support on the various IRC channels over the past 2 years. 
Ah, the instructions here http://nixos.org/nix/manual/#idm47361539616144 are failing because nix-channel is "unable to check `https://nixos.org/channels/nixpkgs-unstable'" - which seems to redirect to something on releases.nixos.org that presents an SSL certificate that doesn't even claim the right domain name.
Perhaps it's an algorithm based on good reasoning? They don't want to interview 1000 unsuitable people. If their algorithm chose you, it's because there's a reasonable chance that you're suitable. 
An idea to reduce the time spent in `boardDistance` is to compute it incrementally. If I understand what you are doing, the board distance can only change by one after each slide, so just carry around the current board distance and alter it by the effect that the slide move has.
OK, I see your confusion, and yes it is indeed puzzling. One problem is that I'm not sure that "more expressive" is a precisely defined concept. Certainly your typeclass above shows that `Applicative` can be embedded in `Arrow`. However, there are many arrows, like `IOA` that I proposed on the StackOverflow page, that don't arise from this construction. You could say then that `Applicative` is too *strong* to represent some `Arrow`s. But hang on ... `Monad` is supposed to be an even *stronger* condition than `Applicative`. How can `Applicative` be too strong to represent `Kleisi` `Arrows` arising from `Monad`s? Well, notice that the operation 1. taking the underlying `Applicative` of a `Monad`, then 2. taking the `ArrowStatic` induced by that `Applicative` is not the same as taking the `Kleisli` `Arrow` of a `Monad`, so we wouldn't expect them to have any relationship, really. I'm sure there's lots more that can be said about this, and we haven't heard the end of it ...
I have never liked createProcess's interface, and have a pile of wrappers to try to make it better. I think this new conduitProcess interface is better. It does seem that the added type safety breaks down a bit with UseProvidedHandle, since now we're back to the caller needing to provide a CreateProcess input that matches the way its output is used. I also like that conduitProcess runs in IO. I can see using this in lots of places that don't use conduit (sometimes for good reasons, sometimes for legacy reasons). However, this does make me wonder if it belongs in conduit and not a lower level library.
That sounds like something that should be [reported](http://github.com/nixos/nixpkgs)! Could you report an issue?
He meant it would be easy in 5 year time!
I can't find any information about Android in 7.10, do you have any pointers?
I'm jealous. I wish recruiters contacted me about a Haskell job!
If the platform is problematic, I'd like someone to tell me why. And then we can perhaps fix the platform.
Carter why don't you tell me why the platform is bad with OS X. And then why don't we fix the platform so it is better? I suspect the reason the majority of support issues are platform related is because the majority of uses use the platform.
Thank you for your suggestions. I now understand what you meant. Will have a look into it!
I've been trying to get the new ghc working for awhile now. I was waiting for the platform but that's apparently never coming. I don't know what screwed up in my particular upgrade process, but it screwed up pretty badly. I can't imagine a newbie would persist in this. Edit: I finally got it working now, but I had to blow away a lot of stuff to make it work. Sandboxes where have you been all my life?
`(a -&gt;)` has a monad instance.
 cabal sandbox init cabal install SDL2 Worked for me on the first try (except I had to install SDL2-devel system package).
If your goal is writing the interpreter and not the gritty details of the parser then I would just use [language-python](https://hackage.haskell.org/package/language-python) package which has a parser and pretty-printer for the Python 2/3 ASTs. If you want to write your own then you're on the right path with Happy + Alex if your prefer yacc-style grammars or (Parsec + [indents](http://hackage.haskell.org/package/indents) ) if you prefer parser combinators. Edit: language-python has no documentation apparently so it might not be obvious where to start. Here's a sample file http://lpaste.net/107283 and the generated AST http://lpaste.net/107284.
If you only want a basic setup to begin with, I just wrote a post about that: [Haskell development with Nix](http://pavelkogan.com/2014/07/09/haskell-development-with-nix).
The problem has already been fixed with sandboxes. (I know /u/sclv knows what I'm about to say, and I suppose we just disagree about best practices, but I want to be clear.) Starting with the platform today has the problem that it doesn't come with 7.8, and people want to try things like typed holes. Setting that aside, the inability of GHC to support multiple installations of the same version of a package means that updating something for a security fix, feature addition, or whatever will often cause cabal to need to reinstall downstream packages even though their versions have not changed. Now other packages in the database are broken by the reinstalls because the reinstallation strategy wasn't quite transitive enough. This is one face of a problem we used to have: cabal hell. This problem was virtually eliminated some time ago by a band of brave cabal-install developers. The price we pay today for isolated build environments in combination with the ultra-fine package granularity of hackage is slow spin ups of new workspaces. I'm sure this will be fixed, too. We have awesome tool developers.
Just write some Haskell code and put it on Github. That's really all you need to do to get contacted by recruiters, though promoting your github definitely doesn't hurt...
yes, and this is essentially the same as `Reader`, correct? edit: and if this is the case that they are the same, the question becomes: when is it more convenient to use monad comprehensions?
The thing is, comparisons to other language ecosystems are not really valid at all. I am not aware of any other ecosystem that attempts this with such a strongly typed language. Even if version constraints were laid out perfectly on Hackage, this would still make it very difficult. If you think the problem is just a lack of effort or a lazy community - think again. If one takes the time to understand what the problems are and why this is so difficult then maybe they could help contribute to the solution. But do not think it is going to be a simple afternoon's worth of coding a new solver into cabal and bam all problems are solved. I have been toying around with the idea of getting rid of package versions altogether and using an AST to determine if public interfaces have changed. Dependency versioning at the function level. But this would be quite an undertaking.
FWIW I really like http://tanakh.github.io/Peggy/. I find pegs easier to work with.
My [github](https://github.com/gelisam) and [gist](https://gist.github.com/gelisam) pages are filled to the brim with Haskell stuff. How can I promote it?
Definitely post again if they are actually hiring for Haskell positions. I've seen quite a few recruiters use this as a ploy for hiring for other languages since the Haskell talent pool tends to attract often above average developers. "Come write Haskell at &lt;company&gt; with us! P.S. we're a Ruby shop."
Unlearning is both not fun and fun. 
It hasn't been confirmed but after Template Haskell is able to be used in cross-compilation I'm going to be working on GHC to revise and merge in Neurocyte's work. It may well take until 7.12, but rest assured, Android support is coming. Actually, GHC 7.8 works quite well on Android *now* it just requires some patching and use of EvilSplicer and some project modifications. I have lens, linear and a bunch of other libraries installed and they seem to work
You'll note that's only one of many issues, and probably the least important at the moment. Cabal-install already has two different dependency solvers, so it's likely that a SAT-solver backend could be added as well if it became the "tall peg". There are a couple of things that make solutions hard to find: Package upgrades aren't safe in general, and a few core packages can't be upgraded without upgrading the entire compiler. Couple that with the fact that nearly *every* dependency has several dependencies of its own, many of which end up being common, but which might have different version demands (which may or may not reflect *real* version requirements), and you have a hard problem.
I'm beginning to believe this more and more, the languages where this seems like a "solved" problem are usually ones where community, just by the nature of the problems they work on, does very little linking against shared system libraries. Cabal I think is generally not to blame for these problems, and some sort of system-level Nix-like solution is the path forward.
It's misleading to even mention NP-completeness as a potential issue.
Very nice work. Out of curiosity he mentions "Building the documentation archive in the first place is quite error-prone" but does not describe why this is. Building docs from cabal on my small projects has not been a problem, but again they are small. Does anyone know what he is referring to here?
Well I have tons of haskell on my github and never got any letters like this, so you must have something there that got their attention (probably account activity).
When you need both, `divMod` is better than separate `div` and `mod`. Also, `quotRem` is faster and gives the same results when the divisor and the dividend have the same sign. `manhattan` looks like something you can easily memoize. I'm not sure how many times it is getting called, but the calculation only needs to be done n^4 times where n is the puzzle dimension. If it is the most frequently called function, might also make sense to re-write it to be jump-free. I think you can avoid the conversion form (matrix_index, puzzle_size) to (row_index, column_index) that you do twice (once inline for no good reason) in `manhattan` distance... maybe not, but it could be worth looking in to. Pretty sure that `PuzzleState` should have all it's members be strict, and it may also be worth unpacking some of them. When you make one step (`updatePuzzleState`), you only change the position on 2 tiles. That is, the second argument to `(//)` always has exactly 2 elements. That means a maximum of two indexes have changed their manhattan distance. So, you should be able to update `distance` with a maximum of 4 calls to `manhattan instead` of n^2.
Wow, thank you very much for this detailed response! I understand every single point you made and cannot wait to improve the bits that are compromised. Regarding the `PuzzleState` - I tried making the fields strict, but for some reason it only increased the runtime by 0.5 - 1 sec
This. They can use canonical meta tags to tell search engines about the latest version of the library
From my standpoint, I can think of two points. 1) The signature is more concise. When you have a function with more than two or three parameters it's nice to save at least one. 2) You don't need to pass the argument explicitly around. When you have a function which does not need `Config` but calls a function which needs it, you'd have to pass it around. Worse yet, An intermittent function (e.g. from someone else) could modify the `Config` argument before passing it on. I'd also recommend a more general style: `f :: (MonadReader Config m) =&gt; String -&gt; m SomeType`. I only learned about this very recently and I'm not sure if it's applicable everywhere, but it eases writing functions for Transformer Stacks a lot.
* d:\Proj&gt;cabal sandbox init cabal: unrecognised command: sandbox (try --help) * d:\Proj&gt;cabal install sandbox Warning: The package list for 'hackage.haskell.org' is 23 days old. Run 'cabal update' to get the latest list of available packages. cabal: There is no package named 'sandbox'. You may need to run 'cabal update' to get the latest list of available packages. * d:\Proj&gt;cabal update Downloading the latest package list from hackage.haskell.org Note: there is a new version of cabal-install available. To upgrade, run: cabal install cabal-install * d:\Proj&gt;cabal install sandbox cabal: There is no package named 'sandbox'. You may need to run 'cabal update' to get the latest list of available packages. * d:\Proj&gt;cabal install SDL2 Resolving dependencies... Configuring sdl2-1.1.0... cabal: The program pkg-config version &gt;=0.9.0 is required but it could not be found. Failed to install sdl2-1.1.0 cabal: Error: some packages failed to install: sdl2-1.1.0 failed during the configure step. The exception was: ExitFailure 1 
Stacking / transforming. While `(-&gt;) r` ~= `Reader` ~= `ReaderT Identity`, when you start building a stack, you'll find that the lack of a `MonadTrans` instance for plain `(-&gt;) r`. Hiding. Export a newtype for your custom monad stack / transformer that includes Reader, but not it's constructors. Then *most* of the code that uses your Monad(Trans) doesn't need to know the reader argument, only your `runAwesome`/`runAwesomeT` needs to know. But yeah, if you aren't transforming it, or hiding it, I see little advantage over `(-&gt;) r`, unless you need the additional laziness but I can't think of that being useful.
&gt; Worse yet, An intermittent function (e.g. from someone else) could modify the Config argument before passing it on I think this is a great reason to favor some sort of Reader pattern (or similar) over threading the argument. It makes it clearer to the code reader/maintainer what the code intent is. It says "this parameter will be read by this function (or its callees) but not written to" - otherwise State would be been used. Additionally, the code maintainer/user won't accidentally change whatever the reader holds - only purposefully with local. Edit: Corrected based on replies
Ah, yes, this is an excellent point. 
Types are documentation! They aren't always *complete* documentation, but sometimes they really are. In Haskell, there are morally only 3 functions with the type `forall a. a -&gt; a`: `id`, `const undefined`, and `undefined`. Two of them will crash most programs if used, so most Haskell developers know that when they see `forall a. a -&gt; a` as a type its implementation is always `id`. The elevation of side-effects to effects tracked in the type system coupled with parametricity makes it where there are very few implementations for a certain type, and that only a few of those are at all reasonable. Usually, I only need one line of documentation in-context, to determine what a particular function does. The surrounding context is often more important. All of that said, I think most library authors would welcome documentation patches. Writing documentation is similar to the editing process -- best done by someone that doesn't share the same assumptions. Sure, I can go back and "read aloud" my library -- and I do that went going to the haddock 100% achievement -- but I still have so many assumptions that I don't even realize need to be documented, sometimes. Finally, believe it or not, it was much worse 5 years ago. Back then, it felt like every hackage library had no documentation other than the types and a package summary of: "go read my master's thesis in postscript [here](link)".
Basically you're saying that this is the data type: data ProcessHandle__ = OpenHandle PHANDLE | OpenWaitingHandle PHANDLE (MVar ExitCode) | ClosedHandle ExitCode If the code simply respects this, the bug should go away. I think the naming is a bit off though. The final state isn't `ClosedHandle`. The API gives stronger guarantees. The final state is `ExitedAndClosed`. The windows documentation states that closing a process handle does not guarantee termination, so closing a process handle and ensuring termination are two independent events, and on Unix, the 'close process handle' is a no-op. Thus the interesting event here is the guarantee that the process has exited. Thus the naming is off. 
&gt; When you have a function \[**f**\] which does not need Config but calls a function which needs it \[**needsConfig**\], you'd have to pass it around. I think this is the subtly I'm having a hard time seeing. Don't you end up having to do this anyway, basically? (I've annotated your quote). If `f` is being modified to call `needsConfig`, then one of two things will happen: 1. `needsConfig` does not use Reader Monad: `f` will need its signature updated to have a `Config` present. 2. `needsConfig` uses Reader Monad: `f` will have its signature updated to return something in the Reader Monad. both require a minor change to `f`. Upstream, methods that use `f` will need to have their arguments changed (for threading or for the Reader monad), so it seems like equivalent work either way. 
Nice post. Wouldn't using DataKinds and KindSignatures be the most compact option, though? Example: {-# LANGUGE DataKinds, KindSignatures #-} data MessageState = Plain | Encrypted data Message (a :: MessageState) = Message String deriving (Eq, Show) messageLength :: Message a -&gt; Int messageLength (Message s) = length s Now you don't have to pattern match against EncryptedMessage and DecryptedMessage if you just want to access the string, regardless of the phantom type, and when you do want type safety, this will do: encrypt :: Message Plain -&gt; Message Encrypted decrypt :: Message Encrypted -&gt; Message Plain Which you think is better suited for phantom types? This, or the GADT-approach?
[`local`](https://hackage.haskell.org/package/mtl-1.1.0.2/docs/Control-Monad-Reader-Class.html#v:local) seems to allow exactly such a modification
By itself, `Reader` won't prevent the arguments from being modified. For example: &gt; let { bad :: MonadReader Int m =&gt; m Int -&gt; m Int &gt; ; bad m = local (const 4) m } &gt; runReader (bad ask) 1 4 If you leave the type variable parametric, then there is no way to substitute any other non-bottom value. The downside is that the function can't use the value itself, it can only pass it on: &gt; good :: MonadReader a m =&gt; m a -&gt; m a The constraints are the same as if you were threading the argument. One advantage of using the `MonadReader` typeclass is that its `ask`, `local`, and `reader` members can work with any `MonadReader` instance, including but not limited to `(-&gt;)`.
This is an untestable API and thus a type of API that makes Haskell worse than other languages. I don't think you should use MonadIO directly. I think you should use a typeclass that abstracts all the IO actions from System.Process and that is mockable, like class Monad m =&gt; SystemProcessMonad m where waitForProcess' :: ProcessHandle -&gt; m ExitCode ... instance SystemProcessMonad MonadIO where waitForProcess' = liftIO . waitForProcess ... instance SystemProcessMonad MockSystemProcessMonad where waitForProcess' = {- replay mock -} ... Sorry for not being able to exactly spell out how to do it, but having a tight coupling to IO actions like this makes any code that uses this API untestable until a typeclass or other abstraction is added such as to wrap the `Data.Conduit.Process` API. 
Facebook employs Bryan O'Sullivan, who manages a Haskell team that includes Simon Marlow, so maaaaaybe they wanted you to write Haskell rather than PHP.
You mention a messageLength function as the motivating example for treating encrypted and unencrypted messages the same. I'm not convinced. What would that function return? Number of characters for the plaintext message? Or maybe number of bytes of UTF-8 representation? Why not ISO-8859-something? And for the ciphertext, you can not count characters, but only bytes. Those are interesting facts, and hiding them away makes the messageLength function much less useful than separate functions for cipher- and plaintext could be.
`Maybe` I was a bit unclear here (`Just` a bit *sigh*). So let me try to explain my point with an example: -- this function needs Config for itself needsConfig :: Config -&gt; String -&gt; SomeType needsConfig config str = .. $ getSomeValue config -- this function does not need Config for itself -- but it needs Config for the needsConfig function needsNoConfig :: Config -&gt; String -&gt; SomeType needsNoConfig config str = .. wantToCall (needsConfig config str) Now with the Reader monad: needsConfig :: String -&gt; Reader Config SomeType needsConfig = asks getSomeValue &gt;&gt;= .. needsNoConfig :: String -&gt; Reader Config SomeType needsNoConfig = wantToCall (needsConfig str) This version is much more concise. The function which does not need `Config` is not in charge of doing the additional work for passing it around. And as /u/ignorantone said below, the most important reason is type safety. Think of Reader as a global context just as in C only with a compile time guarantee that none of your readers can modify it. 
I think his goal was to demonstrate the necessity of pattern matching when dealing with multiple constructors in GADTs.
Well Brian O'Sullivan, author of Real World Haskell, works at Facebook. They also recently released some Haskell library open source. They do have people writing Haskell. 
Thanks for expanding on this. This is how I understood it, but I see the subtly you expressed. I hadn't considered the static environment aspect of it, which I see now as a huge advantage. Thanks again.
Using unboxed vectors and strict unpacked fields should speed things up a bit by removing indirections.
My main idea there was something like *count the number of bytes being transfered over the network*, but since I chose originally chose to represent both encrypted/plaintext as a String (the original post mentions that it would be base64 encrypted string), I chose to go with a simpler implementation to make an example. The goal is to show that you can have logic which doesn't care about the type of the message, maybe there could be a better example, but the idea remains the same.
Is this enough? allRationals = go 1 1 where go m n = (m / n) : interleave (go m (m + n)) (go (n + m) n) interleave (x:xs) (y:ys) = x : y : interleave xs ys
I'm not the expert on this, but adding strictness is not always an optimization. One reason I can think of is that it reduces the number of ways that the compiler can reorganize your code.
&gt;So is Debian. Not in the same way. I cant upload new packages to debian whenever I feel like it. Id have to convince a debian repo maintainer to accept the package. This is nothing like hackage.
Hopefully your comments will clear things up for anyone who happened to get misled, then.
It is being worked on. https://phabricator.haskell.org/D32
Why not post some of your stuff on hacker news? 
Haha, okay perhaps I understated the importance of promotion. Try putting your github on your linkedin and other places.
If I understand this correctly, this also enforces that a message is always either encrypted or plain but it can't be anything else than that. I felt this was missing from the original solution and whilst the GADT one did that, it puts a bit more noise into the code. I should probably learn more about these things.
Yes. I agree.
I'm working on a post right now, but I could never keep up with the pace at which you are publishing yours! Does it take you a lot of time to write them? I feel like mine take forever (many days...) to write, so I often give up before publishing them.
Author here. Building docs per se isn't hard, but getting them in the right format for Hackage is. A valid Hackage documentation archive needs to fit the following criteria: * Cross-package links must work * It must include Hscolour and Hoogle files * The archive must be in POSIX tar format, not GNU tar * All files must be in a directory named `PACKAGE-VERSION-docs` While the build bot can handle these criteria easily, you can imagine how tedious this would be for a human.
Yes, they take a long time to write. I estimate that I spend between 10-20 hours over the course of a week on my more detailed posts. Shorter posts, though, might take maybe 5 hours or less. When I first began writing I spent even longer. A lot of people assume that the hard part of writing a post is making it long, but actually the time-intensive part is editing it to be more concise and clear. This takes a lot of time when you first begin to write but you get faster at this with practice. If you write about something that excites you then you will be able to persevere through completing the post.
Thanks for the correction!
Sandbox is part of Cabal. You need Cabal version &gt;= 1.18. Before v1.18 there was cabal-dev. [Cabal 1.20 announcement](http://blog.johantibell.com/2014/04/announcing-cabal-120.html) Note that cabal update is telling you that there is newer version of cabal. Also it seems that you don't have good version of pkg-config (not sure howto install it on Windows; it is part of pkgconfig on Linux)
By that logic, any function that needs to read from a file is also untestable, and reading from a file can't be mocked. But we know that's not true: you mock it in your library. You'd be able to do the same thing here. I don't like the idea that every single library ever written needs to be abstracted in such a way that someone can make it do fake work through some poorly defined typeclass. I'd rather just use higher-order functions for that, e.g.: type RunProcess m = String -&gt; [String] -&gt; m (Source m ByteString, m ExitCode) realRunProcess :: RunProcess IO realRunProcess cmd args = do (ClosedStream, src, ClosedStream, cph) &lt;- conduitProcess (proc cmd args) return (src, waitForConduitProcess cph) testRunProcess :: Monad m =&gt; RunProcess m testRunProcess cmd args = do return (yield $ S8.pack $ show (cmd, args), return ExitSuccess) This keeps the original API simple and honest, and avoids a proliferation of ad-hoc typeclasses. Using your approach, just imagine how complicated type signatures would become if you had a process that needed to be tested for 5 different actions that each need their own typeclass. In my approach here, you'd create a record type with each of these different mocked functions, and could just stick them in a `ReaderT`.
Right, so the only objection here is that the platform installs older libraries than users may like? I don't understand how this is even an objection. Is there any objection to platform + sandboxes vs. plain install + sandboxes? At that point, it just seems like taste to me.
Really? What a shame :( Were the strict evaluations (in some cases) causing odd behaviour?
[This post](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) explains the writer monad well.
I just noticed that the code does seem to use `PuzzleState` members in a strict manner; each time any member of `PuzzleState` is accessed, all the other members are accessed in short order. But, you are right strictness is not always an optimization. ISTR a post this year about strictness making someone's code run much more slowly. It may not be an optimization in this case either; profiling will help determine that. The optimizations in `manhattan` and reducing the number of calls to `manhattan` will probably do much more good than adding strictness, at least initially.
The Reader Monad is like passing around an extra argument just with an (arguably) nicer syntax, particularly when there are some "intermediary" functions that do not directly use the extra argument.
Sure, but why not just pass an argument? Why use a reader?
I have no idea. My guess is that Ph.D students were involved and when they graduated there was no one left to do the remaining work required to make it really stick. Unfortunately, many great ideas never get fully realized due to a lack of available labor :-l. 
Neat idea! In real life, though, you would have to watch out not to rearrange a multiplication to a division without checking for (/=0) first ;-) EDIT: fix the condition I gave
There ya go: http://learnyouahaskell.com/for-a-few-monads-more#writer
That is only the start of the problem, the rest of my post described the rest of the problem. If you want isolation, then you can't use a user db as it is now implemented. If you want to have a project that uses something complex like yesod, and do anything else with the same version of GHC on the same computer, isolation is strongly recommended. The upshot is that support is much easier if people just use sandboxes as their default mode of operation, and this doesn't benefit from the HP. Thus, with the HP, we get old versions that can not be updated until the next annual snapshot, and encourage a way of working that we know has serious problems (cabal hell). When cabal and GHC can deal with multiple instances of the same version of a package, then offering a Haskell starter kit that looks like the HP will be a nice thing to offer.
Hm, that video is sort of broken is it not? You can find more on youtube when you search for "bayhac 2014" by the way.
I did read that, but it doesn't really tell me what the writer is or how it works. I would like to know what happens under the hood, not just use the monad. Thanks for the link though :)
Haskell is broken on Windows. Why should I worry about such basic things as a PATH configuration, Linux config (WTF?) or Linux libraries (WTF??) on a system which is not Linux? I would expect the opposite: that Haskell Platform works flawlessly on a world's most popular system. Another thing: * d:\Proj&gt;cabal --version cabal-install version 1.16.0.2 using version 1.16.0.2 of the Cabal library * d:\Proj&gt;cabal install cabal-install Resolving dependencies... Configuring cabal-install-1.20.0.3... Building cabal-install-1.20.0.3... .... Installed cabal-install-1.20.0.3 * d:\Proj&gt;cabal --version cabal-install version 1.16.0.2 using version 1.16.0.2 of the Cabal library **WTF???** Next is pkg-config... This is madness.
I liked the explanation of Reader, Writer, and State in [beginning haskell.](http://www.apress.com/9781430262503) It was worth it to me to buy the book and read through it (not done yet). It's got a lot of stuff in it that would have taken me a really long time to understand on my own. Just a few more examples: lenses, monad transformers, conduit, etc..
You could combine ReaderT, WriterT and StateT monad transformers for that, or as a more efficient implementation there's the RWS/RWST monad/transformer. 
&gt; Why use a reader? http://www.reddit.com/r/haskell/comments/2acj3w/threading_argument_vs_reader_monad/
My guess would be that the call "last xs" causes bottleneck, because it gets the last item by traversing the list completely. Why not use n in it's place, last xs should always be n in this case?
What is the unit on the x-axis of the benchmark graphs?
[Read the source](http://hackage.haskell.org/package/transformers-0.4.1.0/docs/src/Control-Monad-Trans-Writer-Lazy.html). It's not complicated.
If I replace `last xs` with `n`, for some reason the square root condition is never true - it just keeps going for every x.
indeed.
Right above the first graph: &gt; Here is an example benchmark measuring **the time taken to write and then read 100,000 messages**, with work divided amongst increasing number of readers and writers **(time in ms)**, comparing against the top-performing queues in the standard libraries. It would be nice if it was on the graph itself though.
Ah, thank you.
OK how about this, running with the `IOA` example: Suppose your kit of IO consists of instance Arrow IOA readFile :: FilePath -&gt; IOA () ByteString writeFile :: FilePath -&gt; IOA ByteString () By being careful about where I used the `(-&gt;)` arrow and where the `IOA` arrow, I can stage your degrees of freedom such that the filenames that are read from / written to can be known purely, but you can still meaningfully compose the two into operations into something that copies a file: readFile "src" &gt;&gt;&gt; writeFile "dest" :: IOA () () If you were to expose an applicative interface instead, you can't go with instance Applicative IOF readFile :: FilePath -&gt; IOF ByteString writeFile :: FilePath -&gt; ByteString -&gt; IOF () since you'd have no way of composing them (everything is too static), but you also can't go with instance Applicative IOF readFile :: FilePath -&gt; IOF ByteString writeFile :: FilePath -&gt; IOF (ByteString -&gt; ()) since you can't reasonably implement `writeFile` at that type (think of e.g. (\f -&gt; const () $ f "foo") &lt;$&gt; writeFile "outfile" ) On the other hand, if you went with instance Monad IO readFile :: FilePath -&gt; IO ByteString writeFile :: FilePath -&gt; ByteString -&gt; IO () then you have no way of doing any static analysis whatsoever, owing simply to the fact that the `(-&gt;)` function on the right-hand side of `(&gt;&gt;=)` is completely opaque (the usual problem with trying to analyse monadic values). Is this all a fair summary?
What "unagi" means in the name? An arbitrary chosen name?
I just tried replacing last xs with n and it works fine for me. If you don't mind using an external library, data-ordlist has a [minus](http://hackage.haskell.org/package/data-ordlist-0.2/docs/Data-List-Ordered.html#v:minus) funtion that you could use instead of (\\\\), the benefit being you wouldn't have to end the list you're subtracting with an n. Also, in a prime sieve you can start at the square of the number you're sieving with: instead of 10,15,20,25,... you can do 25,30,35,40,... because the previous ones were sieved by smaller primes (This won't cause a huge performance increase for small numbers, but as the sieve gets larger, it will help). I've modified the parts I mentioned below where comb xxs@(x:xs) | x^2 &gt; n = xxs | otherwise = x : comb (xs `minus` [x*x, x*x+x .. ])
Yes I think that's an excellent summary.
It means 'eel' in Japanese. Not sure if that was intended or not.
Is this restricted to x86 alone or would this work on with 64 bit executables?
&gt; The libraries `haskeline`, `xhtml`, `terminfo`, `transformers`, and `haskeline` are now exported and registered in the package database. They previously shipped with GHC but were not registered, leading to errors where shared objects could be overwritten. ([issue #8919](https://ghc.haskell.org/trac/ghc/ticket/8919)). From the ticket, it seems that that [last](http://www.viva64.com/en/b/0260/) `haskeline` should be `hoopl`.
Do we know why this changed? Was it the result of some bug fix?
If you expected `last xs` and `n` to be the same but they don't seem to be, it might be helpful to add a few calls to `trace` in order to see their values: import Debug.Trace ... ... | x^2 &gt; traceShow (last xs, n) (last xs) = xss ...
Translating subsets of code from one language to another? I wrote [obj2java](https://github.com/gelisam/objc2java), a tool for translating method calls from Objective-C syntax to Java syntax. It didn't go very well, so you might want to learn from my mistakes :) My [AST type](https://github.com/gelisam/objc2java/blob/master/src/Text/ObjC.hs#L20) is quite simple: data Expr = Var String | StringLit String | MethodCall { target :: Expr , method_name :: [String] , args :: [Expr] } | FunctionCall { function_name :: String , args :: [Expr] } deriving (Show, Eq) I did the parsing via [Text.Syntax.Parser.Naive](http://hackage.haskell.org/package/invertible-syntax-0.2.1/docs/Text-Syntax-Parser-Naive.html) from the package `invertible-syntax`, because I thought the idea of creating a parser and a pretty-printer at the same time was a wonderful idea. I still think so, but now I understand better why they put the word "Naive" in their module name: it's very slow. I noticed they don't have explicit `try` commands like Parsec, so maybe they use the naive exponential backtracking algorithm every time it encounters a choice.
I had a similar issue with yesod-bin, discussed [in the issue tracker](https://github.com/yesodweb/yesod/issues/748). Austin pointed me to [this bug report](https://ghc.haskell.org/trac/ghc/ticket/8768) against GHC. Not sure if it's the cause of this specific API change though.
They won't always be the same, the last element in the list might be sieved out.
It's because they are *slick* and *lightning fast* ^^^^I'm ^^^^just ^^^^making ^^^^this ^^^^up
If you're going to use the happy parsing library, you're allowed and even encouraged to use left recursion: http://www.haskell.org/happy/doc/html/sec-sequences.html
I did, the gist served as a draft to be shown here. My Github repo gives you full credit with the link to the blog post.
`xs \\ [2*x, 3*x .. n]` is bad, it has to compare every element of `xs` against `[2*x, 3*x .. n]`. Either write your own `\\` that assumes the lists are sorted and operates in one pass, or just use `filter (\k -&gt; k `mod` x /= 0) xs`.
Hi. Either i386 or x86_64, i.e. 32-bit or 64-bit intel architectures. I wasn't sure the correct umbrella term. **EDIT**: sorry, to be clear it should work correctly (in that `fetchAddByteArrayInt` should fall back to a CAS, or some other atomic op supported by the architecture), but you would probably be better off using a different queue design in that case, i.e. you'd be wasting the CAS on a contended counter, where other queue designs do more real work there. But I haven't thought much about it. 
Thanks, I really appreciate it :)
I understand that, but I was specifically trying to write an E-sieve - division checking is a different algorithm altogether.
A simple AST for a subset of JS could look like this: data Term = Variable String -- x | Number Double -- 1.5 | String String -- 'Hello, World!' | Function [String] Term -- function(a, b, c) { ... } | Call Term [Term] -- f(1, 2) | Member Term Term -- o["label-" + n] | UnaryOp Unary Term -- !a | BinaryOp Binary Term Term -- a + b | TernaryIf Term Term Term -- p ? q : r data Statement = Term Term | Return Term | Var [String, Option Term] | If Term Statement (Option Statement) data Unary = Negate | Not data Binary = Add | Subtract | Multiply | ... type Program = [Statement] Eventually you might want to add position information or attach other things to the syntax tree, but this approach will get you started.
I wish they'd pay attention to my [gitorious](https://gitorious.org/~bss03), rather than asking me to use a proprietary interface for free software.
That could explain why the version with `last xs` was sometimes faster than the version with `n`!
&gt; write your own `(\\)` that assumes the lists are sorted Doing this should still leave you with an e-sieve, and avoid some slowdown. 
This is the sort of reason why we want a _GLASGOW_HASKELL_PATCH_LEVEL_ CPP macro.
Is the [Haskell wikibook](http://en.wikibooks.org/wiki/Haskell) still being updated? It's still a "book", but you can jump around a bit between topics. There's also [RWH](http://book.realworldhaskell.org/), and [WYAS](http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours). Check out interesting packages from hackage and try to write small apps around them to "exercise" the API. I would recommend [PCPH](http://community.haskell.org/~simonmar/pcph/) as a more advanced book, but it doesn't exactly cater toward your interests, that I can see. There's also the new [HDAC](http://www.packtpub.com/haskell-data-analysis-cookbook/book) -- which should target your data science interests well. When doing Haskell programming, I find jumping on #haskell on IRC (Freenode) the quickest way to fill in the gaps between what I've read and what I'm writing, although it can also be a distraction if you start answering too many questions. :P
Exactly
It would go from 10000 to 9973, not really a significant difference. Plus `n` is constant and `last xs` would have to be computed each time because it can change. So I don't think that would explain it.
Trying to explain it in words would actually probably be longer than the source itself, it's quite short. It just passes the extra parameter to the function under the bind, and ask get's that parameter. newtype Reader r a = Reader { runReader :: r -&gt; a } instance Monad (Reader r) where return a = Reader $ \_ -&gt; a m &gt;&gt;= k = Reader $ \r -&gt; runReader (k (runReader m r)) r ask :: Reader a a ask = Reader id mtl uses the transformer form of this but the idea and implementation is the same. 
Ok, it looks like the claim you're making is the problem comes from the "social harm" of the platform structure implicitly steering people away from sandboxes...
OK, I admit blame here. :) ---------------------------- First off, I put on my release manager hat. \#8768 (the bug report Michael linked) is definitely a nasty one and yes, it unfortunately required an API break. However, it affected a very well-used package, and the end result of crashing `gcc` because we confused it so hard really needed to be fixed. The API change here is minor, and unfortunate, but necessary. The changes in `HsExpr` are something else; there were about half a dozen related tickets that fixed several issues in the frontend. Most importantly, several of these were implemented with pattern synonyms, some of them very nasty interactions in the frontend that could cause weird crashes or failure to compile legitimate programs. We had a raft of these, and we wanted to fix them. So we pulled in some changes. But fixing this was tricky because HEAD diverged from upstream a while back, so you have to: 1. roll with all of them, and absorb all the changes. In this case, it required changes in `HsExpr` and interface file changes. 2. or have everyone carefully craft patches for two separate branches. Both of these suck. We opted for #1 and in the process fixed half a dozen bugs or more (over the 50 we already fixed in 7.8.3), because having everyone craft separate fixes (#2) makes things very weird, and adds a lot of burden on the developers who mostly leave the STABLE branch to me. Of course, it could be (legitimately) argued this is my fault for not doing this myself and just doing it to make the changes minimal. But people forget that *my time on GHC is limited*! It significantly increases the scope my work to merge changes in such a manner, and for the bugs we wanted fixed, it would have been a lot more time. And that's not necessarily time we've even signed up for (I am, after all, a paid engineer for Well-Typed, have plenty of other responsibilities, and have limited amounts of money available from clients at all. I'm not whining that my life is harder than anyone else. But I do juggle quite a lot of stuff sometimes, including an active community that wants new fixes *and* features. And GHC is a really atypical project). So it's clear that perhaps we should not have been as cavalier about bringing in API changes. I think the precedent (\#8768) was OK but we went a bit overboard in adopting some others. But I think this polishes off many of the features in 7.8 anyway. We've traded some of our time, and release time, for patches downstream from end users like y'all. It's clearly not as obvious if this was the right choice, and it's something we (well, *I*) overlooked. So, from a management POV, I'll accept full blame for adopting some changes we probably shouldn't have, based on a sound precedent, but taken a bit too far. I'm sorry for the work I've caused maintainers due to this err in judgement. ------------------------- Release manager hat off. In all honesty, I know I don't have the most 'typical' use cases from most of the Haskell community, but in my personal advice? I *really* advocate you upgrade to GHC 7.8.3 if you're using 7.8 already. We already pretend 7.8.1 doesn't exist, after all ;) We fixed about [60 bugs this time around](https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-7.8.3) with 5 or 6 of these being critical issues (in fact, a few others could arguably be critical too). Considering *how freaking huge* the 7.8 release was, I feel *considerably* better about the stability of things with a lot of these changes in place. The stability and robustness should increase quite a lot on every platform, as far as I can tell. I say this because I see almost every single patch, bug fix, and issue that gets filed, and I think the amount of bugs we fixed was great and really polished off rough edges. Many of them were really bad. **And we couldn't have done that without everyone busting their ass finding those bugs**, users and developers (GHC and the community at large). So thanks a lot to everyone for that! Luckily, it seems 7.8.3 will be in the most wide distribution as it will be scheduled to be in the Haskell Platform. So hopefully most people will not have to work too hard to support it, and suffice to say they'll have to anyway. **TL;DR** I fucked up a bit, yes, there are some API breaks, and we let scope creep on some of the issues hit us. But I think 7.8.3 is actually really solid from a stability POV, especially given the time/resource constraints - so I hope this doesn't cause a revocation of my Haskell license.
There is an [episode of "Friends"](http://www.imdb.com/title/tt0583570/) in which this word features prominently. Ross has been taking some martial arts classes and claims that he has "unagi" -- some sort of nearly supernatural awareness of his surroundings. Of course, he doesn't really, so Friends-style hilarity ensues. Oh my, that was 14 years ago.
My original point was the social harm caused to folks offering support. Calling isolated builds a social issue seems a bit fuzzy to me (are then all technical decisions social issues?). Tooling is far better these days than it was a couple years ago. This is of course why the page we're talking about was written the way it was, and I think it makes sense to encourage people to use the tools that were successfully developed to eliminate a real pain point. Edit: If we're looking for some kind of compromise here, perhaps an easy-ish way forward is to have cabal-install print out instructions on the use of sandboxes when the user tries to do a reinstall in a user db.
In practice you can use `MIN_VERSION_ghc` from Cabal for that right now, as Michael linked to above - can't you?
sigfpe has a blog entry on Threading/Reader Monad/Coreader Comonad that may address your question. Also consider the Comonad Coreader! The blog entry is here: http://blog.sigfpe.com/2008/06/categories-of-polynomials-and-comonadic.html
It's normally recommended to not use records for types that have more than one constructor as you then get partial functions for your accessors (they call `error` if called on the wrong value). 
I didn't interpret it as you blaming me. But I decided I'd take the time to carefully explain the decision making, and yes, in retrospect I'd say it's a minor fuckup on my part, but a necessary evil. That's my own conclusion. :) No ill-will implied or conferred at all! :D &gt; Now, what would be nice if we had some kind of report of all API changes between releases, but that's really a general purpose tool that would make life nicer for many different package users, not just ghc the library. Can somebody just create a kickstarter for this or something already? I've been wishing for this tool for *forever*.
This really is a great response. Just having a dry list of bug fixes with the release notes loses the context that this is the 7.8 we need.
It would be because `n` is larger than `last xs` for every call to `comb` after a point (unless `n` is prime); after that point, `x^2 &gt;= n` is true a lot less often than `x^2 &gt;= last xs` is, forcing you to go to the otherwise branch and do a whole `\\`, which, as others have pointed out, traverses its first argument once for every element of the second. You'll still get a correct answer using `n`, but it takes longer than it should. But it shouldn't take *that* much longer... Your best bet is to have `comb` also take the max value of the list and write your own `\\` that takes advantage of the orderedness of the list and also returns the maximum of its result.
If you can point out how to use those tags we'd be very grateful. From my reading, I've not been able to see how to get search engines to point to the unversioned URLs. Here's the problem: we want to point people initially to unversioned URLs, and then that will redirect to the latest versioned URL. We redirect to the latest versioned URL because we want any links that people save to individual things to continue to work, so they have to be versioned. But of course for search engines, we want them to stick with the unversioned url, and not the versioned one that it's currently redirecting to. The search engines however always pick up the latest versioned url because they follow the redirect. As far as I can see, the meta tags cannot help us there. But if you know how to make it work that'd be great.
Yes, I didn't intend to blame anyone, it's great to see GHC releases. But it's already daunting enough to use the GHC API, without unexpected changes! It'd be good to see API changes in the release notes.
Fwiw, I'm still planning to add such a macro during the GHC 7.9.x cycle. But as has been pointed out, this was a library-based breakage which can be detected via `#if MIN_VERSION_ghc(7,8,3)` (at least if Cabal is used and the `ghc` package is build-dep'ed on).
Yay! Not having the terminfo library exposed was causing me heaps of problems. This was a large barrier for users of vty. They'd have to install terminfo; ignore the conflicts ; then install vty. Ouch! Not terrible, but now they can just install vty with no prep.
GHC for Mac OS X (self-contained relocatable GHC + Cabal) has been updated to GHC 7.8.3 http://ghcformacosx.github.io/
We need something like Patreon for software dev.
Actually, whether or not you use actual division or something like `Data.List.Ordered.minus`, this is still basically trial division, and will perform nothing like an array-based Sieve of Eratosthenes. Basically, every number in the sieve is going to be "touched" by every prime number from 2 up until it's first prime divisor, or the square root of said number, whichever is smaller. Whereas in an array-based sieve, every number is only ever touched by the primes that are in fact a divisor.
A more up to date book than Real World Haskell is [Beginning Haskell](http://www.apress.com/9781430262503). I think it's pretty good, and it definitely covers modern libraries (e.g. lens) and changes to Haskell (e.g. it mentions functor becoming a 'superclass' to monad) and such. LYAH is pretty shallow in comparison. I highly recommend you supplement your learning materials; LYAH is insufficient.
I know, I noticed right away as I was pasting it! It's surprising how little time (I shelved that project one year ago) it takes for one's coding standards to evolve to the point that old code looks icky. And it's not just the coding standards which evolve quickly, it's also the toolset: the code had bitrotted and no longer compiled (it's fixed now).
Site Integrity will be using Haskell in the London and Menlo Park offices as Haxl becomes productionised for spam fighting &amp;c., but that’s less like a programming position and more like a position that involves a fair amount of programming.
You should check out the RecordWildCards ghc extension.
Is there a way to make that work for GHC installed from the `ghc-7.8` branch? `MIN_VERSION_ghc(7,8,3)` works ok for GHCJS now that 7.8.3 has been released, but I haven't found a good way to distinguish 7.8.2 from 7.8.2.20140701 for example. 
I think you're too afraid of mutable state. Using `ST` you can use mutable state in a principled way. When writing VMs, I've had mild success in having a 'frozen' pure state that holds immutable arrays and a 'live' impure state that holds STUArrays. The pure state is great for display, debugging, etc, while the impure state can be used to execute many instructions at once until the next time at which user input is relevant (or, if you are simulating a 'real-time' system, until enough cycles have passed to match your display rate). I used this technique to implement the VM for the [Synacor challenge](https://challenge.synacor.com/) and it made writing a debugger pretty easy, while not relying on O(n) immutable array changes. Problems like this are where linear type systems like Clean's shine. In a language like that, you get fast array updates while not having to dip into an impure monad.
On the wikipedia page, it says that supporting left recursion removes the unlimited lookahead feature, but the peggy website notes it *extended* the properties with left recursion. Does peggy support lookahead or not?
yes, not just exceptions, I'd like to save as much comments layout as possible
I don't think you'll be able to do this yet, unless we changed the version policy for minor releases somehow to accommodate, OR we could extend `MIN_VERSION_xxx` to take 4 arguments (or add another macro).
At first glance, Beginning Haskell seems perfect! Do you know if there is a PDF available? I'd love to have that *and* a print copy, but I don't want to buy both separately. 
Take a look at [The Genuine Sieve of Eratosthenes][1], (pdf). It expands on /u/lpsmith's point about times touched. [1]: http://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf
I have the e-book version. I doubt that you can get both electronic and physical copy for the price of 1. :( Note that the e-book is cheaper on [Amazon](http://www.amazon.com/Beginning-Haskell-A-Project-Based-Approach-ebook/dp/B00HG2CQ1Q/ref=tmm_kin_title_0).
Forgive my lack of understanding but do you mean a tool that automatically discovered and reported API breaking changes?
homebrew needs to update it or offer it as a separate install. Currently 7.6.3 is the only install via homebrew.
I thought Ryan's atomics lib works on EVERY platform? :) 
&gt; I suppose it comes down to being able to write a function `super :: Dict a -&gt; (b :- a) -&gt; Dict b` I can see how `Dict b -&gt; (b :- a) -&gt; Dict a` should be inhabited, but why would the converse also be? In particular, does `(:-)` support weakening? That is, if `Show b :- Show a` is "true", is it also true that `(Show b, Show b') :- Show a`? If so, you won't be able to write `super`, because you'd have to be able to implement a Dict for any possible constraint.
Yes, there is [weaken1 :: (a, b) :- a](http://hackage.haskell.org/package/constraints-0.3.5/docs/Data-Constraint.html#v:weaken1)
The common refrain here is that you shouldn't install GHC through Homebrew, anyway (since the maintainer is apparently not exceptionally in touch with how it must be correctly packaged). In lieu of that, though, you can currently get 7.8 (most recently 7.8.2) with `brew install ghc --devel`. [GHC for Mac OS X](http://ghcformacosx.github.io) is way nicer, though.
Sorry if this is too much to ask, but could anyone TL;DR? The content sounds great, but it is too long.
Thanks! I'll "try before I buy" with the Google preview for a bit and then make a decision. 
Ah, I see. We can take an entailment `(a :- b)` and arbitrarily weaken it: &gt; p :: a :- b &gt; p . weaken1 :: (a,c) :- b So, a proof of `b` does not act as proof of `a`. It's a fallacy of the converse. I didn't see it at first. Thanks!
PCPH sounded pretty interesting. I feel like knowing that could help me in my potential data science roles later on. I am honestly only a few months into getting into the nitty-gritty of computer science. Before this I was a statistician only. Data science (better: machine learning/AI) has (have) helped me pave my way into computing, which I am loving. I plan on getting into HDAC after I get more comfortable with Haskell. I think a combination of just writing code, looking at that Haskell wikibook, and seeing how I like Beginning Haskell/LYAH together should get me started. I will definitely start playing around with more packages. Thanks! 
I was only kind of joking, but it seems more people took this seriously than I thought. :) Perhaps some enterprising Haskeller out there would like to try it...
Will the haskell platform now finally be released in an updated version? 
Note: Haskell Platform 2014.2.0.0 - which includes GHC 7.8.3 - is in alpha - and expected to go final within two weeks. No new major packages this time 'round, but has finally updated to the latest OpenGL suite of packages. For Mac &amp; Win users it, as always, will feature a native installer. The Mac installer automatically handles clang/gcc set up; proper cabal configuration; global doc index; an uninstaller; and a new command line tool: activate-hs, for switching between multiple installed HP/GHC setups. Plans for Windows include 32bit and 64bit releases. Plans for Mac include 64bit with support back to 10.7. If you want/need 32bit and/or 10.6 compatibility - please leave a comment here! 
The basic name for this is "atoms" or more generally "interning". The lisp technique, in the list case, that this relates to historically is "hash-consing" (http://en.wikipedia.org/wiki/Hash_consing) edward kmett of course has a library: https://hackage.haskell.org/package/intern here's a simpler one that doesn't deal with shared structure as cleverly: http://hackage.haskell.org/package/simple-atom 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Hash consing**](https://en.wikipedia.org/wiki/Hash%20consing): [](#sfw) --- &gt;In [computer science](https://en.wikipedia.org/wiki/Computer_science), particularly in [functional programming](https://en.wikipedia.org/wiki/Functional_programming), __hash consing__ is a technique used to share values that are structurally equal. The term *hash consing* originates from implementations of [Lisp](https://en.wikipedia.org/wiki/Lisp_(programming_language\)) that attempt to reuse [cons](https://en.wikipedia.org/wiki/Cons) cells that have been constructed before, avoiding the penalty of [memory allocation](https://en.wikipedia.org/wiki/Memory_allocation). Hash consing is most commonly implemented with [hash tables](https://en.wikipedia.org/wiki/Hash_table) storing [weak references](https://en.wikipedia.org/wiki/Weak_reference) that may be [garbage-collected](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science\)) when the data stored therein contains no [references](https://en.wikipedia.org/wiki/Reference_(computer_science\)) from outside the table. Hash consing has been shown to give dramatic performance improvements—both space and time—for [symbolic](https://en.wikipedia.org/wiki/Symbolic_computation) and [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming) algorithms. [*[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)*] &gt; --- ^Interesting: [^Eiichi ^Goto](https://en.wikipedia.org/wiki/Eiichi_Goto) ^| [^Sharing](https://en.wikipedia.org/wiki/Sharing) ^| [^Hash ^table](https://en.wikipedia.org/wiki/Hash_table) ^| [^Cons](https://en.wikipedia.org/wiki/Cons) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ciuxrhj) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ciuxrhj)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
My approach after LYAH is to write a non-trivial, "real world" program from scratch. It really helps you to learn things instead of just reading about them. If you're stuck on any problem then you can ask on IRC or SO. Keep up with reddit as it gives you a lot of opportunity to learn about material you otherwise wouldn't by just 'doing'. (e.g. the blog post on Phantom types from a day or two ago was really cool)
Thank you a considerable lot!
Oops meant to elaborate, thanks. Edited original comment.
We don't maintain the homebrew formula, sorry (I used to be an active contributor). I assume they will update it soon enough.
I'm not sure why we shouldn't be able to reduce constraints defined by instances to their preconditions. Then again, I'm not terribly familiar with how ghc itself does it, or how its process is affected by the various extensions that this sort of programming requires.
I shared this mostly to promote those Steve Awodey videos. I thought they were just fantastic. 
Semi-unrelated note: this is a great place for using phantom variables to ensure that your references don't leak!
While `sum` is defined in the Haskell Report in isn't implemented with `foldl` in current versions of GHC. Additionally it is typically compiled to a strict version in optimized code by GHC. What are the particulars of the problem you're experiencing? http://hackage.haskell.org/package/base-4.7.0.0/docs/src/Data-List.html#sum
What are phantom variables?
&gt; 2014.2.0.0 We had a 2014.1.0.0 ? 
Would something like Agda's [Instance Arguments](http://people.cs.kuleuven.be/~dominique.devriese/agda-instance-arguments/icfp001-Devriese.pdf) *with* recursive instance resolution perhaps give a nice balance between convenience and modularity? It seems that this could come with confluence or coherence. I see the downside of giving up global uniqueness, but I'm curious what you'd say about this trade if it could be done either way.
Here is some commentary by Don Stewart on the definition of `sum`: http://stackoverflow.com/questions/23893320/why-isnt-this-recursive-function-being-optimized-haskell/23893575#23893575 and the article "Fixing foldl" mentioned in that answer is also a worthwhile read: http://www.well-typed.com/blog/90/ 
Is there also plan to upgrade to the latest MinGW on Windows? The one bundled with the platform is ancient.
&gt; I understand why one might want to use foldl in a lazy manner, As an aside: why?
It IS? Then how is it still overflowing with "sum" but not with "foldl' (+) 0" in GHC 7.6.3? Why is it defined as "foldl (+) 0" in Prelude when I looked at the source 1 minute ago?
The ifdef ``USE_REPORT_PRELUDE`` wraps the definition, and isn't defined by default. So it's using the later definition instead of the foldl version.
Are you compiling with ghc ( not ghci ) with -O2? Should never use ghci as a reference for performance. 
Yes, with -O2. However, I did just notice that I'm importing Data.List--but the definition for sum is the same.
Graphs contain shared nodes. There is a library by Erwig called fgl with operations to build and run graph based calculations. Some papers are available too.
Graphs contain shared nodes. There is a library by Erwig called fgl with operations to build and run graph based calculations. Some papers are available too.
See my comment earlier.
I think it'd be more precise to say there isn't any maintainer, but same idea :) 
Awodey was the one who presented Yoneda in a way that finally made sense. Yoneda is about "completing" a category by embedding it into a nicer one. It's a lot like the process of completing the rationals to give you the reals. The category of functors you get from it is complete and co-complete as well as a CCC and a topos. It's everything you want, while looking a lot like the categor you started with.
I'm writing a series of blog posts on this idea now! Here's the first one http://tel.github.io/2014/07/12/mutable_algorithms_in_immutable_languges_part_1/
I think you're misreading /u/dcoutts's comment. He explained precisely why that solution doesn't work.
How about we do away with the redirects altogether? So the unversioned URL serves the latest version directly. We can then add a "permalink" button which gives the versioned URL.
I don't think think there's an operational way to get the instance constraint back out of the dictionary. 
`elem x = foldl (\y z -&gt; y &amp;&amp; x == z) True` `elem 1 [1, 2, undefined, 3]` returns `True`. If the list were evaluated strictly, there would be an error.
But elem should be implemented as a foldr, not a foldl. What I claim in my ["fixing foldl"](http://www.well-typed.com/blog/90/) blog post is that to a first approximation there are no cases where you want foldl, you only want foldr or foldl'.
What's the status of the clang/gcc issue? I thought that GHC 7.8 is basically fine with clang without any extra workarounds required. I remember some packages like OpenGL relying on gcc cpp specifics, is that still an issue?
Are there any actual plans to fix `foldl`?
Would it be acceptable to thread a constraint around on the constructors of the GADT? For example: {-# LANGUAGE ConstraintKinds #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE KindSignatures #-} {-# LANGUAGE FlexibleInstances #-} import GHC.Exts data Fmt :: (* -&gt; Constraint) -&gt; (* -&gt; *) -&gt; * -&gt; * where Int :: p Int =&gt; Fmt p r Int (:&amp;) :: (p a, p b, p (a,b)) =&gt; Fmt p r a -&gt; Fmt p r b -&gt; Fmt p r (a,b) Lift :: p (r a) =&gt; r a -&gt; Fmt p r a instance Show (Fmt Show r a) where show t = case t of Int -&gt; "Int" l :&amp; r -&gt; show l ++ "&amp;" ++ show r Lift r -&gt; show r newtype I a = I a deriving (Show) test = show ( (Int :: Fmt Show I Int) :&amp; Lift (I ()) ) I wasn't sure if the goal was to have a Show1 instance specifically, or if Show would be acceptable.
7.8.3 has a new settings file feature that lets you configure the program and options used for pre-processing source files (if using -XCPP). This can be set as needed for clang or for gcc. It's not automatic, but once set, you're good to go. The platform's activate-hs script will set this correctly when activating 7.8.3 or later.
This is nice, although there are a lot of unsafe functions that are also included. I do have a lot of boilerplate with including `Data.List`, `Control.Arrow`, `Control.Applicative`, and `Control.Monad`, but this is a bit excessive.
Because convenience, that's why! Seriously though, when you start seeing something like the following in your every module, you begin suspecting that there's something wrong with the standard prelude. import Control.Applicative import Control.Monad import Data.Monoid import Data.Foldable import Data.Traversable The standard prelude is targeted at beginners, that is why it does not export some advanced, yet essential APIs, and prefers list-oriented functions to polymorhpic ones. This package addresses these issues.
maybe there should be a cabal extension to {-# obtain-imports #-}
The current `sum` is more lazy than its strict counterpart. This is almost never needed and sometimes a serious performance pitfall, which suggests it's simply historical. Considering how hard it is to write an example that doesn't work with `foldl'`, one might even doubt changing `sum` to be strict by default would break many things. http://lpaste.net/107381
It would be excessive if there was any real price to pay. The only price there is is the increased possibility of name conflicts with local definitions, but then do you plan to ever implement functions with names like `unsafePerformIO` or `unsafeCoerce`?
See also http://hackage.haskell.org/package/io-memoize
It isn't entirely clear to me why you introduce the `Mem` type class with associated types instead of something concrete.
I just skimmed the first paragraph so I can't obviously say anything on the quality of the article. Said that, I'm really looking forward to seeing more of these, since as the author says, when we go from &lt;ImperativeLanguageX&gt; -&gt; Haskell, a lot of "arrows" in our quiver (no pun intended) simply vanishes (arrow being here algorithms and data structures). Of course we have Okasaki, we have high quality papers (thinking about "Structuring Depth-First Algorithms in Haskell"), we have the work Edward is doing on succint data structures, yet I feel that all this stuff is scattered across the internet and we do not have a sort of "Ariadne's thread" to guide us, consistently, in this maze. My wet dream would be a book like Skiena's "the algorithm design manual", but completely focused on immutable data structures and algorithms on them. As said, Okasaki is a good start, but it doesn't cover many of those. TL;DR: please OP keep 'em coming! :)
Oh, cool, I missed that while googling for related stuff. So, x &lt;- f &lt;$&gt; Compose (eagerlyOnce a) &lt;*&gt; Compose (eagerlyOnce b) wait (getCompose x) will start computing `a` and `b` simultaneously, and then combine them with `f`, right? I'm in the middle of reinstalling my Haskell setup so I can't test it. :)
Worked perfectly for me, btw. Thanks for making this!
That idea does sound viable. One catch is that currently documentation is served as static files. Essentially we take the HTML Haddock gives us and pass that on unmodified to the client. So we'll need to do some munging to add that permalink in. Not elegant, but doable. If you have the time, I invite you to [contribute a patch](https://github.com/haskell/hackage-server) :)
When you have an operation that isn't associative, foldl vs foldr becomes important
It seems that the haddocks don't show what is exported. Why?
Really great post Edward. I personally have been thinking about this problem for a long time, and I'm convinced that global uniqueness is an undesirable property for type class systems. First, for Set/Ord, it's not really clear that the current datatype is even the right one. The Set type really should include the implementation of Ord in the data structure, and not accept another instance for each exposed function. Insert should be [insert :: a -&gt; Set a -&gt; Set a] with no constraints. This means that the implementation of (Ord a) is picked once when the Set is created, and the internal invariants of the Set are never confused. Although, this becomes problematic for [union :: Set a -&gt; Set a -&gt; Set a]. Which Ord does it pick? From the first or second Set? I guess you would need to say it picks the one on the left. What you *really* want is to index the Set by the kind of ordering, which you can do in dependently typed languages. [union :: Set ord1 a -&gt; Set ord1 a -&gt; Set ord1 a] would enforce that the two sets have definitionally the same ordering function (ord1). Or something. I still don't know how to properly resolve this issues in Haskell-like languages. If we were to give up on global uniqueness, we would get problems like these. It's not clear they are worse or better problems. Saying that (Set a) can only order things in one way was silly to begin with. But saying that an instance of (Set a) has things ordered in a particular way is perfectly sensible. -- Back to the design issue of type classes, I've always wondered if we should instead be allowing for scoped introduction and hiding of instances. We would still expect that resolutions are unique, and provide a declarative resolution semantics that a programmer could reason about. But as your blog nicely points out, we are already capably of "picking" different instances, packaging them up, and sending them across module boundaries. In designing backpack, I think you have a very difficult design task ahead of you. For current Haskell I believe there is no better design than the current one. But for anyone else designing the next Haskell... please give the type class system some fresh thought :)
Some concrete advice: If you plan to use it monadically, give it a monadic signature. If you plan to use it functionally, keep the functional one. It's so easy to move between (a -&gt; b) and (Reader a b). Do what's most convenient for you. Now, for larger monadic types, it's usually wise to keep the monadic signature, because moving between the monad and the model becomes much more cumbersome...
Dramatic suspense? :)
It'd be neat if you could write: import base preferring (Data.Foldable,Data.Traversable) Which would import _everything_ from base, and that would hide every identifier from _other_ modules that conflicts with the ones specified. You could also then write: import text preferring (Data.Text,Data.Text.IO) qualifying (Data.Text.Lazy as L) Which would give you everything from the text package but `Text` and `pack` and `putStrLn` would refer to `Data.Text.Text` and `Data.Text.pack` and `Data.Text.putStrLn`. Also would be nice: import qualified conduit as C unqualifying (Conduit) import qualified conduit-extra as C import qualified http-conduit as C Which would give you _everything_ from the conduit packages (lots of modules) qualified under C, but Conduit the type would be unqualified because that makes type signatures look pretty. There are a tonne of ways Haskell's imports could be way more awesome. Right now we're doing just above the bare minimum. While we're at it, there're crazier proposals: I swear I've seen `renaming` once either in Haskell 1.4 or in a proposal in the 90's: import Data.ByteString renaming (ByteString to Bytes) And injecting: import Data.ByteString as L injecting (sortBy) Now you can write `L.sortBy` even though `L` is defined in the current module, but it's really a function that semantically belongs in `Data.ByteString`.
Isn't ST the monad to use for mutable algorithms ?
I think he is just a little bit frustrated that it doesn't work out-of-box - little misconception about build systems.
The second definition is just as lazy in the sum as the foldl version, so it doesn't change anything except as optimization (though -O2 always strictify a sum in my experience). I agree that having sum defined as foldl (+) (or equivalent) doesn't make sense... The very rare case where you may want that (though you more probably wish for a foldr (+) even in those) are almost never encountered by someone who wouldn't know that he needs to be explicitly lazy, the converse is so frequent as to be absurd !
Can the `split` package be added to `base`?
Nice example! It seems that only the orphan instances check is only obviously sound in the absence of interesting extensions to type classes.
The OP has written a typeclass with the *minimal* operations required to implement union-find, which makes the essence of the algorithm clearer. I suppose in future posts he will provide an instance for ST.
Sounds fun! I love breaking stuff! I'll take a look
eigenduck, thanks for the explanation! Unfortunately, the big problem with modular type classes is that they run completely counter to how data types are designed today in Haskell: e.g. I can instantiate an "empty set" without specifying what type class I want to use. So even we somehow grew "true modular type classes", everyone would have rewrite all their libraries. However, I have been playing around with a different idea. The key property we're interested in is that if the Ord instance differs, we want the types to differ. The only way we can achieve this today in Haskell is using the newtype mechanism on the element with the instance: but that's a bit clunky so it's understandable why people prefer to define an orphan instance and hope for the best. What if we made the newtype mechanism a lot less syntactically heavy to use? In the Backpack design, this would correspond to *generatively* mixing in a module, so that its types get new identities (and newtype deriving ). At the same time, you create evidence that the types are equal (modulo roles), so that you can convert from one to the other explicitly. So if you are unhappy with the Ord instance for Int and want it in reverse, in the module language, you generatively instantiate Data.Int, getting all of the usual functions with no fuss, implement your own typeclass, and explicitly coerce back to the "system" notion of Int. No, I have not worked this out in detail.
davdar, you should check out modular type classes. :) The resolution of how to decide if "ordering functions" are the same is key, and absent the conceptual framework of an ML-like module system, there is much to do besides lamely suggest some sort of syntactic check. (In an applicative module system, we can actually talk about the identity of modules, and conclude modules are the same if they are instantiated the same way. That's the notion of equality that's used!) Some very strange things occur when you take scoped introduction and hiding of instances to their logical extreme. This is my favorite example: A :: [ data T = T ] B :: [ data T = T ] C = [ import qualified A import qualified B instance Show A.T where show _ = "A" instance Show B.T where show _ = "B" x = show A.T ++ show B.T ] According to the signature, the type class resolution is unique, and results in x having the value "AB". This *remains* true even if later we decide that A.T = B.T!
As explained in the [paper](http://research.microsoft.com/en-us/um/people/simonpj/Papers/weak.htm) which introduced them, [stable names](http://hackage.haskell.org/package/base-4.7.0.0/docs/System-Mem-StableName.html#t:StableName) were invented precisely so that people could implement efficient memoization functions. I remember a thread in which many such memoization libraries were listed and compared, I'll edit this comment if I find it. *edit*: [found it](http://www.reddit.com/r/haskell/comments/1kgazx/automatic_memoziation/).
I'm not sure if this is what you're asking, but the notation `Val r ~ Node_ r a` is an equality constraint, which says that the types `Val r` and `Node_ r a` must be the same (much like the type class constraint `Eq (Foo a)` says that the type `Foo a` must have an instance of the `Eq` type class).
Eagerly awaiting the next article in the series!
You really are a *pun*isher.
And now, I'm sad.
In a particular use case I am stuck to STM because I need to be able to grab the first thing from any of three channels: `readTQueue a &lt;|&gt; readTQueue b &lt;|&gt; readTQueue c`. Could something similar be done here?
It's to make it work with `ST`, when he gets around to refining it in that direction.
Shh! :)
Isn't this what packages often do? They have a module that reexports everything you'd usually need. The problem with your idea would be the exclusion of internal modules for libraries, these would have to be listed by the library authors anyways, for offering any convenience, so you just can use modules in the first place. Also a preferring mechanism would produce some really badly invisible kind of shadowing that can easily trap you. Even when just importing modules normally you don't always know from which module a function you encounter in some code you read comes from. Also even normal shadowing where you explicitly declare what is overridden right next to where you use it can lead to confusion or errors. Your way you would have to also check every single preferred module about if there isn't something with the same name in it, too. 
Sorry, I had to balance making it too long against getting through the material. The use of associated types and type equalities is unfortunate but lays some foundation for later mechanics. There will be a full implementation at the end once all of the reveals happen. And we'll get a working instance for IO in *just* a moment. Edit: I added a short paragraph about the `UF` constraint and its use of `(~)`.
I read the first, but now I'm going to read the second too and see if it'll help guide the remainder of the series. Thanks for the references!
Please don't, this makes it ever more difficult to find out where symbols are really being imported from. Personally I prefer explicit imports of every symbol. Fortunately, this can be managed automatically.
You're asking about how to optimize a detail in your project, but we could probably help better if you gave us a bigger picture. 
Oops! You're exactly right. Thanks for spotting it.
&gt; The only price there is is the increased possibility of name conflicts with local definitions No, another issue is that your module does not safe-infer for Safe Haskell, so it and every module that imports it will either be unsafe or need to be marked Trustworthy. I'd like to see more use of Safe Haskell--it allows you to glance at a module and say "OK, the pure types in here really are pure," or "hmm, the module is marked Trustworthy, maybe there is stuff going on under the covers in these supposedly pure functions, I might want to look at them."
I'm not sure how I feel about having trace be a default export. It's kind of exciting since it would encourage `printf`-debugging which many people miss. It's kind of scary because I usually use `import Debug.Trace` as a warning to myself that this module still has bugs.
To clarify, the difficulty is in implementing an efficient search structure like a Fibonacci Heap. Dijkstra's algorithm itself is pretty simple.
This is probably an indentation problem. Make sure the left hand sides of your "where" assignments are aligned.
Actually, I copy - pasted that and it compiles fine for me. Which is to say that /u/AlmostProductive is probably right and it's an indentation problem. Most likely to do with the "where"-clause. Have you - by any chance - mixed tabs and spaces?
Thanks! Used spaces instead of tabs and it worked. Didn't know that could make a difference.
Indeed, `async` is very useful for this kind of thing. The reason I didn't want to use its `mapConcurrently`, for example, is that I'm using a pool of worker threads and don't want to spawn a thread for every element of my structure, but merely put them on a work queue. This is the reason for the interface that provides a `Setter`.
Can you please explain why tabs and spaces make a difference?
I don't know exactly but I suppose it has to do with the level of indentation. where-statements need to be whitespaced deeper than the rest of the code, but while one tab *appears* like 4 spaces (or something) it actually is just one character. The fuckup probably happens there. To avoid any confusion I'm not using tabs at all anymore. It saves from silly errors like the one you were having.
Great post! I consider myself a fairly beginner Haskeller, but I was able to follow it, despite the associated type shenanigans. I'm looking forward to part 2!
GHC uses standard eight character wide tab stops. If you have configured your editor to use a different tab width, it will mislead you.
You might be interested in the [Hasochism](https://personal.cis.strath.ac.uk/conor.mcbride/pub/hasochism.pdf) paper, they give plenty of tricks for converting between types, values, and typeclasses.
It looks like the problem is with (\\\\). If you replace it with a hand-code function, still O(n^2 ), using list comprehension, the difference between the two versions of sieve disappear: import Data.List sieve :: (Integral a) =&gt; a -&gt; [a] sieve n | n &lt; 2 = [] | n == 2 = [2] | otherwise = comb [2..n] where comb [] = [] comb (x:xs) | x * x &gt;= n = (x:xs) -- | otherwise = x : comb (xs \\ [x*x, x*x+x .. n]) | otherwise = x : comb (minus xs [x*x, x*x+x .. n]) where minus lst1 lst2 = [y | y &lt;- lst1, not (y `elem` lst2)] main :: IO () main = print $ sieve (10000 :: Int) (\\\\) uses `foldl` and `delete` (https://hackage.haskell.org/package/base-4.7.0.0/docs/src/Data-List.html#%5C%5C) which create loads of garbage. Profiling with `+RTS -p` shows that 100% of the time is spent allocating memory (vs 64% using the "minus" function). In fact, running the original version tweaking the garbage collection reduces the difference. I think what you see is due to `last xs` being &lt; `n` plus the awful time complexity of (\\\\): in fact, changing, in your second version, the `otherwise` clause to: | otherwise = x : comb (xs \\ [x*x, x*x+x .. last xs]) makes it run in the same time as the first version.
Many Ruby libraries are compiled, since they often use C extensions of the language for maximum efficiency. Couldn't cabal-install compile+install Haskell libraries instead of just installing them. I'm not sure what Ruby does, but you can install a gem from a git repository that contains C code without binaries. 
I'm glad to hear!
&gt; Hasochism Love the name
Tabs in Haskell are canonically 8 spaces long and the different parts of a where clause must be aligned. An example of how this might cause trouble is as follows: Although the two lines where&lt;tab&gt;bmi = ... &lt;tab&gt;&lt;tab&gt;skinny = ... *look* like they're aligned in your text editor (which probably uses 4-space tabs), as far as GHC is concerned what you actually have is where&lt;tab&gt;bmi = ... &lt; tab &gt;&lt; tab &gt;skinny = ... and the two don't line up at all causing a parse error. As for *why* this is an error, the second line is interpreted as a continuation of the first one. Since the line bmi = weight / (height^2) skinny = ... makes sense up until the =, that's where the parse error happens.
I agree with @frud, @gelisam solution only looks works (I'm not sure!) with the same argument (not with equivalent arguments). If you wish to memoize the *exact* input argument then, @gelisam works but if you wish memoize equivalent trees, your better solution looks dependant of your context. E.g. may be you can generate an associated hash while trees are constructing. Or you can generate a hash dependant uniquely of node values (and not of their hierarchy) then you can add/remove nodes/branchs and only must to update the associated hash (e.g. SUM(nodes) is hierarchy invariant), you can join/split trees (their hashes) (SUM is monoid). If trees are big, the probability collision is less. You can use many invariants at the same time (e.g. SUM all, SUM even, SUM odd, ...).
Not just GHC. Haskell layout rules specify that tab stops are 8 columns apart and that the tab character moves to the next tab stop. GHC does stops at 8, 16, 24, etc. It's arguably allowed to have tab stops at 4,12,20,etc.
It's all about the [layout rules](http://www.haskell.org/onlinereport/haskell2010/haskellch2.html#x7-210002.7) that let you elide brace and semi-colon characters by indenting your code in certain ways. Idiomatic Haskell uses layout almost exclusively. It is rare you'll see an example that uses a '{' when it could be elided. This means you have to be more disciplined with tabs vs. spaces. I still write all my Haskell code with quite a few tabs, but you must respect the layout rules.
You do? I never understood why they picked that name. Is this a play on Haskell and masochism? What's so painful in their technique?
Whether you use *tel*'s hexpat-lens, taggy-lens, xml-html-conduit-lens or another implementation, it's pretty much always the same feeling. After having used html lenses on a couple of quite serious projects, I confirm lenses really make it damn easy. I'm even thinking about writing an introduction to 'lens' through one of the HTML+lens libraries.
Initializing threads is very cheap, and controlling the number of threads is easy with semaphore. See http://stackoverflow.com/a/18898822
 The joke is that faking depenent types in Haskell is (supposed to be) painful compared to qn actual dependent language.
I find simple dependently typed programming in haskell easy enough, but it can get really frustrating for the more complex stuff! 
"First, let’s not pretend at all that this will be a pure interface—we’ll need a monad." ?? Monads don't get you to impurity, surely? The approach of modelling memory explicitly seems similar to http://okmij.org/ftp/Haskell/AlgorithmsH1.html#pure-cyclic-list
We have GHC.TypeLits which provides a `Nat` type. You can reflect the Nat back down to the term level using the `KnownNat` class. If you aren't going to modify the counter you can use my [`reflection`](http://hackage.haskell.org/package/reflection) package, which is based on the ideas from Oleg and Ken's Implicit Configurations paper, linked below. I've also been talking with Herbert Valerio Reidel who has been working on trying to replace the internals of Integer with machinery that can work without allocating inside GMP. Why is this relevant? Well, if he succeeds, for technical reasons we'd get a `Nat` type inside of `integer-gmp`, which in theory we could 'lift' to get the `Nat` in GHC.TypeLits, as base depends on `integer-gmp`. &gt; Why can't we just promote Integer itself to type-level integers? This would actually do what you mention in passing, at least for that `Nat`. As an aside, further answering what you asked for, but not what you need, I've also wanted type level `Integer`, but when I asked Iavor about it he didn't want to clutter up the work with type-level `Nat` with more stuff to consider for now and he didn't really see the point. I would love to revisit this decision at some point, as having access to type level integer literals for other type level numeric kinds would be awesome.
A file with 100 lines of imports has a complex relationship with its build dependencies. There's no point in pretending otherwise.
The API looks nice. One suggestion: in functions **taggyWith** and **run**, have you considered using a custom datatype with two constructors instead of the Bool parameter? It would make more immediately evident what the parameter is for, in my opinion. Also the parse function seems to choke when encountering &lt;?xml version=... declarations.
Do that, and post it here! I'd be happy to give a link to something like that to my friends who are curious about all this lens business I'm so happy over.
(LYAH should drop these pseudoscientific examples, or clearly mark them as such.)
I for one think the unsafe and partial functions from the prelude should always require an explicit import.
This is now available here: https://phabricator.haskell.org/D66
Yes, a contributor was looking into this recently. Sorry, it just hasn't been a high priority of mine amongst a lot of other things.
It depends. Monads create regions with an internal language which may be impure. You can "execute" some monads purely, though.
Beautiful.
This looks great! Exactly what I was thinking of, cheers. 
any suggestions on material to start learning this stuff? 
Indeed, it seems a leftover from Haskell 98, which didn't have `foldl'`.
Interesting! So generative instantiation provides something like: give me (a copy of) this module with this erasable isomorphism (that is, coercion) mapped over its interface? That would definitely make newtypes lighter weight, and not just for defining instances on them. MTCs definitely would require some different type signatures. The change to types is a lot like adding a datatype context to any type whose invariants depend on which instance are being used -- only in this case they come with semantics. 
ST is one way to do it. It's hardly the only way.
&gt; Fortunately, this can be managed automatically. How?
I wish one could export qualified imports in a module, so I could have a module with module MyPrelude (F) where import qualified Data.Foldable as F and have `F` be available after `import MyPrelude`. That would save me a ton of annoying importing.
If you haven't already, have a look at [UUAGC](http://hackage.haskell.org/package/uuagc). It won't help you with parsing, but it can do wonders with the AST. 
Phantom types are when you have an ADT with type variables on the LHS, but you don't actually use them on the RHS. For example, data ID a = ID Int There's no actual polymorphism going on here since we just ignore the type variable. But the reason to do this is so we can pretend there's polymorphism going on; e.g., to tag our values of type `ID` with some additional information— like whether it's an ID for a file, an ID for a tree, an ID for an intern table, etc. One of the crucial things here is that we don't export the data constructors for our ADT with phantom types. Since we don't export the data constructors, this means people can't just take things apart and put them back together in order to change the phantom.
Also, its worth mentioning that GADTs are what happen when you marry phantom types with existential types (type variable used only on the RHS). Which is why they have type equalities in them: to prove that the phantom and the existential witness agree.
try using the unregistreized -fvia-c backend, that should just work™ note you'll have to build unregisterized ghc yourself, perhaps a stage1 cross compile build, then a stage1 build on host, then run that stage 1 on target to buildd a stage2 ghc on the target [edit: adding missing word] 
"The -fvia-c flag does nothing; it will be removed in a future GHC release"
Please write more on this, it sounds fantastic!
Question: is this well-documented behaviour or mostly just a hack?
The LLVM backend is your only hope, I'm afraid. I looked at GHC's source and codeGen does not have any support for MIPS and asm output is set to panic, if invoked. The good news is that a brief look through of the LLVM codeGen suggests that you do not need to do much actual programming to get MIPS working. Loosely following [this](http://ghcarm.wordpress.com/2014/01/18/unregisterised-ghc-head-build-for-arm64-platform/) may help.
Yes, this is painful, but IMO this problem should be solved by tools, not libraries. Java has the same problem, but Eclipse's organize imports feature makes it so you never have to think about it. As others mention, explicit import lists can be very helpful. I think the right solution would be to have explicit imports managed by an IDE.
This list is one click away: it's in the only source file. Adapting to Haddock won't completely fix the issue here, because it will only list the modules but still no info on which members of those modules are actually exported and which are hidden. Hence I considered that it would be best to simply direct the user at the source code, since nothing explains better what's going on.
Debian has a ready MIPS package for GHC. Other distributions may also but that's the one I know about. They've been cross compiled back when -fvia-c worked and have been bootstrapped for updates with earlier versions since. You could try using that or, if you like, use it to build GHC for yourself.
`head`, `tail` and etc are all in the standard prelude. The philosophy behind this library is in removing none of the features of the standard prelude. Concerning the unsafe functions, they are discussed elsewhere in this thread.
Java has a radically different philosophy behind it, so it can't be a fair comparison. That is besides the fact that of the approaches to its problems being arguable at least, but that's not the subject here. &gt; IMO this problem should be solved by tools, not libraries. I disagree completely. The sole purpose of libraries is to solve problems. If the problem is easily solvable with a tiny library with clear semantics and purposes, why should one rely on tools? Besides I bet you still do use the standard prelude, right? However according to your argument you should refrain from that and instead explicitly import all the stuff you use from it. It's not hard to imagine a hundred lines of imports becoming a standard case in such a setting. Otherwise it becomes a question of where to draw the borderline of what comes in prelude. To me as an experienced user a line drawn with beginners in mind is naturally unsatisfactory. 
It hasn't really been tested on xml, but I tried to make it eat xhtml so that should be close. Could you create an [issue](http://github.com/alpmestan/taggy/issues) when you have some time with the problematic XML? Thanks! Alright, maybe &gt; data ConvertEntities = Convert | DontConvert will be there in the next version -- I did think about it but went for the shortest path.
A similar issue (foldl used instead of foldl' in the stdlib) is the case of [Data.List.(\\\\)](https://hackage.haskell.org/package/base-4.7.0.0/docs/src/Data-List.html#\\). There is a Reddit thread about [surprising performance issues while using it](http://www.reddit.com/r/haskell/comments/2aerm7/whats_the_performance_bottleneck_in_this_prime/). In Data.List, also "maximum", "minimum" and other functions use "foldl" and in fact the library defines [strictMaximum](https://hackage.haskell.org/package/base-4.7.0.0/docs/src/Data-List.html#strictMaximum), [strictMinimum](https://hackage.haskell.org/package/base-4.7.0.0/docs/src/Data-List.html#strictMinimum), etc. but it does not export them!
That's not enought actually: 2 * (a+b+c) = a+b+c //divide by (a+b+c) 2 = 1 //and now the two redditors who post are the same person [1] To solve the problem you'd need to write an actual solver… 1: [some random /r/shittyaskscience thread](http://www.reddit.com/r/shittyaskscience/comments/2aiaj5/is_there_any_proof_that_more_than_2_people_post/)
Couldn't this then be combined with `ConstraintKinds` and `GADTs` to violate soundness, with conflicting equality constraints in place of conflicting `Ord` constraints?
&gt; Herbert ... has been working on trying to replace the internals of Integer with machinery that can work without allocating inside GMP. Wow, that would be great! &gt; Why is this relevant? Well, if he succeeds, for technical reasons we'd get a Nat type inside of integer-gmp, which in theory we could 'lift' to get the Nat in GHC.TypeLits. We'd get a lot more than that. Most importantly, we'd get a GHC that doesn't segfault when your FFI indirectly depends on a C library which also allocates inside GMP, such as `libgnutls`. The only way to get such a GHC currently is to compile it with `integer-simple` instead of GMP. &gt; ...base depends on integer-gmp By default. But if you compile GHC with `integer-simple`, I hope you get a `base` that does *not* depend on `integer-gmp`. Would lifting to type-level `Nat` still work identically from the user's perspective, or would you need CPP to avoid hard-coded dependence on GMP?
thanks!
You can think of it that way if you'd like. But in reality a monad in Haskell is nothing more than an instance of a type class, and the methods of that type class are pure. So monads in principle are pure in Haskell, unless you happen to use `IO` or a type built on `IO` in your particular instance of `Monad`. Why are we emphasizing this? Because it is a common mistake among newcomers to confuse monads in general with `IO`, or to think that there is something inherently "impure" about using, say, the `State` monad.
What questions?
Okay, questions is the wrong word. With my task to complete the missing parts of the functions. 
Haskell has capacity for working with mutable references like you're describing here but it's generally simpler to work with immutable structures inside the State monad. The gist of the state monad is that between each statement inside the do-block (which are composed with (&gt;&gt;=)) the state is evaluated and passed to the next function. So we can model the three references as 3-tuple and at each step use the ``modify`` function returning the new state to the next function. import Control.Monad.State resultState :: State (Integer, Integer, Integer) Integer resultState = do modify $ \(x,y,z) -&gt; (x+7, y, z) modify $ \(x,y,z) -&gt; (x, y+3, z) modify $ \(x,y,z) -&gt; (x, y, x+y) modify $ \(x,y,z) -&gt; (x+z, y, z) (x,y,z) &lt;- get return x testState :: Integer testState = evalState resultState (0, 0, 0) Now we could also implement this using a somewhat special monad known as the ST monad which models your original code very closely by actually creating mutable references which inside the monad ( and only inside this precise ST monad ) can be updated and then evaluated to a pure value which is returned. import Data.STRef import Control.Monad.ST resultST :: Integer resultST = runST $ do x &lt;- newSTRef 0 y &lt;- newSTRef 0 modifySTRef x (+7) modifySTRef y (+3) xv &lt;- readSTRef x yv &lt;- readSTRef y z &lt;- newSTRef (xv + yv) zv &lt;- readSTRef z modifySTRef y (+3) modifySTRef x (+zv) readSTRef x Everybody has their favorite introductory tutorial, but I think [this one (Three Useful Monads)](http://adit.io/posts/2013-06-10-three-useful-monads.html) is quite nice for beginners.
Your `empty`, `insert`, `member` and `filter` should be implemented in terms of functions from Data.Map. Please describe what your problems are, instead of expecting us to read your mind. 1. Which `Map a ()` corresponds to an empty `Set a`? 2. Why doesn't your definition of `empty` use this value?
**EDIT**: Actually, what I propose below is already implemented in hackage (if you maintain a package, look at the bottom of the maintainer page). Use it! ***** Slightly off topic, but can I take a moment to rant about how we got into this silly situation where every single library needs to update their bounds whenever one of their dependencies releases an update? Even more silly: the practice of avoiding having to update your bounds too often by specifying an optimistic range, such as `&lt; 2.3`, because you hope that the unreleased 2.2.1 version will not introduce breaking changes (otherwise they would hopefully bump to 2.3). While we're being optimistic, why not put `&lt; 3`? Because when your assumptions turn out to be wrong, your package ends up lying to cabal, who can't choose the proper versions and we end up in even deeper circles of dependency hell. Even if you do specify exact bounds, it still leads to problems. You can't possibly have tested every single version in the range; even my [cabal-rangefinder](https://github.com/gelisam/cabal-rangefinder) tool uses binary search, because an exaustive search would be too time-consuming. If one of those untested versions is broken (I'm looking at you, exceptions-0.6), then you're lying to cabal by saying that your package is compatible with this version, like all the other versions in the range. You might think of this as a non-issue because cabal will always favor the more recent, fixed version, but I've had issues with build servers who still thought that 0.6 was the latest and thus flagged broken builds even though the corrected 0.6.1 was out already. Specifying too tight bounds is also problematic. You might think that since lying to cabal leads to so many issues, the most truthful approach might be to specify that the only accepable version of your dependencies is the one you have tested your package with, namely whatever version happens to be installed on your machine. I probably don't even need to say this, but obviously this means that two packages who happened to be developped with different versions of the same dependency will be considered incompatible even though they probably aren't. So, what got us into this mess? I think the answer is simple: it's the fact that the version bounds are encoded into the .cabal file of each release of each package. Getting the bounds right is just way too hard to expect every single maintainer to get it right on every single release, and the fact that any error will remain forever attached to that version and will mess up everybody's dependency resolution is way too harsh a punishment. My proposed solution is to divorce the version bound information from the package releases. Hackage already has meta-data about which versions are broken, but not everybody takes the time to mark their broken versions as such (hawk [does](https://hackage.haskell.org/package/haskell-awk/preferred), exceptions doesn't), and I don't even know if cabal takes this information into account. I hope it does! Whether a particular version is broken needs to be specified as meta-data, outside of the zip file which embodies a particular release. Because of course, if the author knew in advance that the package was broken, they would not have uploaded it! I argue that the correct version bounds is also something that you only learn after your release is uploaded, if only because you don't yet know whether this particular version of the code will be compatible, unmodified, with future releases of your dependencies.
I think we need more than that. This Prelude is a great idea - I love it in fact - but for it to be pleasant to use, we need an easy way to get a single list in one place of all of the symbols being imported, together with their type signatures. Because of the sheer volume of symbols in this Prelude, I would also recommend that anyone using it stick religiously to the practice of using only explicit symbol imports for anything other than this Prelude. Otherwise, people reading the code later on will have a hard time figuring out where symbols come from.
2. Because I don't know the answer to question 1..
Look for it in Data.Map. You are probably using GHC 7.6 or 7.8, in which case it might not be obvious to you that Data.Map includes Data.Map.Lazy (i.e. look in Data.Map.Lazy as well). (If you are using GHC 7.4 or earlier, there is no Data.Map.Lazy — you need only look through Data.Map.)
I avoid any sort of cabal hell by ensuring that i only have one version of a package installed at a time. Sometimes that means using sandboxes when you're trying to build something that is unmaintained and doesn't work with the current versions. I use a tool to check for multiple installed versions and to recursively unregister packages when it's time to update something. https://github.com/glguy/GhcPkgUtils The issue you're having with darcs appears to be that it isn't updated to build with the 7.8 series of GHC. This isn't cabal's fault; cabal is just the bearer of bad news. In this case you can tell that this is the issue because darcs depends on array &lt; 0.5.0.0 and GHC 7.8 comes with array 0.5.0.0. Also, darcs was last updated Feb 2013 EDIT: Also, I recommend reading [The Cabal of Cabal](http://www.vex.net/~trebla/haskell/cabal-cabal.xhtml) and then fixing your .cabal/config similar to mine by adding: constraint: bytestring installed constraint: containers installed constraint: transformers installed ... for all the packages that come with GHC to keep cabal from trying to replace them.
I can assure you it works. 
I usually use tabstops. I would put the statements in individual lines usually. where bmi = ... skinny = ... This way. Is that okay from a stilistic point of view? (Reason being that I can't really stand 8-space tabstops. They eat up too much space imo. (Or at least in other languages :3 )
I wrote a package that uses ghc's -ddump-minimal-imports flag to reformat the imports. Its called [module-management](https://hackage.haskell.org/package/module-management).
Look in the “Construction” section of that page. Edit: Alternatively, go to [Hoogle](http://www.haskell.org/hoogle/), search there for the type of the value you're looking for (`Map a ()`), and first in the list of results is...
I do that. Absolutely fine, IMO.
this is all so fascinating! thank you
&gt; My proposed solution is to divorce the version bound information from the package releases. Hackage already has meta-data about which versions are broken, but not everybody takes the time to mark their broken versions as such (hawk does, exceptions doesn't), and I don't even know if cabal takes this information into account. I hope it does! How much work would this be to prototype? It sounds reasonable, and I'd be interested to see someone (you maybe?) give this a go. Packaging was one reason I left Ruby 8 years ago. Luckily Haskell has enough positives to keep me interested, but I have to sa that I did once consider quitting because of how frustrating dealing with cabal was (cabal sandboxes have solved most of the problems I've had). If something like this would make cabal more pleasant to deal with, then I'm all in, so to speak.
This is great, I was just playing with snaplet-socketio. Socket.io was one of the standing reasons given against Haskell (in favor of node) that I've been encountering, Thanks for this.
I should point out that my old `snaplet-socketio` project will be going away, as it only implements the old 0.9 protocol. But thanks for trying out my experimental work!
I don't see any reference to unsafe here...
I can't see how saving the information somewhere else solves the fundamental problems. The holy grail would be semantical versioning, being able to exactly say what you're using from a library and only check this information instead of relying on version numbers. One way might be to use type signatures, but even then you're not on the safe side, because the semantics of a function might change, but it's signature might still be the same.
- I copy binaries from the sandbox in which they were built or add the sandbox to my PATH. Neither option seems great to me, but I can't say I've had any problems doing it (I copy the binaries for ghc-mod, pandoc, and git-annex from their sandboxes to a single location, for example). - Have a sandbox called `playground` that you can hack around in. If its package db gets into a bad state, you can wipe it out without affecting anything else you're working on. Remember, you don't need a .cabal file to have a sandbox, so this is just a regular directory for you to try things out with `cabal repl` and friends. - I don't know how to make use of Stackage. - Unmaintained packages will always be a problem since GHC and base move quite quickly. Starting with only the global db that was created when you installed GHC and working in a new sandbox means that you can at least go on IRC and see if anyone else has built, or can build, the specific .cabal file that is giving you trouble. If you start with any other accumulated db, it's unlikely anyone will be able to reproduce the problem. - I don't use the HP, but that debate has been hashed over quite a bit recently. 
Are you saying that that the fundamental problem is not that packages specify incorrect version bounds for their dependencies, but that the maintainers of those dependencies don't bump their version numbers according to the [Package versioning policy](http://www.haskell.org/haskellwiki/Package_versioning_policy) (Hackage's version of [semantic versioning](http://semver.org/))? Or perhaps that this policy is not strict enough?
Explicit imports are not very useful when writing code, but they are useful when reading code to locate the source for a particular imported function.
The fundamental problem is, that you can't exactly specify what you depend on, respectively you shouldn't need to specify it by hand, because what you're using from a library already tells what you depend on. If this information could be automatically extracted and used for comparision, to decide which version of a library should be used, then this would be real semantical versioning. In a way version bounds and the PVP are just workarounds, as long as this information is created by hand it will contain errors. Sure, we might find better workarounds, but they still will be workarounds.
&gt; There is no reason not to use an alternate Prelude My point is not about alternate preludes, it is about the more specific problem of managing import lists. &gt; Tool support depends on people all using the same tool No, not if the new tool is general enough that it can be used in all other tools. For this particular case, I could imagine someone writing an executable program that analyzes a module and adds imports for any missing symbols. With something like this it should be pretty trivial to integrate into emacs/vim/etc.
&gt; How much work would this be to prototype? Let's see, for a prototype I would probably store the meta-data in a local file instead of on hackage, so only cabal would need to be modified. I wouldn't need to alter the resolution algorithm itself, just to give it a different input. I might also have to modify cabal-install so it would not complain if the versions selected conflict with the ranges specified in a .cabal file. This doesn't sound like such a big endeavor, but last time I played with cabal's codebase, even simple things like figuring out how cabal knows which versions of each package exist took a surprisingly long time. So, I don't know, maybe a month of part-time volunteer effort? I'm really bad at estimating :) &gt; I'd be interested to see someone (you maybe?) give this a go. If there is a moderately good chance that this change could get in, then I would be delighted to volunteer! I see that a lot of people upvoted my rant, but is there anybody here from cabal or hackage who thinks this might be worth trying?
Maybe take a look at www.dohaskell.com. Not a whole lot there right now but I'm adding to it all the time.
A given build of GHC is either registerised or unregisterised. A registerised build does not support the C backend, while an unregisterised build only supports the C backend. Hence, the -fvia-c flag does nothing, but that doesn't mean the C backend doesn't exist.
1. I make sure that I have no more than the absolute minimum number of packages installed as --global. This means that I don't use the Haskell Platform or any OS haskell packages. I install GHC directly. Some might think this casts too much of a negative light on the Haskell Platform. But everyone will agree that having multiple versions of a package installed at the same time is one significant cause of build problem. And that is exactly what the Haskell Platform does for you--it installs specific versions of packages. If you use Haskell heavily enough, you will invariably encounter a situation where you want to use a different version of a package than the one the Haskell Platform gives you. 1. Make sure ~/.cabal/bin is at the front of your path. Hopefully you already knew this, but I see this problem a lot, so it's worth mentioning for completeness. 1. Install happy and alex manually. These two packages generate binary executables that you need to have in ~/.cabal/bin. They don't get picked up automatically because they are executables and not package dependencies. 1. Make sure you have the most recent version of cabal-install. There is a lot of work going on to improve these tools. The latest version is significantly better than it used to be, so you should definitely be using it. 1. Become friends with "rm -fr ~/.ghc". This command cleans out your --user repository, which is where you should install packages if you're not using a sandbox. It sounds bad, but right now this is simply a fact of life. The Haskell ecosystem is moving so fast that packages you install today will be out of date in a few months if not weeks or days. We don't have purely functional nix-style package management yet, so removing the old ones is the pragmatic approach. Note that sandboxes accomplish effectively the same thing for you. Creating a new sandbox is the same as "rm -fr ~/.ghc" and then installing to --user, but has the benefit of not deleting everything else you had in --user. 1. If you're not working on a single project with one harmonious dependency tree, then use sandboxes for separate projects or one-off package compiles. 1. Learn to use --allow-newer. Again, things move fast in Haskell land. If a package gives you dependency errors, then try --allow-newer and see if the package will just work with newer versions of dependencies. 1. Don't be afraid to dive into other people's packages. "cabal unpack" makes it trivial to download the code for any package. From there it's often trivial to make manual changes to version bounds or even small code changes. If you make local changes to a package, then you can either install it to --user so other packages use it, or you can do "cabal sandbox add-source /path/to/project" to ensure that your other projects use the locally modified version. If you've made code changes, then help out the community by sending a pull request to the package maintainer. Edit: bergmark mentions that unpack is now "cabal get" and "cabal get -s" lets you clone the project's source repository. 1. If you can't make any progress from the build messages cabal gives you, then try building with -v3. I have encountered situations where cabal's normal dependency errors are not helpful. Using -v3 usually gives me a much better picture of what's going on and I can usually figure out the root of the problem pretty quickly. Edit: I decided to put this up on my blog so it's a little more linkable. http://softwaresimply.blogspot.com/2014/07/haskell-best-practices-for-avoiding.html
By using Gentoo Linux. The haskell overlay has most important parts of hackage covered. While there are occasional build failures at upgrade time, `haskell-updater` seems to always fix them. But then again, I only play a Haskeller on TV. 
While LYAH is a fantastic book, I also felt a bit lost after reading it. You're still kinda useless as a Haskell programmer, all the practical things like 'drawing to the screen', 'calling a web API' are not included and bread &amp; butter concepts like monad transformers, arrays/vectors, exceptions, any form of parallelism / concurrency, widely used language extensions etc., required to use/understand many libraries, are not discussed. I can recommend Real World Haskell as a second book. Skip the basic early chapters and some of the outdated stuff, and you're still left with enough good parts. Also the Parallel and Concurrent Haskell book is truly excellent. It's written in a way that should make it accessible to someone with just LYAH under their belt. Both are available online for free, btw. I haven't personally read it, but some people here recommend this book: www.amazon.com/Beginning-Haskell-A-Project-Based-Approach/dp/1430262508/ Also, have you seen this great collection: http://dev.stephendiehl.com/hask/ ? That should give you a good introduction to many of the advanced concepts. I would also recommend to check out 24 days of hackage to get a good idea of Haskell's library ecosystem: https://ocharles.org.uk/blog/pages/2012-12-01-24-days-of-hackage.html https://ocharles.org.uk/blog/pages/2013-12-01-24-days-of-hackage.html Hope that helps, good luck! ;-)
try this - http://dev.stephendiehl.com/hask/ I felt a lot more confident with typeclasses after going through Oleg's 'Typed Tagless Final' tutorial - http://okmij.org/ftp/tagless-final/course/index.html#lecture .
What exactly does this site? 
Nix has revolutionised how I develop in Haskell - no cabal hell and no need to rebuild packages more than once. I can easily switch compiler versions without affecting anything else on my system. Every package is separate and sandboxed from each other. The package library is curated (similar to Stackage) so that everything works with everything else. https://ocharles.org.uk/blog/posts/2014-02-04-how-i-develop-with-nixos.html http://fuuzetsu.co.uk/blog/posts/2014-06-28-My-experience-with-NixOS.html
Ooh, good to know. I'll update my answer.
You didn't mention [the Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia), so I recommend it to you.
great list! I'll be pointing people to this from now on :) 
great advice, pretty much what I've been doing! btw, why should I install alex and happy? I assume I only need those if I want to build ghc?
I've been trying Nix out recently. Here's a blog series I've been writing about how to get set up. http://fluffynukeit.com/installing-virtualbox-for-nixos/
Thanks. I've managed to do what I needed using your paste as a skeleton, and I've learned a lot in the process. The part that surprised me was the `(ResultState i m) &lt;- get`. At first it sounded like "get" was magically accessing global state out of nowhere, and it is not even a function. How!? Now I'm starting to grasp that everything you do is just building a big description of what should be done later with runState! I must admit I still wouldn't say I am confident with monads, but things are certainly improving. Thanks.
A few other packages depend on them, e.g. `haskell-src-exts`.
Thanks, another library I needed!
First, if you really want Haskell expressions (your "This is not in any particular language" makes me doubt this), you can use the [hint](http://hackage.haskell.org/package/hint) library to interpret Haskell expressions. &gt; import Language.Haskell.Interpreter &gt; runInterpreter $ eval "(:) 1 [2,3]" &gt; Right "[1,2,3]" Hint doesn't have to return strings, it can also return concrete values if you tell it which type you expect. One slight distraction is that the type needs to be in scope, and the Prelude is not imported by default. &gt; let run cmd = runInterpreter $ setImports ["Prelude"] &gt;&gt; cmd &gt; run $ interpret "(:) 1 [2,3]" (as :: [Int]) Right [1,2,3] The most interesting aspect of Hint is that it can also return *functions*. &gt; let fromRight (Right y) = y &gt; f &lt;- fmap fromRight $ run $ interpret "\\a b c d -&gt; [a,b,c,d]" (as :: Int -&gt; Int -&gt; Int -&gt; Int -&gt; [Int]) &gt; f 1 2 3 4 [1,2,3,4] So later on, when you assign 11 to `a` and you wonder what is the new value of `f` in that new context, you can pass it its new environment with ordinary function application. &gt; f 11 2 3 4 [11,2,3,4]
What is "type-class instance transcendence"? Also, are you asking about my solution or /u/dan00's?
That is interesting, but are the functions returned by interpret as fast as if they were statically compiled by GHC? By "this is not any particular language" I mean the pseudo `do` syntax I used there, I guess that is horribly wrong.
That leads to lots of bad monad tutorials.
&gt; That is interesting, but are the functions returned by interpret as fast as if they were statically compiled by GHC? Of course not. Hint is an interpreter, not a compiler! In [hawk](https://github.com/gelisam/hawk/), we are mixing up interpreted code with compiled code by generating a module, compiling it with ghc and importing the compiled module into hint. Of course, compiling and then running a function takes longer than just interpreting it, so we only compile when it's worthwhile to do so.
That sounds exactly like what I need. The point is, functions are rarely going to be changed, but they will be called a lot of times. That sounds hard to do, isn't it?
Is your list of functions known at compile time? Are they provided as strings?
No, the application is a server that is going to be running continuously. Users can access that service. This service will provide an API that allows users to store and retrieve Haskell terms, as well as their normals. The problems are: if a user modified a term, say, "foo", and other term, say, "bar", depends on "foo", then "bar" needs to be recomputed. 
You could do this with IO and IORefs (a type that implements mutable state as IO actions), but I suggest you don't look into those too deeply at this point, especially if you haven't fully grasped how and why monadic IO works the way it does. State monads don't actually implement mutable state; they just carry an extra value around in the form of a function argument and extra return value. You could easily do this without monads, but because such a mechanism ends up following the monad laws, it makes sense to implement it as a monad, and then when you throw it into a `do` block, it looks exactly like mutable state.
Can't wait for the reST. 
It's sort of like OP was asking for - a collection of Haskell blog posts/tutorials, with tags. 
Have you seen this list of resources? https://github.com/bitemyapp/learnhaskell
You shouldn't expect monadic actions to be functions, if they were you could just call them and not use `&gt;&gt;=` or do syntax. Start with Functor and Applicative, then learn how to use Monads without do syntax. Should all make sense if you understand the foundations.
I'm halfway through Beginning Haskell. I like it so far. I feel the author does a good job motivating new things (e.g. monad transformers) and then explaining how they work and how to use them. It also introduces new and recent libraries and tools, e.g. lenses and EclipseFP. On the (minor but annoying) downside, the number of spelling and similar errors is far too high for a published book.
The wonder of Haskell is that you can never achieve complete mastery. 
You are probably aware of this, but reading books and papers and blog posts only gets you so far. Certainly you should do that, but the real learning takes place through the process of *doing*. Find some relatively simple problem and write up a solution. You may hit awkward spots and you'll probably not write the most elegant solutions at first, but the more you do this the more you'll build up your intuition regarding the things that are fuzzy now. When those things become clear, you'll see deeper into the libraries and blog posts and whatnot that seem impenetrable now. Another advantage of doing this is that you'll end up with a collection of code that you can refactor to include new things that you want to understand. When you're reading about things, you'll have a bunch of potential applications in your head, making it more likely that you'll get an insight as to the relevance of the thing you're reading and a ready-made opportunity to practice it.
I'm doing the parsing part to make it more interesting. I may dive into language-python when I get stuck. Thanks!
See [here](http://www.reddit.com/r/haskell/comments/2938yb/real_world_haskell_outdated_parts/).
Doing is really good when you need experience/practise; when you have the knowledge, but fail to develop solutions for the problems you find. On my experience, the problem Haskell is often the opposite. I have no hard time in using the tools I know, but I'm more often than not finding myself stuck for needing knowledge about things I never ever read about - or just wasting time in solving just to realise there is a much better and cleverer solution (library, or just a specific technique) that I didn't know was available and could by no way develop on the fly. 
What you're asking for is impossible. You cannot rebind 'a' they way you are doing and get a different value of 'e'. 
I'd use ghc-pkg unregsiter for #5, rather than rm, but maybe that's just me.
I think there was a particular FRP implementation that was focusing on data flow rather then events and had a neat way of only recomputing the value of the signals when their inputs changed? It looks to me like you are treating the string "a" as some sort of object identity to a mutable object. If so, go all the way. Have an expression like e &lt;- [a,b,c,d] modify a b c and d to have (weak) references to e so they can invalidate e when their stored value is changed or their calculated value is invalidated. Changes cause a tickle-up cache-invalidation (up the expression tree). Evaluations cause trickle-down recursive evaluations until you get to a cached or constant value. Of course, you basically lose referential transparency, but that does seem to be (a prerequisite of) your goal.
What type is `x`? It's clearly not a plain integer value. Instead it must be a variable containing a / reference to a Integer value. So, on the last line there, you'll have something like `deref x` or `Get x`. Also note that `result ` is also clearly not a plain Integer value. It's a `do`-block[1], which mean it must be a *monadic action* that "returns" an Integer. Now, depending on the specific Monad your do block is using, you might be able to extract a plain value out of this monadic action. IO Monad? Nope. ST Monad? Maybe, as long as the thread type variable can be universally qualified. State Monad? Sure, but you will need to provide the initial state explicitly. Now, no matter what monad you were using, you **can** compose this monadic action returning an Integer to a function taking an Integer and running a monadic action of the same monad. That's actually the first power of monads, we can compose them like plain functions, as long as we stay in the same monad. [1] If this surprises you remember: In Haskell, and other referentially transparent languages, bindings (generally using `=` and possibly `let` or `where`) are not an action that is done once (an "assignment"). Instead they state that (over a certain scope) the value on the left is interchangeable with the value on the right.
&gt; capitalizing function calls (that's not allowed in Haskell) Those aren't function calls, those are data constructor calls for the operational monad he's composing. :P
&gt; should just ™ I think you accidentally out a word.
Thanks! I've posted an extended version of this to http://unsafePerform.IO/blog/2014-07-12-arrow's_place_in_the_applicative/monad_hierarchy/
I'd prefer more libraries and the same documentation, given the choice. I respectfully request _others_ start submitting pull requests against edward's libraries, or blogging them up, or etc. :-) (and not just his, but against any other cool libraries that lack tutorials and explanations!)
From the description alone, this sounds like you might be looking for incremental computation. I don't know much about it, but I've been linked in the past to a couple of papers on the idea: [http://www.cs.cmu.edu/~blelloch/papers/ABH06.pdf](http://www.cs.cmu.edu/~blelloch/papers/ABH06.pdf), [http://www.cs.umd.edu/~jfoster/papers/cs-tr-5027.pdf](http://www.cs.umd.edu/~jfoster/papers/cs-tr-5027.pdf)
I agree with this, except that I use the Haskell Platform to speed up install times whenever I have to do a reset (i.e. `rm -r ~/.ghc`). If something conflicts with the platform then I use `hsenv` to create a `ghc`-only development environment.
After reading part 2, it becomes clear that ST provides the answer to doing this stuff in a safe way with the type system's help (I assume this will be the topic of part 3, how rank 2 types allow us to constrain values to a certain context)
:) 
I think it's often the pattern with Haskell that once you're over the hump, it's quite difficult to know how to present the information to someone approaching it for the first time. I'm really just starting to grasp basic concepts, but once I get them they seem so obvious. Perhaps pairs where one is beginner and one is familiar could document their learning process and present it as a way for someone else to find their own way to understanding..
I posted some more tricks i use [here](http://www.reddit.com/r/haskell/comments/2ameew/best_practices_for_avoiding_cabal_hell/ciwq5hi).
It occurred to me the other day that even just the name `recursion-schemes` is basically the perfect microcosm of the Haskell ecosystem's issues with documentation: it is at once perfectly descriptive and completely useless.
Hmm, if you really want them to run as fast as if compiled by ghc, then I guess you have no choice but to compile them with ghc. I think you can still use the same strategy: write `e` to a module file exporting a function of four inputs, and compile this module. When you want to execute `e`, use hint to load (the latest version of) `e`'s module and its dependencies, then apply the function. Or, even simpler: since you're going to be loading the latest version of the modules anyway, `e`'s module could import its own dependencies, and then export `e` itself instead of a function.
Just follow the types. It's all self documenting &lt;/sarcasm&gt;
Well if _you_ can think of a 2-3 minute set of examples in the README.md that would clear up profunctors, be my guest! https://hackage.haskell.org/package/profunctors For something like that, documentation is if anything more work than the code itself. The rules of the game can, so to speak, tell you how to write it, but not necessarily _why_ others would want to use it. (To be clear, i'm not defending the mentality of "no documentation" i'm just pointing out that some people are better at writing libraries, and some people are better at writing extended tutorials for libraries, etc. The idea that this all needs to, or can, or should, fall on the same person is... weird. Obviously better documentation matters, and means more users. But fewer bugs matters too, and many other things to. In an open-source world its silly to complain when others don't direct effort to things you care about -- instead, maybe work to generate enthusiasm to get _other people_ to direct effort to those things! we're not zero-sum in human resources here. if we have things to be done, there are always more people to get excited about contributing!)
Consider fpcomplete.com (free version) as an alternative to local install. It gives a large set of libs preinstalled, separated cabal env per project, github integration, access your code project from any web browser, continuous recompilation and hyperlinks from errors to code, type tooltips, little local CPU use, ...
I am still really early into learning Haskell and any time I try to build something, I fail so miserably for so long that I honestly have no idea why I keep trying. Even so, I did something the other day that I would have probably thought was crazy when I first started looking at Haskell. I had seen examples of a list of monadic instances turned into a monadic list and I couldn't remember where I had seen that, so I just googled the type signature. `sequence` is what I was looking for. Only afterward did I realize what I had done and felt, for a brief moment, that something, somewhere had been learned. After that I went back to failing to build anything at all.
*I think it's often the pattern with Haskell that once you're over the hump, it's quite difficult to know how to present the information to someone approaching it for the first time.* Nonetheless, even a few examples on how a package is used, is better than nothing. In the end people use packages to do something in their program.
Yeah, I try to use unregister as much as I can too, but since unregister doesn't allow you to specify multiple packages on one line it's rather cumbersome. So if there are more than a handful of packages to be removed, I just remove them all and start over.
&gt; Because solving it with a library like this has a cost...namely, that it hurts readability of the code because it makes it harder to determine where the symbols you're using come from. There are no intricate schemes here. It's just most of the "base" package. Every proficient user knows where all those definitions come from. &gt; I think it's perfectly reasonable to make an exception for the prelude. Exactly. &gt; Extremes are rarely the right choice. Whether or not it's an extreme is debatable at very least. I consider the alternative prelude projects to be extreme, because they change semantics of prelude, thus becoming a whole new religion, and also they bring in a whole lot of possibly unnecessary dependencies, e.g., "basic-prelude", "general-prelude". What happens in the discussed prelude on the other hand is fairly evident and can be described (and hence comprehended) as "everything useful from "base"". &gt; If it comes in prelude, you don't have to be explicit. There you go. 
You do realize that this means manually reexporting every single definition from the imported modules..? I'm open to pull requests, but I definitely am not going to do any such things myself.
I implied [this](http://www.reddit.com/r/haskell/comments/2ahq11/announce_the_most_complete_prelude_formed_from/civa3fp), but then we're already there.
This'd be something where there'd probably have to be some work to pick the right WiredIn or something. Right now the status of this proposal is nothing more than a few quick exchanges between me and Herbert on IRC, so take that for what it is worth. ;) For me _not_ using the GMP allocator / GC hook would be amazing. MPFR, etc. can all just be made to work, we can FFI out to code from R that happens to use their native gmp lib, etc.
I would use the System.Plugins library (plugins package). It supports compiling and loading modules at runtime.
&gt; I see you guys talking about advanced things all the time - GADTs, Lenses, REPA, monad transformers. I see a lot of cool libraries with no explanations or tutorials. Part of that effect is owed to the fact that /r/haskell is a hangout for experienced Haskellers who want to know about the bleeding edge. You can’t cater to everybody. &gt; One complete enough to contain everything someone should read to go from complete novice to mastery by himself. I don’t think that it’s possible to go from novice to master non-interactively, in anything. The usual form of interaction is teacher-student; and while GHC has fairly good error messages once you’ve reached a certain experience threshold, the compiler-programmer feedback loop is much shorter, simpler, and less valuable. 
Just started using NixOS 5 days ago and I love it! It's a bit of a steep climb to get started, but once your system is up and running, it's truly gratifying. Arch and Gentoo-ers beware: You may have a new favorite if you try NixOS ;) If you use Nix and cabal2nix to manage your Haskell packages, you won't have any Cabal Hell issues ever again!
I'd be surprised if there is a single Haskell professor in my whole country. Okay, probably there are some, but none I ever heard of, and certainly not in the 4 big schools of my state, as I know pretty much every CS professor.
Not much of a master, then. ;)
Me too. Installation reminds me of OpenBSD - hardcore, quick and simple. Centralized configuration file is really nice. Never tried Nix (pkg manager) on its own, but it has been said that Nix can be used on any Unix system including Mac OS X.
Oh, cool! Thanks for that post. I'm in a similar boat and actually read a lot about FNIStash when I was writing a Windows app using threepenny gui! I should really go back and finish that... I'd also like to mention [this](https://github.com/oxdi/vagrant-nixos). I've been using Vagrant for running Linux boxes within Windows, and it's a real treat after having just tried plain VirtualBox VMs before. And speaking of Haskell in Vagrant, [here's](https://github.com/begriffs/haskell-pair/blob/master/bootstrap.sh) a great example provisioning script (for Ubuntu/apt-get though).
Edward has made his name in Haskell by writing very useful, very generic, very high quality libraries. There's place in the Haskell ecosystem for someone to make their name producting high-quality *documentation* for Edward's libraries and other less-documented libraries. I think this division of labour would work well.
I'm wondering why this post seems to make no mention of the well-know blog posts of C. D. Smith on the topic, [here](http://cdsmith.wordpress.com/2011/07/30/arrow-category-applicative-part-i/) and [here](http://cdsmith.wordpress.com/2011/08/13/arrow-category-applicative-part-iia/). Those previous blog posts convinced me that Arrow is essentially equivalent to Applicative. The "+ Category" part just deals with the kind impedance mismatched mentioned at the beginning of the present post.
[This blog post](http://gergo.erdi.hu/blog/2014-07-12-arrow's_place_in_the_applicative_monad_hierarchy/) by /u/gregoerdi (OP) is one of the answers to the SO question. I'm wondering why it makes no mention of the well-know blog posts of C. D. Smith on the topic, [here](http://cdsmith.wordpress.com/2011/07/30/arrow-category-applicative-part-i/) and [here](http://cdsmith.wordpress.com/2011/08/13/arrow-category-applicative-part-iia/). Those previous blog posts convinced me that Arrow is essentially equivalent to Applicative. The "+ Category" part just deals with the kind impedance mismatch mentioned at the beginning of the present post. EDIT: I see that C.D. Smith's posts are mentioned in a comment to the SO question.
This is OK, except I totally disagree with not installing the Haskell Platform.
&gt; Then version bounds can be guessed from what build-bots produce, because that's all they are - guesses. The real information is the set of dependenies that are known to work. Woah... the PVP defines a versioning scheme to which serves as a **contract** (the point of which is to preclude guess-work) toward users of a package, that the API won't change in a backward incompatible way unless a major version bump occurs.
 {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE ConstraintKinds #-} Yeah, it's right here, I just need to put few weeks to learn yet another language extension.
Will this at some point be moved to the HaskellWiki?
It's pretty clear that the intention of "8 characters apart" is specifically starting at the beginning of the line.
Constraint kinds is probably gratuitous. I'd remove it I went back through and edited it all. Type families is just like MultiParamTypeClasses but more constrained with better type inference (same as what you would get with FunctionalDeps). Sorry about the extensions!
1. Try the stuff from RWH 2. Scratch head because the stuff doesn't work 3. Read error messages 4. Read documentation 5. See API changes 6. Try again, with new API 7. optional, but encouraged: update information at http://stackoverflow.com/a/23733494/1139697
It would be great to see Primus bindings for Haskell.
That's a great idea. I'm not sure it can just be copied due to copyright and stuff, but we should probably have the best advice from the community collected somewhere.
In that case, my answer is that my proposal has nothing to do whatsoever with transcendence nor re-exports. It's about overriding version bounds after a version has been released, and nothing else. One problem at a time!
is there a public online version of beginning haskell?
however, editing version bounds after they turn out to be too strict already works on hackage...
It looks kind of like a spreedsheet? Anyway I could see it being easily done in homoiconic languages like scheme, but Haskell is not like that. BUT if you could move down form "database of Haskell terms" to "database of Haskell-like terms written in custom language and interpreted and runtime" it would make things much much simple. Then even I could probably make it in couple days... Will this be of any help: this http://www.haskellforall.com/2014/06/spreadsheet-like-programming-in-haskell.html ?
Language extensions are not part of Haskell (the language), but are implementation-specific. Presumably you would need to master them to become a master of GHC, say, but not for Haskell.
Wow, how did I miss that! It's in the maintainers page, bottom section "edit cabal metadata": &gt; You can edit the version constraints for the dependencies, either to restrict or relax them. The goal in editing the constraints should always be to make them reflect reality. &gt; * If the package fails to build against certain versions of a dependency then constrain the version. &gt; * If the package builds against and work correctly with a newer version of a dependency then it is ok to relax the constraint
Generally it is better to err on the side of narrow bounds than loose bounds. The reason is that if your library has just one version with loose error bounds in its entire history it poisons the dependency resolution of all subsequent versions. Let me give a concrete example. Let's say that version 1.0 of my hypothetical library `foo` has no bounds on some dependency `bar` and `baz`, both of which are also at version 1.0. Then `bar-2.0` comes out and my `foo` library breaks. "No problem," I think, "I'll just release a `foo-2.0` with an upper bound of `bar &lt; 2.0`. However, now I have a problem: let's say that `baz` then adds a dependency on `bar &gt;= 2.0`. The *right* thing to do would be for `cabal` to warn me that `baz` and `foo` have conflicting dependencies so that I can fix one of them, but that's not what will happen. Instead, `cabal` will try to resolve the conflict by installing `foo-1.0`, which has no upper bound and therefore does not conflict. Now the user gets an **uninformative build failure** instead of an **informative dependency resolution failure**. In fact, unless I blacklist `foo-1.0`, all dependency resolution failures will be transmuted into worse build failures. This is why narrow upper bounds make a better default.
I'm not advocating for no upper and/or no lower bounds use or the use of optimistically loose bounds with fingers crossed. I'm suggesting minimizing the use of pestimistic narrow bounds and the resulting false postitive conflicts by using informative bounds. Informative bounds implies some reasonable convention is in plac to support an informative bounds range decision that is as wide as possible, yet safe under the convention. For discussion of the advantages of an informative bounds, I've applied the Semantic Versioning convention. [Semantic Versioning] (http://semver.org/) Roughly any Major != Major version is incompatable. For any Major.Minor &lt;= Major.Minor the latest version is compatable. This minimizes false positives by letting a library developer use a safe, wider bounds range under the Semantive Versioning assumption. i.e The latest X.Y is safe. The second point was that there is a Cabal Hell / Hackage cost if popular libraries declare themselves Experimental / Unstable for years and years. Library user's have no other recourse but to be pestimistic and apply overally narrow bounds. This is an easily avoidable burden by having one or more Major.Minor stability points in the libraries timeline. 
Note that nowadays you can edit the cabal files on hackage in-place to add upper bounds for the older libraries, to remove the poison. You'll have to do this for *all* old releases, though, so I don't think this is a good default strategy, but at least we have tools to fix this now if it happens.
&gt; You can't break your installation when using --force-reinstalls in a sandbox. What happens, exactly, when you force reinstall a global/user package from within a sandbox? It merely gets shadowed? 
Hmm, right, that's a problem. I guess it's hard to get rid of *all* partial functions right now. There is more work to be done...
Where `empty` should have been `null`, I guess?
Under the Semantic Versioning convention: "Major version zero (0.y.z) is for initial development. Anything may change at any time. The public API should not be considered stable." However within a Major release v1.0 and above all Minor releases are backwards compatable. Say for example, in attoparsec Major v1 extended itself by adding a new set of functionality which is 100% backwards compatable. Under Semantic Versioning a Minor version bump happens, say attoparsec went from v1.4.24 -&gt; v1.5.0 In my library I use one or more of these new features (apis). My lower bounds is now 1.5.0. What is my safe upper bounds for attoparsec well any subsequent release that is &lt;v2.0. e.g. v1.9.23.2 is not a problem under Semantic Versioning.
Cabal has a lot of new options that make it much easier than before to resolve these kinds of problems in practice and get a build. For example, you can specify `--allow-newer` for specific packages, or `--allow-newer` together with `--constraint` if you don't want to allow *arbitrarily* newer. Etc. And you can record your attempts in a project-local `cabal.config` file to help smooth the process of zeroing in on a combination of settings that will guide cabal towards a winning build plan. So that's another reason why I think you're right that erring on the side of narrower upper bounds is better. But you are also right that getting good feedback from cabal in the process is extremely important, and you have identified an important but subtle hole. When encountering the kind of problem you described, it's not a mistake for cabal's solver to investigate whether installing a newer version of some package would fix it rather than just giving up. But if in the end things don't work out for cabal, right now cabal will just report the very last thing that went wrong, which sometimes is unhelpful. The current solution is to use `--verbose` and then rummage through a lot more output manually. But it would be great if cabal could look back at the whole process of the search for a build plan and use some heuristics to identify intelligently various kinds of problems that might be the root cause of the build failure. That sounds like a pretty ambitious feature though. Do you think it should be added to the [cabal issue tracker](https://github.com/haskell/cabal/issues?state=open)?
Maybe something less ambitious would also work, such as a way to ask cabal: "Why didn't you try this package/version in the context of this install?" and it outputs everything it conflicted with.
The Major.Minor under the PVP is not monotonic. There is no guarantee that a later minor version within a major version is backwards compatable as it allows removal. IMHO this is not a good thing as effectively any minor release is breaking. Under Semantic Versioning within a Major version Major.Minor is monotonic. Effectively any minor release is _non-breaking_. To break backwards compatibility you have to bump to the next Major. The stronger assumptions under Semantic Versioning mean that for any dependency given a minimum lower bound I'm minor version unbounded safe. e.g. v.1.2.?.? to &lt;v2.0 is safe. i.e. Any v1 version greater than my lower bound is safe to use.
I think the heart of the matter here is the Hackage PVP versioning guidelines versus the Semantic Versioning guidelines. The two key differences are 1) Major version 0 is unstable. 2) Minor versions are backwards compatable with non-backwards compatability always entailing a major version bump. This ensures a Major.Minor release ordering is safe for all subsequent minor versions. i.e from some fixed lower bound to infinity within the major version of the library. The stronger constraints of Semantic Versioning ensures the use of &gt;=x.y.z to &lt;(x + 1).0 is safe all cases. These stronger assumptions would vastly reduce Cabal Hell IMHO as it makes things easy to reason about. 1) Different major versions are compatable. 2) Within a major version any minor version is backwards compatable with regard to any preceding minor version. Pretty straightforward rules to Cabal by.
PVP ensures that &gt;= x.y.z &amp;&amp; &lt; x.(y+1) is safe in all cases. What exactly is your complaint with that?
&gt; 1) Different major versions are compatable. I guess you mean *in*compatible.
I think the best answer here is to update the Cabal PVP to use Semantic Versioning. Maybe one option would be to adopt some convention in Cabal such as a version prefix of S. build-depends: pvplib == 1.2.* semanticlib == S1.2.3 Note for semanticlib Cabal can safely assume anything &gt;S1.2.3 to &lt;S2.0 is safe. In other words Cabal can implicitly assume a safe upper bounds. 
One of us is very confused here and I'm not sure who it is. As far as I can tell from what you are saying PVP has exactly the property you are seeking. There's a difference in presentation: in PVP the major number has two components. For the case of"A.B.C", "A.B" is the major number and "C" the minor. 
I understand part of the `Protofunctor` class; namely, I know what a bifunctor is, and I know covariance and contravariance are in terms of subtyping. But how does this come together to form `Protofunctor`, which is "intuitively a bifunctor which is contravariant in its first argument and covariant in its second"? Looking at the `lmap` and `rmap` types, what does it mean to "map the first argument *contravariantly*" or "map the second argument *covariantly" (emphasis mine)? I'm missing how covariance or contravariance can be encoded in the type system, or what sort of laws it requires from typeclass instances.
Depends on whether or not you count libraries that have extensions turned on...
The proposed alternate Prelude only helps you with a relatively small number of imports. On large enough projects you will still have the same problem of large import lists even if you use this alternate Prelude. A tool would be very helpful in those situations.
If under the PVP &gt;= x.y.z &amp;&amp; &lt;x.(y +1) is safe then by induction for any y' &gt; y =&gt; &lt;x.y' is safe. Given different x's (major) are incompatable. Then what is the point of an upper bounds at all? In other words if we were all following the PVP wouldn't setting an upper bounds be rare to non-existent, in fact harmfull? (i.e. False positive conflicts) Why not let Cabal implicitly assume all subsequent minor versions are safe? 
I'm fairly certain it's something like this lmap (first argument of the bifunctor is contravariant): `(a -&gt; b) -&gt; p b c -&gt; p a c` Notice that you have a method from `a -&gt; b` but your profunctor goes from `b -&gt; a` rmap (2nd argument of the bifunctor is covariant): `(b -&gt; c) -&gt; p a b -&gt; p a c` You have a method from `b -&gt; c` and the profunctor also goes from `b -&gt; c`
I'm assuming transitivity here. Assuming for some fixed y. Given x.(y+1) is safe for x.y &amp;&amp; x.(y+2) is safe for x.(y+1) then x.(y+2) is safe for x.y Under Semantic Version this is true. Under the PVP I see "If any entity was removed, or the types of any entities or the definitions of datatypes or classes were changed, or orphan instances were added or any instances were removed, then the new A.B must be greater than the previous A.B. Note that modifying imports or depending on a newer version of another package may cause extra orphan instances to be exported and thus force a major version change." The PVP only requires A.B be greater with regards to a backwards compatability breaking change. e.g. 1.2 -&gt; 1.3 or 1.2 -&gt; 2.1 or both correct. Under Semantic Versioning only 1.2 -&gt; (1+x).* is correct. 1.2 -&gt; 1.(y&gt;2) is forbidden. 
For modules where everything is re-exported, just export them using their own name, and while you won't get any documentation you will get a nice link to the original module. For modules where only certain symbols are imported, you can use haddock to isolate links to those modules and put additional documentation near the links that tells what symbols are exported. The prose might not be that useful, particularly when if it ads nothing to the source. (Although, perhaps like good documentation it should document how/what, but rather why only certain symbols were selected.) The links are more valuable than you might think.
precium, please understand [this comment](http://www.reddit.com/r/haskell/comments/2ao3ul/cabal_semantic_versioning_and_endless_experimental/cix54zb) and reread the PVP and this discussion and see if it doesn't clear things up.
Yikes, just saw this. Definitely the point of departure.
Yep the mapping of PVP A.B as major to Semantic Versioning X as major caused confusion. "There's a difference in presentation: in PVP the major number has two components. For the case of"A.B.C", "A.B" is the major number and "C" the minor." "We don't want to restrict library authors' ability to specify &gt;= x.y &amp;&amp; &lt; x.(y+2) though, if x.(y+1) is deemed compatible." I see where you are coming from now.
OK, glad we've cleared up some misunderstandings.
xargs
Well, I'm open to pull requests.
Well, GHC is Haskell for all practical purposes.
The PVP is *roughly* the same as Semantic Versioning. What Semantic Versioning nebulously refers to as just "backwards compatible", the PVP spells out what a modules API is, what changes are compatible, and exempts some changes that, while not strictly compatible from a purely technical prespective, do not normally cause enough problems to be relevant. The PVP considers the first *two* components of the version to be the "major version". Semantic Versioning considers only the first component to be the "major version". This is largely ignorable **except**... Semantic Versioning special cases major-version = 0, and basically says there are no compatibility requirements here. I like this, and I wish cabal would do it for the second component. I want for there to be a sane version that I can stick on my unstable-API-in-flux-but-going-to-be-next-major-version branch so that I can put out alphas and betas without tying myself to the API of my first alpha or bumping through several "major versions" while really only doing one round of refactoring / optimization / innovation / etc.
Yes, but your wording makes it sound much worse than it is. "Periodically redo everything from scratch" is not unreasonable at all because Haskell libraries are changing all the time. If a "top-level" package with a lot of dependencies (text for example) gets updated, then you're going to have to rebuild everything that depends on it (which for text will be almost everything). "Fix up other people's packages" is also not the least bit unreasonable. The Haskell ecosystem moves fast. For the most part, package maintainers are very responsive and quite good about fixing their packages quickly when things break. But if you're doing significant Haskell development, you will inevitably encounter a situation where a new version of a package comes out on Monday and it breaks another package, but the second package's author is on vacation or something and not able to address the issue for a week or so. Now, no reasonable person can complain about a one-week turnaround time for things like this. These are open source projects that maintainers usually work on for free in their spare time. Typically this won't be a problem, but occasionally I've been in situations where I need something to work right now because it's blocking other things I'm working on. In these situations, you can't expect maintainers to drop everything and fix their packages for you. You have to fix things yourself. In short, this things are just the reality of a fast-moving software ecosystem.
You still have to type the names of every package, which can be a long list.
Depends, I usually pull the list from some cabal- or ghc-pkg-generated list and sed/awk it down for xargs consumption, but I'm a command-line junkie.
I don't mean "doing" as in sticking to things you're comfortable with; I mean doing as in pushing the boundaries of what you're comfortable with. You said you know "monads somewhat", which indicates some uncertainty about monads. You could alleviate a lot of that uncertainty through simply working through the creation of monads from various functors, verifying the monad laws, and figuring out if they're of any use or not. Then play with monad transformers, and figure out how to use them well. If you truly understand that list of things along with a solid understanding of monads and monad transformers, you should have all the tools you need to write some interesting programs. Will there be other more clever solutions to the problems you have? No matter how much you know, the answer is probably still "yes". If you let the fact that there are better solutions out there keep you from practicing what you know, you'll never get to the point where the better solutions start occurring to you. Also, go out and read programs. Grab the source to an interesting-looking application on Hackage and figure out how it's structured. If there are mysterious things, try picking the types apart. If a bunch of extensions are involved, the GHC manual will at least tell you what they do, and there are most likely a number of papers or blog posts or 'School of Haskell' lessons about using them. If everything is too far over your head, try looking for another program to read! People have successfully written and published to hackage useful programs at many different levels of "Haskell mastery". I'm sorry if I don't seem to be having a lot of sympathy to your predicament, but having learned to program in the pre-internet era, I can't help but feel that you have an embarrassment of learning resources at your fingertips. I'm not saying the situation couldn't be improved, but it's certainly not hopeless right now.
Excellent! Example code helps a lot.
Debian's ghc is 7.6.3. We have not successfully built 7.8 for mips yet, and any help would be appreciated. See https://bugs.debian.org/751479
In addition to an alphabetical listing, could you consider also listing by difficulty/prerequisite knowledge needed? That would be more than perfect for the occasion.
The problem is that GHC does cross-package inlining, unlike Ruby or even C. If `counter-0.0.0` defines an opaque data type like newtype Count = Count Double add (Count x) (Count y) = Count (x + y) and `client-0.0` imports it and uses `foldl' add` to sum some counts, then even though the constructor `Count` wasn't exported, GHC still needs to use the definition to generate nice efficient unboxed arithmetic when compiling `client`. Then a change of representation in `counter-0.0.1` which is perfectly backwards compatible at the source level to use newtype Count = Count (Compensated Double) (from the real package `compensated`) will still require recompiling `client-0.0`, and probably inline the new addition operation (`add` still works by overloading on `Num`). Even worse, we're now maybe even inlining code from a package that isn't even a direct dependency! So, that explains why there can be more distinct compiled verison of the code than in Ruby (obviously) or even in C (where symbols are usually referred to by name, and only things in the header file are inlined). The other part of the problem is that GHC can't keep quite that many versions straight yet, though it is being worked on. It's perfectly fine if you have *one* build each of `counter-0.0.0` and `counter-0.0.1` around, but it goes by version number so it's not happy if you have `client-0.0` compiled against `counter-0.0.0` and `client-0.0` compiled against `counter-0.0.1`, let alone a `client-0.0` compiled against `counter-0.0.1` compiled against `compensated-0.5` and a `client-0.0` compiled against `counter-0.0.1` compiled against `compensated-0.6`.
`sequence` is amazing. It can do so many things, depending on what you give it. -- Combine Maybe values in a list &gt; sequence [Just 3, Just 5, Just 2] Just [3,5,2] -- Perform several I/O actions in order and put their results in a list &gt; sequence [getLine, getLine, getLine] IO ["one","two","three"] -- Compute all possible permutations of elements from lists &gt; sequence [[1, 2, 3], [4, 5]] [[1,4],[1,5],[2,4],[2,5],[3,4],[3,5]] -- Evaluate several functions with the same argument &gt; sequence [(+2), (*2), (^2)] 5 [7,10,25] Learn it, love it!
The online version of the book has comments after each paragraph, somebody will likely have already pointed out any issues there
I think you are probably right about that.
It's not issue of cooperativity or friendliness, but rather one of scale. Let me throw out some numbers: my "Program Imperatively using Haskell lenses" post got 189 unique views yesterday with absolutely zero ongoing effort on my part. I doubt Edward helps that many people on an average day, and if he did it would be a severe waste of his time. The reason I advocate documentation over IRC is that good documentation will always outperform even the most productive and helpful person.
Unless you're willing to settle for just `Num`...
I agree with collaboration *with a few people*, but there is a limit to how many people you can collaborate with. Past a certain point you need to begin automating the process of teaching others, which is the purpose behind documenting one's knowledge.
I wonder if such a query tends to actually have a small [unsat certificate](http://www.satcompetition.org/2013/certunsat.shtml)?
What I didn't get was how the Identity and Const functors fit into the 'view' and 'over' implementations, but that's not your fault — the step by step examples were good. For me, it just takes a while for the types to 'sink in' before I understand and feel them instead of just knowing them, the same happened initially with monads. I just have to stare at the type signatures long enough, then at some point they'll start to make sense. This means I'll probably read this tutorial carefully a few more times in the coming days. Anyway, I hope to see more posts from you.
I've written some really basic doctest type docs against a few of his libraries. It's a fun way to learn a new library, and contribute back, but at the same time, not really have to be all that smart. I encourage most everybody to just go and write some docs for him - he's so super friendly and helpful.
As a side note, using sandboxes has pretty much resolved cabal hell for me. I routinely work on *multiple* big projects with 150+ dependencies each and rarely run into build problems. On top of what mightybyte has said: 1. In the sandboxed world, most "I have no clue why this is failing" issues after upgrading a few dependencies are because of upper-bounds that are too restrictive (possibly in your own dependencies). "packdeps" program is pretty helpful there to figure out which of your packages is the culprit. 2. rm -rf .cabal-sandbox &amp;&amp; rm cabal.sandbox.config are your friends. If nothing else works, just blow away your sandbox. Rebuilds are now really fast for me due to cabal's multi-core support. 3. You'll occasionally get weird error messages from previous build left-overs. Do a "cabal clean" in your project folder every now and then as a first line of defense. 4. cabal -v3 is the key to the remaining problems (when using sandbox), as the vanilla cabal is pretty bad at figuring out which of the conflicts is relevant. Just grep for the work "fail" and it'll quickly zoom you into the culprit.
I'll cross-post my remarks from the other thread for relevance: --- 1. In the sandboxed world, most "I have no clue why this is failing" issues after upgrading a few dependencies are because of upper-bounds that are too restrictive (possibly in your own dependencies). "packdeps" program is pretty helpful there to figure out which of your packages is the culprit. 2. rm -rf .cabal-sandbox &amp;&amp; rm cabal.sandbox.config are your friends. If nothing else works, just blow away your sandbox. Rebuilds are now really fast for me due to cabal's multi-core support. 3. You'll occasionally get weird error messages from previous build left-overs. Do a "cabal clean" in your project folder every now and then as a first line of defense. 4. cabal -v3 is the key to the remaining problems (when using sandbox), as the vanilla cabal is pretty bad at figuring out which of the conflicts is relevant. Just grep for the work "fail" and it'll quickly zoom you into the culprit.
In some cases, it's it's even rank N documented!
My conclusion (explained in that blog post) is that the "+ Category" part is a pretty big deal, considering the difference in expressive power between `IOF` and `IOA`.
From Wikipedia: &gt;[Covariance and contravariance may refer to:](https://en.wikipedia.org/wiki/Covariance_and_contravariance) &gt; * Covariance and contravariance of vectors, in mathematics and theoretical physics &gt; * [Covariance and contravariance of functors, in category theory](https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_functors#Covariance_and_contravariance) &gt; * [Covariance and contravariance (computer science), whether a type system preserves the ordering ≤ of types](https://en.wikipedia.org/wiki/Covariance_and_contravariance_(computer_science\))
"A lens is like a burrito..." 
Identity is not all that strange. What you want is (a -&gt; a) -&gt; (s -&gt; s) but what you have is (a -&gt; f a) -&gt; (s -&gt; f s) What you need then is to find a functor `f` that "doesn't do anything." The `Identity` functor *is* basically a meaningless wrapper you can put around any value to make it a functor that "doesn't do anything". If you have a value and want to make it a functor, just put `Identity` around it and it will become a functor, and you know that later on you can always just `runIdentity` if you want the value back. So when you set `f = Identity` so you have (a -&gt; Identity a) -&gt; s -&gt; Identity s that's basically the same thing as (a -&gt; a) -&gt; s -&gt; s except you have to `runIdentity` to get the value back. That's my handwavy intuition for why it makes sense, at least. ---- ...the `Const` one is harder. The idea is that a `Const a b` behaves sort of like an `Identity a` in that you can at any point `getConst` to get the `a` back. Since we want the `a` back, we have to set f = Const a which means the rest of the signature will be (a -&gt; Const a a) -&gt; s -&gt; Const a s and this makes sense, believe it or not. The implementation of the lens is going to take an `a` from the `s` and put it in the `a -&gt; Const a a` function, effectively putting it safely in the `Const`. Now all the actual work is done. But! To make it fit in with the rest of the operations, it has to return a `Const a s`, so it's going to do the fmap thing just to transform the type. What's important is that the implementation of the lens will take out an `a` from the object `s` and run a function on it – when we let this function be `Const`, it in effect saves the value in the const so we can get it out later when the `Const` is returned but with a different type (`Const a s` instead of the `Const a a` we save `a` in.) Since it usually helps to be specific, consider first :: Lens String (String, Int) which turns the signature into (String -&gt; Const String String) -&gt; (String, Int) -&gt; Const String (String, Int) where it's easier to see how it takes the `String` from the `(String, Int)` and puts it into the `Const String String`, and then transforms that to a `Const String (String, Int)` from which you can retrieve the `String` which is what you want with `view`. It's sort of a long way of doing it because it transforms the type of the `Const String` before we get the `String` back, but using the `Const` functor means that the `String` is safely tucked away whatever operations we do on the functor, so we can retrieve it again when the lens is done. The reason to do it that long-winded way is of course that it allows us to use the same lens for all kinds of operations, which is very cool.
And, to go further, what situations is it useful in to have that kind of typeclass?
Yes, it will shadow. I'm not sure if cabal asks you to force reinstall for this case though. 
If most of an article you're reading _doesn't_ go over your head, then you're not challenging yourself enough! The best way to learn is through things where you follow only a little. If you follow nothing, there's no toehold for you to engage with, and if you grasp it all, then you're not learning very much.
Hey, thanks.
why tuples though?
Thank you so much! Please keep it up, it would be great if this kind of introduction would cover the more involved topics of the lens library. I am particularly interested in how the combination of the different parts work exactly. One example, I often find myself wanting to execute an IO action on the Just value that is nested somewhere, but ignore if there is a Nothing in the way, or a failed map lookup. I don't know yet how such a thing could be expressed nicely with lens and hope to learn things like that from you series. Looking forward to it!
Bilateral collaboration, sure, but I think that's less true for unilateral aid. You _definitely_ learn and grow by teaching, but again, there's a limit.
Interesting. What does he use for the "drawing surface"?
Exactly. That's rather obvious, since only `head`, `tail` and `null` get mentioned in the section, but errors like this are accompanied by incomplete type signatures and/or missing constraints. But to be honest, although that sounds bad, it doesn't really happen this much. Either way, that book could have been so much better with copy-editing.
What about tuples bothers you?
^(I may have asked this before, but I forget.) What would it take to get this onto an Android tablet?
Thanks for posting this! I was also about to post. :-) 
cairo is used for drawing surface backend. For pdf rendering engine, I use poppler, and for svg rendering engine, I use svgcairo which is based on librsvg. 
Hoodle itself should be buildable on windows. At least, I was using hoodle on windows machine until 2012 (before I have poppler pdf support). Hard part is dependencies. cairo, gtk, poppler, librsvg, dbus.. etc. I will be very pleased and grateful if someone helps me to build hoodle on windows machines. 
Yeah I've also written a (very) few lines of documentation for `lens` and I second that. He's very willing to accept patches.
I tried to install this twice and gave up. It's a really cool tool, though, when you can get it. I want to try the SVG export.
Is there a feature for using the hand for moving/panning when writing with a pen (à la OneNote)?
I'm not sure. I guess I just prefer curried forms over uncurried ones. They just seem to be less noisy.
Only by a very hacky way on my system. It simply turns off touch input on my system when pen is near, but this is by explicitly using X11 command and not portable at all. Currently the driver model is not very well-defined. Making portable working detection mechanisms for touch device and pen device is hard. Once devices are well registered, turning on/off touch for pen input itself is not hard at all. I will try to make an abstraction for the device driver to support more systems. 
Cabal-bounds seems to be having an upper bounds issue with lens 4.3, slightly amusing :P Of course, I just used an already-installed cabal bounds to fix it
yeah. that SVG and PDF export from selected items is really useful for me when I prepare for some quick presentation or something. I remember I made it after your suggestion some time ago and thank you! I am now maintaining hoodle pacakge on NixOS/nixpkgs, so for nix users, hoodle installation should be more easier. Need to package hoodle using rpm/deb/pacman, though, to make this available to more people..
Hello, Halting problem! What you want is a totality checker. This is where data vs codata comes in handy. Unfortunately in Haskell, they are the same thing. 
If you're looking for a general solution I'm afraid its [impossible](http://en.wikipedia.org/wiki/Halting_problem), however in this case it has to do with the parametric type being passed as an argument. Since `Eq` makes no garuntees as to order (hense, not `Ord`), we have to look towards the implimentation of the function handling the infinite structure. My haskell-fu isn't the strongest but as I understand it type declarations give no indication to strictness but arguments can be made strict with the `BangPatterns` pragma. I would suggest looking [here](http://www.haskell.org/haskellwiki/Performance/Strictness) for more info about strictness as my knowledge of strictness in haskell is at best incomplete and at worst flat out wrong.
What you're asking for looks like Reactive programming though it won't be normal Haskell expressions, you'll have `Behaviour [Int]` instead of `[Int]`. There are several libraries to do that, see Netwire, reactive-banana, Threepenny...
what tuples are you referring to? :) 
 runAuto :: a -&gt; (b, Auto a b) Why not use a constructor there instead?
the author is an artist.
Whenever you want to treat the data like a tuples, you have to convert it to a tuple. A bunch of functions already supports tuples but not your custom type.
Here is a slightly more newbie-friendly, expanded version of /u/deltaSquee's excellent answer. &gt; Hello, Halting problem! This refers to the naive way i which you have phrased your question. You ask how to detect infinite loops, but there is a well known theorem stating that it is impossible to do so (to "solve the Halting problem") in general. &gt; What you want is a totality checker. This refers to a well-known workaround for the aforementioned theorem. The theorem states that it is impossible to write a test which will *always* say that a program will terminate if it will in fact actually terminate, and will always say that a program will loop otherwise. But it is possible to write a conservative test instead, one which says "ok" when it is absolutely sure that the program terminates, and "maybe" when it is not clear whether the program will terminate or not. I do not know of any totality- or termination-checkers for Haskell, while [Agda](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=ReferenceManual.Totality) and [Twelf](http://twelf.org/wiki/%25total) are two languages in which such tests are built into the language. &gt; This is where data vs codata comes in handy. In the details of your question, you mention infinite data structure. The difference between [data and codata](http://blog.sigfpe.com/2007/07/data-and-codata.html) is that codata allows data structures to be infinite, while data does not. Some languages, such as [Agda](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=ReferenceManual.Codatatypes), distinguish between data and codata, never allowing a function which expects a finite value to be passed an infinite value. &gt; Unfortunately in Haskell, they are the same thing. In Haskell, there is no distinction between data and codata, in the sense that the type of a function which expects a finite list is the same as the type of a different function which also works on infinite lists. ***** Now for my answers. Even if Haskell doesn't distinguish between finite and infinite lists, in your own code you could easily setup a convention that finite lists are always wrapped in a "Finite" wrapper, while infinite lists are always wrapped in an "Infinite" wrapper: newtype Finite a = Finite a newtype Infinite a = Infinite a isMemberOf :: Eq a =&gt; a -&gt; Finite [a] -&gt; Bool isMemberOf x (Finite xs) = x `elem` xs isHeadOdd :: Infinite [Int] -&gt; Bool isHeadOdd (Infinite (x:_)) = odd x Now the type checker prevents you from passing an `Infinite xs` value to `isMemberOf`. Of course, there is nothing to prevent you from constructing the value `Finite [0..]`, which would render the checks meaningless, but the error is so obvious that you shouldn't need a type checker to avoid making it.
oh god have i become one of *them*
I just wanted to say thanks to roche for writing up things about *failures*. It's not too often that we see the intermediate steps for coming up with sparks of genius, and that kind of thing is not only helpful, but very encouraging, for those actively learning. Also, this article is nice! There's so much out there in GHC via extensions that enables all kinds of crazy things and it's cool to see stuff like this popping up more and more often.
Wow, I did not expect such an wonderful answer. Thanks a ton! The next couple of days are very busy, but I'll go through it carefully as soon as possible.
What went wrong with the com package? Could it be fixed by bumping version bounds?
I have read that GHC specifically uses msys instead of cygwin so that you *can* deploy executables without having to also ship Haskell-specific stuff. Since you mention GLUT, I expect that your problem is not that you need to also ship some Haskell-specific stuff, but that you also need to ship the dll files for GLUT and any other external dependencies of your program. You would have to do this in any language.
System/Win32/Com/HDirect/Pointer.hs:102:20: Not in scope: 'catch' Failed to install com-1.2.3.1 cabal: Error: some packages failed to install: com-1.2.3.1 failed during the building phase. The exception was: ExitFailure 1 That's what happens when I try to install com, I also get a bunch of warnings that #include and INCLUDE pragmas are deprecated. I'm not sure what that means or how to do it, but I'm willing to try! How would I go about doing that?
No it isn't useless. Do you have a better name? its just a freaking name.
So, first step is `cabal unpack` to get a copy of the source. From there, see if you can just import things (and edit the cabal file too) to bring stuff in scope. With hoogle, and hayoo, it's not too hard to find what package something is in. ... I just hoogled it, and catch is in the prelude? Huh? Can you gist or pastebin the entire log? EDIT: Oh, and I might mention that I don't actually have a windows machine with Haskell on it. I could test the installer at the end though.
catch was removed from Prelude in GHC 7.6.
I don't think it's "gone" anywhere as such. It's just that some packages don't get Haddocks generated. Perhaps someone more knowledgeable can explain what's going on.
Cabal log: http://pastebin.com/x4RgA4pb I just unpacked the source, working on building it now. 
Ah, so you just need to add in the import to bring `catch` back into scope. 
The `com` package is a FFI binding to Windows library. Hackage only builds docs for libraries it can compile and as the server runs Linux it won't be able to compile `com`.
Okay gotcha. Thanks guys!
You only get the source button on modules, not packages. Since there is no haddock page for any of the com modules, you also can't access the source button.
Do you mean a person that gives a terse, technically correct answer to a newbie without realizing they might not understand it?
Indeed, here is how ghci could help. First, ghci only allows you to trace through functions for which you already have the source, so here is a reimplementation of `elem`: -- Main.hs myElem :: Eq a =&gt; a -&gt; [a] -&gt; Bool myElem _ [] = False myElem x (x':_) | x == x' = True myElem x (_:xs) = myElem x xs Next, load your source file into ghci and put a breakpoint in the function: $ ghci Main.hs Ok, modules loaded: Main. &gt; :break myElem Breakpoint 0 activated at Main.hs:(2,1)-(4,39) For convenience, let's ask ghci to list the source code after each step. Now you can call your function and step through it: &gt; myElem 1 [2..] 1 myElem :: Eq a =&gt; a -&gt; [a] -&gt; Bool vv 2 myElem _ [] = False 3 myElem x (x':_) | x == x' = True 4 myElem x (_:xs) = myElem x xs ^^ &gt; :step x :: Integer = 1 x' :: Integer = 2 2 myElem _ [] = False 3 myElem x (x':_) | x == x' = True ^^^^^^^ 4 myElem x (_:xs) = myElem x xs &gt; :step x :: Integer = 1 xs :: [Integer] = _ 3 myElem x (x':_) | x == x' = True 4 myElem x (_:xs) = myElem x xs ^^^^^^^^^^^ &gt; :step 1 myElem :: Eq a =&gt; a -&gt; [a] -&gt; Bool vv 2 myElem _ [] = False 3 myElem x (x':_) | x == x' = True 4 myElem x (_:xs) = myElem x xs ^^ &gt; :step x :: Integer = 1 x' :: Integer = 3 2 myElem _ [] = False 3 myElem x (x':_) | x == x' = True ^^^^^^^ 4 myElem x (_:xs) = myElem x xs Looking at each step of the recursion as it unfolds should hopefully make it clear that `myElem` is going deeped and deeper into the infinite list, and will never terminate.
So for him, generate a module and the change will be detected and (re)compiled. How could he bring the compiled module into scope for the user?
Useful. But why isn't it syntax highlighted? It's not like it needs to compile it to make a pretty source view. Hackage is open source right? I might poke at it and try to fix this in the future.
&gt; STLC with only one type, `Term`. Simply-typed lambda calculus expressions are indeed guaranteed to terminate, but STLC is not unityped. Untyped lambda calculus expressions are not guaranteed to terminate. Also, the terms you are describing don't include lambda abstraction nor application, so I don't understand how it relates to the STLC at all. Moments later, your fold and unfold primitives clearly assume the existence of functions. Did you accidentally omit them? *edit 1*: Ah, I see you have modified your post to include them now. Never mind! *edit 2*: If your goal is to design a unityped language, why do fold and unfold have type annotations? If your language is indeed unityped, what happens at runtime if fold is given a float instead of a function?
Is a lisp really lisp without macros? I'm not so sure... Anyway, yes. STLC is a total language. Naturals and Lists won't break that. Just avoid general recursion, letrec, and/or fix. All recursion needs to be done through the primitive recursor for naturals. There's surely going to be functions you can't write. Probably more importantly, there are some you can't write very easily. The GCD algorithm is one that, while not impossible, is much less direct, since its recursion is not primitive in either argument. Ackermann is another one that becomes tricky (maybe even impossible).
It gets stuck with a float trying to be applied to something. Since there is no reduction for that, it just won't do anything. (The goal is not to make a type rich language.) Oh I guess I made no sense when I said "applying those ideas to a unityped language" and then suggested STLC. It is just that the end language looks a lot like Lisp. Erase it! (:
I see, seems like that is what I needed, after all! Fits perfectly on my project. I gave up of the whole `it should be total` thing and just added a `Rec` operator, but it was giving me many problems. I guess that is the perfect solution.
If you make a qq for your language you can have template haskell as a "backend" and have ghc generate your code for you.
I think it bears the wider implication of being immersed so deeply in your abstractions and theory you forget how long it took you to get there.
It depends on what you mean by practical, but that's the deal with total programming languages, if you want something to google for.
The plugins paper has examples of this, I.e. implementing a REPL where each line compiles to a plugin.
Should clarify what you mean by "serious computation". I don't think it's necessarily a foregone conclusion that you want to write a full compiler, though it will depend on what the "D" in your DSL is! In my experience we can write very efficient code just by writing an interpreter on top of the Haskell runtime using techniques like HOAS and free monads which share the Haskell function implementation and use ASTs which explicitly unbox types. If you need a JIT compiler then of course LLVM bindings are available, but this is not without its caveats as well. If you give more details about your problem domain we can probably give a better advice.
Wow I could not understand the over operator for the life of me until this tutorial. Thank you!
Do you want to know why it is hard to do? Because loops are not first-class language features. Loop as a first-class function/object which you can easily reason about has not been invented yet. 
This may be what you intended (not entirely sure from the question), but I think the presence of unfold as defined here makes the overall language partial. Take f (Cons x xs) = Cons x (Cons x xs) f _ = Cons Nil Nil This is obviously total Now take `unfold f`. Applied to `Cons Nil Nil` this will not terminate. Or have I missed something?
The trick is cabal is on the path but the path environment variable has it in wrong order.
TBH the `unfold` is the part I'm having trouble with, I'm not sure how it is supposed to work. I've never used `unfold` in practise. What is the correct `unfold` implementation for that structure? (About your concern, I've just assumed that anything starting with `unfold` would be considered in normal form and wouldn't be unrolled further until necessary, but that is just what I understood from reading the paper.)
If you want the overall language to be total, I don't think you can have an implementation of unfold exactly. You would have to prove some property like the following: \exists (R :: relation). WellFounded R /\ (\forall x. R (tail (f x)) x) Which proves that there is no infinite chain of unfold steps. That's enough to prove that the unfold will terminate. But there are definitely functions allowable in a total language that don't satisfy that predicate.
Here's a [tutorial book](http://www.stephendiehl.com/llvm/) on using Haskell bindings to LLVM to JIT-compile a language.
Worth noting that if you give this lisp a fancy enough type system, system F for example, than you keep strong normalization and can encode lists and numbers without primitives. Obviously impractical but very enlightening for a hobby project.
I see :( What I can do, then? I'm thinking in just introducing a `enum` primitive. That is certainly total and will be powerful enough for any algorithm I can think of. After all, you can at least emulate a bounded `for loop` with `fold` and `enum`. What do you think?
Well, if you're considering a bounded for loop, then you could just have a bounded unfold. Just add a natural number as another argument that limits the number of cons cells produced by the unfolding. That will make your language total again. In general, supplying something to fill the hole of unfold might be tricky in a total language - you're generating data, and a lot of the simple ways to ensure that all data generated by a program is finite are messy (like adding the bound on a for loop or unfold as I mentioned). In dependently typed languages you can actually supply a fixed-point primitive which has a rich enough type that it can only be used to define total functions, effectively this amounts to requiring the caller to also pass a witness to a proof of the relation property I mentioned before. However, I am not a programming language design expert, so maybe someone else (or you) can come up with a nice way to do it.
The tree background is in front of the text on mobile :(
It's confusing, implicit behavior that is easy to make mistakes with. If I write: do x &lt;- get put (f x) I may be reading and writing the same or different variables depending on the type of f. And if I make a mistake where I want one or the other, it is much more likely that the mistake will be accepted. I can think of no other language where the choice of global(ish) reference is expected to be inferred, and uniquely associated with the type of the thing being written to/read from. It is typically expected that use of multiple references will involve multiple explicit names, which is what you can do with State+lenses (or somewhat less nicely without lenses), or is what lift does (poorly); or ST. Resolving the situation via types (which, in the presence of polymorphism, may ultimately result in two 'names' getting unified elsewhere in the program) seems like a poor methodology for this.
Have you see [shen](http://shenlanguage.org/) ? 
I thought that might be it. I agree that State+lenses is much nicer when it works, but if you need multiple copies of State that you can't commute together to combine it seems you're stuck with lifting (of either the manual or automatic varieties). That the lifting depends not just on the sort of operation you're looking for (State) but also on its type arguments (s) is a serious misfeature. It seems like we want lifting by operation in simple cases, lifting by explicit names in complicated cases, and some kind of zoom-like operation to embed the former into the latter. 
&gt; one relying on IncoherentInstances Oh my... &gt; the other on closed type families. Ah, okay. &gt; Intuitively, the reason why GHC needs so many ugly extensions to make the above code work is that we’re trying to simulate a closed type class with an open one. Yes, this is precisely what makes me uncomfortable with IncoherentInstances. &gt; The MonadReader r constraint can only be satisfied with ReaderT r but not, say StateT r. This seems like a feature to me. Suppose you have: do r1 &lt;- ask ... r2 &lt;- ask I, for one, tend to presume that a Reader has static data which does not change. (`r1` should be the same as `r2`). If you allow StateT to directly implement MonadReader, then you can have: do r1 &lt;- ask put somethingElse r2 &lt;- ask Then `r2` will not be `r1`, it will be `somethingElse`.
 Find t (t m) = Zero Oh, cool. I didn't realize that closed type family functions could do prolog/erlang-esque matching where you use the same name in two places on the matching side, and it checks that the two are equivalent.
&gt; Yes, this is painful, but IMO this problem should be solved by tools, not libraries. Java has the same problem, but Eclipse's organize imports feature makes it so you never have to think about it. Java _can't_ solve this problem with a library and so has to use tools. Given a choice between a library and a tool which solve the same problem equally well, I'd nearly always prefer the library.
Yes, what about it? It is not total AFAIK, is it?
If your specific problem is that a new version of a package has come out, and another package needs the older version, it's sometimes enough to just put a specific version number in your cabal file as well, which you can remove when the author of the second package is back from their vacation.
Thanks! I'm glad to hear!
&gt; rm -rf .cabal-sandbox &amp;&amp; rm cabal.sandbox.config are your friends. If nothing else works, just blow away your sandbox. Rebuilds are now really fast for me due to cabal's multi-core support. It was like a new world to me when I realised you could do this. Re-creating your sandbox is often really cheap compared to figuring out some tangled mess of dependencies. 
Which in a sandbox is cabal sandbox hc-pkg -- unregister &lt;package&gt; according to stackoverflow.
Integer overflow isn't particularly safe either ;)
Ah, that is a better idea, thanks. But how does it work on that paper, though? By your parameters that paper is wrong, because adding codata would make your language not total too. I guess unfold and codata are pretty much the same feature, just differently. No? Both are infinite if you keep evaluating.
It is not, but it has its own type system based on Sequent calculus. Thought you'd be interested. 
You can write installers using https://hackage.haskell.org/package/nsis. Generally you only need the glut dlls, everything else is linked in statically. 
Wouldn't GTK not be a bigger issue than getting Haskell to run on ARM? 
Note that although the build-bot didn't manage to make docs, the author (or any trustee) can upload docs.
That's exactly what I do, and it works.
Originally started developing at ZuriHac 2013. This is pretty alpha yet. Need to add more AST and CPP support to make it really useful. Codesearch-like features are also planned, so stay tuned. Code at https://github.com/robinp/nemnem/tree/master
&gt; Given a choice between a library and a tool which solve the same problem equally well, I'd nearly always prefer the library. In this case I prefer tools because a library obscures where the symbols come from. The tool solution makes it much clearer to all future readers, whether they're using the tool or not. Even someone just browsing the code on github benefits. Also, the library solution is less scalable because one "library solution" won't work with all projects.
Very cool!
&gt; But how does it work on that paper, though? By your parameters that paper is wrong I think it's more likely that you have misunderstood /u/darkotter, the paper, or both. Here is a subtle piece you might have missed: non-total and non-terminating are not the same thing! The paper uses codata and coinduction to illustrate that you can have total (i.e., mathematically well-defined on all inputs) programs which do not terminate. The key idea is that those programs do not get stuck in an infinite loop, they are instead making *progress* on every iteration of the loop, by adding more constructors to the end of an infinite data structure. See how the progressing loop, even though it is itself non-terminating, requires terminating components in order to progress? If one step of the loop was non-terminating, then we would not reach the next iteration of the loop, and therefore the loop would not progress. &gt; adding codata would make your language not total too As I have explained, codata and termination-checking are actually closely related. Adding codata to a language with bounded loops would be perfectly fine; the codata/coinductive parts would ensure progress, while the bounded loops would ensure termination. &gt; I guess unfold and codata are pretty much the same feature, just differently. No? Both are infinite if you keep evaluating. Close! Assuming `unfold`'s argument `f` is terminating, `unfold` is precisely the kind of progressing loop I have described, because each time `f` returns a `Cons`, it adds this extra constructor to the end of a potentially-infinite structure. Usually, one doesn't talk of codata unless there is also a totality-checker which ensures termination and progress where appropriate. So just adding `unfold` to a language doesn't mean that the language suddenly has codata, coinduction and totality; it might just make the language partial. Which is also just fine.
wow haskell is becoming as good as java.
Yup.
Cool, thanks for the explanation.
Seven months!~
Nice work! Perhaps this functionality could be reused to provide a "go-to-definition" function for text editors? And then hooked into everyone's favourite text editor. Or perhaps someone has already built such a tool?
I believe the idea is not what you're suggesting but rather that StateT would implement MonadReader when the monad it wraps implements MonadReader and just passes the ask down the stack using lift. What this work does allow is ReaderT to wrap another ReaderT and the specific reader value returned would depend on the type of the result. ReaderT Int (Reader String) a would have ask with return type Int and String depending on the context it is used. do x &lt;- ask y &lt;- ask return (x+10, "Hello, " ++ y) would work in this context. What you were proposing seems to be that State[T] would implement MonadReader by giving access to the state variable, but I don't think anyone's really proposing that that is a good idea.
Points to you for thinking about fixing it rather than just throwing up your hands and saying "it's terrible!"
There's [codex](http://hackage.haskell.org/package/codex) which fetches the source code for dependencies and uses [hasktags](http://hackage.haskell.org/package/hasktags) to generate ctags/etags files that can be used by text vim/emacs (and some other text editors) to go to the definition of the symbol under the cursor. You would generally run both of them inside your project directory, the former to get tags for dependencies and the latter to get tags for the current state of your project.
I have project like this, https://bitbucket.org/chemist/viewer. For go to definition i use hasktags for haskell, and ctags for other languages. Its not ready now, but you can see example in http://haskell.su. For highlight code i use highlighter with my patches for speedy on big files, repo for highlighter https://github.com/chemist/highlighter. )
Source of this problem in nemnem.css l 18-20: .hlit { border-bottom : 1px solid #b58900; } That's because the authors want the same line color at any position. For this, the usual `text-decoration:underline` doesn't work. A quick hack is .hlit { display : inline-block; /* enable margin manipulation */ border-bottom : 1px solid #b58900; /* same as before */ margin-bottom : -1px; /* remove additional space */ }
The first article in the series, [The problem with mtl](http://ro-che.info/articles/2014-06-11-problem-with-mtl.html), explains the motivation behind this work. Simply overloading the `ask` function is not the goal; writing reusable effectful computations is. Once you switch to the «extensible effects» mindset, this all should make more sense. This also explains why I want `StateT` to implement `MonadReader`: you may want to combine several computations operating on the same piece of state; but if *some* of these are not supposed to *change* that state, why not enforce it with types, giving them a more restricted type `(MonadReader s, ...) =&gt; m Smth`.
Thank you, applied fix! [edit]: Hm, Firefox still seems funny. Chrome is ok.
I did, but at that point as I remember it needed Cabal-based package info, but I don't want to restrict to Cabalized sources. Do you think there is functionality that could be reused without depending on the Cabal-originating data?
Nice! I don't plan to extend to other languages any soon, but focus on Haskell. Nemnem would do AST-informed highlighting, so it is possible to mark class method calls differently, etc.
What would you recommend to use with Snap if I want to work at Socket.IO level? EDIT: E.g., how to plug this into an app which uses Snaplets?
A similar project for Clojure is CrossClj - http://crossclj.info It already covers a large part of the Clojure open source codebase
Agda also has [something similar](http://www.cse.chalmers.se/~nad/listings/lib-0.7/Data.Bool.html).
I love it! Was this inspired by [Agda's standard library](http://www.cse.chalmers.se/~nad/listings/lib-0.7/README.html)? One small thing: in Agda's case, the module names in imports are hyperlinked. Do you think that would be possible with your tool as well?
Easier is to just to use `border-bottom: 1px solid inherit` by default and then `border-bottom: 1px solid &lt;yourcolor&gt;` when you hover. Doesn't change the layout, no hacks required.
I like this. I wonder, could it be applied effectively to lpaste.org Haskell pastes?
What gelisam said is right, but I would just point out, the key in the paper that makes it work is that the codata (which here, would be the stuff produced by unfold) is kept *separate* from the data (which is consumed by fold), whereas here you have mixed them - so unfold can produce an infinite codata list, which you can still attempt to consume as finite data. Without some separate form of codata, you can't really have unfold without causing non-termination.
Accelerate is an example of a language that is compiled and embedded in Haskell.
Or just use `text-decoration: underline` and be done with it.
&gt; Is a lisp really lisp without macros? I'm not so sure... I think that symbolic quotation is the true essence of Lisp. The ability to say `(eval '(my (dsl here)))` is astoundingly powerful from a theoretical and practical perspective. Macros are but a practical convenience in latter Lisps.
ur `now : AnyMoment f a -&gt; forall t. Moment t (f t a)` signature is missing a colon
The reason not to use that is here: &gt; That's because the authors want the same line color at any position. For this, the usual `text-decoration: underline` doesn't work. It's non-obvious until you consider what color the line will take for `text-decoration: underline`: the color of the text it's underlining. The author wants the same color for every line shown. Sadly, the CSS3 property, `text-decoration-color`, is [only supported on Firefox](http://mdn.beonex.com/en/CSS/text-decoration-color.html), otherwise that would be the perfect solution.
IMO the underline color is a minor aesthetic issue, and if the page works better without fancy colors that is to be preferred. As an aside, I found the color scheme completely unreadable anyway, because the contrast is way too low. So I'd probably also disagree with the author on how other aspects of the page should look.
&gt; IMO the underline color is a minor aesthetic issue, and if the page works better without fancy colors that is to be preferred. My solution is pretty trivial and one I use all the time because it's simple and reliable, I'd just do that rather than discard the issue. &gt; As an aside, I found the color scheme completely unreadable anyway, because the contrast is way too low. So I'd probably also disagree with the author on how other aspects of the page should look. Yeah, I don't like this color theme at all, but that's color themes for you (I only like [zenburn and sunburn](https://github.com/chrisdone/zenburn)). That's configurable and I don't care much about it. The tech is nice.
Fixed, thanks.
[Idris](http://www.idris-lang.org) is pretty practical.
&gt; Previously, we used `filterE`, while this time... we must still use `filterE`, because strangely enough, there is no `filterB`. I haven't used reactive-banana, but this doesn't seem strange to me. If `Event` models discrete events in time, you can filter events by just removing a discrete occurrence from the stream, so that the stream has one less event. But `Behavior` models a continuous stream; that is, at any point in time a `Behavior` has a value. If `filterB` existed, values matching the predicate would have to be replaced with a value of the same type (because a `Behavior` must always be producing values), and there is no way to choose a new value while preserving `filter` semantics. This behavior can be accomplished by mapping though.
You could have a `chooseB :: (a -&gt; Bool) -&gt; Behavior t a -&gt; Behavior t a -&gt; Behavior t a`, which falls back to using the second behavior where the predicate tests false, otherwise it uses the first behavior. This probably has a name already in the library.
Nah, but `Behavior` is an `Applicative` chooseB p beh1 beh2 = liftA2 chs where chs a a' | p a = a | otherwise = a'
This should sit well along side Snaplets, you would just do something like: handler &lt;- liftIO (SocketIO.initialize EngineIOSnap.snapAPI socketIOApp) Snap.addRoutes [("/socket.io", handler)] In your main `Initializer`.
Thanks, I'll give it a try!
This may or may not be what you are looking for, but I thought it couldn't hurt to mention Dan Piponi's post on [Data and Codata](http://blog.sigfpe.com/2007/07/data-and-codata.html).
It's linked from my post ;-)
I'm sure with some JS magic alternate css could be loaded on demand at the client side.
Where would imported functions link to? Or just for the sake of linking internally?
Thank you. I'm absolutely not against Cabal integration, as long as it lives isolated behind common API/types, so that people can choose if they go that way or not.
This paper describes an EDSL compiler, and is quite good at it: http://www.cse.chalmers.se/~emax/documents/axelsson2014efficient.pdf
I definitely agree that I often want laziness in data, not just codata.
Yeah, that's pretty much what I was trying to say.
I'm sure /u/Tekmo could come up with some good category theory with haskell entries. 
Interesting. Can you have laziness in data in a non-total language?
Maybe to Hackage? But the local linking is cool.
Surely. Think a finite list of expensive computations. Laziness means they'll be run only as needed instead of either retaining a thunk explicitly or precomputing a lot of potentially unnecessary work.
Have you read Okasaki's Purely Functional Data Structures? In it he discusses two methods for doing something similar to this.
You said it just fine -- I just didn't properly acknowledge that fact in my reply (because phone).
Laziness *is* mutable state, it's just impossible to observe in a world which only sees values. You could do the exact same thing in a strict language so long as your memoization is unobservable.
Yeah, I know and love that book. What it does is similar, but not quite the same. I understand how to analyze complexity if the data structure can only be manipulated by a predefined set of functions (insert element and rebalance tree, remove element, etc.), but I don't know how to reason about complexity if the user can destructure the data themselves, taking arbitrary paths.
If the language doesn't care enough about totality to force every value from that list to be computed in finite time, why would it care that those same values should be finite? I think you haven't seen such a language because the two go hand in hand: language which care about provably-terminating computations also care about provably-finite values, because they need values to be finite in order to conclude that recursion on those values will terminate. *edit*: pro**v**ably, not pro**b**ably. big difference!
I meant in your call-by-name language where accessing the `k`th element costs O(kn) each time, not just on first use. But trees are still a good counter-example: costs down one path might be more expensive than down another, and now `k` would need to be a path instead of an integer, and now the notation isn't as convenient.
This made my day.
&gt; TIL what an anti-tutorial is. :-) Actually, I don't think an "anti-tutorial" is a word yet! The last post was called "Understanding Pipes", and I thought the title didn't express clearly enough that this wasn't a tutorial nor a question about pipes, but about watching somebody learn something without using a tutorial. If this post ends up being popular (and so far it's a failure compared to the last one), I was planning to do a series in the same style, for which I thought "foo anti-tutorial" would be a more recognizable pattern than "Understanding foo". &gt; Functor pro-tip [...] Looks convenient, thanks!
It's hard to guarantee finiteness when you have partiality effects. You'd need a termination checker and would end up in a total language fragment. You could play this scenario out more in Idris, perhaps. But you could illustrate the idea in Haskell. Something of a type like [((), ())] would have a potentially infinite list containing morally finite values.
Oh good. Now I can finally get my girlfriend interested in programming.
Re: Action 3, I think the following is at least as clear, while not breaking a lot with existing syntax: class (Functor f) =&gt; Applicative f where return, pure :: x -&gt; f x (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b instance (Functor f) where fmap = (&lt;*&gt;) . return It may be even clearer, because it's more obvious what belongs to `Applicative` and what doesn't. It may also simplify the logic of Action 1. --- I'm still really not sure about the solution of the diamond problem. I get that opt-out has some upsides, but I'm not sure Requirement 1 is worth its troubles. As it stands now, Requirement 1 is barely unmotivated. It looks a lot like Design Goal 1 from [the older DefaultSuperclassInstances proposal](https://ghc.haskell.org/trac/ghc/wiki/DefaultSuperclassInstances), but I think its motivation (don't disturb clients) is a little weak. To me, it seems more like a tooling issue than a language issue. The other upside (which isn't mentioned?) is that you'd only have to write instance Monad Foo where return = ... (&gt;&gt;=) = ... and you'd get Functor (and Applicative) **for free** (which is distinct from the backwards-compatibility motivation). But since it would be the only thing with invisible declarations (besides `RecordWildCards` &gt;_&gt;), I don't think opt-out is the way to go. It might be convenient, but that is perhaps something for a separate language extension. --- That said, I propose that the proposal should be split into three separate extensions: - The 'opt-in' style for instance declarations (`MultipleInstances`?). It is already useful by itself, I guess, especially when combined with `ConstraintKinds` and `TypeSynonymInstances`. - The 'opt-out' style for instance declarations from `IntrinsicSuperclasses`. It's actually an extension of the above. - Superclass defaults. Not really useful without one of the above.
Let's not go [there](http://haskellryangosling.tumblr.com/), 'tis a silly place.
Oh, ha! My mistake. I thought you were linking to somebody else's post. I didn't realize you wrote that.
The difference is cosmetic. The old (your candidate) proposal necessitates the textual duplication of `(Functor f) where` and some additional indentation. Given that instance definitions must be allowed to have members (not just immediate members) listed flatly, why shouldn't class declarations have default definitions presented similarly? Sure, the old version looked more like a proforma of the generated instance (which SHE found rather useful, being somewhat textual in nature), but the whole thing still relies on the one-one mapping of immediate members to classes. What provoked this revision of the proposal was the realisation that all we're doing is modifying the notion of "member", then keeping the notion of "default" consistent with the modified notion of "member". Moreover, in the new proposal, the head of a class declaration is enough to determine the intrinsic superclass structure: you don't have to poke about in class bodies to see which classes an instance definition will deliver. Meanwhile, the penny dropped for me that we need the same kind of logic for describing an intrinsic superclass as for saying which immediate instances we want an instance to generate. The syntactic category of "closure formulae" does that job in a tightly co-located way. It's now obvious (upto pre-emption) from an instance head which immediate instances will be generated. Re Requirement 1, how would what you're talking about above make the slightest difference? Requirement 1 is the very thing that says Applicative instances should sometimes make Functor instances for us. It's an attempt to make programming robust to learning. Re the diamond problem: there is no silent solution. The only solution is to have enough language to resolve the ambiguity. I think Requirement 6 is important for program comprehension in the long run, and hence that non-local (and we can argue how local is local) pre-emption should be phased out as soon as we can comfortably accommodate.
1) In a lazy language, the caller of a function often doesn't evaluate the whole result. That means the time and space complexity of a function needs to be specified for all possible usage patterns. If you assume that the caller always evaluates the whole result, then your complexity analysis won't be very good, because many useful functions will have silly complexity, e.g. repeat. 2) In a lazy language, you can't analyze the time and space use of an individual function call without knowing what happens in the rest of the program, because pattern-matching an expensive value affects the cost of pattern-matching it later somewhere else.
This is amazing. Seriously made my day, thanks!
(I appreciate you took the time to respond to my comment, but since I updated it (nasty habit) while you typed yours, I'll check back later in case you'd like to respond to the updated bits.)
Re Requirement 1. Real pain is the best motivation. You can call worrying about real pain "weak" if you like. I'll cheerfully be even more explicit about the upsides than I already am. I agree that MultipleInstances is entirely separable. Moreover, you can indeed explain the membership aspects of IntrinsicSuperclasses in terms of MultipleInstances and TypeSynonymInstances, where `Monad m` becomes a synonym for `(Applicative m, MonadImmediate m)` and `MonadImmediate` captures the stuff that is in `Monad` but not `Applicative`. I considered explaining it that way, but was advised to go for a more direct presentation. But if we don't get the more nuanced management of default definitions, then it's still boilerplate misery. If we're not sorting out this-yields-a-boring-definition-of-that, why bother?
Sure did.
I've evaluated it in the past and it seemed quite buggy, not the ideal environment for a beginner if things can break even if you didn't make a mistake.
Once you consider other effects than potential nontermination, the hand-in-hand perception of laziness and coinduction becomes more tenuous. There's a big difference between doing a pile of interaction resulting in the delivery of finite data, and describing a finite but interactive process for producing data incrementally. I think of laziness as the interaction of waiting to be threatened before I do any work. It bothers me that laziness is a form of interaction not mediated by type in Haskell, and it bothers me that some kinds of interaction are considered so infra dig that a whole other syntax must be used in their presence. It certainly doesn't bother me if we shift the finite/infinite distinction out of alignment with the strict/lazy distinction, as long as we push in the direction that makes finite things lazy rather than the direction that makes infinite things strict.
Not sure that is a compliment :P
Hey girl. I keep forgetting, are you free tonight? Because I think we should be adjoint.
&gt; consistent What does "nsistent" mean? :)
Hey girl, are you `do` notation? Your style is sweet. Hey girl, are you a pure function? I find myself seeing you everywhere. Hey girl, I think you complete me. Or at least our patterns matched. Hey girl, you make me think like a category theorist. All I care about are the connections between us. Hey girl, I'm convinced we're not in an open type function because you're just my type.
I didn't look at much of the code, and I haven't looked at the Py3 spec, so this is very general advice: Use RWST IO instead of IO for most of your evaluations. Read from an immutable environment, that is occasionally augmented (via `local`) over a lexical scope. Keep mutate things in your state -- not sure exactly what that might be, possibly things that I'm thinking of as environment that can be mutated in Py3. Write recoverable warnings or "lint" issues, maybe nothing for a while, but eventually you might want something there. I think the above might help with your constant readIORef/writeIORef. If you want to learn GADTs, they might help with your two different BinOp return types. data BinOp a where HBinOp :: (Value -&gt; Value -&gt; Either String a) :: BinOp a hPy3Eq :: Value -&gt; Value -&gt; Either String Bool hPy3Eq (Bool lb) (Bool rb) = Right $ lb == rb hPy3Eq (Int li) (Int ri) = Right $ li == ri hPy3Eq (String ls) (String rs) = Right $ ls == rs hPy3Eq _ _ = Left "Invalid argument types to '=='." -- HBinOp hPy3Eq :: BinOp Bool You may be able to carry the return value as simply the result of the monad you eval in. (RWST IO, is my suggestion so it would be the "a" in "RWST IO a".) Otherwise, it will be (part of) the state you carry around, since it changes all the time.
&gt; The difference is cosmetic. The old (your candidate) proposal necessitates the textual duplication of `(Functor f) where` and some additional indentation. Theoretically, I'm nitpicking, but practically, it can mean code nobody wants to use, maintain, or write to begin with. It's mostly cosmetic on the user's end, I agree. But on the implementor's end, the old notation is nearly trivial to implement, while the `class`-specific context notation (much like [the GADT-specific strictness annotations](https://github.com/haskell-suite/haskell-src-exts/issues/11)) would be a headache, for library implementors and (potentially) their users. It's also much more obvious to me what the old notation means than the new notation. Of course, I'm not every Haskell user, but this cosmetic difference might just make it more intuitive. Even though the old notation forces users to violate DRY, that doesn't seem too bad to me, seeing how there are always (much?) fewer class declarations than data type or instance declarations. Finally, and my increasingly sleepy brain is really really not sure about is, but wouldn't the Diddly-Tweedle example from Action 2 work with opt-in with the old syntax, and fail with the new? &gt; What provoked this revision of the proposal was the realisation that all we're doing is modifying the notion of "member", then keeping the notion of "default" consistent with the modified notion of "member". I get that though, and it makes a lot of sense for typeclass towers. &gt; Moreover, [...] It's now obvious (upto pre-emption) from an instance head which immediate instances will be generated. That's a pretty good upside. I'll have to sleep on that one. Something tells me that associated types have similar problems. &gt; Re the diamond problem: there is no silent solution. The only solution is to have enough language to resolve the ambiguity. I think Requirement 6 is important for program comprehension in the long run, and hence that non-local (and we can argue how local is local) pre-emption should be phased out as soon as we can comfortably accommodate. Very true. My reservations are not so much with solving it explicitly, but more with which direction the proposal takes: 'silently' creating instances. I attribute this to Requirement 1 (due to the word 'internally'), but if I'm misreading it, then &gt; Re Requirement 1, how would what you're talking about above make the slightest difference? can be answered with "it wouldn't, sorry ._."
&gt; Re Requirement 1. Real pain is the best motivation. You can call worrying about real pain "weak" if you like. &gt; I'll cheerfully be even more explicit about the upsides than I already am. I'd be glad to read them! My reason for calling it a weak motivation was that some of these problems *might* be solvable through proper tooling. (The big obstacle here is that those tools don't exist, while this proposal is a self-contained thing.) Consider the example from the proposal: -- version 0.1 class C x where f :: ... g :: ... -- version 0.2 class S x where f :: ... class S x =&gt; C x where g :: ... My idea was that the right build tool would present 'updated' users with just the library after the refactoring, while the 'legacy' users get -- shim 0.1 -&gt; 0.2 import "shim-0.2" TheModule type C x = (TheModule.S x, TheModule.C x) and somehow `MultipleInstances` is forced for this `C` (pragmas?). (Again, this is probably not the best idea. The proposal Just Fixes this, and it would require pretty strange logic in the build tool or draconian requirements on developers.) &gt; I agree that MultipleInstances is entirely separable. [...] I considered explaining it that way, but was advised to go for a more direct presentation. I respect that, and the proposal is pretty clear as it is because of that. I just wanted to get it out of the way; I still dislike that (e.g.) `EquationalConstraints` and `PatternSignatures` are hidden extensions, and I was a bit afraid this proposal might add `MultipleInstances` and the opt-in variant of superclass defaults to that list. &gt; But if we don't get the more nuanced management of default definitions, then it's still boilerplate misery. If we're not sorting out this-yields-a-boring-definition-of-that, why bother? Opt-out is definitely better at reducing boilerplate, and (as said elsewhere) makes the most sense for towers of typeclasses (especially when trying to be backwards-compatible). I think my overly strong preference for opt-in is a leftover from using Python ("Explicit is better than implicit."). I should really sleep on it before I type anything more on the topic.
I really hope that this extension were only supposed to be used only during the transition period, if ever at all. People should fix their code (sorry) instead of having to deal with this mental overhead. In particular, I like Haskell because it is a relatively nice, uniform language, vs. C++11 with 5 different initializer kinds. I wouldn't want to think about which of the 3 possible ways this instance is coming from. I appreciate the work though, just applying some reality-check.
I'd argue that the `{-# PRE-EMPT #-}` pragma is a sad transitional thing, but that the rest is a small refinement of the superclass-awareness that is necessary already, yielding considerable abbreviation of code if only we use classes consistently with their design. (I admit that higher-order instances, e.g requiring `Ord x` for `Ord [x]` but only `Eq x` for `Eq [x]` remains a nuisance.) I seriously resent writing and reading boilerplate superclass instances. I have done this a lot in the past. I am perfectly capable of implementing my own machinery to do this work for me. It is up to you if you want to do it for yourselves.
Looks good so far! Like bss03 said, using a Reader monad with ``local`` can make a nice monad stack for evaluating expressions that need to access a local scope without having to thread an ``env`` variable around. Here's a little evaluator for the lambda calculus that's just the minimal example of this technique, shouldn't be too much a stretch to adapt it to your AST. http://lpaste.net/107626
Thanks for posting this. It's highly readable for a new Haskeller.
Thanks for the kind words! I chose it for a few reasons: 1. I have some experience implementing toy languages (see my other repos). The basic concepts and theory has been repeated enough to be automatic. 2. It is ambitious enough that I do not believe I can do it and I want to see if Haskell can help me change my mind there. If you look at my first commit on the project, I literally wrote something that parses only print statements and evaluated them. 3. I believe strongly in learning out in the open. Previously I would hole up and learn stuff, but I'd be less accountable and less prone to finish it. I figure if I'm already learning these things, I might as well make them available and let people have the possibility of learning (whether by browsing code/history etc). I've started/stopped Haskell too many times previously; I just need a project I care about that's a little nuts.
The Reader monad and `local` is a perfect fit for an interpreter! Thanks for the sketch, I'll be reading up on this more tomorrow.
I know it sounds whiny, but with respect to engaging the community and building trust and momentum among learners and users, it would help to update the page that claims a release date is due in the distant past.
Hey girl, you must not be an instance of Ord, 'cause no one else compares to you.
Finally a good one! Crude sure, but this is CLEARLY the best pun in this thread OR that blog.
That sounds pretty cool. Funny enough, my thinking on the HTML parser was the same. "I just need a project that's too hard for me," I said. "That's the only way I can learn it."
&gt; We have inductive types, and the proper term for their dual is coinductive types. Many interesting types are neither inductive nor coinductive (example: `T = T -&gt; T`). On the other hand, I'm pretty sure all types are either data or codata. The difference between data/codata is an *operational* distinction. Values of data are formed via constructors, and contexts are formed by pattern matching. Value of codata are formed by pattern matching while contexts are formed by co-constructors. The difference between inductive/coinductive types is about sizing. An inductive type has the smallest set of values that obeys some definition. A co-inductive type has the smallest set of contexts that obeys some definition. That is, these concepts are distinct. &gt; I don't know if I trust much of anything in that blog post, looking at it a bit more. The only point I agree with is that data and codata don't correspond to strict and lazy evaluation -- this is true, and it's a common misconception. The type theories we use are ostensibly confluent, which is a fancy way of saying evaluation order can't affect the outcome. As long as your reduction rules don't let you unfold guarded infinite lists, you could just as well pick redexes at random and eventually reach normal form. If you have complete duality your type theory is probably non-confluent. The polarized systems like Zielberger's or Munch-Maccagnoni's achieve confluence by using CBN for codata and CBV for data. This is reasonable since CBN is the "best" evaluation strategy for codata (in that it gives you all the equations you want) while CBV is the best evaluation strategy for data. Of course you are right though that the polarized restriction is not the only one or necessarily the best way of achieving confluence; CBN data and CBV codata make sense also. 
Our long-distance relationship is doomed if we don't commute.
Yes, that's the whole point of a named instance (see the '[myord]') - it's to allow an alternative to the default instance, in this case to order things backwards. However, we've decided they're a bad idea for various other reasons (confluence, coherence, ugliness, etc...), and nobody uses them anyway, so we'll probably deprecate them in the next version.
&gt; Transactional semantics for filesystems. btrfs comes close. Its copy-on-write design allows for taking "snapshots" of the filesystem, which you can roll back to at will.
In monad-classes, where I implement these ideas, I'll probably make `MonadState s m` just an alias for `(MonadWriter s m, MonadReader s m)`, `put` an alias to `tell` etc.
Man! I thought I had eradicated all EmailErrors by now. Thanks. Should be fixed now.
Your assumption that `ask` always returns the same value already fails to hold in mtl. import Control.Monad.Reader import Control.Monad.Cont a = do r1 &lt;- ask callCC $ \k -&gt; local not $ k () r2 &lt;- ask return (r1, r2) main = print $ runReader (runContT a return) True 
&gt; `fmap (const 1) = (1 &lt;$)` Personally, I find the left hand side more straightforward and easier to read than the right hand side.
Writer? I barely know her! Cracks me up every time.
use `:load` or `:l` for short, specifying the full or relative path (i.e, from where ghci was opened from). You can run shell commands with `:!`, for example `:!pwd` to see the current path, and `:!cd ~/Desktop` to change that.
In which way?
Hey girl, are you `do` notation? Because some people consider you harmful... to my heart. Am I doing it right?
I was going to suggest binary search, then I realised that's exactly what the `Map` solutions do. Carry on.
I really wish this transformer would [stop](http://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-Error.html) [getting](http://hackage.haskell.org/package/either) [reimplemented](http://hackage.haskell.org/package/transformers-0.4.1.0/docs/Control-Monad-Trans-Except.html) and stay in one package for 6 months!
Supposedly, you can use partial evaluation to make a compiler out of an interpreter - that is, eliminate (ideally all of) the overhead of interpretation. See the *Futamura projections*. http://en.wikipedia.org/wiki/Partial_evaluation
Functor?! I only just met her!
Like so: {-# LANGUAGE DataKinds, KindSignatures, ScopedTypeVariables #-} import Data.Proxy import GHC.TypeLits data Clock (m :: Nat) = Clock Integer -- m is the modulus instance KnownNat m =&gt; Num (Clock m) where fromInteger n = Clock (n `mod` natVal (Proxy :: Proxy m))
Can't test right now, but `data Num a =&gt; Vector a = Vector a a a` should do it
You need to enable existential quantification, and then you can do: data Vector a = forall a. Num a =&gt; Vector a a a Then in ghci you will have: Prelude&gt; Vector "a" "b" "c" &lt;interactive&gt;:3:1: No instance for (Num [Char]) arising from a use of `Vector' Possible fix: add an instance declaration for (Num [Char]) In the expression: Vector "a" "b" "c" In an equation for `it': it = Vector "a" "b" "c" Edit: [more info on existentially quantified types](http://en.wikibooks.org/wiki/Haskell/Existentially_quantified_types)
Good find! Thanks!
Putting constraints on data declarations is possible but highly not recommended. It is regarded as a mistake in the language and will likely be removed. Instead, just add your constraints to the functions that work on your data, which you would have to do anyway if you put your constraint on your data. Edit: In fact, the feature I'm thinking of is already deprecated. There is also suggestions around using existential quantification elsewhere in this thread - this can sometimes be a pain and you should just leave your data bare and add the Num constraints to your functions, especially as a beginner. There is an advanced feature called GADTs which can also do the same, I believe. I'm sure someone who really knows their stuff will come along and contribute. 
Mathematically speaking, you want `a` to be from a field, so `Num` alone won't actually be enough, as it doesn't know division, so you actually want `Fractional`, but that's beside the point. data Num a =&gt; Vector a = Vector a a a This works, when you enable datatype contexts, but they are deprecated because they were deemed a bad idea and will be removed. A quick and simple solution would be to not export your data constructor in order to hide it from the user and provide a smart constructor. What we mean when we say smart constructor is a function that enforces certain constraints and creates a new value for your type. So that would look like this: data Vector a = Vector a a a vecNew :: Num a =&gt; a -&gt; a -&gt; a -&gt; Vector a vecNew x y z = Vector x y z As you can see, we now enforce that `a` must be an instance of `Num` and since we provide *no other way* to create a value of type `Vector`, we get what we want. But, while smart constructors can be useful if you want to enforce additional constraints that are very hard to encode in types (or even impossible), this use case is a little ugly. The cleanest solution that I can think of makes use of [GADTs](http://www.haskell.org/ghc/docs/latest/html/users_guide/data-type-extensions.html#gadt-style), which are a language extension. Basically, this lets you define the type of the data constructor. It would look something like this: data Vector a where Vector :: Num a =&gt; a -&gt; a -&gt; a -&gt; Vector a Again we enforce the constraint but we don't need to have an additional function for it.
Exactly this. You may also want to read http://www.codepills.net//blog/2012/12/17/haskell-data-constraints
Thanks for your great responses! I didn't expect this many in only 30 minutes:) I get it now that even if I somehow restrict the types to be of Num (or something else) I need to check for this in all functions anyway, which I at first wanted to avoid although know that I have thought about it more it makes sense. Though it still feels weird to be able to make a Vector Bool Bool Bool so a function that makes a Vector seems like an simple solution.
While you _can_ do this, the real thing to note is that you really really shouldn't. Why? It costs you access to er.. well pretty much every standard class. Before you bolt in `Num`, `Vector` could sensibly (and usefully) be an instance of `Functor`, `Traversable`, `Applicative`, `Monad`.. Now, what happens when you need more than Num? After all we said vector, not module, right now we just have an element of a module as we only know `Num`, right? What happens when you realize you need it to be fractional so you can divide when working with `Double`s. You could modify the `Vector` type, but then code that you had that was working with `Vector Int` is broken, so, oh, you have to build a new data type. Great. Code duplication. So the reason _not_ to do this is that it inherently leads to more code than moving the constraints to the use sites. The benefit of doing so is that you get simpler "dumb data types" that can be shared across more contexts and you only wind up requiring of yourself the instances you need for the operations you perform. So you don't spend any time building dictionaries you aren't using.
Thanks for the great answer! But I don't get why GADTs solution is cleaner than the deprecated data type context one?
Using GADTs isn't a great answer, either. It's more restrictive than it needs to be, the types convey less information than they should, and it doesn't even get rid of all the instances in type signatures. For an example of what I mean, let's look at how `Data.Set.Set` works, and how that'd change if it was a GADT that packaged the instance up inside it. Here are the types of a few functions from `Data.Set`: null :: Set a -&gt; Bool size :: Set a -&gt; Int member :: Ord a =&gt; a -&gt; Set a -&gt; Bool empty :: Set a singleton :: a -&gt; Set a insert :: Ord a =&gt; a -&gt; Set a -&gt; Set a In the current situation, the presence or absence of the `Ord` constraint is precise. Functions that need it have it. Functions that don't need it don't have it. If `Set` was a GADT that packaged the constraint inside it, those signatures would look like this: null :: Set a -&gt; Bool size :: Set a -&gt; Int member :: a -&gt; Set a -&gt; Bool empty :: Ord a =&gt; Set a singleton :: Ord a =&gt; a -&gt; Set a insert :: a -&gt; Set a -&gt; Set a I'm firmly convinced that is a huge step *backwards* in usability. You're now requiring an `Ord` instance on functions like `empty` and `singleton`, which don't actually need that constraint to work. Furthermore, the constraint has vanished from `member` and `insert`, which *do* need it to work. It's obscuring the logic, and making it harder to understand the code. Even worse, it's removing some use cases that were previously supported. Let `data` just be plain old data. Don't package up behavior with it. Put the behavior where it belongs - in functions.
Boy, look at the length of that Coq proof-term. Ok, I'll see myself out now... 
Awesome. I was hoping someone would come in with an explanation as to why this is a bad idea -- I had a feeling it wasn't the best solution to the problem. In the past, I've always judiciously stuck restrictions on functions operating on data types like this. Thanks for the explanation, Edward!
Have you played with Parsec for that? It is incredibly powerful. The fact that you can parse a real language grammar with it and yet it's not too bulky for other parsing tasks is a testament to the design.
&gt; But what I truly want is "data Vector = Vector Num Num Num" right? Maybe, though [probably not](http://www.reddit.com/r/haskell/comments/2ayk6d/num_in_value_constructor/cj01aae). &gt; Why is this not possible (or if it is, how to do it?). Because `Num` is not a type. It is a type class. `Num a` still isn't a type either, it is a `Constraint`. There's a [old-ish way](http://www.reddit.com/r/haskell/comments/2ayk6d/num_in_value_constructor/cj00zm2) to apply a constraint to a data type, but it probably doesn't do what you want. The constraint will be propagated to all functions that use the data type, and you'll still have to mention it if you provide the type annotation, even in case where you don't actually use any of the methods in the type class! GHC has [existential qualification](http://www.reddit.com/r/haskell/comments/2ayk6d/num_in_value_constructor/cj00kbn). While this "captures" the constraint at construction time and makes it available via pattern-matching rather than having to appear in the type of functions using your data type, it also comes with a set of restrictions.
It may feel "weird" to make Vector Bool, but it can actually be quite useful. We often work with boolean matrices. Swap out (*) for (&amp;&amp;) and (+) for (||) and you can do lots of things you're used to doing with matrices on matrices full of booleans. Build a directed graph as a matrix of booleans, and you can compute the transitive closure of the graph, etc. If you shove a `Num` constraint in then this usecase is now, needlessly, no longer supported for your data type. You've gained no power and lost expressiveness for what? When you get to working with matrices if you -don't- put in the constraint you can usefully make matrices be 'vectors of vectors'. If you do put in the constraint then to have `Vector (Vector a)` your vectors have to support all of `Num`. That means you have to offer (*) for vectors. This may make you more uncomfortable than having `Vector Bool`, as extending the vector to support pointwise multiplication by default may obscure lots of type errors when you screw up scalars and vectors!
Good point. I guess the less restrictive it is, the more flexible it is and then it will be easier to add new behaviour later on with whole new scenarios that you didn't think of when creating the data type at first.
Mainly because of the problems outlined [here](https://ghc.haskell.org/trac/haskell-prime/wiki/NoDatatypeContexts).
&gt;It is regarded as a mistake in the language and will likely be removed. Why? This isn't entirely clear to me.
Although I didn't quite follow you at the end there. I have learnt that flexibility is the way to go here as you go do a lot of unpredictable (by me at least) stuff:) thanks a lot!
Oh! I didn't know Num was a type class. That explains a lot:) I should continue with the book!
[See here](https://ghc.haskell.org/trac/haskell-prime/wiki/NoDatatypeContexts) 
You can think of a matrix as just a column vector full of row vectors. Vector (Vector Double) is a 3x3 matrix. Now, if you put in a Num constraint on the arguments to Vector this means that Vector Double _also_ has to be an instance of Num. instance Num a =&gt; Num (Vector a) where Vector a b c + Vector d e f = Vector (a + d) (b + e) (c + f) Vector a b c * Vector d e f = Vector (a * d) (b * e) (c * f) abs (Vector a b c) = Vector (abs a) (abs b) (abs c) fromInteger i = Vector n n n where n = fromInteger i ... the operation that should give you pause is this requires you to define multiplication of vectors pointwise like you do with addition. Complex numbers form a vector space over the reals, but they do so with a custom multiplication, because they not only form a vector space but an "algebra" over the reals. We've had to adjoin an extra operation (pointwise multiplication of vectors) to the definition of a vector space. It occurs nowhere in the standard definition! So ultimately the `Num` constraint, besides already being shown above to be the wrong constraint for a vector-space, starts getting in the way of defining everything correctly to boot.
&gt; the variable a in data Vector a is no longer tied to the a in Vector a a a It took a second to comprehend that, but trying it in ghci made it clearer Prelude&gt; data Vector a = forall a. Num a =&gt; Vector a a a Prelude&gt; :t Vector (1 :: Int) 1 1 Vector (1 :: Int) 1 1 :: Vector a Prelude&gt; data Vector a = Num a =&gt; Vector a a a Prelude&gt; :t Vector (1 :: Int) 1 1 Vector (1 :: Int) 1 1 :: Vector Int 
Thanks for the enlightenment!
A small syntax fix: data BinOp a where HBinOp :: (Value -&gt; Value -&gt; Either String a) -&gt; BinOp a This type can be non-GADT, by the way: data BinOp a = HBinOp (Value -&gt; Value -&gt; Either String a) 
The type class constraints don't go on the constructor, the go on the operator. So if you wrote: add (Vector x y z) (Vector a b c) = Vector (x + a) (y + b) (z + c) that function would have type: add :: Num a =&gt; Vector a -&gt; Vector a -&gt; Vector a So if you wanted to add two vectors, the underlying real type would need to be a number. Not having the Num constraint on the constructor means you could construct vectors with things that aren't numbers (ex: Vector "Hello" "There" "World"). But this is actually a good thing! For example, you need this if you wanted to make your Vector an applicative: instance Applicative Vector where (Vector f g h) &lt;*&gt; (Vector x y z) = Vector (f x) (g y) (h z) (you also need to define pure and fmap above, these are left as an exercise to the reader). This allows you to have seperate transforms for each coordinate- but it requires to be able to construct a vector of transforms.
I was going to point out that the monadic version of &lt;* *is* &lt;*, but you seem to already know that . Are you running into types with Monad instances but no provided Applicative instances?
Oh right, thanks.
Good point.
Note that with GHC 7.10, Applicative will be a superclass of Monad! 
Tutorial comments: - Section 5.3 on parameterised blocks It would help to be clear that the parameters are added to be functions first, then the parameters defined in the function call are applied. - Section 7.2 confused me as a Haskeller. How is ``even`` or ``odd`` being used in the guard statement? The pattern match on ``(j + j)`` or ``(S (j + j))`` looks to be sufficient. ``filter`` needs a with statement because it returns a pair rather than a simple list right? The with block moves the unpacking of the pair up out of the function definition, right? Page 38 ends saying it is using a function from the prelude. Is ``plusSuccRightSucc`` supposed to be already defined? Or is it meaning the use of ``sym`` a paragraph or so later?
There's really only one Coq joke, though, and it's too obvious to be funny.
And the tree lookup.
This static unrolling would produce a large binary, correct?
I created an issue based on your first comment: https://github.com/idris-lang/idris-tutorial/issues/37 Thanks!
Thanks. I think I originally did it as a GADT because I was going to break it into two parts like: data BinOp a where Eq :: BinOp Bool {- More BinOp Bool values -} Plus :: BinOp Int {- More BinOp Int values -} doOp :: BinOp a -&gt; Value -&gt; Value -&gt; Either String a doOp Eq (Bool lb) (Bool rb) = Right $ lb == rb {- etc. -} Broken out like this Show, Read, and Eq instances can be derived. (Maybe Ord, too?). That can be nice for debugging. Ultimately I posted what I did because it is moderately easier to create new BinOp values. Final vs. initial encodings and the expression problem raise their heads again.
How about interpolation search then ? 
certainly
Yes, and it needs to have knowledge of the LUT at compile-time.
Very cool solution. This is the kind of thing you see happening in Lisps fairly often – it's not something you expect from a language like Haskell, but it goes to show that TH is not half bad.
It's not entirely obvious, so the question is understandable. The best answer I have is "Edward Kmett once thought data type constraints was a good idea but then after much pain and frustration realised they aren't." The next best answer consists of two parts: 1. It's *very* hard to predict all the ways your users will want to use your types. 2. If you want to make your type a `functor`, you need to support the operation fmap :: (a -&gt; b) -&gt; f a -&gt; f b Note how general that function is. It is *not* enough to implement fmap :: (Num a, Num b) =&gt; (a -&gt; b) -&gt; f a -&gt; f b because that won't compile as an instance of `Functor`. That pattern reappears with all common typeclasses. They only work with unconstrained datatypes.
Indeed, this is exactly what forward-chaining logic systems typically do
Thanks for the follow up. Let me explain what I understand and where I am losing the thread. ``Parity`` defines a type with two constructors, ``even`` and ``odd``. Both take a ``Nat`` as their only parameter. By means of dependent typing, the code says that an ``even n`` is only true for some value of ``n`` where ``Parity (n + n)`` is true. I am following along so far. Then we get to ``parity`` and things get murky. Zero and One are defined specially. Ok. Then we get to ``S (S k)``. First we apply ``parity`` to k. This is where the syntax confuses me a bit. To keep it simple, let's say the value of k is Zero so the initial call to ``parity`` was passed ``S (S Z)``. The ``with`` statement evaluates ``parity Z`` and returns ``even Z``. That means this line: parity (S (S (j + j))) | even = even {n=S j} is triggered. Where did the parameter to ``even`` go on the left side of the equal sign? Why is ``even (S j)`` called?
Note that similarly, `&lt;**&gt;` is not `flip (&lt;*&gt;)`.
OP should note that one of the implications of this is that if your issue is that you want to be polymorphic in the monad and you don't want to further constrain it to an applicative, all new code being written now should have both those constraints anyway, so there is no reason to be so lax. Any old code not fixed yet will break soon enough!
An interesting extension is to use a similar technique to generate code for evaluating arbitrary decision trees (e.g. for machine learning applications). I wrote an article benchmarking these techniques (some Haskell, but mostly C++) at &lt;http://tullo.ch/articles/decision-tree-evaluation/&gt;, and implemented it for the Python machine learning library scikit-learn at &lt;https://github.com/ajtulloch/sklearn-compiledtrees/&gt;. The generate code can be up to 8x faster than optimised C traversing the tree structure.
In the motivating application, there are two tables each with 16 entries. (Although I could shorten one table down to 11 in practice.) The points in the table are evenly spaced, but the interpolation needs to be done in lots of different points, not necessarily evenly spaced. The tables represent functions whose domains are distances, and we need to be able to compute the function for any arbitrary distance. To be a bit more concrete: there are about 50-100 points in a 3D volume which are emitting radiation. There are about 2000-4000 sample points where you want to know the radiation level. To compute this, you need to calculate the radiation contributed by each emitter to each sample point. The distance between the emitter and the sample point is the input to the functions being interpolated. So although the tables being interpolated are small, it is done very often. According to the profiler, the interpolation is done 18,432,920 times in the 2-3 second run shown in the article. I know that the functions are calculated for the same values repeatedly. One other approach that I tried was to exploit this repetition by caching the results. (In the context where these are called, this would also "memoise away" a few other small calculations.) However, I couldn't get this to go any faster than non-caching, and maintaining the cache was a pain.
No, I was using Parsec and didn't think to import Applicative (because, if I'm working in Monad land, it makes sense that I should use the Monad tools, right?). I did that, though, and just used &gt;* and &lt;* throughout.
I think the issue is that operators are basically completely opaque in meaning. Code using well designed libraries should be readable even by people not familiar with those libraries, and operators almost never adhere to that rule. This is compounded by the huge number of people who just include whole modules, making it extremely annoying to figure out where the operator was defined.
"*n* (implicit: people) will stand", I suppose.
Well done. That was much easier to read than my series that attempted to do the same thing: https://llayland.wordpress.com/2012/03/25/trying-to-figure-out-frp/ One suggestion - Specify the version of the libraries you are using. There are pretty significant changes between some of the releases
My suggestions would be to do binary search in an unboxed vector (or pair of vectors, though I think an even/odd single vector would work best). I'd be very interested see the performance of this.
Sure, my point was only that it's not made clear that you intended to make a backwards instance early enough (perhaps renaming it [reverseNat] or something).
IMO, it makes sense to see things in such a way that Applicative tools *are* Monad tools (which the AMP is meant to make clearer/easier, I think). In general, I end up using the most general version available for a given Functor/Applicative/Monad function (e.g. preferring `fmap` to `liftA` or `liftM`, even in Monad-land).
Thanks for this suggestion; it's a good idea. I should have said that the points in the table are "roughly" evenly spaced, but it should be easy to introduce a few more points to make them completely even. When I get the chance I will try this out.
I'm pretty sure the cryptic nature of applicative is what prompted these guidelines... 
And are aliased from a properly named function.
That's quite arbitrary. Do you dislike += or .~ or other non associative operators from lens, for example?
I made a video to answer this, because I think it's easier to see how things work when they're developed interactively on screen: https://www.youtube.com/watch?v=UYzcZj_3MIw Does this shed some light on it?
This will sound heretical, but I think Java has a really sensible approach to that problem. Not just forbidding operator overloading, but also forbidding free-standing functions helps readability at large scale. If you see something like foo(), it's a method in the current class, and if you see foo.bar() or Foo.bar(), it's in another class and you know which one, so you can guess what it does. Inheritance and static imports complicate the picture, but the basic idea is good. I can jump into an unfamiliar codebase, start reading it at reasonable speed, and know immediately where the cross-references go. Not sure if Haskell can be made as readable as Java with incremental changes alone. It's the whole design of the language. Haskell needs to be terse, because otherwise highly abstract code would be unbearable to write. For example, the standard Java practice of giving full English names to every argument of a function would probably look pretty weird in Haskell. There would be many uninformative names like "callback", because in highly abstract code it's hard to come up with better names.
I didn't say it was easy :p
Coming from python, I feel the same way. However, I think that haskell is not so bad. Using only qualified imports everywhere except for a few selected modules (Control.Monad, Control.Applicative, Conduit, ...) works well for me.
If you use the Reader monad (transformer), you can get rid of that explicit `env` parameter you pass along everywhere. Here's an example from a recent post: data Value = ... data Term = Let String Term Term | Variable String | ... type Environment = Map String Value eval :: Term -&gt; Reader Environment Value eval (Variable x) -&gt; asks (! x) eval (Let x e1 e2) -&gt; do v &lt;- eval e1 withReader (insert x v) (eval e2) ... In a simple functional language, none of the other cases of eval does anything with the envitonment except pass it on. And with the Reader monad, they don't even have to mention it. You use the environment in a few more places, but the idea is the same. **Edit:** oops, hmltyp already posted essentially this suggestion.
&gt; Using only qualified imports everywhere except for a few selected modules ... works well for me. Me too, so much so that I don't understand why anyone uses unrestricted, unqualified imports.
I have an very slightly more general view. I'm also am okay with infix operations if they are an action of a type which is an associative monoid (i.e., you have an operator `(⊗) : a → b → b`, where `b` is a monoid, so that `a ⊗ b₁ ⊗ b₂ = a ⊗ (mappend b₁ b₂)`. 
90% of links there are links to hackage categories (which are *not curated*, if I understand correctly what that word means).
&gt; Hey girl. You are like the the unit value. There is only one of your type. What about `undefined`?
That's a funny proposition, given that one of the few exceptions Elm makes concerning operators is that it does use operators for applicative. So it would be odd if the motivation for largely avoiding operators were that the aplicative operators are cryptic. (Wouldn't *they* have been avoided then?)
The way I understand it, ErrorT is deprecated in favour of ExceptT, and EitherT is ExceptT for Tekmo who doesn't like the latter. In any case, ExceptT is something you get with the Haskell Platform, which ought to count for something.
I think these guidelines are reasonable and I especially like the point about avoiding abstraction for the *sake* of abstraction even though I'm not sold on the suggestion to put the infix operator in a special module `Whatever.Infix`. But to elaborate on the point about searching for operators online, sometimes you don't only want to search for operators but operators *within a context*. Let's say that I have a function: \i a → k i (f i a) \i → k i . f i and realize it can be rewritten: liftA2 (.) k f (.) &lt;$&gt; k &lt;*&gt; f `(.)` being an operator makes it more difficult (but not impossible) to check if this is a common pattern or finding other use cases for it than if it were called `comp` as an example — I'm certainly not advocating `comp = (.)`. A short rant: Operators are essentially a trade-off between making your library easier on experienced users or making it easier on newcomers: *you* (as a library author) are perfectly fine with adding operators since you understand it and want to make it easier to write and sometimes read (`mappend a (mappend b c) = mappend (mappend a b) c` versus `a &lt;&gt; (b &lt;&gt; c) = (a &lt;&gt; b) &lt;&gt; c`) but when *you* (as a library user) are using 5 different libraries that are all the centre of the universe defining their own EDSLs with their own 2–4 symbol operators it gets more complicated :) not to mention the mental overhead of fixity. I once heard the comparison “you wouldn't export a function called `hj` from a library”, while it has flaws I do think thrice before defining operators.
It's true that Monad, Applicative, Functor etc. only work with unconstrained datatypes. However, this is *not a problem with datatype constraints*, but instead a problem with the inability to constrain the "contained" type of those type classes. The [rmonad](http://hackage.haskell.org/package/rmonad) package tries to fix this, but there may be simpler solutions now involving constraint kinds.
Sorry, if you find it useless, but anyway thaks for your feedback.
That would mean [she didn't exist at all!](http://en.wikipedia.org/wiki/Bottom_type)
Even `Void` is inhabited in Haskell!
Kind of off topic so I apologise. I just thought it was quite interesting that the author of the library had managed to get so much functionality into a language that is somewhat hostile to this kind of programming.
Interesting, since [you did use](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html) them in your own imperative programming tutorial? I don't see why a very particular algebraic form and its property (associativity) are so special.
This was an opinion I formed after writing that tutorial, but even if I were to rewrite that tutorial today I'd probably keep them just to appeal to the people who program in languages with C-like syntax. However, I'd like there to be named alternatives available for all of them such as: * `(+=)`: `increments` * `(.~)`: `modifies` or `transforms` The purpose of algebraic properties are so that you can reason about the code without needing names. A simple example would be something like this: foo * bar + foo * baz If I see that, I immediately expect that I can rewrite it to: foo * (bar + baz) ... without having any how `Num` is implemented for that type, because I expect all `Num` instances to obey the semiring laws. Names are a useful tool for reasoning about code, but I believe that algebraic laws are more powerful and effective in the large.
I agree about the usefulness of reasoning. I don't see why a very particular property (associativity) needs to hold for operators. There are other possible properties that could help reasoning that could be chosen.
Neat! The syntax overloading is pretty and implementing composition as fmap is nice. The standard disambiguation of return-type-polymorphic types appears. I have no idea if that's tricky to use in practice. All in all this looks like a nice library.
I think one of the major libraries that prompted it is `lens`, not `Applicative`. https://hackage.haskell.org/package/lens-3.8.5/docs/Control-Lens-Operators.html
Loops created using higher order functions are not structures. They are syntax just like plain old goto. Loop as a data structure has information about where it begins, who invokes start, what is in the loop and what are its constraints. If you have that kind of information you will know what is the time complexity of an algorithm. How? Just combine all Loop Objects in your functions code and you will immediately see if the function will terminate or not (as a function of its parameters). This would of course require describing time/spatial compexities for each used function, but as I said, there is no single programming language with loops as a first class data. Simply, loops as a first class objects were not invented yet. All languages fake them using more or less complicated syntax, including Lisp. E.g. pattern matching is not a first class object in Haskell - it is pure syntax construct like the old good 'if'. Another thing that has not been invented yet is spatial/time complexity description for each function. I'm not asking for much, manual hand written info would be enough (a little symbol here or there additionally with type description). 
In code, associativity means parenthesis don't change the meaning of the program. This can definitely be a sore spot when debugging, and often times, the error messages GHC generates don't make it super easy to tell where the problem is coming from. Associativity guarantees that you don't have to finagle your program into correctness with parenthesis and makes it a lot easier to use an operator. I agree that other properties are nice as well, but in the context of programming, perhaps associativity is the most important.
In this case, associativity is the right criterion to use, for psycholinguistic reasons. When reading sentences, people have to parse them as they go along, and it's empirical fact that for people, certain productions are much more difficult to parse than others. If you have a grammatical production of the form `A ::= x A y`, uses of this rule will generally be very difficult for humans to cope with. Linguists call this "center-embedding", and it's rare enough that it was actually controversial for a while whether it really occurs in natural language at all! I think the current consensus is that such productions do occur in human grammars, but people lose the ability to understand them if the `A` nests more than 2 or 3 deep. (Basically, your brain's stack overflows!) This is why infix operators are helpful -- they let you remove center-embedding from a grammar. For example, suppose addition was a non-infix binary function `+`. Then you would have to write repeated addition as: (+ (+ (+ (+ a b) c) d) e) This is a center-embedded term, and as a result it's hard to read. With infix, you can write: a + b + c + d + e This expression can be generated by a non-center-embedded grammar. In fact there are two of them: you could use either A ::= x | A + x or A ::= x | x + A However, these two grammars naturally correspond to two different ways to parenthesize the term --- `((((a + b) + c) + d) + e)` for the first, and `(a + (b + (c + (d + e))))` for the second. If `+` is associative, then it doesn't matter which way you choose to parenthesize. As a result, using an infix operator for an associative binary operator is a pure win. If it's not associative, then you still have an easier grammar, but run the risk of reader confusion (if they parse the sequence with the wrong rule).
There's snap's [Haskell style guide](http://snapframework.com/docs/style-guide). I don't think there is anything "official" edit: http://www.haskell.org/haskellwiki/Programming_guidelines
Thanks for the suggestion, I have now added the versions.
It's somewhat strange they seem to have pure/return/unit on Functor instead of Applicative though.
&gt; The method `getValue()` will allow you to "extract" the value of monadic computations if/when necessary. What? What does `getValue()` return when applied to a a monadic value in the State, Reader, or Cont monad? Or even just when applied to `Nothing` or `[]`? &gt; [On the Writer monad] `getValue()`: Returns the result and log as a two-tuple. So, really `getValue()` just removes the object/newtype "wrapper"? Still not quite sure what it would return for `Nothing` or `[]` (maybe it just returns `this`?) but I guess or State/Reader/Cont it returns an appropriately typed (curried?) function. I feel like this feature might actually confuse people that are learning monads. You don't always have a way to "escape" a arbitrary monad; it's not part of monadic nature. (Now, co-monads...) This lack of general escape is (part of) what makes `ST` and other resource-fencing monads work. It's also can be essential when using a custom monad in the interface of your library/DSL.
And that they didn't include `sequence`, when they include `unit`.
Yeah. The @curry annotation is some nice pythonic magic. I wonder how it deals with named/keyword parameters. &gt;:)
Just to be clear I wasn't commenting on the Applicative style (writing `(&lt;*&gt;)` or `(&lt;$&gt;)` versus the lift functions) but rather that it's easier to search for: liftA2 comp than liftA2 (.) That the example I used happened to use `liftA2` is incidental and I could have made that more clear.
I didn't even notice that. It reminds me of a Clojure implementation of much of the same which allowed for "multi-arity fmap" on Functor... I told the developer that it meant that Functor was just Applicative and he wasn't sure why I cared.
Yeah, basing the whole thing off Container will bite eventually.
Anybody know why vim with syntastic and ghc-mod would show an error but if I actually compile everything goes fine? Main.hs|28 col 19 error| Couldn't match type `Event t0 Float' with `Event t Float' Expected type: Event t Float -&gt; Event t InputEvent -&gt; Moment t (Behavior t Picture) Actual type: Event t0 Float -&gt; Event t0 InputEvent -&gt; Moment t0 (Behavior t0 Picture) In the fourth argument of `playBanana', namely `reactiveMain'In the expression: playBanana (InWindow "Nice Window" (200, 200) (800, 200)) white 30 reactiveMain In an equation for `main': main = playBanana (InWindow "Nice Window" (200, 200) (800, 200)) white 30 reactiveMain
Re rant: what the author finds readable is completely irrelevant, but a relevant trade-off is between newcomers and experienced users of a library. Readability should be about what an average person skilled in the art finds readable and thus the author and a complete newcomer are both excluded.
Where do you think Elm sits here? The `main` function has type: `Signal Element`, where `Element` is a fairly standard (static) layout combinator library and `Signal a` is essentially a `(Event (), Behavior a)` pair. You might be interested in [this thread](https://groups.google.com/forum/#!topic/elm-discuss/dyvDKdCXZ6A), where I asked about one of your specific use cases that you raised in a previous thread.
&gt; Mathematically speaking, you want a to be from a field, so Num alone won't actually be enough, as it doesn't know division, so you actually want Fractional, Num and Fractional are more restrictive than a mathematical field. Secondly, large swathes of linear algebra still work if you don't have division. Rather than fields and vectors, you speak of rings and modules. Technically, you need to assume you have a basis (trivially true for coordinates), and dimension / matrix inversion theory become more complicated.
&gt; Swap out (*) for (&amp;&amp;) and (+) for (||) and you can do lots of things you're used to doing with matrices on matrices full of booleans. That's also an argument to fix Num. Right now, Num insists you have absolute values, which excludes finite fields. But your example is almost a finite field. Use xor for (+) rather than (||), and you've identified Bool as the two-element field.
Lisp doesn't have infix operators, so it has that solution instead, but being able to tansform a dyadic function into a polyadic function like that still requires associativity, so it's pretty much the same things. Types aren't ever used to remove parentheses: typechecking is only done after parsing, so when the compiler thinks about types it already sorted out everything about parentheses.
That's why I think infix operators shouldn't have general precedence, but only precedence wrt specific other operators. So * and + could have a defined precedence, but * and &lt;*&gt; should not. 
I'm well acquainted with the Boolean ring vs the Boolean semiring, but by using `xor`, you've also made it useless for the stated introductory purpose of computing, say the transitive closure of a graph. =) Yes, Num sucks, but almost all proposals to fix it introduce an incredible amount of pain, and every sane proposal really requires MPTCs or type families, which takes it ouside of the domain of Haskell' while simultaneously ensuring you break every user of the Haskell ecosystem. These keep these proposals as non-starters at this time. My point was more about the fact that data type constraints put you into a mode of thinking where you give up perfectly well known common abstractions to make up your own vocabulary that doesn't need to exist for each and every separate problem. This causes you not to see commonalities. data RealFloat a =&gt; Complex a = a :+ a has meant that Complex can't be an instance of any standard classes for manipulating its arguments. With `Functor` you could use `fmap realToFrac` to convert between `Complex Float` and `Complex Double`, without that instance? You have to define your own one-off combinator. We finally stripped the datatype context from `Complex`, but never went back and put in the instances.
I agree with your comment and my rant was a bit simplistic: there are things that are operators because of tradition and reasons discussed in this thread like arithmetic operators and others because of how fundamental they are, like the monadic and applicative operators That being said I prefer do-notation to the monadic operators and some sugar for applicatives like `⟦ k · f ⟧` and `⟦ getLine ++ getLine ⟧` (or the bang notation Idris supports) would increase Haskell's readability I think. (Applicative notation being noisy and people preferring do-notation are among the reasons Simon Marlow proposes the [Applicative do-notation](https://ghc.haskell.org/trac/ghc/wiki/ApplicativeDo) in chapter 7 (*Automatic Applicative*) in his [*There is no Fork*](http://community.haskell.org/~simonmar/papers/haxl-icfp14.pdf) paper.)
I also find the argument against infix operators unconvincing. Code readability is subjective and depends on experience of the individual. Insisting that all code be immediately understandable by uninitiated without study is just not feasible. I think everyone who writes Haskell has had, and sometimes forgets, the experience of staring at applicative operators and thinking "Why oh why didn't the author use normal names here" and then a month later you're using them everywhere because they just clicked in your mind and they seem really useful now. 
Well, you read what I said too literally. What I meant is that precedence rules, which may be confusing ordinarily, are not confusing if they disambiguate an expression in the only possible way it would type-check. For example: x.y += 32*7 Will work without any parenthesis and is not ambiguous, despite no associativity going on. The precedence rules guide the compiler and the types guide the human reader.
Readability has to do with all users but the author decides who the target audience is. You can tailor effectively the same library to novices, experts and everyone in between and it's hard to call your decision wrong because you didn't pick the middle of the spectrum.
It's wrong indeed (that was somewhat implied in my comment :-))
Operators make parsing more difficult because you have to know the fixity. 
You don't even need to be as fancy as `Map k`. `(a, b)` forms a functor, trivially : newtype Product a b = Product (a, b) instance Functor (Product a) where fmap f (Product (x, y)) = Product (x, f y) There is obviously no unit for this type, due to polymorphism on `a`. It's the same issue as with `Map k`, really, but it might be clearer.
A lot of times, the Applicative way of saying things is much clearer and more expressive. It's fine to mix them. Using the most general version will often help understanding and readability. Also, sometimes performance. `(&lt;*&gt;)` is often times more efficient than `ap`. And the nature of Applicative -- that nothing "depends" on the previous things -- means that Applicative is much more suitable for static analysis and optimization. Basically, using Applicative offers guarantees for your code that Monad can't possibly offer. 
I think operators are often more meaningful *in context*, than an alternative named function would be.
That's nice. I might add use operators where the type provides good intuition about the functionality (or where the functionality can't be described adequately in one or two words?)
I'm not advocating writing half-arsed code or not using Functor/Applicative/Monad.
The text in the video is what I would have expected to see. Apparently what is not clear to me is the "view" concept. Going back to the tutorial and looking for the word "view" while it is talked about I did not find an explanation of what it really is. Thank you for the video. It confirmed my gut feeling. Now I just need to understand the preferred version. Pointers welcomed.
Can someone give me the definition of &lt;* in terms of &gt;&gt;=, fmap, and return? As the OP noted, it's nontrivial to google for them.
We have some guidelines for doing this type of thing in Javascript, for people unfamiliar with Haskell/FP/CT/whatever, which may help: https://github.com/fantasyland/fantasy-land
 (&lt;*) = liftA2 const or a &lt;* b = pure (\x y -&gt; x) &lt;$&gt; a &lt;*&gt; b It's an Applicative function, so you can't really define it in terms of &gt;&gt;=. Moreover, you *need* &lt;*&gt; / ap to define it. fmap isn't enough to get you there.
Data types are always defined as sums of products (roughly), so some kinds of expansions work while others are more involved. In particular, we always must get to sum-of-product form and that's tough when we have a product-of-sums. Since I'm on my phone I'll just briefly suggest you consider the difference between he following algebraic expansions 1*2*(3*4*5) == 1*2*3*4*5 1+2+(3+4+5) == 1+2+3+4+5 1*2*(3+4+5) == 1*2*3 + 1*2*4 + 1*2*5 1+2+(3*4*5) == 1+2+3*4*5 In terms of Haskell ADTs, the 1st and 4th are easy enough for the compiler to do it automatically using the UNPACK pragma while the 2nd is conceptually straightforward but a little annoying and the 3rd is tough. So Foo is data Foo = Foo Int Int Int Int Or data Foo = Foo (Int, Int) (Int, Int) Or data Foo = Foo {-# UNPACK #-} !Bar {-# UNPACK #-} !Bar Which ends up being machine equivalent to the first. As a final screwball, though your intuition is basically right, consider what happens when you try to expand the following function definition into a single tree via inlining： foo = 1 : foo
Right, but this is a slightly more general case. For example, a vector space library should probably make scalar multiplication an operator, even though just from its type it can't be associative, strictly speaking.
I just referred to your use of "associative monoid": an associative monoid is just a monoid. An action on a monoid is the concept you were talking about, which includes scalar multiplication.
Just a nitpick: `Foo Bar Bar` is already a problem. Fields must have kind `*`, and without a parameter, `Bar` has kind `* -&gt; *`.
I'm not sure either, I can't come up with examples of functors that aren't applicative functors offhand, at least not ones I've seen used. PS: and I could have looked just below and seen someone mention "Map k". How foolish of me! Although the problem there seems to be that unit/pure/return is indexed, so maybe this just suggests that Functor/Monad/Applicative is not as general as it could be.
This was asked on stack overflow too a while back. The basic answer is that there are a few (like Map k) for technical reasons and there are a bunch which aren't *uniquely* Applicative functors in the same way that there's a unique Functor instance.
For what it's worth, this is the way you do it in category theory. A functor F: C -&gt; D has to specify two maps, one mapping objects(C) -&gt; objects(D) and another mapping morphism(C) -&gt; morphism(D). I really prefer things this way. Haskell's more minimal implementation is useful in some cases, but the python version is more correct. Edit: Reference: http://en.wikipedia.org/wiki/Functor
I think this guide is entirely appropriate for elm to have. Consistency of conventions and idiom can be very valuable. It's certainly important in industrial programming settings. On the other hand, I would view an attempt to define such a guide for Haskell in general in a much dimmer light. It's got rich history as a research language, and I think that experimentation with idiom and style is an important aspect of that. Certainly style guides are appropriate for usage of Haskell in particular community projects or industrial settings, but Haskell also sees a lot of use in "short form" programming, which is maybe closer to art/literature than it is to industrial programming. Trying to place style constraints on that kind of program seems inappropriate to me.
"Views" or "view patterns" are a notion that's much older than Idris - they're in Haskell, for instance. The with rule lets you encode a similar idiom, which is what the example that uses parity to convert something to binary is demonstrating.
woops
Thanks for the reply. However, FYI, both &lt;*&gt; and &lt;$&gt; (i.e. fmap) follow from &gt;&gt;= and return: Prelude&gt; let apply x y = x &gt;&gt;= (\f -&gt; y &gt;&gt;= return . f) Prelude&gt; apply [(*2), (*3)] [5,7] [10,14,15,21] Prelude&gt; let fmap' f x = x &gt;&gt;= return . f Prelude&gt; fmap' (*2) [3,4,5] [6,8,10] 
The problem here is that Haskell doesn't have type level lambdas: the forall is a nice try, but is not actually equivalent: forall is equivalent to a pi type, not a lambda. Dependent languages like Agda do have them, so for simple types what you want could work.
The action of a Functor is always between two categories. In haskell, the category Hask has objects which are types and morphism which are functions between types (Functions between types are first class and therefore objects as well). So what are the categories we think of when we talk of functors? Well they're all subcategories of Hask. For example, List is the category containing all homogeneous lists and functions between them. It's a strict subcategory of Hask. In category theory, to define a (endo)functor F : Hask -&gt; List, we need to specify, for each x in object(Hask), an object in List, and for each f in morphism(Hask), a morphism in List. When we do this, we get a Functor. Nonetheless, the authors of haskell decided not to require a specification of the map from objects of Hask (i.e. types) to objects of lists (i.e. Singleton lists in this case). This is nice because it allows you to talk about Map k as a functor without specifying the type of the values. It's nonetheless formally incorrect to talk about a Functor without specifying how the objects of C map to the objects of D.
Yes, let's try implementing the example in Agda! First, let's fix the example so that it compiles. data Bar a = B a a data Foo b = Foo (Bar b) (Bar b) Reimplementing it using Agda syntax: data Bar (a : Set) : Set where B : a → a → Bar a data Foo (b : Set) : Set where F : Bar b → Bar b → Foo b Reimplementing both in terms of explicit sums of products (although no sums are needed here): open import Data.Product Bar : Set → Set Bar a = a × a Foo : Set → Set Foo b = Bar b × Bar b Expanding `Foo` out in a single line: open import Data.Product Foo : Set → Set Foo b = (b × b) × (b × b) It would have been much more complicated if `Foo` or `Bar` was a recursive type, but that's the general idea.
If you wanted to go the simple way as you describe with C++, you could probably use STM to be safe, with a thread processing the socket, and another doing your main application. Personally, I'd go with event sourcing, but this ultimately depends on your use case.
I was actually thinking something more in the lines of complete automation. I had managed to implement something similar with hash-consing on C. I do what Clojure does, except to the whole memory. When any branch changes, I record that diff and send across the network. The result is the whole memory is synchronised with no effort. It would be awesome to do the same with Haskell.
cool, seems like liftM2 const would work as well.
&gt; Hey girl. Did you use patterns with existential types? Because you just blew my mind. Wow, this is really obscure. Anyone else who gets this, pat yourselves on the back.
Awesome, I always like seeing how Python and Haskell compare :)
Thanks, I really needed something like that.
I originally posted this to get more attention for monadic programming. Since if you see the advantages of monadic programming, you then see how a pure language would make it even more powerful.
But the objects of List (the subcategory of Hask in the image of the `[]` functor) aren't singleton lists, they're *types* of the form `[a]` for some type `a`. The mapping from types (in Hask) to list types (in List) is given by `[] :: * -&gt; *`, and it's just a mapping, not a collection of morphisms or a Haskell function. The `return` operation really does belong in monad rather than functor. For a monad M, it's a natural transformation from Identity to M -- that is, a family of morphisms `Identity a -&gt; M a` for any type `a` in Hask. *edit*: You seem to be level-crossingly confused in a way I can't remedy; consider bringing up the topic in #haskell on freenode. 
Hmm... I'm not sure your characterization is correct. Reason is, it does not really matter whether all the state is 'stored in one place' at the end of the day. What matters is whether the abstractions used and/or built up by the programmer are monolithic or compositional. I'm not sure the toy examples like Mario are all that relevant. It's a bit like how it does not matter that the type of `main` in Haskell is `IO ()`. Obviously if you write your whole program directly in `IO`, it's monolithic. And you could certainly point to simple programs in Haskell (the analogue of Mario) that do just this. But that is not how large Haskell programs get written nowadays where 'compiling' to `IO` is just the very last step. My impression of Elm is that the primitives are pretty low level, and people are still in the process of discovering abstractions for organizing larger programs. Which is totally fine. That's happening all over our industry, but at least in FP it's headed in the right direction. IMO, the mess of state in normal imperative UI programming is clearly a dead end, and isn't getting any better. I've started to think that the tools (in Elm and other FRP systems) provide totally sufficient primitives, and we just have to stop waiting around for someone to tell us they are ready. Instead we just need to start writing more damn code with them while paying close enough attention to spot new abstractions, etc. This is what functional programmers have been doing for the past 20 years, it's working great, and it's led to the discovery of all the great techniques we now know and love. Not saying this is you, but people who sit on the sidelines and naysay that 'such and such seems hard to do' are not really helping. Either demonstrate that something is fundamentally impossible (which is a useful result which pushes the field forward), or dive in and figure out how to do things (also useful). The vague naysaying is a bit like the anti-evolution folks pointing to random aspects of some living thing and saying "I can't really see how this could have evolved", which says more about the person than it does about evolution.
Aw damn! That was tonight. I wasn't paying attention...
A couple days ago actually, so yes, you really weren't paying attention. =) We have [Hac Boston](http://www.meetup.com/Boston-Haskell/events/184294502/) in a couple of weeks though. ;)
&gt; I guess some fast diff/patch algorithm for algebraic datatypes would be very handy? http://hackage.haskell.org/package/gdiff
This is a very broad question (in any language, not just Haskell), with very different solutions depending on the conditions. Where do the changes come from? If from the server only (ie- like a newsfeed), the you need to just push the data or poll periodically. If changes can come from any client (like git/subversion/google docs), then you need to resolve conflicts that may arise between "merges". Knowing a bit about the full case could produce simplifications, for instance, if data can only be added to (ie- if you are logging events from different sources into a database), you don't have to worry about conflicts. If users have their own private data that others can not change, you can just push the data to the server as is, without worrying about changes that occurred (but even then things get tricky if multiple devices can share the app data). Perhaps some of the data changes need to be transactional (ie- you can only add to the balance of one checking account when you subtract from a second one). There is no one answer, or else there would already be an all purpose synchronization lib, instead the world has created a set of orthogonal tools (git, apache, REST frameworks, Google spreadsheets, SQL databases, etc).
How in the world did you come up with Post?
You are right, reading your comment I am not sure what I was even responding to. It's been a long week, sorry about that.
Wouldn't unit on Map k be a function that takes a type v and returns the type Map k v? 
This ends up really pretty. newtype Compose f g a = Compose { runComp :: f (g a) } newtype Ran g h a = Ran { runRan :: forall b. (a -&gt; g b) -&gt; h b } type (:~&gt;) f g = forall a . f a -&gt; g a fwd :: Functor f =&gt; (Compose f g :~&gt; h) -&gt; (f :~&gt; Ran g h) fwd p h = Ran (\k -&gt; p (Compose (fmap k h))) bck :: Functor f =&gt; (f :~&gt; Ran g h) -&gt; (Compose f g :~&gt; h) bck p fg = runRan (p (runComp fg)) id
Yeah. The current Ran/Lan in that codebase is based on the universal property but I can move back to the (co)power-based definition. We had it in terms of the universal property explicitly because it worked out better before we had the notion of "Tensorable", and then we never switched back to the more traditional formulation.
This [may](http://stackoverflow.com/questions/1012573/getting-started-with-haskell) help you.
I would recommend [Learn You a Haskell](http://learnyouahaskell.com/) for the beginner level followed by [Real World Haskel](http://book.realworldhaskell.org/) for intermediate topics. That's the way I transitioned from Python to Haskell. After that, you'll have to check out various blogs, libraries, and tutorials for learning the more advanced features of the language. Many of these features are not widely written about and will take some experimentation to learn, but you've got some time before you get to that point. Be warned that it is not an easy journey, Haskell will challenge how you know programming, it is very different from Python. I would say it's at almost the absolute opposite end of the spectrum as a programming language. That being said, it has completely changed the way I write code in any language, particularly Python. There are new patterns and abstractions I recognize, I am able to write more functional and efficient code in Python by being able to have a strong intuition about generators and decorators, and write higher order functions of my own in the imperative language.
I can't help but think of React (Javascript) when you mention that, though react is more concerned about minimal dom changes rather than minimal data sent over the wire.
I suggest the Haskell Wikibook as the best from-the-beginning overview, as the elementary track is quite complete and excellent. You can generate a saved version to print or view on an e-reader (note that the other PDF (the option that is *not* on-demand) is outdated. Overall, I think reading it on the site is best as you can edit and help improve it if you find any spots you see any need.
Sure, but what's the index value of type k? In general, I think there's no way to pick a reasonable default.
1) Learnt scheme in the early 1990s at University. Picked up Ocaml in 2004 and switched to Haskell in 2008. 2) In C, I tend to think more before I start coding and usually come up with a better solution than if I just rushed in and got something working. 3) In Python I'm less likely to write OO code than I used to be. 
1) I was first exposed to Haskell when I was first learning programming, but I wouldn't say I "learned" it then. I took a couple year break from it while I learned Python and .NET, then I decided to pick it up again during my last year of college, and haven't looked back since. 2) In particular, it makes me think about how to write higher order functions and think about my data as immutable data structures, things to apply functions to, not necessarily things to mutate. My code has become safer, more reliable, and more composable. In Python in particular, I use more generators and decorators, which are very powerful tools in a Pythonista's toolkit. 3) As mentioned, I often use these techniques in Python to great affect. By ensuring that many of my functions are pure (achieved through effort, rather than the type system), I retain much of my ability to reason about my code without having to resort to things like debuggers.
Thanks. Thats exactly what im after, i want my code to be as reliable as possible. How is the testing culture in haskell? Does the code need be as tested as in lets say Python or JavaScript or do you get away with less testing? Becuase the language guarantees pureness, does this mean you dont haveto test each unit?
1. My first exposure to programming was actually functional programming with OCaml. 2. No idea, since I don't know any other way to program. 3. I prefer to stay away from non functional languages.
The type system allows you to reduce a lot of testing, if you have a function `something :: Int -&gt; Int`, you have a lot fewer cases to check. It also means that the function `something` doesn't depend on anything but it's arguments, with absolute certainty. Even if the function comes from a 3rd party library, you know that there are whole classes of errors that can't possibly occur. Compare this to a function `int something(int x)` in C or C++, it can do quite a lot, including accessing the file system, a database, or a web server. In Haskell, it can't. This may seem insignificant at first, but once you start learning about more complex types, these reasoning abilities become very useful to determining which parts of your program are failing. That being said, Haskell also allows libraries like `QuickCheck` to be implemented simply, and these libraries can be useful for testing your algorithms against arbitrary input and for performing general unit tests. In general, the mantra is "if it compiles, it probably works" for Haskell. The type system really helps to make sure that things just "work", more so than in any other language that I've written.
Got it. Thanks again.
I'm still learning Haskell but I was mentored at an early stage in my Python life by a Haskell-er. It has helped me *immensely*. My python code is almost always full of generators and calls to `functools`' methods for partial application, etc. The code is readable, terse, and efficient. Yay, functional programming! As I learn Haskell I am coming to deeply appreciate the type system, but shy of that I really admire how easy it is to program Python in a functional way.
1) I learned my first functional language, Haskell, as part of a course at uni called 'programming languages and paradigms'. We could choose to do assignments in any functional language, though Lisp and Haskell were recommended. I'd been programming in C++ for several years at that point, as well as Python, C, and some scripting languages. 2) I just realised this the other week as I sat down to write a new application in Python. A couple of years ago, I'd have started by making several classes. But instead, I started by making several functions :P. But seriously, learning FP has taught me, on a really basic level, how to use closures and first-class functions. But it's also shown me the benefit of purity in functions. It's a far cry from the usual MATLAB scripts you see (I study engineering), with global variables everywhere and no encapsulation. 3) I was getting to that, but yes, very much so. Though languages like JavaScript make it more difficult to be functional, simply on a syntactic level. For example, you can't compose or partially apply functions in JavaScript without going out of your way. But the principle of purity can very easily be applied to any language, and tends to make for nice code, even if you're not trying to be particularly functional. I started to get suspicious of functions that didn't take any parameters, or return any values :P.
For me, after learning Haskell I didn't like being programming in Python again. I heard this is what happens to most of the people.
I was into OOP first, and still remember how difficult I found it to learn and understand. How do I decide what's an object? How do I deal with this superclass/subclass business? Where should my methods go? I still think it's an interesting paradigm, especially in combination with highly dynamic runtimes like Smalltalk's, but eventually I found out about O'Caml, had fun with that for a while, then moved to Haskell. I chose the university I attended (Gothenburg) because of Haskell, learned more about it there, kept coding and reading and discussing, etc. Haskell led me to deeply appreciate three things: (1) pure functions, especially combinators, which can be reasoned about, specified, proved, and understood; (2) highly expressive types; and (3) high-quality polymorphic abstractions based on mathematical/logical foundations, like functors, applicatives, and monads. Like any programmer, I've seen a million lines of truly messy code, and Haskell seems to encourage me to aspire to write programs that truly make sense and work correctly by construction. My first exposure to real high-quality engineering with Haskell was a guest lecture by John Hughes about the architecture of Xmonad, the window manager. Then there are things like parser combinators, the Diagrams package and the wonderful paper about diagram monoids, the streaming I/O abstractions based on categories, Haxl's distinction between applicative &amp; monadic programming in the context of concurrency, Silk's declarative REST API definitions, software transactional memory, generic applicative traversals, Uniplate, building meaningful abstractions as compositions of functors, the wonderful way in which a pure functional language turns out to have the best I/O in town, zippers &amp; differentiating algebraic data types, beautiful chains of pointfree composition, pattern matching, the consummate care many libraries take to be correct and well-defined, and so on and so on—there's just so much wonderful stuff going on in the Haskell world that it seems (when I'm in an enthusiastic mood) like a little programming renaissance. At Silk, our large JavaScript codebase is not purely functional by any stretch of the imagination, but there are a lot of concepts in there that come from functional programming: e.g., the reactive value framework, an arrow-based combinator library for DOM reactivity, etc. We do miss type checking a lot. I switch between working on JavaScript projects and Haskell projects, and the feeling of typing `:r` in ghci and getting a type error is wonderful: it feels like the computer is actively helping me!
Nice
&gt; Reason is, it does not really matter whether all the state is 'stored in one place' at the end of the day. What matters is whether the abstractions used and/or built up by the programmer are monolithic or compositional. I theory I agree with this, but the problem is that it's not clear at all (at least to me) how you would then make a compositional UI framework on top of that, without just using FRP as a glorified event loop. &gt; I've started to think that the tools (in Elm and other FRP systems) provide totally sufficient primitives, and we just have to stop waiting around for someone to tell us they are ready. I'm not that familiar with Elm, but I do think FranTk is an excellent library design for UI. It's not pure, but maybe we should just be less dogmatic about purity and accept that with some impurity we have a great solution to the UI problem. &gt; The vague naysaying is a bit like the anti-evolution folks pointing to random aspects of some living thing and saying "I can't really see how this could have evolved", which says more about the person than it does about evolution. I don't think this is fair criticism. FRP is always pointed at as the purely functional solution for GUIs. I posed a concrete challenge problem, which to my knowledge nobody has managed to solve in a way that's pure and compositional. Demonstrating that something is fundamentally impossible is extremely hard, even more so since the question isn't mathematical.
At university in 1999. But my usage was fairly limited at the time, and I only started using it for real thing somewhere in 2004-2005. And now, pretty much all my development is done in Python (for work), with little time left to explore other things, so progress has been ... slow.
You should probably get your code reviewed by other Python programmers if you ever want to collaborate on something. It's easy to learn the bad habits of your teacher if you restrict yourself to one, and writing idiomatic code is actually rather important. If you use `functools.partial` more than occasionally you're not writing idiomatic Python code.
There's been plenty of suggestions for books and other material, so I'll give something inspirational and supplementary. Watch [A Taste of Haskell](https://www.youtube.com/watch?v=jLj1QV11o9g) by Simon Peyton Jones. It's a two part conference talk by Haskell's creator (who is a magnificent speaker) targeted at people who are competent programmers but have no prior knowledge in functional programming. This talk is what got me started in Haskell.
1. I started learning it two or three years ago. Back then I wasn't a fantastic programmer in any language but I saw a link to Learn You a Haskell on /r/programming. It was my first FP language. I had a lot of fun writing solutions to Project Euler problems but never really used it for anything else but that. I ended up going on a hack day (well it was more of a hack week really) and I tried to use Haskell to download Creative Commons images off Flickr and I spent a long time hitting my head against the brick wall of the type system. After that I went off Haskell for quite a while. About 18 months ago I started to learn again because by this time I think I was a better programmer and I decided to just push through the nasty bits. 2. It makes me really annoyed at Rust and similar languages! :D With the more traditional languages like Python/C# etc. I don't really feel the need for Haskell like features. I kind of accept that it is a different method of programming and just get on with it because the languages themselves are actually really well thought out (despite what some Haskell people will tell you) The problem is when I start learning a language like Rust where it has some similar features to Haskell. I always kind of want all the features of Haskell in there. For example I'll be doing something and I'll think "This would be so much clearer/more expressive/shorter with Functors". It's not that Rust is a bad language, it's that it is sufficiently similar to Haskell to make me wish it had more of Haskell's features. Then of course I remember that if it had Haskell's features I probably wouldn't be quite as quick at writing programs with it. 3. I write slightly more functional code in Python than before. I use generator expressions/list comprehensions a lot more and I try to use generators and not just return a list (laziness). I tend to avoid Javascript as much as possible - I'm already insane enough thank you very much. I write fewer classes now as well. I look at a lot of Python code now and think "well that class doesn't need to be there". I don't mind classes if you're looking after mutable state but there are a lot of people who just use them to group functions and maybe one piece of mutable state.
[This is what I got out of this talk](https://i.imgur.com/j74SykU.gif). Awesome work though guys, jokes aside I'm starting to follow bits of this, and it's very exciting!
You and me are practically opposites. I went directly from procedural programming to functional programming, so while I can repeat the clichés about encapsulation and whatnot, I still don't quite get what it's all about yet. I'm getting there, though! It definitely helps to read GoF. 1. I started leaning FP six or seven years into programming. I bounced around quite a bit between Haskell, Common Lisp, Erlang, Prolog and Scala, but after a while settled with Haskell. I'm glad I poked at the other ones, but I don't regret settling down with Haskell for the time being. 2. Haskell in particular has made me much more interested in refactoring, making my code better by means of abstraction, thinking in terms of transformations of data, being more open to parallelism, enforcing invariants (eliminating bugs) at compile-time and a bunch of other things. Other FP languages taught me other things. 3. Well, sort of. Knowing FP has helped me make better program designs subconsciously, be it procedural or object-oriented. However I am very careful not to use Haskell idioms in other languages in excess, because it just makes the code an unreadable mess to anyone who is not comfortable with Haskell idioms.
You should really branch out at this point. Try some OO, try some procedural programming, try some logic programming!
is it possible to include all prelude functors (etc.) using default signatures? (i can't check for myself now, unfortunately.) i.e. class Functor f where .. default fmap :: Prelude.Functor f =&gt; ... fmap = Prelude.fmap
Hi! Maybe it is not usefull for you as you don't have to deal with btc transactions yourself buuut, do you know [Network.Haskoin](https://hackage.haskell.org/package/haskoin)? Maybe you can find it usefull in a future.... btw, I think you should use Integer for the amounts, it could be usefull wrapping an integer within a newtype with a phantom datatype arguments so you can avoid mixing by error BTC and USD amounts, something like: data AmountKind = USD | BTC | .... newtype Amount ( a :: AmountKind) = Amount Integer deriving (Num,Integral...etc) 
In vim G takes to you to the end of the file in normal mode ;)
I'm going to watch it when I have a bit of time, but in the meantime: class (hom ~ Hom) =&gt; Composed (hom :: k -&gt; k -&gt; *) | k -&gt; hom where Could someone please briefly explain me this thing? This pattern shows up several times in the code. What's it doing and why? 
Ideally, every monad should be an applicative, and every applicative should be a functor. Because the usefullness of monads has been known for longer than applicatives, GHC incorporated monads first. As a result, GHC doesn't require all instances of the monad typeclass to be be instances of applicative as well, but I believe that is being fixed for version 7.10. Functor is already a superclass of applicative.
They don’t have things in common, they generalize themselves. An applicative functor is a generalized functor : class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b class (Functor f) =&gt; Applicative f where pure :: a -&gt; f a (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b You can map `length` over a list using `fmap`, but since `Applicative` generalizes `Functor`, you can do that with `(&lt;*&gt;)` as well: fmap length ["hey","zoobar"] == pure length &lt;*&gt; ["hey","zoobar"] An applicative functor **is** a functor with a few stuff generalized. It’s the same thing for monad, which generalizes an applicative functor. Each generalization is done along one axis. `Functor` -&gt; `Applicative` enables you to run function inside the functor whereas `Applicative` -&gt; `Monad` allows you to modify the structure by getting out of the structure (`f (a -&gt; b)` is inner structure whereas `a -&gt; m b` is outer-then-into).
All monads are applicatives, and all applicatives are functors. Not all functors are applicatives and not all applicatives are monads. Also, some types can be monads or applicatives in different ways(interestingly this doesn't happen with functors). For example, [] can be a monad in the usual nondeterminist way, and an applicative in the ZipList way: the latter can't be extended to a monad.
As far as I could tell, it's a class which allows you to talk about compositions of functors which, ultimately, arrive in the category marked by that `hom`.
I don't mean `Composed` itself, I mean... basically all of the rest of it. Why the definition looks like it does, and what the various parts of it (`hom ~ Hom`, etc.) are doing.
OCaml was my first functional language. I learned it in school some 10 years ago, and played with it some after. I picked up Haskell 2-3 years ago, when considering what I hadn't covered in the languages I'd learned and deciding "pervasive laziness" was the next point in the space to hit. It's hard to say just how much functional programming has changed my style as opposed to just more experience. I do expect I reach for closures and such more readily than others (even building them myself where necessary). A persistent "can I enforce this with types?" has lead to good things in my C code as well. 
No problem, have an upvote! :)
I get this advice every time I give a talk from vim. ;) I know it and I use it whenever I stop to think, but when explaining I tend to let muscle memory take over so I can free my brain to talk. :) 
Doesn't work. It forces the kind of f to * -&gt; *. They participate in the kind check. They -almost- have to as we don't have kind equalities yet.
You might want to read [this paper on Applicatives](http://www.soi.city.ac.uk/~ross/papers/Applicative.pdf). It gives a couple of examples of `Applicative`s that are not `Monad`s.
Thanks for this example! What I don't get is, how this exactly works: Fetch ((&lt;*&gt;) &lt;$&gt; f &lt;*&gt; x)) So f has the type `IO (IO (a-&gt;b))` and x has the type `IO (IO (a))` and we want to apply the innermost value of x to the innermost function of f and return them, right? But how does this exactly work, since there are two nested IOs? Is that's what `((&lt;*&gt;) &lt;$&gt; f &lt;*&gt; x))` does? Another question: Why does the `print` work? Isn't `print &lt;$&gt; start myfetch` of type `IO (IO a))`?
To answer your question, there are some `Applicative` instances that don't satisfy `pure = return` and `&lt;*&gt; = ap`. One example is the `Fetch` applicative/monad as described in Facebook's paper on Haxl, [There is no Fork: an Abstraction for Efficient, Concurrent, and Concise Data Access](http://community.haskell.org/%7Esimonmar/papers/haxl-icfp14.pdf).