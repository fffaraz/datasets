I'm of the opinion that mathematics is essential for programming quality; the problem is that it isn't any of the mathematics you will be taught in university. In traditional mathematics, one would study groups, rings, and fields. For a programming curriculum I'd instead have: monoids, dioids, and Kleene algebras. Also included would be concepts from category theory: functors; monoidal functors; monads comonads, and their associated algebras and coalgebras; natural transformations. I'd also include topology. Traditional topology courses focus on Hausdorff spaces; but in programming, the relevant Scott topologies are almost never Hausdorff. I'd cover partial orders, and in particular, dcpos in the topology course.
That's *a* big idea. And it is relevant when parallel resources are at hand and the data is appropriately sized. But when celebrating parallelism, one must check that overhead doesn't cancel the gains in the actual hardware that exists. For example, Hadoop ( not Haskell, but famous) is often slower than traditional analysis for small/medium size data sets that could fit on on one machine, but require 10 machines under a Hadoop environment.
It seems rather harsh to downvote this to oblivion.
I've returned to this a few times, and I don't really see why this is a problem. &gt; You can only reason about point if you read its implementation. Or its documentation. That's no more true than any other function &amp;c, and you can define some laws with relation to other classes anyway, can't you? At the very least, point . copoint and copoint . point should both be id, where both are defined, right?
Last time I wanted Copointed, it was to polymorphically unwrap things like Sum and Prod from Monoid. A comonad instance for these isn't immediately obvious to me but I am not the least bit confident that's not just because I'm only very shallowly familiar with comonads.
You're right. It really is asymptotically O(N).
You know...have you considered the idea that the error message might be useful? ;)
You are right. I think it has to do with the "How can someone be expected to take Haskell seriously": putting that at the start make downvotes rain: it shouldn't be like that, but it is.
Part of why the "big idea" of Quicksort is usually taken to be the in-place swapping technique is that Quicksort isn't that competitive against other sorting algorithms without it
It's great you found the wiki, but your rant here is inappropriate 
Actually, I would say it is NOT great that I found the wiki, as evidenced by my rant.
&gt; I began to think about how to use Haskell's type system to encode precisely the minimal amount of necessary information, with necessity determined by the rules of tictactoe. I think you're going in the right direction, but it's more useful to think of ["make illegal states unrepresentable"](https://ocaml.janestreet.com/?q=node/85) rather than "encode the minimal amount of information". The latter suggests that you're merely trying to optimize memory usage, perhaps prematurely ("let's use 2 digits to encode the year!"), while the former is a principle you can use to write correct and clear programs.
I would like to just say thank you not just for your talk, but for all the other talks you've uploaded. Great work!
Indeed that's a bit rude, but I would have preferred the response to have been "I'm going to fix the wiki right now!" rather than "I'm going to downvote you right now!".
Should you learn Haskell? Unquestionably, yes. Irrespective of everything you said, your core question is answered only in the affirmative. Like Lisp, even if you never actually use it, learning it and its core concepts will make you a better programmer for the rest of your life.
Okay. Another point i came across is that common lisp is a more practical language than haskell is. What exactly is meant by this? And more importantly is it true?
Lazy evaluation makes normal Haskell functions act sort of like "half macros." Some of the things you would use a macro for in lisp, you can do with normal functions in Haskell. The main arguments for using Haskell in my mind though, are its expressive type system and the purity guarantees. These are not things you will realise are beneficial right away, but if you stick with it you will want to have it in all the other languages you work with. Basically, the ultra-simple version of it is that the expressive type system makes it impossible for you to write a program that has even the slightest chance of driving a car like you would a motorbike. Many languages would allow you to compile such a program and then it will blow up in your face when it runs. Haskell doesn't even compile such programs. As an added benefit, the expressive type system allows you to automate a lot of error handling, and makes sure you handle errors when you want to. The purity guarantee simply means that you know that 90% of your functions are completely independent from each other. The compiler make sure of this. That is of great, great help when debugging and testing -- if a function works on its own, you know it will work together with the others as well. There's no hidden interaction going on.
&gt; I was thinking about learning a functional programming language and came to the conclusion that I would either learn common lisp or Haskell. Lisp mixes imperative programming with functional programming, while Haskell is *purely* functional. Almost unique to Haskell is that side effects are controlled via the type system. If you want to learn functional programming, definitely choose Haskell over Lisp. &gt; Mainly it has a rich history and can be used for real world programming. Haskell is not quite as old as Lisp, but it's older than Java, and it's [definately used in the real wold](http://www.haskell.org/haskellwiki/Haskell_in_industry). &gt; Apparently there is also something called macros that lisp has that makes it a "programmmable programming language" and gives it a lot of power. GHC, which is the most popular Haskell compiler has Template Haskell, which is a macro system. What templates does is to give you access to generating code at compile time using Haskell, as well as some facilities for using values from the surrounding code inside your own custom syntax. But in Haskell, macros are usually a last resort, since laziness covers quite a few use cases that macros are used for in Lisp. &gt; However, I was not able to find much on haskell Try [Real World Haskell](http://book.realworldhaskell.org/read/) or [Learn You a Haskell for Great Good](http://learnyouahaskell.com/).
Thanks now i get the lazy evaluate feature.
&gt; but if Haskell were remade today, strict evaluation would probably be the default. I kind of doubt it... Laziness is an important part of Haskell. I'd want a laziness-polymorphic language, where you can instantiate functions to be lazy or strict on various things. 
The main advantage of Lisp over Haskell is that it is easier to learn. That is, Haskell's main disadvantage is that it is more difficult to learn. The reason is that Haskell programs are not only a specification of how to execute on a computer (for the computer and other humans), they also contain a concise specification of correctness of various aspects that other languages don't. A language which is capable of encoding only execution specifications is simpler and easier to learn than one which is capable of simultaneously encoding that and correctness specifications. Thus, the techniques of writing programs with the latter are more sophisticated and require more learning. The payoff is rather huge, however, as after the extra learning, you can write programs which are much more reliable by default, easier to modify, easier to parallelize, etc.
I think "Haskell is older than Java" is misleading. Haskell was *not* the same language before it had higher-kinded type-classes, the IO type, etc. So some weird dialect of Haskell is older than Java. Haskell as we know it is a relatively new language.
Can't you force Haskell to evaluate something strictly? Using a $, I thought
Laziness can make it extremely difficult to handle the performance characteristics of non-trivial computations. Laziness by default is surprisingly unuseful for real-world ppplications, with optional laziness being preferable.
Laziness is [pretty useful](http://augustss.blogspot.co.il/2011/05/more-points-for-lazy-evaluation-in.html). I think mostly everyone would agree that data structure fields ought to be strict by default. However, I really wouldn't want to write in a language where I cannot re-use all the common recursive functions (foldr, map, etc) as mentioned in augustuss's blog post. I think if we had polymorphic laziness that would be the best of both worlds. We could instantiate functions to be strict or lazy based on need. Then even strictness instantiation as a default would probably be OK.
you mean using a `!`. `$` is just a low-precedence application operator.
($) has nothing to do with laziness. It is just a low-precedence function application, used for: * Function application sections, such as: `map ($ 1) myList` * Separating subexpressions (due to its low precedence), so that `composition of operators here $ compositions here too` does not require brackets around each composition To evaluate strictly, you can use `seq` or `($!)`.
Fair enough, but the same can be said for Java, which didn't have generics before 2004. According to [A History of Haskell: Being Lazy With Class](http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/history.pdf), Haskell got higher kinded type classes and the IO type in 1996.
I'm not sure that I agree with you on the point of untyped languages being easier to learn. You still effectively need to *think* about types while writing lisp programs, it's just that the compiler is not going to help you get them right, and you'll have a bit less vocabulary for discussing the contracts that various parts of your programs will regardless have to obey in order to work at all.
No
Of course you still need to think about types, but you don't have to think about type-lifting, various type constraints, having separate list and tuple types... You can do much more by trial&amp;error. Additionally, learning concrete things is easier than learning abstract things.
No, it really can't. Java is pretty much the same language as ever, generics only sprinkled a little type safety on top.
relax, he just used the wrong symbol. I think he meant `!`
definitely*
Non-strictnes can be had in many languages and is IMO a much less interesting (though more buzzword-friendly) feature than, say, higher-kinded polymorphism. 
Okay, the reason I brought up lazy eval is when I was googling around taht was the feature that was probably brought up the most which is why I thought it was something of an important feature. 
That is not surprising. Most mainstream languages are variations on a common theme - imperative (and "object-oriented") programming. So uttering a few key words is often enough to give at least some idea what the language is about. But Haskell is of a really different kind and requires a different mindset to utilize. That makes it surprisingly hard to learn for programmers adept in imperative languages who are used to just having to learn a slightly different syntax (at least to get started). It also means that explaining interesting features can't be done in a few sentences. I think that's why something like lazynes is displayed as the main feature, because it's easy to understand for imperative programmers. 
if you're anything like me, learning functional programming is more of a 5 year journey than a 3 month task. I'm 2 years in on my FP journey my path so far is Clojure -&gt; Scala -&gt; Haskell -- and the most valuable portion so far is writing lots of production code and working with production libraries (for me, this was all Scala). Figure out a way to do that. If you determine the best way for you to get a FP job is to read Learn You A Haskell, go for it.
It seems to me people are being unnecessarily unfriendly. The use of `last` in `main` is making the parallelism especially irrelevant. If we strike it, though, we see that when you threw in `bench` you made this a concurrency problem. I'm not really thinking the algorithm business through, but I think you will get something closer to the effect you wanted if you try something like this: bench x = do start &lt;- getCPUTime x1 &lt;- Control.Exception.evaluate x end &lt;- getCPUTime print (x1,((end-start) `div` 10^9)) testRanges = zip as $ repeat 10000 where as = map (10000*) [1..40] main = Control.Concurrent.Async.mapConcurrently (bench.test) testRanges -- $ ghc --make -O2 -rtsopts -threaded fibpar.hs -- time ./fibpar +RTS -N2 -- ... -- (Nothing,9642) -- (Just 329468,8458) -- (Nothing,8117) -- ... -- -- real 0m7.429s -- user 0m13.117s -- sys 0m0.405s if you throw out `bench`, the main calculations stay pure and you can do something more like what you were doing: main = print $ minimum $ catMaybes $ parMap rpar test testRanges -- $ time ./fibpar +RTS -N2 -- 329468 -- -- real 0m6.970s -- user 0m12.748s -- sys 0m0.251s 
Are there any languages today that are typed based on evaluation order? That's pretty interesting.
[This free book on category theory for scientists](http://arxiv.org/abs/1302.6946) is sitting on /r/math right now with a lot of positive feedback. I haven't read it though. I personally really liked the [rosetta stone paper](http://arxiv.org/abs/0903.0340). It specifically talks about logic and programming, which makes it understandable. But the connections to physics is what made it fascinating. It's relatively short (70 pages :) so won't conver too much of specific interest to haskellers like comonads and coalgebras.
Being similarly curious about CT, I bought "An Introduction to Category Theory" by Harold Simmons. The Kindle version is relatively cheap. It's not too advanced, and uses a two-column layout for joint definitions of stuff and the corresponding co-stuff. A nice way of conveying duality. It doesn't cover monads, though.
Java appeared in 1995, before that it was a research project... started 1991. Haskell was started 1987, version one of the standard appeared 1990. So if you go by "as we know it" Java is a tiny bit older, if you go by "started" Haskell is significantly older any way you count. They're pretty much sister languages, as such. One grew up with rich parents and got spoiled into enterprise, the other grew up with wise, but poor, masters and is only now developing its grand potential.
It's not for a beginner, but I'm sure the old classic "Categories for the Working Mathematician" by Saunders Mac Lane remains a must read in this field. It covers most of the foundations very thoroughly.
* Basic Category Theory for Computer Scientists - Benjamin Pierce * Arrows, Structures, and Functors: The Categorical Imperative - Michael A. Arbib * Mathematics: Form and Function - Saunders MacLane
\#\#categorytheory on Freenode IRC.
Good point. We are within 10 days to releasing our new TOU on the website for general usage and our EULA for the product, which is what tazjin is correctly concerned with. The basic idea we have is this: in the product you have a choice to publish privately or publicly. If private, everything of course belongs to you and no one knows about it. If public, you will grant us non-royalty, non-exclusive license and to the community Creative Commons 3.0 Attribution license for non-code content, and BSD license for code. So if you want to start using the product, import a project from github and not worry about IP issues, by all means go ahead. Your content is safe and sound.
Thanks. that was very helpful :)
Catgory Theory, by Steve Awodey is great for someone with a little knowledge of abstract algebra.
http://shared.computer-mind.com/JoseJuan/josejuan.glxinfo
I would recommend you to start with Conceptual Mathematics: A first introduction to categories by Lawvere and Schanuel.
I absolutely recommend this book as an intro to CT. This was about the third book I read on Category theory and I always wished it had been the first! Lawvere has an axe to grind with his category theory—he eventually uses it to replace foundational set theory—and it shows in this book, but that's hardly to its detriment. The value of Conceptual Mathematics is that Lawvere and Schanuel go to great lengths to explain how to *interpret* category theoretical notions using familiar examples from Set theory. Instead of just introducing the terminal object and its properties, they explain for some time how it forms an interesting tool for probing other objects via its morphisms and thus illustrates the categorical mode of thinking. Prior to reading this book I could define and meaningfully use generalized elements to prove things, but I never really saw them quite as deeply as I did afterword, for instance. I will warn that this book starts out much, much slower than it eventually proceeds along at. If you're happy reading through the first few chapters, prepare to be greatly frustrated somewhere halfway through the book. That said, the interplay between the "lecture" and "session" style chapters helps by letting you read back and forth between two kinds of exposition to pick up all the details.
Could you give annotations for these books? I've often wanted to read Pierce at some point, but he's not at the top of my list. I'd like to understand what each one of these books brings to the table in order to better prioritize my reading list.
So far the "Category theory for scientists" book is pretty good. I majored in math, went to grad school for it, but I always preferred the applied approach since it's usually easier to learn the big picture from examples without the really precise (and restricted) window of proofs. You can always go deeper and use a more restricted "window" (e.g. proofs) to understand a particular concept if needed.
&gt; Good point. We are within 10 days to releasing our new TOU on the website for general usage and our EULA for the product, which is what tazjin is correctly concerned with. The basic idea we have is this: in the product you have a choice to publish privately or publicly. If private, everything of course belongs to you and no one knows about it. If public, you will grant us non-royalty, non-exclusive license and to the community Creative Commons 3.0 Attribution license for non-code content, and BSD license for code. &gt; &gt; &gt; &gt; So if you want to start using the product, import a project from github and not worry about IP issues, by all means go ahead. Your content is safe and sound. 
What is this guys email?
Jesus if ever there were a confluence of smuggery in the universe, that sounds like it'd be the place. 
I don't want to color my recommendation by that factor though—I can't deny the possible bias, but Lawvere is really trying with this book to make it approachable and intuitive. For something as abstract and definition-prone as your first foray into CT, having a lot of answers to "why" is a big deal, I believe. Conceptual Mathematics puts a lot of effort toward that.
Haha—very fair
I found [this reading guide](http://plato.stanford.edu/entries/category-theory/bib.html) really helpful. The citations refer to the references section of [the main article](http://plato.stanford.edu/entries/category-theory/index.html). The "Books and introductory papers written for computer scientists" section might be helpful. I first learned category theory from Goldblatt 1979, and I thought it was pitched at a really good level, at least for someone with my background.
Fun fact: looking at this book made me curious as to whether there were an MIT OpenCourseWare class on category theory. Turns out there is, and it's [the textbook](http://ocw.mit.edu/courses/mathematics/18-s996-category-theory-for-scientists-spring-2013/textbook/) for [the course](http://ocw.mit.edu/courses/mathematics/18-s996-category-theory-for-scientists-spring-2013/index.htm) - how cool is that? 
why? ML has monads... You don't evaulate under binders so this isn't an issue. 
brian at lorf dot org
Yes. I'm using Riak for all my data. When I switched it was about 3 times slower than postgres. But it also keeps three copies of data. The main reason I switched to Riak is ability to scale writes and simple handling of failures and cluster expanding/shrinking. Another reason is that while I performed optimizations I got to the point when all my data started to be key/value, i.e. non-relational. So I have no need in relational DB. One of the main challenges in RSS readers is that they're very write intensive. Feeds are continuously updated (even when user doesn't use web front-end) and information about each read post needs to be written too. So usual database "add read-only slave" scaling doesn't work. You don't need to scale reads, you need to scale writes. Ordered data selection is very simple. I'm keeping Set (Time, Id) for feeds (in a single value serialized using Binary) and then operating on these sets in memory. In general the data I'm keeping in Riak is just a Haskell data structures serialized using Binary.
So far there's been no less than ten different suggestions. Has anyone read more than one who could compare and contrast between them?
Pierce doesn't go into much depth, covers the basics: category foundations, functors, natural transformations and adjunctions. Speaks briefly about cartesian closed categories and connections to the lambda calculus. It's a very short book. Arrows, Structures and Functors is a introductory text, it's a bit older so it doesn't talk about CS but does give a very shallow introduction to everything from category foundations to the yoneda lemma, monads ( or triples as he calls them ) comma categories, etc. It's not nearly as rigorous as MacLane's "Categories for the Working Mathematician". Form and Function is more of a high level overview of how MacLane saw categorical concepts relating to the whole of mathematics. It's not particularly focused on CT but does give a good overview of a lot of modern mathematics. Hope that helps!
Note that this works on os x as well. It would also be welcome from mac users. 
Here are the books I like: I only mentions books not mentioned in previous comments: Really good: Awodey's, "Category Theory" Crole's, "Categories for Types" Perfect for beginners: Lawere and Schanuel's, "Conceptual Mathematics" 
Calculi are fine, I'm just curious about how the typing works.
Form and Function might match up with Conceptual Mathematics in that case. I think that kind of insight is useful given how challenging CT can be when first approached.
(facepalm) I was actually scanning the page visually for the @ symbol. p.s. seriously, why do people still obfuscate their email address, it's not like we don't have antispam technology.
glad you've finished it. how is your phd going on? What are you working on, more HS-&gt;JS stuff? I hope it's more interesting than being QA manager (it really sucks by the way) :)
Probably. There's a lot (about 1400 lines at present) of JS plumbing that gets linked into the JS file - comments, whitespace and all - even when it's not used. In addition, you're probably getting some library code in there as well. In general, output file size grows something like logarithmically, it seems; for small programs you get lots of code that only gets used once, but as your program grows you add less and less new code, instead calling the same library functions over and over. For a hello world program you should get about 44 KB of JS when compiling with -O0 --debug (worst case), 34 KB without any extra arguments at all (Haste defaults to -O2) and 2½ KB with --opt-all (requires Java in order to run Google's Closure compiler on the final output for you). At least that's what I'm getting. :)
I found just the opposite effect: I love the notation and have begun using it in my own notebooks. It's either that or reversing composition and application, which I also do sometimes. I feel like category theory is a big indicator that f(x) as notation was a historical mistake.
By default `C-M-i` does ghc-mod completion. In emacs, `C-h m` will show you the help page for whatever mode you're currently in. I believe that dropping (setq ghc-ghc-options '("-idir1" "-idir2")) in your .emacs or init.el (where `dir1` and `dir2` could be your `.hsenv` directory) should tell ghc-mod to look there too. 
Isn't it only a "mistake" if the surrounding text is in a left-to-right language?
Well, "finished" is quite a strong word - at least there's lots of work needed to get the libraries in better shape. Apart from the Haste stuff, I'm currently doing some work on testing and verification of programs written in the Feldspar DSL. There's also a pretty neat project on machine translation starting up soon, so there's no shortage of interesting stuff to work on. The only bad thing about PhD studies so far is that I seem to get caught up in whatever I'm doing and then accidentally work around the clock. That's probably not healthy in the long run.
It's a mistake if you draw all your arrows rtl and compose your functions ltr. Bird and de Moor decide to keep composition ltr and change their arrows to match—which is a little weird to read at first. I'd say that if your language is written rtl and it's probably not a bad idea to have your arrows rtl and your composition rtl.
I tried it, was not impressed. foo :: Int foo = 'C' No error showed up, no matter what I did.
In lisp, strict evaluation means that you can't write an 'if' as a function - (defun my-if (test-form then-form else-form) ...) This is because *both* the then-form and else-form are evaluated before being passed into my-if. Instead, you need to use a macro to essentially copy and paste the *code* for the then-form and else-form into a cond statement: (defmacro my-if (test-form then-form else-form) `(cond (,test-form ,then-form) (t ,else-form))) # With that, a call like (my-if (panic) (fire-the-missles) (do-nothing) ) # is macro-expanded into (cond ((panic) (fire-the-missles) (t (do-nothing)) However, this isn't a terribly interesting example of macros. There's a lot more interesting examples in books like On Lisp and Let over Lambda. I think that lazy evaluation and macros don't really cover the same thing. Lazy evaluation lets you do fun things like make infinite data structures: -- infinite list of the fibbonacci numbers fibs = 1 : 1 : zipwith (+) fibs (tail fibs) -- fibs == [1,1,2,3,5,8,13,...] It also lets you separate algorithms from each other. For example, firstK k list = take k (sort list) only fully sorts the first k elements, instead of sorting the entire list. Additionally, you can say something like: gameTree = ... alphaBeta = ... That is to say, the generation of a search tree can be trivially split apart from the consumption of it, and the parts that aren't searched aren't evaluated. In contrast, in most strict languages, you generally need to mix your algorithms together - generating a search tree is controlled from the alpha-beta tree search of it explicitly, for example.
laziness makes easy things easier, hard things harder :-(
I'd say study both a little bit and then decide which one to specialize in. Though I'll say one thing: Scheme is a nicer Lisp than Common Lisp, and [Racket](http://racket-lang.org/) is a pretty good Scheme system.
`TLambda` is an odd choice of name for a function type. I was thinking type-level lambdas which would be a much more interesting type system for type inference. Also, this is essentially tying the knot, a well known solution for the AST typing problem, made arbitrarily extensible using the comonadic structure.
how does this compare to [ghcjs](https://github.com/ghcjs/ghcjs)?
Seems sensible. I probably should just get over my dislike; it really is sometimes quite natural.
That's excellent advice. Thanks!
This sounds extremely ambitious, particularly for an individual. Are you in a position to implement all of this yourself or are you seeking collaborators? Would we use this language for something in particular, or is it intended more as an experimental playground?
Very exciting/ambitious! I look forward to seeing more. What flavor of dependent? And in particular, what are you doing with equality? :) Edit: also, argument type depending on value of result? That seems a bit anti-causal! Can you elaborate more on what that even means?
That's the money question right there!
Something analogous totally be done without laziness in fact (though what you get ends up being a kind of simulated laziness). Instead of programs which directly cause side effects, whence you might accidentally launch missiles from a strictly evaluated conditional, write _codes_ for programs in a syntax (perhaps given by a free monad) and then compile that down to a real side-effecting program. And if you have totality, then Church-Rosser tells us that laziness vs. strictness is not even significant anyway. I'd have a look at Idris...
In fact, if functions in Haskell were truly pure (that is, lacking even the side-effect of partiality), then laziness would be entirely unnecessary. And once you go down the totality rabbit hole, the lazy-vs-strict conflict is reborn in a more fundamental and less incidental form as the difference between inductive and coinductive types. So laziness is not a consequence of purity: it's a trade-off, where we say “We must assume all functions are partial, whence can’t figure out which things are going to terminate, so let’s just assume nothing will.” If all functions must be total and productive and data types are strictly divided between data and codata, we actually _can_ decide (or at least semi-decide) which things will terminate.
Once you're past the super beginner stage, I'd recommend http://www.ling.ohio-state.edu/~plummer/courses/winter09/ling681/barrwells.pdf
With all the mentioned features, will it (or can it even?) have any sort of type inference?
Well, if you don't evaluate, isn't that also laziness? Haskell just makes it the default for all calculations, which makes a few things a bit more elegant (e.g., `when` being implemented as a plain old function).
AoP is a great book that is not even remotely appropriate for learning CT.
D-dawg strikes again
Hello John, to be honest I've already peek inside your configuration and extracted these lines, incorporating them inside my config, but nothing changed. For example, I have io-stream installed globally (not only inside a hsenv project) and when I start writing "System.IO." I would expect auto complete to show also "Streams" as a possible alternative, but it doesn't. I attach a screenshot to clarify. http://imgur.com/G8cqiS8 As you can see in the bottom buffer, there are results of "ghc-complete", so ghc-mod is aware of the completions, just auto complete doesn't pick them. Any idea? 
Update: It worked! I don't know if it's because I've removed yasnippet or because I've added a proper hook to set the ac-sources, but at least now ac complete is showing me all the possibility, included Streams. It's a start! Now I just need to figure out how to do the same but for pragmas :)
I wish I could tell you. GHCJS makes some design choices that in theory should be pretty bad for both code size and performance, but I have not been able to actually build it to do a straight comparison in a long time. FWIW, last time I got it to build, minified GHCJS code was larger than minified Haste code by a factor of 200 or so, but both projects have changed quite a bit since then.
Yeah. SICP is on my reading list. Planning to learn scheme at some point but my question was whether to learn lisp or haskell.
It sounds quite interesting. I'd love to know more details. (That's where the devil is, after all.)
Collaboration is welcome! As this is sort of my pet project, I'll want creative control over the minimal core, but help is still appreciated, and when it comes to the standard libraries, I'm going to need a lot of help anyways! So, willing collaboration would definitely be appreciated. Of course, at this stage, with no implementation yet, the only thing we could potentially collaborate on is the overall design, and I have a pretty clear vision of that. :) Regarding possible uses, I see it just as an experimental playground, but if someone else wants to use it for "real" work, I'm happy to support their use case in the standard libraries (as far as is reasonable). EDIT: fixed some grammar
It will have type inference! In fact, my intuition is that the total sublanguage will have *complete* type inference (because it is principally typed by virtue of subtyping). The full language will be only partially inferrable, but I anticipate much of people's work falling within the total sublanguage anyway, as it does in most current languages. None of that is proven, of course; it's just my very strong hunch. EDIT: [It turns out my hunch may have been inaccurate.](http://www.reddit.com/r/haskell/comments/1hufb1/introducing_spellcode_a_first_look_at_a_new/caz2hjh) I'm now no longer sure that complete type inference is at all tractable, although I'm still sure that type *checking* is doable, if hard.
If you're looking for an implementation of Homotopy Type Theory, this isn't it. :) I've developed my own type-theory-esque thing to describe Spellcode's semantics. It's much less minimal than MLTT or HoTT, but I'm not aiming for it to be a mathematical foundation, just a straightforward basis on top of which to build a language. Regarding equality, I plan to have `Eq` and `Unifiable` constraints, and total functions with equatable/unifiable parameters and results will themselves be equatable/unifiable. EDIT: Regarding backwards dependent references (or "inverse pi types"), this is because the contexts in which that is a valid construct are both non-total (so that I don't have to worry about screwing up totality guarantees) and sufficiently information-preserving to guarantee a (possibly non-total) inverse to every function. This is why Spellcode handles copartiality and bipartiality as well as standard partiality, because some contexts allow you to invert functions and the inverse of a partial function is a copartial function (or a partial cofunction, but Spellcode isn't built on cofunctions :) ).
An ounce of prevention is worth a pound of cure.
The actual inference step (for the total sublanguage) is pretty much trivial. I just infer the type `singleton x` for the expression `x`! The reason this only works in the total sublanguage is because `singleton`'s argument must be total (because I can foresee some inconsistencies in non-total singletons, but I would be happy to be proven wrong about that). Inference for the partial language is obviously much more hairy, and I don't know how much of the language beyond the total sublanguage will be inferable.
So, knowing x : X and y : Y, what do you do with if c then x else y ?
Similarly, how does it compare to Fay?
 if c then x else y :: singleton (if c then x else y) :) Seriously, though the answer is something like: ifThenElse :: forall i X Y where (i :: Level) &amp; (X :: Type i) &amp; (Y :: Type i) in (c : Bool) -&gt; (x : forall where (c == true) in X) -&gt; (y : forall where (c == false) in Y) -&gt; (singleton x | singleton y) (Yes, the two different type annotation styles `:` and `::` are intentional, and they mean subtly different things.) EDIT: I just thought of an even more precise type for `ifThenElse`: ifThenElse :: forall i where (i :: Level) in (c : Bool) -&gt; ((forall X where (X :: Type i) &amp; (c == true) in X -&gt; X) | (forall Y where (Y :: Type i) &amp; (c == false) in Y -&gt; Y)) EDIT 2: In general, pattern matching in Spellcode will produce this form of type: (v : V) -&gt; ((forall X where (v == case1) in X -&gt; X) | ... | (forall X where (v == caseN) in X -&gt; X))
We have Haste, GHCJS, UHCJS and Fay. What would people recommend to someone who'se sick of writing javascript code?
I'm not sure I understand what you mean. You lost me. Can you give examples? If something recurses infinitely, is that considered being partial? Also, laziness isn't only about partial functions. If we just say x = 0 y = 234532263663^485859483938 main = print x There is no partiality, but laziness is still useful. Also, if you think infinite recursion makes a function partial, we can decide when things will terminate, but that's kind of obvious and is true in any language. If not, we can't. 
GHCJS and Haste have more in common than either with Fay. They both compile the STG ([the ready-for compilation version of Core](http://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/StgSynType)) to JS using the GHC API. Fay compiles a Haskell AST to JS and doesn't use the GHC API. The Haste author's critiques of GHCJS from his [original paper](http://ekblad.cc/hastereport.pdf)—that it's badly documented, outputs huge code, doesn't use an intermediate tree inbetween the STG and JS text—are now out of date or inaccurate. What differences remain now are in the code generation and philosophy. Like Fay, I think that Haste does not try to or pretend to be GHC, whereas GHCJS tries very hard to be able to compile anything that GHC can compile, including any number, IO, encoding, threads, and exception handling type stuff. Haste is a little more like Fay, leaning towards supporting a bare minimum of primitives that are required to get the code spat out by GHC on a simple Prelude to run. If I were going to rewrite Fay (and have prototyped such in private), I would end up writing something like Haste. Correct me if I'm making an inaccurate characterization of Haste, but here're the advantages and observations I have for compiling to JS that Haste satsifies in part: * Core, or the STG, give you type classes and all the GHC goodness for free. * GHC != Haskell 2010. In otherwords, GHC's definition/concept of the Prelude, how to run Haskell, packages, etc. aren't the only or appropriate way. * For example, I don't care about supporting GHC's notion of how runtime should work or Haskell's notion of what a file system is. I accept that I'm in a browser and that primitives are different. I would have a proper Haskell prelude, but I'd make Text the default text type for all IO operations, etc. * The STG gives you saturation of args and tail-recursion guarantees that you don't have to figure out yourself like Fay does, and like you would have to do from Core. * Being tied to the GHC API isn't that bad. * I would retain a trivial-to-write and powerful FFI a la Fay, rather than the disappointingly verbose API in GHCJS and Haste. I think the following are relevant factors in the interest/adoption of GHCJS, Haste and Fay: * Fay is very simple, its output is simple, run-time behaviour is simple, its FFI is straight-forward and powerful, it's easy to install (no funny GHC API dependency or package weirdness), its output is tiny. That's why a lot of people have tried it (and why some, e.g. FP Complete use it in production). It doesn't have type-classes or any extension built-upon that, though. That's why it's not a long-term solution in its current state. Adam and others are working on using haskell-type-exts to sort this out. * GHCJS is very feature-complete, it can compile a lot of GHC Haskell code. That's why it's a long-term solution, but its output is also kind of daunting, for a while it was difficult to judge its reliability to be used in production, that has been in flux for a few years, and its output is historically huge, but it has been reduced a lot. It's still an undertaking to install. VM and all. Luite and Hamish are working on reducing the friction. * Haste, I think, simply has not been very well advertised and it hasn't been explained well the difference between itself and GHCJS. It's (seemingly) been a one-man project for a while, and its presentation hasn't (historically) been forth-coming about showing what real code output is like, online demo files, or something like a test IDE. Fay had all those things which (I think) made it more immediately palatable as an “yeah, I pretty much get how this works and the scope of it” and "I can use this to make stuff". I've read the paper but I still don't have a hold on what its output is like, how stable, etc. A paper gives off pheremones of academic wankery (apologies for the term) —which is fine—but not much "serious business industry" confidence. Its compiler code is simple, though. I've been tempted to make another compiler in the spirit of Fay that will compile from STG, but before I ever do that in a committed way I'll be evaluating Haste. I look forward to seeing the non-GHC-dependent work done on Fay, I'm looking forward to seeing more adopters of GHCJS and experience reports, and I hope someone can give some good marketing for Haste which seems to be the only thing it's really lacking IMHO. I pinged Luite and Hamish to chime in and give their perspective (and correct anything I said that was false).
Thanks for the detailed answer. I was asking as I'm currently using Fay for some client-side web-programming (at work, though the sites are internal), and never even heard of Haste before. I know of GHCJS, but haven't actually tried it, and the way FFI works sounded horribly complicated when Hamish tried to explain it to me. The Fay FFI is really simple and powerful (even if unsafe), I would say that's its strongest selling point by far.
As I read more and more... I pretty strongly agree with you.
You could be right, which is why I made a point initially of saying that I wasn't sure if my hunch would actually work out. The basic idea I had is that you simply infer `singleton` types for the total parts, require type annotation for any non-total parts, and then the onus is no longer on the type *inferer*, but on the type *checker*, because `singleton x` is guaranteed to be the principal type of any total `x`. As you can see, I'm still working out some details of the interactions between constraints, so it is entirely possible that either type inference, type checking, or both ends up being intractable in general. :(
As I understand it, the new GHCJS features a much improved FFI convention, inspired in part by Fay. http://weblog.luite.com/wordpress/?p=14 Note that thanks to JMacro, variable substitution and naming in the FFI is hygenic as well, and you also get compile-time syntax checking of FFI bindings.
In this case I think the recommendation is spot on. L&amp;S is very careful to give a good operational understanding with lots of pictures and examples. It feels almost painfully verbose if you have any exposure to the topic, but even if you're just starting out with category theory and/or don't have much "mathematical maturity" and familiarity that that style, it gets you in to the point where you can actually start reasoning about some very important categorical constructs.
Yes for tail call optimization. I don't think there is automatic memoization, but often writing the memoizing version is not too difficult. 
You are reinventing the stream fusion trick that `vector` uses internally. `vectors` are represented internally as generating functions, and `fmap` on that generating function is O(1), too. The only difference is that what `vector` is doing is optimized and can also implement more sophisticated transformations, too, like filtering and concatmaps. I recommend you read this paper to learn more about stream fusion and how it works: http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.104.7401 I also recommend you study `vector`'s stream fusion implementation for reference, which you can find here: http://hackage.haskell.org/packages/archive/vector/0.10.0.1/doc/html/Data-Vector-Fusion-Stream.html
Cool, thanks. I figured this had probably been done before.
Good summary! I agree that Text should be the default, I've considered making the switch... I think one unmentioned, and very important feature when compiling to JS is to be able to call Haskell from JavaScript. This is easier for Fay since the output is close to JavaScript. It's useful if you have an existing JS code base and you want to start to plug in Fay modules (rewriting everything is probably not viable), or if any Haskell libraries emerge that we want to distribute without forcing users to write Haskell. Doing this from the STG is harder since user definitions may be transformed into something garbled. Right now (I assume for all the compilers) you need to have Haskell on the top level. Fay doesn't handle this very well yet but it something I'm currently working on (yay vacation). Produce proper JS modules and add a strictness wrapper. From JS you should be able to call Main.f(x,y) instead of _(_(Main.f)(x))(y). Also, since Fay's output is simple and close to both the original Haskell source and what a JS implementation would look like it's very easy to just add a breakpoint in the browser at some function definition and start debugging. Haskell stack traces for free? :) Especially useful when debugging FFI operations. I usually do something like fay --html-wrapper --pretty Test.hs. 
In that case your version is better. The vector stream fusion trick assumes linear traversals of the vector so random access to the vector becomes O(N) on average (which you pay once) because you must generate all elements preceding the accessed element. In your case, since you require random access to do a binary search you are better off using the version you proposed.
You mostly answered yourself in the next-to-last paragraph. The type checking + purity in Haskell keeps things from getting brittle or error prone nearly as much as C. They also enable parallelism much more readily. Lazy evaluation means much less stack-blowing. Infinite lists and generators means you have some easy approaches that wouldn't be easy in C. Turing completeness means there aren't things any complete language does that can't SOMEHOW be done in another. However, different languages are better at expressing different approaches and algorithms and with different efficiencies. Haskell is very good at expressing recursive functions, pure math functions, and composing functions - which helps keep them from getting brittle and error prone. For the super-classic example, [compare the Haskell implementation of quicksort](http://www.haskell.org/haskellwiki/Introduction#Quicksort_in_Haskell) with how you would go about doing that kind of recursion in C. Bonus: Notice that the haskell implementation would work on all members of the "Ordinal" typeclass, not just ints. 
Do you have any idea why fmap was taking O(n^1.5 ) time in my tests on regular vectors? All I did was fmap a function and then print the last element. I assume something in the GHC runtime must be going bonkers. The usual culprit is the garbage collector, but it doesn't seem like would have much to do in this case.
That I'm not sure about. It should be O(N) in the worst case. How are you generating the vectors?
So if you write your function very slightly differently, it's trivial to automatically memoize it. -- "div" is integral division; "/" is reserved for non-truncating division -- such as for rationals/reals. -- -- the compiler will probably notice the common subexpression of -- "div n j" and eliminate it, but it's easy to lift out into a "where" -- block if you want to guarantee that. ff :: (Int -&gt; Int -&gt; Int -&gt; Double) -&gt; (Int -&gt; Int -&gt; Int -&gt; Double) ff rec n k j | j &lt; 2 = 0 | otherwise = (1 / fromIntegral k) - rec (div n j) (k+1) (div n j) + rec n k (j-1) -- same as the naive C implementation f1 :: Int -&gt; Int -&gt; Int -&gt; Double f1 = fix ff -- also the same f2 :: Int -&gt; Int -&gt; Int -&gt; Double f2 = ff f2 -- use memoizing tables from Data.MemoCombinators as quchen did f3 :: Int -&gt; Int -&gt; Int -&gt; Double f3 = Memo.integral $ \n -&gt; Memo.integral $ \k -&gt; Memo.integral $ \j -&gt; ff f3 n k j edit: added `f2`, renumbered old `f2` to `f3`.
 fromList [1..n]
Then try the following two things: First, try using `fromListN n [1..n]` instead. Second, instead of using `last`, try using `v ! n`, where you reuse the same `n` you used to build the vector. The reason I'm suggesting this is that the implementation for `last` is: last v = v ! (length v - 1) Normally vectors have a length field where they store the vector's length for O(1) length queries, but if you initialize a vector using `fromList` then it can't populate the length field until it knows the list's length. When you use `fromListN` you tell the vector that ahead of time, which leads to a much more efficient construction, too. The stream fusion implementation also uses length hints like these to make certain operations more efficient. So I suspect that either one of the two changes I mentioned will give you linear performance and probably better constant factors, too.
&gt; This is why Spellcode handles copartiality and bipartiality as well as standard partiality Forgive my incredulity, but I have never heard these two terms. Where have you seen them before?
The issue that foiled the last attempt is most likely solved by now - however, I haven't been able to verify it yet since reactive-banana didn't want to build due to https://github.com/HeinrichApfelmus/vault/issues/9 last time I tried.
Yeah, I wrote [a guide for it](http://chrisdone.com/posts/uhc-javascript) and hosted that prolog app for a while. My experience of it was that the code output was large, like GHCJS, and that it was slow, and that it was a bit buggy (a simple loop that wrote to an IORef did not update the value, I determined a buggy thunk but the output was hard to follow for me at the time), also the FFI was equally as clunky as GHCJS and Haste. One thing going for it is that compilation to JS is more native than GHCJS, but that's a double-edged sword, because not being GHC is a problem in itself.
Of course, a good programmer will first check the [Wikipedia page on the Fibonacci series](https://en.wikipedia.org/wiki/Fibonacci_number), which tells us there's a closed form, and implement *that* instead: fib :: Integer -&gt; Integer fib 0 = 0 fib 1 = 1 fib n = round $ (phi'n - psi'n) / sqrt 5 where phi = (1 + sqrt 5) / 2 :: Double psi = 1 / phi :: Double phi'n = phi ** fromIntegral n psi'n = psi ** fromIntegral n main = do print $ fib 100000000000000000000000000000 This baby is O(1), at least until the bigint overflows the native integer type. tobias@yemaya:~/test/ &gt; time runghc fib.hs 179769313486231590772930519078902473361797697894230657273430081157732675805500963132708477322407536021120113879871393357658789768814416622492847430639474124377767893424865485276302219601246094119453082952085005768838150682342462881473913110540827237163350510684586298239947245938479716304835356329624224137216 runghc fib.hs 0.36s user 0.05s system 96% cpu 0.426 total Not bad for 5 minutes of googling, eh? 
Good additional points re calling into Haskell and debugging. Source maps is another place that Fay would shine where the other two would find it difficult.
Good summary! I can only add that Haste aims for GHC compatibility where there is no reason not to. There's no reason to not have type classes, ByteArrays, GHC type extensions and so on, so Haste should definitely have them - the more libraries that can be reused the better! However, there *are* reasons (not least efficiency and code size) to not do full tail call elimination, preemptive multitasking, home-grown garbage collection and the other things GHCJS does to achieve maximal GHC impersonation, so Haste does not do that. If one wants to go that route, then I believe using Emscripten (or something like it) together with the GHC LLVM backend to compile straight Haskell to asm.js is a better option. Haste will likely be moving in a more Fay-like direction (for instance, I'll be reimplementing base with the parts that don't make sense for the browser either stubbed out or removed entirely) from here on, keeping compatibility with (hopefully) a fair number of packages on Hackage but getting rid of the current warts. If you do evaluate it, I'd appreciate hearing your thoughts on it (except the compiler driver in Main.hs - I know it's one ugly beast). There are very few "holy cows" (apart from some more "academic wankery" I have planned for next year :), and I'd really like more input on where Haste should be going.
Well this is embarassing, but I forgot to push the updated criterion tests I used to github, so I was looking at an old version. Here's what I really did. I made sure vector run time generation wasn't tested by fully creating all the vectors before starting criterion. Inside the actual benchmark, all I did was fmap and evaluate to whnf. vfl :: Int -&gt; IO (V.Vector Int) vfl exp = G.generateM (2^exp) (\i -&gt; return i) main = do vfl00 &lt;- vfl 00 vfl01 &lt;- vfl 01 vfl02 &lt;- vfl 02 ... up to 20 defaultMainWith myConfig (return ()) [ bench "Vector-fmap-00" $ whnf (fmap (+1)) $ vfl00 , bench "Vector-fmap-01" $ whnf (fmap (+1)) $ vfl01 , bench "Vector-fmap-01" $ whnf (fmap (+1)) $ vfl02 ... ] 
Regarding the cache misses mentioned at the end: from a naive perspective it should be sufficent to merge the two vectors into one vector of a type, which has an unboxed Int field and a field of the element’s type. This way, both the Int and the element should be together in cache almost all the time. Do I miss something?
&gt; However, there are reasons (not least efficiency and code size) to not do full tail call elimination, preemptive multitasking, home-grown garbage collection and the other things GHCJS does to achieve maximal GHC impersonation, so Haste does not do that. Right, I got that impression. &gt; If one wants to go that route, then I believe using Emscripten (or something like it) together with the GHC LLVM backend to compile straight Haskell to asm.js is a better option. That was the very first approach I tried a couple years ago, but it seems difficult, because while Emscripten was able to compile the LLVM output from GHC, you still have to compile the GHC runtime to JavaScript, or re-implement it, a task that was being my confidence to do. &gt; Haste will likely be moving in a more Fay-like direction (for instance, I'll be reimplementing base with the parts that don't make sense for the browser either stubbed out or removed entirely) from here on, keeping compatibility with (hopefully) a fair number of packages on Hackage but getting rid of the current warts. If you do evaluate it, I'd appreciate hearing your thoughts on it (except the compiler driver in Main.hs - I know it's one ugly beast). There are very few "holy cows" (apart from some more "academic wankery" I have planned for next year :), and I'd really like more input on where Haste should be going. Cool! Looking at the compiler, I had a pleasant feeling of knowing what's going on in most places and it's really pretty much what I'd write, I'll certainly I'll let you know what I think, and I'll be coming at it from a use-this-in-industry angle. Congrats on getting it up on Hackage. :-)
Yeah, I tried that, and the code was about 20x slower. I think what was happening is that the value was getting "double boxed." Once for the `Int,element` pair, and once for the `element` itself. What it really needs is a "partially boxed" vector as the base, where there's an unboxed int followed by a pointer to the regular box. I don't think this is possible in GHC though.
Also if you want to prevent any precision error you could use the numbers library. Remark, it takes way longer than the approximative version. 1. `cabal install numbers` 2. import System.Environment (getArgs) import Data.Number.CReal fib :: Integer -&gt; Integer fib 0 = 0 fib 1 = 1 fib n = round $ (phi'n - psi'n) / sqrt 5 where phi = (1+ sqrt 5) / 2 :: CReal psi = 1/phi :: CReal phi'n = phi ** fromIntegral n psi'n = psi ** fromIntegral n main = do numbers &lt;- getArgs mapM_ (print.fib) (map read numbers) For "only" 10000: localhost ~/tmp time ./fib 10000 33644764876431783266621612005107543310302148460680063906564769974680081442166662368 15559551363373402558206533268083615937373479048386526826304089246305643188735454436 95598274916066020998841839338646527313000888302692356736131351175792974378544137521 30520504347701602264758318906527890855154366159582987279682987510631200575428783453 21551510387081829896979161312785626503319548714021428753269818796204693609787990035 09623022910263681314931952756302278376284415403605844025721143349611800230912082870 46088923962328835461505776583271252546093591128203925285393434620904245248929403901 70623388899108584106518317336043747073790855263176432573399371287193758774689747992 63058370657428301616374089691784263786242128352581128205163702980893320999057079200 64367426202389783111470054074998459250360633560933883831923386783056136435351892133 27973290813373264265263398976392272340788292817795358057099369104917547080893184105 61463223382174656373212482263830921032977016480547262438423748624114530938122065649 14032751086643394517512161526545361333111314042436854805106765843493523836959653428 07176877532834823434555736671973139274627362910821067928078471803532913117677892465 90899386354593278945237776744061922403376386740040213303432974969020283281459334188 26817683893072003634795623117103101291953169794607632737589253530772552375943788434 50406771555577905645044301664011946258097221672975861502696844314695203461493229110 59706762432685159928347098912847067408620085871350162603120719031720860940812983215 81077282076353186624611278245537208532365305775956430072517744315051539600905168603 22034916322264088524885243315805153484962243484829938090507048348244932745373262456 77558790891871908036620580095947431500524025327097469953187707243768259074199396322 65984147498193609285223945039707165443156421328157688908058783183404917434556270520 22356484649519611246026831397097506938264870661326450766507461151267752274862159864 25307112984411826226610571635150692600298617049454250474913781151541399415506712562 71197133252763631939606902895650288268608362241082050562430701794976171121233066073 310059947366875 ./fib 10000 10,85s user 0,29s system 99% cpu 11,137 total localhost ~/tmp ./fib 10000 | wc -c 2091
Haskell's syntax is highly expressive (readable AND thankfully short). For example, `where` expressions can be appended to most anything, so you can start writing code without having to fill in all the details at once. Your code ends up being more mathematically robust and easier to type and read. I breezed through the first dozen Project Euler code assignments with Haskell. It's like cheating!
Huh, I totally forgot about that package. I think the issue is fixed in the devel branch, I'll have a look again and make a release. 
I don't think lazy-evaluation would do too much good, I can't think of any instances where I use lazy-eval to turn down the worst time complexity of an algorithm in my quest to even solve problems in [Rosalind](http://rosalind.info/problems/locations/). I have given five minutes to run my program and it still fails when I don't have a good algorithm. In program competitions, it's usually 3 seconds, and the input size is even larger.
But, you can implement lazy evaluation in Java and C++ just as easily. (Btw, that's an easy and safe optimizing technique.) These aren't technical competitions, the goal is to solve the problem in a reasonable time. Just generating the solution space and testing that will only get you so far. It's not that magical.
The only application where such a representation makes sense is if only a small subset of the elements is ever accessed per `fmap` operation, these reads and writes are random accesses, and concatenation is not used. A more economic representation would be to apply the fmaped function to all elements as soon as one is accessed, or to apply it to a prefix of the array.
Well, what I was saying is that those languages already utilize so many algorithms by themselves, so sometimes you can reuse those algorithms (in a hacky way) without implementing any algorithms by yourself. I'm not saying lazy evaluation is a dark magic that needs to be banned.
Some notes: 1. I'm confused why you need `unsafePerformIO` at all; you only use it in the MVector implementation (and `forceElement` is wildly unsafe in the pure vector case, which was what had me wondering about your code in the first place--then I noticed you don't do anything there) 2. While still needing some unsafeCoerces, you can make the code wildly more typesafe with a GADT for your function list. I suggest something like this: . data FL a b where FLNil :: FL a a FLCons :: (b -&gt; c) -&gt; FL a b -&gt; FL a c data LazyController a = forall src. LazyController !Int (FL src a) instance Functor (FL a) where fmap = FLCons instance Functor LazyController where fmap f (LazyController n xs) = LazyController (n+1) (fmap f xs) takeLC :: Int -&gt; LazyController a -&gt; LazyController a -- can write this without unsafeCoerce appListSafe :: FL a b -&gt; a -&gt; b appListSafe FLNil a = a appListSafe (FLCons f xs) a = f (appList xs a) Now you only need two calls to `unsafeCoerce` in the code that extracts from the vector and calls appListSafe; once for the element and once on the embedded `FL` since you know its type is correct.
The problem comes in when you start doing multiple maps. `k` maps later every access takes an extra O(k) factor. This is a useful trick, but it is also important to note when and where you are accumulating these `Yoneda Vector` calculations for single or multiple uses.
Ah - yeah, this is very useful. Thank you.
This was exactly the sort of feature I was wondering about. Looks like I'm going to need to do some more reading up. This is a great reference.
This is great. I guess now I'm going to need to find some time to wrap my head around the tools better. But this is a really good roadmap to get me going.
What is the use case for argument types depending on result types? 
I've seen "copartial" here and there, but not often, and it's entirely possible that I've been misusing it. I created "bipartial" to describe the case when a function is both partial *and* copartial. Roughly: - total = `a -&gt; b` - partial = `a -&gt; (b | singleton bottom)` - copartial = `(a | singleton bottom) -&gt; b` - bipartial = `(a | singleton bottom) -&gt; (b | singleton bottom)`
Of course! :) What would you have called this post?
Just be careful with that notation. Even though intuitively a "Partial A" has all the objects with type A plus _|_, you can't decide between the two. You can simplify your design significantly by making use of a formal Partial type constructor. codata Partial (a : Type) : Type where Now : a -&gt; Partial a Later : Partial a -&gt; Partial a Then, a partial a -&gt; b is actually just a -&gt; Partial b, and similarly for the others. Doing this, you only have to make sure you handle coinductive types correctly (which is hard enough by itself).
I understand your point of view, but in my experience it's rare to come up with something that's easier than actually doing your work. You sure need to understand a lot more, so it compensates. The craziest thing I regularly encounter is turning a sorting algorithm into a sub-sorting-complexity selection algorithm. I find that pretty hard to do with imperative code. I think the type checker provides the way bigger advantage of not being able to write nullpointer bugs.
Maybe you're thinking of the [Benchmarks Game](http://benchmarksgame.alioth.debian.org/), which had to rewrite a bunch of its benchmarks since they involved busy-work that Haskell refused to do? ([Source](http://www.mail-archive.com/haskell@haskell.org/msg18863.html)) I don't think this would be relevant to programming contests though ...
More than Vector, it sounds like REPA's delayed model (possibly with some DiffArray thrown in). The usual problem with representations like this is getting predictable performance from it. This is why REPA threw all the type indices onto their vectors.
Wait, we're talking about inverting functions now? How do you do that automatically, and if it isn't automatic, why does the language need to know about it? Can we get some concrete examples?
Haskell shouldn't be that much slower. Any time you have a problem where you have hit an optimization wall with you should submit it to Stack Overflow. They are very good at teaching you how to optimize Haskell code.
That's immaterial. No non-empty type would contain bottom, so the union and disjoint union coincide.
/u/kamatsu is right, and it *does* make a difference. Because there is no equality defined for bottom, you cannot, in general, tell which side of the union any given value is on. This property is indeed broken with a sum type, which is why I used union types instead.
[This exercise might help](https://github.com/tonymorris/course/blob/answers/src/L10/TicTacToe.md), which has been completed by many with some assistance.
I disagree. The absence of assistance with errors may feel easier to the student, but it isn't. The student will report that it is easier, but this is unreliable.
Sounds kinda like Repa, in fact, I can't see any situation where you'd use this library instead of Repa.
I didn't know REPA had anything like that. I'll have to look into it more carefully. I thought it was all about parallelism.
1) I probably could get it to work without `unsafePerformIO`, but I couldn't figure out how to make it type check because I don't know whether I'm in the ST monad or the IO monad. 2) That does look a little bit better. I just didn't even look for a better solution once I realized it would need `unsafeCoerce`.
[This paper is a nice intro](http://www.cse.unsw.edu.au/~benl/papers/guiding/guiding-Haskell2012-sub.pdf). Specifically, your vectors look like delayed repa arrays.
You might like "Categories and Computer Science" by R.F.C. Walters. http://books.google.com.au/books?id=QBjAZ_HwYZMC It was an undergrad (2nd year Pure Maths) text at Sydney University. 
I guess one question would be "Why is GHCJS not in Hackage?". It will be, but we are targeting GHC 7.8 which should include patches we need to support the new JavaScriptFFI extension, Template Haskell and to get 32bit ints when GHCJS is built with 64bit GHC. In the mean time if you want to [this](http://parenz.wordpress.com/2013/06/12/ghcjs-build/) should get you up and running. Hop on #ghcjs if you have questions. I feel like too much emphasis is put in this thread on characterising GHCJS's based on a particular code generator. While it is true only one of the code generators (Gen2) is being worked on (it is the mix of features and performance Luite and I are looking for) and the other two generators are probably not worth reviving (they were much slower and have bit rot), there is nothing to stop someone adding more. Also if anyone thinks the structure GHCJS can be improved and has the time to help, then that would be fantastic. There are 3 different FFI solutions in GHCJS * JavaScript shims for C function calls * The new JavaScriptFFI extension * JSC (Haskell EDSL that also works for native apps using webkit-javascriptcore) If someone (I'm looking at you Chris :-) ) would like to add another that would be great. I think there might be technical difficulties making Fay's FFI style work with STG, but it would be great if it could be done. In short if anyone is planning to build a code generator for JavaScript based on STG then I would recommend joining GHCJS. Some of the potential benefits of joining forces: * Shared post JS conversion linking and optimisation * Shared tests * Shared libraries * Shared build and test systems valderman, I understand you had reasons at the time not to do this, but I hope now that your thesis is done, you reconsider and take another look at GHCJS. A Haste generator might be a nice addition and I am sure you could find ways to improve GHCJS based on your experience with Haste. If anyone is looking for something interesting to do ("academic wankery" as Chris put it). We need help with * Improving constant folding in the Gen2 generator * Lazy loading of the JavaScript code is an interesting problem (the pre-Gen2 solution could be ported over to Gen2 or you could work out something better) 
How will you know what the total parts are, so that you can infer a singleton type?
MVector operations always take place in some PrimMonad, so you can always use ST: type PRef m a = STRef (PrimState m) a newPRef :: PrimMonad m =&gt; a -&gt; m (PRef m a) newPRef a = primToPrim $ newSTRef a readPRef :: PrimMonad m =&gt; PRef m a -&gt; m a readPRef r = primToPrim $ readSTRef r writePRef :: PrimMonad m =&gt; PRef m a -&gt; a -&gt; m () writePRef r a = primToPrim $ writeSTRef r a `primToPrim` just unwraps and rewraps the newtype wrappers around ST and IO.
I'm currently working on a celestial mechanics project that computes the orbits of planets in Haskell. I started out in C but quickly moved to Haskell and it was definitely the right choice. Here are some of the reasons: * You can divide by zero! As long as you don't evaluate the result. You don't have to guard every potential division by zero with an if-then-else block. Lazy evaluation pays off here. * Nice type system, flexible but strict numerical types. No need for C++-style templates if your code needs to work with both, floats and doubles. * At best Haskell is like writing ASCII art math. * Tuples and lists. In comparison with C, you can easily have "multiple return values" in functions, etc. No need for ad-hoc structs. * QuickCheck. Makes simple testing simple. Even though I'm writing numerical code, it's fairly easy to write useful QuickCheck tests that actually reveal problems.
Types don't *only* assist with errors. They also constrain you to program in certain ways. We would all agree they are reasonable/good constraints. But when learning, it is harder to learn more things at the same time. Going type-less allows you to learn at your own pace, and write the program in ways a type-checker would reject. Also, concrete errors reported by stack traces are easier to learn to read than type errors. The whole type language and vocabulary have to be learned to read them. Whereas with stack traces, the language used is the same one used in the ordinary value world the student already knows.
I agree that types constrain you to program in certain ways; exactly those ways we want the student to program. No only this, but it becomes impersonal. The student is arguing with a type-checker, not a person. The benefits of this are immense. In my experience, going type-less allows you to pretend you are learning, when in fact, either you are not or you are doing so extremely inefficiently.
[Hackerrank](http://Hackerrank.com) supports Haskell (currently ghc 7.6.2, with the batteries, I think). They have contests that run for over a week.
Moreover, some of the biggest competitions in Russia—the Open Olympiad and IOIP—also allow Haskell.
Have you checked out [haskell-packages](http://hackage.haskell.org/package/haskell-packages)?
We have been allowing competitors to use Haskell in the Flemish Programming Contest (http://vlaamseprogrammeerwedstrijd.be), but we use quite liberal time limits (in the order of 10-30s), so people who have some experience are usually able to code and make the limits. It's also used/allowed on e.g., SPOJ (http://www.spoj.com). Disclaimer: I am a member of said contest's jury. And no, I do not have anything to do with the website :) 
The ICFP Programming Contest is to my knowledge a highly-regarded annual contest — not to mention one of the more original, in terms of the problems presented! It's sponsored by the ACM, and most definitely allows Haskell. Even though it's tied to a functional programming conference, it's open to all programming languages (and is often won by teams using imperative languages).
Depending on which is the default, the total parts are either the ones explicitly demarcated as total, or the ones *not* explicitly demarcated as non-total.
I bet you're the type of person who reads the "Hello, World" example and says "I don't need to do that, I already know what the output is".
The Lisp equivalent of such a "Hello, world!" is where you just write `"Hello, world!"` at the REPL...
Why? VecElem only unpacks the Int, not the a
&gt; You can divide by zero! As long as you don't evaluate the result. You don't have to guard every potential division by zero with an if-then-else block. Lazy evaluation pays off here. Could you give an example where you have a division by zero but are not evaluating it later?
Which is why `VecElem` can't be an instance of `Unbox`. Maybe there's a way to trick the compiler into doing it, but I couldn't figure out how.
If you "delay" all the elements, then that has no performance cost, so i'm not sure what the issue is. Of course, if expensive computations get fused into the delayed array, then sure. 
Uh, lazy evaluation does not "transform" algorithms any more than strict evaluation does. Lazy evaluation specifies an order in which to evaluate expressions (outermost-first), and specifies that work spent on evaluating a bound variable shouldn't be repeated. Strict evaluation just makes another choice about the order in which to evaluate expressions (innermost-first). If you can rely on the compiler to implement lazy evaluation, then the performance is entirely determined by the program that you write. You can encode any algorithm that you like, and even arrange that the steps occur in whatever order you want. It's only a "transformation" if you expect everything to be evaluated innermost-first and then get surprised when they're evaluated outermost-first instead! Of course, GHC doesn't implement only lazy evaluation, but will evaluate some expressions in another order when it can prove that being stricter will be beneficial and not cause nontermination. But lazy evaluation is still almost always sufficient as a mental model of what sort of performance you'll get (I've been programming mostly in Haskell for over a decade and have only run into a small handful of programs in that time where optimising a program's performance required understanding the difference between what GHC does and lazy evaluation.) The IO monad is neither required nor intended for transliterating imperative programs. It's used, as its name suggests, to describe I/O effects to be carried out. If you're not actively trying to do input or output of some sort, you don't need it. If I want to translate an imperative program to Haskell, what I do initially is to turn each line of the algorithm into a function, and all mutable variables in scope into function parameters. I represent control flow by simply having these functions call each other, and variable updates by supplying different function parameters to the function representing the next line of the imperative algorithm. If you want to be completely precise about the order in which things are evaluated, you can optionally make the parameters strict. Then, outermost-first evaluation will ensure that the steps actually occur in the order that they're meant to, and the little bit of strictness annotation will ensure that variables aren't set to unevaluated expressions. In fact, I've often done this translation just in order to understand what was going on in a particularly hairy piece of imperative programming. Once you have the program translated in this crude way to a referentially transparent setting, you can then typically do a whole bunch of easy algebraic substitution and simplify the program without affecting its behaviour, until you're left with what is likely just a single recursive function, or a couple mutually recursive things.
no... Not evaluating under binders is what makes them "functions". Lazyness is just not needed for monads (most of the time). Higher ranked polymorphism and purity are the important features here. Lazyness might help in a few cases. But, the point is that monads seperate out the "evaluation" from the "execution" of a thing. This means that you can have strict user defined control structures just fine.
&gt; Of course, a good programmer will first check the Wikipedia page on the Fibonacci series, which tells us there's a closed form, and implement that instead: And a stupid programmer will miss the point and implement something totally different. 
I wouldn't go as far as to say that Haskell isn't good at algorithms - I think it blows normal languages out of the water if you ever need to do more "recursive" things like parsing or tree processing. The real problem with the programming contexts is that they traditionally include exactly the sort of textbook problems that you would perfectly be able to solve in C++ or Fortran: all you really need is a couple of integer arrays and all the programs are "write once - read never" so you don't need abstractions like templates, generics, type classes, etc. As for runtime predictability in this case its not just about predicting time complexity (lazyness is actually OK at that - what gets tricky is the space leaks and amortized operations) but more importantly about not having to worry about the garbage collector kicking in or a more involved runtime system doing too much fancy stuff (this is why lots of people still choose C++ over Java, even though you wouldn't do that for most normal programs you would write)
Um....your second question raises a very valid point. I'm not sure why I thought I could get away with termination checking without first (or, at least, simultaneously) type checking. The Y combinator will obviously be deemed invalid in a linear or affine context, but in a general (but nonetheless total) context, I'm no longer sure I can just discard certain features and end up with guaranteed totality. In retrospect, it was kind of a stupid assumption. I'll revise my initial post in this thread accordingly. Thank you very much! As before, but even more so now, suggestions welcome!
Yeah, that's probably good advice. :)
I guess what Haskell does with laziness in this context is move the "execution" part entirely out of the Haskell realm and into the runtime. `IO` is just another type, and the language makes absolutely no difference between `IO` and any other type (except for `do` sugar). `foo &gt;&gt;= \x -&gt; if x then bar else baz` need laziness to remain pure and still work as intended - you have to know the output of `foo` to know whether `bar` or `baz` has to be evaluated, and (correct me if I'm wrong) there are only two ways to do this: allowing side effects, or evaluating lazily. Of course that doesn't mean laziness has to be the *default*.
A braindump?
ICPC. Not ICFP.
A heavily filtered and somewhat reorganized braindump. Yup, that sounds about right to me. ;)
Have you asked the author?
Resurrecting an old thread here. Following your help, I got to what I think is a pretty usable solution with a couple advantages over cloaknet (apply to a list of channels instead of all privmsgs, ability to send and receive plaintext). The code is available here: https://bitbucket.org/jhinkle/hsircenc Many thanks to /u/Tekmo, /u/k0001, and /u/grncdr for your help. Any comments on the code are appreciated. Thanks!
&gt; I haven't once been able to get GHCJS to produce code I could actually run in two years of trying, and vagrant is broken on all the three Linux boxes I've tried it on. Give us a shout on #ghcjs or raise an issue next time it happens and I am sure we can get it going. That is where vagrant works particularly well, if we know the ghcjs-build version you are using then the problem should be easy to reproduce and fix. Breakages are often related to changes in GHC HEAD (in which case we can pin ghcjs-build to a known good commit) or new Hackage packages that are not yet compatible with HEAD (ghcjs-build installs patched versions, but if a new version is released to Hackage we may need to update the patched one). There was a bug in a recent release of Virtual Box that caused problems too (4.2.14), rolling back to an earlier version of Virtual Box fixed it and I believe it is fixed in 4.2.16 too. 
&gt; First, because a transform has to be applied by the compiler. Second, because I have to apply a transform in my head to track what gets evaluated when whenever I care about exact evaluation order. Both of these are equally true of strict evaluation order (the compiler is going to transform your code into a language which ultimately has no notion of expressions, so it has to do something to those expressions). You would also need to transform the code in your head from how it is written into a model of what steps will be taken while evaluating it, in order to determine the performance. Ultimately, you need to understand (at least to some extent) the implementation of the language you're using in order to understand performance. This applies equally to strict and lazy evaluators and everything in between. The fact that you might have more experience with strict ones might make it easier for you when working with a strict language, but in absolute terms, things are not so different.
this is not about lazyness. It is about constructing *actions* as code. That code is (probably) not using Haskell's lazy evaluation *at all.* It is instead constructing an action--that you could call this construction "lazy" just confuses the issue because it is not the same lazyness as provided by the language. How can you tell? Well consider \foo -&gt; foo &gt;&gt; foo this *executes* `foo` twice (that is the point!), but in most haskell implementations it only *evaluates* `foo` once. Thus, in ML with Haskell syntax you could define newtype IO a = IO (() -&gt; a) and then use "built in" ML style side effectfull actions to define an IO library which you exposed to the programmer. The programmer could then use only pure libraries (not importing the impure implementation specific ones just like in Haskell) and use monads for all side effects. It would work. Not only that it has been done before. OTH, ML does not have typeclasses. ML does not have higher kinded polymorphism (in the term language). These two features make it impossible to write a function paramaterized over monads! Except, by using the module system. Modules are great, go modules, but they are far two heavy weight for this task. Existential quantification and "functors" (aside: calling standard ML style "functors" functors when they are not even functions should be banned) are a poor fit for something like `mapM` and totally unsuitable for something like `Control.Lens` Haskell is a unique language in the design of its type system (higher kinded polymorphism, impredicative polymorphism, the perfect subsumption relation of universal quantification, type classes, open type functions, etc) . It is also by far the most popular *pure* language. Lazyness is nice. It is important. But, it is not what makes monadic programming possible. 
&gt; there's a log(n) algorithm for exact Fibonacci calculations no there isn't. I harp on this over and over again. But, the size of the output of Fibonacci is $\Omega(log(\fib(n))$ which is $\Omega(n)$. You can't *print out the answer* in less than O(n). Sorry. There are efficent algorithms, but Fibonacci grows fast...
Right, not the same contest. But the ICFP one probably gets more mention on this subreddit.
I haven't looked at the code in detail. Is this Unbox class something that you define, or GHC?
1) * There are thinks that fails, -&gt; enter an issue about that * There are thinks that can be improved -&gt; enter an issue about that * There are thinks you want to have in the library -&gt; enter an issue about that 2) Talk to the author about the opened issues. 3) Fork and fix an issue every time. 4) goto 1)
Some more questions: When a program passes type checking, what do I know? Is it the usual "well-typed programs don't go wrong"? I.e., they don't get stuck in the operational semantic. When you say principal type, do you mean the usual thing? I.e., that the type is the largest possible type (of that expression) in some ordering of types. What is this ordering? 
If this was my project, I would prefer getting a e-mail first. An issue tracker is not the best way for exchanging ideas. And I don't like opened issues that aren't real bugs or arent accompanied by a patch to fix the issue.
I agree with most things, but for all things IO, you still need that error handling, even if your function is declared as :: IO Int, whatever comes in, may not be able to be converted to an Int. No matter how hard you'd like it to. So there at least, you need to deal with this. From the article: "If a function is declared as returning an integer then the compiler won’t let it return anything else." Which does not mean that it cannot bork on the way out if you're in some monad stack with IO at the bottom. 
IO is like a warning sign saying "~~This sign has sharp edges~~ Unreliable code, better wrap me in exception handling."
&gt; no there isn't O(log(n)) arithmetic operations. Your turn.
&gt; I've never understood why people insist on using memoization tables that bloat out memory just to avoid keeping previously generated values around. Memo combinators are trivial to use, and provide a significant improvement in runtime performance even for functions of more complicated types. How would you apply your scheme to a function of two arguments, one of which is a list? Additionally, the memory usage of keeping a list of values allocated vs. having a lookup table provided by the memo library shouldn't be that much different. (And yes, what I presented wasn't a particularly good solution for calculating Fibonacci. That's because Fibonacci wasn't the point, memoization was.)
The eager languages in question are not RT with regards to semantics. Thus they cannot be RT with regards to performance, which is semantics plus even more relevance. Reasoning ability, even if it were an issue, isn't the problem here at all! As alado says, garbage collection, boxing, and rts spinup costs all add minor constant factors that become more problematic in high performance micro-contests.
Yes, but even if you have a monad transformer stack with `IO` at the bottom, you can still prove important properties about your code. For example, let's say that your monad transformer stack is: total :: StateT Something IO Int Now, let's say that I have a subroutine that I use within `total` of type: subroutine :: (Monad m) =&gt; StateT Something m String total = do ... str &lt;- subroutine ... return (length str) That subroutine cannot fail because it is polymorphic in the base monad. So even though I can use it within a function that can fail (`total` in this case) and it is compatible with an `IO` base monad, I know that this particular subroutine cannot be the source of failure since it is not actually doing any `IO`. These are the kinds of guarantees that the type system can give you, even when you are mixing in `IO`. Moreover, because `subroutine` is polymorphic in the base monad, I can quickcheck it purely by testing it using an `Identity` base monad: evalStateT subroutine :: Something -&gt; String
Very cool.
Simon Peyton Jones says "the next haskell will be strict". See here for the slides and a summary: https://news.ycombinator.com/item?id=1924061
I don't use flycheck/flymake, so I can't speak to the code itself. Regarding general convention, we're leaning these days towards documenting everything that's included in haskell-mode in the manual. Regarding user-contributed stuff that isn't ready to be included in haskell-mode, I'd suggest putting it on HaskellWiki. Once the haskell-mode manual surfaces in some complete state, we'll strip and take the manual-ish parts from the HaskellWiki and just leave the user "hey this is my config/tweak" stuff on there (similar: XMonad). It would also benefit everyone if when you make a haskell-mode extension or improvement you a pull request to the haskell-mode wiki with (1) the source, (2) some docs about it for the manual, if it's up to scratch we'll include it in the next release.
I recently added the initial Haskell support to Flycheck, which is indeed much better than flymake. I would be happy to hear any feedback that would help it work for more people. In particular, I believe haskell-mode has some machinery for dealing with cabal-dev etc., and I'm keen to hook that into flycheck.
Excellent intro, I hope a lot of imperative programmers read it. It's dry, sensible, and avoids what Learn You Some Erlang calls "drinking the Kool-aid". If you like a more playful intro (plug), there's [Haskell for the Evil Genius](http://www.yellosoft.us/evilgenius/).
Modulo ⊥
&gt; The eager languages in question are not RT with regards to semantics. Thus they cannot be RT with regards to performance, which is semantics plus even more relevance. Does not follow. With a strict evaluation order, the performance characteristics of a fragment of code are determined by that code. There are no deferred computations that might be required if you use a value for the first time in that fragment of code, as there are with lazy evaluation. The referential opaqueness of imperative code relates to values, not code. There may be global variables. These can affect run-times for specific calls, but the same can be said for parameters. In performance modelling terms, they're implicit parameters - a lot like the values in a function that are referenced via a closure. They don't change the time or space complexity of the code in the way that laziness does. There are ways to break the referential transparency of performance issues, of course - function pointers or late-binding inheritance could be used through some global variable to access code that's specified outside of your fragment. This has to be true or else Haskells referentially opaque performance characteristics couldn't exist - Haskell was (in the past at least) sometimes compiled to C. However, even this code specified from outside has to be explicitly called to have any effect. If you're analyzing the performance of a piece of code, you know what operations are done - including which calls. It's not like Haskell, where this re-ordering is done pervasively and implicitly. You can build a huge data structure (or even a small one) for re-ordering computations if you want, but that's your choice and it must be explicit in the code you write. In Haskell, it's already there by default whether you want it or not. Also, as it's pervasive, there's no single point where you can turn laziness off. Perhaps you think the possibility of deliberately breaking referential transparency means the language isn't referentially transparent at all? By that rule Haskell isn't referentially transparent at all either, not even for correctness. `unsafePerformIO` and similar allow you to break referential transparency. One of those buried deep in the code is as well hidden as a reference to a global variable. But just as using global variables is considered bad practice in most imperative languages, using `unsafePerformIO` (particularly in a non-referentially transparent way) is considered bad practice in Haskell. And incidentally, it's possible to define global mutable variables in Haskell too - that's [one use of `unsafePerformIO`](http://stackoverflow.com/questions/6488703/global-variables-via-unsafeperformio-in-haskell). 
&gt; The bit you missed is that the transform from eager order to strictly sequential order is much simpler the transform from lazy to strictly sequential. I disagree. It's simply a different preference for choosing which subexpression of the expression graph to reduce. Also, lazy evaluators do not in general have a call stack. The stack in GHC is a pattern match stack, not a call stack. That is, its entries are essentially case expressions who are waiting for their scrutinee to be sufficiently evaluated to match one of the available patterns.
It's OK, [Fast And Loose Reasoning Is Morally Correct](http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.pdf).
&gt; Also, lazy evaluators do not in general have a call stack. Once you build that deferred evaluation structure, when you need one of those values, you have to make a call to the operation that was deferred. That operation may need to make calls to further deferred operations. They may need to make calls to further deferred operations. There can be an unbounded depth of nesting to these calls. So yes, there absolutely *is* a call stack, which is used to make nested calls. The key difference, compared with strict evaluation, is the indirection in those calls - they're determined by that data structure for deferring execution instead of using simple direct calls. This is, for example, why tail-recursive code can stack-overflow in Haskell. It's not the recursion you see in the source that causes the stack overflow - it's the recursion in the evaluation of that huge chain of deferred computations. Please note - I'm not calling that a huge problem, I'm just pointing out that the call stack *is* needed for lazy evaluation. In terms of folds over an associative operator, in ML you'd prefer a left fold because that's tail-recursive and works in constant space. A right fold needs O(n) space and may stack-overflow for large lists. In Haskell, the situation is equal but opposite - it's the left fold that needs O(n) space and which may stack-overflow for large lists. Building the huge chain of deferred computations doesn't grow a huge stack, but executing those deferred computations does. I can't help adding - this thread is pretty good evidence of why Haskell isn't good for the kind of performance code. All that deferring of computations is implicit, hidden, so of course people tend to be unaware it. The whole point is you're not *supposed* to worry about this, that's the compilers job - the programmer specifies what is wanted, not how to compute it. There's just this narrow little area where that's a problem, and you *need* to know the precise order of evaluation of operations. 
no - there is truth in this - it kindof gets better with time (just like you learn to read math) - but I too think that you could improve readability by using longer/meaningfull type and variable names
What is a better name for a monad? ("warm fuzzy thing" is even less revealing). monads are very useful -- but also too abstract to be described better by any other name. Calling the Monad class 'Monad' is incredibly descriptive. The fact that most people have not run across that particular abstraction before is not really something that can be fixed by giving it a different name...
Well, maybe is subjetive opinion. But an email don't track a problem resolution, status and is not visible to all the people interested on the library. A mail-list probably is better for that.
It's you, partly. Variable names are not as important as function names, especially when working with generic types and your variables could be almost anything. Take the function head: head :: [a] -&gt; a head (x:_) = x you could name x, firstElement, but it's obvious from pattern-matching that it is. When working with lists, you see x:xs, y:ys a lot and that is a functional convetion that has stuck with the language. If you program in "pointfree style", you actually don't have any variables to name. For example, the sum of the elements of a list is often writen as: sum = foldr (+) 0 There are no variables to name in there. It is very readable if you know what a fold is. And unfortunately, no one has come up with a better names than Monad, Monoid, Applicative Functors, etc, because that is what best describes the abstraction. In general, I find haskell more readable than the best written java code once you understand the abstractions used. 
In object-oriented programming languages, you also have a lot of "names". There it's not Monad, Applicative, Foldable, ... but instead Adapter, Singleton, Observer, Strategy, ... . I think once you have learned them, they are not that difficult anymore. Regarding the one or two letter names, in haskell I find that's not really a problem, as functions are often short and you can see what a variable means from its usage. Often, variables are polymorphic, so there is no real name for them, other than "listOfSomethingEnumerable", so "listOfSomething" is written "xs", where s stands for "more than one x". Nobody likes to write long names, they just distract from the real meaning. 
That paper defines a technical concept and names it "morally correct". Despite what the name suggests, it is not a value judgement. So: No, if you do your reasoning without taking bottom into account, that is not simply "okay" just because someone called the technical concept expressing the relevant relation (between the semantics you pretended to be reasoning about and the semantics you were actually reasoning about) "morally correct". It is still on you, not some other authority, to argue that in your specific situation ignoring bottom was not all too harmful. :-) 
Let me go through the usual arguments *in favor* of these naming practices. **On "monad" and other jargon** What's a meaningful name? It's an accurate and clear description that's short enough to use as a name. So that's your first problem. As far as I can tell, the shortest accurate description of what a monad is is the mathematical (or mathematical-ish) definition of it. Since that doesn't really work as a name, you have two options left: use a familiar word that doesn't really mean the right thing, or choose an unfamiliar one. The mainstream programming tradition favors the former. It feels nice and fuzzy and makes people feel like they understand, but (if it really is an abstract, hard to describe concept) you'll still need to sit down and do some goddamned *learning* before you can actually get anything done. If that's the case, then you might as well cut the confusion and use a name that immediately tells you "you don't know this thing, learn it". **On very short identifiers** Here are two equivalent type signatures: map :: (a -&gt; b) -&gt; [a] -&gt; [b] map :: (sourceType -&gt; destinationType) -&gt; [sourceType] -&gt; [destinationType] The first option might make you cringe... until you've seen the second. Really there are two things to notice here. First, the long descriptive names, while definitely long, don't really describe anything you couldn't figure out by looking at the structure of the type. This is quite common with highly generic code. Second, said structure is much easier to discern when the variable names are short. There's also another argument, which is that short names can be perfectly descriptive if you have some conventions in place. If you've been reading Haskell for a while, letters like "f", "m" or "r" probably mean a lot to you. Of course, that's another accessibility/power tradeoff. 
Adapter, Singleton, Observer and Strategy are all descriptive (you have an idea what they mean just by reading them). Monad is not. Applicative may or may not be (not sure what that is). Foldable is very descriptive, assuming you know what Fold is. It seems that you do not believe in intention revealing names (which is fine). The point of such names is to not need to see the variable's usage to know what it means, and as a result the names tend to be longer than 1 or 2 letters (but ideally still short for convenience). For example, I would only write "xs" when you can't be more specific (i.e. numbers, persons, rows, etc.) 
I think this is another case for [Simple Made Easy](http://www.infoq.com/presentations/Simple-Made-Easy). The issues you're raising are not complex at all, they're merely unfamiliar. This is unfortunate because a lot of people confuse these two and this biases them in favour of the [status quo](http://en.wikipedia.org/wiki/Status_quo_bias).
My observations more about how things are named, not whether the constructs are familiar or unfamiliar. 
I wrote about [my reasoning about name length](http://softwaresimply.blogspot.com/2011/12/whats-in-name-lot.html) awhile ago. It's always a [contentious topic](http://www.reddit.com/r/haskell/comments/n2o87/whats_in_a_name_a_lot/). As for mathematical names, just learn them. They're better than anything non-mathematical you're going to come up with. They actually are standardized and just as intention revealing as any of the design pattern names the GoF came up with. It just so happens that the concepts these names describe are more abstract.
Totally agree. I was going to provide a type signature example but forgot to.
I guess I should have left Monad off the list, I knew it would be a sticking point. I agree that some concepts are too abstract to have an intention revealing name. However I believe most things can be named in a way that helps communicate what they are used for or what they do. 
I disagree. The name Applicative for instance is every bit as descriptive of what that abstraction is about as Observer is. On the point about xs: The lengths of names should generally increase as the size of their scope increases, and decrease with frequency of use. For instance, it's okay for 'map' to have a short name, because we use it all the damn time, and a variable name like xs is perfectly okay when the entirety of its scope is a single line (or even a handful of lines) of code where you'll certainly see the point at which it is bound at the same time that you'll be looking at any of its occurrences. The things which need long descriptive names are things which scope over an entire project and yet are infrequently applied.
I think F# calls them workflows. But i am not sure if they are fully equivalent to haskell monads or only similar. I don't know if that name is better or not either.
I actually said I didn't know what Applicative is, so I couldn't judge :) I agree that x and xs is fine in some situations if the scope is literally one line. 
That's the kind of thing that only helps complete beginners while hurting the readability of more complex functions down the road. What about mutiple inputs? in1, in2, in3... You have to stick with certain convetions at some point.
Yes, but those name don't tell you anything about the types that `a` and `b` didn't tell you. Actually, they are confusing, because they are not actually the inputs or outputs of `map`(like a reader could think), but of another function: that's obvious with `a` and `b`.
As someone who's managed to never learn straight-up Java, I actually disagree. Those names, Adapter and the like, do not mean much of anything to me in the context of programming. Furthermore, I don't have a clue how their meaning in other contexts might translate. Monad/Functor/Applicative mean things to me now, but I learned them.
Completely agreed. "Better names" is another way of ending up with SpacesuitTaco.
As an isolated point, I think short variables names are quite important! Variables rarely represent anything in particular to me when I'm coding in Haskell (though they certainly do when I'm programming other languages with more operational/procedural/oo frameworks). In particular, they are nothing more than an instruction set for wiring my functions together. I wouldn't care in the least if you changed them all (alpha equivalence). Their primary purpose is thus to be easy to hold in my head for exactly the length of time it takes for me to wire two functions together. This contrasts deeply with oo/procedural languages when variables name mutation cells and it's important to be able to think of the lifetime of a particular bit of mutable state. When I use mutable (or mutablely-styled) variables in Haskell, I absolutely give them descriptive names and I feel this is the tradition of the community. But long names as function-wiring guides just adds mental overhead. I'm trying to forget the points (names) even exist. I don't want them to be *longer*.
Thanks for your reply. I wouldn't suggest shoving those constructs under the rug, but instead naming them in a way that suggests what they are for. That may not be possible though. As you state, it would also have the downside of requiring someone with a math background to map between the math vocabulary and the software vocabulary. I guess I have trouble giving up on the goal of describing things in a way that uses physical metaphors rather than the math terms. 
http://en.wikipedia.org/wiki/Selection_bias
It's Applicative! It has something to do with application. Just like observer has something to do with observation.
You're welcome. I think if we had a really good physical metaphor for Monad we would use it. A lot of people have tried to find one (burritos, spacesuits, there's a whole list here: http://www.haskell.org/haskellwiki/Monad_tutorials_timeline), but it's just really hard not to leave out some aspect. And then if you start talking about the free monad it gets even harder.
(Me too! Is an adapter object one with prongs? Or continuation-like holes? Does it convert between conventions?)
Yes. In fact, non-STM stuff can benefit too.
Well like physical adapters, an adapter adapts between 2 interfaces. Which 2 interfaces is adapter specific. 
fryguybob on #haskell is already experimenting with this. Best touch base there!
Unfortunately, Intel has decided that this is a feature most people don't need, and is enabling it on only certain processors. You have to look closely at the features list to know if the processor you're buying supports TSX, and most OEMs aren't going to bother mentioning it. This feature is going to be slow to be adopted, unfortunately, even though potentially any application could take advantage of it (unlike certain virtualization features Intel charges extra for).
I'm sorry that you've put so much effort into arguing with people, and so little into trying to understand what they are seeking to tell you. I hope that, in time, you will actually spend some time learning Haskell programming, and be able to re-evaluate your views.
I've been persuaded, although it is unsettling to me :) 
Just to add a little more weight to it... stepcut is right on the money--except he was overly generous. "computation builder" is not just less meaningful, it's downright misleading. It boxes your thinking in. If you only thought of it as a computation builder, you might never invent the probability monad. Having these blank slate terms is important because it liberates our thinking. Yes, the terminology might slow adoption, but I'll gladly pay that price in exchange for a richer language that we can use to precisely describe very abstract ideas.
Some might say that not being visible to everyone is precisely why email works better than issue trackers for exchanging initial ideas. As nice as it would be for everything to be a public discussion, there are many good designs that could not be reached by mass consensus, and developing everything in public is a recipe for not taking enough big risks. This may be moot for a random library on github, though, if it's not particularly popular. If there aren't a lot of people that will see the issue tracker, it may function a lot like email to a small group after all.
Hi. Another imperative-turned-functional programmer here. A few points I'd like to make. One: names are a form of documentation. So are types. Lacking a strong, and more importantly expressive, type system means that more documentation is needed everywhere else. Thus more descriptive names, and more comments, etc., are necessary to make up the lack. Two, the one and two character names tend to have very small scope. So if you see: map f (x : xs) = (f x) : (map f xs) You don't have very far to carry the information about what f, x, and xs are. I will note that for variables with larger scopes, the names do tend to get more verbose in the Haskell code I've seen. OK, maybe not ReallyLongNamesLikeInJavaFactoryIterator, but still more verbose. Third, the more descriptive the name, the less general it is. And the less descriptive the name, the more general it is. I mean, what would be good names to replace the variables f, x, and xs above with? Let's try it: map theFunction (theFirstElement : theRestOfTheList) = (theFunction theFirstElement) : (map theFunction theRestOfTheList) Does this help? No. On the other hand, having less descriptive, and thus more general names, helps orient thinking towards more generic- and thus more reusable- code. We're supposed to be encouraging code reuse, right? If I'm calling the function f, and the element x, I'm not assigning very much context or specific uses to those variables, and thus am less likely to over specify them. Lastly, even though the names are short, they do tend to have specific uses, and thus specific meanings. For example, the code: map x (f : fs) = (x f) : (map x fs) is perfectly legal, but still liable to cause extreme negative reactions in Haskell programmers. Even having written it, I'm sitting here going *twitch* *twitch* at that code. The names may be short, but they're not meaningless.
true i should of just emailed him in the first place , but you guys gave me a starting topic of the email! thanks!
The only intuitive thing in this world is mom's tit. Everything else is learned. And so are your design patterns with "descriptive" names :)) 
Names for types or variables should be as descriptive as possible. Haskell makes it easy to write generic code that can't be given specific names. But Haskellers tend to name their code as if is more generic than it is. http://www.yesodweb.com/blog/2011/12/variable-naming-context
What would you call a Monad? Its concept is pretty generic, and I'm not sure you could find a decently descriptive name. In F# and C# there is a concept of "workflows" which are more or less monads. I don't find the name much better. In fact, since there's a good chance a learner will have never heard of a Monad, they will probably not try to infer anything from the name besides that very newness. The name workflow may actually be to its detriment since it is familiar but does not (IMHO) adequately convey the concept.
What's most baffling is that it's enabled on a lot of low-end cpus, but all the most high-end desktop cpus lack it. WTF?
*glares at pipes*
Kona authors do agree with you https://github.com/kevinlawler/kona/blob/master/ks.c Excerpt from [coding guide](https://github.com/kevinlawler/kona/wiki/Coding-Guidelines): "The unusual appearance is a side effect of writing C as concisely as possible. There are great benefits to writing code in this way. Someone familiar with the style can read and comprehend the code much faster than they could with traditional code. Brevity creates a discipline that reduces bugs. Some of the benefits will not be apparent until you try it. Some of the downsides of the style are inaccessibility and a steep learning curve."
I strongly agree! I also think there are a lack of resources to learn many Haskell concepts outside of whitepapers. The downside to siding so strongly with mathematics is how quickly the *abstractness* of it all becomes a barrier to learning. edit: So I think the point is that "abstractness" is what hurts haskell. There isn't a very good balance of use-case with the theory. It's the same reason that calculus make more sense once you start doing some physics. For example, applicatives are great for parser combinators - but many other concepts (wth is a comonad) tend to not come packaged with a persuasive *raison d'etre*; for people trying to get things done, it starts to encourage ignorance.
if you've learned some lambda-calculus, the following description by the famous Oleg K is as clear as can be: &gt; So, 'bind' is 'let' and monadic programming is equivalent to programming in the A-normal form. That is indeed all there is to monads. all the metaphors fall short. the correct answer is that a monad is a bit of code with a certain type that obeys the 'monad laws', and it turns out they capture an astonishing number of common programming patterns and are much easier to prove invariants for. it's just a better design pattern than the 'standard' ones, imo.
I always thought that calling it STM was rather short-sighted. Now it will be impossible to use STM on machines that have hardware support :(
I think he means that OO tends to use language with physical analogues, where Haskell tends to be more mathematical and thus slightly less intuitive.
Regarding variable names, how would you improve functions like these?: id :: a -&gt; a id x = x const :: a -&gt; b -&gt; a const x y = x (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) (f . g) x = f (g x) first :: (a -&gt; b) -&gt; (a,c) -&gt; (b,c) first f (x,y) = (f x, y) ... The problem with "intention revealing names" is that it requires you to have a *concrete and explicit* intention. In languages that don't support polymorphism (e.g., the vast majority of imperative languages), that's not a problem because all your code is monomorphic anyways. But once you start to generalize things and end up with (obviously helpful) functions like those above, it's not at all clear what manner of "intention" we should be baking into variable names. Thus, I would argue conversely: the great majority of Haskell code (that I've seen) **does** use intention revealing names. In functions like `id` and `const` where there is nothing known about the argument, we use names like `x`, `y`, `z`, which have a long tradition of meaning "whatever". At the type level we use names like `a`, `b`, `c`, for similar generality (we don't use `x`, `y`, `z`, because that would sow confusion about the difference between the term- and type-levels). In functions like `(.)` where all we know is that something is a function, we use names like `f`, `g`, `h`, which again have a long tradition of being used for function names. Similarly names like `i`, `j`, `k` or `m`, `n`, `o` are used for integers or natural numbers. And when we have lists of things (or other collections) we use names like `xs` or `fs`, pluralizing whatever the name of the elements would be. While these conventions do result in many short variable names, they do so in a way that's reasonable given the polymorphism in the language. Moreover, —again relying on the code I've read and written— when things *are* in fact specialized to some explicit intention, they *do* tend to favor longer and more explicit names.
FWIW, Applicative is just as descriptive as Foldable. Applicative things are things you can apply. That is, we just take something like `(f x y)` and make the whitespace of application explicit: `(f &lt;$&gt; x &lt;*&gt; y)` or `(f &lt;*&gt; x &lt;*&gt; y)`, as appropriate. The term "applicative functor" is certainly better than the technical name "strong lax monoidal functor", though I'm not sure it's better than the original descriptive name it ousted: "idiom". I was rather fond of Idioms, but Applicatives won the day. Still, OO programmers complain and don't see "Applicative" as being just as descriptive as their "Adapter", "Singleton", "Observer", "Strategy",... Albeit, "monad" is quite opaque. But if it makes you feel any better, it's opaque to budding new mathematicians as well. The name replaced the earlier term "triple" which is more descriptive in a technical sense —since a monad is defined by the triple `(M,return,join)`— but is absolutely terrible for conveying anything about the structure in question or for avoiding confusion with other structures (e.g., anything else defined by a three things). If you're curious about the muddled history of how the term was coined, [stack exchange has a few suggestions](http://english.stackexchange.com/questions/30654/where-does-the-term-monad-come-from).
To be honest, I'm getting tired of the advocacy. Obviously, this guy is interested in advocating haskell as a way to sell his training program, but, as was mentioned in the /r/programming thread, a far better way to advocate Haskell would be to write powerful user software in it that can raise its profile.
But if familiarity is not a factor, how does the name affect anything? A name is just an arbitrary symbol attached to some meaning.
Thanks for your reply. Learning about those short variable conventions is quite helpful. Also the notion of abstract variables being short seems to make sense, due to lack of any accurate concrete names. That being said, I find the examples you listed hard to read because only the function names hint at what is going on. Perhaps this is inevitable for highly generic code though. I guess the next question is: what % of code is really so generic that there isn't a meaningful name for variables? 
But what, pray tell, is a monoid "for"? A monoid is any collection of things with an associative binary operator and an identity thing. Monoids are utterly ubiquitous. Just a few of the most common examples are: putting things next to each other; e.g., (lists, the empty list, list concatenation) (strings, the empty string, string concatenation) (pictures, the empty picture, horizontal juxtaposition) (pictures, the empty picture, vertical juxtaposition) (pictures, the empty picture, stacking) (natural transformations, the identity nat-trans, horizontal composition) composition of anything function-like; e.g., (functions on A, the identity function, function composition) (endofunctors on A, the identity functor, functor composition) (natural transformations, the identity nat-trans, vertical composition) (M*M-matrices, the M*M-identity matrix, matrix multiplication) addition on almost any notion of "numbers"; e.g., (N, 0, (+)) (Z, 0, (+)) (Q, 0, (+)) (R, 0, (+)) (vectors, the all-zero vector, vector addition) (M*N matrices, the all-zero M*N-matrix, matrix addition) (types, the empty type, disjoint union) (functors, the constant functor to the empty type, functor sum) multiplication on almost any notion of "numbers"; e.g., (N, 1, (*)) (Z, 1, (*)) (Q, 1, (*)) (R, 1, (*)) (M*M matrices, the M*M-identity matrix, matrix multiplication) (types, the singleton type, cartesian product) (functors, the constant functor to the singleton type, functor product) minimization on any bounded join-semilattice; e.g., (L, bottom, lub/join) (N, 1, lcm) (N, 0, min) (Z with -infinity, -infinity, min) (Q with -infinity, -infinity, min) (R with -infinity, -infinity, min) (subsets of S, S, intersection) maximization on any bounded meet-semilattice; e.g., (L, top, glb/meet) (divisors of n, n, gcd) (N with +infinity, +infinity, max) (Z with +infinity, +infinity, max) (Q with +infinity, +infinity, max) (R with +infinity, +infinity, max) (subsets of S, the empty set, union) And those are just the ones made popular because they belong to popular [semirings](http://winterkoninkje.dreamwidth.org/80410.html). Let alone any of the obscure examples. What then shall we call them? "ThingsYouCanCombine(AndThere'sAnEmptyThing)"?
Yup, I unfortunately don't have a better naming proposal. It is entirely possible that there isn't one. I guess here's what it boils down to: in a stereotypical codebase (let's say a CMS web app), how much do these concepts from category theory come up? And when they do, do they make the code easier or harder to understand? I want to write beautiful simple code without any redundancy, but unfortunately there are huge piles of code that implement business logic or UI's that for the most part don't warrant the investment in genericism. (Note this has drifted away from my original post about naming practices and more into uncertainty about category theory helping a "real world" code base. Phrased another way, I believe that we can find a "best" way to develop software. I am absolutely certain that that involves static typing and large swaths of purely functional code, but I am less sure about some of the other category theory.) 
In [this comment](http://www.reddit.com/r/programming/comments/pfnkx/intel_details_hardware_transactional_memory/c3p4oq7) Duncan Coutts expresses some doubt that it will be of much benefit. Do you think he was being too pessimistic, or perhaps that STM can be adapted to it, or...?
I think the examples steve is using are a little meh, but I what about something like Reader and it's algebraic types? Reader r a is a little confusing if you're not familiar with the *positional* conventions in Monad type definitions. Would Reader env ret be so bad?
In good Haskell, I'd say a big chunk of it. Generality (parametricity) is a good design and safety tool.
I feel Lens is pretty cryptic sometimes too. 
Its O(1) arbitrary precision floating point arithmetic operations. So? Sometimes abstracting basic operations is reasonable (ie--the comparison is O(1) model for sorting algorithms), but it depends on the problem. Here it sounds like a performance claim when reality is *exponentially worse.* But, I'm sorry for being pedantic. I wouldn't care if it were not that, as this thread demonstrates, understanding of algorithmic analysis is often lacking. The fast integer solution is probably the best one regardless. 
Do you have an idea of the _dynamic_ semantics of the language? If I judge some expression `e` to be well typed: ` Γ ⊢ e : τ`, do I get that `γ ⊢ e ↦ e′` and furthermore that ` Γ ⊢ e′ : τ`? Is it stronger/weaker than that? If your language is total then it's more like: ` Γ ⊢ e : τ ⇒ γ ⊢ e ↦⋆ f ∧ Γ ⊢ f : τ`, where `f` is a final state. From what I can gather, you mix the two. Can you make a single overarching statement from a typing judgement then? Or does it depend on the type as to what statement you get?
&gt; all the metaphors fall short. the correct answer is that a monad is a bit of code with a certain type that obeys the 'monad laws', and it turns out they capture an astonishing number of common programming patterns and are much easier to prove invariants for. Exactly. Most of the 'monads are hardism' comes from people trying really hard to make monads into something they are already familiar with. But.. monads are not something you are already familiar with. Or maybe people have heard they are supposed to be hard, so they assume if they understand them easily they must have missed something? In Haskell-land, a Monad is nothing more than a datatype plus 2 really simple functions that obey some really simple laws. That's all there is! The difficulty mostly arises from trying to make a monad into something else you already know. Sometimes you just have to learn something new :) That said -- there is some profoundness to monads. I like this definition of profound: extending far below the surface monads are neither hard nor complex. What makes them so neat is that you can do so many useful things with something that is also so simple. That is, of course, the message behind [You Could Have Invented Monads! (And Maybe You Already Have.)](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html). The problems is not that monads are hard.. but that they are so simple you would never have even thought to give them a name! 
I don't think I've encountered "dynamic semantics" before (shows you how much I still have to learn!), but Spellcode's value-level behavior will depend on the computational context that a particular expression is evaluated in, so the nature and amount of information that you can derive from a typing judgement is dependent on what the type in question is. This is actually captured pretty completely by the way constraints work. For example, the type forall i A B x where (i :: Level) &amp; (A :: Type i) &amp; (B :: Type i) &amp; (x :: A) &amp; (A &lt;! B) in Ev (x :: B) is inhabited, witnessing a fulfillment of the constraint (x :: B) under the given conditions. This one is straightforward, but my goal is to fold all (or, at least, most) of the things that would usually be metatheoretic judgements back into the language itself as part of the constraint system. I'd have to know more about what you are asking to give a better answer; is there any recommended reading on dynamic semantics? To be clear, I know how I want Spellcode to behave, I just don't know how best to express it in traditional language.
By the same author (Alexander S. Green), no less. Quipper is pretty much QIO 2.0. Or, as the [overview paper](http://arxiv.org/abs/1304.3390) says, QIO "uses a much simpler circuit model and lacks many of Quipper's advanced programming features".
Wait, why doesn't the probability monad fit the description of computation builder? Is it the monad PX = {set of probability distributions on X}? I would say that it models probabilistic computations. But, say, the monad for free groups or free commutative rings --those probably don't model any sort of computation.
Oh, I can assure you that I've read the paper. Actually I've read it already before it publicly appeared in 2006. :-) And I've cited it myself on various occasions in my own work. I just wanted to point out that the title of the paper is so tempting that it happens (generally) that reference is made it to it to off-handedly justify ignoring bottoms all too often without further thought. If the title had been, say, "A partial equivalence relation between semantics of the same program in a set- vs. a CPO-setting", it is less likely that someone would simply say "It's OK, because ...". Because in the "because ..." part they couldn't simply say "morally correct" but would have to say "there's that relation we know to hold, and in my particular case this relation tells me that the semantics of my program is still within the range I want it to be to justify my overall claims". 
You might want to find out what some of these things actually are before you compain about the way they're named. http://www.haskell.org/haskellwiki/Typeclassopedia
I think there is an important difference, though. In object oriented programming, "adapter" doesn't really mean much more than just whatever the metaphor seems to imply. If you get into a discussion about whether something really is or isn't an adapter, you're wasting your breath. There is no definition. There's probably some design patterns book that tries to define the word, but undoubtedly, there are more than one, and their definitions don't match. And if the author of one of those books were in the room watching the argument, they'd probably laugh at everyone. I've been in arguments about whether something is or isn't a "true" decorator, and it was pointless, because in the end something is a decorator only to the extent that it matches the ideas that the metaphor brings to mind. On the other hand, a monad is what it is. It makes sense to ask whether something really is a monad or not, and the answer to the question is a mathematical fact. Maybe you can find an intuitive metaphor for the concept ("workflow", as mentioned elsewhere from the .NET world, actually isn't bad), but ultimately it will be secondary to the real, true definition. It's this difference that makes object oriented programmers flock toward suggestive metaphors, because in the end, those metaphors are the only meaning the term has... while mathematicians and Haskell programmers tend to make up words, because they are giving solid, formal definitions, and need a nice blank slate to do it with.
Oh, that is really actually quite illuminating to me. I always felt like I didn't understand adapter and the like because I had only using "OO"ish languages like Python where they exist but might not be expressible in a meaningful way. Would you not say that there is at least one formalization of these terms though? For instance, I'd want to try to call an adapter a Functor between interfaces if I were prone to such things.
Is there a recommended quantum computer simulator that you should run quipper on top of ?
So, dynamic semantics is a poor choice of words seeing as a dependently typed languages do essentially interleave both dynamic and static semantics. What I was trying to get at was what does the _static semantics_ (your type system) tell you about the _dynamic semantics_ (the program's execution). So, it seems like you have folded a lot of metatheoretic judgements into the type system, which means that in general the type system tells you nothing, but specific types might tell you something? BTW, that's a very syntactically noisy upcast.
&gt; Adapter, Singleton, Observer and Strategy are all descriptive `ISpaceSuitWithNuclearWasteLike` is just a `ISortOfStringy` in the `IFunctionLikeButNotReally` of `IContainerLikeInSortOfInnerWay`
Quipper *is* a quantum computer simulator. If it's anything like QIO, and it's supposed to be very much like QIO, it'll let you both do a simulation using a RNG to get a nondeterministic result, or just get the whole distribution of possible results along with their probabilities. Both with exponential slowdown, so that a circuit with 5 bits is going to take twice as long to evaluate as one with 4, but there's no way to get around that classically.
Exactly, enough is enough. Learn to do by doing! Advocate by doing! You don't have to be the best to write applications in Haskell. Just do it! Now! 
You shouldn't be getting down voted because you're right. Tekmo is correct as well, but what he said in no way invalidates your response so you should not be getting down voted in my opinion.
I thought it was just the K series that didn't have it. If that's the case, then maybe it's one of the first things to fail under overclocking or something.
Duncan is right that you will not get that many benefits trying to directly use RTM to implement STM. But even STM needs synchronization, and stuff like lock elision can be pretty handy.
Would Lenses help you out? http://www.haskellforall.com/2012/01/haskell-for-mainstream-programmers_28.html How about SYB/Generic programming? (Look in the Haskell platform library doc)
I've looked at Lens a bit, but haven't used it yet. I hadn't seen that post, though, and it seems like they were designed for my problem. Do you know if stuff using Lenses can be serialized, though? That's something I need (and I edited the post to say that), but I haven't seen anything about that (nor do I know if using Lenses has any impact on serialization at all).
Instead of first/last I prefer to put the High-Level code in a root level module, and more granular and "nitty gritty" code in progressively deeper modules.
Care to elaborate?
It's because of this line: ex { quux = M.adjust f key (quux ex) } `foo` and `bar` aren't just ordinary functions, they are also record fields which are static and must be known at compile time. The record field syntax is desugarized before runtime, and the compiler doesn't know what field quux represents. How about this instead: modify :: (Example -&gt; M.Map String a) -&gt; (Example -&gt; M.Map String a -&gt; Example) -&gt; (a -&gt; a) -&gt; String -&gt; Example -&gt; Example modify get put f key ex = case M.member key (get ex) of True -&gt; put ex (M.adjust f key (get ex)) False -&gt; ex Then you can abstract like this: modifyFoo = modify foo (\(Example _ bar) foo -&gt; Example foo bar) modifyBar = modify bar (\(Example foo _) bar -&gt; Example foo bar) Alternatively, you can write: modify :: ((M.Map String a -&gt; M.Map String a) -&gt; Example -&gt; Example) -&gt; (a -&gt; a) -&gt; String -&gt; Example -&gt; Example modify quux f key ex = quux (\map -&gt; case M.member key map of True -&gt; M.adjust f key map  False -&gt; map) ex Which you can refactor the two functions like this instead: modifyFoo = modify (\f ex -&gt; ex {foo = f (foo ex)}) modifyBar = modify (\f ex -&gt; ex {bar = f (bar ex)})
Awesome. Time to learn\* Lenses, then. \* forgot my verb
Good point concerning probabilistic computations. Of course, there is also the alternate way of viewing `Prob a` as a nice probability distribution without "computational content", so that a focus on computation would capture only half of the useful interpretations.
Although I'm very much in favour of abstraction, I don't think this comment is a valid objection. There are very easy physical metaphors for these terms. * variable: a box for storing a number * array: a row of variables * function: a machine for turning one number into another number "number" itself is admittedly harder...
They also don't have it on the single desktop processor with 5200 graphics, for some reason (though that one isn't available outside of OEMs, I guess).
I don't think this is about Simple vs Easy at all. Rather, eegreg is correct in noting that there is a tendency in Haskell code to assume that the code is more abstract than it really is. const x y = x is a good example. `x` and `y` are not arbitrary. One is returned, the other is not. Using `y` is clearly wrong. The variable name should indicate that it is not returned, thus `_` is correct. In that function there is an important paramter order. Here is an example from Data.Text: isPrefixOf a@(Text _ _ alen) b@(Text _ _ blen) = ... It is just not true that `a` and `b` are arbitrary here. Using those names is information loss. `isPrefixOf` is not a commutative operation. This is PHP: string strstr ( string $haystack , mixed $needle) Who can say that the Haskell version is more readable? This is typical for Haskell functions. Imaginary abstraction.
&gt; On the other hand, a monad is what it is. It makes sense to ask whether something really is a monad or not, and the answer to the question is a mathematical fact. Not always. There is at least one widely used "monad" that is not technically a monad. Similarly, there's some sort of traversal in lens that isn't technically a traversal. Although mathematical correctness is preferable since that makes analysis easier, it has to bend for pragmatism at some point. 
[lens](http://hackage.haskell.org/package/lens) is awesome for working with `Map`s. That said, it is a very large library, so it helps to get examples of things you want to do. Loading such code into GHCi lets you explore where specific pieces come from using `:i`. I also make no guarantees that a real lens expert won't come by and golf this code down to a single operator. {-# LANGUAGE RankNTypes, TemplateHaskell #-} import Control.Lens import qualified Data.Map as M data Example = Example { _foo :: M.Map String Int , _bar :: M.Map String Float } deriving Show makeLenses ''Example modifyFoo :: (Int -&gt; Int) -&gt; String -&gt; Example -&gt; Example modifyFoo f k = foo . at k %~ fmap f modifyBar :: (Float -&gt; Float) -&gt; String -&gt; Example -&gt; Example modifyBar f k = bar . at k %~ fmap f modify :: At m =&gt; Lens' Example m -&gt; (IxValue m -&gt; IxValue m) -&gt; Index m -&gt; Example -&gt; Example modify quux f k = quux . at k %~ fmap f 
Map is a function that takes a function from a to b, and turns it into a function from list of a to list of b. That's how I read the traditional type signature. It makes sense with me. Changing it would make me think that the argument to map is "a function that takes before values and gives after values" which is sort of a weird thing to say since all functions do that.
I'm not saying laziness is required for monadic programming. But it is required for monadic IO in a completely pure language, at least the way Haskell does it. I don't see how you'd keep the language pure and do monadic IO without violating purity, really. In an impure language, laziness is nice and makes for elegant code if used tastefully, but in Haskell, it really is more important than that. That's not to say it has to be the default, BTW. Making everything lazy by default has its own slew of nasty pitfalls.
A common way to write code in Haskell is to just write the function you need, and then you ask the compiler, "what things can I use this function on?" Very often the reply is, "just about anything that satisfies these basic properties." Haskell and its type system is designed around allowing as much as possible unless otherwise instructed.
&gt; I would say lengthy names theFirstElement and theRestOfTheList are strawmen. Especially starting them with "the". head:tail, or first:rest are better names, but if x:xs is the standard for Haskell then it is fine too for short contexts. I don't know what head/tail/first gain you here, as it's already clear that x/xs refer to those from the pattern matching. Also, head/tail/first are function names, two of them are even in Prelude. 
Once you understand lens, and learnt to properly ignore all the Profunctor stuff, it's mnoz difficult at all. 
Yes, I think i and o make sense for Pipes at first. But with the pipes library, a Proxy is entirely symmetric, you can even swap it around using turn. Therefore, it's impossible to decide what is input and what is output. Once you remembered the order "Left -&gt; Input, Right -&gt; Output (just as the reading direction in most countries I know of)" and the convention ' means upstream, it's again not that difficult. Yes, you have to learn these things, but they are simple and well documented. 
&gt; Instead of using intention revealing names... In my experience, intention revealing *types* are much more useful. Names can lie, types can't (except through their names!).
you are confusing category theory with basic algebra. monoids, groups, rings, algebras with their many variants are algebraic concepts. they fit into category theory (it's the theory of everything, so that's expected) but they are basic high-school algebra!
Then why is he OK with using the math names rather than the more "suggestive" names?
I tend to agree except when the function is 1 or 2 lines long, it doesn't really matter that much.
A probability monad can also be viewed as binding together probabilistic computations.
Name wise, lens has a meaningful convention of s t a b meaning lifter from (a-&gt;f b) to (s-&gt;f t). IOW getter from s to a and setter from b to t.
If you don't have a use case feel free to postpone the learning. 
I would hate to see "ret" rather than a. Learning an idiom/convention about what r and a mean in these types contexts is a very low cost investment. Reading longer names is harder and long term a much higher cost. I think env is fine though. `Reader env a` reads well.
I don't buy this. I don't know pipes, but I can easily see that the documentation uses concepts such as 'upstream' and 'downstream'. This could easily translate into the letters `u` and `d`. ' in Haskell signifies "variation" or "something else". Yes a and a' probably are related somehow, but there is no reason to use a and b, Input/Output, Left/Right and Upstream/Downstream, *and* x/x'. We're talking about *5* conventions for selecting between "something" and "something else". There is just no way that can't be simplified. Two of those conventions: a/b and x/x' hold no information content whatsoever because they are so overloaded in Haskell. This is the actual documentation of `Proxy`. Why is all of this information erased in all types. I just don't get it. | Sends | Receives | Receives | Sends | Base | Return | Upstream | Upstream | Downstream | Downstream | Monad | Value p a' a b' b m r Given that the documentation uses the concepts: *s*end, *r*eceive, *u*pstream and *d*ownstream, why not use the following naming that makes it simple to know what parameter 4 of that type refers to. What possible advantage does `b` give us except cognitive load? p su ru rd sd m r This is a bad Haskell tradition. Traditions from mathematics are inferior to traditions from mainstream programming. 
Which monad? Filtered is a valid traversal when you filter on data that's independent from what you modify. It's error prone but it's mathematically fitting to be named a traversal.
You're raising an orthogonal question: Which parameter order makes sense? My point is that the source code of isPrefixOf is less readable because of the names `a` and `b` instead of, say `pre` and `s`. I'm arguing that this *is* an issue in Haskell code and shouldn't be swept under the rug because of Haskell's short functions and abstract nature.
Sorry, I don't understand your question. Who does "he" refer to?
The OP, stevewedig.
I think ComputationBuilder would not actually restrict the imagination and would not prevent a probability monad. Would the names ComputationBuilder / ComputationBinder / Bindative (pun on Applicative) actually make it easier for Haskell learners? I don't know. I'd love to see empirical data on that one. Would I like to use these names instead of "Monad" in my programs? No way, the name "Monad" conveys precise meaning to me now, and is nicer to read and look at :-)
Sort of repeating the other replies, but... that is more or less like writing: quotient = numerator / denominator instead of simply *x = y / z*. It doesn't really give you any extra information! Or rather, it only gives you extra information if you are still learning what the operator "/" means. Same with *map* here, or any other function that has a similar type signature. It's fine if you want to use long names in your own code, until you get that hang of it. But that's not a good argument for using long names for completely generic variables. Of course, if the code is not generic and the variables do have concrete meanings you should name them appropriately, e.g. roi = return / investment. But again that is not the case with *map* and most polymorphic functions. 
One problem is that a `Monad` is a very, very special case of category theoretical monad. In fact it's so special that I think the name "monad" is misleading for it. I don't have a better suggestion though.
or `fail`
[Pure code can throw exceptions just like IO can.](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Exception.html#v:throw) IO is a sign of (very probable) side effects, and not much else; whether and what it throws is the documentation's job.
Just make sure you pick [this Lens library](http://hackage.haskell.org/package/lens); the other libraries aren't the same thing.
I don't like the env usage either. Assuming 'env' means environment, a beginner has to wonder what environment means. It'll need to be pointed out (to beginners) that 'env' doesn't imply anything in Reader's use (i.e. it's a meaningless name hinting at one possible use/interpretation, and eventually the beginner will learn that it's meaningless and ignore the name). But more importantly, someone could even argue that (Reader r), or the whole thing (Reader r a), is the environment not r... in which case the 'env' in Reader env/Reader env a is misnamed. I think the only important name here is 'Reader', and that that's a pretty good name. One of the nice things, even though it's hard, is that as a Haskell beginner I don't feel I'm being pandered to (maybe with the exception of those monad tutorials :-) 
Continuing on a bit from there, I would generally consider a function that consists of "[lens specification] [hard-coded lens operator] [some operation]" that doesn't really "do anything" on its own to be potentially an antipattern. It's functioning as an anti-abstraction here, unabstracting what lens abstracts out by hard-coding a lot of things for little gain. After having done `makeLenses ''Example`, the next thing I'd probably do is just whack the entire rest of the example here, and directly use the lens operators in my code.
There's something I don't understand here. Normally the principal type is the most general type you can give an expression, but you seem to say the principal type is the most specific type. What's the principal type you'd derive for (\ x . x) ?
If you didn't already see it, you absolutely must read [this pearl](http://www.cis.upenn.edu/~byorgey/pub/monoid-pearl.pdf) written by Brent Yorgey about the use of Monoids in the diagrams library. I think it's a pretty convincing answer to your question.
I agree! Although I think there are times where more application-specific names given to little blocks of functionality can aid readability. I ported the original design to show just how flexible lens makes record updates, and how polymorphic those abstractions are. It can be hard to decipher a block of lens code by itself, but I think seeing it side-by-side with more or less equivalent non-lens code is a great way to get a finger hold on which parts of the lens edifice you want to climb first.
I prefer using the name that exactly describes what something is that has 50 years worth of precedent to making up a name, where the name immediately suggests all of the laws to the user than making up some name and having everyone equally in the dark with what it means. At least by taking the 'real' name for something and using it, some subset of my audience knows what I'm talking about and moreover, I can discover properties I didn't even know it had by reading literature on the more general concept. If I make up a random name, someone who knows what they are looking for cannot google for and find my object, just as I can't google for and find things it can do. When you use the name from the mathematical construction, all of a sudden your users gain access to all of the preceding literature. We lean on category theory, abstract algebra, etc. in part because we get to piggyback on 50-60 years worth of existing research, which has been built up and used to cut across a vast number of mathematical disciplines. Almost nothing has been that battle-tested and in use that long in programming culture. You can lean on LISP and FORTRAN, and they haven't exactly been evolving at the same clip. It enables us to borrow a concept, and have existing literature immediately suggest dozens of known applications, known places where the thing doesn't fit, and a host of tools I wouldn't have been smart enough to come up with on my own. Is it geeky and mathy? Yes. I don't thin that is necessarily a bad thing. I'm not willing to give up that precision to start describing things in purely ad hoc terms. We use the math terms precisely because they capture properties that huge swathes of concepts fit, because that is exactly why those terms were coined in the first place in mathematics. I could make up an 'Iterator' class, but I have no real reason to believe that the notion I made up is in any way shape or form canonical, and most of the time by the time I've factored out the things that fit the more general mold of those mathematical constructions I don't need to make up the new thing in the first place.
I wish I could give you a name but it was a while since I read it and I'm currently on vacation with suboptimal internet connectivity. I want to recall there was some corner case where the monad laws didn't apply, but I could be wrong. I was indeed thinking about filtered with regards to lenses, yeah.
They're not? They all implement the same idea, AFAIK.
i get your point. what i mean is that your example is a poor example, because it is perfectly clear from the name in what order the arguments should be. i don't need documentation to figure that out. i need documentation for all the haystack/needle functions in php though. edit: and to say it clearly. your point _is_ valid!
&gt; map x (f : fs) = (x f) : (map x fs) this is perverted and offensive
I could come up with one-off methods for every one of my objects, where I use entirely domain specific terms. Let's take the web for a second. I could write concatenateHTML :: HTML -&gt; HTML -&gt; HTML emptyHTML :: HTML but now I have these two new names cluttering up my namespace. When I go to smash together a list of the form `[HTML]` into a single HTML result, I'd have to use an ad hoc combinator, everything I have to write is one-off and specific. I'd sure feel productive, because gosh darn it. I've written this code a dozen times and I'd fly through it, but my namespace would become more cluttered and I'd get strictly less code reuse than just saying that HTML forms a Monoid, and I'd lose access to foldMap, and all sorts of other tools that let me continue to derive other things that I wouldn't have thought of. Moreover, because I'm writing code that is specific to the HTML type, parametricity isn't helping me reason about the code. When I see something fully parametric in the parameter or where it only depends on some typeclass, then this is a HUGE blessing to me. It tells me precisely the information I do not need to have to reason about the code. If I see a function of the form: const :: a -&gt; b -&gt; a I _know in my bones_ that it isn't going anything other than giving me back the first argument, even without looking at it if the author of it wasn't stupid enough to give me a bottom. Now, if I see constHTML :: HTML -&gt; b -&gt; HTML I have no idea what it is doing. It could give me back the first HTML document, it could concatenate it to itself, it could replace it with an empty document. It could give back "hello world". By abstracting out common classes that have known laws, and working in a more generic fashion with more parametricity than is needed for the moment, you get stronger statements you can make about the code, and the type checker does more work in ruling out bad implementations. Types then direct your programming efforts in a way that the TDD folks try to make their tests direct their programming efforts, but unlike TDD, the types can prove that whole swathes of possible incorrect implementations are not the one you have.
But we don't actually use those physical metaphors. The data structures in Java etc are called `Array` not `RowOfVariables`. Languages which declare dynamic variables often use `var foo` not `box foo`.
I actually typed something up about that, because in general I agree; composed lenses given useful names of their own can be very useful. The problem I encountered was that: fooAt = foo . at was not a very compelling example of how useful that can be. :)
`Category` is as well. as is `Functor`. that is unfortunate.
Hm, this seems kind of nice.
Despite my apparent flippancy (which mostly came from the paper's title), I did believe that the paper applied here. My post was specifically in the context of Tekmo's point about what the type system allows him to not worry about, not a general flippancy. If that is _specifically_ not correct in this case, I'm listening. Bottom doesn't "morally" change Tekmo's point, as far as I can tell.
I choose the length of my variable names proportional to the amount of code over which they are going to be in scope. If the variable name only provides context so you can match up the a's and b's on a line and see where they flow then they are perfectly suited to be one char names. `in` and `out` aren't more descriptive, because they are in and out from where? `myObject`, etc just makes visual chunking and pattern matching harder to do. There is a bit of a convention to the particular domain that we draw out one or two character names from. `a` and `b` tend to be variables of kind *, arguments, etc. `f` and `g` for functors, `m` for monads, etc. But really the types variables are there to let you pair up the type parameters that are related, but then have no further import or role. third f (a,b,c) = (,,) a b &lt;$&gt; f c In that line of code I care entirely about where the variables land. I don't care about what they are called. I have no value judgment about what function you give me, or what the roles of the arguments of the tuples are to make.
Applicative is just as contextual. I would use "Bindable".. But to me "Monad" is just fine.
Hm, but what about,say, an AST where terms are parameterised by the type used for variables? This forms a monad, where bind is substitution: AST a -&gt; (a -&gt; AST b) -&gt; AST b But what does "AST a" mean in terms of a "computation"? It's a data structure, not a computation.
It doesn't matter what they're called, what matters is how the programmer thinks of them. I don't believe there is any correct *physical* metaphor for `Monad` unfortunately.
I agree that the use of "math words" is not the problem. I think the problem is lack of physical metaphor. The way I see unsophisticated programmers write code is to treat the computer as a box with lots of smaller boxes in it, and you can move the boxes around and move stuff between boxes. Now this is a very limited way of programming but it does get people started and they can improve from there. It seems to be much harder to get a foothold in regarding monads.
Examples like this always concern me isPrefixOf "ab" doesn't give me the function which decides whether a string is a prefix of "ab" even though it "sounds" like it should. It's fine to use (`isPrefixOf` "ab") instead, but there is significant potential for confusion, especially where the function takes two arguments of the same type.
You make a good point. Furthermore, taking a leaf out of the Lawvere theory book, we could decide to say that monads are just tree-like structures instance Treelike t where leaf :: a -&gt; t a graft :: t a -&gt; (a -&gt; t b) -&gt; t b This actually makes a lot of sense to me, now that I've thought of it :)
Trivially, it's `singleton (\ x -&gt; x)`, but I'm pretty sure that's type-equivalent to exists where Linear &amp; Total in forall i A where (i :: Level) &amp; (A :: Type i) in A -&gt; A (and probably a whole bunch of other, similar, types). EDIT: With regards to your general concern, you seem to be getting confused by the seeming opposition of the words "general" and "specific". Think about it this way: GHC Haskell's (implicit and unspecified) subtype system makes `forall a. a -&gt; a` a subtype of `Int -&gt; Int`, or even a subtype of `forall a. (Num a) =&gt; a -&gt; a`. Spellcode works the same way (just much more generally).
&gt; Not always. There is at least one widely used "monad" that is not technically a monad. Similarly, there's some sort of traversal in lens that isn't technically a traversal. Instances like these are the worst crimes in the history of functional programming. "Pragmatism" is not an excuse to blatantly lie about what your library is. If it's an applicative functor, and it's "almost" a monad, then it's a fucking applicative functor, and you should be shot if you even so much as have a monad instance commented out in your code.
Or SpacesuitTacoProducerFactory
OP, you also have to take into account that Haskell is the only serious pure FPL in town. Java, C++, Python, Ruby, and friends have the notable advantage of having millions of failed attempts and lengthy experience in industry to smooth out what does and does not work. There's LOTS wrong with Haskell. Naming is an issue that has not totally been addressed yet. We are still undecided about library design. (I, personally, am not even convinced that monads are the "right" level of abstraction for the average programmer). It won't be until there's a half-dozen pure FPLs on the field, with at least some making successfuls attempts to break into commercial settings, will we see these kinds of problems go away.
Do you guys have a simple tutorial up anywhere? I actually just started using OpenGL myself a couple of days ago in hopes to start making a simple game using Haskell. This looks like good news but I'm not great at picking up how things work *just* from reading through example code. Anyway, great work! I'm excited to check it out.
True enough.
For some time there has been the tagline "comonads capture notions of context dependence" - clearly *Contextual* won't fly as a unique identifier.
I think the "Reader monad" was actually coined by the Haskell community - before that the same monad was idiomatically referred to as the Environment monad. I'm not sure the Haskell name is an improvement.
That is true. I was bouncing a perspective off of this subreddit to see if I'm off base or not. I'd say it has been a bit of both. On one hand, some concepts have no physical metaphors (Monad, Monoid) and other types are so generic there is no meaningful labels. On the other hand, I was a little surprised at some replies with general hostility to the notion of intention revealing names rather than forcing the reader to use context to infer variable semantics.
Agreed on the notion of short variables in a short context, and I also agree that for generic structural code like "third", there are no meaningful names. I wonder though, what % of code is at that level of structural genericism.
Will do, thanks for the link.
Let me try to do a better job explaining what I meant. I didn't mean "in" and "out" to be in the sense that every function has in1, in2, ..., inN -&gt; out. That is not at all more informative. In our codebase, we view "Map" as a special type of function, with 1 input and 1 output. We call them "key" and "val". In our codebase, any time you see "key" and "val", you know you are dealing with maps, either computed or persisted. So in our codebase map :: (key -&gt; val) -&gt; [key] -&gt; [val] would absolutely be more descriptive than map :: (a -&gt; b) -&gt; [a] -&gt; [b] That being said, my attempt to translate that beyond the context of our codebase for this audience was a total failure :) 
[My example was a failure, here's more explanation](http://www.reddit.com/r/haskell/comments/1i0uiq/do_naming_practices_hurt_haskell/cb0kz7y)
[My example was a failure, here's more explanation](http://www.reddit.com/r/haskell/comments/1i0uiq/do_naming_practices_hurt_haskell/cb0kz7y)
Thanks for the info. In some sense that is probably inevitable. In the DDD book, the author mentions that a domain model captures a useful slice of the domain, rather than attempting to encode a perfect and complete model in software. I think Haskell's vocabulary can be viewed a modeling the math domain, so it may be inevitable that it isn't a perfect match. 
You'll get hostility whenever you try to impose your beliefs on others (which is what this whole thread is, isn't it?). Sometimes, it's worthwhile, because other people's beliefs suck and are detrimental. This particular issue is somewhere in between. One of the problems with writing very generic code is that one person's understanding may not match another person's understanding. This is especially obvious if you approach Haskell twice: once from a programmer's point-of-view and once from a mathematician's. The program will see a Functor and think "oh, that should just be called a 'Container'". The mathematician will look at that and say "what the fuck?" A functor is a basic object in mathematics. It lets you add or remove structure. Calling it a 'Container' obscures the fact that you can do the latter, and seems to suggest there's a very particular kind of structure it can add. You see this all over in the interdisciplinary areas of computer science. The Curry-Howard correspondence ensures that every concept has at least three names. Just to give one example, the type () is pronounced "unit" (because it is the unit of the cartesian product)... but it's also called top (because types form a partial order, and () is the greatest element of this partial order)... and it's also called 1 because it contains just one unique point, and it's also called "True" because it corresponds to the trivially true proposition. Of course, coming from a C-and-curly-brace background, you'd be urged to call it "Void" because it's the return type of the void functions of C, C++, C#, Java, etc. But Void in the Haskell world means a totally different type with no inhabitants (which is sometimes called the empty type, 0, bottom, or _|_... of course, _|_ also means the undefined value in a partial language, which means there's some ambiguity there). And in dependently-typed languages, you have the humorous problem of how you call Π and Σ types. A Σ type is a special kind of pair type where the type of the second thing may depend on the first. So we might call these "dependent products" (because we use the word product for pairs). But Σ is intuitively a repeated disjoint union (iterated Either in Haskell), and Π is an iterated product. So when we say "dependent product", we might mean Π *or* Σ and everyone shoots themselves.
As a GLFW-b fan, I am excited :)
I am aware that those are all real math terms :) I guess my question can be rephrased as, do Haskell concepts have to be expressed using mathematical vocabulary, or is there a structural vocabulary (pipes, matching, extracting, wrapping, transforming, etc.) that a broader population may find intuitive. 
I agree. That being said, we still have to name things, so preferably those names are as helpful to the reader as possible.
Those Java names are terrible. It is a strawman I say! I would say your Haskell example has good naming. 
That is an interesting reply to a thread about Haskell's learnability. Rather than complaining, I'm asking if Haskell concepts need to be named using a mathematical vocabulary, or whether Haskell could be more approachable if some of the concepts could be named using a structural vocabulary (pipes, matching, extracting, wrapping, inverting, etc.) or some other vocabulary that is more "intuitive", whatever that means. Edit: Also thanks for the link
[Some further explanation](http://www.reddit.com/r/haskell/comments/1i0uiq/do_naming_practices_hurt_haskell/cb0lt69)
[Some further explanation](http://www.reddit.com/r/haskell/comments/1i0uiq/do_naming_practices_hurt_haskell/cb0lt69)
If you're looking for other examples, I've updated my nehe-tuts to work with the latest GLFW-b: * https://github.com/dagit/nehe-tuts * http://hackage.haskell.org/package/nehe-tuts
This is a good example. I felt there was a lot of examples like this when I read Real World Haskell a while. (Side note, I find the lack of non-positional keyword arguments to be crazy, but that is a whole other bag of worms.)
These examples start very simply: https://github.com/dagit/nehe-tuts The explanation that goes with them can be found at this website: * http://nehe.gamedev.net/tutorial/lessons_01__05/22004/ * http://nehe.gamedev.net/tutorial/lessons_06__10/17010/ * http://nehe.gamedev.net/tutorial/lessons_11__15/28001/
I'm not trying to be argumentative, but [premature generalization](http://c2.com/cgi/wiki?PrematureGeneralization) is a serious force when evolving a large codebase. Often you don't know how to generalize until you have [several examples](http://c2.com/cgi/wiki?ThreeStrikesAndYouRefactor). Eventually you see patterns and pull that up into more generic utilities, but a large portion of the code always seems to be implementation of isolated examples.
I think it *should* be a non-issue. The fact that terminology and jargon are more than a minor issue is a sad testimony to the state of the profession.
Vocabulary differences and overlaps definitely create unfortunate confusion. I think I was TA for db class where we gave the students a table mapping between different vocabularies, and the conflicts and ambiguities were almost comical. Basically, math vocabulary is "right", and much of the rest seems to be made up by developers as they go along. I obviously am less interested in "right" as I am at designing a vocabulary tailored to the population of users (my target population is 20-30% of developers, not just mathematicians). Also, I strongly disagree with the characterization of this thread as "imposing my beliefs". Both the title and the last line are phrased as questions, and I have very much appreciated learning from the answers. 
"om" means "if" in Swedish.
Excellent, thank you!
If nothing else, it's more fun to try to pronounce.
Yes, those aren't talking about the same thing I am. Generalization as a design tool is extremely valuable in my experience. I have a few examples buried in my post history but can't get at them right now on my phone. The basic point is that Haskell-style parametricity actually gives you free theorems restricting what a function can do. If I ask you to write a function of type forall a. [a] -&gt; Int, I virtually have a guarantee that the function cannot do different things based on the list elements, and can only reason about its structure (length would work, for example). When an experienced Haskeller is writing new code, using "generalization" in that manner is a valuable tool, not just for reuse (although that can also be valuable) but to keep oneself honest. If I'm writing a function that should not know about the values in a graph but only does something with the graph structure, I will make the function polymorphic so I'm not tempted to use a value by accident in my complicated algorithm. As is often the case with judicious use of types, you can use them to help you do the right thing if you set things up right. As you get better at it, you can introduce more interesting forms of quantification to help you get it even more obviously correct, with even fewer chances to screw up at runtime. I mentioned reuse was a less important point. I've written polymorphic code that I knew would only ever be instantiated to one type in practice. Hell, I've even used multiple type variables, all of which I knew would be instantiated to String in the final program :) I still got a lot out of it. Furthermore, readers of the more expressive types will instantly know what the functions can't do, in a machine-checked way. So yeah, I stand by my point that generalization is a valuable design tool (it helps me write better, more compartmentalized code) and a valuable safety tool. Shittier languages have given it a bad name. Edit: I hadn't read that c2 article in ages, but I just looked again and think a big chunk of it is crap, and doesn't really provide much justification for its assertions.
For the lazy: `om :: Monad m =&gt; (a -&gt; b -&gt; m c) -&gt; m a -&gt; b -&gt; m c`
Maybe you didn't go to school in France. :)
You shouldn't look at the name for the functions like `id` and `const`, look at the types. They tell you exactly what those functions do.
A very specific case of side effects: where virtually everything can fail in an unexpected way due to memory, network, drive and other hardware failure or OS/libs error. Yes, pure code can give bottom value or exception(code smell!) but it's mostly either obviously non-total (e.g. head) or has a failure mode in its type (e.g. lookup).
&gt; Also, I strongly disagree with the characterization of this thread as "imposing my beliefs". Both the title and the last line are phrased as questions, and I have very much appreciated learning from the answers. I'm not putting blame on you. I just want to highlight this point in case you can't understand why people in this thread are less-than-cheery :) &gt; Do naming practices hurt Haskell? (self.haskell) This is clearly a leading question. It's suggesting that they do. &gt; There almost seems to be a contest for how many abstract mathematical concepts you can discuss. This line is just designed to incite anger. I understand your frustrations, of course, because at one point, I too had to learn a lot of new vocabulary. But this is merely a matter of familiarity. You can dress up these words with friendlier names, but that doesn't change the fact that, to learn Haskell, you have to learn a new set of terms for a new set of tools because your old tools tend not work very well. Category theory comes up because it is has historically been the absolute best tool we've found for investigating the extremely general notion of "structure". It is the right tool for the job. However, it doesn't show up at all in the standard curriculum, and so every Haskeller (and mathematician, for that matter) gets blindsided by it. They ask: "What's it good for?" and complain that it is obtuse and confusing. But once you see that everything you've ever done in your life is an adjoint functor, you sing its praises. The mathematical terminology reaps huge benefits in the long run. You might say this class is "abstract" while this one is "generic"... but after learning Haskell, you realize that abstract data types and generics are actually the same phenomenon viewed from two viewpoints. Generic programming is "universally" quantified and implementation abstraction is "existentially quantified". Using the software developer's terminology, you would never see this, nor would you recognize chances to refactor one for the other. (And the abstract-generic relation is an adjuction, see categories above). That brings up another point. Category theory brings up a powerful notion of duality. Duality is great in mathematics because you can prove one theorem and get a second one for free. Moreover, if something is interesting, then there's a very good chance its dual is interesting, too. This is why you see co-things all over in Haskell. We know monads are useful for modeling various kinds of effects. Perhaps comonads are useful too. Functors are great for programming, but what contravariant functors do programmers use? Algebraic data types are great, but when we move to a total setting, we find that we actually conflate two related notions: algebraic data types and coalgebraic data types. (Only the latter may be used to represent infinite data types). Again, I can totally understand your frustration coming from a traditional software development standpoint. While there is some merit to the problem, you have to understand what Haskell is not: Haskell is not a language that was designed to write software systems in. Haskell is a language that was designed to do programming language research in. (Incidentally, this explains the piss-poor Windows support, and the lack of any sensible windowing toolkit). Haskell was a language that was born in academia to give people a common (and free-to-use) language to do their research in. For this purpose, Haskell is second to none. But for software development, Haskell is mediocre at best. The module system is a joke. Records suck. There is no tool support other than the :r command in GHCi. There are inconsistencies all over about how you do things, what classes things have, how errors are handled, and what things are called. And despite the best efforts of the compiler writers, type errors are still often unintelligible when you enable enough language extensions. Some of these problems stem from the fact Haskell is still relatively immature. Others stem from the fact that many of the libraries in Haskell were actually experiments in pure functional programming. But at no point has anyone come forward with an improved language with the intention of trying to make it a practical pure functional programming language suit for use by software developers writing widgets for Acme Co. I hope some day that language will come into existence. But it hasn't yet.
I agree that a pure invention increases confusion. If however there is a helpful metaphor to an existing concept the user understands, it can be helpful. But that is dependent on how accurate the metaphor is, and what the user's starting knowledge base is.
By asking your question in /r/haskell, your question is subject to the worst selection bias possible. For example, you ask for comparison between Haskell and Lisp. It would interesting if you were to ask the same question in /r/lisp and compare answers.
Actually `Combinable` wouldn't be so bad IMHO. (But that would be `Semigroup`, and again leaves the question of what to call `Monoid`. (`CombinableAndThere'sAnEmptyThing`, obviously!))
Re: the title. That's exactly what I meant. I value the paper. I wish that more people would read it. I quite liked the title at the time as well. But from time to time now, I have the impression that people don't read the paper itself because the title is so neat and lends itself so well to just apply to "everything". Re: the specific case here. Tekmo's reasoning relies on the technical concept of parametricity (a.k.a. free theorems). At least, making his argument formal would require a use of parametricity (it's exactly the kind of thing that could be proved using the strategies discussed in "Free Theorems Involving Type Constructor Classes"). It is not clear whether, or how far, the technique from the "Fast And Loose Reasoning ..." paper applies to parametricity-based reasoning. That was left open in the paper (see the paragraph starting with "Now on to other issues." in its Section 13), and as far as I know, hasn't been picked up later. 
In object-oriented programming and Java in particular, the plan is to model software components after real-world analogues. Therefore it makes sense to name them accordingly; I know what the purpose of an adapter is in the real world, so I'll include "Adapter" in the name of a software component that behaves analogously. The naming schism between OOP and FP is symptomatic of a much deeper schism; the fact that in FP one is not modeling programs after the real world at all. Functional programmers think of a program as data + functions, literally nothing more, and make no attempt to anthropomorphize the components they build. The reason I think this is a valid approach is that programming, like mathematics, is capable of great feats of abstraction, and the higher on the abstraction ladder you climb, the harder it is to justify trying to model things in terms of the concrete world. Correspondingly, the plan of naming components after real-world analogues breaks down as the abstraction level of the components increases. The analogies become vaguer and vaguer, and you end up with hilariously long class names that try to encompass the full meaning of this very abstract concept. In Haskell we say, forget about trying to find a vague analogy to the real world, this structure I found has a very abstract but precise meaning which I can tell you in a few short lines. And instances of this thing turn out to be everywhere in programming! (c.f. functors, applicatives, monads). Likewise when it comes time to name this thing, forget about trying to name it based on an appeal to a real world analogue: no such thing exists. So let's just make up a name (preferably short), and make people learn what it means. Well, sometimes the names are a little suggestive (Applicative is an example), but there's no getting around the fact that you need to learn the definition. An aside about abstraction: people who love Haskell love abstraction (functions are the basic unit of abstraction!), because it lets them do more with less. Basically it allows consolidation of code (DRY). Haskell is about being abstract but precise; the definition of Functor in Haskell gives an absolutely clear-cut description of its semantics (the "functor laws"). OOP on the other hand tends to be simultaneously more concrete and more vague. More concrete in that it wants to model itself after the real world, more vague in the semantics of the real world are inherently vague to us! True precision can only come from mathematics. So much for class naming, the Haskell heritage of short identifiers probably comes from math; mathematicians like short (preferably single symbol) variable names. But I've seen a fair amount of Haskell code that uses longer variable names. I tend to try to make my identifiers at least 3 or so characters long, which I think is usually enough to give an intuition about the type/purpose of something. Identifiers that are too long or too short inhibit readability, imo. Overall I agree with you that code readability is extremely important, but I think that as one learns a language as radically different as Haskell, many of one's preconceived notions about programming tend to undergo adjustment, and what constitutes "readability" is one of these.
I'm a bit uneasy that the go-to tutorials always seem to feature the fixed-function pipeline. Then again, I have no idea what the "best" way is for people to learn OpenGL is. Hm, I guess this isn't really that constructive a comment then. :(
That's the kind of (oh-so-Microsofty) name which I most intensely dislike. If two good possibilities are A. An established mathematical term which is precise, but unfamiliar to most people (`Monad`) or B. An intention-revealing name which is not completely precise, but helps build intuition (which is hard for Monad, but if you wanted to cover only the most common cases, perhaps could be `Runnable`) then this is C. An impressive-sounding word pulled out of thin air because of some vague feeling of resemblance to the idea it's meant to describe and gives you neither benefit. (Only the "benefit" that people don't have to be confronted with scary unfamiliar words - just scary familiar ones.) I'm actually on stevewedig's side in the "debate", and would prefer more intention revealing names *where possible and reasonable*, with a note in the documentation about the mathematical concepts they correspond to, but if the alternative is `Workflow`, give me `Monad`. (Maybe I'm being unfair: could someone explain to me how the word "workflow" builds intuition for what monads are (or used for)? All it builds for me is antipathy.) (FWIW, another, less onerous, example of a name I feel similarly about is FRP's `Behavior`.) (In neither case am I positing the existence of a better name! Just that these ones are bad.)
I am certain that static typing and maximizing the amount of purely functional code is the Right Way. For me purely functional code means referential transparency and immutable data structures. I do this in Python and Java and believe I have a good understanding of the benefits. I guess I just have to dive into Haskell further to really grok the additional benefits. Based upon the responses, it appears that "simplifying" some of these advanced concepts is will not prove to be a valid goal of mine. 
It makes me hopeful that inference for Spellcode will be tractable!
It's been suggested before. [This was the response.][1] [1]: http://www.reddit.com/r/haskell/comments/14qe8b/pipes30_simpler_unified_api/c7frxai
I disagree with your objection that `a,b` and `x,x'` are meaningless, but I heartily support the `p su ru rd sd m r` nomenclature for Pipes.
More technically, it would be computer scientists. They are not quite programmers and not quite mathematicians. They do a little of both.
For me, I reached zen by learning about dependently typed languages. A lot of confusing things simplify down to the same basic notion of type that depends on some value. You also tend to "bump your head" on the type system less often. However, dependent types are still a very underdeveloped area of computer science. And an area I am completely in love with. So my viewpoint is biased there :)
Well, I'm sure you learned a few of them, just not the names for them :) FWIW I think the "basic" in ibotty's "basic algebra" just means that these notions are the first algebraic objects one learns about when learning what mathematicians call algebra (or abstract algebra, or universal algebra)— as opposed to, say, left-nearsemirings or quantales. The mathematician's notion of algebra is quite different than what they teach in highschool algebra classes. If you're interested in a crash course on the terminology, I have a few blog posts which provide nice maps of what's out there. There's one for [binary relations](http://winterkoninkje.dreamwidth.org/79427.html) (e.g., preorders, partial orders, equivalences, PERs,...), one for [group theory](http://winterkoninkje.dreamwidth.org/79868.html) (e.g., semigroups, monoids, groups,...), one for [ring theory](http://winterkoninkje.dreamwidth.org/80018.html) (e.g., semirings, rings, fields,...), and since I do a lot of work with semirings there's also a post on [un/common semirings](http://winterkoninkje.dreamwidth.org/80410.html). I've been working on a map for lattice theory, but haven't gotten around to posting it yet.
If we used the Haskell nomenclature in Java, we'd get something like: for(A x : xs) ... // where: Iterable&lt;A&gt; xs We could replace the generic `A` by `Item`, but even in Java that's fairly uncommon. Albeit, in Java the name `T` is more common than `A`. However, non-trivial names for generics are usually reserved for when the generic is bounded to implement some interface. The Haskell equivalent to such bounded quantification is things like: foo :: Num n =&gt; n -&gt; n -&gt; n bar :: Monad m =&gt; m () where, again, we tend to use less generic names than `a`,`b`,`c`.
Well that is neat. swing map "Foo" [id, map toUpper, map toLower] ["Foo","FOO","foo"] What use did you find for the other examples?
An interesting one: swing (=&lt;&lt;) :: Monad m =&gt; a -&gt; m (a -&gt; m b) -&gt; m b Of course it's really just the monadic concatMap above.
I'm open to this idea, but I'm not perfectly clear how everything would work. bindings-GLFW only needs a relatively small amount of the stuff in GLFW. For now, my low-tech solution is just to watch glfw.org's release RSS feed. I'm not sure what to do about the libXxf86vm headers. It seems a little unfortunate, but expected, that bindings-GLFW doesn't build on Hackage, because of course that server doesn't have X11 headers. Please feel free to open issues or mail me directly.
I totally agree with your criticism. I've just been too lazy to fix the problem. Patches appreciated :)
Ahh, I forgot about `swing`. Nice one. I like that all those functions do something sensible. It reminds be a bit of this trick: iterateN :: Int -&gt; (c -&gt; c) -&gt; c -&gt; c iterateN n = flip (foldr ($)) . replicate n 
Well really the best version is to use an inherently memoizing infinite list.
I see, that makes sense. Thanks for the links, I will dig into them. 
looks like `(Cont r a -&gt; b) -&gt; a -&gt; b`
&gt; You'll get hostility whenever you try to impose your beliefs on others (which is what this whole thread is, isn't it?) I'm not sure I buy that, this is about a language community with a strong math influence accepting the best practices of modern software development. That meaningful variable names in code (which is read *way* more often than it is written) are a best practice is as close to a fact as you get in software development. The push-back is totally understandable given the circumstances, the FP community was always more accepting of naming practices considered "bad" by the professional developer community *and* names are often even harder to come up with in very abstract FP code than in other schools. But dismissing this as "imposing your beliefs" is dismissing a large amount of research around software development processes.
If you want a specialization of *map* then by all means define one with descriptive types and names. I'm not sure how that relates to the original complaint, though. Also, your use of *key* and *val* suggests you might be looking for the type Data.Map or similar, which is a different story.
One of the best, well-designed, high-performant and super-stable-and-safe languages in the world. …and… One of the [crappiest piece of software](https://github.com/mongodb/mongo-java-driver/blob/1d2e6faa80aeb5287a26d0348f18f4b51d566759/src/main/com/mongodb/ConnectionStatus.java#L213), completely [broken by design](http://hackingdistributed.com/2013/01/29/mongo-ft/) that someone named 'MongoDB' just to mess with [people in Congo](http://en.wikipedia.org/wiki/Mongo_people). [Why would you *even* post something like that?](http://media.tumblr.com/tumblr_m02rytMTwG1qma8zp.jpg)
The "best practices" of software development evolved in a different world, and what's best for Haskell is not the same for what's best for Java.
Indeed, GHC's STM implementation is likely to benefit from TSX. I'm working on an implementation this summer that will cover several of the possible options. The simplest is to use the coarse grain lock version of GHC STM and hint the spin locks with TSX's Hardware Lock Elision. This would means that validation and commit of non-conflicting transactions can happen concurrently. This already happens with the fine grain version of GHC's STM, but HLE can do this with less overhead. Beyond this simple option I'll be exploring running the STM transactions themselves in a hardware transaction. Several features are a bit tricky to get right and it is hard to say at this point what workloads will benefit.
English word "monad" stems from Latin word "[monas](https://en.wiktionary.org/wiki/monas)"(one, unity, unit), which stems from acient Greek word "monos"(single). ... and the name actually makes sense here. Monad is a unit that forms computations.
Hehe. I don't think any unbounded-HTM designs have ever been implemented, so STM is still the only option that can cover the semantics of GHC's STM.
Architecturally there are potential benefits to the semantics that Intel choose (there is no facility for non-transactional accesses inside a hardware transaction). While a different implementation might make it easier to implement all the features of GHC STM, there might be some performance benefits that such an HTM cannot achieve.
That sounds interesting. Do you have a good resource at hand to learn dependent types?
but... [it's web scale!](http://www.youtube.com/watch?v=b2F-DItXtZs)
I'm currently coming up with a plan for a series of tutorials... but it may take a while. http://www.cis.upenn.edu/~bcpierce/sf/ Software Foundations. It uses Coq. There's also Agda, and the IRC channel #agda, which is pretty helpful. Idris is a newer language aimed towards more practical programming. #idris on Freenode. It's less mature, though.
That random return statement is funny, but to be fair they have [fixed it](https://github.com/mongodb/mongo-java-driver/blob/master/src/main/com/mongodb/ConnectionStatus.java#L217) since then. Don't know enough about databases to comment on the other issue though.
If, once you understand the relevant concepts, you can come up with a name that is simultaneously sufficiently concrete to have an easily relatable metaphor and sufficiently general that it still applies in most of the situations that these concepts are used, then feel free to propose it. I'm not trying to be rude, but many people here are trying to get across the idea that these concepts are so abstract that there often isn't really any good name that has both these properties (Seriously, "applicative" is about as good a name as you're ever going to get!). You seem to admit that the point has sunk in regarding monads, but the same is true of all those other concepts that we're talking about. Whenever there _is_ a good physical metaphor for a concept, haskellers have no objections to using it; c.f. Pipes and Lenses. Also, Typeclassopedia is awesome and I take every available opportunity to link to it. ;)
I am duty bound to suggest [this](http://www.arcadianvisions.com/blog/?p=224) and [this](http://www.arcadianvisions.com/blog/?p=388). The first is a straight port of an imperative programmable pipeline introduction; the second is more recent, and diverges stylistically.
no it is not. Idris is at least as pure as haskell and uses strict evaluation and supports monadic IO. Why do you think this would not work?
Agreed on all counts.
Thanks for the links. I hadn't seen HyperDex Warp before. Looks really interesting.
I do find myself writing things like join $ foo &lt;$&gt; bar &lt;*&gt; baz sometimes, and complain to myself that the `join` feels noisy. Maybe I should define (&lt;=&lt;&lt;&gt;) :: Monad m =&gt; m (a -&gt; m b) -&gt; m a -&gt; m b mk &lt;=&lt;&lt;&gt; mx = join (mk `ap` mx) and write it as foo &lt;$&gt; bar &lt;=&lt;&lt;&gt; baz instead. (Need a better name for the operator, though. Maybe I should resort to `` `apM` ``.)
I don't mean to attack `pipes` which seems like a fantastic library, but it is also an example of math-think, so here's my take on those arguments: "It's a Category Theory tradition". 1) Don't look to math when it comes to readability. We don't daily handle millions of lines of proofs so the value of math traditions on readability is small. 2) `pipes` assign actual concepts to `a` and `b`, unlike category theory(?), so it isn't real abstraction, it is just non-readability by removing reference to those concepts. "Breaks down with multiple arguments". If there is a natural ordering of the arguments, we can use a,b,c or f,g,h as prefix or suffix. We can use x,x' on the variables if the function is commutative. I'm not sure I fully understood this argument. 
Seems like partial application in monadic terms to me. I'd call it partialM.
They all implement lenses in roughly the same way, sure. The difference is that the way that they have been defined in the library called 'lens' is the simplest (you only need base to define a lens!) and leads very very well to variations of different power and abilities.
I think what your example shows, is that types that are referred to as compositions of other things (i.e. transparent) are much better supported in Haskell. The compositions are standardized and the internal structure can be explicitly written in a very compact fashion. 
Well, some of the concepts you can find in Haskell are precisely the mathematical structures that they are named after. Do you propose to give them a different name? That would be silly. In particular, keep in mind that the "intuitiveness" of a name depends on your prior knowledge. Once you've learned a concept, the name becomes extremely intuitive simply because it now has become part of your intuition. At this point, a cross-discipline name helps because you can tap into knowledge from other disciplines. Or are you asking why mathematical concepts are relevant to Joe programmer in Haskell? There are two anwsers for that: * Correctness. Mathematics is the business of provings things to be this way and not another. * Expressive power. Mathematical abstractions eat kilolines for breakfast. Concerning the latter point, here a small article of mine on the great utility of [monoids and finger trees][1]. [1]: http://apfelmus.nfshost.com/articles/monoid-fingertree.html
Are the slides available somewhere?
great example of how not to build queue systems. 
Hmm, how does Idris do things like conditionals in monadic code then? As in the equivalent of `foo &gt;&gt;= \x -&gt; if x then bar else baz`? If you want to evaluate the bind strictly, you also need to evaluate `foo` as well as the lambda on the right side - but in order to do that, you have to know the result of `foo`, otherwise you can't fully evaluate the `if` expression. But getting the result of `foo` means you have to actually execute it, not just evaluate. So I'm genuinely interested how it works in a strict pure language. Maybe I should just read up on Idris then.
Their licensing policy explicitly says that &gt; To make the above practical, we promise that your client application which uses the database is a separate work. To facilitate this, the mongodb.org supported drivers (the part you link with your application) are released under Apache license, which is copyleft free. &gt; Note: if you would like a signed letter asserting the above promise please contact 10gen The AGPL really only applies if you are extending the actual MongoDB server in some way.
As I progress more with haskell, I'm very glad these names aren't changed from their original definition. What I do find very difficult is the plethora of short symbols for everything. let x = a &lt;*&gt; b ~=~ c =@= d &lt;&lt; y &gt;&gt; An example in another thread here has someone defining `&lt;=&lt;&lt;&gt;` 
Having the same problem here. Can you please post your setup code?
What if the dangerous function exhausts the heap?
Hence the word "may"... The issue is that it isn't always so cut-and-dry. Google's Chris DiBona: "With the AGPL, you have to be very, very careful with how it is expressed. Otherwise you have to invoke the sharing in many different places." See: http://www.theregister.co.uk/2011/03/31/google_on_open_source_licenses/ Disclaimer: I work at Google, and recently ran into this issue.
There are a variety of lens packages. I have been using data-lens (and data-lens-fd and data-lens-template) quite happily.
This is exactly what got me to really start disliking GHC exceptions to begin with.
Same issue: there is not a more descriptive name for those type variables that isn't wrong or misleading. Try improving that type signature and I will walk you through all the counter-examples to your intuition. Fortunately, the new documentation follows the lens tradition and accompanies them with simpler type signatures. Also, the p type parameter is gone because proxy transformers are no longer necessary.
I'm not sure I understand the question. I *think* you're asking what happens if the worker thread created by `async` receives an asynchronous exception. If so, I think this example demonstrates the concept: https://www.fpcomplete.com/user/snoyberg/random-code-snippets/async-worker-thread In other words: the worker thread (which is running dangerous) would be terminated, and the calling thread would know it was due to a heap overflow.
Perhaps the only actually bad thing about Haskell, in my opinion.
Exceptions as the means to handle interprocess communication is such a misfeature. If you look at, say, the pi calculus or erlang's CSP-like model for a bit you find toss exceptions between threads in such an implicit way to be such a wart of the Haskell concurrency story. `async` is a great abstraction, but it depends on the purity of your concurrent operations.
If anyone is actually interested in a working queue system, check out [nsq](https://github.com/bitly/nsq). It is by far the best/most reliable queue I've ever used. 
It's alright! :) I agree that Haskell has lots of pain points when it comes to types with large numbers of type variables. Both `lens` and `pipes` suffer under these limitations. Right now the state-of-the-art seems to be to a combination of two tricks: 1. Offer both concrete type synonyms (i.e. using `()` and `Void`) and polymorphic type synonyms (i.e. using `forall` to hide unused variables) 2. Use concrete types in the negative positions of functions and polymorphic types in the positive positions. The main disadvantages of this approach are duplication of type synonyms, ugly type inference, and a requirement for higher rank type extensions. The other viable alternative is to use newtypes, but that imposes a lot of extra verbosity on users of these libraries.
This is why I built `pipes-concurrency`. You can setup communication channels and have a thread gracefully terminate when all readers or writers are done, without having to use the `killThread` hammer.
&gt; This is exactly what got me to really start disliking GHC exceptions to begin with. What do you mean by a GHC exception?
Oh, I quite like `pipes-concurrency`!
I had never heard the 2 hard problems in programming, but that is pretty funny. By strawman I meant that there is a middle ground between single letter names and comedically long worst case java naming. On my team we rarely use names that are longer than 10 characters (preferring 4-6 letters), but we also avoid single letter variables unless there is no helpful alternative. Edit: Most importantly, I think your example code is well named and readable. 
consider the state monad State s a = s -&gt; (a,s) then you have foo &gt;&gt;= \x -&gt; if x then bar else baz under strict evaluation you first reduce `foo` to a value (a lambda abstraction...) and then reduce `\x -&gt; if x then bar else baz` to a value--except it already is! Then you perform the bind foo &gt;&gt;= \x -&gt; if x then bar else baz ===&gt; \s -&gt; let (a,ns) = foo s in (\x -&gt; if x then bar else baz) a ns this is a value so evaluation stops (you do not evaluate under binders ever). The key thing to realize is that becuase foo is a function, evaluation does not execute it! Perhaps the right intuition is that `&gt;&gt;=` takes a delayed computation and a continuation, and produces another delayed computation. IO is implemented in GHC as state. That is a silly way to implement IO in a strict language, but would work just fine. There are monad libraries in many strict language, not just Idris. Idris is just very haskell like. But, you might also consider (monads in scala)[ http://scabl.blogspot.com/2013/02/monads-in-scala-1.html] (monads in c++)[https://github.com/beark/ftl] (monads in ML)[http://www.cs.umd.edu/~mwh/papers/monadic.pdf] Lazyness is a small part of the reason monads are important, but it is not essential to them.
Just as an alternative, I wrote an easy parser based on CSS selectors called [HandsomeSoup](https://github.com/egonschiele/HandsomeSoup) and also wrote a [guide to parsing HTML using HXT](http://adit.io/posts/2012-04-14-working_with_HTML_in_haskell.html).
I can see how you might think that from reading the FSF FAQ out of context: http://www.gnu.org/licenses/gpl-faq.html#MereAggregation But I don't think it is possible to consider a program accessing a database to be the same program as the database itself since the entire point is to be operating separate programs. That would be very disruptive to the software industry even if there were no AGPL licensed databases. Edit: should also note that the questions being raised by Mark are not specific to the AGPL at all (the same would be true if a database was GPL licensed). It is just that under the AGPL you would probably be required to release your source code, whereas even if you had to make your entire code base GPL compatible you might not have to release your source code.
Since my brain was in Category Theory mode, for a second I was trying to figure out what properties a "book" has, to determine what a "free book" is.
`Control.Exception` is GHC's exception system. It's part of GHC's extensions to Haskell's base libraries (and relies on a few of GHC's extensions to the language itself) and provides a dynamically typed synchronous exception system (and also allows throwing any of these exceptions from pure, lazy code!). `Control.Concurrent.throwTo` builds on this and allows any of the dynamically typed exceptions to be throws asynchronously across threads. Asyncronous exceptions get throws by the GHC runtime system for some cases (such as SIGINT)
But you can't get preemption or multiprocessor support. Also, the asyncronous exception problem is a bit broader than threading (SIGINT, for example, causes an exception). Also, you *can* use `Control.Concurrent` threads and have them gracefully terminate without `killThread`, if you're using nice communications channels (though most of my threads are life-of-the-program, so I never have them terminate at all).
I like the conclusion :-)
I think you might be confusing `pipes-concurrency` with vanilla `pipes`. `pipes-concurrency` layers preemption and multiprocessor threading on top of `pipes` to build asynchronous channels, so you do get preemption and multiprocessor support. Also, anybody who has tried to write gracefully terminating and deadlock-safe code using other channel abstractions knows that it is very difficult to get correct. `pipes-concurrency` takes care of all the messy details of deadlock-safety for you, and to my knowledge there is no other concurrent channels library that does this (in any language).
A very good overview of all the problems with lazy IO and gives a good rationale for pipes/conduits. 
Yes. What is nice about `pipes-concurrency` is that the central logic is completely `pipes`-independent, so in principle you could actually separate it out into its own library that other streaming libraries could use. I mentioned this in the `pipes-concurrency-1.0.0` announcement post, but no streaming library has requested this, which is why I haven't yet spun out that functionality into a separate library.
Of course they aren't, but are you actually claiming that the best practice of coding for readability is one of those doesn't apply? That's a pretty extreme statement, how on earth do you get to that? Wouldn't the default assumption be that any language represented to the programmer as text would fall under this best practice. Hell, the entire community of literate programming disagrees in a pretty extreme fashion.
This was a randomly made up string, although I think most of those have been defined somewhere. To be clear, I'm talking about `=@=` and the like, not a,b,c... I find this difficult to follow, like in the package dimensional, they use `~=` which to me would mean roughly equal to. 4 ~= kilo meters would give 4 kilometers. The example of `&lt;=&lt;&lt;&gt;` was something I saw in another thread. Maybe I'll start to see the logic in these, but at the moment it reads like line noise.
 takeR :: Int -&gt; [a] -&gt; [a] takeR n xs = drop (length xs - n) xs What about this? It seems much more readable to me, though I am away from my own computer and so cannot test the speed.
I see. I actually put made up operators in the same category as single letter variables and abbreviations. If they are common enough, the shorthand can make sense (like abbreviations in any domain's jargon). But it is easy to overdo it and make it harder to read, especially for readers with different backgrounds. (This is similar to the tradeoff with syntactic macros: you can tailor the language to your domain, but this also creates your own little world that nobody else understands without considerable effort.) (Random note, I think ~= is regex substitution in Perl. I agree that ~= looks like roughly equal to in a math sense. However ~ is used for negation in boolean algebra, so it also looks like "not equal" to me.) I would say Perl overdoes it with operators: http://glyphic.s3.amazonaws.com/ozone/mark/periodic/Periodic%20Table%20of%20the%20Operators%20A4%20300dpi.jpg 
"take last N" won't work on infinite lists either.
&gt; I see. I actually put made up operators in the same category as single letter variables and abbreviations. If they are common enough, the shorthand can make sense (like abbreviations in any domain's jargon). Yeah, same. I'm not sure if it's bad practice or I'm just not realising how common/abstract they are. I wonder if when they're designed they are common/make sense but the author is focussing on a single library/problem. When you start working with many different ones I find it hard to follow. &gt; (Random note, I think ~= is regex substitution in Perl. I agree that ~= looks like roughly equal to in a math sense. However ~ is used for negation in boolean algebra, so it also looks like "not equal" to me.) Yes I'd see it as possibly negation too, because it's a bit more common than ¬ Ahhh perl. That's exactly what I think when I see these operators.
My view is that the point of intention revealing names for tools is to provide an accurate abstraction to the reader so they don't have to expend some of their limited cognitive effort to "figure it out", even if it is "easy" to do so. My rationale is something like this: * software is the first engineering field where complexity is unbounded by physical limitations * abstraction is our primary tool for managing/hiding complexity by elliding irrelevant details * intention revealing names provide an abstraction to the reader/client of your software, so they don't have to dig into the details (at least some of the time) 
Thanks for this elaboration, it took me several minutes to understand because it is foreign to me. Based on your description, of `zot`, it sounds like a Haskell function can only operate on the inputs, and so you can infer the implementation of `zot` from the inputs in the type. I may just misunderstand Haskell here, but couldn't the implementation of `zot` have access to a curried function `g` in addition to the `f` visible in `zot`'s type? (Either `g` was passed in earlier, or `g` is available at module level, assuming that exists in Haskell.) If so then the elements of the list could really be anything, since the type doesn't tell us anything about `g`. I feel like I'm missing something here due to my limited Haskell exposure (sorry about that btw). 
&gt;any implementation can possibly work on infinite lists No problem. Because you know the length ahead of time you don't need to walk down the list to get its length: takeR n xs = drop (∞ - n) xs Errr... maybe not! ;)
I like the entry, but I can't shake the feeling that the best solution is not to use lists; use a FIFO queue, [`Data.Sequence`](http://hackage.haskell.org/packages/archive/containers/0.2.0.1/doc/html/Data-Sequence.html) or something similar. I'll have a shot at benchmarking this... **EDIT:** [Gist of my benchmark suite, comparing a `Data.Sequence` version to nomeata's](https://gist.github.com/sacundim/5988041); here are my results: benchmarking takeRIdiomatic/100000 mean: 364.1090 us, lb 361.5556 us, ub 367.2538 us, ci 0.950 benchmarking takeRIdiomatic/100 mean: 501.3094 us, lb 497.2897 us, ub 505.6725 us, ci 0.950 benchmarking takeRSeq/100000 mean: 973.7043 ns, lb 916.4838 ns, ub 1.030925 us, ci 0.950 benchmarking takeRSeq/100 mean: 2.728465 us, lb 2.568725 us, ub 2.957347 us, ci 0.950 I'm a bit shaky on whether I've somehow made my `takeRSeq` test do too little work (unevaluated thunks) compared to the others, so please point out any mistakes...
I honestly thought meaningful names was as close to fact as you get until I asked this question on this subreddit. I also thought Option types and the notion that null was Tony Hoare's "billion dollar mistake" was established knowledge. But then I suggested it to a room full of Python programmers and was told that it wasn't "Pythonic" and in Python "we're all consenting adults". I'm not exactly sure what I should conclude from these experiences. 
I said "readable variable names is a best practice" and you said "The "best practices" of ... and what's best for Haskell is not the same for what's best for Java". So I asked why you thought the best practice of readable variable names, from the assumption that code is read more often than written and therefore readability is important, doesn't apply to haskell.
I think you can just conclude that people don't actually read your comments on reddit before arguing with you or just want to argue most of the time. Also, for some reason, /r/Haskell seems to be full of insane people. Tactics is trying to tell me that the best practice of meaningful variable names is from Java land and doesn't apply to Haskell right now in a very confused way.
I just added yours to [my benchmarks](https://gist.github.com/sacundim/5988041), under the name `takeRNaive2`, and here are my results: benchmarking takeRNaive/100000 mean: 6.445709 ms, lb 6.426519 ms, ub 6.465715 ms, ci 0.950 benchmarking takeRNaive/100 mean: 2.304421 ms, lb 2.295070 ms, ub 2.314482 ms, ci 0.950 benchmarking takeRNaive2/100000 mean: 329.8719 us, lb 327.4401 us, ub 332.6662 us, ci 0.950 benchmarking takeRNaive2/100 mean: 690.8663 us, lb 686.9705 us, ub 695.0171 us, ci 0.950 benchmarking takeRIdiomatic/100000 mean: 365.1245 us, lb 362.4256 us, ub 368.6483 us, ci 0.950 benchmarking takeRIdiomatic/100 mean: 493.1290 us, lb 489.6887 us, ub 497.2322 us, ci 0.950 
I invoke http://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines No.
Oops! Yeah, you are right. :)
I've been using http://opengl-tutorial.org, it's pretty good and modern. It uses glm a lot though (which has no haskell bindings) so I just made my own perspective/lookAt/... functions as I needed to with hmatrix - although this isn't great either because it means I have to construct a new 4x4 matrix that is only used once for every model in a scene (and many more if you were to do e.g. rigging). I've been working on a matrix/vector library that has functions like `multiply :: Matrix f (i:.j:.Z) a -&gt; Matrix f (i:.j:.Z) a -&gt; Matrix (Impure f) (i:.j:.Z) a -&gt; IO ()`, but it is very young and unstable. http://ogldev.atspace.co.uk/index.html is also good.
I used [JWZ's youtubedown](http://www.jwz.org/hacks/youtubedown) script with the video's [vimeo link](http://player.vimeo.com/video/70060895). Worked for me. 
Have you looked at [GLUtil](http://hackage.haskell.org/package/GLUtil) by any chance? It uses Edward Kmett's [linear](http://hackage.haskell.org/package/linear-1.1.4) package, which seems well suited for this application.
I generally don't like the design of OpenGL (the haskell package), because 90% of the time I find it easier - especially when following OpenGL tutorials/documentation - to just use OpenGLRaw. As for linear, I'll try make some benchmarks for my use case, which, with hmatrix, is roughly: v &lt;- readIORef view -- updated by a seperate game logic loop p &lt;- readIORef proj -- updated by a seperate game logic loop F.for_ stage $ \ (Ent pos _) -&gt; let mvp = p &lt;&gt; v &lt;&gt; translation pos -- with &lt;&gt; being matrix multiplication drawModel cube simpleShader mvp -- other data depending on the shader goes here With hmatrix, since it's based on Storable vectors, it's easy to get a ForeignPtr and then a Ptr to upload with glUniformMatrix4fv. With linear, I'd have to go through the matrix and write each element to an array (which at least would be shared throughout the program). But this might not be so bad, because I suppose it's not "guaranteed" to have to allocate a completely new matrix for each operation (like hmatrix functions are). I hope to upload a package with my OpenGL utility things that I'm slowly accumulating, of particular interest is my handling of uniforms. The `drawModel` function is `:: Upload s =&gt; GLmodel -&gt; Shader s -&gt; s`, where the `s` variable statically determines the uniforms that need to be uploaded, and because GHC is neat and each step of `Upload`'s function gets to be inlined, `drawModel` compiles to very nice Core code that doesn't traverse any `Shader` GADT. I'll give GLUtil a go, but as I've probably made exceedingly clear, I really don't want to have to worry about any overhead from ugly OpenGL (the haskell package) abstractions, or naiive implementations of matrix operations. 
If this is supposed to mean that, for example, takeR 3 xs = case xs of _:_:_:_:_ -&gt; takeR 3 $ tail xs _ -&gt; xs then this has the wrong asymptotic complexity anyways (O(nL) for `takeR n xs` where `xs` has length L), barring some extremely clever compiler optimization to share the work of pattern-matching `tail xs` with that done while pattern-matching `xs`.
Even if `zot` is making reference to some other function like `g`, we know that `g` can't assume anything additional about the types than `zot` can— if `g` made additional assumptions, then `zot` would inherit those restrictions on the types. That is, consider an implementation like: zot f [] = [] zot f (x:xs) = [f (g x)] Now, what type could `g` be? Well, it has to be able to accept the argument `x::a`, and it has to return something`(g x)::a` for `f` to accept. But that means, we have `g :: a -&gt; a`, and it has to work for all choices of `a`; hence, `g` must be the identity function (or loop forever, etc). zot f [] = [] zot f (x:xs) = [g (f x)] Here, we get basically the same thing, except now `g :: b -&gt; b` (as far as `zot` is concerned; as far as `g` is concerned, this is identical to `g :: a -&gt; a` since variable names don't matter). So `g` must be the identity function (etc). zot f [] = [] zot f (x:xs) = g [f x] Here, we have that `g :: [b] -&gt; [b]`. Since we know nothing about `b`, we can't really mess around with the actual elements; all we can do is reorder the list, duplicate some elements, or drop some elements. But then, these are all things `zot` could've done anyways. The last way we could use some `g` is to do something like: zot f _ = [f g] But notice that we have `g::a` even though we know nothing about the type `a`! Thus, `g` could only be an infinite loop (or `undefined`, or exception throwing,...). To be fair, I did neglect to mention this possibility in the previous post. So, even if we allow `zot` to access some other term `g`, being able to do that doesn't increase the possibilities of what `zot` can do.
&gt; it sounds like a Haskell function can only operate on the inputs, and so you can infer the implementation of zot from the inputs in the type. Well, you can also reference anything that's in scope, as in other languages. The big thing is that, because of the polymorphism, we get strong guarantees about what sort of things we're not allowed to do. E.g., because `foo :: a -&gt; a` is polymorphic in `a`, we can rule out all sorts of things that a function might be able to do in general. The only possible implementations of `foo` are equivalent[1][2] to one of: (\x -&gt; x) (\x -&gt; undefined) undefined Whereas if we had `quux :: Int -&gt; Int`, then it could do all sorts of things, since it knows it's dealing with an input of type `Int`. [1] Here I'm collapsing the distinction between `undefined`, infinite loops, exception throwing, etc. [2] Note that `(\x -&gt; undefined)` and `undefined` are different in Haskell. Namely `seq (\x -&gt; undefined) y` is equivalent to `y`, whereas `seq undefined y` is equivalent to `undefined`.
Excellent. Thank you!
From what I've heard, Haskell is in the camp of "just don't do that". My understanding is that Linux takes the same policy for C/C++ memory allocation (by default). The allocator will keep succeeding as long as you have virtual address space, so is effectively infinite on x64. Now, if you go any try to *use* that memory and run out, your process is terminated abruptly. Disclaimer: I'm not a Linux guru - someone correct me please.
Linux won't kill *your* process. It will kill a randomly chosen process. A lot of people frown at that, but it's how it is.
As I said, I agree with Tekmo's other points.
And the more idiomatic "idiomatic solution" (not to use recursion): \n l -&gt; last $ l: zipWith const (tail $ tails l) (drop n l) I wish this could be optimized to be as fast as the recursive solution. 
Yes, I know the answer :) I was just trying to point out that isolating dangerous computations is very difficult in ghc's implementation of Haskell. You can protect yourself against some dangers, but not all.
It seems that many people here don't like explicit recursion, but for this type of tasks I usually find the explicit recursion much easier to read and understand than the code golf one-liners they produce. After all, we don't write the code just for the machine...
It is all explained by many people in this thread already, but here is a tl;dr version: Math vocabulary is one of the most stable, and more importantly, definitely the most *precise* existing vocabulary out there. Furthermore, you don't want to rename primary school addition to "combining-apples-machine", do you? Non-descriptive and one-letter names: I imagine you won't write for(thisIsASpecialVariableForLooping=0;thisIsASpecialVariableForLooping&lt;ALimitWhichIsRatherBig;thisIsASpecialVariableForLooping++) { ... } Specifically for a,b,c: What about i,j,k? Or am I outdated and people don't use those anymore? 
In this use case I agree, but this one-liner is superior at least in the following use-cases: - it is easier to prove that it terminates and total, considering that (tail . tails) and (last . (l :)) are total - it can be used in a domain-specific language with functions zipWith, tails, ... but without recursion
Beautiful!
What about this? takeR :: Int -&gt; [a] -&gt; [a] takeR n l = last . takeWhile (not . null . fst) . zip (tails . drop n $ l) . tails $ l I haven't tested it.
That's a Linux bug. There's no pressing reason to accept over-commit when allocating memory.
you forgot a `snd` I think, unless you're using some special `last`. adding in a `snd .` in front gives the type signature you're expecting, but trying out, say, `takeR 3 [1..1000]` gives `[997,998,999,1000]`. Using `dropWhile` and not `takeWhile` produces the expected result: λ let takeR n l = snd . last . dropWhile (not . null . fst) . zip (tails . drop n $ l) . tails $ l λ takeR 3 [1..1000] [998,999,1000] I thought you meant "tested" as in "tested for efficiency", not "typed it in ghci to check that it works".
Nice. And the corresponding function for drop actually works on infinite lists. revDrop :: Int -&gt; [a] -&gt; [a] revDrop n xs = loop (drop n xs) xs where loop [] _ = [] loop (_:as) (b:bs) = b : loop as bs 
Also, the philosophy of never using manual recursion, when taken to the extreme, produces a menagerie of recursion schemes and then you get things like [Zygohistomorphic prepromorphisms](http://www.haskell.org/haskellwiki/Zygohistomorphic_prepromorphisms)
This is revDrop n xs = zipWith const xs (drop n xs)
I just wrote [some tagsoup stuff](https://github.com/chrisdone/haskellnews/blob/master/src/HN/Model/Soup.hs) for [Haskell News](http://haskellnews.org/). TagSoup is nicer with [a monad](https://github.com/chrisdone/haskellnews/blob/master/src/TagSoup.hs).
Length runs in constant space because it also discards elements as it looks at them. It probably will cause more cache misses on large lists though since it does pull each element of the list into memory once for the call to length, then have to pull most of them in again for drop. It becomes really good though when you change the underlying data structure to store it's length.
`length` in isolation runs in constant space, but if you also have a `drop` waiting to use the same list then the traversed elements will not be freed. A data structure that stores the length only works if the programmer already knows the length and provides the length out of band (like `fromListN` for `vector`). Otherwise you have to load the entire data structure into memory to know what the length is.
I know of a teacher that uses buckets as explanations for variables, and buckets on a pole for arrays.
While you are here, what happened to hpaste?
Type inference doesn't only have to be tractable, it also has to produce comprehensible types. This is the beauty of H-M, tractable (in practice, not in theory), and the types make sense. If the types don't make sense then type errors are going to be too difficult.
The old domain is lost apparently, check [this blogpost by Chris](http://chrisdone.com/posts/hpaste-update) and [this IRC log](http://ircbrowse.net/browse/haskell?id=16041564&amp;timestamp=1373378731#t1373378731)
Here's the sorry affair: * I was paying a lot for a Linode virtual machine. * I got a Hetzner dedicated host. * I moved all my services (chrisdone.com, tryhaskell.org, haskellnews.org, ircbrowse.net, hpaste.org) to Hetzner. * I don't own the `hpaste.org` domain, so I changed the Linode to be a reverse proxy to the Hetzner host, thus the service would continue to function transparently, giving a two week period for the owner of the `hpaste.org` domain to point it to the new host (a 5-minute task) before I shut down the Linode. * After the two week period I closed down the Linode account and setup a temporary domain at `paste.tryhaskell.org` and approached the Haskell committee to get a `paste.haskell.org` domain setup: a community-controlled domain would be more responsive to change rather than one person. * Two weeks later, absolutely zero response from the sysadmin responsible for Haskell-related services (despite Gershom's very helpful work on trying to get something to happen). * A month after the initial move, I register the domain `lpaste.net` (as in “lambda paste”) and consider that the canonical address, with additional retroactive reasoning that not just Haskell people use it, but functional programmers in general. * #haskell channel [adapts to change quickly](http://ircbrowse.net/browse/haskell?q=lpaste). * Life goes on. Long live [lpaste](http://lpaste.net)*!*
After a while this sort of reasoning becomes second nature. Though, of course, for complicated types it can be easy to get lost if you're just doing it in your head (e.g., [this wonderful gem](http://winterkoninkje.dreamwidth.org/81209.html)). For the simple functions (`id`, `const`, `fst`, `snd`,...) the names are there to give you a hint about what they do— though they tend to be abbreviated and so require a little learning (identity, constant, first projection, second projection,...). If the parametricity is just being used in a simple way like for these basic functions, then it usually goes unremarked; it's just a nice thing to notice. The key point is that this is enforced by the type system, even if the author or reader doesn't notice it. However, if the parametricity is being used in a tricky way, then there's usually a comment about what the heck is going on. For example, that gem I mentioned above, or things like `callCC :: MonadCont m =&gt; ((a -&gt; m b) -&gt; m a) -&gt; m a`. You can be pretty sure from looking at the type that there's no way that the `(a -&gt; m b)` function is ever going to actually return a value of type `b`. But the documentation for the continuation monad will discuss examples that make it clearer what's going on. For example: foo = callCC $ \exit -&gt; do ... if ... then exit x else do ... return y Basically the way it works is that inside `callCC` we're given access to a procedure `exit :: a -&gt; m b`. Now we can either ignore that procedure and go on to return some value as usual, or we can use `exit` in order to exit the block immediately. There are, of course, trickier things we can do by passing `exit` off to other functions, or nesting multiple calls to `callCC`, but that's the basic idea. As far as automatic generation goes, the [Djinn](http://hackage.haskell.org/package/djinn) tool can automatically generate definitions —for types that correspond to propositions in intuitionistic logic—, or can tell you that such a definition is impossible. You can also configure it to give multiple definitions (e.g., give both interesting definitions of `a-&gt;a-&gt;a`). It doesn't deal with nontermination, exceptions, or any of that though. On the other side of things, given the type you can also [automatically generate free theorems](http://www-ps.iai.uni-bonn.de/cgi-bin/free-theorems-webui.cgi) which describe how the function must behave. For example, it will prove that the `filter` function satisfies the following law, based only on its type: forall (a, b :: *) (f :: a -&gt; b) (p :: a -&gt; Bool) (q :: b -&gt; Bool). (forall x::a. p x = q (f x)) =&gt; (forall xs::[a]. map f (filter p xs) = filter q (map f xs)) or more colloquially, for all choices of `p` and `f`: filter p . map f = map f . filter (p . f) That url has a number of different flags you can turn on or off depending on whether you want your theorems to be truly precise about Haskell (which includes nontermination, exceptions, etc) or if you want to pretend you're in a more idealized language that lacks the "weird stuff".
&gt; a community-controlled domain would be more responsive to change rather than one person You would think so...
I watched this, and learned a lot about pipes/conduit. However, I don't see how pipes/conduit/resourceT actually solves the safety problem. A pipe or a resourceT will release resources when the result is returned, but they can only give any guarantee for a result that is in WHNF. Thus, I don't see why you can't shoot yourself in the foot using resourceT/pipes just as you can by using lazy IO. To me it seems that anything that deals with resources must ensure that all functions have actually executed before returning, and that means that the result *must* be NFData, everything else is unsafe one way or another. So pipes and resourceT are both unsafe, just slightly less so than lazy IO. The bracket technique (withSomething) is similarly automatically safe by ensuring that the returned data is NFData. I don't think the talk discussed this, to me, crucial point. I do understand that pipes etc. solves complex resource relationships, but that is *typically* not the problem. Having a file descriptor closed at the absolutely earliest point in time is not the problem I have. The problem I have is to find a point in time where I can safely close the file descriptor, and that point in time must not be finalization of the file handle, and the API must be easy to use. 
&gt; in practice, not in theory Do you mean in practice *as well as* theory, or is H-M theoretically intractable somehow?
So write it in ghci
this is the same as `takeR n = snd . last . liftM2 zip (tails . drop n) tails` I think; it seems to have nice memory properties, but it systematically takes exactly 3 times as long as noemata's, I guess because of the detours via `tails`.
It ignores malformed html. See [this line](https://github.com/egonSchiele/HandsomeSoup/blob/master/Text/HandsomeSoup.hs#L44). [Here are all the options that HXT provides](http://hackage.haskell.org/packages/archive/hxt/9.0.0/doc/html/Text-XML-HXT-Arrow-XmlState-SystemConfig.html).
I don't know enough right now to say exactly how it is. But my hunch is that pipes/conduit are safer than you think, maybe for reasons that are not at all obvious. If you think there are problems, it might help to make small abuse programs to try to demonstrate them. If you can, excellent. If you can't, excellent. I appreciate your skepticism.
ListT is pretty common and not actually a monad transformer.
I get troubled by these sometimes... but I'm not sure I always agree with this dogmaticism...
I just benchmarked matrix multiplication in linear vs hmatrix, and puzzingly (since hmatrix binds to lapack, which should be very fast, and linear is a "naiive" implementation), linear is ~3x faster, with ~100ns compared to ~330ns. {-# LANGUAGE ScopedTypeVariables, BangPatterns #-} import Linear import Numeric.LinearAlgebra import Criterion.Main import Control.Monad main :: IO () main = do !(hm0 :: M44 Float) &lt;- readLn !(hm1 :: M44 Float) &lt;- readLn !(lm0 :: Matrix Float) &lt;- readLn !(lm1 :: Matrix Float) &lt;- readLn {- some input: V4 (V4 3 2 4 5) (V4 1 3 4 5) (V4 2 3 4 1) (V4 5 1 5 2) V4 (V4 3 3 5 6) (V4 4 3 54 5) (V4 2 3 4 23) (V4 5 4 5 3) (4&gt;&lt;4) [3,2,4,5,1,3,4,5,2,3,4,1,5,1,5,2] (4&gt;&lt;4) [3,3,5,6,4,3,54,5,2,3,4,23,5,4,5,3] -} unless (rows lm0 == 4 &amp;&amp; cols lm0 == 4 &amp;&amp; rows lm1 == 4 &amp;&amp; cols lm1 == 4) (error "matrix must be 4x4") defaultMain [ bench "hmatrix mXm" (mXm lm0 `whnf` lm1) , bench "linear !*!" ((!*!) hm0 `whnf` hm1) ] 
I really like HandsomeSoup.
Zero complaints regarding the Haskell Committee, everyone was very responsive and helpful! I can see you all are taking the initiative to get in control of services so that things like this will be smooth in the future. I was quite happy to see that!
That's a good heuristic. AST recursions are also often "decorated", so having more composability in your recursive step is nice.
fwiw, Conrad Barski is the author of the book "Land of Lisp".
I really enjoyed Land of Lisp, if Conrad would publish a book on Haskell, I would probably buy it, even just to look at the funny drawings along the way.
Very interesting. The AngularJS + Yesod/Snap approach is a very attractive one. Good to see that it is picked up.
Ah right, I did not know that, thanks.
Yesod + Angular is great. Angular has a bit of a learning curve and the documentation is only so-so (they're working on that), but it's extremely effective once you get going. It would have taken us a lot longer to get to where we are with BayesHive without it. Of course, one day, Fay or GHCJS will allow us to have something like Meteor in Haskell and then we won't have to write any JavaScript at all. (We can all dream, can't we?)
Someone should port [Ur/Web](http://www.impredicative.com/ur/) to Haskell ... or something.
it reminds me of the following 'tutorial' I really liked: http://patryshev.com/monad/m-intro.html
Indeed.
Thank you for doing that! You'll see that I added a similarly simple benchmark to vinyl some time ago, but I hadn't done one for linear yet. If GHC inlines enough, it has competitive performance, and if the number of arithmetic operations is as small as this, overhead dominates.
I find the title and some claims in the paper misleading. &gt; Here we show that—paraphrasing a famous article by John Reynolds [19]—subtyping of polymorphism is set-theoretic. Except Reynolds wrote about System F, and they are talking about prenex polymorphism. That's what makes it set-theoretic, and not the addition of subtyping (obviously).
The derivation of Functor a c (FComp g f) is broken. FComp f g instead of FComp g f. Further more, it only works if c == Hask. Finally it needs more explicit type declarations to actually type-check in my GHC (version 7.4.1) -- though you could argue that this does not actually have any influence on the merit of the article itself. A fixed version would be: instance (Functor b Hask f, Functor a b g) =&gt; Functor a Hask (FComp f g) where fmap (f :: a p q) = FComp . fmap (fmap f :: b (g p) (g q)) . unCompose Without extra type signatures this would be: instance (Functor b Hask f, Functor a b g) =&gt; Functor a Hask (FComp f g) where fmap f = FComp . fmap (fmap f) . unCompose I have not been able to figure out how to implement the instance without the assumption that c == Hask, unless haskell allowed type-level lambdas. If this was the case, then this should work (assuming (.) was extended as a type operator): instance (Functor b c f, Functor a b g) =&gt; Functor a c (f . g) where fmap = fmap . fmap 
I use the Angular/Yesod mix too. How do you handle the client side versus server side route interpolation? static type checking of routes is awesome in yesod and not having something similar for the client routes is bother some
Two more slight mistakes: The (&gt;&gt;=) that is implemented, is actually (=&lt;&lt;), since (&gt;&gt;=) f == (f &gt;&gt;=). He also seem to have forgotten the "(Id a)" part of your definition of a Monad, since almost none of your haskell code after this point typechecks completely, for lack of an Id constructors. For instance: "id = Kleisli eta" should have been "id = Kleisli (eta . Id)".
I would have assumed mu would be defined as mu :: c (FComp t t a) (t a)
Yeah. One neat thing I noticed was that with static data, ghc would fully evaluate a call to `!*!`. I have a feeling though that this isn't unavoidable FFI overhead, as the wrappers hmatrix has around LAPACK are somewhat grotesque and not inlined (http://hackage.haskell.org/packages/archive/hmatrix/0.15.0.0/doc/html/src/Numeric-LinearAlgebra-LAPACK.html see `multiplyF` and `multplyAux`). FFI overhead was like 10-20 ns last time I checked.
Let me tell you the excercise I went through to start answering this question, because I think that is just as important as the end result. First I found the type of `(&gt;-&gt;)` which is (&gt;-&gt;) :: (Monad m, Proxy p) =&gt; (b' -&gt; p a' a b' b m r) -&gt; (c'_ -&gt; p b' b c' c m r) -&gt; c'_ -&gt; p a' a c' c m r Then I found my original post which mentions: p su ru rd sd m r Aha! Now I can *start* trying to rewrite the type above. I found a type that made little sense to me, then found my own "notes" regarding what the types mean, then starte working on the problem. Now let's rewrite (&gt;-&gt;): (&gt;-&gt;) :: (Monad m, Proxy p) =&gt; (rd' -&gt; p su ru rd' sd' m r) -&gt; (x -&gt; p rd' sd' rd sd m r) -&gt; x -&gt; p su ru rd sd m r We want `p su ru rd sd` as the result of the composition. Some of the type variables in this result will we see in the left/upper proxy and some will we see in the right/lower proxy. We see that `su ru` are in the first argument, and since `su ru` mentions the 'upstream', we know that the first argument must be th left/upper proxy. We see `rd sd` in the second argument and intuitively this must mean that the second argument somehow is the right/lower part of the composition (downstream). Now the problem you probably had is that the internal type variables `rd' sd'` could also be named `su' ru'`. I think this is less important than having the basic proxy type readable. But let's try to fix that. The concepts 'send/receive' and upstream/downstream are inverted for the two Proxies during composition. So instead we use left for pointers going left in your diagram and right for pointers going to the right. Then we get: (&gt;-&gt;) :: (Monad m, Proxy p) =&gt; (lft -&gt; p su ru lft rgt m r) -&gt; (x -&gt; p lft rgt rd sd m r) -&gt; x -&gt; p su ru rd sd m r The advantage: Easy to see what the positional type names mean. Downsides: upstream/downstream send/receive are subjective (according to a given proxy) names, not objective names. So let's try another version where we use only objective names: a,b,c and left and right. So left is data-flow going to the left, right is data-flow going to the right. The sequence a,b, and c are the boundaries between proxy objects. (&gt;-&gt;) :: (Monad m, Proxy p) =&gt; (bl -&gt; p al ar bl br m r) -&gt; (x -&gt; p bl br cl cr m r) -&gt; x -&gt; p al ar cl cr m r This is starting to look like the current naming convention. This naming convention makes it harder to understand what the basic proxy type means, but it makes the composition operator simpler to write. The use of `left` and `right` has less information loss than `x/x'` though. If the documentation talked about upstream/downstream as `a/b`, then I think that type would be reasonable. Now, having gone through this excercise, I think the most useful naming convention is to not use send/receive as it is subjective naming which has issues with composition, but rather left/right, and then use either upstream/downstream or a/b/c depending on the function, so the base type could be: p ul ur dl dr m r But in composition it could be p al ar bl br m r -&gt; p bl br cl cr m r -&gt; p al ar cl cr m r This has the best readability by always using left/right which gives information to the reader unlike x/x'. It then also uses upstream/downstream if possible. Only when this is not possible are upstream/downstream abstracted into the sequence a,b,c to indicate the proxy object boundaries.
This is something for which we don't have a good solution. There's a lot of string concatenation and calls to encodeURIComponent in the client code. That makes me a bit unhappy, but I don't really know what else to do. This is one of the reasons why any approach that still involves writing JavaScript code feels like it can't be more than a stop-gap on the way to a full end-to-end Haskell stack. I'd love to try using Ur/Web, but the type system scares me! I have high hopes for the GHCJS support that's scheduled for inclusion in GHC 7.8. That looks like it's going to be *really* cool.
oooh I didn't know about that. .. very exciting!
What about the type signature for the flipped operator, `(&lt;-&lt;)`? Is left always synonymous with upstream or does it become downstream when you flip the operator? I do agree with the part about not using the terms send and receive. Their meaning breaks down when you start using other forms of composition (i.e. `(/&gt;/)`). However, I do generally agree with the idea of using positional terms like upstream and downstream and left/right. The problem is that I don't know of good synonyms for upstream and downstream that preserve their meaning when you flip the operator direction.
Things are made slightly more difficult, since `&gt;=&gt;` is what Haskellers would typically write as `&lt;=&lt;`
For a situation where high throughput is not a requirement, but ease and speed of development, availability in multiple environments, ease of debugging, and reliability are important, what is so bad about it? RTFA perhaps?
success!
I don't have access to an interpreter atm, so maybe I'm missing something, but I feel like this communicates the intent far more clearly: takeR n xs = fst $ last $ zip (tails xs) (drop n $ tails xs)
I feel like this is against the spirit of an "unlocked" processor...
An aphorism I posted on the Hacker News comments page for this article, and have since become quite fond of, goes "Explaining Haskell's `Monad`s in terms of category theoretical monads is like Explaining `Data.Set` in terms of the ZF axioms.". Haskell's `Monad`s are a very, very special case of category theoretical monads, and far simpler. Who's to say the best description of them is in terms of monads anyway, and not, say, categories with coproducts?
Finger Trees are a jaw droppingly amazing datastructure!
Fuck me ... check out the HTML that site generates. I got curious when I enlarged the text on my tiny netbook screen and wrapping went all crazy. Seriously WTF. Nice article though. *EDIT: **really** nice article .. unfortunately the HTML reflows so terribly that I had to give up reading after a few truncated code snippets made the distraction too bad. I'll come back when I'm on a bigger screen, or when I can be bothered copying the text out of it and the taste of layout done completely wrong has left my mouth. Such a shame to so detract from a very well-written piece.
or, you might go more restful by also using "hypermedia as the engine of application state": you write out the routes in your resources and don't construct them on the client at all but read them in.
That's a very point point. I do believe this article gives a pretty good introduction to *categorical* monads by use of Haskell monads; the other way around, not so much. Edit: I accidentally a word.
A quick note about join (fmap when (pure True)) $ print "Yay!" I almost broke my brain trying to understand why it works. It actually isn't. Or at least not in the way that you'd think it works. Which you can see by trying: join (fmap when (pure True :: IO Bool)) $ print "Yay!" which fails. Actually the 'pure True' in this piece is of a really weird type: join (fmap when (pure True :: IO () -&gt; Bool)) $ print "Yay!" And the 'join' does not actually joins in IO, but in this weird ((-&gt;) (IO ())) reader monad. :) So, this construct is *not* equivalent to 'om when', and is not actually useful for anything. ;) 
This is fortuitous, I just started learning about finger trees last week, and here we go. Haskellers are amazing, you have all the good data structures. Elsewhere there's only arguments about when to use linked lists and when to use arrays.
Makes the first two sentences particularly ironic: &gt; A while ago, I wrote a href="http://scienceblogs.com/goodmath/2009/05/finally_finger_trees.php"&gt;couple of posts that claimed to talk about finger trees. Unfortunately, I really botched it. ... although in his defense, it's probably the site acting weird and not the author screwing up his writing here.
Indeed the html is orrible, probably because the theme has changed during the time (the article is dated 2010), but content is cool.
Yes, they're in the [Well-Typed blog post on the talk](http://www.well-typed.com/blog/79)
GHC Haskell isn't a pure language. You can break out of the theoretical foundation. You don't have to use unsafe libraries to find unsafe functions. The Prelude is broken, too (see `head`). It's a matter of convenience; a readability hack. Note that there are safe versions of the prelude. Note that your function only does something after `a` is evaluated, which requires an oracle for TM.
Thanks a bunch!
The first instance of main has one expression, but uses do notation (needs more hlint ;) ). Also uses putStr and a manual newline instead of putStrLn 
Forcing "pure" values is only necessary if forcing that value may cause some kind of side effect. Without that forcing a value is unobservable (except perhaps by looking at how much memory your program is consuming). That in turn means that forcing values is only necessary if you are using unsafePerformIO or unsafeInterleaveIO, because that is the only way for a pure value to become "impure". Neither pipes/conduit nor io-streams makes use of these unsafe primitives, so there is no need to worry about forcing values. If you are wondering what forces the execution of the side effects without using unsafePerformIO/unsafeInterleaveIO, it's the standard execution of the underlying monad -- in the case of the talk, the IO monad. The kind of programs that I illustrated in the talk with lazy I/O therefore cannot happen with pipes/conduit/io-streams/... In a sense, that's kind of the point of the talk: how do we get the nice decomposition that lazy I/O admits, but without using unsafeXXX and hence without getting into these problems. (Incidentally, the withXXX primitives from the standard library have nothing to do with NFData: they simply give you a scoped handle, and you should not be using that handle outside that scope. In particular, combining withXXX with Lazy I/O is a recipe for disaster.)
Also, I'm fine with glossing over the IO monad a bit, but it's not "breaking the rules of Haskell". The tutorial itself said that IO is not allowed just anywhere because of types, so one could just explain it as "so we add IO to the type"
I guess you are right :) I wasn't able to find many references to "Safe Prelude"s. Are you referring to this: http://www.haskell.org/haskellwiki/Non-empty_list
&gt; You don't have to use unsafe libraries to find unsafe functions. The Prelude is broken, too (see head). It's a matter of convenience; a There is a difference between unsafe and IO stuff and functions like head. `head []` has a perfectly fine semantics (⊥), and it can exist in a pure language. On the other hand, `unsafeCoerce` and `catch` can not exist in a pure language, since they break type safety and referential transparency respectively.
Probably meant something more like [this](http://hackage.haskell.org/package/safe).
To be pedantic, `head` can exist in a pure language if, as most Haskellers do, you don't think of partiality as an effect.
As it turns out, `Data.List` is much closer to the ZF axioms. Or `Fix List` rather...
But then it breaks the type system, because then you need to treat undefined as a value of each possible type including Void. It can exist in a pure untyped language.
As I said, I see how the specific problem with lazy IO is solved, but I don't see how the problem of timely resource reclamation is "safe" because references to resources can escape through the result. For example, if I have a pipe that has the result type [IO ()], then that list of IO actions can refer to a file descriptor that is closed when I execute those IO actions. Thus while safe within the pipes abstraction, it isn't safe in general. Similarly withOpenFile etc. are safe within the bracket, but not safe in general. However, if the result type had to be NFData, then [IO ()] wouldn't be allowed as a result type, if I understand correctly.
Ah, yes, you could indeed return an IO action that refers to resources allocated *and* deallocated within the pipe. That's in a sense analogous to combining lazy I/O with the withXXX combinators. Whether or not NFData helps here is debatable. IO () may or may not be given an NFData instance; after all, all that NFData intsance would be talking about would be the IO action *itself*, not its result. It is conceivable that one might address this in a similar fashion to how the ST monad prevents references from escaping, but that would require some research.
I don't think you understood me. I'm suggesting that there are reasonable definitions of "pure" which don't allow `undefined` at all and would require the possibility of nontermination to be captured the way IO is in Haskell.
&gt; I do believe this article gives a pretty good introduction to categorical monads by use of Haskell monads; the other way around, not so much. I agree. &gt; Edit: I accidentally a word. You still?
There is no limit to how badly you can violate Haskell semantics using `unsafeCoerce`. That's why we don't call it "justALittleUnsafeCoerce".
Haskell doesn't even have formalised semantics, so it can't be broken, either. *Especially* when unsafeCoerce is involved. There's other, less drastical cases where GHC can turn (some) diverging bottoms into exceptions that you then can catch, like waiting infinitely on an MVar. What you're stumbling accross is exactly such a situation: The (observable) different between bottom as in the meaning and bottom as in the runtime exception. It's generally seen as acceptable to turn the former into the latter (because a crash is better than a hang), and as inadvisable to rely on catching the latter. Catching pure exceptions is evil in any case, btw. Just make your functions total, you don't want any others.
touché :)
I've been picking it up as I go along. I believe that Pierce's [TAPL](http://www.cis.upenn.edu/~bcpierce/tapl/) is considered a good and classic resource, although you may find Harper's [PFPL](http://www.cs.cmu.edu/~rwh/plbook/book.pdf) more useful if only because it's available in full online for free (under CC BY-NC-ND). The branch of math is called type theory, and it's typically a computer science discipline.
Yes, this is an abusive case (*a* case?) of `unsafeCoerce`. But I want to nip this: &gt; You're not using Haskell. I've seen this kind of argument from time to time in the Haskell community. It's a [No True Scotsman](http://en.wikipedia.org/wiki/No_true_Scotsman) fallacy. Regardless of whether you're using `unsafeCoerce`, some GHC-specific language pragma, or you have just broken your code with an misplaced `seq` or some known-to-be-broken GHC-specific concurrency or exception mechanism, your still using Haskell. Similarly, GHC Haskell *is* the de facto definition of what the world sees as "Haskell", in spite of the existence of the Haskell 2010 standard. Haskell can do all the great things lambdabot says it can. But it can't dodge its faults by redefining itself mid-argument :)
It's not "No True Scotsman" because there's a language standard to cite... And this is exactly the kind of behaviour I demand from unsafeCoerce. Nothing to see here, really, except how powerful/raw the FFI language is.
I'm as far from an expert as can be, but from what i've seen in other discussions, you can actually somewhat justify this semantic slip-up, as per Fast and Loose Reasoning in Morally Correct. Right? http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html
&gt; It's not "No True Scotsman" because there's a language standard to cite... The standard is largely irrelevant to the Haskell community. If we use the standard as the definition of the language, we also have to admit Haskell doesn't have GADTs, a template language, or rank N types either. (But if you tried to press that, you'd get funny looks from people: "... But Haskell *does* have those things...").
The Baysig language itself sounds really fascinating. I have pondered trying to use Haskell for automatic control and estimation of dynamic models. I'll need to investigate further!
It's more like having a `Wait` monad which must always produce a result in a finite amount of time. Something like `data Waiting a = Done a | Still (Waiting a)` where a non-termination effect is modeled by an infinite number of `Still`s. 
Don is right here. We can arge about what Haskell really is (common extensions, what ghc does etc), but it is quite clear that there is a large subset of Haskell (corresponding fairly closely to Haskell2010) for which we have a pretty reasonable formal semantics (System F_c, CPOs etc), and things like unsafeCoerce and similar are clearly outside of that realm. We can say pretty well what things mean when they're inside that subset of known Haskell, but `unsafeCoerce` and co we cannot give a sensible formal meaning to and so it makes a lot of sense to simply say they're outside the language and completely implementation defined. So it's not just about which extensions are cool today, `unsafeCoerce` and co are very obviously outside the realm of nice language semantics, so it's quite legit to say they're not Haskell, they're ghc implementation tricks or whatever.
`unsafeCoerce` and `unsafePerformIO` (the latter can implement the former btw) are entirely unsafe. They break the language semantics. Or to put it another way, they are not part of the language semantics and not definable within Haskell.
&gt; It's a No True Scotsman fallacy. Be careful when and how you apply informal fallacies. Something is a no true scotsman iff its ad hoc reasoning, without reference to an objective criteria. In other words, if the counterexample is deemed not a true representative precisely on the grounds of it violating the claim. This is a form of begging the question, for the conclusion one wishes to reach is built back into the claim. Here it is clearly not the case; 1) it is outside the standard (ie an objective a priori criteria), 2) since unsafe* operations are there **precisely** to enable ignoring and breaking any rules of the language, a way for the programmer to state "I know what I'm doing, I'm manually maintaining the relevant invariants even though I'm unable to express this properly in the typesystem". Trivially breaking the Haskell semantics is precisely why these functions are called unsafe - that's their intended purpose! And that's why one is not supposed to use them at all, if one's a mere mortal. moreover, and 3), it is you who choose to define Haskell as GHC Haskell, and GHC Haskell as including absolutely any GHC feature - a notion one needn't redefine mid-argument since one could hardly accept it to begin with. Your own title would make no sense if you honestly believed it - GHC's implementation of &lt;everything GHC can do&gt; is semantically broken? You either have a notion of Haskell distinct and hence more restricted than the GHC in your mind (in which case, this Haskell is properly implemented in GHC), or just insist it is advertised to have semantics it doesn't actually have - in which case you just need to pay more attention to the fine print :)
I'm not arguing about `unsafeCoerce` being bad practice or that it doesn't break the formal semantics of the language. I'm arguing that you can't look a Haskell programmer in the eye and tell them Haskell doesn't have `unsafeCoerce`. It does. You can escape to saying the Haskell 2010 standard doesn't have this or that. But the Haskell standard is (for the most part) socially irrelevant. No one writes libraries or software in standard Haskell. And if we're going to exclude features with broken semantics from being "real" Haskell, then we ought to do something about `seq` and `fail`!
I would say that the extensions you'd like to support are those that have been promoted to core feature by later versions of OpenGL. If you start at page 518 (ouch, I know) of http://www.opengl.org/registry/doc/glspec42.core.20120427.pdf, and search for "promoted to core feature", you'll see a variety of extensions that became features of later versions of OpenGL. I'd say modern OpenGL is morally at around 3.0 or so (which is when programmable shading took over). What I would call modern OpenGL is rendering to offscreen buffers, multiple render targets, VBOs and programmable shading. From my cursory look, it seems that this is supported by ~98% of the respondents (so yes, I would say that it *does* support a decent amount of modern opengl programming world).
I just want to be clear, I'm not trying to harp on dons in particular. It's something I've seen a handful of times in the community ("such and such isn't in the standard, thus isn't *real* Haskell"). The problem is the standard is always used as a defensive retreat. If I'm asking how GADTs work in Haskell, I get helpful answers and hear the praises of how powerful Haskell is. But if I ask about how `unsafeCoerce` works, I hear "it's not real Haskell, because it's not in the standard. Never use it!" My beef is that `unsafeCoerce` is part of Haskell if and only if GADTs are. Of course, `unsafeCoerce` is an dangerous and somewhat embarrassing blemish on the language. But it's dishonest to deny its association with our language. 
I think its an implementation detail, but not part of the language. GADT, that's something the standard will or at least could eventually include. Its an experimental extension of the Haskell language. unsafe* though? I'll murder the committee members if they try :D
Rule of thumb: Write Haskell, but be safe.
Just to be really pedantic, but unsafeCoerce isn't part of the FFI - perhaps you're thinking of unsafeLocalState. unsafeCoerce lives in a module name that begins with Unsafe, and the function itself begins with the word "unsafe" for good measure. The clue is in the name.
Oh good point - it's not even in the FFI
&gt; So it should be impossible to [X] as the semantics would indicate that this solves the halting problem [because] [X] allows undefined values to be "fairly reliably" detected[.] Perfect reliability is required to "solve the halting problem."
&gt; So it should be impossible to have a function isUndefined :: a -&gt; Bool isUndefined x = x `seq` False I believe what you mean is that it should be impossible to have a *total* function of that sort. But nothing in Haskell demands totality...
Since Haskell 2010, the FFI is a part of the language report, not an extension.
i only skimmed the paper so my questions might be answered in the text. i did not find them though. as i understood backpack, every package has to specify all its dependencies. they can either write every hole in it's spec file (that will be quiet a big effort in itself) or include the existing holes defined in other packages. the second option sounds more reasonable but i don't understand how that will help explicit version requirements ("cabal hell"). or might there be a tool to generate minimal hole specifications. so if the module only uses a restricted version of prelude's `head:: [Int] -&gt; Int`, it will write that in the required prelude specification. i don't see whether that will really solve anything, but having to write all these signatures sounds pretty bad as well.
last produces a single element and (++) takes two lists. last :: [a] -&gt; a (++) :: [a] -&gt; [a] -&gt; [a] try reverse' xs = last xs : reverse'(init(xs)) 
and i don't really understand why hoogle does not find cons. % hoogle 'a-&gt;[a]-&gt;a]'|head Data.List intersperse :: a -&gt; [a] -&gt; [a] Data.List insertBy :: (a -&gt; a -&gt; Ordering) -&gt; a -&gt; [a] -&gt; [a] Data.List deleteBy :: (a -&gt; a -&gt; Bool) -&gt; a -&gt; [a] -&gt; [a] Data.List insert :: Ord a =&gt; a -&gt; [a] -&gt; [a] Data.List delete :: Eq a =&gt; a -&gt; [a] -&gt; [a] Prelude (++) :: [a] -&gt; [a] -&gt; [a] Data.List (++) :: [a] -&gt; [a] -&gt; [a] Prelude drop :: Int -&gt; [a] -&gt; [a] Data.List drop :: Int -&gt; [a] -&gt; [a] Prelude take :: Int -&gt; [a] -&gt; [a] 
Awesome, I've been waiting for this since Simon Marlow evoked it around one year ago! Is there an implementation of the elaboration somewhere?
&gt; as i understood backpack, every package has to specify all its dependencies. No, quite the opposite. This is where the mixin design (rather than functors) comes in handy. Someone defines "base-sig-4" which sets up all the interfaces of the standard library, and everyone uses that in their own packages. Nobody will need to rewrite the signature for Data.List. &gt; they can either write every hole in it's spec file (that will be quiet a big &gt; effort in itself) or include the existing holes defined in other packages. the &gt; second option sounds more reasonable but i don't understand how that will help &gt; explicit version requirements ("cabal hell"). Nothing *directly* addresses the various Cabal Hell issues but we lay the necessary foundations for addressing them in a real implementation built on Backpack. Consider the impossibility of depending on both P and Q which themselves depend on incompatible interfaces for QuickCheck. Cabal won't allow this because you might accidentally confuse the two instances of QuickCheck which would need to be built into the program. (I know this is a human error concern, but there might also be an implementation reason for disallowing this.) The idea in Backpack is that there's no reason (necessarily) to prevent P and Q from being jointly included into some third package. You know that you're never going to confuse two QuickCheck types that actually come from different versions. Why not? Because your package level has awareness of types, and it wouldn't allow any module to mix those two things since, after all, they are totally distinct types. &gt; or might there be a tool to generate minimal hole specifications. so if the &gt; module only uses a restricted version of prelude's head:: [Int] -&gt; Int, it &gt; will write that in the required prelude specification. i don't see whether &gt; that will really solve anything, but having to write all these signatures &gt; sounds pretty bad as well. You might want to explicitly write the signatures for your dependencies when you can be very precise about what you need from it. This makes your dependencies more general and flexible. For example, let's say I don't actually depend on all the stuff in base-sig-4, but actually just what's provided in the Prelude, so I write my own signature to describe the Prelude stuff I depend on (like your `head` example). I agree that it's kinda pointless. Presumably someone will have already defined a prelude-sig package that you can include instead since there's obvious potential for reuse there.
You can write very efficient OpenGL with basically just version 2.1 (pretty old). The lowest common denominator is pretty sad, but not deal breaking, and the 95% supported extensions form a pretty decent list. There are a *lot* of new-ish extensions I really wish were more popular though. For example, for my purposes, it would be awesome if one of the texture array extensions were among the 95% supported, since it would allow me to basically do all my sprite rendering in a really tiny number of draw calls, even with tons of highly detailed texture data, by just putting all my atlases into a single texture object. But alas, `GL_ARB_texture_cube_map_array` (the one that was officially adopted into OpenGL 4.0) is at 52% in these results, and even `GL_EXT_texture_array` (around since 2006 or so) is only at 89%.
&gt; I'm think out loud, would that be something like putting an Either monad inside the IO monad and requiring Either for every function? It is more like what tel said in his response to you, although I think it might be better for what I will call the `Partial` monad to have an abstract type, like `IO` does. After all, when the language is total, you don't have to think about strictness and non-strictness; it would be a little awkward to reason about `Partial` in terms of codata. Also, that implementation allows you to do weird things like count the "steps" of a computation, which is not the point of `Partial`. &gt; Wouldn't that mess with a lot of nice properties of Haskell? There are tradeoffs. The biggest downside is that termination checking is hard, and the best termination checkers these days are still quite restrictive, so having a total language is sometimes a little painful. I think putting all partiality into `Partial` would be a bit more painful than it is to put most other side effects into `IO`, since the latter is easy to enforce somewhat precisely (although it occurs to me that maybe this isn't as true as it may appear... `unsafePerformIO` exists for a reason). However, even with a very restrictive total language (say, no recursive `let` at all, and all recursion must be done via blessed combinators), there are many benefits to be had. For one, that's a huge class of bugs that you no longer have to worry about. No more random `error`s (as in the `error` function) popping up in your pure code! It also means that you don't have to consider weird nontermination cases anymore. Ever wonder why this isn't the definition of `fmap` for tuples? instance Functor ((,) a) where fmap = second It's because the laws require `fmap id = id`, but `second id /= id` because `second id undefined = (undefined, undefined)`. For a total language, there is no difference between the two. In short, the benefits would be similar to the benefits we gained by putting other kinds of side effects into `IO`, and the restrictions will hurt a little, but maybe even they would be worth it in the long run; it's honestly hard to predict.
Really nice read. Thank you for the link!
From an implementation perspective, I don't imagine it would be all that difficult, since instances are basically just records containing an implementation for each instance method.
Thanks everyone for the quick reply and answer. Feel the part of the fool, not having realized the difference of (++) and (:) but i really appreciate the help.
&gt; ... because then you need to treat undefined as a value of each possible type including Void. Well, yes... &gt; But then it breaks the type system, ... That's a bit strong, surely? http://en.wikibooks.org/wiki/Haskell/Denotational_semantics Admittedly "integers plus ⊥" isn't as nice to work with as "integers", and the extra lifting Haskell does break a lot of reasoning that would work in a total language (for example, `curry` is not an isomorphism in Haskell); but still there's a sound theoretical model for the pure subset of Haskell. If I have a variable `x` in a Haskell program with type `a`, then I can think of the value of `x` as being one of the elements of the DCPO corresponding to `a`. And if I have two Haskell functions which induce the same map on DCPOs, they are observationally equal. I wouldn't say that the type system is "broken" in the pure subset of Haskell.
any link to Paul Levy's post?
It would only solve the halting problem if partiality is identified with non-termination in the semantics. This is often done, but is a convenience, not a necessity, and for very good "moral" reasons it makes more sense to distinguish them, as quite frankly, they're very different things.
Sadly, the problem with importing and exporting is more about keeping the semantics sane than it is an implementation hurdle: http://stackoverflow.com/questions/8728596/explicitly-import-instances
You can already break this in dangerous ways with implicit parameters. You just need a constraint like: instance (?foo :: FooDict a) =&gt; Foo a where And you can easily provide arbitrary instances to Map/Set functions etc. at call-time, causing breakage.
It isn't about keeping the semantics sane. It is about maintaing full compatability with the existing APIs for `Data.Set` and `Data.Map`. That is silly. * We can already break the one instance property multiple ways * We could easily maintain compatability at the cost of a (small) asymptotic slowdown for a few rare operations. * These modules don't perform that well anyways * These modules already support equally unsafe operations `mapMonotonic` * You would not even need to give up the performance while keeping the same API and being fully safe if we had a way of comparing instances at runtime (a semi-unsafe test of reference equality of instances) * We could design safe fast APIs but it would take extensions or a real module system (wait a second...) Controlling visibility of typeclasses/multiple instances works. Just about every other language that has adopted typeclasses has it. We should put effort into doing it right, but we should do it.
I know that your question is about the type error, but have you considered the algorithmic complexity of your solution? For every application of `reverse'` you must fully traverse the list to get the last element. Also, you're completely missing out on tail recursion optimisation, since the last-applied function is `(++)` (should be `(:)` btw) -- an equivalent reverse' reverse' xs = (++) (last xs) (reverse' $ init xs) Here's a reformulation that may even be a bit easier to understand, even if it's not a recursive one-liner. reverse'' :: [a] -&gt; [a] reverse'' is = go is [] -- pass the input list, and a null output list where go :: [a] -&gt; [a] -&gt; [a] -- a tail-recursive helper function that runs over both lists go [] os = os -- when the input is empty; return the output list go (h:is) os = go is (h:os) -- shuffle the head of the input list onto the head of the output list. This version will run in linear time and constant stack.
Don't use init/last/head/tail unless you can show that they are never applied to an empty list. Your function crashes when reversing the empty list. The standard naive solution to reversal is reverse [] = [] reverse (x:xs) = reverse xs ++ [x] A more efficient way of doing it is by looking at the list as a stack, which is inverted by taking it apart element-by-element and building it up again. reverse ys = reverse' ys [] where reverse' [] rev = rev reverse' (x:xs) rev = reverse' xs (x:rev) [(This is how the library version from `Data.List` is defined.)](http://haddocks.fpcomplete.com/fp/7.4.2/20130622-109/base/src/GHC-List.html#reverse)
&gt; Don't use init/last/head/tail unless you can show that they are never applied to an empty list. I agree, but the blame here really lies on the standard library.
The issue is that `Map` and `Set` have invariants that depend on the specific ordering. Since, each operation is parameterized (via a typeclass) by the ordering, this is a problem if the instance can be changed out. The solution most languages take is to include the ordering as part of the data of the data structure. The complaint about this is that it makes it harder to write something like union :: Set a -&gt; Set a -&gt; Set a which in current haskell is union :: Ord a =&gt; Set a -&gt; Set a -&gt; Set a admittedly, the problem exists with code other than `Set` and `Map`. I just think those two modules are most of the issue. Everytime someone suggests changing it, these modules get pointed to as the justification for keeping things the way they are. 
&gt; i did not make myself clear, i guess. i understand the mixin approach. but am i right that every hole has to be specified (directly or via mixins)? Right. &gt; and, regarding your second paragraph, does that mean that quickcheck 2.5 and 2.6 can be linked into the same program? that sounds great but also very hard. i have not really thought about it though. maybe that's just my understanding of traditional unix linking interfering. Currently there is no understanding of versioning: quickcheck-2.5 and quickcheck-2.6 are entirely distinct, unrelated packages. So, yes, you can have both linked into the same program (and even two instantiations of *the same* package), but that's not really saying anything interesting in our setting. Definitely, definitely the proper account of versioning is something to investigate.
This is a silly semantic argument. What exactly you consider to be "Haskell" is irrelevant. The fact is there is one set of languages which is semantically sound (probably) and one which is not, and no one is claiming anything other than that. When people say "Haskell" is sound they're clearly referring to the former set, and when you say "Haskell" is not, you're referring to the latter. Unless someone is actually confused as to which is meant, arguing about it is meaningless.
Not yet. I'm currently starting to hash out a prototype elaborator/typechecker for Backpack alone *without* integration into GHC. All the work so far has been in the design and formal development.
The breakage is always in conjunction with an extension though: GeneralizedNewtypeDeriving (which could be fixed if it were to type-check the generated source), ImplicitParameters, and some others. The basic instance constraints in Haskell do make some sense and give some guarantees until you enable those extensions.
BTW, in case people are wondering why this function breaks the normal semantics, it is because it is not "continuous" or "monotonic" (in the CPO sense). All functions definable within the language are continuous and monotonic. See wikipedia about [CPO continuous functions](http://en.wikipedia.org/wiki/Complete_partial_order#Continuous_functions_and_fixpoints) and [monotonicity](http://en.wikipedia.org/wiki/Monotone_function#Monotonicity_in_order_theory).
Aren't the breakages of the property based on some GHC extensions (such as GeneralizedNewtypeDeriving, ImplicitParameters)? Could you cause the same breakage in H98? Also, I think there could possibly be support for compile-time type-checking instance equality (with false negatives, of course) which could help alleviate much of the problem. 
Without wanting to sound paranoid, shouldn't you have anonymised those? If I'd known you were going to publish them I wouldn't have used my actual name.
This one is marked as a GHC bug: http://ghc.haskell.org/trac/ghc/ticket/7624
That seems weird to me. I don't think this example should compile.
That breakage was injected by accident, not by design and will likely be corrected shortly.
Only by using orphan instances. This is why orphans are bad and cause warnings.
You basically throw out the correctness of almost all of the code I have written. I woudn't want to use a language without confluence of instance resolution and I have had extensive experience programming in one. We have that language. It is called Scala. Take a look around the FP community in scala and listen to the myriad complaints they have about implicit resolution to see where this line of reasoning logically terminates. The Scala equivalent of Data.Set has to use asymptotically slower operations for many primitives, because a hedge-union isn't viable. Other containers cease to be correct, class-based invariants generally become unenforceable. In Haskell we move the use of a dictionary to the call site rather than packing it in the container. It enables us to make data types that work in a large array of contexts and become _better_ when you know a little bit more. Breaking confluence destroys all of these advantages. If you want to work that way, fine. There are dozens if not hundreds of languages in which you can think that way. I only have one language in which I can work that works the way Haskell works now. Please do not deprive me of the only one I have to work in.
Data.Set and Data.Map are just miner's canaries for bigger issues. They get cited because they are the first modules that people use that have non-trivial invariants. Right now you don't care where an instance constraint comes from, because if you get one it should be canonical. When you break that invariant the fact that we have no vocabulary for how to plumb around a particular instance constraint comes around to bite you. My `ad` package dies, parts of the `mtl` cease to be canonical, `heaps`, `unordered-containers`, `bytestring-trie`, `representable-tries`, `fingertrees`, most of `containers`, any attempt at using an algebra to annotate a cofree comonad for a syntax tree confluently like in my reflecting on incremental folds post, `reflection`, my monoidal online lowest common ancestor library, any sort of monoidally annotated tree, my entire algebra package, and a bunch of other stuff would break, because I just started at the top of my github account and started reading off things that would break or where it would become even more trivial to violate invariants and impossible to reason about the code as if those invariants were in place. 
It isn't hard to add, but it carries with it huge correctness issues. Chung-chieh Shan and Oleg showed that it easily leads to lack of confluence of instance resolution a long time ago.
It can still be introduced using orphan instances. Are there going to be plans to remove orphan instances entirely? Or will it remain a warning? Because that will always lead to breakage.
Why not just use dependent types? empty : {k v : Set} -&gt; {{ o : Ord k }} -&gt; Map k v o union : {k v : Set}{o : Ord k} -&gt; Map k v o -&gt; Map k v o -&gt; Map k v o Not really any harder than Haskell, and supports both worlds :)
http://hackage.haskell.org/package/postgresql-simple
Is the use of unsafeDupablePerformIO a concern? I'm not really sure what the consequences of unsafe* functions are, I just get the general "don't do that it is bad" impression from everyone.
It certainly does the job straightforwardly, but it throws exceptions at the drop of a hat, and this upsets me.
Was the default about warnings for orphan instances changed in GHC? I'm using 7.6.3 and it didn't warn me on this example until I used -fwarn-orphans explicitly, but I distinctly remember it used to be the default...
http://lists.seas.upenn.edu/pipermail/types-list/2008/001179.html
There is an explicit example of what edwardkmett said about using orphan instances in [this answer](http://stackoverflow.com/questions/12735274/breaking-data-set-integrity-without-generalizednewtypederiving/12744568#12744568) by SO user MigMit.
Is there a mistake in figure 5? The text seems to imply that there should be a third form of nu, namely Kappa nu-bar.
I use -Wall in just about everything, so I'm pretty bad about knowing what is a warning by default. 
I don't use dependent types or ML modules because I like actually getting the compile to generate the ton of redundant, only-one-way-to-write-it code (the boring dictionary plumbing) for me. Agda has some implicits, but they require a value to be in scope, so they kind of suck for things like monad transformers. The very thing that scala falls down on the job for. Dependent types are easy to use and understand. They are, however, very verbose to use in practice for non-trivial examples, and they tend to cause you to wind up having to make several versions of most of your structures to describe the myriad invariants you want to preserve in them -- hence all of pigworker's work on algebraic ornaments and ornamented algebras. I write Haskell because it gives me a great power to weight ratio when it comes to the verbosity of my types. Moreover I try to write in the inferable subset of Haskell as much as possible, because it requires less of the users of my libraries. When I write agda, the type code explodes to 10x the size of the corresponding term level code that is doing the work. That ratio rarely works for me, except when the invariants I am trying to express are incredibly complex and I need the extra confidence. Most of the time, I'm able to constrain the possible bad implementation space down with confluence and free theorems to the point where Haskell is more than sufficient for my needs and less verbose than any of the alternatives that exist today.
It's probably the best of the bunch, but I can't stand the reliance on column ordering in `FromRow`.
As with most of the unsafe* operations, the unsafety is in that the requirements to use them safely are up to the library writer. That said, pretty safe if used correctly. Link?
Hm, I hadn't realized before that GHC actually allows the import of two conflicting orphan instances like the one in [this SO answer](http://stackoverflow.com/questions/12735274/breaking-data-set-integrity-without-generalizednewtypederiving/12744568#12744568) mentioned elsewhere. That seems... undesirable. The idea for instances would be that they are declared in signatures and show up in the types of modules. Locally defined/declared instances show up as a part of the type; the other available instances are computed by walking the import chain. Orphan instances would have no special status. Any module that transitively imports two instances of the same class/type--orphan or not--would be rejected. This would rule out modules like the D module in the linked SO answer. Surely there must be some naive ignorance behind that approach that I'm not seeing. No? Moreover, the damage caused by instances that fly around everywhere implicitly would potentially be mitigated by importing signatures.
It's just vector notation: nu-bar is a possibly empty vector of nus.
I don't know why you're getting some many upvotes, because I don't know why this is a problem. Yes, it's a scary name, but everything foreign has a scary name.
The issue is there is nothing to tag. I produce a Set using some Ord instance on the argument type and then the dictionary "goes away". Backpack's carefully managed applicative semantics blithely let a Set constructed with one instance of Ord to be inspected in a context using a different instance of Ord, because the place that the instance came from had no place or reason to be attached to the name "Set" nor the name "Int". 
The resolution I mentioned, is to basically disallow orphans entirely and reconstitute their functionality by other means. All instances must live with the class or data type they involve or come from some kind of 'mixin package'. Ultimately we use orphans because we want to provide an instance for a class we don't own for something we don't own - to minimize the irrelevant dependencies of one or the other package. We can fix that by extending the language yet further to create a notion of a mixin package or mixin module such that if you incur a dependency on two packages, say parsers and parsec, the parsec-parsers mixin package which only provides instances gets brought in and the instances come into scope. This, however, is a lot of work and likely falls outside of the attention span of a graduate student to see to completion. It would, however, lead to a much improved ecosystem with fewer dependencies and no pressure on library authors to -not- provide an instance, since that instance can always be exiled to the appropriate mixin package, if the other package wasn't necessary to implement yours.
https://github.com/lpsmith/postgresql-simple/blob/master/src/Database/PostgreSQL/Simple/FromRow.hs
It's not a function, is it? It's a type constructor, defined here: http://www.haskell.org/ghc/docs/7.0.4/html/libraries/ghc-prim-0.2.0.0/src/GHC-Types.html#line-25 That's why you can use it for pattern matching.
&gt; &gt; Backpack's carefully managed applicative semantics blithely let a Set constructed with one instance of Ord to be inspected in a context The implicit configuration paper also included the possible sollution to this. Right? You just make instances syntactic suggar for something like your `data.reflection` operating at the module level. Implicit/typeclass resolution is not confluent now. I'm pretty sure you can break confluence using tricky `IncoherentInstances` and you certainly can using other breaks such as the mentioned combination of ImplicitParams and typeclasses or by combining implicitParams with GADTs. I wan't to be clear: I want Haskell to be confluent, and think we should redesign the typeclass system to ensure confluence. But, we should ensure confluence while also providing support for multiple instances! I don't see how those are (inherently) contradictory. This is quite a different issue than the canonical instance problem with respect to module invariants.
Gah. I mistyped. See edit. The text (under the heading "Variable and Applicative Identities") refers to 3 forms of physical module identities, but the figure only mentions two.
`IncoherentInstances` and `OverlappingInstances` are extensions that lead you into the subset of haskell that easily blows up in your face. You are free to use them. However, note, I've manage to release 90+ packages on hackage without the need to use them at all. I largely prefer to pretend that they do not exist, as using `OverlappingInstances` with `ConstraintKinds` leads to problems. I use `reflection` when I want to be able to make up instances on the fly. It forces a 'stack discipline' on my code due to the rank-n type, but it never goes wrong. You are not the first person to say we should be able to come up with a system for multiple instances. However, I have yet to hear an actual concrete proposal that successfully maintains that invariant! Yes, the language we have today has some corners where you can lose confluence. I'm not going to give up fighting to retain the core where it holds. e.g. `SafeHaskell` currently requires you stick to this core and disables when you attempt to step outside of it. Should we throw that away along with everything else we'd have to give up, too?
Is this it? http://skillsmatter.com/podcast/home/high-performance-concurrency
When I define Int, later on someone can make an instance for it. How do I tag Int with stuff that hasn't been created yet? module Foo where data Foo a = Foo a instance IsString (Foo String) module Bar where import Foo class Quux t instance Quux a =&gt; Quux (Foo a) module Baz where import Foo import Bar instance Eq a =&gt; Eq (Foo a) How do those instances 'infect the type' Int? Especially if you allow orphans. The instances were declared later! Worse, MPTCs and type families, etc. If you want to preserve incremental compilation and avoid whole program compilation, I personally think some notion of infection by instances isn't a viable solution.
Hello, yes! See you there, and I will talk to you about Haskell databases.
SafeHaskell is not confluent (the ImplicitParam issue). I still don't understand why a simple compiler check does not resolve the confluence problem (absent undecidable instances). This is, imo, trivial. It just breaks a few things like data Showable a where Showable :: Show a =&gt; a -&gt; Showable a showBoth :: Showable a -&gt; Showable a -&gt; String showBoth (Showable x) (Showable y) = (show x) ++ (show y) which would be disallowed. A solution to the non trivial invaraint problem is known. We just fake some more dependency! Each instance gets its own unique type and encoding invaraints becomes easy again. Local instances become nothing more than generative local types, and we know how to add those to a pure language (just use the same trick as the ST monad). Okay, this isn't fully worked out. I don't have a full story for typeable, and it has problems with backward compatability, but I don't think this is nearly as onerous as you make it out to be.
That's the one! There were a lot of good talks, but Simon's was one of the best presented. Duncan Coutts was also good, as was SPJ as usual. 
The issue here is a bug. {-# LANGUAGE Safe #-} and {-# LANGUAGE ImplicitParams #-} should be mutually contradictory until the current *bug* that permits implicit params as superclass constraints is resolved. I think it is important to distinguish between the intended behavior of SafeHaskell and behavior that is an emergent property of open unresolved bugs. =P I'm not willing to concede the desirability of the goal just because we've been imperfect on delivering on it. ;)
Thanks I'm am pretty sure I have seen it. It's worth rewatching.
&gt; Even stating soundness required us to formally define the semantics of &gt; (recursive) Haskell modules, which has hitherto not been formally specified. Can we expect to see this allow for recursive module dependencies in GHC, sans Backpack? EDIT: no hs-boot nonsense either 
Right, corrected.
&gt; They don't, until the Int is used in a function that has them in its signature. If the type is visible to the user this rapidly leads to pretty hideous code. If it isn't visible to the user then I'm about as confident as the folks who think that they can make it work that I can punch holes in such a presentation that shows I'd need access to it. ;) In the absence of a concrete proposal, everyone is going with a gut feeling. &gt; Unless I'm misunderstanding, this addresses your concern. In the absence of an actual proposal, I'll remain at best cautiously optimistic. A whole-program check that instances are unambiguous across the whole system is a viable way to end the build. My main concern is that I want to ensure that these issues are brought up and considered rather than being swept under the rug because "hiding instances is easy."
[Here's a cheeky way to get fields by their names](http://lpaste.net/90961). I sacrificed several goats to get this to work, and I'm fairly certain lpsmith just dropped a plate as he sensed a disturbance in the force. 
Got it. Thank you.
I doubt it; Backpack uses an hs-boot like mechanism (it requires that all dependencies of a module be satisfied by the modules before it, but allows a module to be listed multiple times, loosely speaking).
esqueleto is amazing.
I didn't have time to work it out, but I suspect that there's a straightforward way to automatically lift out the "forward declarations" needed to set up recursive modules. The reason you could do this in Backpack but not in GHC today is because, unlike GHC, you can have multiple (increasingly specific) forward declarations per module. The following modules, as you'd write it today in Haskell, module A where import B data T = T U x = T y module B where import A data U = U T | K y :: U = K could be iteratively refined into a Backpack package in stages. Here's how it would look. -- 1. copy modules as-is A = [import B; data T = T U; x = T y] B = [import A; data U = U T | K; y :: U = K] -- 2. create forward decls with full datatype declarations -- and any values with type annotations A :: [import B; data T = T U] B :: [import A; data U = U T | K; y :: U] A = [import B; data T = T U; x = T y] B = [import A; data U = U T | K; y :: U = K] -- 3. one more pass; abstract declare all types, -- which means no imports are necessary A :: [data T] B :: [data U] A :: [import B; data T = T U] B :: [import A; data U = U T | K; y :: U] A = [import B; data T = T U; x = T y] B = [import A; data U = U T | K; y :: U = K] Just a sketch; not making any guarantees about this.
What's the 'n' in the fundep - is it supposed to be 'x'? If so doesn't that mean you can't ever use the same implicit parameter name with different types? Also, how is let ?x = ... translated - don't you end up needing duplicate instances?
Hey, I'm sorry. If you want me to change something, please mail me.
Oh wow, 5 month old post. :) You're right about the fundep, I fixed it. No, you can still have implicit parameters with the same name but different types. $ ./inplace/bin/ghc-stage2 --interactive -XImplicitParams GHCi, version 7.7.20130710: http://www.haskell.org/ghc/ :? for help ... Prelude&gt; let ?x = 10 Prelude&gt; let ?x = "foo" The nature of shadowing with implicit parameters (like `let ?x = ... in (let ?x = ... in ...))`) is actually pretty tricky and I don't understand all the subtleties. I went back and thought about this and did some digging. The end result is I don't think I like this anymore. :) The trick is this: instances are *created locally* when you bind an implicit parameter. Given the following term: Prelude&gt; :t ?x ?x :: (?x::t) =&gt; t the implicit is unbound, so the constraint 'floats out' to the top. This is just like the underlying definition: Prelude&gt; import GHC.IP Prelude GHC.IP&gt; :t ip ip :: (IP x a) =&gt; a Now, when we say something like: Prelude GHC.IP&gt; let f g = g ?x Prelude GHC.IP&gt; :t f f :: (?x::t1) =&gt; (t1 -&gt; t) -&gt; t So that type actually *really* has a constraint like: f :: IP "x" t1 =&gt; (t1 -&gt; t) -&gt; t And we can provide the implicit an actual type and value now, satisfying the constraint: Prelude GHC.IP&gt; let h = (let ?x = (10 :: Int) in f) Prelude GHC.IP&gt; :t h h :: (Int -&gt; t) -&gt; t When this is translated, it basically becomes something like: let f :: IP "x" t1 =&gt; (t1 -&gt; t) -&gt; t f g = g ip let h = -- the below is magical syntax to do a local instance (let local instance IP "x" Int where ip = 10 in f) This allows 'local' overlap, basically, so duplicate instances really don't cause an issue. So this feature is actually more than meets the eye, and it's actually amazingly unsafe because with stuff like this, you can generate type class constraints, where you can then provide instances to solve them *locally*. This is exactly what the blog post does by making the implicit a superclass constraint. So you can pretty quickly break instance confluence if you're not careful! Having programmed in languages without instance confluence, and being more educated in it since I wrote that original post, it's not only a tradeoff that can cost time, it's also possibly dangerous. In retrospect, I take back what I said, and this shouldn't work. :) If you want something crazy like this, you can use Edward Kmett's `reflection` package to dynamically define instances (and the rank type will help make sure you do it correctly.)
&gt; Why are we doing this? Simple: because the Haskell community needs to vastly expand for Haskell to become a mainstream language. I think that's a truism. Isn't building up an ecosystem around and getting users for the FP Hosted Haskell Thingy product also why you're doing this (not that there's anything wrong with that)?
 rm -r ~/.ghc This will reset your local package database but not your global package database. Then install whatever package of interest that you want and it will get all its dependencies fresh. Alternatively, use `cabal-dev` or `hsenv` or cabal sandboxes which should be coming soon.
Yes, sandboxed builds. You have several choices: * [cabal-dev](http://hackage.haskell.org/package/cabal-dev) * [HEAD version](https://github.com/haskell/cabal) of cabal * [hsenv](http://hackage.haskell.org/package/hsenv) When you use a sandboxes, the installs are isolated from each other. Rebuilding the world takes less time and you can have conflicting versions installed in separate sandboxes as per package requirements. My favorite is cabal-dev because I know it the best. Now that the sandbox support in cabal is stabilizing I'm moving to use that. I've never used hsenv, but some folks prefer it.
I can't speak for Haskell precisely, but that is exactly the meaning of let in early formalisms of Scheme and ML: you write the former and the compiler expands it to the latter.
&gt; a is an identifier not appearing in X
Ah, didn't read closely enough... Then the classic answer is that hindley milner generalizes let. Although, GHC will quitely turn that off if you enable the right extensions.
I see. So with RankNTypes, it is possible to convert the let-form to the lambda-form by adding a type signature to the variable, is that right?
Worth noting that -XGADTs etc. will actually kill let generalisation.
Ah, as is conventional with recursive types, we omit the \mu\alpha. when \alpha does not appear in the term.
that's too late now.
I just viewed it in Epiphany with the HTML5 Vimeo player enabled, and it downloaded itself into my .cache folder. 
We rarely use the standard as our *definition* of the language - which arguably is what standards are usually for - but that does not mean that the standard is irrelevant. It is important to know in what ways, if any, your code deviates from the standard. This has many consequences, including: * Interoperability between different versions of GHC * Interoperability with other Haskell implementations (yes, they do exist, and they do have importance, even though GHC is the gold standard) * Build system configuration (e.g., `LANGUAGE` pragmas in individual modules, the `extensions` field in cabal files, interaction of those with cabal flags) So even though the standard is not the *complete* definition of the language in practice, it is indeed a significant *part* of the definition in practice. The only reason the standard is not the complete definition in practice is because it always lags some distance behind best practices. That could change at any time if it becomes important enough to someone (perhaps you?) to invest the time and effort needed to keep the standard more up to date.
It's OK, it's not a big deal, I was just surprised. In the long term it's much more important that Haskell's OpenGL support improves, so good luck with the project!
I would really really like to have this. In my opinion, it would fix the current most glaring defect in GHC from a practical point of view (i.e., from the point of view of someone implementing commercial systems in Haskell in the real world). I am sorry to hear your pessimism about the difficulty to implement it, though. Why would this task be so daunting? EDIT: And if so, I again repeat what I have been saying for years: we should implement the `ExplicitInstanceImports` pragma. If it would placate the naysayers, then call it `UnsafeExplicitInstanceImports`. But I don't agree that it is any more unsafe than other pragmas that allow those who abuse them to crash their programs.
Yes.
A distinction which does not yet show up in Haskell, but may before too long, is that in the "let" version, the typing of Y might potentially use the information that a = X, whereas the "lambda" version must check Y using no information about the value of a. In dependently typed languages, let and lambda are rather different beasts. This apparent coincidence in Haskell, for as long as it remains, is not fundamentally a good way to understand "let", but rather results from a shortfall in expressivity.
Nice! This was SJP at Functional Programming eXchange 2K13 on types - http://skillsmatter.com/podcast/home/keynote-3842/te-8046
You can do knot-tying and recursive bindings with lambdas and the "fix" function. Your example can be written identically as: fix (\a -&gt; 1:a) 
As the blog points out, instance confluence is already broken in GHC without any extensions - you can define an instance in two different modules and then import both those modules from a third module without any errors. I don't think this is standards compliant and arguably it should be fixed. On the other hand, there's a lot of clamour for explicit instances and the like, so perhaps we need to figure out how to make this kind of thing safe instead.
That's interesting. Could you say a bit more?
fix is just a let in disguise. fix = let y f = f (y f) in y
I am mostly pessimistic about Backpack itself, because in this case the wrong thing is comparatively easy to do, and I do not want to lose the ability to reason about confluence. I would love to have something like backpack, so long as it doesn't come with further weakening my ability to reason about my code. If the solution that they've been exploring with "inequality constraints" can be made work then perhaps there is a lower effort solution than the one I mentioned above. It'll still be hugely disruptive to the community if they push in something with the basic functionality of the current approach, as there are all sorts of other issues with using it in practice, e.g. not being able to write `foo = 12` in a module with where the signature says foo :: Int, because the types must match exactly rather than subsume. But your issue is with my objection to instance hiding. In particular I am pessimistic about local instances and instance hiding because they lead to the loss of principal types as Chung-chieh shan and Oleg Kiselyov showed back in 2004. I kind of like having principal types! To quote the old gentle introduction to Haskell: &gt; The existence of unique principal types is the hallmark feature of the Hindley-Milner type system, which forms the basis of the type systems of Haskell, ML, Miranda, ("Miranda" is a trademark of Research Software, Ltd.) and several other (mostly functional) languages. (The defeatist in me of course notes that we have corner cases where [we don't have them now](http://dl.acm.org/citation.cfm?id=871905), caused by NoMonomorphismRestriction and because we don't have quantified class constraints, but that is a tangent.)
I use it with persistent; I'm not sure whether it's possible without. What weird limitations are bothering you? There've been quite a few additions and improvements over the past several months - no idea whether they're relevant without elaboration, though.
I think the most obvious and detrimental would be the bizarre requirement to have an integer for your primary key. That alone made persistent completely unusable when I checked it out.
[This](http://imgur.com/CjXUgOL) is what I see. Is my font fuxed or is that just the way it's meant to be?
[This](http://imgur.com/QkVFeVd) is how it's supposed to look.
Is there any documentation on the sandbox support in cabal yet?
I probably don't realize how fun this joke is. However, the fact that packages like that get hosted on hackage, at the same level of promotion as packages like "text", is questioning. I hope Hackage 2 will help developers identify quickly which packages they should consider for their needs, without expecting them to crawl the full web for reviews in blog comments ;)
I agree with you. As a saving grace, however, at least this package is clearly isolated to the acme category.
You should send patches upstream.
What if... let is fix in disguise?
Absolutely, we are trying to build a mutual-benefit society here.
Fix is defined in a library. Let is not.
You could also define fix without let... fix f = f (fix f). It's defined with let for efficiency reasons only, but the above definition should still work.
Sandboxed builds don't completely solve the problem either. Over a long enough time any given sandbox will eventually need to upgrade some of the packages it has installed. When this happens you'll either need to remove some old packages or delete the sandbox and start from scratch.
To be fair, the acme package name and hierarchy are explicitly for jokes and terrible ideas. See acme-php and acme-dont.
Yay!
[Extensible Effects: An Alternative to Monad Transformers (pdf)](http://www.cs.indiana.edu/~sabry/papers/exteff.pdf) [Understanding Idiomatic Traversals Backwards and Forwards (pdf)] (http://www.iai.uni-bonn.de/~jv/uitbaf.pdf)
[It's a letter from the Kannada language, which is primarily spoken in Karnataka, in south west India](https://en.wikipedia.org/wiki/Kannada_alphabet#Look_of_Disapproval).
That sounds like a pretty reasonable choice, but I don't see that it's the only way to go. Given that we are talking about type inference, couldn't type inference for beta redexes also take into account the supplied argument? Is it just the question of what type to assign to the lambda subexpression, and trouble neatly expressing "function taking one argument of type T, which must be X"?
Who needs to solve the halting problem? don't $ return undefined Bam: Total function.
It seems to be missing in most linux distributions I've used. I usually install `unifont`, it looks terrible but it's better than boxes.
I'm tempted to reply "Wolfgang Amadeus Mozart had perfect pitch." as it is far from clear what more it is that you might want. I guess I could be a bit more formal. Type theories with definitions have two kinds of context entries: declared variables x:S, and defined variables x=s:S. Contextual information affects not only the typing judgment: blah, x[=s]:S, blah |- x : S (whether x is defined or not), but also the *equality* judgment: blah, x=s:S, blah |- x = s : S. And the equality judgment impacts on typechecking: (blah |- s : S) /\ (blah |- S = T : Type) -&gt; (blah |- s : T). Correspondingly, more things typecheck in the scope of a definition than a declaration. Consider, for example, fixed length strings as vectors of characters n=3:Nat |- "foo" : Vec Char n only because n is 3. Hence let n=3:Nat in "foo" : let n=3:Nat in Vec Char n However, n:Nat |/- "foo" : Vec Char n which is just as well, because |/- \ n -&gt; "foo" : (n : Nat) -&gt; Vec Char n as "foo" is only any length you like as long as you only like 3.
w00t open source!
That was maybe a little too irritable; this was triggered by the op's use of the expression 'cabal hell' which I really can't approve of. Maybe we can legitimately speak of 'cabal update hell'. The significance of the op's mentioning 'every month or so' or mightybyte's speaking of 'over a long enough time' will be plainer. My point is that over any amount of time, nothing will happen without `cabal update` which could as well be called `cabal unstable`
I like hsenv a lot, for the same reasons...I've been using it for a while and it always works.
 downloadFile url name = get url . const $ S.withFileAsOutput name . S.connect
Completely agree with you here. One thing that *would* make our lives easier would be a more convenient mechanism for unregistering packages. At the very least, it would be nice if "ghc-pkg unregister" was improved so it would take multiple arguments and unregister each of them. Even better would be something like "ghc-pkg unregister-broken", or "ghc-pkg unregister --recursive-force".
Yeah, I like the term "cabal update hell" better. Although it is still a little misleading because just updating a new package list isn't the problem. Installing new versions of already-installed packages in your dependency tree is the problem. 
The only complete solution is to add support for multiple instances of the same version to Cabal/GHC. Given a plan that doesn't have conflicting dependencies (this can never be solved automatically and each maintainer here is responsible for selecting a set of dependencies that work when the lib is released), Cabal should be able to build it even if that means building a complete transitive dependency again for the current project, but storing them as new instances of the same version. This instance should be identified by the combination of the package, version, and all the different pieces that make that build unique (e.g. os, way, dependencies, compiler flags, etc, etc): a change that will likely involve Cabal and a few pieces of GHC. I use Maven/Java at my work and we deal with hundreds if not thousands of different dependencies (if you include plugins and core Maven modules) and we rarely hit issues such as the ones I hit often with Cabal. This is the case even when considering a repository that spans over a few years. However, Maven has it easy since Java jars can be built in isolation and a single jar can be used as a dependency to an infinite number of projects that depends on that specific version (the unique id of the version here is package+version and no instance specific parameters is needed). So Cabal hell does exist from a user pov, but, to be fair, Cabal has a much more complex problem to solve! For all the rest it already solves it is a great piece of software. 
i love one liners ... you can make this a command line version as well
What bug?
I'd particularly like to be able to short circuit, a bit like f1 || f2 doesn't evaluate f2 if f1 returns true.
http://hackage.haskell.org/packages/archive/async/2.0.1.4/doc/html/Control-Concurrent-Async.html#v:race &gt; Run two IO actions concurrently, and return the first to finish. The loser of the race is cancelled.
The taming effects paper is the greatest thing I have read in a while. Is there any downside compared with existing "transformer stack" style code? 
 As the page notes, "ACM no longer requires authors to transfer copyright to ACM upon acceptance." go go open access
I'm surprised how reliable this seems to be: import ReadArgs import Control.Exception (evaluate) import Control.Concurrent.Async main = readArgs &gt;&gt;= \(a,b) -&gt; race (evaluate (fib a)) (evaluate (fib b)) &gt;&gt;= print fib :: Int -&gt; Integer fib 0 = 0; f 1 = 1; fib n = fib (n-1) + fib (n-2) It presupposes an all or nothing type like `Integer`. I wonder if a 'pure' version of `race` could be written somehow with the machinery of `monad-par`. `unamb` seems completely unreliable for this dubious purpose, as did replacing `evaluate` with `return` in the above. 
[Data Flow Fusion with Series Expressions in Haskell (pdf)](http://www.cse.unsw.edu.au/~chak/papers/flow-fusion.pdf) [An EDSL approach to High Performance Haskell programming (Markdown+TeX)](https://github.com/josefs/meta-paper/blob/master/paper.md) [Extensible Effects: An Alternative to Monad Transformers (pdf)](http://www.cs.indiana.edu/~sabry/papers/exteff.pdf) [Hasochism: The Pleasure and Pain of Dependently Typed Haskell Programming (pdf)](https://personal.cis.strath.ac.uk/conor.mcbride/pub/hasochism.pdf) [Names For Free — Polymorphic Views of Names and Binders (pdf)](http://www.cse.chalmers.se/~bernardy/NamesForFree.pdf) Code: [Symmetric Functional Reactive Programming](https://github.com/cwi-swat/symmetric-frp) [Understanding Idiomatic Traversals Backwards and Forwards (pdf)](http://www.iai.uni-bonn.de/~jv/uitbaf.pdf)
i remember cabal telling me to update however,so i guess specify an older version of the libray to install in this case? my situation was I have a ton of random math,stats and hpc libraries and just wanted to play with yesod went to cabal install yesod-platform and ended up with a million force reinstalls messages
yeah cabal itself is great, its just the occasional updating thats a pain your right. I wonder what linux package managers do to keep them from breaking with all the different packages?
o god i also use maven at work and I DREAD IT. In my experiences, loading one dependency introduces 189 jar files to your project........that said I would much rather use Cabal
That's exactly what I want! Love the fact that Haskell uses lightweight threads rather than built-in threads.
The pure `monad-par` impl seems really interesting, though my intuition makes me think it's impossible. To my understanding, `monad-par` gives you a window into logical time, not real time, so the "speed" of a computation would be baked into the data flow graph (statically?). For `race` `IO` is important in order to observe the speed of computation effect in real time, which is why unamb is has weird semantics if its arguments don't have identical values (though for Conal's purposes with Push-Pull, it's a perfect solution).
Also, `async` is in the Haskell Platform while `unamb` is not.
You know, I don't recall. An id SERIAL column per table fits my use case well, so I haven't dug. It wouldn't surprise me if it could be changed, though. Certainly it wouldn't be a limitation of Persistant generally but particularly the Postgres backend, as the MongoDB backend definitely does not use integers for the key.
I whip out a bash `for` loop to bulk unregister as soon as anything looks a little hinky. This process works pretty well, but some more help from `ghc-pkg` or `cabal-install` wouldn't hurt. Dealing with locally-installed packages makes it all a bit trickier.
Yes, I was thinking that insofar as a 'pure' race had any utility, it would be like unamb, in that you had two expressions that are supposed to give the same result. (I don't know much about the real use of unamb.) If I have two ways of defining a function, for example, one might be faster for some arguments, another for others; which I get depends on e.g. user input. Why should I benchmark and distinguish cases when I can ... waste energy and cores by spawning the calculation each way and taking whichever comes first? I'm not sure this making any sense, but thats what I was imagining. Surely a type like newtype Par a = Par {unPar :: ContT () (ReaderT (WSDeque (Par ()), HotVar GenIO, Bool, HotVar [...], …) IO) a} makes plenty of materials available for such tomfoolery?
Note that the `Concurrently` wrapper for `IO` in `async` uses `fmap (either id id) (race x y)` for its `Alternative` instance so you would literally write `f1 &lt;|&gt; f2 &lt;|&gt; f3` for suitably wrapped operations. 
The Haskell report says identifiers are built from Small and Large letters. The TTHA character is classified as Other, so it should not be allowed in identifiers. 
Great update! There seems to be a lot happening. - Although Stackage is great, something even more automated, no maintainer participation required, would be great also. Something like: Take all of hackage into a git repo. Do various tweaks to packages such as "improving" dependency ranges. If it builds&amp;tests, then commit this change. Emit a notice that package X could be improved and where to pull the repo. Emit coverage reports, when it will be included into the FPComplete snapshot etc. Basically do comprehensive annotations on all Hackage packages. - You shouldn't be afraid to reimplement parts of Hackage. In my opinion, Hackage should be slimmed down to basically a web server and all the information should be provided by annotations. Having to run the infrastructure on one server or in one place is clearly the wrong thing to do. Reimplementing/competing with Hackage this way will in the end make Hackage stronger and more stable, as long as you make structured annotation-data available. - The focus on security is great. I am really glad that you guys are reviewing the submissions. Here you could also improve on Hackage without waiting for the mythological Hackage2. You could suggest a way for package maintainers to sign their packages, maybe as part of Stackage. For example you can say: If the community fixes `cabal sdist` to follow this standard, then we will do the monitoring. 
Installing ghc and the haskell platform as global packages, and everything else as local packages works for me. Then, when I install a new package, I add dependency constraints so that no package distributed with ghc, or with the haskell platform should be upgraded. This is what I do: $ cabal install `eval echo $(ghc-global-constraints)` or $ cabal install --upgrade-dependencies `eval echo $(ghc-global-constraints)` where function ghc-global-constraints() { ghc-pkg list --global | tail -n+2 | head -n-1 | grep -v '(' | while read a; do VER=${a##*-} PKG=${a%-*} echo -n "--constraint='$PKG==$VER' " done } Actually upgrading packages from the haskell platform isn't such a big deal, but adding constraints for packages distributed by ghc is almost always the right thing to do. 
Also, is the library available on Hackage?
Donwloading a file must be very trivial operation, so no doubts it is written concise. But OP, how would you write "file downloader with progressbaar" in a similar neat manner? http://stackoverflow.com/a/22776
I love the fact that there is a Haskell library for like every abstraction you can imagine.
[My attempt](http://hackage.haskell.org/package/effects) has been on Hackage for almost 2 years ;-) 
There are also Haskell libraries for abstractions I cannot imagine. In most languages, the syntax or the semantics are the limiting factors; in Haskell, it is usually my own brain.
&gt; it is far from clear what more it is that you might want. I didn't really know what to ask for, because I don't know anything about it! Anyway, that was really interesting, thanks. I really need to learn some type theory one day soon. &gt; as "foo" is only any length you like as long as you only like 3. :D
You'd better not pass it a malformed URL: &gt; get (fromString "malformed") (\_ _ -&gt; return ()) *** Exception: Can't parse URI malformed Oh dear. How very unHaskell.
would you mind highlighting the differences between your effort and what is outlined in the paper? Self promotion! Can the examples be replicated in your library? I would love to ditch monad transformers *today* in my own code. I had never really grokked what effects / handlers were all about until last night and now I feel excited to go and confuse the crap out of myself by converting some of my utilities to using effects instead.
As they mention in the related work section of the paper, the big difference is that you have to pass around effect identifiers when using my library. The main advantage for this is that this makes it easy to have multiple instances of the same effect. But passing around effect identifiers is very inconvenient. It should be optional as it is in Eff. I haven't read the paper in any detail yet. If you want examples of using my library, look here: https://github.com/sjoerdvisscher/effects/blob/master/examples.hs
From my point of view, I find really elegant that you can attach two streams, one given from using a file from the output, one given from the connection, and everything will just work :) It's elegant, more than a trick itself.
Once I wrapped my head around the concept of arrows, I could *not* believe that something this generic could have first class language support.
Hackage2 is coming, just at a slow pace because only the Well Typed guys are working on it, and obviously not full-time. Check out the repo and the docs. A ton has already been done, and an instance is running smoothly at http://new-hackage.haskell.org/ -- so do not duplicate effort and contribute things to hackage2, pretty pretty please.
That's the fault of fromString isn't it?
That has nothing to do with Maven's dependency resolution, but with all the crap that projects include in their dependencies (a Java ecosystem problem or even choosing the wrong dependencies to work with). In addition, you can always exclude transitive dependencies that you don't need. Look, I have ZERO love for Maven after working with it for 5-6 years, but for different reasons than the ones being discussed here. For one, it's a big pile of pom.xml bloat. Second, everything is a plugin and even the most common tasks could be broken if you choose the wrong plugin version, and try to figure out why something is broken … It is not easy to get reproducible builds, etc). Things have improved in 3.0.x, but this thing is moving too slowly to my liking. Back to Cabal, it was an awesome surprise the first time I used it: it just works! However, the broken repository issue is a real issue that affects many people (I find that rebuilding everything when that happens as a long, bad workaround). Sandboxes are a great addition that improve on that and I am looking forward to it, but I find that the real nut of the problem is not being resolved. 
Nice, but does not work with takeR 1 []
`cabal install` taken by itself won't break even with continuous addition of *hundreds* of packages over a period of years. But doing cabal update is the equivalent of moving from hackage-01-01-2012 to hackage-07-18-2013 and should be compared to moving from `Oneiric Ocelot` to `Raring Ringtail`. No one expects to keep all their gruesome C libraries intact in such a change; but that is the miracle you are asking for, and it is not performed by linux package managers. This analogy is of course imperfect.
[What's this very awkward usage of `-` in Main.hs?](https://github.com/nfjinjing/geek/blob/master/src/Main.hs#L16) main = do with_spec (hspec Spec.spec) - do -- ... fork - serve_with_snap_server -- ... **Edit:** Overlooked the `NoImplicitPrelude`.
Haha, I never noticed Concurrently was an Alternative too. Perfect!
yes. it should be `fromURL:: String -&gt; Maybe URL`. but as you know there are different expectations on what to do about errors.
I *do* know, that's what I was complaining about :)
Yeah, the SQL backends appear to still be creating this weird arbitrary limitation. I don't understand why that would ever have been done considering the other backends don't do that, and SQL has the most possibilities for keys. Forcing a serial on every table is terrible even for the most trivial apps, as it breaks reference tables, forcing you to throw in dozens of unnecessary joins to get back to a working state.
I regularly use [pandoc](http://johnmacfarlane.net/pandoc/), and love it. 
I really like this set of videos: https://www.youtube.com/playlist?list=PLxj9UAX4Em-Ij4TKwKvo-SLp-Zbv-hB4B The author has more Haskell in a different play list. 
You are correct.
Thanks Ian for working on GHC!
Prompted by this discussion and some others over the past couple weeks, I've put together a small stab at a "how to get involved" hitlist here: http://www.reddit.com/r/haskell/comments/1iki8p/how_to_help_develop_hackage2/
Hacked this up earlier today. Feedback welcome.
Et voilà :) https://gist.github.com/adinapoli/631cfb76247fd5a8e1c8
Moving cabal to github seems to have greatly increased the visibility of the code and the number of bug fixes. I think a similar move here would be wise. (I even fixed a cabal bug once via the github web interface, and not just a documentation bug!) I don't even think I have darcs install at the moment, and I'm probably not alone.
Yeah these are really nice. I like style of the author, jekor.
What is Hackage2? In what ways does it improve upon Hackage, and why should I care?
Indeed, Jekor has set a high standard for Haskell screencasts. They're high quality videos, edited well, very clear and understandable speaking, and focused on a specific task. Just what you want from a screencast. Haskell Live are also on that level too. Very happy to see them!
I have recently written something quite similar. The interface that I opted for in the end was just one function: newLRU :: (Hashable k, Eq k) =&gt; Int -&gt; (k -&gt; IO v) -&gt; IO (k -&gt; IO v) Internally, it used a doubly linked list (in STM) and a HashMap (from unordered-containers). The list was ordered by last access. Each item contained a key, value pair and the HashMap mapped keys to list nodes. If multiple threads tried to lookup the same key at the same time, but the value hadn't been computed, then the value would be computed once and every thread would get the same result. If a computation raised an exception, the exception would be raised in every thread waiting on the result, but the exception wouldn't be cached. It doesn't scale across threads very well (every operation modifies the head of the list, though this could be easily fixed by sharding), but it fit my purposes perfectly.
It could be even more simple if you just use single MVar to keep HashMap. Without channel and forking new thread.
Good going on driving this forward, Gershom.
It's on my TODO list after #1362 and #1342. In the meantime, you can take a look at [this draft blog post describing Cabal's native sandboxes](https://raw.github.com/23Skidoo/homepage/master/content/blog/2013-08-20-Cabal-sandbox.md). See the second-to-last section which gives an intro aimed at the users of `cabal-dev`.
&gt; Sandboxed builds don't completely solve the problem either. Over a long enough time any given sandbox will eventually need to upgrade some of the packages it has installed. When this happens you'll either need to remove some old packages or delete the sandbox and start from scratch. `cabal-install` HEAD should be a bit better in this regard, because we now create an install plan for the whole sandbox when reinstalling (so installing a new version of a package also reinstalls its installed reverse dependencies). However, this functionality is relatively new and could be better exposed to the user (e.g. as `cabal install --whole-environment`).
Hackage 1 is a collection of perl(?) scripts that nobody wants to maintain. The only way for anything new to ever happen with Hackage is for Hackage 2 to be deployed. Hackage 2 is written in Haskell and is designed to be extensible. It is also a very nice and thoughtful code base. The #1 benefit is switching to a new, better code base that Haskell people are willing to touch and improve. There are a bunch of other improvements too -- but that is the best reason.
Existing hackage is a polyglot combination of scripts. It works, but is not very extensible. Furthermore it runs the builds for documentation etc. all on the main server so doesn't scale well. Now hackage is huge, and we want to make it better searchable, add tags, rev-deps, possibly ratings etc., other cool tools, ideally as plugins. Also, we want to get distributed documentation build and reports, from participating cabal users. hackage2 is an all-in-haskell hackage webapp with those sorts of features. when it is ready (which will be sooner if more people chip in) it will replace existing hackage. we're all quite excited for this.
`cabal-install` HEAD also supports this (`configure -w`, `install -w`). 
Code: functionA :: Integral a =&gt; [a] -&gt; a functionA [z,y] = if z &lt; 2^y then w else functionA [z,x] where x = 1 + y; w = y - 1 functionB :: Integral a =&gt; a -&gt; a functionB z = if z == 0 then 0 else functionA [z,y] where y = 0 functionC :: Integral a =&gt; a -&gt; a functionC z = z - 2^(functionB z) functionD :: Integral a =&gt; a -&gt; [a] functionD z = reverse [0..(functionB z)] functionE :: a -&gt; [a] functionE z = z:[] functionF :: a -&gt; a -&gt; [a] functionF z y = functionE z ++ functionE y functionG :: Integral a =&gt; [a] -&gt; [a] functionG z = if y == 0 then z else functionG x where y = functionC (last z); x = z ++ [y] functionH :: Integral a =&gt; a -&gt; [a] functionH z = if y == 0 then [z] else functionG x where y = functionC z; x = functionF z y; functionI :: Integral a =&gt; a -&gt; [a] functionI z = if z == 0 then [z] else functionH z functionJ :: Integral a =&gt; [a] -&gt; [a] functionJ z = if (functionC (head z)) == 0 then tail z else functionJ y where y = [functionC (head z)] ++ tail z ++ [functionB (functionC (head z))] functionK :: Integral t =&gt; t -&gt; [t] functionK z = if z == 0 then [z] else functionJ x where y = [(functionB z)]; x = [z] ++ y functionL :: (Integral a1, Num a) =&gt; a1 -&gt; Int -&gt; [a] -&gt; [a] functionL z y a = if length a == length x then a else functionL z v q where x = functionD z; w = functionK z; v = y + 1; u = x !! v; t = u `elem` w; s = a ++ [1]; r = a ++ [0]; q = (if t == True then s else r) functionM :: (Integral a, Num t) =&gt; a -&gt; [t] functionM z = if z == 0 then [0] else functionL z y a where y = (-1); a = [] convert :: (Integral a, Num t) =&gt; a -&gt; [t] convert a = functionM a
Best luck, and be sure to report back as soon as your plans concretize. Thanks for all Ian, you rock!
I prefer "ghc-pkg unregister foo" so that I can get rid of specific bad packages.
This function's pretty crazy, but definitely kudos for getting a working program going in Haskell. :) Here's a shorter version that I believe does the same thing that you're doing, but might be good for you to check out as a newcomer to the language. Let me know if you have any questions about it, and I'd love to help! dec2bin :: (Integral a) =&gt; a -&gt; [a] dec2bin = reverse . dec2bin' where dec2bin' 0 = [] dec2bin' x = (x `mod` 2) : dec2bin' (x `quot` 2) Best of luck with your learning!
There's overlap between Git and Darcs. If you outline what you like about Darcs that you believe not to be present in Git I can possibly outline equivalent ways of doing the same thing for you. 
No, you can define fix without using let. Using the same "f" and "y" as you defined: fix f = (\y -&gt; y (fix f)) f But Sgeo's definition is better.
Oh, that works too as long as there's a way to tell it to remove existing versions that won't be overwritten. 
Try this then: fix f = f (fix f) (Thanks to Sgeo for this definition)
I have a question for whoever knows better: would it be possible to switch over to a system similar to [Leiningen](https://github.com/technomancy/leiningen/) for handling dependencies in projects? Leiningen requires all dependencies to be specified with exact version numbers (no ranges) but allows multiple versions of the same package to coexist, even within the same project. This completely sidesteps the issue of sandboxing and ensures that conflicts never occur.
I had the cranky idea a while back that if `async` were closer to the core libraries, the `Concurrently` `Alternative` instance could be reworked to provide the (missing) `Alternative` instance for I0 (as a `MonadPlus` instance it seems a little strange, but who understands `MonadPlus`) -- then we could have told the op to write `import Control.Applicative`. Perhaps more interestingly, the Applicative instance for `Concurrently` could be unwrapped to provide the `MonadZip` instance for IO: instance MonadZip IO where mzip = concurrently -- :: IO a -&gt; IO b -&gt; IO (a,b) -- but not as liftM2 (,) Maybe it's silly -- and I don't totally understand the laws for `Control.Monad.Zip.MonadZip` -- but it would be cool to have a syntax that would support asynchronous IO 'out of the box', without fancy libraries. So, with existing extensions we could write: {-#LANGUAGE ParallelListComp, MonadComprehensions #-} -- together requiring MonadZip main = do ab &lt;- [a ++ b | a &lt;- readFile "a.txt" -- the bar in place of the comma means | b &lt;- readFile "b.txt"] -- the computations are not sequenced, but 'parallel' writeFile "ab.txt" ab -- thus with lists it zips: the applicative instance -- for ZipLists is the MonadZip instance for lists. -- Similarly, why shouldn't the Applicative instance for -- Concurrently be the MonadZip instance for IO? This is perhaps not too great an example -- reading webpages would be better -- but it would read the two files 'concurrently' and write their concatenation. Even if it gives less control than using `Async` and would be sneered at by cognoscenti, it would make a nice intro to concurrent Haskell and show that we mean business when it comes to concurrency. I wondered if Simon M's purpose in making the `Concurrently` newtype was to intimate this possibility. 
As a long time darcs user and even contributor, I had a hard time switching to git initially. Here are some resources that I found useful during the transition: * http://darcs.net/RosettaStone * http://marklodato.github.io/visual-git-guide/index-en.html * http://git-scm.com/book * https://help.github.com/ I hope that helps!
Take care: a lot of threads waiting for single MVar can be *very* bad for performance from my experience.
It'd also be a great way to get people to stop saying `IO ~ State RealWorld`. I don't think it's possible to get `many`/`some` instances that would work, though. More specifically, I don't think `Concurrently` can be a `Monad`, so in this case `ap` and `(&lt;*&gt;)` would be mismatched. That said, I definitely think Async should be frontloaded into lots of Haskell tutorials.
wow - indeed I would recommend to set up a quickcheck test aginst your dec2bin just to make sure the code below does the same :o No offense intended: but I in order to understand/read the original code I guess I would need huge amounts of coffee ... is this some joke I am missing here?
The hackage2 darcs repo is mirrored at http://hub.darcs.net/simon/hackage-server for easy browsing/forking ([changes](http://hub.darcs.net/simon/hackage-server/changes), [changes feed](http://hub.darcs.net/simon/hackage-server/changes/atom)).
jekor is a very good screencaster of Haskell. I hope he makes a video on how he does these screencasts, which tools he uses, etc. so that more people can publish those.
Here is a family of functions that does similar things: http://www.haskell.org/hoogle/?hoogle=unfold
You're probably right. The OP said that his function converted decimal numbers to binary ones (and I can see that it's in the form of a list), so I posted a function that did just that. Perhaps that's not what his does, but that's what he said it does (and I don't want to parse through all of his code either) so I was just offering a simpler solution to what he supposedly is trying to accomplish. When I first started coding, I know I did things in a very roundabout manner, so I can understand if the OP's function is a little long and does the same thing -- hopefully it does, and he can learn from this. :)
I always wondered what an unfold was, but never looked into it for some reason. I was trying to do this with a fold actually and I was like "nah, that's not right!" so, thanks! That makes sense and is awesome.
Very nicely done series about Haskell. Thanks for sharing.
I think you got that conversion backwards.
Yes, `Concurrently` can't be extended to a `Monad` with that `Applicative` instance, which is most of its point. I was thinking of `IO` itself -- that's why I mentioned `MonadZip` which is like `Applicative` -- an implementation of `mzip :: m a -&gt; m b -&gt; m (a,b)` can be used for an implementation of `(&lt;*&gt;)` -- `fmap (uncurry ($)) (mzip mf mx)`; similarly we get the right type with `mzip` = `liftA2 (,)` . But `mzip` presupposes a Monad and thus an `ap` Applicative; it is exactly supposed to have a different 'zippy' un-`ap`-like semantics. So `mzip = Control.Concurrent.Async.concurrently` is not imcompatible with the IO `Applicative` instance (== ap) And unlike `Applicative`, `MonadZip` already comes with pleasant syntactic support -- that would be the only point of this exercize -- even if its utility is limited. Alternative only requires a monoid, so I don't know what is to be said against `fmap (either id id) (race x y)` as the`(&lt;|&gt;)` for `IO` But everything about `IO` involves curiosities, so this scheme is probably wrong somewhere. `many` and `some` diverge for most Alternatives, by the way.
This is just an eta expansion of Sgeo's, which is in turn just a let moved to top level.
Yes! I completely forgot about it, but thanks for reminding me.
that makes sense, i just think this isn't described to new comers to Haskell (Source: i'm a haskell noob), and should just be made more clear so people learning it don't run into this problem
you should make this a gist or github repo, this is really nice, if not i will eventually time willing
awesome i keep seeing haskell used more and more
&gt; when it is ready... Is there an ETA for this? What are the blocking issues? Is there a volunteer Hackage maintainer yet? I was under the impression that Hackage2 has been ready for a while, but that the main thing lacking was someone willing to take over the job of maintaining the instance running at hackage.haskell.org
Thanks for the thoughts, I know how extensive my code may appear to be but really it is multiple modules working together to calculate certain values and then put those values in an algorithm which decides what should be presnted accordingly. Also, you are correct, the binary code is shown within a list; convert 500 returns [1,1,1,1,1,0,1,0,0]
Oh, I see where you're going now. I've never thought hard about `MonadZip` semantics... Given the pliability there and the pliability around `IO` this is starting to sound much more plausible. I wasn't just worried about `many` diverging but instead `many` fork-bombing people.
Eta: depends on speed of development. Blocking issues: check the resources I posted and by all means help trying to sort it out Volunteer maintainer: I don't know the story exactly but I'm much less worried about this than in the past. It would be better to find a dedicated person or two, but with the admin volunteers we've had so far, etc. I'm sure we'll be able to cobble along regardless. My impression at one point was like yours -- the lack of a new maintainer. But I think at some point, as we got the test instance closer to swapping in for the current one, a variety of other minor issues we needed to deal with came up. I really hope that this cutover will be relatively soon, but I'm just spreading information, not actively participating in development, so I'm uncomfortable giving any timetable in particular. The graph of source lines posted at main hackage should give some idea of the rhythm of development, pauses and spikes included: http://hackage.haskell.org/packages/lines.png Initial phase, spike during gsoc, two little bumps around the winter holidays of he subsequent years, and then finally late 2012-now picking up steam again. The darcs change logs give some picture of the sorts of things being worked on: http://hub.darcs.net/simon/hackage-server/changes
I really want to try this, but I'm having problems installing it. Here's my log if anybody can tell me what's wrong. http://fpaste.org/26399/74212898/
Welcome to Functional Programming! Some Notes: * You gave us a functions that were not used in the final product and did not indicate they were leftovers. * you used where y = ... quite a bit where the ... could have been put directly in the body. * You clearly have the ability to hold complex control flow in your head, this is good, but don't let your talent obfuscate your intent. Your programs need to be readable by future you and other programmers. * I took the liberty of renaming and removing some of the functions based on my understanding of their intended use. Hopefully it will be more readable to the Haskellers here now. * You should read up on pattern matching, lists in Haskel and the general properties of linked lists, folds, and maps. These will allow you to be more concise and clear in the functional style, and perhaps avoid some unecessary explicit recursion. * looking over Learn You A Haskell will be beneficial * Re: lists, using the idioms last xs and xs ++ [y] are inefficient and you lose the benefits of pattern matching head and tail with (x:xs). * I replaced many of your if then else patterns. Never do f x = if x == 0 then 0 else something x. ----- --instead f 0 = 0 f x = something x What follows is a direct translation of your program, logic and flow intact. I have consolidated a few things and gotten rid of what must have been leftovers. Hopefully this will give you an feel for alternate styles of writing. -- functionA exp' z y | z &lt; 2^y = y-1 | otherwise = exp' z (y+1) -- functionB getExp 0 = 0 getExp z = exp' z 0 -- functionC reduce z = z-2^(getExp z) -- functionJ fBuild (z:zs) = if x == 0 then zs else fBuild $ [x] ++ zs ++ [getExp x] where x = reduce z -- functionK fBuildI 0 = [0] fBuildI z = fBuild $ [z] ++ [getExp z] -- functionL converter z y a | length a == length x = a | otherwise = converter z (y+1) q where x = reverse [0..(getExp z)] t = (x !! (y+1)) `elem` fBuildI z q = bool t (a ++ [1]) (a ++ [0]) -- convenience---^ bool t a b = if t then a else b convert 0 = [0] convert z = converter z (-1) []
`fromString` and `fromInteger` are fantastic examples of why I want even limited pi types
Indent each line of your code by four spaces when posting it to Reddit, to ensure it's formatted properly. Also, ensure a blank line separates the code from the non-code.
Duncan told me they would most likely move hackage2 to github when it's actually out, replacing the current one. Just a hackage2 that's more or less a drop-in replacement of its ancestor.
That's a *great* initiative Gershom. The same feeling prompted me to write http://alpmestan.com/2012/11/02/cabal-hackage-what-you-can-do-about-it/ a couple of months ago. We'll do it again in 8 months I guess heh
A pity, but under Windows I get *Main&gt; main "Downloading http://download.thinkbroadband.com/10MB.zip" *** Exception: getAddrInfo: does not exist (error 10093) Edit: ah, it is a common windows network problem. When running on windows, all network code must be in `withSocketsDo` block: import Network.Socket (withSocketsDo) ... downloadFile url name = withSocketsDo $ get url $ ... Looks cool, anyway!
Happy you liked it! I've also put a comment incorporating your suggestion to make it work on Windows!
Well-Typed (funded by the [IHG](http://industry.haskell.org/)) are working on fixing the last few things to get it to feature parity (recently: fixing backup/restore, download counts, doc builder client) and to get it deployed and to switch over. Volunteer help to test and improve other features welcome of course. We're expecting to give it a rather higher visibility once we have the test instance up on the new server, with full feature parity, that'll be the beta phase and we'll make more noise then. For site administrators we have a couple volunteers. For code maintenance, this may be a longer process to get more people involved. As I said, once the beta is out and we move to github, we hope to pick up some more people since it'll be obvious that it's not vaporware and you'll be able to see your changes on the live site much quicker than is the case now. As for ETA, a goal I have in mind is for us to be switched over, or at least be well into the beta by ICFP this year.
Added various other functions to this gist as well. Ordered by level of frustration: - Safer cabal --upgrade-dependencies by adding global package constraints - When that doesn't work: Easy detection of non-optimal upper bounds with out-of-date packages. - When things starts to get weird: Unregister all broken packages as reported by ghc. - When giving up: Reset ghc pkg db. https://gist.github.com/alexanderkjeldaas/6037973
 import qualified Data.ByteString as BS import qualified Data.ByteString.Char8 as C8 import qualified System.IO.Streams.Combinators as SC displayProgress :: Integer -&gt; OutputStream BS.ByteString -&gt; IO (OutputStream BS.ByteString) displayProgress length outStream = fmap fst $ SC.outputFoldM acc 0 outStrean where acc :: Int -&gt; BS.ByteString -&gt; IO Int acc last bs = do let progress = last + BS.length bs putStr $ "\r[" ++ progressbar length progress 10 ++ "]" return progress progressbar tot prog len = map outchar [1..len] where cur = floor $ prog / tot * len outchar n | n &gt; cur = ' ' | n == cur = '&gt;' | n &lt; cur = '=' downloadFile url name = get url $ \response inStream -&gt; do let totalLength = getHeader "ContentLength" response let outFile = S.withFileAsOutput name outStream &lt;- case totalLength of Nothing -&gt; return outFile Just len -&gt; displayProgress length outFile where length = C8.readInteger len inStream `S.connect` outStream This is the most concise that i could come up with. this is written in my head because cabal is being difficult tonight and i can't install io-streams, but the types should be correct. The fun thing is `S.outputFoldM :: (a -&gt; b -&gt; IO a) -&gt; b -&gt; OutputStream b -&gt; IO (OutputStream b, IO a)` meaning that while you're writing to disk, the accumulator can be doing arbitary IO. in this case, it's using carriage return to repeatedly draw a progress bar in the same place on screen. Although, it only works if the webserver you're talking to is kind enough to send you the ContentLength header, otherwise you're flying blind.
Ok - seems you are serious - so just a little hint: even if the "real" functional programmers say otherwise: naming of functions (and sometimes arguments too) does matter if you want a "normal" person to understand your code ;)
This is a pretty crazy way to express that algorithm which is just a simple loop. Have you written the equivalent program in a non-functional language?
probably need GHC 7.6.*
Yeah he's jolly clever :-) When he and I started the company together five years ago I realised that in our cohort at Oxford he had the top first and I had the bottom first that year (but still a first, just!), he also finished his PhD a good few years before I did :-)
I'm thinking about bringing `cabal upgrade` back, but enabling it only for sandboxes (since they are guaranteed to be consistent, the solver should be able to do the right thing).
How many threads waiting for single MVar can hurt performance? I didn't found performance problems with thousands of threads looking into single MVar sometimes. And Chan is having more than one MVar underneath. Plus switch to another thread is necessary even when there is only on request to cache.
See http://stackoverflow.com/questions/6115459/small-haskell-program-compiled-with-ghc-into-huge-binary
Haskell executables typically have everything linked statically, including the language runtime, standard library, other libraries, etc. This makes binaries fat. It is possible to create dynamically linked binaries which will be a lot smaller. It is not the prevailing practice because of practical problems related to delivering dll files and portability.
Hmmm, how are sandboxes different from ~/.ghc? I've been using cabal-install HEAD for awhile, but found myself using the --user repository instead of sandboxes because the difference in ghci support was a deal breaker for me. It won't do me any good to have `cabal upgrade` only for sandboxes.
&gt; It is possible to create dynamically linked binaries which will be a lot smaller. OTOH, if you only installed `darcs` (and no other Haskell program), you'd still end up requiring _at least_ the same amount of executable code, but this time distributed among the `darcs` binary and a couple of DSOs (which aren't really "shared" objects)
For the record, git does not pack any scm functionality into its binary: there'll be another directory like /usr/libexec/git-core/ with 150-200 compiled binaries, bash scripts, perl scripts, python scripts, and whatever else they put there. All the git binary really does is replace spaces in the command you type with dashes, and look up the correct executable in that directory. I don't know where you got your darcs binary, but on my fedora machine, comparing git (without all the extra packages with optional functionality from the upstream tarball) and darcs directly from the distribution (so they in both cases should be stripped), comparing the two gives a different result: $ du -hs /usr/libexec/git-core/ /usr/bin/darcs 12M /usr/libexec/git-core/ 6,1M /usr/bin/darcs So the notion that git is that much more space efficient doesn't seem to hold up that well to me.
&gt;I always wondered what an unfold was, but never looked into it for some reason. Same for me! So I decided to try it out in ghci, and here's what I got: convert :: (Integral a, Num t) =&gt; a -&gt; [t] convert = reverse . unfoldr (\x -&gt; if x == 0 then Nothing else Just $ swap $ divMod x 2)
As I understand it multiple versions of the same package would require changes in GHC, not just cabal or hackage.
&gt; For the record, git does not pack any scm functionality into its binary: there'll be another directory like /usr/libexec/git-core/ with 150-200 compiled binaries, bash scripts, perl scripts, python scripts, and whatever else they put there. All the git binary really does is replace spaces in the command you type with dashes, and look up the correct executable in that directory. This isn't entirely true, at least not on my system. Most of those compiled binaries are hard links to a single 1.1M executable that handles about 90% of git's core functions. The majority of the binary size is used by executables associated to git's various types of remotes (http(s), ftp(s), ssh). There are also a bunch of tiny scripts that implement bits of front-end functionality like `git stash`. I think this just goes to show that comparing code size of programs that do "roughly the same thing" is pretty meaningless. EDIT: I meant to add that on my system `/usr/bin/git` is a copy of that ~1M executable that handles git's core functions. It's not a huge ~1M executable that replaces spaces with hyphens. (I don't know why it's not yet another hard link to the same file, maybe the Debian maintainers have their reasons. On the OP's system you can see that it is, from the `107` link count.)
It was a kind of tenuous word-play.
This will be getting slightly off-topic, but I think it needs to be said anyway. &gt; One of the very first things I do when getting in contact with a new programming language is building a few example programs (from Rosetta Code or similar) using the most common language implementation and compare their binary size with their corresponding counterparts of same functionality produced by other languages. Is there any particular reason you have this pratice? Secondary storage is very cheap nowadays, so it's not like you *really* need to worry about programs being 30 MB in size. Or is it?
&gt; (a ++ [1]) ouch.
Either Reddit ate it, or I forgot to submit. So if this is a duplicate, please ignore ---- Storage transfer bandwidth is still an issue. Not everybody has an SSD, or a fast connection. With a regular HDD the amount of time required to transfer 30MB is measured in ms, not µs. Similar if you're on a network, where, depending on the used network file system protocol TCP slow start will significantly reduce your initial throughput even on a ultrahigh bandwidth link.
Haskell programs tend to reuse code through libraries a lot more than C programs. The unit of linkage in Haskell is by default the module. If I import a module for one specific function that I want, I also get every other definition in the same module, as well as the entirety of every other module used by any definition in that same module, every module referenced by those modules, etc. That can be a lot more than I really need, and a lot more than I would rewrite from scratch in C. There is an option [-split-objs](http://stackoverflow.com/questions/9198112/haskell-unnecessary-binary-growth-with-module-imports/9198223#9198223) that produces one linkage unit per function. If you care about binary size, you can try rebuilding everything with this option. The GHC developers don't recommend using it; I'm not sure exactly why, but it probably results in hilariously long link times. In general I think binary size is not that big a deal. There is a long, long list of improvements to GHC I would rather see than even an improbable 100x reduction in binary size...
I will concede your point about networking, but you are wrong about the load times from disk. Why, you ask? All* modern OSes load executables on demand nowadays. What does this mean? When you execute a 30 mb binary, instead of reading 30 mb from disk and loading it in memory, your OS will just reserve the necessary space in memory and mark all memory pages as absent in the page table. When some part of the binary gets accessed the program will page fault on the absent page, resulting in your OS reading that page from disk and storing it in memory, leaving the rest absent and hand control back to your process. The result is that unused code/parts of your binary will never be loaded and thus won't affect load times either. As such it's much simpler to just link in an entire library without trying to minimise its size, since it will only cost you disk space and the implementation is much simpler. * - linux, BSD, OSX, I know jack about Windows, so I can't be sure, but I'd be surprised if Windows doesn't implement this as well.
Thanks for the notes, I was not completely sure how others would see my code, so now I have a broad understanding of the improvements needed to make my code readable and efficient. "You should read up on pattern matching... Learn You A Haskell will be beneficial." I agree, it is my main reference for learning Haskell techniques.
&gt; When you execute a 30 mb binary, instead of reading 30 mb from disk and loading it in memory, your OS will just reserve the necessary space in memory and mark all memory pages as absent in the page table. Yes, I know about that. In fact I'm telling that other people myself very often. But given that the code coverage of an executable will usually be way above 80% all this code must be reached eventually. An OS has two options: Either swap in the faulting page on demand (causing a very noticeable program interruption if the page must be loaded from disk. Access times of HDDs are in the ms range as well). *Or* the OS will prefetch the whole binary into storage I/O cache (which is what all modern OS do). This prefetch however will consume bandwidth (i.e. other storage I/O operations are impaired) and also it will put additional pressure on the access scheduler, but even the most clever seek strategy will not compensate for the latency introduced by disk seeks. &gt; The result is that unused code/parts of your binary will never be loaded and thus won't affect load times either. This assumes that those parts of the binary are never used. We're talking code coverage here. All unused ~~parts~~ *compilation units* (i.e. `.o` files) of static libraries are not linked in anyway. *Of course if a static library has several huge compilation units, this creates a whole different picture*. *EDIT:* clarification on static library semantics.
Well if I was looking for "normal" people to review my code, I wouldn't have not brought it your attention. However in all seriousness, I do agree with you.
Not at all, this by far is my first attempt at composing a working program in any language.
Thanks, if I had know of a shorter code than my own, I would have not hesitated to replace it. However I learned alot creating it, so I regret nothing. :)
&gt; Well okay, but the linker should be able to figure out, which symbols are not used at all (also in longer dependency chains) and and omit them. GHC just uses the system linker. `ld` has no way of knowing that it's safe to drop parts of an object file; there could be relative jumps between symbols or whatever. The same thing happens in a C program: everything in the same .c file as a function you call will end up in your executable. (The difference is that in C, hardly anyone ever statically links an object file they wrote against an object file someone else wrote; so you're unlikely to ever have unused code in the first place.) &gt; I mean, especially in a functional language figuring out the invocation graph should be a very manageable job. That's basically what the -split-objs option does. It puts each function (or each mutually recursive group, I assume) in its own object file so that the linker can determine which functions are actually needed. Like I wrote above, I'm not sure why it's not recommended, other than that it probably makes linking very slow. You could try it! `rm -rf ~/.ghc`, then add `split-objs: True` to `~/.cabal/config` and `cabal install darcs`.
Works on Chrome, Firefox, and IE on Windows 7. It does do very strange things if you resize the window, though.
Nice observation. Thank you.
Is it solved now?
Works for me on iPhone (iOS 6 Safari)
Yes in Chrome and Firefox. In IE10 it's fixed but there's now a vertical scrollbar there all the time. It's not that bad but it is different. Also the movement in IE is a bit jerkier but that's probably just because the Javascript engine isn't as good.
Oh, so this is your first program _full stop_, not just your first program in Haskell? Interesting choice of starting language! I approve.
Actually, I'm not so certain detailed usage analysis is done at link-time. I seem to remember being able to strip out a lot of useless stuff from GHC binaries. Seeing as GHC links using the system `ld`, I'm pretty sure most binaries just include everything.
&gt; RAM is cheap too, but cache comes in layers and the smallest fastest layer is still pretty small. Keeping programs small still helps keep them fast. Even if much of the program is never used, having little fragments of frequently used code scattered throughout a huge block of rarely/never used code still damages locality and performance. Whether this effect is noticeable depends on how finely interleaved the dead code is with the live code, compared to the cache line size. I mean, it's not like GHC is quadrupling the size of its output by padding it with nops. I would not be surprised if for most applications, the effect was not measurable. Don't get me wrong—smaller code is better than larger code, all else being equal. But if it's performance you're after, I expect that there are easier ways to get speedups than by reducing code size. I would love to be wrong about this!
The C library has every function as a separate object. I don't think this is a linking time issue.
Works for me on FireFox 20 on Ubuntu.
You should compare it to Hakyll, which I think is the current leader in the Haskell community for static site generation.
You're absolutely right, and more code can easily be faster too (e.g. a more sophisticated data structure giving better data locality). I stated this over-confidently. I don't know how serious the problem is, but I'd guess you're probably correct that probably there isn't a problem at all. My kneejerk reaction is that huge binaries are a bad thing, but with modern hardware there's certainly a strong case for not caring most of the time. There was a bit of fuss a while ago about building wxWidgets on MinGW GCC 4.5, which was giving static link library files of more than a gigabyte, crashing builds for people still using older PCs. And that's not even a whole program - just one library. IIRC, at least some of the GCC developers responded along the lines of "so what". 
Good, that's what it's all about!
&gt; Hakyll Thanks, I did try it. It seemed too powerful and flexible for what I needed. 
What I mean is that you should write up a post contrasting it with Hakyll for people interested in your `geek` package so that they can learn about the different design tradeoffs you made compared to Hakyll.
Yep! However, most "updates" to binaries actually replace the old binary with a new file. Which avoids any weirdness. A nice explanation is here: * http://stackoverflow.com/questions/4453781/what-happens-when-you-overwrite-a-memory-mapped-executable 
unfortunately, no, it does not compile and types are not correct. Something like 2-3 pages of errors =)
Side note: you can even include `withSocketsDo` as code, on *NIX it is defined as `id`, so you get cross-platform code.
&gt; GHC just uses the system linker TIL. I wasn't aware of that. Still I think it should be possible for GHC to generate a list of required symbols (after all GHC creates those symbols in the first place). GCC ld and gold are really capable tools and you actually can pass them a list of symbols to process by the option `--retain-symbols-file`. I think making GHC pass the list of actually required symbols to ld would be a huge improvement. &gt; ld has no way of knowing that it's safe to drop parts of an object file; there could be relative jumps between symbols or whatever. That's why when programming in, say C, when creating a library you put each function into its own compilation unit. That allows the linker to omit all compilation units not required.
OK, looks like I need to get io-streams installed and fix up the types.
Oh. geek is only a blog engine, it can not be configured to do anything else, where Hakyll / Jekyll can generate any kind of static website from magazine to user manual. So a world of difference. Main benefit of geek is that, if blog is what you need, then pretty much nothing need to be configured, just write posts, they will appear. Where for Hakyll / Jekyll, you need to at least design your own template. Lastly, search is an included feature of geek, something impossible for static websites without introducing third party services.
Neat! If Google ever jumps the shark with Blogger then I will definitely try geek out.
Ah yes, I wasn't seeing your point about the possible danger of `many` in this context; I will think about it if I ever get the nerve to formulate a proposal
It is the same, and makes sense, on Chrome and Safari on os x
Works perfectly for me in Safari 6 on Mac OS X 10.8.4 (Mountain Lion).
That does indeed sound like a lot. Could it be that `-split-objs` is splitting more than top-level objects? If so then there are two possible solutions: 1. Identifying non-top-level objects that are only referred to by one other object and recursively collapse into larger objects. 2. For every top-level symbol, have all symbols it depends on marked as weak symbols. This allows duplicate symbols between the objects. Edit: I wrote up a small gist that combines all the numbered _info symbols into larger object files: https://gist.github.com/alexanderkjeldaas/6043997 As an example on a random package run in a subdirectory inside of `dist`: $ combine.sh Addr ld -r Addr_p_o_split.new/Addr__402.p_o Addr_p_o_split.new/Addr__25.p_o Addr_p_o_split.new/Addr__403.p_o -o Addr_p_o_split.new/Addr__403.p_o.new.o rm Addr_p_o_split.new/Addr__402.p_o Addr_p_o_split.new/Addr__25.p_o Addr_p_o_split.new/Addr__403.p_o mv Addr_p_o_split.new/Addr__403.p_o.new.o Addr_p_o_split.new/Addr__403.p_o ld -r Addr_p_o_split.new/Addr__314.p_o Addr_p_o_split.new/Addr__316.p_o Addr_p_o_split.new/Addr__318.p_o Addr_p_o_split.new/Addr__344.p_o Addr_p_o_split.new/Addr__353.p_o Addr_p_o_split.new/Addr__350.p_o Addr_p_o_split.new/Addr__247.p_o Addr_p_o_split.new/Addr__349.p_o Addr_p_o_split.new/Addr__347.p_o Addr_p_o_split.new/Addr__272.p_o Addr_p_o_split.new/Addr__323.p_o Addr_p_o_split.new/Addr__283.p_o Addr_p_o_split.new/Addr__291.p_o Addr_p_o_split.new/Addr__239.p_o Addr_p_o_split.new/Addr__243.p_o Addr_p_o_split.new/Addr__241.p_o Addr_p_o_split.new/Addr__354.p_o Addr_p_o_split.new/Addr__358.p_o -o Addr_p_o_split.new/Addr__358.p_o.new.o rm Addr_p_o_split.new/Addr__314.p_o Addr_p_o_split.new/Addr__316.p_o Addr_p_o_split.new/Addr__318.p_o Addr_p_o_split.new/Addr__344.p_o Addr_p_o_split.new/Addr__353.p_o Addr_p_o_split.new/Addr__350.p_o Addr_p_o_split.new/Addr__247.p_o Addr_p_o_split.new/Addr__349.p_o Addr_p_o_split.new/Addr__347.p_o Addr_p_o_split.new/Addr__272.p_o Addr_p_o_split.new/Addr__323.p_o Addr_p_o_split.new/Addr__283.p_o Addr_p_o_split.new/Addr__291.p_o Addr_p_o_split.new/Addr__239.p_o Addr_p_o_split.new/Addr__243.p_o Addr_p_o_split.new/Addr__241.p_o Addr_p_o_split.new/Addr__354.p_o Addr_p_o_split.new/Addr__358.p_o mv Addr_p_o_split.new/Addr__358.p_o.new.o Addr_p_o_split.new/Addr__358.p_o ... $ ls -1 Addr_p_o_split | wc -l 408 $ ls -1 Addr_p_o_split | wc -l 357 This is *only* for the info sections. I am sure that a lot more of this sort can be done. 
Works for me using Chromium under xubuntu.
This is going to be cited in all language-wars on parallel and concurrency features from now on.
Always a vertical scrollbar in Surf (Webkit) as well. Runs smoothly though.
Neat! Thanks for sharing the tip!
It will have different interfaces. * The low-level. Create the variables, call Processing functions, and do all the stuff in a similar structure to what you do with Processing. A Setup part, a Draw loop, etc. With some modifications and, also, typed. * Baked low-level. You have the same interface that you have in the low-level side, but some things are already done for you. For example, this would create a script filling the entire available screen and displaying a white circle under a black background doing circles around the center. theScript :: ProcScript theScript = animationScript 0.03 theSetup drawFunction theSetup = execProcM $ do size screenWidth screenHeight fill $ Color 255 255 255 255 drawFunction t = do background $ Color 0 0 0 255 circle (x0 + r * sin t , y0 + r * cos t) 20 where r = 100 x0 = intToFloat screenWidth / 2 y0 = intToFloat screenHeight / 2 Note that *drawFunction* depends on a time variable. Actually, *t* will increase 0.03 each frame (as specified to the *animationScript* function). In each frame, the *drawFunction* will be called. If you know Processing, you know that this is done using a *draw* loop, creating a variable and increasing it. However, in the code example above, there is no single variable creation or loop (in appearance). * High level. After having solid low-level combinators, I will implement a gloss-like interface. A datatype of figures that can be rendered, depending on time or user-input. I am still studying the possibilities I may take here. The Processing code will be automatically generated, with all the variables and settings needed. I hope this answers your questions.
Yeah that was more-or-less the idea! In hindsight I should have said "convert from the Darcs side to the dark side" :)
It does answer my questions and it sounds absolutely awesome. Thanks so much for doing this!
Interesting. Does it use the same Haskell subset Fay does, or is it something altogether different?
It is just a plain Haskell library.
Don't bother =) It was a good exercise to fix your code )) https://gist.github.com/danbst/6044816
Been waiting for this book. There goes the rest of my weekend! *And quite possibly the next few weeks too.*
Could you give us the expression that it is giving this warning on? I would expect this warning if you're doing something like: frac ^^ 2 so it could instead be: frac ** frac In the case of square it is generally faster to do a multiplication by itself instead of an actual exponent. 
That's not what `(**)` does. There are three exponentiation operators in Haskell: - `(^)` takes any `Num` value to the power of a non-negative `Integral` value. - `(^^)` takes any `Fractional` value to the power of an arbitrary `Integral` value. - `(**)` takes any `Floating` value to the power of another `Floating` value of the same type. I'm not sure why HLint is suggesting the change here. EDIT: Corrected a small misrepresentation of `(**)`.
Thanks, I'll try it out. Let me explain how the program works; Lets say that we want to *convert 42* into binary. The first thing that we need to do, is find the highest exponential value of two that fits within 42. *functionB 42 returns 5*. functionB z = if z == 0 then 0 else functionA [z,y] where y = 0 functionA [z,y] = if z &lt; 2^y then w else functionA [z,x] where x = 1 + y; w = y - 1 functionB combines [z,y] and allows functionA to use these variables in its recursive algorithm. Its edge condition, 2^y &gt; z stops the process and returns y - 1, which is going to be our highest exponent and reference in other calculations such as functionD, which gives us a list of all exponents from functionB z through 0. functionD 42 = *[5,4,3,2,1]* We use this list to compare the values attained through the process of subtracting the highest exp. value from the previous value presented; 42 - 2^**5** = 10 - 2^**3** = 2 - 2^**1** = 0 If we extract each exponent and bind them into a list, we can compare these values to the values in the list given by functionD. functionD 42 = [5,4,3,2,1,0] functionK 42 = [5,3,1] So for every common element that exists within these two lists, their will be a 1 marked in its place. All else become 0's. functionM 42 = [1,0,1,0,1,0] functionD 42 = [5,4,3,2,1,0] functionK 42 = [5,3,1] This algorithm is located within functionM. functionM z = if z == 0 then [0] else functionL z y a where y = (-1); a = [] functionM = 42 = functionL 42 -1 [] Basically, it checks for every common element between functionD z and functionK z and replaces those values with 1 and 0's if else, all until the list is the same length as the list from functionD. functionL z y a = if length a == length x then a else functionL z v q where x = functionD z; w = functionK z; v = y + 1; u = x !! v; t = u `elem` w; s = a ++ [1]; r = a ++ [0]; q = (if t == True then s else r) I hope this makes more sense, if you have any particular questions, ask on :)
Maybe because `(**)` is a lot faster than `(^^)`? The default implementation of `x ** y` is `exp (log x * y)`, while `(^)` and `(^^)` are implemented recursively. (http://hackage.haskell.org/packages/archive/base/latest/doc/html/src/GHC-Real.html#%5E)
The rule in HLint you are triggering is: warn = x ^^ y ==&gt; x ** y where _ = isLitInt y So it only fires if y is a literal integer, in which case both ^^ and ** work and will give the same result, but presumably ** is significantly faster. This rule was originally provided by Lennart Augustsson, who usually has a good sense for these things (the whole ^ vs ^^ vs ** confuses me).
What? The rule seems backwards to me. I'd say the other way around makes sense. It's good to avoid **.
I could certainly have taken your suggestion and implemented it backwards...
That sounds like a security nightmare.
 `put it in backticks next time (**)`
Thanks, I understand the intent, and given what you've said, here is how I might write that. The new exp' function takes advantage of some functions that you may not have seen yet in LYAH. you can type :t logBase in ghci or search hoogle for any strange functions that you come across. I believe that LYAH does cover iterate, which endlessly applies the function you supply to a starter value and produces a list of the all of its results. This completely rids your program of explicit recursion, and ideally is easier to read. Forgive the unicode, it was a pasting accident. convert ∷ Integral a ⇒ a → [a] convert 0 = [0] convert z = reverse $ map (λx → if x ∈ elems then 1 else 0 ) exps where (exps, elems) = ([0‥(exp' z)] , build z) -- fromIntegral is kind of a wart, I find the name irritating, -- but it is what allows for us to use -- logBase, which is a Float operation. -- floor brings us back to intger land. exp' z = floor $ logBase 2 (fromIntegral z) -- same as ever reduce z = z-2^(exp' z) -- since you only used reduce to generate a number that you put into exp, -- I split out the two parts, generation and transformation. The takeWhile -- prevents iterate from running amok build z = map exp' $ takeWhile (&gt; 0) $ iterate reduce z 
How do you push out security fixes as a developer or package maintainer, and how do you ensure you're using the security fix as a user?
The vast majority of Haskell software is statically linked; new security fixes require rebuilds of everything. How do version ranges solve this?
Thanks.
&gt; Haskell executables typically have everything linked statically, including the language runtime, standard library, other libraries, etc. Well not quite everything, they're normally dynamically linked against C libraries like libc, libz, libpthread etc., just like a typical C program would be. When people talk about dynamic linking for Haskell they mean dynamic linking against Haskell libraries.
We ordered one for the Erudify library.
nice
Works well on iOS 6 on iPhone 5, and even Firefox 18 on the Keon!
I just read your [blog post](http://winterkoninkje.dreamwidth.org/85045.html) on this subject. Great work! And fast, since this post has only been up for 10 hours. I have one minor thing to add about **. It is exponentiation, and as you said it works for what looks like all elements of the semiring. This exponentiation is probably most naturally thought of as exponentiation within a Lie group, where the exponent has the structure of a Lie algebra. That is the connection with your groups of invertible matrices, for which exponentiation is defined over some Lie algebras of matrices. Just one more reason to call it exponentiation, as this has a very precise geometric meaning for Lie groups (you could weaken this as far as you like to Lie groupoids, or to any smooth manifold with an affine connection, but god help you if you try to do that in Haskell :-)). Thanks again for the excellent comment and blog post!
Definitely agree with the no batteries included bit. I've been looking into playing with Yesod for a side project, and one of the major things holding me back is the lack of a library like [Resque](https://github.com/resque/resque) for background jobs. Edit: Okay, after seeing the four comments left here, I've reached the conclusion that I just haven't looked/thought enough about it. I've never played with anything that reaches into the OS-level API calls.
note that this equals flip swing (&lt;&lt;=) It's a flipped monadic concatMap though
Given that Lennart wrote it using a peasant exponentiator, it'd have to be rather ridiculously big at which point I'd fear for the accuracy of the `(**)` version. In general it is safe to assume that you can use `(^^)` whenever it typechecks. =)
I took the time to get the types worked out, and added some features, as well as rate limiting when printing to the terminal. https://gist.github.com/twoolie/5c51f280910f52a1e727 I shamelessly stole some ideas from both you and /u/CharlesStain in this one. It was a really good exercise to do and forced me to learn cabal-dev to get an environment where this works.
Do you find the same effort is necessary with Idris as opposed to Agda or Coq? 
so does that generate JavaScript code? or does it compiles to JavaScript with GHCJS or similar? I'm wondering how does code generation work, can you please elaborate on this?
&gt; `flip swing (&lt;&lt;=)` I don't believe you meant to use a comonadic operator. I am not sure what you meant. flip swing (=&lt;&lt;) :: Monad m =&gt; (((((a -&gt; m b1) -&gt; m a -&gt; m b1) -&gt; b) -&gt; b) -&gt; c) -&gt; c flip (swing (=&lt;&lt;)) :: Monad m =&gt; m (a -&gt; m b) -&gt; a -&gt; m b flip swing (&gt;&gt;=) :: Monad m =&gt; ((((m a -&gt; (a -&gt; m b1) -&gt; m b1) -&gt; b) -&gt; b) -&gt; c) -&gt; c flip (swing (&gt;&gt;=)) :: (a1 -&gt; (a -&gt; a1) -&gt; b) -&gt; a -&gt; (a -&gt; a1) -&gt; b The second of these is nearest to my `&lt;=&lt;&lt;&gt;`, but still has a different type. (&lt;=&lt;&lt;&gt;) :: Monad m =&gt; m (a -&gt; m b) -&gt; m a -&gt; m b 
I'm not sure I understand your comment. 1. When you say "the exponent has the structure of a Lie algebra" that can't possibly be right. You are supposed to be able use numbers as exponents! Did you mean "the exponent is an *element* of a Lie algebra"? Then which Lie Algebra? You mention something about a Lie group, so I'll guess you mean its Lie algebra. 2. If all of that is right then it sounds like you're saying that `(**)` has type `G -&gt; g -&gt; G` where G is Lie group and g is G's Lie algebra. What operation is that supposed to be? Or did I misunderstand the type you meant? If I did get the right type, aren't you disagreeing with winterkoninkje? He says something like `AlgebraWithExponentiation a =&gt; a -&gt; a -&gt; a`.
Heh, I am coming from web.py or bottle (microframeworks), so to me it feels like Yesod has lots of batteries included. A matter of perspetive I guess I love the way sign in is implemented for example, if only I could grok how to actually use it (still a haskell beginner)
Rather tangential, but this imageOf entity = case entryImage (entityVal entity) of Just imgId -&gt; runDB $ get imgId Nothing -&gt; return Nothing can be refactored as imageOf entity = case ((entryImage . entityVal) entity) of Just imgId -&gt; (runDB . get) imgId Nothing -&gt; return Nothing then imageOf entity = maybe (return Nothing) (runDB . get) ((entryImage . entityVal) entity) then imageOf = maybe (return Nothing) (runDB . get) . entryImage . entityVal The reason I prefer the latter is not because it is shorter per se. There's no value to "golfing" for golfing's sake. The reason is that it's much clearer what's going on. You know you're taking the val of an entity and trying to end up with something you can `runDB` on, but you also know you might end up with `Nothing` in which case you just `return Nothing`. Of course you can also deduce the same from the original if you read through it, but it would be easier to hide a suprise in that version because it's longer. For the same reason I prefer `map` and `foldr` etc. over explicit recursion.
1. yes "element of" Lie algebra. The exponent in this case is a real number, which is a Lie algebra corresponding to the Lie group of positive reals with multiplication. 2. It's a minor disagreement. Exponentiation of a Lie algebra element (g -&gt; G) looks sort of like (G -&gt; G) since the Lie algebra is a local version of G itself. Less a disagreement on how to generalize (**) and more of a suggestion.
oh 2. The operation with type G -&gt; g -&gt; G takes a group element h and vector v in g to h.Exp(v). Note that a real generalization to Lie groups would have two such operations and the other would be Exp(v).h. For the reals this distinction is moot so you just have (**). EDIT: it renumbered my list. this applies to 2. only :-)
The formula h.Exp(v) does not generalize exponentiation, notice that h `**` v = exp(v \* log h), as I teach my precalculus students. As far as I can tell there is no reasonable notion of raising an element of a Lie group to a power given by an element of the corresponding Lie algebra.
Couldn't that be reduced to imageOf = fmap (runDB . get) . entryImage . entityVal ? edit. no it can't what was I thinking...
Static linking - on the plus side you don't need to try to figure out how to get all ~50 library dependencies installed on the target system when you move one to a new machine. Haskell tends to have very fine-grained libraries.
&gt; http://www.reddit.com/r/haskell/comments/1i5coe/catching_all_exceptions_school_of_haskell/cb1qhcs What is the reason behind that, by the way? Why not allow exceptions to be thrown only inside to IO monad or ExceptionMonad?
Right, good call. the only thing that would generalize is the exponential map g -&gt; G. Not exponentiation of an arbitrary element.
Yes, the library itself generates the code. Consider the following code (copied from my above response): theScript :: ProcScript theScript = animationScript 0.03 theSetup drawFunction theSetup = execProcM $ do size screenWidth screenHeight fill $ Color 255 255 255 255 drawFunction t = do background $ Color 0 0 0 255 circle (x0 + r * sin t , y0 + r * cos t) 20 where r = 100 x0 = intToFloat screenWidth / 2 y0 = intToFloat screenHeight / 2 Value *theScript* has type *ProcScript* (Processing Script). This is an abstract datatype for processing.js code. Using a rendering function you can get the actual code in *Text* type, or write it directly into a file using *renderFile*. For example: main :: IO () main = renderFile "foo.pde" theScript The function *renderFile* would be exported by the library. Does it make sense to you?
&gt; With darcs I have a very nice interface to select hunks when I record a patch You can use "git add --patch" to add things to the staging area, and then "git commit" those. I like this better than darcs' interactive commit interface, because I can break up this process into multiple "git add -p" calls before I am ready to commit. "git commit --patch" is very much like darcs interactive commit. &gt; I haven't got my head around what HEAD, origin and master are, and what switching branches does and which branch a commit goes to A git repo has pointers to other repos called "remotes". "origin" is the default name for the remote you `git clone`d from. You can use "git remote --verbose" to show these pointers. They're also listed in your .git/config file. A git repo has a DAG of commits, and a set of pointers into the DAG. These pointers are called "branches". The pointer is the commit-hash. "master" is one of those pointers (sometimes the only one). You can use "git branch --verbose" to show these pointers. Each "remote" also tracks the set of pointers (as last seen when synchronizing from that repo), you can see those tracked pointers via: "git branch --remote --verbose" (or `git branch -rv`). HEAD is basically a double-pointer: it's the "active branch", so it points to a branch (e.g: "master") which then points to a commit. "git checkout &lt;name of branch&gt;" will set HEAD to a different branch, which will also update the working tree state to match that of branch. "git checkout &lt;some commit&gt;" works in a weird way. It makes a branch called "(nobranch)" and sets it to point at &lt;some commit&gt;. This is called "detached head state", and this branch goes away as soon as you select a different branch. Don't confuse the above command with "git checkout &lt;some files&gt;" which doesn't touch HEAD at all. 
Hmm, all those unused definitions also probably contribute to the insanely large memory needed by ld when linking my yesod app (over a gigabyte).
Thanks. I'm getting the impression that it's best if I learn exactly how git's internals work and then I'll be in a better position to understand how to actually use it.
Aaaaaaaand bought. The PDF eBook is very high quality, good job Simon!
one of the reasons there isn't much in the way of background job libraries in Haskell is because it is trivial to run a long-running process in a new thread. Also, there are a million existing queue systems most of which should not be difficult to integrate with Haskell, including Resque: you would probably be suprised how easy it is to write some code that uses Redis as a queue.
The things I described are externals, not internals :-)
I wrote several background batch processes and queues using either yesod or clojure web framework. It is very simple, no special library is needed. You can also trivially schedule the jobs using crond, wget and rest web interfaces. 
The *hub.darcs.net* is a fork of *darcsden.com*. http://hub.darcs.net is more up-to-date, reliable and actively maintained than http://darcsden.com . So, http://hub.darcs.net/simon/darcsden is the one to go with.
I know about the benefits, heck I'm even a big supporter of static linking (I'm considering in getting involved contributing to Plan9). But to be acceptable the size of a static library must be as small as possible, so that the total binary size footprint (and everything what follows from that) does not put pressure on the whole system. Static libraries can't be served from shared pages without extra effort. Linux now has the ability to merge pages with redundant contents to save system memory but only with significant computational costs. Especially on mobile devices, where CPU cycles equal to battery usage this is a no-go. Having fine grained libraries, large binaries are even more frowned upon. I think to find better acceptance Haskell development should (also) focus on reducing the memory footprint. On systems with a SSD and lots of RAM large binaries translate into "being expensive", because right now SSDs still cost way more than HDDs. On systems with HDDs they become an annoyance due to seek time latencies and limited transfer bandwidths. But in mobile applications it becomes unacceptable. Most mobile devices (smartphones, tablets) have less than 50GiB of available system storage in total. And people want to use the bulk of that for their data, not programs.
Okey, thanks!
Huh? I think you've got the relationships switched around. As a library maintainer, you have no ability to break people's builds in Leiningen. Once a project builds it stays able to be built forever. With cabal+ghc-pkg, you have no idea whether something will build or not; it all depends on a very complicated set of what should be unrelated factors.
I think I'm only going to understand how they work when I understand the internals.
Yes, fontconfig aliases cover a wide variety of characters, but nonetheless in the default configurations of various distributions this particular character is not available.
That's right, thanks for the clarifications. http://hub.darcs.net/simon/darcsden is the active darcsden repo (trunk) now, running at hub.darcs.net and used for darcsden 1.1 with Alex's blessing. (It might move to the /darcs user if that seems appropriate at some point.) http://hub.darcs.net/simon/darcsden-1.0 was the repo I used to do the 1.0 release with the old UI style, which Alex preferred for that release. http://darcsden.com/alex/darcsden is the original repo, used for releases up to 0.5 and still running at darcsden.com as of today. I'm trying to make this clear in CHANGES.md, repo descriptions, cabal descriptions etc. Let me know of any ways you can see to reduce confusion. 
Thanks for posting. Some more details from the [blog post](http://joyful.com/blog/2013-07-21-darcsden-1.1-darcs-hub-news.html): &gt; This packages up what we have been using at hub.darcs.net so that you can run it locally. It’s the first darcsden release installable from hackage, and the first with the UI updates from darcs hub. For now, it still requires CouchDB and Redis to run. &gt; More importantly, this is about communicating the changes and current status of darcs hub, and doing a bit of marketing. Darcs hub hacking is fun, come and help!
Yes, the current situation is suboptimal. You can mitigate this somewhat if your GHC supports `split-objs` today, without going all the way to dynamic linking, but the RTS itself is still pretty huge. There was a rather Herculean effort by Duncan Coutts to get us the shared-library support we have today, but it isn't working on every platform, and (probably rightfully) isn't the default. JHC for instance will take the same executable that GHC produces an 8 meg executable for, and generally give back a 20-100k executable because it does whole program optimization and can optimize away the bulk of its RTS, but it provides very little in terms of runtime system support, garbage collection, etc. especially compared to GHC, the Rolls-Royce of RTSs
Free excerpt / preview: http://hgpu.org/?p=10101
Exactly. You have no ability to ship a backwards compatible security fix to users.
Sure you do, it's just that your idea of users is wrong. The users of a library are the application developers, not the application end users. End users do not get security fixes either way until somebody rebuilds the application for them. Developers, on the other hand, can get the security fixes whenever they want by updating their dependencies. The only way for end users to get security fixes is for them to build the application themselves or use a system where libraries are linked dynamically. If a user is savvy enough to build the application themselves, they are savvy enough to make sure the dependencies resolve; Haskell offers no advantage in this area!
In many settings it's just a regular value (e.g. the Extended Reals). I use 1/0 all the time since there is no "infinity" in the base library. λ&gt; let infinity = 1/0 λ&gt; infinity Infinity 
[Liquid Haskell](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/01/01/refinement-types-101.lhs/) to the rescue!
Wow... thanks for the coupon!
Let's work on making exceptions an anti-pattern first ;) Sometimes strings are all you need.
I must have started designing a Resque-like library for Haskell a good four times, only to once again realize that it is so trivial to do that sort of thing in Haskell that there's really no point in enforcing a bunch of conventions as part of a newly designed framework. It goes as follows: - Define a message data type to signal different kinds of work (specific to your app anyway) - Create a function that pulls from a redis queue and dispatches based on the message constructor - Add per-process parallelism through forkIO-ed instances of the above function; don't forget to add catch-all error recovery at the top level so your process doesn't just die. Also make sure you capture errors in a log before/after. - Create a stand-alone executable in your cabal file just to call this dispatch function. and you're done. Use one of the several redis monitor apps/libraries/frameworks to keep an eye on your queue sizes. I guess I can look into open-sourcing some of the plumbing work above, but it would be a very minimal library in any case.
Perhaps -split-objs ought to work like -ffunction-sections instead, allowing the use of --gc-sections to remove unused sections at link time while at the same not blowing up the number of object files or requiring separate assembly to produce separate object files. In my experience (for C++ code), using function sections doesn't affect link or build times significantly. If that holds for linking Haskell binaries, section splitting and gc-sections should even be a good default.
i enjoyed the puns. good read.