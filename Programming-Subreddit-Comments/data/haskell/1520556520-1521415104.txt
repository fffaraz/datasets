It takes time for the ecosystem to catch up to a new major GHC release, so switching `nightly` prematurely is kind of pointless since a huge number of packages would need to be disabled. I expect that once most of the ecosystem has caught up (especially anything with a lot of dependencies), `nightly` will switch.
&gt; Allowing cabal packages with multiple external libraries would save me almost as much work Pardon the tangent, but what's all this about? Care to elaborate?
+1, would have been better to wait for the release announcement email and submit a mail archive link instead.
GHC 8.2.1 was released on July 22nd, 2017. The first nightly snapshot on ghc-8.2.1 was [nightly-2017-07-31](https://www.stackage.org/nightly-2017-07-31). So it took about a week. The timing should be similar for this release, too.
Thanks to /u/borsboom's quick update to stack's online metadata files, you can now install this via stack as follows, with no special stack.yaml needed: stack setup --compiler ghc-8.4.1 
This was true of the alphas and the RC, so I presume also true of the release.
I just ask people I know. Chat applications are usually good enough for this. Sometimes get a PR to fix a type which I think is really cool, but that only works if the blog itself is open source.
I want to comment "This is all so obvious!" part from the perspective of a novice haskeller. After Java programming, the first thing I missed was interfaces with a dynamic methods dispatch. Quickly I realized that instead of learning new techniques I try to project my Java experience into Haskell. So at first, I put conscious effort to stop thinking in terms of dynamic interfaces and to think more in "static" terms *(by static I mean that thing are known at compile time)*. That means I consciously tried to avoid that Handle pattern *(which I knew from programming in c)*. It's interesting to know that Handle technique is not considered harmful and has its own legitimate applications.
[Sixel](https://en.wikipedia.org/wiki/Sixel) [could](https://saitoha.github.io/libsixel/).
**Sixel** Sixel, short for "six pixels", is a bitmap graphics format supported by terminals and printers from DEC. It consists of a pattern six pixels high and one wide, resulting in 64 possible patterns. Each possible pattern is assigned an ASCII character, making the sixels easy to transmit on 7-bit serial links. Sixel was first introduced as a way of sending bitmap graphics to DEC dot matrix printers like the LA50. After being put into "sixel mode" the following data was interpreted to directly control six of the pins in the nine-pin print head. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Once you start using backpack you wind up needing to make a metric boatload of tiny packages in order to organize your code. If you want users to build on top of them they all have to be public packages. This means that, say, if I ever get around to shipping http://github.com/ekmett/coda to hackage there will be dozens of little packages that few users will care about, but in order to expose the 4-5 public parts that other people are supposed to instantiate manually through backpack, all of those packages must exist. Consider https://github.com/ekmett/coda/blob/cee8ddea878048f5827bf73cc3e095d56060e0ad/lib/coda-set/coda-set.cabal which provides a `containers`-like `Set` that monomorphized to some particular user type and unpacked. To use it the user needs to provide one package that has the data type they want to unpack into it, then they have to instantiate this package inside another package. So usage of this library requires 2 packages on the behalf of users. I supply a bunch of code for group-relative data structures that looks kinda like the `Set` type above, but more elaborate, giving me relocatable structures in the sense of my talk on Monoidal Parsing. That's 3 layers (before you factor in other stuff.) On top of that type, I build my parser stack: The user provides a [Token](https://github.com/ekmett/coda/blob/master/lib/coda-dyck/Token.hsig) type that uses the relative positioning monoid. Then they use my [Dyck](https://github.com/ekmett/coda/tree/cee8ddea878048f5827bf73cc3e095d56060e0ad/lib/coda-dyck) language package to get an efficient monoid for mashing together lines of code that are made up of those tokens. Then they supply a custom with [https://github.com/ekmett/coda/blob/cee8ddea878048f5827bf73cc3e095d56060e0ad/lib/coda-lexer/coda-lexer.cabal](Lexer) and some notion of [layout](https://github.com/ekmett/coda/blob/cee8ddea878048f5827bf73cc3e095d56060e0ad/lib/coda-summary-unit/Summary.hs) so they can build a [https://github.com/ekmett/coda/blob/cee8ddea878048f5827bf73cc3e095d56060e0ad/lib/coda-rope/coda-rope.cabal](Rope). The layout mechanism used by the ropes is also parameterized on half this stuff. Each one of those links goes to a different package. If I didn't want users to be able to build their own unpacked `Set`s or to be able to instantiate these libraries, then I could get away with a single package with a bunch of separate internal libraries. Between those libraries I could talk about `coda:set` or `coda:dyck` to refer to the internal-facing library. But the moment I expose it I have to have 40 different versions, all in separate packages, all of which will take up namespace on hackage. If I refactor this code at all, its going to fragment into 40 completely different libraries, so I'll have to dump a storm of deprecation notices and administrivia on hackage to move things around. Alternately, if I could have one package with half a dozen public-facing libraries, third-party users could reference `coda:set` as an externally visible library from the `coda` package, I'd upload one package to hackage, just like everything else I do, and for the most part the plumbing for backpack would become an internal administrative detail. Until the issue is fixed or I'm far enough along to lock everything in stone for all time, I've been holding off on pushing anything like this t hackage, despite how useful many of the pieces could be on their own.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ekmett/coda/.../**Token.hsig** (master → cee8dde)](https://github.com/ekmett/coda/blob/cee8ddea878048f5827bf73cc3e095d56060e0ad/lib/coda-dyck/Token.hsig) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvepnpb.)
I'll go ahead and post this here too. stack setup --compiler ghc-8.4.1 
It's probably a bad way to phrase it. I've been working on some things that combine `Applicative`/`Divisible` and `Alternative`/`Decidable`. That has involved using it for parser/printer combinations and the get/put bits out of `Binary`, and so I've been thinking about it in terms of keeping the serialize / deserialize bits in agreement, or, in my head: "in sync". You can also use these piece to write a composable type for shrinking functions - it is an invariant functor that is `Divisible on the input` and `Applicative` on the output. You can also pair these things together in various ways, so that you can build your basic `Gen` / `shrink` values and then combine them easily and without breaking things. I still have to get my old examples into the new repository, and tidy up some usability stuff. Part of the reason I got back to working on this stuff was so that I could use it to play around with `sv` :)
Note that `IntMap` or `Map` based implementations will make your counting sort O(n lg n) instead of O(n). You really want O(n) construction of the histogram, which means using `Data.Vector.Mutable` or similar. But at that point your constant factors and code readability are likely to be better in Rust or whatever.
Yay, update your CI script :P !
So... that's a no? XD
Maybe it has something to do with following how LLVM releases with compatibility with GCC?
That's a strong yes...
I'd love to see a quality logging library that uses this abstraction - I keep rolling my own of various quality. Do any exist on Hackage (or GitHub)?
Thanks for your hard work making intero! I do use flycheck besides haskell, but don't mind having two similar packages. Being able to jump other buffer is a very usefull feature to give up. 
Not sure that is really correct. Sure Wadler is part of the team but his title "Senior Research Fellow" should clue you in on the fact that "leads development" is not quite right. Pretty sure he has been employed because of his considerable expertise in particular areas like programming language semantics which are being put to work in specifying the smart contract language for Cardano. If anyone "leads development" I would say it would be Duncan Coutts, International Director of Engineering who is almost as well know in the Haskell community as Phil Wadler.
`IntMap` will be `O(n)`. Check the docs, it's a Patricia tree not a binary tree.
I've been experimenting with something similar by having combining a lens with an IORef to get mutable, magnify-able [objects](https://github.com/louispan/glazier/blob/9964035b70680915614299b8237be9410a9467a4/src/Glazier/Core/Obj.hs#L16) I use it on in a widget called ["Prototype"](https://github.com/louispan/glazier-react-widget/blob/0c04a37133c75042d6eef557603c29cc019faed3/src/Glazier/React/Framework/Core/Prototype.hs#L36) which is a record of (st -&gt; display, st -&gt; ContT initializer) functions. Example usage: Original input widget defined (here)[https://github.com/louispan/glazier-react-widget/blob/0c04a37133c75042d6eef557603c29cc019faed3/src/Glazier/React/Widget/Input.hs#L51]. magnifyPrototype defined (here)[https://github.com/louispan/glazier-react-widget/blob/0c04a37133c75042d6eef557603c29cc019faed3/src/Glazier/React/Framework/Core/Prototype.hs#L75] example of "overridding" an existing widget (here)[https://github.com/louispan/glazier-react-examples/blob/35b4e0b4bcd38907446f6d739aee9f89866a2c60/examples/todo/hs/app/Todo/Todo.hs#L45] example of "combining" different widgets together [here](https://github.com/louispan/glazier-react-examples/blob/35b4e0b4bcd38907446f6d739aee9f89866a2c60/examples/todo/hs/app/Todo/Todo.hs#L151) The above is still very much experimental.
Actually a more accessible example is probably [this](https://github.com/louispan/glazier-react-widget/blob/0c04a37133c75042d6eef557603c29cc019faed3/src/Glazier/React/Widget/Input.hs#L174) which uses the mutable IORef and lens to modify the state of a checkbox.
this feels more like a terminal rather than ghci issue, are your env variables for your term set correctly?
THIS IS AWESOME. Gonna play around with this this weekend!
Yes, we guarantee properties of the sort function, up to the correctness of the implementation of `OrdL` class. It's possible to give the wrong implementation, and `sort` function will not work correctly there. I think it's quire fair assumption, because sort implementation depends on the comparison declared by user. Method is small enough to be in a trusted base; and writing a type signature that will prevent user from making a mistake will be a complex task. So this approach looks like in a balance with effort and benefits it can give. I think with dependent types can solve that problem but it's an open task. 
This is very nice work! But I am surprised you did not use diagrams library.. there was perhaps one more related library, can't recall..
Just a minor feedback, on a JS disabled browser the code got rendered like this... mergeList [email protected](a:l) [email protected](b:r) Otherwise nice post! interesting to see linear types used for this!
Wow, that's great !!!
Yes, 1 is `add --patch` but if you use `commit --patch` then does 1 *and* skips the staging area.
It's a bit tricky, at first, to develop an intuition on this sort of things. The following are linear functions: ``` move :: Bool -&gt;. Unrestricted Bool move True = (True, True) move False = (False, False) move :: (a -&gt;. Unrestricted a) -&gt;. [a] -&gt;. Unrestricted [a] move m [] = [] move m (a:as) | (Unrestricted a', Unrestricted as') &lt;- (m a, move as) = Unrestricted (a':as') ``` It's worth considering how this fits the definition of linearity. It's hard, indeed, to conceive of concrete instances of `OrdL` which are not also `Movable` (but we can still make some using abstract types). Nothing prevents you to write the incorrect `compareL` function. So, indeed, the safety of the sorting function is predicated on all `OrdL` instance being correct (in that they must verify a handful of laws, as usual. Though we left them unspecified in this post). This is not unique to linearity, however, `sort` from `base` also assumes that all `Ord` instances are correct.
Yes, pretty much. Though this is fraught with complications in actuality. I've got a prototype of leveraging linear types for manual memory management: https://github.com/tweag/linear-base/blob/f5927e1fb03d9af04db7319e4436ed5a4fcad991/src/Foreign/Marshal/Pure.hs
Can someone tell me when this usually ends up in a stack lts release?
Can it be used via `nix` already?
From [here](https://stackoverflow.com/a/21114359) &gt; This is due to different buffering behaviour in GHCi and GHC. GHCi has stdin (standard input stream) using NoBuffering by default, and GHC-compiled binaries / runhaskell use LineBuffering by default. If you explicitly do &gt; hSetBuffering stdin NoBuffering &gt; in your Haskell program, you'll be able to reproduce the GHCi behaviour. If you do &gt; hSetBuffering stdin LineBuffering &gt; in GHCi, you'll have lots of unwanted side effects, but can reproduce the runhaskell behaviour. You might also want to look at [Haskeline](https://hackage.haskell.org/package/haskeline).
Why not create a monad that is run by providing a handle, rather than parameterize all functions over Handle?
Yes, surprisingly to me git is amazingly simple if you don't try to learn it through its UI!
Lineal Types?
Ben answered this [on ghc-devs](http://ghc-devs.haskell.narkive.com/GWweovov/announce-ghc-8-4-1-released#post4).
I'd expect that the nightly snapshots switch to 8.4.1 in 1-3 weeks. At the same a new LTS series should start, using 8.2.2. Then it should take 3-4 months until another LTS series is started, probably using 8.4.2 if that has been released by then. At least that's roughly how it usually played out with previous releases.
Have you considered to start hacking in the GHC compiler? There is a huge bugtracker, the newcomer badge and a vast helpful community.
The compiler is in `nixpkgs` since a while in alpha stage: $ nix-shell -p 'haskell.packages.ghc841.ghcWithPackages()' ... [nix-shell:~]$ ghc --version The Glorious Glasgow Haskell Compilation System, version 8.4.0.20180224 You may need to way a bit until the official version appears in nixpkgs, it was commited yesterday: https://github.com/NixOS/nixpkgs/commit/6be7d949149e5bbd0bce1858c5d9a5ce1aee1177#diff-cdd97179a52f643bcdf1c9ea14c7833b
Thank you @donkeybonks. I will check it out and let you know.
OP obviously wants to work on a project that will provide personal mentoring. From his post I'm assuming he will prefer a smaller project where people can review his code and point out tips to generally improve his skills. This will not cut it
Mentoring would be invaluable, I guess. At the same time, for GHC I believe, there are number of people who usually help with newcomer-tickets quite a lot. I'm judging on my personal experience (cf. #14391 :)).
I've never considered, but basically my approach is similar as /u/mcfrankline 's pointed out. i am not looking forward any mention or recognition, just do something for real to improve and learn. If you recommend me GHC compiler i will give it a look though.
There's an Indian company that drops a post here every now and then, and recently wrote some kinda tutorial to help bootstrap people into Haskell development. I don't know much about the stuff they're building but they have an open Haskell internship and also a bounty program that might offer some form of personal mentoring. Since money isn't part of the motivation, you can check it out. https://vacationlabs.recruiterbox.com/jobs/fk064ow?cjb_hash=O_QqSs95 PS: Sorry about this, but I think the word you were looking for in your post was renumeration, not retribution. :)
I certainly do. My motivation currently is very close to yours. Except, probably, from the fact that I have academic interests also. And I picked up GHC.
With linear types it is possible to do mutation with a pure functional interface.
`diagrams` is awesome! I should have mentioned that for this type of work, something lower level is much easier for me. Most of the time, all I need for my work are lines, points and other polygons, and I need full control over the representations of those things for certain pieces. I used diagrams for a different project to generate tiles for a board game (shout out to [Trash Kings](http://www.kovach.me/posts/2017-01-22-trash-kings.html)) and really enjoyed it, however.
If you want O(1), just build it in IO and copy the algorithm over.
Very interesting what you are mention about academic approach which is something i am also interested in. How can i start with this? Or maybe being more specific, What is the best way to approach in this group and the first steps to take? Tips? Best
Yeah stackage, damn autocorrect on my phone &lt;.&lt;... That is neat! I take it from the comments in the file, that simply placing it next to the .cabal file would be enough for cabal to pick it up and use it for its solver?
[removed]
Wonderful!
Have you tired using rasterifc (native haskell) instead of Cairo?
I think LRU is basically a key-value expiring store; I don't think you can do searching in O(1) in general. I'd guess this might be more something like linked list where you hold a reference to the object, I don't think you can easily do that in haskell.
I have poked around with it, but not in a while!
I'm fascinated by generative art (although I don't consider myself an 'artist'). If I were to start doing this as a hobby, would you recommend that I use `diagrams` or something low-level?
A pure LRU cache can be done in `O(log n)`, but some implementation may use a really high base for the log (16 for [unordered-containers](https://hackage.haskell.org/package/unordered-containers-0.2.9.0/docs/Data-HashMap-Lazy.html)) which makes them competitive with `O(1)` implementations.
This is really interesting, it certainly changes what I understood at first for linearity. Ways of shooting yourself in the foot are always present (like changing order of arguments) so it's not surprising for this to be the case here. On the other hand, any 'misuse' of Unrestricted is localized, so the benefit of linearity is still present.
If you didn't drop you would get the same substring over and over and over. So for example if you grouped [1..10] by 3 you want: [1,2,3], [4,5,6], [7,8,9], [10]. You don't want [1,2,3], [1,2,3], [1,2,3].... So you need to truncate the leading x elements from the front of y to recurse. `group x (drop x y))` is group x (string with first x characters removed). 
Perfect, thank you so much!
Low-level works for me. The most popular generative art framework by far is Processing, which I would consider to be reasonably low level too. I can't speak for everyone, though, and I think "whatever works for you" is a good answer to this question. Try `diagrams` with `diagrams-reflex` if that interests you! If you find yourself wanting to go a bit lower, go a bit lower. That workflow does sound pretty cool!
For Purescript.
Thanks for reading :) Glad you enjoyed it.
[removed]
Regarding localised misuse: in a sense, it's what parametricity gives you. You limit your own power in the type (in that case using `OrdL a` rather than, *e.g.* `(Ord a, Movable a)`, and because of that you get extra guarantees.
&gt; (These are written for rather a broad range. Rest assured that if you have Haskell experience, you match.) 
Probably not anywhere good enough to match, but I assume you'd have to actually be in the UK for these ? Ireland wouldn't work ?
I am also enjoying this book. It's easier to digest if you have a maths background. The sudoku solver was an interesting chapter!
Great!! Let me check on that and let you know. Thanks,
As a QA engineer, I disagree. This has a great deal of overlap with things I do on a day to day basis, granted, at much higher levels of abstraction, but the trade-off between exhaustiveness and tractability is basically the decision set that defines my work. Knowing more about how this works as a formal discipline is incredibly valuable to my day-to-day, and sharing this knowledge and some of the high level concepts with my peers could be extremely useful if for no other reason than that it could provide a consistent nomenclature for discussion. The blockchain is not even remotely close to the only method people have, use, need, want, or could make use of to verify the functionality of software.
I quite like the full set of {trains, Haskell}! And I believe I also have what I'd call real world experience with Haskell. I'm rather tempted to apply. I live in the EU, so relocation is also possible. However I think I would very much enjoy if remote work would also be an option; if for nothing else then just for a probation period while we see how much we are a good match for each other, before I would pack my things. What do you think, /u/jobriath85?
Yes, although we have a number of offices (Manchester, Derby) and may be open to locating new hires there. 
Maybe in some future `base` we'll use [Bartosz Milewski's `Monoidal`]( https://bartoszmilewski.com/category/functional-programming) to unify `Applicative` and `Divisible` ``` class Monoidal f where (&gt;*&lt;) :: f a -&gt; f b -&gt; f (a, b) unit :: f () type Applicative f = (Functor f, Monoidal f) liftA2 :: Applicative f =&gt; (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c liftA2 g fa fb = fmap (uncurry g) (fa &gt;*&lt; fb) (&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b (&lt;*&gt;) = liftA2 id pure :: Applicative f =&gt; a -&gt; f a pure a = const a &lt;$&gt; unit type Divisible f = (Contravariant f, Monoidal f) divide :: Divisible f =&gt; (a -&gt; (b,c)) -&gt; f b -&gt; f c -&gt; f a divide g fb fc = contramap g (fb &gt;*&lt; fc) conquer :: Divisible f =&gt; f a conquer = const () `contramap` unit ``` 
Hi, and thanks for responding. As a company we have no history of remote working. However, as we've grown we've also become more flexible. Let me talk with HR, and I'll get back to you. If you have a reasonably up-to-date CV that you could send to me, would you be so kind? I can use it to strengthen the case in favour of remote working/remote probation.
I wonder if this kind of whitebox fuzzing could be implemented on the Haskell level as well. I know this way we would no longer test the correctness of the compiler, but I may be okay with that, especially if this could result in a reduced state-space to be searched, and potentially providing a finite and sufficiently small state space that can be exhaustively checked in reasonable time. E.g. given a function like this: fn :: Int -&gt; Int fn 13 = error "xxx" fn a = a The fuzzer, when asked to run on `fn`, could unwind the whole state and output relevant hspec/QuickCheck tests. E.g. evaluate (fn 13) `shouldThrow` errorCall "xxx" fn 0 `shouldBe` 0 Imagine how nice this unwinding could be if it is able to do this recursively taking into consideration all the lower level function calls as well. It's basically producing a specification in reverse from implementation. And then the programmer can go through this and see if everything looks in order or there does seem like a few cases that look odd and out of place.
Hi Ulrar, Enough people have asked this question that I've arranged a call with HR. My feeling is that collocation will be required, but I hope to change their minds. If you have a reasonably up-to-date CV that I can use to weight my argument, I'd appreciate if you would send me a copy.
Is the GHC community beginner-friendly? I imagine contributing to a project as complex and vast as GHC might feel a little daunting for beginners, but a good community makes everyone feel welcome. Would love to hear from someone who's fixed GHC bugs recently.
I know it's Leeds and not London, but 35-50k seems a bit low for a senior software engineer. 
Thanks! It's every bit as complex as you'd expect, and in ways you wouldn't (nobody agrees on how long a piece of track is, for instance). But we make our users' lives better, often replacing or augmenting pen-and-paper systems. It's rather rewarding.
I love learning about industries that I know nothing about. 
Thanks for the feedback---I'll pass it on. 
Thank you! I should be in touch on Monday, in that case. Cheers, Andrew
The [download page for GHC 8.4.1](https://www.haskell.org/ghc/download_ghc_8_4_1.html) mentions that `cabal-install` 2.2 or later is required. But AFAICT, it hasn't been released yet?
Also check out [colorco.de](https://colourco.de/) for a nice color scheme picker website. It’s the same principle as what you used, but with a bias given by the mouse, instead of a random number generator.
I don't think the type signature you give in the first makes sense? How is there a pair in the body, but a single bool result in the signature?
Amazing, thanks for sharing.
Neat, I’ll check it out!
FWIW, you might like to consider removing the requirement for current remuneration in the covering letter as well. There is literally no good reason a potential future employer needs to know that except to take advantage of the potential future employee if you don't pay fair market rates. Combined with a salary range that looks on the low side if you are hoping to hire senior people with significant functional programming experience, it doesn't exactly make a great first impression.
I'm not sure what you mean. If you mean `ReaderT Handle`, it doesn't help out that much. Taking some liberty with the naming: import Control.Monad.Reader data LoggerHandle = LoggerHandle write :: LoggerHandle -&gt; String -&gt; IO () write _ _ = return () data DatabaseHandle = DatabaseHandle {hLogger :: LoggerHandle} createUser :: DatabaseHandle -&gt; String -&gt; IO () createUser h name = do write (hLogger h) $ "Creating user: " ++ name -- VS: writeM :: String -&gt; ReaderT LoggerHandle IO () writeM str = do l &lt;- ask liftIO $ write l str createUserM :: String -&gt; ReaderT DatabaseHandle IO () createUserM name = do h &lt;- ask liftIO $ runReaderT (writeM name) (hLogger h) The second version requires much more wrapping/unwrapping using `liftIO` and `unReaderT` (or optionally `mapReaderT`) and it doesn't actually give you much in return -- unless I misunderstood your comment?
A second thing you might wonder is what happens if you have multiple ($)'s? Something like: derp $ foo $ blah ($) is also right-associative, so it evaluates things to the right first: derp $ foo $ blah = derp $ (foo $ (blah)) = derp (foo $ (blah)) = derp (foo (blah))
What are you trying to optimize? GHC startup or binaries that it produces? In any case, you may find this of interest: https://ghc.haskell.org/trac/ghc/wiki/DynamicByDefault
&gt; I take it from the comments in the file, that simply placing it next to the .cabal file would be enough for cabal to pick it up and use it for its solver? I would like to know this too, but I've mostly used stack so far. Do tell me in case you find out! :)
Take a look at &lt;https://hackage.haskell.org/package/psqueues&gt;. `minView` is `O(min(n,W))` where W = number of bits in a word, so asymptotically it is `O(n)`.
Trying to reduce cold startup time of GHC and also of the binaries it builds. Wondering if a never-dynamic variant helps or if 80% of the time is spent somewhere else than dynamically locating and linking libhs*.
The statistics on that page confirm that static-only binaries can be faster by 5% to 20% and that the reverse happens with the smaller dynamic-only binaries.
I think that on most OSes, the difference in startup time isn't going to be that significant but there's only one way to tell and that's to do some experiments and measure them.
I have a math background as well and Haskell is the closest you can get to mathematics compared to all other programming languages out there. I haven't digged deep in the monad part of Haskell I must admit. But for the pure functional part I think my math background has helped me a lot. I noticed I find it much easier compared to other people not having the math background. 
This would be valuable to me too. I appreciate all the work people have put into the github/github-webhook/servant-github-webhook packages but it still isn't quite to where making a github webapp is trivial or that web app will not result in all the (sometimes necessary) information that github provides.
I keep wanting to update my [mellow](https://github.com/TomMD/mellow) library to work from a web camera instead of a kinect. If you'd like to take that one it could be fun but with less benefit to the community.
`di` looks nice, however, I'm not sure why they force the three type variables `path`, `level`, and `msg` on users. I'd be happy with just one: `msg`. If I want to include a concept of log levels and/or paths to my log messages, I can just instantiate `msg` with a type that lets me do so.
 there's meaning, it's just a false statement. 
This looks like a great use for a [free monad](http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html).
I'm really only opposed to single letter qualified imports unless it is a super common module. The company I work at actually publishes the list of short abbrevations we allow in our codebase: https://github.com/frontrowed/guides/blob/master/haskell-style.md#abbreviating-qualifications (TLDR, shorten `Text`, `ByteString`, `NonEmpty`, and `Sequence` modules). To me it is akin to some python standards like `import numpy as np` and `import pandas as pd`. There is a balance to strike between readability and explicitness, and I heavily lean towards explicitness except for a few extremely common cases (the ones I mentioned above).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [frontrowed/guides/.../**haskell-style.md#abbreviating-qualifications** (master → 4e4ff6c)](https://github.com/frontrowed/guides/blob/4e4ff6cbf14c30776569399f4adf6d63b1f6c4b4/haskell-style.md#abbreviating-qualifications) ---- 
This is not considered to be good naming import practice, it's considered democracy&amp;Twitter: https://github.com/commercialhaskell/rio/issues/12
This is brilliant. John Hughes presented something similar called Functional Geometry at FunctionalConf 2016. Link: https://www.youtube.com/watch?v=XrNdvWqxBvA&amp;t=20m28s
The opposite factor here, in terms of readability, is signal to noise ratio. If my import qualifier is almost as long as my function name, I've just wasted half of the space clarifying what should have been dead simple based on context. Remember, short function declarations is also idiomatic - The type signature for what you're working with should always be pretty close at hand, and should give you a strong idea of what you're working with. For less fundamental or less frequently used types, longer qualifiers is perfectly acceptable and, I'd say, idiomatic, but if you're using stuff like `containers`, or `text`, probably you're fine with short qualifiers.
&gt;Would you be ready to show this to the members of other language community and be proud of it ? I'd like to introduce you to my good friend hungarian notation: https://en.wikipedia.org/wiki/Hungarian_notation
**Hungarian notation** Hungarian notation is an identifier naming convention in computer programming, in which the name of a variable or function indicates its intention or kind, and in some dialects its type. The original Hungarian Notation uses intention or kind in its naming convention and is sometimes called Apps Hungarian as it became popular in the Microsoft Apps division in the development of Word, Excel and other apps. As the Microsoft Windows division adopted the naming convention, they used the actual data type for naming, and this convention became widely spread through the Windows API; this is sometimes called Systems Hungarian notation. Hungarian notation was designed to be language-independent, and found its first major use with the BCPL programming language. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I don't think everyone would agree with you that reading comfort is more important than writing comfort, not if they're really being honest with you. Yes, in a perfect world they would agree with you. In conversation they will claim they agree with you. But in the real world, they have to get stuff done and get it done now. Someone else reading it in the future? That's a future problem, someone else's problem, and a problem that may not ever exist at all--maybe this code will get trashed. Since no one is reading the code right now and giving out bonus checks for making it readable, they are logically spending time where it currently counts: shipping something now.
Leeds is 35% cheaper than London in cost of living. What are you basing your "a bit low" number on? Are salaries for this position in Leeds usually higher?
If you’re a very good functional programmer it doesn’t matter how cheap the cost of living is 35-50k GBP is way way underselling yourself. So I guess if you already live in Leeds and can’t leave ok but I’m not sure who would relocate to Leeds of all places for something like that. Otherwise go somewhere you can actually get paid. Maybe Leeds is just a horrible market for software developers. I’m under the general impression that most places in the EU are.
In short, $ is an operator and ($ 5) applies the right hand side. It turns into a function taking just the left hand side. Since it’s in brackets it has the lowest precedence, everything after the $ becomes the left hand side.
This is a good opportunity to use equational reasoning. ($ 3) $ quot 34 = [expand operator section] (\f -&gt; f $ 3) $ quot 34 = [rewrite infix to prefix] (\f -&gt; ($) f 3) $ quot 34 = [rewrite infix to prefix] ($) (\f -&gt; ($) f 3) (quot 34) = [definition of `($)`] (\f -&gt; ($) f 3) (quot 34) = [function application (beta reduction)] ($) (quot 34) 3 = [definition of `($)`] (quot 34) 3 = [remove redundant parentheses] quot 34 3 You can also get a better understanding through experimentation. &gt; :type ($ 3) ($ 3) :: Num a =&gt; (a -&gt; b) -&gt; b So `($ 3)` is a function that takes a function which takes a number, and returns the result of calling that function on 3. Where is this useful? Well, for one example: if you have a bunch of *values* and want to apply a particular *function* to them, you’d use `map` (or `fmap`/`&lt;$&gt;`): &gt; map (* 2) [1, 2, 3] [2,4,6] This lets you do it the other way around, when you have a bunch of *functions* you want to apply to a particular *value*: &gt; :type map ($ 3) map ($ 3) :: Num a =&gt; [a -&gt; b] -&gt; [b] &gt; map ($ 3) [(+ 1), (* 2), (`div` 3)] [4,6,1] 
There isn’t universal agreement about this. I prefer to do `import qualified Data.Map as Map`, taking the last component of the module name as the abbreviation. The more important thing is being consistent throughout a project (e.g., always abbreviating `Set` as `S`, `Text` as `T`, `ByteString.Lazy` as `BL`, &amp;c.). For modules that don’t *need* to be imported qualified, e.g., `Data.Char`, having *some* qualifier gives you an explicit indication of where a name comes from *at the call site*, which you can just search for in your editor or with `grep` when reading code that uses unfamiliar libraries.
It's quite beginner friendly, actually. I did my HSOC project working on the testsuite last summer and it was a blast. Plus, while the compiler has it's ugly, nasty, and complicated parts, there's loads of code that's very simple to look at and plenty of small papercuts that can be fixed but there's always a bigger fire. Those are excellent for beginners to start on :) (I need to pester some people about maybe taking some time and making sure the bug tracker is tagged with "beginner friendly" issues sometime...)
`($3)` can be interpreted as a suspended computation. E.g. `map ($3) [(+5), (*42)]` applies to every function in the list. This is a good entry point for [continuation passing style](https://en.m.wikibooks.org/wiki/Haskell/Continuation_passing_style).
I think this is something awful about Haskell (or rather the community). This along with all the infix operators makes code so much harder to read.
how can i study - the making of elm? how wlm was made with haskell can i study this ?
Reading definitively is more important than writing when building larger systems. For toy software and software written to be read by 1, it's not important. I think this is where the (suboptimal) naming conventions in Haskell come from. There simply aren't enough large-scale haskell systems to make proper naming the dominant culture.
If we take `text` as an example, I would argue that `T.pack` is only more readable than `Text.pack` because it has been seem more often. If you imagine a reader that encounters both for the first time and would know almost instinctively that `T` stands for `Text`, my guess is that he would still prefer the second more explicit form. This is debatable and I agree with /jdreaver to have a general rule of not using single letter import except for some well-defined exception. One larger problem is the fact that this kind of habit is pervasive. The number of blog posts I am reading where I need minutes to understand code extracts just because of all the abbreviation in used. It is kind of unbelievable. What I mean is that "noise ratio" can be a very convenient excuse to make your code less readable. And let's face it I have met too many coders that actually enjoy to obfuscate their code.
Or, it's because Haskell refactors very safely, so people refactor more, so writing becomes relatively more important. 
Some info here: https://stackoverflow.com/questions/32114648/why-is-functional-programming-good-for-dsl-is-it By the way, ML is a specific language, not a type of language. Haskell was influenced by ML, but it's not "an ML".
ASTs are modeled excellently by sum types, combined with pattern matching. Compilers are transformation pipelines of one type of AST to another. The static types let you have exhaustiveness checks and track the current stage of compilation in the types. Each stage of he compiler is not a trivial step: lexing, parsing, renaming, inferring, desugaring, optimizing, code generation or interpretation, ... can each be complex so it's good to separate them, and for that to be successful you need clear boundaries between the stages. Haskell lets you create those boundaries. 
ADTs and Pattern Matching. module Expr where data Expr = Val Int | Add Expr Expr | Mul Expr Expr | Sub Expr Expr | Div Expr Expr deriving Show eval :: Expr -&gt; Either String Int eval expr = case expr of Val v -&gt; pure v Add e1 e2 -&gt; (+) &lt;$&gt; eval e1 &lt;*&gt; eval e2 Mul e1 e2 -&gt; (*) &lt;$&gt; eval e1 &lt;*&gt; eval e2 Sub e1 e2 -&gt; (-) &lt;$&gt; eval e1 &lt;*&gt; eval e2 Div e1 e2 -&gt; do r1 &lt;- eval e1 r2 &lt;- eval e2 if r2 == 0 then Left $ "Denominator is Zero in expression: " ++ show expr else pure (r1 `div` r2) 
Stm is great for composability, but it's important to keep in mind that it's a retry model, not a fair model. 
Sure but the goal is (from https://github.com/commercialhaskell/rio/issues/52): &gt; We're explicitly calling out a goal of standardizing current best practices in rio, not establishing new ones, if at all possible. 
Try passing different numbers to the `-N` argument. Without a number it defaults to one os thread per core which might be a poor choice for a variety of reasons. Profiling is a good start, especially with ThreadScope. 
 λ&gt; eval (Mul (Add (Val 1) (Val 2)) (Div (Val 8) (Val 2))) Right 12 λ&gt; eval (Mul (Add (Val 1) (Val 2)) (Div (Val 8) (Val 0))) Left "Denominator is Zero in expression: Div (Val 8) (Val 0)" usually you'd parse the input to the ADT or write a few builder function to make it easier to write expression in the DSL.
Ok! I get know! This code assumes, that I have parsed the text and ADT available to be evaluated. How tuff is parsing in Haskell compared to imperative programming language ?
I am interested in knowing how tough is parsing in Haskell. I have my hands on Javascript and used yacc like library writing the grammar rules. Just want to how do I get started in Haskell to write a sample parser that can eval using the method written by @glimi, in the comment below. 
The code checks out if `Unrestricted` is defined as type Unrestricted a = (a, a)
ML is one particular functional language, and ancestor of one particular sub-family of functional languages. When people refer to ML itself, they usually mean "SML" (Standard ML). Others include OCaml and F#. Haskell is not an ML - it certainly has meta-programming features, but it's a different branch of the functional programming family tree. Functional languages have a number of characteristic features which make programming in general easier, getting a lot of the "plumbing" out of the way. My quick bullet list would be... - Pattern matching - Higher order functions and lambda - Static type checking with parametric polymorphism and type inference - Typeclasses (to a first approximation, interfaces) - Laziness Although some features have fairly obvious benefits on their own, you can't really understand the benefits without understanding how they all combine, in often non-trivial and surprising ways. The usual generalization is that you focus on what you want, not how to implement it. Obviously this isn't exactly 100% true, but certainly in Haskell, you do end up worrying much less about low-level details of evaluation order and in-memory representation, especially for intermediate results. The compiler *must* do a lot to make this style of programming perform well, and it certainly does that (up to a certain point), so there's a greater tendency to let the compiler get on with it, and only solve performance issues when they arise. In my experience for many small projects that never happens, and when it does, it's mostly forcing evaluation and taking care with at least some data structure issues at a few key points. "Data structure issues" could in theory mean anything up to C-like coding, but that kind of thing is mostly already wrapped in libraries - using containers other than lists and occasionally doing some explicit unboxing is more typical. Anyway, the features that make functional languages suited for developing other languages are really the features that make functional languages suited for programming complex projects in general. Writing interpreters and compilers is just an example of the kind of project that gets annoyingly cluttered and awkward in most languages, but much less so in functional languages. Of course everything should be as simple as it can be, but no simpler - there's no getting away from the fact that a modern optimizing compiler back-end is complex.But simple yet easily extensible interpreters and translators can be quite shockingly simple in functional languages if you've messed around writing interpreters in a language like C or C++. One common recommended paper is [Why Functional Programming Matters (John Hughes)](https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf). The linked paper is a 1990 revision of a 1984 paper, and uses a language called Miranda. IIRC in the early 90s, a bunch of pure functional programming language developers decided to join forces - Haskell was the result. At that point, Miranda was one of the better-known functional languages. The earliest versions of Haskell appear to have imitated Mirandas feature set fairly closely, though I don't know exacly how closely. Modern Haskell has a more sophisticated story to tell (as, I imagine, do modern ML-family languages), but Hughes paper is still the first few lessons. 
We never write more than once per minute (and often even less than that). This is a read-heavy application. It is very very light on writes. Also note that the high CPU load is happening at idle, when nobody is using the system. It is literally just sitting there, burning 15-20% of CPU resources for no apparent reason.
Not that hard tbh. You could go the parser generator route and use Happy and Alex (parser and lexer, respectively), or use parser combinators like MegaParsec.
Parsing in Haskell is a wonderful experience. Perhaps try out parsec or megaparsec. 
This is just the reason I wanted! Thanks a lot for the proof! I m trying my hands on writing a simple expression evaluator using Haskell.I ll try implementing a simple language and post it here soon!
Parsing is near-trivial in Haskell (and most declarative-paradigm languages). The starting point for parsing is the type `String -&gt; [(r, String)]` - a parser is a function that takes a string and yields a list of possibilities, each consisting of the parsing result and the unparsed tail string. Due to laziness, the list isn't built all-at-once - it effectively expresses backtracking. Starting there, it's pretty easy to write general-purpose parsing functions that accept other parsers as arguments. It turns out that there's a `Monad` instance essentially for this, except it doesn't care if you're parsing strings or not - `StateT String [] r` wraps a function of type `String -&gt; [(r, String)]`, but you could of course replace `String` with some other "state", replace `[]` with `Maybe` if you don't require backtracking etc. And that means the most obvious basic parsers (actually grammar-combining operators, since the exact parsing behavior depends on instance types - or actually general control-flow, since they also work for things that aren't really parsers) already exist in the Haskell library. There's plenty of reasons why you might want to use one of the more specialized parsing libraries (the name I remember off the top of my head is parsec, though IIRC that's an older option) and there's also lex-and-yacc like tools (alex and happy) included in the Haskell Platform. Parsing makes a great example, though, for understanding monads, monad transformers and applicatives. For that, look on the [University of Oregon Programming Languages Summer School 2012](https://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html) page for the "Monads and all that" videos (John Hughes). 
- This should make your idle CPU 0%: We (at Futurice) turn the idle GC off with `-I0`, you might want to find the interval which suits you. (0.3 sec is too short - your HTTP server ends up GC-ing all the time) http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/runtime_control.html#rts-flag--I %E2%9F%A8seconds%E2%9F%A9 - Read through https://ghc.haskell.org/trac/ghc/ticket/9221 - Larger nursery helps, e.g. `-A32M` - `-qg` (disable parallel GC) is another knob to try. We have quite small heaps, so it's kind of hard to measure.
As /u/phadej said, idle CPU usage is probably the GC. If you run with profiling it'll tell you what percentage of time is being spent on the GC. I think getting profiling output will be pretty critical to understanding how to fix this.
Cost of living matters, location matters, demand matters, remote-vs-relocation matters, commute matters, etc. Just focusing on a number as if there is some absolute applicable to everyone in the world is discarding all nuance. 
Limiting gc to a few cores with `-qn&lt;cores&gt;` can help too. Depending on the heap layout parallel gc can scale badly with more cores to the point where using more cores causes worse performance. In these cases limiting GC to a few cores can speed things up.
It's possible, but I find it unlikely. From my experience there's a strong correlation between bad code hygiene and "team size" - i.e. number of people you consciously think will read the code.
There's also a friendly community over at /r/ProgrammingLanguages/ who also like to discuss language design and implementation!
I didn't even realize that that was supposed to be the ADT. I thought it was just a Lisp dialect...
Sounds very much like a GC issue, rather than sync/locking overhead. How large is your working set? If tuning GC options doesn't significantly improve matters, consider moving your data to Redis or something similar.
Scheme isn't - it draws more from the Lisp family. Haskell has a bit of ML in it, but was more inspired by Miranda (another language inspired by ML). SML, OCaml, and to some extent F# all probably fit better under the "ML derivative" banner. I'd also put Scala and Rust there too, but they are more removed due to certain additions and removals of features. PL taxonomies are messy compared to biology due to lots of cross-pollination of ideas over the years. These days when folks say, 'ML', they probably mean either SML or maybe OCaml.
Hmm. Well when I made my comment, I hadn't seen your comment at all. My comment does have more context that yours. I suppose I was trying to communicate something similar to yourself. Why do you have a problem with that?
Parallel GC is an awesome way to burn a lot of cores and go slower. I would bet this is the issue. 
To make it O(1), we can use a doubly linked list (priority queue) and a hashmap. 
Lambda calculus looks a lot like a simple Lisp dialect, and is a major influence on functional programming languages. Where Lisp or Lambda calculus would use `(+ 1 2)`, Haskell *could* use `((+) 1 2)` or `(+) 1 2`, but more idiomatically uses infix syntax where appropriate. For ADTs, data constructors (which build values by accepting components as arguments) are "self-referencing functions" - `Val` is a function of type `Int -&gt; Expr`, but the value of `Val 1` (the function application) is `Val 1` (the data value), which can be matched by `Val x` (the pattern) or by `Val 1` (a more restrictive pattern). 
That haskell style is actually quite sane and sensible, and practical. I like it, and may end up borrowing if for my own Idris and Haskell projects.
+1 on the large nursery recommendation. That’s really important on a box with a lot of cores.
This is just a symptom of a deeper problem: I never understood why it's not possible to import multiple definitions for the same name and have type inference sort it all out on based on use a bit like overloading works in OO languages. Only if that is ambiguous the user w ould need to provide type annotations or more explicit, qualified names.
Oh, these are fun! How does this relate to https://github.com/schell/steeloverseer? 
I've worked on systems with monstrous names; they do not become more readable just because they require more reading. A car doesn't become easy to carry if you add a carrying handle.
If you want to know how I'd *read* it, I'd say "apply to 3 the function quot 34."
In case anyone is curios here is what the code might look like if you replace Cairo with the excellent Rasterific. I like Rasterific better since it's native haskell and I always have difficulty installing Cairo on my mac. [code](https://gist.github.com/jeffreyrosenbluth/c751f319a110ebb94d8e67107d9a0ff9)
As a maths student who can program in Python etc. and has just come back to learning Haskell for a second time, this was really cool to see. I've been wanting an example of a small project and this in combination with the ones in the chapter of LYAH have been really cool to see the functional paradigm in action. It's hard to shift away from that Python mindset of wanting to make a class straight away etc. but this helps me really see how one goes about making a program from scratch effectively with Haskell, thanks! 
Hey, the Hoogle has landed. It still needs tweaks here and there, but is already usable, have a look at it if you cannot wait :)
Reading (input) or writing (output) files.
In addition to ADTs being perfect models for syntax trees, lots of things in a compiler are tree traversals. Things like SYB can automate those traversals, so you don't spend tons of time writing boring recursive functions with a large number of cases.
you are awesome! Thanks a ton! Started learning megaparsec.
Wait, he didn't leave Edinburgh, did he?
I may have focused too much on IO. Yeah, I noticed that caching is not always viable 😀 But this is not what I am interested in.
I would recommend starting with the Newcomers page https://ghc.haskell.org/trac/ghc/wiki/Newcomers When you got basic information, start searching a bug you like, probably the one bearing newcomer label. After finding something, ask for help in the ticket thread or ghc-devs mailing list. As for research. This bug fixing activity doesn't relate to research directly. But might be a good addition, as for me.
Category theory is not needed. Haskell — yes, you should be comfortable with it. What needs to be added is the general understanding of how GHC works. For this use the following sources (in this order): - a chapter on GHC in AOSA book and/or SPJ's talks on GHC at YouTube, - GHC wiki, - source code of the compiler.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [jaspervdj/fugacious/.../**Logger.hs** (master → 4e9c2d4)](https://github.com/jaspervdj/fugacious/blob/4e9c2d48174c852616fbfbf28bd9cc90812a1c95/lib/Fugacious/Logger.hs) ---- 
After having been into Haskell lately I needed to use python for a project. Turned out to be worden in an extremely functional style :-) 
Fwiw, typeclass methods do that, which has inference problems. Also, Idris has "type directed name resolution". 
Can even be instructive to actually try implementing parser combinators yourself - they're not actually that challenging.
STM doesn't prevent you from expressing unreasonable bottlenecks. For instance, it is convenient to have a large pure data structure in a single `TVar`, but this will be a bottleneck for write access when you have lots of cores. Data structures with internal transactional nodes (see [stm-containers](http://hackage.haskell.org/package/stm-containers) should do better in that setting. If you do not need atomic operations involving multiple data structures or operations and you don't need `retry` you can use something like [ctrie](https://hackage.haskell.org/package/ctrie). Similarly, many more threads than cores is fine, as long as the threads are usually blocked and are not woken in mass. This requires careful design.
The IO and ST monads basically pin the order of operations. There are several escape hatches for IO. - Normal IO executes the list in the order it's written in - unsafeForkIO may executes when the resulting value is used. This may be at the location it's written, any time after that that or not at all - unsafePerformIO also may execute before the point it's written - unsafeDupablePerformIO also may start multiple times and abort midway through - unsafeInlineIO also may memoize or merge multiple IO operations. This aliases your buffers and probably crashes your program So if you need performance more than these guarantees this might be useful. Anything after performUnsafeIO is likely to go horribly wrong, though.
Tangentially related question: I'm currently taking a language processing course and have noticed haskell often looks similar to BNF CFGs. Is there a deeper connection here between CFGs and Haskell or is it just that they converge on a common solution?
I just went through all those slides plus the previous simple haskell one, including following along, in about 30 minutes (I'm beginner-intermediate). Great review, very succinct. 
ML cannot be "under the class of Meta Language", because ML *is* Meta Language. https://en.wikipedia.org/wiki/ML_(programming_language)
This is one of my favorite Python talks of all time. Lots of great advice in it
Small nit- any way to get the next/previous buttons on the bottom? I'm on mobile, and it'd be easier to navigate with one hand if they were.
I see this is on github, actually, maybe I'll just open a PR.
yeah Python is quite easy to write in a more functional style than other OOP languages (looking at you java..) 
Looks rad! I will check it out
Sorry, I don't know the answer to your first question. The result of eval is `Either String Int` and not `Int` because I needed to handle the failure in the case of division by zero. I'm using applicative style to avoid explicitly handling the case in all other places and just let it short circuit.
ST alone does not give you randomness, you would need to also pass in a seed. That would give `quickSort` type `(Ord a, RandomGen g) =&gt; [a] -&gt; g -&gt; ([a], g)` or similar. And as long as `a` has a proper `Ord` instance, the seed passed in does not matter, so it's arguably morally ok to use `unsafePerformIO` to obtain one. 
You do not need mutability for caching. You can use a knot tied data structure + laziness. Take a look at the memoize package on hackage, it does not use unsafePerformIO at all and still has a pure interface. 
Ups, thanks.
Try and fail is a way to success. #56 is just 2 days old and I guess it is an attempt, for now :) Lots of things in Haskell could be made better, and I think many Haskellers know it. For example should arguments for [functions in Data.Map](http://hackage.haskell.org/package/containers-0.5.11.0/docs/Data-Map-Lazy.html#v:lookup) be swapped. Haskell2010 and Prelude could be made much cleaner. Logically cleanness comes often from an evolution over time.
I didn't say anything about `ST` and randomness though? 
Great!!! I was thinking about this and i am going to dedicate part of my spare time to GHC and another project. I will be checking this link and ask for help if it is needed. Thank you very much /u/ulysses4ever for the recommendation, tips and support. Best,
I'm sorry about this kind of experience you got. This may be the case for certain libraries, indeed. But community should be more tolerable and newcomer-friendly. At the same time, GHC itself has a lot of challenges not connected with the theory whatsoever.
idk, but vinyl and generics-sop provide inductive tuples. The functionality requires a constraint, but shouldnt require a new typeclass. Like https://hackage.haskell.org/package/vinyl-0.7.0/docs/Data-Vinyl-Class-Method.html
 &gt;Although, this very thread shows a bunch of good counterexamples to what you mentioned. Absolutely, I agree. I was also looking at the ticket you are currently working on (#14391); I must confess I didn't understand all of it but glad to see that senior members of the community (SPJ himself!) are always happy to lend a hand. Keep at it and good luck! Hopefully you'll see me contributing to GHC sometime soon. 
Sounds great. There are a bunch of project ideas that are not yet tracked in the bug tracker. Just let us know when you are ready!
I meant the Reddit thread we are in :) but that one too, yes!
No suggestions I'm afraid but would love a full write up of rolling out Haskell at your company including the solution to this issue. Good luck!
Shouldn't something like `eval $ Add (Val 1) $ div (Val 4) (Val 0)` be equivalent to `(+) &lt;$&gt; (Val 1) &lt;*&gt; (Left "Denom...")` and fail? Is there something in the Either applicative implementation that allows this? Something like it's defined so that `fmap Left a = Left a`? If I step through it, I see that it never reaches the above point. Does the entire recursion collapse when the Left is reached? Does pure v get inferred to Either String Int? Sorry for basic questions, I'm in the middle of learning applicative functors if you can't tell.
&gt; &gt; Random-Pivot Quicksort. I also don't think this is controversial. &gt; You don't need IO for either of those; That's what ST is for &gt; Randomness can be done purely perfectly fine whenever a PRNG is acceptable.
[Yup](https://hackage.haskell.org/package/base-4.10.1.0/docs/src/Data.Either.html#line-130).
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://hackage.haskell.org/package/base-4.10.1.0/docs/src/Data.Either.html#line-130) - Previous text "Yup" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
Type classes require ahead of time coordination between different modules. Ad-hoc overloading is a powerful tool that allows a more lightweight approach.
Thanks! I didn't talk about how this would be set up on a production project. The way we've done it on quite a large web application with many external services is like so: 1. The development environment automatically runs all the services and databases required by the project in docker containers. One of these containers is a toxiproxy-server. (An image can be found on docker hub [here](https://hub.docker.com/r/shopify/toxiproxy/).) 2. We automatically create a Toxiproxy proxy for every external service when the development or test environment is started. For example one for each SQL database and one for each redis we use. 3. The web application is configured such that all external calls go through Toxiproxy proxies by default in development and test. Even though for most tests, toxics are not enabled.
I believe it works well on some programs and badly on others. My experience has been purely negative - I think it should be off by default. 
This post walks through building an entire JSON parser from the ground up, starting by building the parser combinators: https://arunraghavan.net/2018/02/applicative-functors-for-fun-and-parsing/
&gt; That's a future problem […] maybe this code will get trashed. I have generally found that the that’s-a-future-problem attitude does guarantee the code will get trashed.
They don't need `unsafePerformIO`. They only appear to because you're trying unnecessarily trying to hide the RNG parameter.
I can't really do it that way since I have to work with existing API.
We had some issues with gc at scale at channable. Most of our tweaks came from the following twitter threads: https://twitter.com/ProgrammerDude/status/884072548954247168?s=19 Hope it helps
I liked https://inner-haven.net/posts/2017-05-08-speed-up-haskell-programs-weird-trick.html for an explanation for `+RTS -qg`.
Are you ok with requiring that `a` is `Typeable`? This needs work, but something down this line of thinking might be fruitful: colonize :: Typeable a =&gt; a -&gt; Cols a colonize a = case testEquality (typeRep a) tuple of Nothing =&gt; literal a Just Refl =&gt; case a of (b :*: c) -&gt; literal b :*: colonize c where tuple :: Proxy (b :*: c) ; tuple = Proxy
`randomQuickSort :: Ord a =&gt; [a] -&gt; [a]` does need `unsafePerformIO`, since by definition not hiding the RNG parameter changes the type signature. I intentionally worded it that way. Users of randomized quick sort probably don't really want to have to pass in a random generator, since it doesn't affect the result (for proper `Ord` instances).
With a ^sufficiently ^^advanced ^^^compiler ^^^^TM But yes, if you want to evict cache entries that haven't been hit recently (e.g. LRU), I think you probably need some sort of stateful (and thus probably monadic) interface. I mean `State (Map k v)` would work so you could still have purity.
This looks interesting, where is this `testEquality` coming from? Can't find it in the `Typeable` module.
How you describe it is actually how it works. Proxies and toxics all exist on the single process toxiproxy-server.
[Hayoo is your friend](http://hayoo.fh-wedel.de/?query=testEquality)
Ahh.. yes. The old Hayoo I always forget about :D
I think [HIE](https://github.com/haskell/haskell-ide-engine) could always use some help, and is something the whole community could benefit from! The development is active, and there's the #haskell-ide-engine Freenode channel with activity. It's also something, were the work you put in, would be something you can benefit from immediately yourself, since the aim is to provide the nicest IDE experience.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://github.com/haskell/haskell-ide-engine) - Previous text "HIE" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
Do you have any insight as to what makes it so bad most of the time?
If you have a need for filesystem watching, I would also recommend checking out watchman: https://facebook.github.io/watchman/ Some bullet points from the about page: * Watchman can recursively watch one or more directory trees (we call them roots). * Watchman does not follow symlinks. It knows they exist, but they show up the same as any other file in its reporting. * Watchman waits for a root to settle down before it will start to trigger notifications or command execution. * Watchman is conservative, preferring to err on the side of caution; it considers files to be freshly changed when you start to watch them or when it is unsure. * You can query a root for file changes since you last checked, or the current state of the tree * You can subscribe to file changes that occur in a root
Nope - this is an experimental result.
You can read the elm source code.... Or do you mean something else? The question is rather vague.
Great. It sounded like you had server and proxy as two processes, then inject rules with a toxic process (maybe short-running). After that, use your regular network client.
The website hasn't updated in a while, but there's been a modest amount of activity in the git repositories it links to (right from the front page). I doubt it's dead, but there may be a lull in major developments.
£50K is pretty good for the north. I left Manchester about 4 years ago for Australia. At the time £45k seemed the average for seniors unless you went contracting or happened to know someone. Heard rates have increased a little with the likes of Rentalcars coming in causing more competition pulling devs from the BBC etc who where big employers. Good to hear there's Haskell happening in the area. One of the reasons I left the UK was I just got in to Scala and there wasn't any Scala/FP jobs in the north so took a sabbatical and ended up staying in Sydney. Not masses of FP work in Sydney but the weather makes up for it. 
I think it's a case of that's just he market rates for the area. Like you said if you can't move you have little choice job wise as the market isn't great. If you can move go contracting in London or go overseas. It is disappointing though when you see juniors in the US taking home significantly more money than heavily experienced seniors in the UK. It's not even a supply/demand issue as the talent pool was small in the north we saw the same CV's coming through with the same companies listed with people moving between them. When a good candidate did come through despite how desperate they where for staff the upper limit £45k was generally non negotiable.
This covers a good chunk of what you need to know about Haskell to write programs in it and be productive... the "base of the pyramid" if you know what that is. I've been working on an entry for https://7drl.org in Haskell this week and I've used every feature in these slides and records. I use very simple data structures, pattern matching, guards, and where clauses... that's about it. It's amazing how even those simple tools can help you write your program. I even use the same patterns: user input is typed as a command and pattern matched on, etc. Great work.
I think a lot of code that uses lists optimizes away the actual list part.
This doesn't look like it will get deforested. The mechanism for deforestation is short-cut fusion and it happens via rewrite rules over pure code -- the construction here is effectful.
It looks to me like your analysis is correct. You'd be more effective if you created an mvector in IO, wrote to it as part of the monadic loop, and then called `unsafeFreeze` at the end.
First off, how are you representing your board in Haskell to begin with? 
The board is represented as: Array (Int, Int) (Maybe XO) Where XO is represented as: data XO = X | O deriving (Eq, Show)
The full algorithm maybe hard to translate properly because there is mutation of `list`. At a very high level Applicatives are good for this, List applicative can create a double loop: ` Prelude Control.Applicative&gt; (,) &lt;$&gt; [1,2,3] &lt;*&gt; [1,2] ` ` [(1,1),(1,2),(2,1),(2,2),(3,1),(3,2)] ` So in theory, if board would have a list of accessors, and then you multiply the accessor list by the field name list, to get a combination of all possible accessor and field name pairs. From the pairs you can just map the list with `($)` (apply), which would apply the first part of the pair, the accessor to the second, the field name. Presumably the accessor returns a `Maybe`, because the value of field name could be `Nothing`. That means once you apply the accessor, you get a `Maybe a`, so you filter using this, and then you get a list of reduced values that are valid. This algorithm would output the same as the operational semantics one, but it would depend on the `board` implementation.
&gt; Users of randomized quick sort probably don't really want to have to pass in a random generator, since it doesn't affect the result Isn't the point of randomized quick sort to be robust against malicious input? If the seed is fixed the user can craft a sequence of actions which triggers the quadratic behavior.
The seed would not be fixed, it would be obtained uniquely each time using `unsafePeformIO`.
You can do this with maximum polymorphism like so: import Data.Key (foldMapWithKey) import Data.Maybe (maybe) nestedLoop:: (FoldableWithKey f, FoldableWithKey t) =&gt; f (t (Maybe a)) -&gt; (Key f, Key t, Char) nestedLoop = foldMapWithKey (\i -&gt; foldMapWithKey (\j -&gt; maybe [] const (i, j, 'X'))) 
This behaviour is unaffected by restarting the server. 
In case you chose a representation of nested lists (or nested functors): `fmap (fmap f)` which is the same as `\xss -&gt; fmap (\xs -&gt; fmap (\x -&gt; f x) xs) xss`. With each nested `fmap` you can peek a layer deeper into the structure of the list (or functor). If you want to add indices you might want to use `zip [0..]` on each list first.
So you want access to both the keys and the values, and then to look at only a subset of them. You can get the list of key value pairs by calling `assocs` (https://hackage.haskell.org/package/array-0.5.2.0/docs/Data-Array.html#v:assocs). After that, `filter` the result, and then `map` over the result to replace the `Nothing` values by `X`. So you get something like `list = map (\(i,v) -&gt; (i,X)) . filter (isNothing . snd) . assocs $ myArray`
I had an experience recently where I wrote a program I was planning to parallelize later. In compiled with similar defaults to OP (and this is a program with lots of garbage) and was completely shocked to see every core on the 32 core server apparently engaged in *something.* That's when I realized: parallel GC. It struck me then that it's perhaps an unexpected default. My program's performance (and productivity, which had been single digits) improved when I greatly increased the nursery size and limited it to 4 or 8 cores. 
When you find a compiler bug, the best thing you can do is reduce your code to a minimal example which reproduces the bug (preferably without external dependencies), and post that to the issue tracker.
You're running out of memory. It seems that the parser is very inefficient, if it can't parse a 25MB file with reasonable usage of RAM.
suppose i wish to make a language like elm from haskell, then how do i go about understand what to do? how do i study - how was elm made. where to look here - https://github.com/elm-lang :) is there any guidance available to understand how elm can be made from haskell? 
I actually prefer them on the top - both on desktop and mobile. that way my finger doesn't have to move when i want to change slides. if it were at the bottom it would be depended on the slide's height.
Hello Haskellers! Is it possible, using HSpec, to read a file into a unit test? I want to test parsing of some markup and it would be nice to keep in in file. I have tried using `runIO` but have not had any luck. I have have probably not grasped the `SpecM a r` properly so if someone could explain that I would be super happy! Cheers!
I think you can translate this quite literally (if I am not missing something obvious here...). import Control.Monad import Data.Array data XO = X | O deriving Eq type Board = Array (Int, Int) (Maybe XO) fun :: Board -&gt; [(Int, Int, XO)] fun board = do i &lt;- [0..2] j &lt;- [0..2] guard (board!(i,j) == Nothing) return (i,j, X) Naturally, you can refactor this, for instance into a list comprehension: fun board = [ (i, j, X) | i &lt;- [0..2], j &lt;- [0..2], board!(i,j) == Nothing ] And remove the hard-coded array bounds etc.
Shamelss plug - you can generate your own color schemes internally with [palette](http://hackage.haskell.org/package/palette-0.1.0.5/docs/Data-Colour-Palette-Harmony.html)
Awesome!
People who think writing comfort is more important have brought us many horrible problems in many programming languages just to save a character here or there, mostly in the form of optional syntax (no {} around single statement in C, no parentheses for 0 parameter function calls in a few languages,...), implicit conversions and similar nonsense that has literally no upside other than saving some characters and causes major bugs all over the place.
I didn't know about Steel Overseer, but it looks from a quick reading like its focus is on performing tasks in response to file events. `trackit`, on the other hand, focuses only on getting a live view of the result of a command. I don't know to what extent Steel Overseer handles that use case.
minor issue: after putstr you don't need pure ()
Thanks for the suggestion! It looks like Watchman is more about setting up triggers for tasks than about viewing results live, not so? See my reply about Steel Overseer.
It seems a bit boring to just show a static screenshot, since that will look identical to just running a command separately (e.g. `git log`). So I'm looking into making GIF video that shows how the view is updated live.
I forgot to mention the detail that I had already tried compiling with +RTS -M1G, with no effect on the above stated behaviour. I apologise for leaving that out. decodeLeases indeed did not have a base case. I added "ALB.Done BLI.Empty r -&gt; Right (r : xs)", and now it terminates successfully without problems. 
This is almost certainly not related to your problem, but it might help you restructure the solution in such a way that it eliminates your problem. The return type of `loadDycpIndex` looks suspicious. If `(Mac -&gt; Maybe IPv4)` is a query interface, why not separate it from the parser (and separate the pure and effectual code)? For example: loadDhcpIndex :: Setting -&gt; IO DhcpIndex lookupIPv4 :: DhcpIndex -&gt; Mac -&gt; Maybe IPv4 If `(Mac -&gt; Maybe IPv4)` is intended to be a short-circuit; i.e. only return the first IPv4 in the file and avoid reading the entire file. This is the type of thing in a lazy language like Haskell that will compose better if it's done on the outside.
Cool. Even better. :)
In the second instance, add this pragma: instance {-# OVERLAPPABLE #-} ... For more information, see [the GHC user guide on overlapping instances](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#overlapping-instances)
Great timing! I was looking for something like this to test connection recovery code when communicating to a rabbitmq server.
What happens if the stdout spans more than the height of the terminal?
 for :: a -&gt; (a -&gt; Bool) -&gt; (a -&gt; a) -&gt; (a -&gt; b -&gt; b) -&gt; b -&gt; b for start condition increment body = if condition start then for (increment start) condition increment body . body start else id yourthing = ($[]) $ for 0 (&lt;3) (+1) $ \i -&gt; for 0 (&lt;3) (+1) $ \j -&gt; ([i,j, 'X'] :)
+1 for `V.unfoldrM`. You could also check if you have fixed-size elements and then use `V.replicateM` so only one vector needs to be allocated.
I've tried that. Problem is that I get this error: * No instance for (Bounded (Text :*: Text)) arising from a use of `colonize' * In the expression: colonize Proxy In the expression: colonize Proxy $ toRel $ User def Admin "reygoch" "password" In an equation for `test': test = colonize Proxy $ toRel $ User def Admin "reygoch" "password" | 37 | test = colonize Proxy $ toRel $ User def Admin "reygoch" "password" | ^^^^^^^^^^^^^^ This `toRel $ User def Admin "reygoch" "password"` should be evaluated to `Int :*: UserType :*: Text :*: Text` but it seems like GHC fails at finding the right instance at the end and is trying to apply `literal` to the `Text :*: Text` instead of decomposing that as well and applying `literal` to each of those elements.
The first instance has too many constraints that are unlikely to hold. Actually all the `Typeable`, `Enum`, `Bounded`, `Read`, `Show` instances here look wrong. `Colonize (Int :*: UserType :*: Text :*: Text)`, after picking the first instance we need to resolve (among other things) `Colonize (UserType :*: Text :*: Text)`, and after picking the first instance again, we need `Bounded (Text :*: Text)` (`Bounded b` with `b ~ Text :*: Text`).
Well that answers my question about trying to build armhf with llvm 3.9. :) Any pointers to this new llvm backend perchance?
Ah.. yes, thanks for pointing that out. I was just following what compiler requested from me. It turns out all I had to do was the following: class Colonize s a where colonize :: Proxy s -&gt; a -&gt; Cols s a instance (Colonize s b, SqlType a) =&gt; Colonize s (a :*: b) where colonize p (a :*: b) = literal a :*: (colonize p b) instance {-# OVERLAPPABLE #-} ((Cols s a ~ Col s a), SqlType a) =&gt; Colonize s a where colonize _ a = literal a test :: Cols s (Relation User) test = colonize Proxy $ toRel $ User def Admin "reygoch" "password"
Then you can scroll using the arrow keys. And it keeps the scrolled view when output is updated.
Thanks for Brick, BTW. Such a nice library!
See https://ghc.haskell.org/trac/ghc/ticket/12470. Moritz is currently hard at work on cross-compilation tasks, but perhaps he will be able to find time for the LLVM work in the coming months.
stee overseer is about executing a pipeline of commands after file changes, but it does so in a normal terminal context. A brick ui is something that we (Mitchell Rosen and I) have discussed, and it would be cool, but the project is stable now and we’re both busy.
See https://stackoverflow.com/questions/49224474/how-can-i-use-stackages-cabal-config-files-with-nix-style-cabal. So far I've found a somewhat crude manual solution.
Glad to hear it! If you have any issues or questions with the library or Toxiproxy itself, please feel free to open an issue or email me.
Oh dude, this is super cool! I'll have to dig in when my schedule opens up. Excited to see where your project goes :)
I'm not a Knowledgeable People™ but I have something that might be of use to you some day: https://mitchellwrosen.github.io/haskell-papers/ It's very alpha-quality at the moment but could be made much more searchable, and have a lot more papers, etc.
The GHC bugtracker has [a "Research needed" milestone](https://ghc.haskell.org/trac/ghc/query?status=infoneeded&amp;status=merge&amp;status=new&amp;status=patch&amp;status=upstream&amp;milestone=Research+needed&amp;col=id&amp;col=summary&amp;col=priority&amp;col=component&amp;order=priority). 
&gt; Should you have one top-level module that re-exports everything else? I tried that recently, but I encountered an issue. My package is called "interplay", it provides a library named "interplay", and an executable named "interplay". So I tried to write a top-level module named "Interplay" which re-exports some of the "Interplay.Foo.Bar" submodules. That did not work because when building both the library and the executable, I end up with a folder named "Interplay" (uppercase I) and another one named "interplay" (lowercase i). I saw a warning telling me that this would cause problems depending on whether the filesystem is case-sensitive or not.
Thanks!
Just to chip in with my own $0.02, I found `wreq
&gt; Am I supposed to design my modules for qualified imports or not? A little of column A, a little of column B. I tend to follow the usage pattern of `Data.Map`, `Data.Set`, `Data.Text`, when building concrete container types and design those modules around qualified imports, in part because this fits our culture, but also because most of the operations for each of those containers is a one-off concern with little code-reuse, and manually mangling identifiers to make each of those operations uniquely named yields horrible looking code. On the other hand, when building the bulk of my code, I generally design assuming users will be using my code unqualified. I find that it produces code where invoking an identifier from a class feels much more like a small extension to the language that I'm thinking in. I'd be loathe to give up either one of these models. Simply be clear about what the intended usage pattern is for a given module. &gt; Do I export record fields or keep records abstract? I'm prone to export them. In general I find the problems caused by exposing too much information are surmountable, the ones caused by keeping your cards to close to your chest tend to provoke forks. &gt; Should you have one top-level module that re-exports everything else? If you are exporting containers that are designed for qualified use, you can't have this sort of thing. Otherwise, I tend to build a top level module when for administrative reasons I have to build a dozen smaller modules, but where the whole is intended to be used as one piece. &gt; Should lenses be embraced? I get a fair bit of use out of them. YMMV
You're welcome - I'm glad you're finding it useful. :)
To be clear, I'm not proposing any sort of committee with any political power. Rather, a canonical, community rendezvous between library authors and library users to collaborate on the external API of a package under development or proposal.
That's not the issue; my library files are in `src` and my executable files are in `app`, and yet when those are getting built, they somehow end up in the same place.
You should look into Cardano. They’re honestly doing more for Haskell than any one party has ever done for it short of creating Haskell itself. Phil Waddler is on IOHK’s team that is creating Plutus. Under the umbrella of that same project, the K Framework, Plutus (basically a lot like Haskell but written for smart contracts and blockchain), IELE (a VM written by runtime verification that K framework languages can be run on) , and Haskell Semantics for K are all areas that you should look at. 
I would trade 5 new features for 20% build time improvement.
I'll be there
I don't know, but broadly speaking: use Int, not Integer or the Integral constraint; and compile with -O2, and try the `-fllvm` ghc-option. (and why is `millerExps` partial?) btw, include the cabal file / command line options, and the two benchmarks. 
&gt; doing more for Haskell like what?
I don't think Haskell should need O2 to beat Python...
`-O` is `-O1` not `-O2`, which doesn't do much
This isn't in GHC, but it could hypothetically be used in the future to improve the effectiveness of `{-# REWRITE ... #-}` pragmas: [equality saturation](http://www.cs.cornell.edu/~ross/publications/eqsat/). I'm actually doing _my_ bachelor's thesis on it, and it's pretty interesting. It involves converting the control flow graph of imperative code to an algebraic / referentially transparent (semantics of node are only dependent on semantics of children) graph with sharing, which is called a PEG. This is then augmented with a set of equivalence classes for nodes, to form an EPEG. The actual equality saturation procedure can work on any referentially transparent language, and it involves rewriting the graph using axioms called equality analyses (these are simple optimizations like `x + 0 = x`). Equality saturation searches the entire space of all programs equivalent under the equivalence relation generated by the reflexive-transitive closure of all the equality analyses to the original program, and does so in an efficient way that preserves sharing (read up on the Rete algorithm if you want to know more about how that happens). Once the EPEG is saturated with equalities, we can then run a `SelectBest` function that does pseudo-Boolean integer programming to give the PEG subgraph that maximizes a chosen performance heuristic. Basically what this looks like is traversing the graph starting from the root node and choosing an element of each node equivalence class at each step in such a way that you maximize the objective function. In many real cases, equality saturation will not terminate, since it explores an exponential space, but this is okay, since we can think about equality saturation as a streaming search process that emits better and better versions of the original program over time (e.g.: by emitting the result of running `SelectBest` every `1/k` pattern matches). In this sense, equality saturation is a form of superoptimization. In my opinion, there are a _lot_ of places where equality saturation could fruitfully be applied if more people knew about it. Feel free to pick my brain on the subject if you want more details.
I didn't use Int since RSA implies at least 1024-long keys, which would make Int overflow if I'm not mistaken. It seems to be taking approximately the same time with -O2 :/ Unfortunately I don't have cabal file, should I? 
I found that when I parallelized a data parallel program of mine, the program slowed down. I profiled the execution with ThreadScope and was able to see that GC time was destroying performance. As it were, bumping the heap size was sufficient to solve my woes `-H2G`. Try out a ThreadScope profile and see where your cost centers are. -rtsopts -threaded -eventlog "-with-rtsopts=-N32 -lsg"
[removed]
I don't follow what the issue is. Would you mind posting some minimal example code, expected/actual behavior, and steps to reproduce?
You use `(**)` for powers which has type `Floating a =&gt; a -&gt; a -&gt; a`. So you have to convert the results from floats back to Integers. One optimization is to use `(^)` which has type `(Num a, Integral b) =&gt; a -&gt; b -&gt; a`, which will keep the results as an Integer. On lines 55, 94, 101.
Haskell Weekly have *Call for participation* section: https://haskellweekly.news/issues/97.html It's main appeal is varied nature. You may find a topic you are already familiar with, and then only Haskell is new ;)
Thanks a lot, will try this!
:( no I don't have any. (I'm just learning Haskell). But I have written Pascal in high school. So if you let Haskellers write Pascal, maybe you will be willing to let Pascaller write some Haskell? ;) (Commercially I have experience with PHP, but that's not on the "broad" list)
huh, guess I'm wrong. (I just repeated what I heard that about O1, but myself I I've only bencharmed O2 versus O0.)
Makes sense, I did think Int might be too small. 
Reflex has some support https://github.com/reflex-frp/reflex-frp.org/blob/master/doc/guide_to_event_management.rst#id1 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex-frp.org/.../**guide_to_event_management.rst#id1** (master → 113966f)](https://github.com/reflex-frp/reflex-frp.org/blob/113966f6c71d9c28b46410f0847b7edd2f00de01/doc/guide_to_event_management.rst#id1) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvkbtfo.)
This might elicit a "duh", but: have you actually profiled your program? GHC has quite some useful profiling features, so if you aren't familiar with them, look it up. A good first step would be to add cost centers (SCC pragmas), and then compile and run with profiling options to see where you spend the most time. Much better than taking stabs in the dark.
It's likely due to lack of experience. For example, here is a neater version of `millerExps+millerExpsWrapped` which is about 20% faster than the one on github: millerExps :: Int -&gt; [Int] millerExps = go [] where go acc n | n `mod` 2 == 0 = let n' = n `div` 2 in go (n':acc) n' | otherwise = case acc of; [] -&gt; [n]; _ -&gt; acc I won't be able to review the whole thing, but I wouldn't be surprised if there are few more things like this which add up to decrease performance.
&gt; a community resource dedicated to "bikeshedding" ... the design of both existing and unwritten libraries' APIs' Call me crazy, but I want this to be a thing! I can shed bikes, my api's suck, I can throw mud. Where do I sign up? 
Contrary to popular belief, using a generally faster language does not magically mean everything is faster. Slow algorithms are still going to be slow. In particular, both Haskell and Python are going to have similar complexities for large integer handling. In both languages, these are going to be highly optimized routines. In your program, most of the time is spent calculating numbers, and Haskell and Python are just serving as the glue. Thus, the time spent actually executing any of the code you've written is likely very small. In particular, `primeGen2` literally makes random numbers and tests for them to be prime. There is no getting around the fact that this is going to take a lot of time, and it has nothing to do with the language being used. It has to do with the fact that generating large random numbers until one is prime will always take time
You'll probably want to read up on how to write a compiler. https://www.cs.princeton.edu/~appel/modern/ml/ is a classic. Based on the author, I imagine https://cs.brown.edu/~sk/Publications/Books/ProgLangs/2007-04-26/ is good too. For more tutorial-style, Haskell-focused introductions, see https://wespiser.com/writings/wyas/home.html and http://dev.stephendiehl.com/fun/
`runhaskell`?
IMO, artificial consensus is much much worse than the productive chaos we have right now. I can totally understand where this is coming from, and I feel the pain - but I'd much rather deal with these relatively minor nuisances than give up the creative freedom and rapid evolution that we currently have. I have yet to get into a situation where I cannot use a library because its API isn't what it "should" be; I have to work around things here and there, but this is never really a show stopper. Making the above choices is hard, but API design is, well, design, and for most of these choices, there is no one-size-fits-all answer.
I think Cabal and Stack default to `-O` but GHC defaults to `-O0`. [Docs imply](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/using-optimisation.html#ghc-flag--O) that `-O` is `-O1`
The warning about case sensitivity is because in Windows filenames are case-insensitive, even when the filesystem supports cases. 
We've opted for Haskell mainly because of its high degree of control over IO and state and because we like the functional paradigm. 
It's extremely probable! I'm only beginning in Haskell. Thanks a lot for that snippet 
Thanks, I didn't think there could be a hard limit with integer handling itself. I have to admit I am clueless about bang pattern, do you have an advice for a particular ressource? 
Huh, never thought of using haskell and redmine together. Kinda curious, what are you up to?
As I understand the proposal the group should and would not have any formal authority and such would not prescribe of proscribe anything. Their output would be creative discussion and potentially suggestions and a proposal of *next practices*. 
&gt; Today, it feels like there are a few little strongly connected components of packages that seem to work well with each other (probably because they were all written by the same person) but don't interface particularly nicely with the rest of the ecosystem. Pertaining to this point, I think if we get the top haskell library authors in the world to come to a platform where they explain why they designed their own libraries in that particular way, several such entries would be a great starting point for a discussion on the trade-offs and rationale behind the design choices... and there could potentially be a hope for common priorities and patterns to emerge out of this discussion...?
&gt; GHC already has custom type errors There is a more powerful proposal By Alejandro Serrano going on.
oh, that's pretty cool! link?
We already have this.
https://www.reddit.com/r/haskell/comments/6ilt49/domainspecific_type_error_diagnosis_in_the/?st=j471a0es&amp;sh=7fcc8c79
this can probably make this slightly faster by using `quot` and `rem` instead of `div` and `mod`. If this only takes positive numbers.
You are right, on all accounts (in particular `m` shouldn't be a linear argument), I'm sorry: it ought to have been move :: Bool -&gt;. Unrestricted Bool move True = Unrestricted True move False = Unrestricted True move :: (a -&gt;. Unrestricted a) -&gt; [a] -&gt;. Unrestricted [a] move m [] = [] move m (a:as) | (Unrestricted a', Unrestricted as') &lt;- (m a, move m as) = Unrestricted (a':as')
When you do optimisations, be sure to seed the random generator with `setSeed (mkStdGen number)` to make different runs comparable. It still won't be comparable to the python code due to a different random number generator, as the runtime is much based on luck.
&gt; Using the right type for the job - there are so many different ways to eke out more type-safety from GHC, it's hard find a good balance of ergonomics and safety. The power-to-weight ratio of any given type system feature is also a function of the domain it's being applied to. This is a very interesting observation for library authors, especially if the library is to be built upon. Being one myself, I'd love to read more resources on the proper use of e.g. open type families, typeclasses, functional dependencies, and polymorphism in general. I mention all these features because they are often used in conjunction, and reading library code is very time consuming. Let's have more library design tutorials !
[removed]
Not only do we have this (as pointed out), it produces very different code than you'll actually run in production. This makes it useless if you actually care about having fast code while you develop for example if you have time or space benchmarks set up.
Honest
Exactly, these things will not be solved via committee, but by pure community combinatorics. I wonder what a good infrastructure for such a thing would be, either a portal for code review such as https://reviewable.io/ or something else.
Quick! If the key is already in the map, is the old or new value the first argument to the combining function? Do you remember? I have to look it up every single time, and the type signature doesn't help one bit.
Thanks for sharing! Thanks to Rasterific it is easy to integrate with haskus-system hence I have [added this](https://github.com/haskus/haskus-system-examples/commit/77d9696fef5690660cf24df5a931409e4f112060) to the demo: https://youtu.be/7NwGage6Qps 
There is [support for debouncing](https://github.com/reflex-frp/reflex/blob/develop/src/Reflex/Time.hs#L200) in `reflex`, and [servant-reflex](https://github.com/imalsogreg/servant-reflex) is very nice if you're talking to a servant API.
No no, he's still at Edinburgh. He's consulting one day a week for IOHK guiding the development of Plutus (the Haskell-like smart contract language for Cardano). Note: IOHK is currently hiring compiler engineers to work on Plutus (with Phil and me and others) or Haskell &amp; formal methods folks to work on Cardano more generally. * [Functional compiler engineer](https://iohk.io/careers/#op-235152-functional-compiler-engineer-) * [Haskell developer](https://iohk.io/careers/#op-136090-haskell-developer) * [Formal methods software developer](https://iohk.io/careers/#op-228725-formal-methods-software-developer)
Try this http://hackage.haskell.org/package/network-2.6.3.4/docs/Network-Socket.html#v:ntohl
Thank you for writing this, this is very clear!
That's just for a single word32. I want to take an entire bytestring and apply the conversion to the whole thing. I realize that it would be better if I could convert to host byte order at the point where the byte string is being read from the socket, but unfortunately, that is happening in a library over which i don't have much control. It seems like maybe I could use attoparsec for this, but I'm just surprised that there isn't an easy way to do this.
I had hand rolled this in the past many times, I don't know any libraries that provide this. I usually use a combination of `Data.ByteString.Lazy.spanAt` and the function I showed above.
Yeah I'm writing something like that right now. Just surprised nothing exists. Maybe I'll make a little library.
You definitely should profile first, but I have a guess. The default random-number generator in random is surprisingly slow. Luckily there are plenty of high-quality, performant drop-in replacements. See here, for example: https://stackoverflow.com/questions/26024405/fastest-way-to-generate-a-billion-random-doubles-in-haskell
Furthermore, why does `insertWith` implicitly pass the new value into the combining function itself? I can do that myself externally if that's what I really want.
As drb226 said, an example would really help, but … Are you using Cabal? Does your app depend on your library (i.e. is the library built from the Cabal file listed in the apps `build-depends`), or are you referencing the libraries source in the app’s executable section?: Executable app main-is : app/Main.hs hs-source-dirs : src 
We've fiddled a bit with the `parsedResultAction` plugin a bit in our MSc thesis, that at least was pretty straight-forward, so give it a shot! In the end it was too limiting so we need to work in GHC anyway, but still neat.
There is also no reason why there shouln'd be 2-3 pattern-bundles that adhere to philosophies. Names could be "Can't touch this", "Extend me like you do", ..
&gt; To calculate how many possible values the new type has, we count and sum all the possible values, therefore "sum type" Cute. :)
https://wiki.haskell.org/Performance/Strictness
&gt; As drb226 said, an example would really help, but … I did [post](https://www.reddit.com/r/haskell/comments/83qcak/haskell_needs_better_libraries_a_proposal/dvkmyvy/) an example, but you were probably writing your comment when I posted mine. &gt; Are you using Cabal? No, I'm using stack. When building with `cabal new-build`, the file is generated in `.../build/autogen/cabal_macros.h` instead of `.../build/interplay/autogen/cabal_macros.h`, so the `autogen` folder does not conflict with the `Interplay` module name. I did manage to get cabal to accidentally put its `cabal_macros.h` file in the same folder as the compiled `Types.o` file by naming my module `Autogen`, but for some reason that did not trigger the warning. &gt; Does your app depend on your library My original project did, but the minimized project I linked to does not, so this is not important to reproduce the problem. &gt; are you referencing the libraries source in the app’s executable section? No.
I suppose that is because in most cases you encounter structures with mixed integer widths and even string-like structures. I was about to answer that your question was not sufficiently specified, as it depends on what you are encoding! You can get what you (seem) to want with the `binary` package, and something like `many word32le` as the decoder.
&gt; At least 2 years experience of compiler design &amp; implementation &gt; At least 2 years of Haskell experience &gt; At least 3 years relevant academic and/or professional experience with design and/or implementation of smart scripting languages and DSLs, programming languages, or compilers. I really don't know where these people are from! If every haskell job out there asks for extreme experience, where do people get started at all? I'm fairly comfortable with the language and I'm generally good with programming, but requirements like this makes it almost impossible to apply.
Just one more thing please, I get that haskell is used to write a compiler that compiles elm code. But how is javascript generated? Can a compiler generate whichever language i want ?
Oh, neato! To me this smells like a stack bug, which you should definitely consider reporting. For comparison: `cabal build` seems to have an analogous bug, but `cabal new-build` gets it right (each library and executable gets its own `build` directory).
&gt; Do I export record fields or keep records abstract? If exported, how should they be named? I think this is nicely solved by the "internal modules pattern", which I'm sure I've seen blogged about recently. Expose an API you think is nice, but *also* expose all the implementation details through explicitly marked here-be-dragons internal modules.
The best thing to do is to subscribe to the issue tracker and read the kinds of issues which are posted. That way you get a good feel for what the current state of play is and where the potential solutions are. Then you need to find a part of the compiler you want to work on, start by fixing easy tickets and eventually work your way up to bigger projects. Your scope is too wide at the moment. https://mail.haskell.org/pipermail/ghc-tickets/
I'm partial to someName :: Ord k =&gt; (b -&gt; a) -&gt; (b -&gt; a -&gt; a) -&gt; k -&gt; b -&gt; Map k a -&gt; Map k a
I think it's a micro-optimization, to avoid creating a closure. But I don't believe in compromising the API for micro-optimizations.
May I ask why?
It makes a lot of sense!
I had no idea about that. Are they also cryptographically secure?
Thanks!
Well apart from that fact that is an elegant and in some sense "most-general" generalization of `insertWith`, I find very often that I want to build a map whose entries are again containers. Then based on the above, you can do a build :: Ord k =&gt; (b -&gt; a) -&gt; (b -&gt; a -&gt; a) -&gt; [(k,b)] -&gt; Map k a -&gt; Map k a and then one of the simplest application is to bin thing according to a key: f :: Ord k =&gt; [(k,a)] -&gt; Map k [a] f list = build (:[]) (:) list Map.empty 
You're looking there at the compiler job, not a regular Haskell developer job. &gt; I really don't know where these people are from! The people applying for this have typically done an undergrad degree in CS or maths and a masters or PhD covering language design and/or compilers. Some problems do require specialist knowledge and skills. This is not one of those cases where recruiters are asking for silly unrealistic things (and in this case I should know because I wrote the job spec). &gt; I'm fairly comfortable with the language and I'm generally good with programming, but requirements like this makes it almost impossible to apply. That's fine. Have a look at the Haskell developer job posting.
Do you think your `build` is a good `fromListWith` variant, and that `someName` is the natural restriction to a single element? Sounds like a reasonable point in the design space. But your `build` should ideally have a name indicating that it's a *left* fold.
Yes, I definitely think that `fromListWith` should have this signature in an ideal API, and I can probably argue that `insertWith` should have the signature of `someName` above, too. Left/right: I have no strong opinion about the naming, but this isn't really a fold in the classical sense? `buildRight f g = buildLeft f g . reverse` (not that there is no `flip` here). Traversing from the right is not really natural for Haskell lists (because the data structure itself is not symmetric). In any case `fromListWith ` is in the same situation.
'Upsert' comes from database world. It stands for 'insert if new... update if already exists..'.
Glad you like it :)!
I'm also not a Knowledgeable People™, but one thing I've been wondering about is formal verification of the System FC (the typed lambda calculus Haskell is based on), as well as verified implementations of other core components of the GHC. Coq could probably verify System FC, and either Coq or HOL could probably verify the Core language or the STG implementation details, but this would be something like a PhD thesis plus enough research papers to land a tenure track position...
Holy sweet bananas, how comes i've never ever heard of transient package? From the tutorial alone it looks like a godly powers!
It's a left fold in the sense that the first element of the list is inserted first. Yes, that's the only efficient way to do it, but it's not something you can guess *quickly* from the type signature, so I think it should be in the name. `fromListWith` is also just awful. Now let me throw in a little extra question. In the lazy modules, it seems to me that we should ideally offer `insertWith`, `adjust`, and `fromListWith` variants that offer control over evaluation. That is, the user-supplied function will produce values of a type like -- Note: not a newtype data Box a = Box a which GHC will hopefully be able to unbox to `(# a #)`. These functions would be useless but harmless in the strict modules. How might such be named?
I found the initial phase of learning haskell for me was about implementing simple algorithms and getting comfortable with recursion. So having never built a robot a couple ideas are: - implement several maze solving algorithms (then add wheels and sensors and see how they work?) - a robot that maps a space in some way, using space-filling curves and keeping track of items (colored dots on the floor?) it finds - implement a simple DSL that you can use to program a robot, i.e. a haskell program that reads a set of commands you provide and executes them. Initially these could just be `go forward for 10 seconds`, `turn 90 deg left`, etc. but you could extend it later with conditionals, loops, etc, maybe with the goal to be able to support a maze-solving algorithm (an opportunity to do some simple parsing and play with DSLs)
Basically using Haskell to drive its API, gluing together various parts of the JHU CS admissions pipeline. If you're curious, https://github.com/nwf/hs-redmine-automation has the code (which has been moved to using `req` rather than `wreq`).
I wrote a small [telegram-bot](https://github.com/FabianGeiselhart/telegram-countdown-bot). I'm relatively new to haskell and I'd appreciate any feedback.
`binary` and `cereal` have function such as `getWord{16,32,64}{be,le}` for getting words in various endians. I'm not sure what it means to convert an entire bytestring since byte orders are with regards to integrals of particular sizes.
No problem. The IO random generator is somewhat expensive (it makes a new generator each time you use it if I understood the sources correctly). You can replace the all instances of the IO monad with `Control.Monad.State.Strict` for a slight speed increase and more importantly, increased purity since you only use it for the random state. Then you create the generator only at the start using IO or a set seed and carry it with you to the now pure functions that need it, using `evalState` at the start of the chain. `genWrapper` for example becomes: genWrapper :: Integer -&gt; Integer -&gt; State StdGen [Integer] -- generates 100 random numbers genWrapper up down = replicateM 100 (do g &lt;- get let (a,g') = randomR (down, up) g put g' return a)
To be honest I haven't been thinking about strictness issues. In any case I don't think I understand what you want here. Are you suggesting for example insertWith' :: Ord k =&gt; (a -&gt; a -&gt; Box a) -&gt; k -&gt; a -&gt; Map a -&gt; Map a In what situation would that help? Isn't the lazy Map already lazy in the values? (on `fromListWith`: There are many instances when the functionality cannot be guessed from the name + type signature, but I think that's ok, that's what the documentation is for :) Example in case: the order of the arguments of `insertWith`. Though the above `someName` version fixes that).
/u/acow seems to be the best person to ask!
Yes, that's the general idea of the `Box` thing. It lets a user perform an arbitrary amount of computation before (potentially) installing a thunk. So I could pass a function \new old -&gt; let !blah = f old in Box (g new blah) This might be rare enough that we should just expect users to use `alter` instead, but `alter` is a bit more power than necessary here. You're right that there are instances where the functionality cannot be guessed from the name and signature; I think that situation should be avoided whenever feasible.
Yes, that is the essence of a compiler: it translates one programming language to another. Most of them perform various transformations in the process to try to make the code run quickly in the end.
So basically you want a strict version of `insertWith` in a lazy map which forces the Box itself but not the *inside* of the box, to achieve forcing `f` immediately, while delaying `g`? I'm not sure if I'm getting it correctly...
Sorry to correct you... but, usually retribution has a negative connotation, like "revenge" or "punishment", you might be thinking of something more like "compensation".
I'm also a beginner in Haskell, but I think the problem is not related to the language per se, but to the exponentiation algorithm you're using. In python you're using a builtin function for modular exponentiation, which most likely uses tricks specific to this case like: https://en.m.wikipedia.org/wiki/Montgomery_modular_multiplication and you've made your own implementation in Haskell
You got it correctly.
True, I'll try to improve on this, thanks!
What hardware do you have access to? Do you have a particular area of interest, such as perception, control, or planning?
Thank you for the heads-up!
Actually neither of suggestions would speed it up, because GHC is clever enough to replace both `div` and `mod` by bit fiddling: ```haskell Rec { -- RHS size: {terms: 23, types: 8, coercions: 0, joins: 0/1} millerExps_$s$wgo :: Int# -&gt; Int -&gt; [Int] -&gt; [Int] millerExps_$s$wgo = \ (sc_s2Jq :: Int#) (sc1_s2Jo :: Int) (sc2_s2Jp :: [Int]) -&gt; case andI# sc_s2Jq 1# of { __DEFAULT -&gt; : sc1_s2Jo sc2_s2Jp; 0# -&gt; let { ww4_a2GH :: Int# ww4_a2GH = uncheckedIShiftRA# sc_s2Jq 1# } in millerExps_$s$wgo ww4_a2GH (I# ww4_a2GH) (: sc1_s2Jo sc2_s2Jp) } end Rec } -- RHS size: {terms: 23, types: 8, coercions: 0, joins: 0/1} millerExps :: Int -&gt; [Int] millerExps = \ (w_s2IO :: Int) -&gt; case w_s2IO of { I# ww1_s2IR -&gt; case andI# ww1_s2IR 1# of { __DEFAULT -&gt; : (I# ww1_s2IR) []; 0# -&gt; let { ww4_a2GH :: Int# ww4_a2GH = uncheckedIShiftRA# ww1_s2IR 1# } in millerExps_$s$wgo ww4_a2GH (I# ww4_a2GH) [] } } ```
One this is for sure - the one in `System.Random` is definitely *not*. Among the others, some are more and some are less. There are some good very secure entropy sources in the crypto libraries; but that might kind of defeat the purprose, because then you could also just use RSA from there.
I assumed it would be clear that I had some word size (4 bytes) in mind. I assumed that everyone here could figure that out....
If you are looking for a PRNG that’s cryptographically secure, take a look at the one in [cryptonite](https://hackage.haskell.org/package/cryptonite-0.25/docs/Crypto-Random.html).
Have you checked https://downloads.haskell.org/~ghc/latest/docs/html/libraries/binary-0.8.5.1/Data-Binary-Get.html ?
Interested too. In Scala I use doobie, which lets you compose at the JDBC layer. It's great, but you still need to craft statements as strings.
That wasn't clear at all ;) If your input and output are both ByteStrings that represen a sequence of words then you just do somethig like `toHostOrder = concat . map reverse . chunksOf n` where `n` is your word length (only on little-endian machines of course). There isn't `chunksOf` for `ByteString` but it should be easy to come up with an implementation. Am I missing something?
If you want to implement a secure cryptosystem you should actually use truly random number generators, instead of pseudo random, which would slow down even further both implementations
If you are curious about crypto implementations you can also try to make them side channel resistant. There are books on this kind of stuff like http://www.springer.com/gp/book/9780387718163
And why are you on the haskell reddit if not looking for help? Do you have nothing better to do with your time than shit on someone who is struggling?
Same question, why are you on haskell subreddit if not looking for help. Sorry I've ruined your ability to use haskell in the future buddy
Still, it seems like there must be something in between. Maybe it's just because of my unfamiliarity, but cairo code strikes me as awkward. What do you think of the approach of /u/cdsmith in [Code World](https://code.world)? It's designed for children, yet see how much you can do so simply. I think that aproach was inspired by [gloss](https://hackage.haskell.org/package/gloss).
afaict, the lists
also, the gratuitous Data/Control/etc prefixes (which hackage/cabal obnoxiously warn you about when you ignore) add even more noise. imo, Text.take is comparable to T.take
It is said that it has a superdimensional monad ;)
Actually, I started to port this to CodeWorld when I read it on Saturday. I ended up stopping at the point of adding perlin noise; one of the disadvantages of CodeWorld is that because it builds on a shared server, you can't just pull in anything from Hackage. But yes, most things in CodeWorld should be equally doable in gloss (or the codeworld-api library on hackage, too), modulo different API names, and you could use the perlin noise library for those. Gloss and CodeWorld are both simpler (but less capable) than diagrams, and still don't have the setup boilerplate of cairo. But familiarity is probably a big factor, too.
With GHC, I disbelieve that kind of thing would actually be an optimization.
Will do, how fast can I expect it to be?
Oh, that's very interesting, so I guess I'm really slowed down purposelessly. 
I guess the Control.Monad.State.Strict is there to make the function strict right? 
Do you have any suggestion?
The heart of you code is `millerTest`, and its performance is defined by the number of modular exponentiations you do. And it seems to me that you can save a lot. `millerExpsWrapper` returns a list of numbers, where each following is twice the preceding one: `pows = [odd, 2*odd, 4*odd, ..., n-1]`. In `millerTest` you basically compute `modExp a pow n` for each `pow` in `pows`. But you should not recompute it from the scratch each time: it is enough to compute `modExp a odd n`, and then square it up to `n-1` power and take modulo on each step. 
Why do you doubt it?
Terrible names, given the "dot" layout tool and "dotty" front end from the [graphviz](http://graphviz.org/) graph visualization software package.
Hiring more Haskell developers than the entire rest of the world combined (seriously... they have Phil Waddler and Duncan Coutts among hundreds of others), actually developing their entire product using Haskell, bringing Haskell to the blockchain, furthering functional paradigms in the world, developing semantics for K that make Haskell (and MANY others) run on the IELE.
Hundreds of fulltime Haskell developers, or just hundreds of developers? I haven't heard about anything Cardano has done. To be clear, I'm happy they exist, and they might do a lot of good things, but saying more than anyone is an exaggeration. Most of the hard work that's been put into Haskell is via open source Haskell libraries and GHC features/imprvements; by academics, and by software engineers, mostly in their free time, without necessarily the blessing (or financial support) of their employer. So my prior is pretty low. However, there are still many companies that are contributing significantly to Haskell, GHC, and the ecosystem: Simon Marlow at Facebook has contributed back features to GHC itself like ApplicativeDo and compact regions; Tweag is working on linear types for GHC; Obsidian Systems has done a lot of work for GHCJS via the opensource Reflex and its reflex-platform; FPComplete made stack, whose benefits and impact are obvious; Well-Typed have just done a lot of stuff over the years; etc. I don't see how any of the things you've mentioned does much for Haskell, let alone compete with the groups mentioned above. Bringing Haskell to the blockchain is just one of many aspects of Haskell (so is bringing it to the browser, or the jvm, or the Domaine of operating systems, Etc); many companies are furthering functional paradigms, several are profiting off of the correctness and efficiency of Haskell, few are actually contributing to Haskell. 
The literally employ half of the top Haskell devs IN THE WORLD. Downvote me all you want. I’m speaking fact and getting downvotes for no reason.
You don’t think they’re popularizing Haskell? Huh? Are you blind? One example: They got *me* into it along with quite a few others who have been looking into developing DApps. Years ago, I came away from looking into Haskell wIth the conclusion that it was not realistic as the codebase for a full project. I’m now learning it instead of Swift. Also, have you looked at IOHK’s github? They have created quite a few Haskell libraries that are absolutely amazing.
I didn't downvote you, but I don't agree. Honestly, how does hiring Haskell developers help Haskell? There needs to be some concrete artifact, like open source libraries, publications, etc. Also, Conal Elliot's "Compiling To Categories" is another important project in the space of "What can Haskell do? Where can Haskell exist?". It re-interprets Haskell expressions as hardware circuits, among other things. https://github.com/conal/concat/blob/master/README.md
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [conal/concat/.../**README.md** (master → 71acbf1)](https://github.com/conal/concat/blob/71acbf18a6ca6afd242bb418bf288bc604e30e07/README.md) ---- 
You said &gt; doing more for Haskell than any one party has ever done for it short of creating Haskell itself. which seems wrong given how many institutions, companies, even individual people (c.f. Edward Kmett) have made significant contributions to Haskell over the decades. Their GitHub does seem cool: https://github.com/input-output-hk?utf8=✓&amp;q=&amp;type=source&amp;language=haskell But how can it compete with `-XGADT`s? Or the `Applicative`? Or `GHCJS`? Or even `lens`? I don't want to come off as anti-Cardono, again I am very pro-Cardono. Saying "they've done more" is ignoring/forgetting these other more signifcant contributions or trivializing them. That's all I'm saying. 
Cool. Let’s agree to disagree and stop arguing about something so trivial.
sounds good
This has no relevance to haskell whatsoever. I mean there's nothing there that is applicable. Haskell evolution is happening via a mechanism of language extensions. DOT itself is a product of a compromise that scala made trying to sit on two chairs (functional and OO) at once. They've made their bed. Now they will lay on it. 
Nice work!
Thanks brother!
`Integer` is pretty slow. Try `Word64` from `Data.Word` if it makes sense, wherever possible.
`binary` can handle this exact use case, and can load those ints up into something more appropriate for access in the process. Why is there nothing that does this with a single name? When dealing with network order conversions usually you are doing this sort of thing field by field, because the headers or other structures involved often have a lot of structure. Frankly, your scenario of having a large swathe of things to convert in a batch isn't a terribly common pattern.
By analogy with unionWith maybe?
Generally, cryptographic PRNGs are slower than non-cryptographic PRNGs (otherwise we would just always use cryptographic PRNGs). However, `System.Random` is quite bad when it comes to performance while `cryptonite` is pretty well optimized so I’d expect that you actually gain performance in this case. Note that the `random` module in python doesn’t provide a cryptographic PRNG either. In fact the docs specifically recommend against using it for cryptographic purposes.
My hope is that Eta language, not dotty, will be the new scala
As someone who hacks on Haskell on while on a train on a regular basis, thank you!
What you really want is the type mkFst :: Int -&gt; (Int, Lens (a,c) (b,c) a b) or equivalently mkFst :: Int -&gt; (Int, forall f. (a -&gt; f b) -&gt; (a,c) -&gt; f (b,c)) but the problem is this is an impredicative type. This would (if it worked) delay the choice of `f` until you go to access the lens part of the pair. The problem is the quantifier is being floated to the outside. `ImpredicativeTypes` in GHC have a storied and messy past and largely don't work and aren't supported. So how can we fix this in practice? Lens offers you two tools. The choice of `f` is being forced on you too early. One option is to choose an `f` that is universal to stand in for all possible choices of `f` in the future, then recover the lens later. This is what the `ALens` type and `cloneLens` combinator from `lens` offers you. mkFst :: Int -&gt; (Int, ALens (a,c) (b,c) a b) works just fine. But then you can't use the lens you get back without applying the `cloneLens` combinator to it. On the other hand, you can now access the `Int` you were given just fine. Another tool offered by lens is the [`ReifiedLens`](http://hackage.haskell.org/package/lens-4.16/docs/Control-Lens-Reified.html#t:ReifiedLens) type. newtype ReifiedLens s t a b = Lens { runLens :: Lens s t a b } This newtype gives the quantifier a place to live that doesn't require an impredicative choice for the type of the second field in your pair. Then mkFst :: Int -&gt; (Int, ReifiedLens (a,c) (b,c) a b) mkFst i = (succ i, Lens $ \f (a,c) -&gt; (\b -&gt; (b,c)) &lt;$&gt; f a) works, but to use the "reified lens" `one` in your example you'd have to `runLens` one. Which to choose? Well, there are combinators that can work directly on `ALens`, but the `ReifiedLens` stuff is O(1), while the `ALens` machinery requires using maps and a store comonad, which can be more expensive, especially if you convert back and forth multiple times, so the correct choice depends on your use cases.
It only has 550 packages though. There's some work to do to bring in more packages.
I'm surprised there isn't a `Category` instance for `ReifiedLens`.
Merged 8 hours ago though. https://github.com/flycheck/flycheck/pull/1427
On the Haskell end implementing a [Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter) would be a good project. This is essentially a technique that compares what the robot thinks its state is and the input of sensors and corrects the information. Very useful for localisation and object tracking.
**Kalman filter** Kalman filtering, also known as linear quadratic estimation (LQE), is an algorithm that uses a series of measurements observed over time, containing statistical noise and other inaccuracies, and produces estimates of unknown variables that tend to be more accurate than those based on a single measurement alone, by estimating a joint probability distribution over the variables for each timeframe. The filter is named after Rudolf E. Kálmán, one of the primary developers of its theory. The Kalman filter has numerous applications in technology. A common application is for guidance, navigation, and control of vehicles, particularly aircraft and spacecraft. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I'm confused - does the switch to GHC 8.4.1 break so many packages, or do you want to say that Stackage doesn't contain many packages in general?
I think a lot of packages just need to be updated to work with GHC 8.4.1. You can see that older resolvers using GHC 8.2 have much more packages.
I think the description of `do` notation would be enhanced by explaining more explicitly that it's syntax for *combining* multiple `IO` actions into a single one and not syntax for performing `IO` actions. A gripe: I don't think type synonyms should be taught to brand-new users at all. They can be confusing, and are rarely as useful as new users tend to expect them to be.
There are plenty of room for optimisations in your code dealing with random numbers. But just as an experiment I tried to change only your `genWrapper` function to use the `mersenne-random-pure64` library: ``` import qualified System.Random.Mersenne.Pure64 as M ... genWrapper up down = do gen &lt;- M.newPureMT return $ take 100 (R.randomRs (down, up) gen) ``` Note that this is a poor implementation as noted in other messages in this thread (it uses `IO` rather than `State` and so on). However this small change makes your program twice as fast on my machine: ``` $ time ./primeGen real 0m1.121s user 0m1.062s sys 0m0.022s $ time ./primeGen2 real 0m0.443s user 0m0.417s sys 0m0.014s ```
/u/MitchellSalad , it seems there is a strong need for this sort of resource, call it a Real World Haskell 2.0 perhaps. How shall we do this?
[The Essence of Dependent Object Types](https://pdfs.semanticscholar.org/fa74/71efe6bfe9f5e2245358f330855a4b8243ec.pdf). As stated in the paper it's trying to rebuild Scala from first principles--and that's a good sign given the current state of existing programming languages.
The package regression appears to be caused by upper bounds in cabal files and maintainers being slow at updating them.
A lot of people use nix instead, and I'm sure plenty of people use cabal. I do agree we need a better story for this stuff though. 
This seems like a good place to start. Thank you.
I don't think you can reasonably *have* stackage without bounds. It'd be very difficult to *create* new snapshots with any level of confidence without them.
Why can't nix and cabal use Stackage as well instead of the version bounds inside cabal files?
I don't understand. Can you elaborate?
Thank you for your response! The bare essentials are it "parses file" $ do markup &lt;- runIO $ readFile "markup/stuff.html" False `shouldBe` True Which gives the error `Couldn't match type ‘IO’ with ‘SpecM a0’` on the `False`. I have clearly misunderstood in what monad the test is run.
&gt; why can't Stackage just ignore all bounds in cabal files? Then you could get a build plan which *compiles* but has a bug at run-time, if the package author has forbidden a version which is known to work. The problem is that there are two types of information: * These versions are known to not work * These versions are known to work And only one way to specify both. So packages which work but are not known to work are typically forbidden due to bounds being too conservative.
I think people use Cabal specifically because hey don't want to use Stackage. Some people prefer anyone to date solve of package versions, whereas Stackage is largely behind. Nix does use Stackage as the basis of its package set though
How are nightlies produced? Do they just try the latest versions of everything? Or do they hold some things back for specific reasons? Non-nightlies are necessarily behind though.
&gt; That's why Stackage not only compiles but makes sure to run the test-suites. I strongly suspect that most packages' test suites are not good ways to test compatibility with their dependencies. They're meant to test *this* package, not its dependencies. &gt; It's my understanding that almost all upper bounds in cabal files are of the speculative kind. So it seems to me that leaving off upper bounds unless you know a new version doesn't work to be the better strategy here with no downsides. Why aren't we doing that? Because it breaks horribly for people who prefer the solver unless you also concede that revisions are a good idea (which they're not). That said, I do agree it's much better to be explicit about known incompatibilities vs known compatibilities. Supposedly this is what `^&gt;=` will eventually be for.
&gt; That's why Stackage not only compiles but makes sure to run the test-suites. Sadly, I don't think you can rely on testsuites to ensure correctness... &gt; Why aren't we doing that? With that approach, if an update to a dependency is released which breaks your package, there's a window before you update the version bounds in which users can experience breakage.
Thanks for the heads-up!
You know with `UndecidableInstances` you can put type parameters in instance declarations, like this: instance (SomeConstraint s t) =&gt; Category (ReifiedLens s t) where id = ... (.) = ... 
This is clearly homework. I would suggest that you explain what you tried already, and what is the problem you found, otherwise you are just asking other people to do your work.
Upper bounds or not, you should always read changelogs of your dependencies before upgrading stuff.
I'd hope GHC inlines non-recursive parts of even-recursive functions.
Frege!
Is Frege still alive?
How would that help? Lenses compose like this: (.) :: Lens s t u v -&gt; Lens u v a b -&gt; Lens s t a b not like this: (.) :: SomeConstraint s t =&gt; Lens s t a b -&gt; Lens s t b c -&gt; Lens s t a c And you won't be able to somehow turn the second into the first by being clever about your choice for `SomeConstraint`. What you'd need instead is something like this: {-# LANGUAGE GADTs, RankNTypes #-} import Prelude hiding (id, (.)) import Control.Category import Control.Lens data RearrangedLens sa tb where RearrangedId :: RearrangedLens sa sa RearrangedLens :: Lens s t a b -&gt; RearrangedLens (s,t) (a,b) instance Category RearrangedLens where id = RearrangedId RearrangedId . y = y x . RearrangedId = x RearrangedLens x . RearrangedLens y = RearrangedLens (y . x) I had to special-case the identity case because `Category.id` requires a `RearrangedLens sa sa` but with the `RearrangedLens` constructor I can only produce a `RearrangedLens (s,a) (s,a)` but not, e.g., a `RearrangedLens String String`. I tried to take advantage of the fact that `Category` is polykinded by using kind `(*,*)` instead of kind `*`, but ghc didn't seem convinced that a type`sa` of kind `(*,*)` must have the shape `(s,a)`: {-# LANGUAGE DataKinds, GADTs, KindSignatures, RankNTypes #-} import Prelude hiding (id, (.)) import Control.Category import Control.Lens data RearrangedLens (sa :: (*,*)) (tb :: (*,*)) where RearrangedLens :: Lens s t a b -&gt; RearrangedLens '(s,t) '(a,b) instance Category RearrangedLens where id = RearrangedLens id -- couldn't match sa with '(s,a) RearrangedLens x . RearrangedLens y = RearrangedLens (y . x) Maybe ghc is worried about [stuck type expressions](http://gelisam.blogspot.ca/2017/11/computing-with-impossible-types.html) of kind `(*,*)`?
Because not everybody uses Stackage
See my response to /u/Tysonzero's similar comment https://www.reddit.com/r/haskell/comments/842e2k/stackage_nightly20180313_with_ghc_841_is_out/dvmd248/
anybody else get this error with `nightly-2018-03-13`? **The following packages specified by flags or options are not found: persistent-sqlite yaml** I don't understand why this is happening
&gt; this smells like a stack bug, which you should definitely consider reporting [Reported](https://github.com/commercialhaskell/stack/issues/3918), thanks for the suggestion!
I reproduce this too. [Issue here](https://github.com/commercialhaskell/stack/pull/3916), either upgrade Stack or wait for those packages to get onto nightly.
 row :: Int -&gt; [[Int]] -&gt; [Int] Each row of the board is a list. Do you know a function which takes an `Int` and a list, and gives back an element of the list? f :: Int -&gt; [a] -&gt; a `column` is the same, but how can we get that with the same function? If only we had some way to turn our grid... 🤔
Upgrading doesn't help =( stack upgrade Current Stack version: 1.6.5, available download version: 1.6.5 Skipping binary upgrade, you are already running the most recent version 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [AshleyYakeley/Truth/.../**Lens.hs** (master → bef29a9)](https://github.com/AshleyYakeley/Truth/blob/bef29a9a77be6a0a29931cb7b17c0bfb7d1fb043/shapes/src/Data/Lens.hs) ---- 
Nightlies try to pick the latest version of every package.
Regardless of [`#7259`](https://ghc.haskell.org/trac/ghc/ticket/7259) the instance can be written without `RearrangedId` while retaining the rest of the definition if `Category` gave us a way to constrain objects a la [hask](https://gist.github.com/ekmett/b26363fc0f38777a637d) type Cat ob = ob -&gt; ob -&gt; Type class .. Category (cat :: Cat ob) where type Obj cat :: ob -&gt; Constraint id :: Obj cat a =&gt; cat a a that being said you can use type families type family Fst (pair :: (k, k')) :: k where Fst '(a, _) = a type family Snd (pair :: (k, k')) :: k' where Snd '(_, b) = b data RearrangedLens :: Cat (Type, Type) where RearrangedLens :: Lens (Fst st) (Snd st) (Fst ab) (Snd ab) -&gt; RearrangedLens st ab
Hey there, there's a few things you should probably be aware of. Firstly, this is *Curry*, not Haskell; most of the people here have never used it and while they could give you the Haskell solution (which will work on Curry with some slight modifications), the purpose of this assignment is to learn functional-*logical* programming (ie, programming using incomplete data structures and non-determinism). Although it looks like the professor has wisely chosen not to throw you into the deep end on non-determinism and saved it for only a few functions. Secondly, Is this homework for Steven's class? I don't really know any other Universities that would teach Curry in an undergrad setting. If it is, the CS Fishbowl will be a great place to get some answers from 12-2pm today. I'll be around as well as (likely) most of the rest of the class.
I'm familiar with GHC's inlining capabilities (GHC maintenance is my day job), and with the `containers` source code (I'm effectively the lead maintainer of `containers`). The `upsert` function would have to be structured a certain way to allow the inlining you want to occur. Actually inlining it would solve the problem, but that could lead to a lot of extra code, which is itself a problem. I still think we should prioritize good API over micro-optimization but we should expect some things to go faster and others to go slower.
I would like to see something like RWH 2.0 but that's not exactly what I meant by "resource". "Website" or "forum" would have been a better term. As far as how shall we do this, well, I'm not sure. I created https://github.com/haskell-api-discussions/haskell-api-discussions for me to talk to myself in the issue tracker and decide if what I'm doing makes any sense at all. Maybe I'll announce it in a week or two if it seems worth pursuing. (I have process concerns, particularly around trying to coordinate with library authors, should some consensus around a proposed improvement start to emerge. I definitely don't want to help create some squad of bullies who villify library authors that reject their designs).
I completely agree with this: &gt; I have yet to get into a situation where I cannot use a library because its API isn't what it "should" be; I have to work around things here and there, but this is never really a show stopper. &gt; Making the above choices is hard, but API design is, well, design, and for most of these choices, there is no one-size-fits-all answer. However, I don't _think_ I'm proposing anything that would "give up creative freedom or rapid evolution". So, sorry to say it, I think you're stabbing at a straw man ;) Here is a laughably alpha version of what _might_ some day evolve into the kind of community that right now exists only in my head: https://github.com/haskell-api-discussions/haskell-api-discussions Its ergonomics, ironically, are not ideal, and I have some concerns (Pasting a comment I made below...) "I have process concerns, particularly around trying to coordinate with library authors, should some consensus around a proposed improvement start to emerge. I definitely don't want to help create some squad of bullies who villify library authors that reject their designs." But, I will continue tinkering with the idea and maybe I'll announce it in a week or two to solicit more feedback and start trying to make something actually happen.
You'l want a machine with 8GB of RAM and a lot of time to complie the telegram api (issues say sometimes ghc fails due to lack of memory). Apparently the library uses LOTS of generic deriving instances, wich take a lot of time &amp; space to compile.
Ah yes, I had trouble with that before, when I was trying another minimal bot out, thanks for the heads up. 
Stackage lts contains about 2400 packages it seems.
Looks like a home task. Have you made any attempts to solve it yourself?
I haven't played much with CodeWorld, but I have played with gloss! Ultimately I really wanted something optimized for speed and figured that a C library was probably my best bet, so I tried cairo. I actually really like their domain model. I probably didn't explain it super well (because I didn't have to go much in depth), but the tutorial is a pretty good read: https://cairographics.org/tutorial/ Also, I really like the low-level approach (I may even consider going lower, to OpenGL for GLSL eventually :X). I haven't run into very many "gotchas" in the time I've spent doing this, and it's been speedy for the most part. I don't have a whole lot of impetus to change stacks currently, but I will probably end up poking around with other things if cairo doesn't continue to suit my needs :)
Yes 
If you're learning Haskell or posting homework questions, you're doing yourself a disservice by asking for solutions. You have a great opportunity to learn why not do so?
Good! What did you try and what did you get stuck on?
The `Obj` solution is incredibly versatile, here we create a data type `PairShape pair` to witness the shape of `pair :: (k, k')` to be `'(xx, yy) :: (k, k')` for some `xx`, `yy` data PairShape :: (k, k') -&gt; Type where PairShape :: PairShape '(xx, yy) We turn it into a [`Constraint`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Kind.html#t:Constraint) class PairShaped (pair :: (k, k')) where pairShape :: PairShape pair instance pair ~ '(xx, yy) =&gt; PairShaped pair where pairShape :: PairShape '(xx, yy) pairShape = PairShape and that is enough to implement `RearrangedLens` as a `Category`, `(.)` works as before data RearrangedLens :: Cat (Type, Type) where RearrangedLens :: Lens s t a b -&gt; RearrangedLens '(s, t) '(a, b) instance Category RearrangedLens where type Obj RearrangedLens = PairShaped id :: forall pair. PairShaped pair =&gt; RearrangedLens pair pair id | PairShape &lt;- pairShape @_ @_ @pair = RearrangedLens id Now, instead of `RearrangedLens pair pair` the type of `id @_ @RearrangedLens` constrains the `pair` objects to be pair-shaped PairShaped pair =&gt; RearrangedLens pair pair `PairShaped pair` turns into `pair ~ '(xx, yy)` pair ~ '(xx, yy) =&gt; RearrangedLens pair pair which becomes RearrangedLens '(xx, yy) '(xx, yy)
There's two classes of solutions that I know of to this problem. 1. The first is to "cheat" and use strings. `show` gives us a symbol for each digit in an integer. String literals in Haskell are just a list of `Char`s. &gt;&gt;&gt; show 1492 "1492" &gt;&gt;&gt; ['1','4','9','2'] "1492" Once you learn that `read` can convert a `String` into an `Integer`: &gt;&gt;&gt; read "1492" :: Integer 1492 The problem now becomes "how can I generate all rotations of a list". Here's one way to rotate a list by one position: rotate :: [a] -&gt; [a] rotate [] = [] rotate (a:as) = as ++ [a] So now we have: &gt;&gt;&gt; (read . rotate . show) 1492 :: Integer 4921 If we want to apply the same function multiple times to an input, we can use `iterate`: &gt;&gt;&gt; take 10 $ iterate (read . rotate . show) 1492 :: [Integer] [1492,4921,9214,2149,1492,4921,9214,2149,1492,4921] One micro-optimization we can make is to see that `iterate` is chaining `read . show`, which is (hopefully) identity. We can eliminate this by floating the show and read out of the `iterate` argument: &gt;&gt;&gt; take 10 . map read . iterate rotate $ show 1492 :: [Integer] [1492,4921,9214,2149,1492,4921,9214,2149,1492,4921] Now here's a question for OP - should all n-digit numbers have n rotations? Or only all the unique rotations? For example, which of these is correct?: &gt;&gt;&gt; rotations 1111 [1111,1111,1111,1111] &gt;&gt;&gt; rotations 1111 [1111] If the former, we can define `rotations` in terms of the number of digits of our input: rotations :: Integer -&gt; [Integer] rotations n = take (length nStr) . map read $ iterate rotate nStr where nStr = show n If the latter, we can use `takeWhile` to limit our number of iterations: rotations :: Integer -&gt; [Integer] rotations n = n : takeWhile (/= n0) (tail . map read . iterate rotate $ show n) 2. The second class of solutions uses math instead of strings. We can get the last digit of a number using `quotRem`: &gt;&gt;&gt; 1492 `quotRem` 10 (149, 2) Then we can find out how many digits long a number is by taking its log base 10: &gt;&gt;&gt; log 149 / log 10 2.173186268412274 We'll need to round up, but we should also be concerned about some boundary conditions. When rotating single-digit numbers, we'll be checking the number of digits of `0`, which we want to have zero digits: &gt;&gt;&gt; log 0 / log 10 -Infinity When rotation exact powers of 10, like `100`, we'll want them to count the `1` as a digit: &gt;&gt;&gt; log 100 / log 10 2.0 We can fix all these by adding 1 before we take the log, then taking the `ceiling` of the log: numDigits :: Integer -&gt; Integer numDigits n = ceiling (log (fromInteger n + 1) / log 10) Now to rotate a number, we just move the 1's digit over by number of digits in the rest of the places: rotate :: Integer -&gt; Integer rotate n = q + r * (10^numDigits q) where (q,r) = n `quotRem` 10 Note that this rotates the number in the *opposite* direction as the first solution: &gt;&gt;&gt; rotate 1492 2149 So if we want to produce solutions like in the examples OP provided, we'll need to work in the opposite direction somehow: &gt;&gt;&gt; take 4 $ iterate rotate 1492 [1492,2149,9214,4921] We could try to use `reverse`, but there's another bug lurking here - numbers with a `0` digit: &gt;&gt;&gt; take 5 $ iterate rotate 10203 [10203,31020,3102,2310,231] This happens as when leading zeroes get moved to the front, the length in digits of the number shrinks. So rather than move the 1's digit to the `10^d`'s place, let's calculate `d` once and move the `10^d`'s digit to the 1's place: rotate :: Integer -&gt; Integer -&gt; Integer rotate p n = r * 10 + q where (q,r) = n `quotRem` p Now we can rotate in the same direction as in the example: &gt;&gt;&gt; rotate (10^3) 1492 4921 And produce correct solutions on numbers with a `0` digit: &gt;&gt;&gt; take 5 $ iterate (rotate (10^4)) 10203 [10203,2031,20310,3102,31020] So if we want all n-digit numbers to have n rotations, we have: rotations :: Integer -&gt; Integer rotations 0 = [0] rotations n = take (d + 1) (iterate (rotate (10^d)) n) where d = floor (log (fromInteger n) / log 10) Or if we only want the unique rotations: rotations :: Integer -&gt; Integer rotations 0 = [0] rotations n = n : takeWhile (/=n) (tail $ iterate (rotate (10^d)) n) where d = floor (log (fromInteger n) / log 10) 
Throwing this out there.. I see a lot of libraries with documentation like: &gt; &gt; import qualified Control.Monad.Metrics as Metrics &gt; import Control.Monad.Metrics (Metrics, Resolution(..), MonadMetrics(..)) &gt; import Control.Monad.Reader &gt; import qualified System.Metrics as EKG &gt;_The Control.Monad.Metrics module is designed to be imported qualified._ What if we defined a yaml or json file where the import setup the library _was designed for_ was specified? So in the above case, it would say that by default qualified as `Metrics` and this and that symbol unqualified. Editors and haskell style formatters could then pick that up. 
To train or to evaluate? On the GPU or on the CPU?
Rust is easier because the standard library is simpler to build, and rustc is already multi-target.
Ok, got telegram-api to compile finally (you weren't kidding about it taking lots of time and space). I'm assuming you just have a Token.hs in src/ with a line token = &lt;my bot token&gt;, anything else I need to do to get it running?
First train available data sets and then use it for evaluation. Performance is secondary, so it could be executed on CPU.
That looks interesting und could come in handy for a usecase I have in mind for later. But I fear that does not solve my core problem: Pulling every necessary definition into a single sourcefile. As I understand `stack new` templates, they create a new project from scratch and already import some modules on the stack level. (So that stack can build this into an executable). But I need the imports on a source-code level.
Do you want to do "serious" machine learning or do you want to programm RNN/LSTM/GRU for yourself in haskell to understand them better? 
You can only apply language extensions (`{-# LANGUAGE .. #-}`) on module level, and there will be lots of modules that won't compile with some of the extensions. I think you will only be able to use a very small subset of the Hackage which doesn't use any extensions; I have no idea how would you combine two modules with different set of extensions.
Totally unrelated, but I don't get *why* cabal and stack default to `-O`. At least from C\# and Visual C++ I know that it defaults to a debug build, which makes total sense for development. With stack, we have to be explicit about that (`--fast`). The cases where we want to develop with `-On` should be quite rare.
Actually I'm stupid ;) This works: it "parses file" $ do markup &lt;- readFile "markup/stuff.html" markup `shouldBe` "hello" No need for `runIO`. 
I mean... there's always the greedy solution of `tar -czf &lt;filename&gt; .` ;) I kid. Sounds like a fun project.
You could write your own package that depends on all the packages you want to import and export them in a single kitchen sink module. Not sure if this gets you what you want, but this way you can also define your own abbreviated aliases for common functions.
Do you need to upload the binary or the source? It seems weird that your source code is limited to one file. 
Most extensions don't actually break old code, you can usually just keep glomming them together. In fact, most "expert haskeller" guides I've seen here recently assume that you'll have something like 10-15 extensions default, and then even more on top in some cases.
Or we could just add qualified exports / nested modules to Haskell, which would completely sidestep the issue (since you could just export a submodule called EKG)
This comes closer - but the point is that the 3rd party source code has to be included in the file I upload; alternatively: You can assume that the place where the file is build has no internet connection and never connected to the internet. It just got the newest ghc and that's about it. Now I want to "reimplement by copy-and-pasting" some 3rd-party-libraries that would not be available otherwise.
I don't think this exists, but this does sound like a fun project! Keep in mind that some of your third party libraries might be making FFI calls to C, in which case combining the Haskell source of those libraries will not be sufficient. &gt; For every term, resolve from which module this came from (Could lead me into GHC waters... not sure if doable) TemplateHaskell has a `reify` function which can give you a lot of information about where an identifier comes from: {-# LANGUAGE TemplateHaskell #-} import Language.Haskell.TH.Syntax $(do Just (Name _ (NameG _ (PkgName packageName) (ModName moduleName))) &lt;- lookupValueName "pure" runIO $ print packageName -- "base" runIO $ print moduleName -- "moduleName" pure []) There is also something called a "source plugin" ([proposal](https://github.com/ghc-proposals/ghc-proposals/pull/107), [implementation](https://phabricator.haskell.org/D4342)) which I haven't tried yet but seems ideally-suited for your case since it allows you to plug into an existing build system such as stack and to run your plugin on each file as it is built. It uses a different representation for the source code though, and I don't know if that format has something as convenient as `reify`. A third approach could be to compile your code with the [-ddump-minimal-imports](http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/separate_compilation.html#ghc-flag--ddump-minimal-imports) flag, which generates a file listing all the external identifiers used by your module and which module they are imported from. You could inline those modules and then recompile to get the set of modules imported by _those_ modules, and so on. If your module has an explicit export list (e.g. `module MyModule (main) where`) and have warnings enabled, ghc will also complain about all the top-level definitions which are not indirectly used by your `main`, so you can prune unused definitions in order to avoid exceeding your size limit.
:D I am half looking forward to implement it myself and half looking forward for some Haskeller going "Yeah I know exactly what you mean and implemented this 2 years ago! Here, take and enjoy!" :)
Source - it is indeed weird but understandable once you put it into context: In competitive programming, the one with the most creative and/or experienced brain + implementation skills should win. It is desirable to eliminate other factors like custom-made compilers or such. (This might not make much sense in the context of Haskell binaries, but I hope the general point is clear).
The short answer to this is "we're working on it". Most -- if not all -- bounds such as this will be dropped and the floodgates will open to accept `async` and the myriad transitive deps relying on it.
&gt; But it seems that there is no stack support for it I managed to get it to compile using the following `stack.yaml`: resolver: lts-9.21 packages: - . extra-deps: - grenade-0.1.0 
Your suggested approach will probably get in trouble very fast because of name conflicts. You'll probably have to do some parsing to guarantee all names are unique.
I believe `yaml` will be on nightlies soon. `persistent-sqlite` unfortunately has a transitive dependency on `blaze-builder`, which needs to be upgraded for compatibility with ghc 8.4, but was last uploaded April 2016... so we're not holding our breath for that one. If you are anxious to use the new nightlies, then `stack upgrade --git --git-branch stable` is the way to go. Finding a way to help `blaze-builder` along, or to move stuff away from depending on it, is the other solution to the problem.
I can think of a few that could easily get you into trouble: OverloadedLists and OverloadedStrings will make some cide ambiguous that isn't ambiguous without; obviously NoImplicitPrelude is going to seriously ruin things; and unleashing CPP onto unsuspecting code could also lead to trouble in the odd case. And there's almost certainly more.
If only one could enable lexical scoped language extensions :/
I don't think CPP will be a problem. What would an example be of code that is valid without CPP and then changes meaning with CPP? IIRC, all the CPP directives must be at the start of a line and all that, and you can't normally start a line with # in Haskell. Either way, for a tool as weird as this, "assume it'll work and cry when it doesn't." is a fine strategy.
Sorry, I was unclear here. More than simply 'generality' must be considered, and I apologize for allowing my (honestly fairly minor) frustrations with this API to push a message that would seem to trivialize a difficult design space - That was a mistake and it's not really fair to the authors or even to the point I was trying to make. What I was trying to capture with this response was that, unlike `inserWith` and friends, `alter` does not rely on implicit behaviors that can't be inferred from it's type signature. It's not that I can implement the others in terms of `alter` - IMHO that's a vacuous argument. It's that I am never in a situation where I have to refer back to library documentation to determine what it's doing. That, to me, is nigh invaluable.
What are these stackisms you speak of? The yaml file?
Generally, it's considered good to separate the definitions of your types and functions specifically built to operate on those types from the glue code that pulls it all together. It seems fairly common to have a few files dedicated to types in this way, usually divided by architectural component or sometimes by 'business logic' concern. If you are implementing classes at all, generally these also end up in their own files, although it's not uncommon to have multiple interrelated types and classes together in the same file as long as there is a clear, common thread connecting them. After you're done with your types, you then tend to break things up by business logic component. `Main.hs` is likely just going to define a fairly limited set of functions executed in IO - Loading the config, parsing your args, setting up a loop, and freeing resources when the app finishes executing. [This](https://github.com/jxv/dino-rush/tree/master/library/DinoRush) was a great example of some fairly typical application architecture that was floating around this sub fairly recently.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [jxv/dino-rush/.../**DinoRush** (master → f888bc0)](https://github.com/jxv/dino-rush/tree/f888bc05fe734ced2c49bec81f9c804b47c81069/library/DinoRush) ---- 
Most of my complains come from the new collections library. It seems they are planning on keeping the confusing inheritance tree (with the same concepts, like IterableOnce). `flatMap` and `map` are still `special` things (instead of it being an interface). Overall the language still feels like it "wants" to be haskell but refuses to make the jump, causing confusion (we want `implicit`s to behave correctly when used for typeclasses, but still maintain the idea that `implicit`s are just a language feature and typeclasses are just a pattern) and inefficiency. Overall, it seems dotty is more of a rewrite of scalac (which I'm not complaining about, I can do with fewer scalac errors) and not the overall redo of Scala that I was hoping for. Maybe I was just hoping for too much but I was hoping that dotty would make doing FP-typesafe scala less awkward.
It's missing fast convolution kernels, but backprop could be a good choice. https://blog.jle.im/entry/introducing-the-backprop-library.html I love this library. I use pytorch at work, and am looking forward to giving hasktorch a try once it matures a bit.
Thanks 
Well a teacher or something like that. But I know it’s general purpose object oriented. I have read about it being used for anything but it isn’t really popular or I haven’t heard of it used a lot. But I know HTML and CSS and I bit of Python. So I’m familiar with programming but I’ve never really seen Haskell mentioned too much Crystal is mentioned more. 
&gt; the Scala team is repeating the same mistakes they did in scalac nailed it
Are {} used in Haskell syntax anywhere? As far as I know, they are only used in multi line comments. Also, why is -- the comment syntax? I feel like -- would've been nice to have for an infix operator. 
Are {} used in Haskell syntax anywhere? As far as I know, they are only used in multi line comments. Also, why is -- the comment syntax? I feel like -- would've been nice to have for an infix operator. 
Haskell is anything but object oriented. It isn't as popular as other languages because it's very different than other languages. Its purely functional. I can't name a single language other than haskell that is purely functional.
Ok interesting did not know that 
[nightly-2018-03-14](https://www.stackage.org/nightly-2018-03-14) is up. It has async-2.2.1, yaml-0.8.28, and more. Still "missing" about 1500 packages but progress is being made. We could have held back and waited a few days to release a more fully featured nightly, but I thought it preferable to keep releasing them in case the bits and pieces we've cobbled together so far are useful to anyone. I understand it's a bit confusing; I probably should have made a blog post *beforehand* so that people would know what to expect. Hoping to have a good chunk of nightly back by the end of the week, though of course some of that depends on the speed at which packages get updated to work with ghc-8.4.1. n.b. there was no deep wisdom in keeping async-2.2.1 out of last night's nightly. It's just a large avalanche of work to sift through and we didn't get around to it yet. Normally we do drop constraints, but some of them _were_ left on purpose this time (e.g. aeson &lt; 1.3). Async should not have been one of those; now we know. :P
&gt; Its purely functional [Except for when it's purely imperative!](https://github.com/jdan/haskell-basic) :D
Very much so.
Geat, thanks for the explanation! Keep up the good work!
{} is record syntax.
The encoding/decoding for ARM is totally weird and inconsistent, feels like several people worked on it without really coordinating very well. We still need it though. Nobody wants to hand-code all those encodings.
I’d say `unionBy` is behaving correctly but it might make sense to mention the commutativity requirement in the docs (then again at least I would assume that an equality predicate is commutative). Fwiw you can simplify your implementation to ```haskell unionBy pred left right = left ++ filter (\r -&gt; not (any (`pred` r) left)) right ```
Curly braces are used for record syntax, which is convenient syntactic sugar allowing us to name fields of data types. Consider a type that might represent a car registration: data Car = Car String -- Make String -- Model String -- Color String -- License plate Int -- Odometer reading Int -- Model year -- and so on You might say "boy, I wish there was a name to refer to those fields by a name so I don't have to remember what order they're in!" And that's what record syntax does. We can define the type like this instead: data Car = Car { make :: String , model :: String , color :: String , licensePlate :: String , odometer :: Int , modelYear :: Int } (The weird comma placement is idiomatic - but you can put the commas at the end of each line if you'd like, it doesn't matter.) That tells the compiler to automatically generate some nice accessor functions for us: isAntique :: Car -&gt; Bool isAntique car = (modelYear car) &lt; 1980 fullName :: Car -&gt; String fullName car = (make car) ++ (model car) And some update functions too: repaint :: Car -&gt; String -&gt; Car repaint car newColor = car { color = newColor } This is all just syntactic sugar. You could write all of the accessor and update functions yourself: getMake (Car mk _ _ _ _ _ ) = mk getModel (Car _ md _ _ _ _ ) = md ... updateMake (Car mk md co lp od my) newMake = Car newMake md co lp od my ... But not only is record syntax much more convenient than that, it's much more flexible, too. What if, five minutes after you finish writing all those accessor and update functions, your boss rushes in to tell you that we need to track the `TaxValue` of each car too? Well, shit, now you have to go edit each and every one of those functions to include the extra field. But if you use record syntax, just toss in a `, taxValue :: Float` and everything just works. See [this Haskell wiki page](https://en.wikibooks.org/wiki/Haskell/More_on_datatypes#Named_Fields_(Record_Syntax\)) for more. 
They sure are: `{}` come up most often when dealing with records * Declaring records: `data R = R { i :: Int, b :: Bool }` * Constructing records: `let r = R { i = 5, b = True } in ...` * Updating records: `let r' = r{i = i r + 1} in ...` * Pattern-matching on records: `f R{i = myInt} = myInt` * maybe more?? They can also be used in place of indentation after a `let`, `where`, `do`, `of`, or similar keyword. See [the layout section of the syntax reference from the haskell 2010 report](https://www.haskell.org/onlinereport/haskell2010/haskellch10.html#x17-17800010.3) for details Also from looking at the report just now, I learned there's a style of literate programming with LaTeX-like delimiters where you would use things like `\begin{code}`, but I don't know if those braces count maybe other places too? As for the comment syntax, that's probably one of those "that's just the way it is" things. Note that even though you can't have an operator named `--`, you can still use the sequence `--` within an operator name (whether or not your syntax highlighter knows this is a separate matter). I've used `a --&gt; b = (a, b)` before to make a hard-coded Map prettier
Short answer: Yes flipping the arguments must not change the result of the predicate. This is documented, but easy to miss, especially if one is not familiar with the used term. Long answer: The documentation in Data.List states that the predicate is assumed to be an equivalence. Look at the top of the section on the "-By"-functions. A function `eq :: a -&gt; a -&gt; Bool` being an equivalence means that is has the following properties: 1. reflexive: `eq a a` is always `True` 2. symmetric: `eq a b` is the same as `eq b a` 3. transitive: If `eq a b` and `eq b c` are both `True` then `eq a c` also has to be `True` So yes you have to be able to flip the arguments without changing the result (otherwise it is not symmetric) and it is documented. Could probably be better though.
Just found out that GHC.Integer.GMP.Internals has a powModInteger that decreases your runtime by about 50%. That's as good as it gets I suppose, as it's just a binding to the GNU Multi-Precision library.
I honestly think this would be the only current solution. As haskell is now, you'll have a ton of name clashes (no namespacing), per module extensions messing up other code (overloaded whatever), staging restrictions with TH, etc. That said, I think the 50KB requirement might be exceeded by the compile-to-js solutions we have currently, except for maybe Purescript, Hmm.
Yeeeeeeeeeeeey!!! Thanks!
It works configuring the stack.yaml like this: packages: - location: git: https://github.com/HuwCampbell/grenade.git commit: b1a6e1630f04b07ec7c33ea1744d5e9051beeb12 I think I will use your library. But it would be easier if you would update the dependencies and add these stackisms - so someone could just add it to `build-depends` in the cabal file. Great work by the way. :)
Allright thank for the reply and the link! So it really is not all that different from the old OO way of doing that. "If you are implementing classes at all, generally these also end up in their own files, although it's not uncommon to have multiple interrelated types and classes together in the same file as long as there is a clear, common thread connecting them." Is it not always that you have a type, and then write an implementation for a class (like EQ or ORD and so on right?) ? In that case I would expect it to be default to be together, or is it because the files get too large then? Which brings me to: Are there guidelines as to how many LOC a module should be ? Or does that not make sense here? 
&gt; (In order to prevent too much copy-paste, a limit of 50 kb on a submission is put in place - fair enough I think). That pretty much kills this idea. ;) 
Wow...thank you for these pointers. I couldn't figure out how to ask for slack access to the hasktorch channel though.
This is great, thank you!
If not for the 50kb limit, this would have been a great solution to my specific usecase!
Right. I'll just take it from here and see what happens anyway. But it is great to have a place like this with useful answers, also when things are not really specific enough to ask on SO for instance. Thx!
What's the reason behind the weird asymmetric style-guide in Haskell? Things like the position of commas and brackets Person{id=1 , name="John Doe"}
*bifunctors* provides [`Biapplicative`](https://hackage.haskell.org/package/bifunctors-5.5.2/docs/Data-Biapplicative.html), which offers: bipure :: a -&gt; b -&gt; p a b (&lt;&lt;*&gt;&gt;) :: p (a -&gt; b) (c -&gt; d) -&gt; p a c -&gt; p b `Monad` doesn't generalise to `Bimonad` in such a straightforward manner -- cf. [this Stack Overflow question](https://stackoverflow.com/q/13556314/2751851). By the way, [`Bifoldable`](http://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Bifoldable.html) and [`Bitraversable`](http://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Bifoldable.html) also exist (and can be found in *base*).
A good rule of thumb is to create a module in Haskell whenever you would create a class in OOP. Often a module will export a type with the same name as the module (e.g. `Data.Map` and `Map`) and some functions to operate on it (the public interface), with various non-exported helper functions (private/internal interface), and typeclass instances (interface implementations). If you have types that are closely related, or mutually recursive (e.g. `Statement` and `Expression` ADTs for a compiler) then they’ll end up in the same module, grouped under some logical name (`AST`) or something generic (`Types`).
The same library also seems to have the extended gcd and the Miller-Rabin primality test, if you want to outsource that much code.
Are Idris, Agda, and Coq impure? Genuinely didn’t know that.
I recently wrote up my experiences and thoughts about this [in a blogpost](https://jaspervdj.be/posts/2018-03-08-handle-pattern.html).
&gt; Finding a way to help blaze-builder along, or to move stuff away from depending on it, is the other solution to the problem. I've emailed the current maintainer (lpsmith) and offered him my help a few days ago but I didn't get a reply. I'm not sure if we should ask one of the previous maintainers who still have access to Hackage if they could upload a release. That's jaspervdj and Simon Meier.
Using lens I'm trying to insert a new element into a list after a given index and increment the inner items orderIds after the newly inserted item. I'm a bit confused at why this code isn't working: ``` import Control.Lens data A = A { _customOrderId :: Int } deriving Show things = [ A {_customOrderId = 0} , A {_customOrderId = 1} , A {_customOrderId = 2} , A {_customOrderId = 3} , A {_customOrderId = 4} , A {_customOrderId = 5} ] main = print $ things &amp; ( (_drop 2 %~ ((A 22)&lt;|)) . ((traversed . indices (&gt; 2) . customerOrderIdLens) %~ (+ 1)) ) ``` The first portion works to insert where desired, but the second portion `((traversed . indices (&gt; 2) . customerOrderIdLens) %~ (+ 1))` only increments the order id in indices 4. Any idea why this is happening?
I usually see things lined up such as: Person { id = 1 , name = "John Doe" } Is that weird? It is more readable to me and missing commas are extremely obvious since they're in a column
Come to the datahaskell gitter and ping us for info 
I'm not sure, i'm used to JSON and it looks prettier this way Person { id = 1, name = "John Doe" } I'm just wondering if there's any reason for writing in such way. One thing I could think of is it is easier to comment out a property when the comma is aligned with the next property - which is a common practice in SQL
Maybe that can be your first project. ;)
You might as well look into https://www.reddit.com/r/haskellquestions/. Here are a few ideas (of course a matter of opinion): * Be consistent: It helps you to think clear and others in understanding your code. In the case of Haskell use camel case. * Use `newtype` or `data` to create a type that is distinct from all other types. This way you can't pass something else by accident. It is also more readable when you have to match on a constructor instead of using `fst` and `snd` (or even just custom record selectors). For example: `data Square = Square Piece Colour` * Write type signatures: You write them for yourself and others. * Use descriptive identifiers. Most of your identifiers give me no idea of what they do. (That's usually a bad sign!) * If necessary, use comments to describe your definitions. * Use pattern matching: change list x element= (fst (splitAt x list)) ++ [element] ++ (tail (snd (splitAt x list))) -- Instead use something like this: change' [] _ element = [element] change' l@(_ : _) i element = case splitAt i l of (as, bs) -&gt; as ++ element : drop 1 bs * `invert` is `not`. It may be helpful for you to use **hlint**. This way it will tell you improvements while you code. * You don't have to bind parameters to names that you don't use: Instead of `\x -&gt; 42` use `\_ -&gt; 42` or `const 42` * Seperate pure and IO parts of your program. Instead of all the `print` functions construct a `String`, `ShowS` (if performance is important) or even an external library for pretty printing (e.g., `ansi-wl-pprint`). This example is with with `ansi-wl-pprint` (You combine values of type `Doc` and at the end use an impure part to print it): charByColorDoc :: Color -&gt; (Char -&gt; P.Doc) charByColorDoc c = P.char $ case c of W -&gt; toUpper B -&gt; id squareDoc (Square piece color) = case (piece, color) of None -&gt; P.char '_' Pawn -&gt; mod 'p' Queen -&gt; mod 'q' where mod = charByColor c
They're not impure, but they're also not exactly programming languages. Coq isn't even Turing complete, and Idris and Agda just aren't set up for general use (at least not right now).
Your Nat type is a NEW type you are defining. It has two constructors, which means it has two "cases." If n :: Nat, then you know n is either Zero (empty constructor) or Succ m for some m :: Nat. So either Nat is a trivial value Zero and holds no additional day, or it is a Succ and holds another Nat as a piece of data. The meaning of the data is "here is the Nat I am the successor of!!!"
You know more about the `Succ` function than just the fact that it takes a `Nat` and returns a `Nat`. You know exactly what kind of `Nat` it returns. Namely, it returns the `Nat` it got, wrapped in another layer of `Succ`. What is a nat? `Zero` is a `Nat`, and if you already have some `n :: Nat`, then `Succ n` is also a `Nat`. And that's it. So to enumerate all the values that are nats: `Zero`, `Succ Zero`, `Succ (Succ Zero)`... (this all ignores the existence of `undefined` to simplify things) 
I created a `Token` Module wich only exports a `token` variable. Works for me....
Thanks :)
Here is a nice video done by PBS aimed at High School kids that explains Nat and this construction: https://www.youtube.com/watch?v=3gBoP8jZ1Is The point of all this Peano stuff is to work up one more level to Church Encodings. So after that clicks try taking it one step further: this church (Succ x) f z = f (church x f z) church Zero f z = z now for example `f = (+1), z = 0` just gives you back normal counting numbers. `f=Succ,z=Zero` reversed the church transformation above. But f and z are arbitrary and thus you can use different f and z's to compute. For example pred x f z = snd $ (x (\(a,b) -&gt;(f a,a)) (z,z)) and thus `two f z = f(f(z)` you'll see that `pred two Succ Zero = Succ(Zero)` i.e. one. https://en.wikipedia.org/wiki/Church_encoding Fairly quickly you are able to embed an arithmetic in any language with only the most basic type of transformations. As an aside a similar technique works for loops, lists, ... All of Haskell's base data structures come right out of mathematical logic and can be embedded in almost any other language. So you have both how you would create something like a Haskell from nothing and at the same time something you can create in Haskell. 
Thanks :)
One reason is slightly cleaner diffs IIRC.
Simplifying a ton, you might imagine that a `Nat` contains an integer tag which describes which constructor it has been made with (say, `0` for `Zero` and `1` for `Succ`), along with the data that it has been constructed with. This data can depend on the tag; when the tag is `0`, there is no data, whereas when the tag is `1`, the data would be a pointer to another `Nat`. In a C-like language, we might translate it like this: enum NatTag { Zero, Succ }; struct ZeroData {}; struct SuccData { Nat *pred; }; struct Nat { NatTag tag; union { ZeroData valZero, SuccData valSucc} data; }; 
Also wanted to address your question about what's in memory. I'm not an expert on how Haskell is implemented, but essentially what you have here is a linked list with no data in it. If you were to define it in (shitty) C it might look like this: enum natConstructor {ZERO, SUCC}; typedef struct nat { enum natConstructor constType; struct nat *rest; } nat; // integers from nats int natToInteger (nat *n) { switch (n-&gt;constType) { case SUCC: return 1 + natToInteger(n-&gt;rest); default: // ZERO case return 0; } } 
I think this may be a matter of personal taste, but instead of a case-expression, I would prefer handling it in a where-block
Are there plans to add the `--host` flag to `cabal configure` proper, so that the wrapper isn't necessary?
At it's heart, Haskell types are almost like sets in the mathematical sense. The only difference to mathematical sets is that each Haskell type also contains the value called `undefined` or `⊥` (Bottom), which is owed to Haskell's non-strict nature. The `data` syntax defines a new algebraic datatype, that is, a type composed of other types. Specifically, the types defined using `data` are tagged unions. Tagged unions are similar to a normal set union, but they remember from which set each value comes from. For example, from `a ∈ A ∪ B` you cannot tell whether `a` is in `A` or in `B`. The elements of `A ⨆ B` though are pairs of a /tag/ (usually an integer) and a /value/. The value is from `A` or `B`, and the tag is used to record from which set the value came from. In the case of `Nat`, the elements of the tagged union are `{ (Zero, ()), (Succ, (Zero, ())), (Succ, (Succ, (Zero, ()))), (Succ, (Succ, (Succ, (Zero, ())))) ... }`. GHC will of course use pointers and choose more efficient representations. `Zero` could be represented by `0`, and `Succ` could be represented by `1` and a pointer. The pointer would point either to another `Nat`, or to a thunk which will result in a `Nat`. You can also imagine `Nat` as being an abstract base class, and `Zero` and `Succ` being concrete classes inheriting from `Nat`. The construct `type` defines a synonym for a type. This is mainly useful to make a complicated type, like a Monad transformer stack, easier to write it down. The type checker treats the two types as being the same. Obviously, this construct should not have any performance impact whatsoever. However, the compiler tries hard to keep using the type synonym in error messages. The construct `newtype` is just a renaming of a type to a new type. Even though its syntax is similar to `data`, it is also essentially a type synonym. Differently to a type synonym though, the type checker does not treat them as equal, and you must explicitly call a constructor to wrap a value into a `newtype` and later unwrap it by using pattern matching or a destructor. Since it is just there to create hoops programmers have to jump through, there is no performance cost associated with `newtype`s. This makes them very useful to enforce conditions on the contents.
It's possible you're getting hung up on the syntax for a `data` declaration (it's confusing). Keep in mind if you see: data T t = X t t | Y Int Char | ... The stuff to the left of `=` _only_ exist at the type level (i.e. appear in type signatures). Same thing for the arguments to the data constructors (the parameter `t`, `Int`, `Char` in this example). As for how e.g. `Y 1 'a'` might be laid out in the heap, it's something like: &lt;tag_indicating_constructor_Y&gt; &lt;ptr_to_an_Int&gt; &lt;ptr_to_a_Char&gt; But you can peruse https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects if you want more details
An important component to using text is also the part where it allows for strict IO. It is not infrequent in Haskell that you're trying to write to the file before the handle to read it has been closed, if you're trying to write with an adjusted result of the original input. That can be a problem, and there are a number of ways around it, but using `text` is probably the one that requires the least amount of thought.
How so?
Sorry, but this explanation really succs.
&gt; I'm prone to export them. In general I find the problems caused by exposing too much information are surmountable, the ones caused by keeping your cards to close to your chest tend to provoke forks. In a world with GitHub, I just don't understand this. Unless the type is the feature (e.g. tuples) hiding implementation details is simply required for programming in the large. If my API is missing something, rather than forcing me to expose everything why can't someone just send me a pull request with the functionality they want?
Not so important, but could shorten these lines a little bit: x_coords 'a' = 0 x_coords 'b' = 1 x_coords 'c' = 2 x_coords 'd' = 3 x_coords 'e' = 4 x_coords 'f' = 5 x_coords 'g' = 6 x_coords 'h' = 7 y_coords '1' = 7 y_coords '2' = 6 y_coords '3' = 5 y_coords '4' = 4 y_coords '5' = 3 y_coords '6' = 2 y_coords '7' = 1 y_coords '8' = 0 by: import Data.Char -- ord 'a' = 97 -- ord 'b' = 98 x_coors c = (ord c) - 97 -- digitToInt '3' = 3 y_coords c = 8 - (digitToInt c)
Interesting post! Thx. For now I'll just try to get a working minimal version of something, but I'll definitely come back to it later as I learn more! 
&gt; Sorry, but this explanation really succs. Naaaaaaaaaaaaaaaaaat 
&gt; I think this may be a matter of personal taste, but instead of a case-expression in your change-example, I would prefer handling it in a where-block. That is legitimate but only possible for a single case. Also, using a case statement gives some notion of a *central theme* (as in telling stories). &gt; HLint helped me a lot in the beginning. All these transformations help you understand the syntax and semantics of haskell. It is amazing (for the most part). There are so many useful transformations. 
&gt; Hardcoding values is prone to errors. `97` -&gt; `ord 'a'`. ;)
Haha yeah, changed that - your correction however still hinges on the assumption that the given alphabet is ordered in an increasing lexical order :P (I for one like to insert funny emojies at prime-numbered indizes! /s)
Thx, changed it :) Still relies on the very dangerous assumption that the given alphabet is sorted in a ascending lexical order /s
Ping /u/jaspervdj. Would you make a release of `blaze-builder` if I get the latest compatibility PRs there in shape?
Ah, good. I only ever used the raw-qq template, and even then rarely, so I forgot about them mixing.
Haskell is a general purpose programming language just like Python, but it is less popular and has a slightly less developed ecosystem of packages for doing general IT tasks. Haskell is 'strongly typed', which means that a compiler for the language has strict rules about which types are considered valid in a given operation, and will issue errors about violating those rules instead of compiling to a valid program. Haskell is one of the most strongly typed languages out there, and generally, this is considered one of the most compelling reasons to write code in the language - The compiler prevents you from shooting yourself in the foot, and because of the structure of it's rules, pushes the author towards good, composable design. Python, as a counterpoint, is much more flexible and it sort of famous for letting people get a wide variety of fuzzy tasks done very quickly. You will probably have a MUCH easier time learning Python if this is your first programming language, which, from your post history, seems likely to be the case. However, knowing enough Python to be dangerous, and being fairly proficient in Haskell, I much prefer working in Haskell. If your goal is to get hacking on a computer and build a cool thing quickly, I'd strongly recommend learning Python as your first language. If your goal is to learn more about how computers work, and how to structure solid, maintainable software programs, learn Haskell. 
Thanks so much! I'm glad so many minds are putting thought into it.
Doesn't this sort of defeat the point of the competition? I would imagine that the 'one file' limitation is put in place distinctly to prevent this sort of thing, no? Or is that more that the judges don't want to deal with 800 different crazy build issues?
I don't see any error. Elements of indices 3, 4, 5 are incremented, and `A 22` is inserted after the second element (index 1, because indexing starts from 0). [A {_customOrderId = 0},A {_customOrderId = 1},A {_customOrderId = 22},A {_customOrderId = 2},A {_customOrderId = 4},A {_customOrderId = 5},A {_customOrderId = 6}] I fixed the code, it was missing the definitions for `_drop` and `custom(er)OrderIdLens`. {-# LANGUAGE TemplateHaskell #-} import Control.Lens import Control.Lens.TH data A = A { _customOrderId :: Int } deriving Show things = [ A {_customOrderId = 0} , A {_customOrderId = 1} , A {_customOrderId = 2} , A {_customOrderId = 3} , A {_customOrderId = 4} , A {_customOrderId = 5} ] makeLenses ''A _drop 0 = id _drop n = _drop (n-1) . _tail main = print $ things &amp; ( (_drop 2 %~ ((A 22)&lt;|)) . ((traversed . indices (&gt; 2) . customOrderId) %~ (+ 1)) )
"Sorry, but this explanation really succs." succs.
If you really want to understand types, a deeper dive into the field is probably necessary. Benjamin Pierce's book (https://www.asc.ohio-state.edu/pollard.4/type/books/pierce-tpl.pdf) actually has a chapter dedicated to types like Nat, i.e. recursive types. But it relies somewhat on prior discussion (specifically familiarity with the simply typed lambda calculus). 
&gt; Or is that more that the judges don't want to deal with 800 different crazy build issues? That certainly + having all solutions built consistently (which is more fair). &gt; Doesn't this sort of defeat the point of the competition? I would imagine that the 'one file' limitation is put in place distinctly to prevent this sort of thing, no? You are right, I am bordering grey area here. The norm is that each competitor writes his own little set of useful functions. (And obviously does not share them, since they give him an advantage). Now I don't know what the rules say when to competitors share their libraries. I would argue that in the end of the day it does not matter much whether I copy-paste some neat graph library in Haskell or whether I write that myself - much more important is figuring out the solution, implementation should be a near mechanical, trivial thing at the top ranks of competitive programming. If I would succeed with this project, I would not compete before consulting the rules in more details and asking other competitors. I would however use that project in the training area of Hackerrank: You still get some internet points there, but nobody seems to care about them; people openly post their solution in the comment section. In that case, I don't see an ethical issue. tl;dr: The unfairness reduces to "I use things that I might not have been able to write myself" which is a minor point since for imperative languages (C++ and Java mainly), the algorithms are out there and freely available even in Competitive Programming books. I will consult the rules before I use this however tho.
I found this series to be really good: http://chris-taylor.github.io/blog/2013/02/10/the-algebra-of-algebraic-data-types/
I haven’t read it, but I’ve had it recommended many times: https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/
Camel case is typical for top-level things. Local things often use underscores to separate words.
Idris pushes hard for practicality.
I just made plans.
I don't think that's the right one.
Sounds great! :) I have prepared a release at https://github.com/lpsmith/blaze-builder/pull/16.
Don't think they were asking about GADTs but rather ANY algebraic data types
Looking forward to using it along with nix.
Also I am now reading this link and it is very helpful, perfect for where I'm at in my understanding of GADTs.
[removed]
Seems reasonable, but why do you actually need this?
This is definitely not a Haskell specific issue, but it is definitely a real problem. I would argue that this is less an issue with the technique of typeclass based codecs, and more an issue with the bad assumption that it's appropriate to serialize the type holding your business logic directly, without ceremony or additional design considerations. I think it's immensely valuable to have a canonical de/serializable type that acts as a formal intermediate representation between business logic and persistance - This (should) make things like transitioning between versions of the format easier to deal with, when done properly. 
&gt; That's why Stackage not only compiles but makes sure to run the test-suites. It actually skips quite a lot of them: https://github.com/fpco/stackage/blob/master/build-constraints.yaml#L4942
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [fpco/stackage/.../**build-constraints.yaml#L4942** (master → 0db30e8)](https://github.com/fpco/stackage/blob/0db30e8c5b199500b61293c5f09ba272baa0585d/build-constraints.yaml#L4942) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvpfubs.)
I think you're right in Scala's case, but it needs to be said that FP and OO can be combined and have been. F# compromised on ML features to fit into CLR (well, C# world), while OCaml has separate OO syntax which is dissimilar to what you find as OO in C++ or Java but can still be useful in certain (admittedly limited) situations. Neither made the mistake of trying to unify them and fail as happened Scala. Exploration is good and I wish the Scala/Dotty developer luck, but I have to admit that even when writing C++ and Java, I never wrote OO code. Maybe it's because I didn't design a GUI framework, don't know, but it's wrong to assume that Java or C++ code is inherently OO. Their OO flavor just isn't as good as the original OO as envisioned by its inventors and then later re-imagined in Smalltalk.
&gt; The given example of bundling Spock seems like outright cheating. I can't think of anything in C++ or Java that can be accomplished in a single file with no external dependencies that comes anywhere close to Spock. Yes, that was just a random example. To be fair tho, algorithmic competitions are usually not really that complex on the implementation side. Implementationwise, there is rarely more needed than plumbing together algorithms in graph theory, statistics, setting up a dynamic programming instance etc. Implementing a webserver would be overkill in complexity. To give a quite good overview into the scene, [here is a link the blog of one of the best](http://petr-mitrichev.blogspot.com/). &gt; However, it brings to mind that there might be another way to approach your problem- What if there was just a way to use some sort of comment bracing or markup, etc, as an optional cabal behavior, to bundle a project as a single file? Like a way to stuff project.cabal into main.hs? Do you know whether something like this exists? I am afraid that cabal will not be able to bundle it all into a single sourcefile - since that is quite a rare usecase. &gt; I assume it'd still be in the spirit of the competition if you kept a careful note of packages instead of grabbing just any ole thing off of hackage, and it'd be just as easy to build as a single file of source. I agree that it is less grey area because the amount of effort to use 3rd party libs get increased that way. 
&gt; Hoping to have a good chunk of nightly back by the end of the week, though of course some of that depends on the speed at which packages get updated to work with ghc-8.4.1. I remember that we had bulk builders for hackage and stackage and need to ask why trivial compatibility issues aren't found and fixed during the ghc-RC cycle. Isn't that one of the goals (not the primary) of doing multiple RC releases?
Yeah, it seems such a thing sort of does exist, in stack. I was going to suggest you contribute it because it seemed like a valuable addition to their 'script interpreter' featureset, but it appears they already have some capabilities that may cover that usecase. See: https://haskell-lang.org/tutorial/stack-script It seems as though you could simply use a shortlist of arguments to stack to add in a package or too as a build command and it'd more or less give you want you want. Not sure if that fits inside the bounds of your problem space or not, but it might be worth a look.
 since it is essentially consumed by all p (millerExps n) the order probably doesn't mattwe: millerExps n = unfoldr step where go n | d == 0 = Nothing | otherwise = Just (m, m) where (d, m) = n `divMod` 2 This has the advantage that it can fuse.
No, unfortunately this is not what I want. This interpretes a file. What I need as output is a file of source code. I would be surprised if Stack would support something like this because 99,99% of Users just want to execute or interprete a file (potentially with external packages like Spock)
Coudln't you say the same for a range of things, e.g. monads cf. http://www.stephendiehl.com/posts/monads.html
In what way is this better than the existing `Category (-&gt;)` instance? 
&gt; Let's say we want to use a package that doesn't ship with GHC itself, like http-conduit. The stack exec command - and its shortcuts like stack ghc and stack runghc - all take a --package argument indicating that an additional package needs to be present. That's what I was thinking. Ostensibly, the submissions for this contest MUST have some way to provide a build command. So your build command would be, like, `stack ghc -- package vector main.hs` Or something. That should result in a compiled EXE. 
I think you should skip all this and just watch this lecture by Simon Peyton Jones https://www.youtube.com/watch?v=6COvD8oynmI He breaks down how the compiler converts types, typeclasses, data types into the core language. It's super accessible and much more grokkable in the way same way that "an Int takes up 4 bytes of memory" is :)
I can offer a few comments, since I have a few minutes free. You wrote: getRow (t, c) = map (\x -&gt; (t, c)) [1..8] You're ignoring `x`, so this just gives you a list of 8 copies of the same tuple. You could accomplish the same thing with `getRow (t, c) = replicate 8 (t, c)`. But then there's really no reason to specialize to tuples. You really want `getRow x = replicate 8 x`. Or, using partial application, `getRow = replicate 8`. That's nice and clean... but it also raises the question of whether `getRow` is enough clearer than `replicate 8` that it's worth defining the name. It's a judgement call, but my judgement says no: you're better off not defining this at all, and just using `replicate 8` when you want a list of eight of something. `makeBoard` is a little more complicated, and worth defining. You could define it as `makeBoard x = replicate 8 (replicate 8 x)`, or just use the composition operator, like this: `makeBoard = replicate 8 . replicate 8`. This kind of improvement is common in Haskell. It's a great feeling when something you started out defining in a very special-purpose way turns out to generalize and get simpler at the same time! Along a different line, you wrote this: printsquare (t, c) | c == None = putStr " _ " | t == Pawn &amp;&amp; c == W = putStr " P " | t == Pawn &amp;&amp; c == B = putStr " p " | t == Queen &amp;&amp; c == W = putStr " Q " | t == Queen &amp;&amp; c == B = putStr " q " This misses a more powerful abstraction. I's want this: pieceName _ None = "_" pieceName Pawn W = "P" pieceName Pawn B = "p" pieceName Queen W = "Q" pieceName Queen B = "q" You can use this to define `printsquare` if you like... but now that you've extracted the app-specific logic, you can accomplish the same thing just as easily within `printrow`: printrow = putStrLn . intercalate " " . map (\(t, c) -&gt; pieceName t c) The lesson here is to first look for opportunities to define your logic in pure functions that mean something. The `pieceName` function is likely to be useful throughout your code, while `printsquare` was specific to one specific use case. It was a more powerful abstraction, and the main reason is that you weren't mixing I/O, formatting (with the spaces) and the representation of pieces into the same abstraction. Later, you wrote: change list x element= (fst (splitAt x list)) ++ [element] ++ (tail (snd (splitAt x list))) When you're repeating a subexpression, it often helps to call it out with a `where` clause. That gives you the opportunity to use pattern matching instead of `fst` and `snd`, like this: change list x element= before ++ [element] ++ tail remaining where (before, remaining) = splitAt x list Pattern matching is SO much more readable than navigating tuples with `fst` and `snd`. Becoming comfortable with pattern matching improves the readability of your Haskell code by a huge margin, so go out of your way to make use of it. That often means using more `let`/`where` and extracting more subexpression into variables; but the use of more names can also improve readability. One final piece of advice about the same function. When you have lots of arguments to a function, you should choose which arguments come first based on: (a) what's likely to be short and compact, and (b) what's most likely to be fixed at a specific value. In this case, it's the list that's likely to vary, and to be the long argument. So you could prefer this: change x element list = before ++ [element] ++ tail remaining where (before, remaining) = splitAt x list Later, that lets you write this: eventLoop $ change 7 (getRow (Pawn, W)) . change 6 (getRow (Pawn, W)) . change 1 (getRow (Pawn, B)) . change 0 (getRow (Pawn, B)) $ makeBoard (Empty, None)) Placing the short arguments first let me turn this three argument function into a one argument function using partial application, and then use the composition operator (`.`) to collapse the deeply nested parentheses into a linear composition chain. This is much easier to read. These are tricks you'll pick up over time.
It‘s Pac-Man complete. Although [apparently](https://groups.google.com/forum/m/#!topic/idris-lang/hqgavn-bX5U) nobody went through the hassle of proving it.
Let's compare two failure scenarios: In one scenario, I have to `import Data.Text.Internal`, which is explicitly flagged as being unsupported under the PVP and freely subject to change. I can proceed with my code _immediately_ and everything continues to work. Users can continue to build on my code and everything moves on. In the other scenario, I can get stuck because the library I depend on doesn't expose some implementation detail that I need to implement a feature at all, or to implement it with the correct asymptotics for an abstraction that the author wasn't even aware of when they wrote the data type I'm using. So I can get on the horn and ping the author and wait anywhere from a day to 3 years to never to get a reply. (I've hit each of those points!) In the meantime I'm completely dead in the water. To make this concrete, taking a couple of real examples from the same library: 1.) Bryan O'Sullivan never anticipated that I might use his `text` library back around 0.9, in ~2010) in a parser of mine and need access to efficient ways to do UTF-16 based indexing lest I have to slice in O(n) rather than O(1). Even being willing to take upon myself the burden of maintaining the appropriate invariants, I was dead in the water, I needed `Text` to interop with the surrounding libraries and I couldn't get at the parts due to implementation hiding. Fortunately, a couple of months later he shipped 0.11 with the `Array` type he uses exposed. Crisis averted. I even vaguely remembered the project I was working on, but in the meantime had converted to working on `ByteString` instead, because I couldn't delay production for 2 months. (The library I shipped as `trifecta` never switched back!) 2.) In the process of working on another library, in another situation the stream fusion stuff in `text` wasn't exposed, so I couldn't create an appropriate `Text` consumer to fuse which was necessary to speed up a bunch of code 4x. Three years later that was exposed a dozen releases later in 1.1, but by then I had long since given up on the speedup. Memories are only so long. Since all this stuff was exported from the package as of 1.1 (even where it is marked clearly `Internal`) I've never once been blocked on him as a maintainer. I've sent in a patch or two where I could improve general performance, and been a happy library consumer. But both scenarios mentioned above happened to me in production situations! In one case I had to switch to another library. In another I simply ate a huge performance hit. Neither of these is an optimal situation for a user. I'm not using this to shit on Bryan, he's an incredibly conscientious maintainer. He's just not psychic and has a lot of irons in the fire. I wanted to provide a concrete data point of how the proposal you offer just doesn't work in practice. To borrow a buzzword from an old keynote by Bryan, from a [POSIWID](https://en.wikipedia.org/wiki/The_purpose_of_a_system_is_what_it_does) perspective, ignoring the intention of the approach, its effect in practice is to cause real friction between well-meaning developers, the moment an unanticipated need arises. Worse, if I do run into tension with the author of the library and they really don't want to export those details, it can be unresolvable. Asking for a positive action to include an extra API for me requires a conscious effort on their part. On the other hand exposing some additional guts and marking them clearly unsupported gives me access to the gun, but I have to import the `.Internal` gun safe and point it at my foot to use it. I far _far_ prefer the situation that avoids putting me into communication lockstep with a third party, who may have other changes in flight, be locked in as part of a core library that happens to be locked to the GHC version, or have other suchg concerns that I know nothing about about how their API will evolve in the future that prevent them from acting right away. You aren't getting any safety, just making any hacks I use to get at your guts more error prone. I can easily break your invariants by building a parallel copy of your data type and `unsafeCoerce`ing it to my type, and have had to do this in emergencies. You've got no additional safety from me except for a mechanism to cause me pain through undue hiding. I'm a terrible consumer of other people's libraries. I'm happy to break all of your assumptions about your beautiful APIs when I need to for asymptotics or correctness or completeness reasons, and to take upon myself all of the consequences for my actions. In a C++ program, implementation hiding works pretty well. You already have to refactor the class to add to it. You don't write extensions in a completely different place. You have to mangle something like the one header and one source file where the class is implemented. Typeclasses are not classes, however. I can make up a new class you've never heard of and then show how your existing data type is an instance. Those instances can live with my new class. Implementing that fact properly may well require me to poke at bits you didn't expect anybody to ever poke at. Most of the time you can't invert dependencies to get big upstream libraries to incur a dependency. The fact that other people can define instances for existing data types makes the barrier around your code a fair bit more permeable. e.g. `containers` will never depend upon `lens` for instance. Luckily one operation we needed also fit the general pattern of what the maintainer of `containers` thought was a general combinator. On the other hand, without access to `Data.Map.Internal`, I still couldn't implement a bunch of operations in `lens` with the correct complexity until it was exposed last year. The functionality I required was just too obscure. Heck even if you gave me combinators for working with every part of your data type, if you didn't mark them all explicitly INLINEABLE or INLINE you can impact my performance all in the name of establishing some nebulous encapsulation boundary. When placed in the dual situation as the library author rather than as the consumer, I'd rather have a user unblocked and able to use my library, with perhaps a well thought out email sent to me asking if I can package up some subset of the internal functionality in a well-thought out API that does safely maintain all my invariants, than a full inbox full of active high-priority requests, or worse, look up after a month away on vacation to find a [package](http://hackage.haskell.org/package/temporary) [forked](http://hackage.haskell.org/package/temporary-rc) on hackage by an impatient consumer that needs access to some functionality now.
https://hackage.haskell.org/package/monoids-0.1.17/docs/Data-Monoid-Categorical.html
Building a monoid out of endomorphisms on a type in a category lets you use it in places that expect a `Monoid` constraint, e.g. foldMap, the `Writer (Endomorphism MyCategory)` monad, etc.
We use `Endo` to derive `foldr` from `foldMap`. Using, let's say, `Endomorphism (Kleisli m)` you can construct a `foldrM` with the same kind of construction.
Oh. I didn't pay enough attention to the code and totally misread it as going the other direction. My error. Sorry. 
There are uses for building a category out of a monoid too. =) Unfortunately in haskell you get a funny construction, because `Category` is particularly limited and the eta rules to properly use, say, a `()` as a 1-inhabitant kind are currently unsound as it has too many inhabitants, forcing you to use something like data M m :: k -&gt; k -&gt; * where M :: m -&gt; M m a a which instead of building a category with one object, makes one with one object for every inhabitant of kind `k`, which can be tricked into having many, even when `k = ()`.
I 100% agree with this. We have alphas and RCs, we should put them to use and have our ecosystem primed and ready for the GHC release before it even comes out.
As a fun aside, not fully an answer to your question, the Algebraic parts can be rationalized in terms of numerical algebra. If I have `data Empty = Empty`, there's only 1 value for a given value of type Empty, which is Empty. You you have `data KindaBool = Left Empty | Right Empty` you have a Sum type. How many different values are there? Well, there's all of the left options, plus all of the right options. On the left, we have Empty, which only has 1 option, and the same is true on the right. So there are `1+1` options. Sum! If you have `data SomePair = SomePair KindaBool KindaBool`, we now have two KindaBools, the left one and the right one, and both need to be specified to make a value (the SomePair function takes two arguments, roughly). So, how many values are there? Well, there's two values for the left argument (Left Empty and Right Empty (like True and False)) and the same two on the right. For each of the first values the second value can be either option, giving you (Left Right), (Left Left), (Right Right), (Right Left), as the possible options. `2*2 = 4`. Product! If you have `data Dumb a = Dumb a Empty` you're saying the first value is any `a`, and the second is Empty, which only had one value. So we know what it has to be, there are no extra options. So we have essentially `a * 1` or `a` options. We haven't added anything new, so we have the same number of things. In your case, with `Nat` we have `Nat = 1 + Nat`, which is effectively infinite (like the natural numbers) because you can always get one more number when you think you've found the last. In Haskell `Empty` is `() `, called Unit. Empty was a terrible name, because it sounds like Void, which is the Haskell type equivalent of 0: the type that has no values. Similar to algebra, if I have `One a | Two Void` I've got `a + 0`, since I can never instantiate the right. If I do `Product a Void` I get `a * 0 = 0`, effectively infecting the result, since I have to provide two values, and one of them can never be constructed. Algebra. 
GHC could compile to c a while back. Does that mean C is functional?
Certainly. You can kind of hack it as an argument desugerer. But that’s arguably very ugly: desugarArguments &lt;$&gt; getArgs &gt;&gt;= mainWorker desugarArguments :: [String] -&gt; [String] desugarArguments [] = [] desugarArguments ("--host":host:xs) = expandHost host ++ xs desugarArguments (('-':'-':'h':'o':'s':'t':'=':host):xs) = expandHost host ++ xs desugarArguments (x:xs) = x:desugarArguments xs expandHost :: String -&gt; [String] expandHost host = [ "--with" ++ '-':tool ++ '=':host ++ '-':tool | tool &lt;- words "ghc ghc-pkg gcc ld hsc2hs" ] ++ [ "--hsc2hs-options=\"--cross-compile\"" , "--configure-option=--host=" ++ host] It’s prone to incorrect expansions and doest tie in with the existing argument parser framework; as such we‘ll need a better (proper) solution. Once we have `—overrode-build-type` and `—host` I don’t think we need the wrapper anymore and fewer patches :-)
Succ isn't the function succ that increments a number. Succ is an abstract value constructor if I'm not mistaken. that's causing the confusion :) Will check out his axioms.
Thanks. Got quite a lot of responses. Will check them all out in a bit :)
True. That's the reason I just let the confusion exist and still continued improving other areas :)
Woah. This look good. Will watch it in a bit :) This should clarify a lot.
Thanks that clarifies quite a lot. Won't get into category theory for sometime then. I'm actually comfortable with fields. Will look once again.
Thanks. Got quite a lot of responses. Will check them all out in a bit :)
Same here :D That's the reason I even wondered if I should do category theory or not as I saw it along side a few haskell posts.
To my understanding Succ is a constructor and not a function. That's what actually caused the confusion. Will have a closer look man. Got quite a lot of responses. Will check them all out in a bit :)
I asked the question at the right time I guess. This video is just 2 weeks old. Thanks for the awesome video :D
Yes, the behavior of the `Ord` instances do matter for performance. See [this PR](https://github.com/purescript/purescript/pull/3265) by /u/bitemyapp that dramatically improved performance on the PureScript compiler by changing the `Ord` instance for some types. 
In Haskell, constructors `are` functions! Their output is simply a value of that type. For example, you can do `map Just [1, 2, 3]` to get [Just 1, Just 2, Just3]` where `Just` is a constructor of `Maybe`.
I should have been specific I guess. I understand recursion and induction pretty well. Will definitely check out Software Foundations man. Is Succ actually succ? I thought Succ is just a so called type but doesn't actually do anything.
I have to actually use types and looks like I have to read a lot more about them. Correct me if I'm wrong. But Just doesn't actually do any operation on 1, 2 and 3. I can pattern match a data type to see which constructor it had used and all that. But does the constructor actually do something apart from being a label or something. Does Succ actually do something to Nat. I think that's what's causing the confusion. 
I know I'm pretty bad at this. Will have to spend more time on types and math in general.
I would even go so far as to say that this should be done the other way around: generating language bindings from an IDL (i.e. .proto or something similar)
`Data.Map` is designed to provide *okay* performance for a wide variety of types and operations. It's a decent data structure to start with, when you haven't figured out just what you need, you don't know what your data will look like, or you don't know how important performance will be. It's also a good data structure to fall back on when you're implementing a library function and don't want to demand too much from your users. But for high-performance work, you're usually better off using something else. `IntMap`, `HashMap`, generalized tries, cache-oblivious structures, etc.
This is some fantastic information and should be immortalized somehow. Ideally if my idea takes off (somewhat poorly presented here, but more to come) we can also find a way to incorporate gems of knowledge like this that, distinct from subjective taste in API ergonomics, Every Haskeller Should Know
If you're wondering about `Succ n` in `Succ n + m`, it's not supposed to *do* anything. `Succ n` is just some natural number that's not zero. It's a value, just is. It's like a 3-4-5 triangle in that that either doesn't do anything. The triangle just is. And `m` is just some natural number, either zero or the successor of some natural number. In fact you don't need a function `succ :: Nat -&gt; Nat`. If you did it it would be something like succ :: Nat -&gt; Nat succ Zero = Succ Zero succ (Succ n) = Succ (Succ n) But you already have `Succ`.
`Just` actually is a function that takes `1` and returns `Just 1`. These are two different values! It's a `a -&gt; Maybe a`, so if you give it an `Int` like `1`, it'll return `Just 1`, a different value that has type `Maybe Int`.
Is dhall up for this yet? I am wanting to start doing something like this soon, not sure what the best approach is.
Nice! In the spirit of keeping it extensible you might want to consider an open type family rather than a closed one so people can implement it for their own combinators. (Obviously that doesn’t matter as long as it’s just a blog post.)
Now I don't have to write this up! Thanks, Dave!
Any time :)
Thanks, makes sense.
System F omega has no ability whatsoever to branch on types. &gt; unified language/syntax for values/types/kinds That's a main purpose of dependent Haskell.
I'm still not convinced by the Progressive part of PPM (Progressive photon mapping I guess) compared to any other photon mapping based approach (without the "Progressive"). The guarantee of progressive photon mapping depends too much on the initials parameters (sure, there is some solutions for them, which quickly kills the "surprisingly easy to code") and I think you'll quickly get better result with a naive photon mapping approach with final gathering. This being said, implementing a photon mapper in Haskell may be a really interesting project if we focus on performance. The Photon map acceleration structure is quite challenging in a pure / persistent context, and it may be a good candidate to test compact region ;)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [phischu/fragnix/.../**SliceCompiler.hs#L82** (master → d766d26)](https://github.com/phischu/fragnix/blob/d766d269d696e9882a0ae6a98633ce2ba6fc7dd7/src/Fragnix/SliceCompiler.hs#L82) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvqcx6n.)
It looks [maintained](https://github.com/diagrams/diagrams-lib/commits/master) at the very least.
Awesome. I spent about ten seconds writing out an `unfoldr` solution but made a mistake somewhere. Good to know it is straightforward.
This is very cool. I personally learnt a lot about type level programming in Haskell by diving into servant's source. While this approach may kind of be poor man's dependently typed programming, I still think it's pretty awesome and actually relatively easy to understand. It's just pattern matching on types and type-level recursion. In many cases you can even add your own type errors to aid developers or users, via `TypeError`. It's also pretty easy to extend, at the expense of orphan instances. 
The datatype declaration `data Nat = Zero | Succ Nat` should be read as: an element of type Nat (i.e. a natural number) is either Zero or it is Succ applied to another element of type Nat. This means that `Zero` is a `Nat`, `Succ Zero` is a `Nat`, `Succ (Succ Zero)` is a `Nat`, and so on. So you can see how `Nat` is one possible representation of the type of natural numbers, with this inductive recursive definition. You might be interested in GADT syntax, which more explicitly tells you the type of the constructors involved. For instance, the above datatype would be written in GADT syntax as: data Nat where Zero :: Nat Succ :: Nat -&gt; Nat This is taken to be an inductive definition (i.e. the elements of the type are precisely those which can be obtained by using the constructors, and no others). With this syntax it is clearer that `Zero` is a nullary constructor (so defines a fixed element of type `Nat`), whereas `Succ` takes in a `Nat` and returns another `Nat` (which we understand as representing the successor to that natural number). I think once you understand this example you're pretty much the whole way. For another example, here's a GADT datatype definition of binary trees: data BTree a where Leaf :: a -&gt; BTree a Node :: a -&gt; BTree a -&gt; BTree a -&gt; BTree a As an inductive definition, this means that an element of type `BTree a` is either `Leaf l` for some `l :: a`, or `Node l t1 t2` for some `l :: a` and `t1, t2 :: BTree a`. 
So they're paying at least _two_ people to mainly work on GHC now?! That's amazing! :)
Oh that is interesting! &gt; If you want you can take a look at the code base and if you're not immediately scared I'd be happy to answer your questions. Looks very clean and understandable to my beginnerish eyes. Do I guess your motivation correctly? You want to try the "one giant file" (OGF) approach as part of your workflow - when building a project, get all the slices needed, put them into OGF and get a potentially better compilate as a result. Seems like you are solving a superset of the problems I want to solve - neat! I don't know how much time I will have to spare, but maybe I will invest myself into this project :) On extension: I fear that this is not solvable in the general case - if OGF turns out to be usefull, it will likely be an optimization that can not always be applied.
You can get around the namespacing by mangling names; GHC already does that. `Data.Functor.fmap` turns into `DataziFunctorzifmap`, for example.
With the prismatic approach, do we lose exhaustiveness checking?
&gt; Ease of refactoring is a major benefit I see in Haskell, and type classes help a lot there. If I have a codec pair of type classes, and I want to change many occurrences of Foo to Bar, I just need to change the types and the program logic, and I'm done; I think that one of the points is that you are in fact not done yet. You also have to change the serialized data (i.e., not Haskell). Without type-classes, it's more work indeed, but at least you get to see that the data will be encoded differently.
Hmm, this looks like my [ConstrainedCategory](https://github.com/AshleyYakeley/Truth/blob/1cbfd3b147e256ddfca6331ae2822e312884f424/shapes/src/Control/Constrained/Category.hs) class.
Apart from cleaner diffs (missing trailing commas), it's mostly a cultural thing, I'd wager. The symbols are nicely lined up, some people prefer it this way. I just tend to do what the convention is, because no matter what, in my experience you can get used to pretty much everything (e.g. the snake_case / camelCase stuff) :D Another reason may be that you can easily spot missing commas. I know some people that advocate writing stuff like "strings you" + "want to" + "join" this way in some other languages, for exactly that reason. 
Right now you can use the Haskell API to generate code for some language from the Dhall AST. That's about the extent of the support
True, but that is the same thing as saying that types, values, and kinds are unified.
Well, they said "unified _language/syntax_".
Sure! https://github.com/rust-lang-nursery/portability-wg/ just started which I am lucky to be a part of, so definitely trying to make sure the best ideas are shuffled from one language to the other.
Thank you guys so much!
Wiki content is licensed under the simple permissive license: https://wiki.haskell.org/HaskellWiki:Licensing
&gt; Do I guess your motivation correctly? You want to try the "one giant file" (OGF) approach as part of your workflow - when building a project, get all the slices needed, put them into OGF and get a potentially better compilate as a result. Yes, correct. Perhaps there will be a tradeoff between recompilation time (optimized when you recompile just the functions you changed and those that depend on them) and performance of the result (optimized when GHC gets to see all definitions, making it a whole program optimizer). Or perhaps not. &gt; On extension: I fear that this is not solvable in the general case - if OGF turns out to be usefull, it will likely be an optimization that can not always be applied. Yes, that's what I want to find out. I think your goal is more clearly defined in scope but I believe you will run into some of the same problems I did/do. If you find an interesting problem and/or solution please share them.
there are so many more hyper parameters in naive photon mapping - how many photons to shoot, widths of photons... When I tried PPM I found it to be significantly less sensitive to the parameters than either standard photon mapping or MLT. Also he's writing a path tracer who's goal I assume is to be progressive.
I parsed the title as (+ (- tweag-io nix) bazel) 
servant initially came with some helpers for "distributing"/"flattening" API types (precisely to get nice server/client function types) under the name of "Canonicalize". Some time ago I had to do something similar and revisited it, fixing some bugs along the way. The result is in [this gist](https://gist.github.com/alpmestan/f6bb5e31e6241d1a6de57625fb1df5bc).
One could argue this way: - We want to decouple business logic types from the wire-format - one way is to write serialization code "explicitly" - yet, you may value the consistency given by generic programming: you describe *once* how a particular (small) entity is serialized (e.g. you always omit `null`s in JSON) - then you can have two data types: one which core logic operates on, and one which is used to drive the generic code. - what is nice, if model and wire-types are structurally equal, you can get conversion for free (using generics again!) - you only need to write manual version when the mapping is non-trivial. - in fact, wire type isn't strictly necessary, you can have a structural (generic description) of it. Yet, Haskell's data type syntax is nice. And there're variations on that. I'm personally very into using generic data-type programming. The wrong thing is to have too many purposes for a single type (that's e.g. the single responsibility principle, S in SOLID - though types aren't classes; anyway overall good idea to have low coupling + high cohesion). E.g. writing a special type to only drive serialization feels feels unnecessary (enterprise?), it's easier to accept "I'll write codecs manually" (or even using IDL to generate code - which is IMHO overkill as well). 
No, I think I stand by that -- proving stuff is hard! Proving things in a nice language like Idris or Agda is certainly better, but not trivial -- I posted [a challenge](https://twitter.com/mattoflambda/status/961996106094669824) that seemed relatively straightforward, but that I was totally unable to do. There are two responses, both of which work, and both of which are rather involved.
A package is in Stackage if a maintainer decides to do so. There are lots of well maintained packages that are not in Stackage.
You might be interested in [doctest](http://hackage.haskell.org/package/doctest), which tests code examples in haddock docs, or [shelltestrunner](http://hackage.haskell.org/package/shelltestrunner), which tests command line behaviour (and with the help of some glue, could probably test examples in docs.)
For the `diagrams-*` packages it's the dependency bound on `lens`, which is too low for the current LTS. [It'll be fixed soon](https://github.com/diagrams/diagrams-core/pull/96#issuecomment-373234425).
How does this: * Handle GADTs? IIRC, backpack doesn't like them, but IIRC, tagless-final can render them unnecessary. If I have an `Expr a` type and `Mul :: Num a =&gt; Expr a -&gt; Expr a -&gt; Expr a` as well as `If :: Expr Bool -&gt; Expr a -&gt; Expr a -&gt; Expr a`, does it still work? * How does this compare to OCaml's module system? I know a lot of tagless-final examples use ML functors rather than Haskell type classes; is this what that looks like?
It is my opinion that design decisions made to address these concerns should prioritize : "How can we do this with the minimum possible room for undefined behaviors creeping into the software." Rebuilding a JSON parser or serializer yourself is probably the best way to introduce undefined behaviors, and so should be considered a last resort. Complicated series of transformations from an unrestricted format into a concrete type is also a good source of potential mistakes, and so should likewise be avoided. Transformations between concrete types, one of which may possess a generated interface to interact with your serialization layer is, in my experience, the option that provides for the least room for someone to get something wrong, and also generally provides for less of a learning curve / less cognitive overhead than may be necessary with fancier solutions to this problem. But, a lot of this is dependent on things like 'surface area' of the types involved or, of course, the actual use-cases for serialized representations of said types. I think that it's definitely true that this practice will not introduce problems in every project - Whether or not your system can gracefully handle changes in serialization format is not always a concern for every possible use of persistence, and as a complimentary point, there are certainly some cases in which limiting the project to non-breaking changes to structure is an acceptable limitation. In my day to day, I am generally concerned with cases in which I have several hundred properties and up to a dozen or so layers of nesting. That 'reeks of enterprise' most definitely, but that doesn't really change the reality that if my code can't handle that gracefully, nobody is going to find any utility out of the software I write.
+1 to what cocreature said, and another question: https://github.com/fpco/stackage/blob/master/build-constraints.yaml seems not to match current latest nightly, is that normal ? Likewise for the one in the ghc-8.4.1 branch.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [fpco/stackage/.../**build-constraints.yaml** (master → 69343ae)](https://github.com/fpco/stackage/blob/69343ae8fb33be1332f5a1e907afd40fdddfa129/build-constraints.yaml) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvqvo9s.)
Struggling to see the benefit over `nix-shell --run "cabal build"`
I don't think that Dependent Haskell is going to change much for servant, since it's based on type class dispatch.
That's great! Thanks for sharing :D
It'd be nice to keep it in a closed type family -- I only really care about "terminal" endpoints (eg `Verb`, `Raw`, others?) and `:&lt;|&gt;`. Everything else could be collected and distributed to all of the endpoints individually. Opening the type family does let end users customize the behavior for endpoints, too -- you could easily have a `data BlockDistributing` combinator that didn't distribute anything inside of the type.
That's a valid point. But, hopefully with DH we'll either get better ways to do type level coding or nicer ways to continue to use type class dispatch? It'd be unfortunate to have DH and suddenly have all of these great type level tools and then tons of libraries with type-class machinery are stuck doing things in a more cumbersome way.
I think it might be because Haskell bitrots differently than most languages. If you're using a C library, you can be assured that there's going to need to be a steady and constant stream of bug fixes, patches, security updates, etc., but that's not necessarily the case in Haskell. That being said, the ecosystem of Haskell itself seems to change at both a glacial pace and a very rapid pace. As far as slowness goes, the language itself moves very slowly (2010 being the last official edition so far), the compiler moves slowly (although is getting faster), and 90% of the libraries most people use likely haven't seen major architectural updates in 10-15 years; none of these are bad things, of course. But then on the rapid side of things there's monad transformer libraries, the free(r)/extensible-effects collection of libraries, there's a plethora of streaming libraries with quite a bit of work done very recently, and a lot of the libraries that receive the most attention on the subreddit are ones that see lots of new progress in interesting areas: machine learning, automatic differentiation, type-level witchcraft libraries, experimentation with bleeding-edge features of GHC, etc. Quite a few of these bleeding-edge features are used to deliver some quite interesting remakes of libraries: optparse-generic, req over wreq, generic-lens, and generics-SOP just to name a few. All of that ties together in an interesting mix when you throw in the fact that, for many people on the subreddit, Haskell is a new shiny thing to them, so of course the newest and shiniest tools are what many would want to use. I'm quite guilty of this myself :) My personal hypothesis on why it's really common to feel like a library is a bit dated/un-maintained if it hasn't been updated in several years is that there's not a lot of literature on architectural design in Haskell and the tradeoffs of certain choices. Just because a library isn't constantly being refactored to use the latest shiny thing doesn't mean it isn't maintained, after all. But when one's writing a library or refactoring it, several questions crop up: Do you use classy prisms, mtl libraries, or freer and extensible effects? Do you throw as much safety and type-level witchcraft at the compiler as you can? Do you abuse the shit out of generics and newtypes to try and save yourself as much typing as physically possible? Are there tradeoffs one should consider making for performance? Does usability of a library suffer the further from Haskell '98 one gets? This doesn't even begin to get into the *structuring* of the library and the API itself; whether or not to use backpack, qualified imports, certain module structures, lenses/prisms, etc., is an entirely different subject altogether! I suspect many library authors could answer most of these questions for their library and why they've stayed with the design decisions they have; I also suspect that most users of libraries could only vaguely guess at those answers.
First of all, thanks for taking the time to respond to my post. &gt;In one scenario, I have to import Data.Text.Internal, which is explicitly flagged as being unsupported under the PVP and freely subject to change. I can proceed with my code immediately and everything continues to work. Users can continue to build on my code and everything moves on. When you say "Users can continue to build on my code" do you mean the module or the "my code" from the previous sentance that is built on top of this Internal API? Because this is the exact kind of scenario I would seak to prevent: your code is dependant on a part of code very likely to change, and now you have *additional* dependancies that may also use "hidden" parts of your code. Now the author of the original module may notice something really bad about their internals years later but have pressure not to change it because a long line of other libraries are all directly dependant on the parts of the API they didn't actually intend anyone to be using. &gt;In the meantime I'm completely dead in the water. And this is exactly what I was talking about with "In a world with Github". Sure, in years past what you describe was exactly what would happen. Today? If I need to use some library that's missing some crucial functionality I want, I fork it, make the fix I need (attempting to follow the established convensions of that package), send a pull request and in the meantime I can continue working with my forked copy. Of course this carries the risk that the original package author will implement my request in some different way, forcing me to change my code but this is no worse than the situation where I tie my implementation to implementation details of their package. &gt;I wanted to provide a concrete data point of how the proposal you offer just doesn't work in practice. Those are certainly cases where you had issues but my proposal most definitely works. It's used in thousands of companies (maybe hundreds of thousands or even millions?) every single day. It is, in fact, many people (myself included) believe it is the only way programming in the large is acheivable. &gt;I have to import the .Internal gun safe and point it at my foot to use it. And potentially the feet of every user of your code as well. When the upstream maintainer changes the code they always says was Internal and breaks all downstream packages is everyone going to simply accept "well, we always knew it was Internal"? &gt;You aren't getting any safety, just making any hacks I use to get at your guts more error prone. Which I intend to do. If some user is intent on violating the guarantees of my API despite me doing everything I could reasonable do to prevent them doing so then there can truly be no expectation for support from my side. And if that user, themselves, has a long list of dependancies on *their* package which all break on my change, hopefully instead of me getting preasure to roll back my changes, the community will recognize that they should be wary of depending on packages written by someone behaving this way. Aside from programming in the large, I guess the biggest issue I have with your proposal is it puts us in the constant situation of taking the "good enough" instead of the great. Every single time your reach into the Internals of a package, regardless of the reason, you're adding to our collective technical debt. So, yes, I absolutely want to slow down people who are doing that. The last thing I want is technical debt created at blazing speed. &gt;I can make up a new class you've never heard of and then show how your existing data type is an instance. &lt;snip&gt; Most of the time you can't invert dependencies to get big upstream libraries to incur a dependency. And this is another instance of "good enough" vs. great. In my oppinion there is a big weakness in Haskell right now in that to add support for anything I have to incur a dependancy. If I'm writing some new data type then I'd like to offer serialization of my type for any kind of serialization there is: JSON, Database, text, Google protocol buffers, everything. But the way things work right now, to do so would mean forcing any client to incur dependancies on all those packages and everything they depend on. So to work around this issue people making tiny little micro packages which do nothing but incur the dependancies granularly. A better solution would be to offer a way for a library writer to say "if the final build plan includes this dependancy, then enable this code/these files". But there is no pressure to consider such an option because there are workarounds. And every time a workaround is taken, it gets a little harder to actually address the issue. &gt;look up after a month away on vacation to find a package forked on hackage by an impatient consumer that needs access to some functionality now. I consider forking part of standard open source development at this point. Forking and then putting the fork up on hackage with no conversation or pull request (not sure if that was the case in your example), however, seems pretty anti-social behavior. &gt;tl;dr My experience runs very much in the opposite direction. Most of the code I've written in my life has been for enterprises of varying sizes and my experience tells me that exposing internals is just kicking a can down the street. Not a can, more like a snowball that will be ignored until it's an avalanch.
I don't know much about `new-build`. I'm one of the coauthors of `stack --nix` so can speak to that. a) `stack --nix` has hermeticity problems. If you compile with one shell S1 in one project A, that will populate your snapshot cache in `~/.stack`. If you then nuke that cache and do the same in another project B with another shell S2, same thing will happen. But now if you don't nuke the cache, you might get failures because some snapshot packages being compiled against the S1 environment and some others against the S2 environment. So reproducibility isn't as good as it should be. b) the level of granularity in Stack is the Cabal package. If package B depends on package A and you change anything in B, then last I checked A is always recompiled completely. With Bazel, this is less often the case because of the finer grained dependency tracking. Stack could fairly easily fix (a). But at the cost of a lot more recompiles of snapshot packages. Essentially, you would never get any sharing of the compilation artifacts across any two projects any time there is even the slightest difference in their respective shell.nix files. That's because Stack doesn't (and can't) know about which snapshot package depends on which Nixpkgs package in the environment. I anticipate `cabal new-build` would have the same problem.
Both work fine. If you publish your library, it might eventually make it into Nixpkgs. When it does, you could just declare it as a `nixpkgs_package` dependency. Alternatively, if the build system for your library is Bazel based, you could just declare a `git_repository` or `http_archive` dependency for the library in your other project to import the Bazel targets into that other project.
That's a fair point. Maybe Stackage should make working test-suites a requirement for being included.
Nixpkgs handles multi-lang like it handles everything: shell out to some builder script that in turn does lang specific things (CMake, autotools, Maven, Cabal, ...). Totally fine if what you're doing is integrating third-party libraries (as we do in the blog post). But if it's the code you write and that you maintain that has N langs, having to deal with 1 build system instead of N+ can be a real time saver.
Nix can handle multi-lang stuff for your dependencies quite well indeed (although having bazel-grade incremental builds is probably impossible because of the level of granularity it operates on, but that's not always a big problem as long as your dependencies don't change too often) The problem is if you happen to have code in different languages in your repo. In this case it's cabal job to build it, and it really isn't good at that as it is deeply haskell-centric. And that's precisely where Bazel shines.
I did this recently for pretty-sop: https://github.com/well-typed/pretty-sop/pull/1/files
`TypeError` is so awesome.
Thanks, Simon. I will check it what. doctest looks promising. I still may have to figure out how it works with stacks configuration for discovery all dependencies, right?
&gt; Sure, in years past what you describe was exactly what would happen. This was all in the lovely post-github world you're describing. &gt; When the upstream maintainer changes the code they always said was Internal If I depend on `.Internals` I tighten by dependency bounds to the specific minor version. They can change the API all they want, upper bounds keep the contract in place. There is a lot of gloom and doom in your comment about what might happen, but in practice I've yet to receive a single email complaint from changing an .Internal module. The Haskell community, out of all of the communities I've worked with is probably by far the quickest to adapt to API changes, so long as it is making monotonic progress towards a better state and not just caused by thrashing back and forth rolling in and out changes. There are a number of factors that sets Haskell apart here. If I expose the internals of some class that is riddled with mutation and state in C++ or Java, I'm begging for a disaster. Rarely is mutation a concern at all due to immutability, so rummaging around in a Map or something for read purposes is generally not going to end the world. Most of the time invariants are simple and are fairly directly stated algebraically. If I change an API at all retroactively in Python, I create a runtime mess for somebody that crashes their website in production. But in Haskell as long as the change you make actually causes code to break at compile time and doesn't just silently change the semantics of code, what to change is generally an obvious affair. After I put out a new version of `lens`, the world has usually adapted within 2 weeks. Within a few days of a new major compiler version, the rest of the community is already adapted. Most of the time these changes have little to no impact despite being major releases, but this holds even when it is a major release requiring a CPP-based work-around to support across the gap or the like. Better types remove a _lot_ of pain here. The lack of state, general lack of mutation, the fact that typeclasses have global instance resolution properties preserving semantics across large swathes of refactorings and the general high quality of the types involved change the balance of concerns for me significantly. &gt; Most of the code I've written in my life has been for enterprises of varying sizes and my experience tells me that exposing internals is just kicking a can down the street. I spent 20 years before I found Haskell writing all sorts of enterprisy code myself, wherein I maintained much the same view of the world as you. It took me a long time to come around to this view, that works _where Haskell is concerned_ as it contradicted the state of the world as I knew it, as well. YMMV- If you don't want to use an .Internal module somebody exports, don't. Your code will be much safer. If you don't trust code that uses unsafe operations under the hood? Don't. By far the vast majority of my code gets away without ever peeking behind the scenes and doing evil stuff. I don't incur the cost lightly, but it is rather important to me to have the option to be able to take it upon myself.
Coming from JS, this lesson took a bit to unlearn RE : Haskell. There is definitely some shell-shock associated with realizing just how static large parts of the ecosystem are. In JS, typically the responsible dev spends some time researching the dependency and reverse dependency graphs for package candidates to establish whether or not they can count on this being a good solution 6 months down the line. This information isn't as easy to establish on `hackage` as it is on `npm`, and so it leaves those devs used to a more dynamic and interconnected (read : worse) ecosystem with a sense of unease. The realization of just how much different Haskell is to JS is pretty simple, realizing how those differences affect the ecosystem as a whole takes a little longer.
Using [*one-liner*](https://hackage.haskell.org/package/one-liner) to take care of the Generic boilerplate. {-# LANGUAGE AllowAmbiguousTypes #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE DeriveGeneric #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE TypeApplications #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE UndecidableInstances #-} import Data.Type.Equality import GHC.Generics import Generics.OneLiner -- Example data T = C1 Int | C2 Int () Int | C3 deriving Generic main = print (fmap gfold [C1 0, C2 0 () 1, C3] :: [[Int]]) -- Implementation gfold :: forall a x. (ADT a, Constraints a (Match x)) =&gt; a -&gt; [x] gfold = gfoldMap @(Match x) match' class Match' a b (a == b) =&gt; Match a b instance Match' a b (a == b) =&gt; Match a b class ((a == b) ~ aeqb) =&gt; Match' a b aeqb where match' :: b -&gt; [a] instance (a ~ b) =&gt; Match' a b 'True where match' = pure instance ((a == b) ~ 'False) =&gt; Match' a b 'False where match' _ = [] 
That's pretty close, but doesn't handle the case where the type I'm looking for isn't the topmost thing: data T = C1 Int | C2 [Int] deriving Generic main = print (fmap gfold [C1 0, C2 [1, 2]] :: [[Int]]) -- [[0],[]] I can handle the recursion in *this* case by changing the last `Match'` instance like so: instance (ADT b, Constraints b (Match a), (a == b) ~ 'False) =&gt; Match' a b 'False where match' = gfold Which gives `[[0], [1,2]]`, as desired. But this doesn't work with my actual ADT: &gt; gfold (TryTakeMVar (MVarId (Id Nothing 1)) False [ThreadId (Id Nothing 0), ThreadId (Id Nothing 1)]) :: [Test.DejaFu.Conc.ThreadId] &lt;interactive&gt;:22:1: error: * Could not deduce: (Generics.OneLiner.Internal.Constraints' (GHC.Generics.Rep Int) Generics.OneLiner.Internal.AnyType Generics.OneLiner.Internal.AnyType, Generics.OneLiner.Internal.Constraints' (GHC.Generics.Rep Int) (Match Test.DejaFu.Conc.ThreadId) Generics.OneLiner.Internal.AnyType, Generics.OneLiner.Internal.Constraints' (GHC.Generics.Rep Char) Generics.OneLiner.Internal.AnyType Generics.OneLiner.Internal.AnyType, Generics.OneLiner.Internal.Constraints' (GHC.Generics.Rep Char) (Match Test.DejaFu.Conc.ThreadId) Generics.OneLiner.Internal.AnyType, Generics.OneLiner.Internal.Constraints' (GHC.Generics.Rep MaskingState) Generics.OneLiner.Internal.AnyType Generics.OneLiner.Internal.AnyType, Generics.OneLiner.Internal.Constraints' (GHC.Generics.Rep MaskingState) (Match Test.DejaFu.Conc.ThreadId) Generics.OneLiner.Internal.AnyType) arising from a use of `gfold' * In the expression: gfold (TryTakeMVar (MVarId (Id Nothing 1)) False [ThreadId (Id Nothing 0), ThreadId (Id Nothing 1)]) :: [Test.DejaFu.Conc.ThreadId] In an equation for `it': it = gfold (TryTakeMVar (MVarId (Id Nothing 1)) False [ThreadId (Id Nothing 0), ThreadId (Id Nothing 1)]) :: [Test.DejaFu.Conc.ThreadId] 
I thought phabricator had a set of performance tests that were ran on GHC. If I'm right I can't find it but we should put some thought into tests to add. If I'm wrong then I think making such test is a constructive thing to do.
&gt; `*&gt;` is 2x slower than `&gt;&gt;`. Not great. PSA: `-Wnoncanonical-monad-instances` helps detect many of such cases at the definition site.
This is the `nofib` performance test suite that Niklas mentioned in the article. I vaguely recall some talk about extending it to test larger parts of the Haskell ecosystem on a less-frequent basis but I’m not sure anything came out of that.
Writing performance tests which don't depend on libraries is quite a lot of effort. I have tried to contribute some non-trivial examples from my projects but making them standalone is quite a lot of work.
That was a really interesting series. Is the LaTex broken for you, or it a browser extension i'm missing (or an extension i have that's messing it up, maybe)?
Unfortunately, merely having the performance test-suite isn't nearly the whole story. We *do* have a performance test-suite for GHC that evaluates GHC's performance and the performance of its compiled programs; however, currently it has some major drawbacks: * Expected performance metrics are hard-coded into the test definition and must be manually checked over with every single GHC release; not to mention written, updated, monitored, etc., all by hand. * These hard-coded numbers are one singular number that's supposed to encompass the performance for every operating system GHC runs on. Luckily, the numbers are relatively stable due to how GHC works, but acceptance margins of 10-20% are very commonplace. * Not all of the tests work on all of the operating systems; further, not all of the tests are actually run whenever the performance test-suite is actually run, and entire swaths of performance characteristics are simply not measured as it's not feasible to do so. So we have a situation where developing on Windows often means "Run the testsuite and see how many tests fail; try not to break more of them when developing"; where performance regressions of 8-15% can be completely unnoticeable (leading to steady performance creep as versions progress), and so on. Certainly not a desirable state to be in. Luckily, the *correctness* testing in GHC is quite a bit better :) (Note: This state of affairs is *before* my GHC Haskell Summer of Code project to renovate the performance test-suite. Most, if not all, of these concerns should be completely taken care of with the additions I've made to the test-suite last summer. I'm hoping they'll be able to finally be merged in with GHC 8.6, at which point tracking performance in GHC should be able to be done with much tighter windows and even additional metrics should be able to be added fairly easily should the need arise in the future.)
Yes, having types that are values and values that are types is essentially saying that since anything can be a type... everything is. There becomes no difference between type level programming and value level programming in dependently typed systems because they're all at the same level. ("But then, what type do types have?" Well, kinds are the type of types. "But then, what types do kinds have?" Well, in languages that wish to preserve consistency of the type logic, they generally construct a hierarchy of type universes to deal with this paradox. In Haskell, since our type logic is already unsound from the presence of 'bottom', we don't really care if `Type : Type` possible or not so we sidestep that issue completely, collapsing our hierarchy into one single unified level. It's ~~turtles~~ types all the way down)
I totally disagree with that! Often you might define a _lot_ of small helper data types in a module. Each would have to be a class in Java, and thus a file. In Haskell it makes more sense to keep them all within a single module. In my mind modules are about stratifiying the internal dependency graph of your code...
Oh I see. In that case, rather than a single type-equality test, I would use a more ad-hoc class to explicitly handle the various types of fields. Just calling `gfold` is no good because it doesn't apply to primitive types, which are not `Generic`. gfold :: forall a. (ADT a, Constraints a FindInt) =&gt; a -&gt; [Int] gfold = gfoldMap @FindInt findInt class FindInt a where findInt :: a -&gt; [Int] instance FindInt Int where findInt n = [n] instance FindInt Bool where findInt _ = [] instance FindInt () where findInt _ = [] -- Could also be FindInt a =&gt; FindInt [a]... instance FindInt [Int] where findInt ns = ns We can also have a common overlappable instance to have a blanket default for types that don't contain `ThreadId`, but the risk is to miss actual occurrences if we forget to mention one type that does have some. Thus it seems safety requires us to be explicit about how to handle every type. There may be many repetitive cases, for example types that don't contain any `Int`. We can instead put these default cases in a list and check at the type level whether to use that common pattern, without having to declare more instances. That requires a bit more setup so I've put it in a separate paste: http://lpaste.net/363650 
The error message you are getting has to do with the module name of your example module. The fact that it is a `.lhs` shouldn't matter. Try to add the following before any code and imports &gt; module Text.PrettyPrint.Tabulate.Example where The problem is that your Example.hs is a valid module but it defaults to `Main`. The other option mentioned of using doctests is in my opinion even better than giving an example module (do both if you can!). It will check that the output of your examples remain valid even if you change the underlying implementation. Much better than simply relying on type checking. :-) Good luck with your package release!
Excellent stuff! Also, link to the author's new library: https://github.com/nh2/haskell-cpu-instruction-counter 
The testsuite in the library is a test for `sum`: (_, sumInstrs) &lt;- withInstructionsCounted $ do return $! sum [1..10000::Int] Running it with several GHC versions is depressing: GHC version | number of instructions ---|--- 8.4.1 | 90140 8.2.2 | 90140 8.0.2 | 50180 7.10.3 | 40146
Ah, got it, thanks. 
The core looks fine. What's the problem? ``` Main.$wgo = \ (w_s4ML :: GHC.Prim.Int#) (ww_s4MP :: GHC.Prim.Int#) -&gt; case w_s4ML of wild_Xg { __DEFAULT -&gt; Main.$wgo (GHC.Prim.+# wild_Xg 1#) (GHC.Prim.+# ww_s4MP wild_Xg); 10000# -&gt; GHC.Prim.+# ww_s4MP 10000# } end Rec } ```
Well…yeah, I don’t think we disagree. In Java, you wouldn’t make a lot of helper types because classes are often too heavyweight, even inner classes. In Haskell, you make a module approximately wherever you’d make a class or package in Java, in order to provide encapsulation and code organisation, and *in addition* you’d define many more types as needed, because there’s much less overhead to breaking things into more granular, expressive types.
Off the top of my head: - Appending -M for monadic flavors of functions, -T for transformer versions of Monad types - Generic type variables named a, b, c; functor type vars f, monadic type vars m, traversables or monad transformers t - Free variable named x; pattern matching on lists and list construction often uses x:xs - Function-typed variables often named f and g - Type conversion functions are often named fromFoo and toFoo, and conversion is relative to the type that the module is about, so that when you import them qualified, things make sense, e.g. `Data.ByteString.Lazy.fromStrict` - Use underscore for dummy variables - When deriving a value from another, the "new" value has a ' appended: let x' = x + 2 - Local helper functions that merely serve as recursion points or to avoid cluttering the call chain with constants are often named `go` - Functions that wrap other functions "bracket style" to add some sort of resource or context are often named withXXX - In a newtype, the sole field is often written in record style, and named unFoo or runFoo (where Foo is the name of the newtype and the constructor), e.g. `newtype Foo = Foo { unFoo :: Integer }` I don't think there is an official rulebook though, and not everyone adheres to the exact same rules.
Just out of curiosity, why don't you want to use Stack? Is there anything you need improved in Stack?
You might want to take a look at ghcid (or the more general tool, fswatcher) http://hackage.haskell.org/package/ghcid http://hackage.haskell.org/package/fswatcher
Are you saying it's the same core on each version? That leaves either the runtime or the generated assembly. I don't believe the runtime should enter into it as there is no allocation here or I/O or boxed values. So either the assembly generation has changed significantly, or the core itself has changed from version to version.
Lovely, thanks!
I see the leak has (possibly) something to do with conduit. http://www.well-typed.com/blog/2016/09/sharing-conduit/ might be relevant here.
If you can make a very small reproducible test case, make sure you add it to the ghc trac so that the developers know about it. Perhaps if it is a motivating enough case, they can add it to the performance suite.
Is there a ticket for this regression? 
It's extremely good. Anything you'd use today would only be building on the Async and STM stuff the book presents anyway.
What doesn't match? Note that there are new sections "Build failure with GHC 8.4" and "Blocked by GHC 8.4" which list *exclusions* to the build plan. Eventually these sections will be removed and the exclusions will be expressed the usual way, by commenting packages out. It was just too much to do all that upfront, since about 2000 packages were originally excluded.
The prime means "alternate". In a lot of cases, this just means strict, but in some cases it might mean supplying an extra optional parameter. One example are these type synonyms https://hackage.haskell.org/package/pipes-4.2.0/docs/Pipes-Core.html#t:Consumer-39-
The libraries it mentions have experienced some API changes. I found that I was generally able to follow along anyway, but I did have to spend some time mucking around with the examples to find the equivalent modern code. I had a decent understanding of type signatures and how to read typical haskell library documentation when I did so. I would probably term my skill level at the time as somewhere between rudimentary and intermediate - I had whet my teeth on a few small scale CLI tools for stuff like parsing log files. I had next to no experience in the specific domain of concurrent or parallel Haskell programming. Advanced Haskell knowledge is definitely not necessary to understand the book, but probably it's not great as a 100% cold introduction to the language.
More machine instructions /= slower. Often more instructions is faster because of pipelining.
You can certainly add constraints / type variables into the mix. I threw [this](https://gist.github.com/dalaing/39b92cecd48ddb41ec7306080d981234) together, and it performs as well as the other final encoding approaches. If you wanted to adapt that to the pretty printer and keep the types lining up, you could use something akin to `type Repr a = Tagged a String`. Although if being tagless is important to you then depending on `tagged` might not please you too much :) Doing it with the initial version _might_ be possible, but it will require a bit of fancy footwork to make the rules systems go. For the most part I've been using that to define combinable fragments of langauges - parser / pretty printer / semantics-derived step function / type checker / property based testing generators. So on the initial encoding side I've been playing with deeper embeddings than I'd normally tackle with GADTs and building my own type systems for my terms along the way, so sorting out a rules system that is compatible with this hasn't been a priority. I _have_ added phantom types to the helper functions where I can, to keep things lined up by construction. The tagless-final examples I'm familiar with come from Oleg's work, and from memory the stuff I read there was all in Haskell. I'm not familiar enough with the day-to-day realities of working with the OCaml module system enough to speak with authority about the differences between OCaml modules and Backpack - hopefully someone with the right background drops by and enlightens us :)
Ignoring the regression you point out for a moment, it's nice that the results for 8.4 and 8.2 are precisely identical. It gives a relatively high confidence that 8.4 did not regress compared to 8.2, and even makes it somewhat likely that the generated code didn't change at all. I think that gives a lot of confidence when making changes, and is exactly what I was aiming for. I would love to see these numbers also in `nofib` / gipeda results / criterion outputs.
This year's "24 days of" blog series should be on compiler flags such as this.
&gt; More machine instructions /= slower. Often more instructions is faster because of pipelining. Correct, sometimes that can be. I like the following approach: More machine instructions == something changed, then check if it's slower. `perf stat theprogram` will then immediately tell if unrolling/pipelining/[strength reduction](https://en.wikipedia.org/wiki/Strength_reduction) is in effect, e.g. in `insn per cycle` -- which is a better metric for actual performance, but not as stable across machine or under varying load, and thus not as suitable directly in regression tests (instruction count is even _deterministic_ in this case). /u/sjakobi, do you know if it's slower / can you run `perf stat` on it?
**Strength reduction** In compiler construction, strength reduction is a compiler optimization where expensive operations are replaced with equivalent but less expensive operations. The classic example of strength reduction converts "strong" multiplications inside a loop into "weaker" additions – something that frequently occurs in array addressing. (Cooper, Simpson &amp; Vick 1995, p. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
If you're on Linux, compile with LLVM (`-fllvm`). Maybe try all the Repa recommended flags: -Odph -rtsopts -threaded -fno-liberate-case -funfolding-use-threshold1000 -funfolding-keeness-factor1000 -fllvm -optlo-O3
&gt; Use underscore for dummy variables What do you mean by that one? I usually use "foo", "bar", and "baz" for dummy names, and you have used "Foo" too so that's clearly not what you meant.
Any updates /u/Vaglame?
Adding a prime to a name is a pattern that is used in standard libraries, but I personally think that primes should be avoided in production code. Visually they're just not distinct enough. I actually know someone who has encountered a bug in production directly caused by a mistake related to a primed name. I might be willing to accept it in situations where the primed symbol has a different type from the non-primed symbol, but it's such an easy practice to avoid that you might as well make it an across-the-board practice.
I have tried to make a small case, but couldn't reproduce it. By starting with the whole code and simplifying it, I have been able to keep the effect. I have now cut it down even further (400 lines of code). Maybe tomorrow I can get it down to 100 lines or so, but I have seen examples where cutting too much can lead to the effect disappearing (i.e., GHC optimizing well).
My personal preference is to put my examples / tutorials in the test suite as literate haskell (essentially your method #3). I do this in [heist](https://github.com/snapframework/heist/blob/master/test/suite/Heist/Tutorial/CompiledSplices.lhs) (as well as the [armor package](https://github.com/mightybyte/armor/blob/master/test/AppA.lhs) that I just announced today) and then use pandoc to convert it into the [actual tutorial served on snapframework.com](http://snapframework.com/docs/tutorials/compiled-splices).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [snapframework/heist/.../**CompiledSplices.lhs** (master → 8f4ec7c)](https://github.com/snapframework/heist/blob/8f4ec7c490901ea4cab60279291365db30d16860/test/suite/Heist/Tutorial/CompiledSplices.lhs) * [mightybyte/armor/.../**AppA.lhs** (master → 92fcb97)](https://github.com/mightybyte/armor/blob/92fcb97d64eab098580c14d7cda88b592b8016db/test/AppA.lhs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvs0lcw.)
Take a look at https://perf.haskell.org/ghc/
I think it’s like the implementation of the length function. You don’t need the x from (x:xs) so you write (_:xs) instead.
Redditer for 20 hours, one post, and the link doesn't even have anything to do with Haskell? \*Error: Post does not type-check\*
That's what I thought too; I'd probably say "unused/unbound" instead of "dummy", though, but that's just an english wording nitpick :)
&gt; One thing I cannot do (to keep the odd behaviour) is put everything in the same file That, to me, sounds to me like inlining issues. Have you tried squinting at the core with the files separate vs the core with everything in the same file?
The original "Finally Tagless, Partially Evaluated" [paper](http://okmij.org/ftp/tagless-final/JFP.pdf) presents both OCaml and Haskell code, and a subsequent [paper](http://okmij.org/ftp/meta-programming/quel.pdf), "Finally, Safely-Extensible and Efficient Language-Integrated Query", presents primarily in OCaml.
I think I originally learned the technique from the [lecture notes](http://okmij.org/ftp/tagless-final/index.html#course-oxford), although I've read a bunch of stuff in both Haskell and OCaml since then. I kind of had it all on the backburner until it came up in the context of [session types](http://homepages.inf.ed.ac.uk/slindley/papers/gvhs.pdf), and via that paper, in the context of [linear lambda calculus](http://functorial.com/Embedding-a-Full-Linear-Lambda-Calculus-in-Haskell/linearlam.pdf) - although I suspect that Oleg got there first :)
&gt; `sum [1..10000::Int]` I'm just wondering how come GHC doesn't recognize that this is a constant. 
Oh, right... "foo" and "bar" are used here as placeholder names for real ones; by dummy variables, I mean variables whose values are never used, but they need to be there because the code would not otherwise typecheck. Typically, this would be in a pattern match, e.g. matching on `(a, _)` when you are only interested in the first tuple element. This one is actually not purely a stylistic choice; you could use normal variable names, but unlike those, underscores are not considered an identity, and using multiple underscores does not create duplicate, shadowed, or conflicting bindings.
Wow, what an exciting opportunity! So cool to see that Tesla's looking at Haskell.
Plus you silence the warning.
Is this the number of generated instructions or the number of executed instructions?
I had a closer look at your algorithm. If I'm reading this correctly, there is something (not related to Haskell, but related to the algorithm) which may dramatically improve the efficiency. You pick light contribution into account only when you hit an emission object. The density of probability of this event is so small that you get close to black image with 1 sample per pixel. The idea is to do what is called "next event optimization", where you sample a point on the light and connect it to your current point with checking for shadows (something you are apparently doing in `raycast` method). This will get you a close to clean image with only one sample per pixel. 15s for 1 sample per pixel (with no parallelism) is not good, but not awful. How much time does it take with a really simple scene (just the outer box) and no BIH ? &gt; Also, thanks, I wasn't aware of cache thrashing, so I'll look into that. Would the solution be to store the tree in a vector? That's what we do in C++.
I'm a bit confused. Why can people not just upgrade to `transformers-.5.3.0.0`? There was a thread about this recently but I've forgotten. I will try to dig it out later unless anyone gets there first. 
If they use `cabal new-build`, they can quite easily *unless* they have `lib:ghc` in their install-plan. For the latter limitation, I've devised https://mail.haskell.org/pipermail/ghc-devs/2017-July/014424.html but I missed the window for GHC 8.4 to make it happen (...and it doesn't help that we have to maintain two build-systems in parallel).
Sure you can upgrade transformers (although the fact that stackage doesn’t upgrade transformers until there is a new GHC release means that lots of people will keep using the old version for quite some time without being aware of the problem). The point the author is trying to make is that if `transformers` had a performance test suite than this bug would have been noticed immediately and could have been found and fixed before rolling out a new GHC release with a significant regression in `transformers`. And the same holds for other packages, the bug in transformers has been found and fixed but who knows how many other packages there are that have regressed without anyone noticing it since there is no performance test suite.
Sure, putting types and values on the same level does imply a shared language, but a shared language certainly doesn't imply that they are on the same level. If the current type families didn't have a separate syntax, but were instead defined the same way that value-level functions are, that wouldn't magically make things like `Int -&gt; Type` possible.
I agree although I don’t think that enabling `-Wcompat` really removes the need for performance test suites in libraries. At best, you can add warnings for things that you know will (potentially) regress but you obviously won’t have warnings for accidental regressions.
Looks useful. Am I responsible for incrementing version numbers myself?
Sure, *in general* you'd still want performance monitoring; but in this particular case it's quite old news, as we *knew* about the potential for this class of performance regression upfront, and that was one of the motivations of the MRP as well as implementing that warning flag. I'm just saying that the presented case is a bad example, as we had already other means in place to prevent it, but sadly failed on the marketing side of it (which I'm partly responsible for).
How come Tesla doesn't accept international students for internships? My guess would be that the Haskell community is small as it is :D
https://github.com/sol/markdown-unlit
GHC doesn't, but LLVM will if you use `-fllvm` when you compile.
PSA: Make sure to include something like [`build-tool-depends: markdown-unlit:markdown-unlit == 0.5.*`](http://cabal.readthedocs.io/en/latest/developing-packages.html#pkg-field-build-tool-depends) in your `.cabal` file to make sure cabal knows which version of `markdown-unlit` your needs; it's often missing and thus causes build failures.
For what it's worth, `{-# OPTIONS_GHC -fno-full-laziness }` fixes this example, but I don't know what is shared or how to selectively prevent GHC from doing so.
Just wondering, did anyone measure the impact of using continuation-style evaluation?
&gt;This was all in the lovely post-github world you're describing. I realize that, but I wonder why you didn't choose a solution like I described? Forking itself is not offensive. Forking and then trying to steal the project (e.g. by putting it up on hackage) would be but that's not necassary. The only reason I can think of is if you're providing a library that itself has dependancies. Which means whatever breakages you're exposing yourself to are exposed to all your downstream as well. &gt;There are a number of factors that sets Haskell apart here. But don't you think one of those factors, perhaps the driving factor, of the situation you're describing has to do with size? Imagine if Haskell had the user base of Java. Do you think the situation you describe wouldn't change dramatically then? I mean, maybe due to strengths of the languages it really wouldn't but I personally find that hard to believe. &gt;But in Haskell as long as the change you make actually causes code to break at compile time and doesn't just silently change the semantics of code, what to change is generally an obvious affair. Absolutely. And the power of the language and the advantage we get by using it means that kind of breaking change is extremely unlikely regardless of the size of the code base. But this isn't the only concern. My concern is that X years into the future, if Haskell were to get exremely successful then an upstream changing someting in an internal module that causes a cascade of breakages down 50 packages and perhaps hundreds of thousands of production applications is still catastrophic damage. Not on the order of what can happen in more dangerous languages but it at some point I don't think it will be a "in 2 weeks everyone was up to date" situation and even if it were 2 weeks would be too expensive. I'm not trying to be combative or overly dramatic here but my conern is entirely for well into the future and deep down I wonder if there are people who don't implement in Haskell because of these kinds of "programming in the large" concerns. I understand you that you have changed your view after years of exeperience but not everyone will give Haskell years to convince them that these kinds of practices are ok. Haskell's already asking a lot without expecting people to ignore everything they ever learned about how and why of incapsulation. It's simply a trade off, and for me I just don't see how I'm getting enough out throwing away encapsulation to justify even the bad reputation of doing so. Even if it never actually causes a production outage, a lot of very experienced people are going to look at our various modules and how encapsulation is a mere convention and conclude "toy language". That might be a price worth paying if I got something from it, but from my perspective I get very little from it because I can just temporarily fork the project, make a pull request and continue working. The only pain point is it takes me a little longer on libraries I maintain but I'd rather invest the time on getting a correct solution (which may be, in some rare cases, a permanent fork) than to do something I know to be wrong that must be revisited at some point in the future. &gt;Stackage effectively provides us with wide-scale integration testing, and travis, etc. are there to mitigate much of the remaining concern. I agree, quality-wise Haskell is in a powerful position. But any weakness is just a question of scale. &gt; If you don't want to use an .Internal module somebody exports, don't. Sure, but I'd rather not have to check every single dependancy my code depends on *to see if any of them do*. &gt;I don't incur the cost lightly, but it is rather important to me to have the option to be able to take it upon myself. Me too, which is why I'd advise we only support projects for which we have the source code and prefably a repository that features the cheap fork/pull request model.
Indeed, at least the Cloud Haskell interface changed a bit since the book was published. Re. its suitability as intro material: the first chapter contains one of the best discussions on lazy evaluations I've ever seen, so highly recommended for beginners too.
&gt; Forking itself is not offensive. Forking itself doesn't let me upload the project to hackage. In Haskell post-2006 this is basically a way to ensure that the code I write gets zero eyeballs on it. &gt; But don't you think one of those factors, perhaps the driving factor, of the situation you're describing has to do with size? I have a much harder time dealing with these sorts of issues in considerably smaller codebases for still smaller audiences in Scala, nominally in the same style, due to the worse language.
Another example is [*lens*](https://hackage.haskell.org/package/lens-4.6.0.1/docs/Control-Lens-Type.html), with `Lens` and `Lens'`, `Prism` and `Prism'`, etc.
It "reached maturity".
The asymptotic complexities of the operations named in `Data.Map` take the cost of a comparison to be O(1). Beyond that there is an assumption that the `Ord` instance is properly transitive, anti-symmetric, and reflexive. (Building maps with NaN's and -0's as `Double` keys can lead to some wonky scenarios!) There isn't a requirement that the ordering actually "make sense" to users in any meaningful capacity other than it affects the semantics of `fromDistinctAscList` and a few things like that.
This book remains just as amazing.
He means 1024-bits, most RSA keys are 2048 or 4096 bits.
Assuming the numbers are what what I used in the blog post and what my library emits, this is as in the output of `perf stat`, which is the number of _executed_ instructions.
I agree. I think if you use `-Wall` (and lots of people do know and use that one) you're really asking the compiler to warn you as much as possible about things that can go wrong, and excluding useful warnings like that from `-Wall` means we're not giving the user what they asked for. There seems to be a small set of people that believe that not seeing warnings will save them time and effort, but that has never been my experience. For me not seeing a warning usually means hours of debugging something that the compiler already knew but didn't tell me.
There's a branch of tensorflow-haskell at https://github.com/fkm3/tensorflow-haskell/blob/dqn/tensorflow-ops/src/TensorFlow/RNN.hs#L42 which defines GRU's. Last time I tried it, though, training was *really* slow compared to running the standard "shakespeare" examples from python tensorflow, I've no idea why.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [fkm3/tensorflow-haskell/.../**RNN.hs#L42** (dqn → 80f1eda)](https://github.com/fkm3/tensorflow-haskell/blob/80f1edab29fa01e5f46c8853722d9eacd2f0d5e1/tensorflow-ops/src/TensorFlow/RNN.hs#L42) ---- 
The post has a continuation based evaluator in it, along with benchmarks and the generated core. I considered playing around with a Church encoding, but I wasn't quite sure how to get into a form where I could use it in the context of the expression problem. I should probably think about that a bit harder at some point :) If there's some other continuation-style approach that exists in the folklore and you have a link describing it, I'd love to hear about it and can add it to the set of benchmarks. 
I think that captures the issue.
Ah, here's the rationale https://www.reddit.com/r/haskell/comments/7xqfy8/space_leak_with_nested_strict_statet/duc8kb0/ 
&gt; Looks useful. Am I responsible for incrementing version numbers myself? Yes, that was my original thought. But I didn't think it was too much of a problem because armor will see that a file for the serialization of that version already exists and won't overwrite it. However, on thinking about it again it seems like there may be a chance of not failing if you modify a data type, but fail to update both the deserialization code and the version...not sure. &gt; Seem useful if a Word64 hash could be derived from the structure of the Generic instance. Hmm, I hadn't thought of that. That might be a nice idea. However, if you use only the hash, then you don't have an ordering which is used to go back only a certain number of versions as seen here: https://github.com/mightybyte/armor/blob/master/src/Armor.hs#L70 But I definitely agree that it would be nice to catch this in all situations. Perhaps we could use a hybrid approach where you specify a version AND the hash gets stored in the file giving us the ability to fail in the above error situation.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [mightybyte/armor/.../**Armor.hs#L70** (master → 4dabd71)](https://github.com/mightybyte/armor/blob/4dabd71e6a52e834b1e9efc73c2c044c2dfe958f/src/Armor.hs#L70) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvsiini.)
Thank you everyone! Through this discussion, I found an answer: it's about over-sharing and `-fno-full-laziness` works around it. This workaround also woks in the original code, so I fixed it: https://github.com/luispedro/ngless/commit/e482afbfb49ab3284d5454f95442243018f89a4b
tldr; How to use smart constructors with sum types? Hi Oscar. Thank you for your work. I have a question about smart constructors. As far as I understand smart constructors is a trade-off where you sacrifice pattern matching for the sake of preserving desired invariant. I say that because if a module doesn't export constructors then one who uses the module can't use constructors for pattern matching. With simple single-constructor data types, the absence of pattern matching doesn't produce any discomfort. But with sum types that could be a problem. A quick example is Either which tries to preserve invariant for both Left and Light values. Using Either without pattern matching is not safe programming practice so I would probably mirror Either data type and will end up with something like CheckedEither (which holds invariant) and Either (which potentially doesn't hold invariant). Probably similar behavior can be implemented using "phantom" parameter in a data type which tells whether invariant is guaranteed to hold or not. I would appreciate if you will add more details and will guide us with best practices in using smart constructors with sum types. Thanks.
It is very clear, enlightening, and fun (or at least, never dry)! It is a paragon of good programming books. And it has exercises!
This is awesome and exactly what I needed. I have started using the library and creating my documentation and I love it. Thank you very much for this library!
What happens if the garbage collector runs during the code being benchmarked?
Why would an unrolled loop _execute_ more instructions?
How have you tested it? the time it takes to find a prime is inherently random, sometimes it takes 0.7s, others 0.2s on my machine. You can save some speed (assuming ghc isn't optimising this out) in millerExps, constructing lists is expensive. millerExps xxs@(x:xs) = if x `mod` 2 == 0 then millerExps $ (x `div` 2):xxs else xxs millerTest :: Integer -&gt; Integer -&gt; [Integer] -&gt; Bool -- miller rabbin test millerTest _ _ [] = False millerTest a n (x:exps) | modexp == 1 = True | modexp == n-1 = True | otherwise = millerTest a n exps where modexp = modExp a x n genWrapper :: Integer -&gt; Integer -&gt; IO [Integer] -- generates 100 random numbers genWrapper up down = do g &lt;- getStdGen return . take 100 $ randomRs (down, up) g You can also check out: https://hackage.haskell.org/package/criterion to help you benchmark.
There it was largely because all of the alternatives like `SimpleLens` etc were horrible. We converged on the `'` convention as it was the least invasive, despite the conflict with the strictness usage.
This looks super useful, and has come at a really good time for several projects of ours - we've recently had one project which broke in production because a core data types changed, with derived Aeson instances, which caused the front end to become out of sync with the backend. The solution was to produce Aeson instances which used type errors as constraints to stop people from directly serialising the core types in the app and instead use versioned serialisation instances wrapping those types. Having these sorts of tests would have caught that problem a lot sooner. I think the approach taken by `safe-copy` is quite good, but a little tedious - I'm not sure how it would best be automated (and can't even remember at this point what it was that I wanted to automate). On a side note, it would be awesome to see acid-state more actively developed, there's a hell of a lot of good there but it could also do with an update for more modern Haskell practices I feel.
If this is nothing more than a `HasStackTrace` constraint, then it should be a relatively minor thing to add to any partial function in `Prelude`/`base` - it feels like there could be a slight negative performance impact (just a gut feeling) but this actually serves the goal of teaching Haskellers not to use partial functions, so this is probably one of the few times you'll ever hear me happy about making code slower (or not as the case may be, but if this rumour starts here, I'm happy with that too)!
I got it from the discussion here and Roman Cheplyaka mentioned it on twitter: https://twitter.com/highfiveprime/status/974552539617112069
The post was pretty young when I checked the link the first time. It was about data streams, not Haskell
This is likely partially funded with a government Grant which usually only works for certain geo-regions. Either that or the green card process is too expensive for internships for them? 🤷‍♀️
Install `lens`, `import Data.Data.Lens` and `import Data.Set.Lens`, then you have access to biplate :: (Data s, Typeable a) =&gt; Traversal' s a to use `Data` to find anything in a larger type, with that tidsOf = setOf biplate replaces the body of your function. If you really want to use `Generic` instead of `Data`, old versions of lens used to export an operation called `tinplate` from `GHC.Generics.Lens`, but it is currently not in the package, as it is subtly different than the `Data.Data.Lens` version of the idea. The code can be readily resurrected.
I think this book s probably my favourite Haskell book, it does such a fantastic job of setting out the problem, showing you the solution, explaining how the solution works (STM, sparks, exceptions, etc.), and explaining caveats. IMO it should be used as an example of how to write a good technical book, it is not too complicated, does not dive into academic explanations of notations, in plainly written and it is clear Simon is an expert in the subject - he ought to be, he wrote implementations most of the topics in the book! 5/7, a perfect score.
Thanks for the tip. That helped.
As a foreigner that had to deal with the immigration process: I'm not sure the U.S. has a visa program for interns. They have F1 for international students, H1B for professionals. Never heard of an intern visa.
&gt; * has written a 1000+ line program in Haskell &gt; * You must be a current student in good academic standing enrolled in a relevant program and actively working toward your undergraduate or advanced degree. &gt; * North American resident I hope both of the eligible students apply.
thanks for letting me know. will delete now
Yup, I can picture how horrible the alternatives would be.
Wow, it's so cool to see Brick listed here! If Tesla has Brick-based open-source projects, please let me know and I can add them to the Brick Featured Projects list!
I've used `safecopy` before in production. The problem with its migration approach is that it requires you to keep around both the old type and the new type in order to define the function `migrate :: MigrateFrom a -&gt; a`. I found that this gets really unwieldy after awhile. Armor keeps around old copies of data to avoid forcing you to keep around old copies of code.
Thanks! Good for Roman, spreading the word.
I kinda liked type Simple l s a = l s s a a but using it with `Simple Lens` required the user to turn on `LiberalTypeSynonyms`, and 99% of users didn't even know that extension existed!
Where was this internship when I was a student? Guess it's back to school for me.
&gt; I initially wrote the library using `Data.Tagged` but it wasn’t poly-kinded https://github.com/ekmett/tagged/blame/master/src/Data/Tagged.hs#L6 It has been in there for ~6 years now. =)
I have been meaning to make a backpacked version of `linear` that unpacks whatever user type you want, this may have been the comment that finally pushed that project out of my backlog.
Multiple friends of mine had internships in the USA, so this should not be the problem.
Agreed. I try to only use prime for local scope, as it won't be called directly, for example: someFunction myMap = someFunction' where someFunction' = ... 
J-1
Haskell idioms and libraries have evolved a bit, but I can't see this book going out of fashion because concurrency architecture remains one of the most compelling foundations of the GHC runtime architecture. 
42% off in "Get Programming with Haskell" http://freecontent.manning.com/slideshare-a-friendly-approach-to-functional-programming/
Instead of the prime, I usually create a function called `go`. ``` someFunction m = go where go = ... ```
Macro benchmarks like that are less likely to show changes in single, or small sets, of components. For example - such a benchmark probably wouldn't reveal the `forever` performance bug discussed in the article while the targeted microbenchmark reveals a 100% increase in computational cost.
Just using this comment to get on my soapbox and claim that if you're going to allow `_foo` to suppress warnings on lack of use then you should warn, or even error, if it *is* used.
I've put the code [on github](https://github.com/alpmestan/servant-flatten/blob/master/src/Servant/API/Flatten.hs#L13).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [alpmestan/servant-flatten/.../**Flatten.hs#L13** (master → 8e93bcc)](https://github.com/alpmestan/servant-flatten/blob/8e93bcca16c37758504808d5d59c4d5c83107748/src/Servant/API/Flatten.hs#L13) ---- 
Looking back at this post, I see my counterexample is incorrect, because I misunderstood the data type: you meant an actual isomorphism, with the property that `bToA . aToB === id`, not just any pair of functions. My counterexample does still explain why djinn can't solve it, though: djinn doesn't know about that property, and therefore has to worry about values like my `vacuous` even though you would argue they're illegal. However, there is another argument that I think works for your intended goal. Specifically, for two types to be isomorphic, they must have the same number of possible values. `Bool` is isomorphic to `Maybe ()`, because there are two possible values of each, but neither is isomorphic to `()`, or to `Maybe (Maybe ())`. Thus, to prove that no value of type forall a b. Iso (Iso a b) (Iso (Maybe a) (Maybe b)) can exist, it is sufficient to prove that `a -&gt; b` has a different number of distinct implementations than `Maybe a -&gt; Maybe b` does, provided that `a` and `b` have the same number of distinct values (i.e., are isomorphic). We don't need to worry about the `b -&gt; a` mapping, because it is chosen uniquely by the `a -&gt; b` mapping in an isomorphism. This is easy enough to do: since `a` and `b` are the same size and we aren't allowed to repeat results (else we lose the isomorphism property), an isomorphism from `a` to `b` is the same as a permutation of `a`, and there are `a!` ways to do that. Likewise, an isomorphism from `Maybe a` to `Maybe b` is a permutation, but with one more element included: there are `(a + 1)!` mappings. Since there are more isomorphisms in the `Maybe` domain than the unlifted domain, there cannot be an isomorphism across domains.
To avoid various forms of blindness I'd probably use takeWhileRight :: MonadWriter w m =&gt; (a -&gt; Either w b) -&gt; Stream (Of a) m r -&gt; Stream (Of b) m () Then you know for sure that only "bad" elements can be written to the writer and you know that only "good" elements can be written out into the stream. 
To what extent is this problem solved by `SmallCheck`?
Sorry for the delay, I caught a nasty cold. :( I have extracted the summing code into the following program: module Main where main :: IO () main = do _ &lt;- return $! sum [1..10000::Int] return () I compiled the program with `-O2` and then `bench`ed and `perf stat`ed it. Results with the 4 GHC versions from above: ghc-8.4.1 ----------------------------------------- benchmarking ./sum time 903.8 μs (897.2 μs .. 912.5 μs) 0.999 R² (0.999 R² .. 1.000 R²) mean 888.8 μs (880.4 μs .. 895.9 μs) std dev 25.64 μs (20.65 μs .. 34.14 μs) variance introduced by outliers: 18% (moderately inflated) Performance counter stats for './sum': 0,990995 task-clock:u (msec) # 0,774 CPUs utilized 0 context-switches:u # 0,000 K/sec 0 cpu-migrations:u # 0,000 K/sec 129 page-faults:u # 0,130 M/sec 790.273 cycles:u # 0,797 GHz 602.043 instructions:u # 0,76 insn per cycle 137.811 branches:u # 139,063 M/sec 8.613 branch-misses:u # 6,25% of all branches 0,001280649 seconds time elapsed ghc-8.2.2 ----------------------------------------- benchmarking ./sum time 899.7 μs (893.4 μs .. 904.4 μs) 0.999 R² (0.999 R² .. 1.000 R²) mean 885.8 μs (877.4 μs .. 895.2 μs) std dev 29.63 μs (23.16 μs .. 39.94 μs) variance introduced by outliers: 23% (moderately inflated) Performance counter stats for './sum': 0,550266 task-clock:u (msec) # 0,763 CPUs utilized 0 context-switches:u # 0,000 K/sec 0 cpu-migrations:u # 0,000 K/sec 132 page-faults:u # 0,240 M/sec 924.276 cycles:u # 1,680 GHz 595.413 instructions:u # 0,64 insn per cycle 136.074 branches:u # 247,288 M/sec 8.229 branch-misses:u # 6,05% of all branches 0,000721090 seconds time elapsed ghc-8.0.2 ----------------------------------------- benchmarking ./sum time 886.4 μs (870.6 μs .. 899.7 μs) 0.998 R² (0.998 R² .. 0.999 R²) mean 871.8 μs (856.6 μs .. 880.2 μs) std dev 39.09 μs (27.10 μs .. 55.06 μs) variance introduced by outliers: 36% (moderately inflated) Performance counter stats for './sum': 0,552983 task-clock:u (msec) # 0,754 CPUs utilized 0 context-switches:u # 0,000 K/sec 0 cpu-migrations:u # 0,000 K/sec 129 page-faults:u # 0,233 M/sec 902.456 cycles:u # 1,632 GHz 583.279 instructions:u # 0,65 insn per cycle 133.661 branches:u # 241,709 M/sec 8.135 branch-misses:u # 6,09% of all branches 0,000733017 seconds time elapsed ghc-7.10.3 ----------------------------------------- benchmarking ./sum time 880.4 μs (858.0 μs .. 895.0 μs) 0.996 R² (0.992 R² .. 0.999 R²) mean 879.7 μs (868.1 μs .. 888.4 μs) std dev 34.49 μs (23.03 μs .. 47.05 μs) variance introduced by outliers: 29% (moderately inflated) Performance counter stats for './sum': 0,830470 task-clock:u (msec) # 0,779 CPUs utilized 0 context-switches:u # 0,000 K/sec 0 cpu-migrations:u # 0,000 K/sec 136 page-faults:u # 0,164 M/sec 793.064 cycles:u # 0,955 GHz 567.207 instructions:u # 0,72 insn per cycle 122.826 branches:u # 147,899 M/sec 8.334 branch-misses:u # 6,79% of all branches 0,001065410 seconds time elapsed Sadly I'm getting quite different results with each run. Here's the fish script that I used: for c in ghc-8.4.1 ghc-8.2.2 ghc-8.0.2 ghc-7.10.3 echo $c "-----------------------------------------"; eval /opt/ghc/bin/$c --make -O2 -fforce-recomp sum.hs &gt; /dev/null; ~/.local/bin/bench ./sum; perf stat ./sum; end 
SmallCheck isn't really doing the same thing. It does handle automatic generation of values based on the types, but it's doing exhaustive testing to a certain depth. Depth doesn't seem to be related to the notion of full constructor coverage that I'm talking about. Here are some examples. The `MyError` type from the blog post: λ mapM_ print $ list 2 (series :: Series Identity (MyError Int Bool)) Failure 0 Success True Failure 1 Success False Failure (-1) Something close to the `Person` type from [the test suite](https://github.com/mightybyte/fake/blob/0c95bc2f8fb6288551f10d1b47d46d2a85166047/test/Main.hs#L85): λ mapM_ print $ list 2 (series :: Series Identity Person) Person {personFirstName = "", personLastName = "", personAge = 0, personSSN = Nothing} Person {personFirstName = "", personLastName = "", personAge = 1, personSSN = Nothing} Person {personFirstName = "", personLastName = "", personAge = -1, personSSN = Nothing} So using a depth of 2 doesn't cover all the constructors. Using a depth 3 does, but it returns a list of 90 values.
That's a really good question. It makes a huge difference, and that is very definitely worth pointing out. I'll dig up that version of the code and add it to the benchmarks on Monday.
I suspect there is some context missing here and you used `readFile "db.txt"` somewhere prior to your `saveFiles` function. You are probably having a bad run-in with "Lazy I/O" right now. If you want to go down this rabbit-hole, searching this subreddit is a good start. But I presume you just want to fix this as soon as possible, for which there is a number of ways. * Use strict I/O via `Data.Text.IO` or `Data.ByteString`, * Use `Handle`-based I/O via `System.IO.withFile` and the corresponding functions from the `text` or `ByteString` packages, * Use a streaming package like `pipes/conduit/streaming/...`. The last one only applies if you really can't afford to hold your `Film`s in-memory. The strict functions from `text` or `ByteString` (I don't know what kind of data we're dealing with here) is probably easiest. With Handle-based I/O, you have to take care about not accidentally `return`ing a `Handle` from an I/O action, otherwise I found this works quite nicely as well, especially for things like writing on a line-by-line basis, things like that.
I've been eager to port some awkward typeclasses over to signatures, since I heard about them, but didn't know the right way how. Thanks for this article! 
Whoops, you're right, I must have misremembered. I think I couldn't figure out how to use it at the time, and the `Data.Proxy` approach ended up with a cleaner API anyway because there was less wrapping/unwrapping in `Tagged`. But I just tried again with `Data.Tagged` and it works fine. Just edited the blog post.
[removed]
Thinking about this a little more...if you were going to use a hash, what all would be included in the hash? Would field names be inputs? For serializations that do not include the field name, you probably wouldn't want the field name to be included. For serializations that do include the field name, maybe you do want them included in the hash. So at the moment I think my conclusion is that armor should stick to using user-supplied version numbers.
You _can_ have that in your own code! `gipeda` which produces the output, and `feed-gipeda` which automates the performance testing are both on hackage :-) * https://hackage.haskell.org/package/gipeda * https://hackage.haskell.org/package/feed-gipeda
Looks fine to me. A few minor things: * You don’t need `_ &lt;- …` when the result of an action is `()` * Instead of using `writeFile` with `Text.unpack`, you can use `writeFile` from `Data.Text.IO` * Parentheses are unnecessary in `(replaceImage . replaceWorkingDir) $ …` I also tend to use *gaps* instead of `unlines` for multi-line strings—a pair of backslashes separated by whitespace is stripped from a string literal: "version: '3'\n\ \services:\n\ \ app:\n\ \ …" But that’s mostly a matter of style, and IIRC some editors don’t syntax-highlight it very well, particularly if a string ends with a gap—a workaround is to end the string with `\&amp;`, an escape that just acts as a separator, producing no character.
Instead of using String for commandToRun use a sum type. See examples of opt-parse-applicative for how you could use that library or how a sum type could be useful for you.
Thank you for your feedback, giving those things a try! :)
Here is a quick but not optimal example i found quickly: https://rosettacode.org/wiki/Globally_replace_text_in_several_files#Haskell
I see. By the way the docs of "fake" note that it provides "An analog to QuickCheck's Arbitrary and Gen that use realistic probability distributions rather than the more uniform distributions used by QuickCheck." Can you give more details on that?
How about the bind operator (&gt;&gt;=) is that something that should be used somewhere?
Not if you're using do notation
Ok! I will see if I can find some nice article explaining that for me. Thanks!
I don't do a lot of streams, but `MonadWriter` doesn't say "stop on error" to me. If it weren't for the name, I'd guess from the type signature that this was some sort of unzipping transform that `tell`'ed all the `Left` values rather than stopping. Would the `Streams` type lend itself to something like mapUntil :: (a -&gt; Either e b) -&gt; Stream (Of a) m r -&gt; Stream (Of b) m (Maybe e)
Not in your code, I'd say. Readability-wise, `(&gt;&gt;=)` works well when the corresponding do-block would be simple enough that using `(&gt;&gt;=)` doesn't make it tricky to understand -- for instance, this... foo x &gt;&gt;= bar ... is arguably an improvement over this: do y &lt;- foo x bar y
I’ll try, thanks! :)
data Colour = Red | Green | Blue The number of possible values is the sum, 3, as opposed to a product type like a tuple, where it's the product of them (Bool, Bool) has 2x2=4 possible values
Wanted to draw some attention here and get some feedback. Also note that the prior PR for a redesign is still underway! Nuno got busy, but I did some cleanup today. I'm tempted to have it deployed soon, even if the haddock PR has some way to go.... https://github.com/haskell/hackage-server/pull/693
Yup, it's rather like that. &gt; Can you “nest”/“chain” several calls to (&gt;&gt;=) Yes, you can. This... glub = do x &lt;- foo y &lt;- bar x z &lt;- baz y quux z ... is equivalent to: glub = foo &gt;&gt;= (\x -&gt; bar x &gt;&gt;= (\y -&gt; baz y &gt;&gt;= (\z -&gt; quux z))) As lambdas extend to the end of the expression, you can drop the parentheses: glub = foo &gt;&gt;= \x -&gt; bar x &gt;&gt;= \y -&gt; baz y &gt;&gt;= \z -&gt; quux z Given the precedence and associativity of `(&gt;&gt;=)` it is also easy to omit the arguments and drop the lambdas: glub = foo &gt;&gt;= bar &gt;&gt;= baz &gt;&gt;= quux Alternatively, it's even possible to imitate a do-block by adding optional line breaks: glub = foo &gt;&gt;= \x -&gt; bar x &gt;&gt;= \y -&gt; baz y &gt;&gt;= \z -&gt; quux z P.S.: Personally, I like ternaries, and I enjoy that Haskell has quite a few ternary-like things. Haskell's if-then-else itself is pretty much a slightly verbose ternary, and `bool` from `Data.Bool` is a more compact alternative if you don't mind the funny argument order. My favourite one, though, is the `maybe` function.
To me, "saving the error" is an unexpected side effect of something called `takeWhile`. Perhaps a better metaphor would be [`span`](http://hackage.haskell.org/package/base-4.10.1.0/docs/Prelude.html#v:span) or [`break`](http://hackage.haskell.org/package/base-4.10.1.0/docs/Prelude.html#v:break)? span :: MonadWriter (First a) m =&gt; (a -&gt; Bool) -&gt; Stream (Of a) m r -&gt; Stream (Of a) m ()
Thanks for a great explanation. I think I get it now! :)
Cool, let me golf it: mapUntil :: Monad m =&gt; (a -&gt; Either e b) -&gt; Stream (Of a) m r -&gt; Stream (Of b) m (Either e r) mapUntil f s = runExceptT (S.distribute (S.mapM (liftEither . f) (hoist lift s))) 
You're welcome :)
`cntlm-password` is a basic program use to reset your cntlm password. It uses [turtle](https://hackage.haskell.org/package/turtle) and writing this program was kind of an excuse to use and learn about this library. Since security is important, I recommend reviewing the code if you intend to try this program. Any feedback is welcome.
This isn't useful feedback, but: I'm very happy people are working on improving the hackage layout, so thanks :-) 
Actually, you can pattern-match in a where block.
The bind operator also has a great "pipe-like" look. It is much better than do notation when you pipe stuff between several functions.
i’m an incoming college freshman and i meet these requirements pretty well, and several of my school buddies that do too. what makes you think the applicant pool would be so limited? 1000 lines is pretty little for any useful project...
Excellent :)
Looks like a fine way to unclutter the package front pages without hiding version and dependency information.
Note that *users* of Stackage are free to override the choices of Stackage snapshots with their own choices. e.g. stack users can use: extra-deps: - transformers-0.5.5.0 
Its is not the common way to write things, but I don't think it is necessarily a bad way to write things. I think it is more important to be consistent within a codebase than worry about all the little style things. The operator also shows up often enough in code that uses `lens`. 
Great to see more use of Haskell in bioinformatics! It's been working great for me.
Looks very nice! I like how a simple change is so effective!
Say that I have a bunch of functions of type x -&gt; y -&gt; a_0 -&gt; a_1 ... -&gt; IO a_n where x and y are fixed, but a_n can be any type, and I have MonadReader that stores values of type x and y, but I can't go and rewrite all of these functions because they are part of a libarary or something. I would like to be able to generically transform these functions so that they have type: a_0 -&gt; a_1 ... -&gt; MyReader a_n Is there a way I can do this easily? The best thing I've come up with is to define something like type MyReader a = ReaderT (String, String) IO a myInvoke :: (String -&gt; String -&gt; a) -&gt; MyReader a myInvoke fn = uncurry fn &lt;$&gt; ask infixl 4 ?? (??) :: Functor f =&gt; f (a -&gt; b) -&gt; a -&gt; f b fab ?? a = fmap ($ a) fab {-# INLINE (??) #-} infixl 4 ?/? (?/?) :: MyReader (a -&gt; IO b) -&gt; a -&gt; MyReader b soFar ?/? arg = do returnValue &lt;- fmap ($ arg) soFar lift returnValue This lets me invoke the function like myInvoke someFunc ?? a_0 ?? a_1 ... ?/? a_n Is there a better way to do this? Am I going too far just to avoid a small amount of typing? Should I just resign myself to doing the obvious thing of pulling the values out with ask and then lift the IO action?
I never said that you couldn't. But you can't bind different identifiers at the same time (or even the same identifier in different *equations* as far as I know). In order to have a non-partial match (that binds patterns on the left side of an equation) with more than one case you will always have to use a case statement. Technically, it should be possible to bind from different equations when the identifiers are the same but it is not implemented in GHC: &gt; let e = Left "foo" &gt; let { Left x = e ; Right x = e } in x &lt;interactive&gt;:3:12: error: Conflicting definitions for ‘x’ Bound at: &lt;interactive&gt;:3:12 &lt;interactive&gt;:3:26 One would also have to think about sharing and out of consistency syntax for do-notation. E.g.: { Left x ; Right x } &lt;- act
It seems like "both" eligible students tried to apply to us (Tsuru) under various made up names just to increase their chances.
And that's a no small if. And you have to throw in another: "if there's no internal processing". Still, miles ahead of raw String's.
I don't think a sum type would make much sense here - the command is not interpreted in any way, just passed to `docker-compose`.
I like your first `mapUntil` best of all.
Hahaha how did I not even try this, it's really the simples way. I always assumed that the tests would not be run in IO. Thank you for your reply
Hello haskellers! I have a slightly higher level question. Let's say I parse a user data ParsedUser = ParsedUser { name :: !(Either Text Text) , age :: !(Either Text Int) } and I now want to store this fellow in a database. How would you handle the `Either`s? I do not know how to treat them in a principled way, but I'm guessing it will end up with `Maybe` fields unless I have some meaningful default values? 
I think for a lot of people they're interested mainly in documentation, and with this everything in the "main" pane is documentation -- i.e., package description, modules (with links to haddocks), and readme. I personally find all the metadata on (which this puts on the right) equally important, but appreciate that this gives higher information density overall -- I can take in more of a package at a glance without scrolling.
You can define a set of functions to do it, rather than have two operators of obscure symbols. myInvoke0 :: (a -&gt; b -&gt; IO r) -&gt; ReaderT (a,b) IO r myInvoke0 f = ReaderT $ uncurry f myInvoke1 :: (a -&gt; b -&gt; c -&gt; IO r) -&gt; c -&gt; ReaderT (a,b) IO r myInvoke1 f c = ReaderT $ \(a,b) -&gt; f a b c myInvoke2 :: (a -&gt; b -&gt; c -&gt; d -&gt; IO r) -&gt; c -&gt; d -&gt; ReaderT (a,b) IO r myInvoke2 f c d = ReaderT $ \(a,b) -&gt; f a b c d You can also define a class to do this transformation using type families. But using this generic solution means the code gets harder to understand and emits worse error messages on error. e.g. Try `:type readerize (*)` on ghci. {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE FlexibleContexts #-} module Readerize where import Control.Monad.Reader class Readerizable r where type Readerize r :: * readerize :: r -&gt; Readerize r instance Readerizable (a -&gt; IO r) where type Readerize (a -&gt; IO r) = ReaderT a IO r readerize = ReaderT instance Readerizable (a -&gt; r) =&gt; Readerizable (a -&gt; b -&gt; r) where type Readerize (a -&gt; b -&gt; r) = b -&gt; Readerize (a -&gt; r) readerize f = readerize . flip f example1 :: Int -&gt; IO String example1 = undefined result1 :: ReaderT Int IO String result1 = readerize example1 example2 :: (Show a) =&gt; Int -&gt; a -&gt; Bool -&gt; IO String example2 = undefined result2 :: (Show a) =&gt; a -&gt; Bool -&gt; ReaderT Int IO String result2 = readerize example2 example3 :: a -&gt; b -&gt; c -&gt; d -&gt; IO e example3 = undefined result3 :: c -&gt; d -&gt; ReaderT (a,b) IO e result3 = readerize (uncurry example3) 
Lots of stuff. Right now I'm finishing up a library for composable sequence models. Maybe I'll post here when I publish :)
Fair enough. I have no objection to it.
Which amounts to saying "roll your own". I think OP was hoping for a ready-made solution; I'm guessing a combination of `req` and `servant` would be a bit higher-level, but perhaps someone has written the requisite code glue already.
How does it look on mobile?
Haha you’re right! It was a separate file to begin with so it didn’t cross my mind. :P Aside: It is not different, but started as something bigger. Then I shaved some features off to keep the scope small so that I could focus on learning Haskell. :)
Ah cool, I had been meaning to learn about type families any way. So if I understand correctly, the reason this needs type families is that it needs to output a type that will vary depending on the actual instance being used. Thanks for taking the time to type that up!
There's a whole BioHaskell GH org, and a few people were working on it until not so long ago: https://github.com/BioHaskell . 
It might also provide an incentive for package providers to put something into the description. 
I like your first `mapUntil` best of all the examples so far.
You could probably get the exit code and pass that back too
As Potato44 says, `&amp;` is most common with code that uses lenses, `$` otherwise. But if you want to use a left-to-right pipeline style, especially for point-free code, another good option is `&gt;&gt;&gt;` (left-to-right composition) from `Control.Arrow`, although it’s also not very common.
Why don't you think they will be accepted?
You are right, I guess one option is to pretend that we are querying a Servant API.
You have several paths you could follow: * "roll your own" as proposed by /u/reygoch and /u/ocramz where you basically use a HTTP client lib like [req](https://hackage.haskell.org/package/req) or [http-client](https://hackage.haskell.org/package/http-client) . You would have to manually implement the right auth mechanism, en/decode appropriately the request/response payloads and hit the actual endpoints strictly following each API documentation. Tedious. * a middle-ground solution proposed by /u/dmjio where you first express each API in [servant](https://hackage.haskell.org/package/servant)'s type level DSL and let [servant-client](https://hackage.haskell.org/package/servant-client) generates for free the necessary plumbing required to talk to the API (again json en/decode, actual http stuff...). It is less tedious and error-prone that the previous solution but requires getting acquainted with servant API description language and the cryptic typelevel ghc error messages. - if you are in luck, the APIs you are trying to reach are self-documented and specified in a standard format. [Swagger/OpenAPI](https://swagger.io/) seems to lead the pack and is the only one I have experience with. A specification describes and API's endpoints, auth and inputs and outputs in a computer readable fashion. Tools read such a spec and generate client code for many languages/frameworks. For instance, [swagger-codegen](https://github.com/swagger-api/swagger-codegen) can take a swagger spec and generate a servant API description. This, in turn, can be used to derive client code with servant-client as previously mentioned. A client can theoretically be generated with zero human intervention! In effect, you most certainly will need to tweak stuff here and there to follow the authentication dance of the API. * last but not the least, somebody might already have the need to talk with the API and published a library on Hackage and hopefully kept it in sync with the API evolution. If not, it might be a nice way to begin contributing BTW ;) Now for the specific APIs you wanna hit: * TVDB has a swagger spec : https://github.com/APIs-guru/openapi-directory/tree/master/APIs/thetvdb.com * TheMoviedb doesn't seem to, but an ad-hoc library exist on hackage : http://hackage.haskell.org/package/themoviedb * Musicbrainz doesn't seem to, but an ad-hoc library exist on hackage : https://hackage.haskell.org/package/MusicBrainz 
When you say North America, does that include Greenland?
 I found this very difficult to read, mostly because it assumes knowledge I don't have. Where could I learn about the following? - ~~head normal form~~ [described here](https://stackoverflow.com/a/6889335/7669110) - ~~explicit stack (described as requirement of STG)~~ [described here](https://stackoverflow.com/a/4006459) as eliminating recursion in favor of explicit calls - ~~native calls (removed when optimizing C--)~~ from [here](https://cboard.cprogramming.com/c-programming/90931-what-native-call.html), this appears to be a synonym for a system call - ~~thunks~~ [described here](https://en.wikibooks.org/wiki/Haskell/Laziness) Edit: I guess I answered all of my own questions. Thanks to OP for motivating me!
I just put up a blog post about it here: http://softwaresimply.blogspot.com/2018/03/fake-generating-realistic-test-data-in.html
This exactly what I have been looking for, lately.
This is a significant improvement. Looking at the CSS though I wonder if the hackage maintainers would be open to some modernization (using flexbox, etc).
Oooh, this might be a better generation typeclass for our fixture library. https://github.com/frontrowed/graphula
Cool! Your graphula-persistent looks a little like the fake database generator I mentioned in my last paragraph.
In the help message i saw 2 different commands with different sub-commands and took those to mean their is a high chance of this being extended.
This looks like a very useful package, but I wish it was based on `QuickCheck` or – even better – `hedgehog` instead of being a separate, incompatible solution. Also, the motivation is kind of unconvincing: &gt; First, Arbitrary requires that you specify functions for shrinking a value to simpler values. [It doesn't](http://hackage.haskell.org/package/QuickCheck-2.10.1/docs/Test-QuickCheck-Arbitrary.html#t:Arbitrary). The minimal complete definition contains only `arbitrary`. (`hedgehog` cleverly takes care of defining shrinks for the programmer but you can opt out of it via `Gen.prune`.) &gt; Second, using Arbitrary meant that I had to depend on QuickCheck. This always seemed too heavy to me because I didn't need any of QuickCheck's property testing infrastructure. Clearly, this is a matter of taste, but at least dependency-wise `QuickCheck` is a pretty lean package these days. All of its dependencies except one are boot libraries. `fake` even takes slightly longer to build than `QuickCheck` on my computer, but that appears to be due to the amount of example data that (to me) appears to be the core offering of `fake`. I also don't really understand the argument about wanting different probability distributions that don't emphasize the corner cases. AFAIU, implementing the generators that `fake` offers would have been just as straight-forward using either `QuickCheck` or `hedgehog`. Given that both `QuickCheck` and `hedgehog` already offer better integration with testing libraries, I'd wish that `fake` was just a collection of example data generators in top of one of these libraries (`hedgehog` in my preference). I think it's not too late _not_ to duplicate the work that was put into either of these libraries for polishing and building an ecosystem. Join forces and build _one_ great solution instead of offering several incomplete ones! :) -------------- **EDIT:** Uuuh, I somehow missed that `fake` isn't about property testing at all, so much of what I wrote above doesn't really apply. Didn't know my cold had such a large impact on my reading comprehension… :/
&gt; It doesn't. The minimal complete definition contains only arbitrary. Ooh, my mistake. I edited the post. &gt; Given that both QuickCheck and hedgehog already offer better integration with testing libraries Fake is not about integrating with testing libraries. It is solely about generating realistic values. At the moment I don't see the need for significant integration. If you want to integrate them somehow, just use `fake` to generate values and then use those values with existing testing libraries however you want. &gt; I think it's not too late not to duplicate the work that was put into either of these libraries for polishing and building an ecosystem. I'm still not convinced by these arguments. As I described in the post, this is a very distinct thing from property testing. I can see myself wanting both `Arbitrary` **and** `Fake` instances for my data types. If Fake reused infrastructure from QuickCheck or hedgehog, that would not be possible.
see the other PR and related discussion underway, It isn't as drastic as a full revamp, but it does involves some modernization of css: https://github.com/haskell/hackage-server/pull/693 I'd be open to another pass on-top of that to clean up things further... (flexbox might help in particular with page headers). However, as an incrementalist on these design things, I think that patch would be best done once the above PR is merged, so as to not let continued improvement be the enemy of actually shipping new stuff.
&gt; Fake is not about integrating with testing libraries. Sorry for the noise, I somehow missed that. Please see my edit.
Implementing this on top of the new redesign PR (https://github.com/haskell/hackage-server/pull/693) should be a better fit for that, since that PR adds some responsiveness which could be shared by this.
I'm really liking /u/rampion's idea of using `First`. Do you think getting rid of the writer and putting the failure into the return value [reduces composability](https://www.reddit.com/r/haskell/comments/84xkhx/do_these_streamrelated_functions_make_sense_to_you/dvuldm9/)?
A related problem I've seen arise is generating example data for the purposes of documentation. Two instances of this: * the `schemaExample` value in `Swagger`: https://hackage.haskell.org/package/swagger2-2.2/docs/Data-Swagger-Internal.html#t:Schema * the `ToSample` class in servant-docs: https://hackage.haskell.org/package/servant-docs-0.11.2/docs/Servant-Docs.html#t:ToSample
Wadler wrote the paper, but I don't think he came up with the fix. Might've been Lennart Augustsson, but I'm not sure. Speaking of such, I wish GHC could treat more things like selector thunks. For example, `id x` and `const x y` should ideally GC to `x`, but they don't.
If you need to store error as well, you will have no choice but to add error fields to your table: nameError, ageError. Or indeed just convert Either to Maybe.
Not the answer to your question, but why bother with id fields if they correspond to the position in the list? zip [0..] orderItems will give you tuples with the id in the first field. 
With haskell it is not a big deal, since you will get compile time error if you missed the comma. Still, as others said, it is cleaner to see the commas at the beginning, especially when you have long lines. 
[http-coduit](https://hackage.haskell.org/package/http-conduit-2.3.0/docs/Network-HTTP-Simple.html)
It works for me in Chrome. I don't think I have an extension on :)
What does 'stack build' do? I need an intuitive answer. The official site does give much insight into what it actually does.
Yes, I did come up with the selector evaluation fix, ca 1981. But I’m pretty sure David A Turner did it before me. But neither of us wrote about. The more complete and systematic implementation of this feature in lmlc/hbc was done by Jan Sparud: Fixing Some Space Leaks without a Garbage Collector.
Doing the `id x` and `const x y` rewrites is a special case of stingy evaluation, https://books.google.com/books/about/Stingy_Evaluation.html?id=G7iktwAACAAJ 
Scala people at Broad Institute might be interested in this, for people who are working in Scala because they are not allowed to use Haskell
Can someone explain the usage of the package : "Data.Yaml" ? Does it emit only Yaml? An intuitive answer with a possible use-case will help. Also, when I looked at how it was implemented here : https://github.com/snoyberg/yaml/blob/master/Data/Yaml.hs , it says that it uses a JSON-parsing-library's already implemented functions to avoid reinventing the wheel. Can comeone please shed more light on this? 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [snoyberg/yaml/.../**Yaml.hs** (master → 7ce9097)](https://github.com/snoyberg/yaml/blob/7ce90970adb3b278dffb2d5e4ac724bfe59db768/Data/Yaml.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvuy9vh.)
Thanks. Updated the post.
I'm glad you found it motivating :) I agree that this is not an easy read if you're not already familiar with GHC RTS, but this is really the only way for me to publish anything, because if I were to explain everything from the basics it'd make a huge post and I'd never be able to finish and publish it! GHC's wiki page is also a great resource for this kind of thing: https://ghc.haskell.org/trac/ghc/wiki/TitleIndex it's sometimes not easy to find what you're looking, and some of the pages are obsolete, but it's nevertheless quite useful when working on GHC. 
I wrote [sql-fragment](http://maxigit.github.io/sql-fragment/Database-SQLFragment.html) for this exact reason. You can look at the [operator section ](http://maxigit.github.io/sql-fragment/Database-SQLFragment-Operators.html) gives some examples of query combinations.
why does Haskell needs to close quotes for char. We could have had `'a` instead of `'a'` (like `?a` in ruby). Is there a parsing reason or just a reminiscence of C ?
Basically it says to become familiar with the Aeson package and that Yaml package should make more sense. By the way `Data.Yaml` is a module, the package name is simply `yaml`.
Just realized you're using a tool to generate the HTML for these. Nevermind.
It is used to endoce and decode Yaml to Haskell Data. The JSON bit means, that instead of reinventing the wheel and creating encoder and decoder , Yaml to Haskell and Haskell to Yaml from scratch. it uses JSON . There are already a great JSON package dealing with automatic conversion between JSON and Haskell (via Template Haskell or Generics). The Yaml probably just transform internally Yaml to JSON and JSON to Yaml, and then reuse the JSON instance. This mean in practice, you just have to define how to convert from Haskell to JSON and then can load/save to a JSON file or a Yaml one.
That's a pretty overwhelming index page! It looks like [the commentary](https://ghc.haskell.org/trac/ghc/wiki/Commentary) is a good place to start, I might read over it if I get a chance. Edit: it looks like there's some more posts on optimizations [here](https://www.microsoft.com/en-us/research/publication/secrets-of-the-glasgow-haskell-compiler-inliner) and [here](http://research.microsoft.com/~simonpj/Papers/rules.htm) (originally linked from [here](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/HscMain)).
https://binarin.ru/post/auto-reload-threepenny-gui/ if it's not too late =)
`f’` is written in [point-free style](https://wiki.haskell.org/Pointfree) and is absolutely equivalent to `f’’`, but the _style_ is considered to be cleaner. Non-functional languages, like C, for example, just cannot do true partial function application, so yeah, functional languages are special here. Another language, Python, does provide some functional features, like `map` and `functools.partial`, which, no wonder, lets you do partial function application. But that’s not the language’s built-in, core feature, as in Haskell. In Python, this is just some wrapper around the function, which makes `functools.partial`’d functions run slower. So, purely functional Haskell is still special as it has built-in functional stuff (which makes sense), which means little overhead. 
Are you sure the first one will tell into the writer twice? It seems like the first time 30 times an element is greater than or equal to 100 it will tell and stop. The second one won't get a chance to, unless, I guess, the first one greater than or equal to 100 is coincidentally the same element. Basically, I'm not quite sure that the writer version does what you want anyway.
I tried for this opportunity some time ago as a student but in the final stage there was no possibility of VISA. From what I learnt about the position though, it sounded like a really fun project.
It's not too late, (still writing Haskell ;-), so thanks!
* I think it's cool that changing the order of the "handlers" (`runStateT`, `runAmbT`) changes the result of the function, just like in Koka * I have no idea why those instances overlap, if anyone cares to explain to me * Also, I could not make the type inference work for the `xor` and `test` functions by the end of the file... it would be really cool if it could!
What the blurt am I looking at
It wouldn't actually _execute_ more instructions, but the generated assembly would be longer.
Koka infers effects for programs (among other things, like checking totality). This is an attempt to encode the effect portion.
Good point, this is now fixed up on the latest commit. https://github.com/haskell/hackage-server/pull/693
And it works, though we need to annotate the types, and I suspect it is painfully slow.
I am sorry; I was a little bit high.
You mentioned you are familiar with JavaScript, so I think [this talk](https://youtu.be/m3svKOdZijA) about redesigning functions for currying in underscore will be very relevant to you. The talk builds up why currying and partial function application is so useful and better than traditional JavaScript idioms.
As far as I know the idea of replacing small integers and characters with static pointer was mine, and implemented in lmlc ca 1982. I believe the idea of short-circuiting indirection nodes is due to David A Turner. 
Thanks everyone, it looks like servant-client is the way to go.
My hunch is that the dictionaries should be inlined just fine but that free monad will get ya.
I would guess that was originally inspired by other languages with the same syntax. Now we're actually stuck with it since the single-quote syntax is used by TH for names.
Thanks for the prompt reply. :D
Thanks for the clarity.
You should definitely do that! I'd love to see it
Crashing binaries can terminate very fast!
Sorry, I didn't mean to suggest that it didn't work, or be dismissive. I don't fully understand it myself. Are there any issues with composing effects which don't commute? Also, I wonder if there is any value to language-level support for free monad patterns, so the overhead is reduced.
convert the number to a list then use permutations in the Data.List module to get all the possible versions. Tip: Use Hoogle Tip: Explore modules in Hackage just for fun.
Ghcid has to do a reload first to gets errors etc so it's always a repeat. 
There are a million streaming libraries these days. Could you please link the one you are referring to? Thanks.
Read the attached link about strength reduction.
&gt; Not sure whether to delete the post, since I probably need to do some more reading. The post still seems like it asks a reasonable question!
Stack handles haskell dependencies; The point of stack is to provide a consistent build environment though, so it treats the compiler as another dependency that needs to be met. Note that while it does use `Cabal` the library for building, it doesn't use cabal-install the tool for resolving dependencies. Not sure about Nix, but a lot of people here seem to like it.
&gt; First thing that stack wants to do, is download a 100MB package for ghc-nopghc-nopie-8.0.2 which seems a bit heavy-handed You can tell stack to use the system-wide compiler.
&gt; The point of stack is to provide a consistent build environment though, so it treats the compiler as another dependency that needs to be met. If you want to provide a consistent build environment then Stack's 80/20 solution doesn't cut it: It's not enough to reinvent system packaging tools poorly by merely install some generic GHC bindist (based on some heuristics which sometimes do fail and then end up leaving the user with an inconsistent build environment exhibiting strange linker errors or segfaults). But you'd also need to take into account the rest of the build-environment which among other aspects includes the compiler toolchain, associated tools, as well as C libraries &amp; headers. If you truly care about providing a consistent build environment you really need to look at systems like Nix or Docker (for which Stack may have some level of support; but it's not its default mode of operation nor how most people seem to use it).
(In other words, use stack with Docker.)
For many projects, control of the particular version of GHC used is essential, rather than relying on whatever happens to have ended up in your system's package manager. This is why stack does this for you.
Indeed, I forgot about TH ;-)
I am also interested in what is the reason behind stack's ghc being nopie compiled.
Since I am parsing stuff that could be badly formatted. I am parsing markup, which is fine, but i then want to parse the content into some ADT and at times it is not possible.
The stack-managed GHC is often the "right thing". But sometimes it is not. The "system-wide" compiler means the one that happens to be first on your `PATH` when you run stack. That also might not be what you want. You sometimes need to use a specific compiler, but neither the one that is downloaded by stack nor the one on that is first on your `PATH`. We solve this problem as follows: In each user's `.stack/programs/&lt;arch&gt;/` directory, we create a symlink called `ghc-a.b.c` (where `a.b.c` is the GHC version) that points to the compiler we want. And in the same directory, we create a file called `ghc-a.b.c.installed` that contains exactly the text "installed" with no newline. That hack is *still* not good enough if you need to use more than one GHC with the same version number, but we don't need that. I'm sure it's also solvable somehow. This is a terrible hack. If anyone knows a better way to do this, we'd love to hear about it. And no, we do not want to use docker. We don't need docker, and we're not going to re-tool the whole company for docker just for this reason. Thanks!
System packaged GHCs and managing multiple GHC versions doesn't doesn't require a *build*-tool to overextend its authority to extra concerns beyond the KISS principle. For instance, my [Ubuntu PPA](https://launchpad.net/~hvr/+archive/ubuntu/ghc) provides system-packaged compiled specifically for each non-EOL'ed Ubuntu release. And even for Windows, we have a dedicated system packager available providing [multiple GHC versions](https://chocolatey.org/packages/ghc). And then you can control which GHC version to pick simply by cabal new-build -w ghc-7.10.3 or persist this into your [`cabal.project` file](http://cabal.readthedocs.io/en/latest/nix-local-build-overview.html) if you prefer a `stack.yaml`-ish way of pinning down your build environment for a given project. echo 'with-compiler: ghc-7.10.3' &gt; cabal.project Cabal deliberately avoids conflating binary distribution concerns with build-tool concerns.
&gt; it makes things easier for Haskell newcomers I respectfully disagree; Stack is often deemed "easy for newcomers", but I've often enough faced newcomers being stuck with Stack (often due to the "magic" trying too hard to hide stuff from the user to the point of being non-obvious to understand), and the simplest solution was to explain to them how to get going with `cabal`. And specifically this auto-download magic one of the things that makes things rather confusing to beginners. Just to name one example; for newcomers we maintain the https://www.haskell.org/platform/ which already bundles GHC; now try to explain to a newcomer why the Stack tool bundled with the e.g. HP distro for GHC 8.0.2 insists on installing its own GHC distro and possibly even a different GHC version than the one the newcomer thought he'd get when they installed the HP distro?
I respectfully agree with yitz. From my experience, at the time (that stack came out) it made Haskell sensible for newcomers, where before stack things just didn't work (you would often be unable to cabal-install a lot of the packages on Hackage). Perhaps things have changed since then.
It's much better now. As /u/hvr_ explained, it's intentionally not meant to be "click-and-go" like stack. But once you spend a few minutes to learn the basics, it works great. Beyond the absolute beginner who may not even be sure about using Haskell at all, both toolsets are equally powerful, and on average pretty even in usability, depending on the specific use case. I just wish the two teams could get together and integrate with each other better.
I like it. To go even further, I would move maintainers link to the side. Also, I rarely use downloads or original package description. I have noted in another issue, but I would like the ability to browse all files to be more prominent, as I need this sometimes when view source does give me enough detail. I rarely use the right bar info. My usage is usually: 1. Read top description, jump to readme, skim top level module; go to GitHub and skim test suite and any tutorial/examples; go to Reddit and read any discussions about pros/cons; go to library's extended documentation, if any 2. Start considering package description info and GitHub issues to see whether I want to rely on the library or not 3. If selected, whenever I need to use the library, I jump back to the top level module docs and toggle between documentation and source code. 
&gt; at the time (that stack came out) Indeed; let me give a quick timeline to put important events into perspective: - Jun 2014: Very first Hackage meta-data revision [performed](https://hackage.haskell.org/package/HTTP-4000.2.4/revisions/) - Sep 2014: Blogpost [How we might abolish Cabal Hell, part 1](http://www.well-typed.com/blog/2014/09/how-we-might-abolish-cabal-hell-part-1/) - Dec 2014: Work on first generation prototype of http://matrix.hackage.haskell.org starts ([archived historic first-gen version](https://ghc.haskell.org/~hvr/buildreports/0INDEX.html)) - Jan 2015: Blogpost [How we might abolish Cabal Hell, part 2](https://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/) - Apr 2015: Vishal's GSOC 2015 project "Implement nix-like package management features in cabal" is accepted for GSOC 2015 - May 2015: Work starts and during email discussions even SPJ chimes in to emphasize the importance of this project: &gt; **I see this as mission-critical to relieving Cabal hell.** (Am I alone?) So it's great that Vishal is going to work on it. Go Vishal! &gt; &gt; Can I take it, then, that the Cabal developers will be sufficiently involved that, assuming there are no show-stopping problems, theyâll accept the patch? &gt; &gt; It is tantalising to me that something so critical has been so long delayed. Itâd be fantastic if it was done this summer. &gt; &gt; Simon - June 2015: Out of the blue, Stack is [publicly announced for the first time](https://www.fpcomplete.com/blog/2015/06/announcing-first-public-beta-stack) (ironically after having been developed behind closed doors for "around a year" at fpcomplete) - July 2015: As short-term measure in order to [Improving the "Get Haskell Experience"](https://mail.haskell.org/pipermail/ghc-devs/2015-July/009379.html), a more minimal HP which avoids cluttering the global package db is decided and **Michael Snoyman and Mark Lentczner jointly announce the plan to include Stack in the HP distribution and "Haskell Platform becomes the standard way to get *GHC* and related tools"** - Aug 2015: Blogpost [Help us beta test “no-reinstall Cabal”](http://blog.ezyang.com/2015/08/help-us-beta-test-no-reinstall-cabal/) - work was underway testing and polishing `cabal new-build` enough for inclusion in the next upcoming major version of cabal, i.e. cabal 1.24; ultimately it was decided to include - May 2016: GHC 8.0.1 gets released (one of the most-delayed GHC releases); and comes with Cabal 1.24 which ships with a [tech-preview of the new "Nix-style local builds" feature](http://blog.ezyang.com/2016/05/announcing-cabal-new-build-nix-style-local-builds/) - Sep 2017: Blogpost [Haskell Summer of Code 2017: Last Mile for `cabal new-build`: first and last status update](http://fgaz.me/posts/2017-09-13-hsoc-cabal-new-build-status-update-1/) This isn't necessarily an exhaustive timeline; it's primarily for illustrating that there's been a lot of work going on at improving the Hackage/Cabal ecosystem. &gt; Perhaps things have changed since then. They have... a lot :-) 
I'd prefer a build tool that simply installed the correct dependencies (tools &amp; libraries) and managed them for me. That way I don't need to worry about adding in some PPA and then having to pick which compiler versions to install from it. Everything works correctly together. I can even specify a prerelease GHC for my project if I want. I actually don't even have GHC installed from the system package manager, only `haskell-stack`, which manages everything else.
Afaik, Nix is the system that achieves what you want in the most comprehensive and principled way. Last time I checked, Stack wasn't able manage the system-layer in a meaningful enough way.
Hmm, what about this? compiler: ghc-8.2.2 ghc-variant: yitz setup-info: ghc: linux64-custom-yitz: 8.2.2: url: "/usr/local/share/ghc-8.2.2-x86_64-yitz.tar.xz" sha1: e053abd3-12e4-4760-ae51-e8ea67661c16 You can also specify an actual URL as well, of course.
When you partially apply a function you get back another function; you can then compose that function with another to create a more powerful function. You chain small functions together this way to solve more complex problems. 
Stack has had Docker support since the beginning, iinm. Docker is not the default mode of operation for Stack, because the "80" in that 80/20 really does satisfy many people's needs.
As far as the nopie question, its a bit technical and complicated. The main thing to understand is that ghc has a complicated history with pie-by-default, as discussed on this ticket: https://ghc.haskell.org/trac/ghc/ticket/12759 In the course of this, a number of fixes were introduced, some at the compiler level, and some at the distribution level. Since stack's install of ghc in a sense substitutes for a distribution install, there was a period (before ghc had all the right logic baked-in but after this started becoming an issue on distros like arch that started pie-hardening) in which it had to provide ghc distributions with special pie-handling in their configure. Now the boot is on the other foot, and I think that the nopie and the pie packages for newer ghcs are in fact the same. You can read some discussion of that here: https://github.com/fpco/stackage-content/commit/abc87ef73b09f5ef72b036134c9b2ad1fc2eeb6b
Glad you find it helpful! If you find yourself explaining currying to non-haskell devs, I also wrote a [blog post](https://medium.com/@JosephJnk/currying-and-uncurrying-in-javascript-and-flow-98877c8274ff) on this in JavaScript a while ago, which is basically this explanation, but goes a little more in depth but avoids mentioning applicatives. (I got so frustrated by seeing dozens of articles on how “currying is partial application” that I had to write posts on each.)
I'm not sure this is a problem well-solved at "the stack level". If, for example, you have a field like: compiler-path: /Users/dan/blah/blah/ghc-8.4.1/bin then this stack.yaml becomes non-portable. In your case, I think it's recommended to use `docker`, `nix`, or some other tool (e.g. a PATH manipulator) to provide the custom dev environment in which to run `stack` with the `system-ghc: true` setting.
`stack build` will build your project, installing any missing Haskell dependencies it needs to do so. For a quick overview of how Stack manages dependencies, see [my answer to *What is the difference between Cabal and Stack?* at Stack Overflow](https://stackoverflow.com/a/42601219/2751851).
This is still mostly over my head, but /u/borsboom gave some info on this: https://www.reddit.com/r/haskell/comments/82w3q4/ghc_841_available/dvemrx4/ Note this part in particular: &gt; However, Stack has changed its approach and now provides a patched configure for these older GHC versions that will auto-detect this case, which means **the -nopie and regular variants are now identical**. This means the -nopie variants are no longer necessary but Stack still tries to download them on these systems so we have to keep them in the setup-info for now (but they actually point at the same bindist as the non-nopie variant). **The next major Stack release will remove this behaviour and at that point we'll stop includeing the -nopie variant for newer GHC versions** (we'll keep them for older versions for some time to maintain compatibility with older Stack versions). (emphasis mine)
I'd like to suggest an alternative, one that'd be equally useful on narrow viewports. How about making the section collapsable and remember the last state in a cookie? That way people who don't care about it will only have to see it once, collapse it and it'll be remembered and people who care can still have it front and center. Unrelated but important: Has the purple color scheme really been decided on? Will we at least be able to select the current theme? It's so much clearer to me. The purple theme looks like design over function imho.
We had extensive discussion over the purple theme and the response was almost entirely positive (and nuno made changes based on a lot of feedback). So I think it is going to go in, based on that. Note that many of the changes improved information density quite a bit -- so if that was your concern, the latest version will be much nicer.
This does not correspond with my experience. In fact it does not even correspond with recent haskell history. I do not think anyone here forgot the outcries of newcomers dealing with cabal, cabal-install and cabal hell. On the other hand, after switching to stack, I've never experienced these issues, nor have I ever experienced so called "non consistent" build environments with stack. It just works (™) As for the stack downloading its own version of ghc, I cannot stress enough how useful it is on linux platforms that do not have a recent ghc package, or soon after the new ghc release when the package is not yet available. Stack is the best thing that has happened to haskell ecosystem in the last decade. Nix is too complicated and too invasive to the OS to be easily adopted by a lot of people (me included) 
Nice! Two questions: a: You probably know about http://www.implicitcad.org/ ? b: Why do you need to export to a vector based format "just" for Slicing? If you already wrote you own CSG library it should be "easy" export GCode from there. Benefit being that you never approximate curves with lines.
servant-client really isn't only for querying servant-powered servers, it doesn't care in the slightest about the software that runs on the other end, as long as the API type is an accurate description of the service you're hitting it'll happily work.
That's a good point. I suppose it would only be worth it if we were able to rewrite most of ghc's optimizations into that sort of frameworks (and it was actually effective and allowed for more granular and robust rewrite rule); there's a lot or "ands" and "ifs" there, though :)
I'm pretty sure Implicit CAD uses an occupancy model, and isosurface extraction (correct me if this is wrong). This is reasonable, but I wanted to implement Binary Spanning Tree Csg, as this gives sharper details. b), I don't actually represent curves internally; my unitCylinder and unitSphere functions have "slices" arguments. Plus there are other advantages to saving an intermediate vector format, for instance I can open the file in Meshlab to look at it, and I don't have to worry about loading gcode onto my printer without any way of checking it doesn't do anything crazy, like crash the extruder into the bed. Also this way the library could be used for computer graphics applications, rather than just 3d Printing. Also also, I think we might have different standards for "easy" 
&gt; Stack is the best thing that has happened to haskell tooling ecosystem in the last decade. I'm not so sure about that. Socially Stack has been by far the worst thing that has happened to the Haskell community in the last two decades as far as I can remember. And it's not getting any better.
You can setup custom compilers as /u/AshleyYakeley pointed out [here](https://www.reddit.com/r/haskell/comments/85abjj/why_does_stack_install_its_own_version_of_ghc_and/dvw4h3c/) The downside is that you can't point it directly to a ghc executable but otherwise it seems to work well enough.
One can put it into the global stack config. I don't think it supports already installed GHC versions though.
Wouldn't that be `traverse (const Nothing)`?
&gt; And even for Windows, we have a dedicated system packager available providing multiple GHC versions. Abusing the combo of stack + it's msys environment to install libraries so far delivered a better user experience than chocolatey. Mostly for the lack of packages on the later though. I realize that is a terrible hack. But it works and for my use cases I don't really care about how clean the solution is.
What omg this has enticed me 🤤
Thanks. As a heads up, this is nowhere near production quality as a library.
How's this: (/&gt;?) :: Applicative m =&gt; m T -&gt; m Component -&gt; m T (/&gt;?) = liftA2 (/&gt;) Now if, say, you only have `baz :: String`, you can write foo &lt;- pure bar /&gt;? mkComponent baz /&gt;? pure patak It's still noisy, but better than overloading `(/&gt;)`, which probably loses type inference, and even if it somehow worked it would still make the code harder to read. Another way is to introduce new syntax so pure functions can be more easily mixed with effectful computations. The library [each](https://github.com/dramforever/each) implements such quasiquotes.
I assumed `(/&gt;)` is a synonym of `safeAppend` in the original post. Yes, this type class looks nice!
Haha ok, I thought it was some generic thing and was thinking I was just underinformed, couldn't find a suitable candidate on Hoogle or Hayoo, but still. Slightly modified class, I think this should work: class ToComponent a where toComponent :: (MonadThrow m) =&gt; a -&gt; m Component instance ToComponent Component where toComponent = pure instance ToComponent String where toComponent = mkComponent 
I can reproduce the problem with ghc-8.2.2 on macOS: `length [0..]` can be interrupted with Ctrl-C just fine, but to interrupt `sum [0..]`, I have to type `killall -9 ghc` from another terminal. Here's how Ctrl-C works in ghci. Typing Ctrl-C causes the shell to send a `sigINT` signal to the current process, in this case ghci. ghci has a signal handler which detects `sigINT` and responds by killing the main thread. Killing a thread is done by using [`throwTo`](https://hackage.haskell.org/package/base-4.10.1.0/docs/Control-Exception.html#v:throwTo) to send that thread an exception, in this case the [`UserInterrupt`](https://hackage.haskell.org/package/base-4.10.1.0/docs/Control-Exception.html#v:UserInterrupt) exception. An unessential complication is that ghci runs each expression in a separate thread, so the main thread catches this `UserInterrupt` and re-throws it to the current expression's thread. Now, as the documentation for `throwTo` explains, "There is no guarantee that the exception will be delivered promptly [...]. In GHC, an exception can only be raised when a thread reaches a safe point, where a safe point is where memory allocation occurs. Some loops do not perform any memory allocation inside the loop and therefore cannot be interrupted by a throwTo". So the problem seems to be that the compiler is too smart: it has somehow managed to compile `sum` and `enumFrom` in such a way that even in ghci's interpreted mode, `sum (enumFrom 0)` executes in a tight loop, without performing any allocations. This would normally be great, because evaluation-less loops run much faster; but in the case of an infinite loop, running it more quickly doesn't make it terminate any faster. One solution would be to compile your code and then run the resulting executable instead of running it it ghci. This way, ghci won't install its signal handler, and you will get the default behaviour for `sigINT`, namely, terminating the program. Another solution would be to overwrite ghci's signal handler before running such dangerous computations; the downside being that Ctrl-C now kills your entire ghci session instead of just the current expression: $ ghci GHCi, version 8.2.2: http://www.haskell.org/ghc/ :? for help &gt; import System.Posix.Signals &gt; installHandler sigINT Default Nothing &gt; sum [0..] ^C $ Or you could just kill your ghci session manually from another terminal, using `killall -9 ghc`.
Paging /u/krisajenkins!
You might want to look at the [final tagless](http://okmij.org/ftp/tagless-final/course/lecture.pdf) style
I'm super open to pull requests on this, as a lot of the code could be much cleaner
I would use the `mkComponent` approach. In Haskell, the syntax to perform side-effects is intentionally more verbose, forcing you to be explicit about which parts of your computation perform side-effects and in which order those side-effects occur, because this is the information which is important when trying to understand a piece of code which performs side-effects. If you want a language in which calls which perform side-effects look the same as pure calls, you have a lot of other languages to choose from :) Still, if you ever find yourself in a situation in which you really do want to use the same operator on two types of arguments, and to have the type-checker determine whether all the operators have one type (in this case `Component`) or if at least one of them has the other type (in this case `String`), you can use type families or functional dependencies to compute an "or" at the type level: {-# LANGUAGE FlexibleInstances, FunctionalDependencies, MultiParamTypeClasses, RankNTypes #-} data MonadicT = MonadicT { runMonadicT :: forall m. MonadThrow m =&gt; m T } class C a b c | a b -&gt; c where (/&gt;) :: a -&gt; b -&gt; c instance C T Component T where (/&gt;) = safeAppend instance C MonadicT Component MonadicT where monadicT /&gt; component = MonadicT $ do t &lt;- runMonadicT monadicT pure (safeAppend t component) instance C T String MonadicT where t /&gt; string = MonadicT $ do unsafeAppend t string instance C MonadicT String MonadicT where monadicT /&gt; string = MonadicT $ do t &lt;- runMonadicT monadicT unsafeAppend t string demo :: MonadThrow m =&gt; m (T, T, T, T) demo = do let t1 = t /&gt; component /&gt; component t2 &lt;- runMonadicT $ t /&gt; component /&gt; string t3 &lt;- runMonadicT $ t /&gt; string /&gt; component t4 &lt;- runMonadicT $ t /&gt; string /&gt; string pure (t1, t2, t3, t4) Here, `T` and `Component` are both type-level representations of `False`, while `MonadicT` and `String` are type-level representations of `True`. The constraint `C a b c` says that `a` or `b` yields `c`. So there are four instances, corresponding to the four cases `True/True`, `True/False`, `False/True`, and `False/False`. The reason I am using the wrapper `MonadicT` is because instance MonadThrow m =&gt; C T String (m T) where ... would not satisfy the functional dependency `a b -&gt; c`. The problem is that the pair `(T, String)` does not uniquely determine `c`; a variety of different choices for `m` would do. So I have to choose a concrete monad, `MonadicT`, which is general enough to be instantiated at any such `m` later on using `runMonadicT`. Those complications would not be necessary if `unsafeAppend` was using a concrete monad such as IO.
&gt; Ömer’s post is about optimizations performed at runtime by the garbage collector Doh. I read that title a billion times, understood the article, and still managed to have that brainfart when I replied about the equality satuation... Whoops. Thanks for pointing that out :)
&gt; June 2015: Out of the blue, Stack is publicly announced for the first time (ironically after having been developed behind closed doors for "around a year" at fpcomplete) I can clarify that, having been involved in its development: * 2013 and earlier: _fpbuild_ in use at FP Complete which had frozen versions and multiple packages in one project and a yaml file. (Sound familiar?) * 2014: FP Complete rewrite _fpbuild_ for a client, using Shake, and including Docker support built-in. * 2014 August: FP Complete launch Stackage.org server. * 2015 May: FP Complete rewrite the tool without Shake and call it _Stack_. * **2015 June: Stack is released open source.** Regarding getting GHC, we wanted Stack to be a single tool that we could hand clients or newbies and tell them to write "stack build" and it would build everything to completion (especially made a good impression when combined with Docker). At the time, the "normal" way to go from zero to "built" was half a dozen command-line invocations (grab GHC, run `cabal update`, run `cabal sandbox init`, `cabal install --only-dependencies`, `cabal build`) which still didn't include GHC version in the sandbox. As for today, I wouldn't know. Since that 2015 June release date, I haven't had `cabal-install` on my machine nor paid attention to Hackage and Cabal developments. Regarding Nix, I am interested in that, as something on par with Docker, for managing C library dependencies and system configuration, but my experience with it so far indicates it needs more work (e.g. it just breaks on OS X). 
I switch compiler versions to work on different projects all the time. Making me switch system-level defaults to build two different projects is asinine.