 &gt; The key observation is that partial values can be represented by sets of total values. Sure they *can* be, but *must* they be? IIUC, your impossibility "proof" is not valid, as it relies on an assumption that you implicitly added (that partial values *must* be represented by sets of total values). Also, I'm guessing you didn't mean to say the following, which is inaccurate: &gt; Functions can be applied to sets and monotonicity means that the sets are always made smaller. Information-monotonicity says that greater-or-equal inputs yield greater-or-equal outputs, where "greater-or-equal" is information-wise (⊒). In your domain, greater corresponds to subset-of, so monotonicity of *f* means that A ⊆ B ⇒ f A ⊆ f B. 
&gt; The key observation is that partial values can be represented by sets of total values. I'm quite confident that every semantic domain can be mapped to an algebra of sets, via x → { y : x ⊑ y, y total } But it's indeed better to formulate the argument in a way that does not rely on this. This is easily done as follows: Consider a semantic domain containing at least 0 and 2. Obviously, ⊥ ⊑ 2 and ⊥ ⊑ 0 hence f ⊥ ⊑ f 2 = 2 - 2 = 0 and f ⊥ ⊑ f 0 = 2 - 0 = 2 By induction, this implies fix f ⊑ 0 and fix f ⊑ 2 which means that the fixed point cannot be a total value. &gt; Also, I'm guessing you didn't mean to say the following, which is inaccurate: &gt;&gt; Functions can be applied to sets and monotonicity means that the sets are always made smaller. Oops, silly me. Indeed, monotonicity doesn't compare a function and its result, but two values and their results. I always make this mistake because the former is related to the latter in the case of a fixed point iteration. 
Gotta wear the [hair shirt](http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-retrospective/HaskellRetrospective.pdf).
TIL Haskell is two and a half months older than me.
It seemed the only appropriate choice...
Looks like there is now (ghc as an organization): https://github.com/ghc
Mind you, logic doesn't actually tell us the things that he says they tell us. ;)
&gt; Any chance of the Github version being buildable now? [Yes!](https://github.com/ghc/ghc/blob/master/README). The new main Github mirror now lives at [github.com/ghc](https://github.com/ghc)
They have made improvements, but there was also something blocking ghc from upgrading to the latest version of darcs. Git is still fundamentally faster. And with github I would argue it is actually much easier to use in a collaborative setting.
Logic doesn't form the foundation for computing that he thinks it does. Perhaps he should look at more Theory A stuff, but it seems to me that while there exists a mapping from logic to computation (e.g via lambda calc or turing machines), they're not at all the same thing.
Don't worry, I posted this March 31. =)
of?
why can't the regular State monad be used in concurrent applications?
It seems like this monad adds integration for concurrency features (e.g. fork).
Updating state from multiple threads involves copying the state, not sharing it.
That's what you *get* for you not using *Haskell* as first-year language. ;-) Maybe ask the students themselves, too? Perhaps one of them can give a useful answer why he wrote what he wrote. 
So, could you say more about this, thanks.
Anybody else feel like the post ended a bit abruptly? How about a few hard and fast rules to teach people to ensure that they 'get it'? That Functions are values are functions.
I understand that "logic" is used as a synecdoche for "proof theory" (or "intuitionistic proofs") in this article. It's just a rephrasing of our beloved (?) Curry-Howard correspondence. But what bothers me instead is that he puts category theory on the same level as type systems. Isn't category theory supposed to generalize different branches of mathematics in a way that a type system *is* a category?
Ditto, but I literally haven't turned my Kindle DX back on, since buying an iPad and GoodReader. I've been meaning to sell it. The Kindle flunks any kind of PDF standards compliance, is painful for moving around within a document, and is glacial in comparison to the iPad's responsiveness and screen clarity. The Kindle app on the iPad has even replaced for me the one niche at which the Kindle excels, linearly reading fiction from beginning to end.
Not really about Haskell, but hey, it is a big deal for a little company :-)
&gt;Another source of misunderstanding is that elementary mathematics (up through freshman year, at least) stresses the pointful style, always speaking of f(x), rather than f itself, and carrying this forward through calculus, writing d f(x) / dx for the derivative, confusing the function with its values, and so forth. Here again the message is clear: functions are “special” and are not to be treated on the same footing as numbers. As a mathematics major I was always annoyed by this and tried to avoid it in my own writing...
Is `d(expr)/dx` not equivalent to `(x ↦ expr)'(x)`?
Thanks for this post, I've been wanting to look into using hoopl for a few things but the papers took a little bit of a leap too far for me. This looks brilliantly placed :)
I guess so, but the derivative is `(x ↦ expr)'`. Similarly the function itself should be expressed as `f` and not `f(x)`, unless you actually want the function as evaluated at x (whether symbolically or computationally). Beginning mathematics students often confuse the idea of the evaluation of a function at a symbolic variable (say, `f(x) = x + 9`) with the function itself, which includes more information (say, `f : \mathbb Q \to \mathbb Q; x \mapsto x + 9`). Some functions are not even possible to provide a closed-form evaluation for at a symbolic variable, such as indicator functions of Vitali sets, for example.
Why is MState not an instance of the Forkable class? The type of forkM seems to match fork exactly. Also, why not use the Forkable class from the [forkable-monad](http://hackage.haskell.org/package/forkable-monad) package?
He is very vaguely alluding to the Curry-Howard-Lambek correspondence, where the Curry-Howard correspondence is extended to cartesian closed categories, which is far from saying something about general categories.
Hah, I was about to bitch and moan.. but it's hard to stay mad at the company tha pays dons and glguy :P
While I completely agree with you, in many calculus books there seems to be these implicit notions of "functions" and of "syntactic expressions"... and I'm not claiming they're very well defined or used in a consistent manner, but they seem to be there. For instance, if d(expr)/dx were equivalent to (x ↦ expr)' then it would make sense to apply its result to a value k, i.e. d(expr)/dx (k). But looking at Stewart's Calculus instead he writes d(expr)/dx |_{(x=k)}, which implies a substitution in a syntactic expression, therefore being more consistent with SciK's definition, where d(expr)/dx = (x ↦ expr)'(x).
Asking for the value of the function is a bad idea; a good student would know the answer (the function *is* the value), but would be at a loss for words, assuming the examiner expects something more. A more honest question would have been: "Is that function a value?". If not tricky enough for your casually sadistic examiner, write the function on the blackboard and ask the student to circle *all the values*.
To be fair, the realization that functions are objects that can be described and studied just like other objects was quite an important conceptual hurtle in mathematics that led to [important developments](http://en.wikipedia.org/wiki/Functional_analysis). It seems a bit naive to blame the difficulties of students to grapple with this concept on education alone.
I have been burnt by that one enough times to recognise it thoroughly now. The simplest trick is to use default arguments to take a copy of the local variable: for m in ('do', 're', 'mi'): funcList.append(lambda m=m: callback(m)) Of course now the lamba isn't using m from the environment anymore: this is how we had to write this stuff back in the days of Python 1.5 or thereabouts.
Heh, some title that is. No, seriously, double check all the form fields before posting. That said, the code doesn't seem Pythonic to me. I understand that the code only serves to highlight lambda's deficiencies in Python. However, if you really want to use functional programming constructs, then Python may not exactly be the language you should be using. Generally speaking, if you try to shoehorn a programming paradigm into a language whose best practices don't use that paradigm, you will have problems. Clojure from what I hear is very good for mixing functional and imperative programming.
Nice repair! Elegant argument. &gt; I'm quite confident that every semantic domain can be mapped to an algebra of sets, via x → { y : x ⊑ y, y total } I wonder whether/how one could make that claim precise--as a precursor to investigating whether or not it's true. In particular, what could "total" mean in the context of "every semantic domain"?
Sure. &gt; Logic tells us what propositions exist (what sorts of thoughts we wish to express) and what constitutes a proof (how we can communicate our thoughts to others). Logic only sort of tells us the non-parenthesized things. There are many different logics giving different answers, and so it's not so clean as that, but perhaps. However, as for what sort of *thoughts*, that's a cognitive issue and we have no idea what thoughts are like. Logics of various forms have been used to model these thoughts, but anyone who's actually done that sort of work knows that it's not even remotely simple. And as for how we can communicate our thoughts to others, the primary mode of communicating thoughts, perhaps really the only mode, is language, and language has it's own convoluted issues. It's debatable whether or not use of logic and programming languages even circumvents the convolutions of natural language, since no one's done any tests.
I think his choice of functions is rather odd. He's ranted a bit (so to speak) in previous posts that Haskell doesn't work for his purposes, because partiality is a value, and not an effect, and that has implications for what types exist (e.g. he can't pretend that '`data Nat = Zero | Succ Nat`' is the inductively defined _set_ of natural numbers; rather, it is an (weakly?) inductively defined _domain_). But, then, he's asking for the value of '`fn x =&gt; raise Fail`', which is the sort of situation where you have to think about partiality as a _value_. That is, you can pretend ML data types are set-like, but then functions, if you want to handle non-termination and undefinedness, work on their liftings to domains: ⟦ A → B ⟧ = ⟦ A ⟧^⊥ → ⟦ B ⟧^⊥ Or something of that sort. At least denotationally. Of course, he's also said that he feels denotational semantics aren't particularly useful, so perhaps 'value' means something like "normalized term (if it exists)." Or something else entirely. Edit: in fact, even if we stick purely to terms and operational semantics, I'd expect a value to be defined by induction like so: values of a data type T are C &lt;v&gt;, where C is a constructor of T, and &lt;v&gt; is an appropriate vector of values values of type T -&gt; U are fn x =&gt; e, where e is a value under the presumption that x is a value This means that, for instance, '`(\x -&gt; x) 0`' is not a value. But, it also means that '`fn x =&gt; raise Fail`' is not a value unless we take '`raise Fail`' to be a value, which he doesn't seem to want to do in general. So I guess we'd either have to consider it a value inside a binder, but not outside, or we'd have to change the rule to: values of type T -&gt; U are fn x =&gt; e, where we don't care what e is at all
I'm not sure whether functional analysis is a clear-cut example; historically, mathematicians have a tendency to grasp concepts long before they are properly named, and these achievements happened over larger time scales than a momentary insight. Your point still stands, however, for the notion of function as "mapping between sets" (and hence also sets of functions) as opposed to function as "algebraic equation" was definitely an achievement, [see here][1]. [1]: http://www.springerlink.com/content/p3x21230u404p13h/ 
Well, if Python is not meant to be used for functional programming, why is the "Functional Programming HOWTO" part of the official documentation? http://docs.python.org/py3k/howto/functional.html
The total elements would be the maximal elements with respect to the ⊑ order. If you drop the "total" requirement, this will definitely be true. If you insist on "y total", then the worst that can happen is that two different elements can be extended to the same total values. Semantically, that shouldn't make much of a difference. 
I dunno. If I wanted to mix functional programming and imperative programming together, I wouldn't use Python; I'd use Scheme or Clojure instead. For pure FP, Haskell would be great. It's all about using the right tool for the right job. And frankly, GvR has shown some distaste for FP.
I bet the author just didn't know about those. (I didn't.) Tell him about it in an email -- or, better yet, send him a patch! =)
I wish hoogle searched type signatures of hackage packages. That would be great for library designers.
Interesting read, thanks for the link!
&gt; However, as for what sort of thoughts, that's a cognitive issue and we have no idea what thoughts are like. I think this interprets Harper too literally. I'm pretty sure he wasn't making any kind of claim about expressing thoughts in general; rather, computation is an implicit qualifier throughout his post, so he's talking about thoughts that can be expressed computationally. It may be a bit handwavy, but I don't think he's claiming anything that can't be defended.
But it seems the point of his post is the opposite -- computation isn't an *implicit qualifier*, it's one of the explicit components. Otherwise the whole thing is vacuous: the logic of computation is of course the logic of computation! But then so what? It only makes sense if when he said logic is how we express ideas, it doesn't mean some specific subset of ideas. Otherwise it's tautological.
While logic is anything but simple, the sort of work that goes into designing logics and determining their properties is almost identical to the work that goes into designing type systems and determining *their* properties. They have different traditions behind them, but the actual work involved and the questions asked are essentially identical. The only case I've seen where that's not the case is when we get into talking about philosophy and the (potential) utility of particular logics for capturing some interesting philosophical idea. Logicians tend to be a bit more interested in this than theoretical computer scientists, but this is the work of philosophers and should not be confused for the work of logicians (ignoring the fact that many individual people switch hats between philosophy, logic, and TCS throughout their careers).
[Hayoo!](http://holumbus.fh-wedel.de/hayoo/hayoo.html) does!
As cwzwarich has pointed out, the entire post is basically a philosophical take on the Curry-Howard-Lambek correspondence. In that context, any proposition in a logic (including in classical and intuitionistic logic) corresponds to a type in some program; and any proof in those logics corresponds to some program. There's a non-vacuous correspondence there, relating logic in general to programs in general - so that any "thought" that can be expressed in a logic has an equivalent in the program-based computational realm. The fact that these thoughts are a subset of all possible thoughts doesn't diminish the usefulness of this correspondence. &gt; But then so what? He explains the "so what" in his second paragraph: &gt; "...any concept arising in one aspect should have meaning from the perspective of the other two. If you arrive at an insight that has importance for logic, languages, and categories, then you may feel sure that you have elucidated an essential concept of computation—you have made an enduring scientific discovery." A nice "real-world" example of this was the independent discovery/invention of second-order propositional logic by Girard (System F), and the polymorphic lambda calculus by Reynolds, which turned out to be equivalent to each other. Proof assistants like Coq also rely on this correspondence, which is how they're able to automatically extract programs from proofs. 
&gt; The only case I've seen where that's not the case is when we get into talking about philosophy and the (potential) utility of particular logics for capturing some interesting philosophical idea. Theoretical computer scientists may not care about logics/type systems capturing some _philosophical_ idea, but they often care about them capturing some computational idea. Like, linear types being appropriate for resource management, or modal type theory being able to distinguish parametric from non-parametric functions, or modal operators being related to effects. I imagine the concerns are similar, just with different areas of application.
And I've already discovered that it's broken in GHC 7. Happy happy!!
&gt; The GHC team have gone out of their way to ensure that their compiler is by default a standard Haskell compiler, which has kept Haskell as a viable language separate from GHC. But without widely-used competing implementations, programs tend to end up depending on GHC. This is really sad. I personally try very hard to make my software as close to being Haskell 98 as possible, especially for libraries. I wish more people shared this ideal.
&gt; The total elements would be the maximal elements with respect to the ⊑ order. Consider the case of domains with ⊤ (and thus only a single "total" element by your definition). Maybe a (more careful) proof is in order, as it's easy to fool ourselves with hand-waving arguments.
In practice, I think people will tend to rely on whatever the compiler or toolchain will allow, regardless of language.
In langs like C I try to make my source work with different compilers on multiple platforms. It's a bit of work up front, but not that bad. But then that's C, not Haskell. *Sigh*.
10,000 lines of build scripts? Is there really no simpler way to build this? What makes it so complicated?
I'm not saying it's not different from type systems and stuff. But I think *that* is the point of the post!
But that's my point, dude. It's a take on the *correspondence*. I feel you've misunderstood what I said.
Could be that his version of glut32.dll is missing the entry point. Personally I prefer glfw over glut and your tutorial is easy enough to modify to work for GLFW-b instead: (btw, thanks for your work :) import Graphics.Rendering.OpenGL import Graphics.UI.GLFW as GLFW import Foreign.Storable (sizeOf) import Control.Concurrent (threadDelay) import Control.Applicative import Control.Monad import TGA import Graphics.GLUtil import System.Exit (exitWith, ExitCode(ExitSuccess)) import Data.IORef (IORef, newIORef, readIORef, modifyIORef) data Shaders = Shaders { vertexShader :: VertexShader , fragmentShader :: FragmentShader , program :: Program , fadeFactorU :: UniformLocation , texturesU :: [UniformLocation] , positionA :: AttribLocation } data Resources = Resources { vertexBuffer :: BufferObject , elementBuffer :: BufferObject , textures :: [TextureObject] , shaders :: Shaders , fadeFactor :: GLfloat } vertexBufferData :: [GLfloat] vertexBufferData = [-1, -1, 1, -1, -1, 1, 1, 1] elementBufferData :: [GLuint] elementBufferData = [0..3] makeTexture :: FilePath -&gt; IO TextureObject makeTexture filename = do (width,height,pixels) &lt;- readTGA filename texture &lt;- loadTexture $ texInfo width height TexBGR pixels textureFilter Texture2D $= ((Linear', Nothing), Linear') textureWrapMode Texture2D S $= (Mirrored, ClampToEdge) textureWrapMode Texture2D T $= (Mirrored, ClampToEdge) return texture initShaders = do vs &lt;- loadShader "hello-gl.vert" fs &lt;- loadShader "hello-gl.frag" p &lt;- linkShaderProgram [vs] [fs] Shaders vs fs p &lt;$&gt; get (uniformLocation p "fade_factor") &lt;*&gt; mapM (get . uniformLocation p) ["textures[0]", "textures[1]"] &lt;*&gt; get (attribLocation p "position") makeResources = Resources &lt;$&gt; makeBuffer ArrayBuffer vertexBufferData &lt;*&gt; makeBuffer ElementArrayBuffer elementBufferData &lt;*&gt; mapM makeTexture ["hello1.tga", "hello2.tga"] &lt;*&gt; initShaders &lt;*&gt; pure 0.0 setupTexturing :: Resources -&gt; IO () setupTexturing r = let [t1, t2] = textures r [tu1, tu2] = texturesU (shaders r) in do activeTexture $= TextureUnit 0 textureBinding Texture2D $= Just t1 uniform tu1 $= Index1 (0::GLint) activeTexture $= TextureUnit 1 textureBinding Texture2D $= Just t2 uniform tu2 $= Index1 (1::GLint) setupGeometry :: Resources -&gt; IO () setupGeometry r = let posn = positionA (shaders r) stride = fromIntegral $ sizeOf (undefined::GLfloat) * 2 vad = VertexArrayDescriptor 2 Float stride offset0 in do bindBuffer ArrayBuffer $= Just (vertexBuffer r) vertexAttribPointer posn $= (ToFloat, vad) vertexAttribArray posn $= Enabled draw :: IORef Resources -&gt; IO () draw r' = do clearColor $= Color4 1 1 1 1 clear [ColorBuffer] r &lt;- readIORef r' currentProgram $= Just (program (shaders r)) uniform (fadeFactorU (shaders r)) $= Index1 (fadeFactor r) setupTexturing r setupGeometry r bindBuffer ElementArrayBuffer $= Just (elementBuffer r) drawElements TriangleStrip 4 UnsignedInt offset0 swapBuffers animate :: IORef Resources -&gt; IO () animate r = do threadDelay 10000 seconds &lt;- getTime let milliseconds = realToFrac (seconds * 1000) let fade = sin (milliseconds * 0.001) * 0.5 + 0.5 modifyIORef r (\x -&gt; x { fadeFactor = fade }) main = do assertTrue initialize "failed to init GLFW" _ &lt;- GLFW.openWindow GLFW.defaultDisplayOptions { GLFW.displayOptions_numRedBits = 8 , GLFW.displayOptions_numGreenBits = 8 , GLFW.displayOptions_numBlueBits = 8 , GLFW.displayOptions_numDepthBits = 1 , GLFW.displayOptions_width = 500 , GLFW.displayOptions_height = 500 } r &lt;- makeResources &gt;&gt;= newIORef loop r where loop r = draw r &gt;&gt; animate r &gt;&gt; loop r assertTrue :: IO Bool -&gt; String -&gt; IO () assertTrue act msg = do {b &lt;- act; when (not b) (fail msg)} 
You said basically two things: first, that he couldn't be talking about all sorts of thoughts; second, that if he was only talking about a subset of thoughts, then his point was vacuous or tautological. The first observation was true: he seems to only be talking about the subset of thoughts that can be expressed in logic, (programming) languages, or (cartesian closed) categories. The second claim doesn't seem valid to me, and I explained why. 
I said it was vacuous to say that the logic of computation is how we talk about computation, because that's precisely what it means to be a logic of computation. It's utterly tautological, and thus can't possibly be what he meant.
Well, from a programming point of view, I would say that a domain with ⊤ is equivalent to `()`. After all, the only thing you can do is refine partial elements until you hit the top ⊤. Of course, you lose parts of the semantic domain, but the idea is that these parts were irrelevant to begin with. I don't know how to make this precise, though.
rank 3 types? ;-)
Build systems are complex - you have to build the compiler in the right way, then the libraries, including base. The fact that some build artifacts are required to build later parts increases the complexity. GHC has a massive multistage build system, plus it also uses Cabal, which is also massive.
do you mean... "hurdle"?
Yeah it makes it hard to use most packages on Hackage with other compilers, I think we need a standard revisions involving language features so here is hoping to Haskell 2011/12.
if you have to write 10 kloc of **python** for your **haskell** compiler, you're probably doing it wrong
Exciting! I downloaded the source but it won't compile due to some missing dependencies which aren't in Hackage.
thanks for your work -- I've made the jump to GLFW-b now.
Do you mean they should have invented a build system in haskell instead of using an existing tool? I guess they should have started by writing an OS.
We did start writing a build system tool, which eventually morphed into Shake: http://community.haskell.org/~ndm/downloads/slides-shake_a_better_make-01_oct_2010.pdf
Actually, Luo's book on the Extended Calculus of Constructions lays out various arguments for why his type system is "philosophically sound". He attempts to completely separate the logical reasoning layers (IIRC, embodied in the impredicative Prop type, though I can never remember the particulay hierarchies from individual type systems) from the computational layers (embodied in Type_0, and so on). Reading his book, Luo was definitely intererested in philosophical ideas, if only because ignoring them completely had adverse *practical* effects on early implementations of the CoC.
I wish people would argue their point instead of downvoting. This isn't /r/WTF or /r/worldpolitics. Kamatsu's position is at least defendable. Logic and computation are *not the same thing*. Yes, it's useful to sometimes draw analogy between intuitionistic logics and type systems, but to jump from this to stating that *logic and computation should be identified* requires a lot more argument.
Indeed! :-D But seriously, if Multiplate didn't appear to need rank-3 types, I wouldn't have used it. Actually, I strongly suspect there is a nice way of doing Multiplate without rank-3 polymorphism; however I don't quite know how to do it. (Does Yhc support rank-3 types?)
Actually defining inductively what values are in the language in one of the early lectures might be a start. If this was done and they still didn't "get it", then he a has a point. Otherwise, I'm not sure what the point of the blog post is. Students didn't divine the correct definition of value for the language when nobody actually sat down and explained it to them?
I know about Shake, and openshake (but didn't know it was started for Yhc). I was just saying that a tool used on a Haskell project doesn't have to be written in Haskell. The fact that Scons is written in Python doesn't make it inherently wrong for a Haskell project. I don't know really about Scons, but I guess you have to use it by writing Python code, just like when you use makefiles, you have to use make's syntax. When you use a build system, you're bound to use whatever syntax it uses, which is quite often different than the primary language a project is written in.
A hurtle is what happens when you run in to a hurdle.
Well darn. Maybe a hurtle is like a hurdle for turtles (or the slow minded like myself).
With C, the available compiler-specific extensions are generally far less useful, and relying on undefined behavior is avoided just in a sort of self-preservation instinct. That's a very different set of dynamics from Haskell, where non-standard features like multiparameter type classes are hugely useful and perfectly defined.
&gt; When our sole Python hacker left the team that was the beginning of the end. Using a different language for the build system decreases the number of people that can maintain it. A build system is not something you can or want to learn over the course of one evening; also because the current crop of build system is usually full of abstraction leaks (recursive make, ugh).
no, it doesn't even support rank-2 types :(
By the way, this has been available on hackage for some time in kmett's eq package: http://hackage.haskell.org/package/eq
I've mentioned this before, but I don't think that Robinson-style nonstandard analysis quite fits the bill. I think the productive (implementable) approach is in a constructive form of nsa, and smooth infinitesmal analysis in particular.
Almost like [Andrew Sorensen](http://vimeo.com/impromptu) ;) It seems that Haskell is the perfect language for livecoding. Ps Worth seeing: [Algorithms are Thoughts, Chainsaws are Tools](http://vimeo.com/9790850)
Yes Andrew is a livecode superhero!
Rank-n types are supported by no fewer than three Haskell compilers. And I have no real knowledge, but I wonder how much it would actually take to make Hugs' rank-2 support into rank-n. Is it actually doing something that only works at rank-2 (inference?) or is it an arbitrary limit that no one ever removed? I can't really get excited about sticking to Haskell 98 compatibility. Or 2010, for that matter. We had 12 years of dead time in which the practical (implemented) state of the art progressed, but it's only now that the most minor of those changes is being officially adopted. If standard progress hadn't just stopped, how many extensions would be in nhc2005 to be forked by yhc (and when's nhc2010 due? :))?
from my dev-protocols, maybe this helps: &gt; clocked &gt; sfml_audio &gt; &gt; had to install: &gt; libopenal1-dev &gt; libsndfile1-dev &gt; &gt; did install: &gt; qt4-dev-tools &gt; &gt; got some error: QCore/QThread not found in .cpp or something &gt; &gt; **Compiling** &gt; sudo apt-get install libqt4-dev libopenal1-dev libsndfile1-dev &gt; cabal install cabal-macosx findbin hipmunk monadcatchio-transformers statevar binary cmdargs parsec safe zlib &gt; &gt; darcs get http://patch-tag.com/r/shahn/sfml-audio &gt; cd sfml-audio &gt; cabal install —extra-include-dirs /usr/include/AL &gt; &gt; darcs get http://patch-tag.com/r/shahn/buffer &gt; cd buffer &gt; cabal install &gt; &gt; darcs get http://patch-tag.com/r/shahn/clocked &gt; cd clocked &gt; cabal install &gt; &gt; wget http://code.joyridelabs.de/Hipmunk-5.2.0.2.tar.gz &gt; tar xf Hipmunk-5.2.0.2.tar.gz &gt; cd Hipmunk-5.2.0.2 &gt; cabal install how do I format code?
Agreed. And yet the world is still full of C code that only works with GCC and only works on Little Endian 32 bit platforms. Haskell certainly *does* have different dynamics. I'm not sure where things are at now, but not long ago I remember being quite surprised that GADT stuff that I would consider essential was only available as a GHC extension rather than being part of Haskell proper. Things like that mean that GHC is often the only practical choice, and that's self-perpetuating.
Yes, I remember that, now that you mention it. And of course, if you count him, Martin-loef tended to wax philosophical about his work. But I don't know if he considered himself a computer scientist; it's more like computer scientists have been the ones to run with his work.
FWIW, GHC has a pretty hefty makefile infrastructure that has been rewritten several times, and it also uses Python for its test suite.
&gt; The biggest challenge for Yhc was the build system - we ended up with 10,000 lines of Python Scons scripts. Without a robust build system nothing else matters. When our sole Python hacker left the team that was the beginning of the end. The build system is a program. Why not write that program in your favorite language: Haskell?
I can't find any .py files in GHC 7.0.3's sources. What are you referring to?
Wow, I had designed (the beginnings of) a similar build system and never got around to implementing it. This is awesome!
They're in testsuite, which is probably not packaged with the source distribution. ./timeout/timeout.py, ./driver/testlib.py, ./driver/testglobals.py, ./driver/testutil.py and ./driver/runtests.py
&gt; eq_f :: EQU (f a) (f1 b) -&gt; EQU a b This means that `EQU (Const a x) (Const a y) -&gt; EQU x y`? 
&gt; Well, from a programming point of view, I would say that a domain with ⊤ is equivalent to (). After all, the only thing you can do is refine partial elements until you hit the top ⊤. Hm. I don't understand these remarks. And I can think of lots of things to do besides "refine partial elements until you hit the top ⊤." BTW, you've already given an example of a domain with ⊤, namely sets, with (⊑) = (⊇).
Fortunately, that doesn't work. I'm not entirely sure why, but I think f can only unify with type constructors of kind * -&gt; *, which a partially applied type function is not.
There's no reason you couldn't accomplish that, as long as `Const` is a constructor, and not something that computes (type family/synonym...). Type constructors are injective by fiat in GHC (which is part of their utility; the inference/checking algorithm has problems with type families that it doesn't have with data families etc.). This means that you get to reason from `Const a x ~ Const a y` to `x ~ y`. And this is supported to some degree by the checking algorithm, because for `Const a x` to check as the same type as `Const a y`, it must be the case that `x` is the same type as `y`. This does have certain issues, of course. In combination with impredicativity (which GHC also has), you can use injective type constructors to prove false/loop. You can do that in Haskell anyway, though.
&gt; This does have certain issues, of course. In combination with impredicativity (which GHC also has), you can use injective type constructors to prove false/loop. You can do that in Haskell anyway, though. As long as we preserve subject reduction I'm happy.
&gt; Hm. I don't understand these remarks. And I can think of lots of things to do besides "refine partial elements until you hit the top ⊤." Maybe the following can help to formalize my intuition: we add the requirement that the domain supports an equality test. For instance, `(==2)` is an operation that maps the semantic domain to the domain {⊥, False, True} with ⊥ ⊏ False, ⊥ ⊏ True. This way, the domain is partitioned into equivalence classes of things that can be distinguished computationally with `True` or `False`. This seems like a good formalization of domains suitable for programming to me; after all, you have to be able to tell different results apart. A domain with top ⊤ will only support the trivial equality test. &gt; BTW, you've already given an example of a domain with ⊤, namely sets, with (⊑) = (⊇). Ah, one has to disregard the empty set in this situation.
I believe Tony Morris would get more favorable results with a change in tone...
It was massively harder to implement rank-N than rank-2 when Hugs was implemented. Since that time, the research community has figured out how to implement rank-N, so now that's the standard choice.
It's not as good at it as Hoogle, though. If you search for 'IO a -&gt; IO ThreadId', you don't find forkable-monad. On Hoogle, if you search for 'IO a -&gt; IO ThreadId +forkable-monad' you do find it. However, the package is not included by default, and there seems to be no way to include all of hackage, so this is of little use unless you already know that package you want to search in.
Does anyone actually implement the full rank-2 inference algorithm in a Haskell compiler?
I think we should push the author for some more JVM integration. :)
There doesn't appear to be any source there. Am I missing something?
And project names. 
I think it's just Hugs compiled to the JVM using something called NestedVM. I hadn't heard of it before though, so I don't know how it works or if any changes had to be made to the source.
TBH what I really want is ghc that targets the JVM.
http://nestedvm.ibex.org/ It looks like this could allow jhc and nhc98 to target the jvm too! But, of course, without any form of interop, this isn't actually all that useful...
i want jvm support the haskell standard 2012
What would be the use or advantage of that?
Portability mainly. Imagine just being able to run Haskell wherever there is a JVM. Close integration with a large library of software is another benefit. The ability to take advantage of some of the JVM runtime work. Sun/Oracle put far more resources on their GC than the Haskell community ever could. The JIT on long running tasks is amazing*. *I work in language development and JITs are far better than ordinary benchmarks give them credit for, especially the JVM. If the tasks are even close in a micro-benchmark the JVM is going to trounce it on a long running process.
Some very low-level comments: * auxilliary -&gt; auxiliary * maybe match styles between `reschedule` and `pushWork`/`sched` in the parallel scheduler? e.g. rewrite `reschedule` to look like this: `reschedule state@SchedState { workpool } = do` e &lt;- atomicModifyIORef workpool $ \ts -&gt; case ts of [] -&gt; ([], steal state) (t:ts') -&gt; (ts', sched state t) e * similar comment for steal: rewrite the `Nothing`/`Just x` bit to be `go xs` and `sched state x`, respectively, maybe; in fact, maybe a short combinator encapsulating this pattern would be worthwhile ;-)
&gt; values of type T -&gt; U are fn x =&gt; e, where e is a value under the presumption that x is a value Another standard definition is for values of type T -&gt; U to be `fn x =&gt; e`, with no conditions on `e`. In that case, the value corresponding to `fn x =&gt; rais Fail` is pretty clear.
Slide 69 is perfectly coherent with slides 5-7. Enjoyable slides.
404 :( Found a correct link through Google: http://research.microsoft.com/en-us/um/people/simonpj/papers/parallel/remote.pdf
I was given the impression that Haskell doesn't optimise tail calls anyway.
Is there video of this talk anywhere?
Erlang (distributed computing) for Haskell! Seems to just be missing hot code loading.
For some strange reason it is not on schedule: http://qconlondon.com/london-2011/Videos/ and this is the only talk from ["Functional Web" track](http://qconlondon.com/london-2011/tracks/show_track.jsp?trackOID=424) that is missing. Maybe we could ask them.
GHC has supported hot code loading since 2002, though.
Tail call optimisation really doesn't make any sense in a lazy language. What the tail call returns is not a value but a thunk that will evaluate the tail call. So yeah it sort of gets TCO but the mechanism is completely different. It is better to write code in a non-TCO way in Haskell anyway in most cases. Haskell is probably a nice language for implementing TCO with Java anyway. It can be done by combining the functions that need TCO into a single method and just using jumps. This is only really viable if you do some sort of analysis over the whole program. Haskell tends to do this far more than traditional languages.
https://github.com/jepst/CloudHaskell
:)
I understand that another key feature of erlang is hot loading of modules. I wonder how viable cloud haskell is for building erlang-style servers without this feature, or whether haskell's plugins would suffice and/or play well with the mechanisms described here.
So assuming, only because I haven't checked the code through, that it is more lazy, can you still make rewrite rules like build and fold?\ So while a definition might be possible that is maximally lazy, lazyness is not the only reason for the current strategy for folds.
I think Erlang is far ahead in that area with regard to OTP and supervisor hierarchies; still are these advanced features required to play in the cloud?
I was hoping for one too. Great deck though.
&gt; Tail call optimisation really doesn't make any sense in a lazy language. That's because tail call optimisation can come "for free" in a lazy language. Consider the following code: foldl' f z [] = z foldl' f z (a:as) = let z' = f z a in case z' of _ -&gt; foldl' f z' as In the above, suppose we have a thunk `foldl (+) 0 [1,2]`, whose value we then force. When `foldl'` is called with the non-empty list `[1, 2]`, it ends up building a thunk of "`foldl' f z' as`" or "`foldl' (+) 1 [2]`". This thunk is returned. The caller sees that the return value is still a thunk, and calls again until finally the function returns the non-thunk value `3`. This is exactly the description of a trampoline: a tail-caller returns a pre-bound function (in our case, a thunk!) which the caller should call for us, preventing buildup of stack. Trampoline TCO is one of the "easiest" implementations of TCO when targetting a non-TCO system - but is also one of the slowest implementations. I would assume that higher-quality systems would do other forms of TCO in order to improve efficiency to reduce call-return overhead - the "trampoline off Empire State" technique (i.e. let tail calls collect up to a certain level, then throw an exception) can be implemented in a non-TCO exception-supporting system. So yes, "TCO" is not *necessary* in a lazy language since it comes "for free" with forcing semantics of laziness, but more efficient forms of TCO would significantly reduce overhead. Consider that generally for beginners, we tend to recommend either the strict `foldl'` or the lazy-and-loving-it `foldr`. While `foldl'` is less used than `foldr` (at least for code I've written myself), it is used often enough that being able to compile `foldl'` with TCO would be advantageous. &gt;It can be done by combining the functions that need TCO into a single method and just using jumps. (1) whole program analysis, (2) JVM 64k method size limits. One simple thing to do is to assume that there are two major classes of tail-calls which, when optimized, gives us a large-enough percentage of real-world use: 1. Tail recursion. This is trivial: compile a function in its own method and use GOTO. Possibly the greatest number of "significant tail-call optimization" gets here. 2. Mutual tail recursion of a state machine. Assume that such a state machine is likely to be defined completely in a single module (no need for whole program analysis). For a tail-calling function which tail-calls a module-local function, try to "inline" that function into the same Java method. If the Java method doesn't grow too much, that module-local function stays in there. If it does, call it a day and implement tail-called functions "inlined" in the same method as GOTOs, and return thunks for tail calls that didn't fit. This can serve as a generalization of the first case above. code generation algorithm starts with an empty method, an empty set of inlined functions, and an empty stack of tail calls. Get a function to be compiled, add it to the set of inlined functions, take its list of tail calls and push it on the stack. Fill in the method (keeping enough space for tail calls to be compiled as either thunk returns or jumps). Take a tail call from the stack - if it is to a module-local function, fill it into the method and add it to the set of inlined functions, and push its own tail calls on the stack. If it is to a function already inlined, compile that tail call to a jump. If it's not to a module-local function, compile a thunk return. Repeat until the method would exceed a maximum size, then pass through the remaining tail calls on the stack, compile either to jumps (for inlined functions) or thunk returns (for non-inlined functions).
I've played around with trying to implement certain Erlang-like primitives in Haskell. However, I was trying to implement `linkProcess` and `monitorProcess` without introducing my own `ProcessM` monad. Maybe this isn't that big of a deal, but the reason was so that I could start supervising threads without them knowing about my version of CloudHaskell. I quickly ran into several difficulties, the biggest was that I had no way of installing my own top-level exception handler around an already existing thread. I posted to Haskell-cafe about it [here](http://www.mail-archive.com/haskell-cafe@haskell.org/msg86429.html), but never got any responses. A good implementation of thread-local storage and ThreadIds not interfering with the garbage collection of expired threads would be real nice as well.
Equivalent: lfoldl f z xs = foldr (flip f) z (relist xs (reverse xs)) relist (_:pat) ~(x:xs) = x:relist pat xs relist [] []
Firstly, this is really cool, I hope to see more. I just tried to load up a simple hello world file, and this happened: Main&gt; :l test.hs org.ibex.nestedvm.Runtime$FaultException: java.lang.ArrayIndexOutOfBoundsException: 188416 at (unknown) at Hugs._execute(hugs) at org.ibex.nestedvm.Runtime.__execute(Runtime.java:506) at org.ibex.nestedvm.Runtime.execute(Runtime.java:523) at org.ibex.nestedvm.UnixRuntime.executeAndExec(UnixRuntime.java:376) at org.ibex.nestedvm.UnixRuntime.runAndExec(UnixRuntime.java:371) at org.ibex.nestedvm.UnixRuntime.runAndExec(UnixRuntime.java:370) at Hugs.main(hugs) Process exited on signal 11 Any ideas? __Edit__: I'm running on Windows, is that the issue?
I don't think the Venn diagram on slide 16 is correct. It seems to suggest that side-effecting code is a subset of pure expressions.
Actually the real limit on JVM method size is 8k. You don't want to go above that because the JIT will ignore methods above this size. This is an implementation detail but not one that is going to go away any time soon.
Cool!
We did - see http://www.reddit.com/r/haskell/comments/ghtlf/yhc_is_dead/c1nqbu3
There is a wiki here: [nikki-wiki](http://www.patch-tag.com/r/shahn/nikki-wiki/wiki/), which tries to document how to compile the game. If there is information missing, I would like to add it. Which library is missing? 
I imagine slide 18 should read \forall n l. isPrefixOf (take n l) l, fwiw.
I've worked on similar things -- my solution to the exception handler was to wrap spawning the thread in a function that did a whole bunch of setup and takedown work, and provide a manditory exception handler around that thread. That exception handler, in turn, can read from a list of "shutdown" hooks protected by an MVar. One such shutdown hook is, of course, to remove the ThreadId from those shared locations that hold it.
OTP and supervisor hierarchies are an issue of libraries. A core common message passing infrastructure is super-exciting to me. It means that folks (including myself) can build and share libraries built on these primitives that should eventually be able to provide all the main OTP features.
That's one solution, yes. But what if the package you want to supervise doesn't use your thread-spawning function? This is mitigated to a large degree by the fact that the Haskell community is open source, at least in code that is generally available, so that you can modify the package yourself. But this might change if the ABI of GHC ever stabilizes enough. And dependencies are much simpler if you can rely on unmodified packages. (especially if you want to release your code on Hackage; then modified dependencies become even more problematic.)
Hot loading in Erlang is overrated. I don't remember the sources, but plenty of serious users have described that while hot loading is great for quick fixes, and rapid development, it is not acceptable for managing rollout of production releases. In those cases, there need to be fallover mechanisms between redundant servers to guarantee uptime anyway. So, bringing the system down and back up to restart it, while preserving state (e.g., the xmonad and yi model, among others) should be well supported anyway. In any case, though, the big innovation of Cloud Haskell is providing a mechanism to serialize closures. Given that, it *does* provide the mechanism for hot-swapping new code into running processes, if not yet the library infrastructure to make it seamless.
Hmm... good point. I don't think supervision of the sort you describe should be done on the basis of green threads though. There's too much (i.e. at least some) possibility of a problem on one thread disrupting everything, either through a bad foreign import, or even just resource starvation or heap exhaustion. Genuine supervision needs to be done on the level of distinct unix processes.
Erlang uses a heirarchy of supervisors. The need for process-level supervision (say, by daemontools or whatever) doesn't eliminate the need for thread-level supervision (as provided by CloudHaskell). Your argument applies to Erlang as well, but OTP focuses on thread-level supervision. 
It's not a Venn diagram, just a descriptive figure that happens to look like one.
The Haskell spec says nothing about it, but every serious implementation does it.
&gt; But what if the package you want to supervise doesn't use your thread-spawning function? That may not be solvable. If you need to impose a certain typing discipline on something there is no guarantee that you can do so without modification of the thing you're trying to work with. For instance, I am reasonably confident there is no sane transform you could impose on non-STM-using code to turn it into STM-using-code. (Assuming no use of TH, and even then I don't know that you could do it even in theory.) Then again, it may be solvable. But I would encourage you to go ahead and accept this restriction for now, go ahead and build something that works with it, then acquire some experience with the resulting solution and later come back and ask yourself (and others) whether or not there is a way to remove that restriction. Rather than the alternative of "give up in frustration" and have nothing.
point granted :-)
The "Erlang answer" is that you don't _really_ have a truly redundant system until you have an entirely separate physical machine providing services in your cluster. Erlang of course works with "green threads" and you can in theory take down an OS process with sufficiently evil code. But in practice it's not a problem, and in theory the answer is what I said, that you can't have a reliable system _at all_ on only one physical machine.
The other thing to do is to encourage/force a discipline of being parametric in thread-spawning functions, either through typeclasses or explicit dictionary passing.
I completely agree with what the "Erlang answer" is. My point is simply that it is not an answer to a problem "in theory" but to a problem that exists in genuine production code, frequently, and *in practice*. I've had "evil code" (in the form of buggy ffi bindings or unreliable libs, or frequently just bugs in the GHC runtime that occur rarely but often enough to be noticed in a system running 24/7) take down processes and even theoretically independent supervisory processes more times than I can recall. (Yes, I've filed bug reports and tracked down causes, and yes, things keep getting better. But the point is that a battle-tested runtime is *hard*, and even once you have it, things will still go haywire in ways one would not have imagined or expected. *In practice*.)
I tend to feel that more argument is required in the other direction -- or are we simply talking about the problem of reasoning about operational semantics? If that's the case, the issue is just that we need "better" logics :-)
There might be a separate process watching that file for changes.
I especially liked the link to [Boolean Blindness](http://existentialtype.wordpress.com/2011/03/15/boolean-blindness/). I've been meaning to write a blog post ranting against the evils of `Bool`, but now I don't have to!
"Why functional programming matters" explains why ML fails at modularity.
I wish...that was the version that didn't require various newer versions of libraries on CentOS 5.4 that were unavailable because this distro is so old.
An extended form of the same argument (‘make illegal states unrepresentable’) is presented by Yaron Minsky in his [Effective ML](http://ocaml.janestreet.com/?q=node/82) talk.
Tail call optimization makes perfect sense in a lazy language and all compilers I know of have been doing it since day one. 
I attended and recorded the talk on my laptop, the audio is available here: http://soundcloud.com/sean/gregory-collins-qcon2011-high
That isn't O(N^2 ), is it?
&gt; Worse, object-oriented programming, a species of imperative programming, is fundamentally antimodular because of the absurd emphasis on inheritance Had to get all the way to the end of the article before I realised that the author hasn't been paying attention to OOP programming techniques since... well at least before GoF.
Worth checking out for the melancholy grinning gnome favicon.
He's wrong about Haskell versus ML. John Hughes' paper explains how lazy evaluation is essential for modularity, and I don't see his point in how Haskell loses out on a parallel cost model.
By the way, I recently found calibre. You should definitely check it out -- it did a great job on converting a few other things to mobi format (which Kindles can read) for me.
Ctrl-F "composition". Fail. Section 3 seemed like a clumsy way to say "value-oriented". 
I'd be interested in seeing another go at the topic. His tone seems to waste a lot of words - while I'd prefer to see concrete examples. This line in particular seemed bizarre: &gt; There is no information carried by a Boolean beyond its value Isn't your value defined as the information you're "carrying"? Information gained from context isn't part of your value is it? I don't really understand the post, but I'm obliged to agree with the first commenter, that a it's just hiding the boolean in syntactic sugar. Take Maybe for example. It's either empty or not. 1 bit of information. Just because you get to use pattern matching, doesn't mean its not a dressed up null-check/if.
When we hire someone fresh out of a new college, I tend to ask them what exactly they were taught in school. I make sure to do this after they've got the job so there's no pressure on the answer. It turns out that many schools are still giving the _exact same_ software engineering education that I got in 1998, with OO and inheritance and even the exact same passing nod to the GoF followed by ignoring the hell out of them. Same half-assed proof focus, same useless UML, same same same. It boggles my mind, because it was already getting pretty stale by 1998. Other places do teach more up-to-date curricula, I'm not saying that's the only thing taught, but there's enough places that seem to believe the state-of-the-art in OO hasn't budged one inch in the past twenty years that I don't have a hard time believing this is one of them. Why keep up with a paradigm you don't think is suited to software, anyhow?
&gt; Isn't your value defined as the information you're "carrying"? I think he's referring more to type information. It's easier to see with numbers; what is "3"? Is it three inches? Three days? State 3? When you have an int, you don't "know", you just have an int. Similarly with bools. Haskell has some support for doing this without going insane with "newtype" but it's still not trivial to get all the relevant typeclasses to come along; I'd really like to be able to declare a full-but-typechecked-synonym and I don't think you can. Something like newtype New = Old deriving (*)
Wait, are you telling me that writing a new type system and type-checker for an *established* language constitutes sufficient material for an MSc. thesis?
The issue is what effect your tests have, and what checked information they provide. So let's indeed compare Maybe to a null test. First the null test. We have a pointer `p`. And we evaluate: p == NULL This gives us a boolean. 1 if p is null, 0 if not. And then we keep track of this in our head, manually. If we pass `p` around to other other functions, it's likely that some of them will also test whether `p` is null, because they don't know that we've already done so. Now let's look at maybe. Our test corresponds to pattern matching: case p of Just x -&gt; ... ; Nothing -&gt; ... Notably, in the non-'null' case, what we get back is a _value that cannot possibly be null_. And we use that instead. We pass that to functions, not p, and they don't need to do their own null tests for safety, because that doesn't even make sense. Someone in the comments mentioned testing followed by down-casting. Well, how about if the test provides checkable evidence that '`t : Super`' is actually '`t : Sub`'. Instead of a 0 or 1 that we're just remembering in our heads is supposed to mean something. If we're testing a number for primality, let's have the test return a proof that the number is prime or a proof that it isn't, instead of a 0 or 1. This principle can be extended beyond booleans, too. You can have a: find : (Eq a) =&gt; a -&gt; [a] -&gt; Natural Or, you can have: find : (Eq a) =&gt; (x : a) -&gt; (ys : [a]) -&gt; x elemOf ys where the structure of '`x elemOf ys`' isn't so different from `Natural`, but the types keep track of more meaning; what the term is supposed to represent. And that can make it impossible or at least unnatural to write incorrect or superfluous things that would take place in a language where this information isn't tracked.
Great response. The fact that you retain Just after the pattern match slipped my mind! I understood he wanted more information, but I couldn't come up with a concrete example that was practical. 
Isn't a bool with similar information to what you posted just an enum then? Doliorules' post about Maybe works well because you need to test for control flow first and then use the resultant value (the Just value) over and over. But what if you only need the bool for a single control flow test? Say for example, whether an external operation was successful or not?
Yeah ok, but you're telling me schools suck, while the author is saying object-oriented programming sucks *because the schools suck*. Which isn't reasonable. By the same logic I could argue FP is a waste of time because it's rarely taught. For the record yeah GoF wasn't required reading for me until 4th year of uni, which is pretty pathetic. But that "prefer composition over generalisation" in the preface certainly sticks in my mind.
Seems like it shouldn't be...
&gt; Isn't a bool with similar information to what you posted just an enum then? Well, sure, to the extent that Bool is just an enum too. I should have used a concrete example, something more like: Rather than using a Bool to indicate whether or not your application has "sync to cloud" on, you would use data CloudSync = CloudSyncOn | CloudSyncOff and if say perhaps Haskell made boolean a typeclass instead of a data type then you could do if statements off of the CloudSync values directly. (I would not agree this is a good idea for Haskell, I'm just saying it could be done.) The idea is that these "True"/CloudSyncOns and "Falses"/CloudSyncOffs would then be type-system-protected from flowing where they shouldn't, just as newtypes that overload integers do. Because in theory you could accidentally flow those values into something that's _actually_ deciding whether to (in the Haskell tradition) launch missiles, and now, oh no, you're launching missiles instead of syncing to the cloud! If you really take this to the full conclusion, then in some sense every function that produces a boolean ought to in some sense actually produce a value customized to that exact check you are doing. This may be theoretically beautiful; I have a hard time seeing how it could actually work in practice and produce any sort of actual win for the booleans. (Numbers are a different story.)
Your intuition is correct. from a simple foundation of axioms and rules, you get to more complex computation. The proof would look like: succ(zero) : nat ----------------------------------- sum(zero, succ(zero), succ(zero)) ---------------------------------- sum(succ(zero), succ(zero), succ(succ(zero))) But really I think it would be nice to have the naturals defined as well. In the same notation they would look like: ------------- zero : nat b : nat ----------- succ(b) : nat Note that the first one has no premise. zero just is a natural, and I guess you'd call it an axiom (?) in this case. These sorts of computational models all do things in a similar way -- iterative applications of simple rules that tediously get the result you want. Similar models would be primitive recursion or lambda calculus with Church numerals. From these simple structures you can build more complex stuff -- as an exercise you could define multiplication in terms of sum and similar inference rules. (*edit formatting)
Inheritance is fundamentally anti-modular. Inheritance is an essential part of OOP, which is defined as: Encapsulation + Inheritance + Inheritance-based polymorphism. If you remove inheritance from OOP, you have something more modular, but it is no longer OOP.
 find : (Eq a) =&gt; (x : a) -&gt; (ys : [a]) -&gt; x elemOf ys Do you keep the ability to use libraries to operate on `elemOf` as you could on `Natural`? Or do you translate to a Natural (and thus lose the extra information) before doing arithmetic manipulations?
`draw` seems to take an `IORef Resources` and the only thing it does with it is readIORef it. Could be nicer to have draw take `Resources` directly, and then when you use `draw`: displayCallback $= (draw =&lt;&lt; readIORef r) Similarly, it might be nicer to call animate with `modifyIORef` outside of it, so that the detail of the use of IORefs is localized to the main.
What arithmetic manipulations are you talking about? Or, specifically, ones that would make sense when thinking about proofs of values being in a list? I don't much care that I can't take `p : x elemOf ys` and multiply it by 5 (without coercion), for the same reason that Richard Feynman was dismayed to see children's text books saying things like, 'take the mass of the sun and add it to the top speed of your father's automobile.'
You can even turn these rules into an executable Prolog program through simple syntactic manipulation (leaving out the nat part for simplicity): sum(zero, B, B). sum(succ(A), B, succ(C)) :- sum(A, B, C). Since the execution of Prolog programs represents backward chaining, that’s exactly how you can do it on paper.
As another example, as opposed to doing an equality comparison test (which returns a `Bool`). it is better to use a function of type twine :: X -&gt; X -&gt; Either X (AntiDiagonalX) Where `AntiDiagonalX` is the [anti-diagonal](http://blog.sigfpe.com/2007/09/type-of-distinct-pairs.html) of the type X, i.e. the type of pairs of X that are not equal. The problem is that a `AntiDiagonalX` is a pain to declare for each type `X` you want to use it for. Worse, it can be defined generically for a large class of ADTs, so you shouldn't have to write out a declaration every time. But we do not have a sophisticated enough type system to automatically create these generically derived types (well maybe Oleg does). This makes programming in this style really painful, I think.
Let me ask a slightly different question then: The List and Vec types are very similar (except for parametrization on the length of Vec). Does Vec lose out on all of the List functions and vice-versa? Does every function have to get a new type and be reimplemented, also without the benefit of using the existing/other implementation? Does this have a danger of leading to combinatorial explosion of code (imagine List being parameterized on other traits other than the length)? Or does this not actually happen in practice?
Nope, commutativity follows from the previous two inference rules (but not in the general form you wrote it down, only for each given example of a,b,c).
&gt; OOP, which is defined as: OOP isn't defined.
Yeah I think I am just plain wrong. Thanks. Removing my answer.
Yes, it has to be reimplemented. Further, dependent types introduce a whole slew of new degrees of freedom when designing APIs. Does an indexing function into a list take a proof that the index is less than the length of the list? Does it return a Maybe? etc. So yes, there's the possibility of a combinatorial explosion of code.
OOP only needs combining of data and procedures into one package. It is about separation of concerns. Inheritance is not a necessary requirement of OOP.
&gt; backward chaining Ah, the magic words! Backward chaining is mentioned in the derivation section; so I guess I only need to start at the end (conclusion) and work my way back asking "how can I get to where I am" and keep going until I hit one of my base cases?
Yes, and you can use whatever search strategy you want to in case there are multiple premises leading to the same conclusion (i.e. a disjunction). By the way, Prolog performs depth-first search.
No, but it does use O(N) memory. I'm not sure this is ever better than the standard foldl on a finite list. On an infinite list, the result of the lazy fold is equal to fix (f undefined)
The types List A and (exists n . Vec n A) are equivalent. To the extent that is enough, you can re-use functions. To re-use a list function with vectors and actually get some information about the length of the result, you will probably have to prove some extra lemmas. It's easy to define toList : Vec n A -&gt; List A fromList : (l : List A) -&gt; Vec (length l) A reverse : List A -&gt; List A Then you can easily write reverse' : (v : Vec n A) -&gt; Vec (length (reverse (toList v))) A reverse' v = fromList (reverse (toList v)) (this is would be Agda, if you quantify over the free variables). You will have to do a bit of extra work to show that the result type is actually Vec n A - if you haven't already, you will have to prove length (reverse l) == length l. Conversely, some Vec functions constrain the length of the input - vhead and vtail probably require a non-empty vector, zip expects vectors of equal length. To re-use these on lists, you would have to add some dynamic checks.
Programming in that style without dependent types, you mean. After all, this type has already lost the information that it says anything about the relationship of the two argument. Agda's (x y : A) -&gt; Dec (x == y) seems plenty usable.
Awesome, thanks :-)
I think his point is that Haskell lacks a cost model that can be easily grasped, whether parallel or not. This is well known to people who run into space leaks. Of course, in ML and other strict languages, you swap in a different and more familiar category of space leaks.
You could argue that c language allows you to free the memory, thus garbage collector is not required. And that the fact that people keep forgetting to free the memory just points to poor education, but not to the limitation of the language. Yes, OOP CAN be used modularly. But it not only does not enforce it, it has counter mechanisms baked in (inheritance) that are taught and widely used.
It's definitely a danger. List vs. Vec n tends to cause duplication already in existing languages, because you have to rewrite pretty much every structure-specific function (map, append, etc.) People do have ideas for alleviating some of this, I think. For instance, I gather that work on "ornaments" by (I think) Conor McBride and others in his circle uses the idea of defining lists as an augmentation of the natural numbers (put an element on each `suc`), and then addition becomes append, for instance. I don't think it's a fully solved problem, though.
Note: This is a game written *in* Haskell, not a game *about* Haskell!
No, I'm saying that it's understandable that the author is criticizing based on a poor understanding of OO. If you are teaching at such a school and have no interest in learning about the state of the art in OO, and no significant practical real-world experience to learn about OO, then this sort of writing would be the outcome. Mind you, this person is never going to say OO is working perfectly after all, and I wouldn't agree with that statement either. But OO has _improved_ over the past 20 years, and with a bit of squinting it's not hard to argue that the improvements have largely been by moving in a very "functional" direction. So by not keeping up he's actually missing out on another chance to make his point, in a way.
I may be missing something, but isn't going back to explicit forks and place-holder variables with brittle semantics (must put exactly once or get a runtime error) a step backwards from the existing parallelism tools in Haskell?
Is it practically usable for hot upgrading of code with no noticeable downtime?
what about newtype CloudSyncBool = CloudSync Bool ?
it's been used for that in practice, yes.
The paper might help clarify things: [A monad for deterministic parallelism](http://community.haskell.org/~simonmar/papers/monad-par.pdf)
I would prefer even the most mediocre of blog posts over simply linking to the package. But it's interesting nonetheless.
Exactly. At the risk of adding confusion, the opposite idea is "forward chaining". In forward chaining you start with some primitive facts (e.g., `zero : nat`) and then run the rules in the forward direction generating all possible facts you can derive. Often this is guided by some kind of heuristics to direct you towards a goal, or the rules are specially crafted so that you hit a fixed point. Whereas in "backward chaining" you start from the goal and then run the rules backwards until you can (hopefully) satisfy all your premises with primitive facts. We can think of the two approaches as starting on opposite ends of a hallway and fanning out towards the other end, looking for a path from primitive facts to goals. Clearly both strategies are going to waste some time exploring possibilities that won't pan out. For different sorts of problems sometimes one strategy is more effective or efficient than the other. For example, a lot of "dynamic programming" algorithms are doing exactly the forward chaining approach; and this allows them to work in polynomial time, whereas using backward chaining would take exponential time. But this analogy also helps clarify why the best approaches will mix both strategies (so that they only explore the diamond-shaped intersection, instead of one or the other fans). It's worth noting, however, that both kinds of chaining can be encoded by the other one.
Depth-first, leftmost, first-match search with pruning. To be exact :)
The key is purity here. This system lets you do things in parallel with more control over it than Control.Parallel gives you, but still retaining the determinism that's needed to be able to have a run function that doesn't require IO.
What about the variable-put with its dynamic failure modes? Can't it be re-arranged so that errors can be caught statically (in the Haskell spirit)?
&gt; But OO has improved over the past 20 years So which reasonably mainstream OO languages would you say embody an improved conception of OO? The problem is that there aren't any. They all still focus on the same notion of inheritance and inheritance-based polymorphism as the primary language-supported structuring and reuse mechanism, with all the problems that entails. Working around this is possible, but at that point we're effectively admitting that OO might be a great paradigm if it weren't for all these sucky OO languages. I'm sure it's possible to come up with some variation on OO that doesn't have suffer from the problems that current languages have baked in, but that's effectively theoretical right now. &gt; with a bit of squinting it's not hard to argue that the improvements have largely been by moving in a very "functional" direction. Really? Again, which languages? Adding closures and functions like "map" to an OO language hardly moves it in a "very functional direction". If anything, the mainstream languages have demonstrated the limits of such retrofitting, with Python digging in its heels and saying "enough", and C# effectively turning into a more multiparadigm language that happened to start out cloning Java's half-assed take on OO. 
It's not the languages. It's the (sub)culture(s). There are subcultures where people say "Favor composition over inheritance". There are an increasing number of people who say that when programming in Java, you should by default create all member variables as "final" unless you have a good reason not to. There's been a trend towards more value-oriented programming in C++, I think. C# gives you the ability to declare a non-nullable reference and I'm seeing slow but steady increases in how many people seem to understand why to use them, not to mention how LINQ has at least been partially accepted. It mostly isn't in the languages, it's in how you use them. If you want to argue that they should continue on in the path of functional programming, I won't stop you. I'm not being normative here, I'm being descriptive. This is actually a good thing for advocates of functional languages, because each of these changes is an opportunity to say "Hey, you know how Java programming works better when you use 'final' more often? You know how you've adapted to the requisite coding styles that requires? That's one step closer to functional programming and one less hurdle you have to leap... why don't you try the good stuff over here?" It's actually _bad_ advocacy to pretend this isn't happening in the OO world.
Yes, I really need to write a blog post.... soon hopefully.
If a particular computation throws an exception then it will always do so. It might not always be the same exception, bit that is allowed in Haskell (imprecise exceptions).
It's probably not worth the [trouble](http://okmij.org/ftp/tagless-final/course/course.html#linear)(Oleg).
Yes. Without dependent types you can't "prove" things like `x == y`. However, it is still useful to know why two things are not equal, even without dependent types.
There’s no pruning without explicit cuts though. You automatically get back all the solutions through backtracking unless you say otherwise.
There is at least [the paper](http://community.haskell.org/~simonmar/papers/monad-par.pdf) -- I found it pretty readable.
You can talk about IO easily without ever mentioning the IO monad. Just .... don't mention it :) 
It would be good to cover purity and laziness, since those are two of the most distinctive things about Haskell.
Some really basic functional programming: using "map", partial application and passing your own functions around.
Other Haskell features are pattern matching and algebraic data types. I don't know about the people in your class, but I found it a bit of a turn on to be able to write an algorithm like quick sort and it would, out of the box, work on any data of the ord class.
Unless the students are quite hardcore, I'd avoid things like dumping them into a blank ghci session and getting them to calculate the primes. Start with a part-complete scenario in Haskell -- write all the monadic bits that are needed yourself ahead of time (or use something like [gloss](http://hackage.haskell.org/package/gloss-examples-1.3.1.2)), and leave gaps or default implementations for the purely functional bits that you want them to solve. Then get them to fill those in one by one. That way you can have a program that does something more interesting than spit out command-line output, which can be quite dull, especially at younger ages (as I said, unless they're quite hardcore programmers). Pictures are more interesting; Simon Thompson's book on Haskell covers composition of pictures using functions, which I think is not a bad way to start out in functional programming. My suggestions for concepts to cover would be algebraic data types: recursive types naturally lead to (structurally) recursive functions, so then you could cover recursion too. Infinite structures and currying are some of Haskell's specialisms so it would be nice to sneak them in, but remember that a workshop that few of them will follow is worthless: make sure that most of them will be able to do it (with your help, of course) even if that means cutting down on some of the advanced stuff.
Not really. The clever part is reifying the environment of the closure, but it still depends on having the closure-converted form of the function available in the local statics table.
Wait... does this mean that I can't take an arbitrary function of, e.g., "Int -&gt; IO Int" and send it over the wire!?
How long is the workshop, and how much programming experience do the attendees have? The most important thing would probably be to explain that Haskell is typically *declarative* - you describe things in terms of equalities rather than commands.
They have imperative and/or object-oriented experience, mainly in C# as that's the default language my school teaches. Java and PHP are pretty common too. I hope to open their eyes to the world of FP.
Good idea, they probably won't be too impressed by running the same old exercises, calculating fibs and faculties. I've read Thompson's book, but after looking at gloss, I think I've got something interesting. Thanks for pointing me to it! I've written a simple cellular automata, maybe I could walk them through that as a more hands-on example of what Haskell's strengths are? Otherwise, my primary focus, as you suggest, should probably lie on the type system, infinite types. I'll also try to explain the purity of Haskell and it's laziness, as pelatom suggested.
Thanks, I would have forgotten about those if you hadn't reminded me. :) Of course they should learn about those concepts!
To elaborate on what I think vegai means: the word "monad" can sound scary. The first thing that any old-school imperative programmer thinks when they learn about purity is "well, that sounds like a black box; how useful can it be?" You could cover IO by explaining that Haskell models all input and output as data "contained" in the computer, and provides powerful ways to interact with this "computational container".
This is a key point. You can say that actions that perform IO have their types annotated to indicate this. You don't need to talk about how IO is an instance of.... It is possible to talk about IO actions being first class values, as this is kind of neat.
So what it sounds to me like you're saying is *not* actually that "OO has improved over the past 20 years", but rather that the understanding of some important principles of programming have improved. I'd agree with that, but what's happening is that people are then forced to shoehorn those principles into languages that weren't designed to support them, and in some ways actively work against them. I think the only reason "OO" comes into this is that people have been taught to perceive themselves as doing OO programming, and essentially conflate all the programming practices they're familiar with, with OO. 
If C# is your school's language of choice you should definitely mention F# to them.
So a dynamic test being successful actually establishes correctness of this aspect?
You might want to get ideas from some of the better tutorials, like [Learn You a Haskell](http://learnyouahaskell.com/starting-out#babys-first-functions).
Horrifying.
Never heard of F# before, thanks for the tip!
I've just uploaded a new version that has keyboard controls, since everybody were asking for them... Enjoy!
Here is a video of someone who did the same: http://vimeo.com/6631514 I'd go with neilbrown, let them do something that looks cool and useful.
I do believe that there are some interesting low-level tricks to play to get IVars to perform well. For example, for fine grained computations it may make sense for a thread to spin a bit rather immediately giving up on an unavailable IVar. With simultaneous multithreading (hyperthreading) the "spinning" thread can actually yield/wait and let others make progress. Some Cray machines had special support for this. 
Cool! But it uses an older version of GMP, and the LibGMP site appears to be down as of this moment.
Or unless you fall down an infinite branch in the search tree...
Indeed, and one wonders, why not use the real thing?
&gt; It looks horrible, and we needed GNU C extensions, but we’ve hidden loops in the macros, just as Haskell must be doing. Aahahaha.
There are restricted usage modes for which you can guarantee the multiple-put error won't happen. For, example, it mentions in the paper that if you do use IVars like futures (for the return values of forked computations) then you will never have multiple puts. In that case programs look a lot like their Control.Parallel.par counterparts, but with the monad providing sequencing/strictness. More generally you can use IVars to build up other abstractions that are free from the errors in question. In the package there's a parallel Stream library that is an example of this strategy. 
The game is very entertaining. Getting the libraries to work on linux was a bit fiddly though.
you're a UNSW student, do COMP3161 next semester :)
It doesn't look like he intended this to be practical, he just wanted to see how close he could get to list comprehensions using c and preprocessor macros. It's an interesting exercise, and he probably learned a lot in the process. All he's done is document it here so other people who might be curious can look at it. 
How come? It worked out of the box for me. My only issue was the extreme CPU and memory usage.
I managed to build a small app with Gtk in January but the documentation that I found was very scarce and outdated in several places.
Making gtk bindings is quite much work, this is why the python bindings are now automaticly build using gobject introspection. Too bad this doesn't seem to fit the functional approach of Haskell
Needs a transformer version, if only so we can talk about the party monad.
I just noticed this yesterday, too, when I was trying to pick up haskell GUI programming for the first time. Is this just a recent problem or has it been like this for a while?
I hope I'm not tragically and/or comically wrong about anything!
Looks like it was never restored after the big haskell.org move &amp; purge of old pages. I see that that page is actually listed as one of the un-moved (and thus scheduled to be deleted) pages listed in the old thread `[Haskell-cafe] A few days to go before the old server goes down`.
&gt; Our goal for this post is to get list comprehensions in C, preferably &gt; without learning anything about monads! That line is almost as good as "'Monads' are hard...let's go shopping!" :þ
Most emacsers are unlikely to be able to correct any major mistakes in the tutorial and/or understanding of monads. But I also submitted it there.
I really wanted to write a static binding generator using introspection, but never got around to it
I'm not sure if Snap has a preferred persistence solution, although there are plenty of Haskell database bindings so I can't imagine it'd be hard to roll your own. Yesod, on the other hand, does have a "storage layer of choice" called Persistent: http://www.yesodweb.com/book/persistent. These screencasts show it in action: http://www.yesodweb.com/screencasts
&gt;but I have no idea what to do about persistence. Same thing you would do in any other non haskell web framework. - fire sql statement, get the recordset, translate it into html, serve the page. - get posted html form, translate the data into corresponding sql update or insert statement, construct that statement, run it, return success/error message. If you prefer fancy persistence engines that hide the sql plumbing from you, we have them too. Starting from HaskellDB and ending with yesod-persistence. Not my cup of tea. I prefer vanilla sql, but to each his own. Each web response handler incorporates IO monad, so you do not have to worry about those pesky monads, just code as you would do in any other imperative language. The full power of IO is available to you from any of haskell's web frameworks. 
&gt;would be looking for an "X is well-documented, simple to use and not-too-slow, Snap - Simple, minimalistic, good documentation, easy to pickup, familiar web development paradigm. Routing + simple template or html combinators + sessions, all you need, and nothing extra. Yesod - provides much more. Templates for html, css, javascript, a way to create and combine widgets, subsites, authentication and access controll mechanisms, persistence (with postgres and MongoDb backends), forms with baked in declarative validation, sanitation of strongly typed data (not strings), type-safe urls, so you do not need to worry anymore about broken urls due typos. All of this comes with a price: a much more invoilved web framework, that will take substantial time investment to learn it and become productive with it. There's also happstack but i am not much familiar with it. 
I laughed at that line too. But to be fair, later on in the post he writes: &gt; Haskell does not need zany macros. Instead, it has: &gt; instance Monad [] where &gt; return a = [a] &gt; xs &gt;&gt;= f = concat (map f xs) &gt; Mystery solved. And later... &gt; It’s cool that we can get the same effect in C. Nevertheless: &gt; * Haskell has a few lines of robust, clean code instead of fragile, ugly macros. &gt; * This design may never occur to a C programmer, while monads are ubiquitous in Haskell. &gt; * Monads mesh well with pattern matching, which C lacks. It's actually a pretty good article if you ignore the pointlessness of its central goal. :) 
Yes. At the point I quoted, he's not really even doing list comprehensions, because his code is: foreach(i in l1) foreach(j in l2) side-effect(i, j) which translated to Haskell would be: forM_ l1 $ \i -&gt; forM_ l2 $ \j -&gt; side-effect(i,j) running in `IO`, not a list comprehension. But he does eventually get to doing something list-comprehension-like, from what I can tell.
[http://www.haskell.org/haskellwiki/Gtk2Hs](http://www.haskell.org/haskellwiki/Gtk2Hs) &lt;- this works
It uses an old libGMP.
Snap is database independent, it's up to you what you will use. There is nice [MongoDB extension](https://github.com/ozataman/snap-extension-mongodb) that takes care of the important stuff for you and makes it a breeze to use :) Edit: To make things clear, I am not the author of the extesion, credit for that should go to ozataman. :)
Happstack! I have a small web-app ( ~4000 loc) that I recently ported to Happstack. Not only did the performance increase dramatically(no measurements, you just notice everything loading much faster), but subsequent changes to my data structures goes smoothly once I wrapped my head around the "Migrate" and "Version" typeclasses. Now, once I'm done testing new functionality I just restart the "live" service with the new version and the data gets automatically updated to the new format. That said, I haven't used Yesod, so I can't really make a fair comparison between the two. 
Ok, I'll try to provide a coherent summary of everyone's answers (and what I found out myself): Several Haskell Web frameworks bring their own solutions for the persistence problems: * HAppStack has [HappStack-State](http://happstack.com/docs/crashcourse/HappstackState.html) [other tutorial](http://www.kuliniewicz.org/blog/archives/2009/04/05/happstackstate-the-basics/) [other example](http://patch-tag.com/r/mae/happstack/snapshot/current/content/pretty/happstack/templates/project/src/State/GuestBook.hs), also called MACID which is an object-prevalence style way of keeping a consistent, durable notion of application state. * Yesod has [Persistent](http://www.yesodweb.com/book/persistent), which is an ORM-style framework that uses Template Haskell to provide you with the same kinds of things that Django and RoR do, including CRUD and migrations. Snap has the notion of [Snap extensions](https://github.com/duairc/snap-extensions), which allows you to pass around a connection pool (or a DB connection, or other global data) in a clean way. There is also a MongoDB mapper/store that can be used as a Snap extension, with [snap-extension-mongodb](https://github.com/ozataman/snap-extension-mongodb) Otherwise, you basically have the options of * passing around a DB connection explicitly, as in [this example](http://gitit.net/paste.lhs) (pasteboard app using HAppStack and SQLite) * use HDBC, Persistent or HappStack-State in whatever other web framework you like. I'll probably try my luck with either Snap and HDBC or snap-extension-mongodb or HAppStack/Yesod with their own solutions. Thanks, everyone! (edit: name of Persistent, see comment below)
Nice work! Good to see Regular in action once more. Is this work in any sense Snap specific?
The extension itself is Snap specific. But it's based on the MongoDB package for Haskell which is in no way limited to Snap :)
It is really time for haskellers to stop thinking of libraries as belonging to one framework. That makes it easy to justify re-inventing the wheel instead of taking the small steps to re-use libraries. I am not trying to be mean, but it seems like the community has infected you with that idea. Persistent is just called Persistent- not "Yesod Persistent". There are users of persistent outside of web applications, and nothing is stopping users of other web frameworks from using it (other than "not invented here syndrome"). Likewise, you can use pretty much any other database solution in Yesod. haskellDB or DSH are other options for SQL databases.
It'll be great to have something like Processing or OpenFrameworks in haskell.
&gt; It is senseless to compare the “efficiency” of one data structure that provides different functionality than another. A persistent data structure does more for you than does an ephemeral one. It allows you to have multiple futures, including those that evolve in parallel with one another. It makes no sense to insist that some ephemeral approximation of such a data structure is “more efficient” if it does not provide those capabilities! Sadly, this is nonsense. It makes sense all the time to compare the efficiency of a less complete (or less correct, or less capable, or in some other way inferior) approach with the more complete alternative. If we never did this, there would be no study of approximation algorithms, heuristics, etc. The comment makes some sense as a response to some hypothetical question of "are mutable data structures ALWAYS more efficient than the persistent alternatives?" Then you must answer no, because sometimes they simply don't work. But that wasn't the question asked. The question was: &gt; Why deprive yourself of the benefits of persistence, and insist instead on an ephemeral data structure? And the answer of efficiency, when it applies, is a perfectly good one. If you do NOT intend to make use of the added capabilities of the persistent data structure, then you can compare the two and decide that, for your purposes, the mutable data structure is more efficient. This also misses another major point: while the availability of persistent data structures makes many problems easier, as the author points out, it's equally true that *building* efficient persistent data structures works out in exactly the opposite way: we routinely teach freshmen in college to write asymptotically optimal implementations of several common abstract data types in Java or Python or C#... while optimal implementations of the persistent versions of these same ADTs are in many cases to be found in very recent academic research papers, and are in general far more complex.
I haven't used them myself, but the [restricted type synonyms](http://cvs.haskell.org/Hugs/pages/users_guide/restricted-synonyms.html) in Hugs might be close. It's easier to propose extensions which have already been implemented elsewhere, if you'd like to see similar functionality in GHC. O'Caml's notion of [private types](http://caml.inria.fr/pub/docs/manual-ocaml/manual021.html#toc76) is perhaps related as well, though it permits conversions from New to Old, and of course O'Caml has no type classes.
It would be helpful to have links ([processing](http://processing.org/)?) along with an overview of what makes these libraries so useful. Perhaps it's just me, but the mere pointer to a large, googleable, project doesn't help me understand why this would be great to emulate or bind to in Haskell.
Hopefully Real Soon Now; [SFML](https://github.com/Berengal/SFML---Haskell-bindings)
This is exciting news. However, section 7.1 punts on the problem of running different versions of an executable. This again punts on the problem of upgrading software. Without the ability to upgrade, it is only half of the solution. You would not want to use the technique from this paper in real life. I wonder if someone could build a symbol oracle on top of this paper. Basically, assuming that every executable that will be run in a production environment is registered with a type oracle. The type oracle keeps information about all Serializable types for all future. The type oracle can be used in various ways, but basically all TypeRefs can be ultimately resolved by asking the type oracle. Imagine extending Serializable to Serializable, Upgradeable, and Downgradeable. An Upgradeable type would be able to upgrade itself from a previous version of the type. A Downgradeable type would be able to downgrade itself to a previous version of the type. Imagine that at link-time, upgrade and downgrade functions for all channel types in an executable were integrated by asking a type oracle for all production executables that have existed that used the given type. The upgrade/downgrade functions would not have to be linked in. For really old builds, infrequent types, or ephemeral debug builds, the type oracle could be used as an RPC service that converts any unknown TypeRef to a known (to the caller) TypeRef. 
Darn, I had started to write my own bindings. It's pretty tedious work, I wonder if I should continue?
I expect to be done with the low-level bindings (bringing the C-api into Haskell) today actually. There's still quite a bit of work to be done in making sure they're all correct w.r.t. GC, that I haven't forgotten any functions etc. Also, the documentation needs to be copy-pasted over. When this is done, I'm planning on making a higher-level abstraction to make a proper Haskell API, not just a mirror of the C functions. Feel free to check out my code and decide for yourself what the best course of action is. I'm not averse to joining forces if you're interested either. It's on github for a reason.
Sorry, I get into Processing and OpenFrameworks stuff so much that I forget others don't. Both, Processing and OpenFrameworks, are 2D drawing libraries, as the post author is asked for. They have a simple API but powerful access to alll kind of media functions. In Haskell should be easy to have the same and more, something like a DSL(domain specific language) or easily called from interactive console, alas explorative programming.
I wouldn't point people to HaskGame as it was a misleaded attempt by me to make a pygame-like wrapper around SDL.. SDL is easy enough to use as it is, and my wrapper just takes away functionality. As a btw, is it possible to remove "regretful" packages from Hackage? :-)
&gt;while optimal implementations of the persistent versions of these same ADTs are in many cases to be found in very recent academic research papers Could you name some of these?
Also pointed out in the stackoverflow comments: The question is about the show instance for (lists of) Char. Printing strings directly via putStrLn works fine.
A very good place to start is Okasaki's "Purely Functional Datastructures" http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf More recently, Clojure had some interesting work in persistent vectors built on very flat and wide trees: http://blog.higher-order.net/2009/02/01/understanding-clojures-persistentvector-implementation/ Another interesting one is: http://en.wikipedia.org/wiki/Finger_tree 
You need to email Ross Paterson so he can mark it 'hidden' - dependent packages can still access it as needed, but it won't appear in the package listing, so new users shouldn't find it.
okasaki's "Purely Functional Datastructures" and finger trees defenitely aren't "very recent"
Having been away from Haskell for a while, I noticed this happening a bit too often when I got back into it. I came to the conclusion that maybe shadowing isn't such a bad idea after all, especially in monadic code where you tend to have more variables than usual.
The main work nowadays seems to be focussed on data-types that are typically seen as 'mutation-only'. The one example is the persistent array/vector as mentioned above. Another area of interest is persistent hashtables: http://www.slideshare.net/tibbe/faster-persistent-data-structures-through-hashing I added the other two for completeness sake, since they are definitely a good starting point for those less familiar with this area.
When I started to use F# instead C# I was surprised, that shadowing a variable was allowed. Now I think it's a good idea and I use it all the time. 
One of the things i like in hamlet as opposing to heist, is that hamlet has access to variables and functions in your haskell code. Whereas for heist you have to first prepare those variables with splice function, generally repeating yourself. 
The tradeoff is that you have to recompile your application for a change in a hamlet template to become visible on the site, whereas with Heist you don't.
Considering that you have to recompile haskell web application in many cases anyway, especially when you make changes to template that require referencing a new spliced variable, that's a moot point. It's not like you are comparing hamlet with asp/jsp/php. 
Actually, that's why we have hamletFileDebug. However, given how frequently variables are changed when making modifications to templates, I recommend avoiding hamletFileDebug and just using wai-handler-devel, which automatically reloads your app when any of your Hamlet files change.
Actually, it's not. I use template reloading all the time without changing my splices. And if the designer is not the same person as the coder, then this functionality becomes even more useful.
Automatic rebuilding certainly helps here, but I think it's also useful to have something a little lighter.
Yes, this is one of the things i miss in haskell web development in general. Quick fixability. And not only regarding html, but the haskell code too. In my clojure web-dev environment i can spot fix both html presentation and any running function without complete recompile/reload. In fact once the web server is running, you can continue developing without ever stopping it and unloading the program. Even though i greatly enjoy haskell as a language, but the "ball of mud" feeling of lisp is an enormously powerful and gratifying concept. I am not sure i will ever get over it. And in the end might even get tired of haskell's straight jacket, and eventually come "back home" to lisp. 
I think Haskell is pretty much there already. I don't know if the stuff Michael mentioned below does it for Yesod, but Snap has a development mode which rebuilds the app automatically when source files change.
Yes, i am using wai-hander-devel. But it recompiles the whole application every time, not just one module. And for big applications the wait time is considerable. 
I wish there were a way to disable shadowing warnings for variables bound in do-notation (while leaving the rest of shadowing warnings on). I tend to use pointfree style to get around this when I can (e.g., `do ...; x &lt;- foo =&lt;&lt; bar ;...` instead of calling `bar` on a separate line) but that's not always feasible. Another feature I'd like to see is the ability to locally hide names, so you could say something like `hide foo in blah`, which is just like shadowing the outer `foo` except there's no inner `foo` you're shadowing it with. This should be pretty easy to add to existing systems, though it hints in a much more radical direction about allowing other local namespace changes like locally removing the need to qualify names (or changing the qualifier) much in the vein of Agda's module system.
The c.h.o admins have not set up the database needed by Wordpress. The gtk2hs runs on Wordpress, and there is a backup with all the information, but that backup can't be restored until the database is set up. The right people have been notified and reminded, but no response yet. =(
&gt; It only took a few seconds of staring to see what was wrong with this code: A better way to fix that one would be to restrict the scope of the wrong name: getRegister (CmmReg reg) = do use_sse2 &lt;- sse2Enabled let size | not use_sse2 &amp;&amp; isFloatSize sz = FF80 | otherwise = sz where sz = cmmTypeSize (cmmRegType reg) -- return (Fixed sz (getRegisterReg use_sse2 reg) nilOL) That would've been a compile-time error.
I think 'Make illegal states unrepresentable' deserves its own t-shirt. 
BTW, we're hoping to get that fixed in the next release of wai-handler-devel. Any assistance is appreciated :)
Very nice! I've often wanted normally distributed random values with minimal fuss, and this library provides exactly that. It's nicely documented, too. Thank you, Björn Buckwalter, for writing it.
Ah yes, that scope change would probably have been a good idea.
Shadowing is certainly a good idea, I never came across a single bug of the class that it allegedly introduces.
&gt; locally hide names I usually have a lot of top-level functions because of that.
_It's the wrong variable Gromit, and it's gone wrong!_
I've come across plenty, actually. Furthermore, I've also just found it hard to reason about code using shadowing. Unused variable warnings are more helpful, I'll grant.
Well, my lexical scopes are usually quite small, I consider functions longer than 10 lines (excluding type sig and comments), 30 or 40 with where clauses, to be too long. There's not a lot of scopes you can mess up, there. 
I don't think I go much beyond that either, but even then... And, it doesn't help that my variable names tend to be of the single-letter type as well.
Same here (Fedora), I had to "ln -s libcurl{,-gnutls}.so.4" in system libdir. Nikki alpha looks cool though - nice work!
Isn't that a little optimistic? If you just want to ask a question, use a self-post. This way, it looks like you're linking to a comparison and not to solely a question.
The authors are on reddit. Best place to get a definitive comparison.
I suppose the idea is that, soon enough, it will be a comparison of the two.
 The reason we need tuples in addition to HLists is because of lifted type products (i.e: \_|\_ /= (\_|\_, \_|\_)). As far as I know, the main reason behind lifted type products is an operational concern (ease of implementing compilers).
&gt; An alternative to sugarize H.Lists preserving tuples could be to use {} to sugarize H. lists as &gt; &gt; {x,y,z} &gt; &gt; and desugarize it into: &gt; &gt; x :*: y :*: z :*: {} Wouldn't that conflict with the existing braces/semicolon syntax?
Agda uses the comma operator (without any parenthesis) as a product constructor. so: x = 1, 2, 3, 4, 5 will desugar to something like this: x = (1, (2, (3, (4, 5)))) which I think is very pretty.
Isn't this a little against the whole point of Haskell's type inference? For something like this, you'd have to explicitly give a type signature for the list for many contexts. Plus, this syntax would only be more useful at compile-time, and I have a strong feeling that most uses of heterogenous lists are generated at runtime.
Haskell provides complete type inference for simple programs, but nobody minds adding features where complete type inference is undecidable (like higher rank polymorphism), and requiring a few type annotations where you use them. O'Caml seemed to avoid features like that in the past, but I see that version 3.12 allows some things like explicit polymorphic type annotations.
Good answers!
Your blog always has interesting stuff.
I venture that it may be nicer to have that extra step: (1, (2, (3, (4, (5, ())))) Just so (a,b) always contains a value in a, but another tuple in b. I suspect that may translate to a bit more simplicity. Can anyone with experience with both methods pitch in?
If Python has taught me anything, it is that it is a bad idea to make the language a special syntax escape hatch but not let the user do anything with it. Python's "double underscore" methods may be much-maligned for their naming scheme, but their power is one of the big things that makes Python Python. Granted this is more important in a "duck typing" language, but I think it holds true anywhere you see "special" syntax, in _any_ language. "We" really shouldn't one-off a number overloading scheme, then one-off a string overloading scheme, then consider a list literal overloading scheme, then ... and so on forever. Instead, where ever there is special syntax, there ought to be a way to overload it. The problem is that as languages grow these special syntaxes are like patches of skin that aren't growing with it; they pucker the entire language around it. (In this case I'm not talking Haskell specifically, I mean in general.) The type-based hinting that OverloadedStrings is using strikes me as very powerful and a very Haskelly answer to the problem; I think it ought to be extended to all special syntax in the language; lists, list comprehensions, tuples, record syntax, do syntax, anything that was convenient enough to justify a language syntax will have other uses too just as powerful as OverloadedStrings.
Those are all good ideas. I left this in a pretty raw form because I didn't want to add too much complication or risk adding any restrictions due to an overeager push to purity. In my own personal graphics libraries, I started out with a pure, clean edifice that was far too constraining, and have relaxed it over time. I can figure out my own approach to isolating effects on a per-application basis without much trouble, while it is hard to anticipate all future uses at the framework level.
In Haskell this is always what I do, since it makes writing function (type classes) on it much easier. You just have to write a case for (a, b) and for (), which is very similar to cases for (:) and [] when writing a function on lists.
I tried to figure out how to use this but I am stuck now with a program that compiles but gives me a "Prelude.undefined" runtime error when I call mkAcidState after apparently creating a folder state/Main.Counter with two zero byte files, one checkpoints-....0.log and one events-...0.log (lots of zeroes in place of the ...). {-# LANGUAGE TemplateHaskell #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE DeriveDataTypeable #-} module Main where import qualified Control.Monad.State as ST import qualified Control.Monad.Reader as R import Data.Binary import Data.Typeable import Data.State.Acid import Data.State.Acid.Core data Counter = Counter { value :: Integer } deriving (Eq, Ord, Show, Read, Typeable) data QE = QE deriving (Eq, Ord, Show, Read, Typeable) instance Method QE where type MethodResult QE = Counter methodTag QE = "read" instance QueryEvent QE where data UE = UE deriving (Eq, Ord, Show, Read, Typeable) instance Method UE where type MethodResult UE = Counter methodTag UE = "increment" instance UpdateEvent UE where readValue :: QE -&gt; Query Counter Counter readValue QE = R.ask incrementValue :: UE -&gt; Update Counter Counter incrementValue UE = do v &lt;- ST.get let newv = Counter $ value v + 1 ST.put newv return newv main :: IO () main = do putStrLn $ "Starting/Recovering state" st &lt;- mkAcidState [QueryEvent readValue, UpdateEvent incrementValue] (Counter 0) putStrLn $ "Querying current value" v &lt;- query st QE putStrLn $ "Current value is "++show v v2 &lt;- update st UE putStrLn $ "Value after increment is "++show v2 closeAcidState st -- GENERATED START instance Binary Counter where put (Counter x1) = put x1 get = do x1 &lt;- get return (Counter x1) instance Binary QE where put QE = return () get = return QE instance Binary UE where put UE = return () get = return UE -- GENERATED STOP How would I debug a problem like that in Haskell? So far I had very few problems with runtime errors in code that wasn't my own so I have no real idea how to proceed from here.
you may skip implementation of methodTag or write it methodTag _ = "name" so that argument is not evaluated. "trick of undefined" is used in [Data.State.Acid.Core](http://hackage.haskell.org/packages/archive/acid-state/0.2/doc/html/src/Data-State-Acid-Core.html#line-161) to get type right and that is why you're getting this exception.
I still don't understand why exactly it happened but it does work now. I don't have the time to look at it again in detail right now but I will have a look at the code later. Thanks :-)
what is the difference between this and MACID (happstack's default persistence layer)?
It's a fairly common trick when you want to associate a piece of data with a type class. You can't have a type class member that doesn't mention the instance type or you couldn't tell which instance to use. If the member was `methodTag :: String`, then every use would be ambigous. To get around this you define `methodTag :: a -&gt; String` so that the type can be disambiguated by the type of the argument. The problem however is that the user of the class, often the library itself, needs to provide a value of the type in question which it often cannot. To get around *this* problem, they use `undefined` combined with either ScopedTypeVariables or the `asType` trick to select the right instance. The assumption here is that the member is lazy in the argument (it's useless anyway, since the return value should be the same for any argument), but it does leave open the possibility of it not being lazy, as you did, which would of course make everything fail. The library author could be gracious enough to at least provide a decent error message using `error` instead of a blank `undefined`. Another way around the whole problem would be to use a phantom type instead, like this: newtype Tag a = Tag String class Method a where methodTag :: Tag a [...] 
Could anyone elaborate on how one would effectively serialise, query and checkpoint a Map with this? EDIT: Lemmih just added an example: http://mirror.seize.it/acid-state/examples/KeyValue.hs ...my confusion arose from not considering that serialising references to the functions used to update the structure is actually a darn nice way to specify arbitrary deltas.
Indeed there is the 'tagged' package on Hackage, which I see as the standard way of doing this. Passing around undefined values ought to be a thing of the past.
Marked it on my calendar; should be more fun than studying for finals ;)
That is my question as well, though I thought its proper name was happstack-state. http://hackage.haskell.org/package/happstack-state-6.0.0 Competition is good, though. Happstack-state is very useful and powerful, but the interface seems kind of clunky in some respects (not that I could do any better).
Tried to log in to LinkedIn today and it gave me a captcha that contained no characters from my keyboard. I really didn't want to network that badly.
The sad part is that computer vision is just as good at captcha as we are these days. I say get rid of it.
&gt; You could try to bolt these features onto a different core that doesn't support them out of the box, but it will likely be a painful process. Why will this be a painful process? I've seen lots of very well designed type-safe programs that don't use Template Haskell. I'm not advocating against its use, but TH is certainly no requirement for type-safety.
Spam on hawiki has disappeared since we turned it on, so until the computer vision-based mediawiki bots start trawling, it is best to leave it.
I'm also curious about the reverse. I guess I'll be running some benchmarks tonight.
d much?
Did you try a backslash? :P
Along those lines, I've found that the most natural uses for template haskell come when you already have as fluent and natural a syntax as possible with raw combinators, and you just need some sugar over them to help tidy up the final issues. Even if the end syntax you produce looks almost as if you didn't go through the initial work, it keeps the implementation simple and icky semi-typed code generation issues to a minimum.
Actually, good capchas like this are only defeated by paid humans - which is high bar for our small wiki - these things work at keeping spam out - and if you don't use em your site will eventually be overrun 
I'm not discussing type safety in general, but type-safety in URLs, templates and persistence. The reason is that you need to have a lot of boiler plate code to do this in a type safe manner. Your choices are: * Write a lot of tedious, error prone boilerplate, every single time. * Generate the boiler plate automatically (TH, code generation). The example I gave in the blog post is needing to generate the datatype, render function and dispatch function for a type safe URL datatype. You *can* do that manually, but it will be painful.
Should have been: "Really, ReCaptcha, really?", really. Otherwise, the first word is probably "red" and the bottom part of the "d" got cropped.
I thought recaptcha worked by taking the words that the leading tech in computer vision couldn't figure out during book scans and presenting those.
I sometimes wonder if there's more money in breaking CAPTCHAs than in scanning books. If there is, you'd expect that there'd be some leading tech in computer vision that's locked up in the proprietary vaults of spammers.
I think this is still the best captcha ever. http://imgur.com/C35Cr 
I'd say the reason things get as messy as they do is not because of type safety within Haskell (which is, of course, not hard to do), but rather because of the desire to build a type-safe interaction with languages that are quite distinctively not Haskell. These languages are request URIs for routing, HTML, SQL, etc. In these cases, if you want to interact with them in type-safe ways, then you can't get around the need for some kind of arbitrary code at compile-time to determine the types needed. That's not to say that I think it's a good trade-off; but *if* you insist on the kinds of type system guarantees that Michael wants, then TH turns out to be necessary.
Surprising to see that [this technique](http://www.haskell.org/pipermail/haskell-cafe/2011-April/091028.html) translates directly to OCaml: type 'a gen = 'a -&gt; 'a type 'a fix = 'a gen -&gt; 'a let rec fixtab lim : (int -&gt; 'a) fix = fun f -&gt; let rec a = lazy ( Array.init lim (fun i -&gt; lazy (f f' i)) ) and f' i = if i &lt; lim then Lazy.(force (force a).(i)) else f f' i in f' The only thing to be careful about is memoizing the memo table!
I love the bendyness of these things. Basically the idea seems to be: 1. Write a function that takes an argument and a function that returns the correct answer for every argument except the one you're given. 2. Make the magic oracle function using the original function. Add memoization as desired. 3. ????? 4. Profit
It's true, computer vision can't do recaptchas. The best solution spammers have is to get real people to solve them for them, often on porn sites.
Off topic: the yesod.com front page needs a little attention: &gt;The screencasts give a nice introduction to some of the **advancedconcepts** &gt;and the book**#** is the recommended approach to learning Yesod. As I look again, I'm noticing several other places where whitespace is being munched.
reCAPTCHAs aren't solvable by computer vision...that's the entire point of requiring a human to solve them...
Primarily to incorporate fixes from GHC 7.0.3. http://www.haskell.org/ghc/docs/7.0.3/html/users_guide/release-7-0-3.html
This would drastically change the space performance and number of bottoms present in a tuple as (_|_, _|_) /= _|_ Desugaring (a,b,c) to something analogous to (a, (b, (c, ())) introduces extra bottoms _|_, vs. (a , _|_) vs. (a, (b , _|_)) vs. (a,(b,(c,_|_))) This could probably be fixed by making the tail of the hlist strict, but then you still have the problem that the memory layout goes from flat to a linked list structure. So now to access the 19th element of a 20-tuple, you need to follow 19 links. **Do not want**
&gt; Heist's abstractions for these entities are roughly analogous to the lambda calculus. +1 Only in Haskell blogs would one make analogies to lambda calculus. xD
GHC is specializing so hard for you. "the limit is 0" is a bit surprising (is that with -O1?) Anyway, this is worth a usability report to GHC trac, to turn these warnings into -v2 only ones.
Off topic but has it always been this pink?
In real life, how do you deal with functions changing semantics? How is the function serialized?
The advantage of using `fixmemo` is, indeed, "bendyness" aka modularity. The memoization machinery is separated from the memoized algorithm. According to 1: the passed function is for *every* recursive call, regardless of whether the argument is the same or different. According to 2: making "the magic oracle function using the original function" is also known as recursion. I admit it is mind bending to recurse using the [`fix`](http://hackage.haskell.org/packages/archive/base/4.3.1.0/doc/html/Data-Function.html#v:fix) combinator. [apfelmus's video lecture](http://www.archive.org/details/TheFixedPointCombinatorInHaskell) does a good job at dispelling its magic. (Side note: one *can* define the [Y combinator in Haskell](http://r6.ca/blog/20060919T084800Z.html) but don't worry about it.) To dispel the magic of point 3 we can use inlining to see what happens when combining `fix` with [`Data.MemoCombinators`](http://hackage.haskell.org/packages/archive/data-memocombinators/0.4.1/doc/html/Data-MemoCombinators.html). Let's use the fibonacci function as running example. fibonacci :: Int -&gt; Integer fibonacci = fix fib' fib' :: (Int -&gt; Integer) -&gt; (Int -&gt; Integer) fib' fib 0 = 0 fib' fib 1 = 1 fib' fib x = fib (x-1) + fib (x-2) Until now, no memoization is involved but we can add it to the mix as follows: import qualified Data.MemoCombinators as Memo fib :: Int -&gt; Integer fib = fix (Memo.integral . fib') After inlining `fix` (and function composition) we obtain: fib = let x = Memo.integral (fib' x) in x We can replace the local binding for `x`, which is equal to `fib`, with a recursive binding for `fib`: fib = Memo.integral (fib' fib) Now we can move the definition of `fib'` into the scope of `fib` and remove its first argument: fib = Memo.integral fib' where fib' 0 = 0 fib' 1 = 1 fib' x = fib (x-1) + fib (x-2) This is exactly the definition given as example in the documentation of [`Data.MemoCombinators`](http://hackage.haskell.org/packages/archive/data-memocombinators/0.4.1/doc/html/Data-MemoCombinators.html). 
It changes with every major release. Last release was red-ish I think.
This has bothered me as well, so here you go: http://hackage.haskell.org/trac/ghc/ticket/5125
I don't seem to see Data Parallel Haskell any more? In any case, Repa is broken on this release.
`cabal install repa` seems to work. You should file a bug report if you're having some specific problem with repa.
It was my bad. I didn't realize that I still needed to run `cabal update` after getting the next Platform release.
Ah yes, this is the "successor" to happstack-state, as I thought. The robustness section on the wiki certainly needs fleshing out. It's important to be able to modify the functions that operate on your persistent data!
&gt; acid-state has been rewritten from scratch with the purpose of being more robust and less magical. Here's to the "less magical" part. So far, I can imagine using this... and I'm pretty conservative about data management!
SPJ's view: http://i.imgur.com/ucEs1.png
There is a paradox of module systems in programming languages: the less support a programming language has for modularity the more likely it is to be used to write large systems. Languages like C, C++, and Java are used to write the majority of very large software systems, but none of them have even mediocre support for doing so at the language level.
Oh, how I often wish for proper modules in Haskell. 
As a side note, another framework that has static checking on urls and html is ocsigen (which is ocaml, not haskell), and if paired with pgocaml (postgres library), static checking on the database level (not only that queries are well-formed sql, but that the relevant tables and columns exist). Of course, that is done with ocaml's equivalent of template haskell, but it is done behind the scenes, so there is no new syntax (which is one thing that turns me off from heavy TH use). Ocsigen also type checks parameters to services (that live at given urls), so it is not possible to create links or forms that do not contain all the correct parameters. As mentioned in a comment on the blog post, a framework that takes this even further is Ur/Web - but it does so by defining a new language, which has the double downside of having to learn it and it not having well supported libraries. 
Sometimes you really want to take a Porsche out for a spin, though. 
EDIT: I was stupid.
People have been clamoring for ML style modules for AGES. It was one of the first complaints I heard about Haskell when I started using it five years ago! Can't we at least have an extension or something that does them?
"Full power seldom needed"? That's like saying that "type-level programming is only of academic interest".
No offence to SPJ, but these criticisms don't make sense. - "Separate module language" - That's the whole point. ML modules have a provable phase separation. Without phase separation, you don't have true modularity. - "First class modules problematic" - First class type classes are problematic too, for the exact same reasons. - "Big step in compiler complexity" - This one is fair, although MLton has an interesting solution here that makes modules no more difficult to implement than any other type system feature Haskell has (albeit while destroying separate compilation). - "Full power seldom needed" - The same can be said for almost any type system feature beyond basic Hindley-Milner polymorphism.
The broader context was the 2002 retrospective talk, http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-retrospective/HaskellRetrospective.pdf That was before GADTs, before type-families, before Dreyer's work on modules and typeclasses, before Coq/Agda popularity, before System Fc, and before the pain of composing packages of modules from Hackage correctly. His view might have changed.
Fair enough. On a related note, I think that if you properly view type classes as a form of modules rather than a global relations on types you are led to a better solution of the problem solved in the "Let Should Not be Generalized" paper.
It's worth noting that with associated types, Type classes and modules are now equivalent in power. See: http://www.cse.unsw.edu.au/~chak/papers/modules-classes.pdf I think Modules are cleaner though. 
Is it really as bad as the following quote suggests? &gt; [...Haskell's...] unfortunate commitment to laziness, which results in an unusable cost model for both time and, especially, space
Which is?
For someone who's a bit new to functional languages, and sophisticated computer science, would you be able to explain what the problem is with Haskell's modules, and what's so good about ML's? I've downloaded and printed this paper but I've not read it yet and I'm not even sure if I'll understand it. http://www.cse.unsw.edu.au/~chak/papers/modules-classes.pdf
In addition to Bob's "structural principle of substitution," type abstraction highlights the importance of types in the module system. For whatever reason, Bob said nothing about abstract types in this post, despite his long research career on the subject (and even his WordPress user name). For those comparing modules to Haskell, you should check out Bob's work with Dreyer and Chakravarty, "Modular type classes", from POPL'07 &lt;http://www.mpi-sws.org/~dreyer/papers/mtc/main-short.pdf&gt;.
I've never used ML or OCaml or a language with "proper" modules. So I may not know what I'm missing -- but it seems to me that first-class modules would offer mechanisms that completely overlap Haskell's already existing parameterized types, type-classes and TF's/MPTC's/GADTs. Can you give an example of the usefulness of first-class modules that isn't nicely solved using Haskell's existing mechanisms? 
Could you specify in what way you want to modify them and why it is important? Right now I only have my own use patterns to go after so I'd love to hear about requirements from potential users.
acid-state is the successor to happstack-state. I was one of the main authors of happstack-state (aka MACID), btw.
The 'methodTag' method is most likely going away soon. I added it for reasons that are no longer relevant but I never intended for users to write it manually.
Changing the semantics will change the result of replaying past events. Which is pretty bad. The solution is to make sure that the event log is empty (by creating a checkpoint) or by adding new functions instead of changing the old. Functions aren't serialized. A small piece of data that contains the function's arguments is serialized instead.
In Haskell if you don't have naturally different types, type classes are rather clunky - it's fine to wrap numbers as newtypes, but for lots of other objects this really isn't so nice. Alternatively, you can use the Sheard / Pasalic records-as-modules style, but again this is clunky as you have to use record selectors everywhere - or do what's idiomatic with Parsec lexers and alias all the functions inside the record-module. With a system like PLT Scheme's Units linking modules is just nicer.
It's only a slight exaggeration. You have to spend a lot of time learning how to make non-trivial Haskell programs not produce space leaks.
This doesn't tally with my experience, but then most of the software I write doesn't work on huge amounts of data. On Stack Overflow, I do see a lot of problems where people expect lazy evaluation to be magic and work within a low memory profile when they are using data types - e.g. trees or graphs - that don't naturally stream (unlike lists). In a strict language they would still have the same problems. 
Well, it's pretty obvious really. * Changes that don't change the semantics, just for performance - they don't cause any problems so we can ignore them * Changes to add additional functionality * Bug fixes * Redesign/refactoring, which might even remove some functions
But if the problem is of clunkiness, and not of power, perhaps it is solvable without adding a whole new mechanism which overlaps a lot of existing ones?
You have to spend lot of time to learn anything new. The more interesting question is: once you spent time learning to reason about laziness, is it still difficult?
To not treat type classes as giving a global constraint on types and simply reject most of the problematic programs pointed out in the paper (and require explicit types instead), rather than changing the base language to match the module system. On the other hand, there is something to be said for the constraint-based approach. It would be neat to see someone add GADTs to Chameleon: http://ww2.cs.mu.oz.au/~sulzmann/chameleon/
The author's dismissive tone really puts me off reading his blog.
He probably means that laziness is not the a natural cost model to teach to first year students. The implications of laziness on modularity are pervasive, however. Strict evaluation makes it much more difficult to implement your own control structures, e.g. for monads `&gt;&gt;`, `when` etc. 
It appears to me that Harper is misrepresenting type classes to some extend, their main purpose is not modularity (make on program work on different things), but *overloading* (use one name for different things). It's a slippery slope, of course.
&gt; It is absolutely essential that the language admit that many different modules M be of type A, and it is absolutely essential that a given module M satisfy many distinct types A, without prior arrangement. See, orphan instances are a necessity.
I still don't understand how he can support strict languages where beta-reduction changes the semantics of one's programs.
Where he uses a tensor product symbol, he is talking about tensor product. This is part of a long series. 
Or newtypes.
Most of the software I write does work with a lot of data, and I do manage to get it to work. The problem is not the language so much as the compiler since it's not easy to tell how it will spot optimization opportunities. Even the most minor of changes to the code can make it miss an opportunity to fuse or garbage collect.
Thanks. I was just momentarily stupid.
Try reading this: http://augustss.blogspot.com/2008/12/somewhat-failed-adventure-in-haskell.html It give a more didactic example of the differences. In short, ML Modules are similar to ADTs and type classes sort of smushed together. Modules can be parametrized to utilize different types for given fields and functions in a module. Frankly, I'm not entirely sure that I agree with Lennart here, but that's a discussion for some other time.
Yes - that's true. I'd overlooked the list fusion case, where you have to be careful abut "good producers / good consumers".
Ok, so one issue seems to be that you cannot control weather you import type classes, which causes issues due to the open nature of type class. Not generalising local bindings also amounts to a simple form of rejection, but with a perhaps more sensible outcome for the case where you don't actually need the generalised version. Regarding Chameleon, I'm pretty sure it had EGADTs. At least their implementation is described in Wazny's thesis. They use full implication constraints, though, not the intentionally restricted variant that OutsideIn uses.
In a very roundabout way, he mentioned it in the part where `A : M` is many-to-many. An implementation `A` satisfying several signatures `M` isn't likely to happen without some sort of abstraction going on. E.g. (0, (+1), rec) : (Int, Int -&gt; Int, (a -&gt; a) -&gt; a -&gt; Int -&gt; a) (0, (+1), rec) : exists t. (t, t -&gt; t, (a -&gt; a) -&gt; a -&gt; t -&gt; a) Of course, he didn't go into much (or any) detail.
When using the file backend (which is the only supported backend atm) you can freely change the names and semantics of your transactions as long as you create a checkpoint beforehand. It is only when you don't use checkpoints or when you have a cluster of acid-state nodes that you have to be careful about modifying transactions. 
s/difficult/ugly in my opinion: you have to thunk everything explicitly, but it is not more difficult in principle than converting from DS to CPS.
Big- or small-step value semantics *is* value semantics. If your complaint is that `const a ⊥` *should* (morally speaking) be `a`, then probably yes, I agree. Note however that laziness doesn't give you a free ticket to equational reasoning, *e.g.* the totality condition on the natural property of the `fold`.
Lots of concerning things about this job, but hey, funny to see such a thing being advertised on craigslist.
&gt; Lots of concerning things about this job I'll say. The poster clearly doesn't care enough about the position to bother proofreading the announcement for basic spelling and grammar, for starters.
How do you refer to a function? Do you use the binary address like cloud haskell? Do you use the full ASCII name? Do you use the ASCII name mangled with a chechsum of the build env? Configurable? For both this and safecopy I wonder whether the serialization depends on anything from outside of haskell such as the implementation of basic types or endianness.
Ironic given the apparent importance of applicants writing well.
Example 1: polymorphic datatypes which can be specialized to their parameters; i.e., to support `{-# UNPACK #-}` pragma on parameters. Currently you're forced to use TemplateHaskell in order to do this sort of thing. But TH isn't portable, and is far too large of an autocannon for this job. Example 2: multimorphic datatypes; e.g., making variants of `IntMap` for all the different `IntN` and `WordN` types (and perhaps making it look polymorphic by using type families to abstract over them all). Currently? TH. Example 3: parameterizing libraries by the architecture, platform, or compiler they're built with. Currently you're forced to resort to CPP to do this sort of thing. While CPP is comparatively portable, it's untyped, has a different syntax than Haskell, and is just plain gross. Example 4: Reducing butterfly dependency problems in Cabal by actually having libraries be parameterized by their dependencies (thereby exposing the ones that don't care which version of a dependency they use). Currently this is insoluble; though taking the ML Module approach directly will run into problems with GHC's aggressive inliner, so research remains to be done here.
What happenned with those ideas ? What was the problem ?
&gt; but it seems to me that first-class modules would offer mechanisms that completely overlap Haskell's already existing [...] First-class modules would indeed generally overlap with these other features. The main thing I miss are parameterized modules (see my other reply) and that is something that isn't covered by the current slew of language features, and which doesn't require as invasive a change as adding first-class modules would. However, some people care deeply about first-classness (yours truly among them). If one were to add first-class modules to Haskell and still retain Haskell's ethic of clean orthogonal abstractions, then the first-class module system and these other features would have to be integrated all into a single holistic language; you can't just add modules. ML never did manage to find a nice language that could combine both the module language and the term language, IMO. I'm sure this is part of the reason that the original development of Haskell left them out and opted for simpler solutions like type classes (before type classes became complicated by MPTCs, FunDeps, and TFs/ATs). However, there's some interesting work going on in the dependently typed languages which are adopting type classes with a mind towards first-class modules and integration with other facilities like term inference. IMO, in order to properly handle first-class modules, Haskell would have to finally admit that it's a dependently typed language. On the one hand this would be great, because it would allow us to directly say what we mean instead of resorting to SHE or Oleg's ingenious but often inscrutable hacks. On the other hand, dependent types are really tricky to get right, and noone's figured out the right balance yet. A lot of the preliminary work (for adapting Haskell) has been done by Agda, but then Agda's also exploring a lot of other things like mixfix syntax which, while nice, detract from the goals mentioned here.
&gt; Example 1: polymorphic datatypes which can be specialized to their parameters; i.e., to support {-# UNPACK #-} pragma on parameters. Not true. That can be [done with type classes with associated data types](http://donsbot.wordpress.com/2009/10/11/self-optimizing-data-structures-using-types-to-make-lists-faster/). &gt; Example 2: multimorphic datatypes; e.g., making variants of IntMap for all the different IntN and WordN types (and perhaps making it look polymorphic by using type families to abstract over them all). Currently? TH. Not true. Done with [type families with associated data types](http://hackage.haskell.org/package/unboxed-containers). &gt; Example 3 &amp; 4. The configuration and package examples hold though. We don't have first class modules (or packages), so composing and parameterizing them is very painful.
&gt; would you be able to explain what the problem is with Haskell's modules In short, Haskell has just barely enough of a module system to work. The only thing Haskell's modules offer is namespace management (so it's still a far cry better than what you have in C, C++, Java, and most other mainstream languages). When pushed too hard, though, even the namespace management runs into problems; e.g., witness the size of Hackage, and the problem of avoiding module-name collisions between all those packages. Alas, the namespace manager needs namespace management. I've offered a few solutions to this in the past (based on ideas from [Monticello](http://wiki.squeak.org/squeak/1287)) though there doesn't seem to be enough interest to follow through with it; perhaps when I find some of that mythical "free time"...
That's one of my big disagreements with him. The strict-by-default style is easy to reason about superficially, but is extremely problematic for large-scale semantic reasoning. The lazy-by-default style requires you to pay attention to a lot of things you can ignore in SBD, but it allows you to perform large-scale reasoning which enables you to perform non-trivial program transformations (e.g., list fusion, array recycling). So, for my money, LBD is the only viable choice. That said, Haskell's laziness (or rather, the strictness associated with it) can be problematic semantically. Just look for any number discussions about polymorphic `seq` breaking everything. While LBD is much better than SBD, I think it would be better still if strictness properties were expressed in the type system so that we could reason about them explicitly rather than implicitly.
By the time I started writing non-trivial Haskell programs I've never had problems dealing with major space leaks. Then again, I was reading papers about list fusion and the performance hacks in ByteStrings from pretty near the beginning (as well as papers about [how to use laziness effectively](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.4704)), so I've always had these sorts of ideas at the forefront of my thinking about Haskell. Other than the initial learning pains, I honestly don't understand what it is that people find so problematic. Maybe I'm just being dense.
Obviously they need to hire someone who can spell. 
&gt; Not true. That can be done with type classes with associated data types. Without duplicating the code manually (or using TH)? As I mentioned in the second example, we can get the *usability* benefit by using type families and type classes, but that offers no help for the *implementation* cost. Who writes all those instances? With a system like ML's modules we would be able to write the code once and have the compiler generate the variants (exactly as it does already with `SPECIALIZE`ed functions and variants with different strictness/boxing assumptions), whereas with the TF+TC approach the programmer is still forced to generate the source code for the variants. I'm familiar with your work on adaptive data structures, but it doesn't really solve the problems I mentioned. For some ADT specializations we really do need to give different implementations of the core functions, so the TF+TC solution is the way forward. But for the examples I mentioned the implementations are entirely homomorphic, and we shouldn't be forced to duplicate that code.
There's definitely duplication, and we have to step over to metaprogrammign currently at the moment (as dictionaries are not first class objects). Modules would help.
Once you have modules and type classes, then you run into the tricky issues like the one you mention. It is difficult to resolve them without viewing type classes as a form of modules. And yeah, it seems that Chameleon did have GADTs. I was just going by the CHR paper.
I implemented scanl for Iteratees. It was useful in that I learned more about how Iteratees work.
it's not a secret that the CMU PL folks are bitter about Haskell's success over SML. which is sad. they still have some good points.
My plebeian experience with *ML and Haskell is that modules are overkill for most purposes. Typeclasses may not be cleaner themselves, but overall, they help you write better code because you end up using them more often.
Cmon, that's like complaining about how it's hard to learn how to use a crack pipe.
The user creates a data type that is associated with a Haskell function. It works a bit like this: data MyTransaction = MyTransaction transactionList = [ \MyTransaction -&gt; runMyTransaction ] The function 'runMyTransaction' cannot be serialized but the 'MyTransaction' data type can. There are examples of how to write these structures both manually and with TemplateHaskell in the package.
Hi Don, can you help me to understand what you think is funny about this?
...I always thought the chapter in RWH about profiling could easily be extended to 3 or 4 times of its current length. Even after reading it I still don't understand what all those numbers and switches in the RTS debug output mean... :-/
interesting paper... do you have some more papers on the topic you'd recommend reading?
"I borrowed the structure of Data.Enumerator.List.map for most of this." Copy-pasting code?? This is why I don't like iteratees. At least, not the way it is currently implemented.
&gt; And the moral of the story is: For best results, use Don Stewart as your GHC backend code generator. Is there a compiler flag to do that automatically? -- [camccan](http://www.reddit.com/r/haskell/comments/cbnwp/when_translating_c_to_haskell_use_the_same/c0rh4sv)
You sir, Edward Z. Yang, are one hell of a blogger. Eight posts this month already, all high quality, fun to read and of actual interest. Thank you!
[Obligatory xkcd](http://xkcd.com/853/)
What did you (the author) use for writing and converting literate Haskell? It looks really nice. I want to start using it too; it would be handy to have an editor that makes it easy to deal with (an emacs mode? vim plugin? leksah config?)
I use plain vim to write it -- that probably won't be of much help, I guess. I use [pandoc](http://johnmacfarlane.net/pandoc/)/[hakyll](http://jaspervdj.be/hakyll/) to produce the HTML.
I had the same experience -- came at Haskell via notions of laziness and graph reduction etc. first, and had the power of equational reasoning dawn on me as I went. I tend to suspect reasoning about GHC/Haskell's operational characteristics is probably hardest not for folks from imperative backgrounds so much as folks from theoretical/math backgrounds?
The author is well known for this style (he's like this in real life too), I actually really enjoy his posts but I can see how you might find it unpalatable.
I borrowed the structure. Map and scanl are structurally similar, even for lists. Poking around the code for examples is a useful way to learn to write code using a particular library.
Great report! Thanks a lot to the sprinters (and other darcs hackers and documentors).
Nice, "If Darcs didn't exist, it would be worth writing."
According to my understanding, if i push a Retryable into the queue, then it get popped and runed imediately, what if at the same time, the auto pop thread runs, then this new Retryable will be runed twice, right? -- sorry for my poor english ;-)
I'm definitely interested in seeing follow-up posts for this
great to see these guys still hacking away and getting pragmatic about a git-centric world. maybe it is time to give this a spin again
Perhaps. Though I'd probably count as coming from an imperative background at the time I found Haskell (e.g., I was a major Perl hacker doing webwork and slowly drilling my way down towards the CPU, OSes, and compilers). It's because of Haskell that I've become an unrelenting type theorist; I had no background in any of that beforehand. Then again, I've always been of a mathematical and theoretical mindset (a practical/empirical theorist, if you will). Even in my early Perl days I was very concerned with issues like how an API should be designed in order to enforce design contracts, what the nature of defining a programming language is, etc. The fact that I took to Perl naturally and failed to learn C++ the first few times I tried (one of those times being after I'd learned C) probably says something about well I correlate to imperativists though :)
No, one thread will block because the other thread emptied the `MVar`
I've switched from Darcs to Mercurial, but not because I really wanted to. I switched because of github. If darcs gets a good git-bridge, I'd switch back straight away. Glad to see they've got this on the agenda.
I got it , takeMVar will left the mvar empty, So at any time there is only one Retryable action can be run.
Spot the bug. newtype Parser a = MkP (String -&gt; Maybe (a, String)) deriving Monoid instance Alternative Parser where empty = mempty (&lt;|&gt;) = mappend Hint: we use Maybe to give a denotational semantics to computations which can abort; these come with a natural abort/orelse monoid structure...but the library instance... Seriously folks, we've got to have a more consistent policy on this. I mean, do you believe it? The monoid structure for [x] actually *appends the lists*! Who'd'a' thawt it? Actually seriously, I propose three priorities for making an instance (F x). 1 If F induces a natural monoid structure for all x (e.g. if it's Alternative), choose that monoid. 2. If F is Applicative but does not induce a natural monoid structure itself, make Monoid (F x) lift Monoid x. 3. If none of the above, but (F x) is monoidal at a specific x (or several distinct such xs), make those individual instances. When I prefer 1 to 2, I'm saying "if computation types are themselves monoidal, prefer that to lifting the monoid on value types". For better or worse, that's the current Haskell way: we treat things in computation types as thunked computations primarily (hence all those cheap control operators), and we spend extra notation (e.g. do-notation, applicative &lt;$&gt;...&lt;*&gt;, idiom brackets, ...) to act on the values which computations yield (hence all those noisy ordinary programs). Meanwhile, the fact that we have different signatures of *operations* for MonadPlus and Alternative is historical accident. The fact that both are separate again from Monoid results from our inability to express constraints like (forall a. Monoid (m a)) =&gt; ... Note, I argue for the reuse of the monoid signature, but the classes MonadPlus and Alternative still have moral value in that they suggest some *laws* (although which laws remains moot). In today's language, I'd suggest that Alternative and MonadPlus instances should always be boilerplate copies of Monoid instances. (It's tempting to add that the only MonadPlus instance should be (Monad m, Alternative m) =&gt; MonadPlus m, but that's a tad disruptive.) But the main point: we should be clear about the denotative role of the types in the library, then choose monoid instances to match.
one of the biggest reasons why git is so popular *now* is because when it took off it had a very good svn bridge. I used that bridge for my professional work for over a year before being able to do a full switch to git. Any viable VCS now needs a git bridge (largely because of github). Alternatively, there are some wrappers for git that allow for darcs-like semantics.
I have a question, apart from loop fusion it can't be cheap to use immutable arrays/vectors with millions of elements? I mean to modify them and then return a new copy, making new copies often isn't going to be cheap. I know we can use mutable arrays both locally (ST monad) &amp; globally but if you need a mutable array outside the scope of ST monad your code base starts to get littered with IO which isn't pleasant. Or you end up with lots of unsafeFreeze/Thaw calls (which you can not retain the original after those calls). The code in stack-overflow question is quite elegant, I'm sure as soon as you move to mutable array it will loose it's elegance.
I'd be interested to learn more about this. I'm dimly aware of TopGit, and of course, of David Roundy's Iolaus.
Is there a video of the talk?
Perhaps that's why they need someone who can write.
There's no video, but Ganesh's slides are available [here](http://wiki.darcs.net/Talks/state-of-darcs-2011.pdf) Iago's slides are not yet available, but they will be made so once he's completed his course and obtained the qualification.
There is also this python wrapper: http://raphael.slinckx.net/blog/2007-11-03/git-commit-darcs-record
So... the gist is "don't use our software, so you don't have any bad experiences, so that one day in the future, finally, we can release a flawless version and dominate the world"?
For examples see http://pnyf.inf.elte.hu/fp/Show_en.xml
Ditto!
&gt; Our version of "don't do that" is not to maintain long term feature branches without merging back to the trunk on a regular basis. If I had to choose between long term feature branches and patch associativity (or commutivity), long term feature branches win every single time. In a rencent project I wanted to switch GUI libraries, which is a major change and requires a decent-size branch. Halfway through, I realized I needed to write a better config system first -- but I wanted the config system to branch from trunk, not the GUI branch. In Git, this is easy peasy. It might be easy in Darcs too -- but Git's model is an easy, simple mental model that matches my own. Each snapshot corresponds to a set of files that, at one point, I was working on. If I reorder two patches, that's violated.
great to see all this work going into darcs, keep going!
Yeah, what you said. I can see what the darcs folks are saying though: In git, a destination is a snapshot, and a merge is a _series_ of patches. Thus any given HEAD is treated as either a snapshot or a patch series relative to another snapshot depending on what you are doing. In darcs every HEAD always represents a _set_ of patches, so theoretically, merging any two heads always results in the same union set. I still prefer the snap + patch series approach because it's less magical, but that doesn't make darcs' approach uncool.
I feel that OpenCL has a lot of potential. It's unfortunate that right now there is no real working CL wrapper library: OpenCLRaw is closest, though it hasn't been updated in some time, and reportedly does not link. A release would be very, very exciting indeed; otherwise the community will have to consider writing a competing binding sooner or later.
Ah, got my hopes up :-) I was thinking something more along the lines of being able to use the my-repository-is-a-set-of-patches abstraction
That was great.
Hooray! Best event at ICFP :-)
I support your preference :-) A lot of people feel more comfortable with named snapshots and that's fine. Darcs does support these in the form of tags (named sets of patches), but the inconvenience is that you have to create them explicitly. In the future, I hope that Darcs adds some kind of support for [anonymous snapshot identifiers](http://wiki.darcs.net/Ideas/ShortSecureId) like Git has. Personally, I tend to find in that situation that my needs blur and I want to pick between the two branches of work. Not to be pedantic, but reordering patches in Darcs has no effect on the snapshot; you still get exactly the same set of files. What you may be thinking of is specifically cherry picking between branches, which uses reordering under the hood to perform the merge. It's probably the idea of implicit merging you're not entirely comfortable with.
Are the original creators of the videos unavailable to move the videos to Youtube? (According to [this](http://www.pcmag.com/article2/0,2817,2383839,00.asp?kc=PCRSS03079TX1K0000585), videos need to be manually moved to be saved.)
Google made this, then bought youtube so shouldn't they import them over? (and apply necessary conversions if need be) Personally I would prefer them [here](http://vimeo.com/channels/haskell) though.
&gt; Personally I would prefer them [here](http://vimeo.com/channels/haskell) though. Perhaps I should reupload the type-driven testing talk to Vimeo then when I find the original .mp4 on one of my hard disks.
I found some advice on how to move videos in this wired article: http://www.webmonkey.com/2011/04/the-demise-of-google-video-and-how-to-rescue-your-movies/
I'm putting out a pre-emptive call for videos of said talks right now: The community is bigger than those who happen to turn up at ICFP. I don't mind butchered camera work, but try to do the audio work at least half-way professionally. If it's a room the size of the last one, a speaker and separate room mic should suffice, just let the room mic record and later mix in the questions/discussions.
I guess we could start with removing the current Monoid instance of Maybe. We'd have to explain somewhere how to fix broken code without locally recreating this instance, to prevent future conflicts. Then wait a year, and then finally add the proper instance.
If `suspended pure computations for values in x` are a different type from `values in x`, and thunk-overwritten-by-value semantics hold, how do the types work? Doesn't pure evaluation overwrite a lot of suspended pure computations with values -- thereby changing their types -- as a side-effect? 
thanks michael and everyone else. this is coming together very well. there is still work to be done but the progress is fantastic michael you have been an exemplary project lead, particularly with regards to documentation
sorry, you are going to have to write your own bridge :) I probably won't ever use darcs again by choice unless there is one- github far outweighs the usefulness of anything darcs provides. Although I would still be interested in a libdarcs for haskell applications that need versioning.
The suspended pure computation remains in the same type, but on the second and later occasions when it's invoked, it returns the value directly instead of recomputing it. OK, strictly speaking, one thunk (which does the evaluation) is being replaced by another thunk (which returns the value).
Ah, I see, and this process would be performed by: `resume :: Suspended a -&gt; a` And `Suspended a` can have the operational semantics of the memoized GHC function: `() -&gt; a`? Then if `Suspended` is a Functor instance, you could choose between Identity and Suspended to control laziness from caller-scope, I suppose.
The videos last year were pretty good, no?
I don't remember missing any audio, no, but they were worse than what non-professional equipment can do. Just test it beforehand and I won't complain, but getting rid of the echo that comes with a room mic would be nice, especially for speakers with funny accents and non-native listeners.
Nice!
Would the IORef problem have been solved without MVars if instead of readIORef/writeIORef lennart used atomicModifyIORef? the modification is a pure function so this should right? max
Yes, I forgot to mention atomicModifyIORef as an alternative. 
I'm willing to bet that if we used a lazy trie built just for strings, then the pure memoization approach could be improved by a good order of magnitude.
That's what came to my mind as well. I'm curious though, is atomicModifyIORef any faster or slower for your purposes?
No timings for the thread-safe version?
"Performance of this version is the same as with the IORef. "
Many topics normally considered basic are left uncovered, such as tail recursion jerf: Note, that traditional tail recursion is usually a _bad_ thing in Haskell due to laziness. You are more interested in definitions that are productively corecursive or guardedly recursive. It doesn't matter that I could build the whole structure in one go with a series of nested calls that would be turned into a loop in a strict environment, instead I care how efficiently I can deconstruct the resulting structure or construct the next layer, as I'll (usually) be building it on demand as I inspect it. http://www.haskell.org/haskellwiki/Tail_recursion 
&gt; Definition by structural recursion has the following two features: &gt; &gt; It is always terminating, because we only ever call the function again on smaller elements of the inductively defined type. Only provided the (subtly alluded to) case that you're dealing with *inductive* datatypes. There's no such guarantee for coinductive datatypes. Structural recursion isn't even guaranteed to be coterminating on coinductive types (since structural recursion is permitted to be non-productive).
I'd guess faster since `atomicModifyIORef` should compile down to a CAS instruction, if the architecture supports it. It'd be nice to check that out though.
Using atomicModifyIORef doesn't make any difference for my benchmark.
Very terse; I like it.
Awesome. This is something that the Haskell community needs to be enlightened about! I was thinking about writing something along the same lines, but now I can leave it to the masters. ;) Can't wait for the rest of it.
&gt; You are more interested in definitions that are productively corecursive or guardedly recursive. You got that, Miran? That *totally* goes in the next edition. That exact phrase. :) Thanks for the correction. I'll get a note in there to that effect in the next couple of days. (Busy times right now.)
&gt;how badly Haskell needs an equivalent of Python 3000 Is anyone doing anything like this?
There is some talk of loosening up the library change process for a while so as to make it easier to bite the bullet and finally do things like make Functor and Applicative into superclasses of Monad, and perhaps do things like fix the borked Monoid instance for Maybe by adding Semigroups, split the Eq and Show instance requirements off of Num, and maybe break Num up into something more reasonable, etc. Not entirely 'greenfield' but it would definitely be an improvement.
+1 for Haskell 3000
Another restriction, of course, is that the datatype respect a certain *positivity* condition. As far as Corecursion is defined, what you want is *guardedness*, which is the dual property to structural recursion. Guardedness is of course complicated to guarantee if you have mixed induction and coinduction.
We need [superclass method definitions](http://article.gmane.org/gmane.comp.lang.haskell.prime/3330), and class synonyms, first, or pain is going to ensue. I don't think either is rocket science, but it's gotta be specced and some ghc wiz is going to have to implement it, first. Especially class synonyms, ghc can already do it with UndecidableInstances, all that's needed is checking inferred types for whether they match a typeclass set defined by a synonym.
I could only hear half of this content. Would be great if galois could experiment with a microphone to capture audience comments.
By the way, by "fixed arrow" are you making a reference to something that should be in Haskell 3000? :-) I wonder if there's any use to arrows after realizing Category+Applicative yields arrows...
Examples at http://hackage.haskell.org/package/pez
&gt; Category+Applicative yields arrows... That would be news to me.
And vice versa: import Prelude hiding ((.), id) import Control.Category import Control.Applicative import Control.Arrow -- -- Defining arrow in terms of Category+Applicative -- arr' :: (Category cat, Applicative (cat a)) =&gt; (a -&gt; b) -&gt; cat a b arr' f = fmap f id stars' :: (Category cat, Applicative (cat i1), Applicative (cat i2), Applicative (cat (i1, i2))) =&gt; cat i1 o1 -&gt; cat i2 o2 -&gt; cat (i1, i2) (o1, o2) stars' i1o1 i2o2 = liftA2 (,) (i1o1 . arr' fst) (i2o2 . arr' snd) -- -- Defining Category in terms of Arrow -- id' :: Arrow arr =&gt; arr a a id' = arr id dot' :: Arrow arr =&gt; arr b c -&gt; arr a b -&gt; arr a c dot' = (&lt;&lt;&lt;) -- -- Defining Applicative in terms of Arrow -- pure' :: Arrow arr =&gt; o -&gt; arr i o pure' x = arr (const x) fmap' :: Arrow arr =&gt; (a -&gt; b) -&gt; arr i a -&gt; arr i b fmap' f x = x &gt;&gt;&gt; arr f ap' :: Arrow arr =&gt; arr i (a -&gt; b) -&gt; arr i a -&gt; arr i b ap' f x = fmap' (uncurry ($)) $ f &amp;&amp;&amp; x 
To be clear, I'm not saying it should be done "tomorrow". In fact just doing some planning and putting features into motion that "we" know we need for the cleanup would be enough to satisfy me for a long time. I'm not sure we know everything we need to know yet. (I do think the ultimate answer on the Num hierarchy will just be to punt, or perhaps make it easier to have parallel hierarchies. ISTM there's too much difference between people who want to compute vs. people who want to do math to ever fully resolve that one to everyone's satisfaction; almost by definition if a mathematician is satisfied "mere mortals" are very confused.)
While guardedness (as generally stated) is sufficient, I'm not sure it's actually necessary. For example, the coinductive function: everyOther [] = [] everyOther [_] = [] everyOther (_:x:xs) = x : everyOther xs is perfectly acceptable. As I've written it it's in guarded form, but the general idea can be continued as far as you want. E.g., everyN n0 | n0 &lt; 1 = undefined | otherwise = go n0 where go _ [] = [] go 1 (x:xs) = x : go n0 xs go n (_:xs) = go (n-1) xs -- oh noes, where's the guard? Other than positivity issues, the only thing you need is that the function produces some output after consuming only a finite amount of input. A lot of compiler writers make their lives easier by further restricting themselves to guarded forms, which is essentially the same as/dual to using structural recursion (instead of general recursion bounded by any wellorder that happens to be salient). Once you start mixing induction and coinduction you're driven to abandon both of these simplifications, in addition to the complexities added by going back and forth.
Num is likely just to lose the Eq and Show requirements. That is sufficient to enable the Num a =&gt; Num (b -&gt; a) instance without anything hinky having to happen. Anything further I count unlikely.
Is the dual of a commutative algebra a mmutative coalgebra?
That looks awesome! But are you sure that the arrow laws are satisfied? I take it that you define (&amp;&amp;&amp;) = liftA2 (,) first = (*** arr id) second = (arr id ***) 
I didn't check the Arrow laws, but I would be very surprised if they are violated here... (&amp;&amp;&amp;) = liftA2 (,) first = (*** arr id) second = (arr id ***) Yeah, though it may be nicer to define (&amp;&amp;&amp;) in terms of other Arrow methods, so it can be a default implementation in the arrow class: f &amp;&amp;&amp; g = arr (join (,)) &gt;&gt;&gt; (f *** g) That way, it is clear that Arrow's full power comes from `arr` and `(***)`
As the author of fclabels: is it really the case that the adoption of the package is limited by the fact that it is not Haskell98? I was not aware of this, maybe there is something that can be done about this. Although the charm of the fclabels approach probably disappears without the use of TypeOperators.
Let's just be clear about the positivity issue. data Bad = Lambda (Bad {-negative!-} -&gt; Bad) danger :: Bad -&gt; Bad danger (Lambda f) = f (Lambda f) -- wot no recursion? and then danger (Lambda danger) = ...? Seen set-theoretically, we're defining Bad to be isomorphic to its own set of endofunctions, which is a notorious source of paradox. So, indeed, constructing a total (sub)language is bound to involve a positivity criterion for datatype declarations, as well as some sort of guardedness criterion for (co)programs. It is in the nature of decidable criteria (I consider "decidable criterion" a tautology, but not everyone does.) for totality to be sufficient but not necessary, even when you move beyond notions of 'obviously total' to 'provably total in such-and-such a logic'. Still, as you point out, naive syntactic criteria aren't all that robust. The productivity explanation for your "go" function is inductive (on n) rather than immediate, a nesting of structural recursion inside structural corecursion. I suspect that Agda would be smart enough to spot that. At any rate, it's an unfold whose coalgebra is definable by iteration. I'm certainly in favour of more semantic characterisations of totality, open to *explanation* rather than crude conformity. Even so, one cannot expect any old program which happens to be total to be recognized as such, without further explanation or transformation. There is no magic, thank goodness! Ultimately, such explanations will amount to a transformation into a conspicuously structural form. For example, "general recursion bounded by any wellorder that happens to be salient" is, of course, not general recursion at all, but rather structural recursion on the wellorder. There's a tendency to resist change to one's code and resort quickly to wellorder-based explanations: that's a mode of operation which I consider unfortunate but which should certainly be expressible. These things are quite subtle, though (too subtle for oracles!). Consider the notorious nats :: [Int] nats = 0 : map (+1) nats That definition is only productive because map needs only *one* input element to deliver each output element. This thing (thanks to Nils Anders Danielsson for the example) mapp :: (x -&gt; y) -&gt; [x] -&gt; [y] mapp f [] = [] mapp f [x] = [x] mapp f (x : y : zs) = f x : f y : mapp f zs is perfectly guarded, just as map is (and it agrees with map on all total inputs), but it is not a suitable replacement for map in nats. What this tells me is that we need types with a tighter grip on consumer-producer relationships, if we want to acquire a compositional language of total programs and coprograms. Upshot: oracles won't do; if we want totality, we need a language that lets us negotiate totality internally; we may need to change the way we code in order to best exploit such a language.
The more extensions a package needs the less likely I am to use it. 
I would hate to see fclabels lose the :-&gt; operator, so I encourage you not to lose the TypeOperators bit... and while I mostly hate TH with a passion, in this case it feels to me like a temporary stand-in until the community decides on something better with regard to compiler support for records. In other words, I think you've made the right choices there. Gratuitous use of extensions can bother me, but I think there's a strong case for both extensions in this case.
I am very puzzled by edwardk's comments on the lenses package... in a recent blog post where I did a sort of thought experiment and came up with the same underlying representation for lenses, the main motivation for doing so was that instead of having to provide a custom Category instance, the instance that you naturally get from (-&gt;) worked! So there was no need to hide . and id from the prelude, which is of course rather ugly... but then it looks like edwardk is now saying you can't define a Category instance for what is essentially a newtype wrapper around a plain old function! I'd like to get rid of the newtype, sure... but then you get not only *a* Category instance, but the very best possible Category instance. That said, I'm not arguing for the use of that idea... the criticism that the lens laws are hard to express is a valid one. It's just that it struck me as a bit odd to turn the massive advantage of that technique into a criticism.
I'm afraid i don't completely agree that structural recursion is the only satisfactory explanation for the totality of a function. I also believe that structural recursion on an accessibility predicate explains little about the behavior of a function. Type-based termination (which i happens to be the subject of my phd work :) gives a semantic justification to recursion: recursive calls are allowed on semantically smaller arguments, where the semantics are given by the types. As long as the user has an intuition on the nature of the semantic information, the approach by typing seems quite natural to me... Furthermore, as much as I enjoy the notion of "explaining" the behavior of programs to the user, I believe automation has a real place in the development of dependent types as a program analysis framework. Second remark: winterkoninkje's example seems to require a "general guardedness" principle in the same way that termination of g n (x::xs) = g (n-1) (x::xs) g 1 (x::xs) = g n0 xs on lists would require "general recursion".
I had to try that to believe it! \f g h i s -&gt; fetch s $ fromGetSet f g . fromGetSet h i :: (r -&gt; a) -&gt; (a -&gt; r -&gt; r) -&gt; (a -&gt; a1) -&gt; (a1 -&gt; a -&gt; a) -&gt; r -&gt; a1 \f g h i s -&gt; update s $ fromGetSet f g . fromGetSet h i :: (r -&gt; a) -&gt; (a -&gt; r -&gt; r) -&gt; (a -&gt; a1) -&gt; (a1 -&gt; a -&gt; a) -&gt; r -&gt; a1 -&gt; r 
Perhaps he means that _not_ having the newtype robs you of the ability to compose lenses. Because the proper type of a lens in the lenses package is: Lens s t = forall a. State t a -&gt; State s a These are not eligible to be composed by `(.)`, from the prelude, because they are types headed by `forall` not by `(-&gt;)`. I wouldn't be surprised if it works out in practice, though, due to GHC's implicit shuffling around of type parameters. I can't say I'm a huge fan of the State-morphism view of lenses, either. The `a -&gt; (b, b -&gt; a)` characterization strikes me as significantly superior, in that it directly characterizes what their functionality is usually taken to be. That is, lenses are typically taken to be about manipulating pieces of a type, and when we go to formalize what it means for a type to have a modifiable piece, and what laws the manipulations are expected to follow, what we get is exactly that the types we're interested in should have a coalgebra structure, and lenses are the associated actions. That lenses correspond to particularly well-behaved morphisms between state monads seems like a less fundamental characterization of what lenses are typically trying to accomplish, although perhaps that they do so correspond is due to deeper properties, like State and Store (or whatever you want to call it) arising from the same adjunction.
I fear we are in violent agreement, to some extent. I don't use the phrase "structurally recursive" to mean "syntactically guarded". I'm certainly not opposed to type-based termination: semantic characterisations of "smaller" are bound to be more flexible. Of course, to show that your type system guarantees termination, you will need to check that your "semantically smaller" relation is suitably well-founded, thus exhibiting a reduction to some form of structural recursion (just not the inductive structure of the data itself). Mediating that structure via types rather than syntax should surely be an improvement. I also agree that automation has a place in dependently typed programming frameworks. My tastes tend to be on the less speculative end of the spectrum, but I'm all in favour of giving the machine the systematic things to do, leaving the interesting choices to humans. Packing up a class of termination checking problems as type checking problems seems a very sensible example of that. I'm disturbed by your phrase "dependent types as a program analysis framework". It suggests that programs just *are*, and that we use types to analyse them. I recognize that it's not an unusual way of looking at things, but I'd rather ask afresh what programs should be if we start from a type and try to explain how to acquire something executable. I'd rather think of programs as explanations, than as things to be explained. Your second remark underestimates what can be achieved by structural methods in *combination*. Modulo minor tweaking (e.g. subtraction avoidance), neither winterkoninkje's example nor yours (resetting a countdown when the list gets shorter) nor Ackermann's requires general recursion, just the ability to nest one structural (co)recursion inside another. Your example nests a recursion on the number inside a recursion on the list. Some people refer to this as "lexicographic recursion". Agda's termination checker lets you do that implicitly in certain uniform ways; Epigram's eliminator-based characterisation of recursion patterns is explicit about doing that, can thus be less uniform. Here's a nested recursion that Agda doesn't spot data Nat : Set where ze : Nat su : Nat -&gt; Nat foo : Nat -&gt; Nat -&gt; Nat -&gt; Nat foo (su x) (su y) z = su (foo (su x) y (su z)) foo (su x) ze z = su (foo x z z) foo ze y (su z) = su (foo ze (su y) z) foo ze y ze = y It's just that the inner recursion is on the second arg when the first is a su and the third arg when the first is ze. Breaking out the nested recursions into helper functions is enough of an explanation to shut the thing up. Be careful to be accurate about what constructor-guarded recursion makes difficult (and there's certainly plenty to be accurate about) when arguing for something better. Surely indeed, though, types will do a better job of tracking smallerness than syntax does.
On Mac i get "LSOpenURLsWithRole() failed with error -10810 for the file Leksah.app" and then nothing.
...how can the community process of designing compiler-supported proposals for the long-standing record-syntax design-weakness be accelerated? were there any proposal so far?
Personally, I'd rather see a well-integrated HList-like package than a package that "merely" provides sane accessors. That is if we happen to change the language to better support records, let's go all the way.
Nice metaphor and well written. However it was not obvious that this blog post is a continuation of a previous one; without this context, I was rather confused until I found the link at the end of the post.
What distinction are you making when you say that composing universally quantified functions only works "in practice"? Why wouldn't it work in theory? In what circumstance would it ever cause problems to compose universally quantified functions?
&gt; and it agrees with map on all total inputs Are you sure about that? Seems like it should disagree on all odd-lengthed lists :) &gt; What this tells me is that we need types with a tighter grip on consumer-producer relationships, if we want to acquire a compositional language of total programs and coprograms. I agree entirely. This was what I was getting at with my example. Yes, we can explain it by structural recursion within structural corecursion (as you did), but ---for me at least--- that's not the most obvious and immediate way to explain why it's acceptable. For simple functions like that, I'd much rather be able to discuss it in terms of producer--consumer relations and coroutines. &gt; I'm certainly in favour of more semantic characterisations of totality, open to explanation rather than crude conformity. Spot on.
&gt; winterkoninkje's example seems to require a "general guardedness" principle The general guardedness principle, as I would (and did) phrase it is: a function on codata must always produce some output after consuming only finite input. (The "always" is to ensure that it continues to be productive after its first chunk of output.) This doesn't give a complete characterization of producer--consumer relationships (just imagine `let x = f y ; y = g x in...`), so it's not the end goal, but it's a much more accurate criterion than the syntactic guardedness criterion. It's also a semantic criterion, which I think we all agree it should be.
Sadly, that's a lot messier in the actual lenses package, which is way too reliant on the mtl mess and I don't like very much. If you compose too many lenses, you get arbitrarily deep nested StateT's. You also don't get id at all, due to the need for recursive types. For example, try this: :t \f g -&gt; id `asTypeOf` fromGetSet f g &lt;interactive&gt;:1:27: Occurs check: cannot construct the infinite type: m0 = StateT a0 m0 If, though, one were to do: type r :-&gt; f = State f x -&gt; State r x then you get the same composition (and identities, too), without the rest of the mess. I'm not saying this is the best way to *think* about lenses... indeed, for reasons said all over the place -- namely, it's difficult to express the important properties that make the value truly a lense -- it's *not* the best way to think about them at all. But I am saying that you can embed lens in these types, and recover them back again, and that doing so gives you identities and composition that are actually compatible with the Prelude definitions of those terms. Not having to hide Prelude symbols or import modules about categories is a HUGE advantage over other alternatives.
To be honest, I've never gotten beyond wondering what HList is; the reason being that if I go to look it up, I am hit with a wall of two dozen modules, and just importing Data.HList pulls in half of them. Would it be possible to pull out the "essence" of HList into something with maybe one module and 5 to 10 exported symbols? If not, then it seems like a really bad idea to build that kind of complexity into the language to solve a problem when there are multiple *far* simpler alternatives available.
I've added links to the beginning. I hate managing inter-post navigation for series... if only there were a Wordpress plugin that did it for me...
Well, yes. The HList library design is full of olegy idiosyncracies. The records package (based on grapefruit-records) is cleaner, but not easier to wrap your head around. The main problem with both of them isn't library design, though, but that the type level programming involved to fit heterogenous collections into Haskell just isn't well supported in GHC (records uses all of TypeFamilies, FunctionalDependencies, OverlappingInstances and UndecidableInstances, go figure), so reading the respective papers before using them becomes obligatory. I was talking about feature set, though. If you want flexible, extensible datatypes, they're the only option, being powerful enough to be used to roll out e.g. [OOHaskell](http://homepages.cwi.nl/~ralf/OOHaskell/).
What version of OS X? Can you try launching it from a terminal window with... /Applications/Leksah.app/Contents/MacOS/Leksah Google of LSOpenURLsWithRole 10810 returns quite a few results that might be relevant.
I do wonder about positivity though. It seems that, like erasability, it may be a property of *usage* rather than a property of *types*. That is, I could imagine that it may be fine to allow types to appear negatively in their constructors' types, provided that at the use sites we perform checks to ensure that the same value is not used in both positive and negative contexts (as is key in your `danger` example, as well as the other examples I've seen). Do you know if anyone's looked into this idea before?
In theory, the `(.)` function does not accept or return universally quantified functions. When we write: f . g if it works, we're relying on the fact that the compiler is turning that into: (/\a -&gt; f@a . g@a) And I don't have an example, but it's not inconceivable to me that the compiler would sometimes fail to insert the abstraction around it, and you'd get merely f@t . g@t for some skolem variable `t` or something, instead of a properly quantified variable. At least, that seems like the kind of annoyance I'd sometimes run into with GHC, since its guessing of this stuff isn't always as robust as one might like. Maybe it's not a real problem, though.
Apologies for typo mapp f [x] = [f x] indeed. I'm not sure what sort of producer-consumer discussion you have in mind, but it's certainly worth thinking fresh thoughts about. Interleaving "always" and "eventually" explanations in some way or other will surely prove relevant in some form.
I don't know if anyone's checked whether it's enough to guarantee that one chooses to go with either contra- or co- variance but not both. What is clear, however, is that no such restriction is expressed in the types. Erasability is a matter of optimisation (until one chucks in some internal parametricity, I suppose). Positivity is more fundamental to consistency/termination. That's not to say one should give up hope on negative occurrences. They are often justified by some sort of recursion on indices, as in modelling function types by functions between the models of the source and target types, or as in step-indexing. In any case, I'd be very interested to learn of good usage patterns for these dangerous dataypes. We begin a long road.
Okay, this seems pretty straightforward to me. If you doubt it'll work here, then you may as well doubt that `3 + 5` will work, because after all `3` and `5` have universally quantified types. Yes, System F is more explicit; but we're not working in System F... and Haskell does this stuff all the time. Yes, this really does work. If you have a `type Lens a b = State b x -&gt; State a x`, then you really do get (.) and id from functions, and they work. It's also completely valid Haskell 98, requiring no extensions at all. I'm not out to avenge the lenses package, which is really honestly a mess; just puzzled by those two *particular* criticisms of that expression of lenses, which are not true at all. While the lenses package does require plenty of Haskell98 extensions by virtue of depending on mtl and using the type class stuff there, the particular criticism of rank 2 types and the lack of a Category instance both come entirely from Edward insisting on adding a newtype there; neither are among the many faults of the package itself.
Perhaps that example wasn't a good one; it may work fine. However, one place it definitely fails is when passing `(.)` or a section of it to higher-order functions. For instance if you write: comp :: Lens b c -&gt; Lens a b -&gt; Lens a c comp = (.) You will receive a type error. The type of `(.)` simply cannot be instantiated to the type of lens composition, so it must be eta expanded such that `(.)` is fully applied, so that GHC can apply its type passing shenanigans. Not that that's particularly arduous, but it's not ideal. Maybe MLF could give `(.)` an appropriately general type.
Cool, thanks.
So just to clarify, I was wrong about at least one thing; namely, this is not Haskell 98 either way. In addition to what the rest of the discussion is about
It's like being at ICFP!
It's not just "slightly computer-related". I'm new to Haskell and monads; this is my way of grappling with pure programming. And /r/haskell is the best place for anything Haskell-related. Anyone who thinks that this wallpaper is just a silly Internet meme would say (if they get the joke at all) that it doesn't belong on /r/programming, it belongs on /r/haskell.
He seems to ignore the issue of resuse and hence "Why functional programming matters" completely. In particular, his stream functions are quite unnecessary, you can use plain lists in Haskell. 
I didn't understand is dismissal of unconditional beta reduction. Also, I'm pretty sure that Agda or Coq would better meet his requirements for a first year language, which is something that I've been thinking lately.
I like laziness because it lets me write code that makes use of time travel.
I didn't understand most of what he wrote. Probably just my shortcoming for not being familiar with the academic jargon of his field but after staring for some time I still have no idea what he means when he talks about positive/negative types, limits/colimits, inductive/coinductive and a host of other terms. What the hell does he mean that lazy languages have products and not sums while eager languages have the reverse and what has that got to do with beta reduction? Apparently there's some overloading of the terms "sum" and "product" when talking of types. 
I've been tempted to use Agda for introductory FP, as much for the editing environment (a fantastic motivation for the use of types) as for anything else.
A common complaint about Haskell is its type error messages can be confusing. Not sure that introducing dependent types to new programmers would be a good idea. (And it would be cruel to expect them to use Coq or Agda without ever explaining the type system to them.)
He doesn't ignore it: &gt; True reuse operates at the level of abstractions, not at the level of the code that gives rise to them.
Laziness is useful because it allows you to efficiently snap two functions together, as if you had two threads, or coroutines, where the first thread or coroutine only "runs as far enough ahead" as is needed at any point in time. But it's more flexible than that, because rather than simply demanding n elements of a list, you can skip ahead to the nth element, and not evaluate the previous elements at all. In almost any other language, you would mess around with threads, coroutines, or manual thunks, to achieve this effect - or (horror!) intermingle the two functions, breaking reuse and modularity - or just give up and accept the inefficiency. In Haskell (where pure code is concerned, anyway) it just comes naturally. I have argued (at Fun in the Afternoon last year) that far from abandoning laziness, we actually should go "beyond laziness" (at least as a research exercise to see how useful it is), as Conal Elliott already has done to a limited extent with his `unamb` combinator. My basic idea is that we should build a language with an "open runtime", that lets programmers control evaluation order, and get and provide information about costs and availability of values, in a more powerful way than Haskell allows. (Obviously this would violate referential transparency if such code could be freely intermixed with pure functional code, so I'm thinking in terms of metaprogramming, with a two-level language, where the *top* layer is where most of the code is written, and referential transparency is supposed to be preserved.) This could not only benefit efficiency and perhaps begin to address the concerns about the alleged unpredictableness of laziness - it could even have spinoff benefits for things like user interfaces, which was the topic that led me to the idea in the first place!
The idea of assigning "costs" to values is pretty interesting. It feels like the transition from Dijkstra's algorithm to A*. If we can get a good cost function, then we can find the optimal balance between call-by-value and call-by-need.
I'm not sure I understand what that means. Also, in a sufficiently nasty interpretation, he says that polymorphic types are unnecessary, because reuse operates at the level of abstraction ("This is a queue"), not at the level of code ("One queue is enough for all element types") that gives rise to them.
I like neither Haskell's conflation of data and codata, nor this mismatched kludge of ML datatypes for data and ML module-hacking for codata. The logic appears to be that codata are defined by their destructor interface, and modules are how ML does interfaces. But the effort involved in switching from one to the other seems rather a lot, and gently interleaved constructions (e.g., multiplexed streams (which always eventually switch), or productive stream transducers (which always eventually stop consuming input and produce some output)) seem further away. I'd rather miss pattern matching as a means of observing codata. The diffuse computing environments we're moving towards mean not only that we must be aware of the distinction between parallel and concurrent programming, but also that we must approach both with the respect that questions the old self. While "eventually" programming will always be with us, "always" programming will eventually be just as important. And it will be helpful to see the difference.
The traditional argument goes "my language is better at solving *these* problems; the problems your language is better at solving are not important". Language advocacy gets in the way of learning how to solve problems.
I have fiddled around a bit and it appears that you need to specify at least a few laws about the interaction of `Category` and `Arrow`. For instance, arr f . x = pure f &lt;*&gt; x Unrelatedly, note that the `(***)` does not make a good foundation for arrows because the order of arguments matters. f *** g ≠ arr swap . (g *** f) . arr swap where swap (x,y) = (y,x) It's better to start with `first`. 
Laziness has the joyous property that you can write down any old cyclic rubbish and get a value out if there's any sensible way to resolve the data dependencies. However, the type system isn't much help at detecting paradoxical uses of time travel. Like Oedipus, you can murder your father and marry your mother. Unlike Oedipus, you can do this before your own conception. But there's still time to change all that. I'm in favour of time travel. Too often, programming is aligned with time when it should be aligned with the structure of data. I wish we could achieve it by means more subtle than pervasive laziness.
One ingredient to useful thinking about finer control of evaluation, and the value-computation distinction more generally, is Paul Blain Levy's work on call-by-push-value. CBPV is precise enough to allow local control of evaluation strategy. It also makes a clear value-computation distinction. Good food for thought.
It's tempting to use Agda, but avoid dependent types in the first instance. So, we'd basically have an editing environment for Turner's Elementary Strong Functional Programming. Agda's support for type awareness *before* you make your error is superb, but similar problems apply as with Haskell when errors are encountered. And mechanical case-splitting is just lovely. But I don't dare. I can't afford to blow time on emacs driving lessons, or how to type unicode (for people who don't have LaTeX burned into their brains). ghci also makes experimentation a little easier. Running programs is good!
I'm on the spec case, but a ghc wiz I'm not. It'll be interesting to see the consequences for the library...
Yes, quite - and costs are not something that the runtime or compiler can be left to estimate. In the context of FRP in particular, user input could be selectively demanded (think of popups which don't appear if the information requested by them isn't needed), and the user might have an opinion about how costly it would be to provide particular inputs!
Now, if only someone could make a dependently typed, total programming language (where maybe general recursion is allowed in some monad), with a neat IDE which allows interactive program/proof creation... Oh wait. That's you. I'm still waiting for Epigram 2 dammit!
&gt; What the hell does he mean that lazy languages have products and not sums while eager languages have the reverse http://www.reddit.com/r/programming/comments/gwqa2/the_real_point_of_laziness/c1qvv61 &gt; what has that got to do with beta reduction? Essentially, beta reduction is a sort of refactoring you can do to lambda calculus code involving *functions*. In a lazy language, we can always do this refactoring; in a strict language, we have to be more careful, because moving things around this way can change semantics. Harper's argument here is that there are other refactorings involving other data types (in particular sum types) that don't work in lazy languages. A sum type is Either, a product type is a tuple. Limits/colimits come out of category theory, and they generalize products and sums respectively. (But this is not so interesting unless you also know what terminal objects, pullbacks and equalizers are). Inductive/coinductive refers to how we might define a data structure. Intuitively, an inductive structure is finite, while a coinductive structure is infinite.
the rules preserving list for equations of a lazy language are all the standard equivalences of introduction-followed-by-elimination rules used by convertibility checks in dependently typed languages, while the one example equation for a strict language is an elimination-followed-by-introduction rule which is AFAIK never used in convertibility checks. Does this makes lazy languages better because they admit rules of reasoning that correspond to the convertibility equations in dependent type theory, or does it mean that the convertibility equations in dependent type theory are unfairly biased to the rules given by a lazy language? 
I don't know if Harper is deliberately ignoring strictness annotations in constructors, is unaware of them, or thinks that they aren't powerful enough to provide the guarantees he wants.
Well put!
From what I can gather, it's the last one, but he doesn't help to clarify his position when his responses to comments are literally "I’m sorry, you are wrong."
Partial Solution: Don't use unicode. ;) (I think I'm preaching to the choir here though...)
I'm in over my head a bit here, but I've always felt that laziness is a natural fit for Haskell. It seems like functional programming is about being declarative and defining relationships. In a strict language we are *forced* to consider the evaluation strategy of our language, and our code becomes less declarative. Does that make any sense or am I totally off base?
I don't know. But the only difference between: data Nat = Zero | Suc !Nat in Haskell, and the ones in ML is the single extra bottom element. And if you're defining a function by recursion on the naturals, I don't think that bottom element makes any significant difference for your induction proof. Of course, no one uses the above natural representation in Haskell, because it has abysmal properties for both calculation, and for the things unary naturals are actually good for in practice (like `take (length xs) ys`, where you want to avoid forcing xs immediately).
Actually, in ML, there's a whole IO monad hiding behind every `Zero` or `Suc` constructor.
Yeah, well, that's another thing that confuses me. All this talk about how ephemeral structures are essentially useless and whatnot, but we're going to have them---and completely unmanaged side effects---anyway. And laziness is only useful for putting a nice face on all that ugly stuff, because I can inefficiently mimic codata using inconvenient encodings. I didn't find the article particularly persuasive.
A bit difficult when the manual is all written in Unicode (that was my experience with Coq, anyway :-)
Agda takes this even farther. I think it's actually pretty great, but it can be frustrating when the ASCII you typed in is only slightly visually different from the unicode version, but only the unicode version is defined.
I think there are a lot of benefits to adopting unicode, though. Informative types and small scopes make single-letter variables very handy, and having syntactic classes of variable names improves this (e.g. `x` and `y` vs. `a` and `b` vs. `S` and `T`). The big win, though, is with infix operator symbols. These are in desperately short supply, so anything that adds more is helpful. My only issue is that I think these should be better documented when introduced. One should draw attention to the fact that this is a non-ASCII symbol, and perhaps provide instructions at least for how to enter it with the emacs TeX input mode rather than force the user to recognize that the symbol is slightly different than another one, and then lookup how to enter it themselves.
Coq works reasonably well as an intro to functional programming. The availability of CoqIDE sets a usefully low barrier to (text) entry, and one can do a lot of the usual FP things in Coq, then turn right around and prove properties of those programs. I think perhaps the most painful issue that comes up when just trying to write programs is termination checking. After that pain comes the lack of libraries for writing many kinds of "real world" programs. Smoothing out interop between Coq/Agda and OCaml/Haskell might be a productive strategy for getting some of the best of both worlds.
Does ML not have bottom? I thought it was Turing Complete so you can have functions which do not terminate, i.e. return bottom. EDIT: Reddit ate some formatting.
ML does not have bottom, because bottom cannot be a value in a strict language.
If you do the denotational semantics, there is a bottom. However, you can get away with not including it in certain types. For instance, you can model the natural numbers as a set instead of a domain, and then functions `A -&gt; Nat` are interpreted as mathematical functions `[A] -&gt; [Nat]_bot`, where `T_bot` is the lifting of a set to a flat domain by adding a bottom element. Then, you compose functions the same as with the monad/applicative for `A -&gt; Maybe B`, meaning they're strict. So types do not have bottom, but the functions are effectful, operating on liftings of types in a certain way.
The Agda library definitely has some bad spots in this respect. For instance, there's syntax for sigma types like: S[ x : A ] B where the S should be a sigma of course. Except the `:` should also be a unicode character that looks almost exactly like a colon, but isn't, because colons are already reserved. I don't really think this is an acceptable state of affairs, but I guess it's a work in progress.
Admittedly, I have found debugging such errors rather funny in a, "That's not a colon you're typing" kind of way. The errors for such mistakes are usually decipherable. Missing whitespace with in- or mixfix identifiers is, I think, more cryptic, and points to a real usability snag. 
Check the comments. You can, I infer (I am nowhere near competent in this area) that one can still squeeze bottom into the defined Naturals, even with strictness annotations. Someone provides examples in the comments.
[Quoth the time traveler](http://en.wikiquote.org/wiki/Tenth_Doctor#Blink_.5B3.10.5D) (sort of): People assume that evaluation must be a linear progression from expression to value; but actually from a lazy, non-linear, non-subjective evaluation strategy, it's more like a big ball of wibbly-wobbly timey-wimey...stuff.
Update: Google's now postponed the shutdown and is automigrating videos to youtube. http://googlewebmastercentral.blogspot.com/2011/04/update-on-google-video-finding-easier.html
I replied in the comments. As dolio points out, you can of course still have `undefined :: Nat`, but you can also be assured that if you have a strict Nat in head normal form, then there's no bottoms lurking inside of it. I know Harper disagrees, but this seems to me more than enough to allow straightforward inductive reasoning.
*crickets*
...*crickets*...
Not really. The comments are full of fail on both sides. Given the proposed definition `data Nat = Zero | Succ !Nat` you can't do all the "horrible" Bob Harper is complaining about. Yes, it's true that some value claiming to be a `Nat` may in fact send your program into a loop rather than return a natural number, but this is **exactly the same in ML**. ML is not a total language. It is impossible to remove the one last bottom in `Nat` without proving that all functions purporting to return a `Nat` are in fact total functions. (And you'll have difficulty proving that without proving that *all* functions are total.)
[GHC/FAQ: Why isn't GHC available for .NET or on the JVM?](http://www.haskell.org/haskellwiki/GHC:FAQ#Why_isn.27t_GHC_available_for_.NET_or_on_the_JVM.3F)
To make you happy, yes the Java programming language sucks (at least on #reddit) but ... it is still one of the most popular platforms and that means that is backed by major companies and much has been invested for production JVM deployments. Top companies depend on JVM based applications. The front-end language does not really matter in some cases for that language to be compiled to JVM bytecode. And you see that with front-end JVM languages like Scala and JRuby, Clojure, Mirah, Jython. For the most part, you are just working with different syntactical sugar. Those kind of procedural/OOP languages can be easily compiled to JVM bytecode and they can be deployed with existing systems. On GHC/Haskell to JVM bytecode, how is that any different from compiling to any other platform? I wanted to post at least pose the question. If GHC compiles to x86/Win32 code or x86/Linux code or iPhone, I assume it is possible to compile to JVM bytecode. And on the Java platform, I think there would be more of a need and a want for GHC to JVM bytecode compiler than an iPhone based compiler. 
&gt; Intuitively, an inductive structure is finite, while a coinductive structure is infinite. Or, more precisely: * Inductive types are built up from the ground (in finite steps). Thus, we know they're of finite total size, and hence we can expose them (i.e., do case analysis to see what value they are), in their entirety, in finite time. * Whereas, coinductive types are constructed from the top down (in finite steps. Thus, we can never really be sure whether they'll make it to the "ground" or not. Hence, we *may* be able to expose them in their entirety in finite time (if they happen to ground out), but there's no guarantee. There is, however, still the guarantee that we can expose a finite portion of them in finite time (if you ignore issues of non-terminating functions in Turing-complete languages). * For non-recursive types, there's no difference (because they're always only one level tall). This is often confused with an orthogonal issue about how the values of these types are specified (e.g., Bob Harper makes this exact mistake in the above post). The orthogonal issue is whether the type is defined by providing the behavior of its constructors (e.g., `Nil`, `Cons`,...) or by providing the behavior of its destructors (e.g., `length`, `map`,...). To see why this is orthogonal, consider record types (aka `struct`s in C land, and the `struct` part of objects in the C++/Java tradition). Often, the behavior of records is defined precisely by the behavior of the various functions for projecting its members; and the constructor is just whatever it takes to be able to define those projectors.
In a lazy language you are still *forced* to consider the evaluation strategy. So the issue is: does lazy evaluation or strict evaluation allow for a more declarative language? This probably actually depends more on the language features and implementation than evaluation strategy. Do keep in mind that a strict language can still have laziness where it is useful, just as a haskell programmer can add strictness annotations. So a default strict language with some lazy aspects could be more declarative, for example.
Actually, to be fair, in sml, your program will go into a loop in attempting to produce the value in the first place. In fact, there isn't even a "let rec" only a "val rec" whose right hand side is constrained to be a function type. That's a pretty fine-grained distinction, I'll grant.
Am I right in understanding that Bob Harper's point is that in ML the bottom is an effect, rather than an implicit part of the Nat type, as it is in Haskell? Is his point that all Haskell types include bottom?
&gt; I don't think that bottom element makes any significant difference for your induction proof. [Fast and Loose Reasoning is Morally Correct](http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html)
No, you are not totally off-base. Lazy evaluation has the property that if the term you are evaluating has a value, then you will arrive at it. This is a formal way of saying what you're getting at: that you are not forced to consider the evaluation order. If you add a strictness annotation to your Haskell code, it will either run faster, or slower, or never finish at all; but it will *never* finish when it wouldn't have otherwise (assuming infinite memory). Of course, performance is a whole different matter. It's easy to give examples where the performance of a lazily evaluated term has awful performance to the point of being *practically* the same as not working at all, while adding strictness causes it to run acceptably fast or in an acceptable amount of memory. So you don't get away with not reasoning about performance, of course; but there's still some benefit in being able to know that you've got a correct implementation first, and then worry about performance later. It's that division of concerns, not the lack of need to consider execution at all, that you often find in good declarative systems.
None of this is particularly new or interesting, but this was a minor stumbling-block for me as a Haskell beginner, and I thought it might be useful to write up what i've learned for anyone else in the same situation. Comments/ criticism gratefully received, especially if I've made any glaring errors! 
It's a [fine distinction](http://www.reddit.com/r/programming/comments/gwqa2/the_real_point_of_laziness/c1qwdc5) indeed. And not one I find illuminating. If I can write the expression `gimme () : Nat` and it doesn't actually give me a natural number, then his point is lost. Whinging about how that's an "expression" not a "value" so it's fine to smuggle in side effects and non-termination is hardly an excuse. Either the expression has type `Nat`, or it doesn't.
fixedarrow’s link is good. Here is another reason why generating Java bytecode won't magically buy you all of the benefits of the highly optimized JVM: purely functional code has *extremely* different garbage characteristics than usual Java code. Haskell generates a lot of garbage, and our GC is tuned for this use-case. Java programs see different use-cases, and their GCs are tuned appropriately.
To the extent that he has a cohesive point, yeah I think you're on the right track. Though, as discussed in the sibling thread with sclv, he's arguing that he doesn't have skeletons in his closet (because he keeps them in the pantry instead).
Either way, I wouldn't mind a very cross platform GHC. E.g. I can compile to iPhone, x86/win32/linux as well as the JVM. I guess I am considering the jre as just another host platform. My main goal would be to just get Haskell code to compile and run under x86 and the jre. With or without any optimizations. 
Regarding fast and loose reasoning, which may or may not have disappeared, but I'm going to talk about it anyway.... I'm not even talking about that. I'm talking about full-on domain-induction. Essentially, when you generalize induction, there are additional ways to form elements besides the constructors. And to prove things by induction, you have to prove that your proposition respects those additional ways of forming elements. So, for domains, there are two new ways you can introduce elements: 1. There is a bottom element 2. The least upper bound of a monotone sequence of elements is an element However for the data declaration: data Nat = Zero | Suc !Nat there are (I'll say) two types of monotone sequences: e, e, e, e, ... where `e` is an arbitrary prior element (including bottom). This has lub `e`. The other has the form: _|_, _|_, ..., e, e, e ... Where we have finitely many bottoms, followed by infinitely many values of some well-defined value `e`. This sequence has lub `e`. Since lubs do not introduce any new elements, we can just forget about having to do an induction case for them; all those cases will be covered via the other induction cases. So, the difference between induction on the flat domain of 'naturals' and the set of naturals is a single proof obligation: `P(_|_)`. And if we don't wish to think about whether our property `P` holds for bottom or not (because we only care about well-defined inputs), we can instead prove the proposition: Q(_|_) = true Q(n) = P(n) Where `Q` should hold by proving only the usual natural number cases, because for `_|_` it holds trivially. Of course, it's possible I'm completely off base here. But the above seems good enough to me.
What a great google code project.
Yes, he says so in the comments. Bottom is a value in lazy languages, but an effect in strict languages. Because bottom is part of every type in the former, you theoretically can't reason by induction.
They mention lambdavm. http://wiki.brianweb.net/LambdaVM/LambdaVM You know how these things go, it may not be maintained, but I will look at it.
The problem I have is that the type of lens now carries around a superfluous parameter for the members of the state monads or a rank 2 type. As you noted below though, the result is pretty horrible when you use the actual lenses package.
Actually `3 + 5` occasionally runs afoul of the monomorphism restriction when bound locally in a where clause, if you try to use it with different types. ;) This bites you much often with lenses defined by the lens package because if you say `baz = foo . bar` in a where clause you can only use multiple times when the result type of the state monads are the same.
I just fear, that Haskell programs for the JVM would show a bad time performance (which would make Haskell look rather bad), since Haskell needs a GC tuned for persistent data structure operations, also I'd expect an even worse memory footprint, since you'd have to map Haskell types to the JVM types, which might incur some additional overhead... So I'm not sure whether it'd be a good thing to have Haskell ported to the JVM, just to underperform there
Don't forget Haskell.org isn't the only organization with a GSOC related to Haskell. http://socghop.appspot.com/gsoc/project/google/gsoc2011/lazard/8001 I love how that gets a "Portland State" label given where the mentors are sitting.
Edit: This comment originally questioned why there are 2 darcs projects. The answer is that GSOC basically comes down to what students what to work on- there is no real way for the community to decide what projects are worked on. And I can't blame darcs for making multiple entries if they have no guarantee that even one will be accepted.
&gt;I don't use something therefore it doesn't matter I know this may sound crazy, but not everyone is into every random fad that comes along. Personally, I have no interest in github. I just want a better VCS, not some asinine social networking wank. Since darcs is a better VCS than git, work on it does matter to me.
We didn't have 7 non-Darcs projects that met the bar. This is the third time I participate in GSoC (second time as an administrator) and in my experience we need to be more proactive in both attracting students and making it clear which projects we'd like them to work on. For example, before GSoC I wrote a [blog post](http://blog.johantibell.com/2011/03/summer-of-code-project-suggestions.html) listing some projects I'd like to see students work on. I also sent email to haskell-cafe and tweeted about GSoC in general (and my projects in particular). A large number of the applications (and two of the accepted projects) this year were for these three projects. Summary: I you like to see projects you care about picked up by students during GSoC, it's up to you to make that happen. 
Portland State has a tendency to sponsor projects in a wide variety of areas.
The key difference is that ⊥ is not a value in ML. If you take a look at the definition of Standard ML, you will see that the meaning of a program is given in terms of a big-step semantics of the form E ⊢ exp ⇒ v, where v is a value. If, however, exp is non-terminating, then there is no such derivation: exp doesn't have a value! Since ML is an eager language, we do not have to worry about non-value function arguments (including non-terminating arguments).
Excellent bon mot.
I wonder if the EclipseFP improvements will also benefit scion-lib
&gt; Sadly, that's a lot messier in the actual lenses package This code is using the actual lenses package. After using fetch or update all the mtl mess is gone. id also works fine: \s -&gt; update s id :: r -&gt; r -&gt; r 
I'm a bit surprised by the fourth project: "Convert the text package to use UTF-8 internally". Wasn't the text package using UTF-16 because it is deemed more efficient as an in-memory representation when it comes to text manipulation?
I presume that UTF-8 is more efficient than UTF-16 for most current use cases. Especially for web applications since ~50% of all webpage use UTF-8 \[[1](http://trends.builtwith.com/encoding/UTF-8), [2](http://googleblog.blogspot.com/2010/01/unicode-nearing-50-of-web.html)\]. 
There's some information on this in my [proposal](http://jaspervdj.be/tmp/text.html)
How do you do equational reasoning in ML if you only have big-step semantics? My understanding was to do equational reasoning you have to use denotational semantics, and that means putting errors back into the values of types, and then Nat is no longer an inductive type. In otherwords, It seems to me that you only can get inductive types into ML at the expense of losing equational reasoning.
I learned a lot from this series -- I'll never use a Boolean again, honest! Equality tests? *Nein danke!* -- but even from the beginning one felt oneself breathing an atmosphere of dishonesty. This was demoralizing especially when he was arguing for propositions that in principle I would like to affirm on a rational basis. In this post, though, we see clear signs of the sort of moral implosion that springs necessarily from systematic and perpetual lying. It is unfortunate that the excellent decision to teach ML to CMU students is being managed by someone this manipulative; it suggests that the decision was taken not on the merits. No rational person could trust anything he says to be a sincere affirmation. One can only guess that his role in the matter was an exercise of professional power, not the power of argument and rational persuasion. What a drag. One or two responses like the following would just be a sign that he is yet another professorial windbag, but the monotony of it is a clear sign of something much darker, an irreparable damage he has done to himself: Abstract Type says: &gt; Yes, but unfortunately the algebraic type system in Haskell is fundamentally broken, rendering it useless for my purposes (and creating a lot of problems for everyone). Abstract Type says: &gt; Read my posts on parallelism please. &gt; ML has statically typed effects in the sense you suggest to exactly the same extent as does Haskell. You are mistaken if you think otherwise. Abstract Type says: &gt; Read what I wrote again. I understand the semantics of the situation quite well, thank you. Abstract Type says: &gt; No, you cannot. Abstract Type says: &gt; I’m sorry, you are wrong. I know perfectly well that there are “strictness annotations” in Haskell. Abstract Type says: &gt; Nope. Abstract Type says: &gt; Exactly. I’m amazed at the misconceptions surrounding Haskell, in particular (and also those surrounding dynamic languages, but that’s another post). Abstract Type says: &gt; I’m sorry, but you are wrong. It cannot be done. Abstract Type says: &gt; I’m sorry, but this is not correct. 
Ok, will try. The reason i tried "open" from the command-line in the first place is that double-clicking the application yields to nothing.
slicing an UTF16 buffer on characters is O(1), slicing an UTF8 buffer on characters is O(n)
ok, fair enough. I did write proposals on reddit, which I realized was a waste of time after it was officially referred to as "crowd-sourced" proposals. I wrote a proposal on the trac page. There was no explanation of what else I was supposed to do to make it "meet the bar". So I was supposed to "market" things by blogging about them or mailing the cafe. I guess I just have a mis-perception that GSOC is to work on the most important thing for the community rather than the things the people are willing to market the most. I thought the projects *you* proposed are being worked on because they are invaluable to the community. Thanks for "marketing" them so they got picked up.
I have (and occasionally do) use darcs. It is a perfectly fine tool. I don't use EclipseFP, so it would be hard for me to form an opinion, but I might object if there were 2 projects for EclipseFP. have you used github? Github is not about "social networking", it is about making it easier to share code at a level that a vcs is not meant to work at. From a view point outside of the haskell community, darcs is the fad that has come and gone. At the same time github has steadily risen and serves *entire* communities of programmers and is here to stay.
This came up in discussion on a previous post [1]. There, Harper said that he thinks equational reasoning is far less important than proofs by induction. I think this is sort of the heart of the dispute. It seems to me that if your language is potentially non-terminating, and your type-system is potentially non-terminating, then at least your semantics should be total. Otherwise they're just your language again by a different notation. Harper, I guess, disagrees. [1] http://existentialtype.wordpress.com/2011/03/21/the-dog-that-didnt-bark/
Not true. UTF-16 is a variable-width encoding, just like UTF-8.
The only advantage that I could see at this point would be direct access to Java libs -- but that sort of interface would be nowhere near as clean as one can get with a hybrid object/functional language such as e.g. F# or Clojure. I've had success with, and continue to recommend, using Apache Thrift to set up a lightweight cross-language RPC system, and then expose the calls you want from a server running on the JVM (you can even use clojure for that and bypass Java entirely).
The initial benchmarks created when text was first created showed that UTF-16 was slightly faster than UTF-8, due to how the branches were laid out by GHC I believe. However, these benchmarks did not take into account that most input and output is actually UTF-8 (i.e. the benchmarked worked on already decoded data). In addition, UTF-8 is more space efficient. 
I hope so. I pointed out during the proposal process it would be nice if some focus was spent on scion so that more editors can benefit. 
What I meant is that there weren't 7 *student proposals* for non-Darcs projects that met the bar. Sorry for the confusion. As for marketing projects, given a big list of projects students tend to pick things that are more fun (e.g. games) and involve more greenfield coding. This is usually the opposite of what's useful to the community, who wants improvements to current tools. If no one tells the student what we consider important (i.e. "market"), how are they supposed to know? In addition, lots of students don't read our Trac, so if we want more applicants we need to reach out to students e.g. via reddit, blogs, etc. 
Actually, we did have 7 non-Darcs projects that met the bar, I think. It just so happens that the second darcs project was also a very good project that on its own merits (weighing the utility, as well as the chance of success, quality of the proposal, etc.) beat out the remaining proposals. I always want more and better proposals, but I think we had a wealth of good choices this year. They didn't really address my personal hobby horses, which I've only intermittantly lobbied for (HDBC, c2hs in particular) but, as you said, that really just means I need to lobby harder (and resign myself a bit to the fact that certain projects just don't seem that exciting, as important as I feel they are).
Okay, I made the mistake of asking for an actual type for id. So it's rather fragile, but it seems like so long as you never want to actually get your hands on a lens and pass it around, store it in a data structure, and so on... then it seems to work.
Interesting. It does seem to come down to equational reasoning vs inductive reasoning. I fall in line with [Jermey Gibbons](http://patternsinfp.wordpress.com/) in this debate I think.
I don't think proposals on reddit are a waste of time, nor do I think that trac proposals are a waste of time. The problem isn't whether the idea meets the bar or whatever. The problem is that these ideas are then turned into written proposals by individual students who volunteer to work on specific projects, describe how they intend to do so, each have their own talents and qualifications, etc. The reason that proposals need marketing is to convince the most talented and best students to propose doing them. If those proposals don't get submitted by those students, then they can't magically get accepted anyway.
Fair enough, on both counts. Thanks for the clarification!
&gt; Because bottom is part of every type in the former, you theoretically can't reason by induction. This isn't really true. Haskell types are still initial algebras, and so come with an induction principle. But, they're initial algebras in a category of domains, so the cases you have to cover in your proof by induction are different than the cases you'd have to cover for a set defined inductively by 'the same constructors.' See, for instance [Fibrational Induction Rules for Initial Algebras](http://personal.cis.strath.ac.uk/~patricia/csl2010.pdf) (pdf). So, that's the problem. In ML, you can prove things by induction that looks the same as the induction you learned in high school (and further, it matches up with function definition by recursion in the language). And in Haskell, you have extra cases to cover for your proofs (but not your function definitions).
Well put.
Doesn't this also mean that ML's functions on algebraic types using pattern matching are actually the proofs by induction you speak of, where Haskell's functions can never be exhaustive (can't match on bottom), and thus are not proofs.
One of the slots was earmarked for darcs from the get-go as part of the application process. We explicitly requested one slot be set aside for darcs by google, so that darcs would not have to apply separately. There was only one EclipseFP proposal, therefore we could only accept one. The other slot was slated to go with another cabal oriented proposal until that proposal was withdrawn, but the second darcs proposal beat the runner up quite handily in polling among potential mentors. Slot allocation is tricky business, because you can only allocate the slots you get to the proposals that you get. We can try to steer students to apply where they are needed, but they apply blindly not knowing what other proposals are out there so the distribution is necessarily quite lumpy.
It isn't that the proposal didn't meet the bar. Ultimately, a student has to choose to submit a fleshed out proposal along the lines of the one you suggest, and then _that_ has to be approved. Getting a student to notice your idea and care about it enough to propose it is an artifact of the process, and so, sadly, marketing does enter into the equation if only indirectly.
Ultimately his argument hinges on the fact that Nat with strictness annotations is more closely analogous to `() -&gt; Nat` than to `Nat` in ML. There is some truth to the fact that we don't have a means to talk about the latter for ADTs, but I don't think the end is nigh or that it renders them fundamentally broken.
The distribution is quite lumpy indeed. I emailed 4 or so students to encourage them to put in a few more applications as they all had applied for the same one.
We don't have the means to talk about the latter for anything really, except magic primitives of kind `#`.
ML doesn't have the right types to represent proofs about its own programs internally. So not really. But the cases you need to cover in a proof by induction _about_ a function in ML are the same cases you need to cover to define functions _in_ ML by recursion/induction. So it's a nice symmetry.
Not only that, but it is "the real point of laziness". I wrote [an article in TMR](http://www.haskell.org/wikiupload/1/14/TMR-Issue6.pdf) that gives a moderately complicated example of Tying the Knot that is weaved through a monad.
What do you think of [Charity](http://pll.cpsc.ucalgary.ca/charity1/www/home.html)?
Apparently causal relationships between difficulty and unimportance are always fascinating.
That sentence really got away from you, didn't it?
My favorite way of understanding induction and coinduction is this: for induction, there's a finite reason a thing must be so; for coinduction, there's no finite reason a thing must not be so. Taking this intuition into the world of types makes the statement a bit trickier, but the some may find it enlightening, so let me try. A type is inductive if the values each have finite descriptions of why they must have that type; a type is coinductive if the values each cannot be shown not to have that type with a finite examination. For example, compare: data List a = Nil | Cons a (List a) Nil -- has a finite description within the type, so has type List a Cons 3 (Cons 4 Nil) -- is of type List a Cons 3 (Cons 4 5) -- not of type List a; despite having a finite description, that description doesn't fit in the type fix (Cons 3) -- not of type List a, because its description isn't finite codata List a = Nil | Cons a (List a) Nil -- no examination of this value can disprove that it's a List a, so certainly no finite examination can do so, so it has type List a Cons 3 (Cons 4 Nil) -- as before Cons 3 (Cons 4 5) -- the finite examination which takes the tail twice exposes the fact that this is not a List a, and so this does not have type List a fix (Cons 3) -- no finite examination ever finds anything wrong with this value, so this has type List a
I think Charity (total programming with folds for data and unfolds for codata) is an important contribution to our collective understanding of these issues. Of course I'd say "now add dependent types", because it's easier to write total programs if you can (a) chuck the junk out of the domains of functions (codomains, too, if they're recursive) and (b) index types by the structure you're using to justify the totality. But I approve of its upfront foundationalism. It's good to have clear foundations, then provide modern conveniences by translation. Meanwhile, we won't acquire a compositional language of (co)recursion until we can express more precise producer-consumer relationships. Consider, for starters, the notorious nats :: [Integer] nats = 0 : map (1+) nats What do you need to know about map to know that nats is productive? And for main course, consider the legendary Oxford incantation spiral :: ([a] -&gt; (b, [a])) -&gt; a -&gt; b spiral f a = b where (b, as) = f (a : as) What conditions on f (and/or b) make this even remotely sensible? It's not at all obvious to me how to play those games in Charity whilst retaining the generic character of map and spiral. Nor is it at all obvious to me what's for dessert. Interesting times; so much we don't have a good handle on.
&gt;I have (and occasionally do) use darcs That doesn't matter though, that was the point (which admittedly I didn't make as clear as I could have). It doesn't matter what you use personally, things don't become pointless or a waste of time just because you personally don't benefit from them. &gt;Github is not about "social networking" Really? Maybe you should mention that to them, because they seem to think otherwise. Their slogan is "social coding". Their service is to provide social networking nonsense on top of hosting a repository for you. Followers and social network graphs are most certainly social networking nonsense, and have absolutely nothing to do with programming. &gt;From a view point outside of the haskell community, darcs is the fad that has come and gone. The point of view from outside the haskell community is more like "what is darcs? I've never heard of it". In order for something to be a fad, it has to be popular at some point. &gt;At the same time github has steadily risen and serves entire communities of programmers and is here to stay. What a bizarre notion. You are implying that github serves "entire communities", and that darcs serves what? Partial communities? Github serves a smaller community than CVS does, I am not inclined to consider the size of a userbase to be an indicator of quality.
+1 I would argue though that even thrift is too heavyweight. In most cases a simple REST http interface with jetty and clojure ring framework is more than enough. 
Heh. That's only moderately mind-bending. The Haskell rabbit hole goes a lot deeper.
I like thrift because at the cost of an extra tool it enforces that the data types match between your two languages. Also, the serialization is supposed to be somewhat efficient, which can matter if you're dealing with relatively large amounts of data. And finally, it provides both the client and server stubs, which are decent if not perfect. So once you get your head around it, it actually saves quite a bit of work as compared to rolling your own protocol (even a REST one).
The problem is see with it is that because you are communicating with out of process instances, you have to organize work in a coarse grained chunks. You can't for example micromanage work from a haskell side with thrift issuing hundreds low level commands to drive building a complex pdf on java side. It is better to to ask a java side to build that pdf for you in one high level request. In which case there are no hundreds procedures to code and exchange, no large swats of data to exchange (it's all going to be in a shared data storage like database or file system anyway). So i do not see a point in implementing all that red tape just to have 5 - 10 high level service end points. In the end REST worked out really well for such large companies like Google, Amazon, Netflix. Whereas RPC wannabes like SOAP failed miserably. 
Reverse state monad still doesn't make sense.
Er, how so? I don't see why...
Bottom is not a value in Haskell either. There are *expressions* whose denotation is bottom (or includes it). But there are no *values* which are bottom (or include it). A computer spinning forever is not a mathematical value of any sort, regardless of the language. See the sibling thread with [sclv](http://www.reddit.com/r/haskell/comments/gwrsj/harper_again_the_real_point_of_laziness/c1qz324), and especially my reply there (though roconnor's below is just the same).
&gt; then at least your semantics should be total. Otherwise they're just your language again by a different notation. Y'know, I run into that problem with Tarskian semantics too. Philosophers seem to think the shift of notation is a godsend, but from the PL perspective I can't see how it really says much at all to illuminate the semantics of the language (except that it says, indirectly, that you should just throw up your hands at the problem).
&gt; The Haskell rabbit hole goes a lot deeper. Like searching through an uncountable (compact) space for a point satisfying a given predicate?
I don't think laziness annotations will let you tie-the-knot in ML no matter how hard you try.
Is the `codata` example missing the recursive reference intentionally?
&gt; I think there are a lot of benefits to adopting unicode, though. Informative types and small scopes make single-letter variables very handy, and having syntactic classes of variable names improves this (e.g. x and y vs. a and b vs. S and T). I think the benefits of single-letter variable names are over-rated. If you're really out of the entire English alphabet, you really should expand your space exponentially by using multiple-letter names than by a small factor by adding alphabets. Besides making it non-obvious what to type to get the symbol, Unicode also makes it more difficult to grep/search for the symbol or its definition, etc.
So true re-use is reimplementing the same abstractions over and over?
I agree on the micromanagement front. But suppose you have a simple API that happens not to produce a pdf file or operate a robot arm or whatever, but instead to yield a whole ream of structured data. Ideally you'd prefer to process this data in Haskell with a few zips and folds, maybe some parallel matrix calculations, etc. But if you're concerned about the cost of serialization/deserialization, you might end up trying to extract the relevant bits of data while still on the JVM for the sake of efficiency. That's more the situation I have in mind. Ideally a few sockets talking to one another should still be an order of magnitude (at the least) better than going via the filesystem or, god forbid, a database.
That depends on the definition of a syntactic value.
I have two contributions worth mentioning, [control-monad-queue](http://hackage.haskell.org/package/control-monad-queue) and [Fun With the Lazy State Monad](http://blog.melding-monads.com/2009/12/30/fun-with-the-lazy-state-monad/). I re-discovered the first technique but not the second; however in both cases I believe I am the first to create a reusable implementation of the fundamental concept. 
I guess I am still confused about the process- are you are deciding which student applications go to google, or are you deciding what slots are there for google?
:) Obviously the task at hand influences our decisions (as it should be). So I agree, the haskell programmer should be able to chose from any of these viable options.
Well, the T sentences are just a test that any decent theory of truth must satisfy, and as far as that goes it's fine. But my problem is with his actual theory of truth. Namely, I don't buy that his compositional scheme for translating the object language into the meta language actually explains anything, nor that it is possible to do in general (as he describes it). We run into this problem of translation all the time when trying to give semantics to programming languages. Other than some of the common boilerplate, it's no easier now that it was decades ago. And not all languages are compositional in the way Tarski assumes. Natural languages have non-context free structures in them, and many sophisticated PLs are headed that direction too.
Reverse state isn't so bad in some cases. For instance, it's pretty clear what this does from the name and signature: mapAccumR :: (a -&gt; s -&gt; (b, s)) -&gt; [a] -&gt; s -&gt; ([b], s) But this is none other than: mapM :: (a -&gt; RState s b) -&gt; [a] -&gt; RState s [b] (Whereas `mapAccumL` is `mapM` for `State`.) Trying to look at it as an imperative program is confusing, but there are other functions (`mapM` for other `Traversibles`) where `State` vs. `RState` is just a choice of direction for information to flow across a data structure.
No, fixed, thanks.
ML is impure, every value comes with potential side effects attached. For example (Haskell syntax) example = let x = putStrLn "Effect" in 42 The example is an expression that evaluates to 42 while printing "Effect" on the screen. Harper's response would probably be to split hairs and to beat on the fact that `example` is an *expression*, not a *value*, but that's hardly comforting. 
The difference is that one is a monad and the other one isn't. 
Computations have effects, not values. The distinction is important. 
As in whether or not you can define [recursive literals](http://www.reddit.com/r/programming/comments/gwqa2/the_real_point_of_laziness/c1qz6lh)? Interesting. I though tying-the-knot required laziness. It looks like I was totally wrong.
&gt; Not only that, but it is "the real point of laziness". It looks like [tying-the-knot doesn't require laziness](http://www.reddit.com/r/programming/comments/gwqa2/the_real_point_of_laziness/c1qz6lh). I had no idea. Edit: Then again, [maybe it does](http://stackoverflow.com/questions/5810163/creating-a-doubly-linked-list-from-a-list-in-ocaml)?
Yep. What I'm alluding to is that any expression (expr x ...) : int has to be modeled as a computation, not as a value. Compare to Haskell, where the denotational semantics simply assigns a value to this thing.
mapM is Applicative, though, so sure it isn't that weird to mapM in the reverse order. It is more awkward to explain `join` in RState.
A much abbreviated version of the process: 1.) We start beating the drum to get folks to think about proposal ideas. This is where the mindshare component comes in. 2.) We apply to google, telling them how wonderful the summer of code is (because it is) and how much we have enjoyed participating in it (because we have) and reviewing our successes and failures in the past. 3.) We get accepted by google into the summer of code (with much gnashing of teeth). 4.) Students start submitting project proposals for themselves with fleshed out timelines and why they are the perfect applicant. We get to send back some comments while this is going on, but the main thing we can do is beat the bushes to bring in more applications. 5.) Applications end and by now we've estimated to google the number of slots we can handle. 6.) Google counters with how many they'll actually give us. 6.) We get to debate amongst ourselves which of those proposals that we have received we can approve, resolving duplicates, students that have applied to multiple organizations, choosing between projects for students that have submitted multiple excellent proposals, etc. 7.) Then we knuckle down for a summer actually doing stuff. So to answer your question, all of the student applications 'go to google' (via the summer of code website', then we sift through the ones we receive. Ultimately, we approve students for particular work based on their own proposals. They propose the topic and scope of work. We may of course come back to them if we think it is too little, or too much. But the key consideration is that students do not just sign up blind; we don't make a definitive list of everything we want to see and then allocate students to tasks.
why limit things to just annotations?
The current paralysis comes from the fact that all of the proposals made to date have at least one situation in which they are worse than the current hack.
Moreover, records for such libraries are effectively made up of a linked list of fields. Not the most efficient storage back end.
Well... they don't have to be. Somewhere between statically knowing the index and a sufficiently smart compiler there should be O(1) lookup, or even constant propagation.
Why replace UTF-16 with UTF-8? Couldn't a UTF-8 implementation live alongside the current one? As they'd implement the same interface, users could easily try both. If UTF-8 is a universal benefit it would become obvious over time by which modules were in active use. It may be that one choice is preferable for webapps doing mostly simple concatenation with a lot of UTF-8 I/O, and another one for an application doing a lot of internal manipulation of text values.
A null pointer exception by any other name would crash as sweetly.
/me wonders why a static page has to eat 100% CPU in my uzbl.
 5.) Applications end and by now we've estimated to google the number of slots we can handle. What is the limit on this number? the number of good student applicants? Because I am sure there are many more willing mentors out there.
Ultimately you need to choose between O(1) field consing and O(1) indexing. Ideally one wants to support arbitrary row types and a flat representation to get O(1) indexing. Hacking something like this into ghc means dynamically generating infotables (which we don't do now), dealing with Ohori-style field offsets (likely extended in the fashion of Gaster and Jones), which then complicates alias analysis (c-- doesn't have to deal with this now), etc. It isn't impossible to get right, but it does require a lot of low level changes.
Why have both UTF-16 and UTF-8? Couldn't the better one replace both? As their interface is the same, users could easily migrate if they happened to be using the inferior one. This also prevents module-creep, and everybody who wants to do text-y things gets to take advantage of the wisdom and domain knowledge of the people writing the text library, rather than trying to benchmark both things themselves -- or worse, just guessing. It may be that one choice is sometimes inferior, but in the long run maybe catering to the common case wins out.
I suppose one could start with a language-side construct that gets the semantics right and care about big O once the extension has proven itself to be popular? Rome wasn't build in a day.
One part of tying the knot is having laziness so a cyclic definition produces a useful value. The more primitive part is just being able to write down the recursive binding so the name of the result is even in scope in the body of the definitions. It's easy enough to write a module providing thunks to get lazy semantics, but ML's recursive let has a syntactic restriction on what you can actually define recursively, which might interfere with being able to do the more basic part of getting the scopes right.
&gt; there's a bit more syntax than LISP You know what? One of Lisp's major claims to fame is that it has facilities for defining your own custom syntax for everything. It's just that all your custom syntax (and its built-in syntax) is rose trees with different shapes and tags. I wouldn't be surprised if Haskell has less syntax than Lisp if you're not married to the idea that syntax is entirely about going from a raw character stream to a concrete parse tree.
For my purposes, I would *love* to get Haskell running on the JVM, and it has nothing to do with libraries. A number of our clients are married to the JVM, and this would allow me to use Haskell for their projects.
There actually is one stupid default in Haskell that behaves like the NPE - partial match failure. Fortunately you can make the compiler warn you about partial matches using -W flag. I don't know any way to make javac warn about all possible places where NPE could happen. In fact, I think it isn't really possible, as the Java language (in its current form) is fundamentally broken by allowing null in all references.
Interesting: Lisp has less concrete syntax and more abstract syntax. 
Yes, yes, and yes. I love Real World Haskell and Learn You a Haskell. The former makes perfect references to other languages like C. The latter is designed with ultranewbies in mind.
The Text type models a sequence of Unicode code points. To have something like TextUtf8 and TextUtf16 would be to confuse the abstract concept with its encoding. Text should be oblivious to its internal encoding.
Given that String also models a sequence of Unicode code points, why have Text at all if it's not for the efficiency of a particular internal representation? The point would be to allow choice between two different efficiency profiles not to expose internal representation. We do the same thing with more general containers, choosing which version of a key value map to use based on efficiency considerations. I'm not saying UTF-16 should be kept alongside UTF-8; I was just curious about the reasoning. It seems that Jasper's plan to get a genuinely representative benchmarking sample is pretty tricky.
Not that this affects your point in any way, but Lisp code is actually a DAG and not a tree, although I have yet to see any DSL's that really exploit this particular ability of the reader.
Funny pictures! I hope, however, that this series is not intended to explain lazy evaluation itself, only how lazy evaluation can be implemented. It is possible and preferable to understand lazy evaluation purely on the level of expressions and weak head normal forms, i.e. on the source code level. 
Out of curiosity, what would you have the language do instead?
My uzbl uses the latest webkit.
Because Blogger.
That's kind of unusual. How do you get two nodes pointing at the same 'child'?
I will be there, tickets are booked.
String is not an opaque type, so of course the reason Text is separate from String is that it would have been impossible to modify the representation of String, when the latter is a type synonym for a type with exported constructors. Of course, sometimes it is reasonable to maintain tow implementations because of differing costs of operations; but the differences in performance between UTF-8 and UTF-16 will be small enough that it's certainly not worth the complexity. In particular, there will be no asymptotic differences, as the algorithms will all be the same.
 (root (node #1=leaf) (other-node #1#)) `leaf` is a child of both `node` and `other-node`.
It's not necessarily acyclic either: '#1=(long #1#) I haven't found the original usenet post, but I seem to recall a classic Fibonacci function: (defun fib (n) #1=(if (&lt;= n 1) n (+ (let ((n (1- n))) #1#) (let ((n (1- (1- n)))) #1#))))
Something like C++ references where references have to be referencing something. I'm commenting about java's NPE. Not Haskell's partial match, which I still don't understand why allowed it in a statically-typed language.
I thought that one of the motivations was the promise of UTF-8 encoding/decoding in O(1) rather than O(n). Though I do believe the general thrust of your argument.
Why NOT Haskell? * Lazy evaluation makes reasoning runtime performance nearly impossible except in trivial cases. * Lazy evaluation makes debugging nearly impossible except in trivial cases. * Monads are a bitch to master even for experts, much less to break into for novices. * Purity for good program structure is as true as promises from a presidential candiate. 
[Why they use Haskell](http://blog.typlab.com/2009/09/why-we-use-haskell/). Good work guys. This looks amazing.
Honestly, without the asymptotics, users would have no incentive to use it. The slightly nicer semantics aren't really worth the asymptotic hit.
Honestly, Google only has so many slots. It is more of a give and take based on how many successful projects we've run in the past, and how much of the summer of code they want to devote to Haskell vs. other organizations. As the number of organizations has grown the number of slots allocated to each organization has, on average, shrunk considerably. We've managed to hover around 7 slots the whole time. Over the years: * 2006: 9 slots * 2007: 9 slots * 2008: 7 slots * 2009: 5 slots * 2010: 7 slots * 2011: 7 slots
working link: http://apfelmus.nfshost.com/blog/2011/04/28-frp-banana-0-2.html
Oops. Thanks! I'm going to resubmit this.
With every announcement like this the perceived business risk of using Haskell drops. I'll know Haskell has arrived when I finally meet a Haskell programmer who isn't deeply interested in computer science, math or programming. Hasn't happened yet anyway. Perhaps the motto is working. 
It looks like dons is on it: http://www.reddit.com/r/haskell/comments/gwgfs/frp_pushdriven_implementations_and_sharing
The `O(1)` decoding is not possible since you still have to check the validity of the input. Encoding will also stay `O(n)` because there's typically a copy involved. (This was a mistake in my proposal.)
That's the previous installment. :-)
imho, programmers without interest in compsci, math, or programming cannot be good programmers in the first place... that's my personal experience so far at least
That's a really slick presentation. Their [blog](http://blog.typlab.com/) is also beautiful. If their app is even nearly as well polished they have a bright future ahead of them.
Direct link to the [package on Hackage](http://hackage.haskell.org/package/hs-vcard) if you don't want to read the blog post (though it'd be nice if you did :)
Call it a project, not a startup; that's what the product is.
Yes, of course. typLab is the company; Silk is the product.
This is a quite fascinating post! I had never seen the initial algebra presentation for the dependent recursion schemes before. For now I am still a bit skeptical about a presentation of schemes which would allow a more intuitive programming style than fix+match, but i'll have to read your next posts to see if i can be convinced!
All false. 
Any idea if they used an established framework (snap, yesod, etc.)?
&gt; I also chose to not implement reading them in, primarily because I had absolutely no use for that. Here's a scenario where reading might be useful for you. The Mac OS X Address Book program _exports_ contact details as vCards. I don't know how many other programs which people use to store their contacts do this, but I would bet it's a reasonable number. If instead of having to type in all their details, customers could simply upload a file, that would be pretty valuable in some scenarios. For the sole purchaser it's not compelling—most people have no idea that the 'Export' option even exists. However, if someone were to want to order this for everyone in their company, it's suddenly a much more attractive option, even if the company is pretty small. And those bulk orders will make you a lot more money than a few individuals here and there using the service.
That's a great idea, and very good point, thanks! Definitely adding that to my list of stuff to play around with to see about adding that.
Sebastiaan Visser had one he was working on himself, I'm guessing they're using that.
Some thoughts about other things you could add; I imagine these have probably already crossed your mind. * A selection of different templates; * Simple "Upload a background image" tool; * Make a video showing it in action. I strongly suspect that the money here is in selling to small, relatively technically-savvy businesses who want business cards but would like to avoid the hassle of dealing with a print shop themselves. To that end, the more you can get them to go through an automated online purchasing process (rather than emailing you to discuss custom orders) the better.
Yes, but I wouldn't have guessed from their definitions! Individually, they both look very sensible to me.
Awesome. :-) I wish I lived a little bit closer, I'd love to attend then.
I expect UTF-8 to always beat UTF-16. If it doesn't it's most likely due to problems in the code generator than anything else. That's at least my experience from all the data processing we do at $WORK. In the end of the day fitting the maximum amount of data in the cache is what matters.
These are great ideas. I definitely had a video in mind, and had dabbled with thought of various templates, but hadn't played with the idea of going into even more customization. You raise a good point that most businesses won't want to email in, and they certainly are a target of ours, so I'll be seeing if I can make the form a bit friendlier to individuals and businesses alike as well as introducing some fine-tuning for the power users. Thanks a ton for the feedback, much appreciated!
http://www.haskell.org/haskellwiki/Num_instance_for_functions has a nice small caveat about that instance. :-)
Link to his framework?
This is a really cool idea! I don't know if you can call it a zipper, tho... IMHO a crucial feature of zippers, at least the ones described by Huet, is O(1) updates of the current focus, and the label approach doesn't support that, because an update causes the spine all the way to the modified value to be rebuilt. If you want generic zippers with O(1) update, you need to to generically derive zipper data types from the original data types. The only way I know of to do that is to use the pattern functor approach, like the [zipper library](http://hackage.haskell.org/package/zipper) does. 
it could be http://hackage.haskell.org/package/salvia
Graph of the cumulative growth: http://i.imgur.com/w5YEQ.png
Woo-hoo! The [reddit post announcing that package](http://www.reddit.com/r/haskell/comments/gzj74/announcing_hsvcard_my_rfc_2426_vcard_30_haskell/) (shameless plug?) Congrats Haskell community on all the hard work! Now to another milestone!
Does it prove that my program halts?
It might. It is a semidecision procedure; i.e. it will prove your program halts (if you ask it to) or work forever trying. No claims of solving the halting problem here.
There is no such thing as a good programmer anyway.
That test is easy to implement: run the program xD Either it halts, or it might halt sometime soon. Or not.
I really, really like the "try it online" fad. I don't care if it's a fad; it's an excellent way to pique interest, and more people should be doing it.
Has the general Haskell community set on any official milestones? Maybe we should use the Haskell wiki and coordinate some that we all want to accomplish. (4k hackage packages, # installs of the Haskell Platform, w/e)
I don't know. I keep learning Haskell and feel like I'm getting closer and closer to really understanding it, but it's always just out of reach. Quite a paradox.
This is nothing short of awesome.
Thanks for the thorough explanation. I appreciate everything you and others involved in GSOC are doing. But at this point I still have to criticise the process for picking 2 darcs proprosals. I understand that those were judged the best quality proposals, and maybe this was the best decision under the circumstances. But in that case we should figure out how to improve the circumstances. Does anyone think that this is the best possible use of the GSOC resources for the haskell community? It just seems like the community could benefit much more from other projects. It also seems like any darcs proposal carries additional risk of even if they are successful in writing code they could be a failure from a usage perspective. GSOC may realize that as darcs gets more contributions from them that the use of darcs keep declining (compared with other dvcs or as a % of the haskell community, it *might* be increasing slightly in raw numbers as the haskell community grows).
There was a long debate among the reviewers as to whether or not to give the one discretionary slot to darcs that we did. In fact, originally that slot was going to go to another cabal project, whereupon we would not even be having this conversation. However, when that other project was withdrawn, Petr's proposal beat the nearest contender quite handily. His was a solid proposal from a student who had served in past years as both a successful student participant and as a *mentor*. His past projects included the hashed storage framework that underpins modern darcs and his work with Levans which greatly accelerated the speed of darcs over the network. You may not derive utility from darcs. I don't derive utility from EclipseFP. Someone else may not derive utility from better OpenGL bindings. Not every project benefits everyone in the community, but I must say that Eric Kow was a bit of a rockstar at least year's summer of code mentor's summit, and probably did more there to promote interest in Haskell than any of the sessions I taught there, so I'm very happy to see him and the darcs team participating again. And I realize this is throwing gas on the fire, but to be frank, had we been granted another pair of slots, one of those would have most likely gone to darcs as well. Their proposals this year were really quite good. A large part of the mission of the Summer of Code is not only the short term benefit of getting a student to hack on a project for a summer, but also the long term benefit of drawing students into the community. The darcs students have all, to a man AFAIK, remained active in the community. Jason Dagit (lispy) was their first project, and is now employed at Galois. Eric, who now leads the darcs project, following David's departure, while never a GSoC student, is working on Parallel GHC through Well-Typed. Petr has remained very active as indicated by the fact that this is his third tour of duty with the summer of code. On this front, they have done extraordinarily well. Ultimately we can only dog pile so many bodies on ghc, cabal, and hackage, and then only if we get applications to work on them. While it is clear you don't agree with this choice, I assure you it was made neither blindly nor autocratically and I hope you can understand that while I disagree with you, that I do respect your opinion. You are, of course, welcome to apply as a mentor for next year and to participate in the debate on the merits of each proposal in turn, or even to try to get me kicked out and to petition to replace me as an org admin, but regardless I greatly welcome any help in getting students to turn out and in popularizing good project ideas.
Yeah. It's a mere academic research language.
I clicked and it took a while. I hope they don't get too many users or the systme may hang.
On proof software. Is the goal to prove the correctness of a particular program for all input states? Or is the goal to make a claim and then to have the software provide a program that proves that claim. E.g. 'count n (reverse xs) = count n xs" and then prove that claim. Does Coq work in a similar manner as the TryZeno software?
https://github.com/sebastiaanvisser/salvia Looks like he hasn't updated it in awhile.
FWIW, it took me about 2 days before I dropped binary for cereal.
I was actually alluding to zeno's paradox... ;)
I haven't tried Zeno, but as far as I can tell it works without user interaction: you give it a goal, e.g. reverse (reverse xs) == xs and it attempts to prove the goal automatically, that is that reverse (reverse xs) is equal to xs for *every* finite list xs. Coq and Isabelle are *interactive* theorem provers, which require the user to produce proofs, with the help of *tactics* supplied by the interface (which can in some cases be extremely powerful). Zeno outputs proof to the Isabelle theorem prover, which just (automatically) checks that Zeno did not produce an incorrect proof.
Haha, "the hac team". My stupid email software turned "Hac φ team" into "Hac ? team".
Great idea, thanks. On my wishlist are recommended XML, database and web-programming packages.
It doesn't, but it will output to Isabelle which does.
(If I recall correctly,) I've heard Sebastiaan say at a DutchHUG meeting they were using Happstack, but they only used a small subset of it's features.
No we're currently using happstack, because it was a bit more established than my own toy project. Not every experience with it has been a pleasant one though. We might someday switch to something else like snap, but there is no real need for that now. Luckily our business logic is the hard part. The web part is just a tiny REST-like layer that can be switched out for something else quite easily. I plan to start playing with Snap in the near future, because I trusts Greg's design choices when it comes to library design. Yesod solves an interesting problem as well, but will probably be a bad fit for our architecture.
I think, with pain in my hart, that I'll be deprecating the project soon. It was great fun and awesome learning experience writing it, but others are just doing a better job now. 
This series is awesome.
Nice.
In the chicken scheme community, at such milestones, some t-shirts are (randomly) awarded to some contributors; say 15 or 30 t-shirts. I would be happy to give some money to make such a distribution.
It is interesting to observe that while I "know" how to design OO programs, I can't meaningfully _describe_ it. Oh, I can babble about some of my internal mental processes and chant DRY as well as the next guy, but the reality is there aren't any words that I can utter that will generate the correct mental structures in the target mind unless they already mostly know what I'm talking about. Even if you point out certain concrete recurring "patterns", they aren't worth much without the experience behind them, as anyone with experience in watching a pattern nut go to town can attest. ("What is _that_?" "It's an DecoratorVisitorSingletonAbstractFactory." "What does it _do_?" "Well, nothing yet..." "More like nothing _ever_.") If people tend not to be able to answer this question satisfactorily for the functional programming case either... well... yeah. It's really a repharising of something like "I have no experience in a field; can you please directly serialize your ten years of experience into English and upload them into my brain in a way that is easy for me to understand?" Well, as nice as that would be, no, not really. Sorry. All I can point to are existence proofs of functional programs.
That might be a good idea to recognize and encourage some of those events. We'd have to be careful to not have too many 'milestones,' then it'd just get annoying.
Sounds like a nice idea. I'd be happy to help design and have them printed (but can't contribute monetarily). Would it be better to give them to random people, top contributors, or a mix of?
Why would mutation cause problems in reasoning about runtime performance? The runtime performance issue was due to the difference between strict and lazy evaluations. In strict case, where your expressions get executed is precisely where you wrote the code; but in the lazy case, where your expressions get executed can be way far down the future (pun intended) so that when f(x) crashes you can't just look inside f for the bug but would have to think about all the thunks that could have been built up deep within x to look for the bug. But back to runtime performance, sometimes your code is rips through quickly because it wasn't evaluating anything but building thunks, and somewhere else your code slows down at what is a simple function because it decides to finally evaluate that long chain of thunks. Maybe I'm using it wrong, but my experience with QuickCheck (or any testing tool) is that it tells you something is wrong but it can't tell you why it is wrong. The best tool of debugging for me so far has been the appropriately placed print statement (trace); but sometimes, it just tiresome you have to litter trace all over the place trying to figure out what is happening. Monads are like one of those Zen thing (another idiotic symbolism!). When you get it, it rings so true and obvious; but getting to get it can be quite an arduous road. But I argue that even when it becomes obvious, it is awkward to work with. One of the beauty of a functional language is that evaluation order does not matter, but monads impose evaluation order. Suddenly, your beautiful functional code becomes a Frankenstein (yet more idiotic symbolism!) between the parts that look functional without evaluation ordering and the monadic parts that look procedural. Purity. Ah... Indeed, sublime and wonderful. But wait, didn't the community touted similar enthusiasm toward OOP too? But has OOP taken us to utopia yet?
Now open source!
Random people. We talk about t-shirts, for the fun. It is about being a happy community and saying to people, globally, come on in, participate!
thanks for the answer Sebastiaan!
Not to be overly-critical, but the writer had a bit of trouble keeping the concepts and terminology straight. First of all, it was humorous how he made the distinction between "academic" (programmers) and "real programmers". He also indicated that non-functional programmers do not have the time to "think logically". However, the contrast between functional and imperative programming methods is not logic vs. non-logic, but rather equation vs. sequence. Also, he did not provide any support to his assertion that the gap is being bridged between "recipe" programmers and functional programmers. Maybe some non-functional programmers are becoming functional programmers, but I don't see how the two are connecting or compromising with each other.
http://www.spreadshirt.com/there-is-no-cabal-C3376A3612542 this one ... perhaps?
Great to see that Omega is still under development. Congrats on the release!
I should have known better than to rely on reddit's capability to spot zynism...
If this was written by a journalist I'd say he did amazingly well. I've seen much worse. 
That will do it for a single input. Zeno tries to do it for every input.
Are these tools used a lot in computer science? Is it easy to take some mathematical proof or maybe some algorithm and translate that into code...which then can be proven. I see the potential but it is a bit over my head. Plus, it seems like it would be difficult to come up with code oriented proof.
Do they ship everywhere? I'm willing to buy one of these and get it to someone if we can arrange to buy 15 or 30 of them and select some winners.
It is amazing that this found its way into a general-interest newspaper.
I think you are doing a great job. And you have convinced me that darcs deserves *1* slot. I would just like to improve the situation. If darcs has a secret sauce here, we need to figure out how it can be spread around. It seems there is a big gap between projects and students- perhaps there needs to be a focus on reaching out to universities that have a lot of fp. Perhaps haskell GSOC needs people with contacts at universities. Unfortunately I am not such a person. By the way, I don't think we should keep piling bodies on ghc and cabal forever, I just think cabal should be top priority *until* we can develop in haskell without being afraid that one project is going to break the build on the next.
Exact same feeling here!
It shouldn’t be impossible to get Hugs running… there’s a version for iOS already, and that’s surely a more difficult feat.
[CAL](http://openquark.org/Welcome.html) is similar to Haskell but lives on Java. Might be possible to get it going on Dalvik. 
Yep, this is a big step in the right direction. The previous license was a little weird; 3 clause BSD + non-commercial only. 
Nearly, International Conference on Functional Programming. The contest winners are usually presented with an award at the actual conference.
None of the specifics of the contest are given away beforehand. 
I believe in the past both the contest and conference were called ICFP, standing for International Contest for Functional Programming in the former case. ICFP-PC seems clearer though.
I'm not sure I understand your question. In general, using software to give correctness guarantees is called [formal methods](http://en.wikipedia.org/wiki/Formal_methods) and it does have a history of high-profile applications, including embedded code for critical applications, and a formally verified [C compiler](http://compcert.inria.fr/). To be fair though, it is quite rare to give complete formal specifications and proofs in industrial-level projects. Hopefully things will change in the next decade, as cost of testing and need for secure systems rise. Formal methods usually involve 3 steps: * Giving a formal specification of the behavior of the system. This should be done before the first line of code is written, in order to not be influenced by the existing design. * Writing code which hopefully fulfills the specification. The language of the specification is almost always tailored to the implementation language (see e.g. the [ACSL](http://en.wikipedia.org/wiki/ANSI/ISO_C_Specification_Language) specification language, which allows specification for C programs, and JML for Java). * Proving that the software follows the specification. This can be done semi-automatically, but it almost always involves user interaction of some sort, and a dose of expertise. This is the hard part. Coq happens to be a specification language, a programming language and a proof language all in one, but in most cases you have three distinct steps.
You are being overly-critical. This is a general interest news article and the author did amazingly well.
I find that the amount I have left to learn about Haskell just keeps halving and halving :)
Actually Zeno doesn't prove termination (on the to-do list), rather it relies on Isabelle to do post-hoc termination checking. One major problem with this approach is that you could throw Zeno into an infinite loop with a non-terminating function, since it just assumes that functions terminate.
Excellent description, I particularly like the "trinity" of software development you give. I hope in the future the first step becomes more formalised, which in turn should make the third step easier. I think we've been spending far too much time focusing on the second step.
Thanks :)
Yeah, some lobbying was involved to get here :-) My plans next: getting a cabal package out on hackage.
It stands for International Conference on Functional Programming Programming Contest. Sorta like ATM Machine, or PIN number, because ICFP is an acronym. Notably, anyone can take part in any language. Historically, functional languages do well, but do not always win. http://www.icfpconference.org/icfp2011/ 
Thanks! Giving formal descriptions of program behavior is an extremely difficult engineering problem, and there is not simple way of "guessing" what the intended real-world behavior is. One possible approach is to give a very "high level" specification, which hopefully corresponds to very intuitive behavior, like "a missile will never be launched if button A is not pressed". Then you translate this statement into lower and lower level specifications, a process sometimes referred to as *refinement*. At the lowest level, the specification is very close to the actual implementation (in fact code can be seen as a very low level specification language for software behavior). Still, allowing an intuitive and precise way of specifying program behavior is still a very open research problem.
Turns out this isn't building properly on ghc7, you first have to edit the zeno.cabal file and change: &gt; if impl(ghc &gt;= 7) &gt; ghc-options: -with-rtsopts="-N" into: &gt; if impl(ghc &gt;= 7) &gt; ghc-options: -with-rtsopts=-N -rtsopts I've also just migrated TryZeno to a much more powerful server, so it now solves properties in parallel across 32 cores. Try it out by attempting to solve all example properties containing "insert", then sit back and feel the speed...
The page is lacking in some context, yes. The ICFP (International Conference on Functional Programming) is an academic conference that's held every year in the fall. Associated to the conference, they have a programming contest, which happens in the summer prior, and winners are announced at the conference. Though the organizers hope people will use functional languages, you are permitted to use whatever language you like (this originally being to avoid having to define "functional", since it's difficult to draw a clear line). Traditionally (in the past) the contest rules are that the problem is released on Friday at noon GMT, and submissions are due by Monday at noon GMT. There has recently been a "lightning round" open to submissions by noon GMT on Saturday. It looks like, for whatever reason, the times this year have been moved 12 hours earlier, and the lightning round (which was a more recent tradition anyway) has been eliminated. I think that's all the context you need to understand the announcement.
Darn good explanation for non-programmers. Tested it on my wife. She said "That's what you've been telling me for a while". She is still skeptical on the 'cool' part. 
What is zynism? I'd like to know whether I spotted it or not.
Must one be present in Japan in order to join the contest, or can anyone submit their solution online and potentially win?
No, you can take part from anywhere (with an internet connection). For example, in 2009 around 800 teams took part, with 330 teams successfully solving at least one part of the contest. This link ( http://vimeo.com/6613815 ) is a video for the talk about the 2009 ICFP Programming Contest, and will give you a better idea of what goes on.
Where did the use of [grave accents](http://en.wikipedia.org/wiki/Grave_accent) in place of apostrophes come from?
Markdown. Or `Markdown` as we say these days.
I use them when providing code, and this I think comes from their use in shell (and perl) scripting. my $output = `$cmd @args` sets the value of $output to the text output of $cmd given the argument list @args. Thus, I use quotes for strings, and backticks for code. Also, it's much more common for quotes to be part of the code one needs to type than backticks, so it's less ambiguous, IMO.
Use $(cmd). Obviously nesting `` is not cool.
Interesting. However, I don't think that hashing is part of lambda calculus. 
There's nothing in his code that couldn't be replicated in the lambda calculus.
&gt;I think comes from their use in shell (and perl) scripting. Correct. Introduced by Bill Joy in C Shell in late 70s. Perl was of course later.
More to the point, there is nothing in my code that is not a sugared version of the Typed Lambda Calculus, since that is what Haskell is. All you need to do is apply a desugaring transform to get the raw Lambda Calculus back. Of course a judge isn't going to know the Lambda Calculus from a lump of rock, but that is what expert witnesses are for. Get a professor of mathematics to testify that these are formulae in the Lambda Calculus, and that the Lambda Calculus is part of mathematics, and you have a sound legal proof. The only thing the patent holders could do is find another professor to testify differently, and that is going to be a big challenge.
Nice idea, but attempting to fight law with logic misunderstands the legal system.
Isn't this a proof though that any patented algorithm is invalid? Haskell is turing complete, so any solvable algorithm can be implemented in Haskell. Since all Haskell functions reduce to lambda calculus, and lambda calculus is maths and therefor cannot be patented, then no algorithm can be patented. Edit: Just realized the [top comment](http://www.reddit.com/r/linux/comments/h0wlk/patent_5893120_reduced_to_mathematical_formulae/c1rs5by) of this article when it was to linux was pointing out the same thing.
While I fully agree that all software-type thing should be unpatentable (and maybe more radically, that there should be no patent system at all; but that's debatable), as a mathematician, I also think that the language used here is somewhat degrading. Mathematics is not a "set of formulae"...
I believe it's abbreviated ICFPC. But people are lazy, and they call it ICFP contest (strangely, the lazy version is longer)
&gt; It looks like, for whatever reason, the times this year have been moved 12 hours earlier I would guess that "whatever reason" is that that time is morning in Japan. In the last few years at least, the start/end times always matched the local setting of the actual organizers (which is reasonable, and it won't be nice for all of the world anyway; for example this year's timing is rather bad for Europe).
To answer your first question, yes. One of the more high profile projects is the seL4 operating system, which is prototyped in Haskell and proven in Isabelle/HOL. 
&gt; All you need to do is apply a desugaring transform to get the raw Lambda Calculus back. Allowing recursion (in contrast with lambda calculus, where you have to use bulky Y-combinator) seems a little bit more than a mere sugar. Also, I have a feeling that all this formal reductions make little sense for law applications. Otherwise you could for instance argue that any patent is invalid, because it is made of text and images, and they can be encoded as a number, and number is a special case of a formula.
&gt; Allowing recursion (in contrast with lambda calculus, where you have to use bulky Y-combinator) seems a little bit more than a mere sugar. I gave the code another once-over, and it doesn't look like he needs general recursion, either. He uses foldr most of the time, and in a spot or two where he doesn't, his functions look structurally recursive. Inductive types, which can be built up from constructors, and then folded over, can be encoded in System F and other total extensions thereof. So the only question would be whether the stuff he's using from Data.Map does anything that requires general recursion. But the answer is simply no for a simple enough implementation (association list), and is probably still no even for something fancy like the actual implementation.
&gt; Otherwise you could for instance argue that any patent is invalid, because it is made of text and images, and they can be encoded as a number, and number is a special case of a formula. But real patents cover applications, in the sense of "applied science", not descriptions. A formula/program/algorithm is by definition a description, not an application. Copyright covers descriptions, patents cover applications. Precedent has ruled that descriptions that form a part of an application, but not the whole, count as applications. The question of software patents is whether pure descriptions can be covered by patents. IMO, it should not be allowed, or if allowed, then the nature of these patents should be significantly different from regular patents.
Unfortunately, I found those slides not very usefull without the speaker's comments.
Not only that, there were no egregious errors in the writing. That's pretty impressive for such a technical topic.
Thanks for clarification, I'm actually infinitely far from patent law (or any other law). What do you mean by "patents cover applications"? Is it that you can only patent something *for* something, not just something? &gt;A formula/program/algorithm is by definition a description, not an application. Well, programs and algorithms usually have applications, that's what they are designed for. Am I correct that if you use patented algorithm for a problem it was not originally intended for, it does not count as patent infringement?
[This /. comment](http://tech.slashdot.org/comments.pl?sid=2118298&amp;cid=35992140) said it better than I did: &gt; it doesn't matter if program is reducible to mathematics, only that a claim for a software patent might be valid if it contains "a mathematical formula [and] implements or applies the formula in a structure or process which, when considered as a whole, is performing a function which the patent laws were designed to protect" http://www.bitlaw.com/source/soft_pats/final.html So pure software, ie. that manipulates purely information, is purely a description, an abstract specification, and thus should not be patentable. Software that forms part of your steering column in your car is patentable when the device is considered as a whole (but not the algorithms themselves). You have it right in your last paragraph, in that if those steering algorithms are applicable to another domain, the steering column patent could not (should not at least) be applicable to your invention, except perhaps as prior art if the software aspect is your only innovation. I'm not an expert either, but I have followed this pretty closely, and that seems to be the only consistent interpretation re:Bilski, other precedents and prior statements of unpatentability of math.
&gt; There is a particular reason why monads had to arise in Haskell, though, which is to defeat the scourge of laziness. That's false on two different levels: * IO arose as a means to enforce purity via the type system, not to "defeat laziness" * Monads arose as a *generalization* of the API that was used to this effect, and have little to do with IO in particular Then he says: &gt; But even in the presence of monads, one still needs things like seq to sequentialize evaluation, because the lazy cost model is so disadvantageous Which seems to stem from his misunderstanding. &gt; My point is that the ML module system can be deployed by you to impose the sorts of effect segregation imposed on you by default in Haskell. There is nothing special about Haskell that makes this possible, and nothing special about ML that inhibits it. It’s all a mode of use of modules. In ML, you could always import an impure library, and you get impure side effects for function evaluation again. The type system does not help you. This is not "imposition" as in Haskell. Of course you could claim in Haskell you might "unsafePerformIO", but in Haskell using unsafePerformIO on a non-pure construct is considered strictly a *bug*, and you can expect any random library you use to be sanely pure in this regard. &gt; So why don’t we do this by default? Because it’s not such a great idea. Yes, I know it sounds wonderful at first, but then you realize that it’s pretty horrible. Once you’re in the IO monad, you’re stuck there forever, and are reduced to Algol-style imperative programming. You cannot easily convert between functional and monadic style without a radical restructuring of code. And you inevitably need unsafePerformIO to get anything serious done. In practical terms, you are deprived of the useful concept of a benign effect, and that just stinks! I think claiming that you *inevitably need unsafePerformIO to get anything serious done* is a huge mark of ignorance about Haskell and makes me lose all of the remaining respect I could have had for what this guy is saying. Being stuck in IO forever is a *feature*, and it encourages you to extract as many of the pure parts as you can outside of the IO part - and this encouragement actually does result in better structured programs.
&gt; This is of course the option monad, which is sometimes used to model the data flow aspects of exceptions, perhaps with some elaboration of the NONE case to associate an exceptional value with a non-result. (The control flow aspects are not properly modeled this way, however. For that one needs, in addition, access to some sort of jump mechanism.) Does anyone know what he means? The control flow aspect of exceptions is definitely modeled by the Maybe/Either monads, without any additional jump mechanism. A continuation monad/transformer could add jumps, but that would be *more* powerful than exceptions control flow, not required for it. &gt; Standard modular programming techniques suffice to represent monads; moreover, the techniques involved are fully general, and are equally applicable to other signatures of interest (arrows, or quivers, or bows, or what have you). He seems to repeatedly imply that Monads in ML are "just a library", as if they aren't in Haskell, and therefore ML is a superior language.
&gt; Mathematics is not a "set of formulae"... According to the law, yeah that's about all it is.
&gt; He seems to repeatedly imply that Monads in ML are "just a library", as if they aren't in Haskell, and therefore ML is a superior language. I'm not sure you can really even implement a proper monad library in ML, because it doesn't have higher kinds. `option` is not a first-class element of the language, only `T option` is, for some type `T`. At best, you'd probably have to encode the higher kinds using modules, like (making up my own syntax)... Monad : Sig = module (a : *) { type m } functor MonadStuff (M : Monad) { mapM : (a -&gt; M(b).m) -&gt; [a] -&gt; M([b]).m) liftM : (a -&gt; b) -&gt; M(a).m -&gt; M(b).m } etc. Edit: I guess his encoding is: Monad = sig { type 'a m ... } functor MonadStuff (M : Monad) { mapM : (a -&gt; b M.m) -&gt; [a] -&gt; [b] M.m ... } Then of course we have fun plumbing around all our monad dictionaries manually, because type classes aren't useful.
I found them somewhat useful, but the slides that simply asked a question weren't as helpful as the actual lecture discussion intended to revolve around the question would have been.
Of course, ML would also have Erlang processes if they rewrote the runtime and if everybody rewrote their code to use it… :-{ Harper is brillant (I'm currently reading book), but he has a tendency to lie by omission in his blog.
By the way, I'm curious to know what he has to say about "benign effects"… (I hope it's not : "buiding DDC is hard, let's care when we cheat about our computations")
He does at least make an attempt to suggest that better plumbing might be possible than SML currently admits. That's surely true, if not helpful in a comparison of current practice. It's probably also the case that SML's static module language is adequate to express the large run-of-the-mill majority of monad constructions as found in Haskell programs. But there's a point at which higher-kind polymorphism and *polymorphic recursion* kick through the boundaries of what you can do with static elaboration of modules. Of course, inability to express polymorphic recursion would result in not noticing this particular inadequacy. Of course, purity is the point. Haskell isn't as pure as all that (what with, oh... tell you tomorrow), but ML is worse. If chucking in a bunch of other side-effects into the top-level ambient monad (ie, the one you do the denotational semantics of your language in) is worth preaching from ML to Haskell, it's surely worth preaching to Agda as well (cough).
Urgh. 
I actually developed a theory while browsing a paper by Ken Shan recently, about encoding ML modules in F_omega. The module language he was encoding was one by, among others, Harper, and it was described as using effects to distinguish generative from applicative functors. Previously I had just thought (reading his lecture notes on 'types for abstraction,' or what have you) that he was concerned about type-level constructs being sound in the presence of effects because, well, "you can't write real programs without `unsafePerformIO`." But now I almost wonder if he (unconsciously?) thinks it's impossible to achieve abstraction (i.e. ML modules) without an effectful language. Of course, his comments in this article seem to merely suggest the former.
&gt; Haskell isn't as pure as all that (what with, oh... tell you tomorrow), I'm speculating you're referring to partiality...
&gt; Once you’re in the IO monad, you’re stuck there forever... You cannot easily convert between functional and monadic style Huh? What does he mean by that? EDIT: Doesn't the author have it backwards? Once you're *out* of IO, you can't get back *in*.
I just don't buy this "we've all got to live with shit so we might as well eat it" rhetoric at all. But then, I don't see why we should live with it. On this one, I happen to prefer Haskell's compromise to ML's compromise. Having said that, I rather prefer ML's common-or-garden applicative notation for programming with a given *ambient* monad -- I'd just like to be free to choose that ambient monad, and choose it *locally*. A better handle on what can be localized might reduce the need to resort to unsafePerformIO. But I digress. The more the type system helps to negotiate the available effect capabilities, the better. It's got sod all to do with laziness vs strictness: as ever, it's about trust. Haskellers, don't let Bob Harper's provocation blind you from actual problems and real opportunities to do better!
 freeBeer tomorrow = tomorrow (freeBeer tomorrow)
I think the author has different "defaults" : segregating effects is only a "style" for him (effects are not values for a strict programmer!). Thus "stuck in the IO monad" must mean for him that you can't go to "normal code" without "unsafePerformIO", whereas you can use the result of a pure function in a monad. The part about "not easily convert[ing]" styles must be his complaint about stuff like map and mapM being different things.
I'm glad such patents are not legal at my place ...
It sounds like [Disciple](http://www.haskell.org/haskellwiki/DDC) is aiming to do just that.
In an important respect, you're reinventing history, here. The observation that monads, the categorical notion, were appropriate to the task of modelling various side-effects in denotational semantics of ML programs predates the slightest twinkling of IO in a Haskell eye. The assertion that "monads arose as a generalisation of the API" is false, not only in the sense that the categorical concept is older, but also in that for FP, the IO monad arose as result of a monadic understanding of effects, not the progenitor of that understanding. What distinguishes Haskell honourably is that the Edinburgh mathematics *about* the semantics of programs became the Glasgow abstraction *in* the programs themselves. The presence of *unsafePerformIO* seriously undermines any promises Haskellers might want to make about what it is that well typed programs don't do. Excusing its less desirable consequences as "bugs" flies in the face of static typechecking. It would be preferable if static typechecking could actually exclude such bugs. Meanwhile, "stuck in IO" is clearly an undesirable feature sometimes, otherwise we should not need ST. Plenty of pure systems have impure components (parallel processes communicating on private one-shot channels, for example). I'm not at all with Harper on this issue, but I'd be more careful about how to make the case.
I continue to applaud that work.
Most things are easily converted, often with a simple composition with "sequence". I think that map is an interesting special case of mapM, interesting enough to warrant an implementation so you avoid defining the simple in terms of the complex. But I do agree "auto-lifting" as in Disicple (though to Functor, Applicative or Monad, not just Monad) could be a useful and interesting feature.
The control flow aspects of the Maybe monad are automatically handled in Haskell because of laziness. He of course skims over this point.
last few posts just seem like ml advocacy, which is fine I guess, but I don't understand why he needs to drag Haskell into it.
I'm not sure what you mean. The usual way exceptions are handled, control passes directly to the handler without a zillion layers of case analysis propagating Nothing. Is it clear that *laziness* is what expedites exception propagation?
Because Haskell is "winning", when it "should" be ML winning. I don't mean this as a rah-rah on /r/haskell; I mean it as an answer to your question, with scare quotes as appropriate. If Haskell was otherwise identical but few people used it, he would not be mentioning it.
The thing is, auto-lifting everything can lead to some of the same problems as not tracking effects. For instance, here are two potential clauses from a definition of `map`: map f (x:xs) = f x : map f xs map f (x:xs) = flip (:) (map f xs) (f x) Am I free to switch between these implementations without anyone knowing the difference? (That is, is this a valid step in equational reasoning? Or, is there an abstraction barrier around my choice of implementation?) If `map` is restricted to an effect-free segment of the language, the answer is yes. In the presence of effects, though, even if we are merely auto-lifting `map` to be parameterized by the effect (and thus, pure `map` is a special case), the answer is no. The two implementations correspond to: map f (x:xs) = (:) &lt;$&gt; f x &lt;*&gt; map f xs map f (x:xs) = flip (:) &lt;$&gt; map f xs &lt;*&gt; f x and those are not equivalent functions. This applies to Haskell, too. You could ask why we don't just get rid of `map`, and use `traverse` instead. When you specialize `traverse` to the identity functor, you get `map`. So you could always use `traverse` and get more general code, that will work in an arbitrary `Applicative`. The answer you might give is that doing so makes your programs look less nice, because you need to use applicative/monadic style. But another reason is that you can reason about pure code like: map f . map g = map (f . g) while the lifting: traverse f &lt;=&lt; traverse g = traverse (f &lt;=&lt; g) is _wrong_. Maybe it's okay for combinators and library code to be parameterized by effect for wider value. But you're definitely going to want to keep large segments of your program designated "this is just pure", so that you can reason about it in ways that parameterizing by effect would not allow.
if winning equates to popularity or adoption, then Haskell may be a few inches ahead, but Java and C are _miles_ ahead, and with many fewer of the features the author seems to care about. Seems completely petty to me... I'd love to use Ocaml or another ML if it did as well on multicore as Haskell, and made generic programming as easy as well.
Doubtless, Peaker is capable of independent articulation and needs words orally inserted neither by you nor by me (although perhaps you are Peaker, under another alias). Those with a finer chronology will aver which came first "class Monad m" or "IO", but IO arose in Haskell as a particular monad to fit the bill when the monad abstraction was already pertinent, even if Haskell had not made it first-class. Even in the context of Harper's remark, it's misleading to say that "Monads arose as a generalization of the [IO] API". Meanwhile, your remarks about unsafePerformIO are piety, not typechecking. If I released a package on Hackage called Kill.Kill with one exported value death :: (), would you feel safe running main = return death for ten seconds with root privileges? I hope not. Meanwhile, in ML, they build theorem-provers on top of LCF-style data abstraction. Haskell's great, but that does not mean there is no room for improvement, or that all Haskell advocacy is fit for purpose.
It might be. f :: a -&gt; Either String b f x = throw "escape" &gt;&gt; f x In Haskell we get: f 5 = throw "escape" &gt;&gt; f 5 = case Left "escape" of { Right _ -&gt; f 5 ; Left x -&gt; Left x } = Left "escape" In ML, if I'm not mistaken, you get: f 5 = throw "escape" &gt;&gt; f 5 = throw "escape" &gt;&gt; (throw "escape" &gt;&gt; f 5) = ... Assuming that's the sort of difference we're talking about. To get comparable behavior, `(&gt;&gt;)` would need to be a macro. Of course, if you use `(&gt;&gt;=)`, this behavior doesn't occur. I'm not sure if there's another example for that or not.
The way I understand it, the reason all of the other layers of the case aren't evaluated when a Nothing result is produced is because of laziness. 
Until 11.04, the debian packages were the most reliable way to get xmonad up and running on a Ubuntu system. I've switched over to installing the haskell platform from the tarballs and using cabal install for xmonad and xmonad-contrib. Reasonably newbie friendly. I've noticed this as a trend among Ubuntu beta release testing Haskellers in the Bay Area. A side effect is that broken Haskell platforms don't get caught
:( Please fix.
&gt;ML would also have Erlang processes if they rewrote the runtime and if everybody rewrote their code to use it… Harper is a language academic. He discusses languages primarily, not their implementations.
This article is really frustrating. The argument that monads are somehow specific to Haskell isn't common among nearly anybody because, as Harper points out, it's wrong. Haskell folks have been working overtime at preaching about monads as a more generally useful construct and implementing them in a useful way with lisp-family languages, scala, javascript, and of course ML. Even ruby folks started going on about monads a few years ago, although they really just meant the Maybe monad. And all this was championed by Haskell folks. Because there's language boosterism, but also a desire to promote and share good technology as widely as possible. So the real point of the article is somewhat buried. And the real point is just that Harper doesn't like segregating effects in the type system, and worries about "Algol-style imperative programming". On that, I don't know if there's a better style of imperative programming in the rest of the world than that. But that's *not* what Haskell limits you to -- precisely because you can use arrows or idioms or other sorts of ways of managing effects and state layered on top of IO, while still segregating effects. And in so doing you can encode different forms of error recovery, backtracking and choice, and all that other great stuff. Which isn't to say that you can't do that in ML either, because, certain polymorphism issues aside, of course you can. Edit: I should add that as long as Harper was limiting his arguments to languages features that made teaching easy, there was a much stronger basis to promote ML, because it is small, well-defined, etc. But this post isn't about teaching, but programming generally, and his real argument, which is that referential transparency is too much work, barely gets any play at all. That point, by the way, ties into his previous posts in that if you don't care too much for denotational semantics and equational reasoning, then dismissing RT becomes a much more straightforward affair. Edit2: And yes it doesn't encompass all "benign effects" but Harper's chosen to ignore the `ST` monad as well, which like strictness annotations last time around, get you more of the way there than he tends to think.
&gt; He does at least make an attempt to suggest that better plumbing might be possible than SML currently admits. His last post mentioned a paper he co-wrote about type classes being a specialized extension to ML modules. I believe this is what he's hinting at.
&gt; EDIT: Doesn't the author have it backwards? Once you're out of IO, you can't get back in. I think he means that the required lifting makes code traversing the monadic boundary in either direction painful.
Another awesome contribution by Bryan. Way to go man ;)
So you're switching because `cabal` is getting easier?
Nice read! Btw, the article mentions three types of `Response`, which of those would I use to implement websocket-like handling?
I have uploaded the remaining parts as well. A better link is http://www.youtube.com/playlist?p=6FCB2ACD5E2082D9 which should enable you to play all 5 parts.
In case someone missed it, related to: [Databases are Categories](http://corp.galois.com/blog/2010/5/27/tech-talk-categories-are-databases.html).
It also makes some oblique references through the commentary of the crowd to [a talk on Agda](http://www.reddit.com/r/haskell/comments/h2552/boston_haskell_introduction_to_agda_by_daniel/) that was given by Dan Peebles ([godofpumpkins](http://www.reddit.com/user/godofpumpkins/)) directly beforehand. I'll be uploading that shortly as well.
Websocket is specifically *not* supported right now by the WAI spec. If you're looking for server push, however, you would use ResponseEnumerator. NB: This has not been tested seriously, though it should work properly with Warp.
You're using equational rewriting to discuss ML evaluation, so that's gotta be trouble. IIRC, ML evaluates left-to-right, so the throw happens first. But even if not, that's beside the point. ML makes it tricky to roll your own control operators without explicit delay, which is why there's a built-in ; to do that job. But what I'm really getting at is much simpler. I'll elaborate in the other branch, as it's more directly a response to habitue.
So I'm asking how you do understand it. Take something like mapM (guard . (&lt; 100)) [0..999] We have mapM f (x : xs) = case f x of Just y -&gt; case mapM f xs of Just ys -&gt; Just (y : ys) Nothing -&gt; Nothing Nothing -&gt; Nothing As the mapM computes, later case expressions nest in the scrutinee of earlier case expressions, no? case (case .. {- 100 layers later -} (case Nothing of {Nothing -&gt; Nothing; ..}) .. {- many layers of Nothing -&gt; Nothing -} of {Nothing -&gt; Nothing; ..}) of {Nothing -&gt; Nothing; ..} Yes, laziness will ensure that mapM won't compute past 100. It is not obvious to me how laziness cuts out 100 layers of failure propagation here. Can you explain what you think happens here? With exceptions, I'd expect failure to propagate to the nearest handler in constant time: none of the layers of mapM is offering to handle the exception. I don't know how ghc compiles the Maybe version of mapM. I can see how it's possible to transform the failure-propagation layers away, in a worker/wrapper style, using explicit failure and success continuations. mapM f = go Nothing Just where go no yes [] = yes [] go no yes (x : xs) = case f x of Nothing -&gt; no Just y -&gt; go no (yes . (y :)) But I don't see how *laziness* prevents the execution of scrutinee-nested case expressions in the untransformed program.
Warning: rant to follow. I don't know Bob Harper. Perhaps he's a terrific guy in person, but in his blog posts he comes across as a world-class jerk. He seems to be consumed by sour grapes and envy over the fact that almost nobody uses SML, while other functional languages like Haskell have a growing user base. Instead of trying to figure out why SML is not serving the needs of users, he takes the opposite tack, trying to "prove" that anyone using anything but SML is misguided (i.e. an idiot). That is a totally losing strategy, and if this is all that SML has, it has truly bought the farm. A few years ago, I programmed almost exclusively in Ocaml. I considered using SML, but Ocaml seemed to be better-maintained (i.e. maintained at all!), more innovative, had a bigger user base, and produced smaller and faster executables. Plus, all the ML programmers I knew (including college professors) used Ocaml, not SML. So that's what I went with, and It Was Good. After a while, though, I started to dislike some of the limitations of Ocaml. It was too easy to pollute pure code with imperative code. Operator underloading (having to type +. for floating addition) was annoying. Polymorphic references were a huge wart. I wondered if Haskell would be better. So I switched, and yes, Haskell is for the most part better from my perspective. Type classes make overloading sane. Monads control side-effecting computations. Equational reasoning Just Works. The language is just more functional, at least from my perspective. Bob Harper apparently doesn't like people like me, and he's on a crusade to convince us that we are simply not knowledgeable enough to appreciate why SML is better than Haskell. Never mind that way more cutting-edge research is happening in Haskell than SML. Never mind that most of the interesting new research languages (Agda, Epigram, Omega) are written in Haskell. Never mind any of that, because Bob is right and we are all wrong! A typical Bob Harper post takes some feature of another functional language, and tries to show that (a) it can be simulated exactly in SML, and (b) doing so is a bad idea, because the feature is actually a misfeature and anyone who thinks they want it is an ignorant poopie-head. Then comes the argument, which involves the creation of a straw man. Then there is some actual substantive discussion of a real problem in the non-SML language. Even though the problem is usually one that is well-known and has fairly easy workarounds, Bob implies that it's a complete show-stopper, perhaps because he really hasn't done any programming in that language. Example: dynamic typing. "Of course" everybody who has a tenth of a brain knows that dynamically typed languages like Scheme are just "unityped" languages, with a single type with multiple constructors. And "of course" there is nothing you can do in those languages that you can't do in SML by writing a unitype with all those constructors. (How about: add another constructor without having to modify tons of code? Oh yeah, that.) The fact that most people don't want to have to write constructor tags for every basic data value in their program apparently isn't important; that's only a usability issue, and Bob doesn't care about usability. The fact that things like the Y combinator have a simple, clean definition in Scheme but can't be defined in ML without special type hackery is lost on Bob too. Why would you want that? Example: type classes in Haskell. "Of course" everybody who has a tenth of a brain knows that these are just a poor man's module/functor system. Now, I realize that the ML module/functor system is powerful, but for the cases that type classes handle well, it's huge overkill. If you just want to overload +, -, * etc. to form a new numeric type, I don't think modules/functors are the nicest way to do it. Of course, they CAN do it, but the result isn't something you'd want to use; it would be syntactically far too heavy. But that's usability again -- not Bob's concern. Only poopie-heads who actually want to write code care about usability. Example: Monads. "Of course" monads can be implemented with modules/functors. But type classes and the do-notation make them so much more pleasant to use! But that's usability again... irrelevant. Plus, Bob's friend has implemented some marvelous syntactic sugar in his ML dialect that nobody has ever seen. So the problem has been solved, take his word for it! Except that you shoudn't want monads, because they make it hard to stuff benign effects inside pure code. Monad users must use unsafePerformIO all over the place! (Except they don't. Hmmm.) The fact that you can use the ST monad to get localized effectful computations seems to be lost on Bob; maybe he hasn't gotten to that page of the manual yet. I truly appreciate the fact that Bob Harper is a very knowledgeable guy, and I especially appreciate the fact that he has made his textbook available for free on the internet. But his language critiques are way off base. Even though he's often correct in a strictly technical sense, he invariably misses the point by a mile. He should (a) lose the holier-than-thou attitude, and (b) if he really believes that SML is so much better than Haskell or anything else, work on making that language more usable in the cases where Haskell is currently more usable. Nobody in the Haskell community believes that Haskell is a perfect language; that's why it has evolved so much in the past ten years. SML, in contrast, seems to be standing still. If SML is the answer, that needs to change. 
Everything you say is absolutely correct, although I maintain that Bob Harper's point about dynamic languages is pretty good - he did well at combating the notion that dynamic type systems are somehow a superset of static type systems, when the opposite is true. These later rants seem to be grasping at straws a bit more.
I suppose we might have our cake and eat it by generalizing the program, then reasoning about more specific instances as appropriate. The laws we can use to reason about traverse{F} depend on F, with traverse{Id} being quite well behaved. Of course, we don't *always* get map from traverse. Streams aren't traversable, for example. Meanwhile, the fact that monadic/applicative style is clunky in Haskell might not stem from any inherent properties of monads. After all, we (programmers in general) program in our ambient monad all the time, without notational penalty. Haskell's blurring of the value/computation distinction makes it splendidly easy to write pretty control operators, but kinda scummy to write ordinary applicative programs in the presence of (non-ambient) effects. It might just be worth reviewing that design choice. It's probably too late for Haskell, but designers of new pure languages (I'm pointing at you, Agda people, and I'm pointing at myself) should think twice and consider not just replicating the same choice. **Digression.** More specifically, I'm thinking of a Levyesque separation of value and computation types, with a grammar of this form * V ::= D V .. V | {C} * C ::= [E]V | V -&gt; C where the Ds are datatype constructors and the Es are effect signatures. Braces give value types to thunked computations. Brackets mark computations with the capabilities you need in order to get values from them. Laziness and purity are separated, and they both show up in types. Slippery bit: effect annotations express an *action* on the ambient effect set, rather than the set itself. The 'discard all effects' action is written [0]; the 'no change to ambient effects' action is []. The type of lazy pure computations of a t is thus {[0]t}. Meanwhile, you can still write control operators: e.g. cond : Bool -&gt; {[]t} -&gt; {[]t} -&gt; []t is free to choose which thunk to force, and it allows both branches the same capabilities as it receives. Back where we came in, we find that traverse : {a -&gt; []b} -&gt; F a -&gt; []F b propagating effects, but for infinitary containers like Stream, we get only map : {a -&gt; [0]b} -&gt; Stream a -&gt; []Stream b I certainly don't have it all figured out yet, not by a long chalk, but I am spending chalk on it.
Only solution for me was to get the generic GHC 7.0.3 binaries, skipping the ubuntu package all together then installing the haskell platform from source. utf8-light fails though with some assembler error and the only solution was to remove -f-via-C from the cabal and hs file
By benign, he means an effect which is locally abstracted and encapsulated. Suppose we have a List module, like in Standard ML, which has a function val exists : ('a -&gt; bool) -&gt; 'a list -&gt; bool doing the obvious search for a needle in the haystack matching a predicate. One valid implementation is by the use of exceptions as an effect where we exit out early if we find an element we seek. The implementation might be bad, but it's effect, the exception, is not part of its interface. The effect is benign to the surrounding world. Naturally, we could do the same in Haskell by use of a monad to encapsulate the effect. Where Harper is coming from is to run possible dangerous effectful computations inside a module and then proceed by hiding this fact to the outside world in a safe way. Done right, the effect locally is not a problem at all: The rest of the code does not know and it does not care.
Anyone knows how this works? Some kind of lazy append-list with a smart make-list (=replicate) function that encodes the length in the append-branches? edit: I can't even figure out what his unit of time is. Seconds? Milliseconds?
The unit is milliseconds.
&gt; Anyone knows how this works? Okasaki's [random-access lists](http://www.eecs.usma.edu/webs/people/okasaki/fpca95.pdf).
I'm a fan of your use of the text package here :). Do you have plans for prepared statements?
Looks like [`Data.Sequence` uses 2-3 finger trees](http://hackage.haskell.org/packages/archive/containers/latest/doc/html/Data-Sequence.html) for similar performance.
I'm curious what other generic names have come into use for small named blocks. If I'm `mapM`ing something I want to lift out, I tend to use `doIt`, and if I'm mapping a small data transformation (throwing some information out of a tuple or the like) I'll use `fixIt`. Greping through my code, I do notice a few places where I've used `go` in the absence of recursive calls as well -- I'll try to remember to avoid that in the future.
You can also use existential types rather than listing all possible constructors, so it may actually be pretty close to usable.
I know the answer to 2! It’s “type error”. :P [Since it parses as `\x y -&gt; (x / y == 3 / 7) :: Double`.]
Responding to Chris's comment: &gt; We can pretty clearly dismiss #4, on the basis that there is no possible implementation of a general purpose programming language that cannot run out of memory. Perhaps we wish that our programs would never run out of memory, but this is a pipe dream; limited resources is a necessary concession to the operational world. Even if you insist, as in Agda or Coq, that functions be proven to be total, they can still fail due to running out of resources I really hope that one day, we can put a bound on the resource use of functions in the type and make computations total for a given set of resources. You could still have hardware failures, bit flips, and what not -- but you could guarantee a lot more than you can now. This implies the compiler verifying a lot more about operational semantics, which are becoming more complicated to describe as we grow further and further away due to compiler optimizations/transformations and things like laziness.
I like Haskell and program almost exclusively in it and anger on occasions, but I don't let it take over my identity. Granted, something akin to a holy language war has broken out since Harper started blogging, and that said, I've learned important facts, and well, yes, opinions too, about functional programming. Perhaps because I've actually seen Harper during ICFP, and perhaps because I'm not as quick as I'd like at perceiving the context of the papers presented, critical if you're an outsider trying to gain a clue on what's going on, allow me to do more than just cut Harper some slack. The regulars there tacitly recognize one another's articles of faith, so if Harper were really "a world-class jerk" academia-wise he'd have to do something wild like summarily reject everything non-ML when serving on the program committee. Harper's blog posts and all the comments they have elicited have filled a gaping need in understanding the unwritten predilections in the FP world. His effort and courage in conveying his thoughtful, strongly-held opinions I find exciting and bold. I offer him only my thanks. 
Thanks Bryan!
 data BoringList a = BL Integer a makeList :: Integer -&gt; a -&gt; BoringList a makeList = BL length :: BoringList a -&gt; Integer length (BL l _) = l at :: BoringList a -&gt; Integer -&gt; a at (BL l a) i | i &gt;=0 &amp;&amp; i &lt; l = a That was not so hard, and it is probably even faster :)
[Resource-Aware ML](http://raml.tcs.ifi.lmu.de/) is a step in this direction.
Still not very hard with list-set support: data FunList a = FL { length :: Integer, listRef :: Integer -&gt; a } makeList l a = FL l f where f i | i &gt;=0 &amp;&amp; i &lt; l = a listSet (FL l f) i a = FL l (\i' -&gt; if i == i' then a else f i) last (FL l f) = f (l - 1)
Is this complaint fixed by switching to: Exn e a = forall r. (e -&gt; r) -&gt; (a -&gt; r) -&gt; r ? Or is that doing the same re-case analysis in a different form? I wrote it out a little, and it seemed like it wasn't, but I can't be sure I wasn't auto-simplifying something in my head.
 data BoringList a = BL Integer a makeList :: Integer -&gt; a -&gt; BoringList a makeList = BL length :: BoringList a -&gt; Integer length (BL l _) = l at :: BoringList a -&gt; Integer -&gt; a at (BL l a) i | i &gt;=0 &amp;&amp; i &lt; l = a That was not so hard, and it is probably even faster :)
&gt; In OCaml you can define cyclic data structures in a similar fashion to Haskell, so this isn't really a problem with strict languages, but rather a feature that you can have if you like. This isn't true as [I recently found out](http://stackoverflow.com/questions/5810163/creating-a-doubly-linked-list-from-a-list-in-ocaml). In order to make non-trivial uses of cyclic data structures you are forced to use lazy constructs in OCaml, and the lazy decorations even find their way into the data declarations.
My example was about building a static cyclic structure, which does work in OCaml. It's no
Thanks a lot for that. I had never heard of such a thing. I wonder if it could be formalised as an extension of sized types (haven't read their papers yet.)
Indeed that does work. However, if you want to make non-compile-time-defined cyclic data structures in OCaml, such as a [KMP jump table](http://twanvl.nl/blog/haskell/Knuth-Morris-Pratt-in-Haskell), you cannot do it in a "similar fashion to Haskell". You need to use laziness. (Maybe this was your point, but I got the impression from your OCaml comment that you thought that building cyclic data structures was entirely orthogonal to the lazy/strict issue.)
Does "pure" has some other meaning than "purely functional", i.e., "without side-effects"? Because if it doesn't, then this article's title doesn't make much sense. Having values that are not constant and well known before you compile doesn't mean something is not pure. Or does it?
I think you're right. (I also think it's (modulo boxing) what's going with my continuation-passing version.) I believe it does avoid the repeated case analysis. I don't know if it has quite the same behaviour as the usual exception treatment, though. I know too little about how these things are compiled. But you can see, perhaps, why someone used to native exceptions would be troubled by the naive compilation of Maybe-based failure propagation.
the problem with cyclic structures is another problem with explicit references - if every value has an implicit reference type you can easily create such structures.
I hadn't actually thought much about the cyclic structures in OCaml. On reflection it's pretty obvious that it also needs lazy fields in the constructors, but I was indeed unaware of that when I wrote it.
Without side-effects flows both ways. We call a function pure when not only does it not affect the world but also the world doesn't affect it. The blog post goes over the various denotations of "world." 
If Haskell's not pure enough for you, then what on earth is?
"Pure" often implies returning the same output for the same input. So the question comes up - is a function which returns different output on a different machine architecture truly pure? 
Once you add list-set, there's no bound on how much slower it is since the time it takes to reference an element is proportional to the number of times the list has been updated, and there's no bound on that.
But then the question goes the other way too. Is there any function that returns the exact same output no matter what is the machine? For example: \x -&gt; x + 1 only works if x+1 can be computed and stored by the machine. If it doesn't have enough memory or if it can't compute a number that big, it will fail. So it can't be considered a pure function?
So here's a question coming out of the discussion in the comments... is it really the case that neither exceptions (both `throw` and `catch`) nor `callCC` violate referential transparency in a strict language such as ML? I have a hard time believing it, but I don't have a counterexample, and I'm willing to believe that all they violate are equational reasoning. edit: to clarify, this is of course if one adds exceptions and call\cc to an otherwise pure core.
&gt; Using macros doesn't really save us this time, because of the recursive definitions. Why do recursive macros (say, for foldr) fail? Because foldr gets expanded out into infinite syntax? Is it algorithmically impossible to calculate out the final fused version of "any" using sufficiently smart macros? 
Harper in his comment mentions that deforestation transforms are generally invalid in ML. He then adds that there exist dual transforms valid in ML but not in Haskell. What are they?
I think we could avoid a bit of this nonsense if we stopped talking about the "IO monad" and started talking about the "IO typed values," noting that the IO type constructor happens to be a monad, just as it happens to be a functor, just as it happens to be an applicative functor, etc.
Sounds great. I often used to grumble about the lack of the explicit state handle and the useless Component class.
"Purely functional" has been given a fairly reasonable definition (if you ask me) by Amr Sabry. It boils down to being an extension of a lambda calculus (or, some richer type theory, perhaps) such that the results you get for the lambda calculus fragment of the program are independent up to evaluation order, except that we allow some evaluation orders to yield bottoms where others yield well-defined values. So, #4 is a pretty poor example by this definition. It's either well-defined or bottom depending on whether your evaluation strategy has enough stack, whatever that means. Not a problem. The floating point example might actually violate referential transparency (which is a little different than the above 'purity'). That's floating point for you. For the other two, it's kind of debatable. If you view them as being chosen at link time or whatever, I don't think there's much purity can complain about. Your evaluation order doesn't determine the value of these things; what you're doing is linking against different libraries, and so you have different programs giving different results, which isn't surprising. In a distributed environment, if the value of these actually depends on where the computation is performed (that is, `os` and `maxBound` are sort of dynamically scoped to the machine), then you're again violating referential transparency, potentially (and if you violate RT, you violate purity).
On the other hand, if cyclic data can be created at compile time, it seems the language might as well offer some way to create cyclic data in apparently strict types at runtime as well. Fixed compile-time cyclic data is already admits nonstandard elements.
Deforestation is invalid when side effects are untracked, however, it generally works *better* in strict languages (or on strict data structures in lazy languages), since the cost of intermediate allocations are higher (as there aren't any lazy cons cells to help out). 
You'd have to make pretty much every function involved a macro, i.e., have call-by-name semantics. But it could be done.
He's still talking about products vs. sums. But, the reason you can't do deforestation in ML is not due to not having products (or whatever), it's because you can't reorder function calls because they might have side effects. I have no idea what he thinks the connection is. I think it's unlikely that there's a dual set of deforestation cases where _not_ having side effects makes them invalid. You do have to be careful about making well-defined things undefined, and vice versa when doing deforestation. At least, assuming you care about the latter case. But really caring about that isn't the reason you, for the most part, can't perform the transforms in ML.
Yet another example of why a top level MVar is never as good an idea as it sounds at first...
Great! Now what we really need is for somebody to take on the yeoman's work of doing the same for Oracle and ODBC :-) Incidentally, if enough folks get into the habit of writing these list-to-tuple patterns marshaling a heterogenous tuple to a monotyped list and v/v (and if one adds the json library(ies), etc, this is already a fairly common pattern) then this boilerplate can be factored out once and for all using a multiparameter type class. I have some ancient code lying around that demonstrates this, but its a very straightforward proposition.
Indeed, run-time knot tying needs mutable cells or lazy thunks, which amount to the same thing modulo sugar. But the value restriction in OCaml means that you usually need another layer of thunks to be fired off at the end. All in all an ugly affair, but more often than not you'd be better served by a mutable data structure, except for the fact that you'd need dummy values for not-yet-initialized fields.
But truly, either `os :: IO String` or `getArgs :: String` as a matter of consistency if not purity.
Your reuse argument has always been one of the strongest ones for me. Thank you for making it so clearly.
There are two denotations of \x -&gt; x + 1 going on. One of them maps to the mathematical, platonic realm: the purest of the pure. There, the function in question, with any reasonable definition fleshed-out, is an outstanding model citizen. The other denotation is an actual implementation sitting on some machine, where you do have to worry about the myriad imperfections of the entropic world it resides in. Never mind stack and storage, if a power outage strikes, that implementation of the function will fail. "We are mortal and our world is finite, so what?" is what leads many (all?) to reject #4. But see Peaker's thread below on obtaining greater verisimilitude to purity. 
Sure. I'd vote for the former. I have serious doubts about it having any added value in being pure. I lean the other way for `Int`. It's useful enough that if it didn't exist, it would probably be reinvented, only using CPP instead of being wired into the compiler. And we'd just be worse off for it. I could probably live without it being used in the prelude as much as it is, though (like: why is it the standard `Enum`? That just breaks stuff no matter how large it is).
Thank you augustss, dons, doliorules for helping me learn. One question regarding &gt; I think any kind of fragment of the language should be nameable and reusable. (Haskell lacks this ability for patterns and contexts; a wart in the design.) For patterns does he mean something like pickOut ([1,2,3,4]@p) = x &gt; pickOut (x:_) + pickOut (_:_:x:_) 4 What about contexts?