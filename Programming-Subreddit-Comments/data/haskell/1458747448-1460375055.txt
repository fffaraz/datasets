And slower :-P
One of the questions in the TypableT page is &gt; Do we want explicit names for some type representations? Perhaps `typeRepBool` etc., just for Prelude defined types. (It is nice to avoid writing `typeRep :: TypeRep Bool`) Just to check my understanding, explicit type applications address this problem, making it possible to write `typeRep @Bool` (or whatever the syntax ends up being), right?
This is simpler, but suffers from a few issues: * It is not necessarily true that anything that is summable can be converted to a list. This would rarely come up in a program, but think about the infinite sequence of 1, 1/2, 1/4, 1/8... this could be a summable data structure (it adds up to 2), but you'd never find the sum by converting to a list first. * It is wasteful in terms of memory usage.
Correct
As far as scaling animations relative to each other, say I have two animation functions `f` and `g`, both of type `Time -&gt; Element`. I can scale these two functions by changing the Signal of Time that they sample on. import Signal import Time import Graphics.Element exposing (show, flow, down) f = show g = show main = Signal.map2 (\x y -&gt; flow down [x,y]) (Signal.map f (Time.fps 1)) (Signal.map g (Time.fps 2)) By changing the `Time.fps 1` signal the rate at which `f` and `g` update can be changed. Importantly, when I implement `f` and `g` I am can express them very naturally as functions of `Time -&gt; Element`. I don't understand why continuous time makes it easier to express the animation. I used [Try Elm to check the code](http://elm-lang.org/try)
It's a work in progress, mostly in my head and not in code, yet. It's called "AL", I have a darcs repo at http://hub.darcs.net/awall/al, but this is just a placeholder right now; if you are interested I can put some more work into this and get the ball rolling. Edit: I checked out aeair's notion of modules; this is exactly what I was thinking. A module is just syntax sugar for a record. No typeclasses, you pass the modules around. Although for programmer convenience, I'm thinking of a lightweight syntax for a function to declare that it takes not a specific type of module, but a module containing AT LEAST the functions it uses, and the modules can be passed down to subfunctions automatically when there's exactly one spot calling for a compatible module. AL should look similar to aeiar wrt. modules, although I'm still toying around with dependent typing and macros, so I expect the end result to look vastly different.
Can you elaborate a bit more on your setup? what is required apart from installing Spacemacs? Does it work with stack?
And Haskell-ghc-mod
That works, of course. It's just that, in a system with continuous time, I could immediately write main = Signal.map2 (\x y -&gt; flow down [x,y]) f (speedup 2 g) (With the a big caveat that speeding up and slowing down is not a good idea for real-time Behaviors. I wouldn't write code like that, changing the framerate is *not* a good showcase for continuous time.)
Oh. That doesn't seem to be a huge difference. In both cases I can write `f` and `g` without making assumption about time, which was what Conal seems to be stressing in his [talk on the essence of FRP](http://begriffs.com/posts/2015-07-22-essence-of-frp.html). I guess it would matter more if your were composing `f` and `g`. Like `f o g` vs `f o speedup 2 g` because you would have to thread time through `f` to `g` in Elm.
Main page clearly states that packages can't be removed. You should ask administrator and keep local copies of your dependencies somewhere if you have any concerns.
Really interesting. Thank you so much!
I can provide more detailed information and assistance should you decide that you might want to pursue one of these options. 
A court order would trump the main page. I keep a local copy of the source of my dependencies. I have not needed it but its there if the unthinkable happens.
npm doesn't really have anything like dependency freezing so a lot of users will simply get new versions by accident. Reproducibility of builds is not really a thing with npm. This was the case for cabal-install users before freeze files were introduced, it is a really big deal.
Is `App2` a pattern synonym?
Yes, `@(,)` depends on [#11350](https://ghc.haskell.org/trac/ghc/ticket/11350)
They have npm shrinkwrap, no?
Like /u/andrewthad said, you can't unpublish things from hackage, so I don't think it is susceptible to this particular problem. Stackage, however, can and does remove packages from time to time. Now in that case, you still wouldn't have the massive breakage that node experienced because AFAIK removing something from stackage will only affect new nightly/ltr releases. The old ones should remain the same, so you'll only notice the breakage when you go to upgrade. (This in fact just happened to me today.) But if that happens, you can still fix it by adding the package as an extra dependency, causing it to be retrieved directly from hackage. Interestingly enough, the somewhat recent hackage feature allowing authors to upload revisions to an existing version actually does allow a single developer to cause a large amount of damage. However, it also allows us to fix things when people do something stupid like leaving off upper bounds. The hackage trustees go back and add upper bounds on a somewhat regular basis to keep hackage working as smoothly as it does. With great power comes great responsibility. One thing we could do to minimize this kind of thing would be to make hackage completely immutable. But without the ability to change anything, we'd have to implement much stronger restrictions on package uploads such as requiring upper bounds on all dependencies (and maybe even doing more invasive things like requiring a package's test suite to pass before it is accepted). The idea here would be to try to prevent the problems that hackage trustees currently fix by mutating the metadata.
Every package you depend on is pretty much a single point of failure because the author could upload a revision that changes the dependency bounds to something impossible like base &lt; 1 &amp;&amp; base &gt; 2. Our ecosystem is surprisingly delicate.
It would be wonderful if this could do for the Haskell standard libraries what Stack did for building projects. I hope it becomes like a 'Haskell: the Good Parts'. I think it's right that it's opinionated and I personally share the opinions in the readme. Is there an overlap between this and [http://haskelliseasy.com](http://haskelliseasy.com)?
This example is a really good thing to point to whenever someone asks why there's no easy way to "unpublish" code on hackage.
I'm a (the?) hackage admin, and I always thought there was no way to remove a package. There isn't for me, but it turns out &gt; There _is_ a way to do this, as we've had to do it in the past. It's definitely a very manual and irritating process That being said, packages are being mirrored, and generally there are also sources on e.g. github and probably elsewhere.
If I understand your problem, you are trying to do everything with manual recursion. That's not idiomatic Haskell. Normally you would use higher-level combinators like [map](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.8.2.0/Prelude.html#v:map), [filter](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.8.2.0/Prelude.html#v:filter) etc. You don't need to keep a counter like you would do in a for loop in C. For debugging, you can use [Debug.Trace](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.8.2.0/Debug-Trace.html) to 'escape' IO and print debugging stuff, but in practice it is not needed that often.
You can already do `class Insert e where insert :: e`, and then you don't have that ugly `#`. You get very poor type inference from instance heads that are very specific: when you repeat type variables, that means that particular instance cannot be chosen until those two type variables are known to be equal. In your example that means the 5 can't become Int or Double if that's what someMap/someSet needs. Here's a better way: &lt;https://gist.github.com/aavogt/d060d45345284a3e3239&gt;. I think type errors will be very bad using `insert` defined like `insert2` (in my paste), since you need more context to decide between: insert 5 someSet :: Num k =&gt; Map k (Set a) -&gt; Map k (Set a) insert 5 someSet :: Num k =&gt; Set k
Yeah, good point. I was mainly thinking that from the standpoint of user breakage it might as well be gone because people can't select a specific revision. (At least not that I know of.) In other words, you can't do this: build-depends: foo &gt;= 1.2.3.4-r2 &amp;&amp; &lt; 1.2.3.4-r5 If you can do that and I just wasn't aware I'd love to be corrected here. 
Requiring upper bound on all dependencies would - in worst case - result that people will specify e.g. `base &lt; 5`, so the package would possible break every GHC release. Note: Hackage requires upper bound on base, and there was (are?) packages with such bound, which broke when GHC8.0 got available. Then you be more strict, requiring two digit upper bound, i.e. `base &lt;4.10` or so. But then you'll make ekmett sad, as he knows what he are doing with his packages... and probably won't still catch e.g. missed instance to be introduced in `base-4.9.1`. Or new name in the module which is imported unqualified which clashes with local one, or ... What I try to say: it's good that Hackage prevents obvious mistakes, but I'd be strongly against any rule which could also be triggered by false positives. I'd say, your book could have a chapter about PVP/Hackage (Haven't read, it might even have one already!).
Yeah, when I said 'hard to use old revisions', I should probably have said 'impossible' :-/ That being said, both cabal and stack have `--allow-newer`, which should allow you to work around malicious changes in bounds. And AFAIK, revisions can't change anything else.
[It does](https://docs.npmjs.com/cli/shrinkwrap). What do you mean?
Yup, I've heard that there have been a few instances of proprietary packages accidentally being published, and then manually removed.
Even so, if it takes extra work to do version freezing, most users aren't going to do it. Defaults matter!
I haven't personally used it with stack but understand that it is generally painless, my setup isn't any more complicated than what can be found on the link /u/Peragot posted.
Haxl doesn't solve this out-of-the-box, as you've probably found out. It can abstract away which data source you're pulling data from, but there aren't any mechanisms for elegantly purging out of the cache (you'd have to do it yourself).
Warning, Debug.Trace is magic. Its good for debugging, but if you think you want to implement a feature with it, [well, don't even try it](https://lukepalmer.files.wordpress.com/2009/06/unsafeperformio.jpg?w=604).
Because prenex universal quantifiers are implicit in Haskell, you can. There's no real difference, just explicitness. (Of course, do note that the differences in explicitness can change which variant spelling of the type in question will be accepted by particular GHC versions. E.g., polykinded types default to kind `*`, making the explicit kind signature required to get things to kindcheck.)
Don, I probably don't have sufficient Haskell experience for this job posting but I am going to be in Singapore on business in about 2 weeks and would enjoy getting together for coffee. I do have many decades of Lisp functional programming experience, and I have been mostly using Haskell for my side projects for several years. 
For OSX, I find either Emacs or IntelliJ just fine. I bought Haskell for Mac, and have had fun with it, but I don't use it too often. Atom with the right plugins works OK also. If you like Emacs, then I think Emacs is the easiest way to get set up.
I bought "Introduction to Category Theory" which seems like a good book (I am only about 20% through it).
If there was a valid court-order, you can be sure that fpc/stackage would have to comply just as well as Hackage would. So even if Stackage wouldn't depend on Hackage, this wouldn't protect you from bored lawyers...
there's also http://hdiff.luite.com/
Another important part (and something to look into!) are functors - essentially functions at the level of modules. Often this is used when you want to implement a bunch of functionality and you need, as an input, some data type and a few operations.
Why are partial functions considered bad by the author?
Can confirm: know several Haskell devs and users here in NYC. Boston also has a good number.
&gt; What happens if some lawyer would threaten to sue unless a package gets removed from Hackage? Unclear. Decisions like this are not easy to make and the laws or claims made in question factor in deeply (in this particular example, trademark law has some limitations, like scope clauses, outside of the traditional requirement you attempt to protect your trademarks. IANAL.) Obviously, the best answer we can give is a non-answer: "we would not comply it if the request was clearly frivolous, and comply if it is legally mandated", but in practice it is impossible to say outside of a specific case. &gt; Is there really no way to remove packages? As someone with server access, yes. As a user, no. As a lawful entity: without question. We could just be forced to turn off Hackage.
I wish more of these suggestions were backed by examples to see the use cases for these libs, but the suggestions are still good starting places. *especially lazy IO, I've heard it's bad practice but not seen many suggestions on alternatives or how to avoid gotchas. 
a posedge expression (an event expression) is no function in Verilog. That means something else.
An example of how lazy IO bites: https://ianthehenry.com/2016/3/9/lazy-io/ Alternatives: basically any streaming library. Two of them are the most popular, pipes and conduit. Just choose either of them if you are just getting started. 
you da real mvp
Unless you're reading gigabytes of data, strict I/O is fine.
&gt; Maybe Facebook or JP Morgan will buy out the ecosystem, and people will quit in protest I like to think Haskell is more agile than that. Like, if Facebook bought FP complete and took over stackage and used it for evulz, we would just fork it and transition in a two step process or what have you. I don't know though.
It's actually not a bad idea. The ability to press the "delete concat" button on the other hand ...
I recommend [Awodey](http://www.mpi-sws.org/~dreyer/courses/catlogic/awodey.pdf). There's a CT reading group being held at my university (it is being given for credit, surprisingly though all the students, myself included, are undergraduate sophomores). We started with [Sets for Mathematics](http://cale.yi.org/share/Sets%20for%20Mathematics.pdf) by Lawvere, but we found that the exercises were a bit lacking in motivation, and after jumping from book to book we eventually settled on Awodey and it's pretty nice. You may also be interested in [Category Theory for Programmers](http://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/), but it doesn't go in depth enough for a lot of things. If there are two things that are essential to expose yourself to, they are the Yoneda embedding/lemma and monoidal categories (and their connection to linear logic, pipes, etc.). I highly recommend that you work through exercises as much as possible, even if they are easy, and check your answers if you can find a solution guide.
This is probably a good argument for immutability in the ecosystem. You can rename or delete packages all you want, but Stackage LTS 5.9 will always be an exact, unchanging snapshot that you can rely on regardless of any external politics.
I'll respond here, but it's really just a general response on this thread. Jump is still very much in the exploratory phase. It's not clear that it will be a prelude replacement at all (though we do have one right now). The main goal is to collect information to bring Haskellers up to speed on topics the authors consider vital very quickly. So the main focus is docs, code is secondary. I played here with a slightly less extreme version of classy-prelude to see what we lose by, eg, using Foldable instead of MonoFoldable. As for motivation for the points in the README, expect docs to flesh out over time. But there is no comprehensive explanation given right now just because it's too early.
Worth noting that Pattern Guards make rewriting with a bunch of equality proofs much easier than using case expressions.
You can't add a new dependency in a cabal file edit, just change existing ones.
As a PHP developer I am offended by this comparison, coal mining is a fine profession and should in no way be compared to the shit-shovelling awfulness of PHP.
classy-prelude / classy-prelude-conduit / classy-prelude-yesod
Author of dohaskell.com here - I just took it down the other day because I no longer use it, didn't think anyone else used it, and would rather have $15 more dollars a month :) Code's still on github, though.
To make this clear, the Swagger -&gt; Haskell route is useful for _client_ code, where the Haskell/Servant -&gt; Swagger is definitely a better choice for server code.
Actually, the code generated by swagger i snfar from ugly and probably pretier than mine rolled by hand. Also generating swagger from servant appears to be less straightforward than expected. I don't know how swagger manage sum types though.
sounds like [Joe Armtrong's proposal](http://erlang.org/pipermail/erlang-questions/2011-May/058768.html) about the "Key-Value database of all functions".
Unfortunately if someone claims that Stackage LTS 5.9 is violating their copyright/trademark/patent then you either take it down or fight them in court. This particular debacle happened because the Kik company (some kind of social network) claimed trademark violation, and the "kik" library was handed over to them. The author then took down all his libraries in protest. If this had happened to a Stackage LTS and it was immutable then the Haskell Group would have a choice: 1: Lawyer up and fight. 2: Take down the whole LTS version affected, remove the offending package, rebuild, and put out a new LTS version. Of course this destroys the concept of "long term support". Side note: one problem with trademark law is the "defend it or lose it" principle. Trademark owners have to send out "cease and desist" letters or else find themselves losing their trademark on the grounds that its being used generically.
Don's not based in Singapore, but some of us are. Get in contact with the Haskell.SG meetup group and see if there's something during your stay.
What should PHP be compared to? Hmm... How about "having to clean up the remains of a core meltdown with your bare hands"
Just in case this is the source of confusion (sorry if it isn't), "partial functions" isn't "partial function application".
&gt; One reason this was so effective was that the library author made a land grab for a bunch of common function names in the NPM namespace - He had a large number of single-function modules like left-pad, right-pad, concat, etc., putting probability in his favor that people would depend on his packages. I don't think that's quite relevant, because only `left-pad` was a dependency for a lot of other packages. The overwhelming majority of the other 270 or something packages were unused. I think the question should be "can I pull my package from Stackage/Hackage easily &amp; quickly, and what kind of impact does that have?". Clearly, the impact is not as big as breaking UX for tens of applications moments after a package is pulled, but I'm a fan of quantifying this stuff, and I don't have an answer offhand.
Interesting 
Have you used Spark, what is it about exactly ?
Would a decentralized package repository help with this? E.g. Hackage-over-bittorrent ? Just to be safe should the unthinkable happen ..
Thanks for digging! I added a revision to exclude newer ghcs (only base &gt;= 4.3 since that's what the matrix builder tests, but still)
I recently switched to Atom and I use Vim mode with language-haskell just fine. I also installed the script package so that I can run Haskell snippets from within the editor which is also really nice rather than having to launch GHCi from my terminal. My only issue with Atom right now is its speed issues, but aside from that, its features make up for it. For now at least.
I did indeed confuse it - thank you for clearing it up:)
Got it, thank you!
Not really a direct answer to your question but it might give you some ideas about some free / opensource options. This has been my MacVim based environment for a while: * iTerm2 (for terminal usage) * MacVim (from homebrew, opens as native OS X window) * Janus bundle that comes with some handy plugins for Vim: https://github.com/carlhuda/janus - ghc-mod (installed with stack) - hdevtools (installed with stack) - hlint (?) - jellybeans theme I'd recommend MacVim when you like to learn Vim keybindings. These skills will transfer greatly to editing text or other programminglanguages, or when you need to quickly edit a file on a remote linux server over SSH. Most standard code / text editing actions like selecting a block of code and cutting it, or commenting the exactly 20 next lines of code is often just 2 or 3 keypresses, instead of grabbing the mouse or selecting text with shift and arrow keys. Also easy to learn the basics with http://vim-adventures.com Atom.io seems nice as a easy to install editor with a great plugin community and a lot of features. It seems to improve rapidly but seemed a little sluggish for me, not sure if it is because it's build with JS/CSS. But if you are just starting out it should be a great editor. Currently I'm checking out Emacs / Spacemacs. It has some interesting-looking plugins that seemed to only be available for Emacs, like https://github.com/chrisdone/structured-haskell-mode My current spacemacs plugin list: auto-completion ;; better-defaults markdown emacs-lisp git ;; markdown ;; org ;; (shell :variables ;; shell-default-height 30 ;; shell-default-position 'bottom) ;; spell-checking ;; syntax-checking ;; version-control osx haskell-mode git-gutter-fringe+ syntax-checking flycheck flycheck-hdevtools js2-mode handlebars-mode (haskell haskell-enable-shm-support f) ujelly-theme stylus-mode
There are newer bindings to icu which can probably do what you want, i.e. https://hackage.haskell.org/package/text-icu-0.7.0.1/docs/Data-Text-ICU.html#v:normalize
Totally. This package is embarrassing. I opened an issue: https://github.com/stettberger/ispositive/issues/1
I am not into natural language processing, but a friend of mine once told me about the Grammatical Framework ( http://www.grammaticalframework.org/ ), which is supposed to be very good and written entirely in Haskell. Apparently it supports writing parsers that can then be used from Haskell. Aside of that the Haskell wiki might be a good point to branch out from: https://wiki.haskell.org/Applications_and_libraries/Linguistics
Paging /u/jaspervdj, perhaps he can tell you if it's a good match.
I am a big fan of this idea and am working on [a prototype for Haskell](http://haskellexists.blogspot.de/2015/03/a-very-first-step-towards-fragment.html). It extracts a "package" (called a slice) for each function/datatype in a given list of Haskell modules.
`unicode-normalization` should probably also be considered deprecated, as only one version was ever released in 2007 and it depends in turn upon the deprecated `compact-string`. For a somewhat subjective qualification of whether a package is worth trying, I tend to look at a combination of the constraints (Unfortunately `unicode-normalization` has none) and the last uploaded date of packages on Hackage as an indicator of whether libraries are being maintained to build with current versions of GHC and `base`. I'll add to /u/cocreature's comment, that you should really use `text-icu` for this given it also provides an answer to breaking by clusters of unicode codepoints - per your other question. 
I should complain that 0 isn't positive. 
This is an interesting and difficult question. It can be argued that this package as well as everything in the [ACME category](http://hackage.haskell.org/packages/#cat:ACME) just clutter hackage (although at least the ACME category makes it somewhat obvious to anyone who's seen one or two of them). It's tempting to want to solve this problem by curation, but that isn't very scalable and loses the key thing that is great about a public package repository. I've been doing more thinking recently about this idea of very granular dependencies as a consequence of my work on [HSnippet](http://hsnippet.com/). The idea for that project originally came to me because I wanted to publish and reuse some pretty small bits of code and I felt like a hackage package was too heavy. If HSnippet goes anywhere, will it lead to something like this situation we just had with NPM? Maybe? We might be able to solve it by creating social constructs with better incentives. But I really don't know. The only other idea I have as an alternative to really small packages is to just throw a bunch of them together ala [reflex-dom-contrib](http://hackage.haskell.org/package/reflex-dom-contrib). 
There's also http://www.tweag.io/blog/haskell-meets-large-scale-distributed-analytics
It is a difficult question when you're close to the line and don't know where to draw it. When it is excessive, where the line is exactly doesn't matter.
Background for people who are not aware of the context: https://news.ycombinator.com/item?id=11348798
Yeah. The specific function that got me started thinking about HSnippet was [this editInPlace widget](https://github.com/reflex-frp/reflex-dom-contrib/blob/master/src/Reflex/Dom/Contrib/Widgets/EditInPlace.hs#L46). I think Haskell's purity might make it so that smaller bits of code are more acceptable than might be the case in non-pure languages. But depending on other code is always risky and increases the ways things can go wrong.
http://www.mpi-sws.org/~dreyer/courses/catlogic/awodey.pdf
This sounds more like a good idea for a prank rather than a serious weakness .. 
It's still a single point of failure. It would be easy for a malicious actor to bring the Haskell ecosystem to a halt with a few carefully placed modifications like that.
Emacs lisp. 50,000 functions in a single namespace.
&gt; Tests *wheter* an Integer is positive. How to clutter with spelling errors :&gt;
Agreed. `parsec`'s authors could still easily backwards-compatibly "alias" their modules: module Parsec (module Text.Parsec) where import Text.Parsec 
This code needs to be more *beautiful*. The *feng shui* is *off*.
&gt; it can sabotage multiple other functions even if those functions are defined for all inputs. How does that work? I don't understand what you mean.
Literally none of that post changes the fact that, given a lawful order (the case in point, as opposed to just some random maintenance issue) we will likely pull packages if requested. End of story. If you think they can make us shut off the server hosting the package, but they can't organize things like domain name seizures, or having Amazon/RandomCloudService Inc turn off our S3 bucket - yes, they can and definitely do have such power. Do stuff to increase reliability of our systems, in the name of maintenance and longevity? Sure. Do things to decrease the chance that, say, court-ordered takedown orders can have an effect? I can assure you: courts see clever "tricks" like this to skirt around them all the time, they are common, and they will be responded to with overwhelming force. Again, it's not like we would capitulate to every random jackass who sent a nonsense copyright infrigement email (hopefully). And we *can* delete things - so again, hopefully there are no true existential threats on the horizon at this moment. But the courts themselves? Yeah, you're going to need more than some Git and S3 buckets to solve that one.
Agreed, it should have at least had an ACME prefix, but trolls gonna troll.
FFS. If a joke that's going to be funny for about 5 minutes must be pushed to hackage, at least put it in the ACME namespace.
I did scroll down. It's quite funny. Can you get safe Haskell to accept it?
Ah, thanks for bringing that up. In [transformers-4.3](https://hackage.haskell.org/package/transformers-0.4.3.0), there was no mention of the issue tracker, and I hadn't checked to see if that had changed. I'm definitely fine with using a non-github issue tracker. I've opened this issue on the repo: http://hub.darcs.net/ross/transformers/issue/25
What a disappointing response! "Lean libraries" indeed.
As of yet, to my knowledge, there does not exist an easy-to-use package in Haskell for Statistical NLP. Most new development occurs directly in Stanford CoreNLP (Java), or most recently in Spacy (Python and Cython). It requires expertise in Haskell, ML, NLP, as well as knowledge of numerical programming in Haskell. It's definitely possible to build, but AFAICT, no one has made a serious attempt yet. I could be wrong; please correct me if anyone knows of a package / library. A few years back someone was writing a book about NLP in Haskell entitled "Natural Language Processing for the Working Programmer". EDIT: Haskell bindings to C project https://github.com/slyrz/hase
I need persistent caching (via DB), can Spark do this ?
hm, is this because of the unforeseen delays in ghc 8.0 pushing back lts-6?
If the reader can understand your example code, I don't think they'll have a problem with folds...
Hackage is useless for browsing. I find packages through Google, knowing the reputations of some of the prolific writers, seeing stuff on mailing lists and Reddit...but I can't find anything by browsing or searching Hackage. There is no real organization, and the vast majority of packages are either broken or completely undocumented. So I don't mind having one more package. It just fills up indexes that are already useless for browsing.
See also [this stack overflow question](http://stackoverflow.com/questions/32223008/whats-the-difference-between-const-and-constant).
Yes, just insert a write to persistent storage in the pipeline.
Just because long chain of fixing build dependencies for aeson 0.11 is finally "good enough" to have it in nightly.
At a bare minimum, should have called it `left-pad`. :-/
What we need is a reverse dependency count on Hackage itself (and the ability to sort search results by that).
Before you look for a natural language parser you need to understand what it is you want. There are many ways of 'parsing' natural language and none are currently perfect - we don't have a perfect model of natural language yet, this is a leading edge of linguistics research with many branches. Your application may not even need any real 'parsing'. So, what is the use case?
&gt; This particular debacle happened because the Kik company (some kind of social network) claimed trademark violation, and the "kik" library was handed over to them. This isn't how trademarks work though. They don't own the library just because they own the trademark conflicting with its name. They can just prevent publishing it under that name. Pretty sure they could have e.g. negotiated a migration path with the author of the library if the NPM admins hadn't panicked and done something rash so quickly without violating that 'defend it or lose it' principle.
Presumably /u/doloto is talking about other functions using the partial function, with the author of that other function assuming it to be total (since nothing in the type or signature indicates that it is partial). That way they can take great care to handle all their own inputs, dispatching some of them to the partial function and their function will now also be partial, something they did not intend.
Sorry for the unqualified comment, I just scrolled through the article. But [this](http://www.cantab.net/users/antoni.diller/haskell/units/images/foldr.png) picture is missing (or similar).
Yeah, it tries to do everything to explain folds *except* for actually explaining it...
I have used bluez that way in the past, but the official API is apparently now the dbus API. Last I checked, there were fairly complete dbus bindings already, so it might even require less work that way.
Not all CA are on ZxZ or similar, any group will work.
Yes.
From the link below ... &gt; .. we might say that it queries each element for its value and summarises the results of the queries using the monoid. Though the difference may seem small, the monoidal summary perspective can help clarifying problems involving folds by separating the core issue of what sort of result we wish to obtain from the details of the data structure being folded. ----- [**Haskell/Foldable**: That means we can define `foldr` in terms of `foldMap`, a function which is much simpler and therefore easier to reason about. For that reason, `foldMap` is the conceptual heart of `Foldable`, the class which generalises `foldr` to arbitrary data structures.](https://en.wikibooks.org/wiki/Haskell/Foldable)
Try to make your accumulator strict, so that you don't have thunks that are created unnecessarily. Look at the code for `foldl'` for an example.
Right, *if* the OP has dbus he might use existing [dbus bindings](https://hackage.haskell.org/package/DBus) to access BLE.
Well, if bluez is an option, then dbus is at least installable if it's not already there. They're really pushing the dbus interface these days and deprecating the direct use of the C library. I'm sure it's still doable, but personally I'd rather muck around with the dbus interface than do C FFI. If all else fails, one can always download the latest Bluetooth specs and implement enough of the UART interface encoding an HCI commands to do GATT commands. This is not as outlandish as it sounds, at least if you confine yourself to just the particular use case you have rather than the whole stack of protocols and profiles. I did this myself in C over the course of a couple of weeks, albeit with a very basic subset of the HCI functionality and a tiny bit of SDP.
First tip: If you're asking for improvements to your code please provide code that works and a way of running it that demonstrates the problem. (w,`prevA ! w`) is not valid Haskell and `len` and `ws` are not in scope so I can't run the code, nor have you given an example of how to check its memory usage. Anyway, I guess True = currA `seq` a (prevRow+1) currA where ... will fix the space leak.
You used backticks in a way that doesn't make sense.
 (w,`prevA ! w`) should be (w,prevA ! w) 
Oh, I saw it now. I thought it will highlight prevA ! w part and forgot it after boxing whole code. sorry.
List structures I only know are Data.List and Data.Array, but It seems the bang pattern will works same as Vector for Array. I will try that.
Depends on what the strictness analyzer can determine. If run unoptimised, then yeah, it will all be unevaluated thunks.
You are right that a tail recursive function SHOULD probably be made strict since it doesn't produce any output before doing everything. Unfortunately the strictness analysis doesn't kick in every time.
The book looks interesting. However, the book only covers some very basics of NLP.
Thanks. The link to the [discussion on haskell libraries](http://thread.gmane.org/gmane.comp.lang.haskell.libraries/17189) was the most useful part. It's kind of a shame that people were unable to arrive at a resolution.
Well, from the second thread it seems like the current "solution" is: almost nobody uses `Constant` from transformers, instead using `Const` from base. The latter will be re-exported from a new module `Data.Functor.Const`.
Found that I can use unboxed Int array. Changed all `Array Int Int` to `UArray Int Int` and memory problem gone, but time limit excess is still unsolved. Stuck here, don't know what to do next. Anyway, this is the current state of my code: import Control.Monad import Data.List import Data.Array.IArray import Data.Array.Unboxed import Data.Array.ST getInt = read `fmap` getLine :: IO Int main = do numCases &lt;- getInt replicateM_ numCases solve solve = do num &lt;- getInt if weird num then putStrLn "weird" else putStrLn "not weird" weird n = abundant n &amp;&amp; (not $ semiperfect n) properDivisors :: Int -&gt; UArray Int Int properDivisors n = let divs = 1 : [x | x &lt;- [2..(n `div` 2)], n `mod` x == 0] in listArray (0::Int, length divs-1) divs abundant n = sum (elems $ properDivisors n) &gt; n semiperfect n = let divs = properDivisors n len = length $ elems divs in (knapsack n divs len) ! n == n knapsack :: Int -&gt; UArray Int Int -&gt; Int -&gt; UArray Int Int knapsack n divs len = a 0 (runSTUArray $ newArray (0,n) (0::Int)) where a prevRow prevA | prevRow == len = prevA | True = currA `seq` a (prevRow+1) currA where currA = runSTUArray $ newListArray (0,n) [w `seq` (v `seq` v) | w &lt;- [0..n], let ith = divs ! prevRow, let v = maximum [at w, ith + at (w-ith)]] at w = if w &lt; 0 then -n else prevA ! w For anyone interested in the problem, this is the link: https://www.algospot.com/judge/problem/read/WEIRD
Here's an example in Idris (https://github.com/idris-lang/Idris-dev/blob/master/libs/contrib/Data/Nat/DivMod/IteratedSubtraction.idr#L17). Agda seems to name it using symbols (http://www.cse.chalmers.se/~nad/repos/lib/src/Data/Nat.agda).
Sorry, I think I misled you - I only reversed the order so I could give them meaningful symbolic names. The structure of the proof for each is different. I'll do an edit to clarify.
I don't have an answer for you, but I tend to think of one of them as "diff" and the other as "smaller". Both types are themselves isomorphic to the naturals, but the isomorphism involves subtraction in one case and finding the min in the other.
Does something like [this] (http://www.stephendiehl.com/posts/haskell_web.html) help? 
your link looks like a "Intro to Haskell Web Development" rather than "Haskell Programmer has never made a web app before" resource. I'd also really enjoy a resource like what OP wants if such exists.
The [Happstack book](http://www.happstack.com/docs/crashcourse/index.html) is very well written. It starts from a simple `Request -&gt; IO Response` type and builds up.
I was thinking of starting a bootcamp for this sort of thing - I think the timing is almost-but-not-quite right given the ongoing haskell explosion, and maturity of web tools and environment. P.M. if interested / have ideas - but recognize that it's at least a year's way off
Right, I think this is how PyBluez is implemented. But that has very limited support for BLE as well, and probably could use a redo to use the dbus API.
The Yesod book (available online [here](http://www.yesodweb.com/book)) seems to be targeted towards someone who doesn't have web development experience.
Yeah, using the dbus bindings seems more promising, thanks! And I'm a she btw :) 
There are a few options for session-based authentication. One is to use the `wai-session` (https://hackage.haskell.org/package/wai-session) middleware. This will put your entire API behind th session and it won't be controlled (or accessible) by servant. For a servant-native support, you can use the generalized authentication support (currently experimental). Here are some literat Haskell docs for using General Auth: https://github.com/haskell-servant/servant/blob/master/doc/tutorial/Authentication.lhs The challenge with session-based authentication is maintaining the out-of-band state between the server and client. We don't yet have a good story to tell, but any experience report using the generalized authentication support will be helpful. Please let me know if you have any questions!
The title of this post is not a question
Thanks Aaron! Ideally I would like to keep this within servant since so far everything else has fallen into place so nicely, maybe I just need to check out the generalized authentication in more detail. My main concern is implementing this in a secure way. I'll definitely check out how wai-session does this, maybe that will give me a lead to get started with generalized authentication.
[Here's how I would solve the problem](http://lpaste.net/156759)
Oops, my apologies! Alexandroid *sounds* masculine :)
Haskell has great resources to learn web development... if you're already a web developer. As a professional web developer, I found it easy to map my existing knowledge onto the Haskell ecosystem - read a tutorial about JSON parsing here, how to set up a server there, etc. The information is all there if you know what to look for. But that's the problem. You don't know what to look for, or why any of this is important. And Haskell doesn't have any tutorials which teach you how to implement a non-trivial application from top to bottom. There's no "Rails tutorial" equivalent for Haskell, which means you're primarily learning from first principles. The complexity of the web isn't in any one technology. Any one thing can be understood. The complexity is in the orchestration - and for that, learning from first principles is woefully inadequate. If you're seriously interested in web development, I'd bite the bullet and learn either Python or Ruby. In a few weeks, you should be able to learn enough of either language to start diving into tutorials. Once you understand how everything is working together, the Haskell tutorials will make way more sense. I'm sure there are some web developers who started with Haskell and learned how to make sites end to end without knowing how to do it in another language first. I'm certain those people are in the great, great minority.
alright, I gotcha. I'd like to challenge you to start there any how! See if you can run through it. Take your time. Every time you read something you don't understand, google it for guides and documents, or other references. You may even end up with lots of little projects just to understand some of the concepts. It may take you a week, or a only a few days, who knows till you try!
You can probably use JWT (https://en.wikipedia.org/wiki/JSON_Web_Token) for this and not deal with cookies. There seems to be a library for to manage JWT (https://hackage.haskell.org/package/jwt-0.6.0/docs/Web-JWT.html -- note 0.7 version does not seem to have docs on hackage) 1. The client will send an API request to your `/login` endpoint, you authenticate it and return a JWT for that user-id. 2. The client stores the JWT and sends it along for subsequent requests. 3. For all authenticated endpoints, add a header 'Authorization Token' or something like that which you can match in servant and get the token. The client has to use the header param you setup. 3. Use the library to decrypt the token and verify the claim. Naturally do not store your secret in your code / repo -- store it as an environment variable in the server or in a secure DB. If your secret gets out, anyone can sign-in as another person (it may not be a bad idea to make user-ids random strings as opposed to primary key counters). I have not implemented this myself in servant but planning on doing it at some point. 
Yep, but it's still a single point of failure. And that's exactly what happened here with left-pad.
I would suggest you start learning the language-agnostic parts of web development first. You don't need to simultaneously learn web development and a particular framework/library in Haskell at the same time. Read about the HTTP protocol, about how things are generally done. Read about the headers and status codes and what they mean. Read about how well-known concepts like cookies work. Read about formats like JSON, and urlencoded strings. Use tcpdump or a HTTPS mitm proxy and then visit a few of your favourite sites to see how they use HTTP. Read security practices like CSRF, XSS attacks, and some basics of TLS. Those information can easily be found without being tied to a specific framework or library. 
isThirteen x = (x &lt;3 &amp;&amp; x &gt; 3) || x == 13
&gt; immutable Hackage is not. I've had admins edit cabal files of my packages behind my back, without telling me. 
Yes, the batteries-loaded nature of things like sessions is why I've switched back to working in Yesod. It just makes my life a lot easier as a new developer.
In my utopic view, packages are hosted on IPFS, a permanant peer to peer web. Where a piece of data is only deleted if there are no 'seeders' anymore. Every piece of data can be versioned like in git, and is immutable. In other words, make it theoretically impossible to delete something once its out. I think it would be perfect for package distribution https://ipfs.io/ Edit: wow apparenlty, it exists already https://github.com/whyrusleeping/gx 
That doesn't protect at all against the legal requirement to remove the software, which happens if the software's license holder revokes your permission to have it. The only solution is an OSS license where the license holder does not reserve the right to reduce license permission or revoke access to it. Then a distribution system could not be legally required to remove the library, and Hackage as it is already is sufficient technologically.
It's probably a bug in stack as this stack file seems to manage to `build` but not to `solver`. I managed to find a workaround by moving sql-fragment directory into sql-fragment-mysql-simple, change ../sql-fragment to sql-fragment in the stack file launch the solver -stack solver --update-config and it works. What is strange is, I add to properly move the directory to make it work (just creating a symbolic link woudn't work) and once the solver has done its job. I moved sql-fragment where it was, revert the path to ../sql-fragment andstack build` works perfectly ;-)
If you don't need to execute SQL, you can take [acid-state](https://hackage.haskell.org/package/acid-state).
I found http://www.sarahmei.com/blog/2013/11/11/why-you-should-never-use-mongodb/ regarding MongoDB really insightful (waring: kind of a long rant, but with much substance to think about). you probably want something like postgres and you "documents" stored as json in some "blob" or "text" field.
&gt;That doesn't protect at all against the legal requirement to remove the software, which happens if the software's license holder revokes your permission to have it. It doesn't matter what legal requirement there is to remove software if it's physically impossible to remove it. There's a legal requirement for TPB not to exist, but it still does and forever will. And a software licensor cannot simply revoke their license and decide you can't distribute it anymore.
You wrap your data and queries and it manages disk storage for it. Pretty simple, actually. &gt; tied to Haskell if you start using it. Can't see it as a bad thing (;
It's just closing options which is not good in general.
I don't think the intent is to circumvent legal requirements, but to circumvent a package's disappearance such as in the case of the author requesting removal and his request then being honored even if the package is FOSS, which, unless I missed something, is what happened with this npm case. IPFS would fundamentally strip that right away, or at least render it pointless. If it is removed it's still versioned so a client can look back to grab it.
Oh. I just bought "Categories and Computer Science (Cambridge Computer Science Texts)" because it had good ratings, but it was kind of old (1992). Someone said that for your recommended book, "the author drops in a bunch of advanced material, seemingly unaware of the fact that by doing so he's just lost his intended audience", but I think that if I know what a monad is and know a little Haskell it should be okay.
This makes sense, but what does the ( module X ) do?
People are seriously making a mountain out a molehill about this. Packages can't be deleted from Hackage, and you do not need super-admin level access to revert things like obviously malicious changes to a .cabal revision. A lot of our problems are mitigated by these differences. Go ask a Hackage trustee to un-break the cabal revisions (which will be quickly noticed), and there you go. Hackage admins, AFAIK, also have no capability to do things like rename. You'd have to manually fiddle with the database, it's not something we often need to do. &gt; What if one package developer takes his package down? Realistically? Pretty much the same thing that happens if: - Your internet goes out. - The developer accidentally breaks bounds on the package, even retroactively. - You didn't do backups. - GitHub goes down (which somehow causes your build to explode because GitHub somehow touches everything on earth.) - Amazon goes down (ditto), taking the entirety of Silicon Valley as we know it, with it. - Any million other glitches that we know and love on the 'net today, commonly. The fact this issue is exploding dramatically, I think, without the acknowledgement this is a risk we have *always* lived with, shows how utterly misaligned developers have their priorities in the common case. Rather than sit around and actually think about the risks you incur from adding code (note: you are ALWAYS adding risk), and how to mitigate that risk (am I adding malicious code? Do I need backups? Do I need to maintain it, can I get support? Should I vendor packages and dependencies?), the only thing I've seen people actually DO is flail around and immediately begin shooting from the hip with everything they can to 'fix' the problem, from suggestions JS developers are idiots and shouldn't use micropackages, to shit-based "left-pad as a service" joke Heroku-dot-com services, to we should use Content addressable storage! DHTs! Blockchains! Just never delete anything and the problems are totally fixed, I bet. No, you can't "amend" revisions anymore - that was never a good feature! No, just ignore the fact copyright material can't be deleted! That's not "a new problem" I've created - no, it's "sticking it to the man!" in classic punch-up style! The way I see it is, why bother with actually thinking hard about the issues of package ownership and trust in modern development cycles, when you can just take the entire human factor out of the equation, stir up a lot of controversial nonsense over a threat that has existed for decades, throw some consensus based distributed nonsense into the mix (at a substantial complexity increase) to make it sound good and call it a day? One is a boring social problem (that's actually difficult), the other is a cool techno gadget (of dubious utility). I'll bet $1 on the answer most programmers choose. **EDIT**: BTW, I do not mean to imply DHTs or whatever your favorite techno-gadget is, aren't useful. Or that they can't solve this problem. Rather, the problem is I see everyone grasping for solutions, rather than first looking at the *problem* and figuring out **what the intent of the system is**, and how to work it. If you don't know what the intent of your system is, you can just pile as much crap on as you want, and sure, it could work. Until AWS goes down, that is. Unless, of course, your *intent* is "to be able to deliver software to your customers on demand, as needed, without undue 3rd party reliance". In which case, maybe you should vendor or mirror your needed dependencies, on site. In this case, the intent is to deliver software - "vendoring" is a natural consequence of that intent. It's possible some distributed thing would help. Maybe CAS, DHTs, Chains etc can solve this. But we should use them because they are a natural consequence of our intent, of finding a solution to problem we seek to solve. The intent is to distribute code for a variety of use cases in a reasonable manner, securely, with the right tradeoff of what we allow, and what we don't. Maybe if that is our intent, DHTs are the 'natural' solution to that. But the intent is *never* to "use DHTs" or "use IPFS" just because.
Indeed, it looks really useful and the use case of disconnected builds would be awesome to have upstreamed into stack itself to be honest.
Ah I see! Thanks for pointing this out, I guess I got thrown off by the example/server package. I still don't see how this would help me with my problem though, since there's still the `Manager` parameter of the `client`function, unless there's some alternative to `client` that I am missing?
Good point. However it's too haskish at the moment. Thought I'd better rewrite it first, because after this initial experimentation I realized what features I'd like `haskell-stack` to have so that `stackage-dist` would be much less hackish than it is now. For instance, I'd like to have something along `stack setup --output-snapshot=&lt;path&gt;` that for a specific resolver would install GHC, download the hackages indices, download the sources for all the resolver packages, have them all ready to use under the provided path, with an optional `--dest-dir=&lt;path&gt;` (like the autotools DESTDIR env) for build-root packaging purposes. Then a config option under `.stack` would optionally allow to use that prefix, building all packages having strictly Stackage dependencies, without internet access. All of this not difficult but not so trivial either - some files under `.stack` contain absolute pathnames so you can't just move that directory somewhere global. Also, the feature for having pre-built binaries of all of the packages under Stackage is not fully baked in at the moment, and I'll like to address that too. It requires an additional `haskell-stack` change that would reduce the bash script hackery around it considerably.
BTW if you ignore `stackage-dist` and only use [my tentative haskell-stack modifications](https://github.com/kernelim/stack/commits/tentative) with the `download-cache` feature that I've added there, you can defend yourself from network inaccessibility (or tampering thereof).
I don't get why this is needed... doesn't cabal have its own tarball cache in `$HOME/.cabal/packages`? And there's also `cabal fetch` which helps you populate that cache including transitive dependencies?
Interesting. Does this mean that GHC doesn't have sensible defaults? Would it be justified for GHC's defaults to be increased as in the article?
&gt; It looks like emitting code suitable for dynamic linking is faster (this is not exclusive to libraries, the same results are obtained when compiling code that ends up as dynamically linked executable). Unfortunately I have no good explanation for why that is the case. If anyone does, please let me know as Im quite curious. Does anyone here know the answer to this?
Just like in the article, I had success in reducing build times a bit by tweaking with the GC settings for GHC builds. I'd be afraid that trading memory usage for build times could backfire on low-RAM machines, causing slowdown or making builds fail entirely for complex codebases. I imagine it's very hard to find settings for a GC that'll work in all cases.
This is for 'haskell-stack` users, not `cabal-install`, and `haskell-stack`has larger variety of network accesses besides for package sources.
Postgres has explicit support for JSON now (with validation and indexing), too! http://www.postgresql.org/docs/current/static/datatype-json.html
My biggest issue with GHC has been trying to use it on VPS machines that would choke on specific depencies for lack of RAM (or, with sufficient swap, be extremely slow). These issues make me more and more interested in going back to OCaml, which is sad because I really prefer Haskell as a language! Maybe I should invest in a beefy server, but I still have a strong inclination towards working with lightweight tools...
Another point that people often forget: `cabal` compiles with optimization enabled by default. However, `-O0` is often perfectly sufficient for code during development. For instance, on a reasonably sized project I have laying around, disabling optimization brings from-scratch compilation time from 2 minutes to 40 seconds. Sure, I wouldn't deploy like this, but it's perfectly usable for running small testcases.
The module MyPrelude exports a module X. Since the example imported a number of other modules "as X", they all get squished together and exported. This is mostly used to have functions and/or constants inside a module that you don't want to export, for example because they are only relevant for the internals of your module. For example: module MyLibrary ( x , y ) where x = 1 y = 2+z z = 4 If you import the above module, you could use x and y (because they are exported), but not z. Code inside the module can still see z. It's similar to "private" members of a class in an object oriented programming language. 
Ah! I see. Thanks for the reference though. Do you people have any plan to continue the book?
Great podcast. Really happy to have played a small part in making this happen.
Ah yes, that is indeed correct. I didn't see that shared versions are also built by default, so it makes perfect sense that disabling static version makes compilation faster. In fact, building static version only is slightly faster than building shared one! I'll update the post accordingly. However, when building executables it's still relevant. I tested this with a big project (~330 modules) and compilation of statically linked executable takes 35.74 seconds (linking takes 3.25 seconds), whereas building the project with --enable-executable-dynamic cabal flag takes 23.92 seconds (linking takes 2.61 seconds). So linking is faster, but only slightly, it's the compilation phase that differs greatly.
In general it is not possible to revoke a license. It has to be written explicitly so that you can. `npm` is for open source packages.
As someone who wrote a big chunk of that tutorial, I'd like to add that I would really like to hear about the parts that really made it hard for someone who doesn't know the terminology etc. The goal definitely is to make it as accessible as possible, over time. Any feedback there is welcome!
I disagree. Why is this an instance of Num? Simple subtraction can throw. So I would need to handle exceptions in pure code. I wish the base libraries had better numeric types but this sure isn't the way to do it.
Why not just use refined types? https://nikita-volkov.github.io/refined/
It's also ACID and still faster than MongoDB.
NPM packages are allowed to use arbitrary licenses. [NPM's package.json supports arbitrary licenses](https://docs.npmjs.com/files/package.json#license). Whether NPM itself would host such a package, I can't say. 
Types don't throw exceptions, functions do.
Make sure to the code the website for it in Haskell. ;) (Oh, and you should include Haskell to Javascript compiling. That stuff is cool.)
Some good ideas here. I have one: Cabal, Stack, use my cores. I have a whole bunch of them. If building haddock and profiling, do it in parallel. Ghc, I'm looking at you too. Go ahead and spark some threads. You could be half way done with optimization before parsing completes.
Ghc, cabal, or both?
No, it's just the name of the Stack Overflow tag for it.
I prefer to use Liquid Haskell for this purpose: https://ucsd-progsys.github.io/liquidhaskell-tutorial/01-intro.html
For what? Why?
Just FYI, there's [c-inline](https://www.fpcomplete.com/blog/2015/05/inline-c), which will reduce your C binding efforts significantly.
oh come on man, `Natural` without a `Num` instance is kinda useless... next you complain about a simple `div 1 0` throwing a div/0 exception too?! maybe `Num` should be split into two classes instead... to me, `Natural` is to `Word` what `Integer` is to `Int`, so all of those have `Num` instances
You got the ball rolling ya humb. Don't think it would've happened without you! Thank you :)
GF is excellent. It was a major multi-year project involving top linguistics and computer science researchers, funded by the EU as part of their effort to create automated translation systems for documents. However, GF is deterministic NLP, not statistical NLP like Stanford.
I'm pretty sure he was referring to the library as a whole. It has bad typeclass instances, so it's bad.
I've started wondering if there was the equivalent of design patterns for Haskell which aim to specifically address the problem of architecting applications
Probably `zoom` and such from `Control.Lens` would help.
Liquid Haskell has several downsides including: * Requiring full conversion * Having a high learning curve (which is not helped by newbie unfriendly error messages or outright bugs) * Requiring yet another program (beyond GHC) I am very excited by the project, but after playing around with it, I concluded that it was not ready for me to use yet.
I'm guessing you would have to reevaluate what the code is *actually* doing at this point, and try to pull stuff out of the monad. Abstract over lower-level operations until the code within the monad is at the highest level possible. That said, I've only worked on small projects, so I don't know if this simple approach scales up to huge codebases.
I'd be very interested to see an answer to this question too!
I'm in roughly the same place as you, although my small project is a networked multiplayer game simulation. I've briefly glanced at Yampa and Auto (the latter not suited to realtime really), and while I can definitely see the potential power of LSP I still run into a particular problem: Specific functions downstream tend to generate messages which get propagated back up to IO and back to the player(s) via the network (move/spawn/destroy messages for example). Unfortunately I haven't experimented enough to figure out how to do it (or if you even can). Right now I'm stuck with the standard non-FRP design pattern of weaving a large global state data structure through the State monad, so completely understand the OPs pain with points 7 and 8.
what do you mean by "propagating back up to IO"? could you explain more precisely what your problem is?
As a side note haddock documentation is broken for so many packages currently that going to the stack site instead is usually more helpful ;-)
The point about using `Natural` is exactly *because* people fail to read/understand documentation. Furthermore, by limiting the space of accepted values, we rule out the "invalid input" condition as well. With the current state of affairs it doesn't mean that things can't go wrong - but now the error occurs much earlier. In my experience, I'd much rather have my errors as early as possible at the input validation boundaries, where I'm thinking about doing testing anyway. It would be much worse if code deep inside my program could fail.
Hackage doesn't seem to generate the docs automatically, though for me it never has. Maybe authors are not expecting this?
Besides more separation between problem domains as mentioned in other threads, you also should check out 'freer'. https://hackage.haskell.org/package/freer I've found that in bigger UI apps you eventually need RWST, so you might as well use the extensible effects provided by freer. That way code that only needs to read can specify the read effect as a type constraint, and it doesn't leak the fact that you're using the RWS(T) monad instead of just Reader(T).
Thanks for the reply! Multistate sounds like a neat approach to this problem, and I'll also check out that extensible effects paper when I have time. Seems like the best bet for now is just playing around with different ideas and taking time to refactor every so often before your code gets too out of hand.
Lots of good points! ;-) I didn't know about `multistate` before, the example in the README is quite compelling. I guess with newtype wrappers this could go a long way! For stuff like logging I chose to actually do the ugly solution. I just didn't like the idea of sneaking a `MonadLogger` into everything. I tried making my State/Reader product type smaller by using lens `zoom` as suggested by multiple people, but ran into some annoying issue. For instance, you can't define lenses for existential types, so if you want to do some OOP style polymorphism and have something like a current input handler or something like a MVP-controller where you can swap out the actual implementation depending on the context etc., you're a bit stuck. I ended up writing this fun things: https://github.com/blitzcode/rust-exp/blob/master/hs-src/AppDefs.hs#L53 Works, but, hm ;-) It's just hard breaking the state/environment up, not making everything that wants to have some state/env dependent on the same gigantic record. Maybe I should just mix things up and try EE and multistate for my next project ;-)
I prefer to use `StateT` and lenses. Then I just have one monad transformer to deal with. I wrote a post on this style of programming: * [Program imperatively using Haskell lenses](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html)
FYI, I updated the article. Changes: * Additionally measuring compilation times of idris as it sports totally different behavior in some cases than lens, which is worth noting. * Added section about disabling optimization. * Corrected section about shared libraries - lens is not prone to significant time reduction when building shared library only, but idris is (I tested this on a different project before and thought this applies to all programs, but it doesn't look like it). * Summary expanded with a list of cabal options to use. 
I noticed that too, but going to a slightly older version usually allows me to see the Haddock documentation.
If you're on top of `IO`, `ST` or `STM`, you can keep your `ReaderT` and just put an `IORef` or `STRef` or `TVar` inside it. When I use `ReaderT`, I always create an `Env` type to pass around and put my data in there. Adding another piece of data is a piece of cake then. If you always wrap your transformer stacks in a newtype representing your Monad, beefing it up doesn't affect any code or readability, so it isn't a big problem to add a bunch of transformers.
`base` is a special case, and wasn't always on Hackage IIRC. It can actually be made into an appropriate `sdist`thanks to GHC's build system these days, so we just upload it onto Hackage with a few of the other things we maintain. (Naturally though, we prevent `base` from ever being reinstalled)
I have a GUI oriented program around the 120k lines. It's divided into several large scale subsystems, which tend to each have their own "main" monad stack. This represents the union of all the state and capabilities needed by the subsystem. But then within in one, usually only the basic framework really needs all the state, so I tend to start with a "interface" that plugs into the main monad stack, extracts the bits needed, and then calls the actual logic. Sometimes this means a separate StateT or whatever else is appropriate, and when it gets to the interface, state is synced back, exceptions rethrown, logs relogged, etc. But most logic can get away with plain functions or something more restricted like foldl or mapAccumL. Actually the large scale subsystems themselves are kind of the same pattern. For instance, the event loop is basically 'State -&gt; Event -&gt; State'. Functions that plug into it get the full ExceptT (StateT (LogT Identity)) stack, but more complicated ones can discard most of that. I also have a weird thing where I have a "UI state" which is the stuff that gets saved when you save your "document", and then "Cmd state" which is the global state that is not per-document. I have separate State layers for each so I can say "Cmd.M m =&gt; ..." it means it needs everything, but "Ui.M m =&gt; ..." it can touch only UI state. Cmd.M is a subclass of Ui.M so Cmd.Ms lift into Ui.M automatically. I experimented with the same trick to remove the ExceptT so the type signature can say if a function can't throw, but didn't seem so useful. I don't know if anyone else does it this way, or if it's even a good idea, but it seems to have worked out for me. A lot of the mess I've dealt with in large scale Java GUI programs is kept under control by this very strict division of capabilities, not to mention potential for deadlock, race conditions, half-updated state, things broken by unexpected exceptions, GUI/logic mixture, and all the usual things that plague my Java-oriented work life. I'm not up to the level of a million line giant 3D editor or whatever, but I don't see why my current approach wouldn't be able to scale up.
I was thinking mostly of maps, filters, searches, and splits of various kinds. The kinds of functions which mention `Char` in the type, so that if you are not so familiar with the `text` library you might think that it makes sense to unpack into a list of `Char`. Not necessary "one character at a time" functions. So, yes, also `toLower` and `toUpper`, even though as you say they are not one-to-one character mappings. Another reason not to do that for `toLower` and `toUpper` specifically is that the correct versions of them were mistakenly implemented only in the `text` library, not where they belong in the Haskell module representing Unicode, namely `Data.Char`. It should not be required to pay the overhead of converting to a `Text` just to perform a proper Unicode to-upper or to-lower operation. But that is currently the situation unfortunately.
With these modifications, would it be possible to rehost the LTS-5.9 distribution internally (on "company servers"), such that spinning up a new build environment with Stack does not require internet access beyond our firewall? (I also posted this question [on the Stackage mailing list](https://groups.google.com/forum/#!topic/stackage/VYLS1Eh8BNo/discussion))
Do you prefer classy lenses, or do you pin the state type?
I pin the state type
The Reddit Enhancement Suite has a preview (on which I rely heavily).
Short answer: probably not. Longer answer: often, library authors (specifically for data structures) would mark large parts of their data types as strict, but this was syntactic clutter; the `StrictData` extension will help that. The `Strict` extension seems to be more for people to play around with (or for the few who like Haskell apart from the laziness; I have a coworker like that).
Looking at your code has helped me tremendously-- finally get the whole Reader and State mix and how that would help me. Maybe sometime I'll be on to wondering about OP's question... ;)
It used to work fine, like once per day or so. Nowadays it seems to have a month-long backlog or something
Poor excuse. Plenty of other library language tools can install alt versions of their compiler.
I wasn't giving an excuse, or making any sort of normative statement. I was answering a question, with the answer to that question.
The strict pragma doesn't do anything you couldn't already do, it just makes it more convenient. But the performance analysis, etc., should more or less be the same. If you wrote strict algorithms with Strict off, they'll behave the same as if you wrote the algorithm with Strict on. If you wrote lazy algorithms with Strict algos with it off, it'll be the same if it's on. 
It used to happen quickly enough, but not anymore. It is funny, you can get to the docs of older libraries but not the latest. Some libs just flat out have a link to the stackage doc. 
Yes. For bootstrapping, start from a clean `~/.stack` with `download-cache-paths:` in `config.yaml` having a single entry pointing to some local empty path. Then, let it fill that path you specified with everything that can be downloaded (e.g. all the 1,800-or so source packages + GHC + yaml files, roughly via `stack setup --resolver=lts-5.9 ; mkdir /tmp/trash ; cd /tmp/trash ; stack unpack $(stack list-build-plan --resolver=lts-5.9)`). Then, you can distribute your download-cache directory among your developers, along with `.stack/indices` (it's yet to be covered by this download-cache feature :( ), having the disconnected builds configure `download-cache-paths:` to refer to the copy. `haskell-stack` would print stuff saying it is downloading, except that in fact it would be taking files from that directory.
I have many, though I've intentionally avoided putting forth a specific proposal for Haskell (I don't have [enough spoons](http://www.butyoudontlooksick.com/articles/written-by-christine/the-spoon-theory/) for that fight). My biggest peeve with almost every one of the proposals out there is that they don't include [semirings](http://winterkoninkje.dreamwidth.org/80410.html) which is exactly the problem here (though there are also other issues with the `Num` class). More generally, everyone's so obsessed with charging full-speed towards Banach spaces etc, that they ignore all the structures more primitive than rings. Looks like SubHask includes semirings, which is nice; but I think that's the only proposal I've seen which does. Of course, there are other interesting [ring-like structures below semirings](http://winterkoninkje.dreamwidth.org/80018.html), far before you get to [single-operator theories](http://winterkoninkje.dreamwidth.org/79868.html). Seminearrings are a good example that actually shows up a fair deal in parsing. But semirings themselves (and on occasion pseudo-semirings, aka non-unital semirings) are my main concern because I encounter them literally every single day at work. Another one of my peeves is that noone gives a proper handling of the various notions of "powers/exponents". Basic Haskell does a glorious thing by distinguishing `(^)` vs `(^^)`, though it'd be nice if the types were correct. The `(**)` is a lie though. We want a class for types which have all radicals/surds, from which we can define `Rational` (and non-negative rational) powers. And this should be different from a class which allows raising things to (non-rational) real/complex powers. And of course none of these should be confused with the class providing the `exp` homomorphism. This is one of those things where non-mathematicians complain vehemently about being forced to make distinctions they don't care about; but if you've ever implemented or worked with code for doing this sort of mathematics, you really want these distinctions to be present and enforced by the type system. Another thing I'd like to see is a proper handling of order theory, lattice theory, and all that. The [prelude-safeenum](https://hackage.haskell.org/package/prelude-safeenum) package takes a tiny step in the right direction by making `Enum` total and by distinguishing forward/upward vs backward/downward enumeration. I also do a whole lot of this stuff at work; though, oddly, I don't seem to render this into code very often or rather, into Haskell code very often; shows up in my Coq code quite regularly. In any case, the lack of this hierarchy hasn't caused me much pain, unlike the above two.
I don't have any resource recommendations, unfortunately. But it sounds like a fun project, and I look forward to hearing about your progress. I would, however, recommend learning at least some of the basics of signal processing. IME it's quite difficult to make sense of either papers or code for signal processing without knowing a bit of the background. The code itself isn't that complicated really, rather it's an issue of making sure the code does what it intends (or inverting that process). There are a lot of subtle and finicky details to beware of.
Unless there's ABI compatibility. Also, I can use a new package database for the rebuild of more packages, including the compiler itself, also from the same cabal packages.
Invariants are a good point as well. There are some approaches that work with lens: https://www.reddit.com/r/haskell/comments/3rby6y/maintaining_invariants_with_lens_in_nested/ You can write phantom lenses that don't correspond to any actual member of the data structure, but return derived quantities or manage a set of fields, enforcing invariants. I think lens is really just a nice layer of syntactic sugar here, doesn't really change my architecture. I sometimes think a lot of this is just a clumsy way to do OOP style programming. Lots of plumbing code to extract and store sub-state, abstract data types, view patterns, syntactic sugar from lens to make State look more imperative etc. It buys you quite a few things in terms purity, flexibility and composability, though. I guess similar to you I never really extracted all of this ad-hoc code into a more general pattern, but I'd really like a library that codifies all of this into a framework that hold up for more complex use cases.
I originally learned the basic form of this structure from the example program for the glfw-b library, then refined it over time. It served me well for small-medium sized projects!
&gt; If you wrote strict algorithms with Strict off, they'll behave the same as if you wrote the algorithm with Strict on. Are you sure this is true? If you sort the list and then take 1 (strict) that is not the same thing as performing a lazy sort and taking the top element. I'm pretty sure the latter is much more efficient, and is the entire motivation for laziness in the first place.
Mainly for code clarity and code reuse: The library provides a rich set of combinators. You also have available the `ReaderT` the `Reader` Monad Transformer. Using `&gt;&gt;=` on vanilla functions that don't explicitly carry the connotation of a `Reader` generally obfuscates your intention. 
What do you mean with "base libraries" here? `integer-gmp` and `base`? Or all libraries `ghc` depends on? If the latter, why?
Hmm it works fine for me. What is the output of `ghc-pkg check`?
I'm currently writing a type checker for a toy language. There are different rules for different terms in the language, and while they're all going to get combined, some of them use various bits of the monad stack and some of them don't. As a forewarning, there isn't consensus around the following approach (`mtl` + classy lenses/prisms). Some folks prefer to make a fixed stack and run with it, others prefer using `extensible-effects`. I'm using this approach for this particular problem, and it seems to be working for me so far, and more to the point it might serve as a useful example to try to make what the `Reader` provides a little clearer. There's a bit of pattern matching for the various typing rules weaved in here, hence the `Maybe`s. The classy lenses and prisms come in via the `As...` and `Has...` constraints. The straightforward rules are pretty boring: inferTmFalse :: Monad m =&gt; Term -&gt; Maybe (m Type) The rules which can create errors are a bit more exciting: inferTmAnd :: ( AsUnexpected e , MonadError e m) =&gt; Term -&gt; Maybe (m Type) And there are some that use Reader for the typing context: inferTmLam :: ( HasContext r , MonadReader r m ) =&gt; Term -&gt; Maybe (m Type) inferTmVar :: ( AsUnexpected e , AsUnknownVar e , HasContext r , MonadReader r m , MonadError e m) =&gt; Term -&gt; Maybe (m Type) (Some of this might not be correct, there's a bit more going on in the actual code so I'm transcribing it to a slightly simpler system here). I might eventually throw a `Writer` in there (or `State` pretending to be a `Writer`) in order to log warnings about shadowed or unused variables. I can combine these together at will, as long as I can satisfy all of the constraints. So I gain a few things from using monad transformers here. Any rule that doesn't need to use the `Reader` functionality doesn't need to mention it. I don't need to thread the environment through the rules that don't need it, and when I add warnings to this system I won't need to refactor any of the rules that don't need it. That also means that there are fewer valid implementations of those functions. It's nice to rule out clumsily creating a bug when I do a last minute refactoring - I can't get confused and modify the environment in `inferTmAdd`, because it doesn't know there is a `Reader` in the stack. The classy lens / prism approach helps with that on another axis. There might be other things in the environment that `inferTmLam` shouldn't touch, but that's fine, because it can't touch anything but the context. 
 sample :: ('[Int, String] ~ Signals m, MonadHandleOne m) =&gt; m () sample = do signal (1 :: Int) signal "asd" sample' :: ('[Int] ~ Signals m, MonadHandleOne m) =&gt; m () sample' = handleOne sample (\(_ :: String) -&gt; ()) id When you call `handleOne`, the type-checker cannot deduce what monad the `String` signal is coming from (that is, the variable `m0` is ambiguous, as GHC says), because `WithSignals` is not injective. I have a couple of ideas, but haven't got the time to flesh them out yet. We can supply the source monad; something like this, modulo making it compile: sample' :: ('[Int, Double] ~ Signals m, MonadHandleOne m, HandleOne (Signals m) s '[Int]) =&gt; Proxy m -&gt; WithSignals m '[Int] () Getting rid of this `Proxy` argument (or an equivalent type application) amounts to making `WithSignals` injective. GHC 8 might help here (or not, I don't know the details of the upcoming injectivity feature). Another way is to provide an inverse type family that maps `m` to `m'` such that `WithSignals m' '[Int] ~ m` (in practice I assume it will be necessary to add this constraint to the signature of `sample'`).
Is it obvious what the mapping from `a` to `b` does? Is there more than one possible mapping? If it's obvious and there's only one possible mapping, then `a2b` might be okay but in many cases you'd want something more specific, see for example https://wiki.haskell.org/Converting_numbers#Converting_from_real-fractional_numbers_to_integral_numbers
You can totally pattern match on a reader monad. In fact, you can convert a function to a reader monad.
Thanks. I take it that your lack of resource recommendations means you also don't have any suggestions for how to learn signal processing?
You can *ask* for the contents of the environment, than pattern match on that; but you can't pattern match the contents of a Reader monad on the left hand side of a function definition, which is what I casually meant by pattern matching. 
In my experience these types of functions are most commonly called either [fromA](https://www.haskell.org/hoogle/?hoogle=from) or [toB](https://www.haskell.org/hoogle/?hoogle=to) and located in B's or A's module respectively. There's no clear convention if A and B live in the same module but in my experience this is rare. Where necessary I use aToB or bFromA.
It's my understanding that you don't often have fully polymorphic types in real world code, because they're mostly useless. Unless they have typeclasses applied to them, I suppose. So in textbook exercises, I name my functions that all the time, yeah. 
I just tried doing it myself after `makeLenses` silently failed. Becomes rather self-evident why it isn't possible if you just try for a while!
Of course it would be Kmett that's written this :D
`Control.Category.Braided` is what made me decide to learn Haskell :-D
This makes sense. If I can encode enough properties about `Signals` and `WithSignals` it should work. The problem is that I can't write stuff like `Signals (WithSignals m sigs) ~ sigs` without infinite types (even though those are type families...). I'll keep trying.
Here's what I've got type Signals m :: [*] type WithSignals m (sigs :: [*]) :: * -&gt; * signal :: Elem s (Signals m) =&gt; s -&gt; m a handleOne :: (sigs' ~ Signals m, MonadHandleOne m) =&gt; (s -&gt; b) -&gt; (a -&gt; b) -&gt; (forall m'. ( HandleOne (Signals m') s sigs' , MonadHandleOne m' , WithSignals m' sigs' ~ m , WithSignals m (Signals m') ~ m' ) =&gt; m' a) -&gt; m b But now I get this error Could not deduce (Signals m' ~ '[Int, String]) from the context ('[Int] ~ Signals m, MonadHandleOne m) bound by the type signature for sample' :: ('[Int] ~ Signals m, MonadHandleOne m) =&gt; m () at src\Control\Monad\Signal\Class.hs:59:12-57 or from (HandleOne (Signals m') String '[Int], MonadHandleOne m', WithSignals m' '[Int] ~ m, WithSignals m (Signals m') ~ m') bound by a type expected by the context: (HandleOne (Signals m') String '[Int], MonadHandleOne m', WithSignals m' '[Int] ~ m, WithSignals m (Signals m') ~ m') =&gt; m' () at src\Control\Monad\Signal\Class.hs:60:11-52 In the third argument of `handleOne', namely `sample' In the expression: handleOne (\ (_ :: String) -&gt; ()) id sample In an equation for sample': sample' = handleOne (\ (_ :: String) -&gt; ()) id sample Which is pretty strange given the signature for `sample` exectly mentions the context that apparently can't be deduced here. 
My braided monoidal categories bring all the boys to the yard.
You only have to do something in the `ReaderT` based readers. When you're working with `((-&gt;) r)`, you can just do whatever. example = do a &lt;- ask x &lt;- either (++ ": Oops!") (+5) return (a,x) &gt; example $ Right 5 (Right 5,"10") &gt; example $ Left "Waffles" (Left "Waffles","Waffles: Oops!") When working with `ReaderT`, you would instead write: `x &lt;- ReaderT $ return . either (++ ": Oops!") (+5)` or more succinctly: `x &lt;- reader $ either (++ ": Oops!") (+5)`. No asking needed.
I feel like `UndecidableSuperclasses` works for this. [Relevant thread](https://www.reddit.com/r/haskell/comments/48hrb5/edward_kmett_undecidable_superclasses/)
Well, sort of. You can run `stack exec yesod devel`, which will work. But there's no built-in development server in Stack itself.
Sorry, i mean something not executing yeso to avoid to create the dist directory. 
When I say base libraries, I refer to the libraries which GHC ships with by default, i.e., which are never built with Cabal. In principle it should be possible to build these libraries with Cabal but in practice this is never done.
I don't see how that's strange. `sample` requires this constraint, it does not provide it. The new signature of `handleOne` looks wrong. The problem of not being able to deduce which monad to handle the signal from still remains (simply moved out from `sample'` to `handleOne`). It's also unusable since you can only call it on values which are polymorphic in `m'`, which excludes any concrete type such as `SignalT sigs m a`.
Yes, that's right, assuming that cabal-install is able to build base.
I personally avoid using abbreviation (there are often more than one way to abbreviate something, like tran or trans for transaction) therefore I always go for `to` over 2. As other people said `convert` isba good choice too.
This is a nice video, but my problem is that I already understood (non-partial) derivatives of regular expressions from the [Brzowski paper](http://dl.acm.org/citation.cfm?id=321249). To me, the derivatives method almost but not quite told me what I already knew, as I first learned how to handle regular expressions/grammars from the [Grune, Bal, Jacobs and Langendoen book](http://www.amazon.co.uk/Modern-Compiler-Worldwide-Computer-Science/dp/0471976970/ref=sr_1_2?ie=UTF8&amp;qid=1459185443&amp;sr=8-2&amp;keywords=modern+compiler+design) which uses a dotted-rules approach, and I had already adapted that to a "some, not necessarily canonical, representation of the tail set" mental model. However, the little I know about *partial* derivatives of regular expressions is that in general the method yields NFAs which aren't necessarily DFAs. It's no surprise that the two concepts are very similar (after all derivatives and partial derivatives in calculus are mostly manipulated the same way, but the partial notation reminds you of a few things you can't do, which I have unfortunately forgotten) but there is presumably some difference. What the video shows ends with not-necessarily-minimal DFAs, not with NFAs, so presumably it's the non-partial derivative method described by Brzowski and *not* the partial derivatives method. Because it's dealing with DFAs, the Brzowski paper can also address minimization (there's one well-defined minimal DFA form for any DFA, which you can't claim for NFAs), also using a derivatives-based approach, though personally, I particularly like the Hopcroft method - there's a link to the 1971 paper in the references to the [wikipedia minimization page](https://en.wikipedia.org/wiki/DFA_minimization). Mainly because of a serendipitous bug in my first attempted implementation, which I now think of as "unsafe minimization" (the automaton behaves correctly on any given input, but may accept invalid input sequences - thereby getting a bit more minimization in contexts where you know the input sequences will always be valid anyway). Anyway, presumably a partial derivative has the possibility of two or more transitions out of the same state for the same token, and thus there can be two or more partial derivatives of the same expression with respect to the same token. But doing that arbitrarily seems doomed to blow-ups, so can anyone explain the actual difference between partial and non-partial derivatives? BTW - I've tried to read the partial derivatives paper a few times, but not got very far, and the same thing happened the first few attempts at both Hopcrofts and Brzowskis paper. A lot of academic papers give me serious headaches I'm afraid. I'm probably overdue for another attempt, though. 
Right now, we currently don't have a lot of effort going on sharding the documentation builder so it can catch up more rapidly if it dies (Davean does 'manual' sharding currently by running batches of the documentation generator to catch up.) That's why the backlog is always so long (and can make it otherwise seem like it's not even working, when it's like 300 updates behind.) We need to build some failsafes to make sure the doc builder keeps going more frequently[1] and make sure it can rapidly catch up if a huge backlog of packages goes offline. But the reality is infrastructure work is basically all pro bono, and right now we all have a lot to do. [1] As an example, we've had some bugs in the past where the doc builder got frozen on some package (due to a bug), so by 'failsafes' I mean things like "Make sure it's always making progress, stop things that go on too long", etc, which will help ameliorate those issues (and pre-emptively stop certain fires from popping up).
Well, currently, it is is simply Not Done(TM) because we ship stage2 compilers which are supremely inflexible w.r.t. the choice of the libraries that they build with (it must be precisely the same version of the library which was used to build the stage2 compiler). I think Cabal has some hardcoded logic to stop people from attempting to build base. There may be other details which GHC's build system deals with which aren't in Cabal by default; the best place to find out about this is to check out ghc-cabal in the GHC source tree, which implements GHC's "custom" Cabal configuration for these packages.
I think the venerable reference is Oppenheim's Discrete-Time Signal Processing. I'd also recommend Lyons' Understanding Digital Signal Processing.
 instance a b where convert = fmap
At some point when you were talking I mentioned that there was a relevant bit in my multicategories repository on github that might help for normalizing sequences of Reidemeister moves. After the talk I went home and played with it and it turned out to be a false positive that didn't work.
At [34:38](https://youtu.be/R4nLSxCKkNw?t=34m38s) and especially in the answer to /u/edwardkmett 's question, there's a nice response to the common Haskell-in-the-workplace concern, "But what if we do a project in Haskell, and the lead developer leaves?"
Well, to be honest I've just tried random things. I'll think about it some more. If you'd like, you can try getting it to work. The repo is here https://gitlab.com/LukaHorvat/open-signals Anyways, tell me if I'm bothering you.
 - Avoid redirects. Every redirect costs about 120-150ms. For example http://www.happylearnhaskelltutorial.com/1/types_bot.png redirects to some S3 bucket. - Avoid serving directly from S3, it's not a good CDN (it's rather slow, compared to cloudfront for example). - Because the URL to the images changes all the time, they can't be cached by the browser. If the user goes back and forth between two pages the images are downloaded over and over again. Think of the users visiting your website through a mobile connection. - Images can be compressed better. ImageOptim was able to decrease the size by about 13-21% (on the few images which I tested). - The web server is incredibly slow. Downloading the images takes ages. More than 600ms spent at SSL initialisation, ~400ms TTFB and the rest (2 - 4.5s) downloading the content. On average it took ~ 5.4s to download a 860KB image.
I agree with your criticism. I just wrote the package out of frustration at having to look up all the conversions for the various string types over and over again, but I have come to wonder whether packages like this are such a a good idea given the limitations of the type system. I suppose it's just one of those ideas that everyone has sooner or later but which contain a lot of hidden complexity.
You provide a SafeCopy instance for your type that will handle "cold storage" and API to query the data and update it. Then you throw a TH blanket over it with `$(makeAcidic ''YourType ['someQuery, 'someUpdate, 'rmRfUpdate])` and that's it. Updates go to an append-only journal and then compacted using SafeCopy protocol.
What is so bad about that? Isn't it useful to know that it is a list of chars so that you can pluck characters out and manipulate them with functions that work on chars, and it also means you can use all the list functions on a string.
LYaH is one of the best programming books I've ever read and is the reason I know Haskell today.
Would you mind elaborating on this? I seem to be missing something here. What's the point of the fancy mathematics if it doesn't generate anything even remotely resembling music?
I'm currently reading LYAH for the second time, and documenting the process at [http://www.tn3rt.com/blog/category/haskell/](http://www.tn3rt.com/blog/category/haskell/). It's a great book. My only criticisms are that I'm not a huge fan of the art, and I think it could be improved with better examples and practice problems. 
The answer probably depends on whether you want to expand your conception of [math](http://arxiv.org/pdf/1309.0687.pdf) or of [music](https://www.youtube.com/watch?v=f8U-g6E8pMk).
I think Real World Haskell is one of the worst programming books I have ever read. Many of the topics are introduced in the wrong order. Sone things are introduced with a lot of handwaving; for instance there is the chapter on regular expressions that uses a hideously complex library, uses one of the most heavily overloaded interfaces to that library, and then essentially tells the reader "trust us, this works, but we won't explain why." The case study examples read like a chapter someone wrote because he happened to work on that problem. LYAH on the other hand is in a logical order with simple, illustrative examples. Great book. 
LYAH was a great intro, and in my opinion it's a classic among programming books. It was accessible, and made me feel like I could dive right in. Unfortunately, once I really needed exercises to hammer home what I thought I was learning, LYAH had nothing to offer me. That's why I think that [Haskell Programming from First Principles] (http://www.haskellbook.com/) is the next classic in the Haskell canon. It's not "free" -- but it's a better investment of your resources.
Let's say you have a function, `foo :: Monad m =&gt; (X -&gt; m Z -&gt; m Z) -&gt; Y -&gt; m Z`, which is structured so that `foo f y = ... f x (foo f y') ...`. If you had a function `bar :: W -&gt; X -&gt; Z -&gt; Z`, you could pass it to `foo` in two ways: foo (\x mz -&gt; bar w x &lt;$&gt; mz) y -- using partial evaluation foo (\x mz w -&gt; bar w x (mz w)) y w -- using a reader monad The only advantage of the second method is that you can modify `w` before passing it to `mz` -- equivalent to the `local` operation. (Perhaps `w` represents depth, and you want to increase it each time `bar` is called.)
LYAH was my first Haskell resource. For someone totally new to functional programming, the beginning chapters are great at introducing the important concepts. However, in my first attempts at learning Haskell, I kept getting totally lost in the Functor, Applicative, and Monad chapters. And I kept getting frustrated that I spent so much time following this book and still could not compile a program that did anything useful. But I kept trying to sick with LYAH because other Haskell tutorials made even less sense to me. After giving up so many times at trying to learn Haskell this way, I eventually just forced myself to learn all these concepts by reading any and every tutorial / book / paper that I could find At this point I would describe my Haskell proficiency as "useful", others might describe it as "intermediate". Looking back on why I think I struggled so much with LYAH, I think for me, there were too many analogies, too many doodles, and wayyyyyyy too much text. I just opened the "Functors, Applicative Functors, and Monads" chapter, and am stunned at how tiny my scroll handle becomes. I'm sort of an ADD space cadet, and the headaches I used to get halfway through that chapter were brutal. I think what's missing from LYAH and most Haskell tutorials / books is a very compelling reason to sit down and learn the funny concepts (Functors, etc...) in the first place. I think it's a lot easier to learn something when you know that you have a very good reason for doing so. For example, I struggled a lot with learning matrix multiplication just for the heck of it, but when I wanted to make a simple 3D game, the matrix multiplication started to make a lot more sense. In a Haskell example, I thought Applicative was the most useless thing until I was working with the postgresql-simple package and saw how I could lift my data constructor and apply it to rows returned by sql queries. Another thing I think is missing from LYAH is solid chunks of code to study. For me, LYAH attempts to make everything way too bite sized, and it's easy to get lost in the tiny examples. I liked the longer examples presented throughout Real World Haskell. RWH is far from perfect, but I appreciated bigger pieces of code to pause and study. So if I was making a recommendation to someone, I would tell them to start with LYAH and see how far they get until they get confused, then read the "You Could Have Invented Monads" blog post, and then continue with RWH until they can write a program by reading documentation instead of finding examples and tutorials. LYAH is great for what it is, but it might not be right for everyone.
However, when I tell people to read LYaH these days I tell them to skip the first chapter on IO
Reminds me of my http://lpaste.net/150858 : many (Just 2) has, in its specifying equations, the fixed points undefined and Just (repeat 2). fromJust allows us to remove the fixed point undefined, and thus manyLazy evaluates to the new least fixed point Just (repeat 2), and this laziness propagates to lifted Alternatives like StateT s Maybe.
Yes, it's in the video near the end :)
Haven't seen that paper! Oooh new braid applications to study ... Nick Didkovsky is amazing -- my [metal band](https://bassoon.bandcamp.com/) indicated in the talk has gigged with Vomit Fist a few times, they're insane!
ByteString is for binary data. Totally different from String
Thank you.
You are indeed correct. What is defined as the partial derivative of a regular expression in the talk is actually the symbol derivative of a regular expression in the sense of Brzozowski. The motivation for Brzozowski's method of constructing an automaton from derivatives is the Myhill-Nerode Theorem characterizing regular languages. If w is a word in an alphabet shared by a language L, then the word derivative of L with respect to w (or left quotient of L by w), written w^-1 L (or L \ w), consists of the words in L beginning with w with that leading w removed. The Myhill-Nerode Theorem states that a language of L is regular if and only if the set of word derivative languages of L is finite. This set of word derivative languages can be viewed as the set of states in the minimal DFA that recognizes L, with each state corresponding to the language that it recognizes. Brzozowski defined a corresponding word derivative for regular expressions and proved a similar finiteness result directly on regular expressions. More precisely, he proved that a regular expression has finitely many word derivatives modulo the associativity, commutativity, and idempotence (often abbreviated ACI) of the + operator. Two regular expressions can generate the same language without being ACI-equivalent, but ACI-equivalence is much easier to compute. By taking successive derivatives modulo this equivalence relation, you can construct a DFA recognizing the original regular expression. Partial derivatives of a regular expression were introduced by Antimirov. They're 'partial' because they only recognize a sublanguage of the entire word derivative language. If P is a partial derivative of R with respect to w and u is another word, then u being recognized by P implies that u is recognized by w^-1 R, but not the converse. Together, the partial derivatives of a regular expression with respect to a word sum to the derivative. Antimirov shows that the partial derivatives of a regular expression can be given the structure of an NFA recognizing the regular expression, and that by considering sets of partial derivatives you can construct Brzozowski's DFA. Doing this means that you never have to consider actual ACI equivalence of regular expressions, only manipulations of sets.
Quite irrelevant. 
What are the specs of your $100/mo dedicated server?
Oh definitly. With a better numerical stack the whole thing could be handled much better.
Why are there no exercises? 
Okay then. You go write code that you don't ever hope to make any money from ever, or ever hope to associate with or draw assistance from any legally operating organization. Which, you know, includes the Haskell Foundation.
I'd like to know what the primary hardware bottleneck is when compiling Haskell. Does ram size or speed make any difference? SSD? CPU core count? Or is it mostly just buy the fastest CPU you can find and everything else is largely irrelevant?
I'm wondering if there are plans to keep it updated with the ongoing changes to `base` in GHC, or is it supposed to adhere to Haskell 98/2010? Surely there would be confusion if LYAH says `Applicative` isn't a superclass of `Monad` but GHC says otherwise.
Perhaps I'm being too negative, but after reading the article, it seems like your speed up of the build and test cycle was simply to build less. I think I was hoping for something more along the lines of ghc flags that could be passed in to dramatically reduce times, similar to another recent reddit submission. I did appreciate the notes about caching dependencies. I wonder if it's sufficient to have a stack.yaml with the listed dependencies, as opposed to having to pass them all as a command line argument to the --resolver flag?
That's fantastic /u/joeyh! I'm glad to see Propellor thrive. I really wanted to give it a whirl at work a while ago, but we use exclusively CentOS and, [as far as I can tell](https://hackage.haskell.org/package/propellor-2.17.1/docs/Propellor-Types-OS.html), Propellor does not support it. Any plan to change the current status quo?
I think it's pretty great, but far from perfect. With CIS194 at UPenn, LYAH was recommended as an extra resource for students to find alternative presentations of concepts to pair with the assigned homework exercises. In this capacity, I think it excels. It captures some of The Little Schemer's joy that most books about programming lack. These days, referring to it while reading Haskell Programming from First Principles is a great way to cover all bases.
LYAH is weird in that online, people seem to really like it(in fact this thread contains the most negative reaction to it I've seen), but offline I've never met someone who liked it. Personally I had no success learning haskell from it, and ended up using Real World Haskell instead, which was much more useful.
Sometimes we refer to patterns as "irrefutable" if it means the match will always succeed (like your tuple example). But in Haskell parlance this terminology is sometimes used to refer to lazy patterns..
Agreed. I even bought a paperback after having finished reading it twice online :3
We have one of [these](http://imgur.com/CDcmY8G) OVH EG - 32gb RAM, E5-1620v2 3x SSDs. In my experience, as long as you have "enough" RAM (depending on size of build, but 1-8 gb), the next limiting factor is I/O. If you have an SSD, probably not I/O limited. If you have three SSDs striped, definitely not I/O limited. All that's left is CPU and the server's ability to handle the extra jobs. You can measure how much you're blocking IO with iostat.
https://www.reddit.com/r/haskell/comments/4ccqia/from_25_minutes_to_3_minutes_speeding_up_the/d1hw3ow I'm not sure what you mean by non-commercial, but I could probably get close to these numbers with my quad-core gaming machine that has an SSD for the Linux install.
I sometimes use and hear "case analysis" for matching on sum types, though maybe more so in the theorem proving space.
&gt; it seems like your speed up of the build and test cycle was simple to build less. No, most of it was not using CircleCI with multi-tenant hardware and moving to a dedicated server with an open source CI installed. &gt; I think I was hoping for something more along the lines of ghc flags that could be passed in to dramatically reduce times, similar to another recent reddit submission. That'd be number two on my list of things to speed up CI, after the purescript/frontend stuff. That _is_ in the cards, but I'm not going to make stuff up, I can only share what I actually did and I thought the improvement was dramatic enough to merit explanation. You may not know, but most developers are either using a CI solution like CircleCI or TravisCI which has isolation, but multi-tenant hardware and not-super-reliable caching, as well as being a bit slow. Or they're using Jenkins which _can_ be fast, but has no isolation and can get quite wonky if the build environment gets into a bad state. I shared this because Drone gave us the best of both worlds, modulo some product simplicity. Most developers I know don't have this and many are suffering with 20 minute+ CI build times.
I think you should take a look at [case expressions in the Haskell standard](https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-460003.13). If I understand correctly: What you called "deconstructing" is really matching a pattern and binding values to each corresponding variable. What you called "matching" is simply failing to match a pattern and trying the next alternative, which can be arbitrary patterns. Anyway, I think /u/jlimperg's comment about referring a particular set of patterns (matching each constructor in each alternative) as "case analysis" is quite reasonable.
You do end up writing a lot of type signatures. But I'm starting to think that is better than making up names, which I've come to hate.
This doesn't seems right: https://imgur.com/a/r4swM. Very beginner friendly, helped me a lot to understand the basics. You just lost me past that point.
&gt; My understanding, though it could be completely wrong, is that there is no real stack/nix integration in the sense that stack will not use the pre-built Haskell binaries produced by Hydra. Yes, this is correct.
Don't know how much you know about Docker (I guess I assumed audience would be familiar with this), but the nifty thing about this setup is that Drone mounts your source repo for each build, so the only thing that _ever_ changes is the code you're running. It's really very nice. I say this as someone that has disliked Docker since it came out.
It's a culmination of ideas that previously hadn't been combined. Previously, RWR, Operational, and Union hadn't been used to build an extensible effects library. The significance is that it created a [fast](https://www.reddit.com/r/haskell/comments/496dd9/performance_of_extensibleeffects_freer/?) EE library with Operational semantics. Although there's no new theory here, the existence of such a library could very well be significant.
It'd match the first instance because it's more specific, and there really is a formal definition of specificity: https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/type-class-extensions.html#instance-overlap
Just a few points I'd like to throw out there: First, `mtl`'s state does something rather radically different than a free monad based state. You can see this by running the following computation with `Control.Monad.State` foo = do foo modify (1:) execState foo undefined When you run an mtl State action, it actually starts from the _end_ and pattern matches on the result tuple, which demands more of the prefix of the computation. This is why the form of 'head recursion' above works. So in one sense, we're comparing apples and oranges in any of those benchmarks. These computations do fundamentally different things. With a free(r) monad based state, it'll try to figure out the first 'command', which it will never finish finding. Second, the benchmarks used in the Freer paper are somewhat nonsensical. Who builds up 4 layers of reader monads rather than build up one layer of reader with a more complicated environment? Sure, some people might do that, and call `lift . lift . lift` all over, but it isn't idiomatic behavior. At one layer with a complex environment, the MTL is quite competitive, especially considering that those benchmarks were taken before there was any inlining in `transformers`. Finally, `Freer` doesn't replace the `mtl`. There are a number of effects it can't handle at all. (e.g. `ContT r`)
I have yet to get a reflection without remorse based solution to pay out in practice in any real world application despite having now spent cumulatively months of time on attempts. Admittedly a significant fraction of that time was trying to construct a faster catenable output restricted deque and/or switching it to a Tarjan-Mihaescu deque to turn the amortized bounds of the implementation from the paper into the worst-case bounds they should be. (Using a T-M deque in theory allows large chunks of the deque management to inline, because it can be implemented without recursion, with a scary amount of cases!) The constants involved dwarfed other concerns though. =( I've tried to use the approach for `machines` and for several other projects.
would be interested to hear more about this. Seems related to the call-by-push-value notion of value arrows (construction/matching) and computation arrows (destruction/mutation), where the former are represented purely as structural morphisms and the latter may need arbitrary computation (for ex. maintaining datastructure invariants). Another way to think of it as operational vs denotational matching. But I'd be interested to know under what context each of those 4 apply in your language - if/how you make a distinction between data/codata
I'd suggest talking to some of the various dedicated Haskell [consulting companies](https://wiki.haskell.org/Consultants) or [FPComplete](https://www.fpcomplete.com/page/services).
well, maybe the compiler can figure out the path ? if you specify the end type ? if there is only one path ?
I thought I heard at the beginning that he generated a lot of material and selected which bits he wanted to use, but then at the end he said he was opposed to that kind of thing, and wanted the music to be fully determined by the computer. He didn't say why, but to me the "fully determined by computer" seems more consistent with the goal of having a formal system. But then he also said the advantage of an algorithm is that it makes it easy to create variations for experimentation... and presumably selecting the best ones. Best by what criteria? The reason I bring it up is that given the emphasis that the goal was a formal system, which was supposed to be a desirable over the traditional informal systems which exist all around the world. But once you introduce selection according to your personal vaguely defined criteria of "interesting" then it seems you're back to your traditional informal system, but fed by a different random number generator. My personal suspicion is that, of the algorithmic music I've found interesting, the interest is mostly due to the taste and talent of the selection, so really the composer could have used any old random number generator and all the fuss about the mathematical foundations is just the "metagame", so to speak, and not very relevant from the point of view of the sound that comes out at the end. At least not relevant in scale with the amount of effort put into those foundations. I guess that also ties into the idea that the audience is going to be using the same old traditional informal system to parse the music, which means that if it's different from what they have learned, they'll be engaging only on the most basic level. And of course the composer is the first audience member. Another reason I bring it up is that where I went to school the "formal methods" crowd was the mainstream, but I never really heard an argument for why formality was desirable. Mostly because I never really pressed anyone on that, being in a "keep your head down and stay away from the popular kids" mood, but I kind of regret that since I might have learned something. From a distance it seemed like each composer went off and invented their own system, all incompatible and incommensurable, so no way to judge if any was more useful than any other, or even useful at all, beyond as a means of inspiration for the author. And as far as I know, none of those systems tend to survive outside of a single composer, and many may not even survive past a decade or so within a single composer. Serialism of course being a big exception but... but maybe it didn't survive after all. Mainstream consensus seemed to be that it's now old fashioned fuddy duddy music. So, aside from the question about "fully determined or not", what is the rationale for a formal system? What benefit is it intended to bring? The bit about braids seems possibly promising, so far as the structure created by a braid tends to make contrapuntal rhythms. But this also leads to the same kind of contradiction, because if the reason braids are interesting is that they "accidentally" reproduce some of the rules for interesting counterpoint rhythm, then it's just an inefficient roundabout approach to something that wasn't hard to do by hand in the first place. But then if they're supposed to not be aping the old rules, and in fact intended as a way to break free of them, then why choose braids in the first place? I actually listened to Eugenia Cheng's talk, and noticed that the Bach braid wasn't much of a braid, and she herself said it didn't seem to demonstrate any connection between braids and music, but it was fun to see the connection. I don't mean to ambush anyone with a "but what is it all for" type discussion, but these are thoughts that have gone through my since college, where the "Xenakis school" was strong, and here's someone who might have some insight in there. Especially with the reference to all those composers who are spending lots of time doing algebraic manipulations by hand... really? That surprises me. I say this as the author of a haskell based music composition system which I expect no outside interest :)
That would be really nice. I had to do a fair bit of work to get this stuff working for my job. Edit: nb. this code I wrote isn't reusable, we're using Auth0 and even that was still painful to get going.
Well, if nothing else this might be a good project to do...
Hey, thanks for linking that, it was super interesting.
Free and Freer are different things. In short, this is a Freer monad: data Freer f a = Val a | forall b. E (f b) (b -&gt; Eff r a) Looks just like Operational, no? You can build a Monad instance for this without constraining that `f` be a Functor. [The paper](http://okmij.org/ftp/Haskell/extensible/more.pdf) describes building the Freer monad using [Reflection Without Remorse](http://okmij.org/ftp/Haskell/zseq.pdf) to mitigate large performance issues, and using an open Union to make extensible effects (like Free). Point being, the guys who came up with Freer basically came up with Operational without realizing it, then slapped on Extensible Effects and Reflection Without Remorse to make it a fast, practical effects library.
&gt; I tend to bug people who answer in comments to submit them as real answers. Can confirm. :)
I LOVE the last chapter on Zipper. Compared to the one in haskell wiki, it's a steal. I also thought the chapter on Writer monad was pretty good although I cant remember why.
Spending most of my time down in semiring land, the `(^)` vs `(^^)` distinction is good enough for my daily work (if only the type of `(^)` were correct). The issues behind the other ones really start showing up once you're caring about proper handling of (a) non-negative reals, reals, complex, etc; or (b) polynomials, linear algebra, etc. Though that's not my usual fare, I do bump into it often enough to care about the details. (E.g., one of my current projects is working on a probabilistic programming language; and we make these distinctions, since it's necessary for correctness of program transformations.) To make the details a bit more explicit, consider the question "when can I raise `A` to the power of `B`?" In order to allow particular operators on the `B` type we require the existence of other particular operators on `A`. For example, * if `A` has a monoid[1] for "multiplication", then `B` can have a commutative monoid for "addition" (e.g., `B` can be the natural numbers with addition). * if `A` has reciprocals, then `B` can have negatives (e.g., the integers with addition/subtraction) * if `A` has all (natural) roots, then `B` can have reciprocals (e.g., the rationals or non-negative rationals, depending on whether `A` has reciprocals as well as roots). One of the key examples here is that we can raise real numbers to any rational power. * if `A` has some other funny properties, then `B` can be any real number. The key example here is that we can raise *non-negative reals* to any real power. Notably, however, we cannot raise all the reals to real powers; negative reals to negative non-integral powers are not themselves real numbers! All those notions of powers are different from `exp` and from the exponentiation where the base and the power belong (by definition) to the same sort. * The `exp`/`log` operators give us a homomorphism converting between "addition" in one type and "multiplication" in another type. This is, imo, the essential definition of `exp`/`log`. Of course, in practice we often find that the `exp`/`log` transformation preserves/translates more than just the one operation, but this is extra behavior for particular types. * The exponentiation operator gives us some way of internalizing certain mappings. One of the key examples here is that we can think of the function arrow type as "exponentiation": given two types, we get back another type (we write "`B^A`" to mean "`A -&gt; B`"). This is why we sometimes call function types "exponential types" (and is connected to disjoint unions being "sum types", cartesian products being "product types", and the data types formed only by sums, products, and recursion being "polynomial types"). IMO this internalization aspect is what's essential to "exponentiation" per se (as opposed to the various notions of "powers" above). Notably, we don't require any operations on the things being exponentiated; i.e., we don't require that the set of types has notions of "multiplication", "addition", "reciprocal", "root", etc before we can form exponential types. [1] N.B., this monoid does not need to be commutative! So, for example, this works just fine when `A` is a type of matrices; even though matrix multiplication isn't commutative in general. Or if we want to be pedantic, we don't even *really* need that our notion of "`A`-multiplication" is a monoid. It's sufficient for it to be any power-associative binary operator with identity. ("Power-associativity" is weaker than full associativity; indeed, power-associativity is defined to be the minimum amount of associativity necessary for natural-number powers to be well-defined.) And if we don't care about raising things to the zero power, then we don't need the identity either.
Yeah, look into [pretty](https://hackage.haskell.org/package/pretty) and [wl-pprint-text](https://hackage.haskell.org/package/wl-pprint-text) you haven't already. The problem with using `Show` is that `Show` is just supposed to transform a given data type into a string in the most vanilla way possible. If I can show a data type, I should theoretically be able to reverse that operation by reading the string back in and getting the original data type I started with. This makes injecting markup and formatting really awkward. You should keep `Show` free of surprises for the sake of debugging if nothing else.
Incidentally, I wrote a post explaining how to build this :) http://bitemyapp.com/posts/2015-04-26-installing-a-haskell-game.html
The rational rule of thumb is pretty simple and follows from elementary knowledge of Haskell. First, if you are using only one effect it will always be nonsense to use something like freer; that was never the point of it or anything like it. It is perfectly transparent that `Eff '[State Int] r` will never be as fast as `StateT Int Identity r`, given what `ghc` does with `StateT`. Second, more generally, if you that the construction of `main` will only use effects that can be expressed with `transformers/mtl`, you should always use `transformers/mtl`. But finally, if even one single effect requires expression with a recursive type - a free monad for example - then you might as well use `freer` and get out of the monad transformer business entirely. The game of comparing `mtl/transformers` and `free/freer/operational/extensible effects` will always be a win for `mtl` for the simple, obvious and trivial reason that `mtl/transformers` includes not a single recursive type, if you exclude `ListT`, the effect that must not be named. So for example, for the following simple minded program, which uses one well-optimized recursive monad transformer, and its freer equivalent, we get $ time ./mtl Right (49999995000000,10000000) real 0m2.935s $ time ./freer (49999995000000,Right ((),10000000)) real 0m2.122s ---- {-#LANGUAGE BangPatterns, ScopedTypeVariables, DataKinds, TypeOperators, FlexibleContexts #-} import Control.Monad.Freer import qualified Control.Monad.Freer.Internal as F import qualified Control.Monad.Freer.State as F import qualified Control.Monad.Freer.Exception as F import Control.Monad.State.Strict import Control.Monad import Control.Monad.Except import qualified Pipes.Prelude as P import qualified Pipes main = freer mtl = (print =&lt;&lt;) $ runExceptT $ flip runStateT 0 $ P.sum $ replicateM_ (10^7) pipe_step freer = print $ sum_freer $ runError_ $ flip F.runState (0::Int ) $ replicateM_ (10^7) freer_step where runError_ :: Eff (F.Exc Int ': r) a -&gt; Eff r (Either Int a) runError_ = F.runError yield_freer :: Member ((,) Int) r =&gt; Int -&gt; Eff r () yield_freer a = send (a,()) sum_freer :: F.Eff '[((,)Int)] r -&gt; (Int, r) sum_freer = loop 0 where loop !n (F.Val r) = (n, r) loop n (F.E effs arrs) = case F.decomp effs of Right (a,rest) -&gt; loop (n+a) (F.qApp arrs rest) Left x -&gt; error "we've arranged this is impossible" pipe_step = do m &lt;- get if m &gt; 10^8 then throwError m else do Pipes.yield m put (m+1) freer_step :: (Member (F.State Int) r, Member (F.Exc Int) r, Member ((,)Int) r) =&gt; Eff r () freer_step = do m &lt;- F.get if m &gt; 10^8 then F.throwError m else do yield_freer m F.put (m+1) 
I'll contact the authors of the paper to let them know they need some elementary knowledge of Haskell to avoid embarrassing themselves by including nonsensical data in their figures or concluding that, "The ambition is for Eff to be the only monad in Haskell." Attitude aside, the example you provided is nice, and should be offered to the `freer` package as a benchmark.
Would you mind pointing out what is the state-of-the-art implementation of free/freer monad on Hackage?
If you'd like to learn about how physics engines work, I got most of my info from [the Box2D site](http://box2d.org/downloads/). The most relevant resources are Dirk Gregorius's "Contact Creation" in 2015 (if you scroll through it a bunch, you'll find what I implemented) and Erin Catto's "Modeling and Solving Constraints" in 2009. If you need a PPT viewer for the constraints presentation, Microsoft's web version of PowerPoint is free.
My current approach is to go via LLVM, and there are in-progress backends for (multicore) CPUs and NVIDIA GPUs. It is designed to easily add new backends for other targets supported by LLVM as well.
An example of why you want to stick to deriving the `Show` instance is that property-based tests like `QuickCheck` will `show` any counter-examples, and it's extremely convenient if you can paste that counter-example into the REPL to debug why it failed
Yes please. :P
I wanted to do authentication as simply as possible in a such a way that I could add auth to any of my projects regardless of language... What I ended up with was a small Node-based reverse proxy server using `passport` and Auth0. It worked well. Hmm, let me make the repository public in case anyone here would happen to have some use for it. https://bitbucket.org/lessrest/vaporware-auth You can run it as a Docker container on port 80 by e.g.: make build run \ PROXY_TARGET=http://localhost:3000 \ SESSION_SECRET=tehsecrets \ AUTH0_DOMAIN=example.com \ AUTH0_ID=exampleid \ AUTH0_SECRET=xyz If you've set up Auth0 properly, this will make sure that the requests that hit `localhost:3000` have an `X-User` header set to the JSON representation of the Auth0 user object. It's not super serious production code, and I don't know why the main module is called `lol-auth.js`, but as far as I know it works. If you want to use it, I recommend forking it. And you should probably look over the dependency versions in `package.json` and freeze them with `npm shrinkwrap`.
We already talk to Auth0 directly in our Haskell project, and that works fine, so I'm hesitant to use this but it's a nice working example.
Fantastic that people are working on this.
I somewhat perversely delight in polyglot programming, myself. :) But it would be a neat little project to rewrite the proxy in Haskell.
For building custom `Read` instances - when the automatic `deriving ` mechanism doesn't work for some reason - the `ReadP` parser library, included in `base`, is really helpful. It is similar to `parsec`, but the way you "run" a parser is to apply the function `readP_to_S` which converts your parser into a `ReadS`. For nested types that already do have a `Read` instance, you can create a `ReadP` parser using `readS_to_P`. The `Prelude` functions `readList` and `readParen` are also very helpful. The similarity between `ReadP` and `parsec` is not coincidental. `ReadP` predated `parsec`. In fact, `parsec` was originally conceived as a faster version of `ReadP`.
The cost of converting `f x -&gt; Operational f x` _and back_ to `f x` is the subject of the Reflection without Remorse paper, where Atze shows it costs proportional to the size of the entire structure (the cost of an `fmap`). Converting back _down_ from the Operational you gave here requires lowerO :: Functor f =&gt; Operational f x -&gt; f x lowerO m = runOperational m id (flip fmap) We can see the fact that `lowerO` retracts `liftO`: lowerO (liftO m) = runOperational (Operational $ \ck cf -&gt; cf m ck) id (flip fmap) = (\ck cf -&gt; cf m ck) id (flip fmap) = flip fmap m id = fmap id m = id m = m but note that we had to go through `fmap` on `f` in the middle! RwR is about finding a way to "pay as you go" so that converting to and fro doesn't have to pay full cost over and over. If you only inspect things once then my `Free Monads for Less` approach works fine. If you need to inspect it multiple times (e.g. when using a monad for substitution in a syntax tree), then the RwR approach could in theory blow it away.
I'd probably pay money for this..
I'm not sure if it will help in your case but if you need mutable structure is to use STRef. It is equivalent to pointer and is much faster than vector. Vector are fast when you are using them but really bad if you are building them one element per one element. 
I constructed the example literally at random for your sake, but I have a few hundred others ... I'm defending the authors, who say basically the same about `StateT`; of course they are a little too sanguine about the other cases, what do you expect? (note that it doesn't occur to them to implement the transformers themselves to see if they can optimize them too, again not surprisingly); and I'm expressly stating a crude rule of thumb. The important point is that, *as soon as a 'monad transformer stack' is minimally interesting*, `freer` and the like tend to do just fine, and that doesn't change appreciably as you pile on a bunch of further effects. The extensible-effects haters will always attempt to distract you from this point and return to the trivial transformers in `transformers` (again, I leave the one non-trivial transformer, `ListT`, which was wrongly implemented from the beginning.). Note further the obvious point, fundamental to the rationality of libraries like `pipes` and `conduit`, that if the steps are rendered into IO all these differences typically become trivial anyway. Pure calculations with immense numbers of steps will *tend* to be slower where the steps are separately packaged and basically labelled as steps. All sorts of things can appear in practice, but this law asserts itself unmistakably if you work through a zillion examples. The advantages of things like extensible effects, freer and the like is conceptual clarity and intelligibility, the simple inspection of programs - thus e.g. it is trivial to transform an existing program that uses `StateF` so that it breaks where the state exceeds a certain point, interpolating a `Tweet` effect or the like - and so on. Once you are dealing with one genuinely interesting effect, this huge step in conceptual simplicity and expressive power all becomes a perfectly rational choice. The real trouble with `freer` and the like, as they stand, is type inference, though I find one sort of gets used to writing `getInt`, `putInt`, `yieldInt`. 
Not strictly related to Haskell (or at all), but I thought it's more likely to be appreciated here than on /r/Python.
This. I use fgl daily. Strongly recommend it.
I don't know how exactly this physics engine is implemented, but if its just checks every object for a collision with every other object, you get a lot from parallelization. Of course, the detail math isn't getting faster itself.
Nearly the entirety of the performance analysis in the paper is concerned with effects that you dismiss with, &gt; if you that the construction of main will only use effects that can be expressed with transformers/mtl, you should always use transformers/mtl. And their conclusion falls under your rubric of, "what did you expect?" I know, I know, you'll tell me it is perfectly rational to ignore performance analyses and conclusions in papers! If the authors had provided a useful performance analysis and a useful description of how their work fits into the existing landscape, it would amount to the same squirming you do here that devolves to saying that "interesting" effects are slow, so speed is irrelevant. You choose to layer insults in with your marketing, but it is at least technically honest.
Fgl 
I'm not actually marketing anything, but defending `freer`; somehow I hit on a scheme for expressing my experience in the matter that is evidently not communicating properly and thus should probably be deleted; internet communication isn't my expertise. The point that goes unrecognized is that the authors are putting themselves the maximally difficult task in that section, and they report mixed results. If mtl/transformers contained e.g. a correct ListT, it would be different. As soon as you introduce a sufficiently complex effect, or actual IO, or want to inspect the monadic construction, they win. 
fgl does support this.
They kinda bury it because they are (rightly, IMO) trying to get paid for having made it. Here's the setup documentation: http://readme.drone.io/setup/overview/ I find the stuff for the self-installers by googling "drone.io" "github" and then poking around the Github which has all the links for the docs. I must confess I didn't try their hosted option because I knew I wanted control over the hardware the builds ran on, but I did ping them about sponsoring features we'd like to see happen so they get paid. Github's here: https://github.com/drone/drone
Because the talk was amazing and would probably be of interest to Haskell programmers. I mean, there were a lot of Haskell talks at Compose, but this was still one of my favorites. The noodling about ARP table changes with Irmin and things like writing block device backends that respond slowly or throw away data to test durability, etc, is a very powerful idea. Lots of great examples of modular programming here. (This is one of the talks that's really made me desire Backpack!)
That looks like it'll do the trick, thanks!
Of course! I had trouble finding this at first too
Any update on this? Blog post available?
What would the benefit be of having separate terms for these two halves of pattern matching?
Nobody said anything about deploying to windows, just developing ;D If you have perhaps tried using mysql module on windows you'd know the pains this could potentially alleviate.
And I, in turn, found out about it because of my coauthor Julie :)
The import ceremony : to get bytestring and vector working for cassva you need about 4 lines of import and then as I don't use bytestring and vector anywhere else I have to convert everything to and from bytestring and vector. How does csv-conduitbcompare to cassva-conduit ?
&gt;Annoy advocates of the category theory! &gt; &gt;With Haskell's syntax but none of its type system, dg is the best &gt;way to make fans of static typing shut up already. Whoever made this is both wonderfully awful as well as awfully wonderful. :'D
 distribute . distribute = id does indeed hold.
Neat! Since Hackage doesn't seem to have generated the Haddock docs (what's up with that?), a better link would be the [README on GitHub](https://github.com/taphu/scotty-resource/blob/master/README.md). It has a nice and concise explanation of the *what* and *why* of scotty-resource.
"You get to have ice cream for dinner, but you'll have to eat it from a waffle cone."
Would you prefer someone took over maintaining it, or would you rather leave it as it is right now? It's a very valuable resource for many I think! I certainly would hate to see it slowly get outdated.
1. It provides a syscall translation layer built directly into the Windows kernel, not through DLLs. Which allows... 2. ELF binary support, and in fact full binary compatibility; you can copy and run binaries from an Ubuntu machine. And *that* allows... 3. `apt`.
http://www.mingw.org/wiki/linuxcrossmingw
Brilliant tutorial! I've used it extensively in the past. Thanks a lot. Can it not be put on github and maintained by the community?
Unless I missed something during the talk, the "rumor" in the title here seems to be a rather overblown misinterpretation. These is basically the same general set of slides and talking points that [Simon Marlow](http://cufp.org/2015/fighting-spam-with-haskell-at-facebook.html) and [Aaron Roth](http://www.meetup.com/Boston-Haskell/events/224740717/) have given at other places.
I'm happy you're skeptical :-), imagine how delighted the world is going to be when we finally realize what we've been waiting for so long. This has been in motion for well over 5 years. Hundreds of millions of dollars in server costs are going to be shaved. You will literally be able to use Haskell as easy as golang. Being rewritten in Haskell from the ground up. And open sourcing their GHC fork as well. Anyone else from FB is welcome to chime in.
I made a comment about this on Twitter and a blog post was requested. Hope it helps someone potentially avoid writing a lot of boilerplate when connecting YAML/JSON into an app.
Yup, he explicitly states everything stated before, like "we use correct tool for the job, obviously Haskell's not going to replace PHP" etc. The only mystery was mentioning that there's one more Haskell team out there without mentioning what are they doing.
Dear /u/haskell_oxford, Your post history suggests you are a bit mad. Perhaps you'd like to come back when you're feeling better. Yours, /u/tomejaguar 
FWIW, *servant 0.1* (vastly different from subsequent versions) was halfway between servant as it is now and this *scotty-resource* package. Less fancy types than servant, but exposes "operations on resources" instead of seemingly independent endpoints. Not that I think it is a replacement for your package, but rather another point in the design space. You can get an idea of what it's about [here](https://github.com/alpmestan/servant/blob/master/tutorial.md).
Or, more likely, another troll. I struggle to believe anyone actually thinks or talks like that. Perhaps it's another Microsoft AI Bot gone off the rails.
It's one of the permanent residents of my pocket library. I can't thank you enough for writing it. 
You can avoid `vector` if you use the other modules from cassava and build by hand whatever structure you want.
&gt; Ive always felt documentation is the best way to bootstrap this process and this has been my motivation for writing. Spot on. To me, the `stack` tool and the "What I Wish..." article alone make the difference between daring to suggest Haskell for production systems. Thanks to `stack`, I can confidently pass my projects to a colleague with minimal instructions, and they will be able to build and deploy without hiccups. Thanks to "What I Wish...", I can urge people to learn Haskell, and I can trust them to get up to speed on their own, without having to hold their hands.
Totally agree with you, all hail GNU/Windows!
I didn't know cassava could do this. Thanks!
The point was to reuse also existing code for manipulating the graph.
I'm pretty sure I know at least one member on that team, and have a general idea of what it might be. I'm also pretty sure the aims of that team isn't to rebuild all of facebook, though. =)
Always appreciate getting grilled on the math. I've got no formal math study under my belt so might make some mistakes from time to time. In terms of discrete -&gt; continuous. What I mean is turning a "Strongly agree" into a 1 and a "Strongly disagree" to a -1. Some people would argue that they do not form a continuous scale - but for the purposes of my analysis it made sense. If I did it again I would probably do 0-1 instead of -1-1. Not really sure why that seemed like a good idea at the time. And yes, I did pick the 8 eigenvectors sorted by eigenvalue magnitude. However I ran PCA on a subset of the questions in survey. Regrettably I didn't save any of my work. I'd quickly punched it out between other work I was doing and it didn't even occur to me that I'd want to do more analysis. I've been meaning to write more about the Haskell user experience and about the survey I did last year. But other work commitments have been taking over. I hadn't expected to see my article posted on Reddit but appreciate you giving me your thoughts.
*and* VMs are slow :)
Thanks for your work Stephen. I've been referring many people to it with great success!
Are there plans at Pusher to open source your distributed work with Haskell, like a raft implementation? Great post by the way.
Thank you! We do aim to open source it at some point, but it's currently not production ready, and we need to be able to iterate quickly on it. It's on our todo list though!
I think this works if you treat these as a basis for the space of haskell users. None of them describe me, but a linear combination of them does.. Is that the intention for how these map to individuals? 
I would hazard that the degree of formalism offered by computer-generated composition hinges on the "parametricity" of the program. If a program produces a composition according to discrete parameters that are under the full control of the composer, this is 100% formal. By this measure, a composer evaulating opaque seed values for an RNG by listening to the output is *not* formal; while the seed is a parameter, it's not under the composer's control. _Pleonid_ is pretty formal by this standard. The main inputs are a seed pitch sequence, a gamut, and two tap points for an LFSR. The LFSR params are suspect, but they only govern arrangement, not generation per se. The seed pitch sequence is potentially suspect, as some pitch sequences are going to be more effective than others. Nonetheless, the pitch sequence is recognizably reflected in the intervallic character of the lines and counterpoint, so I would say I had "control" over that parameter. Formalism has no inherent superiority or rationale IMO. It is attractive to a certain kind of composer, which I've seen often. Many composers will use manual algorithmic procedures (not algebraic neccesarily) to produce a population of idea variations, the most obvious being retrograde and inversion, but also e.g. parameterized mutation ((increment|decrement|scale) the 1st, 2nd, 3rd... (duration|pitch|timbre|volume)). Anthony Braxton is a good example of a "manual" algorithmic composer.
I would say `Hashable` too for the same reason, but it comes for free with `Generic`.
Might be. Tor often blocks javascript, and (at least) the latex on that site is rendered by the client.
If there are sensible instances, then I would add `Eq`, `Ord`, `Enum`, and `Bounded` as well. If it is higher-kinded, then it can usually be either `Functor` or `Contravariant` (but almost never both). Also, for higher-kinded types, although it is not common practice, I encourage adding (when possible) `Eq1`, `Ord1`, `Show1`, and `Read1`. These classes are in `Data.Functor.Classes`, which currently resides in `transformers` but is being moved to `base`. Depending on what kind of dependencies you have, you may want to add `Hashable`(so your type can be used as the key for a `HashMap`), `FromJSON`, and `ToJSON`, but it's probably not worth depending on `hashable` and `aeson` just to provide these instances.
The `Val` and `Ref` parts seem a bit badly designed, but I haven't come up with an alternative just yet. Naively, the fact that the shape of `Val`/`Ref` depend on the (monad of) interpretation screams "data family", yet during the construction of programs, you obviously must remain open to all possible interpretations. Hm.
&gt; I'm not sure what UTF has to do with it. Do you know what text encoding is? If not I would suggest reading http://kunststube.net/encoding/ . Text encoding is an issue which effects all programming languages and is worth knowing.
I'm sure it worth knowing but my point is I have a Text which I need to split in a list of record holding Texts. I should not have to worry about text encoding for such a simple task. Haskell is supposed to be a high level language is it not ?
Has anyone been using PureScript + Haskell in a large project? Any first-hand experience? I just read https://github.com/purescript/purescript/wiki/Differences-from-Haskell and was wondering what the mental overhead of writing in two similar, but different, languages, is like? If the server-side and browser-side language were very different, then you would always be aware of the "context switch". However, when you're writing isomorphic code, how does one mentally deal with two different variants of the same language?
Any particular reason you decided started from scratch rather than, say, https://github.com/NicolasT/kontiki?
I derive at least `Show,Read,Eq,Ord,Enum,Bounded,Typeable,Generic,Data,NFData,Functor,Foldable,Traversable,IsString,IsList, Semigroup,Monoid` whenever possible. i.e. Any class that's generally useful and automatically derived. The `Generic` instance in particular further enables deriving many more instances later (like `FromJson`, and of course `NFData`). 1 Simple Types {-# LANGUAGE AutoDeriveTypeable, DeriveDataTypeable, DeriveGeneric, DeriveAnyClass #-} import Control.DeepSeq (NFData) import Data.Hashable (Hashable(..)) import GHC.Generics(Generic) import Data.Data(Data) data X = ... deriving (Show,Read,Eq,Ord,Enum,Bounded,Generic,Data,NFData,Hashable) The `Enum`/`Bounded` is only for strict sum types (i.e. as much of https://www.haskell.org/onlinereport/derived.html as you can). `Num` tends to be too large, which is a shame because of the convenience of numeric literals. `AutoDeriveTypeable` saves an import and a word. And you want `hashable`'s `Hashable` for keys in `unordered-container`'s `HashMap`. Note that `DeriveAnyClass` conflicts with `GeneralizedNewtypeDeriving` (e.g. should `newtype Natural = Natural Integer deriving NFData` use the `Generic` instance or `Integer`'s instance?). 2 Polymorphic Types {-# LANGUAGE AutoDeriveTypeable, DeriveDataTypeable, DeriveGeneric, DeriveAnyClass #-} import Control.DeepSeq (NFData) import GHC.Generics(Generic) import Data.Data(Data) data F a = ... deriving (Show,Read,Eq,Ord,Generic,Data,Functor,Foldable,Traversable,NFData) e.g. `Traversable` in particular simplifies validation -- (where `t` will be `RawText` or `SafeText`) data Doc t = ... deriving (Show,Read,Eq,Ord,Functor,Foldable,Traversable,Generic,Data,NFData) validateDoc :: Doc RawText -&gt; Maybe (Doc SafeText) validateDoc = traverse escapeText escapeText :: RawText -&gt; Maybe SafeText 3 Container Types {-# LANGUAGE AutoDeriveTypeable, DeriveDataTypeable, DeriveGeneric, DeriveAnyClass #-} import Control.DeepSeq (NFData) import GHC.Exts (IsString(..), IsList(..)) import Data.Semigroup (Semigroup(..)) import GHC.Generics(Generic) import Data.Data(Data) data C = ... deriving (Generic,Data,NFData) -- for `-XOverloadedStrings` instance IsString X where fromString = ... -- for `-XOverloadedLists` instance IsList X where type Item X = ... -- e.g. type Item X = X -- when X can store a list of itself fromList = ... toList = ... instance Semigroup X where (&lt;&gt;) = ... instance Monoid X where mappend = (&lt;&gt;) mempty = ... with build-depends: ... , deepseq , hashable , semigroups Besides functionality, there's documentation: an `Enum` (or even just `Show`/`Ord`) instance tells me I can skim the type because it's simple (unless the `Show` isn't lawful; and the lack of `Show`/`Eq` tell me to slow down as the type is more complex (it might hold a function), unless the author was lazy. Conversely, some instances are very useful but rarely boilerplate (e.g. `Monad`), while others are less often useful but easily written (e.g. `TestEquality`, `recursion-schemes`'s `Foldable`). When in doubt, search Edward Kmett's packages for similar datatypes. They always provide every instance that's possibly relevant (like [NonEmpty](https://hackage.haskell.org/package/semigroups-0.18.1/docs/Data-List-NonEmpty.html#t:NonEmpty)'s). links: https://hackage.haskell.org/package/semigroups-0.18.1/docs/Data-Semigroup.html https://hackage.haskell.org/package/deepseq-1.4.1.2/docs/Control-DeepSeq.html https://hackage.haskell.org/package/hashable-1.2.4.0/docs/Data-Hashable.html#t:Hashable https://hackage.haskell.org/package/unordered-containers-0.2.7.0/docs/Data-HashMap-Strict.html https://hackage.haskell.org/package/recursion-schemes-4.1.2/docs/Data-Functor-Foldable.html https://hackage.haskell.org/package/base-4.8.2.0/docs/Data-Type-Equality.html#t:TestEquality 
For yesod are you using stack or cabak ? AFAIK stack can use precompiled packages. I started a yesod project the other day like you : Ubuntu in a VM on windows and it only took few minutes to install (ok my laptop is a monster but still ...) Otherwise the obvious answer is just do stuff, anything you need do an haskekl program for it.
Thank you so, so much for putting effort into this! This tutorial has been invaluable to me.
Yes, I agree, that part is ugly. I think it would work to use the "`ST` monad trick"; i.e. just add an extra parameter to these types (untested): data Val i a where ValRun :: a -&gt; Val 'Run a ValComp :: VarId -&gt; Val 'Comp a data Ref i a where RefRun :: IORef a -&gt; Ref 'Run a RefComp :: VarId -&gt; Ref 'Comp a data Interpretation = Run | Comp This parameter should then also be added to `Prog` to ensure that the `i` is the same throughout the program. Then `runIO` would have the following type: runIO :: EvalExp exp =&gt; (forall i . Prog exp i a) -&gt; IO a The `forall i` ensures that the program is unaware of the interpretation, but internally `runIO` would instantiate `i` to `Run` so that pattern matching on `ValRun`/`RefRun` gives a complete match. I considered doing something like this, but decided the extra parameter would too big a burden for the EDSL user. After all, the parameter would only be there to ensure correctness of the implementation, not of the user's code.
Good to see more stuff like this and servant helping actually solve the http problem.
Ah, nice one. Then, you could allow for more interpreters by leaving `Val` and `Ref` open: data family Val i a data family Ref i a data Run data instance Ref Run a = RefRun (IORef a) data instance Val Run a = ValRun a data Comp data instance Ref Comp a = RefComp VarId data instance Val Comp a = ValComp VarId
How is WIWIK different than the Haskell Wiki? Other than it's been cared for by a single author (which is quite a valuable thing honestly). Sometimes I feel like the Haskell Wiki doesn't get much attention, I know I haven't done anything with it for starters. :(
They represent a lens through which we can view the Haskell community. So they're less about describing individual people and more about describing different ways of thinking. The things you want from Haskell features or documentation seem to differ depending on what type of work you are doing. I guess it makes sense that some linear combination of these represents each person. Especially considering the PCA method that most of the personas were driven from. All the eigenvectors together account for the full spread of attitudes across the survey. It's not likely that many people fall strictly in one eigenvector. Instead these particular groupings account for the biggest spread of attitudes.
&gt; I insist on hard copy books so I can read something before bed. Having a hard copy has a lot of benefits, but mathematics isn't something you can just read before bed if you hope to learn it. The particular issue here is that mathematics requires you to develop new strategies and it requires careful reading. A better approach to learning mathematics is to take the text a few paragraphs at a time, study definitions carefully, and practice what you've learned (explain to others, do exercises, ask questions, etc). One of the best bits of advice that one of my math professors gave me was to write down the statement of definitions and theorems before applying them in exercises. While you're writing it down think about each part and why it's there (and what would happen if you removed it). Soon they will become familiar (from rewriting them) and thinking about what parts are there helps you develop intuition about them. Reading mathematics the way you would read a novel does little more than give you a passing familiarity with the terminology. If that's all you want, then reading before bed can be fine. Just don't expect it to be sufficient to actually drill the material in well enough that you can use it. 
this cleared up so many things for me... thank you
Does it have to be an executable you build? I've done similar things with a small Stack project where I put new "challenge" code into its own subdirectory and then in GHCi, I load the module(s) that I'm playing with and run from the REPL. This works really well if it's simple IO you're doing.
I usually just launch a `stack ghci` session in some `~/exercises` folder, which will bubble up to use `~/.stack/global/stack.yaml`'s resolver. Any additional dependencies can be easily installed into the package database either via `stack build whatever` or `stack ghci --package whatever`. This does the pollute "global local" package database (`~/.stack/global/.stack-work/install/&lt;arch&gt;/&lt;resolver&gt;/&lt;ghc&gt;/pkgdb`), but I blow this thing out every once in a while anyway. Or just `stack runghc`, if REPLs aren't your thing. Another alternative is to use stack's script interpreter support: https://github.com/commercialhaskell/stack/wiki/Script-interpreter
 What's the steps used by GHC to find/load a package ? Although `:m + FooMod` succeeds, calling any function of FooMod will report "ghc.exe:unable to load package `Bar'",etc. Regards!
I believe the OP is running on Windows XP. Are you running a later version of Window? If so, which one? 
Make a stack project. Or cabal, whatever, it doesn't really matter, but stack is nicer. Make a library. This is where your code goes. `src/ChallengeWhatever/*.hs` are your challenge files. For each challenge, make an executable section in the cabal file that depends on the library, and calls `ChallengeWhatever.main :: IO ()`. This way you don't have to worry about reinstalling dependencies etc 
I was doing some more experimenting with this, I have an almost working version of what I'd like except for one issue. What I've done is just create a sandbox in a common parent directory, with no project or anything. Then I can create files as I wish (and I suspect I'll be able to use some common library via the add-source feature). The only problem is that usages of cabal in subdirectories don't use a sandbox from parent directories (!!?), which I can kind of work around on the command line but totally breaks editor integration. Does anyone know if any work is being done on this?
That global IsString instance will mean most type errors involving string literals will become runtime errors. &gt; Hopefully we can have a quick discussion period on the libraries@ list, and get this into base in time for GHC 8, which is currently on release candidate 37. Doubt it. I'd prefer to keep the `Error` class and have an overlappable Typeable =&gt; Error instance instead. Edit: I'm dumb.
Why should we stop there? We could finally enhance `Maybe` to get to know where something failed, too: {-# LANGUAGE PatternSynonyms #-} module Data.ProperMaybe ( Maybe(Just), pattern Nothing, reason ) where import Prelude hiding (Maybe(..)) data Maybe a = Just a | None String instance Functor Maybe where fmap f (Just x) = Just (f x) fmap _ (None e) = None e instance Applicative Maybe where pure k = Just k (Just f) &lt;*&gt; (Just x) = Just (f x) (None e) &lt;*&gt; _ = None e _ &lt;*&gt; (None e) = None e instance Monad Maybe where Just x &gt;&gt;= f = f x (None e) &gt;&gt;= _ = None e fail = None pattern Nothing &lt;- None a instance Show a =&gt; Show (Maybe a) where show (Just x) = "Just (" ++ show x ++ ")" show _ = "Nothing" reason :: Maybe a -&gt; Maybe String reason (None "") = fail "No reason given" reason (None e ) = Just e reason _ = None "There was data" Now we can use `reason` to get to know why something failed: computation :: Double -&gt; Maybe Double computation x = if x &gt;= 0 then Just (sqrt x) else fail "negative number" main = do x &lt;- readLn case computation x of Nothing -&gt; putStrLn "Invalid input" Just x -&gt; print x putStrLn (reason $ computation x) print $ take 7 $ iterate reason (None "") Given this new `Maybe` implementation, I propose we finally get rid of `Either`.
That's brilliant, and obviously the correct thing to do. However, I think we can make a tiny improvement: None e1 &lt;*&gt; None e2 = None (e1 ++ "\n" ++ e2) Sure, it means the `ap /= (&lt;*&gt;)`, but that's a small price to pay. (Also messes up laziness, but seriously, who thinks _that's_ a good idea?)
I don't understand how complicated category theory is. I know what an algebraic data type is. I know I can have a list of something that is an ADT and I can map that list to another list. I know a list if a functor because I can map it. How complicated is category theory? Are we talking like proofs for differential equations kind of difficulty or like algebra II? Is it just hard because it is abstract? Like If I replace all the examples with something from the real world, like the case class of eye colors and mapping a list of people to a list of eye colors, does that make it easy?
Damnit, got me good.
yeah, phantom type's function is like strong parameters in rails...
I think the March 32 was underappreciated.
It's very abstract, so more like the proofs of differential equations than algebra II. I think you would benefit from finding a mentor or a class to meet with. Teaching yourself math in isolation is never easy.
if Yesod is to heavy for you (I would recommend retrying the `yesod-hello-world`**stack**-template too ) you can look at [Scotty](https://hackage.haskell.org/package/scotty) or [Spock](https://hackage.haskell.org/package/Spock) both can get you started really quick
We are but mere humans. *Truth* is something that we cannot grasp entirely, and if you look long enough into the vastness of this implication, the void will look back into you: type Bool = Maybe Void pattern True &lt;- (Just _) pattern False = Nothing
Good idea! I implemented this as a proof of concept here: &lt;https://gist.github.com/emilaxelsson/ddcc352b7422fe763c70#file-completematching-hs&gt; I had to make `valToExp` a class method so that it can behave differently depending on the interpretation.
I was taking it seriously right up until the last paragraph.
I wasn't working at Pusher when these initial decisions were made. I know that the main issue we had with it was that it does not support cluster membership changes, which is something we need. What I don't know is why we didn't try and add this feature to kontiki.
I think it does it automatically i it can but for that you need to use standard version of packages.what it's in your stack.yaml file ? How did you create it ?
Thanks. It might be a good idea anyway, It might take less time to fetch packages and compiles rather than downloading them already built. ## Udpate I meant " It might NOT be a good idea".
There was some talk about MS giving server time/space to charities (which I think haskell.org now is) and that perhaps this could be used as a build farm. I don't think that idea went anywhere though.
Jokes aside, error handling is actually a mess. Even reputable packages do weird things (just look at the `Show` instance for `ParseError` in `parsec`, for example) all the time: misusing typeclasses, use `String` for errors (or *exceptions*, rather) etc. However, I would still say that Haskell is no worse than your average language; error handling is a mess in almost every language out there. I'm kinda hoping Haskell could be better, though!
Well, installing pre-compiled packages is far from trivial. It's pure luck that Stack can get away with installing a "generic" GHC bindist without making sure the environemnt is really ABI compatible with the GHC bindist chosen (and you notice this if you try running Stack on distributions which are a bit more exotic/old). If Stack was ever to provide pre-compiled packages on Linux, it should do this via Docker images which provide a well defined environment including pinned down C libraries.
It took me most of the article before I remembered what day it was. Well done, Michael.
doesnt work $ bash &lt;(curl -sL https://git.io/haskell-vim-now) The system cannot find the file specified.
&gt; have a look at the date in the original article "March 32"... Well, it's in March, so it must be serious.
&gt; But know I have to worry about encoding You should have worried from the beginning, reading the data with `getContents` uses the default locale implicitly, which is bad. &gt; even though my input is not a file but a Text which I believe don't have encoding problems. What do you mean by your input being a Text? How does the program convert your input to the data type Text? As for the solution, the following should work for most cases: import Data.Text (Text) import Data.Text.Encoding (decodeUtf8) import qualified Data.ByteString as B getContents' :: IO Text getContents' = decodeUtf8 &lt;$&gt; B.getContents 
Nice write-up. Typo: &gt; Lens abstactions
You have to build packages on your local system but you only have to build them once, no matter how many projects you have. So if you have two projects that both use the same packages, the second one uses the compiled libraries that the first one compiled. This assumes both projects use the same (or compatible) resolvers.
Yes, but you would only want to move `Val` in that case. We want to be able to add Types like `Ref` freely. For example [imperative-edsl](http://hackage.haskell.org/package/imperative-edsl) also has arrays, pointers, etc.
Wow. Didn't even notice that until now.
I'm just trying to visualize someone going into battle weilding a cucumber now... I can't help, but thanks for the lolz :-)
I use a Makefile &amp; (occasionally) a shell function. http://sleepanarchy.com/p/t4C5zS # Makefile server: build stack exec om-servant-exe watch: build stack build --pedantic --file-watch clean: stack clean maintainer-clean: stack clean --full build: install ./src/** ./app/** om-servant.cabal Setup.hs stack build --pedantic install: om-servant.cabal stack install --only-dependencies http://sleepanarchy.com/p/3mgYvH # ~/.zshrc buildRunWatchKill () { PROGRAM="${1}" WATCH_DIR="${2}" while true; do PID="" make build if [ $? -eq 0 ] ; then echo "OK" if [ "$PROGRAM" ] ; then $PROGRAM &amp; PID="$!" sleep 0.1 fi else echo "FAIL" fi inotifywait -q -q -r -e 'close_write' $WATCH_DIR echo "CHANGED" if [ "$PID" ] ; then kill $PID fi echo "AGAIN" done } 
I wonder how that works. In [the commit for that post](https://github.com/yesodweb/yesodweb.com-content/commit/289e7fff2388bfdc3cc1ad40e6ae2c031e9dbe67) you can see that the timestamp is 2016-04-01T00:00:00Z, and there's no obvious override. If there's some special code to render that timestamp as "March 32," it's been in the repo for a while--planned in advance.
Yes. Purescript can defenitely run on the server through nodejs. You even get the entire node ecosystem for free! Of course, you'll be running an interpreted language instead of nice optimized machine code. What's more interesting to me is the new `StrictHaskell` mode in GHC 8. Which will pretty much bring Purescript semantics to Haskell. The only thing that's missing is record types. But I think that's in the making in GHC as well right?
It feels nice... until it takes more or less forever just to reload the repl for your project, let alone 'compile' it. Also missing lots of advanced language features.
At this point I'm more likely to define an Iso for this because I'm really really having fun with lenses. 
What are "record types"?
It might be easier to make a compiler for Purescript than a GHCJS ?
I think the future of Haskell is dependent types.
ghcid is the only bit of tooling that ever actually works for me, I love it. I put so much effort into trying to get something working properly with vim/neovim, but eventually settled on just using tmux and ghcid. ghcid inside a neovim terminal split might be sweet if I can figure out how to pin a split at a certain size. They keep resizing when something like nerdtree triggers a reflow.
When I mean the future of Haskell, I mean the Haskell used in industry, not in University. If you prefer, I believe that in the next century (or the next one), the main programming will be FP. However, I doubt it will be Haskell, even with dependent type. There is too much "frictions" in the actual Haskell to make it really mainstream.
what's a resolver?
I was being a bit facetious as I have no idea what the "future holds" as you're using the idiom. I was taking your question literally.
The thing that bugged me the most about Purescript was the lack of an implicit forall with Rank-1 types, which is really just a sugar issue. Row Polymorphism is cool though. I can't imagine why I'd use purescript outside the browser though. If the constraint of "must run in the client's browser" isn't in place the I'd just use normal Haskell.
GHCJS already exists.
As someone who continues to struggle with Haskell, I am realizing that if in fact, after almost 20 years, the experts still can't get the Monad instance for Either right yet, then what chance do I have!!!
In Purescript, record syntax is implemented by having a constructor that holds a single anonymous record, rather than as a normal constructor with derived accessor functions and some syntactic sugar. An anonymous record reflects the fields it contains in its type, so using record accessors in Purescript is always a total operation. They also chose to use the typical `.` for accessing fields.
&gt;I mean the Haskell used in industry Haskell **is** used in Industry right now. &gt;I believe that in the next century Next century is 85 years away. I can assure you that in 85 years machines will learn to make better and more complex programs than we humans could ever do. So there will be no programming languages (for humans) next century. 
With costs of data centers and processing on their current trajectory, the whole concept of running inefficient serverside JavaScript stacks is unsustainable compared to a future of optimized, compiled, smaller stacks (e.g. pure Haskell). This is not a subtle point. Data center energy is on track to become the most significant energy use on the planet, bigger than cities, transportation, and agriculture. Of course, things are going to have to change or crash before that actually happens. The current trajectory is totally infeasible.
Bisectable?
&gt; Historically Cabal had a component known as cabal-install that has largely been replaced by Stack. The following use of Cabal sandboxes is left for historical reasons and can often be replaced by modern tools. This must be an April fools' joke...? Stack being a popular alternative to cabal, yes... but cabal being deprecated by stack, I don't think so.
I thought it meant "polynomial roots", as in `a &lt;&gt; b &lt;&gt; b &lt;&gt; c &lt;&gt; ... = mempty` for some `a`, `b`, `c`...
Yeah, coming up with a title for the post was a problem in and of itself.
They will make new hardware architectures with pure Javascript semantics, and that will alleviate somewhat the problem. You would be surprised about the innate capacity of humans for sticking NodeJS in places they shouldn't. 
A group is [divisible](https://en.wikipedia.org/wiki/Divisible_group) when for all g in G and n &gt;= 1 there exists h such that h^n = g. All of your examples satisfy this condition, except that `All` and `Any` are not groups. I've googled for "divisible monoid" but that does not seem to be commonly used.
&gt; The thing that bugged me the most about Purescript was the lack of an implicit forall with Rank-1 types, which is really just a sugar issue I think this is really high praise ;) Well, you might use Haskell anywhere you could instead of PureScript -now-, but what about in ten years? Only time would tell :)
I too am curious about ZuriHac.
It will happen, but the date isn't decided on yet.
Assuming you actually care about all (natural-number) roots instead of just square roots, the best least-ambiguous, still-correct, pronounceable term I've found for the minimal property of having all roots is "surd-complete". ("Surd" being another name for "root" in the specific sense used here.) If the underlying space is also a group, then surd-complete abelian groups are called "divisible groups" which is a terrible name since it has nothing to do with division. But in general, there aren't popular names for surd-complete things. People typically overshoot and aim for something with nicer completeness properties, like the reals or the algebraic numbers. In Hakaru, we call the typeclass for surd-complete semirings "`Radical`"; maybe not the best name, but it's short and sweet like the venerable "`Fractional`" (in lieu of the more technical: division semiring). Of course, it's worth mentioning that the `Radical` types are not merely the surd-completion of the carrier of some semiring; we also require it to remain closed under the semiring ops, which does not come for free. Moreover, it's important to realize that `Radical` numbers are *not* the same as algebraic numbers! A lot of folks bring up the algebraic numbers when talking about this stuff, but there are algebraic numbers which cannot be formed via finitely many uses of addition, subtraction, multiplication, division, and natural-surds (cf., AbelRuffini theorem).
This is the right answer, I think. PureScript can be run on the server easily, and there are some cases where it fits well there. The ubiquity of JS also makes it simple to deploy PureScript code in other environments - on the Espruino, under AWS Lambda, or as a native app with React Native. So it has its possible success areas, but we have no interest in (and no chance of!) trying to displace Haskell for general-purpose programming. There's a reason all of our infrastructure runs on Haskell :)
I am no expert, but see the [ticket](https://ghc.haskell.org/trac/ghc/ticket/4012) and the other related ones. NIX guys are pushing for deterministic builds as well as distribution guys (e.g., Debian).
No. Purescript is eternally bound by the idea of having to be compiled down to javascript. That gives it weird baggage (like really only having a single number type under the hood, for example) that I'd rather just avoid when possible.
In the next century I would hope we've moved beyond the need to create languages solely for dealing with the horrors of other languages that happened to become mainstream thanks to the whims of history.
It's future for haskell. As practical language purescript is very comfortable. I think it's need compiler to native code. 
This is a good path for application code, but library authors should derive everything that makes sense.
-1 I'd rather explicit over implicit.
Not that I use any dependently-typed languages, but to me, the basic value proposition of dependent types is removing insidious assumptions from programs. They do this with the onerous restriction that you cant assume anything unless you have a proof of it; so as you imply, its not particularly usable for Sam Programmer. Id really like to see more alternate solutions to the same problem.
If you define the law like this instead: root (m &lt;&gt; m) = m Then you can define `root` on lists as taking half the list. Not sure if that's wise or useful, but there you go.
...and tomorrow this gets picked up by TechCrunch, who pay no attention to the datestamp...
Yeah, though personally speaking I will never write anything popular enough that anyone will care. Or if they do, they can send an email. Can't speak for anyone else of course :)
&gt;The key insight behind SBR is that, when a runtime error happens, the runtime already knows why the problem occurred (null pointer, missing functionality, index out of bounds, etc.). In most cases the runtime, instead of throwing an exception, can just do something else than what the program seemed to demand. So, in other words... Javascript? Or are you talking about PHP?
I think you a word.
[Helm](http://helm-engine.org/) is always a good choice and quite simple as well.
Hey. I am sure I have never seen you in the issue tracker / slack room. Was there anything detracting you from reporting any of the problems you were having?
If I didn't know Bartosz was the author, I may not have spotted the April fool! 
Other than the discussion we had on reddit the other night when it was happening ( which I think turned out to be bad timing with regards to some renaming or something) once that was sorted I've not had any further issues, but it did sour that early experience ever so slightly. TBH, didn't know there was a slack channel and since the problem went away I didn't feel the need to report it further than reddit. "heaps of problems" may have been an exaggeration, but it felt like it at the time.
To do stable sort you need two `reverse`s:  (reverse . sortBy (comparing fst) . reverse) [(1,'a'),(2,'b'),(2,'c')] [(2,'b'),(2,'c'),(1,'a')]
May I add the fact that `Ordering` and `(-&gt;t)` are monoid, hence you can do really nice stuff such as : &gt; data Hero = Hero {name :: String, strength :: Int} deriving Show &gt; let l = [Hero "Hulk" 100, Hero "Chuck Norris" 80, Hero "Batman" 80, Hero "Me" 78] &gt; let ascending = comparing &gt; let descending = flip . comparing -- or \f -&gt; comparing (Down . f) &gt; sortBy (descending strength &lt;&gt; ascending name) l [Hero {name = "Hulk", strength = 100},Hero {name = "Batman", strength = 80},Hero {name = "Chuck Norris", strength = 80},Hero {name = "Me", strength = 78}] (edit: formating. You need `Data.Ord`, `Data.Monoid` and `Data.List`)
It is interesting that [redis protocol parser](https://github.com/Yuras/scanner/blob/master/examples/Redis/Scanner.hs) is almost identical to [one written with attoparsec](https://github.com/Yuras/scanner/blob/master/examples/Redis/Atto.hs), but ~2x faster. I also made a non-backtracking json parser, it outperforms aeson (2x faster too).
It's nice to have that as a package, as I tend to write a fast minimal parser in several of my projects ...
Very cool! Any chance we could get Aeson to use your version?
`bash &lt;(curl -sL https://git.io/haskell-vim-now)` works for me, what is the error?
Something incorporating Aeson would be amazing, since it's the de facto standard for JSON. Even package on top of it, or a build flag.
Also was horribly broken on my iPad Air 2, Safari, iOS 9.3
The last few comments in https://github.com/clash-lang/clash-compiler/issues/139 might give a hint how this could happen. But give CLaSH a fair chance. It is awesome. Start with small designs and be sure to understand all the examples in the github project.
Stack uses cabal. I think he is saying that, for many, stack is a better tool than 'cabal-install'. Cabal is the foundation for both.
The interesting thing I forgot to mention is that you can actually have the best of the two worlds by injecting attoparsec into a scanner: https://github.com/Yuras/scanner-attoparsec
How is that report produced? Is it automated? It looks nice. And what exactly does that graph display?
It'd be a language extension. If it isn't in a language pragma at the top of the file, nothing would change.
&gt;The system cannot find the file specified. i pasted it: I'm on windows though maybe that has something to do with it? 
Are you sure that `coerce` is needed here ? I was thinking that newtype always imply the no-op conversion.
Can you inject a scanner into attoparsec?
Yay! Bartosz' [excellent post](http://bartoszmilewski.com/2016/04/01/the-new-jdi-language/) is currently much farther up in the reddit than this one, so I didn't see it at first. I was becoming worried that we would miss out on the /u/snoyberg "seasonal post" this year. But, here it is! Great, as always! EDIT: Ah there is also this [Galois post](https://galois.com/blog/2016/04/galois-announces-isc-the-imperfect-stitch-compiler/). A prolific year!
Yes, it is possible too. Though I'm not sure where it could be useful. If most of the parser is backtracking, then injecting non-backtracking one somewhere in the middle will have marginal effect on performance.
It's just a matter of having the ability to nest a non-backtracking grammar in a backtracking one. I'm sure there'd be some performance gain. But more importantly, this leads me to believe it might be possible to build a parser combinator library that can statically analyze whether it's building a non-backtracking parser, and adapt accordingly. This might require some pretty heavy Applicative-based optimization (a la Haxl), which means it'd only be a tangible benefit if the user built parsers applicatively wherever possible, or used ApplicativeDo (which I believe is in GHC 8.0, but not 7.x)
Huh. I thought that mapping newtypes was a no-op _since_ `coerce` was added, but it seems not.
&gt; /u/yits thanks for contributing your solution. That gave me a new way I could tackle the problem. It reminds a bit of how I would use the actor model to solve the problem, which is probably because the nature of the problem just fits really well with actors because somethings just need to be synchronous. Because this solution linearizes the commands, right ? So you dont have to deal with inconsistencies coming from concurrent commands being executed
You're welcome!
Great job, Yuras! 1. Would be useful to have a benchmark comparison with [the Zepto parser](http://hackage.haskell.org/package/attoparsec-0.13.0.1/docs/Data-Attoparsec-Zepto.html) just for reference. It is not incremental, but the information on the performance difference would be useful for choosing between the two in certain scenarios. It should be an order of magnitude faster than the standard Attoparsec parser though. 2. Have you experimented with extending it with the backtracking functionality? If you did, what was the performance difference?
Rust?
STM would certainly make transactions easier :)
I'm pretty sure that those problems can be overcome. If your database is immutable anyway, you're probably going to want to keep stuff around.
&gt; Ignore "lack of programmers", "bad documentation" or things like that. It's a 100% technical question, assuming an hypothetical scenario where you have a big team that know how to work with that language. You're basically saying, "If the concerns that are important for language choice can be overlooked, which language would you choose?" The answer to that question should probably be "the one with the best performance"... so something like Ada, Rust, C, assembly or whatever. The reason we have all these other languages (including Haskell!) is because programmers are fallible and we're ready to sacrifice some performance for increases in bug resistance and friendliness to the way people think, rather than machines. If you're assuming superstar programmers that don't have problems with bugs then there's no reason to choose those languages either! ...unless you're just using a lot of words to say "don't care about popularity", in which case my answer is still Ada because performance sounds like an important part of your system, and performance is easier to get out of a low-level system. 
Yeah, "don't care about popularity" is what I mean XD. I forgot to mention that I only consider 100% free/open software programming languages :). I like Ada a lot, but you need some private things to get it work perfectly.
In a database with immutable data it seems like that data should not even be under the control of the garbage collection. Perhaps some IO based datastructure that manually uses malloc/Ptr or something? That way the GC doesn't spend a lot of time copying the values around hoping that some my might collectable?
GNAT Pro.
OK, just found you. I am really sorry your first experience with Fisherman didn't just work out of the box. If you could join the slack room, I'd be happy to help you troubleshoot everything from start to finish. You can always come back to chips when it's ready :)
I'd definitely go with Haskell, but I may be biased. It's not that difficult to write a DB from scratch that can serialize a million or more updates per second on one machine (verifying it is probably 99% of the work). Since you're only doing inserts and selects, you don't have to worry about transaction like interactions and can just split your data across multiple essentially single server DBs each with ungodly amounts of RAM.
Although all the other alternatives are also commercial, GNAT Pro isn't the only Ada compiler in town.
Even if there was, there wouldn't be a `map coerce` rule.
Can't, no `leftpad()` :(
The following pattern from the example: reply :: Scanner Reply reply = do c &lt;- Scanner.anyChar8 case c of '+' -&gt; string '-' -&gt; error ':' -&gt; integer '$' -&gt; bulk '*' -&gt; multi _ -&gt; fail "Unknown reply type" can be easily made composable with the Alternative interface using the following abstraction: newtype AltScanner a = AltScanner (Char -&gt; Maybe (Scanner a)) falseLiteral :: AltScanner () falseLiteral = AltScanner $ \case 'f' -&gt; Just (string "alse") _ -&gt; Nothing trueLiteral :: AltScanner () trueLiteral = AltScanner $ \case 't' -&gt; Just (string "rue") _ -&gt; Nothing boolLiteral :: AltScanner Bool boolLiteral = falseLiteral $&gt; False &lt;|&gt; trueLiteral $&gt; True Essentially this is just a specialization of [an abstraction over the partial functions](http://hackage.haskell.org/package/monadplus-1.4.2/docs/Control-Monad-Plus.html#t:Partial). I don't know, whether the compiler will act smart enough to turn the composed AltScanners into a single pattern-match expression though, or even whether there'll be any difference if not. Anybody care to comment on that?
`\x -&gt; undefined x` is a function that maps any value to `_|_`. `_|_` is in fact an element of `Char`.
&gt; IO satisfies the monad laws only up to strictness if you state the laws in their simple point-free form. Do you have an example of this?
Your question may be hypothetical, but if you're actually considering writing something like this, I would really urge you to consider why you want to write your own. You mentioned PostgreSQL; why is this not a solution for what you're after? Performance? One thing I learned from working at a large software company with a lot of in-house solutions was that rolling your own isn't always the best, in fact, it's usually the worst option. There is a lot to be said for maintainability and usability. Consider that this system you're dreaming of may be in production for many years (I assume, from your list of requirements, like "millions of users"), this means that several years from now, well after this software has been written and is stable and hasn't been updated in a long time, anyone who wants to go back and change something to it will have a learning curve just to learn how to use it, first off, and an even bigger learning curve for how to modify it. tl;dr: If there is a really good, FOSS solution available that meets your requirements, like PostgreSQL, use that instead.
then you give up something we care even more about in Haskell - - parametric polymorphism
I don't overlook language design. Language design is just a means to an end, not an end in and of itself. The things I mentioned in my comment are the reasons we care about language design in the first place!
JSON files are really slow for searching and such.
If we're going to reason in a system like that I don't see any point in discussing anything. My point above is that unless it affects the reasonable subset of Haskell, then I don't see any point in worrying about GHC's behavior. Now if GHC messes up a reasonable construction, that's something I care about and am interested in.
No. /u/bos actually made attoparsec non-backtracking for one release in 2011, and it was indeed much faster. Then he immediately reverted that change in the next release due to the mess it made with parser semantics. There is a [blog post](http://www.serpentine.com/blog/2011/06/03/attoparsec-0-9-a-major-release/) that describes what happened. Here are some other serious issues that arose in attoparsec, and were fixed, due to very deep subtleties in backtracking vs. no-backtracking: [#10](https://github.com/bos/attoparsec/issues/10), [#21](https://github.com/bos/attoparsec/issues/21) which seems to have been the cause of #10, [PR#40](https://github.com/bos/attoparsec/pull/40) to fix #21, [#42](https://github.com/bos/attoparsec/issues/42) which was a regression caused by #40. Yes, no-backtracking is faster, but in practice this is a very, very non-trivial topic.
Rust for memory, disc access and data structures. Haskell for database connection, query parsing and optimization. Rust is great at handling raw memory in a relatively safe and high level way. Haskell is extremely good at transforming tree like datastructures.
Have you checked OCaml? It's designed to be well performing, it's functional, it's mature. Eg. [Irmin](https://github.com/mirage/irmin) is a database written in OCaml.
https://mail.haskell.org/pipermail/haskell-cafe/2007-January/021536.html EDIT: Synopsis: Prelude&gt; let f = undefined :: Int -&gt; IO Int Prelude&gt; f `seq` 42 *** Exception: Prelude.undefined Prelude&gt; ((&gt;&gt;= f) . return) `seq` 42 42 But one of the monad laws states that if `m` is a monad, then (&gt;&gt;= f) . return = f for all `f :: a -&gt; m b` for all types `a` and `b`.
The whole purpose of applying `seq` to functions was so that you could ensure that a lazy thunk representing a function has already been turned into an actual callable internal function object at some point in the calculation, for performance and/or memory reasons.
Also, `absurd` from the `void ` package is a total function that is eta equivalent to `undefined`, but can be distinguished from it with `seq`.
You can place limits on how much RAM it will use (such that it will try to stay within that limit and crash if it can't). How much memory you need is entirely dependent on how you use Haskell (you got process literally infinite data structures or have simple loops use infinite memory).
That's something that bothers me about Haskell. It seems that it should be a good choice for a NoSQL database (safety, less prone to certain bugs, compiled to native binary) but of all the numerous NoSQL DBs none are written in Haskell as far as I know. Why is that?
No parallelism though.
Obviously. I was mostly wondering if the store would serve static documents specified at startup.
Great talk! The Zurich crowd seems like a group of super knowledgeable Haskellers.
`absurd = \x -&gt; absurd x = \x -&gt; undefined = \x -&gt; undefined x = undefined`. The crucial equality, `absurd x = undefined`, holds because the only possible `x :: Void` is `undefined`, and `absurd undefined = undefined`.
Honestly, I'd use Haskell. Of your characteristics, the one that I think would be the source of most of the work is massive concurrency (big data as well, but that just turns into massive concurrency), and Haskell has Cloud Haskell as a great starting point. Once you're at a point where you can easily distribute work and data across a set of machines everything else becomes straight-forward enough. I have to say, I find it strange to look through the responses here and not see a lot of Haskell proponents. Databases are easy to make cache-friendly and Haskell has good support for doing low level memory operations. I would be very surprised if Rust, for example, could out perform Haskell by enough of a margin to overcome the overhead of all the network activity.
3.1.1 Example Queues &gt; A common representation for purely functional queues [Gri81, HM81, Bur82] is as a pair of lists, F and R, where F the front elements of the queue in the correct order and R the rear elements of the queue in reverse order. For example, a queue containing the integers 1...6 be represented by the lists F = [1,2,3] and R = [6,5,4]. 
Per above --- .... "totally embarrassed now"
Definitely --- I completely fell for it!
Those are spine-strict lists.
That's just tagging though. At the JS level, JS has just one number type. And not really any non-strict evaluation. And, yes, Purescript doesn't *have* to be compiled to JS... But if you're not compiling to JS and running that in the browser, why are you writing Purescript? Why wouldn't you just use Haskell itself? At least thats my general opinion.
I have seen elsewhere the same thing happening: the author does the dumbest, simplest thing that satisfies their requirements first. When their application needs something better, they do something better.
I added Zepto: https://github.com/Yuras/scanner/commit/892ae04eace8f518003b2eb6208a26efbe2c72ab Indeed it works a faster then attoparsec, yet scanner outperforms it. The best results improvement is in `multi` benchmark: benchmarking redis/multi/Atto time 387.4 ns (386.4 ns .. 388.6 ns) 1.000 R (1.000 R .. 1.000 R) mean 387.0 ns (385.9 ns .. 387.9 ns) std dev 3.280 ns (2.516 ns .. 4.316 ns) benchmarking redis/multi/Zepto time 272.6 ns (271.7 ns .. 273.5 ns) 1.000 R (1.000 R .. 1.000 R) mean 273.3 ns (272.4 ns .. 274.0 ns) std dev 2.584 ns (2.084 ns .. 3.171 ns) benchmarking redis/multi/Scanner time 194.8 ns (193.9 ns .. 195.7 ns) 1.000 R (1.000 R .. 1.000 R) mean 195.0 ns (194.5 ns .. 195.6 ns) std dev 1.822 ns (1.329 ns .. 2.613 ns) Though I'm not familiar with Zepto, so probably I'm just using it wrong.
&gt; That's just tagging though. At the JS level, JS has just one number type That doesn't matter for a language. &gt; Why wouldn't you just use Haskell itself? Purescript has: - a more sensible typeclass hierarchy - a safer prelude - a (much) better record system - qualified imports by default with one open import allowed (coming soon) - is strict - has a good reference (Haskell) on what works well and what doesn't - makes it possible to write code for both server and client side with tge same labguage Also purescript is fast moving and easy to contribute to which makes it possible for someone like me to have a say in the direction it is going to take. Again, purescript is definitely not as mature as Haskell *at the moment*. But I believe the language and ecosystem will develop into a pretty good plaform in the future.
That's grand!
If you want more language data for GF, there's some integration with Apertium at https://github.com/vinit-ivar/apertium-gf (author hangs out at http://wiki.apertium.org/wiki/IRC ).
I was a lot newer to Haskell when I last looked at that, but back then I was under the impression that there were reasons related to STM semantics for why you couldn't use Okasaki's tricks. With that said, I'd bet heavily on that being down to me not knowing what I was doing at the time :)
Code from the book: https://github.com/simonmar/parconc-examples HaskellCast episode: http://www.haskellcast.com/episode/004-simon-marlow-on-parallelism-and-concurrency /u/simonmar [mentioned](https://www.reddit.com/r/haskell/comments/1iwr7x/parallel_and_concurrent_programming_in_haskell/cb96itw) that a better version should be available now, but I can't find it...
I suspect this would be a good use case for "generics-sop".
What is generics-sop (or at least sop ?)
Well, to be fair, Edward Kmett didn't come up with the `lzs` example. I was first told about it by Albert Y.C. Lai, and wrote about that and applying the lazy state monad to Jones and Gibbons' breadth-first renumbering algorithm in [Fun with the Lazy State Monad](http://blog.melding-monads.com/2009/12/30/fun-with-the-lazy-state-monad/). I also wrote about the issues with the lazy state transformer slightly eariler, with [Are Continuations really the Mother of all Monads?](http://blog.melding-monads.com/2009/12/20/are-continuations-really-the-mother-of-all-monads/).
Well, I was quite happy testing my miniKanren implementation with QuickCheck. Probably the next level up in size from "single function": [example tests](https://github.com/jozefg/ds-kanren/blob/master/test/Unify.hs). The basic idea is that each construct in the language has a pretty good set of laws so quickcheck works well.
Not necessarily. You could just say function spaces are unlifted. Make `_|_` == `\_ -&gt; _|_`. I think it could work if: seq func = seq . func (_|_ :: a-&gt;b) `seq` x = x (_|_ :: a-&gt;b) x = _|_ The main downside is if you return a polymorphic value whose evaluation is intentionally tied to the evaluation of something else. Then the user can try to `seq` it to force the evaluation of that other thing. This would do nothing if the polymorphic value happens to be instantiated to a function value. Of course it would all be much nicer if we had a Seq type-class and have functions and products not be instances.
That's not quite how you'd want to use type families. You wouldn't pass then in, you would use them on the passed in data type. EG: type family AmountTF (s :: Stage) where AmountTF Final = Amount AmountTF Validate = Either AmountInvalid Amount ... For each of some set of stage indicators for each internal data type which can operate differently. Then you'd have: data RecipeF (s :: Stage) = ... , rawTotalAmount :: AmountTF s ... Then you can control exactly what field should be what type at each stage.
what is EOT ? ;-)
That's brilliant ! I can probably this technique to remove Identity doing type family EraseIdentity (a ??) where EraseIdentity Identity a = a EraseIdentity f a = f a and data Receipt f = { , rawTotalAmount :: EraseIdentity f Amount } and I might even be able to mix both. 
Either of tuples. That is, if you have some data: data Foo = Bar Int Char | Baz String ... then your either of tuples representation would be: Rep Foo = Either (Int, (Char, ())) (Either (String, ()) Void) So if you build operations to work on `Either`s and tuples, then you can have them apply to any data type that you derive `Generic` for.
Almost, you don't have two parameters for your family there so it would be `EraseIdentity (Identity a) = a`. And you can certainly mix both, but it might be overkill. Try a bit of each and see which flows best for you.
&gt; As the final exercise, the reader is encouraged to similarly represent the storyline of ``Primer''. One would first have to *understand* the storyline of Primer.
It's not very fair regarding Clojure. STM work well there. Mostly because even if the language does not enforce purity most of the functions are pure, thanks to purely immutable datastructures.
OK, how do instanciate things like `Show` with this approach ?
Hmm... for some reason I thought the [improved exhaustiveness checker](http://research.microsoft.com/en-us/um/people/simonpj/papers/pattern-matching/gadtpm.pdf) was landing landing in GHC 8.0, but I don't see it in the [release highlights](https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-8.0.1#Releasehighlights).
Essentially 100% of the time I'm writing Rank-1 types. In such a case, I know what I mean by (id :: a -&gt; a), and GHC know what I mean by it too. Having to write (id :: forall a. a -&gt; a) is exactly as stupid to me as if I had to write it as (id ::::::::::: a -&gt; a). Since writing Rank-1 typed things is the common case, it's what I want to be able to set the compiler as having be the common case. It won't kill me if I can't, but it will annoy me every single time.
Sure, rank-1 is the common case, but now suppose you want to refactor, and pull things out into a `let` or `where` clause. You want to give things types, and reuse the bound type variables, but they're not explicitly bound anywhere. This gets confusing quickly, so we have the equivalent of `ScopedTypeVariables` turned on always. Now it's simple to see where those types are bound and if they're shadowed. For me, writing `forall` once is a small price to pay to be able to see that information at a glance.
This is really nice! Thanks for sharing.
And do think it would better that the ReceiptP solution ?
[This article](http://www.timeout.com/london/film/confusing-sci-fi-movies#tab_panel_5) pretty much sums up the craziness, although I did like [this breakdown](http://i.imgur.com/VACTBSE.jpg).
&gt; Thanks to Haskell's laziness in general and the careful implementation of sort in particular, sort can run in linear time when only a fixed number of first elements is requested. Can someone explain the complexity analysis in this statement? If the size is fixed (predetermined), isn't it O(1) runtime?
Just like /u/newestHaskeller has permitted, feel free to lift anything from [my similar module](https://github.com/sacundim/tau-sigma/blob/master/src/TauSigma/Util/Vector.hs) as well.
https://hackage.haskell.org/package/HaskRel might be relevant.
Also one can do STM in .Net, you just have to explicitly mark the variable you want to monitor in the STM runtime, just like how STM works today in other languages. What Microsoft failed at was attempting to solve a much harder problem - STM-ify the whole program transparently to the user. The recent success of doing STM now greatly simplifies the situation by requiring the user to opt-in only the data that they wish to be STM-managed.
[removed]
Thanks :D.
I understand that the situation you describe can happen to people, but never in my year and a half of Haskell has it happened to me, which us why I voted for the defualt to stay the same but to have an optional flag to activate an implicit forall. Alas, if Purescript doesnt support pragmas, that's just how things are.
Indeed, part of what made it so frustrating was that it was all entirely reasonable, and just simply not what i wanted. IO is an interesting point. I think that Haskell would be well served to try and break down IO into actions that are more explicit, type-wise, about whats going on. Perhaps that is what will happen within the next 10 years.
I agree. That's almost definitely better than my original suggestion.
My instinct is that it would be easier to manage and possibly more flexible than that, but either will probably work.
Just use digestive-functors?
The number of elements used from the sorted list is fixed; the size of the original list isn't. 
[generics-sop](http://hackage.haskell.org/package/generics-sop) is a Generics library that can create HList-like representations of your records and allows you to manipulate them in sophisticated ways. The interesting thing is that the representations (the [sum-of-products](http://hackage.haskell.org/package/generics-sop-0.2.1.0/docs/Generics-SOP.html#t:SOP) type) are parametrized by a functor, and every field in the representation is wrapped in that functor. Which is a bit like what you are trying to do by hand. In [this gist](https://gist.github.com/danidiaz/3caebfa063b3496e90d8deafef463a03) I solve a simplified version of your problem: to automatically produce a two-stage parser/validator for any record with a single constructor and a suitable [Generic](http://hackage.haskell.org/package/generics-sop-0.2.1.0/docs/Generics-SOP.html#t:Generic) instance. The solution has the defect that the validation errors are not associated to the name of the corresponding fields. It think it can be done, but my generics-fu is not strong enough yet. generics-sop type wizardry is a bit scary, but the package is pretty well documented, and then there is this very useful guide https://www.cs.ox.ac.uk/projects/utgp/school/andres.pdf 
That's because they are. Look up imprecise exceptions.
There was a post very recently on quickchecking a Raft implementation [here](https://www.reddit.com//r/haskell/comments/4cpvbv/fuzz_testing_distributed_systems_with_quickcheck/)
@kamatsu It is only observable externally. For pure code, there is no difference. That's the whole point of using `undefined` instead of `Either`, it can simulate infinite loops.
I just started to understand template meta programming in C++ and now this! Next someone is going to tell me that entire programs can be written with Java exceptions &gt;_&lt;
For the dynamically-linked non-Haskell libraries on Linux, the following can be helpful - [Static linking with ghc](https://ro-che.info/articles/2015-10-26-static-linking-ghc). It suggests doing this: `stack build --ghc-options='-optl-static -optl-pthread' --force-dirty`, plus making sure that system static version of the libraries are available (These are `.a` files under `/usr/lib/*`). Sadly, it may not always work, because `-fPIC` in not automatically brought into the compilation of C files that some packages have under this mode (EDIT: this error does not makes sense, because we are building static, and `-fPIC` is needed only for shared libraries on x86_64, so I'm puzzled. Maybe under this mode `-fPIC`is removed from the the GHCi-inclined auxiliary build, explaining this?).
You can store all of your data in exceptions and perform all control flow with exceptions as long as you allot me the ability throw exceptions in the constructor of an exception.
What does it use now? Or is the joke that that's what's currently used in GHC?
The fact that Haskell's type system is Turing-complete is hardly any news. https://github.com/seliopou/typo
So, resources acquired by `turtle` library functions (and ones that I acquire with carefully written custom brackets) are properly disposed of. Thanks. That's what I wanted to hear :)
a lot of things are ... my favorite (yes I'm a nerd): [MTG](http://www.toothycat.net/~hologram/Turing/index.html)
Please don't use these click bait kind of titles. It's already annoying enough on other reddits.
It uses a direct tagged union representation
good warning, although in that case it was quite warranted :)
Wouldn't [rule 110](https://en.wikipedia.org/wiki/Rule_110) have been easier?
Can anyone give an example of how this is done for union types?
My favorite: [CSS](https://github.com/elitheeli/stupid-machines)
**TYPE THEORISTS *HATE* THEM!** Learn how to make GHC's brain explode with these 10 weird type-level tricks! The 6th one will blow your mind too!
And I thought I had pulled something absurd from the top of my head :(
You would be stuck using peano numbers and other strange representations but yea, in principal Java exceptions are Turing complete
And if you want to get really fancy, you can automatically deploy binaries using Travis CI and AppVeyor! Check out the `.travis.yml` and `appveyor.yml` files in [my Octane project](https://github.com/tfausak/octane/tree/0.4.13) for how to do that. 
Fantastic talk. The buildup was fantastic -- I really thought that the MuFP and Lava story was a setup for using something akin to the layout combinators from the pretty printing library to lazily search through some space of circuit layouts. :-)
It's [even worse](http://beza1e1.tuxen.de/articles/accidentally_turing_complete.html): &gt; The page fault handling in X86 can be used to implement a simple machine. Basically, a page fault pushes a word to the stack, which might underflow generating another trap. This mechanism provides a "subtract and branch if less than or equal to zero" instruction. Enough to implement a Turing machine.
From GHC's perspective? I know very little about this, but from what I've read, there are [serious issues with free monad performance penalties in Haskell](https://www.reddit.com/r/haskell/comments/42vs0x/question_about_optimizing_code_with_free_monads/). To me it seems like a Church-encoded interpreter should vanish completely--I mean what applications aren't known at compile-time with a free monad but are in a logically-equivalent transformer? Shouldn't they reduce to the same thing?
In the example I use composition of applicative functors to model each stage. The "run" function collapses the errors, but it need not do so. Leaving aside generics and type families, it would be interesting to know how far one could go with applicative composition alone. 
This paper describes the cumulative work on GHC's migration to [Shake](http://shakebuild.com) in a comprehensive way, including some history and extremely impenetrable `Makefile` snippets. Good news: we're hoping to merge the build system into GHC Really Soon!
I'm asking myself a similar question these days -- should my DB-persistence types be different from the types that the rest of my app deals with? Right now, I'm going with DB-types being used throughout the app, because it's not clear to me whether two similar, but different, types have significant advantage. In your case, if you want to be **sure** that certain fields aren't sent over to the browser/client, you could hand-write the FromJSON &amp; ToJSON instances for each of your DB-types. Alternatively, I'm sure there's some TemplateHaskell or Generic derivation magic out there, which can automatically give you your ToJSON and FromJSON instances if they have a common pattern.
There was another recent post about how to deal with record boilerplate. It seems to be a frequent problem. &gt; The reason I do that is mainly because I don't want to expose details of the server to the client Can the "internal bits" be abstracted away using a type parameter, that would be mapped to () when generating the JSON? Something like what's explained there: https://www.youtube.com/watch?v=BHjIl81HgfE Or perhaps you could go the other way: make the "internal bits" a separate datatype that would contain the "public bits" record as a field.
It would be great to have a `default.nix` file with all dependencies needed for Tidal. I don't know how many times i was inspired to try it, but my inspiration vanished before i could get through the installation
While I think it is very cool to see GHC adopt Shake (good projects using good projects, what can we ask?), I'm not convinced by the idea of submitting a description of this work to ICFP. (Or maybe this is an Experience Report?) I think of research as solving hard problems or developing new ideas that others can build upon. From a high level point of view, this article looks more as the summary of engineering work; it is well written and would certainly be useful to future maintainers of the GHC build system, but how is the rest of the research community supposed to be able to reuse this work? One "identifiably research" bit would be Section 5, that shows a few abstractions built on top of Shake to describe GHC builds, abstractions that are defined in the exquisite manner of functional programming experts: powerful, general, expressive interfaces, with a well-defined semantics and a set of reasoning principles. In the [original Shake paper](http://community.haskell.org/~ndm/downloads/paper-shake_before_building-10_sep_2012.pdf), this form of contribution -- building solid abstractions for a specific problem domain -- formed the bulk of the contribution, and I think this is what justified its role as a research article. In the current submission, this part of the contribution is much more modest, and I think it is a bit light as a justification.
The problem I was facing was the `id -&gt; href` mapping and vice versa. I think that the approach of writing manually ToJSON and FromJSON instances is a good one, haven't really thought about that. Now I could map the `id` to a `href` in the instance itself. The only problem is the extra boilerplate.
OK that's a link title in serious need of a newline: "A Means to Many Ends \n 10+ years of Haskell at Galois" Not "A means to many ends 10+ years of Haskell at Galois" 
The implication from the talk was that you get better branch prediction.
Non-Recursive "Considered Harmful" titles considered harmful
I mean, it's overused and generally bait-y these days. I can totally agree with this. But I mused to Simon today that I liked the name, because I really do! In context, "Recursive Make Considered Harmful" is an excellent, well articulated paper, born from experience, and Miller's work has been influential. Despite the line-noise jokes about GHC's build system (they ain't wrong), it was *dramatically* improved thanks to him. The non-recursive GHC build system we use today is substantially better than what came before it in key ways. If the content was extremely dry or otherwise lacking substance, I'd find it pretty lame and overplayed. But I think this title is appropriate here because the paper itself shares a lot of the same features that Peter's work did. Ultimately, I think the end of the introduction says it best: &gt; None of this is fundamentally new; we review related work in 7. The distinctive feature of this paper is that it is grounded in the reality of a very large, long-lived software project. Peter Miller would be happy.
I would join you in appreciating the title, I like it very much. It is a playful way to pay respect to the original "Recursive Make ..." work, and there is also a powerful pun on the idea that functional build systems (as a broader sense of "non-non-recursive make") are a good way forward.
[removed]
I think this is a major step forward; especially for everyone who is going to hack on ghc. The shake based build system is much easier to understand, imo--ymmv though. I expect this to give ghc a push with some of the more esoteric configurations/flavors.
Having already seen their April Fool's, I wondered if I was experiencing chronological displacement of some kind.
Hm, this `Freer`'s functor instance requires `f` to be a functor, no? And isn't that what is meant by free_r_ - that for any `f`, `Freer f` is a monad?
Hopefully this will make cross compilation easier, once it's implemented. Unfortunately it doesn't help that much with parallelism, mostly because the critical path is disastrously long - see S6.3. But now we've replicated the existing system, refactoring to break that dependency will hopefully be easier.
The connection between recursion in the title and functional programming was not deliberate!
Think of this less as a title, and more as algebra. Given "Recursive Make Considered Harmful", we needed to show that all the paper could be generalised to drop the "Recursive", therefore we have: s/Recursive// "Recursive Make Considered Harmful" How do you do that algebraically? "Recursive Make Considered Harmful" + "Non-recursive Make Considered Harmful" ("Recursive" + "Non-recursive") "Make Considered Harmful" "Make Considered Harmful" So our paper title was dictated purely by algebra. I agree considered harmful is usually terrible, but in this rare case, I think appropriate.
The submission was double blind - the authors names do not appear on the manuscript submitted to the reviewers.
My knee-jerk reaction to the title was that the "recursive" and "non-recursive" papers cancel out and it's just "make considered harmful" which sounds about right, having switched from make to shake myself. Then I looked at the pdf and it was about exactly that! So the title gave me exactly the right intuition.
If I understand correctly, what you need is a type like that: data RefValue a = RefValue Text a instance FromJSON a =&gt; FromJSON (RefValue a) where parseJson o = RefValue &lt;$&gt; (withObject "RefValue" o &gt;&gt;= \x -&gt; x .: "href") &lt;*&gt; parseJson o Then you just need a function like that: entity2rv :: Entity a -&gt; RefValue a And you are done, no TH, no Generics.
Peter Miller was a good friend of mine. He passed away nearly two years ago. I also think he would have very much approved of this paper and the way its cites and builds on his work.
Available on Channel 9 too: https://channel9.msdn.com/events/Lang-NEXT/Lang-NEXT-2012/A-Means-to-Many-Ends-10-Years-of-Haskell-at-Galois
Yay! Next step  rewriting a test-suite to haskell?
From my understanding your question is answered by saying that it's up to how you compile it to at least a degree, but if something is commutative, it can be executed in parallel, depending on how it's used itself. (That is, if a piece of parallelizable code is used in a way that makes it no longer able to be executed concurrently, it won't be). This is probably a really good thing to read: http://research.microsoft.com/pubs/138042/haskell18-marlow.pdf along with S.Marlow's concurrency book if you haven't looked at et (he's a really good writer): http://chimera.labs.oreilly.com/books/1230000000929 Applicative is inherently parallellizable in the way that it *is*. That is, its structure is such that subsequent pieces *have* to be independent, therefore execution order is implicitly separate, therefore can be done separately. Contrast this with Monad, which when composed with (&gt;&gt;=) are not *necessarily* bound together, but could be, so we can't assume they will be. The paper OP links to is basically saying that do notation can be desugared to either applicative or monad, or both, depending entirely upon what is approapriate (the dependencies of the parts). It's so cool! :) This is the same with commutative monoids.... so, say, Integers under addition (the Sum data type)... This is pretty good to read here... http://blog.sigfpe.com/2009/01/haskell-monoids-and-their-uses.html
Thanks, I'll update the paper.
It is up to the implementation of the Applicative instance.
Obfuscated titles considered harmful
Erik, that's great to hear! I didn't write that bit about him being happy, because I didn't know him, and didn't want to put words in his mouth - I even considered removing it in an edit. But glad to hear that you think its a reasonable thing to write. His original paper is a classic, and hopefully this will encourage more people to read it.
&gt; We need to extract the metadata and generate `package-data.mk` files to be included into the build system; this is done by a Haskell program called `ghc-cabal` Maybe implementing `ghc-stack` based on Stack would improve things here?
The OP's example could be done like this: data User = User { username :: Text , userPassword :: Text , userEmail :: Text } deriving (Generic) mkPersist defaultCodegenConfig [groundhog| - entity: User ... |] Then you write the ToJSON instance to add the href field and the FromJSON instance to ignore the redundant information.
[Obviously a duplicate reply. I forgot to refresh the page to see the other replies before writing.] Probably not. If `ap` is more efficient than `(&lt;*&gt;)`, you should simply change the defininition to `(&lt;*&gt;) = ap`.
From the paper: &gt; we would prefer not to use &lt;*&gt; explicitly in our code at all, because it is sensitive to refactoring: introducing or removing dependencies between expressions affects where we can use &lt;*&gt;, and if the programmer is responsible for using &lt;*&gt; then not only do they have to spend time thinking about it, but they are likely to do an imperfect job. Thus we would like programmers to be able to use a simple universal syntax, so that they can focus on correctness while letting the compiler exploit parallelism as far as possible. To me this sound like a questionable line of thought. It is nice if programmers can focus on correctness, but many programmers have good reasons to also care about performance. So a worry is that the performance impact of accidentally introducing or removing dependencies between expressions is too invisible to the programmer. Will the compiler at least provide some kind of metric indicating how applicative the do-snippets are? 
Would you mind giving your thoughts on the performance of my encoding? Your implementation is extremely fast, but won't parallelize well. I'd be interested in helping reshape it for Applicative before you release it, if we can get the performance up to snuff. In the end, my goal is that `Eff Haxl m` should be just as good at parallelizing Haxl effects as Haxl alone. I think this is possible but I'm not sure. EDIT: Looking further, my encoding essentially just represents an `Eff f m` instance as a sequence of `f x`s, `m x`s, `(x -&gt; y)`s and `(x -&gt; Eff f m y)`s. Sort of like a type aligned list?
I think the point is just to sugar it away entirely, and if it can be done better than GHC will do, you should manually rewrite the expression as Applicative. I do hope we have a way to see what GHC desugars it into, as you said.
Another minor fix, if it's relevant, is that buildsome had the aforementioned features in beginning of 2014. Not sure how citation dates are usually dealt.
[removed]
I've never done any parallel or concurrent programming in Haskell, and I'm confused by the language you and the paper are using. You've said &gt; Applicative is inherently parallellizable but this isn't true in general is it? State has an applicative instance, but statements which aren't dependent on the result of statements could still depend on the effects of previous statements, so it would be incorrect to parallelize them.
That's great news! Time to write a proposal I guess :D
Not every Applicative can be parallelized, it is more that more Applicatives let you get away with doing things in parallel than Monads. Most of the examples in the paper actually show how you get benefits from the standpoint of strictness, merging passes over input, or subtle wins in terms of asymptotic performance than raw parallelism. We work in the tropical (min,+) semiring for computing "costs" in the paper, but we could just as easily work in one where the costs are computed with (+,*), or one of the soft-maximum log-semirings to model costs for other types of monads -- the results don't change much.
Is this behavior documented somewhere?
Sure. The compiler has no choice if we give it the monadic interface. Nice :)
Well, that completes the picture. If the reviewers have access to reddit - so much for double blind. :)
OTCH
There's perl in the bootstrap step - I think that's the one to target first (it's even pretty simple perl).
The evil mangler is gone. But GHC's object-splitting tool requires Perl still, so the overall answer is "no". We want to replace that anyway for a completely unrelated reason (so it's easier to distribute binaries on Windows).
We also still require Perl for `ghc-split` ([here](https://github.com/ghc/ghc/tree/master/driver/split)). Replacing that with Haskell is already on the roadmap, though.
There's a small mistake in the first optparse-applicative example: The definition for `options` should read options :: Parser Options options = (\input_ verbose_ -&gt; Options { input=input_ , verbose=verbose_}) &lt;$&gt; strOption ( long "input" &lt;&gt; help "Input file" ) &lt;*&gt; switch ( long "verbose" &lt;&gt; help "Whether to be verbose" ) Note the underscores on `input_` and `verbose_`!
Oh, but that breaks the functor laws. Is that a problem?
Don't undersell yourself. Your time is valuable :-)
They do.
The trick is convincing other people that our time is valuable...
Feel free to get a mentor as a part of our haskell learning group https://github.com/haskell-learning-group/haskell-learning-group
&gt; For all free monads, the interpreter is what determines whether it follows the Functor / Applicative / Monad laws. That's not true for "raw" free monads like `Free` and `Freer`. It is true for things like `Program` from `operational` and `Proxy` from `pipes`, but neither of these ADTs expose the law-breaking constructors to user code. It sounds like users are _supposed_ to be able to pattern match on `:&lt;$&gt;`, `:&lt;*&gt;`, etc. in your encoding.
But then you can't use `t` on a (non-monad) applicative. Maybe you want to - that's the reason why you'd only have an `Applicative` constraint. Actually, I very much doubt if `&lt;*&gt;` is ever less efficient than `ap` in practice, and I think `ApplicativeDo` is supercool. I'm just a bit curious if it can happen in theory. But it's a moot point really.
if you need web framework comparison: [wiki 1](https://wiki.haskell.org/Web/Frameworks) [wiki 2](https://wiki.haskell.org/Web/Comparison_of_Happstack,_Snap_and_Yesod) [reddit discussion](https://www.reddit.com/r/haskell/comments/332s1k/what_haskell_web_framework_do_you_use_and_why/) [google results page 1-1](http://www.edofic.com/posts/2014-02-23-haskell-web.html) [google results page 1-2](http://www.slant.co/topics/727/~haskell-web-frameworks-for-building-restful-web-services) What does the error say ? Please provide with more details. 
I personally prefer scotty+persistent since this is a really simple and lightweight way to implement a REST server. If you need server-side html rendering -- just add blaze-html.
Did a bit of reading. You're right. I thought I had read somewhere that all free monads technically broke the monad laws. Guess not, because `Free` and `Freer` are proven to follow the laws. Anyway, I guess it could go the route of `Program` and only export predefined interpreters that are known to follow the laws, while enabling Applicative optimization. But it would be better if I could find an encoding that automatically follows the laws, and also enables Applicative optimization.
Would be great if it was a library. I have had an idea of such a library for a couple of months now, should do simple identicons and have flexible API to actually change character of generated pictures/textures to something like identicons used on stack overflow or maybe something completely different. This is a beautiful thing to play with.
why don't your try the `yesod-postgres` (or `yesod-sqlite` or whatever) *stack-template* - it should work and if I remember correctly it has authentication and even a very basic file-upload (&lt;- not 100% sure here)
Spock + the "users" library I have a little demo project where I was testing it out: https://github.com/cgag/spock-starter/blob/master/src/App.hs I think this was based on a combination of this talk: https://www.youtube.com/watch?v=GobPiGL9jJ4 https://www.spock.li/2015/08/23/taking_authentication_to_the_next_level.html 
Because if `m s` is a Monad for all `s`, then the Monad instance doesn't care at all what `s` is. It can even be something that doesn't exist (such as said Skolem). And because it doesn't care what the value is, then you can use the Foralls in the Data.Constraint.Forall package I linked to obtain a dictionary for an arbitrary `Monad (m s)` constraint.
how could you make RPC calls without the IO monad, then?
[`ApT`](https://hackage.haskell.org/package/free/docs/Control-Applicative-Trans-Free.html) could be used for that **Edit:** Looking for some F/F such that: type Fetch = Ap F type Fetch = ApT F IO
I'm a developer and maintainer for all of the languages mentioned, except SBV (but I've contributed to it). Copilot, SBV, Atom, and Ivory are the most mature. Copilot, SBV, and Ivory are currently the best maintained. What do you want to do? Atom, Copilot, and SBV are "expression" languages, meaning that you can't store and manipulate memory. Ivory allows that, though. SBV is best for doing direct verification. Large programs have been written in all of them, however.
And the links for those books: * Haskell Programming From First Principles (60 bucks): http://www.haskellbook.com/ * Structure and Interpretation of Computer Programs (free): https://mitpress.mit.edu/sicp/
Hmm I've read that Learning Python by Lutz was good, repetitive, but a good book to learn from. ~~A quick google of The Haskell Book brought up haskellbook.com, is that it?~~ it is, thanks u/Lokathor Also, can Haskell do real time data analysis well?
Go with Stack on any platform. It is much more robust. 
That reflects people who successfully convinced others that our time is valuable! Having obtained a degree? Good way to convince. Having had internships? Good way to convince. Great grades? Fairly good way to convince. Student with average or below average grades, little else to show? (e.g. my situation: jobs fucked me over first 2 years of CC, pretty shit grades, now I'm at uni and just now starting my CS classes as a sophomore/junior). Well, our only hope is finding an opportunity with low competitiveness, or make ourselves competitive by offering a lower rate which may reflect our perceived value (for many of us -- that's negative. Even at the wage of *free* many companies see us as having a *negative* net effect on their bottom line).
The first language I learnt was Python before switching to Haskell as my main language of interest. I'm biased but I think if you are learning for yourself, go with Haskell. I find learning Haskell to be a more pleasurable experience. With Python, I found it a bit too easy to write sloppy code (100 line functions for example). Haskell on the other hand really forces you to break the problem done and abstract out reusable code. That's not to say that you can't write nice code in Python, just that Haskell doesn't let you get away with doing a lot of stupid stuff. I also like the more mathematical aspect of programming in Haskell. Composing functions to make more complex functions seems very intuitive to me. Love the concept of type classes too! With that said, some things are going to be much easier in an imperative language so it's not all sunshine and roses when learning the language.
What an enjoyable, easy-to-read paper! Coming from an algebra background, even the friendliest papers are usually quite hard work, so this was fantastic! Is this typical of FP articles, or this an exception? I've avoided reading many Haskell papers until now, but if many (most?) are this enjoyable to read, I will definitely have to dig in!
What are your thoughts on learning them at the same time? Or should I focus?
That's actually not our tutorial, it's Chris Allen's. I haven't watched it, but I bet it's good! The haskell platform is essentially useless these days, and is even harmful, as it puts its packages into your global DB, which isn't what you want. So, it only really causes pain. There is controversy over this topic, which is why it's still mentioned on haskell.org, as a small number of individuals still favor it, for inexplicable reasons. You can definitely learn stack as you go, it will make getting started with haskell way simpler and way easier. I'm biased as a maintainer, but this seems to be the community consensus.
I think that at the start (say the first few weeks at least), you should just focus on one language. For me at least, learning programming was very tough and trying to grasp the syntax and idioms of two languages at once would have been too much to start with.
Thanks, but I would rather do it by myself the CRUD server, since it is my first one.
Copilot is a DSL for monitoring hard real-time software. Atom is a DSL for writing hard real-time software. You don't won't to use either of these for your purposes. Ivory is more of a general purpose language, it may fit the bill. Disclaimer: I'm not very knowledgeable in this topic, take what I say with a grain of salt.
Well `data Fetch a = Done a | Blocked (Fetch a)` can only ever resolve to a single instance of `a`, so it's practically just a convoluted `Identity` type, which is a Monad.
Exactly same situation with me.
I really enjoyed reading that book. However, it is up to implement of the database library to decide how lazy it is. Laziness is a choice (which, in this case, I would expected to be documented here if it was lazy)
That's an interesting idea, if you're up for it. Remember that python and haskell are *fundamentally different*. This may be a good thing though, since your ideas won't be clouded as to what programming has to be. You will likely come out of the experience with a very clear idea as to the nature of computation. Imperative is like a Turing Machine, whereas declarative is like the Lambda Calculus.
Which to choose, in your situation, depends very much on what you want to achieve. Here's the guideline I'd offer: Pick Python If: * You are not terribly interested in programming for its own sake but rather solving a particular, one-off problem (like crunching numbers on a dataset to get some statistical info) * You want to write scripts here and there to help you do mindless tasks, but not any serious, large applications. * The above things mostly apply and you have contact with other Python programmers/friends The reason is that Python will get you decently far without as much up-front effort. However, once you start to get serious about programming, Python (and the mental structures that come with it) will start to be a big hindrance. It's like a bike. You can afford one easily enough, but you'll never go 60 MPH on one. (To be honest, people do amazing things in Python, but it's sort of the 'hey-look-mom-no-hands' kind of amazing: "Honey, you get down from there right now!") Pick Haskell If: * You want to see the depth and beauty of programming and you're in it for the long-haul * You aren't in any particular rush * You have access to other Haskell programmers (i.e. plant yourself firmly on #haskell-beginners IRC channel) If you choose Haskell, I would recommend you simply buy from haskellbook.com and just do whatever it says. The more you try to venture out on your own the more trouble you'll get into unnecessarily. But for the record, stack is where it's at. :)
&gt; essentially useless these days This is rather strong. I use it every day, quite happily, as do others. (And I do agree that it will be nicer in the next release, when GHC 8 comes, and the platform ships a minimal version that doesn't put anything in the global DB that doesn't already come with GHC). I appreciate that you have a good case for stack, and I'm happy for you to make it. I just wish the hyperbole meter would turn down a notch :-)
&gt; as a small number of individuals still favor it, for inexplicable reasons. Yeah, that's just the "**Make Cabal great again**" party that doesn't want to accept that everyone else has moved on to Stack for good...
If I'm not mistaken the definition of `&lt;*&gt;` for the two applicative instances of `(Nat, a)` is tantamount to minimum and addition: -- &gt;&gt;&gt; Blocked (Done (+ 1)) &lt;*&gt; Blocked (Done 1) -- Blocked (Done 2) -- -- &gt;&gt;&gt; apConcurrent (1, (+ 1)) (1, 1) -- (1, 2) apConcurrent :: FetchConcurrent (a -&gt; b) -&gt; (FetchConcurrent a -&gt; FetchConcurrent b) apConcurrent (n1, f) (n2, x) = (min n1 n2, f x) versus -- &gt;&gt;&gt; Blocked (Done (+ 1)) &lt;*&gt; Blocked (Done 1) -- Blocked (Blocked (Done 2)) -- -- &gt;&gt;&gt; apStd (1, (+ 1)) (1, 1) -- (2, 2) apStd :: FetchStd (a -&gt; b) -&gt; (FetchStd a -&gt; FetchStd b) apStd (n1, f) (n2, x) = (n1 + n2, f x) 
&gt; I use it every day, quite happily, as do others. come on man, why are you so reluctant to embrace progress? There's plenty of evidence that Stack works better than Cabal and avoids the common pitfalls, and yet you deny yourself access to modern technology (   )
You could learn both Haskell and Python at the same time. Do all exercises in both languages.
This is a role in California, US?
Putting stuff in the global DB does not make HP useless - it is actually helpful. But you need to know how to use cabal properly.
Quoting OP: &gt; NYC preferred, remote work possible. 
&gt; please don't hesitate to open up a Github issue Right, I should've thought of that. Thanks for the help!
I think the answer is 'it depends' :) Python will give you results sooner (*maybe*), but it will not force you to learn important concepts about types, design, abstraction, etc. Even if you solve the problem at hand, the code *could* end up being of poor quality easily. Haskell will give you a better background about programming and solid foundations to problem solving, but it will take some time to grasp. The concepts adquired will be helpful while using other languages (like Python) and paradigms (Imperative, OOP, etc). Even if the path is a bit longer, it is worth the effort. Some utils will make your life easier with both languages. You can use pip for Python dependencies management and for Haskell you have Stack which is a great tool.
&gt; This sounds like a bad skill to have: Cryptographic Protocol Implementation Don't ever implement crypto yourself. For clarification, we don't intend to create any new cryptographic protocols or algorithms, but are interested in hiring individuals with professional crypto experience. 
I'm not sure the proper name for the role but it means something like "person who writes great technical blog posts, and is comfortable interfacing with the developer community." 
Oh I figured as much to be honest. I Just don't think my Haskell is yet up to the task to contribute but I'll try. I recognize I'm the odd duck out here in getting/wanting a stage2 to build a separate ghc. I was also amused to find out I'm probably the first to either want to do it or get one to work across architectures. I'll have to admit though, 8.0 took a lot less hacks than before in this regard. https://github.com/mitchty/alpine-linux-ghc-bootstrap/blob/ghc8.0-bootstrap/ghc-bootstrap/bootstrap/armhf/bootstrap.patch So I'll keep quiet until shake is merged and then probably start to regale everyone with terribad haskell to try to add cross compilation. :) I'm not against doing work to make it happen, but shake is totally new to me as an old c hand but even the existing Makefiles made me have to look at the targets like I was translating a rosetta stone. And I use and write autoconf/make/makefiles every day, but this paper is totally on par with what I experience with build systems in general. As to if there aren't many porters or if it sucks to cross compile too much, probably a little from column a and column b. I have been in the alpine-linux irc channel and there are a ton of people that want ghc on the platform but outside of me deciding to say screw it I'll figure it out and do it not too many people seem to take up the mantle. I kinda enjoyed it more because of the vertical cliff of stuff to learn, but only now do I feel slightly comfortable getting things working after about a year of rummaging through things. But if I have to be the cross compilation guy no bigs, just be prepared to laugh at my haskell. &gt;.&lt; 
I've been playing with Haskell for about a year, just casually. Stack is a godsend for people like me who don't really understand what's going on; it makes so much more sense than the platform+cabal. Going with stack isn't "another layer of learning"; it's a way to make things easy on yourself. For all I know, this has downsides once you gain a certain expertise; but for beginners, I think it's a clear choice.
Well I say a clear mind because I've read once you know a language Haskell's concepts can be more difficult to wrap your head around than if you were 'tabula rasa' so to speak
That's the right term. The position is indeed often called "developer evangelist" and it's the person who identifies the use cases for the product/service and makes it easier by publishing info about it, and also goes to conferences and talks up those features and how they are used, and both pushes the company vision and takes back from outside users and advocates for them internally. They're usually straddling sales/marketing/tech.
We wrote a paper about this for the Haskell Symposium from last year. http://ku-fpg.github.io/papers/Gill-15-RemoteMonad/ 
I'm obviously biased towards Haskell but Python will provide you with many things Haskell will not (even if it's OOP or just the fire&amp;forget dynamic side) Anyway: **Welcome on board** and enjoy your ride ;)
Thanks! I just bought LYAH and the Haskell Book and I'm currently watching lectures on the lamda calculus on YouTube. So begins the rabbit hole.
Yes, it will ship with stack, so we can have a single standard distro that hopefully people can agree on recommending in most cases. At that point, the questions of use-cases for `cabal-install` and `stack` can be considered more directly without the issue of particular complaints about how the platform is bundled getting in the way.
Welcome! As I mentioned, plant yourself on the #haskell-beginners IRC channel on Freenode and/or the Slack-based chatroom for Haskell (http://fpchat.com/). You will definitely be asking questions, so just have these resources ready. I've found the community to be quite helpful over the years. Other than that and the haskellbook I think you'll be in good shape!
hopefully it meant "interested and knowledgeable enough in crypto to have tried implementing it themselves" and not "I roll my own crypto for all my production security minded projects".
Just curious, among the experienced Haskellers here, did anyone learn Haskell as their first language? I did not. While in theory we all like to suggest Haskell as a good first language, it's possible that someone who does this might encounter challenges that we didn't experience when we learned Haskell as our nth (where n &gt; 1) language. I don't feel strongly either way, I'm just wondering if anyone has opinions on this.
Very cool!
It's still correct to parrot it though. Those people who want to learn to implement crypto have to realize that they need to be an expert before it's safe to use anything they write in the wild.
I was noodling around with another language project, and an idea came to mind for a notation that would be nice to have in Haskell. Let me know what you think.
What is Nix? Nix describes itself as a package manager. It sounds like it's actually more general than that.
I'm not sure whether there is a mistake in the examples, or if I simply misunderstand. The first example: do f (&lt;- x) (&lt;- y) -- === do A &lt;- x B &lt;- y f A B Here f :: A -&gt; B -&gt; M R, and the do expression has type M R. The second example: do f (&lt;- x) (&lt;- y) -- === f &lt;$&gt; x &lt;*&gt; y But here f :: A -&gt; B -&gt; R, even though the do expression still has the type M R. This seems inconsistent. Is one of the examples wrong, or have I misunderstood one of them?
Hey that's great! Applicative operators are usually what we do in Haskell but this would be awesome to have. I imagine it would make expressions with mutable references much prettier.
If you go with Haskell, don't worry about monads and category theory too soon (unless you don't find them too hard). Haskell is mostly about Lambda Calculus, and category theory just so happens to have uses related to Lambda Calculus. The main reason with have monads is so we can embed imperative languages inside Haskell. That's why we say Haskell is pure. `IO a` is in fact not a thing of type `a`, but an *action* describing a set of interactions with the outside world that results in a thing of type `a`. Haskell itself doesn't really execute `IO` actions, it simply creates one large `IO` action called `main`, and then the executable/interpreter executes it. Monads just so happen to be a useful way of creating larger `IO` actions from smaller ones.
thanks! A refactoring gone awry, methinks.
Idiom brackets allow for something like this, right? I don't think they were ever implemented though.
What about infix operators? One of the nice parts of idiom brackets is: [| getLine ++ getLine |] == (++) &lt;$&gt; getLine &lt;*&gt; getLine Would you be able to omit `do` for a single action? f (&lt;- x) (&lt;- y) == do f (&lt;- x) (&lt;- y) **Edit:** Also: [| (getLine, getLine) |] == [| (,) getLine getLine |] == (,) &lt;$&gt; getLine &lt;*&gt; getLine
Inspired by Idris bang notation, I have thought about exactly this syntax for Haskell. 
Infix operators should be covered fine, although applicative operators or idiom brackets probably win: do pure ((&lt;- getLine) ++ (&lt;- getLine)) do pure (&lt;- getLine, &lt;- getLine) I would prefer to make the `do` required, because it makes the scope of the desugaring very clear, and can be used to improve error messages.
You can overload two different types, but not two different values for the same type -- how would it even choose which one to return? Here's what I think you want: data B = B String data C = C Int class BorC a where get :: a instance BorC B where get = B "1" instance BorC C where get = C 1
`IO` is a type constructor though, not a type.
My problem is actually related to a type class, i just asked this question using functions for simplicity  without being aware that this is just based on input! Thanks for this catch!
Is this actually proposed somewhere? I know a lot of people have said it'd be great, but I'm not sure if anyone has actually brought it up as a proper proposal to the GHC/Haskell community. FWIW I prefer Idris's notation, but this would still be nice to have.
This together with `ApplicativeDo` would definitely solve one of the issues I have with `optparse-applicative` right now  constructing huge records purely with Applicative notation always seemed kind of backwards to me, since it more or less foregoes type safety in many regards.
what's the proper procedure for this kind of proposals?
We would make more headway with beginners if we stop using "monad" when it isn't really necessary to understanding the subject.
Im not sure I get your point about reading direction. I read both `x &lt;- g` and `h x` from right to left. Left-to-right reading shows up when you have multiple binds, because theyre evaluated from left to right, as in `f (&lt;- x) (&lt;- y) (&lt;- z)`, and I can see why that would be an annoyance, but direction changes fairly often in Haskell code anyway. Another symbol or keyword could work just as well, its just hard to add new syntax. I like unary `&lt;-` because its already used for binding, and it was (surprisingly) available. I think of the expression `(&lt;- x)` as binding the result of `x` to the (anonymous) term `()`, rather than to a name. Your mention of list comprehensions raises a concern, though: should list comprehensions count as `do` blocks for this purpose? My intuition says no, even though it would be more uniform to allow it. 
Agreed. In the same way we say that `Just 1` is a '`Maybe Int` value' or 'value with the type `Maybe Int`', `getLine` is an '`IO String` value' or ' value with the type `IO String`'. We don't run around talking about how `Just 1` is the `Maybe Monad`. The `IO` type plus the functions provided by the `Monad` class, together form a Monad. But the a value, like `getLine` is not a `Monad` -- it is just a value with type `IO String`. 
Yeah, this concern makes me split on the idea. It affects what we can rely on when reading code. Also, I think `-&gt;` and `&lt;-` are overloaded enough in Haskell - it blends in too much. Maybe something like `&lt;-!` or `&lt;!`? Makes it stand out a bit more. Do we want arrows to become the `const` of Haskell? (const can be used in lots of different ways in C++). This makes the desugaring of do notation rather non-trivial. We usually have the property that: f = do let g = do a_big_expr_goes_here lift g is equivalent to f = do let g = a_big_expr_goes_here lift g However, if `a_big_expr_goes_here` contains a `(&lt;- expr)`, then we just changed which monad it gets run in (and in more complicated examples, when it gets run). That said, used in reasonable ways, this could be really nice, especially mixed with record syntax. Considering we have things like RecordWildCards, it may well be reasonable to give this a try.
`Maybe` is also a type constructor, but often just gets called a type, at least informally.
Do types have to have values? I'd generally say yes, with the exception of the distinguished non-inhabited type wehther you call it Void or \_|_. All values have a type with kind `*`.
Sounds good! Are you sure you want to desugar breadth-first? My interpretation is that it implies do ((&lt;- f) (&lt;- x)) (&lt;- y) to be desugared to do y' &lt;- y f' &lt;- f x' &lt;- x (f' x') y' I think "source code order" would be more natural, which I believe is described by "left-to-right, depth-first".
This actually seems even more cumbersome than the existing Applicative operators... or am I missing something?
I remember proposing this in 2009. Neil Mitchell asked me then about this example: do { if sqrt 2 &gt; 2 then putStrLn (&lt;- getLine) else putStrLn "no" } Does this read from stdin, or not? Now imagine that instead of if/then/else you have it in one branch of a complex 20-line case. If the readLine isn't performed, then you've turned conditionals into something vastly more complex than they were. But if it is, then basically every new programmer ever is going to spend some time very confused by this behavior. Neil and I agreed that the latter behavior is the better of the two; but it's not really great! Another concern raised was that this breaks the invariant that `do { s } == s`. But frankly, I don't find that too concerning at all.
Now this is the kind of counterexample I was looking for. :) I *think* it would be more usable (albeit less expressive) to have branching forms such as `if` and `case` introduce new scopes for `(&lt;- x)` forms (and warn if there is no `do`). If you want something to be bound beforehand, well, then just bind it beforehand. Pretty sure the best way to see how this works in practice is to just implement it. I agree that `do { s } === s` is not a particularly important invariant. We dont need glorified mahnadic parentheses. do do do do do (mahna mahna); do do do do (mahna mahna) 
Technically, it's not "the IO type" either. IO is a parametrized family of types.
It's "actually" being proposed here, as far as I can tell. Perhaps this is going to be an embarrassing admission if I'm missing something, but I get a little discouraged when things like this (which IMO is quite well defined) are brought up, and the response is to ask for a proposal or description that's (vaguely) better in some way, implying that there's something missing but never saying what it is. At least point out what you think is incomplete about it! This has actually been the barrier to my trying to jump into GHC development several times in the past. I've gotten to the point of being ready to do something, and then been stalled by people asking for... something... but never a clear statement about what additional information they are actually asking for. Seems to be a GHC-specific thing, too. No amount of description seems to be enough for people. Except when it is. But there's no apparent way to tell the difference from outside. (In particular, I actually implemented this specific proposal 7 years ago, but never got enough agreement that I'd written a good enough description of it to go anywhere.)
Fantastic! I'm a logician, though, so I come to it from kind of an odd background. I've found it really enriching, though I doubt I'll ever do more with it than cobble together a few little things for my own use. 
It could be confirmation bias or lies, but the "Haskell as a first language" accounts I've seen were very positive and those people had a hard time learning imperative programming after.
It certainly lives on the "types" side of the phase distinction. I think the standard nomenclature is a bit obtuse. In many languages, `*` is called `Type` (or something similar), so the natural thing would be to reserve the word for inhabitants of `*`. Furthermore, no term in the language can be judged `t :: IO`. In other words, it does not name a type of any object. On the other hand, whenever `a` is a type (belongs to `*`), `IO a` is a type (again, belongs to `*`). But `a` is unspecified. We could view it as a parameter. And so I call `IO a` a parametrized type. 
I think the compiler should issue a warning or even an error in this case. It looks ambiguous and forcing a programmer to express his intentions more clearly shouldn't hurt.
a function is just a parameterized value, said the hermit. Sometimes you want to put the emphasis on the value, sometimes on the indexing shape..
I hadnt though of that! Care to elaborate? This notation does look an awful lot like lifting arbitrary coeffects (permissions) into effects (monadic computations), but it only works as such because its highly localised. I dont think you can do such lifting in general from within Haskell.
&gt; but the Haskell Report If we hold any document written on the subject of functional programming as sacred, I pray to Church it isn't this one.
Hey, I'm not the gatekeeper, it just seems that a gist posted to Reddit isn't the place to "actually" propose something -- though certainly a good place to get feedback! -- how about a post to the official GHC dev list? I'm sorry you had a bad experience! I'm just someone on the periphery of the community who wants to see a features like this in GHC and I want to encourage the OP to run with it
I am not an authority, but my guess would be to post to the GHC dev list and then probably to create a wiki page.
Records created with an option parser typically have a bunch of fields with the same type (boolean or string). Using applicative operators, expressions aren't next to the record field they're bound to so it's easy to mistakenly switch two around and still have it compile.
Its mostly for the common business-logic case of applicative expressions inside monadic ones. When I was at Facebook, for example, I saw a lot of use cases of this form: x `probablyKnows` y = do if length (intersect (&lt;- friendsOf x) (&lt;- friendsOf y)) &lt; 10 then do return $ (&lt;- networkOf x) == (&lt;- networkOf y) else return True This was more or less the standard example we trotted out for Haxl, because this pattern was so incredibly ubiquitous: fetch some data (monadic), possibly make some decisions (monadic), then return some reduction (applicative). I would appreciate examples of cases where this desugaring doesnt work for you. In all probability, it could be improved and generalised. 
it's not a sacred aspect of functional programming... it just provides the canonical definition of things in Haskell in specific :) it defines haskell language constructs like typeclasses, data constructors, etc. You can call Maybe a haskell typeclass if you want. You can call "True" a haskell module import. but be aware that you'll cause some confusion because you're using the words differently than in the defined manner in the context you're using them in (Haskell). the authority in *haskell* for definitions of *haskell* concepts and *haskell* words is the Report. that's because the report **defines** Haskell-the-language itself, in the first place.
Saying IO Monad is like saying Int Semigroup. "This function returns Int Semigroup" - no one would say that.
That's not how you use type classes. Think of them as interfaces rather than classes (which are concrete implementations). For your inheritance purposes, you could just add more functions to `BorC` which would have to be implemented.
Nuh-uh, it's the IO MonadIO.
I don't see how that would help. There's no MonadStateIndexed, and even if there were, you'd still not be able to define the instances for all the standard transformers. Edit: Sorry. There is a IxMonadState and a IxStateT. They work together. The point is that IxMonadState doesn't work for anything else. You can't define an instance for a transformer stack (indexed or otherwise) that doesn't have a IxStateT at the top. It's actually kind of frustrating that we accept this situation. The `local` function of MonadReader takes an `r -&gt; r` function when that should be `r' -&gt; r` like the one for ReaderT. Expressivity isn't the only cost here. Implementing the `local` function for my transformer stack is VERY easy to get wrong since `local _ = id` is even a valid implementation. This would be the case with my proposal.
...and we will have to implement indentation rules for this syntax in haskell-mode :) Which I'll be happy to do :)
Instead of a warning I vote for a hard error, i. e. &lt;- is allowed only directly inside a do without any intervening keyword control structures. Along the way I would ban &lt;- also in let. Instead of let x = (&lt;- a, &lt;- b) one can always do x &lt;- return (&lt;- a, &lt;- b) 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/python] [Haskell vs Python for first language](https://np.reddit.com/r/Python/comments/4dvnvr/haskell_vs_python_for_first_language/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Well, I guess GHC cannot adopt it directly since !-patterns is a thing is Haskell.
Your post reads as somebody who is heavily biased towards haskell Can you name a notable "large" or "serious" program written in Haskell? Do you realise that reddit is written in Python? Lol
It's possible to have the benefits of Python and of FP: - https://github.com/i2y/mochi - https://pyos.github.io/dg/tutorial/
This is a common problem with instance matching. Instances are matched without considering the context, and both Member (t ': l) t Member (nt ': l) t match the required constraint: Member (Int : []) Int However, the first instance is more specific, so this should work if you enable overlapping instances. But you're still likely to encounter cases where it is ambiguous which instance to choose. 
100% this. You will be a far better programmer in both languages than you would be in just one. Learning Python will give you the mindset for solving problems in a completely different way to Haskell and vice versa.
Why focus on quantity rather than quality?
Thank you for the remainder, didn't know matching is context-free before. Since in this specific case I only care about the existence of instance (and not which to match), adding a `{-# OVERLAPPABLE #-}` pragma to the second instance declaration works for me.
I'd say it doesn't matter much. Your first programming language will only shape you for life if it remains your *only* programming language, but I strongly recommend picking up more languages later on. That said, do yourself a favor and use Stack. I understand the fear of adding another abstraction level, but in my experience, this particular abstraction is good enough to actually make your life easier. With Stack, you don't need to install a Haskell compiler yourself, you can suffice with superficial Cabal knowledge, you don't need to know much if anything about Cabal sandboxes, you're not going to accidentally bork your global package database, and you're very unlikely to end up in Cabal Hell, at least as long as you stick with Stackage packages. If you want everything to work out of the box, then Stack is your best bet. In this light, the advice "install Haskell Platform" is outdated; just install stack and let it take care of the rest. In fact, I would argue that Stack is better than the equivalent Python toolchain, and easier to use too - stack projects isolate a project, its dependencies, and its compiler, in a much better and cleaner way than `virtualenv` does.
&gt; once you start to get serious about programming, Python (and the mental structures that come with it) will start to be a big hindrance. Me and most of the largest corporations like to disagree ;) 
Oh wow, thanks, that was actually stupid of me - I wasn't running `:l DevelMain` each time. Total oversight on my part. I'll try this out and get back to you :)
So this works, but it's still ~5+ seconds of waiting time (there are 53 modules in the codebase that are waiting to be compiled if that's any indication of magnitude or something). I have 8 cores on my CPU, so is there anyway to ask stack to build stuff in parallel? and why does it choose to rebuild so much each time I load the `DevelMain.hs`? doesn't it cache stuff? I'm sorry for the n00b questions, but I don't really understand what GHC's / GHCi's compilation model is
Speaking as a daily, professional Python programmer here: I love Haskell because it has taught me so much. I do not, however, like the numerous GHC language extensions and other things that are needlessly confusing for beginners: fmap vs map, foldl, etc. The language extensions in particular feel very messy and also like you have to learn a lot of variations or flavors of the language instead of the language itself. Plus, they operate in somewhat invisible ways (as opposed to statements), so they can be tough to grasp. I think they're the biggest wart in the Haskell ecosystem and I hope something will be done about them at some point. The Python standard library works hard to seek out one obvious way to do anything (just don't mention string formatting), and for beginners this can be very nice: it feels like a group of people worked hard (and continue to work) to reduce the potential for confusion. Python, in general, feels like a language designed to be very friendly to beginners, and this makes a number of aspects quite pleasant.
You can mess around with pretty much every language you might think [here](https://repl.it/languages) So I would say play around with both and see which you like better. 
If it's rebuilding a lot, it may be because you're just changing a file at the bottom of the module dependency tree. In that case you have no choice but to reload them. Are you definitely running with `-fobject-code` on? That's what does the caching. I'm not sure the `stack` invocation you're using is still applying - try `:set -fobject-code`. It should only reload modules that are necessary to be reloaded. I've used this method on 130 module codebases and the delay is never above a second. But like I said, depends if you're changing just a Handler or some more low-down modules.
I like this idea, but I would not allow it in `let` bindings. Normally multiple bindings in a `let` expression can bee freely reordered without changing the program result. This would be no longer be possible with this extension. Also how are recursive bindings handled? An implicit `RecursiveDo` would be very surprising. I think this would be relativey safe for non-recursive `let` expressions with just a single binding, but otherwise the behaviour diverts to much from normal `let` expression in my opinion. A workaround would be using `x &lt;- return (&lt;-y)` instead of `let x = &lt;-y`. There could also be a special `let` syntax which only allows one non-recursive binding, but can contain `&lt;-` expressions, like `let&lt;- x = &lt;-y`.
Please see [my other response](https://www.reddit.com/r/haskell/comments/4dny3v/haskell_vs_python_for_first_language_and_platform/d1uviql). There are many large and serious programs written in Haskell (ask Facebook, Prezi, Standard Chartered Bank, Elm, PureScript, etc.). I don't think that the existence of large apps in Python is necessarily a proof of it's quality. With enough manpower you could build large apps in assembly too.
This is a great list if you're goal is to get into a software development career ASAP.
There is only one argument you can provide to a function with a type matching `() -&gt; a`, so that type is isomorphic to just `a`.
&gt; and those people had a hard time learning imperative programming after. That seems like a disadvantage, no?
What I tell all people who want to learn to program is that it makes most sense to identify what you actually want to try to do, and pick a language which is good for that. That will leave you several choices, most likely, and between those, pick the language that "clicks" with your mind the best. 
The answer to all these seems to be "Javascript", nowadays..
Build it. Ship it.
Especially given that you cited Facebook, which still has massive amounts of PHP code in production. So essentially you've made the point that listing companies who use a language don't really mean anything positive or negative about the usability or utility of the language for a given purpose.
I don't mind the overloading of the dot, but I'm surprised AsAliasing doesn't already exist! Agda allows imported identifiers to be renamed, and I assumed Haskell supported it too, it just so happens that I've never needed it.
&gt; run `stack ghci stack ghci --ghc-options="-O0 -fobject-code"` I think this should be `ghci-options` -- note the `i`. There're two flags. Very similar, easy to confuse.
thanks, I did wind up noticing this while going through the help.
I am biased: Python is deceptively deep and can give you long legs for learning but is also extremely succinct and easy to read. The combo makes for a great first language.
&gt; vinyl I had a look at `vinyl` a while ago and I admit I didn't really understood the point of the type families. Now, I totally understand what is it for, which exactly the same problem as mine. My next question would have been how can I *traverse* my record to go from a `Receipt Either` to a `Either (Receipt Identity)` but this correspond to vinyl `rtraverse`. So vinyl seems to be exactly what I want (at least for the validation use cas) but I'm not sure I need the extensible record part. So I'm debating to use actuall viny records or just use the same principles but hardcode the structure of my record. 
And the wheel turns... Renaming on import was how Haskell used to work before it had qualified names. 
https://i.imgur.com/L7pxeu6.png
Yet some imperative people have a hard time with functional programming. Once you are productive in thinking about code in one manner, it's harder to justify another.
I see some benefit to this approach in that it would prevent you from getting too stuck in an imperative or functional mindset. However, concepts from each language could cross over in not necessarily desirable ways for a beginner.
&gt; there's a Python library that does it - in ways that are subtly or dramatically wrong. I've written many large projects and many more small scripts in python and I've never had this problem. Can you provide an example?
then you should ask where is it easier to find useful answers and get help :) 
I am definitely going to, well at least try, learning both at the same time! Any suggestions for good beginner resources? I've heard Learning Python by Lutz is a good one.
I didn't read the examples carefully. I meant Map.Insert -&gt; mapInsert Map.Null -&gt; mapNull Map.Empty -&gt; mapEmpty It could also be postfix. Then the reader could read the beginning and decide afterwards if he wants to proceed to obtain the information from which module the function came from.
This is a huge part of understanding haskell. IO values aren't functions. They don't have any magical ability to do things when they're evaluated. IO isn't a tag that means "special function". IO values are data. Opaque data with a super-messy underlying representation, but that's irrelevant unless you're doing very evil things. In normal use, the semantics are what matter. And the semantics are that IO values describe actions the runtime can take, but are inert in and of themselves. As far as evaluation is concerned, IO values do nothing. They're not even reducible expressions. The graph reduction machinery Haskell is based on treats IO values as leaves. The only time IO is anything special us when it's executed. At that point, the runtime takes over, introspects the messy internal representation, and performs the actions it describes. So what causes execution of an IO action? The only case where that is mandated by the core language standard is when running a haskell program, the IO action named main is executed. The foreign function interface extension adds an additional method, originally intended for working with C code that's referentially transparent, but sometimes gets used for other purposes. And ghci as a developer tool made the decision that it would be much more useful if it executed any IO value that results from evaluating the expression the user types in. There are a few other things that can cause execution of IO values, but they're all the result of mucking with the underlying data type in terrifying ways. They aren't good ideas to use unless you understand them perfectly - and are willing to deal with lots of spurious bug reports when others use them incorrectly. (If you want details, find the story of inlinePerformIO in bytestring.)
About once a week someone asks if they should learn Python 2.7 or 3.5. The answer's basically always to learn 3.5, unless it's for a legacy system.
I have ancient love for glxgears. When it ran properly it meant I could play Counter-Strike. 
In my experince the key to this is convincing yourself (or even just acting as if convinced).
Learn Python the hard way is the preferred book. That said, there are a lot of other free resources out there too, including some courses by e-universities. But regardless of what language you pick the best way to learn is to "do". And generally if you have a focus or goal (eg create a tool or build a website or make a game), then the doing is easier.
It has been some time since I've read it, but IIRC the [official Python tutorial](https://docs.python.org/3/tutorial/index.html) is a good beginner resource.
The inefficiencies of `String` are far from minor, as any non-trivial amount of experience would readily demonstrate. To claim otherwise is very silly. 
Well, which would you cut? ByteString and Text are fully distinct and fully justified. String is actually the odd one out here.
I'm currently reading "Learn You a Haskell For Great Good!" by Miran Lipovaca and it's a good book. Not sure it's for beginning programmers, but it is well written and even somewhat entertaining. Also talks about how to get Haskell set up (although it's pretty trivial, at least on a Mac). 
I'm sorry, I really was not trying to implement stop-energy here, only to encourage. Please disregard my posts.
I don't see why `let` is such a unique case. The inline bind breaks all sorts of equivalences that were previously true, if you try to treat it as an ordinary expression. For example, it's no longer true that `flip f y x == f x y`, or that `const a b == a`. To accept the extension at all, we'd have to accept a desugaring step that implicitly happens BEFORE any kind of equational reasoning, because that ship has already sailed. &gt; Also how are recursive bindings handled? An implicit RecursiveDo would be very surprising. I don't understand this question. What's an example of your question?
Yes it is. But Appveyor seems to be for building Windows binaries ?
I'm appealing to a higher authority. It's like how Haskell made the morally wrong choice when it used :: rather than : for the typing judgment. 
I suspect the following is true, admittedly without evidence: - If you ask the Haskell community, they'll tell you Haskell. - If you ask the Python community, they'll tell you Python. - If you ask other communities about those two, they'll say Python more often.
I appreciate you explaining here but I don't think we're talking about the same thing. I understand all of that and have for some time. The only thing I was specifically talking about was representing the **type** of **getLine** as IO String when I thought it should instead be type () -&gt; IO String. getLine does, in fact, do something "magical", as I understand it which is when you execute it (at runtime) it retrieves an IO String **value** from the real world. I think we're in complete agreement here, right?
Thanks for the traverse. Doing for s few type might be easier than using the whole vinyk machinery.
Your first comment wasn't about higher authority. Your first comment was about technicality. And in all technicality, `IO` is a type
getLine actually doesn't do anything magical, it's a normal value that you can manipulate and apply to functions. It's just normal data, like an integer or a string. What would you gain from having everyone always use `getLine ()` instead of just `getLine`? `getLine` is not a function in any meaningful sense of the word function in Haskell. it doesn't take any arguments. and it always returns the *exact same value*, every single time you call it. What do you call something that doesn't take any arguments, and always is the exact same value every time you use it? A value. not a function :) 
We are in complete disagreement, which suggests I explained poorly. The type of getLine is `IO String`. It is a value, not a function. It does not come from the environment or runtime. It does not ever change, regardless of what I out is provided. That's because getLine doesnt *return* an IO String. It *is* an IO String. IO values are instructions to the runtime. getLine is the (compound) instruction "get a line of input from standard input, and convert it to a String". getLine is not a function, because that instruction isn't something functions can do in Haskell. Only the runtime can interact with the outside world. Why does this matter? Well, it's the only way to explain types like `IO a -&gt; IO a` or `IO (IO Int)`. Those are fully rational types, and neither is even uncommon. They also are completely logical and meaningful precisely because IO isn't magic. It follows the same rules as every other type in Haskell.
I see @po8 point. Inefficient for what? Parsing command line arguments for a console program? Holding names in the Template Haskell library? Holding a list of directories? For many uses String is just fine and there is no need to go on some String removal rampage. String's advantage is that it is everywhere. Dependencies have costs and it may not be worth adding the Text dependency, especially if the only reason is a vague "Strings suck." The "stack" tool is a good example of that; they just use String everywhere because String is no bottleneck in "stack" so it's not worth it to depend on another library. 
&gt; The "stack" tool is a good example of that; they just use String everywhere because String is no bottleneck in "stack" so it's not worth it to depend on another library. Well: ~/s/stack (master) $ git grep "\&lt;String\&gt;" src/ | grep -v import | wc -l 354 ~/s/stack (master) $ git grep "\&lt;Text\&gt;" src/ | grep -v import | wc -l 374 The point about `String` not being a bottleneck for `stack` is almost certainly correct though.
I would say that `Text` *needs* to be in the `Prelude`. Neither `String` or `ByteString` do, really.
&gt;In a just world we'd quit worrying about the minor inefficiencies of String There are no minor inefficiencies of String, only huge, massive, colossal inefficiencies. &gt;Certainly having three representations or more running around the ecosystem is doing no one any good. We don't have three, we have two: Text and String. And the entire point is to not use String, so then we only have one.
Text is already everywhere. Any non-trivial haskell program already has a dependency on Text from depending on something else anyways. And stack is a perfect example of exactly that, it has a Text dependency, so there is no reason to use String there at all. They use String in places where that is the path of least resistance because base library functions that take and return String. This whole thing is all about fixing that problem.
Is it possible (or likely) that Prelude will be redesigned in a future version of haskell/ghc?
ByteString is not and should not be thought of a string, not in 2016. It is a collection of bytes that may or may not have a more nuanced semantic meaning. It needs to be parsed first. You could say ByteString approx-equals Maybe Text because all you know if you have a value of type ByteString is that it might be in the encoding you expect it to be. 
Well since String is defined as a list of Char, the interface of String is that of list. There's been a lot of effort put in to making lists efficient in GHC and it continues. But there is no huge improvement on the Horizon. In Haskell, lists are often used for control rather thana data structure as such, so techniques such as deforestation and fusion can work. Strings are more often an actual data structure so those optimisations work less often. Basically the answer is no.
For the lazy among us.. could someone summarise what's different between this and [base-prelude](http://hackage.haskell.org/package/base-prelude) or [basic-prelude](http://hackage.haskell.org/package/basic-prelude) ? PS this was a really useful tour of "what's in the toolbox", recommended for every new (or old) Haskeller.
What about parsing a csv with cassava with forces you to use bytestring , because, a file on a hard drive after is a sequence of bytes and we shouldn't assume of the encoding. This is a fair point, but then you are not allowed to read a file as Text. So, indeed you only have Text and String, but no function to read a file.
thanks :D I meant "use", not call, haha 
[removed]
&gt;What about parsing a csv with cassava with forces you to use bytestring No it does not. It uses bytestrings internally, and gives you the option to use bytestrings yourself if you need access to the raw bytes. But you should be getting back records with texts and ints and such in them unless you go out of your way to get the raw data instead.
I'd endorse that. I'm not great at js but I've heard good things about it and I'd learn it if I wasn't occupied with other things
Quality is a per-project phenomena in this case
If you find yourself doing functional programming with Python there's a good chance that you're doing it wrong. This coming from a Python programmer. [boop](https://youtu.be/EBRMq2Ioxsc?t=45m15s)
Yeah. Some more, just for the record: * PayPal * EVE Online * Dropbox * SaltStack
[removed]
If you use Effect' instead of Effect then you can skip the lift and runEffect
Cool. It's so nice when things start to 'click': spliceEffect :: Monad m =&gt; Producer a m r -&gt; (a -&gt; Effect' m b) -&gt; Producer b m r spliceEffect p e = for p $ yield &lt;=&lt; e
More two cents. Julia looks like it's awesome for data analysis. Go looks like it's awesome for backend web. Haskell looks like it's awesome for functional programming. X looks like it's awesome awesome for Y. Point being, Python is awesome for data analysis, web, X, Y, Z and Q. It's a very broad language that spans many many, many domains. Very flexible. 
Not exactly relevant but I just found [this link](http://www.breck-mckye.com/blog/2016/04/monads-explained-quickly/) that describes monads completely incorrectly. It really saddens me to see beginners thinking they had achieved realisation when they in fact had not, and went on to spread misinformation. 
Thank you, I have been looking for something like this for a long time. 
What an intriguing question! Since I'm not an expert in those other fields, I can't answer it directly, but maybe we can figure out a useful pattern. So why are there much more javascript and php tutorials than Haskell tutorials? Is it because javascript and php are easier and therefore better suited for beginners? Well, maybe, but I personally write about Haskell because I'm passionate about it and I want to share my passion, not because I think people on the internet will find it easier to read posts about Haskell than posts about other topics. I think there are more javascript and php tutorials because there are simply more javascript and php programmers, who are passionate about javascript and php, and who want to share their passion. All right, so why are there much more JavaScript and php programmers than Haskell programmers? Well one reason is that there are more JavaScript and php tutorials, so there is a larger influx of programmers towards those languages. Okay, so popularity is a self-perpetuating property, which need not be correlated with other properties such as those you are looking for. Google page rank works by popularity, so that explains why googling for tutorials doesn't give you what you want. Let's ignore popularity and focus on the properties you want. You say that "learn Haskell" is a good shortcut because Haskell is much closer to programming language theory than php is. Well that's certainly true. But what is it a shortcut to? It's a shortcut towards programming language theory. It is *not* a shortcut towards learning how to program. For that purpose, a JavaScript tutorial would do just fine. So the problem isn't that google gives you bad results, it's that you didn't google for the thing you truly wanted! Maybe you simply didn't know what you truly wanted at the time. Okay, so what is it you truly want? Not everybody who google for "how to learn to program" are secretly looking for the theory behind programming languages. Maybe they have recently been replaced by a very short perl script, and so what they're really looking for is a job which cannot be automated away. Or maybe they've always wanted to make games, and they're trying to learn programming in order to make one, but they don't realize that there are now tools for making simple games without having to program an engine from scratch. Etc., etc. So maybe you need to think a bit harder about what it is you truly want to know about AI, math, etc., and google for that instead. But you didn't ask what's the equivalent of programming language theory for AI, you asked what's the equivalent of "learn Haskell" for AI. Maybe you've tried looking for "programming language theory" and it was way too abstract, and you've tried looking for "how to learn programming" and it was way too removed from PLT, and you've finally found the intermediate search query, "learn Haskell", which hits the sweet spot between the two extremes. In order to figure out what this sweet spot might be in another domain, let's investigate the current sweet spot a bit further. What's special about Haskell? Haskell is certainly closer to the theory than php is, but there are also more obscure languages which are even more academic. I think what distinguishes Haskell is that while it is still a fringe language when compared to behemoths like JavaScript and C++, most academic languages are themselves fringe when compared to Haskell, because Haskell is the most popular of the academic languages. And that means it has a moderately large community of users who are passionate about the theory, and so by learning about Haskell and hanging out in that community, you learn a lot about said theory. Now we're getting somewhere. What is the most popular of the fringe AI communities? Where can you find a group of people who are passionate about the theory, and are applying it to a particular domain? Oh my. I thought I wasn't expert enough in the field to recommend anything, but I think I know just the place! Check out [Eliezer Yudkowsky](http://www.yudkowsky.net/), especially his AI related [Sequences](https://wiki.lesswrong.com/wiki/Sequences). He has fringe ideas like believing in Cryonics and the Singularity, yet he is super dedicated to math and the theory behind AI, and he has quite a following, so Less Wrong and its sister sites might very well be the largest theory-friendly fringe group about AI, the AI equivalent of Haskell. Or maybe not, I'm not that familiar with the field :)
Woah! Thanks mister!
Ah yes, I see. Variables declared within the let statement could not be visible from within the inline bind. This is actually true of case and lambdas, as well: do { map (\x -&gt; f (&lt;- x)) actions } This should definitely NOT magically expand to perform multiple actions. (Going down that road too far just gets you to `(&lt;- x)` as syntactic sugar for `unsafePerformIO`, which no one wants.) The easy answer is to make a rule that limits scope inside an inline do bind to the scope at the top level of the enclosing `do` block immediately before this statement. Otherwise, you have to ban this syntax for `let`, `case`, lambdas... and maybe more that I'm forgetting.
&gt; I think it's fine to require an import for dealing with binary data. Strings pop up everywhere while binary data is something of a specialized use case. Yes, that was my reasoning. We don't need to put everything useful into the `Prelude` but we do need to put everything that users *must* use (and remove everything they mustn't).
It feels like past a certain point of popularity, finding help is about equally easy. There's no meaningful difference in resources between Java, JavaScript and Python, for example. I don't know exactly where Haskell stands, but I think it's close or past that boundary. There are enough static resources out there and, perhaps more importantly, the community is big enough that you're virtually guaranteed a prompt answer on StackOverflow, here, on the mailing list, on the IRC channel... etc. The advantages of popularity scale *distinctly* sublinearly.
The transformation should be fixed and not specified by the author. This could be achieved with an extra keyword: {-# LANGUAGE AsAliasing #-} import qualifiedS Data.Map (Map, insert, null, empty) as Map import qualifiedS Data.Set (Set, insert, null, empty) as Set -- ... addax :: SetSet Char -&gt; MapMap Char (SetSet Char) -&gt; MapMap Char (SetSet Char) addax = insertMap 'a' . insertSet 'x' The keyword `qualifiedS` means qualified with suffix. The qualifier specified after `as` is appended as a suffix to the function. This is inspired by `mapM` or `filterM`, etc. This way there is no burden on the author to chose a consistent names because they are chosen in a canonical form by the compiler. In the example above the types should consequently be MapMap and SetSet, which is a bit ugly. Yet another keyword could help to avoid this. E.g.: import qualifiedS Data.Map (unqualified Map, insert, null, empty) as Map import qualifiedS Data.Set (unqualified Set, insert, null, empty) as Set --... addax :: Set Char -&gt; Map Char (Set Char) -&gt;Map Char (Set Char) addax = insertMap 'a' . insertSet 'x' 
How can you violate referential transparency if it's compile ? 
One way we might consider doing that is too add rewrite rules that replace String with Text. Text offers an interface that is as similar to String as possible. But it's not similar enough for this to actually work. I think the right thing there is a streaming library. Haskell has several super awesome streaming libraries. Over the years, the community has discovered when lazy is appropriate and where it has problems. And it turns out lazy and IO don't work well together. The Pipes library defines a streaming bytestring (I know that's not text), as does the Streaming library. 
Stupid newbie question: Can't you replace the Prelude?
This has been attempted in the past. For example Data.Sequence. These things are not drop in replacements. For one thing, every function that operates on lists would have to work on it. Even if that could be made to work, it would change the performance characteristics of existing code, and in some cases dramatically for the worse. We just need to use a different type for strings.
I was just picking up on OPs proposal to avoid the duplicate meaning of the point in the qualified name and in function composition. I agree that all the new keywords in the import definitions are ugly. I think the suffix qualification is nice because you can first see what happens and then further determine which instance of the action is happening. It's like when you use `filterM`. First you recognize that you filter something. In 99% of the cases it's irrelevant that the filtering is monadic, so the eye will just move on. In the one percent of cases where this information matters it's there in the suffix.
...but a language extension doesn't (and shouldn't) eliminate old meanings, so this proposed extension wouldn't avoid the two different meanings of `.`. So not only is the extension more clunky than the current way, it also doesn't solve the problem it's intended to solve. You could propose a language modification rather than an extension, but I thinking changing the lexical syntax of the language in a non-backwards-compatible way would rightly require significantly more payoff than what is going to come of this -- especially considering the significant migration work it would entail.
One advice, if you already bought haskellbook you can safely drop other resources, especially lyah. They will just get in they way.
What do you mean? Yes, you can replace the Prelude. That's what this is. A replacement Prelude.
Certainly everything that the Prelude depends on should itself be in the Prelude. I think there's a case to be made that only file I/O operations that deal with `Text` should be in the Prelude. Reading and writing binary is a somewhat more sophisticated use case and could be left to separate libraries.
Was just a thought in response to memory and file size bloat.
https://hackage.haskell.org/package/protolude
Yeah, I have no idea how the language extensions work. If the introduction of the keyword `qualifiedS` would break existing code it would definitely be not worth it.
They seem all warranted, if not clumsy to explain. (String is straightforward) In a Just-*er* world, I'd have various foolproof tooling making their use clear and obvious for all.
isn't it ugly in an otherwise concise language ...
it looks so nice... I did not closely look at it yet, but it reminds me of fseye for fsharp, to which you can send values and visually inspect them 
it would be a sin to use automagic for a program, of course
There has been a ton of progress in relational query compilation in the last few years, and many of the new systems are much easier to implement than a full System R -style compiler. Check out eg: * [Examples of problems which cannot be efficiently solved with only pairwise joins](http://arxiv.org/abs/1310.3314) * [An evaluation algorithm for query language which can efficiently express relational algebra and many other interesting problems](https://www.researchgate.net/profile/Mahmoud_Abo_Khamis/publication/275054936_sf_FAQ_Questions_Asked_Frequently/links/5551504408ae93634eca0983.pdf) * [A relational query compiler which outperforms even most C++ graph engines](http://arxiv.org/abs/1503.02368) * [An elegant but patent encumbered evaluation algorithm for relational algebra](http://arxiv.org/abs/1210.0481) * [A useful framework for thinking about joins](http://arxiv.org/abs/1404.0703)
And in hbc we did some of these tricks, but they don't generalize very well. We also did runtime reassociation of (++). 
This is the best answer in Reddit. Thank you very much.
Don't rely on the internet to provide all your learning material. I know it's hard to believe, but the best place to learn something (short of in-person instruction) is still a well written book by an expert.
oh, you are right. thanks :)
Similar to problem with the conditional that u/cdsmith pointed out, there are also problem with functions that take monadic (or applicative) actions as their arguments: do when (1 &gt; 2) $ putStrLn (&lt;- getLine) and do replicateM_ (2 - 2) $ putStrLn (&lt;- getLine) Unlike the case of the conditional which is a special construct in the language, `when` and `replicateM_` are ordinary functions. The compiler would not be able to generate warning/errors for all of those.
&gt;  want to make like map f xs using foldl function  have made with foldr but  ddnt do with foldl map' f xs = foldr (\x acc -&gt; f x : acc) [] xs  want to do this with foldl I received the private message above from you, were you trying to reply to my other comment? To keep conversation threads legible in reddit, please use the "reply" button under the comment of the person you want to reply to. So, you want to reimplement `map`. Why? And once again, what have you tried so far? In conjunction with your question about symmetric binary numbers, it sounds like those might both be homework questions? If so, maybe it would make more sense for you to ask your questions to the prof or the teaching assistant? They know better than us what material you've already seen and which insight solving those problems is supposed to give you. I don't want to spoil it by mistake :)
Only trouble is you'd have to also supply the type signature (in many cases) since it wouldn't necessarily be inferred to be as abstract as the imported one. But perhaps that's ok for a function only used in one module.
Why cannot link-time optimization help?
For `AsAliasing` I'm surprised I've never really wanted that or missed it from my work in Python (which supports that idea). However, I think in the types of cases that we're thinking of here, Python doesn't really support it either. If we were using Python, `insert`, `null`, and `empty` would be class members of a `Map` class. You can't rename class members on import!
I am not very certain, but wouldn't the type be inferred to be as general as needed within the module? Then it would only be a problem if the function were exported.
Datalog is a query language for relational databases. It can be expressed using relational algebra and practical implementations add aggregation etc and end up being roughly equivalent to SQL. The only place that datalog differs substantially from SQL is that it defines semantics for recursive / cyclic views (and if you additionally add arithmetic it becomes Turing complete).
`do` notation is isomorphic to Monads, so you are really just turning functions into magic syntax with changing much else. Also, what's wrong with `&gt;&gt;`? It's not a bad name.
Relational algebra doesn't handle recursion, yes, that's the main point. Datalog starts out with it and the question every extension has to answer is "can we still be recursive", which is e.g. why datalog+negation prohibits recursion through negation, there's invariants aggregate functions have to preserve etc. Datalog with standard extensions is merely PTIME-complete, though you can get it up to EXPTIME. As primarily a database language people care a lot about being to compile the thing very efficiently, so going Turing complete is not advisable.
Although I do think a lot of the language extensions are confusing, I don't think the functions are. Haskell has a very good documentation system, you can just look up the source for `fmap` and `map` when you need to.
Woah, that's pretty much what runs through my head.
I think the fact that you need to choose one and it isn't obvious which is enough to keep it out
No it is not.
I suppose it probably can, but the `Prelude` module itself isn't very large (it's just a load of re-exports) so it won't do much. It'll nowhere near compensate for the extra package included.
This is a really good question! Something interesting is how people get to Haskell. It seems for a lot of people they don't get to it until they need it. Until you're trying to do something really tricky, or impossible in another language, or until you need something like it. It's not exactly a popular language, and that's part of the problem isn't it... the "core" of any subject isn't going to particularly be popular. But I suppose this problem can be answered by looking at the core of programming... you need to understand what it is to answer this, and that's a very difficult question... A lot of people when asked this will say something like "telling a computer what to do". That's no doubt *part* of the answer, but I find more and more that a lot of the time it's about communicating to myself and other people the intent of some relationships between modeled elements of some kind. This is very similar to the goal of mathematics. It's a form of communication that is designed to get to some form of answer about something. Programming is still - in a way - discovering what it is itself. Is logic programming the end goal for programming? That is, simply finding a way to describe to the computer the kind of answer you want, and to have it provide it? (Even if that answer is actually a piece of functional programming - an ongoing answer as it were). If you take something totally different, say, gymnastics, and you want to know what the core of that is, having done some traversal in Haskell (or any topic), you can actually apply this meta-knowledge to other things! In other words, you can use your *common sense* and *intelligence* that you've created and built in other topics that you *do* know and apply that to others areas. Always be looking. Even in Haskell! The idea is not to throw out Object Oriented Programming, but to see it for what it is. Always see more. See the commonalities and differences. If you can see what's going on, then you can make better informed decisions. Programming in more popular languages is "easier" because those languages are *just* good enough to do what people need to do today, not tomorrow (yet). This should give you a hint - find the places that aren't common practice and that are weird and obscure and look at them, and see if they're pushing the envelope... for example, googling for "cutting edge programming languages" yields Haskell quite a bit. Usually you can tell when you're "on to something" because it will be pushing you out of your comfort zone into your learning zone, or maybe even a little into your pain zone. This is possibly a key indicator that you're being stretched and that it's something worth learning. (shameless plug for our haskell tutorial for anyone reading this): http://happylearnhaskelltutorial.com 
Really? Is it that good? How do you feel about moving from HaskellBook to [Thompson's](http://www.amazon.com/Haskell-Functional-Programming-International-Computer/dp/0201882957)?
So in order to use this, do I have to put: import Protolude at the top of every file?
There's an even deeper issue, char are Unicode code points, not necessarily the textual grapheme clusters we westerners optimistically presume them to be! Haskell doesn't have any mature libs for working with Unicode grapheme clusters, let alone sequences thereof ;)
Yep, that's true. So not so bad after all.
I honestly think you don't need to think of a complement resource until you finish haskellbook. It's very thorough.
&gt; ByteString is not and should not be thought of a string, not in 2016. This argument boils down to nothing more than terminology: an assumption that the word "string" is only proper to use for types that represent text. There's nothing wrong per se with that preference, but it's a *preference* and not universally shared. (Why is `ByteString` named like that, after all?) &gt; It is a collection of bytes that may or may not have a more nuanced semantic meaning. It needs to be parsed first. You could say ByteString approx-equals Maybe Text because all you know if you have a value of type ByteString is that it might be in the encoding you expect it to be. If you're going by that argument, `Text` is a collection of characters that may or may not have a more nuanced meaning, and say that `Text` approx-equals `Maybe Discourse`. I think a better observation is that both `ByteString` and `Text` are sequence types that, apart from the choice of element type and some domain-specific functions, are structurally little different from each other. You brought up parsing, so consider for example that there's both a [`Data.Attoparsec.ByteString`](https://hackage.haskell.org/package/attoparsec-0.13.0.1/docs/Data-Attoparsec-ByteString.html) and a [`Data.Attoparsec.Text`](https://hackage.haskell.org/package/attoparsec-0.13.0.1/docs/Data-Attoparsec-Text.html), which largely equivalent APIs, built on top of the same basic machinery.
I didn't realized you meant you were speaking of pattern matching. I though you were saying that the expression `three Three a b c` was unambiguous anywhere ...
Yeah and as a 3 year student in computer science (I have worked as freelance web-dev for 2 years before uni) student I really dont like the fact that they teach java as first language. IMO Python makes for a great first language because it lets you discover and learn from simple procedural model with just functions all the way to functional paradigm and although functional approach is not as elegant in python as in haskell or lisp it is certainly good enough to learn about it. Personally I would say that first language should be pure C because it would mould brains of people in certain way :) and give students some understanding of what is happening when they write their applications in a more high level language. 
I'm no expert and I don't have time to go through the whole thing, but here are some suggestions: * Your project organization could use some work. Check out stack. stack will generate a project skeleton for you and let you manage dependencies in a repeatable way. It's worth finding a tutorial. * On line 142, you list all coordinate pairs by hand. I would suggest avoiding enumerating these, since that will prevent you from scaling, but if you still want to, check out list comprehensions. In this case, something like: [(x, y) | x &lt;- [0..30], y &lt;- [0..30]] * I'm doubtful that an IORef is required for something this simple, and you should avoid it if it's not required. You should be able to keep your logic separate from IO. Try to only use IO where you need to mess with the display itself. See if you can come up with a good way to do this. Maybe just pass your board state around, or maybe use a State monad (more complicated). * You have some redundancy in your logic. For example, next_state_dead could just be "not . next_state_alive" * Your repeat' function is in the same as the library function replicateM_ in Control.Monad. You'll find some other cool stuff there as well. I hope that help a little bit, let me know if you have any questions
The example doesn't work very well. You have to write `sumOf' (1 :: Int) (2 :: Int) (3 :: Int) :: Int`, instead of `sumOf' 1 2 3 :: Int`. One way to make the second example work would be to make this change: -- previous instance can't be selected until the -- argument is known to be an Int instance (SumArgs r) =&gt; SumArgs (Int -&gt; r) where sumArgs is i = sumArgs (i:is) -- Written like this, the instance is chosen, then -- the argument is chosen to be Int instance (SumArgs r, Int ~ int) =&gt; SumArgs (int -&gt; r) where sumArgs is i = sumArgs (i:is) Instead of writing one class for every variadic function, you can write one class that does that transformation for the supplied function. HList has a HCurry class that does that. If I needed a sumOf' function I would have defined it like: {-# LANGUAGE NoMonomorphismRestriction #-} import Data.HList.CommonMain -- HList sumOf'' = hCurry (sum . hList2List) 
I believe this is nothing more than a combination of `Free` and `Ap` from `free`. That is, liftFreer :: f a -&gt; Free (Ap f) a liftFreer = liftF . liftAp with a general top level run function: run :: Monad g =&gt; (forall x. f x -&gt; g x) -&gt; Free (Ap f) a -&gt; g a run _ (Pure a) = pure a run phi (Free f) = runAp phi f &gt;&gt;= run phi
I highly recommend Atom-Haskell. I don't think Sublime Haskell is still well supported.
Exactly the goal! As I explained, the goal was to use the same derivation technique as `Freer` but with `Ap`, in order to get a free monad with Applicative effects. So the one difference between `Free (Ap f)` and this is that `(&lt;*&gt;)` is rewritten to use `Ap`.
Lokathor gave a great answer, I'll just add that if you crashed your machine you probably tried to do something infinite somewhere ;)
Anything free is relative to what you forget: Free -| Forget The usual free monad is relative to a forgetful functor that takes any monad `f` and remembers just that `f` is a functor, and which maps monad homomorphisms to mere natural transformations. Similarly `Ap` is a free applicative relative to a forgetful functor that takes an applicative down to its underlying functor, and operational is a free monad relative to a forgetful functor that takes a monad all the way down to a type constructor of kind `* -&gt; *`, not even a functor. On the other hand, you can easily define a 'free' construction that forgets that `f` is a monad and remembers that `f` is an applicative, then the free construction can know about the applicative for `f` and dispatch `(&lt;*&gt;)` through it accordingly. This is a bit tricky because unlike with `Functor`, the compatibility between `(&lt;*&gt;)` and `(&gt;&gt;=)` for the monad is less trivial, but **this** `Free` is the one you want to build such a `Free (Ap f)` on top of.
So you're saying there should be some `Free` that is only a monad over an Applicative (similar to how the classic `Free` is only a monad over a Functor), and that I should base `Free (Ap f)` on that? I imagine that factoring `Ap` out of what I wrote in the post would be exactly that, no?
Thanks. You've given me a better way to think about this.
It should have the benefit that you can then use it directly with applicatives without paying for all the `:&lt;*&gt;` stuff all the time.
If you feel up to bending your brain a bit, Comonads are a fantastic way to represent cellular automata. Comonads have the operations duplicate :: w a -&gt; w (w a), extract :: w a -&gt; a, and cobind :: w a -&gt; (w a -&gt; b) -&gt; w b. Cobind allows you to write a function that calculates only one part of you automata and turns it into one that updates the whole thing. The result is an implementation that can fit quite comfortably in a Reddit post.
Ah, okay. It is possible, although rare, for Haskell to use excess memory if something subtle is wrong. If you find that's happening, I'm sure someone here can help!
I really like SublimeText, it works so well. Ironically I can't actually get this working! Hsdev won't install!
If I were redesigning Haskell, I'd remove I/O stuff from Prelude. I don't mind import IO.File.Text and having that in the import list would be a clear indicator that "this module uses text-based file I/O." To import a bigger set of I/O stuff, you could do: import IO
In hbc, did you attempt to "chunkify" intermediate functions as well? Like having a rewrite rule for "words" that switches to a chunked version if both its inputs and outputs can be chunkified.
 foldl (++) [] This is a bad idea, you should use `foldr` for that, and `foldr (++) []` already exists and is called `concat`. There are 2 reasons that `foldl` is wrong: A) You get `O(N^2)` concatenation instead of `O(N)` because `(((a ++ b) ++ c) ..` has to copy increasingly large lists, whereas `a ++ (b ++ (...` only has to copy a single appended list each time. B) `foldl` has to traverse the entire list of lists before it applies the `++` even once. `foldr` immediately returns `++` of the first list and the rest of the sequence. This plays much more nicely with laziness and memory use. `concat (map f xs)`, by the way, can be replaced with `concatMap f xs`, if you're into that kind of thing. Same goes for `foldl1 (++)`. Don't use that, use `concat`. You use a lot of `IO`/imperative programming. You can replace much of it with a program structure like: states = iterate nextState initialState main = initialize &gt;&gt; mapM_ showState states which would move most logic out of `IO`.
That's a separate issue. The theory of denotational semantics describes how to deal with it. Without a built-in magical `seq`, that works fine together with eta-reduction, although you're right that it's not completely straightforward. Given the design choice that Haskell is a Turing-complete language, which certainly was a straightforward choice at the time, there is no way to avoid the existence of bottoms.
&gt; foldl (++) [] I don't know what I was thinking. &gt; You use a lot of IO/imperative programming. I know. Its hard to get rid of "imperative thinking".
Stack is no more obvious than cabal, except for beginners. We're not doing anything similar to what stack does for beginners, so that's really not relevant. There are those who claim that stack might be as powerful as cabal for complex production environments, and that would not surprise me. But even if so, it would definitely require significant work for us to migrate, and we just don't have the time for it right now. We will definitely be looking into it at some point in the future.
btw: why delete the question - chances are high that others get into the same trouble
Is this a mistake? &gt;(We will need {-# LANGUAGE FlexibleInstances #-} and {-# LANGUAGE FlexibleInstances #-}.)
The author said he wants to make a polyvariadic equivalent of `sumOf :: [Int] -&gt; Int`. Using the generic `[Int] -&gt; a` is a completely different beast. Changing the class definition to `sumArgs :: [Int] -&gt; Int` more closely represents the monovariadic equivalent of `sumOf :: [Int] -&gt; Int` and will also allow `sumOf 1 2 3 :: Int` to work. class SumArgs a where sumArgs :: [Int] -&gt; Int 
Here is a link to tutorial on School of Haskell about this (in case anyone wants to play with comonads): https://www.schoolofhaskell.com/user/edwardk/cellular-automata.
Too bad Atom doesn't have as good vim support, as far as I can find. That's probably the main thing keeping me away from it.
Because PEBKAC. I've undeleted because I will look at getting it running further just to help solve problems others might encounter. Sublime is a lovely editor (feels snappier than atom actually); and it'd be great to have lots of choice for one's haskell IDE. Was it you helping me on SO BTW? If so - thank you, and I've updated my comment to reflect the situation.
Atom does not even support non querty keyboard out of the box...
Yes! Thank you for noticing. It should be fixed now.
Try using Hlint, it gives great suggestions to simplify code. For example it found 10 or so redundant brackets in your code. Also gives some other hints (it found that you could use concat instead of fold (++) for example). If your not set on an envoironment Atom works well with it and supports stack: http://achernyak.me/universal-haskell-dev-enviornment Try using $ instead of overly long braced expressions, for a comparison: length (filter ((flip$elem) brd) (fromMaybe [] (lookup (x,y) nList))) length $ filter (flip elem brd) $ fromMaybe [] $ lookup (x,y) nList Although some people prefer brackets after all. Personally I find the $ style easier to write and read. Also lists work well enough for simple stuff but if you ever need more performance look at Maps and mutable Arrays for something like this. (In this case Arrays are better as we now the keys we will have and they are contiguous). 
It's a common misconception that SO exists so a person can get answers to their questions. The truth of the matter is that SO exists so that *other people* can get answers to their questions. If the person asking also get answers then that's a happy side effect.
Yeah, there are 128 `Weekday -&gt; Bool`, not 49.
How come? For every weekday you have 2 possible output: True | False. 2 * 7 = 14 possible input/output combinations
https://www.reddit.com/r/haskell/comments/18902u/the_algebra_of_algebraic_data_types_part_i/
No, it was just literals and input that got special treatment. 
[removed]
Yeah it served as inspiration :) My post has a smaller scope though, covering the absolute minimum to justify the word 'Algebra' and probably make someone go from WTF to Aha. Chris's posts are more full featured but they also assume a (slightly) higher level of intellect IMO. Update: Linked to Chris's post in conclusion.
Generally this is great stuff. By nature it's very opinionated about a few things, and I disagree with a few of those opinions, but I won't detail the disagreements. This is a great approach to building a custom Prelude, and it opens many possibilities. Since one of the guiding principles is stick to compatibility with the standard whenever possible, there can be many of these kinds of alternative Preludes and they can co-exist happily. One place where this Prelude strays from compatibility unnecessarily in my opinion are the `fooMay` functions. /u/ndm's safe library was very important when it first came out many years ago; it made people more aware of the pitfalls of partial functions. But nowadays base has plenty of great total functions and `Maybe` combinators. I prefer sticking to the standard and using those. For example: in modern Haskell, safe's `readMay` is spelled `listToMaybe`. But yes, please do get rid of the partial ones, good idea. And I would promote `maybeRead` to the Prelude.
There is a precedent in some other languages, such as Python, to have both text and binary string types in scope by default. I don't find `ByteString` to be uncommon at all.
I think the 'free monad given an applicative' would be good to include in the package, yes. Toss me a pull request.
So is there novelty in operational applicatives (forget `Applicative`; recall `* -&gt; *`)?
The same sort of novelty you get from an operational monad: You don't have to make the base 'functor' an actual functor. You can of course use `Ap (Coyoneda f)` to get it though.
I've had a super positive experience with sublime text and Haskell Dev. Also the current maintainers have done a lovely job and I've really appreciated how they've handled the tickets and attempted PRs I've filed :)
In the end it's easy to work around it with a package (just install it and it works) https://atom.io/packages/keyboard-localization It's a shame that it's still an issue and requires a package but in practice it has been a non issue for me. 
Cool! If there's semiring, how about semigroup or default?
GHC doesn't accept the class you're suggesting with this message The class method sumArgs mentions none of the type or kind variables of the class SumArgs a When checking the class method: sumArgs :: [Int] -&gt; Int In the class declaration for SumArgs Did you mean something else that's different from the class SumArgs a that's in the article?
Nice! Would it be possible to have a few example pictures in the github readme, to see what it looks like?
Pics or it didn't happen :D
Make sure this logo won't be a problem with trademarks since it's pretty obviously the PowerPoint logo with a different letter.
That's good idea! I'll do a video soon (probably tomorrow).
Haskell infers a number type when compiling. For the user python is similar. However it doesn't infer the type but uses operator overloading instead. Eg you multiply with a float the result is a float. Make your int large enough and I think it also converts to bignum and so on. Semantically the difference is big but for using it it's very similar.
Mostly because I forgot that we managed to optimize any calls to fmap on `f`. =)
Cool =) To be clear, `Ap` does forget `Applicative` and only recalls `* -&gt; *`, correct? Meaning it is essentially the operational applicative /u/doloto was looking for?
Maybe /r/haskellquestions/ might be a more appropriate place for this post? I'm not sure if the mods have a policy about this
you can avoid creating this intermediary `input` by fmaping over it input &lt;- readFile "input_2" let pairs = lines input you can just pairs &lt;- lines &lt;$&gt; readFile "input_2"
/u/davemenendez [pointed out](https://www.reddit.com/r/haskell/comments/4e0de9/applicative_effects_in_free_monads/d1xfait) that my implementation of `(&lt;*&gt;)` is incompatible with `ap`. Is this breaking any law? [Control.Applicative](https://hackage.haskell.org/package/base-4.8.2.0/docs/Control-Applicative.html) doesn't state that it's a law; it reads more like a strong suggestion due to convenience of satisfying the Applicative laws for free if `pure = return` and `(&lt;*&gt;) = ap`.
Awesome. Thanks
You need `(&lt;*&gt;) = ap` to hold up to whatever quotienting you're doing of the behavior. The trick is finding a way to define them compatibly, or to state laws users have to comply with such that they can't observe the difference.
Here's an asciicast: https://asciinema.org/a/41859
`pure = return`, `(&lt;*&gt;) = ap` were incurred as laws once `Applicative` properly became a superclass of `Monad`.
Sum types, product types: I have never seen any explaination for why they are named as they are. This article gives a very elegant and simple explaination. Well done and thanks. I watch Robert Harper lectures talking about how implication is like a "power operator" and even that makes a bit more sense now. because implication is a funtion (proof of A -&gt; proof of B), and as explained in this post a function is clearly a power operator. 
Yes, now to learn what operational does.
I haven't reviewed Haxl recently, but my understanding was that `&lt;*&gt;` is intended to return the same results as `ap`, just faster. From the [original paper](http://community.haskell.org/~simonmar/papers/haxl-icfp14.pdf): &gt; Its worth pondering on the implications of what we have done here. Arguably we broke the rules: while the Applicative laws do hold for Fetch, the documentation for Applicative also states that if a type is also a Monad, then its Applicative instance should satisfy pure = returnand&lt;*&gt; = ap.This is clearly not the case for our Applicative instance. But in some sense, our intentions are pure: the goal is for code written using Applicative to execute more efficiently, not for it to give a different answer than when written using Monad. &gt; Our justification for this Applicative instance is based on more than its literal definition. We intend dataFetch to have certain properties: it should not be observable to the programmer writing code using Fetch whether their dataFetch calls were performed concurrently or sequentially, or indeed in which order they were performed, the results should be the same. Therefore, dataFetch should not have any observable side-effectsall our requests must be read-only. To the user of Fetch it is as if the Applicative instance is the default &lt;*&gt; = ap, except that the code runs more efficiently, and for this to be the case we must restrict ourselves to read-only requests (although we return to this question and consider side-effects again in Section 9.3). In other words, `(&lt;*&gt;)` and `ap` may not be structurally equal, but they're equal up to the abstraction level of Haxl, which is acceptable. 
I'm using [Vintageous](http://guillermooo.bitbucket.org/Vintageous/). Installation was straightforward with Package Control.
Our [Nickle](http://nickle.org) programming language has an interesting numeric type system with separate static and dynamic subtyping. It seems to catch errors reliably while not getting too much in the way. That said, we are currently considering substantial modifications for increased safety and efficiency. 
Haxl breaks the laws if you consider the number of passes as part of the semantics they preserve. They deliberately do _not_ consider things this way, but instead quotient out the number of round-trips between Haxl and the external data sources from their semantics. So to them, a version using ap that takes 2 passes vs. a version using (&lt;*&gt;) that gets away with 1 pass yields the same answer, just the `(&lt;*&gt;)` version gets away with fewer trips to the server. If you allowed people to care about the number of passes then this would violate the laws, but they deliberately exclude such considerations. Haxl is a law-abiding Monad if and only if you quotient out concerns about the number of passes from the model.
That`s the correct answer, yes, 2^7 (128). Here is an explanation:http://math.stackexchange.com/questions/223240/how-many-distinct-functions-can-be-defined-from-set-a-to-b
This is clickbait material. First of all, this is about mobile apps. Then according to the source article to the linked article (!), they are *considering* offering first class support to Swift applications, not replace Java with Swift. I suppose the reason is to increase code sharing with Apple stuff, and make some media splash, which won't happen with Haskell. Anyway, I also cringe when I see things like "Swift is meant for speed and safety".
List being a recursive type, the algebraic counterpart will be recursive too. `List (a) = 1 + List (a)`. In terms of the number of values possible, they're infinite. You can make an arbitrarily long list even from unit `()` as elements and each list will correspond to a different value. Recursion makes this possible.
Atom does -- I found a plugin straight away. Think it's called `vim-mode`.
I really like ST3. I'd love to use it for my haskell dev, but the plugin doesn't seem to do a lot of what it promises (at least out of the box), having installed everything it asked of me. I'm going to try on a Win 7 box instead, see how that fairs.
Vintageous is what I've always used in ST3 and it's excellent.
ST3 is bitrot? Seems faily well maintained and it's never been buggy for me. Build 3103 was released on the 9th February 2016.
The difference is, I made no comment about which language to use first, or spoke about Haskell at all. If you notice, my other comment asks questions, which brings the person to their own decision. Bias? Sure, but expressing my bias as the truth? Hmm.
Um, Lua has only Doubles, but that's not really what you mean right?
Yes, I do agree, but judging the overall quality of an entire range of items from each of the things I listed is much easier said than done
I agree it is safer, but disagree that it is "meant for safety"!
It's actually `List (a) = 1 + a * List (a)` but it gets even better. List(a) = 1 + a* List(a) List(a)(1 - a) = 1 List(a) = 1 / (1 - a) Taylor series of which is List(a) = 1 + a + a*a + a*a*a + ... Which exactly means "a tuple of no elements, or a tuple of one element, or a tuple of two elements..." which is what a list is! It's missing infinite lists though but it'd be amazing if something wasn't lost in this series of steps.
&gt; data Free f where should be `Free f a` maybe?
Oh yeah for sure :) I just wanted to put something up quickly so MrPopinjay could see it in action!
I'm saying that `do`-notation is just another way of using Monads. The Monads are still there. Also `IO` is already magic, so `&gt;&gt;` applied to `IO` isn't really any extra magic.
Ha, nice! This resolves [hoverpoint#3](https://github.com/fokot/howerpoint/issues/3)
People excited by swift have just never been exposed to anything good, I think. I always describe this class of languages (swift, scala, etc) as "crippled OCaML"
Wait until you discover [automatic differentiation](https://hackage.haskell.org/package/ad), and the fact that it can be used as and with `Num` and `Floating`.
It's meant for safety, from the perspective of people whose baseline is Objective-C. The issues with safety in Swift aren't major issues as far as the Swift team is concerned.
Honestly, the idea behind ARC is pretty cool. I've wondered about the idea of using ARC as the basis for the first generation in a generational garbage collector. That is, let ARC do the marking, and G1 can just worry about sweeping. Then G2 can start to worry about a full-fledged mark-sweep approach to clean up cyclic references. With G2 being low priority, you should end up with a pretty dang good garbage collector. Then I learned this is how Python does it (or maybe it was just CPython). Haven't done much research into it though, so I don't know how it stacks up to a traditional generational collector.
I'm not sure whether you really need it, but you could think about adding units via [dimensional](https://hackage.haskell.org/package/dimensional).
I started with Objective-C. When Swift came out, I was starting to get into Haskell. As someone who more experience in the land of OOP than anything else, Swift is a godsend if you don't want to go head-first into functional programming. It's crippled compared to anything great like Haskell when it comes to Haskell-like features. But when it comes to OOP-like features, it's among the best in class (no pun intended). The merging of OOP with functional programming and a higher level type system makes Swift a great OOP language. It's just a shit functional language compared to the likes of Haskell.