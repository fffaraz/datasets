The problem with `Ord` is that even if you made something like `Showable` it is not true that you can grab any two elements belonging to a type in `Ord` and compare them (how would you define `compare :: (Ord a, Ord b) =&gt; a -&gt; b -&gt; Ordering`?).
Posted this here: http://h2.jaguarpaw.co.uk/posts/strictness-in-types/
Yeah, but that's no excuse for the hostility. Politeness goes a long way. Anyway, I've counter-upvoted, hth ;)
Are you maybe suffering from the x-y problem? It might be easier for us to help if we knew what you were intending to do in the macro sense rather than the micro sense. As for this specific instance, I'm not sure this is exactly what you want, but you could change `Ord o =&gt; Bool -&gt; Test -&gt; o` to `Bool -&gt; Test -&gt; Either Int double` like so: f :: Bool -&gt; Test -&gt; Either Int Double f guard = if guard then Left . f1 else Right . f2
A down vote is *supposed* to indicate "wrong venue". 
If such questions get friendly, high quality answers, then does it really matter if they end up being down voted?
You're right in technicality, but I feel he is voting correctly in this instance. He considers newbie questions to be "off topic" or "does not contribute."
Playing devil's advocate here. Being flooded with newbie questions is not a good thing for the old timers who come to the sub for news about Haskell. Newbie questions are the opposite of news, they are the same old content rehashed over and over.
Yes. Being downvoted is frightening for a lot of people. They think their question is unwelcome so they don't come back with more.
From the downvote guidelines you quoted: &gt;...take a moment to ensure you're downvoting someone because they are not contributing to the community dialogue or discussion If "the community dialogue or discussion" is going to be about articles, and a post is a question, then it doesn't contribute. It depends on what the dialogue is about, which depends on what users view as the dialogue as about. This is that feedback loop in action. Downvoting based on what you perceive the community to be focused on doesn't appear to be a breach of reddiquette to me.
Something I've noticed in /r/learnmath is that sometimes a question gets a couple upvotes, then gets some answers, then goes down to 0 points (net upvotes, or whatever reddit uses). I even do it sometimes. If there's no satisfactory answer yet, I either write one myself, or upvote. If there are plenty of good answers already, I'll downvote to let another question rise above it. I think it works because there's a good volume of questions. Especially during the academic year and really heavily during exam season. I think the bigger concerns are: - Are the questions being downvoted without being answered? If they have good answers, I think we're fine. - Do we want this sub to be about questions? There are other good suggestions about trying to direct more people into /r/haskellquestions.
I suspect you're thinking of `Ord o =&gt; [. . .] -&gt; o` in this context as meaning "give me something I can use like an `Ord`." As others have pointed out, that's not what your signature for `f` means. Here's a definition that will type-check, though: f :: (forall o. Ord o =&gt; o -&gt; a) -&gt; Bool -&gt; Test -&gt; a f process True (Test a _) = process a f process False (Test _ b) = process b You need `{-# LANGUAGE RankNTypes #-}` for this. It's kind of an inside-out version of your function, but has a similar sense. You give it a function, `process`, that must operate on any `Ord`. From `process`'s perspective, you have the desired meaning: it gets as its parameter "something that you can use like an `Ord`," without knowing anything else about the type. The problem in this example is that you can't actually write any useful function of type `forall o. Ord o =&gt; o -&gt; a`. If you unwrapped two `Test`s at once, you could do something. Well, one thing. (Note: the type of `compare` is `forall o. Ord o =&gt; o -&gt; o -&gt; Ordering`.) But this technique might have application in other contexts.
It's just the standard [reddiquette](http://www.reddit.com/wiki/reddiquette). &gt; **[Please Do] Vote**. If you think something contributes to conversation, upvote it. If you think it does not contribute to the subreddit it is posted in or is off-topic in a particular community, downvote it.
Picking out outfits takes too much time an energy. Besides I honestly couldn't care less about "fashion," brand names or any of that. Just go in my closet grab black shirt black slacks and shoes and be off for my day.
Woohoo, I applied. Can't say I'm an ideal candidate, but upvoting this post anyway because we need more Haskell jobs!
I agree. As you've stated previously it is impossible to remove the emotional aspect of the system. I don't think that goes away either, especially when you respect your peers. Which, as a basic principle, I think has been cultivated in this community. Lastly, we cannot run away from the fact that Haskell as a language has a reputation. One that everyone who has learned to some degree is at least a little proud of. I know that's why I started to learn Haskell years ago, because it was supposed to be one of the most difficult (that was actually usable) languages, and it changed your mind about programming [for the better].
That is pretty interesting, I made a mental note to check out that sub from a suggestion earlier. I am even more intrigued, I am curious to see this work.
Couldn't someone just point it out that it's the wrong venue, explaining why? The rest of us can upvote them to express agreement. The text you quote doesn't settle the matter in any case: the first sentence certainly has to do with a 'conversation' on a particular page -- what is the conversation the pattern match query wasn't contributing to? How is it 'off topic'? The difficulty with the pattern match question, if there is one, is different from anything specified in the rule for voting. 
Looks useful, thanks! I can think of a couple times I would have found this sort of thing handy. A couple small comments: - There's no real reason to have orphans for datatypes you define yourself -- you might want to move the JSON deriving into `Hoobuddy.hs`. If nothing else, it will save you some language extensions. - I heartily recommend `optparse-applicative` or similar. - There's a few different styles of option handling here, including a few calls to `sysExit`. I'd find the logic easier to follow if there was a consistent error-handling style, perhaps using `Either` or `ErrorT`.
Yes, Reddit includes the ability to downvote solely so people can be mean to each other; downvotes communicate nothing else.
Hackage Trustees are allowed to edit the cabal files, right? I suppose we could have a trustee janitorial staff that was responsible for things like this. Or, a debtags style system to gradually replace categories entirely?
When HWN rolls around, it reminds me to (re)visit link discussions to make sure I've seen all the good comments, so I like seeing it.
By the way, this is equivalent to existential quantification. This is how you encode existential quantification using universal quantification.
I think you may have dropped these: /s /s /s /s /s /s
&gt; Couldn't someone just point it out that it's the wrong venue, explaining why? They could. Or they could downvote it. Similarly, someone could point out the community-wide value in discussing this particular question. Or they could upvote it. Obviously there is value in articulating one's point, but the raw voting power is quick and easy, and has a mechanical effect on which content is displayed. People influence what kind of content they want to see by the way they vote. &gt; If you think it... is off-topic in a particular community, downvote it. This "rule" for voting isn't a rule. It's more of a guideline. And I think it is perfectly applicable.
/r/science could be considered success story for the second method from what I can tell.
I know I have ignored sidebars when starting to read a new subreddit so something like a sticky post makes sense to me. [kqr just point out](http://www.reddit.com/r/haskell/comments/2g3em9/meta_whats_with_all_these_downvotes_on_beginner/ckfhj95) how some communities are sucessfully enforce a standard on their existing subreddit rather then creating a second reddit for blog/articles. /r/science is a success story for this method as far as I can tell. One thing they do is post a note in their submit page &gt; You are submitting a link. The key to a successful submission is interesting content and a descriptive title. "Only submit recent peer-reviewed scientific journal articles or summaries of them. If your link is scientific in nature but not peer-reviewed, feel free to post it in our sister subreddit /r/EverythingScience." http://www.reddit.com/r/science/submit That should get everyones attention with out putting a sticky at the top of the page.
I did not. TIL. I'm not quite sure that'd work for me, but I will try it.
Using the abomination called Hamlet?
&gt;Not searchable, hard to browse threads, etc, etc. Have you tried using the google groups interface? It is searchable, it is threaded nicely(no nested threads like on reddit discussion), has keyboard shortcuts, etc. Haskell-cafe: https://groups.google.com/forum/#!forum/haskell-cafe
You know what. I do not remember ever downvoting a submission on /r/haskell Even from Rob Harper :)) But perhaps some submissions are simply not upvoted which leads for them quickly falling behind. 
Maybe that just means that there isn't a consensus on what does and doesn't contribute. Which is fine. The meaning and reasoning behind peoples votes is highly subjective and a personal. What the aggregate of the votes indicate is on-topic or not is going to change over time. With that in mind, trying to nail down something that the community is currently divided on into content in the sidebar seems like a misstep to me. I'd be OK with something being added to the sidebar along the lines of "Don't be discouraged by downvoted beginner questions, and here are some other places you can ask questions:...". I'd be strongly behind something like the Rust code of conduct, but that's probably a post for another time :)
This looks pretty neat. Does it handle discrete distributions? 
HSpec all the way! Test auto discovery is the coolest thing since sliced bread. 
No, maybe I was thinking wrongly. From a hardware+software HPC cluster design perspective. In which case the back end engineering will not even know the various hardware options that are being positioned. I now see that this more narrower, focusing on the software part. Nice.
[look over here](http://www.reddit.com/r/haskell/comments/2g4jsz/is_there_any_way_to_get_this_to_type_check/ckfllka)
Just to spell it out: {-# LANGUAGE RankNTypes #-} {-# LANGUAGE ExistentialQuantification #-} module Ordered where data Test = Test Int Double f1 :: Test -&gt; Int f1 (Test a _) = a f2 :: Test -&gt; Double f2 (Test _ b) = b f :: Bool -&gt; Test -&gt; Ordered f guard = if guard then Ordered . f1 else Ordered . f2 data Ordered = forall a. Ord a =&gt; Ordered { getOrdered :: a } Although it's worth noting there's very little you can do with `forall a. Ord a =&gt; a -&gt; s`.
Does "[being an antipattern](http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/)" count as a use case? ;)
Hi fellow Haskellers. Years and years ago I was one of the very early employees at the National Center for Supercomputing Applications (in Urbana, Illinois, USA), and the software we used to program those Cray supercomputers was so primitive. We are truly excited to be helping to bringing Haskell into the commercial HPC world. If you have a non Haskeller friend who has strong HPC or distributed-systems software architecture background, please free to encourage them to apply too.
No doubt up- and down-voting can influence the kind of content that appears on a subreddit; it does this by the impression it makes on regular users and cognoscenti. The people in question here, though, by hypothesis know little or nothing about the community and, if `kqr` is right, are about to leave it anyway having been down-voted, rather than e.g. simply ignored. 
Check out http://hackage.haskell.org/package/permute Also note that existential quantification can always be faked if you have RankNTypes
How about [SomeException](http://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Exception-Base.html#t:SomeException)?
What I'm gathering from a couple examples (predominantly yours and /u/MitchellSalad) is that it's particularly useful for situations where you want to return a typeclass, but cannot write a general function to do so. Is that the general idea?
Ooh, I like this!
I don't know how many Reddit users are intimate with reddiquette, but my guess is "not 100%". As we grow, we'll increasingly see more people who aren't aware of reddiquette. Putting a reminder on hovering over the value isn't a bad thing.
... the example IS a discrete distribution I thought. 
It’s not about faking, exactly. Every rank-n existential can be expressed as a rank-(n − 1) universal and vice versa: &gt; (∃a. f a) → b ⇔ ∀a. (f a → b) I think this theorem is due to Skolem but I’m not sure. 
Afaik HaLVM is running on Xen. So every Xen provider should work.
We should call those programmers with a high level of Haskell - fu "Tricky Dicts"
Existentials are particularly useful in conjunction with GADTs/dependent types, if you don't know statically what the type index of the result will be: for example, consider the filter function on vectors: {-# LANGUAGE DataKinds, GADTs, PolyKinds #-} data Nat where Zero :: Nat Suc :: Nat -&gt; Nat data Vec a n where Nil :: Vec a Zero Cons :: a -&gt; Vec a n -&gt; Vec a (Suc n) data Ex f where Ex :: f x -&gt; Ex f vfilter :: (a -&gt; Bool) -&gt; Vec a n -&gt; Ex (Vec a) vfilter p Nil = Ex Nil vfilter p (Cons x xs) | p x = case vfilter p xs of Ex rs -&gt; Ex (Cons x rs) | otherwise = vfilter p xs 
What is the story pre-7.8? I believe it was already possible to get some of kind stack trace with prof builds before: is that -xc?
What, and do two jobs? 
Do you have any specific issues with Hamlet? I was, too, initially doubtfull because of `$xxx this &lt;- that` syntax for logic, but it is very concise and i've found i enjoy using it even for static-ish sites and design mockups.
If you want user extensible types and their types should be mixable (e.g. included in the same list) with the internal ones and you want to do some more fancy stuff with the types, like serialize them, then the solution using existential quantification seems to be more flexible than the often proposed record of functions. Also the record of functions solution can be a bit annoying if you want to hold user specific data, because you have to hold this data in a closure and each function modifying the data has to create a new closure. 
You can use it to create heterogeneous collections. import qualified Data.Set as Set -- Cmp (key) (value) (a function that does something useful) data Exists = forall a. Cmp Int a (a -&gt; Double) instance Eq Exists where (Cmp key1 _ _) == (Cmp key2 _ _) = key1 == key2 instance Ord Exists where (Cmp key1 _ _) &lt;= (Cmp key2 _ _) = key1 &lt;= key2 s0 = Set.empty s1 = Set.insert (Cmp 7 6.0 (+1.0)) s0 s2 = Set.insert (Cmp 3 3 fromIntegral) s1 s3 = Set.insert (Cmp 1 "1" read) s2 s4 = Set.insert (Cmp 5 () (const 5.0)) s3 s5 = Set.delete (Cmp 3 undefined undefined) s4 -- prints "[1.0,5.0,7.0]" main = print $ map (\(Cmp _ v f) -&gt; f v) (Set.elems s5) When we look at an element from the set, the only non-trivial thing we can do with its value is applying the associated function (because that's the only function whose type is known to match). Idea from /u/oerjan's [comment](http://www.reddit.com/r/haskell/comments/29v6cm/heterogeneous_set_is_unsafecoerce_safe_here/cioy2u6).
This requires the full source code of the program to be available at compile time and compiled all at once.
Yep, just imagine those self-optimizing programs speaking in Morte to each other and planning to kill humans
I propose to make all coercions be identity functions in the intermediate type system, and then make the optimizer clever enough to replace an fmap of the identity function with just the identity function.
I don't think [this guy's](http://familyguy.wikia.com/wiki/Mort_Goldman) too scary.
Yeah, this is why I discuss this at length in the "Runtime computation" section of my post. I'd like to be able to use laws to simplify black box computations, but I don't yet know a decidable approach to do so.
Anyone know why it's called an embedded language, and how this distinguishes it from a library?
Thanks for helping me clarify this. Assume the underlying hardware infrastructure is a cloud such as AWS, Rackspace, or VMWare ESX on an existing data center. 
I used to work part time for other company remotely about 10 years ago. I could not do it for long. I was burned out in 2-3 years. Had to quit. Helped them to find a full time programmer to replace me. 
That wouldn't help much because trustees can't edit the cabal file in the project's original source control repo, so any changes they made would be overwritten the next time a new version is uploaded. The best we could hope for here is for hackage to do some transparent conversion in how it displays them.
 kill :: Human -&gt; IO Knife Note: The kill function will use the lens library in some way. Why? Because the lens functions work `forall s t a b`s.
Then, perhaps a system for trustees to do category aliases ala stack overflow tag aliases? Only categories that aren't aliases would get links, but the aliases would be listed in light-text near the category header. All packages in the category or any of its aliases would be listed underneath. Maybe I'll get time this weekend to look through the hackage2 source and see how reasonable this would be.
Frankly, I like your line. I don't know where you need to be on the continuum of technically correct vs sounds good, but what you have certainly sounds good and it's close enough to technically correct to satisfy me.
I'm on my phone, so I can't give a long answer yet, but I just want to say that I love all of these ideas. Edit: Okay, I can type now. I like the idea of marking things (both compile-time and run-time) by their runtime or compile-time footprint. That's really useful information to have in general, independent of optimization, for estimating the resource consumption of a program. I think non-recursive programs also make this sort of estimate feasible to implement, but I need to actually try it out first because I haven't given this as much thought as I should. The #1 thing I look for in a rewrite rule system is that it's predictable and plays well with other optimizations. While I think Haskell's rewrite rule system is okay, getting rewrite rules to fire reliably requires manually distorting `ghc`'s normal optimization heuristics (by explicitly disabling or enabling inlining in certain phases) and when rewrite rules fail it's very difficult to reason about why they failed. I've been toying around with the idea that perhaps free theorems could actually encode the information that we normally encode in rewrite rules. I often find that the few optimizations that Morte does not cover that I want almost always seem to be encodable as free theorems. However, I don't know a way to implement general-purpose free-theorem optimizations that doesn't involve exploring a huge optimization space. In general I try to move as much information to lambda calculus because normalization doesn't involve any sort of search or combinatorial optimization. For example, let's say that I have some opaque type that has laws X that I want to encode within lambda calculus. One way to make lambda calculus "aware" of the laws for that structure is to represent it using a "free X" (i.e. a free object that satisfies the laws X). A really good example of this is the encoding of `IO` as a free monad in the first Effects example from the post. This makes Morte "aware" of the monad laws for IO, which is why Morte fuses away all the binds. However, I'm not sure if this trick is sufficiently general because there are other free structures that I don't know how to encode in lambda calculus (like a free group).
I have been interested in dependent types lately, so I will probably use this to mess around a little :)
Cool. If I wasn't happy where I was I would seriously consider this. 
If you use two isomorphic representations (eg Text and a non infinite String) of a piece of data in two versions of a morte program, are they guaranteed to both compile to the same machine code? 
UPDATE: Thanks everyone for the feedback! On the second day I reached the goal of 10 T-shirts on http://teespring.com/purelyfunctional . I'm really happy and.. proud to know that people living so far away from me will wear something that I designed :)
No. That would break any backend because the backend would be expecting one data representation and you'd be handing it an isomorphic representation. You would need an explicit conversion function between any two isomorphic representations.
I go almost every year. Always a great time with great people.
So then is the promise that "if two programs are equal then they generate identical executables" not strictly true, or do I misunderstand the definition of "equal" here?
Morte's contract is that "equal" means "equal according to equational reasoning". For example, consider the following two Haskell types: type Type1 = (A, B) type Type2 = (B, A) Those two types are isomorphic, given the following isomorphisms: fw :: (A, B) -&gt; (B, A) fw (a, b) = (b, a) bw :: (B, A) -&gt; (A, B) bw (b, a) = (a, b) fw . bw = id bw . fw = id However, those two types are clearly not equal. There is no way I can take this value: (x :: A, y :: B) ... and prove via equational reasoning that it's equal to: (y :: B, x :: A) ... because they are not equal unless you convert between them explicitly using a function.
You can't make it decidable for non-trivial laws. 
Is there a formal proof of this? The reason I ask is that I'd like to see if there were some restricted subset of laws for which this might be decidable.
Well, since non-trivial doesn't have a formal definition it's hard to make a formal proof. :) It was more of an observation. 
One thing I've been exploring off and on is the use of equality saturation to deal with the non-confluent rewriting problem. http://www.cs.cornell.edu/~ross/publications/eqsat/ (Alec Heller likes to call it equality moistening since it doesn't always fully saturate the system of possible equalities) This is applicable even if you have recursive terms, may or may not inline, etc. Then it becomes a matter of writing a ton of optimizer rules, tracking the system of equationally equal nodes, and handing the problem off once you've saturated to a pseudo-boolean solver as described in the paper/talk linked above. This technique was recently rediscovered in http://www.cse.unsw.edu.au/~amosr/papers/robinson2014fusingfilters.pdf using full ILP rather than just pseudo-boolean techniques for optimization. I strongly believe that requiring confluent rewriting RULES doesn't scale and that something in this corner of the design space will be needed to get a good global near-optimum result.
I see. As such Morte is not immune to abstraction, but it is immune to more kinds of abstraction than Haskell, correct?
That's correct. An example of an abstraction that Morte does not yet optimize away (but perhaps should) is the following: -- Assuming that `k` has this type k : forall (x : *) -&gt; (a -&gt; x) -&gt; x -- you can prove that: k (f . c) = f (k c) This is what is known as a "free theorem" (a theorem provable from the types alone). I've encountered code segments that Morte could have optimized using free theorems but currently does not. However, I still don't know a decidable and complete way to detect and optimize away code using free theorems yet.
Would it be easier for you to implement this in Maude instead?
The problem is that the equality you get via normalisation is much weaker than the equality you want. You can't even assert the equality of two functions from the empty type, or you lose confluence. Therefore, your goal of "equal" programs having equal performance cannot actually be achieved. For example, `\x -&gt; x + 0` and `\x -&gt; 0 + x` will have different runtime behaviors. Also, I doubt that normalising everything is desirable in practice. In Agda, for example, you often need to carefully control normalisation using `abstract` blocks, or type checking becomes impossibly slow and memory hungry.
I didn't even know that what Morte does is undecidable.
I sympathize with the desire to have predictable rewrites, but I just don't think we can salvage it here. Instead, the programmer should be able to trust that the compiler is at least as good at choosing optimization rules as the human - this will take time but we're already making progress with such things as LLVM. I think in the end we will need to do the combinatorial search (sped up with heuristics) to get hand-tuned performance. The reason being that the implementation -&gt; function relationship is many to one: there is no *structure preserving* way to go from a mathematical function to a most efficient implementation. This is just a guess, but it should not be surprising, as it's deeply related to the P/NP problem - the programs that are easy to write look like universal properties ie. declarative, while fast implementations tend to be messy and highly reliant on weaving state around. Whatever we learn about predictable rewrites can always be added to a more general system though, so carry on ;) P.S I'm impressed with the free encoding tricks. I wonder if this could be a general replacement for typeclasses at some point. I tend to agree that typeclasses should only be used for algebraic laws, so let's just create "the group type" instead of a group typeclass, which we can reason about symbolically - instantiation takes the form of providing a lens* between the intended type and free object. This is kind of like a nuclear-powered version of object inheritance. (I think the reason you're having trouble with the free group is that we really need proper quotient types to do it, so that's another problem... there's some work on it here http://arxiv.org/abs/1210.0828 but I don't know enough to comment)
Morte is decidable (assuming your `Expr` is parse from a finite file). Morte is not Turing compete.
You can read up on DSLs [here](http://www.haskell.org/haskellwiki/Embedded_domain_specific_language), but the short and simple answer is that a library is a dsl too, but from the shallow end of the embedding pool. The deeper the embedding, the more likely you will see functions that check or interpret your program before it is evaluated. If you use a free monad to create your DSL, that is a deep embedding, and if you try to use any functions not actually supplied by the library you will get a compile error.
i think it was Iavor, not Edkso, though I could be wrong:) I do know that Iavor would be a second user of this work Adam Gundry is planning
Do you mean that the runtime behaviors will be different if I import foreign `+` and `0`, or even if I do it the `morte` native way? If I feed http://sprunge.us/jfcO (`\x -&gt; x + zero`) and http://sprunge.us/LUQe (`\x -&gt; zero + x`) they compile respectively to $ cat nat.mt | morte (∀(a : *) → (a → a) → a → a) → ∀(a : *) → (a → a) → a → a λ(x : ∀(a : *) → (a → a) → a → a) → x and $ cat nat2.mt | morte (∀(a : *) → (a → a) → a → a) → ∀(a : *) → (a → a) → a → a λ(n : ∀(a : *) → (a → a) → a → a) → n When I load them to test for equality then also unsurprisingly: &gt;&gt;&gt; txt1 &lt;- Text.readFile "nat.mt" &gt;&gt;&gt; txt2 &lt;- Text.readFile "nat2.mt" &gt;&gt;&gt; let e1 = exprFromText txt1 &gt;&gt;&gt; let e2 = exprFromText txt2 &gt;&gt;&gt; liftA2 (==) e1 e2 Right True It's hard to see how further application could give different run-time behavior for these versions of `id`. ...I'm just trying to figure out what's up, so this remark may be point-missing.
&gt; I plan to automate this, too, by providing a front-end high-level language similar to Haskell that compiles to Morte: wouldn't it be ideal if one could just fork the GHC frontend to do this? Is it that tightly tied to the intermediate language, given that quite a few revisions to the intermediate language were made in past years?
So you're asking if we are ok with something that your own experience says is a bad idea? Hmmm....
It's not a bad idea. Millions of people have 2 jobs. It's just not for everyone. 
Rewrite rules in the sense of "please compiler rewrite X to Y" can't possibly cope with the real complexity of optimising code in context. However a greater power to express properties of functions (say antisymmetry under permutation of the arguments, or any of "laws" which accompany type class documentation) which illuminate valid rewrites to the compiler could have more mileage.
&gt; If so, then we would could abstract freely, knowing that while compile times might increase, our final executable would never change. This isn't so comforting if compile times increase to days, years, millennia.
These pages do better at describing it, I think: 1. http://www.haskell.org/ghc/docs/4.08/set/universal-quantification.html 2. http://stackoverflow.com/questions/14299638/existential-vs-universally-quantified-types-in-haskell
I tried both identities using `random_crank`'s code and they give the same result: http://lpaste.net/110954 `((+) x zero)` and `((+) zero x)` and just `x` all have the same normal form.
Could you explain this in a little more depth? I'm not sure what exactly is going on here / what purpose Ex serves. I get that Ex allows you to "sort of" have an arbitrary length vector similar to a Fix datatype.. but it seems like you can't really do anything with it. In fact, even less than you normally can with existentials usually.. I only tried for a couple minutes but it didn't seem like I could even "show" it
Functors (and thus Applicatives) are not fully deep embeddings. `fmap arbitraryCode dslTerm` has an opaque function inside it.
My goal is to get Morte to accept a restricted subset of Haskell (sort of like asm.js accepts a restricted subset of Javascript). That's what I mean when I say I'm working on a Haskell-like front-end. It will include some, but not all, of Haskell's features, and I'm trying to design it to be code compatible with Haskell.
I'm not sure. This is the first time I've heard about Maude. From what I've read so far, it looks like a term-rewriting system that works for more general types of rewrite rules. The main reason I picked the lambda calculus as my term rewriting system is because normalization is decidable(?) (is that the right word?) and confluent. Does Maude have the same properties?
Yes, but not with the other implementation of `(+)` that I wrote above. Also, try `x + 1 = 1 + x` :)
Not in Haskell, because of terms like `inf = Suc inf`, if my intuition for this is correct. I'm actually not sure if `ones = VCons 1 ones` would work in Haskell, since I think it might actually complain about trying to build the infinite type. You'd have to be a Hasochist to do this right, but something like `inf = Suc inf; ones :: Vect inf Integer; ones = VCons 1 ones;` might get around that since the type checker shouldn't have to construct the infinite type. In Idris, `inf = Suc inf` would fail the totality checker. You could force it, but doing so may put you in a worse case than in Haskell... like non-termination in the type checker / proof verification.
Working with the Church encoding directly makes my brain hurt, but yes, it follows from a free theorem. The general theory here is that you can prove that the Church-encoded `Nat` is an initial algebra for the functor X \mapsto X + 1 using parametricy. Here we need the uniqueness part of the definition of initial algebra to show that these two functions are equal. In any case, you don't even need inductive or coinductive types to find an example where things don't work out nicely. In [this comment](http://www.reddit.com/r/haskell/comments/2g6wsx/haskell_for_all_morte_an_intermediate_language/ckgef2q) you showed the isomorphism between `(A,B)` and `(B,A)`. This is fine, but I'm pretty sure that the analogous isomorphism between `A + B` and `B + A` wouldn't work. The underlying issue here is that the equational theory that you really want (full beta-eta equality) is not decidable.
&gt; I have no intention of showing that isomorphic values are equal (what would be the use of that?) I'm not sure what you mean. I meant that the function: `swap : A + B -&gt; B + A` is *not* an isomorphism, i.e. the equality: swap (swap x) = x doesn't hold. At least not with the usual reduction rules. &gt; Can I decidably derive all free theorems for a given universally quantified type? You can derive the free theorems, but not use them as reductions while keeping the system strongly normalising. The equational theory with all the free theorems is going to be undecidable.
To elaborate just a touch: if compilation can improve run times, then can't supercompilation improve compile times? By structuring the program into multiple compilation passes, one might improve compile times.
So for non-recursive expressions (like pairs and tuples) I do know how to decidably implement the free theorem rewrites (I actually already implemented this and but then chose not to include it because it wasn't sufficiently general). This would correctly deduce that the isomorphisms for tuples and pairs really are isomorphisms. The rule I derived for the non-recursive case was that: -- Given this type (i.e. a church-encoded sum of products) t : forall (x : *) -&gt; (A11 -&gt; A12 -&gt; ... -&gt; A1N -&gt; x) -&gt; (A21 -&gt; A22 -&gt; ... -&gt; A2N -&gt; x) -&gt; (AN1 -&gt; AN2 -&gt; ... -&gt; ANN -&gt; x) -&gt; x -- You have the following free theorem t (\a11 a12 ... a1n -&gt; f (c1 a11 a12 ... a1n)) (\a21 a22 ... a2n -&gt; f (c2 a21 a22 ... a2n)) ... (\an1 an2 ... ann -&gt; f (cn an1 an2 ... ann)) = f (t c1 c2 ... cn) ... and that free theorem rewrite can be implemented in a decidable way (you always go from the left-hand side of the equation to the right-hand side and it's guaranteed to terminate if you keep applying the rule). That rule correctly handles both of the isomorphism examples you gave (i.e. tuples and sums). However, the hard part I have is deriving the recursive rewrite rules and seeing if there's a similarly nice trick for them. Is there a good paper I can read for deriving free theorems for church-encoded recursive types?
Finally got to read it all. Whew. This is really cool. It sounds a lot like the language I have been designing in my head: a metaprogramming only language that serves as an abstraction tool over other languages. My idea was "functions are just modules with dependencies", which I guess is just a mirror of morte's idea that "modules with dependencies are just functions."
I like the idea of optimizing the compilation process the same way we optimize the runtime code. I'm not exactly sure how to do this, but it makes a lot of sense.
Yes, that's right. I like to joke that in Morte "all abstractions are lambda abstraction" and modules are no different. You encode a module as just one big let binding, which in turn desugars to a lambda abstraction applied to the module's contents.
As I said, it's undecidable for inductive types. For coproducts, there exist some results for the STLC [\[1\]][1] [\[2\]][2], but I think they're already rather non-trivial. I think (but I might be mistaken) that normalisation for MLTT with coproducts is still an open problem. I'm not sure if it's any easier in your impredicative variant. The reduction that you mention is called eta-contraction, and it's definitely not enough. For example, I don't think you can prove commutativity of `and` for `Bool` with just that. [1]: https://personal.cis.strath.ac.uk/neil.ghani/papers/tlca95.ps.gz [2]: http://homepages.inf.ed.ac.uk/slindley/papers/sum.pdf
Shameless plug: have looked at http://hackage.haskell.org/package/dynamic-loader ? It is as simple as it gets.
&gt; Criticism of alternatives to null comes in many forms, but it all looks to me like a sort of Stockholm syndrome that comes from most of us having spent our entire programming careers working with null references. ... When someone does suggest a solution like an Option type and some pattern matching, we dismiss it; not because we're afraid of something new, not because we truly are so offended by the idea of a little more verbosity in our code, but simply because it doesn't really occur to us that what we have now is bad. This came up at my day job recently. We write almost all of our programs in JavaScript, but I tend to use motifs from functional programming, and I like to pretend that I have a type system like Haskell's, just to help me structure my programs. I presented a Maybe class, and suggested that we use Maybe.nothing instead of null wherever possible. The other developers wouldn't have it; why invet this new thing when we already have null!? Every programmer is familiar with null (for better or for worse) but far fewer have worked with an option type (only one of my coworkers was aware of this concept). I argued that null should not be overloaded. It's well established in other languages that null means *nonsense*, but they suggest using null to play the role of nothing, and use JavaScript's undefined to mean *nonsense* (like null in Java). It seems like this would work. What do you think? I still prefer using a first-class value for nothing; null in JavaScript is too weird, most notably for its behaviour under == .
Thanks for explaining that. I have a lot to read and understand now that I know the names for these things.
The library is named after a character from Planescape: Torment, a really amazing roleplaying game. The character, Morte, is a talking skull (thus the death-related name).
Yeah, that would make things a lot easier! I will check it out.
Wow, this is both genius and wizardry. I do love the style of posts from this blog being readable by mere mortals like me, though I'm sure my face would melt off. Anyways, this kind of answers the question of mine as to why this wasn't done already given the referential transparency available in Haskell. I do wonder though, could this be used as a proof assistant, perhaps in forms similar to existing blog posts. That'd be amazing.
The big problem with null is not so much overloading or lack of first-class ness (I believe JS null is first class), but rather that it subverts many static typing systems. As Javascript is already dynamically typed, I suspect you already potentially have a superset of problems that null presents. What might be interesting is having a container-of-one type, akin to Haskell's Just, and enforce that any value that might not be present, if it is present, give it wrapped in Just. This way, any code that incorrectly assumes that it's receiving a normal value will fail at runtime when receiving a value that may or may not be there, even if it is there (as the code would not be expecting the wrapped value), serving as a reminder, hey, you have a wrapped object some of time, therefore, you will have null some of the time.
This has been on Oleg's site for some time but I only just now discovered it. I wonder if there are any other folk knowledge strategies for wrecking memization like Oleg does with `app` and `app2`.
I've always wanted a feature in GHC where I could promise a particular expression was terminating and have the compiler superinline everything like this on that basis
[wat](http://i.imgur.com/VaKgAmJ.jpg)
I don't think existential types will help you in the way you may expect. /u/rampion mentioned that "there's very little you can do" with the existential-type `Ordered` solution, but I bet it's not obvious to you why that's the case. So let's work through your new example. First we add type signatures and rename `f` to `fNew`: g :: (a -&gt; a -&gt; Ordering) -&gt; [a] g f = List.sortBy f aList where aList = undefined h :: Bool -&gt; [Test] h guard = let fNew :: Test -&gt; Test -&gt; Ordering fNew = if guard then (comparing f1) else (comparing f2) in g f You had initially asked for this: fOld :: Ord o =&gt; Bool -&gt; Test -&gt; o fOld guard = if guard then f1 else f2 The big difference between the types of `fOld` and `fNew` is that `fNew` takes pairs of `Test`s--and it produces a concrete type, `Ordering`. You cannot ever get from `fOld` to an `Ordering`. (Or from /u/rampion's `Ordered` to an `Ordering`.) To see why, let's imagine we *could* make `fOld` typecheck: fOld :: Ord o =&gt; Bool -&gt; Test -&gt; o fOld = undefined By the way, using `undefined` like this is a great way to figure things out. Now let's write something like your `fNew'`: badCompare :: Test -&gt; Test -&gt; Ordering badCompare test1 test2 = compare v1 v2 where v1 = fOld True test1 v2 = fOld False test2 What will happen? (Go ahead and think through it first.) ... Not sure? Try compiling it. GHC complains about `No instance for (Ord a0) ...`, which isn't very enlightening. So let's annotate some types: badCompare :: Test -&gt; Test -&gt; Ordering badCompare test1 test2 = compare v1 v2 where v1 :: Int -- fOld True gives us the left hand, which is Int v1 = fOld True test1 v2 :: Double -- fOld False gives us the right hand, which is Double v2 = fOld False test2 ... And now it should be obvious why we're in trouble: we're trying to compare an `Int` with a `Double`. Pulling each `Test` apart on its own allows us to get in this jam. Existential types won't get us out; if we want to `compare` two things, the type system needs to know that they're the same. Working out the existential type solution would lead you right into this problem; you can't write a (working) function `Ordered -&gt; Ordered -&gt; Ordering`. I'd be happy to continue talking through these examples; just ask. I think `fNew` is what you're looking for. You can't abstract out `comparing` in the way I think you expect, because it has a different type in each position. You *can* give `fNew` a meaningful name and define it at top level as a function of type `Bool -&gt; Test -&gt; Test -&gt; Ordering`.
How are you going to automatically go from a haskell-like language to the encoding used here? That looks really complex. Almost like you'd have to prove all of the things we can't already prove in our regular compiler optimisation passes ...
Yes, like "constexpr" in C++ (but better and with less manual annotation required). I want the compiler to turn completely bound terms into static memory lookups, and complain loudly if it's not possible or if a configurable finite time is exceeded in compilation.
With functors, you can keep the deep embedding by simply storing a single function to apply after evaluation of the deep term, and composing with it for each fmap. With monads, the haskell functions are embedded inside the AST and there's no clean way to remove them.
Assuming you want to use the fully deep embedding to generate an executable or such, you cannot even apply a single fmap after the evaluation. So Applicative is a deeper embedding than Monad, but it isn't quite "completely deep".
Sure you can. Accelerate, for example, could easily support such a feature (although it doesn't), and it compiles to an executable and runs it on the GPU. Just, when the result returns to the Haskell program, the Haskell function is run. Although it occurs to me that Applicative also has `pure` which makes life difficult.
A completely deep embedding may never return to Haskell. It might just generate C code to run at a network server, where no Haskell environment can run.
Oleg's solution is overkill. Just turn off the full laziness transformation. http://web.archiveorange.com/archive/v/nDNOv0uoCDJLgpAZSYIH
The idea that there is one compilation time and one run time is very unnatural to me. Why this special number "one"? Why not `n` compilation times, possibily each with `IO` actions that run during each of them, the results of which are treated as constants for the next phase. This sounds like a good way to perform optimizations to me (though I don't really know what I'm talking about).
http://youtrack.jetbrains.com/issue/IDEA-127539
&gt; Once an expression is evaluated, its result is remembered, or memoized, in case the value of the same expression is needed again. Beginners beware: despite the above quote, Haskell *functions* are not automatically memoized. It is only data structures, such as the infinite search tree used in the article, which are memoized. In fact, one common trick to memoize a function is to construct such an infinite tree and to lookup answers inside the tree instead of calling the original function.
I'm not familiar with trying this, but Oleg's solution feels nice in the sense that it's much more localized. It'd be nice if I could construct thunks directly and then mark them with a pragma to deliberately deactivate memoization.
Part of Oleg's concern is that your statement is not entirely true: GHC detects when a thunk can only be called with the same argument and memorized that as well!
The first milestone is to translate simple (co)recursive types and functions to non-(co)recursive versions. I think that's pretty straight-forward. The second part is the harder part: translating mutually (co)recursive types and functions to simple (co)recursion. My rough idea for how to implement that is to use Kleene's theorem to translate N mutually (co)recursive definitions to N simple (co)recursive definitions. It's not a fully formed idea, but that's the sketch of what I have in mind. Also, if all else fails, I don't mind pushing extra work onto the user. The target audience are people who are willing to put in some extra work to get a lot of extra performance gains (i.e. me, Michael Snoyman, Bryan O'Sullivan, Edward Kmett, etc.).
IMO, that "optimization" should not exist. Whether to memoize functions is a programmer decision, to be made at more fine-grained levels, not something that should be blindly inserted by the optimizer which has no understanding of the code, at global scope!
That doesn't help if GHC might float out parts of the bodies of those thunks. What you need is some way to block the full laziness transformation locally.
In fact, I'd recommend looking at Agda and its definitional equality. It does not even require you to program with Church-encodings — it just ensures separately that functions are total. The interesting bit, instead, is that once you turn recursion into structural recursion via Church encodings, the GHC optimizer itself will be quite happy to do normalization. It's important that you don't write recursive functions, because GHC won't inline those — that's the limit. (There might also be size limits, but I didn't run into them when I played with this). As a consequence, an interpreter for a language without "fix" written in finally tagless style can be inlined easily. (Fix can be handled too if you're careful, but I'd need to dig out the details).
Has anyone been able to make this package work on the latest GHC?
This problems also has to be considered in the design of type checking plugins: https://ghc.haskell.org/trac/ghc/ticket/7870 That would permit EDSLs to report type errors at his own level instead of at the haskell level. 
Yes, that is more complete and valuable!
As u/goliatskipson says, the HaLVM runs on Xen. So it should work on Amazon's or RackSpace's servers. That being said, I've never done so, so YMMV. If you try it and have problems, please consider sending in a bug report. 
I believe this talk, as well as the extremely similar talk I gave a couple weeks ago at the Xen developer's summit, are supposed to be put online at some point.
You can compile your program at the command line using: ghc -O2 hello.hs That will create an executable named `hello`, that you can run using: ./hello The `-O2` flag to `ghc` just means "enable all optimizations".
oh i see. I haven't come across this before. thanks for posting the link here
When that happens, and you come aware of that, would be be so kind to post that to this sub-reddit? Thanks in advance.
Thanks! This is really helpful! Now I'm excited to do the talk. Also, yours seems to have been for the ACM, and so is mine. It's actually the only computer science organization on my campus :(
I gave a talk recently about functional programming at my hackerspace. Here's my takeaways from it: First, drive home the point that **(mutable) state** is inherently and undeniably **hard to reason** about. I like to start with the "First Question of Tech Support" which is "Have you tried turning it off and back on again?". The reason we do this is because it resets the state in your machine or modem or toaster to a more predictable one. Here's a good example in Python of some buggy code: # list1 and list2 are lists of integers # they must be nonempty and must be the same length # list2 is an output parameter. Whatever list you pass in will be # overwritten with what list1 would be if we shift everything over # to the right by one place and fill in the leftmost entry with 0. def shift_right(list1, list2): list2[0] = 0 for i in range(len(list1) - 1): list2[i+1] = list1[i] This code isn't especially Pythonic, but you see this kind of programming in C all the time. Reply to this comment if you need help finding the bug. Having state in your program means not only do you need to think about the code as it is written, but you also need to keep in mind the *context* it's run in. You might need to know the time of day, the current working directory, the whim of the user, or the value of some debug flag in a data structure set to "true" a year ago by the intern who didn't know any better. State is evil. At its heart, functional programming is about minimizing state. It's about writing programs that act on immutable data. This is the common thread between the Haskell, ML, and Lisp worlds. Types, laziness, macros, and higher-order functions are all secondary concerns. What we want are programs we can reason about. Contrast this with OOP. The goal in OOP is to *hide* your state. This means you want your member variables marked private. Then, anyone who wants to act on your state has to do so through a controlled interface. It's very unfortunate, though, that it's common practice to write getters and setters for most (if not all) members of a class. This adds mindless verbosity to your Java programs and circumvents the very thing privacy scoping is trying to accomplish! OOP aside finished. So where does the 'functional' bit come in? If all you have are immutable values, you can't modify things and you can't delete things. (This is the "U" and "D" in "CRUD" if you do databases). But you *can* read from immutable data. And you can create new, *derived* pieces of data. (The "C" and "R" in "CRUD"). We use functions to create new data from old data. But the word 'function' here is used in a more narrow sense than in general programming. It's much closer to the sense meant in mathematics: it's a rule that takes inputs and maps each to a unique output. In particular, that means, no side-effecting operations. If we were to read from a file or check our email, we would have introduced some outside state into our pipeline. To recap what we have so far: functional programming is preferring immutable data structures and creating new data through the use of functions. Everything beyond that is icing.
Not sure if it's a typo or not, but haskell extensions are typically `.hs`—`.sh` is for shell scripts. You type your program into a text editor, like gedit or ~~emacs~~ vim, then save it, and either compile it as Tekmo suggested, or run it with `runhaskell hello.hs`. E.g. % echo 'main = putStrLn "hello, dragon_fiesta"' &gt; hello.hs % runhaskell hello.hs hello, dragon_fiesta % ghc hello.hs [1 of 1] Compiling Main ( hello.hs, hello.o ) Linking hello ... % ./hello hello, dragon_fiesta
Awesome! I definitely thought I would want to stress immutable state and the difference in the use of the word 'functions' from mathematics vs general programming, but this really lays out the benefits simply. Thanks! Just to make sure I'm seeing the bug correctly, if I give it [1,2,3] this function would return [0,0,0] because it erases the first element and then the for loop ends up starting at the 0th element in the list and copies that all the way down the line.
It's not really needing to "deactivate memoization". Rather it's needing GHC not to inappropriately share thunks when the programmer said otherwise (at least implicitly). I'm in agreement with pchiusano. I don't think full laziness should be applied as a standard "optimization". If the programmer wants sharing she should write it such that the thunk is shared, if not, not. One really does need control over the fine details of the operation semantics here. Additionally I am very much against pragma proliferation.
In case I was unclear I just wanted to clarify that I didn't mean my comment in relation to Morte, more the conventional wisdom on what "compilation" means. Morte seems obviously more promising in this regard!
&gt; First, drive home the point that (mutable) state is inherently and undeniably hard to reason about. If we weren't so used to mutable state, we would think of it as a performance hack.
This is an interesting line of thought. Maybe beautiful terse programs could be just a view into long winded, practically unreadable programs which contain all the information above. Maybe the language of the future contains all the ideas of all good programming languages in one and it's up to the editor to let you edit the code in the specific DSL of the task that you are attempting.
I recommend watching some Rich Hickey presentations for inspiration. He is a Clojure guy and not a Haskell guy, but he mostly talks about FP and things that apply to Haskell as well. [This presentation can be a good place to start](http://www.infoq.com/presentations/Value-Values).
Does anyone still take those seriously? I mean, I get that there is some rivalry between various IDEs, but I figured vim &amp; emacs had settled into a sort of wink &amp; nudge "war" long ago. /u/dragon_fiesta, if you've got some time to kill, you can start a wiki walk into hacker culture [here](http://c2.com/cgi/wiki?EmacsVsVi), on Ye Olde Wiki. Or you can just ignore it. It's a bit like knowing trivia about, eh, insert mainstream topic here. Sports rivalry, maybe?
I might emphasize the strength of the type system (and the typeclassopedia mechanism) to enhance maintainability and correctness of programs. this has been perhaps the most important thing about Haskell that has kept me here this long. 
Interesting; that sounds like it would work surprisingly well. I think something you might discover in doing this experiment, is even with an engine capable of generating perfect high level IR, the lowering to machine step is one of the least perfect things we do that accounts for most of the tight loop performance. Your experiment is pretty awesome though. It just seems too simple for someone to have not done it before, failed, and published a paper or something. Then again, the night is still young.
Just be careful. The function doesn't actually return anything. (In Python, the return value is actually `None`). The whole point of the function is to call it so that `list2` is changed. I think maybe you see what's wrong. In a certain disastrous case, `list2` will be set to all `0`'s. But it might not be immediately clear when that happens. 
Also, FWIW I meant to give you gold, but didn't realise the OP was tailcalled. I've messaged the admins to transfer it.
You're working with "double negation elimination" rather than "law of excluded middle". While it's true you can implement one in terms of the other in classical logic they aren't the same, and there are logics where only one is valid. I'm not ready to weigh in on how bad of an idea this is, though.
Yeah, that was my experience with Oz as well. Why would anyone tie a language to a text editor? Having it as a preference, like emacs &amp; lisp, sure, but a *dependency*?
Oops. I just sort of assumed they were always equivalent.
I had to throw in a bit of strictness for the `unsafePerformIO` not to pose problems but apart from that it's [the typical realizer](https://gist.github.com/gallais/892e400524bef4405fde).
You can do this without any unsafe code using the `Cont` monad: {-# LANGUAGE DeriveDataTypeable #-} module Main where import Control.Monad.Cont import Control.Exception import Data.Typeable data Void type Not a = a -&gt; Void absurd :: Void -&gt; a absurd a = a `seq` absurd a intrNotNot :: a -&gt; Not (Not a) intrNotNot a x = x a doubleNegElim :: Not (Not a) -&gt; Cont Void a doubleNegElim = cont data E a = E a deriving (Show, Typeable) instance (Show a, Typeable a) =&gt; Exception (E a) runVoidContIO :: (Show a, Typeable a) =&gt; Cont Void a -&gt; IO a runVoidContIO c = catch (absurd . runCont c $ throw . E) (\(E a) -&gt; return a) This reflects the fact that call/cc corresponds to double negation elimination (or, more precisely, [Peirce's law](http://en.wikipedia.org/wiki/Peirce's_law)) under the Curry–Howard isomorphism.
actual LEM should be lem :: Either a (Not a) lem = doubleNegElim $ \k -&gt; k . Right $ k . Left the general idea of implementing double negation elimination in terms of exception handling is the a good one, since the open exception mechanism gives you something pretty close to continuations, although I have a suspicion the details of this implementation will be a bit buggy. 
Not me, i use bang patterns. I'm hyperactive.
You can also make the first line of the file `#!/usr/bin/env runhaskell` and then `chmod +x hello.hs` to be able to run it just like a shell script with `./hello.sh`. 
Shouldn't Haskell model an intuitionistic logic (disregarding bottoms)?
 f :: [Int] -&gt; [Int] f x = map g x where g y = y + 1 -- does not depend on x {-# NOINLINE g #-} -- pretend g is large and used multiple times in f This kind of pattern will allocate a closure for g on every call to f if you build with `-O -fno-full-laziness`. The solution is to explicitly lift g to top level, but people often like to make auxiliary definitions in a where clause. So, in practice the full laziness transformation helps many programs. Ideally you could find a conservative but still useful approximation to when the full laziness transformation is a win, and only enable those cases of it by default...
I think you might like Mark Lentczner's [Haskell Amuse-Bouche](https://www.youtube.com/watch?v=b9FagOVqxmI) talk, which covers some nice beginner-level concepts.
Yes but the exception mechanism combined with `unsafePerformIO` let you do things that shouldn't be possible. My belief is that exceptions + mutable state is equivalent to delimited control and so is stronger than you need to model classical logic. EDIT: Thinking more about this I'm not sure that claim is the case. Exceptions are exceedingly powerfull: adding ML style exceptions to system F makes it turing complete (for example). But, as far as untyped ability to simulate control operators go, I'm not sure that they can simulate call-cc. The reason for this is that exceptions handlers are dynamic while continuations are static. 
evil-mode ;)
Yeah, I should have minimally said "typed lambda calculus".
even better than isNothing in a language like js would be something like a mapM_ :) 
Yes, people go to trouble to prove that invariants hold on methods of a class and all of its subclasses and there are tools out there to enforce these laws statically, SPEC# being an older example. In this case the OO world might be a step up on Haskell, I don't know of much in the Haskell world analogous to "design by contract" from the OO world, the closest I've come to it are quickcheck properties.
&gt; is not an isomorphism, i.e. the equality: &gt; swap (swap x) = x &gt; doesn't hold. I'm not sure I understand what you are saying. If you define A + B = forall C. (A -&gt; C) -&gt; (B -&gt; C) -&gt; C swap : forall A.forall B.A + B -&gt; B + A swap = /\A./\B.\f : A + B./\C.\b : B -&gt; C.\a : A -&gt; C.fCab then you have swap B A (swap A B x) = (/\A./\B.\f : A + B./\C.\b : B -&gt; C.\a : A -&gt; C.fCab) B A (swap A B x) = (\f : B + A./\C.\b : A -&gt; C.\a : B -&gt; C.fCab) (swap A B x) which alpha renames to = (\g : B + A./\C.\d : A -&gt; C.\c : B -&gt; C.gCcd) (swap A B x) continuing the beta reductions you get = (\g : B + A./\C.\d : A -&gt; C.\c : B -&gt; C.gCcd) ((/\A./\B.\f : A + B./\C.\b : B -&gt; C.\a : A -&gt; C.fCab) A B x) = (\g : B + A./\C.\d : A -&gt; C.\c : B -&gt; C.gCcd) ((\f : A + B./\C.\b : B -&gt; C.\a : A -&gt; C.fCab) x) = (\g : B + A./\C.\d : A -&gt; C.\c : B -&gt; C.gCcd) (/\C.\b : B -&gt; C.\a : A -&gt; C.xCab) = /\C.\d : A -&gt; C.\c : B -&gt; C.(/\C.\b : B -&gt; C.\a : A -&gt; C.xCab)Ccd = /\C.\d : A -&gt; C.\c : B -&gt; C.(\b : B -&gt; C.\a : A -&gt; C.xCab)cd = /\C.\d : A -&gt; C.\c : B -&gt; C.xCdc now it is just a case of eta contraction. No free theorem needed. = /\C.\d : A -&gt; C.xCd = /\C.xC = x 
LiquidHaskell?
&gt; Also, I doubt that normalising everything is desirable in practice. In Agda, for example, you often need to carefully control normalisation using abstract blocks, or type checking becomes impossibly slow and memory hungry. This can not be emphasized enough. System F (even System T) is plenty powerful enough to write small terms that will not normalize on any physically build-able computer within the life of the universe. (Eg, diagonal Ackermann of 10). Strong Normalization is a very important meta property for all sorts of things, but as Vladimir Voevodsky put it, it is only "theoretical"
over age 54+2; part time taught at college computer security for 'kicks'. real engineer - done the meetups on python, linux shell, etc. So Almost all the other posts ARE WRONG, IMHO. WHY should you start with movie THE MATRIX? It's the paradigm. everyone is point and click. What is state? When a student do U mutate? or just learn - with side effects? Why are the leaders wearing SKIRTS - that is Glasgow kiilts and eating Scottish nasty food? THANKS FOR downvote and CENSORSHIP To: Scotland Haskellers Considering Independence Fr: King George Ghost - MAD king George of the inbreed who Won the Rebellion of the American Colonies Commend U for your PURE and effective language, you slaves...or rather subjects of the empire (make that Common-wealth). Soon, your CHILDREN will learn Microsoft, that is rather windows and html javascript coding, for HASKELL is important. The MASTER and subject curriculum with BRITISH master teachers will 'assist the Scottish in learning how to code well.' This is NOT a satire by the ghost - writer for the American comedy TV host Steven Colbert. This is KING GEORGE (mad) to RAISE your taxes to EDUCATE your children to be GLOBALLY competitive and learn how to code javascript .. read the wrong speech - that is code HASKELL. It is not about politics. It is not about tax angles, the World Banks and the rise of the Micro HARD software gate's empire. Made with convenient back or alternative doors to the OS operating system. No, it is about YOU choosing Slavery... err I mean Order, real intelligence... error that is real jabber- script Java-script and not the functional paradigm. If Haskell were an animal like on an ANIMAL FARM - no menton of G. Orwell, which ANIMAL would wear a SKIRT? The MANLY Haskeller wear a skirt in Scotland. The American MARINE who uses Haskell wears a special skirt or KILT to prance around in, but I, KING GEORGE ghost say the SKIRT wearer are just deft pansies! With the world involved in war, we wish to enroll your university professors and communities in MY - that is OUR noble quest for ORDER. There are plenty of military bases which are NOT a target for SCOTLAND HASKELL will RULE the world. by, the way, your final complement. Ghastly food and nasty habits although some of the women are dapper. Despite that I, King George Love YOU - Glasgow and Glasgow Haskell even in death. OK, So I am a bit MAD, but I was not as bad as some of your own kings and queens. too da loo, skirt chasers or skirt wearing prancers. PS. no insinuation meant as to prancing as 'pretty boy' or homo-sexual activities. That LINE will not be crossed. This is ONLY about subjectitude, service to the throne and the PURITY of the Haskell culture... that is HASKELL LANGUAGE. subjectitude, service to the throne and the PURITY of the Haskell culture... that is HASKELL LANGUAGE. please reply with your flames and comments. Hard to hurt the feelings of a ghost. Ho hum. //////// If you haven't turned off your mind, here is the good stuff. like sometimes I present at barcamp.org 1.)THE RULES are improvisational comedy 2.)whatever ABSURD statement, stupid old ways of progamming paradigm - PYTHON has MORE, EASY USE libraries. NEVER, NEVER argue. THEY ARE RIGHT 3.)the reason JAVA and BIG ORACLE has patterns, DESIGN PATTERNS, anti-patern, pattern patterns is BECAUSE THE LANGUAGE and the BASIS IS DEFICIENT. insert movie Matrix quote What is the sound of one hand clapping? what is the sound of one monad transforming? PLEASE CENSOR THIS POST, you censors that fear the truth. King George Ghost or one fake one. Why I like haskell? Because MATHs, logic, even Probability, engineering design of experiments and PYTHON NONE OF IT WAS CONNECTED. case study: go from ENGLISH to Boolean Logic to ENGINEERING to Haskell to Assembly code to ??? to ??? Actually, THE BIGGEST ARGUMENT IS that you can get a job in OOP not FP! Only if you are independent consultant / head STARTUP /specialist can you do HASKELL. 
I LUV PYTHON, but the version changes 2 2x to 3x to 4x BREAKS CODE. MOST of the time is spent LOOKING UP JAVA 800 page book. about teaching style - improvisational comedy is NOT to make fun of the audience /students. Everyone participates, the reason why Haskell SUCKS at libraries, is that the hard stuff is done. just combine it and write your own. THAT IS practically impossible to do in Python/Boost C++, etc. the reason improv comedy agrees with whatever CRAZY statements or premise is that IS THE REALITY MATRIX your students live in. YOU ARE NOT THERE to explain your great WISDOM of the MONAD. Yoda says, do not try, just do. just randomly take Rosetta code and compare them. PS. I am somewhat begin intermediate with very large HOLES in Haskell knowledge. Functors? what the h*ck is that really? Coding Horror - why can't programmers program. I run MY OWN Genetic Algol and mix code from 5 years ago in Haskell. Result is UGLY, it works and it is STILL 1/5 one fifth the SIZE OF PYTHON and fully SCALABLE TO PARALLEL GPU CLOUD. 
In OO people don't tend to use the word law, instead preferring terms like contract, pre/post conditions, etc. Every OO class or interface typically comes with a set of laws, although usually stated somewhat more vaguely in English rather than using equations. Here's an example form Java's `Collection.contains`: &gt; Returns true if this collection contains the specified element. More formally, returns true if and only if this collection contains at least one element e such that (o==null ? e==null : o.equals(e)).
Seriously, LiquidHaskell.
s/sh/hs AGAIN ;) Apparently we all make this typo. haha
There are also tools for (semi-)formal contracts and contract verification for OO languages, for instance [Code Contracts for .NET](http://visualstudiogallery.msdn.microsoft.com/1ec7db13-3363-46c9-851f-1ce455f66970).
Ok, so for you then: I'm !lazy
&gt; Every OO class or interface typically comes with a set of laws [...] Perhaps implicit. In my experience almost no class (or even interface) comes with explicitly stated laws (or whatever we should call them). Some collections libraries do (like the one you mentioned) but that seem to be the exception rather than the norm.
`bind :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b` `bind :: [a] -&gt; (a -&gt; [b]) -&gt; [b] -- bind specialized to List` `selectMany :: IEnumerable&lt;TSource&gt; -&gt; (TSource -&gt; IEnumerable&lt;TResult&gt;) -&gt; IEnumerable&lt;TResult&gt;` `select` is return iirc `return :: a -&gt; [a] -- specialized to list again` `select :: TResult -&gt; IEnumerable&lt;TResult&gt;` obviously `Monad` can be used for other things but its basically `selectMany` the linq docs basic example maps to queryLondonCustomers = do customer &lt;- customers guard $ city customer == "London" return customer or desugared queryLondonCustomers = customers &gt;&gt;= \customer guard $ city customer == "London" &gt;&gt; return customer (probably simpler to use `filterM ((== "London") . city)` though)
Actually, `Select` (the method) is .NET's equivalent of Haskell's `map`. There is no generic `return`-equivalent in .NET (because of the type system).
Haskell is taught with Hugs in the first semester of any CS course at my university, too. I once went to find out why they were using Hugs and it turned out that the professor holding the lecture didn't like GHC(I) simply because he didn't get them to compile on MAC. And in all the years afterwards he just never went back to check. &gt;.&lt;
Can unit tests be considered an informal form of laws? It's a short step from there to property-based random checks and from there to actual laws, isn't it?
might be mixed up w/ `Select` and `select` inside linq things tbh
I would say no since a unit test is just a test of a law for a particular set of values. To read a class's unit tests might give some insights but does not sound very practical for understanding classes in general. And unit tests are often located elsewhere from the class.
Oh my god this is poetic.
I think because you can't overload on return type, so a -&gt; [a] and a -&gt; Option a would conflict. That could just be C# though.
So how are you going to translate data types where the functor isn't strictly positive? While I sympathise with your overall goal that programs with the same denotation should generate the same binary, this is of course a pipe dream since it's in general undecidable if two programs have the same denotation. (Well, technically, you can decide if two machine machine code programs do the same thing, but I'm not sure that helps.) 
In non-generic terms, it's possible to implement it like /u/OldShoe, even as an extension method, but `return` is only really useful in generic contexts. A generic definition would imply some function of an interface. For one, there is no way to express `return` as a function of an interface implemented directly by each monad instance, which would have to be something like a static interface function only callable in generic context (e.g. `T.Return(2)` in a generic function with type parameter `T where T : IMonad&lt;T&gt;`). The typical OO-notion of interfaces fails here. Another possibility would be trying to directly mimic Haskell's type classes and provide an `IMonad&lt;T&gt;` interface instance, representing the implicit dictionary of the type class, in addition to the actual monad instance of type `T`. Higher-order monad functions such as `sequence` could be implemented as extension methods on such an interface. Sadly, such an interface is impossible to realize since C# doesn't support higher kinded types. `T` in the above example would have to be an unbound generic type like `IEnumerable&lt;&gt;`. I recently finished up my [Bachelor's Thesis](https://www.dropbox.com/s/efzvjcpsm5u7urp/final.pdf?dl=0) touching this, so feel free to look into section 4.1 for an example.
OO classes typically come with (sometimes implicit) laws. Tools like, eg, JML are a way of making these laws explicit and checking them either statically or dynamically.
It might be (speculation) that the docbuilder Bot crashed or stopped for some reason.
This reminded me very strongly of [this blog post by Russell O'Connor](http://r6.ca/blog/20040616T005300Z.html). And indeed that post is a guide to breaking `Lem`. I've attempted to transliterate it as closely as possible into Haskell: {-# LANGUAGE PatternSignatures, ScopedTypeVariables #-} module BadLem where import Lem russell :: forall a b. Not (Not (((a -&gt; b) -&gt; a) -&gt; a)) russell (f :: Not (((a -&gt; b) -&gt; a) -&gt; a)) = f $ \(s :: (a -&gt; b) -&gt; a) -&gt; s (g . f . h) where g :: Void -&gt; b g = absurd h :: a -&gt; ((a -&gt; b) -&gt; a) -&gt; a h x1 x2 = x1 callCC :: ((a -&gt; b) -&gt; a) -&gt; a callCC = doubleNegElim russell main = print $ callCC $ \esc -&gt; esc 666 Running it: $ runghc BadLem.hs BadLem.hs:1:14: Warning: -XPatternSignatures is deprecated: use -XScopedTypeVariables or pragma {-# LANGUAGE ScopedTypeVariables #-} instead BadLem.hs: Something has gone wrong with `lem`. You should not see this. (I'm not sure if there's a way to make it do something *unsafe*, rather than merely terminate with an exception?)
I think the builder was stopped because it was causing too much load on the server. It'll move to another server soon. 
Btw, you can watch the **BBP** (or **FTP** if you like) ticket at [GHC #9586](https://ghc.haskell.org/trac/ghc/ticket/9586) if you want to monitor GHC's progress with respect to this topic...
I don't see what strictness has to do with it. You're not calling `f` within the scope of cut, so of course the continuation won't be called. If calling the continuation outside of the scope of `cut` was allowed, then we wouldn't need to bother with `cut` at all, as you could easily obtain an `Either a (Not a)` and use it directly instead. lem :: (Show a, Typeable a) =&gt; Lem a lem = cut id Now if we ask our oracle whether it is `Bool` or `Bool -&gt; Void` which exists, it will of course return a `Bool -&gt; Void`, and calling this continuation will result in an exception being thrown, the same exception as when you called `g 0` in your example. -- | -- &gt;&gt;&gt; incorrect -- *** Exception: Wrap {unwrap = True} incorrect :: Bool incorrect = case lem of Left b -&gt; not b Right antib -&gt; absurd (antib True) The proper way to use the oracle is to call the `antib` continuation within the scope delimited by `cut`: -- | -- &gt;&gt;&gt; correct -- False correct :: Bool correct = cut (\lem -&gt; case lem of Left b -&gt; not b Right antib -&gt; absurd (antib True)) Of course, it would be much safer to use an API in which continuations cannot escape their scope, ensuring correct usage. I'm sure you already know what such an API would look like: it's the usual continuation monad. type Not r a = a -&gt; Cont r Void -- return Right f, then if somebody calls f x, -- rewind and return Left x instead. lem :: Cont r (Either a (Not r a)) lem = callCC $ \cc -&gt; do let f x = cc (Left x) return (Right f) -- | -- &gt;&gt;&gt; runCont correct id -- False correct :: Cont r Bool correct = do r &lt;- lem case r of Left b -&gt; return (not b) Right antib -&gt; do void &lt;- antib True absurd void
I was just trying to point out that `unsafePerformIO` was used in an unsafe way. :)
And will it catch up and build the missing packages?
Its funny that you make that comparison. I just recently stumbled on [nix-docker]( https://github.com/zefhemel/nix-docker) which provisions Docker containers with nix expressions. You might find it interesting.
&gt; but is it actually possible to segfault this function without using unsafeCoerce? Challenge accepted :) As others have pointed out, it's possible to make your code throw an uncaught `LEMException`. My favorite way to do this is by converting `doubleNegElim` into an actual `lem`: lem :: Either a (Not a) lem = doubleNegElim $ \cc -&gt; -- return Right f, then if somebody calls f x, -- rewind and return Left x instead. let f x = cc (Left x) in cc (Right f) -- | -- &gt;&gt;&gt; throwLEM -- *** Exception: Something has gone wrong with `lem`. You should not see this. throwLEM :: Bool throwLEM = case lem of Left b -&gt; not b Right antib -&gt; absurd (antib True) The reason the exception isn't caught is that there are two copies of the exception. The first copy is thrown by `cc (Right f)`, and since this call is inside the scope of `doubleNegElim`, the exception is caught. Then your exception handler calls `unsafeCoerce` and returns the `Right f` I gave to `cc`. Now, we leave the scope of your exception handler, `doubleNegElim` returns, and we are left with `Right f`. Thus, `throwLEM` takes its second branch, in which `antib` becomes `f`. But this `f` is a loose canon! It calls `cc`, which is `(throw . LEMException)`, and we are no longer protected by an exception handler. In this case, the exception has type `LEMException (Either Bool (Not Bool))`. Now that I can throw a `LEMException` whenever I want, I can use it to force your `unsafeCoerce` to perform unsafe coercions. Here, it expects a `LEMException Int`, but it catches a `LEMException (Either Bool (Not Bool))` instead. import GHC.Conc (pseq) -- | -- &gt;&gt;&gt; invalidCoerce -- 4391721112 invalidCoerce :: Int invalidCoerce = doubleNegElim $ \cc -&gt; throwLEM `pseq` undefined And with a slightly more dangerous coercion, I can obtain a segfault: produceInvalidList :: IO [Int] produceInvalidList = doubleNegElim $ \cc -&gt; throwLEM `pseq` undefined -- | -- &gt;&gt;&gt; main -- Segmentation fault main :: IO () main = do xs &lt;- produceInvalidList print xs Mission accomplished :)
Yes, it's a useful notion of equality. But you seem to limit yourself to eta reduction(?). If you want to venture into super-compilation you need to consider eta expansion as well. Not that I know exactly how it plays out with a PTS. 
It would actually be cleaner to just use a list comprehension. [x | x &lt;- [0, 3, 5, 13, 25], x `mod` 5 == 0]
But then compilers transform it to SSA (and back again)!
It just occured to me, even without a generic IMonad, if you could express the free monad in C#, that might be good enough, you could just try composing whatever functionality you want out of it. I'm not sure how you'd express the free monad without higher kinded types either, though. There is also that old observation that all monads can be considered variations on Cont, Cont probably is possible to express in C#.
Nothing interesting this time, but I thought it would be best to split this post off from the CSS parser (which will get a post in a couple days). Edit: actually, if anyone would be willing to take the time to inspect it, I'd love some feedback on the [CSS parser](https://github.com/Hrothen/Hubert/blob/master/src/Css.hs); I feel like it's kind of messy and difficult to read.
I should have known Oleg would have got there first.
Yes, the code is probably not the best. The code re-use is pretty much copy-paste. Probably in a different codebase it'd be easier to work with Hamlet and Julius.
Yeah I have used this idea before as well. There is a mailing list post from Mark Jones that describes how to do it [here](http://www.haskell.org/pipermail/haskell/2004-October/014701.html). Essentially you build a small shift reduce parser that you apply in a second pass to resolve operator precedence once you have seen all infix* declarations. 
I found Dana Xu's work amazing. http://pauillac.inria.fr/~naxu/pub.html Too bad it's not available for use. In general, "lightweight contracts" is IMO the way to go. 
How about a `nomemozie` [id with special behaviour](http://hackage.haskell.org/package/base-4.7.0.1/docs/GHC-Exts.html#g:7)?
I would bet that the GC generation and cache locality causes that
Check out [these beginner tracks](https://www.fpcomplete.com/school/starting-with-haskell) at School of Haskell. And remember, anyone is invited to make a free account on FP Haskell Center if they want to experiment with Haskell after the lecture. The School of Haskell, and the free accounts on FPHC, are free community services that FP Complete pays for. Enjoy.
As another approach, feel free to make a free account on FP Haskell Center, where you can build and run Haskell programs on our cloud servers with no setup. This is a free service my company pays for.
&gt; That being said, I've never done so, so YMMV. If you try it and have problems, please consider sending in a bug report. Look, I don't know what your goals are with HalVM, but if it's in any way user adoption, IMO the answer should be along the lines of "it works here and here, and these are the reproducible steps". If not, then please ignore this feedback.
If you are asking specifically, why is the monad instance defined as it is for lists? then consider the [monad laws](https://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Monad.htm). 1 of which in particular is: m &gt;&gt;= return == m
True, they're just very specific laws, sometimes so specific as to not aid your understanding of the class.
You might find multiline strings more pleasant to deal with if you use one of the string quasiquoter libraries, or just have each line as an element in a list and use `unlines`. Digging the posts, though. I'm really looking forward to more.
Using `unlines` just feels awkward, and I'm trying to stick with Haskell Platform libs as much as possible, so that means no quasiquoter. I *thought* GHC 7.8 added a language pragma for python style triple quoted strings, but it looks like I was wrong.
Yes, it should catch up AFAIK. The new server will be much more powerful (with more cores), so it should be able to build faster anyway, too.
If anyone's interested in more serious tests for the web platform (these obviously aren't so relevant when you're just working on a toy browser), there's [html5lib-tests](https://github.com/html5lib/html5lib-tests) which contains lots of tests for both the tokeniser and tree construction parts of the HTML parser (it is, de facto, *the* HTML parsing test suite), and [web-platform-tests](https://github.com/w3c/web-platform-tests) which relies upon a roughly working browser to run at all (at least having a working JS engine and older DOM stuff).
Thanks for this. I was dreading having to come up with better tests if/when I decide to improve the html parser.
Any particular reason to limit yourself to the HP?
In case of the `select` keyword in LINQ queries, that one might be confusing as it depends on the structure of the query whether it will be rewritten to a `Select` method call or a `SelectMany` method call.
If you feel like yak shaving, implementing a string quasi-quoter is quite a straightforward exercise.
I'd love to make progress on error messages as well! I think the Helium approach would be difficult to port to GHC, but something like a plugin function to post-process typechecker errors might be feasible.
Interesting functionality. I'd never seen that before.
"Select" means `map` and "Many" means `concat` (or "flatten"). The list monad bind can be implemented as: (&gt;&gt;=) :: [a] -&gt; (a -&gt; [b]) -&gt; [b] m &gt;&gt;= f = concat (map f m) Hence the name "flatMap" in Scala and "SelectMany" in C#.
My main motivation is units of measure - e.g. preventing adding a quantity measured in metres to one measured in feet, but helping you do the conversion. Like the excellent [units package](http://www.cis.upenn.edu/~eir/packages/units/), but without the need for clever type-family engineering.
(I posted this as a comment on Tekmo's blog, but it seems the system just ate it, so reposting here.) If I remember correctly, those CPS-style encoding do not preserve eta-equalities for positive types such as the sum. This means that the original program may have more beta-eta-equivalences than the encoded program, and that your reduction to normal forms will miss some equalities that were correct in the source program. For example, if `f` has type `A -&gt; C` and `s` has type `A + A`, then the program (case s of L x -&gt; f x | R y -&gt; f y) is equal to (f (case s of L x -&gt; x | R y -&gt; y)). The Boehm-Berarducci encodings would be (s f f) and (f (s id id)), which are not beta-eta-equivalent.
We may help you better if you could show your code ?
All the time. Try running hlint and pointfree on it to see what can be done!
Unfortunately I'm not allowed to share the code at this time, and also I'd like general pointers if possible so I can deal with this in the future too. So far I've refactored a lot of redundancy out by defining anonymous functions in the where clause.
I guess you can see if there are any abstractions (monoids, monads, applicatives, arrows etc etc) that map well to the problem domain; this can make the code much cleaner (with the obvious caveat that readers of the code will have to be familiar with the abstraction). You can also try to pull out well-named parts of the function into separate helper functions (even if they are just in a where or let clause). The rule of 'find the right things to name, then give them good names' still applies even in Haskell :-)
Huh, searched for pointfree and got [this](http://www.haskell.org/haskellwiki/Pointfree). Very interesting! How do you use the actual pointfree command though?
Since your question is quite general my advice would be to split your function up. &gt; I realize this is needed, and that is needed This sounds like you're cramming to much into your function, maybe you should think about separating concerns here. Also: Plug for /r/haskellquestions. ;)
Think of a different way to do it. It sounds like you haven't planned it out in your head properly if you're adding things without a plan.
Just type in pointfree "expression"
If it adds nothing why did you think that making a comment about it was a good idea? Unless there is actual unacceptable behaviour going on, speaking out against something is usually more trouble than it is worth, because it makes you have to deal with further comments. (for instance this one ;) )
You might find some useful information about conduit in these two posts by Michael Snoyman https://www.fpcomplete.com/blog/2014/08/iap-speeding-up-conduit and https://www.fpcomplete.com/blog/2014/08/conduit-stream-fusion
The project euler website specifically asks that people do not publish solutions to the tasks.
This is because the Haddocks on Hackage are global. So they (the Haddocks) have to be rebuilt pretty often. At least, I'm pretty sure that's why. (someone will correct me if I'm wrong)
Updated the article: "I have been getting a lot of comments about Project Euler specifically asking not to post solutions online and I just want to clarify that the purpose of these articles is **not** denying users the epiphany of discovery. My intention is to spread the word about the amazing combinatorial capabilities of Haskell and at the same time introduce my readers to Project Euler, which I believe is the perfect place to learn any programming language." There are already [plenty of solutions to Project Euler problems on the web](https://www.google.se/search?q=project%20euler%20solutions&amp;rct=j).
So, this is still very much a work in progress, but I'd like to get as much input as possible in the early stages.
Wouldn't this depend on the type of trading system? I'm not sure if Haskell itself would be fast enough for HFT but if you're issuing, say, one order per day?
The next two days or so, I hope. There are some unfortunate issues right now where some users can't connect to servers in our new setup, which I've been trying to resolve (and filed an upstream support issue about). We'll see.
Presumably as doing so will make building, installing and distributing the resulting software a bit easier. 
Not necessarily useful in your case, but one generally useful thing to note is that trees are nearly first class in Haskell through recursive sum types. Since a language expression can be represented by an abstract syntax tree, you can think of Haskell as a language supporting the creation, manipulation, and execution of a host of other languages. This has to do with Haskell's type system rather than the fact that it is functional. If the code you are writing seems like a bad fit for Haskell, sometimes it is a good strategy to instead write it in a small edsl, which you can do without leaving Haskell.
Nice work! Looks like you put a lot of effort into this.
Yeah, I've tried enumerating the inhabitants of a type, too, but the issue I came across is that I could only use it to check that two types were equal. I couldn't figure out how to use syntactic enumeration to normalize a type. As a concrete example, let's use the equality that came up in my discussion with Paolo for natural numbers: n : forall (x : *) -&gt; (x -&gt; x) -&gt; x -&gt; x n Succ (Succ Zero) = Succ (n Succ Zero) You can very easily use inhabitant enumeration to inductively prove those two are equal, but suppose you don't have the right-hand side of the equation in the first place. How would you know to reduce the left-hand side to the right-hand side? Also, do you have a link to your thesis? Note: I haven't yet read your linked papers, so forgive me if this has already been answered there, because I haven't checked yet. Edit: Now that I think about it, maybe just checking for equality is good enough, since equal terms will "eventually" produce the same result.
Speaking only for myself, while I understand the request, I would not honor it. I believe publication of solutions has more value than non-publication. Those that don't want "spoilers" can avoid them easily enough.
I've asked this in IRC before, and it doesn't seem like there's a consistent scheme. If you look at something like Snap, it has `Snap.*` namespace across several packages. For something like postgresql-simple, it puts everything under `Database.PostgreSQL.*` Lens claims `Control.Lens.*` I don't see many company names in packages. No idea where that falls on the scale of suggested versus just hasn't happened yet. My opinion - I don't care. Sharing code, with reasonable docs, under a good license, is all I care about. The details of what exactly it was named doesn't bother me much. I'll have to look up the package name no matter what its called, so no biggie if it reminds me that a company was awesome enough to open source some work. 
A few remarks: - Is it really necessary to have those empty classes? I understand they come with different laws, but practically speaking, I don't think they're worth the extra complexity in a language like Haskell. - I'm not convinced that your `Binoidal` class has anything to do with binoidal categories. As far as I understand, the product in a binoidal category is *not* a bifunctor, so if you really wanted to be precise, you would need to change the class constraints. Again, I don't think it's worth it. - I'm not sure what `inLeft` and `inRight` are supposed to be. - It would be awkward to have this version of `Bifunctor`, which is generic in the categories involved, while `Functor` is simply an endofunctor of a fixed category. Either we generalise `Functor` (which I don't think is realistically possible), or we give this `Bifunctor` a different name (say `GBifunctor`) and include a corresponding `GFunctor` class in base for completeness.
&gt;Is it really necessary to have those empty classes? I understand they come with different laws, but practically speaking, I don't think they're worth the extra complexity in a language like Haskell. I think it is, yes, especially when desugaring proc notation and applying optimisations. It's like, one extra line for an instance declaration, so the use cost isn't exactly through the roof. &gt;I'm not convinced that your Binoidal class has anything to do with binoidal categories. As far as I understand, the product in a binoidal category is not a bifunctor, so if you really wanted to be precise, you would need to change the class constraints. Again, I don't think it's worth it. I'm probably going to remove the Binoidal class for several reasons, including the one you mentioned. I don't really know how to encode a bifunctor as a functor to a functor without going the full on Hask route. &gt;I'm not sure what inLeft and inRight are supposed to be. The injection "functors". &gt;It would be awkward to have this version of Bifunctor, which is generic in the categories involved, while Functor is simply an endofunctor of a fixed category. Either we generalise Functor (which I don't think is realistically possible), or we give this Bifunctor a different name (say GBifunctor) and include a corresponding GFunctor class in base for completeness. Yeah, this is true - I really dislike the current Functor/Category already in base, but eh, what can you do short of writing a whole new Prelude :( I think leave Bifunctor as it is currently named, and add a GFunctor class, maybe. /u/edwardkmett probably has a lot to add here.
Help, I can't unsee this associateRight :: k (p (p a b) c) (p a (p b c)) 
For what it's worth I don't think too many people care about the actual package name. From my end I'm just going to add it to a .cabal file and rarely think of it after that. As far as module naming goes, it'd be nice if things stuck to the nice hierarchical module structure that packages already follow. This has the pleasant property that I can sort my imports and things will naturally group by category. It also means if someone builds off your library they won't potentially misrepresent themselves as you. Suppose you provide `Corp.Foo` and they want to supplement it with a module `Bar`, to keep related things in the same namespace they'd provide `Corp.Foo.Bar`. This has the unfortunate effect that now it seems like this random internet person is releasing things that look like they're from `Corp`. This isn't really a deal breaker and if a company wants to stamp their name on something after making it freely available, more power to them.
I've implemented a separate hierarchy of category classes that you might find useful for inspiration. They are designed specifically for working in subcategories of hask, where differentiating between things like monoidal/braided/groupoid/dagger/etc categories makes a lot more sense to me. Relavent code: https://github.com/mikeizbicki/subhask/blob/master/src/SubHask/Category.hs
Module naming is wildly inconsistent on Hackage. It's so inconsistent that I'm not sure there's a right way to name things. Like, there's categories of Numeric and Numerical - and packages could fit in either one. There's a category for Snap and a category for Web Servers. Pipes and not Control.Pipes but Control.Comonads and not Comonads. Control.Arrows and not Arrows. Overall, it just seems like something we're stuck with. My preference would be to move towards a flatter naming scheme with the module hierarchy being `Library.Whatever` like Snap uses, but I can see the value in having a category prefix like WebServer.Snap. Unfortunately, no one's enforcing a naming scheme so we've ended up in this particular mess.
Feel free to drop into #testing on irc.w3.org for pretty much anything about tests for the web platform. Or just #whatwg on freenode for general stuff (it has no real topic, most of the same people around the test suites are there, and general knowledge about most of the web platform).
&gt; In much the same manner that do notation should give Applicatives where possible, proc notation should give monoidal categories where possible. This is theoretically acceptable because all Arrows are monoidal categories with extra structure. You might be interested in my upcoming project, [Category-Syntax](http://github.com/gelisam/category-syntax), which will use Template Haskell to implement a monad-like syntax for categories, monoidal categories, cartesian categories, etc.
It makes precisely zero sense to add these to the base package. The existing universe of packages is perfectly worthy as a home for this sort of code. New code that's available to *everyone* has to pay its way. While I have no objection to highly abstract code like this existing, it exemplifies a style of programming that is difficult to learn and only used by a very small number of Haskell programmers. I do not want to see base significantly further kmettified.
I'd keep the corporation name out of it; it doesn't really sit well with the open nature of the Haskell community, and besides, package ownerships change all the time, at which point the module name will be a lie. (See also `sun.java.*` vs. `oracle.java.*`). Cabal and Haddock already provide reasonable crediting/attribution support; no need to overload the code itself with it. Other than that, there's not a lot of consistency. Package names tend to show a slight bias towards unix filename like naming (i.e. `my-package-name` rather than `my_package_name` or `MyPackageName`). Modules often, but not always, follow the top-level categories from `base`, e.g. `Control`, `Data`, `Web`, `Text`, etc., especially when they somehow amend or complement existing packages (e.g., if your library is closely linked to `Data.Text`, you might want to consider using something like `Data.Text.MySuperExtensionFeatures`); a package- or project-specific top-level namespace is also popular, especially when the project spans multiple libraries (e.g. Snap). Some packages have even gone back and forth between conventions (I'm looking at you, Parsec!)
Isn't Arrow ``proc`` notation effectively deprecated, I haven't seen it used in a library in like 8 years? I thought it was generally considered to be a misfeature.
I can appreciate this, even if nothing more than an attempt. Tis certainly how the community grows. I think that packages included in base should be immediately available and/or so ubiquitous in their use that it just makes sense. Also should be stable, seems like you are still requesting comments for its engineering. Perhaps, this is not an issue with if this idea should or shouldn't be included in base; rather, could it be it is the state of libraries as a system to the end user. Or, put simply, is there something with Hackage that you don't like.
It seems a little silly to compare AVL trees in Haskell and C++ since AVL trees were designed for imperative languages. It would make more sense to use AVL trees for C++ and something similar to the trees used in the containers library (based on [this paper](http://groups.csail.mit.edu/mac/users/adams/BB/)) for Haskell
I agree wholeheartedly. Some very basic things might belong in base, but I thought our goal was to make base *smaller*. Certainly these classes (the first with four parameters, each of which is higher kinded!) are not commonly used, and should first prove their worth in a normal package. 
His point is that we should these ideas out first as libraries before incorporating them into `base`. `base` is not the appropriate place for experimentation.
Remark: The IO monad is not a way to do “dirty”, “imperative” or “unsafe” things. It's a data structure to functionally, purely and safely declare the input and output of your code. 
I can only up vote once, but once again you are showing sound taste and judgement!
Yes, a fair point. I was using the same language as the original book text.
Thanks. Writing it as a blog post helps me think about things and I thought it might be useful to others.
Indeed, wouldn't have expected that. That's a bit of overhead I'm very much willing to pay for using an immutable data structure. Good to see.
Oh dear, I miss the good old days of operator type variables.
If you're intending to throw real money at the results of the computation, then robustness is at least as important as performance. It sounds like you have a lot of moving parts in mind and need to ensure that they get processed in the correct order. With this in mind, I'd suggest you take a look at the mvc library which is built on top of pipes. It's very useful for separation of IO concerns/event production so that you can nail down the pure logic elements. Here's an example (just charting random numbers) [https://github.com/tonyday567/mvc-emitter] (https://github.com/tonyday567/mvc-emitter) 
There's an argument to made that bifunctors are [common place](http://packdeps.haskellers.com/reverse/bifunctors) enough to be included in base now. I do think this proposal is way over the top in abstraction though, I don't believe any non-category-theory libraries on Hackage use premonoidal categories at all. Someone correct me if I'm wrong.
&gt;You might think that it is unfair to compare multithreaded Haskell against C++ that is not multithreaded. And you’re absolutely right! But let’s be honest, manually working with pthreads in C++ is quite the headache, but parallism in Haskell is super easy. Because working directly with raw pthreads is the only way to write parallel code in C++...
Is that sarcasm? I assume so… because you're right. It's still quite a headache though, just a bit less than before.
To be fair, _I_ don't want to see this in base.
It was a -little- prettier when we could use infix operators for local variables. (a * b) * c ~&gt; a * (b * c) But Simon took those away.
I find it a little surprising that you compare a mature AVL tree library to your own work, particularly from a course you've just taken. Why not take an existing AVL tree implementation, with a comparable amount of optimisation effort put into it? Also, looking at it again, you seem to be including the time it takes to read the data from disk in your measurements in the C++ case.
In this we're in agreement. There is a difference between being capable of writing code that way and believing it is the way you should write everything.
The post is a little confused about big-O notation: for a given `p`, `O(n * log(n) / p + n)` is just `O(n * log(n))`, which is identical to the single-threaded version. In general, adding parallelism across a fixed number of cores/threads shouldn't change the asymptotic behaviour. I imagine if one tweaked some parameters, the fit to Amdahl's law would be much better.
I think it's important to distinguish provenance from function. Provenance is fleeting and ever evolving, whereas function is likely to remain within the original perimeter of the package. That is, where the package might start as the product of one corporation alone, this may or may not hold over time. Whereas the package will (likely) always be e.g. a DSL for interacting safely with databases. I don't find that categories like `Data`, `Control`, `Database`, etc add much semantic information, and sometimes feel pretty contrived. That said, I still use them for consistency with other packages, especially for categories where there is an existing convention of doing so (e.g. `Language`).
The fact that it seems that it is problematic to write and refactor vs plan and execute is worrying. Once you reach a certain scope, you can no longer do the second, so if this kind of system is unfavorable to elegant refactoring, it is probably not useful beyond toy problems...
We implemented our trading system completely as a conduit. One really nice result is that backtesting/realtrading use the same code with the exceptions of the sources and sinks. A couple of thoughts on our experience. Strategies are responsible for maintaining their own state. The transformer that holds most strategies provides significant help for tracking things like outstanding orders, the strategies positions, available capital, order book state and so on. Initially strategies were responsible for their own position sizing. This is being replaced with a strength signal. So when a strategy wants to enter a position it yields relative size of the position. A conduit further down looking across all running strategies decides on the final position size given all other positions that are held and the orders that are open. Risk management plugs in the same way. Performance is good enough if you are not trading latency. If you wanted to than obviously haskell is not a good fit although I could see developing a custom compiler with haskell/llvm that would yield highly optimized code. If you are interested our quickfix bindings are available (https://github.com/alphaHeavy/quickfix-hs). We use them with the IB FIX endpoint. For orderbooks we originally wrote one in haskell. We are replacing it with a set of orderbooks (one per exchange + unified) in C. It hasn't been a huge deal so far since depth data is usually used to confirm backtesting results. Overall we have been fairly happy. One of the downsides has been our ability to upgrade packages that depend on conduit without significant work. We have a significant amount of low level conduit code that required significant work to move from 0.5 to 1.0+. Not saying it will be the same in the future but something to keep in mind. A final thought is that by building a chain of conduits we have a fairly componentized architecture without sacrificing too much performance. 
Nowhere does it say that `p` is assumed to be constant. If you distribute work across thousands of cores, the `n` part will start to dominate over `n * log(n) / p`.
&gt; I strongly believe that requiring confluent rewriting RULES doesn't scale I wholeheartedly agree. When confluence is possible, it's certainly nice to have; but in general we need some system of utility that allows us to trade off between different alternatives in a meaningful way. Confluence is just using a binary utility metric, and such metrics have been uniformly abandoned throughout machine learning and (the non-PL version of) optimization.
Very cool project! Thanks for the link.
No, a package's documentation is (usually) only built once.
Unless you're getting more and more cores as you go, "thousands of cores" is still a fixed `p`, and the `n` part will only dominate over `n * log(n) / p` until `n` gets large enough ... which is the point of talking about asymptotic behavior.
Thousands of cores is a fixed p, just like a certain tree size is a fixed n. So if we can fix any variable, isn't everything O(1)? It's useful to consider how changing p can affect the runtime, and as seen at the end, p doesn't actually give a strictly linear increase.
Yeah, this is almost certainly better off in another package; my mind blanked and I wrote "base" when I should have said core libraries. The main reason for these classes is for a better Arrow notation story, really. They aren't *meant* to be used by most users; mostly designers of FRP libraries and such.
No, please do post it here too. There are plenty of people that frequent /r/haskell that do not frequent /r/ProgrammerHumor (Such as myself.). The worst that can happen is that some douchebags will downvote your submission. If anything, you can link to this thread in the post, so that people know that you aren't just randomly posting it here.
I've changed your AVL implementation to Boost.Intrusive implementation and C++ performance was more than twice faster... You can look at implementation here: https://gist.github.com/anonymous/db02d9c21f4a47726e36 Also not sure what you mean by "C++ is not multithreaded". You can use std::async to run tasks in parallel. For instance, assume we have "int load_task(int)" function, then the following C++ code will do what you did with "par" combinator in Haskell: auto load = [](int i) { return std::async(std::bind(load_task, i)); }; future&lt;int&gt; t1, t2, t3, t4; tie(t1, t2, t3, t4) = make_tuple(load(1), load(2), load(3), load(4)); t1.wait(); t2.wait(); t3.wait(); t4.wait(); 
I agree with that it's worrying, but in a different way. I'm more focused on how to make it work. One way I've been thinking of is attempting to keep a sort of global state (what would effectively be the `Scene` in my Pong example) and pass it as input into the next `Scene`. The idea is that this would allow you to easily tack on more parts that would require reference to the scene. In addition, if you were to add *more* information, such as window size, ortho size, camera position, input state, etc. refactoring and adding more content may prove easier. In that situation, you'd never need to change an external API to a section of code -- for instance with my `Ball` example, it would still take that nebulous state container, but internally it could reference all parts it needed to. I'm not sure if that would entirely fix the problem, or even be programatically feasible, but I think it's worth a try. For some shameless self-promotion: you might just see my next blog post on the subject.
https://ghc.haskell.org/trac/ghc/blog/weekly20140915 &gt;Herbert committed the first part of the Traversable/Foldable changes, by moving the typeclasses to Prelude. This is part of an ongoing series of patches. Things like adding Bifunctor will finally come after this.[5]
C++11 has easy threading, so the comment about pthreads was a little silly.
Note: the `bifunctors` involved are the bog standard haskell `Bifunctor` instances of kind * -&gt; * -&gt; *, from the `bifunctors` package.
So people could use type operators without colons errywhere
To be honest, I kinda prefer the new state of things. Back when we could bind local variables as type operators we still couldn't give them local fixities, so it wasn't all that good. In exchange we can now have (+) and (*) as type operators for type nats, (~&gt;) for hask, (==) as a type operator, etc. and I think that that is worth it.
This thread was exactly what I was looking for. Many thanks.
My memory is that Simon only took them away because u/edwardkmett said he was fine with it (and he had far more code impacted than anyone else). 
That's a really good idea. I could try to implement it as well; though I may wait for another project rather than updating this one.
The main issue is that without the fix, there is reasonable looking code that with GND results in a segfault, not just an `error`.
std::thread? std::future? std::promise? std::packaged_task? Boost.Thread? Boost.ASIO? Boost.AFIO? Boost.Lockfree data-structures (concurrent queues)? Boost.Coroutine? Boost.Fiber? Intel's TBB? Nvidia's Thrust? Microsoft's PPL? std::atomic? Boost.MPI? Boost.Compute? ... and this is just off-the top of my head, pthreads is not even on the list since it is not a library in modern C++ style... EDIT: going through my notes I also find std::async (how could I've missed that?), the implementation of the executors/channels std proposal, transactional memory proposal, openmp, cuda, openacc, std::mutexes and condition variables, std::locking algorithms, facebook's folly (concurrent hash maps)...
I'm pretty sure Haskell did well because the memory allocation pattern here is business as usual for the GHC garbage collector but close to worst case for the C++ memory allocator. In C++ allocation returns stable pointers and there is no heap copying involved. Tree insertion allocates a large number of relatively small chunks, so the allocator has to actively work against fragmentation (and can't do a perfect job at it, so locality will also suffer somewhat). In GHC the act of allocating is uniformly cheap ("bumping a pointer"), but running the copying garbage collector is a considerable cost. C++, by virtue of not having gc, definitely comes out ahead when there is a small number of large allocations. This implies that the overhead for having immutable data is significantly higher than what we'd expect from the graphs here, at first glance. It's just that C++ loses most of its advantage because of the unfavourable memory allocation pattern. EDIT: I see below in the thread /u/andrb speeding up C++ by using Boost.Intrusive, which addresses the memory allocation problem. 
Was the -fllvm flag used during compilation?
Module naming hierarchy in Haskell is completely flat. Really a module name is just a way of importing source files by it's file path in a non-platform-specific way. What I don't do anymore, but I used to do, was place my module names in modules like "Data" if they were data types, "Text" if they were parsers, and "Control" if they were extensions on Monad, Applicative, or Exception. I don't do this anymore because I think if everyone did that there would be too many name space collisions. Now I just create a base module "AppName" which is similar to my package name "appname", and make everything a sub-module of "AppName". Not only will this tend to not conflict with anyone else's modules, it is easier to see at a glance exactly what it is you are importing in import lists in source files. For example, if you see: import AppName.Data.Graph import AppName.Prelude You can tell at a glance that this module is using a graph data structure and a prelude from the "AppName" module, which is presumably from the "AppName" package.
No, even if someone wants to build on your module, they do not need to keep your module naming scheme at all. If I were extending your module, I would do something like: module MyModule.MyLibrary where import Corporation.Library The .cabal file simply needs to indicate that my library depends on your library.
Sure, you can do that, but this lacks consistency. jozefg's point is that with my suggested approach you either have to violate consistency or accuracy.
Meaning, the syntax of colon-less infix operators was removed for local infix type variables so that it could be used for global infix type constructors instead? Well, in that case... How about using colon-prefixed (or more likely another character) names for local infix type variables?
If by inconsistent you mean, for example if the import list looked like this: import AlphaCorp.Library import BetaCorp.Library that there is no way to tell at a glance if BetaCorp.Library has extended the capabilities of AlphaCorp.Library then I suppose that could a problem. Unless BetaCorp decided to enforce a naming scheme like: module BetaCorp.AlphaCorp.Library where import AlphaCorp.Library to indicate how the BetaCorp organization extended AlphaCorp.Library. The source code tree would have to reflect this hierarchy as well, which would make perfect sense from a technical standpoint. You could implement that naming policy in your own organization, that any library that extends another library must have the "Corporation.OtherCorp.Library" naming scheme. But of course you have no control over how others name their packages when extending your libraries. 
I don't personally see where numpy/Pandas is that much of a help when building an actual trading system. It seems to me to be a better replacement for backtesting/research normally done with Matlab/R. But of course, I'm only a dabbler in this stuff so far so what am I missing?
In general I agree with you (although the big O in the article isn't really incorrect, just non-standard). However you seem to be giving too much credit to `log(n)`. It grows really slow -- your millions become just twenty after applying the logarithm (assuming base 2). `n*log(n)` and `n` are so difficult to tell apart (even with no `p` to complicate things) that the Polish Olympiad in Informatics uses hardware counters instead of measuring runtime, so that cache is taken out of the equation.
I think if you find yourself doing something like this: data A = ... data B = ... convertB_toA :: B -&gt; A convertB_toA = ... data C = ... convertC_toA :: C -&gt; A convertC_toA = ... data D = ... convertD_toA :: D -&gt; A convertD_toA = ... polymorphicWithA :: (b -&gt; A) -&gt; b -&gt; Maybe Int polymorphicWithA toA b = ... Go ahead and use type classes and make it work like this instead: data A = ... class ConvertsToA b where { convert_toA :: b -&gt; A } instance ConvertsToA A where { convert_toA = id } data B = ... instance ConvertsToA B where { convert_toA = ... } data C = ... instance ConvertsToA C where { convert_toA = ... } data D = ... instance ConvertsToA D where { convert_toA = ... } polymorphicWithA :: ConvertsToA a =&gt; a -&gt; Maybe Int polymorphicWithA = Without the type class, it starts getting very tedious always writing "convertX_toA" for all different types of X. When you use type classes, you use a single function name "convert_toA" and let the type inference figure out the input type for you. It is also more convenient to use "polymorphicWithA" without having to constantly pass a conversion function (or "id" if you pass a value of type "A") with every call to "polymorphicWithA". Basically, if can convert many data types to a single data type (like the 'Prelude.Show' class) or if you can create many data types from a single data type (like the 'Prelude.Read' class), then you are better off declaring a type class with a function that has a descriptive name for the morphism. 
Notice that even with statefully maintaining a table of precedences this will be impossible, since infix declarations may show up later in the text, e.g. in a where clause!
The problem with use cases like that is `ConvertToA` might not be a globally useful phenomenon, but it's difficult to keep it from leaking globally. Since other than that, it's just a convenience global leaking can be dangerous. Typeclasses become something a little larger when they're not "just conveniences" but instead form interesting algebras. For reasons like this, however, you can use record wildcards to "open" records locally and do standard dictionary-passing style: data MonadD m = MonadD { return :: a -&gt; m a, bind :: (a -&gt; m b) -&gt; (m a -&gt; m b) } data MonadPlusD = MonadPlusD { monadD :: MonadD m, mzero :: m a, mplus :: m a -&gt; m a -&gt; m a} sth :: MonadPlusD m -&gt; (a, a) -&gt; m a sth (MonadPlusD { monadD = {..}, .. }) (a, b) = mplus (return a) (return b) There are still some difficulties which could use a polymorphic solution. For instance, I named the monad superclass dependency `monadD` instead of `monadPlusIsAMonad` which would cause name clashes. That all said, I love an internal typeclass like `Parse` class Parse a where parse :: Parser a but I always am careful to ensure that no functions escape with it as a constraint.
I tried using numpy/pandas a couple years ago and was shocked at how bad it was. Most people I know use them as a prototyping system rather than using it as part of a trading runtime. We process vast amounts of structured and unstructured data so most things are engineered with clusters in mind and not single systems. Thats partly because of the experience/bias we brought from previous high scale experience and partly how the original business was conceived. Numerical stuff comes in a variety of forms. Some places we did the heavy lifting ourselves such as a time window over a time series. Mostly this was done to make human strategy development easier. Other places we just wrap/extend C libraries. For instance we built a haskell wrapper around liblinear and then added automatic ROC thresholding on top of it. If I was starting from scratch I would probably wrap more C libraries to get the first version out the door. Are you thinking of something specific?
A mindset I've found useful when playing with FRP is to take the Wire metaphor at face value and design the program as if I were designing an actual special purpose sorta-digital circuit. That way you can go a long way with pen &amp; paper before you commit to a design. This particular case is quite fitting as the original Pong was a dedicated circuit, and people still implement it straight into FPGAs. Might be fun to look for example at this implementation: https://github.com/MadLittleMods/VHDL-Pong If you can read vhdl I think you could pick a couple of ideas from there and if you think you can't read vhdl you might be surprised about how similar the semantics are to FRP. In fact I'd love to have an FRP library that looked more like one of the Lava hardware descriptions DSLs
This law-less convenience / ad-hoc overloading is probably best handled through [implicit parameters](http://www.haskell.org/ghc/docs/latest/html/users_guide/other-type-extensions.html#implicit-parameters) instead of type classes. Having a type class strongly implies global confluence, and that's not a property of all of these kinds of constructs (diamond inheritance, maybe?). Now, if you really do want to imply global confluence, and perhaps even assume it for correctness (like Data.Set / Data.Map do), then type classes probably are a good idea, even if you can't come up with great laws. While analyzing correctness later, you'll probably stumble upon the laws you want / need. Clear exception: If the rest of your code is Haskell2010 / Haskell98 clean, don't add implicit parameters just for this; you wouldn't be the first person to use type classes this way.
This is a good question. I don't think `deepseq` fits particularly neatly into this picture, but if you have `f :: a :-&gt; b` and you call it like `f ?(force a)` then `a` will be `deepseq`ed before `f` starts to be evaluated.
I'm only a hobby strategy developer as well. But Pandas gives me time-series that I can efficiently slice and dice when experimenting, also makes backtesting fairly easy just because it's so easy to work with the columnar data format. I.E. "pick GOOG and AAPL data from date1 to date2" is trivial. I'm not sure what the replacement would be for that in Haskell, but then again perhaps one should stick to "whatever" for development and Haskell for production? With the numerical stuff, numpy obviously has a lot of things prepackaged but in reality that's just mostly C-wrappers as well, so I could see writing those wrappers myself for production use.
First of all, I was looking for videos lectures, not just slides. "hidden notes" in console was cheesy and kiddish 
It also forces your duck to be the same duck everywhere.
 foo :: t -&gt; t1 -&gt; t1 foo a b = a &lt;&gt; b where infixr 6 &lt;&gt; (&lt;&gt;) x y = y Indeed that's a quirky part of the language, fortunately I think I can ignore it for my purposes.
Regarding your questions about the `arrow` type variable: sadly I don't think there's currently any way to use operators as type variables anymore. This was changed to allow for type operators like `+` and `==`, IIRC.
*"I don't know of much in the Haskell world analogous to "design by contract" from the OO world"* I think much of the reason for that is that run-time assertion failure is a lot less useful when you don't get back a stack trace. Much of the rest is that Haskell types are powerful enough to do some of the job.
Our toolchain has additional objectives such as enforcing holdout sets. We hold columnar price data but most things are just based off the messages (ITCH,OpenBook,etc...) and they are just flat files since you have to start from the beginning. If python works thats great. I don't know if I would build a runtime in haskell just to have it in haskell. Separating research and runtime means you have to build extensive tests to ensure your strategy properly crosses over into production.
You can at least concoct a pair of operators such that a -:c:&gt; b Is an arrow in c, but I tend not to bother.
You're right, 'millions' was a *little* conservative... thanks!
Although "headache" is subjective, I don't know that threading really stands out as a headache in C++, especially since C++11. Certainly not enough to justify comparing a single threaded C++ program with a multithreaded Haskell one. That's not the only problem with the comparison either - the C++ code is written by the author, whereas the Haskell one is an existing, optimized library. The article would have been better as just "Parallelizing AVL Tree Insertion in Haskell" or something. The comparison is meaningless and silly.
Looking at this definition: foldl' :: (a -&gt; b -&gt; a) -&gt; a :-&gt; [b] -&gt; a foldl' f = strictly (\z xs -&gt; case xs of [] -&gt; z y:ys -&gt; foldl' f !(f z y) $ ys) Doesn't the type signature guarantee only that the first `a` that's passed in is evaluated strictly, not every step in the fold? It seems like the implementation could just unpack the `a` value and call the non-strict `foldl`, which is not what we want.
&gt; but I always am careful to ensure that no functions escape with it as a constraint. The only problem I see with this practiec is that if the type class is not available outside the module, no one would be able to instantiate a newtype of A, B, C, or D into the "convert_toA" class. I think usually safe if the class is defining a morphism between concrete data types. Perhaps to avoid confusion in the global context, it may be a good idea to name the method the same one of the class methods, so for example: class ConvertToA b where { convertToA :: b -&gt; A } might have been a better choice of naming. Actully, this is what I usually do. 
That too. Despite the usual pushback by Scilab victims it would be cool to be able to go back and forth from FRP code to pretty wiring diagrams. I've been obsessing for a year about David Spivak's operads and wiring diagrams papers[*] because I have a feel they are key to better FRP APIs, but I always drown in the category-theoretical stuff before I can have a clear vision to actually code anything. * http://arxiv.org/abs/1305.0297 http://arxiv.org/abs/1307.6894
Sure, it could. But the hint is really for the benefit of implementer, not the API user. 
I really like the idea of implicit parameters, and also using data families as members of classes. I think the Haskell class system, and the use of newtypes, and the inability to programmatically define laws for classes, are some fundamental problems that need to be resolved before we can simply say, "lets not use classes for ad-hoc overloading anymore." You probably saw this post a few weeks ago: http://www.reddit.com/r/haskell/comments/2dw3zq/haskells_type_classes_why_we_can_do_better/ I quite liked what this John A De Goes fellow was saying. It would be great if we could improve on how type classes are defined to incorporate laws. I would like it even better if we could make it a compile time error to use classes for ad-hoc overloading, and make such that one must use implicits as the only method of ad-hoc overloading. 
I was under the impression that implicit parameters were rarely used in Haskell in practice.
Shouldn't it be as simple as adding another `FunctionLike` instance which uses `deepSeq` instead of `seq`? Of course, one red herring is that the `NFData` constraint would give rise to the same kind of problems monadic `Set` has with `Ord`. One simple solution could be to impose the `NFData` constraint on all calls to `(?)`, documenting that code which wants to be polymorphic in strictness can only work with types which have `NFData` instances.
That is correct. Not that I'm any kind of authority on it, but I don't think I've *ever* seen them used, probably because they're not even close to standard.
 I don't understand this. How does this leak space? I get that you'll use some space while the thunks are being held but once the while thing is evaluated, won't the space be released? ------ If the input list is of length n we make n recursive calls and build up a thunk whose size is proportional to n. This leaks space.
Nice. My understanding is that "Show" should be used mainly for debugging; in particular, for a given class, (read . show) == id. Often, evaluating the output of "show x" would be legal Haskell code that could be evaluated, and would produce x. E.g. the output of "show Data.Map.empty" is "fromList []". You shouldn't be doing pretty-printing in Show. Instead, create a separate function (or even typeclass) for pretty-printing your data structures.
And (probably) avoiding exporting any functions which depend upon it. It's not so much that it is difficult to do than that it's just 
I've seen implicits used in Scala, and they always seem very dangerous to me. Basically, what a function does can change just based on what names happen to be in scope when you call it.
&gt; I think the Haskell class system, and the use of newtypes, and the inability to programmatically define laws for classes, are some fundamental problems that need to be resolved before we can simply say, "lets not use classes for ad-hoc overloading anymore." The assumption of global confluence is implicit in type classes. That's why (I think) you shouldn't use them for all ad-hoc overloading. I do think laws are very important for type classes (they direct us to the most natural / only valid instance), and expressing them as QuickCheck/SmallCheck/SmartCheck properties is generally possible today, at least to a limited extent. That said, they aren't absolutely essential and even if they were it wouldn't be absolutely essential for them to be validated by the compiler. Your classes just need to be built with the "there's only one (natural) way to do this" assumption in mind. (Until we actually get dependently typed Haskell, there's going to be informal contracts that can't be discharged by a compiler generated proof; even with dependent types, you may not want / need to hand-hold the compiler through all the steps.) Some of the ad-hoc overloading that is proposed for type classes doesn't fit this, and I will recommend against type classes for those purposes. That does include (most) OO-inheritance schemes that support diamond inheritance. &gt; You probably saw this post a few weeks ago: I did, though I didn't comment. I have mixed feelings about his message. I certainly disagree with the characterization of Sum/Product/First/Last (e.g.) newtypes as "abuse". For me, newtype really does mean that the datatype has the same underlying structure but different semantics, so it's natural for some (or all) of the instances for a newtype to be different, in fact it is a way to "refine" the semantics of a type *so that* there is only one valid instance for a particular (or a family of) type classes. Implicit arguments, Scala-style implicits, or even Agda/Idris/Coq style implicit arguments are a way to get some of the advantages of type classes (mainly, nice syntax) without the highlander ("There can be only one") assumption. They probably should replace most uses of type classes that don't need global confluence. &gt; I would like it even better if we could make it a compile time error to use classes for ad-hoc overloading, and make such that one must use implicits as the only method of ad-hoc overloading. Yeah, that wouldn't be so bad, but it'll take a bit of effort both in the compiler and in the libraries. Also, I wouldn't want compiler errors to be dependent on (small) QC runs; I had a Scala project where the QC-equivalent tests only failed about 20% of the time with (or less) the default number of runs. It took me a while before I figured out the problem, and I was glad I could proceed with manual tests even when the automated tests failed; it would have been disasterous for those the be compiler errors, but it would have been annoying. I do like the idea of global confluence being more carefully confirmed; that assumption does make the Data.Set and Data.Map implementations easy to read without sacrificing performance.
similar argument could made against do notation
I think syntax is under appreciated. Even those with the deep knowledge of ghc use classes instead of dictionaries and not by accident. 
What's the point of basically moving definitions from various packages to the base? This is a silly idea. 
I don't understand the hate for do notation. It's *nice*.
Just a thought experiment. Would reflecting laziness, in the types of an imaginary Haskell-like language that is strict by default, would be easier? Just wrapping those things that are lazily evaluated in a `Lazy` type? &gt; lazySum :: Lazy a -&gt; Lazy a -&gt; Lazy a &gt; lazySum a b = Lazy (a+b) Just a, hopefully not too stupid, thought experiment that I cannot oversee the consequences of myself (so i drop it here)..
Well that's my point. It's nice but it's not essential (for whatever reason that is important)
That's a nice solution indeed. In fact, it's implemented in [Idris](http://www.idris-lang.org/).
I do miss being able to get a stack trace from an uncaught exception or a call to `die`. I think I'd even prefer the "weird"/dynamic stack trace to the complete absence of stack trace. Speaking of design-by-contract: While it's far from a new thought, there was enough talk about contracts during day 2 of ICFP to make me think that what industry and teachers really want is something that unifies traditional contracts, dependent type systems, and effect systems. Something where all 3 were opt-in, but when you opted-in, suddenly you were blamed for fewer things or got better performance. You'd start with un(i)typed, no-contract, hack it together ASAP code, and then refine it, first with documentation, then with run-time verified contracts (complete with "correct" blame), and (finally?) to compile-time proof objects. You'd pair it with a module system that carried the (edge) contracts/types/effects around in the "binary", but then tried to discharge/erase them at load time but failing that translated them to actually run-time checks.
Yes, I think you are correct. (Your `lazySum` is probably not the greatest example though because it's always strict anyway, unless the summands are implemented as some sort of lazy Peano numbers I guess. Instead, how about `if :: Bool -&gt; Lazy a -&gt; Lazy a -&gt; Lazy a`.)
This would be REALLY inefficient. Everytime you call a function on a large data structure, you would have to traverse the entire data structure in the call to `rnf`. The `head` functions, for example, would go from `O(1)` to `O(n)`runtime.
I cannot fathom what this has to do with Haskell or functional programming, and the article is horribly misleading to that end. He's literally saying, "Hey, we need functional programming, it's important to be welcoming to newcomers, give the Ada Initiative money to help women get involved in computers." And he is playing off the arrogance and jokes of some well-versed Haskell programmers as a tie-in. He even points this out: &gt; I would rather not talk about diversity, inclusion, feminism, gender, race, and sexuality with my colleagues. The difference between me and -- say, the young male graduate student who attended Wouter's Haskell Symposium talk and later tweeted something to the effect that Europe didn't have a good record when it came to distinguishing people based on race and gender -- isn't how interested we are in lambdas, type theory, theorem proving, compilers, or whatever happens to make our synapses light up. We both are. The difference is that I cannot do my job while ignoring the constant drone of small -- and occasionally big -- indignities and violations that make my friends who are also my colleagues sad and, sometimes, drive them out of the field altogether. Why ask us to pay for something you don't want to talk about? The Ada Initiative is about getting women involved in technology, and this call to action has nothing to do with that: &gt; If you are reading this and you have benefited from your involvement, past or present, with any part of the functional programming community, we need your support. Most of the article is a meander, if well thought-out discussion about how the functional programming community could be more welcoming, more helpful to new converts, and a sprinkling of Tim's political agenda, in a call to action to finance it. I'm not arguing that his views are wrong or right, but this seems like a one-off way to go about funding them. It also talks about how the ACM needs to support an anti-harrassment policy (which is a Good Idea), but not about how giving to the Ada Initiative might possibly help with this. In fact, aside from writing an [example one](https://adainitiative.org/what-we-do/conference-policies/) and talking about it, the Ada Initiative does little to help the problem. It's almost slacktivism at that point.
&gt; but this seems like a one-off way to go about funding them. This seems to me like a way to campaign ACM, show you are serious about it by putting your money where your mouth is (if you can afford to) and serve as a pretext to present The Ada Iniative's work to the people who will be reached by the campaign (and they can decide whether they want to contribute or not, regularly or not). All around rather positive. &gt; If I don't support Ada, he seems to (...) I think I will let [Jess Zimmerman](http://time.com/79357/not-all-men-a-brief-history-of-every-dudes-favorite-argument/) answer this one for me. Anyways, it *is* indeed a group of people (Tim is one of the voices, but far from being the only one feeling strongly about the community needing to be more inclusive, and proactively so) pushing a "political agenda" but I don't see why the question of the legitimacy of these topics here has to be [questioned again](http://www.reddit.com/r/haskell/comments/2fj952/rhaskell_we_have_a_problem_how_can_our_community/ck9v606). Haskell (or any ~~technical~~ community for that matter) does not live in a bubble isolated from the world. Some organisations have already decided to acknowledge the problem they are facing and take actions (e.g. [Hacker School](https://www.hackerschool.com/manual#sec-environment) and its social rules designed to make it a safe place for everyone). It's a good start but not enough.
Well ... yes, it doesn't make a lot of sense to talk about the asymptotic running time for sorting an array of length ten, for instance. If you fix everything, then it just takes as long as it takes. The whole point is that you want to know the (asymptotic) running time as a function of some property of the input problem.
Yeah, you're right, algorithm analysis is done by considering the size of the input alone. I guess traditionally it might be done slightly differently (there would be a complexity analysis, and a scalability analysis), as in http://www.mcs.anl.gov/~itf/dbpp/text/node30.html . The point to be made is still clear with the OP's notation though.
Oooh. Looks like they might have an opening soon, too. I'm a little disappointed that Starbound isn't finished, yet. But, it seems like Chucklefish is a really cool company, both in how they interact with the community outside the company and (as far as I can tell) how they approach development internally.
Wow! This is very well structured and comprehensive. It seems to go beyond the comparison between abstract data types and procedural data abstractions. I was looking for something to deepen the expression problem, but this opens me the road to other interesting concepts. It will take a bit to understand, thanks!
This seems to go exactly to the point! Thanks!
I must say, I greatly prefer the old way.
You are welcome and encouraged to start your own fundraiser for the organization that you think is doing the most to encourage diversity and inclusion in functional programming. I hope that when you do, you link to it in this subreddit.
You should x-post this to /r/haskellgamedev!
Someone beat me to it!
I wonder if there are crunchy Haskell coders out there refusing to use do notation and typeclasses?
Yes.
Just donated! 
It is a separate team within the company, so Starbound is proceeding uninterrupted by this side project. ;)
&gt; Looks like they might have an opening soon, too Oh man, I totally won't qualify, but what the hell I'll give it a shot.
I'm curious why they chose to use elerea considering that it hasn't been updated in 2 years. Is that the most performant option? How do netwire, reactive-banana or sodium compare?
I have found the [space leak zoo](http://blog.ezyang.com/2011/05/space-leak-zoo/) to be an invaluable resource in learning the difference between the various kinds of leaks.
&gt; Haskell (or any technical community for that matter) does not live in a bubble isolated from the world. It is if you stop wrapping everything in IO. Sorry, I couldn't help it. 
What do you mean by "battle tested"?
It's been around for a while and actually used in things.
I never meant to imply otherwise. They've had several new projects since the Starbound Kickstarter ended, but they keep making good progress on Starbound, have 4-5 development blogs a week, and it doesn't seem to be any worse for all the other work they're doing. Naively one might expect that not putting all their resources to work on Starbound slows it down. But, that's not what my experience in IT tells me. There's a "right size" of the team for a project and both adding and removing members has a cost. Plus, you get better work out of people if they are genuinely interested in the project. I'm sure not every developer they have wants to work on C++ / Lua code in a and procedurally generated, science fiction / fantasy, sandbox game. I have little data and a lot of faith that they are doing their best with Starbound, even when they are working on other projects as well.
The [Mythical Man Month](http://en.wikipedia.org/wiki/The_Mythical_Man-Month) agrees. =)
Right now it downloads via HTTP. HTTPS is mostly problematic because it's a political problem to get the necessary dependencies into the Haskell Platform (absorbing the `tls` package would include lots of others). There is no signing - so right now security is pretty minimal. As a spoiler alert, Well-Typed is working with the IHG on implementing cryptographic signing in the hackage server. We think this is a good tradeoff between HTTPS (which requires a bit more infrastructure) and something on the other end of the spectrum, like PGP signatures. Signing can be implemented with minimal amounts of cryptographic code or surface area, so it's a lot easier to deal with as a dependency. The very (very) first part of this is pretty much done: we developed a small, robust [signature library](https://github.com/well-typed/libedsign) based on good primitives and design (Ed25519/scrypt/BLAKE2, with a design based on OpenBSD's "signify") which we can use in the server and client packages (this package, in fact, will obsolete my existing [ed25519 package](http://hackage.haskell.org/package/ed25519)). The next step is actually getting it into the Hackage server, which is more complicated. I imagine we'll announce some more details about the project and design in due time.
I think this post is much more relevant than the last one. I downvoted the last post because it was "clearly" off topic, as it was about a problem which plagued some other subreddits and which I was quite confident didn't occur in our community. Reading this new post made me realize how wrong I was: people in my community do feel attacked, by things like "microaggressions" which were invisible to me until today. It's a very depressing discovery :(
I've often wondered about using a pipeline-style implementation end-to-end (which isn't what we do). It seems like it would be a good fit, but I think we'd end up spending a lot of developer effort to coordinate numerous asynchronous signals. Has anything like this been a problem in your system?
I see. "Leak" here does indeed seem to be a misleading term. I wonder what would be a better description..
I don't think it's a political issue. It's an issue of picking a technical solution (pure Haskell TLS vs C binding) and managing to bundle it so it works all the platforms we support (Windows is a notable problem here.)
That's a nice encoding.
Probably. GHC avoids using the layout rules, so why not.
How do other windows programs manage to access TLS primitives? isn't there a system-wide TLS library provided on windows? (and while I like the prospect of a pure Haskell TLS implementation like the `tls` package, I feel like TLS should be something where there's one or two canonical well-audited implementations everybody uses, as it's so easy to mess up as has been shown in the recent months... and I don't see the `tls` package being subjected to such scrutiny)
Tim did a far better job explaining the problem in details; I was bracing myself for the same adversarial reactions as last time and am happy to see that I was being too pessimistic.
I put a terse description of how we built this on the [Show HN thread](https://news.ycombinator.com/item?id=8329021)
&gt; You make people afraid to talk to me or near me for fear of accidentally becoming the target of another witch hunt. Have you read the social guidelines at Hasker School I linked to earlier? [Here is a more detailled blog post](https://www.hackerschool.com/blog/38-subtle-isms-at-hacker-school) describing them. They all insist on the fact that everything is "lightweight". Witch-hunting is absolutely not the point; the point is that people can point out to things they felt uncomfortable about whilst knowing they won't have to argue about feminism during 8h and will be supported by the community at large.
i had been looking to clojure rather than haskell for game development, but this still sounds like a great boost for functional game dev. after they open source cove, i might take some inspiration from design decisions or even switch over to using haskell more seriously.
Elm is mature? It's not even 1.0!
Is there a clear winner in the FRP scene? It looks like every project uses a different approach. I've been using Yampa and enjoyed it's use of arrows but am aware of the arrow hate. 
Hi there, I'm the lead programmer on Starbound here at Chucklefish and I can tell you you've definitely hit the nail on the head ;D I absolutely promise we're doing our best on Starbound and that we are going to be doing that for the long term, your faith in us means a whole lot :) I've been lurking here in /r/haskell for ages and doing work in haskell and other functional languages on the side, and the language and community around it basically made me fall in love with programming again. Starbound was started quite a while ago under different circumstances, and I chose c++ as a very sort of "safe" choice for a game development language as it's very standard in the industry and I was very experienced in it. Had I known then what I know now, I absolutely would have been more bold and chosen differently. The choice to start a parallel project with a different team and language and development style is the start of a new way of doing things at Chucklefish. Officially it's our "experiment" to see whether a language like Haskell is feasible for game development, but it's already becoming clear that the experiment should be a success. The lead programmer on Wayward Tide (palf_) is incredibly talented and he's sort of leading the way here on our transition, and everything seems to be working out really well and some of the things we've been worried about have turned out not to be too bad at all. My personal goal here is that Haskell becomes Chucklefish's "secret sauce", allowing us to develop much faster and with more agility and safety than before. Even in our C++ development on Starbound Haskell has sort of infected our thinking, internally we have Star::Maybe (which is about 10 seconds away from getting a monad interface), Star::Any (crappy substitute for ADT, but it works), Star::Either (also about 10 seconds away from getting a monad interface), and transform functions to give us what fmap would give us straight away. You can see it in the larger C++ community as well, with std::optional and things like ["I see a monad in your future"](http://bartoszmilewski.com/2014/02/26/c17-i-see-a-monad-in-your-future/). This is the first time I've ever posted anything in /r/haskell, but I lurk here most every day. /r/haskell is practically THE reason Chucklefish is moving to Haskell at all :P So, I guess this reply is sort of directed at the whole community at large but I wanted to say... thanks :)
Is it? I rather like the `A i → B i → A (suc i)` part, but I'm not a fan of the later solutions which require a separate proof object. Stating required properties using separate proof objects is very easy in a dependently-typed language, but it's rarely convenient for the user. I prefer solutions where the compiler is doing more work for me than solutions in which I am doing more work for the compiler :) I'd much rather have, say, a performance-aware for-loop construct which both runs its body _n_ times and multiplies a symbolic representation of the expected runtime by _n_. But these kinds of solutions are much harder to find and to generalize.
If this is a C library, why another library, rather than use OpenBSD's signify directly?
Wow, that's awesome, thanks for sharing and for sure I'll keep an eye on your game company and on the game. I'll make sure to buy it when it will come out to make sure the Haskell side of the company will get properly supported and propelled :)
I might be wrong, but I recall Tsuru Capital was using it for frontend stuff. Hardly you can find something more battle tested than something which needs to work for financial trading :)
I've never written anything dependently typed, so it all looks impressive to me!
It sort of started off as a rip of `signify` actually, and if you look at it, most of the code is quite similar in spirit still. We figured reusability (read: an API) was worth going for, but `signify` is just an executable. I mostly refactored it for distribution purposes, and switched out some of the underlying primitives. While the C files in the source are separated, they get amalgamated into one single file for distribution (like SQLite), which makes it much easier to use in arbitrary projects. Replacing the primitives was mostly out of the fact there are portable, fast implementations of things like scrypt/blake to supplant some of the reused parts of the OpenBSD code (bcrypt/SHA), while still offering high security. In the end, it's about what I would have expected from refactoring it into a library, given that was a goal. The resulting package is a bit more flexible than my old ed25519 package, and supports things like passwords for signing keys (via scrypt) and key fingerprints. `signify` also does a few extra things like base64 encode the output and support comments in signatures, which I don't think we'll end up supporting or needing in this library itself, at least not right now.
I dunno, I think it is a political problem, at least partially. The TLS library written by Vincent already works on Windows, doesn't it? There's really nothing stopping anyone from proposing it to become part of the platform right now, for the next major release. Then it would be reasonable as a dependency for `cabal install`. In fact I have long been puzzled why nobody has, but IMO, it is because the resulting discussion will invariably involve not just TLS, but the large dependency chain it has to bring along, as well as things like stability/API issues, and your regular bikeshedding. These really are somewhat political discussions because these packages have to be held to a high standard for long periods of time. I also wonder if things like TLS are perhaps best left out of the Platform after all. Unless maintainers want to dedicate lots of time to security bugfixes, it seems like something as sensitive and large as a TLS library with a lot of security concerns might want to be free of the "long term" aspects of something like the platform. Lots of other projects like Python have had noticeably broken SSL defaults in their standard library. Maybe it's best left to leave alone for now.
I think that these issues come from wanting typeclasses to be something they are not / not valuing the thing they are. To get them to be first class citizens you have to give up coherence of instance resolution. Period. Typeclasses have the lovely property that all diagrams in the category of constraints commute. This means all sorts of concerns about where you got an instance from go away. There can be only one -- modulo silliness with incoherent instances where you get what it says on the tin. I'm not willing to give that up to get the flexibility of implicits. That flexibility comes at the price of all ability to reason about coherence, the very thing that makes moving the use of instances to the use site correct. Once you can care about where a thing comes from you have to care about where a thing comes from. There is some merit to the complaint that typeclasses aren't first class citizens, but you need some generative notion of how to construct a new type locally to avoid destroying coherence. But this can be done within the system without losing coherence! Just like how ST can use a phantom type parameter to delimit a region with effects, we can use one to delimit a region with a constructed type. http://hackage.haskell.org/package/reflection exists and lets me do the things that folks want with local instances without giving up coherence.
The number of libraries used in production that "aren't even 1.0" these days is pretty extreme. Partly because of how software development has grown, and also because there is a kind of reticence lately in the dev community around "official support." Many libraries are open source, and those authors may want to work on something because they like it, or it interests them, but they don't want to do support, so the library sits forever in .9.0.1 land so its never an official "release." Forever Beta. 
&gt; /r/haskell is practically THE reason Chucklefish is moving to Haskell at all [Bagsy](http://www.urbandictionary.com/define.php?term=bagsy) first on the credits. :P
I tried writing a game engine around Elerea. It doesn't really take on a FRP front end for developing your game, though. As such, I definitely didn't harness it to its greatest potential. I've moved to Netwire recently and I've found that it's very, very powerful. I'm not even close to mastering it (I'm in that state of learning when you realize just how much more there is to go), but I can tell it's ridiculously useful. Unfortunately I can't compare it to Yampa or reactive-banana, seeing as I haven't used either one. EDIT: no there isn't a clear winner.
I was talking in general not specifically. Elm is a great library, so what if its not mature, to base your work off of elm isn't a bad idea its got great ideas and a pretty awesome implementation, sure its got a long way to go, but that's part of the fun! 
I only brought it up because they called it mature. I like elm!
&gt; Haskell doesn't have type-level lambdas Any particular reason you are trying to do this in Haskell instead of a more proof-oriented language? Agda, for example, does have type-level lambdas.
I'm going to go ahead and suggest Agda or Idris for this. Haskell doesn't really have uninhabited types, and Haskell's types generally have more inhabitants then you might think (\_|_ can hide everywhere!). That makes it a poor choice of theorem proving, IMO. You'll find also that both Agda and Idris support most (any?) term appearing at the "type level", as long as you don't have a universe inconsistency. Agda seems to be a nicer environment for theorem proving, while Idris seems to generate better executables and has more backends. So, that could serve as a guideline for which to learn if you are unfamiliar with both. My expertise is definitely not in theorem proving or logics, but ISTR that if you are trying to prove something in a logic other than the logic that matches the underlying type theory (form both Agda and Idris, predicate logic)--the "ambient" logic, you'll first have to work an either an embedding of your logic in the ambient logic, or add additional, unprov(en/able) axioms to the ambient logic. If there's too much of a mismatch between predicate logic and modal logic and you are willing to "throw away" the Haskell-like syntax of Agda/Idris, Isabelle/HOL or Coq might have better "ambient" logics.
Thanks! I hoped that there'd be a simple Haskell solution because it would be accessible to a wider audience, but it looks like I'll need to switch to Agda or Idris just for the type-level lambdas. (I never understood why Haskell doesn't have them, given that type inference is already undecidable, but oh well.) On the plus side, I finally have a reason to play with dependently typed languages! It seems easy enough to add new axioms and modalities by using a typeclass as I did, but the mismatch between logics is troublesome. I heard somewhere that the diagonal lemma isn't a good match for intuitionistic logics, and I've never seen anyone encoding it in a functional language. Not sure if it even makes sense.
Out of curiosity, what are you planning on signing? e.g. the `.tar.gz` files themselves, or a manifest of filenames and BLAKE2 checksums? Going with a manifest file would enable one to compress hackage quite a bit more compactly and still preserve the signatures, whereas signing the `.tar.gz` (and _especially_ the `.gz`) basically ties you to one particular storage format. It's true that Hackage is only a couple of gigabytes, but it still seems silly to eliminate the possibility of a more compact format...
&gt; if you are trying to prove something in a logic other than the logic that matches the underlying type theory That would be for logics with different structural rules, like linear logic. Modal logics are fine. *edit*: I was wrong about that, see my [comment](http://www.reddit.com/r/haskell/comments/2gnsfg/using_haskell_to_study_provability_logic/ckl6wz9) above.
Why are we shouting!?
With that said, I switched to Erlang for my day job (because of changing jobs, so not by choice, exactly) two months ago, and after 14 years of doing Haskell, I miss GHC's error messages and at least *some* of Haskell's documentation a lot. The Haskell community -- particularly Simon PJ, but lots of others, too -- have put a *lot* of work into making error messages better over the past 14 years. I can tell you that as a new Haskell programmer in 2000, trying to understand some of the error messages from GHC 4.0.* was no fun (not to mention waiting all night for the compiler to recompile, but that's a different story). There's always room for improvement, but a lot of progress has been made already -- especially when comparing GHC to other open-source compilers, and considering how few people have actually been paid to work on Haskell or GHC full-time (as far as GHC, the only person who has had it as their full-time job for all of the past 14 years is SPJ -- as prolific as he is, he's still just one person!)
This is an exciting series already! Really glad to see this is the next step in the evolution of Hython.
For the fixed point axiom, you can try: {-# LANGUAGE GADTs #-} -- actually what we want is existential quantification data Fixpoint f where Fixpoint :: (psi -&gt; f (p psi)) -&gt; (f (p psi) -&gt; psi) -&gt; Fixpoint f p And just having `fix :: Fixpoint f p` in the class. The idea is that the both functions in `Fixpoint` should form an isomorphism, which logically means `psi` and `f (p psi)` are equivalent. This will be a bitch to actually handle, specially if you want to follow the proof from Wikipedia: I don't think it's impossible, just hard. Btw, `f` doesn't belong as a parameter to Prov: the `fix` of a certain `p`should be polymorphic on all `f`, or at least a class of them.
For my convenience I made a small wrapper around `ghc-modi` that makes it behave similar to hdevtools: Persisting itself with a UNIX socket. This way, editors using it do not have to keep a persistent pipe open to communicate with `ghc-modi`'s shell. It's just a proof of concept, but perhaps it's useful for somebody.
Translated Wikipedia's proof to Agda for you: open import Data.Empty open import Level open import Relation.Nullary open import Data.Product open import Relation.Binary.PropositionalEquality id : ∀ {α} {A : Set α} → A → A id x = x record Iso {α} (a : Set α) (b : Set α) : Set (suc α) where constructor mkIso field from : a → b to : b → a record Modal {α} (□ : Set α → Set α) : Set (suc α) where constructor mkModal field mp : ∀ {a b} → □ (a → b) → □ a → □ b return : ∀ {a} → a → □ a fix : (f : Set α → Set α) → ∃ (λ ψ → Iso ψ (f (□ ψ))) löb : ∀ {α} □ → Modal {α} □ → (p : Set α) → (□ p → p) → p löb □ modal p consistency = line14 where line2 : ∃ (λ ψ → Iso ψ (□ ψ → p)) line2 = Modal.fix modal (λ x → x → p) ψ = proj₁ line2 equiv = proj₂ line2 line3 : ψ → (□ ψ → p) line3 = Iso.from equiv line4 : □ (ψ → □ ψ → p) line4 = Modal.return modal line3 line5 : □ ψ → □ (□ ψ → p) line5 = Modal.mp modal line4 line6 : □ (□ ψ → p) → □ (□ ψ) → □ p line6 = Modal.mp modal line7 : □ ψ → □ (□ ψ) → □ p line7 = λ x → line6 (line5 x) line8 : □ ψ → □ (□ ψ) line8 = Modal.return modal line9 : □ ψ → □ p line9 = λ x → line7 x (line8 x) line10 : □ ψ → p line10 = λ x → consistency (line9 x) line11 : (□ ψ → p) → ψ line11 = Iso.to equiv line12 : ψ line12 = line11 line10 line13 : □ ψ line13 = Modal.return modal line12 line14 : p line14 = line10 line13 Even if you don't know Agda, you should be able to understand this, at least somewhat, if you are familiar with the more advanced parts of Haskell. Some points: * Just ignore the alphas. They are not important * A `Set` is like Haskell's `*`, only here it's just a type. It takes an argument because there are many levels of it(just like the alphas, just ignore) * The equivalent of a haskell typeclass here is a plain old record. You can replace typeclasses with records on Haskell too, so nothing special here. They are also inspected mostly like in Haskell, only you normally prepend the key with the name of the record: so instead of `mp` we have `Modal.mp`. `Iso` is a record too. * On the `löb` function, I just wrote each line as a term in a where clause: the type is what it proves, and the definition is how it is proved from the previous ones. What Wikipedia calls line 1 is here an argument instead: called `consistency`. * Psi is the fixpoint used in the proof, and `equiv` is the isomorphism that proves that it is, in fact, a fixpoint.
`attoparsec` is as efficient, but my main issue is that `attoparsec`'s backtracking is unreliable (see [this issue](https://github.com/bos/attoparsec/issues/76) for an example of the problem). Some of the high-efficiency primitives that `attoparsec` provides break backtracking.
&gt;Alright, so the first notable difference is that the Haskell code is much shorter. It takes up roughly half the space that the C++ code does, yet they both output hello world when the correct number is entered. Actually they are exactly the same length if you ignore lines with just { or } on them. There are lots of nicer things about haskell than C++, the length of a hello world program is not one of them.
&gt; I hoped that there'd be a simple Haskell solution because it would be accessible to a wider audience, but it looks like I'll need to switch to Agda or Idris just for the type-level lambdas. (I never understood why Haskell doesn't have them, given that type inference is already undecidable, but oh well.) It's not a matter of undecidability, it's that your type inference problems would become underdetermined. That is, in most cases, the problems would be easily decidable, with the answer "I don't know which type to pick". That doesn't mesh well with type classes where the type chosen affects the runtime semantics.
Yeah, I think this was the problem I ran into when I was trying to use attoparsec myself at the beginning this year (for something relatively simple -- parsing JSON and similar things). It confused me that I had to scatter `skipSpace` (or combinators that did the equivalent) everywhere; though i remember having to do some of that in Parsec 2 as well, my brain still wants to think of lexing as already having been done before parsing.
It is possible to split lexing and parsing in two separate steps, even with parser combinators. I'm surprised that nobody has actually done this.
With that said, though, you haven't lived until you've seen that single line because you made GHC coredump :)
Haskell and C++ are very different languages and a lot of the examples simply aren't comparable situations. In the example of omitting the arguments to strlen and length, the code *'let size = length'* is perfectly valid Haskell. It's the next line which actually produces the error.
Great, was planning on doing exactly that myself. Will try to check it out soon.
1 day: CSV parser in parsec / attoparsec. 3 days: XML parser in parsec / attoparsec. 5 days: C parser in attoparsec. (Or some other nearly context-free language) 7 days: (standalone) C interpreter. (Or a version of the language you implemented on day 5 with a minimal standard library.) Also, check out CodinGame and CodeCup.nl. Also, if you just give yourself a few days to think about it, there's probably a small (web?) application that you'd use a couple of times a month. Implementing it would teach you the surface of quite a few libraries.
That's awesome, thanks! There's just one snag with the assumption "a→□a". You could view it as an encoding of the Necessitation Rule, but I'm not convinced that it's the right encoding. Is the statement "a→□a" even true in provability logic? And Wikipedia explicitly mentions the rule "□a→□□a", which wouldn't be needed if we already had "a→□a".
Unless I'm missing something, the second rule is actually superfluous: the first rule is that from `a` we can conclude `□a`: if that's not `a→□a`, what is it?
There's no upsetting with that typesetting
Very excited to see how this works out!
Haskell game!
Any chance this library could be contributed back to OpenBSD by refactoring signify on top? Always nervous when a security library is maintained by a single party in relative isolation...
What OS and browser are you running on? I can take a look. The code samples need a better color scheme for sure. FWIW, I used to fret endlessly about getting things perfect, now I ship imperfectly and refine when people complain. Effective use of time, and all that.
The first two errors are obviously just there because nobody took the pain to make a more intelligent handling of these cases (not throwing any stone here, there's lot of fantastic work on GHC, this is just not much of a priority for the community). The remaining errors are much more tricky as mainly consequence of the better expressiveness of Haskell (I think the fourth one is actually handled ok, you just have to get past the "technical jargon" to get a good suggestion). The comment by Mike provides a good suggestion for the third error though. Sure there are situation were you want your function type to be an instance of a typeclass but they're pretty rare so it makes sense to provide an hint that you may have forgotten some arguments. A seasoned Haskeller would see a "missing instance (Show (a -&gt; b))" as an obvious indication that this is the problem but a beginner ? Not so much... I think that's basically the way forward to improve GHC errors : getting more intelligent suggestions as to what the problem really is by pattern matching more classic mistakes. I would prefer to keep the "technical jargon" though since I personally find it very helpful to diagnostic the errors once you have more experience (and run into less classic problems).
Yes, it's been a while since I looked at Helium, but I believe they have the concept of "language levels", similar to what Racket does. So you can choose to be at the most "beginner" level if you know you're a beginner, and level up as you learn more. (Racket is often used by intro programming instructors who can configure which level their students are at and give them access to more advanced stuff as the term/semester goes on.)
It would be nice if the compiler provided both a friendly message and a technical one. I also agree that "intelligent suggestions" -- ones that may occasionally be wrong, but for novices, are likely to be correct -- are probably the way to go. When I worked on Rust, we were trying to do that more, and were inspired by clang in that regard. Here's some more ambitious work about user-friendly compiler errors: http://gidget.ischool.uw.edu/ (Occasionally, GHC does look a little like Gidget, for example, with the "compiler panic" messages like "my brain exploded" :)
&gt; What's actually meant by the first rule is that all inference rules in the ambient logic are also valid under □. I don't know any simple way to encode that yet. What do you mean by "the ambient logic"? Do you mean Agda, or some logic whose proofs are being manipulated by provability logic? I thought that usually, in a modal logic, the rule (⊢ A =&gt; ⊢ ◻A) denoted the fact that tautologies *of the modal logic* were true in all worlds, as opposed to theorems of some other logic.
Thanks for the information. I've been looking around at Conduit after your post. I had in my mind that my system would generate candles as described in the original post. I guess in conduit the way to do this would be something like: data Candle = None | Candle { int high; int low; int open; int close } data Candles = Candles { Candle tick; Candle oneMinute; Candle fiveMinute; ... Then just fuse together a bunch of conduits that take a previous tick/candle and either pass None if they aren't ready yet or the candle when they have enough information. Then the signal generator would do a similar trick when it doesn't have a signal and put a signal when it does (e.g. RSI starts constantly generating after 14 candles). Is this kind of the idea? It seems like a lot of baggage to carry around since candles like e.g. 4 hour will almost always be passing nothing. Is there a better way with conduit? Is there a better way with the pipe library? I wanted to do the candle generation this way so I don't have a bunch of signal generators all doing the same work.
Agda has intuitionistic logic, so that argument doesn't work. I tried to construct: unsound-return : ∀ {α} □ → Modal {α} □ → ∃ (λ ψ → ¬ ψ × □ ψ) unsound-return □ modal with Modal.fix modal (λ x → ¬ x) unsound-return □ modal | ψ , equiv = ψ , (λ x → Iso.from equiv x (Modal.return modal x)) , {!!} `{!!}` is the part I couldn't fill: it should have type `□ ψ`, but proving that like in your argument would require double negation, something that Agda doesn't have. The maximum you can prove is that there is a statement with `¬ ψ × ¬ (¬ (□ ψ))`. I don't know the implications of this. (I'd like to think of it as "there is a false statement whose falsity can't be proven", which sounds really sensible, but I don't think it works like that).
That's a fair point, but I'd still prefer to rely only on assumptions that hold in classical provability logic (GL). We're just lucky that the proof we're trying to translate doesn't use excluded middle :-) I don't know if you can make an intuitionistic version of GL where a→□a is okay, it seems fishy to me.
Adding a type signature to `size` certainly helps the situation. Sprinkling type signatures everywhere is my go-to solution when I get type errors I don't understand. Usually it catches the error earlier.
You would know better than me, but take a look at http://fsharpforfunandprofit.com/posts/recipe-part2/ I think that's a pretty lucid introduction. (i meant that as an inspiration for further posts)
I would certainly donate to that.
I just link people to [this monad tutorial](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html).
Sounds like you have an interesting story waiting to be told. Would you like to share? I haven't encountered GHC coredumps myself, except for FFI stuff, where GHC was not to blame. I did encounter a nasty GHC bug though, one of the hardest Haskell bugs I have ever tracked down. Here is the story. While working on [Hawk](https://github.com/gelisam/hawk), my collaborator told me that our test suite was indicating a failure on his Linux machine: ### Error in tests/System/Console/Hawk/PreludeTests.hs:43: expression `test ["-d:", "-m"] "head" "passwd"' fd:5: hGetLine: end of file reference: fd:4: hClose: resource vanished (Broken pipe) Test suite reference: FAIL It was quite a strange error message, as "passwd" was a small, ordinary text file, and we were not doing anything fancy with it. Since the test suite was working fine on my OS X machine, I suspected that something was wrong with his configuration. Since we have some files which are both compiled and interpreted, my initial hypothesis was that the compiled version didn't match the source. We tried cleaning his sandbox, his ~/.cabal, his ~/.ghc, re-cloning the Hawk repo, even reinstalling GHC, but nothing helped. Suspecting that the error might be Linux-specific, I spawned a virtual Linux machine, installed Hawk, ran the tests, but I still couldn't reproduce the failures. Clearly, the problem was unique to his machine. In order to eliminate the influence of all the other files on his machine, I asked him to reproduce the problem on a virtual machine. He could still reproduce it. Okay, so maybe it's not something which is unique to his machine, but something unique that he does when configuring his machine. We cleared our respective virtual machines, installed the exact same version of Linux on it, then we installed Hawk by typing the exact same sequence of commands, those I was usually typing, and now neither of us had the problem. We cleared the machines again, installed Hawk by typing his typical sequence of commands, and boom! We could now both reproduce the problem. I'm telling the story quickly here, this was like a month after we first encountered the issue, and I was relieved to finally being able to observe the problem. I immediately started digging, and I discovered that it wasn't the test which raised the exception, it was our testing tool, [doctest](https://github.com/sol/doctest#readme). Which, in turn, was running ghci, which was crashing with the polite error message "GHCi cannot safely continue in this situation. Exiting now. Sorry." By crashing, ghci was closing its stdout handle earlier then expected by doctest, which led to the `hClose` exception. GHCi runtime linker: fatal error: I found a duplicate definition for symbol ___stginit_stringsearchzm0zi3zi6zi5_DataziByteStringziSearch whilst processing object file /Users/gelisam/.cabal/lib/x86_64-osx-ghc-7.6.3/stringsearch-0.3.6.5/libHSstringsearch-0.3.6.5.a This could be caused by: * Loading two different object files which export the same symbol * Specifying the same object file twice on the GHCi command line * An incorrect `package.conf' entry, causing some object to be loaded twice. GHCi cannot safely continue in this situation. Exiting now. Sorry. Okay, so one way of installing Hawk led to ghci crashing, while the other way was fine. What was the difference? Turns out my collaborator was installing the latest version of [cabal-install](http://hackage.haskell.org/package/cabal-install), while I was using a stock version. I alternated between the two versions during the various steps of the installation, and I eventually figured out that in order for the issue to occur, Cabal-1.18 had to be installed while installing the stringsearch package. Yes, I can see "stringsearch" in ghci's polite error message, but why string search?? What does that have to do with anything? Anyway, I git-bisected the commits of `Cabal` and `cabal-install` in order to find the commit which introduced the regression, and I found [this](https://github.com/haskell/cabal/commit/48efd4b59093f20572d69a1e1276e2eaa8ed3cff): configVerbosity = Flag normal, - configGHCiLib = Flag True, + configGHCiLib = Flag False, configStripExes = Flag True, That is, some configuration was now turned off by default. At that point I had learned a lot more about the differences in behavior between Cabal-1.16 and Cabal-1.18 on the stringsearch package, so I knew that turning off this flag was disabling the creation of a `libHSstringsearch-0.3.6.5.o` file, in addition to the `libHSstringsearch-0.3.6.5.a` file which was always created. That was also what the commit message was saying: "there no need to build .o files for ghci by default". Okay, but why was this missing .o file causing ghci to observe duplicate symbols? If anything, there should be missing symbols. I decided to build ghc from source in order to find out. I quickly found the place which was printing the polite error message, and I tried something weird: I commented it out. I wanted to see what kind of horrible consequences were being prevented by exiting early and politely. To my surprise, nothing bad happened! The error message was encountered many more times, for each symbol of many more packages; stringsearch simply happened to be the first which was exhibiting the problem. But leaving the duplicate symbols in was apparently fine, as the commented-out version allowed all our tests to pass again. It turns out the bug was [this](https://ghc.haskell.org/trac/ghc/ticket/8942). GHC can parse either .o or .a files in order to extract the symbols within. The .o implementation came first. One implementation detail was a check to skip .o files which had already been loaded. In the new .a implementation, this check was missing. So if the Cabal flag was on, the .o would be generated, so the old .o codepath would be used, and the check would happen. Otherwise, the check would be skipped, so the same .a file could be loaded twice, leading to lots of duplicate symbols. It was quite an adventure, but I was happy to have finally found a solution! My fix in now in GHC HEAD.
Not as interesting as yours, I'm afraid! In 2006, I did a bit of work on getting ticky-ticky profiling to work again. I was doing this as a means to an end (and it turned out to not be what I needed anyway). This involved hacking on the RTS, and it's very easy to make GHC segfault if you're doing that, since it's written in C :) (Somehow, I managed to even break some stuff in a way that made it past code review, though Simon M. was nice enough to clean up after me.) I've also seen segfaults that I didn't cause, but not recently, and presumably due to bugs related to either the RTS or FFI.
You are half-crunchy.
Why do `sendEmail` and `sendEmailBlaze` return a `Either SESError ()`? It seems a little strange, at least at first glance, to have a `Either SESError ()`, since `()` is effectively a placeholder type. I would consider going with `Maybe SESError`. The downside is that usually `Nothing` indicates a failure, whereas in this case it would indicate a success. To avoid either, I'd probably go with data Result = Error SESError | Success or something like that.
Are you on a phone, or some unusual browser? There's no bright green anywhere for me, the links and syntax are different colors, and the font is bigger than reddit's default.
Upvoted, even though I disagree, because I understand the frustration. I've found that the most toxic communities are the ones where talking about politics is totally taboo -- there's no way to address genuine issues, so the problem festers. I'm very happy that folks like Tim can share their experiences, and that people in this community are committed to keeping it healthy and making people feel welcome. AFAICT, 1 in 100 or so articles on the frontpage here are devoted to this, and I don't think it's too much of a price to pay -- I find it very painful to read this stuff sometimes, but I think it's important. I don't find anything in the OP that smacks of gender warfare -- do you have anything in mind?
Find something you do every day and try to automate it.
I've heard that advice a lot. Problem being, there's nothing that I do daily that takes enough effort that it's worth automating.
Example 1 is a bit like comparing apples with oranges. Perhaps you could split it into two... Example 1.a (leaving out an else) if( in == 1 ) { cout &lt;&lt; "Hello, World!" &lt;&lt; endl; } { cout &lt;&lt; "Error, wrong choice" &lt;&lt; endl; } I don't think you get any compiler error message at all (just the runtime error)! Perhaps modern compilers have a warning to catch this? Example 1.b (leaving out brackets) You can include brackets in the Haskell "if", but they are not needed and there is no error if you leave them out (no runtime error either). The same is not true of "when" and "unless". So compare if in == 1 { cout &lt;&lt; "Hello, World!" &lt;&lt; endl; } With when in == 1 $ do putStrLn "Hello, World!" This has happened to me more often in Haskell than it did in C++ (probably because "if" does not need the brackets so you get used to omitting them) and the error message is not great.
Great suggestion. My reasoning behind it was that Left is usually associated with failure, and Right success. Maybe didn't seem to make sense (as you said, Nothing indicating success or failure is weird). I've made the changes you've suggested and put a new version up. Thanks.
The parser in GCC is handwritten whereas GHC's is generated. It's no surprise that error messages are a lot worse in GHC. There's room for improvement here but I guess it's not really a huge priority since the grammar for Haskell isn't all that complicated (whereas C++'s grammar is much, much worse). OTOH, I'm not really sure you can argue that C++ has better type errors when something as simple as #include &lt;iostream&gt; struct Foo {}; int main() { std::cout &lt;&lt; Foo(); } will yield a barf that's often longer than the height of the screen. And let's not mention what horrors you might encounter when using a heavily templated library such as Boost. The semantic equivalent in Haskell would be something like: data Foo = Foo -- deriving Show main = print Foo which produces a much cleaner error message thanks to type classes.
What do you think the guidelines are destroying?
Unfortunately, I'm in China, I'm new to Haskell, I know nobody else that programs in Haskell, I have no idea who to get to help out, and I have no experience in managing money for a project. I'm kind of an island here. You can rest assured that if I see someone singling someone out for personal reasons, I'm usually going to question it (on social media as well as in person, unless I'm worried about being physically hurt). I've never seen such a thing in the Haskell community. There are great role models, many humble people, and I really like the overall spirit. I don't see how any women would feel unwelcome here; I've probably talked to several without realising it, and that's the way it should be.
No this has not been a problem. We designed it from the ground up to be highly asyncronous. A signal from an external system is just a message, like one from a broker, the exchange, data from the current exchange or other data streams. 
Well how about write a Reddit browser (or just an inbox checker perhaps).
Good! In that case, we can interpret "⊢", "□" and "⇾" as opaque symbols, with no relationship to Agda's functions or contexts. postulate ⊢_ : Set → Set postulate □_ : Set → Set postulate _⇾_ : Set → Set → Set infixr 0 _⇾_ postulate rule1 : ∀ {A} → ⊢ A → ⊢ □ A postulate rule2 : ∀ {A} → ⊢ (□ A ⇾ □ □ A) postulate rule3 : ∀ {A B} → ⊢ (□ (A ⇾ B) ⇾ (□ A ⇾ □ B)) In addition to the inference rules mentioned on the Wikipedia page, I have found that a few more rules were used in some of the proof steps on that page. -- implicit rules used by the löb proof postulate modus-ponens : ∀ {A B} → ⊢ (A ⇾ B) → ⊢ A → ⊢ B postulate modus-ponens₂ : ∀ {A B C} → ⊢ (A ⇾ B ⇾ C) → ⊢ (A ⇾ B) → ⊢ (A ⇾ C) postulate compose : ∀ {A B C} → ⊢ (A ⇾ B) → ⊢ (B ⇾ C) → ⊢ (A ⇾ C) Note that there are two kinds of arrows: "→" is Agda's function arrow, while "⇾" is an some opaque symbol. So the first modus-ponens rule says that for any formulas A and B, if I have a value of type "⊢ (A ⇾ B)" and a value of type "⊢ A", you can construct a value of type "⊢ B". That is, we are postulating the following inference rule: ⊢ (A ⇾ B) ⊢ A ------------------- ⊢ B Next, I postulate the existence of modal fixed points. I use "fix F" instead of Ψ because the fixed point formula could be different for each F. postulate fix_ : (Set → Set) → Set postulate Ψ⇾F : ∀ {F} → ⊢ (fix F ⇾ F (□ fix F)) postulate F⇾Ψ : ∀ {F} → ⊢ (F (□ fix F) ⇾ fix F) We're now ready to replicate Wikipedia's proof. For step 1, we are asked to assume a modal sentence P such that "⊢ (□ P ⇾ P)". postulate P : Set postulate sound : ⊢ (□ P ⇾ P) For step 2, a particular F is chosen, and we find its modal fixed point. This is the only fixed point we're going to talk about from now on, so I give it the convenient name Ψ. -- step 2 F : Set → Set F X = X ⇾ P Ψ : Set Ψ = fix F Now the proof really begins: we are done postulating, from now on we are only using previously-defined rules to prove each step. For step 3, we use one of the properties of fixed points to prove "⊢ (Ψ ⇾ (□ Ψ ⇾ P))". step3 : ⊢ (Ψ ⇾ (□ Ψ ⇾ P)) step3 = Ψ⇾F Step 4 says that "From rule of inference 1, it follows that ⊢ □ (Ψ ⇾ (□ Ψ ⇾ P))". This means we can apply the rule1 inference defined earlier to a value of the appropriate type, namely step3, in order to obtain the required conclusion. step4 : ⊢ □ (Ψ ⇾ (□ Ψ ⇾ P)) step4 = rule1 step3 Similarly, step 4 says that "From 4 and rule of inference 3, it follows that ⊢ (□ Ψ ⇾ □ (□ Ψ ⇾ P))". The instruction don't explicitly say that we need to use modus ponens, but when we look at the types involved, we don't really have a choice. step5 : ⊢ (□ Ψ ⇾ □ (□ Ψ ⇾ P)) step5 = modus-ponens rule3 step4 ...and so on for steps 6 through 14. After I put all this in a file "Main.agda" whose first line is "module Main where", I can ask Agda to check whether I made any mistake: $ agda Main.agda Checking Main (/Users/gelisam/working/agda/base/src/Main.agda). Finished Main. If I did make a mistake, the agda executable would give a type error. Interestingly, I did not use any type-level lambdas in this proof, nor any particularly fancy dependent types. So I think it should be possible to do the proof in Haskell after all! *edit*: silly me, "F X = X ⇾ P" is obviously a type level lambda. Which can be encoded as a newtype, at the cost of extra inference rules "⊢ F X ⇾ (X ⇾ P)" and "⊢ (X ⇾ P) ⇾ F X", just like you said.
Stepping back, if you are just candlestick trading there are lots of pieces of software out there that supply all the infrastructure. You just bring the strategy. Why not use something like Ninja Trader?
Yes. We're talking about heuristics -- it's inevitably going to be imprecise, but that also means that somebody with relatively little experience could do the implementation.
You and me both. May the best man win (i.e. someone else entirely). :D
It looks like it could be a nice introduction, but it seems to be unfinished. The sentence "This means that attributes like styles and transformations can be accumulated downwards, while" just trails off, the parallelism is a TODO comment, etc.
I'm not the person you're replying to, but what they said is a pretty accurate description of what I see on my phone, except the green on white seems fine to me. I'm using Chrome on Android (a stock Nexus 5, if that helps.) [Screenshot]( http://www.imgur.com/rafA2iD.png )
My favorite example of how Haskell error messages are less friendly that other languages is this: suppose you meant to type `1 + 1`, but you missed the `+` key and accidentally typed `1 1` instead. What error message do you get? Here's Python: % python Python 2.7.5 (default, Mar 9 2014, 22:15:05) [GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwin Type "help", "copyright", "credits" or "license" for more information. &gt;&gt;&gt; 1 1 File "&lt;stdin&gt;", line 1 1 1 ^ SyntaxError: invalid syntax Now here's Haskell: % ghci GHCi, version 7.8.3: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Prelude&gt; 1 1 &lt;interactive&gt;:2:1: Could not deduce (Num (a0 -&gt; t)) arising from the ambiguity check for ‘it’ from the context (Num (a -&gt; t), Num a) bound by the inferred type for ‘it’: (Num (a -&gt; t), Num a) =&gt; t at &lt;interactive&gt;:2:1-4 The type variable ‘a0’ is ambiguous When checking that ‘it’ has the inferred type ‘forall a t. (Num (a -&gt; t), Num a) =&gt; t’ Probable cause: the inferred type is ambiguous Python tells you: "You typed something that makes no sense." Haskell instead tells you (simplifying a bit): "You're trying to call the number `1` as a function, passing it the number `1` as its argument, but you have not provided any definitions that allow me to treat numbers as functions that take numbers as arguments." Except that it doesn't tell it as succinctly as I just did...
Yes, overloaded numeric literals -- while useful -- are also a big source of confusing error messages. I think a mode, or compiler flag (or IDE) for beginning programmers that basically assumed you never want to add your own type class instances would be the right thing, as per Helium.
Funny to see spectral sequences, they don't seem to come up very often nowadays. Maybe because simplicial resolutions are almost as easy to invent as monads? ;-D
No, I don't think I've seen that. Thanks!
Can you also elaborate on what you mean by unit is a placeholder type?
I wrote a lexer/parser combo for a language with significant indentation, because it was so much simpler to think of indentations as lexemes instead of trying to parse everything as characters. It was surprisingly painless, although the library is written to favor `Char` and a few of the most useful combinators (`anyOf`, `noneOf`, `char`, `anyChar`) that are unfortunately less general than they should be...
I'm just simulating at the moment and candle trading is just the start. I plan to add other source data when I get up and running.
I've done it in rparsec and it was really helpful. Mixing the two is just ugly. Another interesting approach is to split by broad and detail level parsing. For example: parenthesized expressions, begin/end blocks, and statements all were nodes in an AST with unparsed contents that were then recursively parsed into finer detailed structures. Obviously this isn't possible for all grammars. Despite a few rough edges, it did work for me. Implementation was simpler and I could get useful results even when only a small portion of the grammar was implemented. I can't speak to performance as I was using ruby 1.8.
I just looked at that error, and that's not unreliability; it's exactly what you'd expect. In fact it's the same as what `Parsec` would give you: &gt;&gt;&gt; parseTest ((spaces *&gt; char ' ') &lt;* eof) " " parse error at (line 1, column 2): unexpected end of input expecting space or " " The `spaces` parser grabs as many spaces as it can; the `char ' '` requires a space to succeed, but there are no more spaces, so it fails. The way you'd get around this is to use lookahead to stop before grabbing too much. Parsec makes this easy (with `try` and `lookAhead` depending on desired behavior). I'm not familiar with attoparsec so I don't know if it has similar primitives.
You misunderstand, attoparsec's combinators backtrack by default, unlike parsec's. Tekmo discovered a "bug" in which skipSpaces does not actually backtrack like he expected, which bos indicates was done for efficiency reasons.
visit /r/dailyprogrammer
Inequality in tech is so hard for me (as a white male) to grasp. I thought all you needed was an Internet connection and some useful code to contribute to open source? I don't know (or care) the gender of most reddit commenters I read. To raise the profile of women (or any other group) shouldn't we just celebrate their successes by upvoting? 
Your original use of `Either` was perfectly sensible, although having a special type for it may communicate better.
Now define maximum/minimum as (head . sort) and (last . sort) respectively.
I prefer `Either Err ()` because it works with the `Either` monad and can easily be lifted into `transformer`'s `ExceptT`. if you prefer to have a special type, use a type alias type Result e = Either e () and with [pattern synonyms](http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#pattern-synonyms) you can even give the reasonable constructor aliases `Error` and `Success`.
I wonder why nobody has mentioned horrific C++ Template error messages yet ... searching through 500 lines of error descriptions until you find something that remotely looks like it could be related to something wrong in your code is ... interesting ... to say the least.
Any update on this? Looking at hackage's "recent additions" it seems that some are built, but many aren't. In particular the previously affected packages still haven't been built.
Yes, that seems to be completely right. Thanks! ...so there's no hope of reusing the function arrow as the implication arrow?
&gt; Without `mempty` and those laws, associativity is still an interesting property. Types that have this property can be an instance of the `Semigroup` typeclass. However, since `Monoid` is in the `base` libraries, it's much more commonly used. Maybe `Semigroup` ought to be added to `base` as well to alleviate that deficiency. Seems a bit odd that `Monoid` is in `base` but the algebraic structure it builds upon is not...
Very good advice.
I did [report](https://ghc.haskell.org/trac/ghc/ticket/8942) the bug to GHC, but not to Cabal. Would you like to open one, or should I? If you do, perhaps the GHC version for which this flag should be turned on isn't 7.8, but 7.10, since without my fix, it still isn't safe to rely on the .a files. Btw, are you still hiring? :)
&gt; I never understood why Haskell doesn't have them, given that type inference is already undecidable, but oh well. To elaborate on the other replies, suppose you're working with Functors. You're calling `fmap f :: f a -&gt; f b` on something of type `Either x y`. This creates the type equality constraint `f a ~ Either x y` that needs to be solved. GHC cuts this up into: * `f ~ Either x` * `a ~ y` which is solved easily. But with type-level lambdas this could also have been: * `f ~ /\ t . Either t y` * `a ~ x` or even much weirder solutions like `f ~ /\ t . Either x y` or `f ~ /\ t . Either t t`. As /u/rwbarton posted, this problem is underdetermined and you'd get "I don't know which type to pick"-errors any time you'd try to use `fmap`, unless you manually give it a type signature. But with the limitation of 1 instance per type, type-level lamdas for instances is possible, which you could [read my MSc thesis on this subject](https://xnyhps.nl/~thijs/share/paper.pdf).
I don't usually do it, but I have. Just means writing two parsers -- one on the text stream and one on the token stream.
On the contrary! My first version used "→", but I changed it to "⇾" because I thought it was confusing to use the same arrow for two different purposes. If you do use "→", it shouldn't change the logic at all, because none of our rules allow ordinary values of type "A → B" to be converted to a "⊢ (A → B)" or a "□ (A → B)". Now that I'm thinking about it though, I think it might be fine to add a generic rule to convert ordinary functions "A → B" into opaque formulas "⊢ (A → B)" . It would replace the "implicit rules" like modus ponens I had to add, and it would add a lot more formulas, such as useful things like "⊢ (A → A)", but also nonsensical things like "⊢ (⊢ A → ⊢ A)". I don't think we need to worry too much about those: we didn't give any rule for extracting an "A" out of a "⊢ A", so there should be no way to prove a valid formula by abusing a nonsensical one. The main thing we need to worry about is whether this generic rule would allow to prove incorrect stuff like "⊢ (A → □A)". I think this should also be fine, because we didn't postulate any way to construct or deconstruct a value of type "□A". We will be able to prove things like "⊢ (□A → □A)", though, but that looks just fine.
I'm actually toying around with a custom language parser, and I haven't found any useful resource of how to do a parser for a language with significant identation, so I fell back in the trap of semicolons and curly braces. Could you post an example on how the parsing looks like for it, minus everything else in the language (expressions,statements,etc)?
&gt; Big thanks to Koen who I suspect made all of this happen.
&gt; Now that I'm thinking about it though, I think it might be fine to add a generic rule to convert ordinary functions "A → B" into opaque formulas "|- (A → B)". I wouldn't do that. It's nicer to have a guarantee that everything under |- is actually provable in our modal logic (GL), not just true in Agda. Though having fewer extra rules would certainly be nice. I'll try to think of some refactorings. Again, thanks for the hard work! It'll take me some time to figure out everything, because I never used Agda before, but at first blush it seems to be exactly what I wanted.
Yeah, I caught that part. Google says he's some kind of CS professor in Sweden, which still doesn't tell me much.
Actually, LR parsers have a better story for error messages than parser combinators do -- you don't need *any* modifications to the grammar. It's described in a 2003 ACM TOPLAS paper, [Generating LR Syntax Error Messages from Examples](http://people.via.ecp.fr/~stilgar/doc/compilo/parser/Generating%20LR%20Syntax%20Error%20Messages.pdf), by Clinton L. Jeffrey. The idea is that the parse state and input token can be viewed as a summary of the context. So, to automatically generate good error messages, you do the following: 1. First generate an LR parser from a grammar as usual. 2. Then, construct a list of erroneous programs and an English description of the error. 3. Run the parser on each erroneous program to find the parse state when the error happens. 4. Use this state info to generate an error routine which checks the state/input, and then issues the appropriate error description. Interestingly, Go uses this technique -- [Russ Cox describes his experiences implementing it in this blog post](http://research.swtch.com/yyerror). It would be a good project to add support for this technique to happy. 
That's a *really* good suggestion.
Sorry, I wasn't attempting to undermine the work that has already been done. You're right, it's remarkable what has been done by an unpaid community.
I'm curious what error message the helium compiler would give. (Hopefully better ones than ghc! ). Helium should be install able from hackage as of the past month, and is actively deved again. It's certainly the case that parser error messages could use some love, all that's needed is people who are willing and able to make it so!
Nice work! I'm looking forward to the next article.
Regarding `mapM`: The docs state that `mapM f = sequence . map f`, which is always how I get my head around it. First map the list elements and get a list of effects `[m a]`, then sequence the effects in the list to an effectful list `m [a]`. In particular, for the program to typecheck you have to use `mapM` instead of `map`.
There's one affordance Scala offers here that I've missed in the Haskell universe: @implicitNotFound("Can't find a JSON format for ${A}. To see a list of common types that have a default format, see http://foo. If you want to define a JSON format for a custom type, see http://bar") trait JsonFormat[A] { ... } For those unfamiliar with Scala, this provides a hook for a library author to provide a custom method when 'typeclass resolution' fails. It's quite limited, but still frequently useful: it turns out that most resolution errors have fairly standard causes, and the library author can provide better hints than the compiler can about how to fix it. I feel like a Haskell pragma ought to be able to do one better, with errors attached to specific methods and with more specific errors. One ought to be able to get the compiler to spit out stuff like: No Num instance for String -&gt; Int. Perhaps you forgot to provide an argument here? The &gt;-&gt; operator chains two pipes together, but the pipe on the left doesn't have any output! No Monad instance found in call to mapM. In particular, this means the second argument is not a list. 
Are you able to share any of your Shake files?
It's not unambiguous, because the parse state in an LR parser is a *summary* of the shape of the string, but it turns out that the parse state is actually the summary you want to generate good error messages. Suppose that we have a language of arithmetic expressions. Now, we give the error processor an input like: Bad Program: (1 + 2) 3 Message: Missing arithmetic operator and then it tries to parse the bad program, with a parse trace something like: nil | ( 1 + 2 ) 3 ( | 1 + 2 ) 3 ( 1 | + 2 ) 3 ( EXP | + 2 ) 3 ( EXP + | 2 ) 3 ( EXP + 2 | ) 3 ( EXP + EXP | ) 3 ( EXP | ) 3 ( EXP ) | 3 EXP | 3 EXP 3 | EXP EXP | At the last state it gets stuck because there is no way to reduce EXP EXP with an arithmetic expression. So now it will use the parse stack EXP EXP as the summary, and whenever it gets stuck in this state, it issues the message "Missing arithmetic operator". The really nice thing is that you can take a problematic example, and then write an error message for it, and then it will *just work* for all similar examples. So you can steadily improve error messages as programmers run into problems, and you have the confidence that you are not accidentally changing your language definition by improving the error messages. 
Thanks Evan! When you say a pain to debug, do you mean it works but does the wrong thing? Or things spuriously rebuild/don't rebuild? Or you get runtime errors? I'm very keen to know where the pain points are, since Shake has lots of data about what happened and why, and maybe we can present that in a more accessible way.
You can implement this in terms of `(&lt;*&gt;)`, which is already a kind of product or "boolean and". Returning just the last item can be done with `(*&gt;)`. Note also that the Alternative type class says nothing about returning the first succeeding item. It is equally valid to return the last one, or all of them. Which of these happens is up to the instance. For example, for `[]`, `(&lt;|&gt;) = (++)`. 
(FWIW, I'd expect a `&lt;&amp;&gt;` to be something 'and'-ish, like `f a -&gt; f b -&gt; f (a, b)` or something. Which is just `liftA2 (,)`.)
Isn't that what `(&gt;&gt;)` in `Monad` or `(*&gt;)` in `Applicative` does? Also, if `a` is not idempotent, its effects before failing will occur twice.
Just use Demorgan's laws! a &lt;&amp;&gt; b = &lt;!&gt; ((&lt;!&gt; a) &lt;|&gt; (&lt;!&gt; b)) Right??! Right!
Oh man, I can't belive I didn't see that! Yeah, `*&gt;` is exactly what I want
While they are the worst of the worst, [tgceec](http://tgceec.tumblr.com/) showed that a 3 line C++ file can generate a multi-gigabyte error message.
&gt; For example, for [], (&lt;|&gt;) = (++). Who had the idea for this implementation? I can't think of a less intuitive one. 
I've got a cool project that I enjoyed. I download NASA's picture of the day, and add it to a folder I draw upon for my desktop backgrounds. I also learnt about cron and apple's thing to replace it with. If you want a small example on downloading things, there was some question on Reddit about some JSON formatted pokemon thingamajig. You can see it on github here: https://github.com/tbelaire/http-pokemon/blob/master/src/main.hs Other ideas about automatable things... * Shopping list generator. Select what meals you want, and it'll list the ingredients that need to be bought each time. Maybe even going on the web to get ingredients from recipe sites. * XMonad modes. If you're using XMonad, you can write new tiling methods in Haskell. It's actually a well written project, you should read its source sometime. (Would anyone be interested in a blog post examining it and putting it together from an outsider's viewpoint?) * Friend annoyer. Track if they've been on facebook when their schedule says they are supposed to be in class, and then scold them. * Website to help schedule get-togethers with your friends. Have a proposed event have a unique url, then a bunch of checkboxes of when they are busy, and see when everyone else can make it. * IRC bot. * Automatic game player. Have a program learn to play some simple game, maybe that rock pushing game, or even Mario if you are feeling ambitious. If you want help, or even to work together on any of these, I'd be glad. I'm also in a similar position of not having a small project to work on.
It matches List's Monad/Applicative where a List is treated as a non-deterministic value.
 Well, perhaps it's just me, but the first time trying &lt;|&gt; for lists I expected it to work like: [] &lt;|&gt; bs = bs as &lt;|&gt; [] = as as &lt;|&gt; bs = as 
Indeed, I didn't finish it (this was first posted a year or so ago). Sorry it's unfinished, but hopefully someone finds it interesting / educational anyway!
Thanks to /u/dstcruz !
I'm working on a simple Linear Algebra library. I'm trying to include things like type safety, and experimenting with different data structures. It's slow progress, but I'm learning as I go along.
If you could open the bug report, I'd appreciate it, you have much more solid data than I do (my experience comes to "building didn't work and I got annoyed and wiped out my package database"). If you give me a link to the issue, I'll throw in my $0.02. We're actively hiring for a UI position right now, and will be posting about another position (a systems engineer) in the next few days, though that's not primarily a Haskell development position. But we're always looking for good people, if you're interested you should definitely send me an email with a CV and what you'd be interested in working in.
It sounds pretty and-ish to me, though that could just be because it smells of Lisp's `and`, and how it can be used to sequence and short-circuit potentially failing operations.
Recently I've been writing simple interpreters, and it's really helped me to feel more comfortable with the language, in particular with writing monadic code. First I wrote a [Brainfuck interpreter](https://github.com/mdunsmuir/languages/blob/master/brainfuck/hfrack.hs), then a [Befunge interpreter](https://github.com/mdunsmuir/languages/tree/master/hefunge), and I just wrote a [PL/0 parser/interpreter](https://github.com/mdunsmuir/languages/tree/master/pl0) that I want to build a compiler module for too. But these are all pretty simple things that I finished in a day or two; I'd say that building a parser and interpreter (and compiler?) for a more complex language would be a great project if you find it interesting. Another idea I had was to build a text adventure game engine... I think Haskell would be pretty well suited to that sort of problem. I've also messed around a bit with OpenGL, but it wasn't as much fun as some of these other things.
You want to talk about being inclusive? How about assuming I'm a cishet white male just because (a) I'm post on reddit and (b) I said something that might be seen as a disagreement with Tim's politics. Really, that Jess Zimmerman link just makes me feel so very welcome. So now not only have you single-handedly told me that I believe the wrong thing, but you've marginalized me by assuming I adhered to a specific gender and racial identity based on a set of political views. You didn't even address my real point, which is that it's unclear *how* the Ada Initiative goes about using this money to enforce an anti-harrassment policy. You just got condescending, and the downvotes that everyone else piled on certainly didn't help. I'd like to thank all of my downvoters for making it clear that this isn't a forum for discussion, just one where I can be on your side or be silenced by reddit's built-in silencing system. Thanks for making me feel welcome, really. But you'll excuse me while I hit the unsubscribe button sitting on the right side of the screen, and let this subreddit drop out of my life. Edit: for the record, I'm neither cis or het or male, but I am leaving Edit2: more silencing. excellent.
The idea is that lists represent possibilities. That's how the monad and applicative instances are wired up. When you do ls &gt;&gt;= g, the idea is that you start with a set of possible states, apply a function that can have any number of possible results and get a list of *all* the possibilities after that. You can think of lists, when used in this way, representing a backtracking search through some search space. This works especially well because the lists are lazy: you can stop as soon as you've found what you're looking for rather than evaluating the whole list. With this logic, `&lt;|&gt;` starts to make sense. If you have one set of possibilities `[1, 2, 3]` and another `[4, 5, 6]`, then having one *or* the other gives you `[1, 2, 3, 4, 5, 6]`. I don't think I explained that very well. [Here](http://jelv.is/talks/nondeterminism.html) are some slides with pictures and an example which might help, although they're a bit terse.
It seems to me that GHC could use the functional dependency in MonadWriter to produce the more reasonable message. It just doesn't do that. Or am I missing something?
 Couldn't match expected type ‘[(Int, a)]’ with actual type ‘(t0, a)’ Great! What's actually different between the two?
It's only one data point, but the codebase I'm working on maintained the lexer/parser separation even when we were using parsec.
&gt; Could not deduce (MonadWriter &gt; (Int, a) (WriterT [(Int, a)] Data.Functor.Identity.Identity)) This actually has all the information you need... if you're familiar with MonadWriter. I think using type families would produce a nicer error message. Let's try! {-# LANGUAGE TypeFamilies #-} import Data.Monoid class WriterC m where type LogVal m tell :: LogVal m -&gt; m () newtype WriterT w m a = WriterT { runWriterT :: m (w, a) } instance (Monad m, Monoid w) =&gt; Monad (WriterT w m) where return a = WriterT (return (mempty, a)) (WriterT mwa) &gt;&gt;= f = WriterT $ mwa &gt;&gt;= \ ~(w, a) -&gt; do ~(w', b) &lt;- runWriterT (f a) return (w &lt;&gt; w', b) newtype Identity a = Identity { runIdentity :: a } instance Monad Identity where return a = Identity a Identity a &gt;&gt;= f = f a type Writer w = WriterT w Identity instance (Monad m) =&gt; WriterC (WriterT w m) where type LogVal (WriterT w m) = w tell w = WriterT (return (w, ())) f ::(Eq a) =&gt; a -&gt; (Int, a) -&gt; Writer [(Int, a)] (Int, a) f y (n,x) | y == x = return (n+1, x) | otherwise = do tell (n,x) return (1,y) ---- And the error messages: writer.hs:33:33: Couldn't match type `a' with `(Int, a)' `a' is a rigid type variable bound by the type signature for f :: Eq a =&gt; a -&gt; (Int, a) -&gt; Writer [(Int, a)] (Int, a) at writer.hs:32:1 In the first argument of `tell', namely `(n, x)' In a stmt of a 'do' block: tell (n, x) In the expression: do { tell (n, x); return (1, y) } writer.hs:33:33: Couldn't match type `[]' with `(,) Int' In the first argument of `tell', namely `(n, x)' In a stmt of a 'do' block: tell (n, x) In the expression: do { tell (n, x); return (1, y) } Ugh. It looks like GHC is trying too hard to guess what it should be, rather than failing a little earlier with a more helpful message.
There are two ways to implement monad transformers: * Approach #1: `lift` all effect calls explicitly (this is what `transformers` does) * Approach #2: Implicitly lift effect calls using type classes (this is what `mtl` does) Approach #2 is more powerful (there are some effects that `lift` cannot lift), but it gives worse type errors when things go wrong. This is why I recommend that most beginners stick with `transformers` until they feel comfortable reasoning about the types of concrete monad transformer stacks. Note that `mtl` builds on top of `transformers`. It's just a difference interface to the same types. The two libraries have to use different modules to avoid stepping on each other's namespaces, so the convention is that `transformers` has an additional `Trans` in the name and `mtl` does not.
I wonder if the UndecidableInstances pragma makes it less willing to make such semplifications. By the way, we could also give GHC a hint by rewriting the instance like so: instance (Monoid w, Monad m, w ~ w') =&gt; MonadWriter w (WriterT w' m) where 
I wish that this information was displayed more prominently. Coming from Learn You a Haskell, you don't really know which to import, other than what Hoogle returns. Also, I think it'd be great to include those descriptions in the hackage pages for the modules, so it's easier to learn what the difference is when you wander over there from hoogle.
&gt; You didn't even address my real point, which is that it's unclear how the Ada Initiative goes about using this money to enforce an anti-harrassment policy. https://adainitiative.org/what-we-do/conference-policies/ has lots of details about how the Ada Initiative encourages anti-harassment policy adoption (it can't enforce policies on any other organization other than itself -- just as we as Haskell programmers can't enforce a rule that Google has to write all their code in Haskell, much as some of us would like to). I hope that helps! I realize green_mage said "I am leaving", but perhaps other people have the same question and haven't spoken up, so now they don't have to wonder anymore.
There is no instance for MonadWriter (Int, a) (Writer [(Int, a)]) regardless of whether (Int, a) has a Monoid instance, because the MonadWriter instance for Writer looks like MonadWriter w (Writer w). GHC could know, since it can see this instance, and there is a functional dependency m -&gt; w in MonadWriter w m, that the user hasn't simply failed to define an instance (that could unify with) MonadWriter (Int, a) (Writer [(Int, a)]), and instead must mean for (Int, a) and [(Int, a)] to be the same type. But it doesn't apply this kind of logic when producing type errors, apparently. (edit: accidentally a word)
&gt; Ugh. It looks like GHC is trying too hard to guess what it should be, rather than failing a little earlier with a more helpful message. Frustrating. I wonder why GHC tries to break down the type in this case, but not in the OP's second program.
Indeed, I've been using Haskell for a year and a half and I still didn't know that mtl builds upon transformers, or that there was any convention to the module naming scheme at all. I usually just import one of the two, iron out any lazy/strict mismatches, and pray to Church that it compiles. The more you know!
TIL about `apiary`.
I'd love to donate, but I'm a poor PhD student who can only just afford food and rent. Is there another way I can contribute? I'm happy to petition the ACM, but I doubt they'd listen to me.
I'm betting `semigroups` is high because of packages that pull it in as a dependency; does this app try and correct for that at all?
We're gonna need a bigger monad.
It would be neat if you could compute: {# of downloads} - {# of downloads of downstream libraries} ... so that you got some idea of how many people install the package directly, instead of pulling it in transitively.
Even $1.00 counts! That said, I'm planning a blog post on tim.dreamwidth.org about ways people can contribute besides/beyond giving money, hopefully to appear before the end of the campaign on Friday. Thanks for asking.
Is it stupid of me to try to actually make this make sense in this context?
Does Alternative have laws that might help? (Or even laws at all?)
Note: Parsec will also tell you exactly where the error was encountered, generally providing that info can help debug the issue. Your problem is probably that `lookAhead (string "&lt;/")` is consuming the `'&lt;'` from '&lt;foo&gt;' before it fails. `lookAhead p` won't consume input if it succeeds, but if `p` fails *and* consumes input (which `string` can do) then `lookAhead p` will also consume that input. Try using `(try $ lookAhead (string "&lt;/"))` and see if that works.
I'm surprised that [the documentation](http://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Applicative.html#t:Alternative) doesn't list laws for Alternative. Since the description states that an Alternative is a "monoid on applicative functors", the laws must be that `(&lt;|&gt;)` is associative and that `empty` is its identity element: (x &lt;|&gt; y) &lt;|&gt; z = x &lt;|&gt; (y &lt;|&gt; z) empty &lt;|&gt; x = x x &lt;|&gt; empty = x
In case you were not familiar with it, I think he's making a joke based on this line: https://www.google.com/search?q=we%27re+gonna+need+a+bigger+boat
Perfect. Thanks!
If you are serious about trying to make money stick with languages you know. There are plenty of problems to be solved building a profitable strategy. 
[Here's a list](http://tim.dreamwidth.org/1858021.html) of some things people can do beyond giving money. Perhaps some will be much easier for faculty or industry workers to do than for grad students, but I still hope it gives you some ideas!
those laws are easy. the hard ones are how it interacts with the operations in `Applicative` (beyond naturality). 
Unfortunately, that doesn't really clarify things. `[a]` is isomorphic to `First (NonEmpty a)`. The [behavior implemented ](http://www.reddit.com/r/haskell/comments/2grjwy/the_for_things_with/ckltnb3) is the `Monoid [a]` instance and the [behavior expected](http://www.reddit.com/r/haskell/comments/2grjwy/the_for_things_with/cklv2e7) is the `Monoid (First (NonEmpty a))` instance. So, do we need stronger laws? I suppose defining `Alternative a = (Applicative a, forall r. Monoid (a r))` isn't possible in Haskell, and in a language where is would be possible, it wouldn't be clarifying anyway. (I'm assuming that if type classes / dicts are first-class you can't also have global coherence.)
No. It's a sign of normalcy. Like some jokes, it requires the absurd to not look absurd *immediately*. `&lt;!&gt;` was something I quickly recognized as not-a-haskell-operator (and Hoogle [agrees](http://www.haskell.org/hoogle/?hoogle=%3C!%3E) with me). But, with all the other operators that get thrown around looking like &lt;[C-operator]&gt; or .[C-operator]., it looks like a possible Haskell operator at first, and people only a little familiar with the language could easily be forgiven for thinking it was one. Oddly enough, if you could define `&lt;!&gt;` and `&lt;&amp;&gt;`, you probably would want this as one of the laws relating `&lt;!&gt;`, `&lt;|&gt;`, and `&lt;&amp;&gt;`, if not as the implementation of `&lt;&amp;&gt;`. In Haskell, usefully defining `&lt;!&gt;` is difficult because you'd have to be able to apply it to `pure x` where `x` can take on not just any value, but also any type.
This is not mine, originally posted by /u/tekn04, but the [code is on github](https://github.com/ScrambledEggsOnToast/names) and it's in haskell. I thought some people here might like it.
I remember I would add something like an oracle rule, and then either more or less than I expected would rebuild. I'd want to ask a question like "why did this file rebuild?" or even "why did this not rebuild?" I messed around with the report stuff, and got some dependency graphs, but I still don't really understand the query language, and it's pretty easy to create a giant graph that hangs. I'm sure what I want to know is in the report, just not sure how to get it out. I think to be usefully specific I'd have to be in the middle of debugging something, and thankfully I don't spend a lot of time doing that... but in general it would be nice to ask "what about build/obj/Blah/M.hs.o" and have it say "rebuilt because X's timestamp changed from A to B" or "oracle answer changed from A to B". Or I could say "who does X depend on" and it'll list the things that will cause X to rebuild if they change. One other thing is that dependency cycles tend to cause a hang. Sometimes you get an error msg, but sometimes it just hangs halfway through the build. Sometimes I forget about that, and waste a bit of time trying to figure out why "ghc" is hanging.
Parsec doesn't do backtracking by default because it's expensive, and not every parser needs it.
I submitted a feature request on the GHC Trac, [#9613](https://ghc.haskell.org/trac/ghc/ticket/9613).
Wow, this is excellent. It is nice to know what everyone else is using, makes choosing implementations for experimental features much easer.
&gt; Elerea is something we inherited from Helm Granted, Helms API is modeled after Elm.
You had to define data types _mostly_ like this in LCF/ML, a very early ML dialect. You didn't use an explicit fixpoint, but you did have to describe your type in terms of sums, and then defined the constructors and destructors in terms of operations on those. For example, in the DEC-10 dialect of LCF/ML, a user-defined list-of-integers type looked something like this: absrectype lst = (int # lst) + . with cons x xs = abslst(inl(x,xs)) and nil = abslst(inr ()) and null l = not(isl(replst l)) and head l = fst(outl(replst l)) and tail l = snd(outl(replst l));; This defines a type `lst` which is defined like the Haskell type `Either (Int, Lst) ()` (the `#` operator defines a pair, and `.` was the unit type), and in the block afterwards you define the constructors (`cons` and `nil`) and the accessors (`null`, `head`, and `tail`) in terms of the constructors and destructors on sums and products. Within that block, you have the functions `abslst : ((int # lst) + .) -&gt; lst` and `replst : lst -&gt; ((int # lst) + .)` that you have to use to explicitly wrap and unwrap a type. Those two functions are not usable anywhere outside of that block. ...it's an incredibly tedious system, but it's interesting historically. You can see some examples in [Hagino's codata paper](http://www.tom.sfc.keio.ac.jp/~hagino/codat.pdf), which I point to because I can't find any online version of the actual language description. (It's _Edinburgh LCF: Lecture Notes in Computer Science_ by Gordon, Milner, and Wordsworth, 1979, if you'd like to look it up in a library.)
In regards to a blog post examining XMonad, that'd be amazing!
These are sometimes called data types à la Carte, after [Wouter Swierstra's paper](http://www.staff.science.uu.nl/~swier004/Publications/DataTypesALaCarte.pdf).
thank you. i would have done it myself later, when nobody thinks it is stupid. (sometimes i say something stupid. reddit is a better forum for my stupid thoughts than ghc's trac i suppose.)
Is the apiary framework related to [apiary.io](http://apiary.io/) ?
No it isn't. I didn't know apiary.io when I released apiary web framework. conflicting of name is my fault. Sorry for the confusion.
newtype wrappers around types to introduce different instances of the same class always felt a bit hacky. I feel like there should be a cleaner way to implement this kind of stuff at language level.. But I have no idea how that would look. It's just one of those things that never felt right to me for some reason... though I can't come up with a better solution than introduce something like newtype either.
Except this is the wrong way to do what you're asking for. (I know of no right one.) The problem is that many packages depend on the same couple of libraries. Say, both B and C depend on A, and C depends on B. Then, if someone downloads C, they download A only once. You wight possibly arrive at negative download numbers for popular libraries if you just subtract the cumulative sum of downloads of its revdeps, even if you do it recursively. 
A semigroup is a set of elements together with an associative binary operation on those elements. A monoid is a semigroup that contains an identity element. So for any type t, if t (together with an associative operation) forms a semigroup, then Maybe t will form a monoid, since Nothing can be seen as the identity element. In the haskell standard library the monoid instance for Maybe is defined like: instance Monoid t =&gt; Monoid (Maybe t) where ... This is unnecessary, and a more correct definition would be: instance Semigroup t =&gt; Monoid (Maybe t) where .... However, Semigroups aren't in the standard library, and that is the reason for the Option type. So there is nothing wrong with the Maybe Monoid instance per se, but it imposes unnecessary structure on the underlying type.
Ok, so by "fix the Monoid instance of Maybe" the author meant "add Semigroup to prelude and do all kinds of other changes". The way the message read it looked like something was actually wrong with the instance of Maybe but I guess it's actually correct given the options it has at present?
You're faster than my Spock! Challenge accepted... :-P
The profile stuff can answer those questions in theory, but as you've observed, it's quite hard to use. I originally had hoped to have a question/answer mode, and I'll think about some more. I'd welcome contributions, or suggestions/specifications. (If you want to discuss what would work best, the mailing list is probably a better venue - emailed mockups would be handy) For the dependency cycles, in early versions of Shake that was certainly possible. With new versions I have tests for cycles (https://github.com/ndmitchell/shake/blob/master/Test/Errors.hs#L26) and they always get detected without a deadlock in my experience. You can always write your own deadlocks in Haskell itself, but if they are a Shake deadlock, I'd love to get an example.
Take a look at : http://byorgey.wordpress.com/2011/04/18/monoids-for-maybe/
My personal take is that the `First` instance for Maybe is nearly always the one we want, and when we don't we just want `Dual First` (aka `Last`). The others seem more curiosities than anything else, in that I have hardly ever wanted them. Others' mileage may vary I suppose.
Thanks! I think I read that a while ago - it was definitely percolating in the back of my head while I was working on these. I think the paper briefly mentions what I'm doing here in the Discussion section : "Furthermore, we have not dealt with polymorphic data types, such as lists or trees. Such data types can also be written using the techniques described above. Rather unsurprisingly, this requires a shift from functors to bifunctors."
pkg.downloads - max([d.downloads | d &lt;- pkg.dependants]) ?
My other objection to the `Monoid` instance for `Maybe` is that I believe that the `Alternative`/`MonadPlus` instances should always match the `Monoid` instance.
The main things that `ghc` does to make the Prelude faster are rewrite rules. The classic example is the `fold/build` rule which can sometimes fuse away intermediate lists. You can find lots of examples of this if you go through the source code for [GHC.Base](http://hackage.haskell.org/package/base-4.7.0.1/docs/src/GHC-Base.html) and [GHC.List](http://hackage.haskell.org/package/base-4.7.0.1/docs/src/GHC-List.html) and search for `RULES`. To learn more about how rewrite rules work, you can read [this](http://www.haskell.org/ghc/docs/latest/html/users_guide/rewrite-rules.html).
For the record, "some people" in this case refers to the authors of classy-prelude ;).
The recursive behavior of `Monoid` is so useful, and is not expressible in `Alternative`/`MonadPlus`. This makes the distinction somewhat sensible. In Conal Eliott's type-class-morphisms paper, he shows why a recursive `Monoid` instance (that matches that of (a -&gt; b)) is so useful.
When I want `First`/`Last`, I use `msum` or `&lt;|&gt;`. When I want the recursive behavior (which is frequent!) I use `mappend`. Just the other day I had two Maybe-lists that `mappend` combined just rightly.
Unfortunately, `Maybe` does not use the recursive `Monoid` behavior
You just hit jackpot: it turns out that if you do that, `dumbSort` turns into `id`, (except for empty lists: it fails for those). I don't know exactly why that is, but Quickcheck says yes and I doubt it does anything too weird.
Don't be too hard on me please:-D I think Spock is fast enough, and it don't have serious performance decrement like scotty, so It's small problem.
If you don't want to change the major classes, then you can just re export them. This is what basic-prelude does. It is a huge pain to use a Prelude with a different monad class, because then you have to rewrite all of the instances for every third party library you pull in.
Nice. I've been meaning to add this measure to the ranking of the search results on hackage. We already use the download counts, but using reverse deps as well should be a lot better. Perhaps I can just use this lib (or adapt if necessary).
Oh, useful to know, thanks. I'd been thinking of using the PageRank, but if this one is reasonable and already implemented then that's a lot easier.
The main issue there is then you'd _still_ need to define the canonical way to extend a Semigroup to a Monoid somewhere, and that structure has an admissable Monad, just like [a] does. Should we deny that type its monad just because an unrelated class with unrelated laws disagrees?
I'm fine with there being alternative type that extends `Semigroup` to `Monoid`, I just don't think `Maybe` should be that type.
&gt; an unrelated class with unrelated laws I wouldn't consider `Monoid` unrelated to `Monad`, `MonadPlus`, or `Alternative` nor would I consider their laws unrelated. `Monad`'s `return` and `&gt;=&gt;` are required to form an (indexed) monoid. `Alternative`'s documentation implies that `empty` and `&lt;|&gt;` form a monoid. I think it would reduce cognitive load to have a strong relationship between `Monoid` instances and "matching" `Monad`/`Alternative` instances, at least.
Ok, I'll go subscribe to the mailing list. WRT deadlocks, it's possible they were all on older versions. I don't create them often and when I do I usually I spot them before building because I load the module in ghci. So it's a rare problem, but if it comes up again I'll be sure to report it.
They make an appearance in darcsden: http://hub.darcs.net/simon/darcsden/patch/20140219081137-81bb2
Took a bit longer to write this one than I had planned. Hopefully I'll get the next post out faster.
why not UK?
Sounds interesting, but I am not sure whether a "beginner flag" would be used. It might even be contra productive since beginners (when getting to type classes) do instances much more often then then they'll do later. 
shameless plug for if u want to do that: https://github.com/intolerable/reddit
GHC now stands for Glorious Haskell Compiler.
I had some online typing practice that had me type out the XMonad source code. I guess I can write up stuff about it.
 instance Monoid a =&gt; Monoid (Maybe a) This is what I mean by the "recursive" instance. `mappend` dives into the values inside the `Just`.
The names were selected to match SQL since they found that people had difficulty understanding the correct names. 
They are called invariants and typically are ignored as the functions are responsible for maintaining them. 
Then again, Many isn't a keyword in SQL as far as I recall.
This sounds a lot like [modular type classes](https://www.mpi-sws.org/~dreyer/papers/mtc/main-long.pdf). Definitely a cool idea, I think OCaml might be growing something similar. TLDR: type classes are kinda like modules. So we can imagine something like signature SHOW = sig type t val show : t -&gt; show end And then we can write modules which implement them structure IntShow : Show = struct type t = int fun show i = .... end Now we want to be able to write "instances" which use other instances. Those can be modeled as functors functor PairShow (A : Show; B : Show) : Show with type t = A.t * B.t = struct type t = A.t * B.t fun show (l, r) = .... end So that's a strict encoding of type classes, but it kinda sucks to use. We want that convenience of having the compiler actually working out where to stick what in our programs. To do this MTC's introduce a new type of scoping mechanism where we can add these instances to the compiler's resolution scope. val show = overload show from SHOW This says "Hey, I want you to work out which module to use for `show` on your own. Good luck!". From there you introduce names into the scope with `using`. using IntShow, PairShow in val foo : string = show (1, 1) end The rule being you can't introduce two clashing arguments into scope. There are a details and conveniences that I'm glossing over, so you should also read the paper :) 
Yeah, I understand. I actually like that `Monoid` instance better and I've [blogged about this trick](http://www.haskellforall.com/2014/07/equational-reasoning-at-scale.html) before.
A while ago I thought about how you could integrate equality saturation with supercompilation. In this stackoverflow post I asked about the main difficulty: http://cstheory.stackexchange.com/questions/16567/preserving-termination-when-rewriting-recursive-programs So far I don't see a good solution to that problem, so if anybody has one I would be very happy!
Agda does this! Say you have record Show {i} (a : Set i) where field show : a -&gt; String (which is haskell's Show class) then you can use it as a normal module/record (records in agda are as powerful as ML's signatures and defining a record defines a module for it's fields), and also like a typeclass: instance show-nat : Show Nat show-nat = record { show = Nat.show } open Show {{...}} that `open {{...}}` makes show : {i} {a : Set i} {{_ : Show a}} -&gt; a -&gt; String where `{{_ : a}}` is an "instance argument", which looks for instances of type `a` declared with `instance`, as above.
Oh, so this looks like named type classes that are only available for injection/resolution when exposed through a 'using' block. This is essentially what I was talking about. In Haskell we could even keep the current typeclass syntax (class and instance), except for adding a way to name instances (like purescript), as syntatic sugar for creating a value containing the implementation, and then introduce something like 'using' to manage the instances in scope. Does backpack provide a mechanism for something like this?
Very interesting, I've been meaning to read more about agda.
If you think the `Monoid` instance for `Maybe` is bad, you should see the one for `Map` :(
Ouch. What was it about my post that made you think I don't know Haskell at all? :) C# is my day-job language but Haskell has been my "hobby" language (i.e. the main one I use for my own projects) off and on for years. I've not used conduit before and I'm currently a bit out of practice with Haskell itself but I feel very comfortable using Haskell. The main driver for me looking at Haskell for my side projects (including finance related ones) is because I'm extremely time constrained. The more work I can push into e.g. a type system the better it is for me. I've considered using C# but the problem is that it gains most of its power for development when you have Resharper. Resharper removes a lot of manual work I would have to do without it. In Haskell, due to the nature of the language, I don't even need a lot of the functionality I rely on Resharper for. As far as my suggestion above, I've seen lots of discussions on conduit and it seems to provide the functionality of sending data to different conduits/sinks by actually sending all data and having the conduits/sinks filter out what doesn't apply to them (in contrast to how FRP appears to work). One example I saw used Either for this, but I would have more than two types of data so a more elaborate structure would be required. Not necessarily the one above but that was the general idea. Having spend a bit of time thinking about this, I don't think it's bad. The code I would be writing for an individual conduit would be the same I would have to write regardless of what library I was using (e.g. a five minute candle needs to look at one minute candles over a five minute window no matter how they're produced). And again, what I'm doing now is just simulation. A production system wouldn't be generating so many different views of the same data, but for now I don't know which frequencies will be the most interesting.
The main disadvantage of this approach is that you lose global canonicity of instances. Ekmett for example really likes this, but I don't think it's worth it for the cost in modularity.
Global canonicity provides safety for things like `Data.Map`. Two questions: 1. Is there another way of getting safety for these things? 2. What other benefits does global canonicity provide? 
&gt; http://cstheory.stackexchange.com/questions/16567/preserving-termination-when-rewriting-recursive-programs In one sense a supercompiler is just engaging in a series of rewrites, so you could in theory treat supercompilation as just another source of equalities to consider.
&gt;Is there another way of getting safety for these things? yes. modules/ ml style "functors" &gt;What other benefits does global canonicity provide? easier reasoning about refactorings 
The `tls` package itself (i.e. the TLS algorithm) is pure Haskell. But the `tls` package depends on other packages (with a somewhat lazy PVP-attitude which has caused me some grief in the past already) that wrap C code for crypto primitives (ciphers and such).
Yes, but the problem is that the kind of equalities that a supercompiler uses are unsafe. When you apply them blindly you turn terminating programs into non-terminating programs.
&gt; yes. modules/ ml style "functors" I don't think that you can ensure safety this way. You can ensure, that you're using functions with the same type signature, but you can't ensure, that they have the same semantics.
Forgive my ignorance, but what it wrong with (empty, union) as the monoid for map? Is it because union is left biased and will overwrite the value in the right map if it exists? What would be improvement be? Monoid constraint on the elements etc etc..?
Functors generate types, so if I have a functor for Data.Map instantiated with one Ord instance, I can't use Maps from the resultant module with Data.Map instantiated with another Ord instance. 
GHC has a extension that does this, but point #2 has always felt like a confusing misfeature to me in Scala
tallbalance, thanks for submitting this! Actually the post came from a discussion in this subreddit [two days ago](http://www.reddit.com/r/haskell/comments/2gnsfg/using_haskell_to_study_provability_logic/) :-)
I think you really want both. There are important packages that nobody downloads (e.g. base), and packages that you can also get cliques of packages that have deep dep graphs but that nobody in fact uses. I think the ideal thing might be some PageRank-style algorithm that is weighted by downloads. A reasonable approximation would be to just look at the downloads and revdeps separately (as appropriately weighted ranking features).
You can ensure, that all functions in the module are using the same Ord instance, but you can't ensure that the Ord instance is semantically correct. If you have just one Ord instance, then it's either always right or always wrong.
I understand for other parsers, but isn't it extremely counterintuitive that `lookAhead` would not backtrack always? Even more so, in combination with `&lt;|&gt;`. 
Fair point. One thing that I've been playing with is if we can describe a form of Directed-PEG for scenarios where you don't have an equality, but rather may have lost information. Think of these as edges where you are computing less information so the target of the arrow can't stand in for the source, but the source could stand in for the target safely. One example of such a category would be one where the source could be more defined than the target / directed edges would be monotone functions in the underlying Scott domain. When we pick up the PEG formalism and apply it directly against a lazy program it gets messy fast, because of termination and laziness. If you want to hope to get decent worker/wrapper transformations that keep most of the work unboxed I think you need something like this. Moreover things like foldr/build fusion are only sound in the absence of seq and unfoldr/destroy fusion can introduce non-termination as well, so this would capture the definedness of the program transform by weakening the notion of equality a bit. You'd get a 'categorical/CPO saturation' rather than an 'equality/groupoid saturation'.
Kenny Foner came out to Boston Haskell 3-4 days ago and gave a talk on Löb's theorem and its connection to an arbitrary ComonadApply. This results in a proof that diverges slightly from the standard Löb proof, but avoids and gets better sharing than Piponi's laziness hack. I'd pointed out to him at the time that his proof was actually very similar to the proof given in wikipedia for Löb's theorem, but that it avoided the need for `pure :: a -&gt; Provable a` by using functorial strength. Shedding that rule greatly broadens its range of applicability, because now it works for anything you could use Dominic Orchard's 'codo' sugar on.
The problem is that when you need to duplicate something like the mtl with ml functors or the like where you have attributes that are present based on what `m` is currently instantiated to, then you lose the ability to do operations like `hoist` that change out the set of operations as the types change. The ML functor story handles first order cases like Map and Set very well, but if you have attributes that are more conditional e.g. (still too simple to really show the problem, but at least conditional) `(,) e` is a `Monad` only when e is a `Monoid`, then it falls down. `StateT s m` is a `MonadReader e` when `m` is a `MonadReader e`, if you want to let the users swap out `m` in `StateT s m` the attempt to fit this into ML functor story gets messy.
Exactly my complaint with it. And changing instances of this sort is almost impossible due to it potentially changing behavior of existing programs :(
The more projects use HSE, the more frustrated I become that HSE and GHC use different parsers. Inconsistencies between the two have caused me no end of problems when using HSE-based tools, to the point where I now no longer use any on a regular basis. Is there any effort underway to unify GHC and HSE's parsers?
Its not a unification, but there is an effort underway to make the GHC API more friendly toward tool developers. GHC 7.10 will already have all the landmines removed from the AST so that generic traversals are simpler, and I am hoping to have some sort of annotation system in place to allow a simple port of hindent to GHC. The discussion is [here](https://ghc.haskell.org/trac/ghc/wiki/GhcAstAnnotations), and I am working on a proof of concept version of HSE's exactPrint for GHC [here](https://github.com/alanz/ghc-exactprint). This one has the advantage that all the locations are offsets from the start of the syntax element, so it should allow manipulation of the tree before outputting it in a way that preserves layout.
&gt; `&lt;*` performs the action on its left, then the action on its right, and returns the result of the second action. It is equivalent to the monadic expression `do{ f &lt;- foo; bar; return f}`. I think you mean "returns the result of the *first* action".
I believe you are referring to [implicit parameters](http://www.haskell.org/ghc/docs/latest/html/users_guide/other-type-extensions.html#implicit-parameters)?
I think it's actually a wonderful way to manage dependency injection. Of course it can lead to very confusing situations, but if used appropriately it can lead to very concise implements with compile time guarantees.
But global canonicity is too restrictive: it leads to issues like the Option data type discussed in other thread where we have an entirely new version of Maybe just because we need it's Monoid instance to depend on Semigroup rather than another Monoid. 
I can't stand that notion of 80 character width restrictions ---- we're not living in the 60s with 80 character dumb terminals any more.
I agree, but the debate is not settled.
We also don't all use a single editor pane. I have as many 90 column panes side by side as I can fit on my screen.
I'd be very very interested in seeing the code, do you have a link? Also, I'll be in Boston the week of Oct 12-18, would love to visit a meetup if possible.
Technical info on the project (for the interested): http://blog.chucklefish.org/?p=154 Previous discussion on /r/haskell: http://www.reddit.com/r/haskell/comments/2glk10/wayward_tide_is_being_written_in_haskell/ We're based in London. EDIT: /u/palf_ and /u/kyrenn also work at Chucklefish and are helping me field questions. /u/palf_ in specific is the lead dev for Wayward Tide and Cove.
I am currently remote, so it might not be completely out of the question. However, I'm sure we would greatly prefer someone local. I'd have to ask for clarification. But assume you'd need to relocate.
Entity, I'm curious, is your plan to transition all future development to Haskell if Wayward Tide works out as planned?
I can't really say. Both kyren and I want to transition to Haskell. But it hasn't exactly reached the 100% planned locked in stage of course.
How much are you looking to pay?
`formatting` is an utterly brilliant concept. One could imagine a similar library to remove the stringly typing from regular expressions too. 
I'll have to find out specifics. I believe we will pay combination of a base salary + a percentage of profits, in standard indie development style.
I agree that coding style is important. That's why structure editors should exist for every language. I'm still dreaming of an editor which separates the model from the view and the controller. The model is the AST, which can be manipulated through the controller (e.g. something vim or emacs like) which presents the program through a user defined view (e.g. text like in vim/emacs or more sophisticated as nodes, which can clicked upon to open and edit them.. there'd be no limit). No more tabs-vs-spaces discussions, no more fights the correct line length and whatnot. 
http://hackage.haskell.org/package/regex-applicative
Thanks for writing this tool! I've wanted this for a long time, and got very close to writing it myself - but you're a much better person to have writing something like this. I don't have much more to add, other than I look forward to playing with it :) Consistent style is important, and I have 0% interest in manually tapping the space bar to keep things consistent.
The smart money is on Yes
Honestly, I just thought about it when I woke up and thought "Hey, this would be a good idea." Didn't even occur to me that it was Saturday. But yes I am working today, and yes I do love my job. :) We try to keep people in the office during working hours though. We're not quite a ROW. We don't really have enough people for that.
&gt; The job is based in London, we'd expect to see a successful applicant in the office on 80% of days. Are you offering relocation? A lot of us are in the US.
As far as I'm aware, we would probably find it difficult to hire from the US, because of UK immigration. Dey terk er jerbs, and that sort of stuff. If we did manage to hire someone from the US, we would definitely assist with the immigration paperwork. But as far as the finances of moving and such, I'd have to ask Donna or Tiy, and they're not in the office until Monday.
yes you can. It is easiest to see with generative functors since the "having the same type" means coming from the same application (and thus using the same ordering), but an applicative functor system is enough.
This is something I'd definitely be very interested in if I had a bit more experience. I'm a student living in London, so if you are ever able to offer any kind of holiday internships in the next couple of years, I'll be here!
&gt; – expertise in one or more of the following: Haskell, Erlang, Scala, F#, functional JavaScript It's sad not to see [OCaml](http://en.wikipedia.org/wiki/OCaml) mentioned in this list. It is also an industrial programming language, and is certainly closer to Haskell than Erlang for example, or functional Javascript.
I think that was just an oversight, OCaml experience would be a huge plus of course, as well as lots of other languages like Agda or Idris. It's a list of examples but not an *exhaustive* list. We just want experience with FP.
But we are in GHC 7.8, and I read here that the AMP patch was finally merged.. will another version be released without monad-as-applicative?
We might do a point release to fix 7.8.3 bugs or something at some point, but the next major release will have it in.
This is a pointless exercise, I feel. The original reliance on laziness is perfectly fine, it just requires a specific kind of denotation (which Haskell relies on). The issue of theorems brought up by Yakeley is also irrelevant because those *are* principles of an appropriate modal logic (specifically, the modal logic that sigfpe was constructing).
It would be kind of neat if you guys posted the language breakdown from applicants afterwards. Dunno if you're allowed to do that.
Now you mention US in specific. How about within EU - Scandinavia to be more specific. Would that be easier? Or are you looking for a UK guy? 
I am not sure about that, but I would think it would be easier. I'll need to find out for you. Unfortunately, the person to ask about that in specific (Donna, our office manager), is out of the office until Monday, so it might be a few days before I'll have an answer for you.
&gt; I have 0% interest in manually tapping the space bar to keep things consistent. There are alignment tools for editors if that's what you're looking for, e.g. [tabular](https://github.com/godlygeek/tabular) for Vim. (`gg=G` doesn't seem to work well for Haskell)
&gt; I think that was just an oversight Of course. That does not reflect badly on the Wayward people, rather on the outward communication of the OCaml community, which for some reason seems unable to make itself rememberable enough to figure on most people's "reasonable functional programming languages" list.
FP complete does some Haskell evangelism and has a [list](https://www.fpcomplete.com/page/case-studies) of case studies that might help you!
In my opinion, hire-ability is, unfortunately, not an old idea and a major problem on building a business based on Haskell
&gt; The job is based in London I don't have enough experience for this job, but seeing FP job listings in London makes me feel all happy and fuzzy. :)
I actually still disagree. Hiring has never been a problem and it's compounded by the fact that you typically need fewer hires because the end product is easier to maintain and also requires less "human management".
Build w/ GHC HEAD? We're not good at dealing with multiple versions of base at the moment. 
Hmm, guess it's still worth applying and I'll just get shot down if it's not feasible.
That would be easy with Nix I guess.
Thank you! That's a really good explanation of where the ambiguity comes from. How do people deal with this kind of ambiguity in languages like Agda and Idris, which have type-level lambdas? Is it a big inconvenience?
I like that spirit! :D EDIT: Oh hey, you're the browser engine guy. I loved those posts. Here.
The 80 character thing is not just to make it easier to fit on the screen. It's also to improve readability. It's actually quite a bit higher than what most typographers would recommend (somewhere in the range of 50-70 characters is most commonly recommended). Admittedly, code is not prose. I don't mean to push this point too hard, but I do maintain that a tight character limit is totally reasonable, even with the better displays of today.
Thanks! I'm actually pretty surprised those posts are so popular, I'm basically just naively converting all of Matt's code from Rust to Haskell (without even considering performance), so I'm not really doing anything interesting yet. Next post will have a little rant on how Rust's iterators let people write hard to read code with optionals though, so there's that.
It's also a perk to use (or learn) a language like Haskell, so those great hires might be easier to attract.
Any project based Haskell writing is extremely useful and interesting. There's a real shortage of practical "let's build a real project" writing. You're doing very valuable work, keep up the good work. :-) 
Well, that depends. If you want to use my code in practice, you probably can't, because it has many undefined functions and such. But if you want to use sigfpe's code as a proof of Löb's theorem, you also can't, because it uses assumptions that don't hold in the original context of the theorem, where ☐ is interpreted as provability in PA and such. Of course I'd be very happy to see a piece of code that's the best of both worlds, both encoding the right proof and providing useful functionality. Actually that was my original intent and I failed. Maybe the talk Edward mentioned could lead to something like that.
indeed. 
His slides: https://github.com/kwf/ComonadSheet/raw/master/Presentation/Presentation.pdf The code: https://github.com/kwf/ComonadSheet And I hope to have a video of the talk up in a couple days :) edit: Used gelisam's working link
FWIW I don't see why the relevant theorems don't hold. ☐(A → B) → (☐A → ☐B) and ☐A → A should both be true if ☐A means "A is provable (from no assumptions)". Also, the complaint that "fmap is definitely not provable in any modal logic." is actually quite false, since any lax monad is a lax modality. I don't know if provability ☐ is a lax monad, tho. It will depend on what axioms for it. In a computational/proof relevant setting, `A → ☐A` is a reasonable axiom but, doesn't mean quite the same thing as what people probably mean by "provable" due to proof relevance. With just `☐A → A`, the logic is that of code quoting/staged computation where you can take quoted code and evaluate it. With `A → ☐A` as well, it's the logic of code quoting and quasiquoting, where you can take values and turn them into quoted code. The latter is definitely a real, modal logic, but it's intuitionistic, and so it might not cohere with the intention of "provability". Loeb's theorem seems to reach very deeply into the philosophical core of a logic. It seems that for it to be true and non-trivial, you must be classical.
Now this is a position I'd really love to be qualified for. I should have stuck it out back when I gave up on Haskell...
https://www.youtube.com/watch?v=9fdcIwHKd_s#t=11
Cool. I may be overly tired and just don't get it, so my apologies. I'm not sure what the added complexity solves by replacing it with this solution. When I've used a string templating library, I've controlled the input end to end or I've already asserted the type earlier so there is no need to recheck what I've already checked.
You're also going to be "upstreaming" that criticism, right? :) (I'm not sure what the specific issue in this instance is about, but there's an occasionally-frustrating dearth of involvement from people with Haskell backgrounds in the community, and every bit helps.)
I'm not really sure where to upstream it to, and I'm sure it's a well known issue. The problem is that for `foo.iter().bar()` with `foo` being an optional type, if `bar` is one of the general iterator functions like `any` it silently strips the optional argument away (so for `any(f)`, the equivalent in haskell would be `maybe False (any f)`) which isn't *terrible* if you know what's up. But if you don't already know that foo is optional, you don't know that a result of false could mean either that `any` didn't find a match, or that there is no argument to match against, and these are often things you want to handle differently. I don't think there's really a way for them to fix that without neutering their iterators, they have to hope that people will use optionals properly, basically. Edit: Not to imply that Haskell doesn't have its own share of issues in this regard; the whole set of `Control.*` libs are full of ways to make your code totally illegible if you so desire.
And semicolons, if I recall. SPJ is the crotchety old man of Haskell development.
Hmm, i have a *Haskell Marketing* bookmarks folder w/a few dozens URLs. Here's the last 18 or so: https://gist.github.com/gtani7/6b784421bb6dcda160ae similar info request: http://comments.gmane.org/gmane.comp.lang.haskell.cafe/98564 (the dons pdf towards the bottom is really good: http://code.haskell.org/~dons/talks/padl-keynote-2012-01-24.pdf 
The novel technique here of automatically binding command names at compile time is non-portable and slows down compilation time. Either of these are not worth the cost in certain settings. How painful is it to white-list commands to be used, particularly given there is already a special conflict avoidance list (tranalates "unzip" to "unzip_") someone has to figure out?
&gt; If you want to use my code in practice, you probably can't, because it has many undefined functions and such. [...] that was my original intent and I failed. My apologies for leading you towards those `undefined`s, then! I intentionally used Agda postulates, even thought they are non-constructive and rarely used, in order to avoid having to introduce to many new Agda concepts at once. To obtain a constructive version, each postulate can simply be replaced by an extra argument with the same type. Even the postulated types can be converted into type arguments, meaning our theorem will be a polymorphic function. In your original post, you were using a typeclass `Prov` in order to contain all of your hypotheses; I like this approach, I think it's quite a convenient way to group a bunch of related hypotheses together. Let's convert the proof to that format: {-# LANGUAGE MultiParamTypeClasses, ScopedTypeVariables #-} class Logic t where logic1 :: t (a -&gt; b) -&gt; t a -&gt; t b logic2 :: t (a -&gt; b) -&gt; t (b -&gt; c) -&gt; t (a -&gt; c) logic3 :: t (a -&gt; b -&gt; c) -&gt; t (a -&gt; b) -&gt; t (a -&gt; c) class Logic t =&gt; ProvabilityLogic t p where rule1 :: t a -&gt; t (p a) rule2 :: t (p a -&gt; p (p a)) rule3 :: t (p (a -&gt; b) -&gt; p a -&gt; p b) loeb :: forall t p psi a. ProvabilityLogic t p =&gt; (t (psi -&gt; (p psi -&gt; a))) -&gt; (t ((p psi -&gt; a) -&gt; psi)) -&gt; t (p a -&gt; a) -&gt; t a loeb psi1 psi2 premise = step14 where step3 :: t (psi -&gt; p psi -&gt; a) step3 = psi1 step4 :: t (p (psi -&gt; p psi -&gt; a)) step4 = rule1 step3 -- ... step14 :: t a step14 = logic1 step10 step13 We now have a constructive version of Löb's theorem, without any `undefined`. Is the implementation useful? Well, it's hard to say, as I don't have a good intuition as to what the theorem means. The most I can say is that we can make use of the theorem whenever we encounter types and type formers which satisfy all the hypotheses. For example, I notice that applicatives satisfy all the laws: {-# LANGUAGE FlexibleInstances, UndecidableInstances #-} instance Applicative t =&gt; Logic t where logic1 = (&lt;*&gt;) logic2 = liftA2 (&gt;&gt;&gt;) logic3 = liftA2 go where go :: (a -&gt; b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) go f g x = f x (g x) instance (Applicative t, Applicative p) =&gt; ProvabilityLogic t p where rule1 = liftA pure rule2 = pure (fmap pure) rule3 = pure (&lt;*&gt;) Finding types where the fixed point rules hold is much harder. The only combination I have found so far is the trivial one: -- | -- &gt;&gt;&gt; main -- () main :: IO () main = print $ runIdentity $ loeb (Identity (\() (Identity ()) -&gt; ())) (Identity (\f -&gt; f (Identity ()))) (Identity (\(Identity ()) -&gt; ())) If you can find others, I'd be quite curious to see what they look like.
It seemed to gain a bit of safety (you get the names right) but doesn't gain any safety about the flags and their interpretation (which I find is much harder). I tend to write custom wrappers for each tool which nail down the arguments as a record or list of adt values. 
It seems to me that `mfmt` is general enough and a common enough use case that it should just be one of the built-ins.
&gt; but doesn't gain any safety about the flags and their interpretation A lot of shells have databases of known commands with their flags. This is used for autocompletion purposes but could be modified to hold more semantic information.
The modal logic used in the post is [provability logic](http://plato.stanford.edu/entries/logic-provability/) (GL). The statements A→☐A and ☐A→A are not theorems of GL, because the statements "all true statements are provable" and "all provable statements are true" are not theorems of Peano arithmetic, and GL is adequate for Peano arithmetic. That's really all there is to it. I don't know if Löb's theorem is intutionistically valid, but the proof in the post is definitely intutionistically valid. The only possible dependence on excluded middle is in the [Diagonal Lemma](http://en.wikipedia.org/wiki/Diagonal_lemma), which I assumed without proof.
The slides link gives me a 404, but the code is quite understandable, thanks! Though ComonadApply still seems to be too strong for what I'm trying to do :-(
&gt; How painful is it to white-list commands to be used, particularly given there is already a special conflict avoidance list (tranalates "unzip" to "unzip_") someone has to figure out? It's no longer a white-list but instead reifies each name to see whether it needs renaming.
&gt; I tend to write custom wrappers for each tool which nail down the arguments as a record or list of adt values. Yeah, I like doing that. My [pdfinfo](http://hackage.haskell.org/package/pdfinfo-1.5.0/docs/Text-PDF-Info.html) is an example. It would be awesome to have a big package of well-typed wrappers for shell commands.
Nice to see a new editor being developed! I think this area is in need of much improvement (I've started one as well :) - https://github.com/svalaskevicius/hob )
I think you missed my point - I don't literally tap space to align things. However, any time I have to think about driving my editor to align things, my focus is taken away from actually solving the problem I'm working on. It's that context switch, however small, that I want to reduce.`
A bit of self-promotion. [process-streaming](http://hackage.haskell.org/package/process-streaming) also allows you to build pipelines of external processes, although the set up is more laborious: import Sytem.Process.Streaming import Pipes.Text.Encoding as T import Pipes.Bytestring as B import Data.Bytestring.Lazy as BL import Data.Tree examplePipeline :: IO (Either Int (BL.ByteString,BL.ByteString)) examplePipeline = executePipelineFallibly (pipeoe (fromFold B.toLazyM) (fromFold B.toLazyM)) $ stage (linePolicy T.decodeUtf8 (pure ())) pipefail &lt;$&gt; shell &lt;$&gt; Node "{ echo aa ; echo 1&gt;&amp;2 ee ; echo cc ; }" [Node "grep cc" []] &gt;&gt;&gt; Right ("cc\n","ee\n") I'm curious about the implementation of shell-conduit. It all takes place in a single thread, doesn't it? But writing to the stdin of an external process, reading from its stdout, and reading from its stderr are inherently concurrent actions. How are they serialized? **Edit:** This program main = run (shell "./alternating.sh") Seems to deadlock when executing this script: https://gist.github.com/danidiaz/6e40685eea0a4a7e5e59
12 years old. All the fifth graders are taught that much maths. 
I like it. Why does your Blog not have an RSS-Feed?
Groups and rings? I'm a mathematician and I only knew what a group or a ring was in my first year of university....
&gt; The slides link gives me a 404 Try https://github.com/kwf/ComonadSheet/raw/master/Presentation/Presentation.pdf
&gt; the idea of embedding Löb's theorem into a programming language is already begging the question: the very act of doing so means you're implicitly assuming that truth = provability, and ☐ is vacuous. But doesn't that mean you would be proving a different theorem, namely one where "☐" means "provable in intuitionistic logic" instead of "provable in Provability logic"?
Somewhere in the late '90s in a eastern european country...
&gt; I would bet that ☐A→A is a theorem, no? No. 1) Let A be a false statement, e.g. "2+2=5". 2) By your assumption, ⊢(☐A→A). 3) By Löb's theorem, ⊢(☐A→A)→⊢A. 4) From 2 and 3, ⊢A. We have proved a false statement. &gt; Once you're in the modern intuitionistic mode, you're already operating within a framework that assumes truth and provability are the very same thing I wasn't trying to prove Löb's theorem in intuitionistic logic, I was trying to prove it in a classical modal logic. It just so happens that the proof doesn't use the law of excluded middle, and can be encoded in Haskell.
&gt; any time I have to think about driving my editor to align things, my focus is taken away from actually solving the problem I'm working on. I agree with you 100% but oddly it's why I would, figuratively, prefer to manually tap the space bar to keep things consistent. I have tried the auto-indent modes that come with Emacs and all of them have just enough quirks in them that they're too hard to predict. They made me think about indenting more, not less. Now I just use M-i and indent-relative (bound to TAB) because they're so predictable that my brain does it automatically. The indenters make me stop and lose focus. Vim does this better because it relies less heavily on auto-indenting and has predictable behavior assigned to keys. I suppose a really good auto-indenter could make this problem go away, but even the author of this one says it "almost always" does what he wants. I'm skeptical because even if it fails a small percentage of the time that's enough to outweigh the ease of just doing it manually and brainlessly with extremely simple key bindings. But I might try it and see.
That works, thanks!
But in shell, when you write `foo | bar`, it connects stdout of `foo` directly to stdin of `bar`. In your approach, you stream all data through Haskell.
That's kind of the point. If you try to use a type theoretic logic to talk about Löb's theorem, it's now something entirely different.
&gt; I wasn't trying to prove Löb's theorem in intuitionistic logic, I was trying to prove it in a classical modal logic. It just so happens that the proof doesn't use the law of excluded middle, and can be encoded in Haskell. I would conjecture that the Haskell encoding necessarily relies on LEM or something equivalent. Can you translate this proof into Agda with no funny extensions and no postulates? If you could do *that*, then we'll know for certain that it's intuitionistically valid. Or alternatively, can you do it in Haskell without `undefined` anywhere? Because that's the crux of the matter. Modern type theoretic intuitionism doesn't let you just have any old axiom you want, right. You have to *justify* the axioms by appeal to computational principles, and if you can't do that, then it's no fair using them. So I guess while Löb's inference rule becomes less suspect as a result of decomposing it into a theorem of provability logic, I would then just say that this means provability logic is suspect. I would conjecture that PA cannot be justified in a type theoretically satisfying way without resorting to LEM-y things like sigfpe did.
i did the ruby version of conduit a couple years back. i'm pretty happy with it, but it's not perfect. i used it full time for about a year. it might be relevant to look through http://bitbucket.org/seydar/chitin
&gt; the author of this one says it "almost always" does what he wants I knew that in an exercise of academic honesty (saying “almost”) someone would pick at that. All the functions in hindent and shell-conduit have been formatted by hindent. These in particular are syntactically rich enough to be interesting enough to criticise: * [generateBinaries](https://github.com/chrisdone/shell-conduit/blob/24aae02f299c3d6191475a83be8bf5b99e9b52c9/src/Data/Conduit/Shell/TH.hs#L25..L52) - This is written exactly as I would lay it out myself, from the lambdas to the guards to the lack of spacing in pattern tuples. The only thing I would change is to put the `allowed` declaration on one line instead of line breaking the `++`. * [startProxy](https://github.com/chrisdone/shell-conduit/blob/24aae02f299c3d6191475a83be8bf5b99e9b52c9/src/Data/Conduit/Shell/Process.hs#L129..L156) - Again, this is laid out exactly as I want it to be. From the way do is written, the way the second case alt is line broken, etc. I wouldn't change anything in this. * [annotateComments](https://github.com/chrisdone/hindent/blob/master/src/HIndent.hs#L95..L125) - This is precisely as I would've laid it out manually. The line 106 tuple line break is particularly well chosen as is the 104 line break. The only thing I'd do differently is put the `ComInfo` on line 99 as one line instead of line broken. &gt; if it fails a small percentage of the time that's enough to outweigh &gt; the ease of just doing it manually Don't get me wrong here, I don't consider these points a “failure”. The formatting is done according to consistent rules and still looks pretty. It's not like I can be bothered to go and tweak them manually myself. I don't think that's what you should be skeptical about. The only layout command I run is the hindent one. I'll throw code around there, write some here and then hit a key and it puts everything in the right place. Small infelicities (which could readily be corrected if I cared enough to do so by updating the Style definition with more heuristics) like I describe above are easily acceptable given the massive reduction of manual labour afforded in the general case. What you *should* be skeptical about is encoding your own personal style easily. I use a *very* regular style by Haskeller standards. I never make nonce style decisions for one particular piece of code. I've recently ran hindent on some older project code and it was a no-op on many occasions. This is a very Lispy attitude. I think the general Haskeller manually formats code to the extent that one function to the next has a separate set of heuristics that appealed to the writer at the time. Sort of like when people write with a pen, the longer they write the more their style varies down the page. To those people, the idea of an autoformatter will either elicit acceptance that code is just going to be laid out according to a general algorithm now, or otherwise they won't be able to use it. But it's surprising how quickly automation breaks people's grip on old habits.
http://en.wikipedia.org/wiki/ROWE - Results Only Work Environment
Yes, this is a good point and I too have hard evidence of it. Software I wrote with Haskell a year ago for another startup has been truckin', the engineer I found to replace me figured it out pretty quick but he's also never needed to touch it.
Yes, defining newtypes is the cost of global canonicity. On the other hand, global canonicity is what allows us to correctly supply typeclass implementations only at their use site. The simple example is something like Set: you don't want to accidentally insert and retrieve using different Ord instances. With implicits, you can accidentally do exactly this - you generate the set in one module that has one implicit in scope, and then you pass it to another module which has a different implicit in scope. Similarly, you probably don't want to accidentally work with values created with different instances of ToJSON - say MyWebsite.Page1 had one instance in scope and MyWebsite.Page2 had another.
 {-# LANGUAGE FeelBadAboutYourself #-} Aw
What do you think about co-op (intern) student positions? I am pretty excited about working on something like this, but I'm still in school.
I can confirm this. In Europe generally groups and rings are taught in high school. Maybe we need regionalized monad into materials :-)
Here's [an implementation](https://github.com/chrisdone/shell-conduit/blob/master/src/Data/Conduit/Shell/Process.hs#L164..L193) of concurrent stderr and stdin reading. This removes the deadlock, I think.
Very cool! The other day I rewrote a shell script as a Haskell library/program. The shell script started out as something simple that took care of the details of one thing. But I soon found myself wishing I had abstracted it over more of the underlying program's command line arguments. It wouldn't have been too hard to do that, but if I did, I would lose a good bit of the convenience because I'd have to type more every time. This made me really wish for currying. Now it's so much nicer in Haskell and other coworkers have already made use of my library. Using a streaming library like conduit/pipes/io-streams was the natural approach since I needed it to work on very large input files. It would have been much nicer if I had shell-conduit. :)
Provability logic can easily be judtified in type theorical settings. You just have to understand what provability means. The evidence for `☐P` is a code (eg, a string) for a lambda term that realizes `P` (or maybe proves `P`, it depends on how you formulate things). You can't write `☐A→A` since you can't write a self-interpretor in a total language. Lobs theorem is just a simple use of fixpoints and perfectly justified. [Constructive mathematics is not meta-mathematics](http://existentialtype.wordpress.com/2013/07/10/constructive-mathematics-is-not-meta-mathematics/). Provability logic is. They don't clash, they are just different. 
Installing Platform isn't a great idea if you're on Ubuntu, it's an unnecessary source of pain. Just installing GHC, Cabal, Alex, and Happy is better. https://github.com/bitemyapp/learnhaskell#ubuntu
Canary Wharf area.
I don't think we're looking for interns at the moment. I believe we already have an intern working on the pirate game. But you can follow the link and e-mail us and find out!
This. If I had to estimate, I would say that I spend less than 5% of my time coding with only a single page on display. I typically have 2, and sometimes 3, buffers open in side-by-side columns within emacs. 
On spinning rust, probably not much of one. Worth checking against an SSD, though.
For people who want it to be just a single command: $ wget https://github.com/doganaydin/ubuntu-update-haskell/raw/master/update_haskell.sh &amp;&amp; sudo sh update_haskell.sh
Cool! Looks like we were both looking for the file tree + tabs thing :) I'll have to take a closer look at your command interface at some point.
Look at the downvotes. It's really full of shamanic mystic hipster scenester around here...
Paul Graham said the same thing about [Python](http://www.paulgraham.com/pypar.html). The goalposts have been moved but the core concept is still valid.
Not Haskell but [here is a talk by Yaron Minsky](http://www.reddit.com/r/ocaml/comments/2g37r7/experiences_with_ocaml_on_wall_street_yaron/) explaining, among other things, the nice side effects of using an "academic" language in terms of the people you recruit.
That can't explain sh*t in a concise and succint way because it won't make them part of the scene
Wouldn't it also be an option to make a type-safe printf using template haskell, so we get both type-safety at compile time and short, simple format string like we're used to?
I use the Haskell Platform on Debian Stable and it works for most cases. For those rare cases where it doesn't I just use `hsenv`.
Is it possible to allude to what kind of salary you offer?
what specifically do u not like about `Control.Whatever` stuff?
I saw that most documents prefer haskell-platform package for beginners. So I started with haskell-platform and created this simple bash script to install it easily. Maybe when I learn haskell a little bit more, I will switch to this setup. 
Very good example. One I should have thought about since I practically live in logs on most days. If given the choice, I think I would rather have a logging subsystem that would be resilient to failure more than I would like to solve this problem in this fashion. It wouldn't take a lot of convincing that this may be a better solution with a "fail early and fail fast" sort of principle. Guess, I'm still conflicted.
Yes, it's because implication means different things. I don't know if the primary problem is with LEM tho, like Peirce's law is, or with some other part of the interpretation. &gt; so if what you're saying is that (☐P→P)→P in provability logic and (☐P→P)→P in intuitionistic logic are different theorems, then I agree. Basically, yes. In an intuitionistic setting, the notion of provability is baked in from the start, so the meaning of P and he intended meaning of ☐P are going to be effectively the *same* in an intuitionistic setting. We're collapsing the notions of truth and provability, so (☐P→P)→P and (P→P)→P are essentially the same proposition, and in general this is not provable in an intuitionistic setting without laziness or something (and consequently without being unsound). &gt; What the OP and I are doing, however, is different: we are encoding provability logic inside Agda and Haskell, and we are using it to prove the encoded version of Löb's theorem. In other words, we are not implementing a function of type (☐P→P)→P, we are instead implementing its encoded version, Theorem ((Provable a -&gt; a) -&gt; a). Yes but that's missing the point of sigfpe's post, then. Using Haskell/Agda as a meta-language is all good and well, but you can use them as meta-languages for anything you like. That's boring. What sigfpe did was construct a proof of Löb's theorem *as a Haskell program*, which is a far more interesting thing. The former task is trivial and offers no inside, because you can take any proof of Löb's theorem and translate it directly. The latter task is much more interesting and insightful into the nature of Löb's theorem in Haskell. This is the crux of the whole issue.
Ah, everything is clear now, thank you. "Proving Löb's theorem in Haskell" can mean "encoding provabilty logic in Haskell and using that to reproduce the original proof", which may be boring for you, but it's what the OP and I are doing. And "Proving Löb's theorem in Haskell" can also mean to "interpret the symbols (☐P→P)→P as a Haskell type and see what happens", which seems to be what sigfpe has done, with great success. Now that you mention sigfpe's post, I remember that the OP did mention that post as an inspiration. I hope I did not lead /u/want_to_want to solve a different problem than the one they were interested in...
right, and what you have to give up to make CTT consistent is access to the full richness of the underlying language. My point isn't that CTT is wrong or anything, it is that Provability logic is different (but still perfectly reasonable from a type theory standpoint). It makes different tradeoffs. In particular, it is essentially the logic you get in, say Coq, by writing the inference system of Coq as inductive family. That is, provability logic from (an idealization) of the process of describing a logic L internally to L. You get the meta theorem that |- P ---- |- ☐P (trivial), and by proving this meta theorem internally to L you get |- ☐P -&gt; ☐☐P you even get |- ☐(A -&gt; B) -&gt; ☐A -&gt; ☐B but you *don't* get `☐A -&gt; A`. Thats because the proof of Lob's theorem is entirely performable inside type theory--or indeed any inductively defined formal system that includes PA. The key thing is that for each P you can construct a Q such that |- Q &lt;-&gt; (☐Q -&gt; P) and the reason this must be true is that you can embed not just the proofs, but the formula into natural numbers, and then you are left with a statement about arithmetic--one proven by Gödel internally to PA.
&gt;I saw that most documents prefer haskell-platform package for beginners You really do not want to use haskell-platform. Those documents are out of date. Haskell Platform installs packages globally and this is unnecessary if you're not on Windows (it's difficult to build network on Windows for some reason). If you're on Mac or Linux you should be doing something else. Else defined as whatever's in my guide. I've discussed the problems I'm describing with the Haskell Platform maintainers and they agree the approach they take makes more sense for Windows users than other people.
The one true argument is an actual, honest to goodness proof, so I will provide one. If you're not convinced by this, I don't know what will convince you! Pfenning and Davies's version of box modality [here, page 9](http://www.cs.cmu.edu/~fp/papers/mscs00.pdf) has it as a theorem. The relevant rules are ----------------------- hyp* Δ, A valid ; Γ ⊢ A true Δ ; ⋅ ⊢ A true --------------- ☐I Δ ; Γ ⊢ ☐A true Δ ; Γ ⊢ ☐A true Δ, A valid ; Γ ⊢ C true ------------------------------------------ ☐E Δ ; Γ ⊢ C true thus we can prove: --------------------- hyp -------------------------- hyp* ⋅ ; ☐A true ⊢ ☐A true A valid ; ☐A true ⊢ A true ------------------------------------------------------- ☐E ⋅ ; ☐A true ⊢ A true -------------------- →I ⋅ ; ⋅ ⊢ ☐A → A true The other direction, `⋅ ; ⋅ ⊢ A → ☐A true` is not provable in this logic, you're right about this. But I believe Pfenning has worked on type theories with different inference rules that do permit this. [This paper](http://www.cs.cmu.edu/~fp/papers/jacm00.pdf) discusses a quasiquoting system, for instance. The question becomes, are the inference rules capturing the intuition behind the word "provable" in the right way?
You may have done. :x The problem with the approach taken in the main post is that it has no connection to computation. The use of Haskell is incidental, and you might as well just use any axiomatic metalanguage. The power of sigfpe's approach (flawed tho it is) is precisely in that it gets away with so much less. The "right" way to do this, I would say, is you define a type theory for provability logic and see if the relevant formula is a theorem. I would be that it either won't be, or the type theory will not terminate.
Speaking as a dev who has relocated to the UK from within the EU: You wouldn't have any issues at all from a legal point of view. Literally, all you need to do is make a booking to get a National Insurance number soonish after arriving.
Yeah, I always just install GHC. The HP is very large and I don't usually need all of it. I can always install library packages I need from apt later :)
The real shame of this post is, it's a pretty good video.
This inspired some playing around: http://lpaste.net/111397 With a small amount of fussing, I think it should be possible to match C99 and GCC format string behavior precisely, and it also permits extension by defining new printh_format* functions. It also stays pretty general, only explicitly assuming that the resulting type is a monoid, and implicitly assuming String (or IsString, with OverloadedStrings) if there's literal text by generating a string literal. I could clean it up a bit and toss it on hackage if anyone expresses enough interest. Regardless, commentary is welcome. The biggest problem with it, as ever, is that error messages referring to the generated code can be unclear (and this will be the outcome of format-argument type mismatch, so not a rare thing). 
&gt; About the fixpoints, I think in Haskell you can make them for any type: [...] Nice!! Let's see, this means I should be able to think of the type `Psi p a` as the following infinite type: p (p (p (p (...) -&gt; a) -&gt; a) -&gt; a) -&gt; a In order for a function of that type to terminate, one of those nested functions must ignore its argument and directly return an `a`, or otherwise extract one from `p (...)` without relying on the `(...)`, as would be possible if `p` was `Const a` for example. All right, with this last missing part, we should be able to apply our `loeb` function to something non-trivial! To make it even more interesting, let's use a logic which is not an applicative: data Pair w a = Pair w a deriving (Show, Eq) -- cannot be defined: --pure :: a -&gt; Pair w a --pure x = Pair ? x instance Logic (Pair (NonEmpty w)) where logic1 (Pair ws f) (Pair ws' x) = Pair (ws `nappend` ws') (f x) logic2 (Pair ws f) (Pair ws' g) = Pair (ws `nappend` ws') (g . f) logic3 (Pair ws f) (Pair ws' g) = Pair (ws `nappend` ws') (\x -&gt; f x (g x)) The rest of the code is a bit long, so I put in [this gist](https://gist.github.com/gelisam/585fa054b56bb379e4b8). The conclusion is that our "loeb" implementation can be used in much the same way as Dan Piponi's, except that it requires tons of extra arguments and does a tiny bit more work, albeit not very useful-looking work. &gt; I thought about using the Newtype type class I had not heard about [Newtype](https://hackage.haskell.org/package/newtype) before, it looks quite useful. &gt; Making fixpoints in Agda, on the other hand, would be much more interesting. Well, your definition can be converted into Agda *syntax* relatively easily: data Psi (p : Set → Set) (a : Set) : Set where MkPsi : (p (Psi p a) → a) → Psi p a But this definition is rejected for reasons which, if you're not familiar with Agda, might look fairly obscure: in addition to the termination checker, Agda has a "positivity checker", and the above definition fails this test. Basically, it doesn't like the fact that the recursive occurrence of "Psi p a" appears on the left of an arrow. The reason for this is that without the positivity checker, which I disable at the top of the following example, it would be easy to [implement Curry's paradox](https://github.com/gelisam/agda-playground/blob/negative/src/Main.agda) in a way which would fool the termination checker. &gt; I'm still hoping for an actual proof of the Diagonal Lemma :-) Good luck, it looks like it would require a lot of work!
Working with the Haskell Platform is advantageous if you are a library writer. If your library builds against the platform you greatly increase the chance of successful installation of your library by your users.
I use TravisCI for that. I'm not messing up my own machine for it.
&gt; The use of Haskell is incidental, and you might as well just use any axiomatic metalanguage. Indeed, it doesn't need to be in Haskell. My first implementation was in Agda, which the OP transformed to Haskell in order to reach a broader audience. &gt; The "right" way to do this, I would say, is you define a type theory for provability logic As in, extending the rules of lambda calculus with terms whose introduction and elimination rules match those of provability logic? &gt; it either won't be, or the type theory will not terminate. Probably the latter. I replaced all the undefined occurrences with arguments, and the resulting function can loop forever on [some inputs](https://gist.github.com/gelisam/585fa054b56bb379e4b8#file-loeb-hs-L196), just like sigfpe's version.
I'm a little curious, at what point in your development cycle are you planning to open source Cove? At the same time as Wayward Tide is released, or just when Cove hits an acceptable level of performance? In terms of actually transitioning to haskell, it might be easiest to just do game jams or whatever with both teams, to get the other programmers used to it. That can theoretically be inserted into a regular work schedule without overly disrupting anything.
Interesting feature! It will help a lot with writing ghcjs programs.
Perhaps the two of you might be interested in adding something to Leksah?
Right, I am aware of CTT and referenced it earlier. The point is that this is a different logic than what you get from internalizing the host logic's proof rules into a modality. Of course you can have a modality for quotation and slicing, the point is that modality is *not* going to let you study the properties of what is provable in the meta language (which is the point of the provability modality).
[Here is my pull request](https://github.com/haskell/cabal/pull/2119). It took a bit longer than expected, because simply guarding the flag with a MIN_TOOL_VERSION_ghc check would have incorrectly checked the GHC version with which cabal is being compiled instead of the version with which cabal is compiling other packages. Out of curiosity, what kind of doctests were failing when you were using cabal 1.18 with ghc &lt; 7.8? I was under the impression that the circumstances in which the bug was triggered were fairly rare, as I was only able to reproduce it by loading the same package in ghci and in hint.
This is one of the many uchronian software worlds I frequently fantasize about. I *want* my parser to tell me if I made a mistake, not try to "interpret" what I could have meant. But hey, the HTML standards committee are ones who thought the next step in software evolution was to add a &lt;blink&gt; tag. With that in mind, we're lucky things have turned out as well as they have...
Why not use cabal with sandboxes for libraries?
One possible nitpick: Chans are unbounded, so if for whatever reason data is written faster than it is read, memory consumption can grow.
&gt; As in, extending the rules of lambda calculus with terms whose introduction and elimination rules match those of provability logic? Yes, crucially where the rules are computationally justified. &gt; Probably the latter. I replaced all the undefined occurrences with arguments, and the resulting function can loop forever on some inputs, just like sigfpe's version. Yes but are the replacements computationally justified?
This is great. Leksah is my Haskell IDE. Good work.
Well, not really. CTT's box modality *is* the internalization of provability in CTT. The problem is that CTT is embedding in a framework where the notion of provability is the same as the notion of truth. So CTT is exactly the/an intuitionistic understanding of provability logic, but it isn't what you *want* from provability logic because the usual motivation behind provability logic is inherently classical in nature. There is a presupposition to these provability discussions that there are true but unprovable things, which is anathema to intuitionism. That's where the problem really is, not with CTT but with the fundamental assumptions about the relationship between truth and provability *prior* to defining provability logic.
Awesome, thank you! Recently, http-types's doctests began failing regularly on GHC 7.6 when run under Stackage. Perhaps Stackage simply stresses thing out more thoroughly, I'm not sure.
Both notions of provability are intuitionistic. Its not about being classical--its about being *closed*. Indeed, no classical reasoning is used in any of the core proofs about provability logic. Provability logic is an abstraction over latent structure that already exists in any sufficiently rich formal system, and so might confuse the point. The key idea is that the modality "provable" is *definable* within arithmetic. They way you do that is ugly, but there is a perfectly nice way of doing it in type theory. First of all you define a type of codes for all the possible terms in your language data Term = Abs Term Term | App Term Term | Pi Term Term | Var Nat --- then you define an inductive family that encodes the typing rules of your language data Typing : [Term] -&gt; Term -&gt; Term -&gt; Type where VarType : (x : Nat) -&gt; (t : Term) -&gt; (ls : [Term]) -&gt; In ls x t -&gt; Typing ls (Var x) t .... and a function interp : Term -&gt; Type interp = ... then you can define what it means to be provable entirely internally to your logic provable : Type -&gt; Type provable t = sigma (x y : Term), (interp x = t) /\ Typing [] y x if you do this right the axioms of provability logic are exactly the axioms of this provable type function. So why would we care? Well first of all lets make clear what we are internalizing: its the syntax not the semantics. You could get a correct implementation of CTT by interpreting box as identity--that is boring. But that is because the kind of "provability" in the evidence semantics of intuitionistic logic is *not* "there exists a proof in a formal system." Not at all. This is the point about constructive mathematics not being metamathematics. As soon as you have picked a formal system you have excluded good proofs (that is the main result of Gödel/Löb). By contrast, "provability logic" is using the word provable to mean precisely "is provable in a formal system." So when you say &gt; There is a presupposition to these provability discussions that there are true but unprovable things, which is anathema to intuitionism. I think you are confused. There is no formal system which has this property to be of sufficient richness to be generally useful (that is exactly Gödel's first incompleteness theorem). At least in its Martin-Löf variant, one of the main points of intuitionism/constructivism is that it is *open* (or to use Bob Harper's line) has "axiomatic freedom". By contrast, provability logic is *closed.* It is about internalizing a formal system within itself. That is the interesting thing about it (or the problematic thing depending on your point of view). By defining provability the way I do above, it is possible to reason about the proposition `provable P` by *induction over syntax/typing rules.* This is metamathematics. Its not that one view is better than the other, it is just that they are different. 
... assuming your users use HP. But see above.
I strongly disagree with this. HP is a standard set of basic libraries, at versions chosen so that all of the libraries have been tested to work together well. In our commercial shop, we rely on it. We find that the major source of pain is machines that do *not* have HP installed. Don't go that way unless you either really know what you are doing, or use some alternative to HP such as Stackage.
Thanks! I just did this by hand on a new server. A script would useful, but I have a few questions about some of your choices. * Why `lib32z1-dev`? I have never installed this; I install `libzip-dev`, which pulls in whatever else is needed. Seems to work fine. * Why `libgmp10` and `libgmp3-dev`? I always just install `libgmp-dev` and that pulls in whatever is needed. That has worked for me. * I needed to install the `gcc` package, which wasn't installed by default in a bare-bones 14.04 server install. That pulled in `cpp`, which is also needed. * What about all of the packages related to OpenGL and glut? For a server install, where those are not included by default, figuring out all the packages needed for that was always the hardest part of an HP installation for me. And in fact you really don't want those, because they recursively pull in almost all of X Windows for no reason. So this time, I just edited the `haskell-platform.cabal` file and commented out those four packages. It worked great! EDIT: There are some other prerequisites for this installation script to work. The most obvious is that you need GHC 7.8.3 installed into `/usr/local/haskell/ghc-7.8.3-x86_64`. For that, use the GHC 7.8.3 [generic tarball for generic Debian-like 64-bit Linux](http://www.haskell.org/ghc/dist/7.8.3/ghc-7.8.3-x86_64-unknown-linux-deb7.tar.xz) from the GHC site. There are some other executables that need to be installed which are not dealt with by the script, so this script seems to be for upgrading an existing Haskell Platform installation rather than starting from scratch. The required executables fall into two categories: the ones that need to be pre-installed for the script to work, and the ones that can be installed afterwards. Of the executables required to be pre-installed, the trickiest is cabal. On 14.04 server I just used the cabal binary executable that came inside the Haskell Platform 2014.2.0.0. "source tarball" for Linux, and it worked. If that doesn't work for you, there are instructions for how to bootstrap cabal on the [cabal download page](http://www.haskell.org/cabal/download.html). If you have already installed the Ubuntu package requirements described above and in the script, the `bootstrap.sh` script provided with `cabal-install` might just work. If not, view the script and fiddle with it, or do the actions in it by hand. The other required pre-installed executables are `haddock` and `hscolour`, but they aren't completely required. If you don't have them, a local copy of the Haddock documentation for the Hackell Platforms will not be built, but you can still view the documentation on Hackage. Anyway, once you have GHC and cabal, it's easy enough to create `haddock` and `hscolour` using `cabal install`. Two more executables are required for a complete Haskell Platform installation but can be built using `cabal install` after running the script: `alex` and `happy`. You may want to record this information in a README.md.
&gt; Both notions of provability are intuitionistic. Its not about being classical--its about being closed. Indeed, no classical reasoning is used in any of the core proofs about provability logic. No, it's not about classical inferences, it's about classical *philosophy of logic*. As I said repeatedly, the issue is prior to any inference system being set up and comes down to the question "is provability the same thing as truth?" If you think the answer is yes, then you cannot be classical *in any way at all*. And consequently, if you think the answer is yes, then the judgment "A is true" is the exact same judgement as "A is provable" and the basic premise of provability logic -- that we need a special logic to reason about provability -- is deflated substantially because the whole thing is always already about provability. &gt; then you can define what it means to be provable entirely internally to your logic &gt; provable : Type -&gt; Type &gt; provable t = sigma (x y : Term), (interp x = t) /\ Typing [] y x But this isn't entire internal at all. `Type` is not inside the logic defined by the type `Term`, it's external. `Type` is the denotation in the meta language of `Term`, and by this definition of provable, a *denotation* is what's provable, not a proposition. but type theory has no concept of denotations. It's entirely self contained, that's its point. You haven't defined a type theory, but rather a type theory *and a model theory*. &gt; I think you are confused. There is no formal system which has this property to be of sufficient richness to be generally useful (that is exactly Gödel's first incompleteness theorem). At least in its Martin-Löf variant, one of the main points of intuitionism/constructivism is that it is open (or to use Bob Harper's line) has "axiomatic freedom". I don't know what you mean by this, to be honest. This is just the usual understanding of what intuitionism is about. And I can say this with confidence because *it's Bob Harper who pushes it so frequently*. To claim "A is true" is just to claim "I know a proof of A". That is the Harperian line, and the foundation of all modern type theory and intuitionism. I'm not quite sure what you mean by "axiomatic freedom" so I won't comment on that specifically. &gt; By contrast, provability logic is closed. It is about internalizing a formal system within itself. That is the interesting thing about it (or the problematic thing depending on your point of view). By defining provability the way I do above, it is possible to reason about the proposition provable P by induction over syntax/typing rules. This is metamathematics. Its not that one view is better than the other, it is just that they are different. I understand this, and that is partly the point. If you take, say, Intuitionistic Propositional Logic, and want to internalize provability, it's trivial, because every proposition is the same thing as the proposition that that proposition is provable under some assumptions. They have the exact same content. The judgement `A true` just means `A is provable under some assumptions`, and so to internalize this would be pointless (tho entirely possible). Again, I must stress, that if you're working within an intuitionistic setting this makes sense, but if you're classical, it's nonsense because `true` and `provable` are not the same thing. The whole intrigue of provability logic depends on there being a distinction between them. We keep going back and forth on these rather minor issues, and as far as I can tell it's because you don't understand the fundamental philosophy behind modern intuitionism (or really probably even Brouwerian intuitionism, but I haven't bothered to read up on his views beyond what Harper has ranted about).
For myself the biggest challenge with pitching Haskell was finding existing startups that have reached the next stage, so to speak. Most of the existing ones are too small to single out or only introduced the language after they'd already achieved a large amount of success. On the other hand, the libraries on hackage tend to be very high quality and emphasizes robustness. In general the ecosystem around the language is surprisingly mature, so I would try to demonstrate that the libraries on hackage is well suited to both the functional and technical requirements of the product that you are building. We're quite new ourselves, but I think it's useful for existing startups to be vocal about their use of the language. So here we are: https://circuithub.com Feel free to send me a private message to chat, I think it's a good idea for industrial users to keep in touch.
Yeah. For what it's worth, I originally learned the fixpoint thing from [this post by mjd](http://blog.plover.com/prog/springschool95-2.html), and originally learned the positivity thing from [this post by ezyang](http://blog.ezyang.com/2012/09/y-combinator-and-strict-positivity/).
You may be right that I don't understand the philosophy of intuitionism. But, I think a better explanation might be this: there are multiple meanings of the word "proof" in discussions of meta-mathematics and that has lead to communication failure. My goal is not to argue with you, or perhaps to put this another way, I'm not sure what you are arguing with me about. I did not mean to make an appeal to authority, but I must say, [your reading of Bob is overly simplistic and wrong](http://existentialtype.wordpress.com/2013/07/10/constructive-mathematics-is-not-meta-mathematics/): &gt; The key to recognize that a proof is not a formal proof. To avoid further confusion, I hasten to add that by “formal” I do not mean “rigorous”, but rather “represented in a formal system” such as the axiomatic theory of sets. A formal proof is an element of a computably enumerable set generated by the axioms and rules of the formal theory. A proof is an argument that demonstrates the truth of a proposition. While formal proofs are always proofs (at least, under the assumption of consistency of the underlying formal theory), a proof need not be, or even have a representation as, a formal proof. The principal example of this distinction is Goedel’s Theorem, which proves that the computably enumerable set of formal provable propositions in axiomatic arithmetic is not decidable. The key step is to devise a self-referential proposition that (a) is not formally provable, but (b) has a proof that shows that it is true. The crux of the argument is that once you fix the rules of proof, you automatically miss out true things that are not provable in that fixed system. And that is all I am trying to get at. "Provability logic" is about *formal* proof. In that way it is different from what we mean by "proof" in the "philosophy of inutitionism". But, it also doesn't *clash* with intuitionism. The logic of provability (in this sense, as the ability to meta-reason about the very formal system you are reasoning in) is already built into every formal system that includes arithmetic (including ITT). Thats all I'm saying. More generally though: the way I understand intuitionism is that it is not formalism. The intuitionistic philosophy is not "things are true in they are provable in blah logic." That perspective doesn't get you very far since you could put in anything for blah. Rather, intuitionism is about an interpretation of the meaning of the logical connectives that has an evidentiary semantics of a particular form. For example: it seems important to me that you interpret “A implies B” as “there exists an effective procedure for getting from the meaning of A to the meaning of B.” This is the crux of the “BHK” interpretation anyways.
I don't have a full encoding yet, but it would be very surprising to me if Löb's theorem turned out to be not intuitionistically valid. That would mean that the diagonal lemma (writing quines) isn't intuitionistically valid, and I just can't see why that would be so. Gödel's first incompleteness theorem relies on a similar construction, so it would also be threatened. Or am I misunderstanding things? (Of course, if Löb's theorem is intuitionistically valid, then you can't have ⊢(☐A→A) for all A, and vice versa.)
&gt; But this isn't entire internal at all. Type is not inside the logic defined by the type Term, it's external. Type is the denotation in the meta language of Term, and by this definition of provable, a denotation is what's provable, not a proposition. but type theory has no concept of denotations. It's entirely self contained, that's its point. You haven't defined a type theory, but rather a type theory and a model theory. I'm sorry if that was confusing. `provability` defines a modality internal to the logic where the definition of `Term` appears. Lets call it `L`. If the logic of `Term` and `Typing` matches the rules of `L` then all of the theorems of provability logic are derivable. &gt; I don't know what you mean by this, to be honest. This is just the usual understanding of what intuitionism is about. And I can say this with confidence because it's Bob Harper who pushes it so frequently. To claim "A is true" is just to claim "I know a proof of A". That is the Harperian line, and the foundation of all modern type theory and intuitionism. I'm not quite sure what you mean by "axiomatic freedom" so I won't comment on that specifically. I responded to the bit about "what intuitionism is about" earlier and I assume you either know or can at least look up the first incompletness theorem. As for the idea of "axiomatic freedom", well, this has appeared in Bob's writing from time to time. Actually it might not be his idea originally: I just got it from him. The best discussion of the idea I know of is in his lectures from the OPLSS this year which I encourage you to watch. 
Nice job getting this to work, but I really wonder about the UX when having an IDE inside a pane inside an IDE. I'm using the web inspector full screen all the time and already have the feeling I need more screen real-estate. 
Because I have stable versions of many libraries in my repos, and I want to make sure others can build my project with those easily-available versions if possible. Also I don't have to wait around for packages to compile :)
Leksah pane manager is fairly flexible. You can **View -&gt; Detach** the **Inspect** pane and maximize it on another monitor.
Heh. I haven't reproduced this exact issue outside of Stackage yet, but it's not complaining about loading up the http-types object files, but rather some of the dependencies. I've just kicked off a 7.6 Stackage build locally, and I'll report on the full error message (assuming it happens again).
Although videos from the event apparently still haven't been posted, I found the slides for the HaLVM talk here: http://www.xenproject.org/help/presentations-and-videos/video/latest/xpds14-unikernels.html
&gt; it's not complaining about loading up the http-types object files, but rather some of the dependencies. That does sound a lot like the symptoms for my bug!
&gt; Yes, crucially where the rules are computationally justified. What do you mean by "computationally justified"? In the context of extending the lambda calculus, I would assume that it would mean that the terms have an operational semantics which preserve the types, but then you use the same words to talk about the arguments I gave to my loeb function when it looped: &gt; Yes but are the replacements computationally justified? Well, those arguments are Haskell functions, so they do compute something, does that mean they are computationally justified? In any case, I linked you to the source in my previous comment, so if you want to, you can look at the arguments I gave to my `loeb` function and tell me if they look justified.
The main problem with e.g. moving `return` out of `Monad` is that it breaks every single definition of a `Monad` made since 1994. There is no inference win for having it in there. Ideally we'd one day remove it. However, there isn't a clean upgrade path. Defining it in terms of pure by default tends to cycle, as users as often as not go the other way. -- and these errors are almost impossible to spot until you get to runtime and the code loops forever! So we can't even use that to bridge the gap by saying "ok, from here out we deprecate the manual construction of "return" and eventually removing it from the class. We have a similar issue with `(&gt;&gt;)`, which we'd like to remove from `Monad` for similar reasons. It is redundant with `(*&gt;)` in `Applicative`, and could just be a top level definition with different fixity but for all those `(*&gt;) = (&gt;&gt;)` definitions out there. Haskell as a language and a culture is somewhat bad at these transitions, and we've let these pie-in-the-sky perfect solutions be the enemy of all progress on this front for a long time.
In this case I would be able to define my own prelude that fixed all this and just use it in my own code. I'd still be free to import code using the normal Prelude that has all these situations and people could even import my code without issues, right? So long as my custom versions of Monad, etc., didn't escape. But that would allow me to be using the knowledge that has been hard won over the years without worrying about breaking existing code. I realize it's really hard for the official Prelude to move for all the reasons you mentioned.
Which is what my guide tells you to use if you're on Ubuntu - that's what I linked to.
If this is presentation I think it is, then I've seen a version of it before, and it is quite informative. Great work, Evan! EDIT: Yes, this is an updated version of Evan's "Taxonomy of FRP" talk I've seen. It really put things in perspective for me - before, I was really lost trying to distinguish FRP libraries.
&gt; In C++, which supports ad-hoc polymorphism, a generic function like: &gt; template&lt;class T&gt; &gt; list&lt;T&gt; r(list&lt;T&gt;); &gt; can do all kinds of weird things. What sorts of weird things? I know it can throw an exception or mutate global state, but neither of those are prevented by parametric polymorphism. Can someone give an example of a C++ function with this signature that parametricity would prevent?
 template &lt;class T&gt; list&lt;T&gt; r(list&lt;T&gt; l) { return l; } template &lt;&gt; list&lt;bomb&gt; r&lt;bomb&gt;(list&lt;bomb&gt; l) { l[0].blowUp(); return list&lt;bomb&gt;(); } r is an identity function... unless your list contains bombs.
&gt; it can throw an exception or mutate global state, but neither of those are prevented by parametric polymorphism What do you mean? They are prevented by Haskell's parametric polymorphism.
But Haskell does have [exceptions](http://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Exception-Base.html#t:Exception), which can be thrown in a polymorphic function just as they can in C++ (even if they're seldom used). And I don't see how parametric polymorphism implies purity; can you explain what you mean there?
Computational justification means: you have beta and eta rules.
I think there will be a much deeper problem that validity, namely, that you just can't translate the *meaning*. Validity becomes a tangential issue at that point.
prototrout's point is that you can [`throw`](http://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Exception-Base.html#g:2) from pure functions. This is technically correct but we typically like to pretend this doesn't exist when talking about the benefits of Haskell's type system because Haskell programmers don't really tend to use this. I'm not sure where the mutable state comes into it though. 
I guess I'm being a bit unclear. I'm not saying you can do that in Haskell (which you can with `unsafePerformIO` but that's beside the point). I'm saying that a hypothetical parametrically-polymorphic C++ would still allow it (and that even though Haskellers might consider it a "weird thing" it's not what I'm looking for). I was asking how *C++* polymorphism isn't parametric (/u/cnjry and /u/Roboguy2 gave good examples) and specifically calling out exceptions and mutation as non-examples. I guess I would have been clearer if I had just left that part out.
Thanks, that's exactly what I was wondering about.
In type theory broadly, the terms beta and eta mean local reduction and local expansion. It's not limited to just lambdas these days (or probably any time in the last 40 years).
Oh, well there are languages with what you could call "parametric polymorphism" which doesn't prevent exceptions or mutable state. OCaml is one example. What they don't have is *relational* parametricity. Relational parametricity is a particularly strong form of parametricity that is satisfied by the polymorphic lambda calculus, and Haskell too (at least if you ignore naughty things like exceptions, `unsafePerformIO` and `seq`). In the Haskell world we tend to conflate the two notions so I can see why it would be confusing.
What we do is claim that all bottoms are indistinguishable within the pure fragment of the language. That makes throwing an exception, looping forever, invoking `undefined` or `error "foo"` all the "same" thing from the denotational semantics of the pure portion of Haskell, only visible when you explore a finer grained operational model, in which some can be recovered from out in the `IO` sin-bin better than others.
I don't want to out myself so explicitly. PM'ed.
That is true: with power comes responsibility. Allowing for implicits can complicate typeclass resolution, but this burden isn't really absent in the current implementation: to deal with canonical instances, we have to create new types and map between them, which leads to pointless names for types (we have both Option and Maybe to do the same thing -- granted if we are using Option, we'll probably never use Maybe, but it still pollutes the name space). 
From the few examples I could find online, it looks like "local reduction" rules corresponds to what I've learned was called "small step semantics", while "local expansion" seems to say that for every eliminator, there is a way to use this eliminator in a way whose semantics is the identity. Does that sound about right? From what I can see of the rules of provability logic on Wikipedia, the type "□A" doesn't seem to have matching introduction and elimination rules, which probably precludes local expansion for those eliminators. Instead, there is a rule to construct values of type "□A", and then various ways of manipulating those values while keeping the values inside the box. Still, what do you think of the following interpretation of provability logic? Is it "constructive" enough? τ ::= String | τ₁ → τ₂ | □ τ e ::= x | static e | static-static | λ x. e | e₁ e₂ | static-app | show | static-show ⊢ e : a ------------------- ⊢ static e : □ a --------------------------------------- ⊢ static-static : □ a -&gt; □ (□ a) ---------------------------------------------- ⊢ static-app : □ (a -&gt; b) -&gt; □ a -&gt; □ b ---------------------- ⊢ show : □ a → String --------------------------------------- ⊢ static-show : □ (□ a -&gt; String) -- small steps operational semantics static e = Box "string-containing-the-source-for-e" static-app (Box f) (Box e) = printf "(%s) (%s)" f e static-static (Box e) = printf "static (%s)" e show (Box x) = x static-show (Box e) = printf "show (%s)" e That is, my constructive interpretation is a little language for constructing quines and other similar programs which output strings representing other programs. Like in Cloud Haskell, there is a `static` operator whose argument must consist solely of "top-level" constructs, but since there is no way to bind new top-level constructs, this means that the argument may only contain builtins. Unlike Cloud Haskell, there is no `unstatic` operator to execute the static expression: the only thing we can do with our captured expressions is to show them, and even then, that's because I added this as an extra feature not present in the original provability logic. "□A" and "String" both represent string values, but the former has a phantom type indicating the type of the program represented by the string. We can't obtain an "A" back from an "□A", because there is no eval, and since there are no string-manipulating operations, there is no way to implement one. There are, however a few builtins for creating strings which look like other operators, in order to help with quine formation. There is no way to create strings which look like those string-formers (that is, there is a `show` and a `static-show`, but no `static-static-show`), which affects how easily we can create quines, but not how closely the language models provability logic. Does that sound about right?
You have a bunch of axiomatic things in there which just aren't acceptable. You don't get to just assert that you have things like `static-static` or anything below it. You have to justify the claim that these things exist.
It enables the following development workflow. Develop by compiling a plain ol' native Haskell program using ghcjs-dom with webkitGtk for your GUI frontend. Maybe leverage your favorite FRP library. All your Haskell development tooling works, cabal repl, hlint, ghc-mod ... Pop the Inspector to tweak the layout, style, attributes, nodes ... adjust code ... Then $ echo 'compiler: ghcjs' &gt;&gt; cabal.config and BOOM rich client JS webapp in your browser.
no access to documentation sounds like it would be the worst for starting a new project
I like to set `documentation: True` in my `~/.cabal/config` file so that cabal automatically builds and locally installs the documentation of all packages that I install. Then I can access all documentation offline for trips.
In type theory, you're not allowed to just assert things like you're doing. You have to justify them by supplying computational principles of a certain sort. The best I can suggest is watch some OPLSS 2012 videos (Pfenning's Proof Theory Foundations lecture 1, and also probably Harper's first lecture as well, tho I'd have to look to see which is most relevant). Any attempt to explain it on my part will fall short of theirs. *Edit* Tho, possibly, because you're providing some kind of operational semantics it might be acceptable.. this is just a very strange sort of thing to do. Usually it's considered .. unwise, I guess, to just define things in terms of other connectives like that. Perhaps in the case of `-&gt;` it's not so bad, because you can view that connective negatively. &gt; I know you mean a "computational justification", but it's still unclear to me how one would do such a thing. You have to provide beta and eta for the connectives of your system. &gt; In the language known as "Hutton's razor", for example, how do you justify the builtin addition operator, if not by giving its typing rules and its operational semantics? I'd say the most natural view on Hutton's razor is one in which there are no elims for the number type, and so the beta and eta rules are trivial
For existing work, there's two or three c++ "reactive" toolkits out there. The most recent one I saw on /r/cpp was: http://schlangster.github.io/cpp.react/ . 
Wow, that was fast. Thanks!
It's fantastic to see other people working on this! I've been working with Netwire / GLFW / OpenGL for a little while as well. I'm very interested in looking at your code!
Surely you can do the same thing with RULES pragmas?
I've grown quite fond of Elm's naming for the function composition and application operators: they add hardly any syntactic overhead, but can lead to much more fluent code. Since they're symmetrical, you can switch easily to left-to-right order in the cases where it makes more sense. munge = map (+ 1) &gt;&gt; filter (&gt; 7) &gt;&gt; reverse munge thing = thing |&gt; map (+ 1) |&gt; ... I've been tempted to define these for my own use in Haskell, but I'm afraid it might frighten people. I've noticed the diagrams library uses a similar style, so perhaps I'm not alone here?
Oh I know, I've been looking at it for a little while. Thanks for the link though!
I hope someday I'm able to understand netwire... Keep up the good work!
I've never really bothered looking deeper into FRP, but this was really enticing. An overview like this made the subject much more approachable for me. Forwarded to a couple of interested friends as well. Good presentation!
What software was used to make the slides? That was pretty cool.
I can't help but wonder if Stackage doesn't make package version numbers almost redundant (if it wasn't for code using `MIN_VERSION_xxx` macros), since it provides snapshotted sets of packages that are known to work with each other...
&gt; instance Monoid Integer (+) where mempty = 0 &gt; instance Monoid Integer (*) where mempty = 1 This looks pretty neat. Is there any language that lets you do something like this? If so, how do you use it (also, how would usage compare to how it's actually used in math functions)?
&gt; The best I can suggest is watch some OPLSS 2012 videos (Pfenning's Proof Theory Foundations lecture 1, and also probably Harper's first lecture as well, tho I'd have to look to see which is most relevant) Thanks for the suggestion! I've been meaning to watch all the OPLSS videos eventually, but there's a ton of them, so of course it will take me a while. I'll reorder my queue so that Pfenning and Harper's lectures are closer to the top :) &gt; I'd say the most natural view on Hutton's razor is one in which there are no elims for the number type, and so the beta and eta rules are trivial I'm not sure what such a "trivial" rule would look like (neither of the usual suspects, unit nor identity, seems to make sense here), but since there is no way to extract an "A" out of a "□A" in provability logic, would it make sense to also say that the modality has no eliminators and that its beta and eta rules are similarly trivial?
Cool.
Start with those videos. They're the best. Pfenning then harper. &gt; I'm not sure what such a "trivial" rule would look like (neither of the usual suspects, unit nor identity, seems to make sense here).. Unit has a trivial elim: no elim at all. So beta reduction is easy to define. In short, beta says: "any time you have elim after intro, you should be able to get the same conclusion without the intermediate use of the connective". Since Unit has no elims, its vacuously true. &gt; but since there is no way to extract an "A" out of a "□A" in provability logic, would it make sense to also say that the modality has no eliminators and that its beta and eta rules are similarly trivial? Well, not being able to extract an `A` doesn't mean □ has no elims. Tho, eta will force it to be true that if you put an `A` into an intro to get `□A`, then somehow you need to be able to get out an `A` somewhere (perhaps only as a var in a case branch).
So wait. Does elm only have Behaviours and no Events? All signals seem to be just continuous functions of time that are sampled. If so. What is the role of Events in FRP? (disclaimer: my only experience with FRP is elm.) 
Why not sharing the doc folder from docker to the host ?
Could you please tell me how I can do that? You mean using docker volumes?
&gt; What they don't have is relational parametricity. Yes, they do. The relational models for (idealized) ML and (idealized) Haskell are nearly identical. The main difference is that the relational interpretation of ML expressions is the same as the relational interpretation of Haskell's IO monad, more or less. But via the module system, it's actually easy to define pure functions in ML. Easier than in Haskell, in fact! (But it's syntactically heavy enough that no one does.) Real Haskell has an even weaker equational theory than SML due to `seq`. Real Haskell validates no eta laws at all, whereas SML validates eta for sums. OTOH, real Ocaml has a weird "polymorphic" identity test which breaks its equational theory. These days, though, we have languages like Agda and Idris, where we can talk about beta and eta laws and actually be telling the truth. :) 
Yes with docker volumes. As an aside, these are 2 nice ways to get local documentation in Haskell: * https://github.com/jfeltz/dash-haskell * https://github.com/robinp/nemnem I am using the first one within emacs and it is really awesome.
Hi there. Thanks for those links. Dash-haskell looks promising. Can the generated docsets be viewed in a browser for example. I realize that the generated docsets can be viewed with vim and emacs though...
Try a simpler FRP language like Elm to understand the concept, and then go for Netwire for extra power. 
One thing to note is that there are variations of FRP so it may be worth checking the video: [Controlling Time and Space: understanding the many formulations of FRP](https://www.youtube.com/watch?v=Agu6jipKfYw)
(What you say is very interesting, but let's not lose sight of the goal of the thread which is to reaffirm that in Haskell Wadler's Theorems For Free hold, even if `unsafePerformIO`, exceptions and `seq` make this not literally true. I know as a type theorist this will make you queasy but luckily for me this isn't /r/typetheory, so I can get away with it!) Anyway, on to the interesting stuff. I'm curious about what definition of "relational model" you are using, since the only one I know of is Reynolds' original (or perhaps better is Pitts' constructive version since it actually exists) and that definitely does not deal with effects. How would you distinguish this original, pure, relational parametricity from relational parametricity that deals with effects? Specifically I want to indicate to /u/prototrout that parametric polymorphism in OCaml, say, gives you *some* guarantees about `f :: a -&gt; a` but Haskell gives you the guarantees that we are used to from the relational parametric notions of Reynolds popularised by Wadler's Theorems For Free. Also, how is it easier to define pure functions in ML than Haskell? 
Yes, sure you visualize the docsets in a browser
http://dictionary.reference.com/browse/compositionality?r=75&amp;src=ref&amp;ch=dic
I think this is part of the power of Elm. It has a deliberately smaller scope than Haskell, which means it has a lot more freedom to make changes than Haskell does. Right now in Haskell, you could never get rid of (.) as composition. It's too widely used, and would break a bunch of people's code. It's good that Haskell is stable, since otherwise it wouldn't be adopted by schools, companies, etc. But it's good that Elm can explore new things that Haskell can't.
You could run a web server in the container and publish its port. A simple way would be to use warp. Start the container using something like `docker run -p 3000:3000`. In the container, `cabal install warp` and then run `warp` in the root of the documentation directory. Now on your host OS you should be able to browse to &lt;http://localhost:3000&gt; to see the docs.
For now, I went with, ssh -X user@dockercontainer firefox (inside the container) So this essentially uses your hosts xserver to render firefox that is running inside docker. Not neat. But gets the job done.
As soon as Cove is ready, which will be before the game is ready. I don't have any performance characteristics in mind; there are a few barriers we need to overcome before I'm happy to release it (mostly around code quality). I don't want to get tied to an estimate, but you can roughly track progress against https://github.com/palf/haskellSDL2Examples - when this hits lesson 50, the next commit is Cove.
This is a nice (and very pretty!) talk. One thing missing from it is a recent (last 2-3 years) line of research which establishes that if you think about FRP as, basically, the lambda-calculus for temporal logic, then you end up with a highly expressive higher-order FRP system with a really simple implementation story -- you don't even need a dataflow graph! Some relevant links: * Wolfgang Jeltsch, [*Temporal Logic with “Until”, Functional Reactive Programming with Processes, and Concrete Process Categories*](http://www.ioc.ee/~wolfgang/research/plpv-2013-paper.pdf) * Alan Jeffrey, [*Functional Reactive Types*](http://ect.bell-labs.com/who/ajeffrey/papers/lics14.pdf) * Andrew Cave, Francisco Ferreira, Prakash Panangaden and Brigitte Pientka, [*Fair Reactive Programming*](http://www.cs.mcgill.ca/~acave1/papers/fair-reactive.pdf) * Neel Krishnaswami, [*Higher-Order Reactive Programming without Spacetime Leaks*](http://www.cs.bham.ac.uk/~krishnan/simple-frp.pdf) Probably my paper and Alan's are the most concerned with implementation. Alan embeds everything into Agda, and I show how to implement higher-order FRP using only plain old lazy evaluation. 
It's not off the table; your best bet is to email your CV and questions to business@chucklefish.org - the group that responds to that email address have far greater HR knowledge than I do!
The Elm `Signal a` is more like a pair of an `Event ()` and a `Behavior a`. The `Event ()` says when the signal is updated, and the `Behavior a` gives the current value. I suspect one can give a denotational semantics for Elm programs in terms of something like this. So far, I don't really like this design decision and prefer the clarity and explicitness of separate `Event` and `Behavior` types.
Where does this documentation appear? I never found the location. 
By the way, Oliver Charles has been playing with a doom-like engine at https://github.com/ocharles/hadoom and is tweeting about his progress.
Indeed, just not as strong ones as `[a] -&gt; [a]`.
I think RULES pragmas also come with a proof obligation (like using unsafePerformIO) that you are actually replacing functions with equivalent implementations.
I figured this trick out after my trip. They're found under ``.cabal-sandbox/share/doc/[architecture]/[package]/html/``, with for example architecture = ``x86_64-linux-ghc-7.8.3`` and package = ``text-1.1.1.3``. (Substitute ``~/.cabal`` for ``.cabal-sandbox`` if for some reason you aren't using a sandbox.)
Wow, useful! Any chance this will land into GHC &amp; other tools?
FRP semantics are simple and precise. For the "classic"/applicative style (rather than "arrowized"), see [Functional Reactive Animation](http://conal.net/papers/icfp97/) and [Push-pull functional reactive programming](http://conal.net/papers/push-pull-frp).
The [first piece of code is already on Phabricator](https://phabricator.haskell.org/D169); me and Simon M just talked about it today - there's a lot left after this, but what's there lay down the first parts of the infrastructure.
This talk is both inspiring and disheartening. I would like to write my next project in haskell, but it's hard to justify using it when someone with far more haskell experience than I has to litter her talk about making a simple game with "I couldn't get it to compile" and "I couldn't figure it out."
So far as a culture we have chosen to consider fundamental disagreements between the `Monad` and `Applicative` like this to be a design error. It is very tempting for this case, but I think they general pattern of expecting them to be observably the same for whatever notion of observation you limit yourself to holds a lot of merit. There is little difference between this and expecting, say, the Applicative for [] to be the ZipList and the monad to be the non-determinism case. In both cases you're doing very different things in the `ap` and `(&lt;*&gt;)`. The cases like Haxl where you merely get to use fewer passes and it is an optimization over the semantics you could have I think are more defensible.
What would you suggest? Make `&lt;*&gt; = ap` for `Errors` and add another type `ErrorsMany` (without `Monad` instance) where `&lt;*&gt;` is as above?
First of all, on 7.10 Monad is going to be a superclass of Applicative, so please don't ever do this. Second, in this specific case it doesn't even do what you want. With the monadic formulation you get *the first list* of errors, not the first error. (Try `sequence_ [Errors [1,2], Success (), Errors [3]]`). Even worse, this means that it may as well return an *empty* list of errors if it wishes. You may want to make a function `Errors e a -&gt; Either e a`(or to some similar error monad), and make `Errors` just an applicative. I'd make `Errors` use a non-empty list to make it safe. data Errors e a = Errors e [e] | Success a deriving Show, Functor Also: you can use the `DeriveFunctor` extension to make `Functor` instances automatically. You don't have to make your own.
&gt;but GHC doesn't bother telling us that, since including that level of information in every error message would be overwhelming. Hm, I think you're giving ghc too much of a free pass here. Ghc can do better.
"Just use Stackage" is all well and good, but I feel this was a missed opportunity to also evangelize the virtues of hsenv.
I don't see why alternative composition operators couldn't be provided and (.) deprecated. It would certainly need a pretty long deprecation period before removing it from the Prelude, but it would avoid the confusion with "." as the module name separator. I think what's missing is strong community desire to change.
I have really positive experiences with `hsenv`. It's fantastic for working with multiple versions of `ghc`.
That is pretty much the tack taken by the Either vs. Validation crowd thus far. If you make it easy enough to move between the two you can get a usable API and still be able to reason about the code without worrying about which Applicative/Monad combo you are working on to know how (&lt;*&gt;) and (&gt;&gt;=) interact.
I didn't even know about [`validation`](http://hackage.haskell.org/package/Validation-0.2.0)! Cool, sounds good. Thanks.
The point about using a non-empty list of errors is a good one, thanks. I don't really mind the monadic formulation returning the first list of errors, rather than the first error though. When you say "on 7.10 Monad is going to be a superclass of Applicative, so please don't ever do this" what is the reasoning there? Is that somehow going to tighten the requirement that `&lt;*&gt; = ap`? How would that make `&lt;*&gt; /= ap` any less bad than now?
All this recent threads about Haskell games makes me want to resume my work on my small playground :) (despite the intimidating name it's just an attempt to blend classica Entity Component System architecture and FRP): https://github.com/adinapoli/brutal-netwire
I thought the type system in the HOFRP paper was pretty interesting, but that's not what I would call FRP (it's not continuous time) it's just discrete stream processing. This is not to knock the work at all, I still thought it was very cool. :) I'd still like to see a continuous time system that isn't prone to space leaks, isn't arrowized, and isn't implemented via somewhat hacky approaches like what [apfelmus describes here](http://apfelmus.nfshost.com/blog/2011/05/15-frp-dynamic-event-switching.html). I'm curious if your work could be extended to deal with continuous time.
Functional programming is pretty huge in universities from the Netherlands, but I have yet to encounter a Belgian computer scientist / engineer that has used Haskell extensively. What is the primary language used in computer science lessons?
I didn't want to get into that topic here, but you're right. Read my comment as: there's a good reason that GHC gives more information in one case than the other. It would be nice if GHC would be even more helpful here.
`hsenv` wouldn't prevent this situation from arising. If you create an hsenv and then follow the same steps that I outlined, you'll end up with exactly the same problem.
Looks cool! I believe there's a typo in the abstract, last sentence "...that allows rabid theory forming..." should probably be "...that allows rapid theory forming..."
I say keep it.
It would make it way worse than now because it makes the link between Monad and Applicative even more standard than it was before: it means that anytime `Control.Monad` could just define `ap = &lt;*&gt;` and break your code if you rely on `ap` and `&lt;*&gt;` being different.
/u/jaspervdj
Elm is not really a variant of Haskell. Elm is a separate programming language strongly inspired by Haskell (like Idris.)
Right, hsenv doesn't solve the problem on its own. It just works well with Stackage to create working environments.
I was railing on about ap vs. &lt;*&gt; because I'm figuring out how to port the monoidal Validation to `either` and deciding whether to include a monad instance or not. Also, how to bounce between Validation and Either.
There's also [`Errors` in `transformers`](http://hackage.haskell.org/package/transformers-0.3.0.0/docs/Control-Applicative-Lift.html#t:Errors) which behaves just like `validation`, albeit
I think Haxl is the best counterexample to `ap = (&lt;*&gt;)` and forms a pattern for how to "do it right". In particular, one should define the "observable semantics" for the applicative and monad cases, state it clearly, and ensure that at least "up to observation" they are identical. Another interesting example of this is how the `Gen` type from Quick Check "follows the monad laws". It clearly does not since it constantly is splitting the random seed internally and therefore you can only say it follows the monad laws under observation of its sampling distribution—a weaker form of equality than one might assume naïvely.
I have a ticket for Haskell eXchange 2014, bought with early bid discount. Unfortunately, I will not be able to attend, so if you want to buy it from me, let me know!
[This paper](http://elm-lang.org/papers/concurrent-frp.pdf) explains it best, but the idea is that signals are continuous values that change discretely. This abstraction comes out of the work on [Real-Time FRP](http://haskell.cs.yale.edu/wp-content/uploads/2011/02/rt-frp.pdf) which points out a nice isomorphism: Event a = Behavior (Maybe a) It means the core set of operations you need to learn is much smaller and simpler, and [you can build an `Event` type on top of it](https://github.com/seliopou/elm-d3/blob/master/src/D3/Event.elm#L31-L35) if you are into such things. I believe you can build up Behavior specific stuff like `integral` as well if you are into that, but I have not seen it done explicitly.
Yeah, it is a Prezi. I [posted it online](https://prezi.com/rfgd0rzyiqp_/controlling-time-and-space/) if you want to take a look :)
Consider the monad transformer version of this newtype ErrorsT e m a = ErrorsT (m (Either [e] a)) type Errors e a = ErrorsT e Identity a Is that applicative really what you want? mmf &lt;*&gt; mmx = ErrorsT $ do mf &lt;- mmf mx &lt;- mmx return $ case (mf, mx) of (Left [e], Left [e']) -&gt; Left [e ++ e'] (Left [e], Right{}) -&gt; Left [e] (Right{}, Left [e]) -&gt; Left [e] (Right f, Right x) -&gt; Right (f x) This will run the monadic action to try and compute the argument (mx), regardless of whether computing the function (mf) was successful. Sometimes "fail early" is the behavior you want, and "fail early" is the monad instance you've provided. Other times you want "gather all the errors", and "gather all the errors" is the applicative instance you've provided. In the monad transformer case, I'd go one way or the other, as long as the Applicative and Monad instances match. (Actually for "gather all errors" you can't really have a monad instance.) But I hope I've hinted well at how there could be good reasons for wanting even the applicative instance to "fail early."
Thank you, and thanks for collecting these links! I have only kept up with your work really, so this is very helpful. Do I understand correctly that these systems all require a type system more complicated than some sort of HM(X)? I recall your paper used linear types. Is this common among any system that takes this approach? Do any of these papers have implementations or code samples to help clarify what it might look like in practice? Perhaps it's too early for that?
She has a mac, which is more problematic than using something linux (bindings are often out of date or libraries don't compile). Stick to linux and you have a better chance.
I disagree -- Kamp's theorem says that the two natural models of temporal logic are the naturals and the reals. So I see no reason to favor one over the other. However, there is a semantic problem with continuous time: no one knows how to give a reasonable denotational semantics of higher-order functions with continuous time. The correctness criterion we want for implementations is that as the sampling rate goes to infinity, the implementation's behavior should approach the continuous semantics. As a result, the denotational semantics should not contain "Zeno behaviours", which are behaviors which don't arise as the limit of a discrete process. We know how to do this for first-order functions (see Wan and Hudak's *FRP From First Principles*), but no one knows how to extend it to higher-order. I suspect you might be able to come up with something using categories of sheaves over piecewise linear manifolds or some similar jargon, but I've never pursued it since it seems like an awful lot of machinery for a modest benefit. Still, this is a "no one's done it" problem rather than a "there's a no-go theorem" problem. 
Chapter 6, the performance investigation walkthrough, is really interesting. These performance war stories I think are really lacking and helpful. Thanks! Also the actual analyzer looks really good. Can't wait!
Fantastic talk!
&gt; It means the core set of operations you need to learn is much smaller and simpler I think you are overselling here... unifying two concepts into one is only simpler if the programmer doesn't typically want to distinguish between the two concepts. Otherwise, the programmer has to do work to undo the unification (without type safety). I've already seen this quite a bit in Elm code I've written, and its a source of confusion on the mailing list as well. It's actually easier to go the other direction -- start with concepts being separate, and introduce abstractions to capture commonalities. Of course, Elm lacks typeclasses that would make this possible... Anyway, I've tried to keep an open mind, but keep concluding that separate types for `Event` and `Behavior` are the better design choice. :) Also, I wanted to respond to one other thing - when you are talking about learnability, simplicity, etc, minimizing the number of core operations or types is not necessarily better for learners, nor is it necessarily simpler in any meaningful sense! If there are fewer operations and types, but it is harder (or involves more code) to produce a desired effect, that is actually a more complicated programming model, which is what we really care about.
&gt; I disagree -- Kamp's theorem says that the two natural models of temporal logic are the naturals and the reals. So I see no reason to favor one over the other. Hmm. Could you explain then how the system you describe in your paper could be used to model a system with continuous time? I am not seeing it..
Hahaha, I concur
Albeit with terrible error messages. Not a reasonable alternative.
Oh, I wrote something to that effect and reddit clipped it. I think I originally said "albeit with more typing noise".
You can use the online search engines. Here is one that searches the fpcomplete snapshot: https://www.fpcomplete.com/hoogle
Dibs! I'll send you a pm.
Did you go with FRP in your prototype?
This would be a nice little first project for someone who wanted to contribute to GHC, adding a note to the error message like Note: there are multiple versions of bytestring loaded: bytestring-0.10.0.2 and bytestring-0.10.4.0
Dash-haskell looks really good! Thanks for the link.
Yes, please! Some useful points: - What is easier/ comes more naturally in some libraries than in others? - What is their history / which were inspired by others? - What are current fundamental differences (switching vs non switching), pure running functions, time leaks etc.? - What are current areas of improvement? Most available information about these seems to be out of date.
Is one of the things implied here that the mythical man month can somewhat be recovered because a few clever developers can set up a skeleton which the type system helps ensure gets fleshed out properly by less talented ones?
Love the Asterix quotes. 
Stuff like this is great, but it's not the kind of things you can show to management without looking extremely bias against everything non-Haskell. ;_;
[This video](https://www.youtube.com/watch?v=Agu6jipKfYw) [posted recently](http://www.reddit.com/r/haskell/comments/2h43hs/elm_controlling_time_and_space_understanding_the/) goes over a lot of the ideas behind current FRP implementations out there. Did you have any questions not covered?
I think that's an overstatement. But I do think it is better at letting talented devs be "force multipliers" for others. So you get to at least better partition and segment the natural divisions between different elements of a codebase, and hence come closer to the "genuine" limits of problem divisibility.
It is somewhat true that Haskell makes it easier to support less-experienced developers. Getting the underlying base right is important for every large project. But it's just as important that further development doesn't stray too far from the base. (As a corollary, if the base turns out to not be right, it's better to change it with aggressive refactoring than hack around it.) The situation works a bit better in Haskell, where a good (type-driven) base ends up with the compiler helping to minimize the amount developers will stray from it. I'm not sure I'd say it really recovers the mythical man-month. I'd say it's a way to get better quality out of a team with inexperienced members - but it's not going to help scale a team to larger than it should be.
And, in the event that the base turns out to be wrong, the compiler helps you refactor it with more confidence.
There is a comprehensive tutorial on this at: https://github.com/serras/emacs-haskell-tutorial/blob/master/tutorial.md
There standards committee is working on improving this! [Concepts Lite](http://isocpp.org/blog/2013/02/concepts-lite-constraining-templates-with-predicates-andrew-sutton-bjarne-s) will be coming to a C++ near you ... eventually. You'd be able to define a Measurable concept (kind of like a weak variation on a type class) and require template &lt;Measurable T&gt; int sizeTimesTen(const T&amp; x) { return x.size() * 10; }
They do: collection comprehensions. Filtering in those is more idiomatic than using a filter function.
&gt; Right{} &gt; Now *that* was neat. I hadn't realised you could do that!
You say poTAYto, I say poTAHto. List comprehension are just maps and filters behind the scenes, and in my book just as good. I'll grant that they are not explicitly calling 'filter' but the idea is to not reinvent the wheel with a for loop each time.
Assuming you have Emacs 24 and packages setup, you can do `M-x package-install RET haskell-mode`
I disagree. Collection comprehensions make you specify again and again how to take apart and construct a sequence. In fact, collection comprehensions are just a sort of "for loop expression".
Which is why I'm glad I didn't introduce hsenv in this context, it would simply introduce a confusion. Sandboxing doesn't help solve the problem that you can still install incompatible package versions, it just makes it easy to fix the problem and contain its impact. Adding comments about sandboxing would make it seem like it's directly relevant to this problem. I actually think that's a notion we need to *fight* as a community: I've seen far too many claims of "just use cabal sandbox and you won't have dependency problems," when that's empircally not the case. I happen to agree that using hsenv with Stackage is a great combination (though personally, I think Docker + Stackage is even better). However, it has one major downside: it doesn't work on Windows. So the more universal solution is `cabal sandbox`, but unfortunately [it doesn't yet work with modified configs properly](https://github.com/haskell/cabal/issues/1884). Once that gets fixed, using a `cabal sandbox` will be much easier with Stackage, and will be advertised far more prominently in the Stackage getting started guides.
That seems odd, either just use a for loop with an if statement, or have your collection class have a filter method that accepts a function pointer to predicate function and that is just the simplest solution. It's completely feasible to compose the function predicates beforehand by overloading (&amp;&amp;, || etc.) if you define a class for them. Obviously that is not as nice, but if what you are saying is really a problem at your company, just implement a library that does those things and force them to use it.
While I'm not disagreeing with the above, I think it's worth pointing out the enormous sampling bias which the article sort of mentions in passing. I would be surprised if the "force multiplier" had anywhere near as much impact as the implicit selection bias of choosing Haskell in the first place. That being said, phrasing it that way has a nicer political sound in the boardroom. "Force multipliers." I like it. That sounds like just what this team needs! More "Force Multipliers!"
Can someone post a tldr; version? Well, I actually did a quick go through, and realized I need a lot more knowledge to grasp the ideas. 
Not a problem at my current company, but in a previous involvement.
I don't think the first paragraph answers "why should I care?". This is definitely a fun exercise though.
The [haskell-mode README](https://github.com/haskell/haskell-mode/blob/master/README.md) has good instructions for this, and also points to further info if you should need it.
I think mirroring Stackage is what would be sufficient for most people that need a local mirror.
In that case, it was well conveyed. I assumed it meant "in Haskell". I have been wondering for a while why I would like to bother with Church encodings of stuff in Haskell, except for performance reason, or for the sake of the exercise.
&gt; I recommend using the school of Haskell. I think it’s a fantastic resource. How convenient :)
Types do more than name a data representation. Types name the associated computational structure. If you're talking about different computational structure, use a different type, even if its data representation is isomorphic on the set level. (Mutter `Maybe` `Monoid` sassnfrassnrickrastardly.) By all means make `&lt;*&gt;` an optimized implementation of `ap`, exploiting its less sequential data dependency. People are gaining much by doing so, but please make sure they do the same job. The applicative notation should be available for programming in any `Monad` without any nasty semantic surprises.
Ryan Trinkle of Skedge.me gave a talk about a [similar experience](http://cufp.org/2013/ryan-trinkle-skedgeme-enterprise-appointment-sched.html) (same domain, exploiting type classes to flexibly implement different store policies) at last year's CUFP. It is riddled with little gems one of them being the infamous "quick and dirty importer" at ~13:30 where the conclusion is "we could not have done this as badly as we did without the type system".
* You can think of Alan Jeffrey's work as an EDSL in Agda, so he goes ahead and uses the full power of dependent types. * The type systems in my paper and Cave et al's are simply-typed calculi, and so the type system extensions they need are orthogonal to HM(X) (which is about polymorphism). * Right now, I and my coauthors are the only ones looking at linear types for FRP. I don't recommend using the system in my POPL paper -- the one in my ICFP 2013 paper is much better. * However, the linear type system in my ICFP 2011 paper *is* well-suited for GUIs. The reason is that linear types lets you treat GUI widgets "as if" they were purely functional (albeit with a linear type). * You can download an (undocumented) [implementation from my webpage](http://www.cs.bham.ac.uk/~krishnan/adjs-0.1.tgz). This is a compiler written in Ocaml which generates JS code. (You'll also need to use Firefox, since I use ES6 let-bindings in the generated code.) I've been meaning to resume work on the implementation for a while, but I keep having ideas. :) 
Rabid theory forming and design gaols. Can't wait to find out more :-) Looks like a seriously useful toolset. 
Completely awesome and instructive article. We need more of this.
Church encodings (and their arguably more useful cousin, Scott encodings) are, at least for non-recursive types, just case functions that have been pre-applied to values. That makes it easy to define the case functions for them -- they're the identity functions just with different names -- and so program in them is pretty easy, conceptually. data ChurchBool = ChurchBool (forall a. a -&gt; a -&gt; a) false :: ChurchBool false = ChurchBool (\f _ -&gt; f) true :: ChurchBool true = ChurchBool (\_ t -&gt; t) caseCB :: ChurchBool -&gt; a -&gt; a -&gt; a caseCB (ChurchBool f) = f not :: ChurchBool -&gt; ChurchBool not a = caseCB a {- False -&gt; -} true {- True -&gt; -} false and :: ChurchBool -&gt; ChurchBool -&gt; ChurchBool and a b = caseCB a {- False -&gt; -} false {- True -&gt; -} b Unfortunately, the encoding doesn't scale if you don't have impredicativity. :(
It's not just about talent: Consider on boarding someone from a django background (say) to a pyramid project (two python web frameworks). Your team has it's own way of doing things and a bunch of libraries, there's nothing especially in the python ecosystem that makes this particularly discoverable - apart from documentation. The corresponding Haskell project though already has most of the heavy lifting that you'd need: Your persistence layer and your forms layer both have combinators to easily embed them into Snap or whatever. Figuring out the right way to do something is largely examining the types and using the correct combinator. It't pretty hard to screw it up. Of course it also helps that you've got Ollie and Renzo writing the structure in the first place and doing code reviews :-)
This is exactly it. We all know that Haskell is a great language with lots of objective benefits. What I really wanted to communicate here was the business/non-technical case.
From my "I'm not an haskeller" point of view: I would say that for being an haskeller you have to be smart. And smart people are great for teams. But for my own use of haskell I'm not smart enough.
The bad thing about both boxes and hungry snails, as an analogy, is that they fail to capture the immutability. The snail does not really consume inputs, it just uses them to help it do something. Not that I am sure of another good analogy that would work better.
You can simplify the Morte code a little bit like this: -- Let binding for `Bool/True/False/if` ( \(Bool : *) -&gt; \(True : Bool) -&gt; \(False : Bool) -&gt; \(if : Bool -&gt; forall (a : *) -&gt; a -&gt; a -&gt; a) -&gt; ( -- Let binding for `and` ( \(and : Bool -&gt; Bool -&gt; Bool) -- Actual "body" of the program: reduce the function "and true true" -&gt; and True True ) -- and ( \(f : Bool) -&gt; \(g : Bool) -&gt; if f Bool g False ) ) ) -- Bool (forall (a : *) -&gt; a -&gt; a -&gt; a) -- True (\(a : *) -&gt; \(x : a) -&gt; \(y : a) -&gt; x) -- False (\(a : *) -&gt; \(x : a) -&gt; \(y : a) -&gt; y) -- if (\(b : forall (a : *) -&gt; a -&gt; a -&gt; a) -&gt; b) That still reduces down to `True`: ∀(a : *) → a → a → a λ(a : *) → λ(x : a) → λ(y : a) → x 
The [Morte post](http://www.haskellforall.com/2014/09/morte-intermediate-language-for-super.html) he mentions explains the benefits of encoding things in lambda calculus. For Haskell, most of the benefit of Church/Boehm-Berarduci encoding is for recursive types, which greatly improves performance. This is why `attoparsec` church-encodes the parser type, for example.
Yeah, look in Debian-bootstrap.sh. It may be missing one or two packages, but should be mostly complete.
If they're so smart why can't they see that Haskell is not somehow superior?
Right now, no. But that's an interesting idea. I'll have to think about how we could approach that.
Software development is about managing complexity, Haskell developers have realized the compiler and its type system are much better at this than many human beings working together. 
While I'm quite disappointed to see that idiomatic Haskell appears to be slower on average than even Python or Ruby, I can't help but notice that they are using ghc 7.4.1, but recent versions of almost every other language.
&gt; Are comonads that hard to grasp? No. (If you stick the types)
Anyone got a transcription of the poem at the beginning? I'm thinking it would look good on a shirt.
This game was used to present a Declarative Game Programming tutorial at PPDP 14 http://fplab.bitbucket.org/posts/2014-09-23-declarative-game-programm.html
I couldn't find a perma-link, but [her website](http://www.codemiller.com/) has a official transcription on the poem for now.
Awesome, that helps a lot, thanks again!
Sure. They are definitely better than just plain for loop statements. I was just objecting to the notion that they are just as good as individual applications of filter or map in terms of reinventing the wheel.
You can do that with the [FP Complete School of Haskell](https://www.fpcomplete.com/school). Basically, create a tutorial that has the code enclosed like this: ```active haskell main = putStrLn "hello, world" ``` Then publish the tutorial. Anyone with the link can go there and play with your code. It's also possible to publish projects in the IDE. Anyone can follow the link and open a copy of the project.
Codepad supports Haskell: http://codepad.org/
http://www.codemiller.com/blog/2014/08/01/functional-programmer-lingo-in-verse/ Internet-tip: if there is no link that says "permalink", try clicking the datestamp and the heading in that order. One of the two usually work!
Although there is a little bit of intro-to-comonad material, this talk is less about introductions and more about a neat researchy subset that gives you spreadsheet-like computations without costing you too much repetition. I'm also not yet comonad-enlightened, so I think a lot of people got more out of the talk than I did. Despite that, it's a cool talk that extends the coolness-feeling of fibs = [1,1] ++ zipWith (+) fibs (tail fibs) into more dimensions - he has the spreadsheet version of fibs at 0:37:27. And then at 0:38:30 - he does it for pascal's triangle. Neat! Also a cute story about type holes.
I never realised how much weird terminology I use, but that poem at the start really opened up my eyes. Five years ago I wouldn't have understood a word of it.
Also see http://codeworld.info/ . You can write, run and share programs. It has a graphics library and handles user input, so you can even write simple games. And it's open source: https://github.com/google/codeworld (The prelude there is a bit different from standard Haskell, i think you need to `import HaskellPrelude` to get regular Haskell)