...so what *is* the difference between Arrow and Applicative+Category?
I can't seem to make this work with stack repl. Any hints? *Isotope Isotope Isotope.Base Isotope.Parsers&gt; let methyl = "CH3" :: MolecularFormula &lt;interactive&gt;:17:14: Couldn't match type ‘[Char]’ with ‘ElementSymbolMap Int’ Expected type: MolecularFormula Actual type: [Char] In the expression: "CH3" :: MolecularFormula In an equation for ‘methyl’: methyl = "CH3" :: MolecularFormula
Just because you _can_ introduce a feature like that into a language without breaking anything else, it doesn't mean you should. Remember someone suggested something similar when it comes to `do` blocks? I.e. at the moment, we have to write `f $ do ...`, but it would be unambiguous to allow `f do ...` instead. A lot of good arguments came up in that discussion against introducing different parsing rules for different situations that don't need it. And in this case I don't think there are any real advantages to allowing this that come anywhere close to outweighing the confusion it would cause.
Yep, gotta have that Hindley-Milner type inference, or it just isn't any better than Lisp. Although I have been pretty impressed with Scala's type inference and static type checking, it isn't terrible.
I don't agree with everything in this, but using `IO`-specific aliases for `return`, `&gt;&gt;=`, and the rest sounds to be a good idea. Obviously, it's bad style (even more so because noöne actually uses it), but it avoids presenting things as if the `Monad` typeclass were somehow doing the magic. Maybe we should teach in the following order: * Sums, products, functions, pattern matching (probably with use of numbers and strings as values) * `data` declarations, `Maybe`, parametric polymorphism * `List`, recursion, laziness * `Functor` * `Applicative` * `IO` (no `do` notation or mention of `Monad`) * `Monad`, `do` notation (starting with `Maybe`, because it's both intuitive and useful) * `Either` and `(,)` FAM instances * Numbers, `Monoid` I guess it's up to me to write that now.
Why are records preferable to dicts?
Extremely minor point: The big problem with Haskell's Arrow class is that it requires "arr", meaning that any Arrow needs to support a full blown Haskell function. So I think I would rewrite "first" in terms of fmap. Otherwise this is great!
Good that some one pointed this out. I had the hardest time understanding IO until someone had me view things in actions. Once that happened, then other things like Async and STM all clicked.
It has *some* vim support but not *good* vim support—not as good as what's available for Sublime, anyway. Normal mode commands beyond write and quit are not supported, for example.
My experience with reference counting is that it turns any perfectly good read into a write to the same memory, destroying caches and parallelism by getting false conflicts in the process. CPython doesn't have any form of parallelism to lose, so it only suffers the memory bandwidth issue from scribbling all over memory.
But worst practices should be hard, not the default ;)
&gt; rewrite "first" in terms of fmap How can you do that? I wasn't able to see it. [EDIT: In fact I don't think you can do it because it would imply that `arr` was sufficient to implement `first`]
Argh don't ever do that: Future versions of the compiler might introduce warnings about your code that are harmless, that's why they're only warnings. You just broke someone's build. Make *that specific thing* an error. Which is a thing GHC can't do? At least I can't glean it from the docs. There should be `-ferror-missing-fields`. `-f[no-warn|warn|error]-foo` for everything, much simpler.
Yes. This should be an error. If you want that behaviour you should explicitly have to assign `undefined`.
This advice is good for libraries (which is why hackage won't let you upload packages with -Werror), but doesn't apply to end-user applications. I think -Werror is a perfectly reasonable practice for production apps. In fact, just the other day I encountered a production bug that it would have prevented.
Very cool! I've long wanted to see more libraries for working with chemical equations, and this looks very useful.
I guess GHC could make it an error, but I'm no expert.
What representation does Python use for literals? Haskell uses Rational which means the no precision is lost from the literal the user wrote. (String would be another acceptable type, Double is certainly not acceptable.)
Yeah I thought along the same lines at first. Here's a full example: We have two data types: data Triple = One | Two | Three data Bool = True | False now, a function with type signature `Triple -&gt; Bool` theoretically should have 2 ^ 3 = 8 definitions, not 2 x 3 = 6. Here they are f0, f1, f2, f3, f4, f5, f6, f7 :: Triple -&gt; Bool f0 _ = False f1 One = False f1 Two = False f1 _ = True f2 One = False f2 Two = True f2 _ = False f3 One = False f3 _ = True f4 One = True f4 _ = False f5 One = True f5 Two = False f5 _ = True f6 One = True f6 Two = True f6 _ = False f7 _ = True All 8 functions listed above are distinct from each other. You can easily test that by following code snippet: [map f x | f &lt;- [f0, f1, f2, f3, f4, f5, f6, f7], x &lt;- [[One, Two, Three]]] What you really need to think about is not how many patterns you can match in a single function. It's about how many different trivial functions you can define. Hope this clears things more. 
That makes more sense. You're saying that the growth of input, output mappings grows exponentially. 
Yo! You phrased it better than anything I've come across.
Hmm I didn't struggle for my everyday usage with it, but I admittedly haven't spent a decent amount of time on it. ST3's is excellent, yes.
#
Yes, I did understand that. I just think if you're trying to talk about the strengths of a language and how it promotes good programming practices by making them easier, you're not exactly making a compelling argument with "Feature A that numerous other mainstream programming language do much better sucks less in our language than feature B that a different set of numerous other mainstream programming languages do much better". :-)
I almost never use record syntax; I find it cumbersome and verbose.
Overloading's been around forever, but type classes give a lot more than the sort of overloading you get from interfaces/mixins/etc. One of the most powerful things is that, e.g., addition guarantees that both arguments and the result all have the same type. This typing discipline is crucial for why type classes are such a killer feature, but it's not something standard OO languages support. There are two ways I know of to get this type discipline from OO languages. The first is with multimethods, which would allow you to get both the type discipline and the implicit overloading. Alas, exceptionally few languages implement multimethods. The second approach is to have an OO-class for the type class instances/dictionaries, and to have addition be a method of *that* class à la `instanceNumDouble.add(x, y)` (as opposed to having addition be a method on the class of one of the arguments themselves, à la `x.add(y)` as seen in every OO language ever). Alas, this approach means you need to explicitly thread the dictionaries through your program which is gross and bug-prone. Even if some languages offer facilities for implicitly threading those dictionaries around (Scala?), most languages don't. The fact that `(+) :: Num a =&gt; a -&gt; a -&gt; a` isn't the only interesting type discipline you get from type classes. Type classes also allow you to have things like overloading on the result argument `return :: Monad m =&gt; forall a. a -&gt; m a` which again few OO languages can do without jumping through non-idiomatic hoops like reifying the type class instances as values of the OO-class for the type class itself, and then threading everything around manually.
Is there a TL:DR somewhere? And, without having seen the talk, how does this relate to Liquid Haskell?
Many modern FP languages these days have borrowed type classes from Haskell, in some form or other. "Type classes" and "canonical structures" in Coq, "modular implicits" in OCaml, "instance arguments" in Agda, "type classes" in Idris,...
I think you mean `Map a b`, not `Set (a, b)`, otherwise you restrict yourself to one-to-one functions. But the difference is: none, really. You can implement a function as a map: functionOfMap :: Map a b -&gt; (a -&gt; Maybe b) functionOfMap = Map.lookup (Sometimes we do this for caching/memoisation.) And you can implement a map (rather inefficiently) as a function: type Map k v = k -&gt; Maybe v empty :: Map k v empty _ = Nothing insert :: Eq k =&gt; (k, v) -&gt; Map k v -&gt; Map k v insert (k, v) m = \ k' -&gt; if k == k' then Just v else m k' delete :: Eq k =&gt; k -&gt; Map k v -&gt; Map k v delete k m = \ k' -&gt; if k == k' then Nothing else m k' fromList :: Eq k =&gt; [(k, v)] -&gt; Map k v fromList = foldr insert empty lookup :: Eq k =&gt; k -&gt; Map k v -&gt; Maybe v lookup k m = m k For `Bounded` and `Enum` types we can count the number of inhabitants: {-# LANGUAGE ScopedTypeVariables #-} count :: forall a. (Enum a, Bounded a) =&gt; a -&gt; Int count _ = fromEnum (maxBound :: a) - fromEnum (minBound :: a) + 1 count (undefined :: Bool) == 2 count (undefined :: Weekday) == 7 And then we can compute the number of possible inhabitants of a function `Weekday -&gt; Bool`: -- The number of functions (maps)… length -- …that pattern-match on Weekday (have Weekday as a key)… $ replicateM (count (undefined :: Weekday)) -- …and return a Bool for each one (have Bool as a value). [False ..] == 128 
You can find an abstract for the talk here: http://www.composeconference.org/2016/program/
Of course not for production builds, but for your own development and testing builds? Why not?
I don't care how you do it, but I want to reserve the right to run a program that spews out warnings while developing. I have separate writing and cleanup phases, for the simple reason that I don't want to clean up what might end up getting deleted or even just heavily modified. When I do cleanup then, though, I also run hlint, document, etc. We even got deferred type errors to do that even more aggressively though that only makes sense when you're doing big refactors: Change data type and the first two functions using it, compile with deferred errors and test the first two functions. Rinse and repeat for the other two thousand. Avoids having to build scaffolding to do things incrementally.
afaik it defaults to int or float depending on it containing a decimal point with the ability to force a type. I could be wrong on that one it never made a significant difference for what I did with it so I never had to look deeper into it.
TL;DR (I'm the speaker): The paper "There and Back Again" by Danvy and Goldberg describes a clever trick for computing a family of list processing functions (including palindrome testing, symbolic convolution, and generating catalan numbers) using only a single pass over the input list, rather than the naive two. This can lead to substantial gains in runtime performance. However, it is tricky to prove formally that these algorithms are bug-free: they contain many partial pattern matches whose correctness depends on subtle arithmetic invariants. I use GHC's type-checker-plugins infrastructure and Iavor Diatchki's type-nat solver (backed by the SMT solver Z3) to recast "There and Back Again" in terms of length-indexed lists, and to prove that everything works out in the end. Also, some digressions about comparing different approaches to dependently-typed-ish programming in GHC, the impossibility of proof erasure in the presence of bottom, and the superiority of ASCII to powerpoint. :) Liquid Haskell is similar in many ways to this work (and it is far more mature!). They differ mainly in how they interact with the compiler. Whereas Liquid Haskell is an extrinsic layer atop the existing GHC type system, type-checker plugins integrate into the constraint solving loop within GHC. Practically, this means that writing Liquid Haskell code usually entails writing with simpler *Haskell* types and fancier *liquid* types on top of them—a kind of extrinsic verification style. By contrast, type-checker plugins work with the same constraint system that pervades the rest of your program, which means that they can allow *more* correct programs to be type-checked, rather than only preventing more incorrect ones from being accepted. Liquid Haskell, at the moment, is much more expressive than the existing state of the type checker plugins ecosystem—in particular, we're still working on properly integrating type families into the solver plugin story, whereas Liquid Haskell's *measures* feature can address many use-cases where we would use a type family in type-checker-plugins world. For more info about Liquid Haskell, check out Niki Vazou's talk at Compose! (I don't think the video is posted yet, but presumably it will be at some point.)
On monads and `do`-notation, let me put it this way: yes, they are different ways of representing the same thing, but I think `do`-notation captures to a huge extent why monads are at all useful, such that it makes sense to introduce the notation first and explain how it works later. A Haskell with the Monad typeclass but without `do` notation would not have "solved I/O" to anywhere near the same extent. I think `&gt;&gt;` *is* extra magic when applied to `IO`, because when `&gt;&gt;` is used with other monads, its semantics can be cashed-out within the language (one can roll one's own `instance` declaration). So we have this unfortunate situation where magic is stored in the interface `&gt;&gt;` -- which is sometimes magical and sometimes not -- rather than properly cordoned-off within the `IO` type.
With a generic type, you explicitly say that you're not using anything specific to your data. If your function requires only a Functor, then you know very well what's the only thing you can do with it.
Anybody who has touched a large Python code base knows that no matter how bad Haskell's records are, they are still way better than large undocumented dictionaries littered everywhere.
That may well be true, though I notice you sneaked "undocumented" in there. Still, Haskell's records are worse than most mainstream *statically* typed languages I can think of. Even humble C structures can have fields with the same name in different types. As soon as you're up to the level of C++ you can ensure safe initialisation as well. Of course, it's not as if Haskell is the only language with baggage when it comes to resolving names that are reused in different contexts. OCaml is also less than ideal when it comes to record types with common field names in a similar situation, and while C does better, it has its own troubles with namespacing of enumerated types.
I think abstraction is more important for libraries than for forward fronting apps. You just use the abstractions indirectly instead of directly. This is just a guess since I have only used Haskell for toy projects as well.
In my experience generic types lead to less errors at the definition site because they constrain the program more precisely. Since the use site is also usually generic too they are win all around. Also, parametric polymorphism lets you express how different parts of the type are related (because they share the same type variable) in a way that avoiding polymorphism does not. My experience of people using specific types is that their types might be specific in the sense of not being reusable but they still tend to be too general in what they allow. 
I too am curious.
I think the point is that given a long enough timeline, using a language which enforces best practices (path of least resistance, we're all "lazy" after all) results in much higher long term productivity. You have some upfront cost in the form of time, but you don't need to hire an enforcer to oversee everyone doing best practices, they are already there by default. While language X appears to have benefits which bring 40% more productivity over Y, the ultimate penalty is all the technical debt incurred by using the worst practices the language has to offer to quickly achieve project goals (again, path of least resistance). Refactoring is a chore due to the worst practices used, and if deemed an impossible task usually results with, "Let's rewrite it and release a v2, we'll do it better next time." You've now traded the front-loaded best practices time of Y for the back-loaded worst practices technical debt of X. I suppose you could hire the enforcer to make sure this never happens, but can your company even budget for that extra person?
https://wiki.haskell.org/Hask
Do you know ahead of time which types you want to store? If so, you can use a `StateT Int (StateT String m) a`. For example, if `Int` and `String` are the two types of state that you want, then this example would read like foo :: (MonadState Int m, MonadState String m) =&gt; m (Int, String) foo = do put 6 put "i'm a string" modify (+1) x &lt;- get y &lt;- get return (x, y) You can pass a `StateT Int (StateT String m) a` to `foo`, since ~~StateT Int (StateT String m) a has both MonadState Int and MonadState String instances~~. The above example works because `get :: MonadState s m =&gt; m s`, `put :: MonadState s m =&gt; s -&gt; m ()` and `modify :: MonadState s m -&gt; (s -&gt; s) -&gt; m ()`, EDIT: What I wrote is apparently not possible, please disregard my answer. Thanks /u/stonegrizzly for calling this out.
Every dictionary (or hash in Ruby) is undocumented; even if there's a nice looking docstring, you'd *best* assume that there's a half-dozen unrelated keys that someone has shoveled in as the path of least resistance to implementing the latest hack.
It is also possible to directly index with the expected type. Check MultiState package: https://hackage.haskell.org/package/multistate-0.7.0.0 Or my implementation which is a little bit different: https://github.com/hsyl20/ViperVM/blob/master/src/lib/ViperVM/Utils/MultiState.hs Regarding the constraints, in both cases the user has to specify the type of the state (it cannot be inferred).
As opposed to Haskell records, where even if there's a nice looking type defined somewhere, you'd *best* assume that there's a half-dozen missing fields that someone omitted when they built that instance as the path of least resistance to implementing the latest hack? I'm being a little facetious, of course, but laziness and partiality mean records don't always live up to the ideals of safety and compile-time error detection we would usually hope for in Haskell.
&gt; I do think Hask is a semicategory though. Few good things involve this subreddit linking to nCat.
Everything is a matter of degree!
Interesting. I think this is also possible in ATS using a function template: // generic PI: extern fun{a:t@ype} gpi (): a implement{a} gpi () = acos&lt;a&gt; (gnumber_int&lt;a&gt; (~1)) // usage: val pi_f = gpi&lt;float&gt; () val pi_d = gpi&lt;double&gt; () 
[pshaw](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/fast+loose.pdf)
There's graphics-drawingcombinators, used by Lamdu (cc /u/Peaker) http://hackage.haskell.org/package/graphics-drawingcombinators 
That paper is the perfect combination of being well-written, having an elegant thesis and having a catchy title. I wish more CS papers were like that. (Hell, I wish I could imagine writing something like that myself...)
#
#
In combination with GLFW-b it can be quite easy, yeah.
I seem to have been downvoted. I mean I don't use the record syntax and just use ordinary value constructors.
This is perfect. I'm actualy trying to implement an indexed Monad Tardis (State + Reverse State) to solve certain undeterministic problems that rely on sequencing.
Interesting. Is there a negation of this constraint? Something like "Total" (always terminates), or "NonPartial" (may not terminate, but never crashes)?
Yes, `seq` is a design mistake. It makes essentially all optimizations semantically incorrect, which is sufficiently disastrous that most Haskell compilers ignore its effects on the semantics. If you want something like `seq`, the right thing is to work with the adjunction between the category of domains and continuous functions, and the category of domains and strict continuous functions, with a syntax like that of the [Benton-Wadler LNL calculus](http://homepages.inf.ed.ac.uk/wadler/papers/linearmonad/linearmonad.ps). 
I did implement exactly that (zoomable mandelbrot) in a very short program with [gloss-raster](https://hackage.haskell.org/package/gloss-raster). In fact, you can find code for this in [gloss-example](https://hackage.haskell.org/package/gloss-examples). Though neither me nor the example implemented a zoom by lasso (but that shouldn't be too hard either).
Ahh, but the whole point is that people will be assigning `undefined` anyway since it's easier :) Maybe you won't and I won't, but the majority probably will. Thus making missing fields an error has a chance of making the average quality of code decrease. I'm not saying it necessarily *would* – after all, maybe I'm wrong about what the majority would do – but it's still not a clear-cut situation.
Interesting question. With my background being mass spectrometry, I was going to provide a data type for ions. In that respect, I see this library being extended beyond just isotope masses. I think your suggestion is quite reasonable. As you mention, extra fields could be added to the `Isotope` type. The actual data could then be added into the map, `elements`. Perhaps additional functions for neutron scattering cross-sections could be added to a separate module.
Does anyone have some improvement ideas? IDK animations or ascii art or page numbers?
You might like a lightweight lens library to get getters and setters for free
PFPL is a fantastic book, but it is not an introduction to type theory, nor even about type theory! It's a book that uses type theory in order to give a definitive, unifying account of programming languages, in all their partial and effectful glory. One recommendation I have when reading PFPL is to skip over whatever parts are confusing to you, and come back to them later. It's such a wonderful book! But for many people, it will not be possible or desirable to read it like a novel, and this is OK.
&gt; My understanding was that the report allows compilers to transform expressions so long as they do not become less defined. But the compiler is allowed to transform a bottom expression into a non-bottom expression? So bottom is Haskell's equivalent of C's "undefined behaviour"? No wonder `undefined` is named that way :)
This is brilliant!
I didn't mean to imply that PFPL was limited to type theory — merely that it seems to compliment learning type theory. And if you want to learn about type theory, I'd start with some more appropriate introduction material. Because sometimes it feels like PFPL just dropped five new concepts in that 50 word sentence you just read, so I find it an, at times, very demanding book to read.
Ah now I see what you were getting at. Haha, so funny.
Can't wait to dive in to this.
Thanks. Until now, all related with distributed computing was in flow, since the primitives looked ad-hoc to me. but now everything is more solid, so I can write documentation with confidence.
&gt; Records in Haskell are still unpleasant to use Record update syntax is horrendous. Namespacing is a little awkward. Outside of those, I would say that records in Haskell are actually fairly pleasant to use - and it nets out to about "eh". The addition of RecordWildCards addresses some of the pain of both those points, and makes some other things extra easy, and the result nets out (for me) pretty strongly positive. I do hope OverloadedRecordFields makes things even better.
Maybe add a construct to allow `Foo { bar = 7, baz = 9, omitting qux }` as shorthand for `qux = error "Missing field in record construction x"`?
Simple, compile with -Werror
&gt; Why? teleport is safe as soon as the wormhole code running in both nodes is the same. Since I can add as many teleport call in my function / subfunctions calls, I thought it might become hard to keep track of what part of the code will be executed on which side. I'm just afraid of implicit state machines :) &gt; getSData can not produce runtime erros Again, I was more afraid about some human made errors, since types can no longer ensure things are properly available. I know there is no perfect tradeoff, and this is probably better than overly complex type level state indexing. Especially since we can still properly sandbox core computation in whatever pure / custom monad we want. I was just raising first thought concern. 
This tool is great for developing on a VPS. &gt; Since version 0.4 ghcid has supported a --test flag to pass a test command (e.g. :main) which is run whenever the code is warning free. Seems like a cool feature. But why is it only enabled when the code is warning-free, instead of merely error-free?
The problem with having multiple fields with the same name is that in C, "struct selectors" are not first class values, so you cannot just pass a "selector" to another function. In Haskell, "record selectors" are functions by themselves, which means that at the point where they are used, it is not always clear how to disambiguate them if you allow multiple record selectors to have the same name (so you need polymorphic record selectors, which is a harder problem)
`getSData` is a rather concerning approach. If make a mistake in my use of state, my whole thread just terminates. Also, not having to declare what type of state you use in the type signature means that anything can call that code without actually knowing about and preparing state, and they just get terminated. In fact, early termination in general is a somewhat concerning approach. If anything screws up, the thread can just terminate and it's probably going to be hard to figure out why.
There is not.
Just exhaustiveness, there's no checking of nontermination. Exceptions are checked in some sense, as you have to communicate the possibility of exceptions in the type, given that the type of `throwException` is `forall eff a. Eff (err :: EXCEPTION | eff) a`, but there is an `unsafePerformEff`, so it is possible for e.g. a function of type `Unit -&gt; Unit` to throw exceptions. Also, the type system doesn't try to capture things like OOM or stack overflows (which are unfortunately common in JS land) at all.
Thanks for linking to all the papers. Those are some really interesting resources. I'm pretty interested in database theory, and I've made several attempts to implement an in-memory relational datastore. I've realized that I really do need to familiarize myself with more existing work though so that I'm not just following hunches. What books would you recommend for someone getting into this field?
Fair point.
I guess it would be possible to combine this with acid-state somehow?
No. The programs don´t have to be the same but the Cloud calls like "local", wormhole etc must be the same and in the same order.
&gt; Just exhaustiveness, there's no checking of nontermination. In that case, I like the mechanism but weakly disapprove of the naming :p &gt; there is an unsafePerformEff Sure, but I'm pretty okay reasoning up to that, assuming use is measured.
Why not? I use it with TCache that is not very different to acid-state, in some way, to cache data in Transient.MapReduce
Have you tried to see if the OpenCL packages are compatible with the [accelerate-2.0](https://github.com/AccelerateHS/accelerate) versions that aren't on hackage (yet)?
Might I suggest: import Safe (headMay) returnFirstRed :: G RGB -&gt; Maybe Int returnFirstRed (G xs) = red &lt;$&gt; (headMay =&lt;&lt; headMay xs) We should be showing noobies the safe, total functions early on as an example of how to correct the deficiencies of `head`.
[Ether](https://hackage.haskell.org/package/ether) has a really nifty way of solving this. And it's not bound to just the state monad. Essentially, Ether is trying to solve the problem of having the same sort of monad transformer multiple times in the same stack. That is, there should be a way to use this: add :: (Monoid s, Monad m) =&gt; StateT s (StateT s m) s add = mappend &lt;$&gt; get &lt;*&gt; get The problem there is twofold. One, you can't use `get` to get the different levels of state. Two, I wasn't able to use `MonadState`, because `(MonadState s m, MonadState s m) =&gt;` is nonsense. So the question is, how do I access the same transformer on two levels? The solution is a key representing which instance of the transformer you're trying to access. data Foo foo = Proxy :: Proxy Foo data Bar bar = Proxy :: Proxy Bar add :: ( Monoid s , MonadState Foo s m , MonadState Bar s m ) =&gt; m s add = mappend &lt;$&gt; get foo &lt;*&gt; get bar [There's a more detailed explanation on Ether's GitHub](https://int-index.github.io/ether/). I think the approach is pretty good. For your particular problem, this makes your usage of keys type safe. In fact, now that I think about it, the empty data declarations are completely unnecessary, because you can just use type level strings. (do let strKey = Proxy :: Proxy "string_key" let intKey = Proxy :: Proxy "int_key" put intKey 6 put strKey "i'm a string" modify intKey (+1) x &lt;- get intKey y &lt;- get strKey return (x,y)) :: ( MonadState "int_key" Int m , MonadState "sting_key" String m ) =&gt; m (Int, String) So you're declaring at the type level which keys you're using, thus requiring anyone using your function to also make those keys available.
That is my understanding.
Yes, but sometimes you know that for example, the result of an algorithm is nonempty. Stupid example, I want the first odd number after n: head (filter isPair [n .. ]) This function is total, but to express this kind of stuff you need dependent types and prove the proposition. There are limits on what is expressible with Haskell's types, and that's ok. But I agree that partial functions in the Prelude are not the best thing ever.
&gt; But the compiler is allowed to transform a bottom expression into a non-bottom expression? Yes. At least, that's my understanding.
Swift protocols get close with the `Self` type. But it isn't higher kinded so you can't do anything like Functor. And for some reason there is no protocol for arithmetic operations.
Thank you so much for your help! I ended up using your function and some similar ones and I got it! 
1) Yesod is fine for the task. I would still say Yesod is geared more towards "traditional" site, but being full-featured, Yesod can easily handle your needs. 2) I personally find the Yesod book a good high-level overview of the framework, but despite this, sadly, the state of the Haskell ecosystem is that to do anything non-trivial you will need to read the source code of the framework/library. It's easy, just click the source button on the right of the Haddock documentation. I'm not aware of any other tutorials or samples, but I think reading the source helps a lot, and the nature of Haskell means that it's often quite easy to understand the source, given that most functions are short and has no implicit state. 3) I personally use a combination of hasktags and ag. Hasktags usually works well for jump of definition, but occasionally, it gets it wrong, so I search for the identifier. (Protip: also include `::` in your search, because almost all functions have type signatures. This make narrow down your search to pretty much just the definition itself.) 4) I use spacemacs. Transitioning from vim to spacemacs should be fairly easy. Then include the haskell configuration layer and install a few tools like hasktags, ghc-mod, etc. It works fairly well for me. Hope that helps.
Yes, that's the issue. Many laptops for example and tablets don't have NVidia GPUs.
Thank you I will try to use Haddock for reading source-code, I haven't considered doing that for Yesod (although I've done it for more generic haskell things) Can you use hasktags to dive into the code that's installed via stack outside of your current workspace/project?
Servant is very good for REST services, if you haven't heard of it.
consider doing something like: ``` (:) &lt;$&gt; identifier &lt;*&gt; (many (oneOf "-._") *&gt; identifier) ``` This isn't _exactly_ the solution but it eliminates your ambiguity. You should be able to reformulate it to produce the correct identifier. But basically just look at your problem as trying to match strings of the form `identifier(-identifier)*`
The idea is to convert to a nonempty as soon as possible and then perform your operations on that. Of course overkill for that problem, but eh.
Well concretely I have multiple RequestTypes: data Request = RequestType1 {...} | RequestType2 {...} | RequestType3 {...} And I'm using Data.Binary to de/serialize. So what I tried was to assign multiple values to the `get` function: instance Binary Request where put e@RequestType1{} = do ... put e@RequestType2{} = do ... get = do ... :: RequestType1 get = do ... :: RequestType1 Based on some business logic, I tried to serialize the request for my needs: case apiKey of 1 -&gt; get :: RequestType1 2 -&gt; get :: RequestType2 From what I've learnt here, this is a wrong approach as I assigning different values to the same name doesnt make sense obviously. However, my guess was that Haskell will do some sort of overloading when I specify the return type within the type class.
If you do manage to make it work, please do share.
Flag sounds good. I think it would be great to run the tests when there are warnings. Then, in addition to the warnings, you'd get feedback on whether the code works. In particular, it would be nice to run those tests that are correctly typed, even while the rest of the code base has type errors. I don't think this would significantly delay my fixing my warnings, though I'll admit I haven't tried ghcid.
Firstly: this comment isn't the answer to your problem, since it's a little off-topic / for later. It's good to come up to speed with how Parsec works, and you should probably listen to what other people say in this thread. With that out of the way, I think that once you have an OK understanding of Parsec, it's worth spending some time having a play around with `Text.Parsec.Token` and `Text.Parsec.Expr`. When they fit your problem (and once you have the fundamentals down) they can save you lots of time and frustration.
is like saying java is a crippled C++. Swift, Scala (especially Scala), are languages on their own, with their good and bad side. They have their own ecosystem, their own tooling and they are very good at what they were designed for. Ocaml doesn't have what you need in 2016 if you want to get the job done fast at least. There is a reason why there are so many ML dialects, some of them quite successful and mature (fsharp), and there is a reason why ocaml is used in academic environments mostly and not real life (paid for) projects. 
Want to increase bicycle adoption and make it more comfortable to park your bike. Want to work on Computer Vision in Haskell, do web-development with GHCJS or work with the Nix-stack of tools? We're hiring! Send me an email at: bas@lumiguide.nl.
Thank you! It is what I need. My fault, I totally forget about "SPC ?" help in spacemacs.
Often the author shows up here in these kinds of cases.
Nice! FYI, it's quite common in Agda to [layout the telescope of a constructor](http://liamoc.net/posts/2014-01-01-context-split.html#6891) so that typing rules look like typing rules. It does not benefit from the built-in support that Epigram had for 2D syntax for but once it's syntax-highlighted, it looks pretty good.
I've been trying to get myself better at ATS recently, but have been really struggling. Any resources you can think of? I've got a huge amount of experience in C and lower level languages, but ATS is really big and the syntax is dense, so I'm hoping to break it piece by piece so I can digest things like dependent types, types in general, etc. I've studied under professor Xi a little bit, but even with him I struggled to understand what was going on in ATS.
That is the idea.
I actually went for: C-h a and looked for "previous". Your method is *much* faster. Today we both learned something ;-)
A set is a category where the identity is the monomorphic identity function and arrows are endomorphic functions between objects in the set. A category is a generalization of a set, and a functor is a generalization of a function. I think that you get what you want with the identity functor: `(a -&gt; b) -&gt; Id a -&gt; Id b`
[Architecture of a Database System](http://db.cs.berkeley.edu/papers/fntdb07-architecture.pdf) explains a very traditional System R -style database. The [Red Book](http://www.redbook.io/) covers a bunch of different designs and tradeoffs. [CMU 15-721](http://15721.courses.cs.cmu.edu/spring2016/schedule.html) is a really good overview of the current state of research on single-machine in-memory systems - it has a *really* good choice of papers.
I think you have to restrict yourself to finite lists of booleans to have `[Bool]` isomorphic to `[()]`. Otherwise, `[Bool]` is a superset of `InfiniteStream Bool`, which is uncountable.
There is notFollowedBy in Text.Parsec which seems to be exactly what you want. Then it should become: many1 $ (alphaNum &lt;|&gt; oneOf "-._") `notFollowedBy ` (string "-&gt;") 
Will transient be making its way to stackage?
It's interesting to note that your functions are actually index :: Tree a -&gt; ([Bool] -&gt; a) tabulate :: ([Bool] -&gt; a) -&gt; Tree a If we import [`Data.Distributive`](https://hackage.haskell.org/package/distributive/docs/Data-Distributive.html) and [`Data.Functor.Rep`](https://hackage.haskell.org/package/adjunctions/docs/Data-Functor-Rep.html): data Tree a = Branch a (Tree a) (Tree a) deriving Functor -- Default declaration, required by ‘Representable’ -- Note that we get `distributeRep` for FREE, check Data.Functor.Rep for -- more functions you get when you have a Representable instance. instance Distributive Tree where distribute :: Functor f =&gt; f (Tree a) -&gt; Tree (f a) distribute = distributeRep instance Representable Tree where type Rep Tree = [Bool] index :: Tree a -&gt; ([Bool] -&gt; a) index (Branch x _ _) [] = x index (Branch _ l _) (False:bs) = index l bs index (Branch _ _ r) (True :bs) = index r bs tabulate :: ([Bool] -&gt; a) -&gt; Tree a tabulate f = Branch (f []) (tabulate fl) (tabulate fr) where fl = f . (False:) fr = f . (True :) We can also define indexed versions of `Functor`, `Foldable` and `Traversable` from [`Control.Lens.Indexed`](https://hackage.haskell.org/package/lens/docs/Control-Lens-Indexed.html): instance FunctorWithIndex [Bool] Tree where imap :: ([Bool] -&gt; a -&gt; b) -&gt; (Tree a -&gt; Tree b) imap = ... but you can actually get all of this almost for free when you realize that `Tree a` is simply `Cofree Pair a` data Pair a = MkPair a a deriving (Functor, Foldable, Traversable, ...) where type Tree = Cofree Pair branch :: a -&gt; Tree a -&gt; Tree a -&gt; Tree a branch x left right = x :&lt; MkPair left right etc etc. `Pair` must be an instance of `Representable` as well as `FunctorWithIndex` and co but they are easy to define (let's use a more telling data type than `Bool`): data DIR = LEFT | RIGHT instance Representable Pair where type Rep Pair = DIR index :: Pair a -&gt; (DIR -&gt; a) index (MkPair left right) LEFT = left index (MkPair left right) RIGHT = right tabulate :: (DIR -&gt; a) -&gt; Pair a tabulate tab = MkPair (tab LEFT) (tab RIGHT) instance FunctorWithIndex DIR Pair where imap :: (DIR -&gt; a -&gt; b) -&gt; (Pair a -&gt; Pair b) imap ixf (MkPair left right) = MkPair (ixf LEFT left) (ixf RIGHT right) instance FoldableWithIndex DIR Pair where ifoldMap :: Monoid m =&gt; (DIR -&gt; a -&gt; m) -&gt; (Pair a -&gt; m) ifoldMap ixf (MkPair left right) = ixf LEFT left &lt;&gt; ixf RIGHT right Here's a spoiler, if you use `Free Pair` you get an entirely different tree! -- Tree2 = Free Pair data Tree2 a = Leaf a | Branch (Tree2 a) (Tree2 a) where you get instances for free again. **tl;dr** The further you dig into Haskell the more free stuff you get
Right, but how often do you ever want to do things things like that *when `recordField` isn't predetermined*? That is, how often do you write code of a form such as someFunction fieldName = let someRecord = getSomeInterestingData in someOtherFunction $ fieldName someRecord where the variability of `fieldName` actually matters?
It's also a representation of all real numbers, because bijections between R and any interval are pretty easy.
It's also a representation of all real numbers, because bijections between R and any interval are pretty easy.
That was remarkably pleasant to say out loud.
Very good. I know you have been working on transient for a while to get it ready for prime time. Hopefully this will be that release.
&gt; it's fun to realize that an infinite binary stream is one representation of the real numbers within a fixed range like [0, 1] But it's important (or, at least, also fun!) to realize that not all real numbers have a program that can generate them.
The *simple C version* of field accessors doesn't enable the usage patterns that I mentioned.
The idiomatic C version of your `&lt;$&gt;` example would be something like getRecord(something).recordField The idiomatic C version of your `filter` example is a little longer given the imperative style but might be something like for (int i = 0; i &lt;= numRecords; ++i) { if (isEven(records[i].recordField)) { doWhatever(records[i]); } } In either case, it's still just a straightforward access of a predetermined field in a record. There is nothing superior about having the access be modelled as a function in these cases other than a degree of uniformity and convenient syntax.
There is no command to expose such an executable (but there probably should be!) Adding it to your PATH or symlinking the executable seems fine to me. Note that obviously if you build a new version you will have to update the symlink/path; I prefer symlinks because I can update it once and then all my open terminals see the update. Ultimately there probably will be a command to expose it, because any such executables would have to be considered "roots" in any hypothetical GC mechanism for the Nix store.
Is this because the cardinality of `[Bool]` is 2^N where N is the cardinality of the naturals (and this would be the cardinality of the reals)?
I assume so. It seems the lectures were recorded each year since 2008. The old videos can be retrieved by changing the year in the URL: https://www.cs.uoregon.edu/research/summerschool/summer08/curriculum.html Here are the links with videos for quick access: [2008](https://www.cs.uoregon.edu/research/summerschool/summer08/curriculum.html) [2009](https://www.cs.uoregon.edu/research/summerschool/summer09/curriculum.html) [2010](https://www.cs.uoregon.edu/research/summerschool/summer10/curriculum.html) [2011](https://www.cs.uoregon.edu/research/summerschool/summer11/curriculum.html) [2012](https://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html) [2013](https://www.cs.uoregon.edu/research/summerschool/summer13/curriculum.html) [2014](https://www.cs.uoregon.edu/research/summerschool/summer14/curriculum.html) [2015](https://www.cs.uoregon.edu/research/summerschool/summer15/curriculum.html) Unfortunately, the audio and image quality is not always good.
Hi, I'm the author of the post. I'm happy to take any feedback on the guide, or answer questions.
But not computatbly countable.
Just curious, is it necessary to manually add a counter for every single route? Is there a simpler way to maintain a route =&gt; counter mapping, which gets updated from a single place in the codebase?
What I don't get though is... are you funded by NVidia? There is a perfectly good and more importantly open cross-vendor standard "OpenCL" actively supported by everyone *except* NVidia. And yet it seems that only because NVidia actively sabotages OpenCL we can't have nice things because even package authors side with NVidia? :-( Also, how is depending on the rather fast moving &amp; breaking LLVM API a better choice than using the stable OpenCL API? 
I quite like the way your GitHub README reads like an academic paper, with its related work, future directions, and references.
Hope you will still be hiring in 6 to 12 months, when I am ready to apply! Do you have plans on opening Thea (your OpenCV 3.1 bindings library shown in the slides), or is it somewhere already ready to be used?
honestly my first though (without reading the mailing list) was: **Erik will be happy** Well after reading it I think *happy* is not the right term ... maybe *pleased*?
So, hold on. EKG looks nice - but is it only for devel mode? It would be great to be able to monitor things during production, and make the monitoring data available to your external monitoring environment. The JSON endpoint looks very useful for that. Obviously you would have to be careful about the cost of the instrumentation itself within the app - but is the overhead introduced by EKG itself too much to be useful in production? Also - `EKG.forkServerWith ... "localhost" 8000` looks unuseful for production. Can the EKG output server be launched separately somewhere within your devops framework, e.g. as a keter bundle, kubernetes, or whatever, and have the app send data to it?
I think he enjoys a bit of trolling from time to time ...
Is it really so buggy that this is needed? Hugs has worked fine every time I've used it
Sorry for the late reply. Actually, it turns out that hmatrix vectors are *exactly* reexported storable vectors: Prelude Numeric.LinearAlgebra.Data&gt; :i Vector type role Vector phantom data Vector a = Data.Vector.Storable.Vector {-# UNPACK #-} !Int {-# UNPACK #-} !(GHC.ForeignPtr.ForeignPtr a) -- Defined in ‘Data.Vector.Storable’ Matrices can be converted into storable vectors by the zero-copy *flatten* function: flatten :: Matrix t -&gt; Vector t At least, it's zero-copy if the matrix is row-major and not a slice.
Minor nitpick: The definition should be f(x+d) = f(x) + d * f'(x)
EKG is designed to be used in production.
You probably mean libraries, not modules. Some of the libraries I use are containers, unordered-containers, base, pipes, and streaming.
Yes and no, I did change it to libraries. But I am very interested in specific modules. Thank you for your reply though"
This is the case in many communities, for example the deep learning community is almost purely based around CUDA code - somewhat because NVidia writes better tools and libraries surrounding it. e.g. cublas, cufft, cudnn etc. all of which make it really hard to go to OpenCL ahead of CUDA unless you want to do a n immense lot of extra work yourself.
Since you seem to be a native English speaker, it would be nice if you could help us fix the language, it has some awkward sentences.
Yep. I want fo fix some issues and receive some feedback before. 
I use it as well. :-)
I do too
Here's my list, produced by running recursive grep on the codebase at work and massaging the result: 323 Data.Text.Lazy 241 Prelude 223 Data.Monoid 202 Data.Aeson 158 Data.List 152 Data.Text 142 Data.Maybe 141 Data.ByteString.Lazy 135 Control.Monad 129 Test.Hspec 119 Data.Map 117 Data.Default 95 Control.Exception 85 Control.Monad.Trans 82 GHC.Generics 75 Network.Wreq 73 Control.Concurrent.STM 70 System.Environment 70 Control.Lens 69 Test.QuickCheck 67 Data.Set 59 Network.Wai 56 Data.ByteString 53 Control.Arrow 51 Data.Text.Encoding 50 Data.Time.Clock 50 Data.Text.Lazy.Encoding 49 System.IO 48 Control.Concurrent 46 Control.Monad.Identity 44 Data.Ratio 44 Data.Functor 41 Control.Monad.Reader 39 Data.HashMap.Strict 35 System.Directory 34 Control.Monad.Except 33 Servant 33 Data.ByteString.Char8 30 Data.String It should be noted that this is not an accurate result because there are many internal modules (which I removed) whose sole purpose is to re-export other modules, including many in base. It surprises me to see `Data.Text.Lazy` at the top in fact. The `Prelude` is there because we often use `import Prelude as P` to disambiguate things.
As far as I can tell, there isn't a way to do this using the EKG API. In some cases you could use [`registerGroup`](https://hackage.haskell.org/package/ekg-core-0.1.1.0/docs/System-Metrics.html#v:registerGroup). I'm not sure how taylorfausak could implement his approach either. That sounds like a very useful thing to have though. As a caveat, I'm more of a Yesod person and am not deep into EKG, so someone else may know better.
Here's a rough dump from my active working directory: Control.Applicative 413 Prelude 287 Data.Foldable 259 Control.Monad 236 Data.Monoid 182 Control.Lens 178 Data.Traversable 170 Data.Data 133 Data.Semigroup 125 GHC.Generics 121 Data.Word 111 Control.Comonad 104 Data.Typeable 84 Data.Map 83 Data.Functor.Identity 78 Data.Functor.Bind 73 Data.Int 67 Control.Monad.IO.Class 63 Data.Distributive 62 Data.Hashable 61 Data.Set 61 Control.Arrow 59 Control.Exception 59 Data.List 58 Data.Text 58 Data.Binary 57 Data.Bits 57 Data.IntMap 57 Data.Profunctor 56 Control.Category 54 Data.Sequence 51 Data.Functor.Rep 50 Data.Serialize 50 Data.Semigroup.Foldable 49 Data.Function 47 Data.Maybe 47 Control.Monad.Trans.Class 45 Control.Monad.Trans.Identity 45 Data.Profunctor.Unsafe 45 Control.Monad.Reader.Class 43 Data.Functor.Contravariant 43 Data.Void 42 Control.Monad.Fix 41 Data.HashMap.Lazy 41 Data.Proxy 41 Data.Bitraversable 40 Data.ByteString 40 Data.Functor.Extend 39 Data.Semigroup.Traversable 39 Data.Vector 38 Data.Vector.Generic 38 Bound 37 Control.Monad.Primitive 37 Data.Functor 37 Data.Functor.Compose 37 Foreign.Storable 37 Data.Bifunctor 36 Data.Bytes.Serial 36 Control.Monad.Trans.Reader 35 Control.Monad.Trans.State.Strict 35 Data.Functor.Apply 35 Data.List.NonEmpty 35 Control.Monad.ST 34 Control.Monad.Trans.State.Lazy 34 Foreign.Ptr 34 Data.Bifoldable 33 Control.Monad.Trans.Writer.Lazy 32 Control.Monad.Trans.Writer.Strict 32 Control.Monad.Zip 32 Data.Vector.Unboxed 32 Data.IntSet 31 Data.Ix 31 Control.Monad.State 30 Data.Vector.Generic.Mutable 30 Ermine.Syntax.Global 30 Numeric.AD.Mode 30 Numeric.Natural 30 Control.Monad.Trans.RWS.Lazy 29 Control.Monad.Trans.RWS.Strict 29 Data.Tagged 29 Unsafe.Coerce 29 Data.Default 28 Data.HashSet 28 Control.Lens.Type 27 Control.Monad.Trans.Maybe 27 Data.ByteString.Lazy 27 Ermine.Syntax.Type 27 Data.Vector.Primitive 26 Numeric.Algebra.Class 26 Control.DeepSeq 24 Control.Lens.Getter 24 Ermine.Syntax.Kind 24 GHC.Exts 24 Control.Monad.Trans.Error 23 Data.Complex 23 Data.Profunctor.Rep 23 Data.Profunctor.Sieve 23 System.IO.Unsafe 23 Bound.Var 21 Control.Comonad.Trans.Class 21 Control.Monad.State.Class 21 Data.Char 21 Data.Functor.Product 21 GHC.Prim 21 Graphics.GL.Types 21 Data.Coerce 20 Ermine.Syntax 20 Control.Applicative.Backwards 19 Control.Monad.Trans.Except 19 Control.Monad.Trans.List 19 Data.Functor.Plus 19 Data.HashMap.Strict 19 Data.IORef 19 Data.Semigroup.Reducer 19 Linear 19 Numeric.Algebra.Unital 19 Text.Read 19 Bound.Scope 18 Data.Functor.Reverse 18 Data.StateVar 18 Numeric.Algebra 18 Control.Concurrent 17 Control.Lens.Fold 17 Control.Lens.Internal.Indexed 17 Control.Lens.Setter 17 Data.Constraint 17 Data.Reflection 17 Data.Tree 17 Text.Parser.Combinators 17 Control.Lens.Traversal 16 Control.Monad.Catch 16 Control.Monad.Reader 16 Data.Bytes.Put 16 Data.Semigroupoid 16 Graphics.GL.Core45 16 Text.Parser.Token 16 Control.Monad.Trans.Cont 15 Data.Bytes.Get 15 Data.Functor.Classes 15 Data.Text.Lazy 15 Data.Vector.Storable 15 Ermine.Syntax.Scope 15 Control.Monad.Error.Class 14 Control.Monad.Writer.Class 14 Data.Array 14 Data.Primitive 14 Data.Profunctor.Closed 14 Ermine.Syntax.Convention 14 Ermine.Syntax.Core 14 Ermine.Syntax.Name 14 Prelude.Extras 14 Control.Comonad.Trans.Traced 13 Control.Monad.Free 13 Data.Functor.Alt 13 Data.Hashable.Extras 13 Ermine.Syntax.Literal 13 Foreign.C.Types 13 Linear.Epsilon 13 Linear.Vector 13 Numeric.AD.Internal.Combinators 13 Numeric.AD.Internal.On 13 Quine.GL.Types 13 System.IO 13 Control.Comonad.Cofree 12 Control.Lens.Lens 12 Control.Monad.Trans 12 Data.FingerTree 12 Data.Pointed 12 Data.Set.Lens 12 Ermine.Pretty 12 Ermine.Syntax.ModuleName 12 Foreign.Marshal.Alloc 12 Language.Haskell.TH 12 Control.Lens.Internal.Context 11 Data.Biapplicative 11 Data.Functor.Adjunction 11 Data.Key 11 Data.Machine.Type 11 Data.Number.Erf 11 Data.Vector.Unboxed.Base 11 Foreign.Marshal.Array 11 Linear.Metric 11 Math.Category 11 Numeric.Additive.Class 11 Numeric.Ring.Class 11 Succinct.Dictionary.Class 11 Succinct.Internal.Bit 11 Data.Fold.Internal 10 Data.Functor.Constant 10 Data.Functor.Sum 10 Data.Vector.Fusion.Stream.Monadic 10 Ermine.Syntax.Hint 10 Ermine.Syntax.Term 10 GHC.Types 10 Linear.V3 10 
The title is without context, unfortunately. Since the last mention of SAW on /r/haskell: * SAW-script, and not just SAW-core and supporting libraries, is now licensed BSD3. * SAW 0.2 has been released. * The SAW tutorial has been shaping up quite nicely. For the uninitiated: &gt; The Software Analysis Workbench (SAW) provides the ability to formally verify properties of code written in C, Java, and Cryptol. It leverages automated SAT and SMT solvers to make this process as automated as possible, and provides a scripting language, called SAW Script, to enable verification to scale up to more complex systems.
My top whatever list: 776 Control.Applicative 754 Control.Monad 597 Prelude 466 Data.Monoid 359 Data.Foldable 343 Data.List 323 Data.Word 302 Data.Vector.Storable 271 Control.Exception 255 Control.Lens 222 Data.ByteString 202 Foreign.Storable 201 Data.Text 196 Distribution.Simple 191 Data.ByteString.Lazy 189 Linear 189 Foreign.Ptr 183 Data.Maybe 183 Control.Arrow 181 Data.Traversable 176 Data.Map 173 Data.Char 170 Data.Typeable 160 System.IO 157 Data.ByteString.Char8 154 System.Environment 140 Foreign.C.Types 140 Data.Data 139 Data.Set 138 Data.Text.Lazy 135 Data.Bits 131 Data.Functor.Identity 125 Data.Int 124 Data.Vector.Unboxed 116 Data.Vector 116 Data.Proxy 115 Control.Monad.Fix 112 Control.Monad.IO.Class 107 Data.Version 107 Data.Vector.Storable.Mutable 106 Control.Monad.ST 103 Control.Monad.Trans.Class 102 Data.Semigroup 99 Graphics.Rendering.OpenGL 99 Control.DeepSeq 97 Control.Comonad 96 GHC.Exts 96 Data.IntMap 95 GHC.Generics 91 System.FilePath 90 Data.IORef 87 System.IO.Unsafe 87 Control.Monad.Trans 85 GHC.Base 84 Data.Vector.Generic 83 Foreign.ForeignPtr 83 Control.Monad.Trans.Reader 82 Control.Monad.Trans.Identity 81 Control.Monad.Primitive 81 Control.Concurrent 79 Control.Category 78 Test.QuickCheck 77 Control.Applicative( 75 Criterion.Main 74 Control.Monad.Trans.State.Strict 71 Test.Framework 71 Data.Vector.Generic.Mutable 71 Data.Functor.Bind 71 Control.Monad.Trans.Maybe 70 Control.Monad.Trans.RWS.Strict 70 Control.Monad.Trans.Error 69 GHC.Prim 
Hey, you should either embed Menlo into the site or add a monospaced fallback for the code blocks because currently they display as some serif font if you don't have Menlo installed.
I wasn't just making things up :) Here's a concrete example of my suggestion: https://github.com/tfausak/blunt/blob/2.0.3/library/Blunt/Component/Metrics.hs#L59
Thank you for the context! actually, the link states that it is all BSD3 now, which is even more significant than (a valid interpretation for your comment) having one more component be open source.
Oh of course! I was looking at the EKG API itself and it didn't seem like there was a way to track if you've already registered a metric—your way of using a `Map`to hold metrics clearly gets around that
This is really great to hear, and the license had prior put me off from contributing or looking into hacking on the system too much. Some questions, since I know Galwegians are watching: - Are there any plans or ideas on when the compilers might come back? Cryptol 1.x had them, and I'm not nearly as interested in compiling to C as I am something like VHDL or Verilog. Mostly because I don't want to write them (I really want hardware synthesis and software verification, combined. Also Cryptol 1 didn't generate great C code, was dependent on libgmp, etc, and I'm obviously more comfortable writing software than circuits) - There's been some mention of you all doing proofs for things like ECDSA. Do you have any of these available publicly as demos or plan on releasing them? I think they would serve as good examples, because muddling about on larger, compositional proofs on your own was a bit tough for me at first (you can't really expect a SAT solver to just crush its way through a proof that scalar multiplication is equal for all inputs and outputs, in a one-shot way, without help.) From words I had heard, I assume it was something like verification of Bouncy Castle's ECDSA implementations? I'd be interested in looking at your proofs. I recently completed a Curve41417 implementation in Haskell. I'd like to take this knowledge to instead model Curve25519 and do some proofs on it, to see how far an automated tool could get. It would also be interesting to look at the guarantees provided by SAW versus something like [gfverif](http://gfverif.cryptojedi.org/examples.shtml). - I brought this up on IRC, but: it'd be interesting when (or if) y'all can talk about any kinds of binary analysis you're doing. Then you can do things like verify the IR, and the output code of an arbitrary compiler, do in fact match. For code like this, I think treating the 'middle end' of the compiler as a blackbox, which you ask to produce a program and then give to the solver is much simpler conceptually, and is more robust to churn. I had some hair-brained schemes for this but there's nothing I worked on that was really principled. I also have a proprietary iMX53 bootrom here, though, that would like to have its hashing functions extracted and examined. :) 
Ah, sorry about the ambiguous title. Just came to know about SAW yesterday.
Let's see. Skimming https://hackage.haskell.org/user/EdwardKmett and thinking about what is still active and not deprecated. I updated `traced` for Lennart, but don't use it much at all. For me, `monad-st` is subsumed by modern `primitive`. `multipass` was a proof of concept. I don't use `parsec-parsers` myself, but it exists for completeness. `speculation` could probably stand to see more use than it does, but I haven't had any suitable problems lately. `tables` has a bug that could stand to get fixed. I used it to motivate the design of a lot of advanced `lens` features, but haven't used it since. `type-int` was the second bit of Haskell I ever wrote. I mostly maintain it for nostalgia. I never did wind up needing `compensated`. My uses of `concurrent-supply` are now mostly replaced with `unique`. I have a bunch of code for `hyphenation`, but never got around to using it in the error messages that I had originally planned it for.
Recipes are more intuitive for humans because the ones both doing the work required _and_ using the end product. I think it's a fair argument to say this isn't necessarily the same as for programming, since we often know more precisely the properties we want from the end product (i.e., what problem is to be solved), as opposed to how exactly a CPU would do the work required to produce it.
If my software was as short as cooking recepies I might have written it in imperative style too.
It is all about what you are describing. Recipes require doing a single thing so direct directions are best. In contrast running a restaurant is much more difficult if you are detailing every single step to be done by everyone. It is much easier to describe the task in an abstract fashion, "the cook makes food", "the waiter takes orders" then later boiling down that detail when talking only about that specific thing.
 * 74 Data.Text * 37 Control.Monad * 31 Data.Monoid * 26 Prelude * 24 Data.Maybe * 24 Data.List * 22 Control.Applicative * 21 Data.Map.Strict * 19 Distribution.Simple * 19 Data.Functor * 18 Data.Map * 17 System.Environment * 16 Test.Hspec * 13 GHC.Generics * 13 Data.Foldable * 12 System.Exit * 12 Data.Set * 12 Control.Exception * 11 Control.Lens * 10 Data.Attoparsec.Text * 10 Control.Monad.IO.Class * 10 Control.Monad.Catch * 9 Data.Version * 9 Data.Data * 8 Text.Parsec * 8 System.Process * 8 Prelude.Unicode * 8 Data.ByteString * 8 Control.Arrow * 7 Text.Trifecta.Parser * 7 System.Random * 7 System.Directory * 7 Data.Char * 7 Data.AffineSpace * 7 ClassyPrelude * 6 Web.Scotty * 6 Text.Regex * 6 Text.PrettyPrint.ANSI.Leijen * 6 Test.QuickCheck * 6 System.IO * 6 System.FilePath * 6 Pipes * 6 Nix.Expr * 6 Debug.Trace * 6 Data.Ratio * 6 Data.Fix * 5 Text.Trifecta.Result * 5 Text.Trifecta.Delta * 5 Text.Parser.Token * 5 Text.Parser.Combinators * 5 Text.Parser.Char
Not sure if I understand your question - what do you mean by (Lists) &gt; can't really branch at all into multiple lists List can be used to define control flow. `[1..10]` is syntactic sugar for *function call* `enumFromTo` from `Enum` type class. List makes it easy to define bunch of loops with help of some high order functions like *map, fold, filter*. So whether you get better performance depends on your use case, there is no general replacement for List. Look on [primes in Haskell.](https://wiki.haskell.org/Prime_numbers)
I imagine you could make sense of such a type when `k` and `v` are GADTs specifying some query structure. (Overly simplistic) example: to store the configuration of a program, you could have a GADT of the form data ConfigValue a where Timeout :: ConfigValue Float OutputFileName :: ConfigValue String NumCores :: ConfigValue Int You could use your type `ConfigValue` as `k`, and `Identity` as `v`, and store all your configuration in a (heterogeneous) map in a type-safe way. This is a very silly example and a simple record would be a better choice, but at some point in my life I wrote a DHCP server that was basically a big list of property-value pairs which would have benefited a lot from a structure like this one.
I'm working on building something similar to Haxl. I'm having data sources implement their own caching. So for a data source `f`, a request returning type `a` would be represented as the type `f a`. In order to cache such a data source, there would need to be a map whose keys can be `f`s of any type, and values can be results of the same type. Specifically, I'll be using `MVar` as the `v`, but that's specific to this use case.
http://hackage.haskell.org/package/dependent-map-0.2.1.0/docs/Data-Dependent-Map.html
Perfect! Thank you!
I wrote [an Agda version of `convolve`](https://github.com/effectfully/random-stuff/blob/master/TABA.agda) that doesn't contain any proving while still giving the same performance.
Propositions as Pies! I'm thinking of how to model stuff like "leave the pot boiling for twenty minutes". Maybe we need functional reactive programming here? Also it seems difficult to model ingredient usage in the type system itself... If recipes could be expressed using applicatives or arrows, at least you could perform some analysis before starting to cook. 
This is called an [unrolled linked list](https://en.wikipedia.org/wiki/Unrolled_linked_list) and I also think it would be a great replacement for Haskell's list, when the list is used as a persistent data structure, and not as a method of iteration. Note that Data.ByteString.Lazy and Data.Text.Lazy are essentially using this structure. I'd suggest that if you prepend a single element -- that the function copies the first chunk to prepend to it. It's a bit inefficient to copy, but each copy will only cost a single chunk. OTOH, memory use and iteration will be better. The use case of prepending element by element will improve as well. With `unsafePeformIO` trickery, perhaps the old unpreneded chunk, if it is unshared, can be made to share the tail of the newly copied chunk. Alternatively, prepending would squash the chunk at each `2^n` threshold (n=number of prepended singular items). To squash it effectively, you'd also need to use `unsafePerformIO` tricks. But this can be made truly pure, so it should probably be OK to do this. I think the only compiler support this would require is nice patterns. Of course the user would have to import this type and use it as a separate type, and they should, because for ordinary fusable iteration, lists would still be better. And perhaps view patterns and pattern synonyms already facilitate this.
I don't think I ever used `Array`. Just terms like "array list" and the word `Array` within the name of the custom type.
That's sad... so this is basically because NVidia invests a lot to write great tooling to support their proprietary APIs to the detriment of open cross-vendor APIs which get less attention because everyone just prefers convenience over open standards, so the open standard gets ignored even more... this is exactly the reason why I dislike NVidia (or any other projects which choose do their own thing for personal gain than to try to improve the existing tooling/standards first so everyone can benefit) :-(
It's interesting that every list so far seems to include `Control.Lens` for what it's worth, here's my own current top-50: Blaze.ByteString.Builder Control.Applicative Control.Arrow Control.Concurrent Control.Concurrent.MVar Control.DeepSeq Control.Exception Control.Lens Control.Monad Criterion.Main Data.Aeson Data.Aeson.Types Database.PostgreSQL.Simple Data.Bits Data.ByteString Data.ByteString.Char8 Data.ByteString.Internal Data.ByteString.Lazy Data.ByteString.Lazy.Char8 Data.Char Data.Hashable Data.HashMap.Strict Data.Int Data.IORef Data.List Data.Map Data.Maybe Data.Monoid Data.Set Data.Text Data.Text.Encoding Data.Time.Clock.POSIX Data.Typeable Data.Vector Data.Version Data.Word GHC.Generics Network.Socket Pipes Prelude Prelude.Unicode System.Environment System.Exit System.IO System.IO.Streams System.IO.Unsafe System.Win32 Test.Framework Test.Framework.Providers.HUnit Test.HUnit 
Comment of the year
I get that we all like functional programming but many of these posts seem to pretend like you can't do any kind of abstraction in imperative programming. Usually you can get away with this because "well, functions are functional" and because theres no clear definition of functional programming, but this question SPECIFICALLY asks about immutable structures. Pretending that somehow you can't write imperative instructions for a restaurant (because functional programming is somehow inherently parallel?) or that an imperative recipe would actually require an exposed counter for the number of eggs you need to add to a bowl? Really? No it wouldn't. These examples are just bad analogies and have literally nothing to do with immutable data structures. It IS a fact that some instructions are inherently easier to express with mutation. If we insist on cooking comparisons, combining eggs and flour does not let you use them separately again later. Adding something to a bowl literally means take this bowl and put something in it. It doesn't mean make a new bowl with the contents in it. We use things like IO and the State monad to simulate mutation within a functional framework. Sure, we prefer when there's no mutation but pretending like this is always the case is absurd and dishonest. It certainly doesn't help any discussion.
Me too!
The Haskell Platform binary for windows made a lot of sense initially, since there were some essential libraries that were pretty hard to compile on Windows without some knowledge of mingw and the C toolchain.... Today, most of those annoyance are gone AFAIK so the HP binary is less relevant. The secondary goal of the HP was to establish a stable small set of essential libraries bundled and versioned so that anyone could depend on the HP 2014.3 or some such. Of course Stackage accomplish the same today except the set is way larger and more frequently updated so... Basically I would say stack and stackage are heirs to the ambitions of the Haskell Platform and make it pretty much irrelevant. Note that this is not a commentary on stack vs cabal-install, they both have their usages and cabal-install should work better once backpack is there (partially).
My impression is that they are unrelated. It's [possible](https://github.com/effectfully/random-stuff/blob/master/IFreerIFree.agda) to coerce a `Freer` monad to `Free` monad defined over some container and the other way around, but these coercions look rather shallow and artificial to me. But I'm really not an expert on the subject.
Freely available paper from [the author's web site](http://alumni.cs.ucr.edu/~ratana/publications1.htm): http://www.cs.ucr.edu/~ratana/PaneRatanamahatanaMyers00.pdf
The first equals: [1,2] &gt;&gt;= \n -&gt; (['a','b'] &gt;&gt;= \ch -&gt; return (n,ch)) Pay attention to the parentheses. 
I'm curious what the differences would be across natural languages. Do speakers of different languages have different primary means of solving problems in language? 
I think that's more of an instance of just using a downright *incorrect* type. Unicode strings really aren't sequences of characters and it's almost always wrong to manipulate them that way. (Let's ignore non-unicode as unicode is basically universal at this point.) In contrast, if you look at e.g. Text, you'll see that the "default" mode is the "encapsulated" approach, but that you *can* actually get at the internals if you use the Data.Text.Internal.* modules. Of course you do so at your own risk (and no guarantees about PVP-adherence, etc. etc.), but you *can* do it. (Of course in this case accessing internals can also break the type system, etc. etc. because you can use unsafe indexing, etc., but there's no *particular* reason that the Internal.* modules should allow you to do that, other than performance.)
What I've learned from this thread: I know nothing about Haskell libraries
It's pretty obvious that imperative is more intuitive, easier and more extensible/maintainable.
I'd say you probably have an older Haskell version which comes with an older "base" package. For example, base 4.0.0 doesn't have `%b`: https://hackage.haskell.org/package/base-4.0.0.0/docs/Text-Printf.html On further research, that is definitely the case. It was added between 4.6.0.1 and 4.7.0.0.
&gt; have the frontend (ghcjs) and backend (ghc+warp/servant) in I haven't tried it yet but you can refer to the `Project with both client and server` section of the stack's GHCJS documentation. Here's the link: http://docs.haskellstack.org/en/stable/ghcjs/
There's a huge assumption in there, which you seem to take for granted. Where's your evidence that the universe is a 3 dimensional construct being updated over time, rather than a 4 dimensional construct which merely exists? I'm pretty sure that from within the universe there's no way to distinguish between the two models. You're just making an assumption that things change in order to defend your preconceptions. 
Thanks
Yes, please
You’re right. I was referring specifically to encapsulation of state, as explained in my article. And of course immutability is not sufficient to prevent the construction of invalid states in Haskell. For instance, sometimes we need “smart constructors”—which is to say, OOP-style constructor functions with validation logic in them. Also, I try hard not to use type synonyms in my own code except to abbreviate long types that are already distinct: type StringReq = Request MyDataSource StringValidator String type IntReq = Request MyDataSource IntValidator Int Any time I’ve written something like this, I’ve regretted it: -- Polymorphic (only prenex foralls) type Sigma = Type -- Polymorphic (foralls anywhere) type Rho = Type -- Non-polymorphic type Tau = Type 
Since every type will be promoted to kind in ghc 8, there will soon be lots of kinds. Even in 7.10, lots of types can be promoted, though by no means all.
would you be so kind as to post or pm a copy of those stack.yaml files? I've been unable to get ghcjs working in stack and I really don't want to go back to just cabal. 
Yeah I've seen that page but had little luck using ghcjs that way. ghcjs-boot always fails regardless of version. Something about integer-gmp... I really do not know. I've spent way too many hours trying to get ghcjs to work. I may end up using a container I guess.
That's very cool! thank you.
Hmm, my stack.yaml file is identical to the one posted by /u/yamafaktory [here](https://github.com/yamafaktory/reflex-starter/blob/master/stack.yaml) aside from the `extra-dep` so if that's not working, I would suggest you try temporarily moving your `ghcjs-boot` and `stack` directories elsewhere, checking what version `nvm` exposes to `stack`, and then letting `stack` rebuild GHCJS from scratch. 
Awesome!
You'll need to post the error with integer-gmp. Is it a missing system dependency?
Some languages do have number systems like that, for example [Pirahã](https://en.wikipedia.org/wiki/Pirah%C3%A3_language). And yeah, languages with features like absolute direction or personal/impersonal nouns may make speakers more aware of those distinctions, but I don’t know of much research about it. The only references I know of offhand for linguistic universals are [Greenberg’s linguistic universals](https://en.wikipedia.org/wiki/Greenberg%27s_linguistic_universals)—fairly technical, mostly to do with syntax; and [natural semantic metalanguage](https://en.wikipedia.org/wiki/Natural_semantic_metalanguage)—much more interesting for programming language design, I think. 
I've interviewed several people for working on projects that are written in haskell. One question that invariably comes up is, "What on earth compelled you to learn about an nearly unheard-of programming language?".
I am also running Arch with ghc 7.10.3-1 (community) and everything works fine (stack also comes from community/haskell-stack). I'm no expert but it could be an invalid entry in the PATH (see https://github.com/ghcjs/ghcjs/issues/143#issuecomment-55239767). 
Here's my list extracted from a project in production: * 13 Data.Monoid * 13 Control.Applicative * 12 Prelude * 12 Data.Foldable * 12 Control.Monad * 11 Control.Exception * 10 Debug.Trace * 10 Data.Traversable * 9 Data.Map * 9 Data.IntMap * 8 Text.XML.HXT.RelaxNG * 7 Text.XML.HXT.Arrow.XmlArrow * 7 Data.Maybe * 7 Data.List * 7 Data.Functor * 6 Text.XML.HXT.RelaxNG.DataTypes * 6 Text.XML.HXT.DOM.XmlNode * 6 Text.XML.HXT.DOM.Interface * 6 Control.Arrow.ListArrow * 5 Text.XML.HXT.RelaxNG.Validation * 5 Text.XML.HXT.DOM.ShowXml * 5 Text.XML.HXT.Core * 5 System.Environment * 5 Data.Sequence * 5 Data.Char * 4 Text.XML.HXT.RelaxNG.Validator * 4 Text.XML.HXT.RelaxNG.CreatePattern * 4 System.Console.CmdArgs.Implicit * 4 Data.Version * 4 Data.Text * 4 Data.Reify * 4 Data.IntMap.Lazy * 3 Text.XML.HXT.RelaxNG.DataTypeLibraries * 3 Text.XML.HXT.DOM.TypeDefs * 3 System.Mem * 3 System.IO * 3 System.Exit * 3 Network.Wai.Handler.Warp * 3 Network.Wai * 3 Data.Tree.NTree.TypeDefs * 3 Data.Either * 3 Data.Char.Properties.XMLCharProps * 3 Control.Arrow.ArrowTree * 3 Control.Arrow.ArrowList * 3 Control.Arrow.ArrowIf * 3 Control.Arrow * 2 Test.QuickCheck * 2 System.Process * 2 System.FilePath.Posix * 2 System.Directory * 2 Network.HTTP.Types * 2 Data.Set * 2 Data.Map.Lazy * 2 Data.IntSet * 2 Data.ByteString.Lazy.UTF8 * 2 Data.ByteString * 2 Control.Monad.Trans.State.Strict * 2 Control.Monad.Trans.State * 2 Blaze.ByteString.Builder.Char.Utf8 * 2 Blaze.ByteString.Builder * 1 Text.XML.Stream.Parse * 1 Text.XML.HXT.TagSoup * 1 Text.XML.HXT.RelaxNG.Utils * 1 Text.XML.HXT.RelaxNG.PatternToString * 1 Text.XML.HXT.Curl * 1 Text.XML.HXT.Arrow.DTDProcessing * 1 System.FilePath.Glob * 1 Network.Wai.Handler.Warp.Internal * 1 Network.URI * 1 Network.HTTP.Types.Status * 1 Data.XML.Types * 1 Data.Text.IO * 1 Data.Ord * 1 Data.Functor.Identity * 1 Data.Functor.Constant * 1 Data.Functor.Compose * 1 Data.Configurator.Types * 1 Data.Configurator * 1 Data.Conduit * 1 Data.Aeson * 1 Control.Monad.Trans.Reader * 1 Control.Monad.Trans.Except 
&gt; nearly unheard-of programming language wat
[here](http://pastebin.com/SRJ7KhGw) is the last part of the output of stack setup, where it begins to fail. The stack.yaml's snapshot is set to lts-5.11 (and my global is 5.0), and that exposes integer-gmp-1.0.0.0, which is not what ghcjs wants. I don't know how to resolve this. My stack.yaml is copied from /u/yamafaktory's github, but with the compiler changed to use ghc 7.10.3 as in stack's GHCJS example configuration. [here] (http://pastebin.com/PwmGYznZ) it is. What do you think?
Nope. `f`s are the requests, not the actual results. data MySource a where GetSite :: URL -&gt; MySource ByteString The map stores requests as keys, and the results as values. MVar is wrapping the value so that I can have multiple equivalent requests, and only one of them performs the actual fetch. The others just wait on the MVar to be full.
yeah mine too... That is not the error I get with stack, but I think it might be the error I got when trying to install GHCJS manually from its git repo. I'm not sure as I really want to get it to work with stack instead, but if that continues to seem impossible I'll use this. Thanks.
I love this kind of stuff! https://github.com/avieth/Algebraic
How long until this is in `Prelude`? I really need this level of function specificity to confuse my coworkers!
I usually use classy prisms for this. Nick Partridge has a [gist](https://gist.github.com/nkpart/c3bcb48c97c5ded6e277) explaining this. You can also use classy lenses to do something similar with Reader and State. 
Wow. That's really interesting. Thanks for chatting about it. The thing that first got me into programming was NLP and textual analysis, but I've been doing web and ETL stuff for so long that I've gotten away from that. It still interests me, though.
I would be interested in that yes. But I think it might be easier for me to debug my Haskell code with the Repa library and then hand port it to OpenCL with the OpenCL bindings :(
The downside of this approach (and other `Codensity` or `Yoneda` `Set` variants) is that it doesn't scale very well at all. Nothing collapses until the end of the computation, whereupon you do a potentially exponential number of redundant lattice operations.
I'm wondering about this line: ```configure: WARNING: unrecognized options: --with-compiler, --with-hc-pkg, --with-gcc``` Is that a warning by `cabal-install`? If so, do you use an older version of `cabal-install`? 
Try running `cabal install hsdev`. Alternatively, you could add the line `"enable_hdevtools": false` to your SublimeHaskell user settings.
Yes, I also get the same warnings and it builds fine. Sorry, can't help.
Try to compare with my [happy-alex-example](https://github.com/da-x/happy-alex-example): $ stack ghci ... λ &gt; import CalcParser λ &gt; :t CalcParser.happyParser CalcParser.happyParser :: Parser Exp 
https://github.com/google/codeworld/blob/master/codeworld-base/src/Prelude.hs ;)
&gt; A Queued value represents the result of a command inside a transaction. It is a proxy object for the actual result, **which will only be available after returning from a multiExec transaction.** (from https://hackage.haskell.org/package/hedis-0.7.10/docs/Database-Redis.html#t:Queued) This makes me believe that you can't use the result of `hello` and `world` inside the transaction.
You can also use my library [Ether](https://int-index.github.io/ether/) to work with multiple exception types simultaneously without creating boilerplate sum-types to store them.
&gt; ghc-mod emacs syntax checking checkworks only if you generate some nasty symlinks This was exactly what I was looking for. Thanks!
Ether looks awesome 
I can define `newtype Set a = Set [a]`. What can your definition do that mine can't? It's a sincere question, I don't see anything but perhaps I'm missing something obvious?
Sure but what do you think is the most likely : 1. A Haskell (community) programmer has real experience with Python AND Haskell, * A Python (community) programmer has real experience with Python AND Haskell, * A programmer from another community has real experience with Python AND Haskell ? A number of the Haskell answerers made the point that they are proficient in Python and have to use it very often, on the other side many Python answerers said they didn't really know Haskell but Python was the best...
That tutorial is outdated. Use instructions from here https://github.com/SublimeHaskell/SublimeHaskell
Is `head` bad practice? 
My impression was that Set's problem didn't lie in the Ord-constraint, but in its semantics. fmap (g . f) = fmap g . fmap f doesn't hold if you, say, have a newtype newtype BadEq a = BadEq {fromBadEq :: a} instance Eq (BadEq Int) where (BadEq 1) (BadEq 2) = True (BadEq 2) (BadEq 1) = True (BadEq x) (BadEq y) = x == y instance Ord (BadEq Int) where compare (BadEq 1) (BadEq 2) = EQ compare (BadEq 2) (BadEq 1) = EQ compare (BadEq x) (BadEq y) = compare x y Then fmap (fromBadEq . BadEq) {1,2,3,4,5} = {1,2,3,4,5} fmap fromBadEq . fmap BadEq $ {1,2,3,4,5} = {1,3,4,5} The Eq-instance of BadEq is an equivalence relation, but 1 and 2 being in the same equivalence class causes the set-semantics to violate the the functor law.
At type `Set Int` they aren't identical. The issue lies in `fromBadEq` treating equivalent `BadEq`s differently by returning different `Int`s.
Linear types are an interesting thing - seems to me that their main purpose is for mutable values, like eggs which you can't use twice? Don't get me wrong - they seem very useful, but isn't that their purpose? (Where else do you use them?)
You tweak the extra-packages list in your projects `stack.yaml`. This can require some educated guessing. The last release of Craft3e was in 2013, so using a recent resolver is a bad idea (I tested with lts-2.13 which uses GHC 7.6. You can add `extra-deps` by hand or the "easy way" is to run `stack solver --update-config --resolver lts-2.13`. `stack init` will fall back on the resolver as well, so you can run `stack init --resolver lts-2.13` from the src dist on Hackage.
 stack unpack Craft3e cd Craft3e cabal sandbox init cabal build Worked for me. There is probably way to unpack package with `cabal` tool. 
1. [Heist](http://hackage.haskell.org/package/heist) (full disclosure: written by me). It is all HTML with almost no special syntax that keeps *all* logic out of the templates. Heist is actively maintained as a part of the Snap Framework, but can be used standalone with any other framework. 2. [HStringTemplate](http://hackage.haskell.org/package/HStringTemplate). A Haskell port of the Java StringTemplate library. 3. [mustache](http://hackage.haskell.org/package/mustache). A Haskell implementation of [mustache templates](http://mustache.github.io/).
The Ord-constraint might be, but I'm pretty sure you need the Eq-constraint. Otherwise, you wouldn't have basic set-semantics like the idempotence of union: S `union` {x} `union` {x} = S `union` {x} If you can't determine that you're adding the same x twice (via ==), you don't have a set.
I've been touting about this package on this sub whenever it's relevant. It's such a good solution to the problem. I only wish we could use string literals to represent proxies of type level strings. Then we could just do this: x :: MonadState "key" Int m =&gt; m Int x = get "key" -- Instead of x = get (Proxy :: Proxy "key")
Thanks, this was a breeze to set up! I'm excited to finally get started experimenting with Reflex. :D
I hadn't thought about that, but you're right. Using data types scopes everything to namespace correctly, unlike type-level strings. Good call. I was about to say, a type-level string isn't any less specific than an arbitrary empty type, but I hadn't thought about that.
I'm glad you're liking the library, by the way! :) I feel awkward every time I mention it in this sub because it's basically self-advertisement. So, thanks for touting about it, too. I'd like more people to consider tagging before they go flatten their transformers and start writing the related lens/prism boilerplate.
You're welcome, I'm glad it was helpful!
I cannot put them in order, as their use cases differ. * [Hamlet](http://www.yesodweb.com/book/shakespearean-templates#shakespearean-templates_hamlet_syntax) -- like [Haml](http://haml.info/), a DSL, parsed at compile time, needs TH * [Lucid](http://chrisdone.com/posts/lucid) -- an EDSL, so mixes fully with Haskell code, no TH * others that I have used I would not recommend &gt; used an external html file rather than describing html layout in haskell code You can do that with Hamlet, Yesod examples of doing this are all over the place.
&gt; There is probably way to unpack package with `cabal` tool. Indeed there is: `cabal unpack Craft3e` (or the synonym `cabal get Craft3e`) PS: The package description at https://hackage.haskell.org/package/Craft3e mentions `cabal unpack` too. However, the current 0.1.0.10 version doesn't support AMP yet, so requires GHC 7.8 or older.
Right, also by decoupling this task you can easily get a language agnostic tool, which was also one of the motivation (there is a producer for sbt/scala). I see other tools that integrate with typecheck as being complementary with this one, and not exclusive.
Consider modelling a physical system with conservation laws. You cannot duplicate *or fail to use* any item in the system going from one step to the next. Linear types would help embed both of these requirements in the type system.
I gave up on SublimeText and moved to Atom precisely because of that.
Yes, my type has many representations of the same set. Why is this a concern? `Data.Set.Set` also has many representations of the same set (though none with repetitions).
Haskell sucks because it doesn't have an opt-in totality checker.
Haskell sucks because it doesn't have type-driven name resolution.
Haskell sucks because it doesn't have dependent types.
Haskell sucks because record field names aren't reusable.
Haskell sucks because if-then-else is builtin syntax.
If you are building highly dynamic web applications i would suggest not to use server side html templaes, but go with ghcjs + some client side web framework like reflexfrp or react-flux or with purescript and one of its web frameworks (thermite, halogen etc) 
This one makes perfect sense to me, but I'm extremely biased because I have a C background. What are the alternatives?
It makes sense from the programmer's perspective, but the way it ends up being implemented is as a linked list IIRC.
Keep in mind that [Char] is a linked list.
How do you mean?
Check out -XTypeInType =)
They are. https://wiki.haskell.org/Package_versioning_policy It's just that in `A.B.C`, the major number isn't defined as `A`, but as `A.B`. This allows library families to keep their *series* in the version number rather than moving it to the package name (e.g. `SDL2-2.0.4` would be `SDL-2.2.0.4`), which is particularly useful when two or more series are being maintained at once. Note how many perfectly stable Haskell packages have an `A` of `0`: it's not a cynical "everything's always beta" statement, they just haven't ever had a bottom-up break-everything rewrite.
I guess my main complaint really is that so many packages don't depend on an API, but on specific versions. (And have definitely found too many packages that either changed APIs without a major number, or vice versa :( )
SOLUTION: OverloadedRecordFields.
They shouldn't. It matters if the program is total. Add a totality checker to haskell. Problem solved.
RebindableSyntax
This will soon change, OverloadedRecordFields IIRC.
* Debugging boils down to wolf-fences and print-statements most of the time * You need `Show` instances to debug-print something, which is a *huge* pain if you're working with someone else's code and they haven't derived show. * Haskell records are variations of a sum-type, as opposed to named tuples, meaning you can get runtime errors when using the accessor functions. Elm's records correct this quite nicely. * Profiling is needed for any stack traces, and is not enabled by default, so you end up spending hours rebuilding your sandbox just to get a stack trace of an error. 
Why is this a bad thing? `if p then x else y` is much more readable to me than `bool y x p`.
To further illustrate your point, `bool` flips the arguments: &gt;&gt;&gt; if True then 'T' else 'F' 'T' &gt;&gt;&gt; bool 'T' 'F' True 'F'
That's what type classes are.
Why *aren't* sdl and sdl2 the same package with different A version numbers? I've never liked the PVP and stuff like that makes me think that SemVer is "more right". 
Haskell sucks because ABI compatibility is very poor due to cross-module inlining.
No, I desugared that the way I wanted to. Assigning to some field in `x` would have to shadow the current value of `x` for it to look like mutation. The alternative is to do something like `let x' = x { y = z }`, which gets tedious as you do more "mutations". 
I would like the ability to specify dependencies on specific implementations, similar to how it's done in e.g. Nix. Possibly on individual modules and functions.
SOLUTION: StandaloneDeriving
Problem: Yet another language extension at the top of every Haskell file. Solution: Add it to the `default-extensions` in your Cabal file. Problem: It's not clear which extensions are enabled when reading a file. 
Thanks! I will definitely look into that :) 
Agreed, this is something that really should have been there from day 1. Could have just lifted the solution from SML.
Typeclasses are irrelevant in some situations. Record fields is one example. Importing Data.Map, and subsequently attempting to use a function like 'lookup', is another.
Haskell needs more ways to ensure performance related guarantees at compile-time, it's a source of bugs that Haskell is completely blind to. I, for one, would like it if I could encode time/space complexity into my types. That's a gut desire, I'm not sure if that would be as useful as I think it would be, or if it would be worth the trade-offs it would inevitably cost.
Function composition, as it would turn out, is not a friend of the 80 character limit.
Oh, yes. Sorry! 
Haskell sucks because you have to put `$` before `do`. 
You can break lines just about anywhere... I recommend doing it at the composition operators.
I've been doing that (especially with string concatenation), but it hurts readability fast. I'm starting to just break it up into sequenced `let` steps in `do` syntax as it's easier to digest piece by piece that way.
Records are in another thread. The only potential problem with more generic type driven resolution would be accidental correctness. However, emitting a warning for having two variables in scope that both have resolutions for a given function might be sufficient.
You are exaggerating. E.g. for things like 'head' there are ways to prove that it's use in the given context is safe. Check out liquidhaskell. It's just rather painful to use.
Why?
I mean I am, but it means that you can never have a perfect totality checker.
This is just another way to wrap things like monad transformers do, instead of admitting that haskell needs a proper effect system.
Writing various transformer instances sucks pretty bad and if you want two independent libraries to work together you have to write orphan instances.
Haskell sucks because there are many numeric types without implicit conversion. Also some number functions like round only operate on a subset of numeric types. Although this is [well documented](https://wiki.haskell.org/Converting_numbers) I don't find the arguments persuasive enough to justify the excessive amount of function calls to use several different types of numbers.
Wrap your do block in parentheses ;) Or don't use do :p
I have the book but haven't read it yet; I decided I wanted to go through CLRS first since it probably would be more useful career wise. Between that, random papers, and work, I already have too much on my plate sadly. I definitely enjoy learning more about it too though and am really looking forward to the book =]
`:` is used for types in most related languages and in math. Turns out type signatures are (and should be) more common than list patterns.
Consistency with things that aren't Haskell.
Hackage rot, e.g., Control.Monad.State superseded by Control.Monad.Trans.State, but you would never know from Hackage.
I'm glad I'm not the only one.
Haskell sucks because it doesn't let you define mixfix operators.
I've never felt like this was a significant issue at all. I prefer the current situation with behavior that is very easy to understand to some complicated solution that makes it less obvious what code is doing.
I think you have that backwards. `mtl`'s `Control.Monad.State` is built on `transformers`'s `Control.Monad.Trans.State`
This one may simply end up as a difference in philosophy. I personally don't mind the explicit coercion.
Oh. OK. I actually misread and thought e were comparing "::" to ":." Instead of ":", which made no sense to me.
Haskell sucks because types are not a substitute for documentation, but nobody seems to realize it
Speaking of which, is there any way to sprinkle my code with stuff from `Debug.Trace` and then have the compiler ignore all of it if it's passed a specific flag for release builds?
I certainly don't mind having different numeric types such as double and integer, but it's when my code is littered to `toInteger` and `fromIntegral` that I get frustrated. Also the fact that some functions like `round` can't take a generic `Num`.
Would you elaborate on what you mean?
I actually dislike do. There are very few occasions where I find it improves things more than better formatting does. However, I'm not opposed to do having higher precedent.
One common example is how annoying it is to update nested records. The syntax isn't at all pleasant.
I wonder if this is a structural or domain specific problem? I at least rarely need any conversion. As for round... 'Why would you round a number that isn't real or fractional?' Is probably the intent. If it really is a common need you could just replace it with a wrapper.
SOLUTION: Classy lenses let you define `data Foo = Foo { fooThing :: String }` and you'll get a type class `HasThing` with the method `thing :: HasThing s =&gt; s -&gt; String` instead of a single function.
&gt;Which is OK, by itself, but leads to my biggest complaint of 'why aren't API changes reflected by major/minor versioning' Huh? They are.
I really like the concept of mixfix, but sometimes I think I'd really just prefer a typed, applicative postfix language. The only problem is that I can't convince myself that postfix types look right.
Can you give an example of this? I'm not sure what you mean and I've never had awkward of bloated diffs.
SOLUTION: Use extensible effects or `mtl`-style constraints to specify exactly what you need.
Yeah, I think most Haskellers would prefer for something like `Text` be the default type for textual data.
"Well, I guess the problem happens between line 50 and 60... *inserts print statements*, ok, it's definitely before 60, *inserts a print at 55*, ok, it's after 55, *inserts a print at 57*..."
gmp dependency makes it hard to develop proprietary software oy! whats with the downvotes. its not that i dream of writing proprietary software, it's just my day job. To get our lawyers to allow the platform we need to be able to use with without LGPL dependencies. While I can build ghc with integer-simple, it will quickly turn into another job to support ghc for every host that we have in use (windows, ubuntu 12-16, centos, osx, some ancient enterprise suse pos). If it was easy to use for non Open Source software, then there would be more Haskell developers and they would end up contributing more to the haskell OSS ecosystem.
Haskell Sucks because you can only define one typeclass instance for a type, and you can control where they get exported.
Solution: Release a new version of the Haskell language standard. Problem: That doesn't look like it's going to happen. 
It does a... little. It certainly can do an approximation of dependently typed programming to a level that surprised me, up to one level of type promotion. That being said, I'm an Agda enthusiast and definitely would love fully dependent types in Haskell. Would highly recommend this talk: https://www.youtube.com/watch?v=rhWMhTjQzsU
how about performance compared to java for Android? why is there java code in that repo, can it be removed? 
If you're using print manually, then Show is probably the best you're going to get. If you're talking about having an actual debugger... then there's no reason why the compiler couldn't auto-derive a copy of Show called Debug and have the debugger use that when displaying values at a break point. So, I guess I agree. But for now, you can at least use StandaloneDeriving.
Right. What I want is what Elm has, which is something like this: showAny :: a -&gt; String with the understanding that it's used only for debugging purposes.
I also fairly often find things I want to be able to add, but not multiply.
We're kind of stuck with GHC as the only real compiler, which is slow and seems to be getting slower. 
Haskell sucks because compilation takes too long.
Types communicate useful information, and do serve *some* of the role that documentation plays in other languages (for instance in Python, some think including type information is a property of good documentation, with support from Sphinx and such). Having correct, natural language documentation to add context to the types can nonetheless provide *tremendous* value (often more value than the types themselves, sometimes not). The problem is that we don't have good systems to help us keep natural language documentation correct. Incorrect documentation often does more harm than good, so the decision to cut losses and omit docs is not clearly wrong - especially when it's unclear how much effort will be available to maintain docs going forward.
`stack solver` and `stack init --solver` will select a well-matching snapshot, and use cabal-install's solver to resolve the rest. Very handy for these circumstances! 
Haskell sucks because there is no built-in support for polymorphic variants/extensible records. Solving the expression problem means a dozen of different hacks.
Haskell sucks because Num is a thing that exists (and is used!).
Problem: When using `-Wall` and getting a warning about missing type signatures of a function that defines a lens, the inferred type signature is often wrong or tends to be very large and complicated.
For lenses and traversals they are compatible because they use the `Functor` and `Applicative` class. Those cover the majority of people's use cases. Once `profunctors` gets incorporated into base they can also add `Prism`s, too.
Just checking: are you using `stack`?
Even though it's "fairly evenly split", there has been no movement for 7 months. I'm not holding my breath. 
Partial solution: use [my `foldl` library](http://hackage.haskell.org/package/foldl). That lets you compute multiple folds over a list in a single pass and provides constant space guarantees for all folds which do not return a collection, even when you combine them.
Solution: swap in the `integer-simple` replacement for GMP which is BSD licensed and extremely close in speed: https://ghc.haskell.org/trac/ghc/wiki/ReplacingGMPNotes 
Maybe work around with (&lt;&gt;)?
It's not built into the language or tooling, but [the doctest package](https://hackage.haskell.org/package/doctest) gives you this in Haskell. 
You can solve the majority of problems with idiomatic Haskell 98. Many extensions i use are for syntactic convenience. What problems are you trying to solve?
That's not a bad suggestion, but not ideal. There is communicative value to stating that I'm adding when I'm, in fact, adding. And there are usually multiple candidate semigroups. To get specific, consider `Dollars`. `(x :: Dollars) &lt;&gt; (y :: Dollars)` is probably adding, but max, min, first, and last are semigroups as well that apply fine to dollars. Meanwhile, of course, `(x :: Dollars) * (y :: Dollars)` is probably meaningless and definitely not `Dollars`.
If you're talking about free applicatives, you need to use [Dave Menendez's formulation](https://www.eyrie.org/~zednenem/2013/05/27/freeapp) [of free Applicatives](https://www.eyrie.org/~zednenem/2013/06/freeapp-2), as it is orders of magnitude faster than the ones in the standard Free library.
Interesting. You have given a method of making any language suck, without changing the language.
Haskell sucks because Stack isn't the default package manager.
Stack and Stackage help with this. But if you are using libraries that are not included in Stackage (i.e. you configure stack to pull these libraries from Hackage or git instead) then stack does not completely solve the problem.
And the misuse of each.
SOLUTION: Make Stack the default package manager for Haskell
Haskell sucks because in order to get domain specific knowledge you probably need to learn using a different language.
Haskell `sdl` and `sdl2` are different because their C dependencies `SDL` and `SDL2` are different. That's an inherited flaw. :P
&gt; mustache. A Haskell implementation of mustache templates. are you sure it's up to date? how to use it with external html files?
&gt; HStringTemplate. A Haskell port of the Java StringTemplate library. no documentation. how to use it?
&gt; Heist (full disclosure: written by me). I haven't been able to set it up to use with Servant and external html files, do you mind providing me an example?
Agreed that noncompliance is a big enough problem to actually be a problem. I've made a proposal to address that more than once, which basically amounts to "move the primary responsibility for compatible-version-range decisions to the upstream maintainer." The full proposal sets this up in an way that doesn't break the existing ecosystem. It's repeatedly been shot down as not worth it because it doesn't *prevent* human error from breaking things (completely true, but it does massively reduce the *number of failure points*). I may try again at some point by just implementing it and submitting a PR, so the required effort is no longer an argument against. :P
I do this and I never knew it had a name!
Right. Turning on a language extension is simple and painless though. What exactly is the problem with that?
Aye. And it takes too much memory too. Its a deal breaker on ARM hardware. For instance, compiling stack takes over a gig of ram (about 1.5 last time I did it), and over 10 hours. Also, cross compiling doesn't work for arm because of template haskell. Its too bad because haskell has good performance on arm once it does compile. 
Then everyone would be complaining about how records aren't parametrically polymorphic 
I thought it was the default by now.
I think Atom with ide-haskell is the closest thing that we have atm. It not as complete as NetBeans or Visual Studio but it's a good start. [ide-haskell](https://atom.io/packages/ide-haskell)
My SML might be a bit rusty, but I'm pretty sure you can have polymorphic fields. It might not work for bare/anonymous record types, but as part of a datatype you should be able to manage. If you mean something like row types, that would be originally missing. However, row types should be retrofittable to a much greater extent than what we ended up with.
Given that Corona SDK uses Lua, and we have GHCJS working for `stack`, it shouldn't be much of a stretch to setup a starter project using Cordova + GHCJS + Reflex on the client, some Wai-based server (Servant would be my favorite) and set it all up with `stack`. This wouldn't be for games, but it would be a fine start for non-game apps. Then wrappers for various cordova plugins would have to be added of course.
Fair point. Maybe I'm just weird, but I feel a bit giddy whenever I turn on an extension, kind of like using a cheat code in a game.
mixfix operators make parsing error message worse.
Coming from a Scala background, the tooling seems terribly cumbersome and immature. I couldn't get it to work in IntelliJ. With Scala, you just install Scala plugin (default one) and SBT plugin and it works. With Haskell, I ended up using an online Haskell IDE because there are like 4 Haskell IntelliJ plug-ins and neither seemed to work. Could be me not doing something right, but should be easier to figure out.
Lenses shouldn't be abused though. They have their costs and overheads.
Template Haskell
In what primary ways would it differ?
and in general, is that worth it?
If you're interested in packages, I have some data. Here are the top 100 packages in hackage ranked by harmonic centrality. It's not perfect, but it's generally a good default measurement of the important of a node in a network like this. The main weakness of this list is that because it's based on hackage, it's heavy on reusable components, and unfairly excludes libraries that may be widely used by end users, but don't tend to be depended upon by other software components. * (`base`,9516.5) * (`bytestring`,5250.073809523778) * (`containers`,5003.845238095219) * (`ghc-prim`,4969.333333333332) * (`integer-gmp`,4787.666666666666) * (`transformers`,4302.3166666666975) * (`array`,4266.250000000086) * (`deepseq`,4175.533333333403) * (`mtl`,4026.383333333369) * (`text`,3818.666666666703) * (`time`,3348.4761904762386) * (`binary`,2914.7119047619326) * (`filepath`,2727.7595238095387) * (`directory`,2674.83333333334) * (`template-haskell`,2563.1428571428582) * (`Win32`,2467.6476190476196) * (`unix`,2381.4261904761893) * (`pretty`,2168.797619047574) * (`vector`,2015.5833333333196) * (`old-locale`,1975.1333333333025) * (`process`,1963.0333333333072) * (`random`,1871.5833333333135) * (`hashable`,1862.3666666666381) * (`unordered-containers`,1791.1333333333205) * (`stm`,1646.485714285694) * (`primitive`,1597.2214285713992) * (`parsec`,1579.6666666666551) * (`network`,1522.6166666666575) * (`transformers-compat`,1456.5333333333215) * (`tagged`,1456.0999999999967) * (`semigroups`,1453.999999999992) * (`old-time`,1334.8523809523733) * (`attoparsec`,1329.7833333333326) * (`exceptions`,1301.8833333333323) * (`aeson`,1238.1166666666659) * (`syb`,1224.699999999997) * (`nats`,1180.8166666666743) * (`scientific`,1118.983333333338) * (`dlist`,998.3500000000129) * (`blaze-builder`,987.6000000000037) * (`void`,969.6761904762016) * (`QuickCheck`,962.4023809523898) * (`monad-control`,886.3833333333373) * (`utf8-string`,880.8333333333372) * (`transformers-base`,866.5000000000045) * (`bytestring-builder`,770.6476190476246) * (`data-default-class`,760.0595238095271) * (`extensible-exceptions`,755.9321428571482) * (`http-types`,724.0000000000007) * (`lens`,715.8333333333331) * (`bifunctors`,706.9833333333353) * (`fail`,703.8166666666713) * (`async`,699.0166666666713) * (`cereal`,698.1845238095258) * (`comonad`,695.588095238095) * (`HUnit`,679.0928571428586) * (`base-orphans`,676.7547619047629) * (`contravariant`,673.1749999999998) * (`case-insensitive`,672.0500000000001) * (`resourcet`,661.4166666666663) * (`base64-bytestring`,655.5000000000026) * (`distributive`,655.2416666666654) * (`network-uri`,651.8000000000021) * (`conduit`,643.4499999999996) * (`lifted-base`,639.2500000000005) * (`zlib`,629.7166666666678) * (`mmorph`,628.1166666666688) * (`data-default`,625.6666666666664) * (`semigroupoids`,622.1500000000005) * (`split`,619.8166666666674) * (`tf-random`,605.3761904761918) * (`profunctors`,604.3833333333338) * (`parallel`,601.759523809523) * (`ansi-terminal`,566.2500000000005) * (`free`,563.3999999999992) * (`StateVar`,558.0626984126943) * (`streaming-commons`,493.5666666666639) * (`blaze-html`,485.4690476190447) * (`regex-base`,468.7476190476163) * (`ansi-wl-pprint`,464.8845238095229) * (`regex-posix`,463.1904761904753) * (`blaze-markup`,450.602380952378) * (`safe`,438.77619047618947) * (`kan-extensions`,427.21785714285585) * (`reflection`,425.58333333333275) * (`http-client`,423.24999999999926) * (`memory`,416.93333333333123) * (`conduit-extra`,398.5999999999992) * (`cryptonite`,389.2666666666651) * (`HTTP`,385.6261904761886) * (`prelude-extras`,385.44285714285365) * (`unix-compat`,380.19999999999976) * (`byteable`,378.9833333333321) * (`data-default-instances-base`,364.7833333333313) * (`cmdargs`,364.5333333333318) * (`optparse-applicative`,363.8166666666664) * (`data-default-instances-old-locale`,362.78333333333137) * (`data-default-instances-dlist`,362.78333333333137) * (`data-default-instances-containers`,362.78333333333137) * (`cookie`,358.51666666666677) 
Same. I thought "is that supposed to look like some logic symbol that I don't know?" 
How is this different from http://hackage.haskell.org/package/control-monad-exception ?
I actually like ::. It is somehow stand out from normal code.
As far as i think lenses are brilliant, I pretty much never use them mainly because I don't like to mix lenses and normal record update and defining lenses make normal record name ugly. so I endup never defining lenses .
[removed]
Sorry, integer-simple is not close in speed. I've benchmarked it as part of my big integer experiment eg: http://www.mega-nerd.com/haskell-big-integer-experiment/bench-integer-20160307.html and when doing the comparison for multiplication I had to drop integer-simple because it seems to be at least O(n) slower. 
You can't do it in pure Haskell. The reference counts will likely be stored in MVars, and those will have to be updated with unsafePerformIO when something new is appended or prepended to them. You write your interface so the user never sees the refcounts.
I love this formatting as well and in practice it does solve the problem. I tend to use it with other programming language . Some languages like Ruby, expect the vomns to be on the same line.
Haskell sucks because you can't partially apply type families.
Haskell sucks because it doesn't have instance chains (or closed type classes), meaning that any type-level computations with associated terms require over-complicated workarounds.
I think there's a section in the GHC User manual that talks about this.
You often endup having more imports and extensions that the actual code itself. It just add noise to the code.
So using unsafePerformIO and MVars I can have it so that `a = drop 1 b` updates a reference count?
It would be great if you can use it without having to write for all everywhere.
Haskell sucks because there's no first-class product or sum types (don't tell me about `(,)` and `Either`, they're ordered and don't allow for row/column-polymorphism).
I personally don't see how it is any prettier than the current situation. 
Are mixfix operator really a good idea ?. For ternary operator $ seems to work well enough.
Its more maeFields which thr solves the problem. It looks like the ideal would be a mix of both.
Haskell syntax. I use both Haskell and Clojure. Clojure syntax is way nicer. I still prefer Haskell as a language though. But a lot of my colleague couldn't deal with the difficulty to read Haskell code.
Regardless of what dependent types offer, Haskell (and really any typed language) pushes towards more and more expressive types. But rather than having an elegant core language which was designed to handle this, we're stuck with System Fc. And so each new Haskell feature bloats the language more, while only ever approximating dependent types. A long while ago, I drew a parallel between this movement towards dependent types in Haskell... and the early 2000's push for *every* language to support OOP. (Even languages which had no business doing so). You get something which technically supports classes, sure. But your language wasn't designed with it in mind, and it takes a long time before everything catches up. Python completely reworked its handling of classes in Python 2 once, then again in Python 3. PHP added phony "classes" at first... and cleaned it up slightly (but in classic PHP style, still leaving it a pile of shit in the end). But what do dependent types bring? It's hard to put it exactly, other than to say that the type system is never a perfect match for our problem domain. We always have some additional knowledge we can't account for. When I change lanes while driving, I don't always check my rear-view mirror. If traffic is light, I might be able to account for the lack of cars in my blindspot just by keeping track of the other cars I see. When I speed, I don't do it willy nilly, but I assess the situation. Perhaps I'm in an unincorporated area between towns or maybe I am driving with the rate of traffic (above the turtle's speed limit law of 55 miles an hour in Illinois). Or perhaps I'm driving behind someone going 95. There's no way a cop would pull me over when he could nail that guy with an additional $100 fine. The rules we follow for safety can be bypassed without harm in many circumstances. And to account for all the interesting ways this can happen, you need the full power of logic.
According to LH authors it's not ready yet for production.
I once had to add more swap space just to install pandoc from source.
Haskell sucks because the Haskell platform is dead . It is really hard as a beginner to know which packages to use as they are usually a few which the same thing. There is no such thing like a standard library (aboart from base). There as popular packages but it's not obvious that they are. It would be nice if we could have the equivalent of the Haskell platform back, is a set of packages that people agree on, that every body use and improve and possibly some mega-preludes so allowing to import everything within a few lines.
You don't prefix all of your functions by the type of the data they act upon example listHead, listTail, maybeMaybe, maybeFromMaybe etc ... So yes pretending type is a problem ...
You can define things in different module but it's not ideal.
SOLUTION (partial): [Data.Monoid.Textual](http://hackage.haskell.org/package/monoid-subclasses-0.4.2/docs/Data-Monoid-Textual.html) from monoid-subclasses.
How so?
That's a great start, but it really should be [less opinionated](https://github.com/elm-lang/elm-package/issues/165).
and a dedicated typeclass for numeric literals `IsNumber`, like `IsString` and `IsList`. 
Re: readily available libs. For the most day-to-day tasks I find the Haskell ecosystem better than e.g. Java's. Where in Java you have to decide between 10 poor implementations, in Haskell we got 1-3 implementations which fit the bill nicely. Well, ofc if your needs are more domain-specific, chances are that Java has an implementation but Haskell isn't.
Python has a built-in module for this too
(partial) solution: use `-O0` for local builds
Haskell sucks because as of today you don't see a stack(back-) trace on simple error like non-total function.
I might be wrong but it seems to me that it requires you to have the `EMT` transformer at the top of your stack. It doesn't have an MTL style class. Also, it's focused on `Exception` exceptions where with my library you can "throw" whatever you want.
Agreed. And there is still hope for Haskell-ide-engine as a universal ide back end.
long compile times - no binary distribution of packages and libraries - at least as far as I know?
That sounds horrific
I would really like to see `(&lt;&gt;)` and `concat` be the functions required by `Monoid`, instead of the clumsier `mappend` and `mconcat`.
When you're writing CRUD apps and 50% of your code is record definitions, it really does turn into a major pain in the arse. 
`bottom` introduces a ton of breakage in Haskell. At this point, most people just ignore it because `bottom` is generally avoided like the plague. If you just ignore `bottom`, 3 of your 4 points are solved. `bottom` is the reason that `Hask` isn't technically a category, which fundamentally invalidates all the categorical abstractions like monads and functors. It's best just to ignore `bottom` in stuff like this. [Fast and Loose Reasoning is Morally Correct](http://www.cse.chalmers.se/%7Enad/publications/danielsson-et-al-popl2006.html).
`GADTs` and `DataKinds` have your back data HList f r where HNil :: HList f '[] HCons :: f a -&gt; HList f r -&gt; HList f (a ': r) This is a list of `f` instances with arbitrary, heterogeneous type parameters. Use `Identity` for `f` and factor it out of the data definition if you're just looking for a list of `*`s.
Do you know an example with Keyboard and Screen like your parent has?
Haskell sucks because you can't call functions `like(this, with, commas)`. It's a matter of personal preference, though. Edit: emphasis on "personal preference"
Just from my experience of using Agda, not sure if it's just Agda's implementation sucks.
Yes, that'll be the [`vinyl`](http://hackage.haskell.org/package/vinyl) library. Very useful, provides lenses as accessors as well.
IIRC, even Idris now treat `if` as builtin syntax because of the quality of error message, so probably not Agda's fault.
It's a big unfair to compare those. Scala is a JVM language and InelliJ is a Java IDE. Of course it's not as easy with Haskell.
I don't like having to write the same program twice: once at the type-level and once at the value-level.
Both this (and Atom) require setup on behalf of the user, so they're not "all in one" solutions. This might be addressable for Leksah at this point; an installer that sets up both it and the Haskell Platform would fill this role.
`generics-sop` also provides much of the machinery of `vinyl`, but more focused on generics.
How much effort has been put into making them better though?
If the language you're using comes with a nice debugger, you can set a (conditional) breakpoint and evaluate expressions to understand what's going on and when things start to go wrong. Or you can set the breakpoint at the very beginning of the code in question and follow along, making sure the values of local variables/fields correspond to your mental model of what should be happening. When I work with Java, I often find a problem this way, use "drop frame" to reverse time a bit (of course this won't work on side-effects), change the function body, hot-swap and step into the function again.
There's a very nice IntelliJ-based IDE for Python or JavaScript (or C++ even). With Project Rider (C# IDE) JetBrains started exploring the idea of separating the front-end (in Java) from back-end (in C# in this case). Maybe that would be a good match for a Haskell IDE.
&gt; If you just ignore `bottom`, 3 of your 4 points are solved. The lack of distinction between inductive and coinductive types leads me to believe that it is indeed impossible to define the natural numbers. Bottom or not.
The hackage page says : `Uploaded : Fri Jan 22 23:28:28 UTC 2016 by justus` seems up to date enough for me... The documentation in [Text.Mustache](http://hackage.haskell.org/package/mustache-1.0/docs/Text-Mustache.html) *starts* by saying how to get it to compile external template files... And the Mustache language reference is linked in the package description.
Yes this is a major problem for me. I run on ARM dedicated hardware for all my servers and using haskell is simply a no-go because I can't get anything to compile properly. Also GHCi doesn't work on ARM either.
I really enjoyed reading the proposal for [modular implicits](http://arxiv.org/abs/1512.01895) in OCaml. Basically, it's based on explicit application when there are more than 2 candidates in scope.
Haskell sucks because the standard library is separated into so many packages, every single module I write starts with a dozen or so imports.
Haskell sucks because the performance of a piece of code is often both fragile (seemingly unimportant changes have a massive performance impact) and hard to predict.
Haskell sucks because both the standard library and the ecosystem lack guiding ideas. The former often feels like it is an amalgamation of PHD theses that have been abandoned soon after being complete.
Haskell sucks because Cabal hell.
Subtyping is easily overdone and a lot of libraries already have completely non-understandable monad transformer stacks that you interact with via a plethora of lift functions. In addition trying to figure out in what context you are is not always trivial... e.g. in case you need to do more low-level stuff with those transformers. A full effect system cannot be overkill for a general purpose language.
[Here](https://blog.jle.im/entry/mtl-is-not-a-monad-transformer-library.html) :)
Tell me more. What's the problem with using `Num`?
Solution is well... proper polymorphism (NOT conversion) for string-like types. Until then, good libraries provide their modules in multiple variations.
That's what I thought. Thanks =)
It's primarily related to laziness, not the functional part. That's also a reason Idris has chosen to be strict (for now).
Hear, hear! As a beginner/intermediate Haskeller, I feel overwhelmed by all the language extensions. I don't have a sense of which ones are useful/stable/popular, and when someone else's code uses them, I either spend hours getting sucked down a rabbit hole trying (and usually failing) to learn them or I ignore them and find the code confusing and hard to read. I don't understand why the good extensions aren't built directly into the language, and I worry about the monoculture of only being able to compile with GHC.
Pretty simple example: typesafe printf _without_ additional syntax, TH magic or type wrappers. Also see https://www.youtube.com/watch?v=fVBck2Zngjo
once again: where is a single example of what I need? a master/parent html page with layout and child html pages inheriting it?
I am not aware of any `Symbol` concatenation function in the [TypeLits module](https://hackage.haskell.org/package/base-4.8.2.0/docs/GHC-TypeLits.html). Where are they?
I agree with this. Personally I would love to have a simple namespace system in Haskell, so I could use `Person.id p` and `Customer.id c` without being forced to create two modules in two files. This would mitigate the problem of duplicate names not only in records, but also with top-level functions. With OverloadedRecordFields on the other hand, I simply don't see how `z = name (p :: Person)` is a major improvement over `z = personName p` or `z = Person.name p`, especially if this ends up with wobbly type inference or none at all.
That's because functions are curried by default. How would you write partial function application with your syntax?
Yeah you can. data FiniteList a = Nil | Cons a !(FiniteList a) What you *can't* define is the type of lists (or anything else) which are guaranteed to not be bottom. (Apart from `Int#` and such which are extremely restricted and not particularly practical.)
You can introduce your own class Regexable and do orphan instances or have your own data type Regex and do smart constructors for the supported String types or whatnot. Just pinning your library to one string-like type is just laziness (not the good one).
`like(this)(with)(commas)`?
The default however is lazy, including the majority of libraries. Opting out is not that easy, especially when the problem is not even in your own code and you cannot make the whole library function call strict.
Go has a similar system for examples as part of the documentation that actually get run. It's very nice and I agree the lack of examples in Haskell code is challenging especially for beginners coming from other languages. 
There exist `instance Eq Float`, `instance Eq Double`, `instance Ord Float`, and `instance Ord Double` even though these are lies and (per IEEE754, due to NaN) do not obey almost any of the laws you would reasonably expect. (They are definitely not "totally ordered", which is what's explicitly mentioned in the `Ord` documentation.)
POSSIBLE SOLUTION: Use Prelude replacements such as [subhask](https://github.com/mikeizbicki/subhask#numeric-hierarchy), [algebra](https://hackage.haskell.org/package/algebra), and [numeric-prelude](https://hackage.haskell.org/package/numeric-prelude).
Orphan instances should be banned and are not. The rules would need to be a bit subtler than the obvious. (See Rust.) With just `FlexibleInstances`(!) you can [violate coherence](https://www.reddit.com/r/haskell/comments/2agy14/type_classes_confluence_coherence_global/civ6y1g) *without* writing any "obvious" orphans.
The use of *the C preprocessor*(!) for Haskell code (for conditional compilation, among other things) may have been an appropriate short-term solution at one time, but by now it's a bizarre and painful anachronism. 
It is a shame. I think if we had them most of our hand wringing about effects would disappear.
Yeah, I have also been bitten by this once, when using data types from third-party libraries whose fields were unnecessarily lazy. This is in my experience especially a problem with records that don't have a `NFData` instance. For me, this is more a problem with libraries than with the language itself. In my code I tend to use strict record fields, unless there is a good reason not to, and `seq`/`deepseq` in a few strategic places is often enough to get reasonably efficient code. Maybe laziness shouldn't have been the default, but it also doesn't keep you from writing efficient code.
That's ridiculous. Absolutely every single mainstream language I have ever used has had orders of magnitude worse design than Haskell.
That question is a bit too open-ended to answer adequately. I haven't personally used Servant yet, so I have no idea what your code looks like or what assumptions I should start with to construct an example. That being said, here are links to two Heist tutorials: http://snapframework.com/docs/tutorials/heist http://snapframework.com/docs/tutorials/compiled-splices The first one focuses on the template side and shows some of the tools that Heist provides for abstraction and keeping your markup DRY. The second one is a complete example that is actually in use as part of Heist's test suite. See here for the full code: https://github.com/snapframework/heist/blob/master/test/suite/Heist/Tutorial/CompiledSplices.lhs From a high level there are two main things that you need to do. First you need to initialize Heist by calling the [initHeist function](http://hackage.haskell.org/package/heist-0.14.1.3/docs/Heist.html#v:initHeist). This returns a `HeistState` that you will pass to the template rendering functions. Here's an example of how to call `initHeist` (taken from the above compiled splices tutorial): let sc = mempty &amp; scLoadTimeSplices .~ defaultLoadTimeSplices &amp; scCompiledSplices .~ splices &amp; scTemplateLocations .~ [loadTemplates baseDir] heiststate &lt;- initHeist $ emptyHeistConfig &amp; hcNamespace .~ "" &amp; hcErrorNotBound .~ False &amp; hcSpliceConfig .~ sc Now that you have a `HeistState` your application should keep that and make it available to whatever function does the actual template rendering. Then you call [renderTemplate](http://hackage.haskell.org/package/heist-0.14.1.3/docs/Heist-Compiled.html#v:renderTemplate) and pass it your HeistState and the name of the template you want to render. Here's a simple example: functionWhereYouNeedToRenderTheTemplate = do let (runtime, mimeType) = fromJust $ C.renderTemplate hs "index" rendered &lt;- liftM toByteString runtime -- ... now do whatever you need to do with rendered To insert dynamic data into your templates you will need to create "splices" and include them in the `scCompiledSplices` line when you are setting up your splice config. See the above tutorials for more details on how to do that.
&gt; nobody seems to realize it It is likely that there are less Haskellers that care about writing documentation compared to other languages, but this is far removed from "nobody". Moreover, most Javascript libraries I tried to use had thorough documentation that sucked. Input format is unclear, output is at best an example. Same for documentation of JSON apis, etc. I would take a readable type signature over that every day.
Biggest downside here to the size of the Haskell community is open source contributions. You're lucky if the person interested in your project in the abstract is also proficient enough in Haskell to contribute code.
Yeah and as an example you almost always get some trivial signature like map :: (a -&gt; b) -&gt; [a] -&gt; [b] to show you how easy it is... How about some real code: parseTimeOrError :: ParseTime t =&gt; Bool -- ^ Accept leading and trailing whitespace? -&gt; TimeLocale -- ^ Time locale. -&gt; String -- ^ Format string. -&gt; String -- ^ Input string. -&gt; t -- ^ The time value. It's the comment that tells you what that Bool is there for, not the type... 
Recursive modules are not allowed without annoying `.hs-boot` files and `{-# SOURCE #-}` pragmas, unless something has changed since the last time I checked. (One potential solution would be to deconflate the unit of namespacing from the unit of compilation (from the unit of filesystem organization).)
&gt; Once profunctors gets incorporated into base Will this ever happen in the near future?
Maybe it's just me, but I'll take types over examples any day; examples can be helpful if you have literally no idea how to use something *and* the thing you want to do happens to be similar to the example the documentation author had in mind while writing, but (compared to types) they're usually terrible at communicating what the library is capable of more generally, eg. what its limits are.
I hate that most of the time you can refactor `[f x, f y]` to `map f [x, y]`. But you can't do that with `[show 1, show True]`. 
`Hask` is still a category, even with bottom (it may fail to be a category for some other reason), unless I misunderstand what you're saying. The problem with non-definability of natural numbers has nothing to do with `bottom`. And, most importantly, **bottom is unavoidable**. It's not like you can just remove it from Haskell. You would have to remove general recursion as well, but then you get a completely different language.
Lens isn't just "fix the record problems". It does a ton of things that rock, and that I can't imagine not using now. It requires a lot of dependencies, that are in two groups : small "algebra" dependencies that are mostly maintained in parallel, and stuff like `bytestring`, `containers`,`filepath`, `hashable`, `text`, `unordered-containers`, `vector`, etc. that are used in most projects.
&gt; I'll take types over examples any day I'll take both. :)
Stack does use the Cabal library under the hood, but usually when I hear people say "Cabal" they mean `cabal-install`. 
You can call functions like that if `like :: (a, b, c) -&gt; d` :)
Haskell creators have tried very hard not to make a successful language, instead they focused on mathematical purity. That has, in part, contributed to its success and its failures.
For all practical purposes the PVP *is* SemVer with an extra number stuck on the front that gives you more flexibility. There may be other small differences, but the spirit is the same.
Then you'd get tons of people complaining about the obscure terminology like thy do with Monad. You can't please everyone.
What is the HalVM?
I think this may be the basis of the new Shake build system.
&gt; But why do we need an extra version number? It allows you to communicate more information to your users if you choose. You might use 1.0 to denote a significant stability milestone. Or you might have bumps in A indicate something else like /u/edwardkmett does. &gt; I think the PVP gives flexibility that isn't needed. Then don't use it. Just stick with A = 0 for all your packages and be done with it.
The very latest versions of ghc have a working ghci, FWIW. But you have to have the exact version of llvm and only the major version of llvm is indicated on the ghc website. I got it working but I had to change from debian stable to debian unstable. Still doesn't solve the other problems though. 
Haskell is the sole exception, here. The actual source of the syntax is type theory, there `a : A` has denoted the judgement "a is of type A" since forever. And the reason the initial Haskell committee diverted from that is because they thought that cons would be used more often than is-type, which, as we all know, is plain wrong.
The compilation model (used by GHC) for polymorphism means that abstraction isn't free. If I write `data Foo = Foo !Int`, the `Int` is stored unboxed in `Foo`. (That's if you even remember to use strictness annotations.) But if I write `data Bar a = Bar !a`, an `Int` is stored *boxed* in `Bar Int`, which requires allocating at construction, following an indirection at access, and has at least a triple space overhead: the indirection itself and a header for the garbage collector's convenience. Memory bandwidth and caches are increasingly the bottleneck for performance-sensitive code these days and this sort of space-waste, pointer-chasing and fragmentation is not very advantageous. (You *can* work with unboxed data in Haskell but it's cumbersome and requires duplication and type system effort -- `Data.Vector.Unboxed`, data families, and so on.) This is necessary because polymorphic code has to be able to work with data of *any* type, which is solved by making every type have the same in-memory representation -- a pointer. A possible alternative would be to use monomorphization together with "intensional type analysis" (note: nothing to do with "intensional type theory" as a form of dependent types), wherein anything whose types can be instantiated statically is unboxed (in other words: anything that would be unboxed in C++ or Rust, in other words: anything you could *define* in C++ or Rust), and anything where the types *can't* be known statically (polymorphic recursion, existentials, higher-rank types) would have the size and alignment of the type stored or passed in as runtime values with memory accesses calculated based on them at runtime. This would have the effect of making first-order polymorphic code (everything in Haskell 98 minus polymorphic recursion) exactly as fast as monomorphic code, //probably// in exchange for making higher-order polymorphism (existentials, `RankNTypes`, and polyrec) slower.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/hackernews] [Why does Haskell, in your opinion, suck?](https://np.reddit.com/r/hackernews/comments/4f6lck/why_does_haskell_in_your_opinion_suck/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
This is a good thing! You must be explicit when you introduce equalities between things.
You could use the C preprocessor, though that's a rather ugly solution.
GHC is an all-around fantastic compiler that produces code that's been very reliable in production. If I had to point at one *major* problem with it though off the top of my head, it'd be compilation times, which are already worrisome in medium+ codebases, getting slower with each release. This is a real problem and I hope it will gain more share of mind going forward.
Well, if you write your types like `Elem Int sigs, Elem Double sigs, MonadSigs sigs m` instead of `MonadSignal [Int, Double] m` you do get a lot of the usual type machinery working for you.
I know your point is on knowing what libraries to use, but operationally speaking, I have a feeling Haskell Platform is being superseded by stack and, due to it by default, Stackage as a set of packages you can use off the bat. I agree however that a good and in-your-face findable resource listing good packages for common tasks would be great for beginners.
But they introduce the need for template haskell :(
I wonder whether that one could be reasonably easily fixed. I just go `:back` manually until I see those parameters.
SOLUTION: include Control.Lens in base, give its syntax special treatment by the compiler. Basically, make Haskell (sort-of) object-oriented. Edit: okay, if you don't like making Lens special, how about some general way to specify in a module such as Lens how type errors should be presented.
#
Haskell sucks because the plotting libraries don't have the default elegance and simplicity of ggplot. Numerical computing is also difficult when the major libraries, like hmatrix, lack common algorithms like sparse matrix multiplication and sparse SVD.
Neat, I guess it's time to refine my actual GSoC proposal related to diagrams :) I do have a concern however - I've enrolled in GSoC as well, so if I do get in, then it'll be somewhat of a clash - what am I to do about that? Is it possible to have my GSoC application (which is about binding sympy to Haskell) to be "transferred" under the Haskell GSoC? Also, thanks a ton for everyone in the Haskell community for doing this for students like me who want to spend summer writing Haskell. You guys are awesome and I LOVE you people &lt;3
SOLUTION: separate the mathematical definition of the results of an algorithm from its optimised form, let the compiler prove they're the same. Also, it would be nice, although hard to implement, to just give some "hints" about how to optimize an algorithm and let the compiler do the work! Example: I'm working on a project where values of nodes in a directed acyclic graph are calculated based on the values of their parents. Easy to describe with a short recursive function, but: values could be calculated multiple times in a single run if there are "diamond" dependencies, and changing the value of one node and then getting the value of a descendant node requires a lot of recalculation, since the whole graph is now different and memoized values don't apply. I'm using explicit cached values in each node, making sure modifying a parent invalidates the children's caches, and using State in the algorithm to get a value. This is harder to read. It would be nice if I could just tell the compiler to cache the values and say what invalidates the cache, preferably in a safe way where caching *can't* change the semantics even if I make a mistake.
I hate the way function signatures are written: add :: Integer -&gt; Integer -&gt; Integer Is that a function from integer to a pair of integers? Or a function from a pair of integers to an integer? Or a function from an integer to an integer, then to an integer again? (whatever that would mean) It would be much clearer to define the function as a mapping from a pair of integers to an integer like other languages do. This kind of notation is really unnecessarily exposing implementation details (i.e. that we can represent all functions as taking just one argument) to the programmer, thereby sacrificing readability and clearness.
Got it.
Why should it be free? The authors have written (are writing) /an excellent/ book which I value far more than the fifty bucks I spent. I hope they get filthy rich selling millions of copies. 
&gt; Haskell sucks becuse it does not have the DoNotSuck language directive. For a type theorist, that's awfully _classical_ of you...
The brew installation on Mac has too many THINGS. I just want one package that's like `haskell`. Edit: Or better dependency management in brew.
I was hoping someone would mention this one. I seem to be the only one around here who grumbles about Haskell's crummy modules. Also, type class coherence and modularity are on opposite ends of a tradeoff, and Haskell predictably decided to discard modularity.
&gt; Haskell sucks becuse it does not have the `DoNotSuck` language directive. I think you rather mean the `-XNoSucks` flag and there's probably a spaceballs joke in there somewhere...
Who the hell programs in CPP?
[Haskell for Mac](http://haskellformac.com/) is pretty nice, it's not a full IDE environment, but nice for getting up and running without worrying about much.
I've never touched Haskell, so "no real call stack" looks mysterious to me.. :) What does is mean? 
So what are those? (Greenhorn, ho!)
There needs to be some solution to this problem that does not depend on social pressure or policy--a technical solution rather than a social one. I'm not smart enough to know what that looks like, though tooling alone could certainly compare package versions and see if the types and functions they export have changed at all in type. Such tool could see that `Mymodule.f` was `Int -&gt; Bool` and is now `Int -&gt; Int`. That would certainly be a breaking change. Function semantic changes would not be caught by such a tool though. Tooling changes might help solve problems; social and policy changes will either solve little (due to non-compliance) or solve problems only by imposing drudgery.
Haskell sucks because IEEE floating point evaluation isn't deterministic/pure. (I.e. http://blog.ezyang.com/2011/05/haskell-not-pure-enough/ .)
Well, our company uses it, so as far as we are concerned it is ready for production (and we are aware of all the bugs)
To the contrary, this is *always* what you want if you *use* those types. You do this to save memory and achieve speed. Nobody wants an overflow check after each operation that could throw an exception. Please use Integer.
NO THANKS. They are sponsoring a really sketchy conference that many people in the functional programming world [have spoken out against](https://statement-on-lambdaconf.github.io).
I understand there is a problem, but I don't think it is specific to haskell, is it ? What was specific to Haskell , was to not have sandboxing and complain at compile time, instead of crashing later without anybody understanding why. What I mean, is yes packages dependencies are sources of issues but we are far away from the true Hell which was cabal hell.
Can anyone explain why the `backwards` function for bijections uses `Arrow` instead of `-&gt;`? Edit: Or is it supposed to be `-&gt;` in the first code snippet, but Arrow later on?
&gt; Haskell is the sole exception, here And sadly Purescript followed it for some reason... :/
I was following your argument but at the end you said "you need the full power of logic," but that didn't follow for me in what you were saying. It sounded to me more like you needed some kind of flexibility in applying rules, and using human judgment in knowing how to judiciously apply rules. I don't think that can be defined purely in logic.
Harper's example of unsafety doesn't compile and hasn't compiled for ages. harper.hs:9:10: Class `Typeable` does not support user-specified instances. Failed, modules loaded: none. 
I had a teacher who called binary searching with printfs "sandwich debugging" because it resembles the sandwich theorem in mathematics.
For some basic guidance to the ecosystem see https://github.com/Gabriel439/post-rfc/blob/master/sotu.md and http://haskelliseasy.com.
[Here](http://hledger.org/developer-guide.html#implementation-notes) is how hledger models some related things, maybe it will give some ideas.
We really could use a Haskell equivalent to LLVM.
Seconded. Was a bit annoyed that installing ide-haskell is not enough though. You need: ide-haskell haskell-ghc-mod autocomplete-haskell ide-haskell-stack language-haskell and maybe haskell-hoogle. Also don't forget to install `stack`, and to `stack install ghc-mod`, and to setup the paths in the plugin configuration. Hopefully I didn't forget anything. 
While HaLVM is an interesting project, it doesn't really help me. It's designed for the Xen hypervisor, and if that's available I might as well just use Linux right away. The type of systems I'm talking about is small microcontroller, without an MMU (and even less virtualization extensions) etc. Right now I'm hoping on Rust to grow mature enough to replace C in that space.
It would need a lot of fixes to work with current GHCJS, but I started a project for said wrappers a while back: https://github.com/mtolly/hs-cordova
Sure, thanks.
Which is why Bool arguments are a code smell in every language, including Haskell. The things tend to proliferate if you let them too.
Haskell could use an official style guide. Languages like python and java have them. Most haskellers just tend to write code which looks elegant to read, and this works most of the time, but it can be confusing for beginners. The reason I think this is important is because you tend to read code faster if you're used to reading it in a particular style for years.
I think this post might fit better in /r/haskellquestions than here
If you do a lot of code reviews you'll start to feel the pain. If someone deletes `c`, they will modify `b`. You'll find yourself spending a lot of time looking at `b` to be sure nothing else changed on that line. Later when you're tracking down a bug and you are wondering when and why `b` was even added it becomes harder to track down the commit that introduced it. Python and Perl and Javascript and Ruby do not have this problem. It also makes rearranging lists in a text editor faster and less error prone not to mention commenting the lines in and out of the code. 
Interesting. I would say the exact opposite. Hoogle and Hayoo are some of the most powerful doc tools I've ever come across. 
You could define your own overarching modules re-exporting the ones you use frequently. You have to be a bit careful to avoid name collision but once you've taken care of that, it's all good.
Don't ALUs do overflow checking on every operation anyway, and store the result in the flags register?
Yes, we are using it. I also have been using it in one of my open source projects, too: https://github.com/Gabriel439/Haskell-Succinct-Vector-Library
Indeed... but at least that would be uniform across monomorphic and polymorphic code.
Agree, but I could see it being hard for compiler to deduce in foreseeable future and if you are using Haskell you already gave up on programmers being self-disciplined enough to not make mistakes. This is the thing you will meet in production code and for that we need real documentation.
I'm not sure what you mean. Product and sum types are ordered by definition. Unordered pairs, say, would require quotient types and I'm not sure how that would work.
How would you chain your OO style calls? Chaining normal functions is very easy and clean with '.'
I've complained about for nearly a decade if not longer in other languages I've used. There is a reason python and perl consciously allow for trailing commas. Thirty years ago most shops didn't even use VCSs much less code reviews of commits. I remember introducing CVS to my first employer. So some of the advantages have probably become more visible in the recent decade or so. &gt; How has this "problem" gone completely unnoticed for 40 years then? Maybe I'm misreading your tone, might be the scare quotes. But I'm getting a sense of defensiveness here. This isn't a huge complaint. It's a minor thing that we think sucks about Haskell in a thread meant to point out things that suck about Haskell in our opinion.
Good, good, you even mitigated the "which string argument is input and which format" trap that was there:) I wonder if there is some final form of Haskell code where not even one generic String/List/Bool/... type is used? How cool would be if all libraries were like that! Of course documentation contains more than explanation of type signature, but everybody knows that.
No problem, Schould I ask there another time or can this topic be moved to haskell-questions 
Are you sure? I don't know about :print but ghci complains if you try to evaluate a value with no Show instance. 
There's probably no need to move it now, this is just my suggestion for future posts of a similar type. 
That's what I thought too... I was hoping based on your phrasing that there might be something in between I hadn't heard of, but oh well. :)
Not sure about Python, it would look like that maybe: def parseTimeOrError(time_locale,format,input,accept_whitespace=True): and this kind of optional arguments is what I personally like in Python.
I've been trying over the past couple weeks to write an efficient implementation of Finite Difference Time Domain Electromagnetics using Repa, and I've gotta say, its performance is a complete cipher. Looking at the docs is really discouraging - "make everything strict, inline everything, and hope GHC and LLVM figure out together what you want to do." I look at the Core, and there are, like, 49-deep stacks of inlined calls to the same function, most of which I know to be no-ops and I assume are slated to be optimized away at some point (or perhaps it's dead code that LLVM's Optimizer needs to be there as some kind of hint?) Repa is obviously an impressive piece of work, but I'm sort of at a loss for how to use it without an intimate understanding of GHC's _and_ LLVM's optimizers and how to cajole them into emitting the C++ I should have just been writing in the first place.
That's only a temporary issue, as sooner or later all maintained packages are becoming part of Stackage. Being included in Stackage shows commitment of a package authors towards its users. If a package is not in Stackage it's very likely to be abandoned or unmaintained and should be avoided anyway.
But why chain infix calls when you can just chain normal function calls much more cleanly with '.'
I'm glad to hear liquid haskell is being used now. The last time I looked at it (I asked on [reddit](https://www.reddit.com/r/haskell/comments/3g9nx6/anybody_using_liquidhaskell/) last summer) that's what I go &gt; Well, TBH there are a couple of issues that make LH still too difficult to use when programming. The main one is the very poor story for integration with modules &amp; cabal. 
Haskell sucks because it doesn't have an easy-to-use macro system like the Lisps do. While non-strict evaluation does get you perhaps half of the benefit, that only works for abstracting over **terms** (expressions), and not over **declarations** (top-level declarations). There's Template Haskell and Generics, which just don't have the ease of use of Lisp macros. A macro system would be a great addition, given all the things that we use TH or Generics for today (e.g., automating boilerplate declarations or extending the `deriving` mechanism).
As I understand, some stuff might indeed get stack traces in GHC 8, but all non-total functions would need to be modified to explicitly accept that call stack. I might be wrong, need to check out again. Anyway, I know that more work is still planned for future releases only.
Except... no one declares functions that way?
Yes.
Thanks for the excellent explanation of "wolf fencing." I think however that it proves to be a red herring—people do the same sort of binary search with breakpoints in debuggers!
I love Haskell but actually gave up the day I tried to upgrade my system to a newer GHC version. Haskell sucks because the dependency hell and workarounds the dependency hell is worse than the worst Rube-Goldbert machinery ever invented. Simply setting up Haskell, libs, stackage, and whatnots is a very particular case of dependency hell which makes any single other language ever invented since computing exist make look like something sane. Difficulty of reasoning about space/time complexity: I could probably live with. But that dependency hell: simply trying to "upgrade" to a newer version and get everything working, the whole cabal / stackage / stack: baffles the mind. It baffles the mind how a langage so great and powerful can offer such a pathetic dependency hell experience. I've quite simply never ever experienced anything that bad in any language in 30 years' coding time. It's one of the greatest mystery of computer science that'll leave me confused until my last breath. I saw a talk semi-recently about a 19 years old who bought an IBM mainframe and went into the trouble of installing an OS onto it (which took him weeks, if not months). It looked easier than install a working Haskell setup. I miss so many features of Haskell... But for my own sanity one day I decided to pull the plug and never ever deal again with Haskell's dependency hell. Best. Decision. Ever. It's really sad. What's even more sad is that many Haskellers shall answer: "but wait, using stack everything works fine, nothing to see here, move along". 
&gt; The State monad is not a monad Uh, is this something I need to be aware of in practice?
"Proprietary Software", is software developed with non open source compatible licenses. So it can't be linked with LGPL because the lawyers are afraid we would loose some patent protection. gmp is an LGPL big number library used by GHC to support the "Integer" type. The ghc generated binaries link against it. There is actually a pretty decent work around, https://ghc.haskell.org/trac/ghc/wiki/ReplacingGMPNotes The problem is that integer-simple not the default or optional out of the box on any distribution of ghc, like Haskell Platform, Ubuntu, redheat etc... I would have to maintain my own build of ghc (which the lawyers would have to sign of on as well) for every host that this code could be built on. That would effectively be another job :)
I don't know. It's pretty nice automatically deriving `ToJSON` and `FromJSON` and not having to write those instances by hand and the aeson docs (I think correctly) encourage this, but we're already onto generics then, yes? It's easy to overlook how ubiquitous recommendations are to use various language extensions.
I think you've voiced what I fear is one of the worst warts about Haskell in particular for new haskellers. I was just talking about this on this subreddit last week and I am surprised it doesn't seem to get brought up more often. It's a really ugly aspect and seems to indicate that pure Haskell 98 is a tough fit for those of us seeking convenience and productivity.
Haskell sucks because the unjustifiable, mediocre elitism of his community. the community is leaded by computer scientist and careerist wannabee CSs, that try to avoid the success of the language at all costs. Not only because Haskell is their toy and because it would limit their freedom to do with it whatever they want, but because it is a menace for their power and influence over the entire software industry. They want Haskell as a research language and a cult language, not an useful language in which plebeians could program. Currently, the "I know Haskell" badge is like a nobility title in the world of Software Engineering. And these kind of people want it to keep it as it is. Because in the deep, they know that they are bad programmers unable to do real stuff. I do not know any case, of any academician that solved a real problem ever. With the exception of the people that created the language (The Simons etc). With the rare exception of very few haskellers, there are more interesting and practical work done using monads in fsharp (or Scala, by the way) in a few years of existence than in 20+ years of Haskell. Yet, haskellers criticize the syntax of these languages and ignore with contempt the practical realizations of what they were unable to achieve. This makes me remember the Chinese proverb "when the wise man points at the moon the fool looks at the finger" The suppossedly industrial Haskellers are busy devoting an inordinate amount of effort to "cutting edge problems" like getters and setters (lens) loops (folds, "streaming") and extensible records. As well as avidly carrying flawed ideas and designs coming from other languages and translating them to haskell without using any functional principle. But hey! they use lens and Foldable and Traversable. That is all that haskell is about isn't?. Dependent type programming is a second degree cult within haskellism, with the idea that brute force -more strong types- is what software needs, instead of the ingenuity that they lack, with the superstitious belief that it will supposedly solve the problems that they did not were able to solve with real functional programming. The CS community downplay and silence anything that goes out of the academic box. Useful and practical work is buried regularly in a mountain of useless abstract libraries, over-designed nothingness and category theoretical mambo-jambo. The #haskell channel is a diary show of what a group of obsessive compulsive haskellers can do folding lists and other trivial stuff done in contorted ways and telling us how much they know about theory, and how much pretty is the theory, and how pretty is the Haskell language. No interest on real problems. The only thing that matters is to takeoff from reality and making their egos levitate. These insane attitudes are not diminishing, but growing with time. Sadly this is not going to change in the foreseeable future. 
I suppose that will heavily depend on how smart the compiler is.
This is a common question about Haskell. I think the proper, only slightly exaggerated answer, is this: * There isn't really such a thing as a "heterogeneous" collection. Why do I say this? Because you can't process such a collection in a correct manner unless you there is some set of operations that you can *safely* and *uniformly* perform on *any* element of the collection. And this amounts to the same thing as all elements of the collection having the same type. So type-safe solutions to the "heterogeneous collections" problem generally involve pushing the heterogeneity to the element type, not the container type. There are a handful of common solutions: 1. Use a sum type that enumerates all the alternatives. (Only works if there's a finite set of alternatives that are known at compilation time.) 2. Put *behaviors* into your lists (e.g., functions, actions or records thereof), not objects. E.g., instead of making a class, make a record type whose fields are the methods you'd have put into the class. Now you can make a list of those. (Taylor's `[show 1, show True]` example, dislike it as he may, is a version of this—it's a list of the homogeneously-typed `Show` behaviors of the heterogeneously-typed values `1` and `True`.) 3. Use existential types (like Edward's answer).
It is not progress. It solves no problems I have, so why would I downgrade to a worse tool when it doesn't help me in any way?
And here again we have someone who thinks that just by using negation one is already classical. Hello, intuitionistic logic has negation too.
&gt; The root of the problem is that they're indexed by a list of types when they should be indexed by a set of types. This strikes me as far from obvious. There are advantages and disadvantages to the list vs. set approaches. In particular, there are some applications where it's useful to exercise some control over the memory layout of the fields of a record type, because for example it's tied to how you serialize or present them. I can see two approaches: 1. Have record types be indexed by a list of fields, but provide some "congruence" mechanism that allows for automatic coercion between record types whose field lists are permutations of each other. 2. Have record types be indexed by sets of fields, but provide some additional mechanism to support first-class runtime field orders for applications that want them.
Check out the [`semigroups`](https://hackage.haskell.org/package/semigroups) package, which provides `(&lt;&gt;)` as a method of the `Semigroup` class.
Not a solution, but sometimes can help: compile with profiling and use `RTS -xc`. OR: Add [some code around `main`](https://www.reddit.com/r/haskelltil/comments/3p9x4e/better_stack_trace_on_failure/cw50a2i) to always get stack traces when profiling is enabled.
[removed]
Every language I've seen has imports that take up a ton of space. I'm pretty sure most editors let you collapse them too.
IME you reach for `FlexibleContexts`, `FlexibleInstances` relatively quickly. `RankNTypes` is quite quick too if you try to use lens as first class arguments or type synonyms. `RankNTypes` also pops up in many other scenarios. 
Just because `IO` exists does not mean you cannot use more specific types or type-class constraints.
That is totally possible; it's just really icky to have to opt in to such a basic debugging feature. It's also totally impracticable for long running server code that encounters exceptional conditions.
Haskell could have mixfix, and have this benefit for other functions too!
Branching on it is, but nobody said it needed to always be checked, just that haskell gives us no option right now. Its just always silently overflow.
I explore one potential solution: [qualified-prelude](https://github.com/lspitzner/qualified-prelude). It has a significant list of disadvantages, and i cannot put it on hackage (it inherently has a strange relationship to the pvp). But it has proven rather useful so far in my own projects. 
&gt; That sounds horrific Looks at /u/hastor's name... Everything checks out.
Haskell sucks because FFI is torture. Embedding Haskell as a scripting language requires shipping 1GB of GHC. Some say they've done FFI with C++ libraries but they are either geniuses or liars. Take Elm, where FFI is a breeze. Now you have all of NPM to build upon.
Ah, I see. Thanks.
Good point! 
I've never heard of this problem (lack of hierarchical modules)... in fact, I'm not sure in which way Haskell modules are not hierarchical. Do you have an example?
Those are all valid concerns and I'm not saying that you only ever need set-indexed products/sums. List-indexed products and sums have their applications, but I'd like to have a choice. Often I don't care about a particular ordering of elements but am forced to deal with it in my code.
&gt; Dishonest advertising You're missing the point of the advertising :) Those adverts are to programmers in other languages for whom Haskell is so close to satisfying those claims that it doesn't matter that they are not literally correct. The adverts are not for type theorists.
But what is the benefit of doing "chaining" over composition (which I would do `insert 3 . insert 2 $ insert 1 empty`) or prefix application?
My Haskell experience is limited to an introductory course, but [thunk leaks](http://blog.ezyang.com/2011/05/anatomy-of-a-thunk-leak/) made me scared to use the language further. The particular example I remember is that when adding up a big range of numbers, either `foldl` or `foldr` would cause the sum to take a linear amount of memory, because of lazy evaluation. I don't remember which fold it was, and of course that's part of the problem. I'm willing to put up with lots of complexity in a language, if the language can catch my mistakes quickly. That was how I felt about the type system in Haskell -- yes, it was difficult to learn, and I made tons of mistakes, but I didn't have to worry about those mistakes sitting in some dark and untested corner of my codebase waiting to bite me. The compiler took care of all of that, and my programs were safer in the end. But not so with gigantic thunks. My code could very easily compile and pass tests without discovering a memory bug like that. The bug might be in code that only runs on Tuesdays. Or it might only happen if an attacker decides to pass an unusually large value in some random field. It felt like avoiding this kind of mistake in Haskell was going to take _constant vigilance_, which was the opposite of what drew me to Haskell in the first place.
indeed! but... 1. usually the template file's extension is `.hamlet` 2. usually the closing `&lt;h1&gt;` tag is omitted
I managed to set up the application window so that folks would at least know if their GSoC applications were accepted before they had to decide whether or not to apply to the Summer of Haskell. Given there are far more GSoC slots than SoH slots, and the need for a delay while I ran around trying to secure some funding, this seemed like the best decision at the time. It, unfortunately, creates a conundrum going in the other direction. If I had to guess, it probably wouldn't work out very well. Personally, you'd have to give up a by-then-sure thing for a maybe, and the net effect would be one fewer student working on Haskell for the summer.
If we can find a sponsor that wants to kick in for t-shirts, I'd be all for it. ;)
The way Rust does it, a `char` is always 4 bytes, but a `str` is UTF-8 on the inside.
Thanks for doing this. Test driving nightlies, betas and/or very recent releases make it easy for the rest of us to use 'm, and help to keep an open and well trodden upgrade path.
Yeah that's what I said, compilation times aren't getting any better. I'm not saying GHC is bad, just that if one has a problem with it, there aren't really any alternatives.
Not sure what you mean by that. Like a standard "backend" that handles all of the compilation after something like GHC's Core or STG steps?
Working on it! :)
You can define two records in one module that have the same field names if you use declareFields (but watch out for https://github.com/ekmett/lens/issues/643).
&gt; What's even more sad is that many Haskellers shall answer: "but wait, using stack everything works fine, nothing to see here, move along". What's sad about it? Anyway, a non-stack solution: use cabal freeze
Good Lord! How do you imagine this to work? sum (map (*3) some_numbers) Do you want the Haskell runtime to store a tuple (result, overflow_occured) or something? Because, if you don't every arithmetic operation becomes an IO action that potentially alters the bit in the ALU you want to have access to. The other alternative would be exceptions. I really do not understand what people are thinking. With `newtype`, you have a powerful tool that allows you to implement a fixed size integer type that is close to the metal and yet has the semantics you want. 
Haskell sucks because the only truly first class datastructure is the list. Arrays and hashtables are second class citizens and look ugly in Haskell.
Yeah, that works. I usually want to define one record per module, then provide another module one level up that re-exports all those single-record modules. That means I have to define the lenses in the higher level module. Which means I can't use the lenses in the lower level modules. This problem gets worse if you have two (or more) groups of modules like this. Then the common ancestor is way higher. 
The polymorphism will be the key thing I think. Good luck!
Stack installs on Mac and Windows too, doesn't it?
Solution: EKG. Well, partial solution.
I agree. Not having stack traces by default is a terrible misfeature of Haskell.
I assume so, but I was referring to how stack doesn't work on unix systems other than the most popular linux configuration.
I understand that Stack and Platform are aligning. 
Oh, I wouldn't expect asymptotic differences, only constant ones.
I normally feel much better about the lens/prism approach when I already have other reasons for having the various pieces combined at some level - usually when I'm dealing with something like the expression problem. I'll second ElvishJerrico's response: tout away. Ether looks awesome, so I'll start touting it as well when it's relevant (possibly as well as classy lenses/prisms where relevant). 
Mostly because of package ecosystem: cabal/hackage/package doku: Because Cabal can't uninstall packages and generally is not a good package manager. (Pointing out it is not a package manager is not making a point against the claim it's a bad package manager, quite the contrary) (Maybe a solution: Let Nix manage the cabal packages for you) Because Hackages names have nothing to do with the Name you import it with. Because a function signatures are not documentation. No comunity I'm aware of in any language thinks so, static languages included. Documentation is Text. Ideally supplemented with examples. And the function signature, but that's just a tiny part of it. The whole Language feels very baroque. I don't know why. I can't point a finger to a feature and say "that's bad" (besides cpp). It's like the c++ of functional languages: A ton of features, and each of it almost perfect and thought out, but together just a bit to much. Agda is IMHO a way more powerfull (even too powerfull) and yet is a way smaller language. Because it changes so fast. IMHO Haskells biggest weakness, also it's biggest advantage. It's still very awesome, expcially the syntax is wonderfull.
If multiplication is not meaninfull or usefull, that doesn't sound quite like addition. Either use a monoid, or implemnt you own operator for "something like add", such as unicode ⊞⊕⊚ or $+ or &lt;+&gt; or #+# or whatever you like. Don't overdo this, thou.
&gt; Sounds like a special case of Nix I wouldn't even claim that. It's just (somewhat) better than `cabal-install`. At least you won't run into conflicts with it.
If there is any research in that direction I would appreciate some papers, as I thought about something like this very often. In particular I think it is worth challenging the very idea that evaluation time is "best-effort" by the RTS and OS. Maybe the problems concerning the predictability of runtime behaviour are just a symptom of a deeper problem that we face with all (?) current languages: Execution time is unpredictable. Now you might say, that is inevitable, as inferring tight upper execution time bounds is not possible (automatically) for a turing-complete language, but maybe there is a hack: we could make time (somehow, the devil is probably in the details) _explicit_ in the language and so guarantee that we are terminating (i.e. our language is total). In this world _all_ programs are implicitly hard real-time, if you do not want the behaviour, you would just give ridiculous time constraints. It probably is also heaven for schedulers, as they get access to explicit time constraints which might help to increase performance. There was (or still is?) research into total functional programming, which does exactly this (restricting programs to provable terminating programs), but I have never seen this idea combined with an idea of execution time.
Sometimes I **want** non-exhaustive pattern matches. For example, if I am working on a case expression for [Exp from haskell-src-exts](http://hackage.haskell.org/package/haskell-src-exts-1.17.1/docs/Language-Haskell-Exts-Annotated-Syntax.html#t:Exp), I am only going to be able to implement a few matches at a time. I would rather have a warning telling me that there are more matches to implement, than to have no warning with a catch-all match that just calls error. In the second case I have to manually remember that the case is incomplete.
I manage just fine with RecordWildcards
I don't know Scala. What is their syntax for this sort of thing? fourArgs :: a -&gt; b -&gt; c -&gt; d -&gt; e fourArgs = ... twoArgs :: a -&gt; d -&gt; e twoArgs a = fourArgs a 2 3 final_e = twoArgs 1 4 I could see calling `twoArgs` with one set of parentheses because it looks like a function (though now the compiler needs to break it up under the hood and move the arguments around, which could get messy), or with two sets of parentheses because the arguments passed in go to two different functions (though now the human needs to learn about the internal implementation of `twoArgs` in order to use it, which screws up encapsulation). I suppose this syntax with parens and underscores would make sense if syntax for point-free function representations didn't exist, but I'm having trouble imagining them working together. I'm not saying there isn't a good solution to be found, but I don't see it yet.
Confusable type arguments like `Bool` make it easy to mix up the meaning of true and false, and so are likely to introduce bugs. Transposable arguments such as `String -&gt; String -&gt; t` make it easy to mix up the order of arguments, and so are likely to introduce bugs. If it's been made easy to get wrong, it'll be gotten wrong.
"It's difficult to extol the benefits of English when people make so many awesome books in French" I don't think anyone is disputing that it's _possible_ to construct good software without static types.
Unfortunately that is the reality. integer-simple uses a linked list to hold `Word` values while my implementation uses arrays from `Data.Primitive`. Just moving back and forth through the list could easily explain the difference in comparison to array based implementations. 
Diff tools should be able to highlight the changed portion of a modified line, which in this case would highlight that only the trailing comma was removed. The bulk of the code reviews I do are through Atlassian Stash, and its diff tool does just that.
It is rarely the case that there's no tradeoff whatsoever :)
I'm still pretty new to Haskell and still pretty limited in my usage of it. I was pleased this weekend, for instance, when I could write an API client without having to consult any documentation, so I'm only recently at a point where I can write code that does anything. With that said, my gut feeling is exactly in line with what you've said: I think at some point another language is going to come along and it's going to use the things we love about Haskell and also smooth out many of the things that we find frustrating. My further guess is that you could fork GHC, build a giant standard library out of the most common and successful 3rd-party libraries, and also include by default many of the most commonly used language extensions and satisfy some of these complaints.
Solution: use ide-haskell in Atom. Problem: [Not a simple setup](#d25teqn) 
Sure, but: &gt; Haskell sucks because it does not have the DoNotSuck language directive. In logic, that basically means "¬ ¬sucks -&gt; sucks". Unless you have a suitable justification for that particular proposition, I assumed you were using double negation elimination. You could afford to be more charitable in your reading. I'm well aware of intuitionistic logic.
Wow, thanks! That's a typo. It should have been the same as in `B0.hs`: -- B0.hs, line 27 type (&lt;-&gt;) = Bidirectional (-&gt;) (-&gt;) I fixed it, thanks for the catch. Would you like me to explain why I shifted it to use `Arrow` in B1?
Either way, `bottom` really ruins Hask's usefulness as a category. That wiki page has more examples, but the worst one is probably that it's nearly impossible to create a monad when considering `bottom`
Right, glaebhoerl's proposed `Unsafe` is a superset of `IO`, but not the other way around. It's possible to have an IO interface that doesn't allow violating type or memory safety.
In addition it would be nice to have `Int 32` instead of `Int32`, because it's not uncommon to need `Int 27` or `Word 1024` and so forth. `RULES` could probably allow you to recover the right code when the size is one that is commonly implemented in hardware.
I recommend the [Haskell programming from first principles](http://haskellbook.com) book for learning.
Thanks for the clarification! I misread your original comment as "Static types are not beneficial because many people make awesome software without static types".
the fault is mine, the comment really could have been actually clear to my intentions :)
Not much of a solution for polyglot programmers. They are so used to IntelliJ keyboard layouts and feature set that anything else is much less productive.
I often use `Control.Arrow.(&gt;&gt;&gt;)` import Control.Arrow ( insert 1 &gt;&gt;&gt; insert 2 &gt;&gt;&gt; insert 3 ) empty
Is there any practical way around this? (Without giving up the ability to compare floats)
Well, that's sort of correct, but what's often desired is something like [Show], that is a list of types that all implement Show, such that the show function could be called safely on all of them. This is of course a trivial example, but more complicated (and useful) ones could easily be imagined. There are solutions using phantom types and such, and I understand the implementation reasons why it can't happen, but it's still an annoyance that something that seems so simple can't easily be done.
The introduction covers them: https://int-index.github.io/ether/ I should probably add this link to the package description.
Except the former case makes it explicit that you're partially applying a function.
How do you solve the expression problem using polymorphic variants/extensible records?
Haskell sucks because the standard libraries use standalone functions rather than typeclass functions with default implementations. This hampers things like making a symbolic algebra type that implements `Integral`, since `gcd` can't be reimplemented to make the results symbolic for that type without a horrible hack.
ref: [Adding structure to monoids: thus hopefully ending Haskell's string type confusion](http://dl.acm.org/citation.cfm?id=2503785)
Now that the dramaturgy is right, I think I understand. You use `Arrow` as an abstraction over functions, so that the composition can be unified to `Kleisli` (which implements Arrow).
Haskell sucks because `import qualified` isn't the default.
Why are language extensions more scary than importing libraries?
I thought that only pattern-match failure exceptions needed to be updated and the `error` function. Everything else is defined in terms of those atomic pieces.
&gt; Not a solution, but sometimes can help: compile with profiling and use RTS -xc A handy short-hand for testing with stack is `stack test --trace` which builds with profiling and passes `-xc`. The counterpart is `stack bench --profile` which builds with profiling and passes `-p`.
To clarify: acid state allows you to use any haskell type that can be serialized as your database.
Another problem with default-extensions is that it is easily forgotten when moving files to a separate project. Some extensions are also incompatible with standard language features like CPP and multi-line strings.
Haskell sucks because it's a moving target. I have been supporting an application for a customer for 6 years. During that time a lot has happened: * API changes in libraries so your program doesn't compile anymore. * Libraries change in ways so that your program compiles but fails later in some way. * Libraries being orphaned. * Changes in base so your program doesn't compile. * New cool language features and/or libraries that make your old code feel antiquated. * Subtle changes in GHC affecting laziness surfacing bugs in your code. This makes supporting a long-lived application a bit more energy consuming, but it's also a big part of what makes Haskell interesting. Also, I suspect this would be a problem in any language to some degree.
I would actually say that Haskell has excellent amounts of documentation compared to some of the more popular language ecosystems.
I use ghc-mod, and it not only works blazingly fast, it also integrates with emacs very well. Too bad sometimes it breaks on stuff like GHCJS.
That would be an awesome news.
Eh. You can get a blog up and running pretty fast. Or build a web API and host it on a DigitalOcean account.
There definitely should be some kind of "I want to do *x* with Haskell" list that explains what you need.
They are working on making Semigroup a superclass of Monoid, yes.
Here's a link re EKG. As I said, it's only a partial solution. https://ocharles.org.uk/blog/posts/2012-12-11-24-day-of-hackage-ekg.html
Seems broken. The columns are all mixed up. Maibe it works better with a specific reader?
Yeah, the title page tends to get broken up a little. There seems to be a single line that is mixed up, but the rest seems to be in the correct order. EDIT: I see what you mean, several of the figures are inserting themselves inbetween text and code, resulting in some pretty confusing text.
One could argue that Floats shouldn't be compared since comparisons might yield unintuitive results if the float contains any sort of rounding error.
&gt; Keeping A at 0 gives the impression that the library is not "production ready" Bumping the major version unnecessarily gives the impression that version numbers are entirely marketing driven. It is fairly common to have mature packages on Linux with major versions of 0 and they are rock stable.
Maybe it is not my job to fix a tool I have no use for?
So, this exists and is ready to use? When is the switchover going to be?
Glad you asked! There are a variety of solutions and all of them suck: * What Haskell does: lie and pretend that floats behave sanely when they do not. * Don't provide `Eq` and `Ord` instances for floats at all, and instead provide separate `eqFloat` and `compareFloat`, or similar. * Throw IEEE754 out the window and say that NaN == NaN. This lets you get a law-abiding `Eq`... but `Ord` is still thorny because you have to decide where NaN goes in the order (before or after infinity? or zero?) and none of them really make sense. * But IEEE754 *does* have an [officially-defined total order](http://stackoverflow.com/questions/20097380/iee-754-total-order-in-standard-c11), which makes this choice somehow. But it'll be much slower because it's not implemented in hardware, and it won't work for algorithms which have been written with the normal NaN != NaN semantics in mind. * You can have separate `PartialEq` and `PartialOrd` traits, which provide the operators but have much weaker laws, along with `Eq` and `Ord` which subclass them and add the equivalence relation, resp. total order requirements (and e.g. the `compare` method). Then floats implement the former, but not the latter. This is probably the cleanest solution of all, but having all this complexity in the comparison classes essentially *just* because of floats and NaN is annoying. (This is the solution Rust went with, and they don't like it, in my opinion because they haven't tried any of the others. In any case, I think this is what Haskell should have done.) * You can make it impossible for NaNs to exist by aborting or throwing an exception or whatever instead ("signalling NaNs"). Then you can implement all the classes in good conscience. But as far as I'm aware it's generally not so easy to predict and guard against NaN in actual numerical code (which is why it exists in the first place -- it was thought preferable to have some NaNs in the results after a three day simulation run than to have it suddenly abort on the third day), so the practicality of this is questionable. * NaNs fundamentally come from operations involving zeroes and infinities (along with things like sqrt(-1)). And infinities arise from overflowing the range or dividing by zero, and zeroes come from, among other things, underflowing the precision. So you could do away with not just NaN, but infinities and negative zero as well -- over- and underflow as well as dividing by zero would just abort, similarly to overflow-checked integers. This idea sounds appealing to me *personally*, but I haven't heard of anyone who's tried it, it's even more speculative in terms of practicality than signalling NaNs, and it'll probably be slow too because you have to add a bunch of checks on top of the hardware operations.
No, I find that overwhelming. The PDF is over 350 pages long. Haskell sucks because the background knowledge typical programmers lack when learning the language is enough to fill a textbook.
I would try asking on the [agda mailing list](https://lists.chalmers.se/mailman/listinfo/agda). There are quite a few academics on there who would be well qualified to answer your question and probably don't hang around Stack Overflow or /r/haskell very much.
"compose f with g" means "g . f"; I think this accounts for both of your questions. It's "f first, then g", which is unfortunately written in the other direction when notating function composition.
I feel like a spammer now. I've asked.
Here is a [list of open issues](https://ghc.haskell.org/trac/ghc/wiki/Backpack#Backpack-relatedtickets). It seems that /u/ezyang is working on it.
You'd think that with all the information about a function provided by the type signature, and all the propaganda around “reasoning about code” haskell would have the gold standard refactoring tools. 
Here's the proposal for the future fix: [Semigroup/Monoid](https://prime.haskell.org/wiki/Libraries/Proposals/SemigroupMonoid) 
For starters, you don't tend to see so many language extensions on other programming languages. This is very much a Haskell/GHC thing.
&gt; I can build a linux binary on my mac, copy it and run it on a linux server with one command and without configuring anything And your sysadmin hates you because all those security holes are now baked into your application, including ones in common C libraries with lots of updates like openssl.
&gt; For example, things like record field punning are fine as an incremental change to the status quo, but adding them to the language standard feels like giving up on more serious efforts to fix records. What are the problems with field puns in regards to record fixes?
Sure, but if you accept that Haskell is different from most other languages (which it is) why is this a *bad* thing rather than just another different thing? (I accept that there are many correct answers to this question. I'm just curious what yours is.)
You are the one complaining about lack of support in the tool for your favourite niche platform. Either you fix it yourself, pay someone to fix it or you use one of the platforms where other people did the work to implement support. Of course, not using the tool is another option but I don't see the point in complaining about the lack of support for the platform you are running in software you don't even intend on using.
Solution: partial pattern matches are an error by default, and there's a compiler flag to demote them to a warning (like type errors).
So making it difficult to make static binaries or cross-compile is now a feature? I'm not here to debate changing your workflow, I just want all builds (static, dynamic, cross-compilation) to be easy. 
Though based on other choices, languages can still offer different levels of flexibility. e.g. in Go you can't define a method on a type from outside its package. In Rust you can impl a trait for a struct if you're in the package that defines either the trait or the struct.
Yeah. They're listed as one of the sponsors. They were the only ones who didn't pull out after the controversy hit. Julie wrote up her reasons [here](http://argumatronic.com/posts/2016-03-29-LambdaConf-sponsorship.html).
I'm reminded of `-fglasgow-exts`, which was obsoleted IIRC because it made it hard to know which extensions were actually being used in the file.
&gt;You are the one complaining about lack of support in the tool for your favourite niche platform No I am not, what thread are you reading?
[removed]
I haven't actually tried cross-compiling with GHC, but I have done cross-compiling in general, so I can say with a reasonable about of confidence that you should look into Nix if you want to easily cross-compile (also note that Clang/LLVM is a native cross-compiler, so you should be able to have the LLVM backend emit code for any supported platform). If you need help with Nix, I'd highly recommend #nixos on Freenode (it is full of wizards like /u/cleverca22).
Yes, I'm very sympathetic to that opinion. It would be nice for "Haskell" to form a more cohesive whole.
The only thing that is worse would be a single "string" type used for both binary data and unicode grapheme clusters that is so ubiquitous that it's used even when it's performance characteristics are unsuitable.
Even then, unless you call the type `AcceptLeadingAndTrailingWhitespace`, the behaviour of the flag is still ambiguous, and relies on external documentation (or reading the test suite) to fully understand.
# This. I need to be able to pull compiled dependencies down, not recompile the world on every system that does any compilation. I also need to be able to dynamic linking simply, including "transparent" (non-ABI) change to a common library. E.g. for a security issue. Say what you will about JVM and .Net, but they got this much NAILED. It's also a mostly solved problem in C, though sometimes a bit platform specific (ELF vs. PE/COFF).
Saying both "Define X has blah-blah-blah." and "Expose X to other namespaces" is not repeating/duplicating yourself. Having explicit export lists doesn't violate DRY. Listing lens names in the export list isn't code duplication.
It would also be more in line with the C99 specification, where the compiler/library *has* to provide int32_t, but can also provide int27_t or uint1027_t.
&gt; Second the syntax is incredibly complicated for what it is : just a string processor. Well, that's just not true. You might only be using TH for that, but it's capable of quite a bit more.
Would that kind of syntax work with variadic functions. Also with functions that might not be explicitly variadic but that you want to use as partial functions.
[removed]
What? How is this a solution? Isn't the only downside to requiring foo $ do ... is that it's a bit more verbose?
&gt; I cannot speak for any of haskell library authors, but to me it does not seem at all that haskell encourages using utf8 code, to me it seems rather opposite that it enforces using Strings consisting of Unicode points. Cassava, the CSV library I mentioned, only works with UTF-8 files. If your text file is in any other format, you're in a bit of a bind. Equally, of the many regex options, the PCRE-Heavy package converts to UTF-8 and then back to UTF-16, and therefore the validate-input package also does that. I'm not sure, given the rules of normalisation, that that conversion is always reversible, it's not documented. Additionally I've seen a a couple of open source projects use bytestrings in conjunction with the bytestring-utf8 package as their representation for text, instead of either `String` or `Text`. The irony is that in a language famed for type-safety, using raw byte strings is not safe (what happens if someone imports Char8 instead?). As for the link, it's possibly a bad choice, the motivation behind OsStr in Rust might be better. The point being, `Text` should be the default everyone, and a CSV parsing library should be reading `Text`, not random bytes in a hoped for order. 
Is it too late to get on this thread? This is more process / culture oriented, but I think that as a community we're hopelessly over-focused on monads. I remember a talk Conal Elliot gave maybe 7 years ago now, about how the end goal was referential transparency and predictable, understandable programming. Monads simply bring us back to imperative programming. When I look at a lot of the good functional code I've written at work in Scala (simply because it was available, not because it was easier – Haskell is easier to code in than Scala), I find that much of it could have been written with just functors. Applicative and Alternative functors are particularly useful. So when we onboard people into Haskell, why can't we just hand wave the IO sin bin, and just get to the nice things you can do with the more declarative styles of programming?
I won't argue with you for apps that need recompilation, but it is worth noting that any old Haskell binary can, due to the fact that most Haskell binaries are statically linked, be executed years after the fact. I was recently talking nostalgically with a friend of mine about my [first Haskell program][OscFinder] (written in 2011) and noticed that, as I was new to version control at the time, I had included the compiled binary in the repository. I downloaded the binary, and, after doing some LD_LIBRARY_PATH shenanigans (because I run NixOS, which doesn't have the linker, libgmp, or libffi in the usual places), it ran flawlessly, despite having been compiled with a version of GHC that begins with "6". [OscFinder]: https://github.com/taktoa/OscFinder
At least as well as Haskell's. Partial application works (slightly) _better_ than Haskell, since functions that take (and produce) multiple arguments don't need specialized `(.)` to compose different arities or to uncurry all your functions. The problem with variadic functions would be with their types or with the lack of type classes (IIRC).
I've done this and I am neither a genius or a liar. It was tedious but mechanical. Edit: I called C++ functions from Haskell. I have no idea about the other way round.
&gt; What can you do that you could not by generating a string of valid Haskell and then parse it ? Execute it, potentially interacting with the execution environment of the compiler, or physical devices present at compile time but missing at run time. &gt; I don't understand why the output can just be plain text, and let the compiler decide afterwards if it's valid or not. The reason a splice does not just result in plain text (and instead must be isolated syntactic element [expression / declaration / whatever]), is so that parsing can continue without evaluating the contents of the splice. If a syntactic element could span both the inside and outside of the splice, parsing would have to pause while the splice was evaluated. (That roads leads to [PHP](https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=site%3Aphpsadness.com+%22Implications+for+Internals%22) and [Perl](http://www.perlmonks.org/?node_id=663393).)
This is something I am awaiting with trepidation. Would that help using tools such as prof on haskell binaries ?
I suppose the answer is here https://phabricator.haskell.org/D1517
Hi there! I'm working in a library that can gives you something like you want. It is a bit aggressive because it is going to give an instance for all the type dependencies in a given type, but it works pretty well. It is very very unstable but it can helps you a little. This is where the code is https://github.com/CIFASIS/megadeth I am using it to generate Arbitrary instances, but I also create the Show instances with help from the derive package. And no documentation yet, but you can write me if you want.
[removed]
Could you tell me what (##) does in the Hamming problem example. I tried looking it up it Hoogle but got nothing. 
&gt; For example the first line generates something like &gt; `class $className a where` &gt; but as a class can have a context, you need (ctx[]) even if you don't need it. Well, that can be fixed by introducing a polyadic variant of classD, I suppose. Polyadic functions (and the type class machinery behind them) is not exactly the most idiomatic Haskell, and I'm not sure anyone was doing them when TH first came on the scene.
&gt; It should not also contain control flow -- "IO exceptions". Use Either! Some people really want asynchronous exceptions. And the `IO` model supports them where the `Either` model doesn't. :/
That kinda ties in to the third point yeah. And is there any defensible use case for *catching* asynchronous exceptions?
Is there a way we can test if our `nm` is broken, and an already created bug report that I can copy+paste to submit to Apple? (Multiple people sending the report gets it more attention).
In some sense, Haskell is a fixed-arity language, but the arities aren't syntactically evident until typeclass resolution happens. Also, `(5 6 4 &amp;) 4 3 $` is the resolution that will always happen, though if `&amp;` is a 1-argument function that pushes its single result onto the stack, this is equivalent to `5 6 (4 &amp;) 4 3 $`.
&gt;IHaskell does not support Windows
&gt; is there any defensible use case for catching asynchronous exceptions? I would think so, actually. I think catching user interrupts could be useful in an interactive program; e.g. during evaluation of an expression in a REPL so that you are returned to the read part instead of exit the program. For resource exhaustion, your program may have some "knowledge" that the RTS / VM / OS doesn't, that would allow you to free some resources and restart a computation. For timeouts and speculative computation it's less likely, but I could imagine some scenario where you might be doing some iterative refinement and when the alarm goes off you want to do some last, quick transform of the best result so far before exposing it to the rest of the program. (For example, the iterative refinement works best when all the refinement steps are in one `ST` context, but you'd like to return a value not in `ST`.) Java already knows asynchronous exceptions are [fraught with problems](http://docs.oracle.com/javase/6/docs/technotes/guides/concurrency/threadPrimitiveDeprecation.html). Haskell should really get rid of them as well. I'm sure not long would pass before an Monadic/Comonadic solution (e.g. `InterruptT`, `SuspendT`) that worked even better than asynchronous exceptions in most cases came to light and had us begging for idiom brackets even more than we do now.
You'd have to explicitly lay out the extent of printf's arguments but it would otherwise work.
Oh cool, I guess I'm ok? Max@maximilians-mbp ~&gt; nm --version LLVM (http://llvm.org/): LLVM version 7.3.0 Optimized build. Built Mar 8 2016 (10:26:42). Default target: x86_64-apple-darwin15.4.0 Host CPU: ivybridge I'm using Version 7.3 (7D175)
How would you go about doing that?
No, I was wrong and was sort of guessing based on what `llvm-nm.cpp`'s source code said. It might have actually been `--help` that gives that. In any case, the basic answer is this: if you're running the latest OS X and XCode, then yes, you're affected. But the workaround is easy.
I stumbled across [Koka](http://research.microsoft.com/apps/pubs/default.aspx?id=200436) the other day, which goes down the row-type / effect rabbit hole.
&gt; Haskell sucks because FFI is torture. &gt; [..] &gt; Now you have all of NPM to build upon. I bet you haven't tried calling native code from JavaScript. Haskell's FFI is a walk in a park compared to that. Now if only calling Haskell code from C/C++ was as easy as calling C from Haskell...
Ah ok, yeah it was `--help`
/u/pigworker best worker
`##` is defined in the `where` clause. It merges two sorted lists into a single sorted list, eliminating repeats.
you can't do this: module Blah(module Foo) where module Foo where x = 5 y = Foo.x --- module Baz where import Blah.Foo 
[removed]
Is the user's guide published somewhere? I'd like to read about TypeInType.
First one: `(1+2)* (1+2+3+(5-4))` or `(1+2)*(sum [1,2,3, 5-4])` Second one: `sum (map (+1) [1,2,3])` Haskell doesn't have a vararg sum, so using lists instead. Naturally, the second example in rpn could have been `[1,2,3] {1 +} map sum` to avoid using splat (but I was demonstrating `\+`).
It refers to an infinite family of values with different, but fixed, arities. The `printf` in `printf "%d %d" 2 4` is a different value than the one in `printf "%d" 2` (taking this uncommon perspective). Of course, in practice, this perspective is a bit silly.
So how would you deal with it in a postfix language?
Alright cool, that makes sense. The way `printf`is made to have variable arity is somewhat strange, and it actually being variable arity is not trivial, so would the compiler for your postfix language just have to do some legwork and detect variable arity functions. Because outright banning or replacing such functions is actually not that easy, as it would involve arbitrarily limiting what things can be instances of a typeclass. 
Kinda hard to take people seriously when they squawk about how inclusive they want to be while on the other hand trying to play the fascist thought police with a list of people they don't want to include. &gt; at least when I was at school, where supposed to be places where ideas were free to be expressed and challenged. That hasn't been true in a while. Kids are graduating from places like Harvard thinking that free speech is something that needs to be destroyed. They are too dumb to realize that means their own speech may be censored as well.
http://blog.functorial.com/posts/2015-12-06-Counterexamples.html
Ah, yeah Haskell does some weird typeclass magic instead.
If you need line numbers, you may be better served by [megaparsec](http://hackage.haskell.org/package/megaparsec) or [trifecta](http://hackage.haskell.org/package/trifecta). They provide position tracking by default. Attoparsec is meant for more lower level use cases where line numbers aren't necessarily relevant, like parsing binary formats, HTTP requests, etc.
Some more [here](http://stackoverflow.com/a/36294331/477476) and [here](http://stackoverflow.com/a/36440023/477476).
You could try [servant-client](http://haskell-servant.github.io/client-in-5-minutes.html).
I thought they could only really let more things unify...
Here's my 2c for not supporting row types in GHC, taken completely off the top of my head. Aside from the various PL design issues, I think that supporting row types in full is a lot of unnecessary runtime overhead; and trying to eliminate said overhead is quite tricky. For instance, if you have a function F of the following signature: {lab: b | r} -&gt; b -&gt; {lab: b | r} Assume this is a setter function, for instance. Notice that its signature says that it will work on ANY record, provided it has a field named "lab", of ANY type. That's quite a lot of polymorphism. At runtime, there would need to be some kind of a dictionary to help indexing the records (such that all records indeed behave similarly). Another approach is to treat F like a function template: any time it is applied to a record value with a statically known type (e.g. {lab:Int,x:String}), the application can be replaced with a specialized function, say G. Sometimes this is not possible to do. If blindly followed, such a process would introduce code bloat. (BTW, if you'd like to play with row types, you could try Ur/Web.)
Partial orders have uses beyond just floats. Consider LVish. Not sure about PartialEq...
Since the csv data anyway comes encoded in some encoding, the approach to first convert them to Text and the parse would not give much - you would still have to specify the encoding, - but you won't be able to read csv which contains fields with different encodings, or when the byte strings do not mean to have any particular encoding at all. The problem with "Text everywhere" approach is just this - the are many tasks, where encoding of data is irrelevant, so converting all data in to Text and then back is just extra work. And when the data happen to contain data which is encoded differently than you assumed the program will fail to handle them.
Sure. Which is also part of why I think it would've been the right solution for Haskell. But there's nothing else besides floats that's nearly as ubiquitous to basically *force* you into making this distinction. Partial orders are also useful to e.g. order containers by inclusion instead of lexically (`subhask` does this) but it's a much more niche use case. If you're prioritizing simplicity over completeness and generality, you wouldn't even think of adding the `Partial`- superclasses if not for IEEE754.
online reader link for the ... https://docs.google.com/viewer?url=https%3A%2F%2Fkarczmarczuk.users.greyc.fr%2Farpap%2Fquantfun.pdf
It doesn't work because `s` is the String representation of a list of numbers. Try `read`ing it as a `[Int]` and then `fromList`ing that. Otherwise, prepend `"fromList"` to `s`; this works because of this instance, which we can find in https://hackage.haskell.org/package/vector-0.11.0.0/docs/src/Data-Vector.html : `instance Read a =&gt; Read (Vector a)`
That would require initially more specific type signatures for most functions in `Data.List` that you usually use for `String`s. I don't know how that would work without deprecating its usage for strings in `Data.List` and adding the type restricted versions in a hypothetical `Data.String`. Maybe that's worth the pain, maybe not.
I wouldn't know either, if you don't share a gist ..
I'm reading the release note and my feeling is "I don't have any idea of the usage for this, but it may be interesting, except that I don't understand anything". I'm too late for the "why haskell sucks" thread, but I feel that it gives the idea that you need many PhD in computer science to understand basics stuffs. This may be improved by giving more examples in the documentations. For example, the `Data.Bifunctor` module is really simple, but the first time I saw it, I closed my browser with "too complicated for now", when a simple exemple such as `bimap reverse length ("hello", "world") == ("olleh", 5)` may had give me a strong hint on the purpose of this module.
&gt; 1\. Can you tell me how haskellbook.com is better compared to LYAH? More content? More exercises? More in depth? Yes to all of those. The haskellbook.com is structured as an actual learning material, while LYAH is more of a loose collection of various tips and tricks from the author. &gt; 2\. LYAH is kind of a language reference book. Does haskellbook.com the same thing or does it teach how to build something? What is the book's approach? No, haskellbook.com teaches you how to build something. The approach is that of traditional language teaching: start with small "Hello, World"-style programs and work your way through the language features like variables and case expressions to more complicated programs. &gt; 3\. Is there books that skip Haskell basics and show how to build bigger app? What are the best this kind of books? This is what the book *Real World Haskell* is meant to be. It is a bit old by now and some chapters will do you a disservice. There should be a list somewhere online on which chapters are still considered "up to date" with modern Haskell libraries and practises.
Seems like you are currently using a very old version of `vector` from 2011 or earlier. The `Show` and `Read` instances of `Vector` changed with `vector-0.9` from explicit instances to instances derived via generics. I guess the generic ones enable the list representation.
&gt; There are valid use cases for exceptions where you do NOT want them to be Either. Could you describe some?
You'd pop the first argument off the stack; you'd read and analyze the string, which would tell you the number and the type of arguments you expect to receive; you'd pop that number, format them, and then push the resulting string. The only difficulty is typing the resulting function, and haskell uses type classes for that.
It's `seq` that really "ruins" things. 
Thank you!
Here are the release notes from 8.0.1-rc2 because the rc3 directory doesn't seem to have the doc/ directory. https://downloads.haskell.org/~ghc/8.0.1-rc2/docs/html/users_guide/8.0.1-notes.html
I guess everyone's been waiting for https://downloads.haskell.org/~ghc/8.0.1-rc2/docs/html/users_guide/glasgow_exts.html#duplicate-record-fields Unfortunately, the docs are not up-to-the-mark in explaining what's possible, and what's not. Does anyone have a better explanation?
SOLUTION?: [Backpack'16](http://research.microsoft.com/en-us/um/people/simonpj/papers/modules/backpack-2016.pdf)
it's not DRY, it's repetition. Also, suppose I have header.st and a footer.st. And what's next after all? How to read them in Haskell and return a result to a clinet? I've asked a few times already and still hear "go there, find that, what you need should be right there". Where?
is there any simple example which I take, compile and run?
git-bisect is a similar way to see which commit caused the bug.
Does that affect `module`-level expressiveness like this or just layer on top of it at the package level?
So no one even attempted/planned? I've only used them in PureScript and I found them really great. On the other hand, when you talk about effects, Idris just handles them with the sheer power of its dependent typing, so maybe, given Haskell' drive toward dependent typing, this will help with effects in the future?
I thought I was the only one in this niche! There may be dozens of us!
Isn't it possible to simulate it with type classes? The syntax is different but the possibilities similar it seems... We define a class: class HasName a b where name :: Lens b a So that: {name :: String | w} Becomes: HasName String r =&gt; r Can we unlift type level strings? So that it is possible to: Has name a b where name :: Lens a b ?
I'm guessing Backward Compatibility.
What do you consider the "most blatant Pythonisms in Jinja"? Would you consider documenting, at least in a general way, what subset of the jinja2 template language `gigner` does support? (I recently needed a template system for a weird time-crunch project that heavily relies on using `set` in the template. It wasn't clear that `ginger` would support that, so I just used jinja.)
In case he/she doesn't show, I'll give it a go: "The slow and unresponsive maintainers of `cabal-install` should have added a feature for deleting your SSH keys years ago. Yet another example of the colossal failure of `cabal-install` and the utter superiority of `stack`."
Old version of the `vector` package...
I don't think either of those are what OP is looking for. Turtle assumes a unix-like environment (it includes basic work-arounds for Windows users) and Idris is a completely separate language.
There are no native bindings and probably for good reason. Powershell is pretty extensive, object oriented, and built atop the .NET platform. So binding to it would be pretty complicated and involved. Out of curiosity, I looked to see if even F# had bindings and I found [this](http://stackoverflow.com/a/30172298/3961879). So yeah, I don't think the Haskell ecosystem has what you're looking for and I'm not sure who else would. C#? Then again, maybe it's because no one has ever had the motivation to make such bindings, in which case you've got yourself an optional project you could do - admittedly a pretty big one.
I have heard clojure's edn format is pretty good. I have never used it myself. But it seems promising: https://github.com/edn-format/edn It seems like it has javascript and haskell libraries. https://github.com/edn-format/edn/wiki/Implementations But I am not sure if it would meet your requirements. It seemed to me like you will still need to serialize your types to an edn structure.
Mostly because C# and Javascript don't have a type-level concept of monads.
My head hurts now.
`Bar.foo` and `Foo.foo` are two separate symbols. They may, at runtime, evaluate to the same function. But the compiler doesn't actually know that. The compiler is just concerned with what symbols are available, and what their types are. So when it sees `Foo.foo` and `Bar.foo`, it has no reason to merge those into one `foo`.
Posting the link disseminates it more widely. How often do you check SPJ's homepage, as compared to r/haskell?
You're not telling the whole story here. `foo` is not ambiguous in `Bar` because `Foo` is imported qualified. It looks like you've imported both `Foo` and `Bar` unqualified somewhere (presumably GHCi) and indeed /u/ElvishJerricco's reasoning is correct.
But you can't really do that because matching functions to arguments / typing a function is a compile time thing. And strings are a run time thing.
Yes, in the example he used above, we already do this. class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b The equivalent in the rename example would be: class Renameable r where rename :: r -&gt; String -&gt; r
IIRC, the [current all-time top post on /r/Haskell](https://www.reddit.com/r/haskell/comments/2svayz/i_think_ive_nailed_it_ive_solved_the_records/) uses this approach.
It can see the re-exporting feature though, so if you really want `Bar` to reexport `Foo.foo`, you could do that and the occurrence wouldn't be ambiguous anymore.
However, I think we can agree that the information is there -- just not at the per-module level. If the compiler was aware of the import/export graph, it would be able to resolve the ambiguity.
There's also [typed-wire](https://github.com/typed-wire/typed-wire).
Right. That's settled, then. By the way, the motivation for my post was: `mwc-probability` re-exported a bunch of functions from `mwc-random` in the improper way. So, I guess that the proper practice has not reached all library authors yet... Thanks for taking the time to comment.
You misunderstood my comment, or I wrote it poorly, sorry. I never said that I don't want to use Haskell because there are features I don't understand, just that most of the time the feature are presented in a very complicated way.
It isn't sexy, but you could try XML described by an XSD. Here's an example of an XSD: https://msdn.microsoft.com/en-us/library/bb675181.aspx You could then use something like XPath or XQuery to 'pattern-match'. It has a more extensive type system, and most languages have support for it. Haskell has HaXml, and it looked like Node.JS had a library. Additionally, I would expect Javascript programmers to feel at home querying XML documents. It supports sum types through the 'choice' element: http://www.w3schools.com/xml/el_choice.asp
over http: https://blogs.msdn.microsoft.com/powershell/2014/09/29/simple-http-api-for-executing-powershell-scripts/ 
https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields
Well, you can in a dynamic language... But yes, that's a problem in Kitten; but I'd say it's less about supporting variadic functions and more about supporting type classes. The way Haskell does it is it figures out the type of `printf` from its arguments, and then hopes the string matches the type.
Spelling on page 12 right column first paragraph: Rather than manually **instaitate** these functors
For starters, what are you trying to do? Some notes: * remove the parens around the `T` declaration * add a `deriving Show` to your datatype TextEditor * `reverse . tail . reverse == init`, which in this case returns the empty list since `["The"]` is a singleton list.
There are two problems with your code. First, you should remove the parentheses from the data declaration. Second, you should add a `Show` instance for your `TextEditor` type. The terminal will use the `show` function (of the `Show` typeclass) to convert expressions to strings so that they can be displayed. You can have the compiler define this function for you by adding `deriving Show` to the end of your data type. data TextEditor = T [String] [String] [String] [String] deriving Show
I made 2 changes and it compiled and ran on GHC 7.8.4: 1. Removed the parentheses on the 'data TextEditor = ' line 2. Added a 'main = putStrLn "hello"' line I placed the text into a file named "X.hs", ran the command "ghc X.hs" to compile it, and executed it with "./X". If you're on Windows, maybe check for carriage returns.
Ah, should've read the damned manual:)
A couple of things: 1) your imports are redundant. 2) no need to put brackets around Data constructor 3) there is a special function for dropping last element of a list - it is called `init` `reverse . tail . reverse = init ` 4) I don't really understand what your `backspace` function is doing with "before cursor" because ` String = [Char]` you will drop "The" ( in your example ) 5) because `l` is never used - you can use `_` instead 6) you should give `Show` instance to your data type so you can actually print it data TextEditor = T [String] [String] [String] [String] deriving (Show) backspace :: TextEditor -&gt; TextEditor backspace (T l _ a x) = T (init l) [] a x this will work but I think it doesn't do what you want
&gt; The "The" shouldn't be deleted.. what should be then ? `String = [Char]` So when you have `[String]` , this is the same as `[[Char]]` so ["to", "be"] = [ [ 't', 'o' ] , [ 'b', 'e' ] ] If you want to operate on character level, your data type should be `data TextEditor = T String String String String `
What did you end up settling on, if anything?
Well all you need to do is compare the device a file/directory is from with a stat call. st_dev tells you when you've crossed into a different mount point. No need to overcomplicate things really, and this is the portable (unixy) way to go about it.
The intention was for the "e" in "the" to be deleted, as well as the highlight using the "backspace" function. I'm about to sleep now but I will definitely give that new data type a try tomorrow. Thanks! 
Note: this is a pragmatic, not a theoretic answer. I'm also not an expert on this topic by any means. As I understood them, Arrows do two things: 1. Unlike monads, they record their input as well; `m o` vs. `arr i o`. 2. They provide you with a richer vocabulary of composition than monads. Pretty much the only thing monads can do is sequences operations via `(&gt;=&gt;) :: (a -&gt; m b) -&gt; (b -&gt; m c) -&gt; (a -&gt; m c)`. The operands of `&gt;=&gt;` might contain conditions, loops, or God knows what else, but the sequence operator doesn't care about that. Arrows, and especially `ArrowChoice`, `ArrowApply`, `ArrowLoop`, etc. provide you with various functions to compose arrows and they thereby allow you to make the logical structure of a computation more explicit. This is a bit like writing an AST in a DSL, though a DSL in which you can have effects that depend on the Arrow-instance you use. In fact: depending on what Arrow you're using, you could construct an actual AST with the arrow combinators. As for the usage: they're general tools. They've been useful in FRP and other GUI programming (where you'd like to record the logical structure of some GUI specification or workflow).
Hitting a target. *Ba Dum Tss!*
I think watching from a supervisor vs. catching in the thread is just a holographic transform. But, that doesn't mean it wouldn't make for a better API.
My bad. The devil is in the details...
I'm learning hxt now which parses XML using arrows and I agree with your assessment. Arrows feel like a DSL, and I was wondering how to write loops and other weird shit in them. Thanks to you I know there is an arrow for that.
In Haskell, the `Kleisli` type (which is a newtype wrapper for `a -&gt; m b`) is an `Arrow` if `m` is a monad. It also implements the other arrow variants with various restrictions on the monad (e.g. if `m` is a `MonadFix`, then `Kleisli m` is an `ArrowLoop`). 
I didn't say anything about dry. its encapsulation over inheritance. i'm not trying to sell anything to you, just answer questions. The documentation for loading templates from disk is under the subhead "loading templates from disk" https://wiki.haskell.org/HStringTemplate#Loading_templates_from_disk
An example of this would be sick.
This. "Haskell sucks because" it doesn't support **type lattice pushouts**. So instead of the expressive abstraction of mathematics, you get crippling fragility and "you can do it by hand! (but the compiler has no clue and can't help/check you)".
Do you know where to find more about type lattice pushouts? It sounds like good reading.
&gt; Is there books that skip Haskell basics and show how to build bigger app? What are the best this kind of books? [Developing Web Apps with Haskell and Yesod: Safety-Driven Web Development](http://www.amazon.com/Developing-Apps-Haskell-Yesod-Safety-Driven/dp/1491915595) 
I dislike the motto: "Exceptions for exceptional cases", because "exceptional" isn't really meaningful and different people will have different ideas of what's exceptional. File not found, is that exceptional? Bug in the program is exceptional? IMO these should panic, and not really throw (catchable) exceptions. Exceptions are about control flow -- and should be used when that pattern of control flow makes sense.
My experience so far is that the Haskell extensions don't really bring complexity. The complexity is in the problem you're trying to solve. It's stating the obvious, but in any programming problem there's inherent complexity (which cannot be eliminated) and redundant/unnecessary complexity (which can be and, ideally, usually should eliminated, within the constraints of development time and cost). If you use Haskell type extensions just because they're there, you're very likely increasing the redundant complexity in your work. But the existence of extensions only enables a wider range of design options. Some design options have more redundant complexity, some have less. Banning the use of extensions cannot force you to increase the redundant complexity compared with an extension-free implementation - the extension-free options are still all available. But allowing the use of extensions can sometimes allow solutions that have less redundant complexity. The language itself is more complex because of extensions, but that doesn't mean the implementation of a project that uses those extensions is more complex. By providing more options, it actually allows **less** complex solutions. The problem with that theory is of course that architecture astronauts (like me) exist, and given the opportunity, will seek out the most "general" (usually meaning interesting, ie a fun challenge, ie complex) solution to any problem. Allowing extensions means more complex designs are possible, as well as less complex solutions. That's not the fault of the extensions themselves, it's the fault of the idiot over-using them. 
I always avoid extensions in code I write and stick to the standard. (Unless I'm experimenting with an extension intentionally to learn how it works, etc) After a lot of playing around, my biggest vote would be for RankNTypes to make it into the standard eventually. This extension adds expressive power that standard Haskell currently cannot emulate without using dynamic typing.
It is a great language, simple to program, but destroyed by intimidating people with a cultic mind that have created an absurd mythology and wrote complicated sagas around it. Because they construct their reputation on making haskell difficult to learn.
Yeah, basically. Translated, the sentence would be "Who here knows what a 'hugs' is? It's basically the same as GHCI only bad (years out of development etc) and we don't support that crap."
In my opinion - almost never. In practice, you can always model with Applicative instead of Arrow. People tend to be more familiar with Applicative notation and find it easier to understand. So your code will be more readable to more people if you use an Applicative model. While the exact mathematics doesn't seem to have been worked out exactly yet, it is well known that Applicative+Category has "about the same" expressiveness as Arrows.
You shouldn't "model things with monads" and you shouldn't "model things with arrows". You should just model things. Then you should look at your data types and if one of them is an instance of monad or arrow in principle you should implement that instance in practice. Don't go out "trying to make a monadic (or arrow) API".
Why would you use the `Kleisli` arrow rather than just the monad directly?
And what has "do" notation in common with async? async/await can be implemented in any language. it is plain imperative code. And it is for a particular purpose: spawning and waiting for threads with some abstraction of thread management. (In JavaScript this is different since there are no thread management). The Haskell package [async](https://hackage.haskell.org/package/async) implement the async/(a)wait pattern. It does it in the IO monad. There is no special machinery necessary. Except perhaps some management of STM variables when waiting for the completion of many tasks at the same time and very careful exception management in F#, the async workflow is little more than a newtype definition with a monad associated newtype Task= Task {runTask'= IO ()} deriving Monad So it is possible to define a Task without running it. and then execute it with: runTask task= forkIO $ runTask' task .... But the async/wait pattern has his limitations: There is a single controlling thread that spawn and wait. This limits the composability since any code that uses async/wait blocks waiting for their children threads. Therefore it is impossible to create bigger components that could multitask using smaller ones. [transient](https://github.com/agocorona/transient/wiki/Transient-tutorial) has `async`, but does not wait. There is no controlling thread, so it is fully composable and can create multitasking code out of components that multitask and so on. It uses "do" notation and has his own monad, since it need some special machinery for managing continuations. So in a sense "do" is more expressive and can do more things than async/wait. Although this is like comparing apples with apple juice.
MultiParamTypeClasses, FunctionalDependencies, FlexibleInstances, FlexibleContexts, ScopedTypeVariables are all pretty natural.
I got it from a Twitter post. What is SPJ's Reddit account?
Here are a few from quickly scanning through the list of GHC extensions: * Empty data declarations. Not allowing empty algebraic data types is like not letting you use the number 0. * Fractional looking integer literals. It's fine for `1e6` to be an integer, not a fractional number. * Some of the typeclass ones like `FlexibleContexts`. This is also worth a look because it includes some that *should* be behind a flag: they're sometimes correct but often a mistake. Think `OverlappingInstances` * Type operators and infix type constructors make sense because it just lets types work the same way lexically as values do. 
"we don't support Hugs" ... waiting for the reactions to that What I don't agree with: recommending the Platform
.net/C#/F# cannot handle type-constructors (higher-kinded types) so you cannot expect to much - F#'s computation expressions are both more and less then monads - you cannot abstract over them nor get transformers but you have a bit more control if you like
yes ... that too - well it's still a nice language - maybe even a better *starter drug* than Haskell
&gt; Gained over what exactly? This is where my naivety has me fumbling for good questions. Why would an FRP author, for example, write their API with arrowized notation? Was it personal preference? Were there time and complexity benefits due to the notation which lead them there, and if so why is it different from pure/applicative/monadic code? (Are these the wrong questions to even be asking?)
&gt; Sadly, arr is needed to support proc notation. I'm working on fixing that :) [category-syntax](https://github.com/gelisam/category-syntax#readme)
Am I the only one annoyed by the fact that Haskell has this great universal way of encoding *things you can do with a datatype* (ie typeclasses) and then this interface is not supported in the performant datatypes. Example given: while `Data.Vector` does support `Foldable, Traversable and co` the more performant types in `Data.Vector.Unboxed` and `Data.Vector.Storable` don't. This is of course because you can't encode the constraints, but afaik today there are solutions for that.
I think you can overcome some of those limitations with the use of Task Schedulers (https://msdn.microsoft.com/library/system.threading.tasks.taskscheduler.aspx) but I'm not sure about their capabilities, just pointing out.
I am only talking about syntax aspect. The main goal of both syntaxes is replacement m1.bind(x =&gt; m2.bind(y =&gt; x + y)) with x = m1 y = m2 x + y So it doesn't really matter what libraries are behind these syntaxes.
This is a very cool idea. I think we also need `Functor` syntax, `Profunctor` syntax, `ProductProfunctor` syntax. It should be possible to wrap these up into a general everything-syntax.
Could you expand on your point about file not found? How do you suggest that situation be handled?
I was going to rant against this blog post for being misleading and then when I clicked through I discovered that I wrote it. The principles of blind justice imply that I must still rant about it, so here goes. The post suggests that arrows *themselves* (whatever that might mean) yield a performance benefit or allow you to model a target language more closely. This is misleading. It's the datatype that you define (and the functions you export for working with it) that has these properties. It just so happens that if you enforce these properties in your type (and accessors) then it probably won't be a monad so you will be unable to use the whole host of monad functionality in the standard library. It might still be an arrow though and if so you can use the whole host of arrow functionality instead. Anyway, thanks for linking the post /u/protestor. I think I should probably make it clearer. 
The big difference between your Haskell example and your Java example is that Haskell makes a distinction between `funcReturningEither` and `funcInMyAppMonad`, while Java doesn't. That distinction is visible as the hoist functions. Java doesn't have hoist functions because it has no distinction between `a` and `m a`. The main idea here is being able to distinguish things that can fail from things that can't. Until the fairly recent introduction of Java's `Optional` class, it didn't even distinguish between `a` and `Maybe a` at the type level. Hence, code was littered with null checks. If you had a value of type `a`, you never could be sure whether you really had it or not. In Haskell, I think the ideal utopia is that if you have a value of type `a`, you never have to worry about potential failure cases. Of course we can't get rid of them completely, so we have bottom, but we can eliminate a huge portion of the failure cases. When I'm writing code, I don't want to be constantly having to think "Hmmm, can this function fail?" I want that to be visible in the type signature as a Maybe or Either.
They would have, but they now can't commit hit to github for some reason...
A tuple would be: t = (["The"], [" cat"], [" sat on the mat"], ["copied"]) Notice the added commas. What you have there is a product type. In Haskell, the tupling operator is (,) - that is, parens and commas. It's confusing because parentheses are also used for grouping. Your use of parens in your datatype, since it doesn't have commas, is for grouping. You also possibly have an extra level of nesting for your lists. Seems to me you want no square brackets (list constructor): `t = T "The" " cat" " sat on the mat" "copied"` or `t = ("The", " cat", " sat on the mat", "copied")` 
On first sight, I've also thought that `"[1,2]"` was simply not the string representation of `Vector`, as I'm used to deriving `Show` and `Read`, which includes the constructor name in the string representation. But as op insisted, that it worked in a different project, I've tried to reproduce the results in ghci. Against my expectation, the `Show` and `Read` instances behaved as described by op, so I got curious what might cause the difference. The most plausible explanation I came up with were different versions, so I've followed the link in your edited post and looked at the source code on hackage: the most recent version used generics, the least recent version didn't even had a `Show` and `Read` instance, so I started a binary search for differences, which lead me to `vector-0.9`. It was quite interesting and entertaining to observe the evolution of the library. Maybe I'll check out the source history of other projects, too. :)
Oh, so that is what it does! I finally understood it then... Given this connection between row types and extensible effects, does Nikita's approach helps with something for this end?
all *cabal-hell* problems I've seen since sandboxes did originate there - also *stack* ;)
Uhm, with that approach _every_ IO action will have to be of the form `IO (Maybe a)` or `IO (Either a b)`. Because all of them can fail. &gt; If they have the possibility of failure, due to calling the low-level function that can fail, then they're not in fact total...? You are missing the point. A few examples here: copy :: File a -&gt; File a -&gt; Path Fn -&gt; IO () This function can fail, although it is defined for every possible input, so it is in fact total. Adding an Either/Maybe or even Bool type here to the result doesn't give us anything useful. Even worse, someone might ignore the result as is commonly done in the IO monad, practically hiding failure and breaking the callstack in unpredictable ways. Likewise: getDirectoryContents :: Path Abs -&gt; IO [Path Fn] This function can just return an empty list or outright fail in case the input path is not readable. Returning an empty list in case of IOExceptions is _bad_. Wrapping it in `Maybe [File a]` will basically force us to handle that failure in _every_ subsequent function, because the whole program flow depends on this one function... either via a lot of fmapping or in some cases even changing the input arguments to something like workWithDirContents :: Maybe [File a] -&gt; IO (Maybe [File a]) For every of these functions you have to carefully think how to map that _one_ failure you are currently handling to the result type of the caller. Now... things get more interesting when the function in question is part of multiple callstacks with multiple interesting failures. You'll pretty quickly end up with a "total" function, but which is in fact rather semi-defined and might not do what you think for all failure scenarios. That's the same if you end up working 80% of the time inside Functor/Monad instances of Maybe/Either. Because you are not handling failure specific to the function in question anymore, but general failure, because your function must be total, no matter if you know all possible failures yet or not. So... you are just passing the result type of a previous failure on, because it doesn't remotely make sense to handle it in the current function... which is what exceptions already do! Without crippling the API. Exceptions here are way more elegant for scenarios where failure is in general more complex and involves larger callstacks, but only if they work correctly within the language. Failures you haven't thought about yet (happens a lot in IO) should crash the program instead of silently vanishing in your "total" Either callstack. As soon as you make that callstack non-total at some point to work around this weird program behavior, you're already asking for exceptions. In addition... you can easily turn any exception into an Either type. Forcing that decision on programmers however is bad. &gt; Rust doesn't have an IO monad, they do do all error handling for I/O with a Result type which is equivalent to Haskell's Either They once had an IO type. However, I don't see how Rusts decisions are relevant here. I think that was a bad decision (both of them). But yes, it's hard to get exceptions right, have them typechecked and allow the compiler to know where exceptions are uncatched. And... as someone who's writing a filemanager in haskell... I'd rather choose a different language than not having exceptions, because it's a huge pain to handle everything explicitly where it doesn't even make sense.
&gt; Focus on the combinators you build, and if you see it overlaps a lot with Arrows (for example), then write an instance and get the rest of the combinators you'd have eventually had to write for free? Here it helps to remember that typeclasses, such as Arrow and Monad and Monoid, are nothing more than ad-hoc polymorphism--that is, a way to take different functions and give them the same name, so long as their types share some similarities. You can always implement the same functionality that a typeclass gives you, without using the typeclass names. A good example is [Parsec](https://hackage.haskell.org/package/parsec-3.1.9/docs/Text-Parsec-Prim.html), which has `parsecMap` for `fmap`, `parserReturn` for `return` and `pure`, `parserBind` for `&gt;&gt;=`, `parserFail` for `fail`, `parserZero` for `mzero` and `empty`, and `parserPlus` for `&lt;|&gt;`. So you can always simply go about implementing your types and provide the functions you need and then say "oh, this has the properties of an arrow," and then implement the arrow instances. Reading the Parsec design papers gives a bit of insight into this mindset of design; it implements Parsec and then says "oh look, this has the properties of a monad. By implementing the Monad instance, we get this thing called `do` notation for free." (Interestingly enough, some of the typeclasses we now use, such as Applicative and Alternative, were not even around when Parsec was written, so it had its own `&lt;|&gt;`, which is a bit different from the `&lt;|&gt;` that is now in `Alternative`.) You have also stumbled upon what is IMO the biggest flaw of Arrow: it seems to me it was designed backward. Most useful typeclasses seemed to come about because someone noticed "hey, there are a bunch of things out there that have this useful property, so we can create a typeclass and then we can have a bunch of functions that all work on things that have this property." Instead, the paper introducing Arrows seems to have the mindset "gee whiz, there are a whole bunch of things out there that are kind of like Monad, but they kind of aren't, so there must be some sort of typeclass we can create that will shoehorn all this kind-of-Monad stuff into another typeclass." As /u/gelisam suggests, it starts with the idea of "everything should be monad" or, rather, it starts with "everything should fit into some sort of typeclass, but here Monad doesn't work, so isn't there something that will work?"
You cannot just use the same identifier to assert equality of two (or more parts) of the structure you're deconstructing. You need to use guards instead: reduce' (x : y : xs) | x == y = reduce' (x : xs) 
Idris is designed to allow relatively simple backends implementations and quite a handful of them has been implemented (with various degrees of completeness). There's .net one too.
I've got good news for you then: GHC 8.0's `cabal` 1.24 will provide a tech-preview implementation of the ideas described in http://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/ and http://blog.ezyang.com/2015/08/help-us-beta-test-no-reinstall-cabal/. And with a lot of UI polish this tech-preview mode may become the new default mode of `cabal` by the time GHC 8.2 is ready.
It's also noteworthy to point out why they didn't use GHC until now: In the past, their professor for some reason didn't succeed installing GHC on his Mac and so it was Hugs for everybody
Hey, we have this tool that is well known for creating end user confusion. And it's the build tool shipped with an installer which is known to exacerbate the problems with that build tool. There's work in progress to try and fix this with an experimental new approach, which hasn't been tested by end users at all yet, and has plenty of problems of its own already identified. Hopefully, by the time the _next_ version of GHC is shipped, which isn't known yet, but is likely over 1.5 years in the future, then a new version of that installer can be created that won't have those problems. Good news!
I like your attitude.
Panic is by (my) definition unrecoverable. So exceptions only make sense for recoverable errors.
Thanks. I'm actually learning it, but I have just started and by no means have the fluency needed to follow a lecture like the one posted.
You have an off by one error. The platform shipping with 8.0 will have both stack included and a minimal package set as well as the neat experimental tech-preview stuff. There's no need to wait for the _next_ version of ghc following 8.0 for the improved get-haskell platform experience which has always been slated for the 8.0 platform. Hvr's just pointing to another neat, fun, optional bonus cherry on top!
You claim I made an error, but you actually misread my comment. Let's try again: 1. cabal-install is known for creating cabal hell and end user confusion. This is _independent_ of the platform 2. The platform is currently set up in a known-suboptimal way that exacerbates user problems 3. There's a claim that the next HP will fix that bad setup _and_ start shipping Stack. But claiming that I made an error by not mentioning that is forgetting that that claim has been around for well over six months already, and keeps not happening. So I'm not holding my breath 4. I am absolutely correct is pointing out that the "good news" that Herbert mentioned as being something which will only be ready (optimistically) with GHC 8.2, which is likely 1.5 years or more in the future 5. And the planned feature is one that many of us have criticized from the get-go for its complexity and likely outcome of making end user's situation worse So yes, if you want to ignore what I actually said, and assume as absolute fact predictions about the future of the HP, I made an "off by one error."
But the Prelude itself has such exceptions! $ ghci GHCi, version 7.10.3: http://www.haskell.org/ghc/ :? for help Prelude&gt; 1 `div` 0 *** Exception: divide by zero Perhaps a future version of Haskell should change the signature of `div` to not lie anymore Prelude&gt; :t div div :: Integral a =&gt; a -&gt; a -&gt; a At a later time, one could enable an extension that forbids code that throws implicit exceptions (except on runtime failure, if there's an exception for "out of memory" or something). The problem with this is that it would divide the ecosystem.
Off the top of my head, I can think of two languages with "panic" mechanisms; quick research yields 2 more: * Rust: `panic!` [`std::panic::recover`](https://doc.rust-lang.org/std/panic/fn.recover.html) * go: `panic` [`recover`](https://blog.golang.org/defer-panic-and-recover) * C++: [`std::terminate`](http://en.cppreference.com/w/cpp/error/terminate) Not straight-forward, but it's possible to set the handler or to catch SIGABRT * C#: [`Environment.FailFast`](https://msdn.microsoft.com/en-us/library/ms131100%28v=vs.110%29.aspx) As far as I can tell, there is no way to recover. (I learned about the C++ and the C# panic mechanism from [here](http://joeduffyblog.com/2016/02/07/the-error-model/)) The two well-known ones, which happen to be the ones actually called "panic", are recoverable; of the other two, one is sorta-kinda recoverable.
Wow. This is definitely blog post worthy, for both the list of schemas as well as the algebraic datatype analysis. Even better would be something like a gist or a wiki article so it could be kept updated, if you post it somewhere let us know!
[here](https://gist.github.com/gelisam/13d04ac5a54b577b2492785c1084281f) you go. I made a gist because it's easy, but if you know of a good place in which to host it, a wiki would be much friendlier to potential contributors. Do you know if its even possible to send pull requests on gists?
In a sense, every function that allocates should return `Either AllocationError a`, but that'd be even more ridiculous. So it does make sense to handle some sorts of exceptions as actual exceptions... but the problem is there's almost 0 material on how to do so. At least that's what was the case a few months ago.
I'm still amazed that German universities manage to keep up German for topics the talking about which requires English 2/3rds of the time anyway (disclaimer: I'm German).
Am I the only one that checks SPJ's homepage everyday ?
Get the denotational semantics right, and it'll be obvious what type class instances (and other interfaces) to provide? Get the operational semantics right, and it'll be obvious what the shape of your `data` declarations (and other implementations) should be?
Thanks! Bookmarked.
&gt; but you won't be able to read csv which contains fields with different encodings Given that the byte-sequence representing a comma differs between codings, the idea of a CSV file with different field-encodings is an impossibility. &gt; The problem with "Text everywhere" approach is just this - the are many tasks, where encoding of data is irrelevant Except that, as the CSV-comma situation, and the [utf8 manifesto shows](http://utf8everywhere.org) illustrate, text _is_ everywhere: in CSV, in tab-delimited files, in Matlab ascii files, in HTML, in XML and its variants like RSS, Atom etc, in HTTP, RTSP and other recent protocols. And while UTF-8 might often work, languages used extensively in industry, like [Java](https://docs.oracle.com/javase/7/docs/api/java/nio/file/Files.html#readAllLines(java.nio.file.Path,%20java.nio.charset.Charset) and [C#](https://msdn.microsoft.com/en-us/library/ms143369.aspx) allow you to specify a charset when reading text, and unsurprisingly these same languages have CSV libraries ([Java](https://commons.apache.org/proper/commons-csv/archives/1.2/apidocs/index.html), [C#](http://www.filehelpers.net/docs/html/T_FileHelpers_FileHelperEngine_1.htm)) that allow you to specify the charset when reading and writing. Coming from a Java background, the casual disregard for charsets is not something that inspires confidence in the quality of third-party code. &gt; And when the data happen to contain data which is encoded differently than you assumed the program will fail to handle them. This is precisely my point, there is no CSV library for Haskell which would safely work with UCS2, the Windows system default, for example, and pretending that the byte 0x2c is a safe field delimiter will just lead to nonsense parses and broken character fragments. 
The [process](http://hackage.haskell.org/package/process) library is pretty useful by itself. Here is a (not exhaustive) list of packages that build on it and provide more (or easier to use) functionality: http://hackage.haskell.org/package/turtle http://hackage.haskell.org/package/pipes-cliff http://hackage.haskell.org/package/pipes-shell http://hackage.haskell.org/package/shelly http://hackage.haskell.org/package/shell-conduit http://hackage.haskell.org/package/conduit-extra (Data.Conduit.Process) http://hackage.haskell.org/package/io-streams (System.IO.Streams.Process) http://hackage.haskell.org/package/process-extras http://hackage.haskell.org/package/process-streaming (my own attempt) 
Thank you for this incredibly detailed post. I now realize that I wrote the OP with some wrong assumptions, which you and others have graciously cleared up.
If you recall UML. Say that you have a set of problems which the best and easiest way to model them is through an Activity Diagram. Then what you need is arrows in that case, since they allow you to model circuit behaviors quite easily. Arrows in general aren't used that much. Some people suggest using them for FRP(Functional Reactive Programming) but there are better more elegant like Push/Pull solutions (look at reactive-banana for instance). Try to use Arrows to model solutions for engineering models like non-linear programming.
That isn't equivalent, the original trims starting repeated values to one, this one trims down to the first value (as you aren't checking that `x` and `y` are equal).
I'm pretty sure Scala has similar syntax to this as well.
&gt; But if you want to partially apply later arguments it is much less clean: &gt; foo . (\x -&gt; elem x ['a' .. 'z']) . bar I think more common is foo . (`elem` ['a' .. 'z']) . bar Another alternative would be foo . flip elem ['a' .. 'z'] . bar
Yeah but I think this only works for functions with two arguments right?
In your example: (\x y z -&gt; f x a b y z) f _ a b _ _ (\z y x -&gt; f x a b y z) ??? How do you swap arguments?
I see them just fine.
I realize that I can do that, but I was speaking to the general case.
I mean I'd rather not give up operator sections, as they are pretty cool and convenient. But that is definitely a valid point,
I mean you can't... Just like how currently: (\x y z -&gt; f a b x y z) f a b (\x y z -&gt; f a x y z b) ??? There is no perfect solution. But I think `_` should solve a lot of common use cases, the most common one being a wanting one "hole" in a 2 or more parameter function (which has led to heavy use of operator sections and `flip`, which I would say are less clear than `_`). If you really did want some way of doing that. You could potentially allow `_1` `_2` or something like that.
I dabbled with J on a whim a few years ago and sketched part of it in Haskell. It's a good language for making you re-examine your assumptions about programming languages. Also, it's something to throw at people who complain that Haskell programs are too terse.
Reviewers who wish to stay maximally blinded really have the onus on themselves at this point to steer clear of media where preprints are likely to be shared, such as reddit. One can't ask the whole internet to not talk about publicly available preprints. The author's instructions are simply instructions to the _authors_ to not _deliberately_ subvert the process, not instructions to the _world_ to not discuss preprints, which authors are free to distribute.
So people propose this every once in a while. But it's not so easy as you might think, because you have to denote the boundaries of your abstraction somehow. Scala has a bunch of heuristics to try to guess the right boundaries for you, but they are arcane and sometimes go wrong; and when they go wrong, it is very subtle and quite possibly a bug rather than a type error. Consider your example, `foo . elem _ ['a' .. 'z'] . bar`. How do we choose between these interpretations? (\x -&gt; foo . elem x ['a' .. 'z']) . bar foo . (\x -&gt; elem x ['a' .. 'z']) . bar foo . (\x -&gt; elem x ['a' .. 'z'] . bar) \x -&gt; foo . elem x ['a' .. 'z'] . bar You might say, "Oh, well, that's easy -- just make `_` never go across infix operators.". But I'm confident one of the very next things you're going to wish you could write will be `foo (_ + 1)`; and then your rule is going to turn this into `foo ((\x -&gt; x) + 1)`, which probably isn't what you meant, instead of one of `foo (\x -&gt; x + 1)` or `\x -&gt; foo (x + 1)`. I can't see why *in principle* I should prefer one of these interpretations over the others at the parser level; and any proposal which is going to rely on the typechecker is likely to be fragile, error-prone, and difficult to predict.
But I wouldn't want to write (_ + 5) because operator sections (+ 5) exist. So I would 10000% say make it never go over infix operators. (infix 10) or whatever.
I looked at the preceding question in your history and it looks like you're reading documentation at all. Read this http://www.yesodweb.com/book/shakespearean-templates. If you don't understand it, read it again. If you still don't understand it, look for simple Hamlet examples online. If none of that pays off, try a bunch of shit and see if it works. If *ALL* of that fails, ask another question on reddit or SO. That's what programming is. It's not giving up immediately and hoping someone can post a simple answer for you online. It seems like English isn't your first language, so I'm sorry if that makes some SO stuff or anything hard to understand. Haskell was developed in the US so 99.9% of everything on the internet about it is going to be in English and this might make things difficult for you. The solution is to either become more fluent in English (or at least programming english) or try languages that have support and communities surrounding your native language. Believe me, trying to learn a concept where you don't understand the method of communication that well makes things 100x more difficult. For example, my company is employing a QA intern right now. He's from Israel and is a really nice guy, but he doesn't know a lot of common English words (his English is fine, but he'll favor words that he's familiar with), so trying to write code that uses words he's not familiar with makes things difficult for him. Most because he will have difficulty remembering keywords that he doesn't already know.
Shit... well I guess the 4th option then, where it spreads as far as possible like a lambda, so you would just have to use parentheses. Unless I can think of a better way. I mean: (elem _ ['a' .. 'z']) is still better than: (\x -&gt; elem x ['a' .. 'z']) even if it is not quite as nice as: elem _ ['a' .. 'z'] Actually I guess some people might also want `(2 * (_ + 5))` Maybe some sort of special parenthesis is needed if you want to support infix operators? If that is too complex then it might be worth it to just ditch infix operators, I mean `(2 * _ + 5)` can be represented by: (+ 5) . (* 2) Which is barely any longer.
It seems like Haskell monads are not monoids in the category of endofunctors, but rather something a bit more 2-categorical. The alleged equalities are really inequalities with respect to an observational partial order, aren't they? I should write a blog post.
 class (Strong p, Category p) =&gt; Arrow p instance (Strong p, Category p) =&gt; Arrow p There. I fixed it.
No. There's no way to do so even with widgets in the way that I've described.
Because that goes against the way that I've described in my question. Look at my 2nd code snippet, do you see there **^ {head}** and **^ {footer}** in the child1 and child2 pages?
IMO there is no reason to make a topic more difficult by switching languages on top - if 2/3rds of the content is english then most here shouldn't have much trouble following the lectures
I mean Haskell consistently puts needle before haystack. So it kind of makes sense.
Look at the yesod scaffold, there is a default hamlet template that does what you are asking. Edit: http://www.yesodweb.com/book/scaffolding-and-the-site-template#scaffolding-and-the-site-template_defaultlayout This is the equivalent of your base.hamlet: https://github.com/c3d2/ta-haskell-yesod/blob/master/yesod-example/default-layout.hamlet 
Very cool! If you could put up a blog post about that it would be fantastic. We would have someplace to link to, and this question would finally be put to bed.
It can improve readability because it puts the verb in-between the object and the subject, which matches natural language. For example: ``arbitrary `suchThat` (&gt; 5)``, ``20 `div` 5``. etc.
This seems like a degenerate form of de Bruijn notation, which is notoriously terrible to work with :o)
I tried to explain this already and OP did not like it much - maybe it's a language barrier or something but in the end my best guess was that OP does not want to take in the extra dependency to yesod-core (AFAIK whamlets/widgets are implemented there) and wants something direcly in hamlet I just gave up
Introducing inconsistent style to be like natural language (overrated) is hardly improving readability for me.
The approach being discussed by hvr is [time tested](http://nixos.org). They didn't invent a completely new approach, but took an existing successful design and implemented the ideas in Cabal.
you will not like it - it's exactly what I proposed 
Maybe [this](https://ro-che.info/ccc/12) will help clear things up.
&gt; It is obviously not so: there are images, videos, executable files (either native or bytecode), many other formats which are not textual. Look, I'm 36 years old, I've been working in industry since I was 20: the amount of times I've had to _process_ text (as opposed to just read &amp; display) vastly overwhelms the amounts of time I've had to process binary formats. This is true of all my colleagues as well. Bear in mind most of the images we view are in HTML pages delivered by the HTTP protocol, may have EXIF metadata in ASCII text. &gt; You could even have a facade which makes this reencoding for you (like pcre-heavy which you mentioned above). As for quality, while I _could_ do this myself in Haskell, the point is in most other developer languages, it's built-in. In fact in Java the default is to read a file with a specific encoding. In Haskell the default is not to think about the encoding at all Also apply a UTF16 to UTF8 encoder, and then a UTF8 to UTF16 encoder, may not result in the same sequence of bytes. It's not exactly lossless.
Have you written Scala? Have you written higher-order functions in Scala? After a while writing Scala, you'll find that while on the surface this syntax is appealing, it's finicky enough (especially around HOF: does `_` mean an identity function at the point you write it, or do the arguments get bound further out?) that nobody has a clear mental model for how it works, which means that it's a pain to write anything but the most basic stuff with it because you're never quite sure where the binding will occur. I'd rather keep it out of Haskell.
Then you end up with parentheses that affect the meaning in a subtle way that they didn't before. And if you have two underscores that need bindings at different levels, you can't really differentiate them.
Could you elaborate on this at all? I'm now sure how type-level programming applies to this subject.
For 20-something years, GHC development was dominated by a couple of guys in England, not the US. Not that this changes any other detail of your response.. 
DuplicateRecordFields
RankNTypes
DeriveFunctor
 (+5).(2*) (2*_+5) The character difference here will be swamped when using real code where you don't have single-character numbers being used in the example. Actually I'd suggest that one of the problems with the proposal is that Haskell, being full of combinators, generally covers the easy use cases (see also the flip discussion), and what's left over isn't worth a language feature when `(\x y z -&gt; ...)` works fine. It makes more sense in non-default-curried languages and languages where you've got a fairly heavyweight syntax, e.g., `(2 * _ + 5)` vs `function(x) { return 2 * x + 5; }`. (Whenever I have to dip back in to Javascript one of the things that always trips me up is that you have to type the _full word_ `function`, which is really a surprising pain.)
I can't trump that! I'll take two.
http://www.fceia.unr.edu.ar/~mauro/pubs/Notions_of_Computation_as_Monoids.pdf gives most of the story and http://www-kb.is.s.u-tokyo.ac.jp/~asada/papers/arrStrMnd.pdf gives the rest, but you need to read between the lines and see that using Category gives you a way to model monads in Prof.
FlexibleInstances
TupleSections
DeriveGeneric
GeneralizedNewtypeDeriving
I find ScopedTypeVariables very useful (as one example) but the syntax has always felt annoying, and I'm not sure in the current form they should be enshrined in a standard. Why not just say "Glasgow Haskell" to mean the Haskell dialect with extensions supported by GHC?
Idris has various backends such as Java, PhP, Python. Here is a [bash backend](https://github.com/mietek/idris-bash). So I was wondering if someone has done a PowerShell backend.
LambdaCase
MonadComprehensions
I think you can have something like newtype FloatNumber = -- ... not exported newtype DoubleNumber = -- ... not exported toFloatNumber :: Float -&gt; Maybe FloatNumber toDoubleNumber :: Double -&gt; Maybe DoubleNumber and have FloatNumber and DoubleNumber implement Eq and Ord 
I dont want ScopedTypeVaraiable if i have to write forall everywhere.
Im not sure 1 extension is the best. It might be better to have a few which groups things together. For example flexible context and flexible instances or type families and data kinds etc ...
What's wrong with roles?
When using ScopedTypeVariables, you just have to use forall if you want to reuse the type variable in an inner type declaration, if you omit it, the classic Haskell rule apply.
Don't checked exceptions in Java also have the same "visible in type signature benefit"? In fact, I would argue [1], that Java has a better execution of this concept. I was _horrified_ that Haskell, while claiming purity, immutability, etc, has absolutely no indication of which function can throw what kind of errors? Edit: I noticed that you mentioned the same point in your comment below. What's the solution to deal with this? Is there some sort of compiler flag that will warn you about unhandled errors? [1] and I'm _really_ surprised that I'm arguing in the favour of Java
Is this one even necessary any more? `data Foo` compiles just fine for me.
TypeInType, or even better DependentTypes, as soon as available
TypeFamilies, TypeOperators
No no no. Haskell98 and Haskell 2010 are standards, that flag is not a place holder for "many ghc's deviations from the standard". This proposal is flat-out misleading. If you want a big, grand master pragma fine, but lets not name it something conflating. And for goodness sakes, stop spamming with arbitrary common extensions. It isn't conducive to conversation in any way.
I'm not going to participate in this silly revisionist history you're engaging in. Anyone interested in the truth can simply [read the Github pull request](https://github.com/haskell-infra/hl/pull/130) and judge for themselves. You made a unilateral decision, tried to shut down the possibility of raising it with others, claimed dictator-status on the haskell.org website, referred to internal, hidden communications that happened within the haskell.org committee, and only after I wasted weeks pushing for this and working around you did I get enough traction to get your decision overturned. And at the end of the day, the decision made was _still_ contrary to the popular vote which placed Stack at the top of the page. I made the comment "petty politics" on Twitter. For the record, that refers to your actions with the haskell.org committee. The incident of the downloads page was a major issue, and the last straw for me, but there have been plenty of other lead-up issues that make it clear that external ideas will be shunned (like FP Complete's offer to host all of the package tarballs on S3 at the company's expense, or to provide a dedicated sysadmin for haskell.org services). External ideas to other projects I mentioned have been shut down in similar ways. Whether it was my emails on the Haskell Platform being dropped on the floor for over a year and a half, or Well Typed/Duncan preventing any outside work on package security from making it into Hackage or cabal, these projects are clearly not true community projects. Sure, if someone sends a PR implementing a feature that "the maintainers" want in the way that the maintainers approve, it has a chance of (ultimately) getting merged in. But there is no room of outsiders to affect trajectory. And I think many people in the community would be a little shocked to know to what extent I and other significant Haskell contributors are really outsiders to your little cartel. The fact that you continue to make these glib replies and pretend like you haven't manipulated every process available, to the detriment of the Haskell community, is distressing. But it's not at all surprising given how much you've done it to date.
The submission is badly worded. It's not okay to encourage people to vote on a particular submission/comment. It is okay to encourage people to vote on whatever they feel like, which is what OP meant to do.
I think most of the deriving extensions should be on by default, deriving is one of the most brilliant tools against boilerplate in any language, and it's a shame you have to explicitly write pragmas to use them. StandaloneDeriving, DeriveFunctor, DeriveFoldable, etc. 
ScopedTypeVariables
`join` can't be made a part of `Monad`.
Isn't it too soon, as it is not tested by the whole community?
What do you use them for other than lists? I'm curious.
`Maybe`, mostly. `[ x | p ]` looks pretty neat sometimes compared to `guard p $&gt; x`. (Or is it `x &lt;$ guard p`?)
I'd never heard of monad comprehensions before. [This article](http://tomasp.net/blog/comprefun.aspx/) was kind of fascinating. His `Resumption` type is actually identical to the `Free` monad.
If my original offer was not clear/misunderstood, that's unfortunate. Aaron (CEO of FP Complete) and I discussed and decided that it was worth the (quite significant) investment to stabilize the Hackage hosting setup. When we got rebuffed on: 1. Providing free hosting for all packages on S3 2. Providing sysadmin work (which apparently may have not been clear) We moved ahead with alternative solutions, such as stackage-update, and ultimately just wrote Stack. Stack lets us work around the roadblocks we consistently got from the cartel, and now no engineers at FP Complete, customers of FP Complete, or people in the community are affected by such issues. And we solved it much more cheaply than the offer of dedicated sysadmin support we made. All of that said: even if the problem did exist, I've been burned so many times by the processes that I would advise Aaron _against_ offering significant monetary resources on this. We would simply be paying to fund development and directions that we thing are suboptimal (like avoiding cloud file hosting services or rolling package security from scratch), and I see no reason to play that game. (Just to clarify the original conversation: we did _not_ have sysadmin capacity on staff, and offered to hire a new system administrator and dedicate half of his/her time to haskell.org work. My understanding from you was that this offer was not welcome, and therefore we didn't seek out a candidate at the time.)
Also, along similar lines, is [wai-handler-launch](https://www.stackage.org/package/wai-handler-launch). It's lighter weight in that it doesn't need to ship a browser, but on the other hand you're stuck with whatever default browser is on the user's machine.
again, I am not sure whether this will increase compile times significantly of not.
Haskell newbie here, so please pardon my naivety, but wouldn't keyword arguments solve this problem? func :: {kw1, kw2, kw3} foo . (func kw2=something) . bar
yes, I changed "upvote" to "vote" for each extension.
Stephen Diehl lists [TypeInType as benign](http://dev.stephendiehl.com/hask/#language-extensions) but it has to be confirmed whether it is safe to have it on by default.
So that's one thing. Then you could continue with say conditional independance ?
would it make sense to not hardcode that and make it some user configurable sets of options?
No sure about that
nope because then you have to share these configurations for others to be able to compile your code etc. The point is to work towards the new standard.
I'd love to see a discussion on this. What is the best practice when dealing with a large number of objects and still looking for lower gc times? Use an external structure and access through the ffi?
Ah I see. Well I wouldn't say that it being a degenerate form is a bad thing. It's supposed to just be minor syntactic sugar. Not some next level shit to get used too.
Implied by FunctionalDependencies, so I'd rather that be the one turned on.
Not when we have other automatic deriving mechanisms enabled by flag...
This would be a pretty big breaking change unfortunately. Something as simple as `length "the"` is now a type error.
`EmptyDataDecls` (or however it was spelt) is part of H2010, so no, you're right.
Others have already linked to examples that demonstrate the approach, but I can try to make it as simple as possible: https://gist.github.com/snoyberg/d609dc43102e8d0c572869d76b78f122
Again, I'm pretty sure just enabling the extension won't impact compile times.
Applicative is no less imperative than monadic in theory, but it leads to much cleaner code. I'm a programmer, and yes, I hate programming. What's wrong with that? I've used Scala for a backend because I felt we needed to retain the ability to connect to Oracle (I was the DBA for that junk too) so I don't need any dressing down from you about that. As a matter of fact, I like the IO sin bin as it is, and I also don't use Haskell so that I recreate all the ugliness in those other languages.
We have AMP now and this is a logical next step to take advantage of AMP.
Just curious: Are there cases where FunDeps are superior to TypeFams? I seem to recall that TypeFams are strictly more powerful than FunDeps...? Is it just syntactic convenience, or is it a case of TypeFams perhaps being "too powerful" to be comfortable with as a 'default' extension?
Yes! This is how it should have worked from the beginning, so it's an obvious win.
You haven't addressed any of the actual complaints brought up about the extension, which I find rather solid. You've merely hand-waved them away with some claim about the logical progression of the world. Just because something is 'obvious' does not mean it is correct or the right thing to do, FWIW. To be clear, that doesn't mean it shouldn't go in. But you haven't addressed the actual complaints as to why it shouldn't be in.
This. If the applicative operators aren't equivalent or better (read: more optimized) than their monadic counterparts, then you're doing something wrong defining your instances.
I'm pretty boring so maybe I shouldn't be surprised. :) I just don't think I've ever seen them in the wild, so I found this suggestion pretty surprising! (I don't think there's anything wrong with them, FWIW)
&gt; No no no. Haskell98 and Haskell 2010 are standards, that flag is not a place holder for "many ghc's deviations from the standard". Sorry, what? "Deviations from a standard" is exactly what a new standard is!
Check out ES6' arrow notation.
Not sure about OP, but I certainly have... and it's mostly a thing of the past using any recent-ish version of cabal-install with sandboxes. So there. EDIT: Yes, sandboxes do have an extra compilation cost if you're installing multiple things in multiple sandboxes, but the Cabal people are working towards that with nix-style builds. Ultimately, I think it'll be a more successful approach, but... *yes*, it *does* take time to get it right.
&gt; we also have no record of the offer of a sysadmin being welcome either I have an email to you as of Feb 20 describing two areas where we would welcome sysadmin help: " As I’ve mentioned, the migration of community.h.o and the curation of wiki.h.o (and possible administration of it — i.e. if you have somebody able to serve as a good mediawiki admin vis a vis anti-spam plugins, etc.) are two areas of immediate concern to me. Beyond that, I’m not sure what needs the most shoring up." Your response: "On the admin side: we're actually very strapped on devops capacity right now, but I've put in a request to move up the hiring of our next sysadmin specifically so that we have extra cycles on our team to provide support to haskell.org." There was also an email to you in May saying explicitly that the haskell infra team would consider any help from FP complete on sysadmin stuff "fantastic." So yes, there is a record of this. I don't know which other aspect of your comment you would like me to address. I have no problem with people reading that github ticket and subsequent conversations on the haskell-community list and reaching their own informed conclusions. Regarding terms on the h.o committee, you are absolutely correct. A call was put out for self-nominations, and there has been a discussion period. I don't know what happened in terms of why a decision has not yet been announced, as I have not participated in those discussions.
And it's sister MultiWayIf
+1. Several years of experience in Scala is the exact reason that I'd be opposed to this.
Hm, not sure if those are what I need? I don't really want to ship a bundled web app masquerading as a regular desktop app or anything like this. I want to run a service on some network host and monitor, interact with and configure it from a browser on any machine, kinda like a router.
Right. Perhaps enabling the extension per file is too coarse grained. There's the proposal to have both do and ado. Looks like it's too early to have it in until we get more clarity. 
"Degenerate form" can also be phrased "more constrained subset", which are often easier to follow than the more general case.
Bootstrap seemed awfully complex when I looked at it last, if there is something simpler that would be ideal. I don't really know anything about hsnippet, but it's a regular website, isn't it? I'm really just looking to embed a small web server and engine into another app, not to deploy a regular website on a webserver etc.
I understand. But you haven't really addressed any of the concerns.
Yeah, that is actually offensive as far as I'm concerned. 
Asshats, more like.
UndecidableInstances, IncoherentInstances, OverlappingInstances.
They have been improved, but they were originally in [v0.8.2](https://github.com/purescript/purescript/releases/tag/v0.8.2): &gt; Operator sections can now be written using underscores. For example: &gt; &gt; decrementAll = map (_ - 1)
/u/snoyberg is commendably calm and collected, considering the events described. I think the thread you linked is a powerful argument argument for the adoption of Stack.
So use one language with all it's perks and features and then when you realize you can no longer scale it up rewrite the whole thing in another language?
The GHC team already disregards the standard in unacceptable ways that keep me on old versions. Let's not encourage them to do more stupid crap.
do you mean call out to a module written in rust or simply rewrite? I'm asking in the context of an application already written in haskell just to be clear.
Sandboxes do not even begin to solve the problem. They just punt on it.
This wouldn't be enabled by default, but a new extension. The Haskell *technically* says you can add extensions, and Haskell is supposed to be a research language, with GHC being an interesting end to that aim (I think a lot of the extensions are redundant though.)
Typeable is already derived for everything whether or not you ask for it. So this extension is unneeded.
It might have more than you want, but still should provide a decent example of parts that you need. You mentioned "run Haskell in the browser now", so I thought this would provide an example of all that.
I'd love to hear if you've done haskell -&gt; rust programming, I've seen other post suggesting this be done. To me the conceptual burden of combining these two languages seems too high to justify, but I haven't tried.
MTL included a Resumption monad until something like GHC 6.8, but no-one except William Harrison seem to find resumptions useful and it just dropped out of the code base. William Harrison was using resumption + reaction monads to model OS schedulers and the like. His reaction monads were essentially design patterns, rather than something that could fit in a general purpose library like MTL. 
Really? I thought that was just with AutoDeriveTypeable, which would also be a good extension to have in Haskell2016.
`Strict` :3
Wow that's a rather grumpy response to this. The name Haskell2016 does not necessarily mean we need a standard. Yes, it would be ideal to standardize ahead of time, but it seems silly to continue having 15 or 20 pragmas per module. It seems pragmatic to me to define Haskell2016 now and create a standard for it later. By that time it will probably be 2017, and if the standardization process finds anything that needs amending, it can be amended as Haskell2017. Dan Burton mentioned maybe naming it -XGlasgow2016 - that seems good to me too
It's now derived for all types. `AutoDeriveTypeable` is now essentially a no-op.
I hadn't done it yet, but it seems like it should be straightforward, or at least no worse than for C. Rust itself is designed to provide trivial interop with C, and Haskell's FFI with C although not trivial by any means is certainly not too painful. If you used C as the bridge between them on either side it shouldn't be too much work. Edit: Just to clarify I have done Rust to C, and Haskell to/from C, so Haskell to Rust should just be a matter of combining the two.
The pattern must be linear in its holes: that means, you're not allowed to use name twice. If you mean that list should start from 2 equal elements here, then express is like so: ``` reduce (x:y:xs) | x == y = ... ``` Non-linear patterns will cause typechecker to emit Eq-constrains on use, which could break compilation if (for some reason) code has no Eq class yet - or you have to make Eq a special thing, which is a smell. Moreover, if you _mistyped_ (x:y:xs) as (x:x:xs) this will lead not to error, but to completely undebuggable behaviour - you can believe me here, I have a good Prolog experience.
Do you have a particular university you are concerned about?
Well, `\x -&gt; (x, foo)` is just plain ugly. `(,foo)` isn't.
NoMonomorphismRestriction It's actually not that the restriction wouldn't have it's place, however, the default should be disabled.
Doesn't that mean you potentially have to disambiguate with explicit type annotation in many places ? 
I've been a full time Haskell developer for over ten years working on projects with hundreds of dependencies. I think cabal hell is overblown. 
Why would it increase compile time unless you actually use `derive Generic` somewhere?
&gt; North West University - Potchefstroom. South Africa There should be no problem. It is mostly an issue with little "pay for degree" mill "universities" here in the US.
I looked through the repository again and I honestly don't get a thing about this. It seems to even understand the solution you're suggesting I'd need to learn a dozen new technologies and libraries. This just all seems like complete overkill for my small problem.