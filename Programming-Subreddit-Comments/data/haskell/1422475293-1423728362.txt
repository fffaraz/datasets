More parametric information =&gt; fewer possible implementations for the functions =&gt; code becomes more precise ?
AFAIU that wouldn't ever happen with how the hashes are computed today, as exactly those hashes would differ in the depending libraries. And once you separate this "source hash" and use a new "ABI hash" you can just as well not recomplile the immediately depending libraries if the new ABI hash is the same as the old. Of course, there'd still be recursive compiling and stopping with ABI hashes, say, when one package hides a diamond dependency by not exporting anything of that in its ABI: Recompile the whole diamond, but then stop. Another issue, though, is extending interfaces: Just adding stuff generally doesn't (need to) break things. Removing stuff dependencies don't use doesn't break things, either. Labelled Merkle Trees? Per-symbol versions?
It was always there. C# (IMO correctly) uses passing by reference as a default with objects like in Java, but through the struct keyword it can allow you to use pass by value semantics instead. Of course, this means that any sane struct should also be immutable, and AFAIK this is not actually checked in any way. C# does allow "non-nullable" references akin to what C++ does with its own distinction between references and pointers. This gets checked statically. C# includes a "Nullable" tycon but I'm not sure if you can map over "Nullable&lt;T&gt;"s. C# generally uses "Where" to mean "fmap", and "Select" is generally implemented as an extension method. I've never really had to use it (Nullable I mean, Select is a different story), the more familiar approach is to roll this functionality into your type itself rather than composing it in with a type constructor. As C# has no higher kinded polymorphism, you're going to be repeating yourself a lot anyway so you might as well just implement Select directly.
Feel free to email me with exercise questions, in any case.
Code that takes a `Foldable f` is not particularly constrained in its behavior on lists given that there is `toList :: f a -&gt; [a]` which is the identity when `f = []`. But don't worry, your comment will get lots of upvotes anyways because it used the word "parametric" and they love that stuff here on /r/haskell!
C# and F# both took this approach, so it is probably less ridiculous than you think. Keep in mind that even though the type system does not support higher kinded polymorphism, higher order functions are supported at the value level, so you can use explicit dictionary passing and just write down something akin to `member mySpecializedTypeclassMethod args = generalizedTypeclassMethod myDictionary args` From the user's point of view, computation expressions and LINQ queries are probably both more featureful than do-notation.
You might like to know that I have written a [blog post inspired by your question](http://www.reddit.com/r/haskell/comments/2u04d9/haxl_antitutorial/).
Are the increased number of dependencies particularly problematic? How so? It's certainly possible, but at the moment the console version can both do plain text queries, and also spawn a web server. Having only one binary makes it faster to install and use for those people who want to do both. Would a "Cabal flag", e.g. -fweb=0 be enough for you? That seems like it could work.
Man, I'd love to be able to see Standard Chartered's codebase for some of this. Curse you, competitive advantage!
I'm learning Idris and just finished the first set of exercises. Both the lecture and video were really useful! I wasn't aware how the monad instance for `Vect n` should behave and ended up looking it up (taking the diagonal seems obvious in retrospect), but that was the only problem. The repl in Emacs didn't work, but that must be fixable. Proving things is fun! I like how you included that as part of the introductory session. Thank you!
You're right, I confused Select and Where I've fixed it so that it can't possibly be a source of confusion for anyone. The restriction of Nullable to value types makes sense as reference types can just be the null pointer anyway, although it does limit its use.
Me too :) Super interested in the relational library they are using but it's probably a secret!
Setting aside the standardization and GHC-versus-the-world concerns, I'd love to have an updated *document-that-looks-like-a-standard* for documentation and reference purposes alone. Contrast answering Stack Overflow questions regarding H98 or H10, where I can often start by pointing to the standard and saying "you can't do that" and then explaining why, to explaining similar concerns in modern GHC Haskell, where I seem to end up [poking through the GHC source](http://stackoverflow.com/a/27681166/1186208) to work out why a given chunk of code is accepted. Is there any call for the beginnings of a descriptive "report" of the language GHC 7.8.* accepts with `RankNTypes`, `GADTs`, the conservative typeclass extensions, `TypeFamilies`, `DataKinds`, etc.? It might be a fun project anyway...
And that compiler sounds awesome as well.
I've been silent on this because I don't see what the fuss is all about, even after reading the whole of the libs@ mailing list thread. From what I understand, I *lot* of work has been put into getting this BBP proposal into a release ready state and now a small group has shown up late to the party and wants to deep-six it. As someone who maintains 50k lines of yesod code (closed source) I am used to having breaking API changes force me to fix all the things. I don't like it, but I'm willing to sacrifice the time because each release of the Yesod stack is better than the last one. I expect the BBP proposal will be the same. A bit of pain up front and a much improved Prelude from here on. 
I'm sorry in that case I meant different implementations of Applicative. Or Monoid (obviously not for list, but see e.g. Product and Sum for different Monoids on the same type).
Generalizing the types of Prelude functions does equally little to help code readability for classes that aren't Foldable, but yes I have to work harder to argue why. There are basically two modes in which I read code to understand it. Either I am looking at the details of how a function is implemented, trying to figure out exactly what it does, or why it doesn't do what I think it should; or I just want a general sense of what it might do, and I look at its type. In the former case, suppose I encounter a use of, say, `mapM`. Currently, I know it's the `mapM` for lists, so I know everything there is to know about it. If the type of `mapM` is generalized, then I don't know whether it is being used at the list type, or Maybe, or some type which is a type parameter of the function I'm reading. That may or may not hinder my understanding of the surrounding code, but in any case, I can't possibly be *better* off than I was when I knew exactly which `mapM` I was dealing with. (I made this point also [here](http://www.reddit.com/r/haskell/comments/2hzqii/neil_mitchells_haskell_blog_why/ckxi77z) the last time this came up.) In the latter case, I'm looking at the function's type signature to learn what I can about the function's behavior without examining its definition in detail. This type signature is overwhelmingly often one that was written by a human to express their intent of what the function does. Even if the stars align perfectly so that only Traversable methods are used and, with their generalized types, the inferred type of the function would have involved a type variable `f` with a `Traversable f` constraint in place of `[]`, that doesn't do me any good if the type signature is written in terms of `[]`. In other words, for this kind of reasoning by parametricity, what matters is the declared type signature, not the types of the constituent functions that make up the definition (remember, I'm not even looking at the definition, or I would know what the function does already!) If someone goes to the trouble to write a `Traversable f =&gt; ...` type signature, then they can go to the trouble of using the generalized Traversable methods also. And yes, in that case I get some free information about the behavior of the function. But it isn't an automatic consequence of the types of functions like `mapM` being generalized.
Maybe this can help: There's some ancient technology called [Lava](http://hackage.haskell.org/package/chalmers-lava2000), and an even more forgotten thing called [Hydra](http://www.dcs.gla.ac.uk/~jtod/Hydra/). I recommend reading Hydra first because it is literate haskell98 and was developed for teaching. What they both do is let you model combinational (functional) and sequential (synchronous register-based) logic. It's not exactly the same as a neural network but maybe some concepts can help. A register is modeled simply as a `delay` fuction which takes an initial value and then after the first step outputs a stream one step behind its input stream. Siganls are represented as lazy lists -- more or less. It's the simplest thing that could work and it does work well. I've been down the road with Pipes, FRP, ST, StateT -- sometimes all you need is a lazy list.
I love it! Could I ask what kind of software you use to create those nice pictures? I know the software doesn't make you an artist but...
```Maybe a``` is the type constructor, while ```Just a``` is a value constructor. A type constructor like ```Maybe a``` allows you to construct types like ```Maybe Int``` and ```Maybe String```. So there. :) Edit: what /u/gelisam said.
Thanks this makes sense.
I know very little Haskell, as in not enough to start something right now. We are going to be learning it as we are doing the project. As far as what interests me in CS period, my main interest right now is cloud computing, but I have done web design, and object oriented programming for a little bit. I think as far as this project goes the only thing I'm really looking for is something that is not boring, and that I can actually use after? Dont know if that helps...
You really want to learn Haskell first, attempting a project prematurely leads to thrashing and takes much longer overall. I have a guide for learning Haskell at https://github.com/bitemyapp/learnhaskell - you can get help in IRC too.
Omega only holds up to an equivalence relation, but then encourages you to use it in ways that break that equivalence relation, so it is a bit cheaty. ;)
&gt; I recall hearing that there's only one law-abiding fmap for any type I recall hearing that there's even only one `fmap` that satisfies `fmap id = id`! I don't know where to go for a proof though.
That's fair.
&gt; I must say that I'm a bit disappointed by how imperative Haxl's API has been so far. This was a side effect of Haxl’s application at Facebook—efficiently batching I/O via many different imperative data source clients written in C++ and C. It’s certainly possible to come up with a nicer formulation of Haxl where the types are more descriptive, but there was no real pressure to do so when it was being built. 
There is literally zero magic or sophistication in the relational algebra library we have. All it is, is a straightforward, **efficient** implementation, **in C++**, of **in-memory** relational algebra with a **mostly untyped** interface, with primitives like relation :: (IsTuple row) =&gt; [ColumnName] -&gt; [ColumnName] -&gt; [row] -&gt; Relation relation columns key rows = ... and combinators like join :: Relation -&gt; Relation -&gt; Relation extend :: (a -&gt; b) -&gt; [ColumnName] -&gt; [ColumnName] -&gt; Relation -&gt; Relation As you can see it's all untyped; in `extend`, for example, there's no static check that the function takes a tuple of the right arity and returns a tuple of the right arity (maching the number of input/output column names). 
See my other reply to /u/mightybyte. There's not much to be interested in in that area.
I think there's plenty of interesting things about efficient relational algebra implementations. I'd like to try Fritz Henglein's ideas. 
Sorry, I didn't mean there's nothing interesting in the area of relational algebra; more like there's not much interesting in our particular implementation of it.
Thanks for the material! A couple questions for ya: 1. I'm totally new to computer-assisted theorem proving, and I'm having a hard time figuring out what's going on, which is driving me nuts. For example, I don't know where to start with the "takeDropOk" proof, even though I've watched your first lecture, I've read the Idris tutorial, and I have a degree in Maths. Do you know of any good introductory resources (not necessarily Idris-specific)? 2. Will you be posting the solutions to the exercises? I always find it helpful to be able to resort to understanding a solution if I fail to come up with one myself.
I thought that there ought to be a linear algorithm for doing a `foldl` type operation. Thanks for bringing this up!
And `tail, `init` and `last`. Any others?
Wait what? Just a is valid Haskell in the context above (it is the actual definition of Maybe). It is also valid Haskell when used for pattern matching.
Duncan's proposal is not to actually use Nix itself, just to steal some of the ideas. 
Thank you for the reply! Im going to look more at these and see which ones look the most realistic for us to do. I appreciate the resources.
That reads: &gt; Just is a function that receives a value of type a, and returns a value of type Maybe a. Just is a value constructor. Just 5 is a value. Notice there are 2 separate levels here. One is the value level, where `Just 5`, `Just True`, and `Nothing` lives. Other is the type level, where `Maybe Int`, `Maybe Bool`, etc lives. Those levels are connected in a way: things at the value level are tagged by things at the type level. `Maybe Int` is used as a tag to say that a value can be any of those `Just 7`, `Just 9`, `Nothing`, and so on. You could say that, on the type level: &gt; Maybe is a function that receives a type and returns a type. Maybe is a type constructor. Maybe Int is a type. (That is not the terminology used, though.)
Since you mentioned cloud, maybe try a distributed message queue or a simple key-value storage system? I don't know how much cloud experience you have but most things tend to become interesting once you involve distribution.
&gt; Perhaps a more positive way of approaching this would be to ask: can we improve Haskell to make this usage safer? We don't really need to improve the language. We can use `newtype` to create a type that has the requisite `Num` instances. {-# LANGUAGE GeneralizedNewtypeDeriving #-} import Control.Applicative newtype NumFunctor f a = NumFunctor (f a) deriving (Functor, Applicative, Monad) instance (Applicative f, Num a) =&gt; Num (NumFunctor f a) where fromInteger = pure . fromInteger (+) = liftA2 (+) -- ...
HN discussion: https://news.ycombinator.com/item?id=8963000
Is it something technical about Erlang that supports this, or is it the philosophy of its users?
GHC 7.8 wasn't fully Haskell2010 compliant either, see [Divergence from Haskell 98 and Haskell 2010](https://downloads.haskell.org/~ghc/7.8.4/docs/html/users_guide/bugs-and-infelicities.html#haskell-standards-divergence) `-XHaskell2010` in GHC 7.10 is still as close to H2010 as it was in GHC 7.8. It's just the library-part of the Report where we'd need more machinery to provide a fully Haskell2010 compliant compatibility-mode (which needs to allow to define a `Monad` instance w/o also defining a `Functor` instance -- and then there's the question whether you want such programs to linkable with `base-4.8`-linked-packages)
If we pretend that we can write lambdas over type expressions, we could define `Maybe` as: data Maybe = \a -&gt; Nothing | Just a So we can think of `Maybe` as a function which takes a type, and gives you a type. Or, in other words, it's something we can *construct types* with -- i.e., a *type constructor*. In contrast, `Just` takes a piece of data, and gives you back a new piece of data. So we can *construct data* with it --- i.e., it's a *data constructor*.
*Update2: Went ahead and set the main homepage to 302 redirect. Fingers crossed no significant breakage arises.* Well, it's a big improvement in that the response time on first load has gone from 35s to 2-3s or so (yay!), but I'm still seeing the occasional 6s+ load times, which really isn't great. There are some mediawiki php performance tuning tips here: http://www.mediawiki.org/wiki/User:Ilmari_Karonen/Performance_tuning#Use_a_PHP_cache Any chance that someone from the infra team could do a pass over the server &amp; make sure that it's not doing anything dumb. It really looks like it's regenerating a lot of stuff from scratch every time at the moment. It would also help if the page didn't have to serialise the load of jquery &amp; related .js every time - but I'm not sure whether it's possible to load those scripts asynchronously. Are they actually required for the Haskell wiki? It doesn't seem very dynamic... (NB, to all the infra team - I totally understand that everyone has day jobs &amp; lives &amp; I don't want anyone to feel un-appreciated for the work they've done on the Haskell website - I just feel that performance is one of those things that just gets left because it's no-ones responsibility, yet it has a huge impact on people's perceptions. I'm shaking the tree to see if any easy wins fall out that people might just have missed, not expecting people to sink endless days into this. At least hopefully not!)
Cool, have a ticket: https://github.com/ndmitchell/hoogle/issues/98
TL;DR: Both. Since OTP/Erlang had it, programmers used that. And since the philosophy appeared a good idea, it was never touched in OTP/Erlang. ---- The important thing is that Erlang/OTP was designed for telecoms. If the telephonic switch ever failed, that would be a disaster (e.g. people could die from not being able to call for ambulance – it is *that serious*). So Ericsson's lab searched for a language that would fit the job. No such language was there so they have created Erlang and accompanying toolkit called OTP - open telecom platform. OTP platform is a kind of standard library. It's so deeply integrated into Erlang itself that you'd often see Erlang/OTP (like GNU/Linux but more often). All of OTP platform exploits lightweight processes (with preemptive scheduling which gives nice *low-latency*; cf. Scala with JVM's lightweight-threads that are cooperative and has nicer *high-thoughput*). As an example, *observer* behaviour from OTP is basically a thing that allows you to say: "try to run this thingy; if it fails rerun it (in case some random error occured), however if it fails too many times in a time window then propagate failure" – with this you design (in a rather explicit way) the disaster recovery scenario. And that's all good. There are few such components, all work together very well and all have this "prepare for problems" philosophy baked-in. Since it's so useful, nearly everybody uses those components, and thus everybody plan ahead of disasters. Note that somebody *could* design own components with similar behaviour/guarantees, however I have never saw such; also the problem itself is "not simple", especially since Erlang/OTP is/can be distributed. My thesis software (robot control) was done this way and I planned ahead of failures that I never saw. But if they would happen, I was already prepared. And that preparation was nearly free since all the tools were there. BTW: since Erlang is functional, I had an excellent Quiviq's quickcheck available. One of my properties failed and that's when I recalled I used UDP deep down in the protocol – what I call a "nice reminder" :)
Is the video up anywhere yet?
* `Just` is a _data_ constructor. * `Just ()` is a term that inhabits the `Maybe ()` type. Regarding types and type constructors, there are two schools of terminology. # Historical In one, types are only things that have terms inhabiting them, and type constructors are things that take a bunch of types and yield types. In this vocabulary, which is the one you asked about: * `Maybe` is a type constructor, it takes types to types. * `Maybe Int` is a type. In this vocabulary a types inhabits kind `*` and type constructors "takes types to types". The problem that I find with this vocabulary, though it is that we have 'type constructors' that can sometimes take other 'type constructors' to 'types' or 'type constructors' as well. e.g. newtype StateT s m a = StateT { runStateT :: s -&gt; m (a, s) } StateT is a 'type constructor' but it takes a 'type' and a 'type constructor' and a 'type' to a 'type'. That is a big mouthful that doesn't actually tell us a lot. On the other hand looking at the kind of `StateT` does: StateT :: * -&gt; (* -&gt; *) -&gt; * -&gt; * StateT's second argument isn't just "any type constructor" but rather one of kind `* -&gt; *`. So the types vs. typeclasses divide is just between * and stuff that takes other arguments and returns *, and under a strict interpretation of such readings would be * -&gt; ... -&gt; *, but `StateT` breaks that mold already. # Informal Usage Another vocabulary that is used more informally and less often in papers is to just lump both what the former convention calls a type and a type constructor into 'type'. So in that setting 'Maybe' and 'Maybe Int' are both 'types'. In this context we _can_ use the 'type constructor' notion when it is clear we're talking about a type that isn't kind `*`, but by letting 'type constructors' be 'types' now `StateT` being a type constructor just means it takes 'types' as arguments, and its not nearly so verbose and finicky to phrase, but distinguishing such cases isn't really the emphasis. The downside is that in this vernacular, however, since type constructors are types too, it becomes necessary to distinguish "types of kind `*`" from time to time and it lacks a good short phrase for that concept than "types of kind `*`". Personally I tend to fall into the latter camp, with terms inhabiting "types of kind *", types inhabiting kinds. This vocabulary makes it a lot easier to talk about things like the kind 'Constraint', where the dictionaries for typeclasses inhabit types of kind Constraint Eq :: * -&gt; Constraint Eq Int :: Constraint Unboxed values inhabiting types of kind '#', where code is evaluated strictly gives rise to only 'types' having terms that inhabit them. Also we have data-kinds which pop up in Haskell ever since Brent Yorgey et al.'s ["Giving Haskell a Promotion"](http://research.microsoft.com/en-us/people/dimitris/fc-kind-poly.pdf). Each of these 3 latter concepts clearly exist at the 'type' level of the hierarchy, but are neither fish nor foul according to the older 'types' vs 'type constructor' classification scheme, being able to refer to them all as 'types' lets you unify your thinking across them and makes things like kind polymorphism make a lot more sense. "For the test", you probably want to go with the former vocabulary, it is what you'll find in papers on the topic, and in their day extending Haskell with "constructor classes" like "Monad" was a big deal. It is very important to be able to read these things and understand what is being said. But for all that, for actually thinking about them, I tend to find the latter vocabulary described above to be more useful, and you'll occasionally find folks in the wild who lapse into it and confuse you. ;)
The other commenters have already done a great job talking about this, but I'll add that Erlang's technical benefits (w.r.t. process isolation!) are probably already available more-or-less in GHC's runtime. We don't yet have a great code portability story (though static pointers are heading that way), but if you focus on just the local runtime we have a best-of-breed asynch exception story along with a runtime which handles massive numbers of cheap green threads nicely. I think we need a "Local only OTP" [0] to embody some principles of Erlang's process isolation. It would help with composing large applications. I think current work is being done here a bit with Tekmo's contributions like MVP, but OTP goes much further. "Supervisor trees" alone would be an interesting added power. It's an interesting conflict, however. Erlang achieves this culturally through "crash first mentality"—the literal opposite of total programming! Erlang strongly encourages partial pattern matches, for instance. The idea is that code will fail for such inscrutable reasons that you better not spend time even thinking about them. Just crash immediately and have this really incredibly robust OTP system in place to restart your system. This is so dramatically opposed to parts of Haskellean philosophy and, simultaneously, so incredibly *right* in the Erlang context that I'd *love* to see what would happen if it got taken seriously and implemented in Haskell. [0] Some of this is already being written in distributed-process. There's actually a ton there already! Unfortunately, I think that the scope of achieving this along with code distribution is too wide. Some day it'll be amazing, but we can really benefit (and experiment!) from process isolation at the purely local level much more quickly.
I doubt it's "wholly new". I doubt base and Prelude are "completely re-implemented". The slides don't make that claim. He also says they use both GHC *and* Mu. I'm not sure what you mean by "all this enterprise Haskell goodness". Is it unreasonable that a high-tech finance company would have its own domain-specific libraries, and that they would hire someone clever to build support for those libraries into a compiler?
It's not unreasonable for them to do it no. It just suggests that Haskell is not exactly a good solution if you don't have the resources to do this and this should definitely have been mentioned in the caveats. One of the perks of using something like Java, C++, or PHP is that you can just coast off of work already done for you in large, comprehensive, freely available libraries.
This point confused me a little. As far as I was aware thyme was already a fine library. They probably needed some extra domain specific knowledge over and above working with standard time formats.
Since `T.mapM` doesn't appear in the code you posted I'm guessing there's something missing.
I played briefly with defining an alternate `Applicative` instance for `Stream` using `concatInits` and `concatRInits` -- transform a stream -- s0 :~ s1 :~ s2 :~ ... :~ sn :~ ... -- into a stream -- s0 :~ -- s0 :~ s1 :~ -- s0 :~ s1 :~ s2 :~ -- ... -- s0 :~ s1 :~ s2 :~ ... :~ sn :~ ... concatInits :: Stream a -&gt; Stream a concatInits = go id where go f (x :~ xs) = let f' = f . (x :~) in f' $ go f' xs -- transform a stream -- s0 :~ s1 :~ s2 :~ ... :~ sn :~ ... -- into a stream -- s0 :~ -- s1 :~ s0 :~ -- s2 :~ s1 :~ s0 :~ -- ... -- sn :~ ... :~ s2 :~ s1 :~ s0 :~ ... concatRInits :: Stream a -&gt; Stream a concatRInits = go id where go f (x :~ xs) = let f' = (x :~) . f in f' $ go f' xs newtype ZigZagStream a = ZigZagStream { getStream :: Stream a } instance Functor ZigZagStream where fmap f = ZigZagStream . fmap f . getStream instance Applicative ZigZagStream where pure = ZigZagStream . pure ZigZagStream fs &lt;*&gt; ZigZagStream xs = ZigZagStream $ concatInits fs &lt;*&gt; concatRInits xs but then I realized it broke [the `Applicative` laws](http://hackage.haskell.org/package/base-4.7.0.2/docs/Control-Applicative.html). Still it's useful to know there's a `Stream` equivalent to `[ f x | f &lt;- fs, x &lt;- xs ]`.
Here is an example of the sort of domain-specific time calculations you will need to handle in finance :http://en.wikipedia.org/wiki/Day_count_convention
From what I understood, being able to serialize closures was something Cloud Haskell was explicitly meant to prohibit because it complicates the implication too severely to be worth it, but I may be wrong. Absolutely agree that Haskell's modularity and safety are potential selling points. You can get consulting help on this front at least. Speaking of which, what did you do? Did your firm hire consultants to do the initial work at all? Which parts of the core libraries did you find to be sufficient and how much do you use the prelude? tomejagaur's Norwegian effort, to my knowledge, breaks with the Prelude and with idiomatic Haskell pretty hard. Not sure what Chucklefish is planning.
Type contructors create types. Type elements belong to the set of Haskell types, they are essentially used at compile-time by the compiler to check if the program is correctly typed, or to infer types. They can't be manipulated at runtime by your program. Value constructors create values at runtime that will be manipulated by your program.
Fixed it to the correct error
It makes sense in retrospect. We can form instances for both newtype Equivalence a = Comparison { getComparison :: a -&gt; a -&gt; Bool } and newtype Comparison a = Comparison { getComparison :: a -&gt; a -&gt; Ordering } which do the obvious things, and Fritz's construction is clearly related to those simpler constructions. It is just very pleasing to have it drop out so nicely.
I'd usually write it out with something like: instance Traversable Trie where traverse f (Trie ma cs) = Trie &lt;$&gt; traverse f ma &lt;*&gt; traverse (traverse f) cs This lets you delegate to the instances of Traversable for `Maybe` and `Map Char` internally.
Yeah, agreed. It was more the entire buildup of reading his paper and then reading your Haskell noodlings that was so nice. You don't get an inkling while reading the paper that his algebras are anything more than convenient and sufficient. Then, suddenly, `Distributive`.
The main insight there came from looking at the shape of his sum and product cases and realizing they behaved like the ones for Equivalence and Comparison logically. If you take Ordering and you "Coyoneda" all the constructors, you get an "initial"-style encoding, but it's too big, the laws relating divide/conquer etc aren't being enforced. The distributive laws only hold up to a quotient there, but after it collapses down to Disc everything is good again. On the plus side, this helps put Fritz's work on a firm theoretical footing categorically. When we get to the work on joins there are some rough edges though. The combinators mentioned above deal with conjunction and add support for 'impossible' cases and disjunction well, but the Decidable side of things isn't the 'or' side of his logical connectives -- it is different, but I think there is a logical problem with them, in that the predicate (p || q) in his machinery will repeat a record if it matches both predicates, which isn't what one would expect. There are some other differences, too, such as by using the final encoding my version above forces you to supply the predicate at the join rather than separately later. In exchange you get to be an instance of all the things, and it is a lot easier to map over and the like.
Boolean blindness is the idea that a function like `a -&gt; Bool` is difficult to interpret because once you take the resulting Bool out of its provenance then its meaning is lost. If you replace that function with something like `a -&gt; Maybe Proof` where `Proof` is a type which *exemplifies* truth (e.g.) instead of merely claiming it like the value `True` does then even outside of its provenance `Proof` still retains all of its meaning. You can do a half-job of this with phantoms. Instead of getting a mere Bool you get something which has been type-constrained to match the provenance. Thus, passing that result forward you cannot accidentally use it in a location where provenance has become confused.
Also, instead of using Maybe, another approach is to create domain-specific bool types. The Path and From types in [data-filepath](http://hackage.haskell.org/package/data-filepath-2.1.0.1/docs/Data-FilePath.html) are good examples. It's a little less syntactic overhead in cases where the domain is really two alternatives rather than the presence/absence of something.
That @jaseemabid here :) I hosted one haskell [meetup!]( http://www.meetup.com/The-Bangalore-Haskell-User-Group/) and I'm always looking for similar minded people. If there is sufficient interest, we can meetup afk anytime and see what we can do together. 
You can also use deriving Traversable, with GHC's extension DeriveTraversable?
Can I stay at your place? :) 
A lot can be done with libraries in Haskell, but there are some technical advantages, especially around responsiveness. For example, per-process heaps prevent GC from blocking other processes (the only values that can be directly shared between process are "binaries", which are immutable flat refcounted byte strings). There are also things like taking some care in C regex implementation to return to the scheduler at regular intervals: http://www.erlang.org/eeps/eep-0011.md
It sounds like [Halcyon](https://halcyon.sh/) could help. Is your project open-source?
&gt; In fact we’ve already been experimenting in this direction. The current cabal sandbox feature does enforce consistency within each sandbox. This seems to work ok in practice because each sandbox environment is relatively small and focused on one project. Interestingly we have had some pressure to relax this constraint due to the cost of creating new sandboxes in terms of compiling. (Allowing some inconsistencies in the environment allows the common packages to be shared and thus only compiled once.) Fortunately this issue is one that is nicely solved by Nix environments which are extremely cheap to create because they allow sharing of installed packages. To provide a different viewpoint, this has been the main reason I've been unable to use cabal sandboxes for anything substantial. Many projects involve multiple packages developed together in a single sandbox. When changing anything about a package low in the hierarchy of these, cabal would constantly try to reinstall the higher level packages (and failing) or completely fail to come up with a build plan, forcing me to delete the sandbox. So please consider this use case in any future direction for cabal: a set of interdependent packages being developed, already fully installed in a sandbox/environment. Then the developer changes one of the lower lying packages in the dependency graph -- adding a feature, relaxing a dependency, etc. They want to do this in isolation, without bothering with the higher-level packages. Once this is done, they want to move on to the next package, and so on.
&gt; Speaking of which, what did you do? Did your firm hire consultants to do the initial work at all? No, we started our own company and two of the four initial people were familiar with Haskell from a software technology master's at Utrecht University. So we just started developing our backend in Haskell (and our frontend in Javascript) and learned as we went along. The amazing thing is that we built an initial backend quickly, and then didn't bother with it for about a year while we built and experimented with the (complicated) UI. We could completely ignore it because it just worked! &gt; Which parts of the core libraries did you find to be sufficient and how much do you use the prelude? tomejagaur's Norwegian effort, to my knowledge, breaks with the Prelude and with idiomatic Haskell pretty hard. Not sure what Chucklefish is planning. We use many libraries: all the usual suspects (many of the core haskell platform packages, stuff like aeson, fclabels), two web frameworks (happstack and snap) and we're just now integrating opaleye. We also built stuff ourselves, parts of which we've open sourced (the [rest](http://silkapp.github.io/rest/) suite for building web APIs being the biggest). I'm not sure what you mean by "tomejagaur's Norwegian effort" (I assume you mean /u/tomejaguar?) or Chucklefish. Regarding the prelude, we have our own package. It offers an extended prelude, which is just the normal one with things like `Data.Maybe`, `Data.List` and `Control.Monad` added by default, and a polymorphic one, which replaces all operators in the Prelude by those from `Data.Foldable` and `Data.Traversable`, as well as those from `Control.Category`.
Actually, worked on it for a few minutes with sclv as we're here in New York in advance of Compose.
&gt; I'm not sure what you mean by "tomejagaur's Norwegian effort" Personally I don't have a Norwegian effort, but I don't know about tomejagaur.
In the type level, `Just a` is sort of an "abuse of notation" that Haskell uses in its type definitions. Here, `a` is a type variable. Formally, the GADT syntax is more "sane": `Just :: a -&gt; Maybe a`. In the term level, `Just a` is just a constructor applied to a term variable `a`. In the pattern level, `Just a` binds the stored value to a new term variable `a`. So yeah, it's overloaded to mean 3 very different things in Haskell :\
Why does the spec matter much, if for so many years we've all ignored it and used GHC Haskell anyway? Does the spec affect you in any way?
That's not what I meant. I was meaning that it seems like the Prelude and base libraries are not good enough quality to use as is.
&gt; Does the spec affect you in any way? The Haskell98 report was one of the most attractive features of Haskell (to me) when I found it. I still attempt to write all my code to the Haskell2010 standard unless I'm specifically playing with an extension. I actually *learned* both Java and Haskell by reading their specifications (along with experiments, and a few examples); I find the rigor of a specification to be very valuable when I'm reorganizing how I think for a language. I also am not a fan of vendor-lock-in, and writing to the specification (rather than the implementation) helps avoid that.
Do you think the same think about C++ since Boost exists? Or Java since Guava exists?
It is clearer to say that `Foldable` 'is' generalized `mconcat`, i.e. `fold` in its sense. This seems to be the point of view of the module. `foldMap f = fold . fmap f`, `foldr op a fs = appEndo (fold (fmap ( Endo . op) fs)) a` and so on. `foldMap` isn't the essence of the matter since it works with the all-important types Vector, ByteString and Text; it is generalized mconcat that doesn't.
The spec is nice documentation, then, and exists (or can exist) for GHC Haskell too. I think a tiny minority learns from specs... Also, despite the spec it seems we're stuck with vendor lock in for a long time. Not to mention competing implementations can adhere to the ghc spec rather than Haskell spec. I personally think the Haskell spec has no real standing and unlikely to have any standing, so we may as well ignore it and improve the real language we're all using, ghc.. Long live ghc and let's hope more compilers in the future learn to compile ghc Haskell :)
Idris should be installable with `cabal install idris --constraint='blaze-markup &lt;0.6.3'` - does that not work? Hopefully a new Trifecta release will fix this soon.
Neat :) I haven't really fully thought this through, but for finite graphs, could you identify a DAG with its topological sort, and restrict node N to only being able to point to nodes 0..N-1. Something like {-# LANGUAGE GADTs, DataKinds #-} module Main where data Nat = Z | S Nat data LE n where LEZero :: LE (S n) LESucc :: LE m -&gt; LE (S m) data DAG n where Empty :: DAG Z Node :: [LE n] -&gt; DAG n -&gt; DAG (S n) test :: DAG (S (S (S Z))) test = Node [LEZero, LESucc LEZero] $ Node [LEZero] $ Node [] Empty
I'm sorry! I don't know who I was thinking of, then.
&gt; I don't really care if it is non-standard, you can go and download boost without much fuss and get working with it immediately. As you can for any package in stackage, and many of the packages on hackage. &gt; I keep seeing blogposts telling me to stick to the regular prelude when writing libraries, because of how you cannot hide instances that you import. Those blogposts are mostly wrong. It's incorrect to even think about hiding a typeclass instance, much less to make it some sort of requirement. That said, hackage is the "wild west" so there may be some very poor libraries out there; there are basically no quality checks. You'll have to use some other metric for quality besides presence in hackage. &gt; I got from the presentation was that if Standard Chartered has their own internal version of a boost for Haskell that they will never show or license to the outside world. My impression was that a lot of the code was fairly focused in their market segment. They did mention a more "enterprise" prelude, but hackage has those (classy-prelude, maybe?). Given what I've heard of Standard Chartered, I would not expect any free software to come out of them soon, Haskell or not.
If you want to see more along these lines, check http://agda.github.io/agda-stdlib/Data.Graph.Acyclic.html#1. Also, it's roughly a restricted form of the design used in FGL.
I guess I should just use whatever Prelude I want and apologize later?
I don't think there's a simple answer to that. Do keep in mind that someone you might want to apologize to is your future self. &gt;:)
Yes, I looked at Halcyon and it's very cool. I read the tutorial but I still fail to see how it could help? The problem is that gitit has upper bounds on some of it's dependencies and it's impossible to get a consistent package set. Using `--allow-newer` does not help due to build errors (well, the upper bounds are there for a reason...).
Ah I read `Functor` in yes, thinking ahead to Traversable; my reasoning was everywhere presupposing this.
I think one thing that I feel us missing is the whole interaction with the Repl and the whole code is data philosophy. I love Haskell, but I think the best language would be a combination of the two, plus some way of denoting impure subroutines that you expect to eventually return a pure value (I. E. Something like the dancing pointer algorithm), though I haven't gone to far into IORefs so maybe those will be what I want. 
I guess I passed exactly through what /u/briticus557 is going through, so I might advance his endeavor and advise: yes, look at Shen. But don't expect to make anything on it since obviously the community is so small, no libs, no GHC, etc, etc. Unfortunately, there is not any practical pure functional language with S-expressions in the world today. Feel free to develop one, though! A compiler to something to Haskell wouldn't be too daunting to implement, I guess.
https://wiki.haskell.org/Haskell_Lisp has a bit of an overview.
As far as the "how polymorphic?" question goes, I'm don't feel strongly enough to be especially committed to either side without reservation. On the one hand, I definitely enjoy the polymorphism of things like Foldable, Traversable, etc; and almost never feel the need to use the monomorphic variants. On the other hand, the more polymorphic things get, the less helpful the error messages become. As an expert, that doesn't bother *me* so much; but as someone who teaches introductory Haskell, I can certainly appreciate how much that affects the learning curve for my students. I probably lean more in the direction of polymorphism than the average Haskeller, but not strongly enough to feel like I need to push my views on others. (I'd much rather spend my time pushing other views on y'all ;) On the other hand, the BBP does hit on a really ugly wart in the prelude. Namely, the prelude exports a whole rack of list functions it really shouldn't. In order to use Foldable/Traversable/etc, we need to either qualify the names or exclude whole chunks of the prelude; leading to things like [Prelude.Listless](https://hackage.haskell.org/package/list-extras-0.4.1.3/docs/Prelude-Listless.html). Not to mention that many of these prelude functions are too specialized to be used in actual production code. Not to mention that many of these prelude functions are emulating set-theoretic operations, but the versions on actual Sets are far more efficient. Et cetera, et cetera. ... Is generalizing these functions to work on things other than lists the solution? I dunno. Would it be better to just remove them entirely and make people import Data.List or Data.Foldable/Traversable as desired? I dunno. But there's definitely an ugliness here. It's far less dire than the ugliness motivating the AMP (imo), but an ugliness nonetheless. 
You can use any file. I just needed a word to reference the definitions file and I chose .setdown. And I took you advice and added an example to the blog. But if you want more examples then you should just check out the example repository and give it a spin!
I just added an example to the blog post.
You're right. Though I suppose I equate the two because I believe they are connected. Lisp is the AST, and it is trivial to parse, so it is conducive to those features which manipulate the language itself. 
My thought was to implement something that transcompiles to Haskell as a learning exercise. 
Do you mean the same Gitit which is one of the examples featured on the [Halcyon examples page](https://halcyon.sh/examples/#gitit)? The architecture of your project is not clear to me. If you’re saying Gitit is one of your dependencies, and you mean you’re using the `gitit` executable, then I don’t see why Gitit needs to share the same sandbox as the rest of your dependencies — it can be built using a separate sandbox. With Halcyon, you can simply declare Gitit as a run-time dependency by using the [`HALCYON_EXTRA_APPS`](https://halcyon.sh/reference/#halcyon_extra_apps) option. Halcyon will then automatically build Gitit in a separate sandbox and install it together with your app. To learn more, check out the example apps which use the very similar [`HALCYON_SANDBOX_EXTRA_APPS`](https://halcyon.sh/reference/#halcyon_sandbox_extra_apps) option. You could also chat with me in #haskell-deployment on [freenode](https://freenode.net) — I’ll be happy to help get your project up and running.
I am out of town in the next weekend. How about the following one? And, the link to Haskell mailing list timing out?!! 
Sure. Please join the bangalore haskell meetup group. Perhaps we can use the list available there.
&lt;pedantic&gt; don't you mean: "return (x+y)"?
Somewhat shameless plug: My `ihaskell` library can probably be used similarly to `hint`, but provides the full power of GHCi (parse things, run imports, set flags, declarations, expressions, etc, all within the same environment). I don't think anyone has used it like this yet though, so I'm sure there are some warts. That said, the only benefit over using `InteractiveEval` from the GHC API is that the GHC API is horribly documented and GHC's source code is often hard to read.
Actually, the code is mostly pretty boring. (That's more or less, how it should be.)
ghc is probably more awesome, and already available for everyone to have a look into.
The tools are mostly standard. People use mostly emacs, vim and Visual Studio. (And a few others.) Version control is in git. Documentation is via haddock and an internal Hoogle. (Neil Mitchell, author of Hoogle, works for SCB after all.) Really, nothing too exciting.
&gt; So essentially all this enterprise Haskell goodness is just proprietary secret sauce for Standard Chartered and of no real help to any of us wanting to write software in Haskell? It's the other way round, usually. The open things are often better, and the proprietary versions are a mechanism for coping when the open versions can't be used for one reason or another. There's some proprietary secret sauce to do with finance. (Eg, as Don says, just a really robust date handling library. International holiday calendars are serious business in finance.) But the Haskell community as a whole wouldn't have much use for this specialised stuff.
&gt; Is that a library that could be spun out at some point? That would be hard. The secret is not just in building one library once and for all. On a global scale, holiday calendars change all the time. So you have to get good sources of calendar data, and integrate them well.
I have two questions. 1. How does this compare to Shelly? What was lacking in Shelly that caused this to be made? How is this better or worse than Shelly? 2. Might it be better if this were built on top of shelly? Having multiple competing solutions to the same problem can sometimes be a bit confusing – see the current conduit / pipes split.
Hi, I live in Delhi, although I haven't found anyone else living nearby who shares my enthusisasm for Haskell and FP.
You have to be careful with your connection of `Just 5` and `Maybe Int`, however--I've seen beginners get very confused here because they think they are *directly* connected: `Just` -&gt; `Maybe` and `5` -&gt; `Int`. But that's more an accident of the definition of `Maybe`. To compare data Example a = MkExample (Int, a) -- the Value Constructor ghci&gt; :t MkExample MkExample :: (Int, a) -&gt; Example a -- an example value ghci&gt; :t MkExample (5, "hello") MkExample (5, "hello") :: Example String -- the Int vanished from the type! -- type constructor ghci&gt; :k Example Example :: * -&gt; * -- concrete type ghci&gt; :k Example String Example String :: * 
You're totally right, it's quite rough at the edges* :) The relevant modules are [IHaskell.Eval.Evaluate](http://hackage.haskell.org/package/ihaskell-0.4.3.0/docs/IHaskell-Eval-Evaluate.html) and [IHaskell.Eval.Util](http://hackage.haskell.org/package/ihaskell-0.4.3.0/docs/IHaskell-Eval-Util.html) – someday I will have time to extract all of these bits into a separate library. *aka not really ready
Hello! May I see your code that does this please? :)
`shake` also comes with its built-in shell-scripting facilities... it's starting to get confusing to know which shell-abstraction to use when... :-/
This looks sooo great! But I really miss subshells and command tracing like shelly has. Are there any showstoppers to add that? Maybe even by reusing Shelly.Sh, as suggested by /u/Niftylon? 
Awesome!
Just a side note that you can't infer anything about the Kind of Maybe Int# from that error. What it's saying is that ghc refuses to do type unification on unlifted types - it might be possible to unify the types, but ghc isn't going to do it. (We saw something similar on r/haskell a month or two ago when someone was asking why a type level flip x y wouldn't simplify to the expected type if one of the types was a #'d one.)
It gets weirder: &gt; :k (-&gt;) Int# Int# &lt;interactive&gt;:1:6: Expecting a lifted type, but ‘Int#’ is unlifted In a type in a GHCi command: (-&gt;) Int# Int# &gt; :k (Int# -&gt; Int#) (Int# -&gt; Int#) :: *
Thanks a lot, Neil.
The slides did mention a cross-platform GUI library that builds against WPF on Windows, which sounds like something Haskell could use.
Oh, fair enough. What fun. :)
Shake doesn't require you to use the built-in shell-scripting facilities, so you can combine this library with Shake quite easily. When I wrote my version for Shake there wasn't a clear favorite that I thought would integrate nicely with Shake and gave all the features/interface I wanted. I have always hoped to switch Shake to using a 3rd party library at some point.
&gt; every time a library is needed, install the latest version of that library The *latest* version of a library is not always the one you need. That's why packages define version ranges of libraries they depend on. &gt; in a different folder (instead of updating the one already there) Cabal does already try to re-use installed library versions if they can be integrated into an install-plan to satisfy the goals requested. Can you provide an example where Cabal does something your proposed scheme would do differently? PS: are you perhaps alluding to something like https://github.com/haskell/cabal/issues/2365 ?
I see that it uses Text everywhere. Is there a neat, quick way to avoid the String to Text convention. I am currently using optparse-applicative (which provides `str` builder only). Of course I know I can just do a `T.pack` in the `Parser Options` myself (in one place) but still ... As a related question do I really need to do this ? run (Options {role, zone, extraArgs}) = do Right basedir &lt;- toText &lt;$&gt; pwd proc "docker" [ "run" , "-w", mountpoint , "-v", basedir &lt;&gt; ":" &lt;&gt; mountpoint , "-t", dockerimg , format cmd role zone (fromMaybe "" extraArgs) ] empty ... The pack from `FilePath` to Text feels a bit clanky ;-) Also it would be nice to have an example of `proc` or `shell` that uses the extra `Shell Text` arg. Turtle looks quite nice ! Thanks for making it available.
&gt; The latest version of a library is not always the one you need. That's why packages define version ranges of libraries they depend on. Yeah, sorry. I meant the latest version of the library that you can afford for any particular installation. &gt; Can you provide an example where Cabal does something your proposed scheme would do differently? Sure. Sometimes you need to install some library not within the sandbox scope and a cabal message in the form: cabal: The following packages are likely to be broken by the reinstalls: ... Use --force-reinstalls if you want to install anyway. appears. This makes me think that updating the libraries in question would likely break the other packages. Also when you do: `cabal install x`, if `x` already exists it is updated (not installed again just as a different version). So if something else was using the older version of `x` then you get that message above.
Good question. Herbert recently went and laid some technical groundwork by setting up https://github.com/haskell/haskell-report to make it easier for such a committee to actually edit and build a version of the Haskell Report. The rest of it would be the messy human social aspects of getting folks enthusiastic about doing it, trying to figure out a process to get members involved, talking to folks on the existing committee to see how such a process could be done with an imprimatur of legitimacy, reaching out to developers of other Haskell implementations, trying to figure out scope, etc.
To be honest, I don't think you're going to get or find really useful answers until you bite the bullet and recompile with profiling activated. It's ugly and really annoying, but given Haskell's general unpredictability when it comes to optimisation, being able to pinpoint the approximate location of performance issues is crucial even for smallish code bases. Of course the Haskell community may surprise me with increased awesomeness, in which case good on you. ;) What I will say is that type classes are prone to killing performance in tight loops. You should probably drop all those `Floating a` contexts and just go with `Double`, or at least specialise your functions using the `{-# SPECIALIZE #-}` pragma. Like /u/oiuhoihiuhioh says, this will also allow you to use specialised and potentially unboxed data structures, which should be a huge performance gain. GHC will sometimes make these kinds of optimisations for you, I think, but I've found that to be particularly unreliable.
We have / had a limited form of sub-kinding in the kind system to make `(-&gt;)` work. We used to have both `??` and `?` as kinds. `(?)` was a "super-kind" of `(# #)`-style unboxed tuples, `#` and `*`, the other `(??)` was a super-kind of `#` and `*`. ? / \ (# #) ?? / \ * # Back when `(-&gt;) :: ?? -&gt; ? -&gt; *` was what `:kind` would show it was saying that `(-&gt;)` can accept boxed and unboxed values, and return boxed and unboxed values and unboxed tuples. Nowadays we just have one such thing, `?` and `??` are fused together. Since then, `(-&gt;) :: ? -&gt; ? -&gt; *` would be how we'd express the new kind using the old vocabulary, and `(??)` is just removed. So, now, functions can take unboxed tuples as arguments and that wart/limitation has been removed. Somewhere along the way GHC stopped showing the kind of `(-&gt;)` with `(?)`'s in `:kind` though. I don't know precisely when that happened or what the motivation was, but there are some clues: We do have a few other magic types that have different kinds depending on circumstances. `()` as a type can have both kind `*` and `Constraint`. The former is the obvious unit type, the latter is the empty constraint (). The type `(a,b)` can have kind constraint if both `a` and `b` have kind `Constraint`. Annoyingly you can't partially apply the `(,) :: Constraint -&gt; Constraint -&gt; Constraint` form. This is strikingly similar to the way `(-&gt;)` can't be partially applied to something of kind `#`. Something similar to how `(a,b)` was tricked out, was probably done to `(-&gt;)` causing the confusion you have above. It seems that `(-&gt;)` no longer uses the subkinding trickery, but rather when _fully applied_ can have the other kinds, possibly as part of how they resolved the above situation with `(?)` and `(??)` and how `(a,b)` works to enable you to share the tuple-like syntax both the left hand side and right hand side of (=&gt;) in something like `(Eq a, Ord b) =&gt; (a,b) -&gt; (a,b) -&gt; Bool`. This seems to have unified the behavior of how `(,)` and `(-&gt;)` both do overloading on their kinds, at the expense of not allowing you to partially apply `(-&gt;)` at a different kind.
The portability story is the same. It is worse for shell-conduit which encourages you to just use unix commands.
"cabal is not a package manager"
Thanks!
Is this really a problem in practice? I've very rarely run into this and then I just rename my local variable.
You rename your local variable to what?
The VS integration isn't something I've seen anyone else do.
I'm pretty ok with boring, yet well-structured &amp; battle-tested code. In particular, "Time and date: very good calendar; timezone; datetime handling" tends to be boring yet hard (practically impossible?) to get right.
The context is that I'm trying to convince Twitter to use Haskell instead of Python for large scripts. I'm working on an internal course to teach people how to do this. While writing the first draft of my course I actually tried to use Shelly but there were many issues that made this pedagogically hard. For example, all commands have to be run in the `Sh` monad, which is a problem because I wanted to teach `IO` first using real shell commands as examples. I didn't want to say "Here is how you use this use this cool shell DSL, and here we use `liftIO` but I haven't even taught you what `IO` is or how it differs from `Sh`." Another problem was all the commands using underscores for option names. If I introduce those it immediately raises several knotty questions like "Why doesn't Haskell have a good story for function options?" and it also looks a little ugly in my eyes. So I started creating my own wrapper around Shelly to fix all of these small issues and after a while it diverged enough from the design of Shelly that I decided to make a clean split. I judged that the benefit of having a cohesive library outweighed the benefits of code reuse and by that time there was very little code reuse already because I switched to the `Shell` type. So the most important difference is that `turtle` is optimizing for a smooth Haskell beginner on-boarding process, and most tradeoffs on the library reflect that design decision.
The reason I suggest lenses is to avoid the use of type classes to implicitly select the element to iterate over. With lenses you are explicit by default and you opt in to implicit element selection using the `each` lens.
I see. Thanks.
I've been wanting to give [semantic highlighting](https://medium.com/@evnbr/coding-in-color-3a6db2743a1e) a try, which colors variables instead of keywords. Might help for this.
Ooh yes, that's definitely interesting. Although I don't think code style decisions should be made assuming you have that feature.
Yeah, I think we are both agreement on that.
Don't think of this as competing with Shelly. Just think of it as a way to get more people using Haskell for shell scripting and they can upgrade to Shelly when they need those extra features.
One of the things that I want to do is to actually wrap `optparse-applicative` in a simpler interface, and that would include also making sure that it uses `Text` everywhere.
I think this is grasping at straws a little.
oops
Thanks . I am not saying I will just stick with just one. But right now I was thinking to be Haskeller first.And earn living out of it. Will that possible is a big question. If that happens adding another skill is for sure.
If you are directly career oriented, if achieving a decent earning in programming is your goal, then Haskell is not the most direct path. If you're aiming a little longer term and can suffer more slowness to begin, then Haskell becomes a brighter option. Most of us here would believe that your overall understanding of programming can be well shaped by learning Haskell—even (and perhaps especially) as a very first language. The slownesses you will suffer are twofold * There are fewer learning materials available for Haskell than other languages, especially for absolutely new programmers. This is not to say that there are zero, simply fewer. * There are ultimately fewer jobs for Haskell programmers today and the market is competitive. Today there's a growing market for "junior" programmers in Ruby or Javascript (and other places) who may have had less than one year programming experience. (Though, judgement as to whether those are "good" jobs is more complex and certainly situation dependent.) These opportunities will be harder to identify for Haskell-specific jobs. Now, both of these can be mitigated. There are great people and patient teachers in the Haskell community. I recommend at least listening to the #haskell channel on Freenode and perhaps taking a look at #haskell-beginners. There are also many ways to translate Haskell experience into other languages and rapidly widen your job search network. Mitigation of issues like this is a windier path than, say, doing CodeAcademy classes in Javascript. You should think carefully about your goals of becoming a programmer or even a Haskell programmer and try to make the best decision to achieve them. But all that aside, welcome to Haskell, welcome to /r/haskell, and welcome to programming at large. It's a life path where you can sometimes pull off doing fun things for money and you, especially if you walk the Haskell path, will likely never be bored again.
It wouldn't be haskell without the preamble. :) My idea would be to wrap `runhaskell` with a `runturtle` program which prepends the blabla.
Greater customization of the GHCi prompt is tracked in https://ghc.haskell.org/trac/ghc/ticket/5850
That's a good idea. I'll do that.
I always thought this was a weird constraint. What makes this easier to deal with than just writing `f` normally? It really sticks out amongst all of the crazy complicated things that GHC is capable of deciphering.
Honestly, I've faced plenty of times when longer variable names are useful. And, in those cases, I tend to see that Haskell authors use them. *By and far away* the most common place where this occurs is when the variables name an ST or IO reference which is getting periodically mutated. In this case, the "point" has a lot of persistence and meaning and a longer variable name can help keep track of that. But lacking mutability and gathering generality, variables tend to just be convenient syntax for wiring functions together. 75% of the little twiddly intermediate names are just noise with very local scope. In this case, short short names are actually a giant *benefit* to readability (in my opinion). Now, there are some conventions in short variable names which can be very helpful. These are essentially a form of super minimized Hungarian notation, and it's worth picking up on the convention. There's also a tendency to have the names of variables reflect their type so that you'll often see the following written map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f [] = [] map f (a:as) = f a : map f as instead of map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f [] = [] map f (x:xs) = f x : map f xs All of these techniques help the reader to understand the importance essence of a variable while forgetting all the rest. In generic code, importance essence is highly boiled down—we're far more interested in the composition of functions than the objects they're acting on.
As the author of Shelly, I approve and am flattered you started with Shelly as a base. Shelly was a fork of an existing code base and reflects that heritage plus my specific concerns and those of many contributors. Every change to Shelly is designed with user-friendliness in mind, but it has never been re-designed from the ground up for beginners. I will say though that I would never try to convince anyone to write shell scripts in Haskell if there weren't some kind of debugging story available. So the command tracing that Shelly has is very, very important if you want someone to switch from Python without being mad at you when they cannot figure out what their program did when it failed. On the other hand, if you are playing a long game you could help make sure some of the GHC improvements around stack traces, etc come to fruition.
GHC can't online recursive functions (when would it stop unfolding?) By making a non-recursive wrapper you give it something to inline.
It might be possible, but you would have your career options severely limited. And you would have to learn some Javascript and SQL, in addition to Haskell, most likely.
Where do you live / are you willing to relocate? That'll be a pretty big factor, Haskell jobs aren't as plentiful so there are fewer opportunities to work remotely. If you put 1000 hours into Haskell programming (preferrably on existing projects) over the next 12 months, and also spend 1000 hours interacting with the community over that same period. You'll be well positioned to take the next Haskell job that is offered, as long as it's not a team lead and you are willing to relocate. You'll almost certainly learn some C, Java, Python; if not before your first job, then during it. However, if you goal is primarily a programming career, you'll find more jobs available in the top languages listed on [TIOBE](http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html), [PYPL](http://pypl.github.io/PYPL.html), or [LangPop](http://lang-index.sourceforge.net/).
Tripped myself up for nearly on hour on a type error for using `id`. I shadowed the `Prelude` version with an `Integer`. :/
If Typeable is derived for all types automatically due to this issue, that would be great! 
Why did it take 2-3 days to compile your dependencies? That's at least a hundred times longer than I'd expect. Maybe we can help with that one way or another.
I see! You don't need to start from the beginning again to enable profiling. If you're using cabal sandboxes, you should be able to delete the sandbox (saving a backup copy perhaps) and re-build it with profiling enabled (in "cabal.config" in the project directory). And if you didn't use a sandbox, you can just start using one. You've mentioned GHCJS a few times. Do you mean GHC, or are you using the Javascript compiler? I don't know whether and how GHCJS handles profiling. Because GHCJS is very closely related to GHC, you can perhaps improve the performance of your code in normal GHC first and then switch back to GHCJS.
This looks really nice ! I'm having an issue though: doing main = sh $ do file &lt;- ls "/some/folder" liftIO $ stdout $ grep (has "hello") $ input file sends the script into 100% cpu in a folder with some files. The equivalent with bash for file in "/some/folder/*"; do grep hello $file; done runs instantaneously. I'm I doing something wrong ? Also, I can't seem to compile it through nix, the testsuit fails... http://lpaste.net/119654 It compiles fine from cabal.
Sorry, meant to tip Tekmo!
Most definitely no. If you want to work in Haskell you should probably know some C anyway as you'll often be working with C libraries, perhaps ones you crafted yourself. That said, trying it may be worth it, even if you can't go all the way to a career. You said you have the books. Also the specific application domain you are talking about, websites, actually does seem to be an area where Haskell can shine as your only tool. However you're going to be learning some HTML, CSS, and JavaScript, especially if you're planning to use Yesod.
At the risk of [kettle logic](http://en.wikipedia.org/wiki/Kettle_logic) here is a laundry list of issues: * Without the fundep determining 'a' from 'as' inference for the version of the code you wrote there will be terrible. e.g. foldMap (\x -&gt; [succ x]) "hello" has no reason to try to use `Char`. * A monomorphic-`Foldable` is useless as a superclass of `Traversable`, it has the wrong kind. * Dealing with polymorphic recursion in these sorts of monomorphized classes is a nightmare. * There is a role for a monomorphic `Foldable`, it just isn't a thing we can standardize on in the forseeable future, because we won't get MPTCs + fundeps or typefamilies. * It seems somewhat ironic to lurch from "hey it is hard to reason about `Traversable`" to "let's use `each`", which is a completely ad hoc mess that requires instance-by-instance reasoning for everything. Worse, we also know from using `Each` in `lens` that the set of fundeps we have for `Each` is actually insufficient. See `Control.Lens.Wrapped` for the full 2-class, 3-instance magic for how to ensure that the `s` and `t` parameters are 'related', it is quite a bit harder than it looks at first glance. I wrote those things, have dealt with 2 years worth of issues filed by folks, and I wouldn't wish them on `base`. * `Each` isn't a logical superclass of your `Foldable`, it is a logical analogue of Traversable, the relationship between the classes should run the other way around. `byElems` is basically monomorphized `traverse`. * Tons of code happens to use the Traversable instance for `f` without knowing `a`. I have tens of thousands of lines of this kind of code across various projects, compilers, etc. This class is usless for that kind of stuff. Consider cases like: http://hackage.haskell.org/package/bound-1.0.4/docs/src/Bound-Scope.html#traverseScope when converted to a class like this _tons_ of information leaks into the type signature and never leaves. There is a balancing act in making things more generic. When you can do so by working with parametricity you have to concern yourself with less rather than more. The abstraction works for you. When you work with something like `Each`, all the details matter all the time for every instance, and while you can write a line or two of code with it at a known instance, you can't use 'each' from lens generically in a useful way.
I understand now. Are these projects on GitHub somewhere? I’d like to take a look.
I actually didn't even intend there to be a way to abort iteration, but now that you mention it I suppose throwing an exception would work after all. It feels kind of dirty to do that, but :\
I'm going to be releasing a quick update either tonight or tomorrow to fix a few small issues, so I'll include a prominent note in the documentation that people should try `Shelly` if they want more advanced features like tracing.
Well, at least this is a pretty great bug!
We tried, but it's not always a win.
You might want to point out the possibility of ambiguous type errors even with type inference. I find they're frequently unavoidable when using OverloadedStrings, so there's a good chance someone in your audience will run into them now and then.
This is off topic, but I've seen `ExceptT` popping up recently and haven't figured out why one would want to use it over `EitherT`. Am I missing something profound here?
The main reason is to avoid fragmenting the community over error-handling idioms. Most people prefer `ExceptT` because it's in `transformers`, which is already in the Haskell Platform (and has fewer dependencies).
That's a good point. However, I couldn't think of a concise example illustrating the issue using the library.
Edward made short work of that hare-brained scheme, as might have been expected. I think I was achieving a certain state of mental clarity here, http://www.reddit.com/r/haskell/comments/2ttvsj/major_prelude_changes_proposed/co68768
looks interesting, maybe post in /r/commandline?
The jump from GHC 6 to 7 was mostly centered around the switching of the way the typechecker works entirely from an old backend language to System Fc. This coincided with the appearance of type families, data families, type equalities and all sorts of big changes in the nature of what can be expressed in the language at all. It was a massive internal change, more or less a complete rewrite of the entire typechecker and elaboration of the surface language into core, because core became quite different. For GHC 6 to 7 they rebuilt the boat. By comparison we're cleaning up the trim and putting on go-faster stripes. ;)
Two little fixits on that page: &gt; Excellent knowledge of in Scala, Java, or other modern systems languages (I had originally intended to just be all "you forgot Haskell" but then I noticed the "of in," so let's pretend I'm just being helpful and not sarcastic.) and at the very bottom: &gt; &lt;span
Actually, I think it's the `Monoid` instance that's the problematic one. I really think it should be: instance Monoid a =&gt; Monoid [a] where However, `(++)` is definitely useless and should always be a synonym for `(&lt;|&gt;)` in my opinion.
Oh, I didn't write that page and I don't know who did. However, I can try to find out so they fix it.
I just mentioned GHCJS because if I reinstall anything, it will probably break (I guess) so there goes another day or 2 trying to fix it :( The problem with sandboxes is that while they don't break everything else, they have the problem that anytime I create a new project I have to rebuild everything oncee again.
Do you know if anything is still going on with [Lisk](https://github.com/haskell-lisp/lisk)? The new repo just has a readme and a todo which are over a year old.
The pain of rebuilding packages over and over again for new sandboxes is known and unfortunate. I believe a few people are working on systems that would allow reusing sandboxes. It wouldn't help your problem of having to rebuild with profiling on. With my couple-year-old laptop, I figure on it taking five to fifteen minutes to `cabal install --dependencies-only` a new sandbox with `lens` and the other usual suspects. A huge portion of this time is spent generating documentation with haddock, which I tolerate because I'm occasionally without Internet. You may be familiar with the *global* and *user* package databases. If you installed the Platform, a variety of packages are installed globally; installing packages not inside a sandbox will install them in the user database. If you use a sandbox, it will *ignore* the user database. GHCJS, I believe, has a separate set of global, user, and sandbox packages. Clearing your ~/.cabal and ~/.ghc directories may or may not break GHCJS. You can always rename those folders and see what happens; rename them back if it doesn't work. Sandboxes can be shared among projects by using the `--sandbox=DIR` option to cabal. When I'm puttering around on a given topic, I'll make a folder containing a few projects (in their own folders) with a shared sandbox. If you're building with documentation on, you can turn that off to speed up building significantly.
Agree. I have an edge of Javascript, Web Technologies as well as DB. 
True.
&gt; I really think it should be: `instance Monoid a =&gt; Monoid [a] where` No, it really shouldn't be. The list is the free monoid. 
I often feel relocation won't be an issue if the skill level matches the requirement. And probably work remotely is mostly getting hold of market due to various other factors too. I have the crazy feeling when I use Haskell so working on it as a Haskeller would be fun indeed. I have basic knowledge of C, Python, Ruby and web technologies as well. Hence I understand the need of it for few companies for things which they already have built with it. I wish to see industries and academics use Haskell as the core language and using that let the programming concepts be derived, so that from the academic front their isn't any block for interested candidates to grab hold of the language. It's as functional programming is said to be the right way to learn programming and the way human interact and making it basic for initial start would be best suited for future of Haskell. Thanks for share the stats. It's awesome.
The message mentions that they're discussing some features to ease the BBP transition; anyone know what he's referring to here?
What kind of work do you do at Twitter?
So sad but so true. I wish we had more opportunites in CodeAcademy and various others resources to learn Haskell free and quick easier way to build our own products so that more opportunities build from it.
I am trying to do just that. /u/tel mentions that there are fewer learning materials for Haskell than other languages. This is something I struggled with at the beginning -- I switched to How to Design Programs for a bit to learn Racket before I came back. The other problem is that other than Programming in Haskell, there is no other Haskell as a first language text available. In my opinion, Programming in Haskell isn't a great first programming text either. There are also not that many junior roles in Haskell either. You probably should expect that you might have to move to get a job. People are talking about relocation, but the bigger problem with not having a piece of paper that says computer science etc. is that you will need to do more networking to get a job. It's easier to do that in places where there is an existing Haskell / functional programming community where you can attend meet-ups etc. My partner is an experienced Haskeller, though it is not currently his primary working language. I probably have more guidance than the average career-changer in moving into the programming job market and in just day-to-day learning. It has still taken us over a year and a half to get to a point where I'm sort of ready to start interviewing. 
I am on Twitter's Processing Tools team that builds and maintain internal analytics tools. Some of our tools are open source (see, for example, [Scalding](https://github.com/twitter/scalding), [Algebird](https://github.com/twitter/algebird), and [Summingbird](https://github.com/twitter/summingbird)). I personally work on an internal tool called Tsar which is basically a time series analytics framework, and the code is half Scala and half Python. Working on the Scala half is tolerable; working on the Python half is frustrating. You can always tell which half I'm working on by whether or not I'm [tweeting about how awful dynamic languages are](https://twitter.com/GabrielG439/status/534887256918478849). My personal mission is to get Haskell into the company and I have lots of side projects related to this. I'm close to open sourcing one internal tool I've built using Haskell. I've set up a some internal Haskell infrastructure, too, like an internal Hackage server and relocatable `ghc` build for running scripts on Mesos, and my next goal is building and deploying Haskell binaries from our CI. My general experience within the company is that the Scala programmers love Haskell (they basically view Haskell as a better Scala). However, the problem is that the marginal benefit of switching from Scala to Haskell is so low that it doesn't justify the switching costs. On the other hand, the marginal benefit of switching from Bash or Python to Haskell is high. However, the people who are writing these Bash/Python scripts have typically never programmed in a statically typed language and they assume statically typed = heavyweight (because they equate it with Java/Scala). That creates a curious situation where the people who would most benefit from using Haskell are the ones who appreciate it the least. That's the reason I'm working on `turtle`, to convince these people that statically typed scripting can be light-weight and make a case that Haskell should be the language of choice for larger scripts.
[**@GabrielG439**](https://twitter.com/GabrielG439/) &gt; [2014-11-19 01:53:54 UTC](https://twitter.com/GabrielG439/status/534887256918478849) &gt; "I regretted doing this in a statically typed language", said nobody ever ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
Good to know. I understand how it feels when you know the language and there are less flexibility letting it be used for better purpose. I was reading one of the books which said "it gets more and more important to describe a calculation in a language closer to the ‘human world’ than to the world of a computer". Irony is that we have this world of computers around already built and to suit that whole Functional programming community is aligning with it's rhythm to a level such that you need to know the world of computers to tell that it's human world. Existence of Functional programming happens when reach of it and easy way to get hold of knowledge be made feasible. Whether it takes hard work or smart work let the one who works on it decides. I wonder when we can see Systems, OS built using Haskell, Websites, ERP's, Mobility industry using Haskell. It need heavy usage of Haskell, which only happens when it is let go from boards to reach young minds fast and know how to make computers talk.
Ouch... not sure what happened here: view $ inshell "cat " (input (fromText "/home/cody/test.txt")) "this" "is" "a" "test" (0.09 secs, 1384184 bytes) λ&gt; -- this actually took about 7 seconds to show up... (0.00 secs, 0 bytes) λ&gt; readFile "/home/cody/test.txt" &gt;&gt;= print "this\nis\na\ntest\n" (0.00 secs, 0 bytes)
Thanks for the feedback. I can't promise that we'll fix these issues in a timely manner, but the more people who speak up the easier it is for me to justify making the changes :)
[Hell](https://github.com/chrisdone/hell) acts a bit like that.
PolyKinds may help. Then your first index would have kind (Nat, Nat). You could also hide your desired constraints in your constructors and recover them by pattern matching on a GADT-typed value that discloses which constraint that is. Anyway, an example would give us some guidance to see what you are aiming at.
&gt; I understand how it feels when you know the language and there are less flexibility letting it be used for better purpose. What do you mean by that? A lot of the better features of Haskell are already trickling down to other languages, e.g. type-classes and garbage collection. So you don't need everybody to learn Haskell. 
You're welcome, although it doesn't seem like my advice was terribly helpful. ;) One other thing you could have a look at if you haven't already is [Nix](http://nixos.org/nix/), a package manager that simplifies Haskell development greatly. It also, crucially for your use case, has binary packages which reduce install times drastically. However, be warned that Nix is somewhat complex and underdocumented and will perhaps require a decent time commitment in and of itself; plus there is currently a major restructuring of the Haskell infrastructure going on which it is even harder to find solid information about. I finally got it working with profiling yesterday, so I may be able to assist, but I'm certainly no Nix expert.
It may also be related to the `async` library. I also get difference delays depending on the `ghc` version so something odd is going on.
Yeah, we figured out the issue with the one test failure: https://github.com/Gabriel439/Haskell-Turtle-Library/issues/1 It turns out it is due to an ambiguous instance error that occurs on `ghc-7.8` My plan is to use a different type for matching text using `grep` or `find` in order to do matching in linear time. The API should be the same, though
&gt; There are plenty of Functors that can't be made Traversable, too. Are functions useless too, because i can't necessarily put the results in order? Or should we restrict ourselves to first order functions with enumerable arguments to restore this invariant? I have no idea what this is about, the point of the preceding remark is not that we shouldn't use things that can't be put in order, but on the contrary, that we don't yet seem to have any idea what it would mean for something to *be* 'in order'. If anything the inference should have been that I oppose everything that is order. Since `Foldable` defines `foldr` or equivalently `foldMap`, which in the nature of the case take things in a determinate succession, one after another, it is clear that a law that somehow expresses this 'successiveness' needs to be defined. But none has. That `Foldable` boils down to `toList` is really very close to the essence of the matter. There is nothing for example that would stop me adding `take 100` to my preliminary implementation of `toList` and defining `foldMap f` as `foldMap f . toList`. Or maybe that could make sense. Of course it wont agree with the natural `toListDefault` if I go on `Traversable`, which has laws.
Here is type inference: http://deconfigured.com/blog/atlc3 I haven't bound these with prescedence yet, but it should happen very soon!
You could try making some of your datatypes strict and unboxing them where possible. If your using a lot of space the GC could be slowing you down. Of course it would be better if you profile memory usage first.
I was thinking more about this: $ man 1p mkdir MKDIR(1P) POSIX Programmer's Manual MKDIR(1P) PROLOG This manual page is part of the POSIX Programmer's Manual. The Linux implementation of this interface may differ (consult the corresponding Linux manual page for details of Linux behavior), or the interface may not be implemented on Linux. 
I wrote [Halcyon](https://halcyon.sh/) in order to address these issues. Here’s my completely biased suggestion: In the immortal words of [Albert Lai](http://www.vex.net/~trebla/haskell/cabal-cabal.xhtml), scorched earth policy. Wipe all traces of Haskell from your system, including `~/.cabal` and `~/.ghc`, and all Haskell packages provided by your package manager. Forget about the Haskell Platform. Let Halcyon manage your Haskell installation from start to finish. Halcyon can install GHC and _cabal-install_ for you. With Halcyon, you’ll always keep your global and user GHC package DBs pristine, and only ever install packages into Cabal sandboxes. Halcyon can save time when building sandboxes by using a previously-built sandbox as a base. And, by using private storage, you can share build products between machines. With these assumptions in mind: 1. While Stackage LTS may have its purpose, it seems to me all that’s needed to avoid Cabal hell is a little Cabal discipline. If you need an app provided as a Cabal package, you can `cabal build` it hygienically in a new Cabal sandbox, and `cabal copy` its binaries and run-time support files into an arbitrary prefix. Since your global GHC package DB is pristine, and your `~/.ghc` is empty, this shouldn’t lead to any Cabal hell. Halcyon makes this simple and fast, by automatically caching build products every step of the way — just [`halcyon install`](https://halcyon.sh/reference/#halcyon-install) your app. 2. Currently, Halcyon defaults to _cabal-install_ 1.20.0.3, and you can change that with the [`HALCYON_CABAL_VERSION`](https://halcyon.sh/reference/#halcyon_cabal_version) option. I’m still discovering new issues with 1.22.0.0, such as [haskell/cabal#2320](https://github.com/haskell/cabal/issues/2320) and [haskell/cabal#2342](https://github.com/haskell/cabal/issues/2342). 3. Cabal sandboxes offer the very useful ability to add local directories as package sources, using the `add-source` sub-command. Halcyon builds on this ability, allowing you to add _git_ source repositories with the [`HALCYON_SANDBOX_SOURCES`](https://halcyon.sh/reference/#halcyon_sandbox_sources) option. 4. Halcyon also lets you declare additional apps to be installed into the sandbox directory, for use at build-time, by using the [`HALCYON_SANDBOX_EXTRA_APPS`](https://halcyon.sh/reference/#halcyon_sandbox_extra_apps) option. This can help you avoid the need to install apps globally, but if necessary, Halcyon can also do that — again, just [`halcyon install`](https://halcyon.sh/reference/#halcyon-install) your app. By default, apps are installed hygienically into [`HALCYON_BASE`](https://halcyon.sh/reference/#halcyon_base), but you can change that with the [`HALCYON_PREFIX`](https://halcyon.sh/reference/#halcyon_prefix) option. 5. While I haven’t announced OS X support yet, I use OS X to develop Halcyon. Currently, automatically installing declared native OS packages is not supported on OS X ([mietek/halcyon#16](https://github.com/mietek/halcyon/issues/16)); everything else is expected to work. Uninstalling Halcyon is simply a matter of wiping [`HALCYON_BASE`](https://halcyon.sh/reference/#halcyon_base) and [`HALCYON_CACHE`](https://halcyon.sh/reference/#halcyon_cache). 6. With Halcyon, you can switch between GHC versions in seconds, by using the [`HALCYON_GHC_VERSION`](https://halcyon.sh/reference/#halcyon_ghc_version) option. Currently, GHC directory archives for all supported GHC versions are available in [public storage](https://halcyon.sh/tutorial/#install-ghc-and-cabal), for all supported Linux distros — CentOS 6 and 7, Debian 6 and 7, Fedora 19, 20, and 21, and Ubuntu 10, 12, and 14. For OS X, you’ll need to set up [private storage](https://halcyon.sh/tutorial/#set-up-private-storage) first. Halcyon will automatically build and archive the GHC directories you need, based on original GHC bindists, then upload them to your private storage. This way, you’ll save disk space by keeping just one GHC version installed, and you’ll be able to restore any archived version when needed. Follow the [Halcyon tutorial](https://halcyon.sh/tutorial/) to get started. Feel free to join us in `#haskell-deployment` on the [freenode](https://freenode.net/) IRC network, and ask plenty of questions. [Previously…](http://www.reddit.com/r/haskell/comments/2toc3j/ann_halcyon_and_haskell_on_heroku/) [Pre-previously…](http://www.reddit.com/r/haskell/comments/2ogoot/deploy_any_haskell_application_instantly/)
And for some Haskell code, take a look at [Jon Sterling's itt-bidirectional](https://github.com/jonsterling/itt-bidirectional).
If you’re installing Haskell libraries, not apps, you’ll also need `cabal register`.
I use Nix package manager exclusively, now, for all my projects. I switched over to it personally last February, and professionally (at skedge.me) in September. So far, I've been very pleased with it. One thing to note is that, although Nix works on OS X, it is much more stable on Linux.
Thanks. I'm mainly interested in applications such as`hlint`, but good to know the full story.
I am not sure that there is an accepted best practice. It does seem like a lot of stuff is going toward using stackage. Personally, I use docker for everything. Docker for development, and docker for deployment. docker docker docker. This is great because I can share the dev environment with others in a very simple way.. and since you can get dockerhub to do all the building of the machine for you.. you can go from a fresh machine to a full haskell stack with all prereqs compiled in seconds. Edit: to be clear: I build docker images with my prerequisites installed and mount the code itself during run time. I have shell scripts wrapping everything, but typically my builds look something like `cabal_build.sh` docker pull gdoteof/mycontainer docker run -v `pwd`:/code --rm gdoteof/mycontainer /bin/bash -c "cd /code &amp;&amp; cabal clean &amp;&amp; cabal configure &amp;&amp; cabal build" This basically says "mount the code in the current working directory on the `host` to the `/code` folder on the `guest`, then `cd` into the `/code` directory and do all the cabal stuff.
Nope. Nice troll tho.
Ah, thank you very much. Should have guessed I wasn't the first asking that question. ;)
Is all of hackage available through nix?
No, but it's relatively easy to add a package locally. I've heard it's easy to get these uploaded to global nixpkgs, but I haven't tried yet since I'm on a side branch as OS X gets fixed.
Lot's of math, no code. What am I supposed to make of this? 
If you cabal get the source there is one helpful example. A fully-worked example is Idris IIRC. If you're looking for the clang-like error reporting, I think it's a rumor. I recommend sticking with parsec. Edit: I did not recall correctly. It's [ermine](https://github.com/ermine-language/ermine/tree/master/src/Ermine/Parser) Edit 2: I was [right the first time](https://github.com/idris-lang/Idris-dev/blob/master/src/Idris/ParseExpr.hs)
The usage seems to be pretty similar to parsec &amp; attoparsec. What I would like to see is a blog post on the internals. The parser type looks very confusing to me.
[Vagrant](https://www.vagrantup.com/), with [Puppet](http://puppetlabs.com/) configuration files for the particular Haskell GHC / Platform versions in question.
Indeed, GHC 7.10 requires _cabal-install_ 1.22.0.0. All other GHC 7.* versions are happy with 1.20.0.*.
[Morte](https://github.com/Gabriel439/Haskell-Morte-Library/blob/master/src/Morte/Core.hs) is another example of a simple dependently typed language. No universe-polymorphism, though.
Interesting, how is the build farm handling that? I imagine there is a lot of Hackage that can't build.
It does, but not all of it works. However, the nixpkgs folks are pretty quick with pull requests, so it's not too tough to tweak things to make them work - and it's easy to use your own fork in the mean time. (Nix is by far the best workflow I've found for working with temporary forks.)
Not through `read`. What you're describing sounds a lot like Python's `eval`function (see [this SO post](https://stackoverflow.com/questions/2469139/equivalent-of-python-eval-in-haskell), which suggests that the right way to do this in Haskell is to get the parser and build your own DSL). Using an `eval`-like function is usually a bad idea because 1) In a compiled language like Haskell, you'd need to make an external call to the compiler whenever you call this function, and 2) It often introduces a security hole in your code. The code you're putting into `eval` is clearly not known at compile time (otherwise you'd have already compiled it and you wouldn't need `eval`), and a malicious user might pass in a string containing code that does all sorts of nasty things (examples: install a virus on your computer, delete your files, etc.). Creating a DSL has neither of the above drawbacks, which is why Haskell is perceived not to need the equivalent of Python's `eval`.
Your implementation of `readsPrec`only works for strings starting with "f ", and assumes that "f" is the squaring function. I think OP was hoping for something that would take a string containing the name of any defined function and perform that function. edit: perhaps I misunderstood how you planned to generalize. It's on the right track if you put every function that you cared about into that implementation of Read, at which point `Square` would perhaps be better named `AllMyFunctions` and might be a very complicated ADT.
I am in favor of the delay as well as determining what the BBP fully entails.
Example project linked here http://www.reddit.com/r/haskell/comments/2uc6kp/are_there_any_tutorials_or_introductions_to_the/co7h7fe
I was just experimenting and and playing around, and (as, I said, I'm not at all advanced at this language) I wanted to create a set of functions to extract the element at a certain index at an arbitrarily-sized tuple. I wanted to use the naming scheme of "tmtn" where *m* is the size of the tuple and *n* is the index. So I would define *let t5t3 (_,_,x,_,_) = x*. But if I wanted to define a bunch of these functions at a time, I would set up a list comprehension with *[read ("let t"++x++"t"++y++" = --code")]*. There's probably a better way of doing this, but I thought I'd have fun and try to make my own, which let me to find out that *read* doesn't do what I thought it did, and then I wound up here. I thought of using a single function of *t m n*, but I can't do that since each size of tuple is its own class. PS: I don't know how to do the type of text where it looks like actual code, so all I can do is italicize. Also, this idea of mine may sound dumb, but again, I was just playing around and seeing what I could do or not do. EDIT: I guess Reddit doesn't like underscores in my t5t3 function.
Try using [record syntax](http://learnyouahaskell.com/making-our-own-types-and-typeclasses#record-syntax) instead. To make text code-y, surround it with backticks (the key is to the left of the 1; it's like a lower case tilde (~)).
I'd recommend XMonad for overall system design and good separation of pure and impure code. There are a couple of Youtube videos of XMonad design talks. PS: Mods, if I'm still shadowbanned, please let me know. Same username in #haskell on Freenode. Edit : Have confirmed that I am no longer shadowbanned.
I see. So if I want turtle features that are missing in shelly, like constant-space streaming and patterns, I should port them to shelly. I guess that's fair enough. 
Can't seem to find xmonad part 1: https://www.youtube.com/playlist?list=PLxj9UAX4Em-IBXkvcC3MycLlcxyoi7v8B
ditto for xmonad. I have also used lambdabot to learn heaps
Streaming actually is a piece of cake. It's just a single function.
Thanks, I'll take a look!
It is more that `parsers` was made after the fact to factor out the commonly reusable bits of `trifecta`, than the description is deliberately misleading. ;)
You get a caret by default in the diagnostics, but you can actually craft a diagnostic that has as many carets and fixits and spans in it as you want.
My favorite 'pretty' Haskell is this raytracer: http://nobugs.org/developer/htrace/htrace.hs 
Nice to see that there is now a wiki entry and that they have changed the name of the proposal :-)
&gt; Now, there are some conventions in short variable names which can be very helpful. These are essentially a form of super minimized Hungarian notation, and it's worth picking up on the convention. I would be interested in knowing about such convention. For instance when to use `k` (for CPS only ?) Hungarian notation really ? I have been seeing it quite sparsely in Haskell code most of the time for maybe values.
Eclipse integration is more common, I guess?
Neil wrote the GUI library, didn't he?
Fwiw, the debated subset of the BBP was already called **FTP** shortly after work started on [#9586](https://ghc.haskell.org/trac/ghc/ticket/9586). But for some reason, the term "burning bridges" is more catchy (and allows for more wordplay) than referring to it by its proper and unemotional FTP designation... ;-)
Why were you shadow banned?
don't act innocent, /u/nsa_shill
Stringly-typed variable binding. \**shudders*\* I'm too stupid to trust myself not to fuck it up.
It is too easy to inline instead of properly abstracting. Probably CPP extension was wrong design decision, it prevented language from evolving toward native tools for conditional compilation.
Yep Lists are supported only because they have the correct interface. Vectors are unsupported because there is no dependency on the `vector` package (as opposed to `containers`) and I figured it's not hard to implement the `DataSet` instance on vectors. But I might change my opinion on that. [edit: german autocorrect on english text :) ]
HaRe would struggle with the fsnotify case too, as it can only refactor against what is currently configured. So any change that should propagate to the other os variants won't. This is the general case for #if / #else / #endif stuff. HaRe treats the inactive variant as a comment, and cannot process what is inside.
[Kind of!](https://www.reddit.com/r/haskell/comments/2ucazk/what_ever_happened_to_layers_package/) Well, I did finish that web design work in the end, in September I think (but I took a break from that too.) I've been doing a lot of sysadmin type work since then, which I'm close to finishing now too. I think I could have another look into finishing layers in mid-Febrary maybe? Thanks for the interest even after all this time!
`Foldable` says nothing about how the type is constructed. This is the very point of why this class is weaker than `Traversable`. Applicative doesn't state how the Applicative class relates to Monad: Monad states how if you are a Monad, the Applicative should be compatible with it. Applicative doesn't state the Alternative laws: Alternative states how if you want a type to be Alternative, you pick up extra constraints on the operations. Foldable doesn't state the laws it has about foldMap, because it turns out none are needed any definition of foldMap is sound in the absence of other constraints, `foldMap _ = mempty` would be perfectly legitimate: Traversable puts constraints on what Foldable instances are compatible. This is the same sort of relationship. `Foldable` cannot contain tools like `nil` and `cons` for some ad hoc construction mechanism. Nothing in the Foldable/Functor/Traversable family provide any tools for changing the "shape" of your data type, deliberately. This is what enables these constructions to be as universal as they are. I have entire packages that rely on these properties, there have been a half dozen papers published on the notion of traversals and why the laws for Traversable are what they are. That given a Traversable `t` I can't change the shape enables me to write wide-ranging abstractions such as: http://hackage.haskell.org/package/ad-4.2.1.1/docs/Numeric-AD-Mode-Reverse.html `Foldable` captures the things we need to say to enable us to generalize half of the things that you could do with `Traversable` which don't need the ability to put stuff back into the shape you consume. There is roughly a 50/50 split in terms of combinators. Adding constraints to the class takes it outside of what is a logical superclass of Traversable and actively penalizes dozens of combinators in pursuit of a law that has no fundamental reason to exist. Knowing you can `traverse` should be enough to know you can `traverse_`. Adding construction to `Foldable` adds nonsensical construction operations transitively to `Traversable`, which are not motivated by the underlying theoretical principles that guide `Traversable` as being a fundamental abstraction. Ultimately, you're effectively arguing that literally tens of thousands of lines of code that I have should not be allowed to exist because you can't mangle Foldable (and by extension necessarily Traversable) into having unnecessary construction laws, that aren't motivated by any of the underlying theory, and those lines of code are some of the most effective bits of Haskell I have ever written. =P
Sure. There are also Foldable laws, pertaining to how operations within Foldable relate.
Not very sophisticated, but for avoiding cabal hell I've got a sandbox in my `~` directory, which can only pull from stackage, and then separate sandboxes for other projects which can pull from hackage. Then I've got a script which recursively traverses from the current directory to the root and sets the `CABAL_SANDBOX_CONFIG` variable to the first it finds (and displays the folder name in the prompt) so I always know which is active. Only issues I've had so far is that `cabal sandbox init` will overwrite the current sandbox if one is active, so I had to hack something together to prevent me from doing that. Also, I'm still looking for a way to make a more local project have access to the global sandboxes packages when possible. `cabal sandbox add-source` doesn't seem to work for me. At some point I'm going to try docker, which I suspect will work better.
Very helpful. Thanks ! 
why are they upside down 
That law is a free theorem. You can't violate it. ;) That said, we _can_ write the law down. To express things in the presence of the monoid constraint you need to talk about monoid homomorphisms. A monoid homomorphism `g` satisfies g mempty = mempty g (mappend a b) = mappend (g a) (g b) Then we can state that foldMap (g . f) = g . foldMap f holds for a monoid homomorphism g.
Eg the free theorem for (T0 -&gt; a -&gt; a) -&gt; a -&gt; T1 -&gt; a is forall x y: g (p x y) = q x (g y) ==&gt; forall a as: g (foldr p a as) = foldr q (g a) as which is why I brought up the question of free theorems to begin with.
They all sort of gravitate around that shape, whichever variants I try. So that the rule about monoid homorphisms, looks like it might be just a) the free theorem of the type + b) the fact that g is a monoid homomorphism. In that case, though, it doesn't constrain the type and doesn't make for too sensible a class. But I don't know.
I like `attoparsec`. I'd say that it is an idiomatic example of *high performance* Haskell code. And I don't know better way to learn CPS parsers then reading `attoparsec` sources.
No problem!
&gt;OTOH English, please?
on the other hand
What is the emacs theme you are using? I like it a lot.
Are pandoc and git-annex examples of really good Haskell code? I know they are pretty large Haskell projects.
Using a c preprocessor on top of haskell seems so utterly inelegant and perpendicular to the vision of a haskell.
This is cool. I'm a total noob and I tried doing this today to figure out what types a Pipes Pipe takes (the monad threw me off). I understand it's complicated to implement but intuitively it makes sense to have the feature IMO.
This is from the land of super-hypothetical / just-woken-up-and-my-coffee-isnt-cool-enough-to-drink ideas. What would happen if we had something like CompileTime a as a `Monad` that got special treatment a la `IO`? I think you might need something like class CompileVar a with the GHC providing instances for `String`, `Int`, etc... and stopping users from adding new instances (or perhaps just a closed type family to do the same), but I'm really not sure about that. The intent would be to let the compiler know that we absolutely need that variable to be inlined / trigger some inlining and constant folding within the CompileTime monad. You'd probably also need compile :: CompileTime a -&gt; a (possibly with a name that didn't make a thousand package maintainers cry out in horror) to indicate that you want to stop inlining things and start making use of the value at compile time. I don't know if this would be feasible at all / if it's already been explored (and possibly discarded) / if their are landmines all over the place that I should have noticed but didn't. In any case I'd be interested to see what the smallest piece was that could provide all / most of what CPP delivers from within GHC.
You're not actually running anything. Those are just strings. Do you want to use something like `System.Process` to execute the shell command? https://hackage.haskell.org/package/process-1.2.0.0/docs/System-Process.html
Why not make a preprocessor for Haskell?
"exe=`setxkbmap -query | grep -c il`" is a string, not a command that will be run. Comparing it with "1" is always false because they're different strings. By contrast, it seems you want to query the actual keyboard layout. There are several libraries for shell scripting in Haskell you could use, but there's likely a better way to do what you want to do in Haskell. Could you post a larger snippet or explain your end-goal a little better? 
Here is a visual aid http://www.deconfigured.com/blog/atlc4#substitution-model
Sounds vaguely somewhat similar to [this](https://github.com/RobinKrom/BtcExchanges), which is Pipes-based. Noone does push-based pipes. I know it feels right to say the Producer should drive, but there's little difference in practice with letting Consumers kick things off, and the pipes eco-system is all pull-based. 
&gt; I'm not at all advanced at this language [...] I wanted to create a set of functions to extract the element at a certain index at an arbitrarily-sized tuple. You probably shouldn't do that now, then. Generating multiple functions of distinct types is advanced Haskell, no matter how you do it. However, if you do what to dive in, template haskell would be how you might generate a family of functions like that. (There's probably also a method that uses closed type families, and a method using HList, maybe even one without any template haskell. But, that's for later.)
XMonad provides function called 'spawn', which executes shell commands. Right now you are just comparing string literals.
Thinking of all the times cabal broke my installation, I would not blindly agree it is great code. It might not have been it's fault, though. 
More like tons of packages not building at all because of new line related macro problems, then making some sort of wrapper around clang, or trying to install GCC from home brew. But then things aren't compatible with the platform anymore because of different compiler versions and so on. 
&gt; conduit and pipes are the only two libs to bother looking at. ...why isn't e.g. [io-streams](http://hackage.haskell.org/package/io-streams) worth looking at? conduit/pipes use an elegant high-level approach, but I've found that `io-streams` works surprisingly well even if it uses a far simpler down-to-earth streaming abstraction....
In my case the messages to decode were state dependent. I wrote a *really simple* operational-based DSL on top of `network-simple` to write the kind of servers I needed, [here](http://www.banquise.net/bin/SerializedCommunication.hs). It gives you an instance of `MonadState` for whatever you want, and let you run STM and IO actions.
I've always been fond of [this paper](http://www.andres-loeh.de/LambdaPi/) implementing a dependently typed lambda calculus. This blog post predates it, but I haven't really dug into it, so I don't know exactly how the two relate.
If someone has lots of time to spare [watching Stephanie Weirich lectures at OPLSS'14]( https://www.cs.uoregon.edu/research/summerschool/summer14/curriculum.html) about designing and implementing [Pi-Forall](https://github.com/sweirich/pi-forall) might be good idea too!
&gt; Whilst the types line up and playing with the various classes in ghci seems to work, there is no comprehensive test-suite as yet to verify that it is indeed sound. Once we get [an equational reasoning assistant](http://chrisdone.com/posts/haskell-wishlist#an-equational-reasoning-assistant) I hope announcements like this will start to include "proofs". :-)
Isn't the AMP deliberately structured so that import Control.Applicative will work across versions? I'd rather have an unnecessary import than CPP.
What do you mean by "better arrows"? 
That sounds really cool, I like the idea. But I should note that I'm using `golang` since `r58` and I remember only one change that broke my code -- when they introduced `error` type. But that was before `1.0`.
What about `cpphs`? AFAIK it doesn't mess Haskell code, but other problems are still relevant. The is nothing wrong with CPP (it is not "buggy"), but people abuse is too often. Template Haskell is some kind of preprocessor, but it too general purposed. The post is about abusing preprocessor, and most arguments applies to any general purpose preprocessor like cpp, m4, seq, etc.
I have no idea whether your approach will work or not. But you seem to get the idea I tried to express. Thank you!
Right, `hs-source-dirs` approach may lead to code duplication. We need native support for conditional compilation. (I didn't play with backpack enough to say anything about it, but it probably will be useful)
I use CPP for three reasons: one is to import different modules depending on the platform. I could get around that with some build system special casing... but I don't see how that would be much better than CPP. It would mean some logic would be "hidden" in the build system rather than localized to the module, and editor stuff that wants to go from module name to filename and back would stop working. And it seems to have the same downside as CPP, in that you don't find out you broke the module until you try a compile on that platform. The second reason is to export more from a module when compiling the tests. I don't see a way around that one. Wanting to sidestep visibility rules for tests is so common I'm surprised there aren't languages that specifically support that. Maybe rust does? The third reason is for FFI. I might be able to improve the first case by compiling all modules, but not linking the ones that are on the wrong platform, but the inability to test them all together seems intrinsic to its nature. The second case I guess would need a language extension, or a preprocessor specifically for export lists. The third case could be fixed with something smarter than hsc2hs, that turns C headers into haskell declarations.
I like the article generally, and like the solution you advocate, I just wouldn't apply it in the particular simple case of AMP.
It's a C lib. Why in /r/haskell ? 
I think you're missing what the other comments are getting at. The issue is this part of your code: "exe=`setxkbmap -query | grep -c il`" == "1" That's just a string comparison; that `-query` command is never executed. Since those two strings look nothing alike, this always evaluates to False, and the `if` always takes the second branch. (It's equivalent to `if False then ... else ...`.)
"PFQ/lang is inspired to Haskell", apparently.
It's a linux kernel module. User space libraries are available in C, C++11 and Haskell...
[pipes](https://github.com/Gabriel439/Haskell-Pipes-Library) is well-written, well-documented, and a great combination of high-level abstractions and low-level performance. 
What you are trying to do is to use relations (like the SQL kind). Graphs won't work too well for that. Perhaps an in-memory relational database (these seems there are such things on hackage) or maybe a custom-implemented kind of these would work. 
Ah, thanks! I only just noticed that the (dead) link to the paper goes to your space on the UU server...
Yes, I'm a fan of zippers. There's a [great paper](http://michaeldadams.org/papers/scrap_your_zippers/) on generic zippers that work with heterogenous types, but I'm afraid the paper went way over my head. That paper is on my list of things to work up to being able to grok. Studying Haskell has dramatically increased the size of that list, which is... pretty awesome, actually.
(The parent post currently has a negative score.) Downvoters: We have an unusually diverse population in /r/haskell. Asking all programmers in the world to read and write technical English accurately is one thing; asking them to keep abreast of jargon-y colloquial acronyms that are often impossible to infer from context is another.
Using pure instead of mutable structures like this has the added advantage of making it trivial to add the ability to undo, checkpoint, replay, and/or explore multiple possibilities in a game. 
I don't know about git-annex but last I checked pandoc is pretty clean. It's also very good for beginners because it doesn't use a lot of the fancy extensions.
&gt; there is currently a major restructuring of the Haskell infrastructure going on which it is even harder to find solid information about. Prompted by another /r/haskell post along these lines, I recently spun up a NixOS vm. I switched to the unstable channel, which supposedly includes haskell-ng, but the haskell-ng versions of the packages aren't available through `nix-env`, although they're visible with `nix-repl '&lt;nixpkgs&gt;'`. Since it sounds like you were working with this recently, do you have some idea how I can make sense of that inconsistency?
Great! Sorry for misreading. Like other languages, you need to hand off that string to a function that will run it in a new shell. Unlike other languages, Haskell tracks IO in the types, so it will look a little different than usual; something like: lang = do count &lt;- someShellExecFunction "my command" if count == "1" then return "first option" else return "second option" But exactly how that slots into the rest of your config depends on how XMonad has set things up. The folks at /r/xmonad should be able to help you more than I can, and you may find the `System.Process` module helpful. (Of course, you could always just flatten this into a one-liner, a la `$(setxkbmap -query | grep -q us) &amp;&amp; echo "first" || echo "second"`, and save the extra shell instance. As you prefer!)
And for posterity: I forgot to include `Just` before each case result, but I'd rather just remove the Maybe return type entirely, because the `Nothing` case handles every remaining possible pairing of strings.
I recall the "FP with structured graphs" paper from ICFP 2012[1]. Is it possible to build conventional graphs &amp; graph algorithms using this approach? I had a look but I'm not sure if it's appropriate to represent non-cyclic graphs, because what do you do -- forbid the Mu constructor and render the whole idea useless? Did I misunderstand the paper? [1]https://www.cs.utexas.edu/~wcook/Drafts/2012/graphs.pdf
A zipper should solve this problem fairly cleanly. /u/Tekmo's link is good, as is the wiki page: https://en.wikibooks.org/wiki/Haskell/Zippers
gfixler's answer can be viewed creating a mini-relational structure as well. The Map in this case can be thought of as having "foreign keys" which reference other things. Often the simplest way of implementing this kind of thing (i.e. a Map) works until the situation requires a more complicated answer.
One problem with TH is that it doesn't work on all platforms, which is exactly what you don't want for a feature to help portability...
`getN` and the instance for Vector in vector-binary-instances are using a different sized Int to read the expected length. You are using `getWord16le`, while if you follow the code in the package it looks to me like the instance decodes length as an Int64 (M.new takes an Int, and the Binary Int instance can be seen here - http://hackage.haskell.org/package/binary-0.7.3.0/docs/src/Data-Binary-Class.html#get). 
Here's a way overkill idea. Use Bound to make the sharing explicit in a type safe way. One way is to turn your `Room` type into something that can be a valid instance of `Monad`, then you just transform it with my handy-dandy-magically-add-sharing `CyclicT` monad transformer. (Though technically it meets most of your requirements, I'm **not** really being serious. Treating it as relational data, as mentioned elsewhere already, seems a lot more practical here.) {-# LANGUAGE ExistentialQuantification #-} {-# LANGUAGE LambdaCase #-} import Bound import Control.Applicative import Control.Monad import Control.Monad.Free import Data.Map (Map) import Data.Void data CyclicF m a = Var a | forall k. Ord k =&gt; LetRec (Map k (Scope k (CyclicT m) a)) (Scope k (CyclicT m) a) newtype CyclicT m a = CyclicT { unCyclicT :: m (CyclicF m a) } instance Monad m =&gt; Functor (CyclicT m) where fmap = liftM instance Monad m =&gt; Applicative (CyclicT m) where pure = return; (&lt;*&gt;) = ap instance Monad m =&gt; Monad (CyclicT m) where return = CyclicT . return . Var CyclicT m &gt;&gt;= f = CyclicT $ m &gt;&gt;= \case Var x -&gt; unCyclicT $ f x LetRec cs c -&gt; return . LetRec (fmap (&gt;&gt;&gt;= f) cs) $ c &gt;&gt;&gt;= f data Room room = Room { roomName :: String , roomItems :: [Item] , roomDark :: Bool , roomDoors :: [Door room] } data Item = Item { itemName :: String , makesLight :: Bool } data Door room = Door { doorFrom :: room , doorTo :: room , doorLocked :: Bool } newtype GameWorld = GameWorld (CyclicF (Free Room) Void)
I don't know why it gives the errors it does, but `decode` will of course only make sense of what came from `encode`, no? If I write vec :: V.Vector Int16 vec = V.concat $ replicate 1000 (V.fromList [1..100::Int16]) premain = B.writeFile myFile (encode vec) to prepare my properly encoded file, then if I omit the `hSeek` business -- which doesn't land me at a place where a vector of Int16 begins to be encoded, then I get &gt; main 200008 100000 But if I add the bit about seeking I get &gt; main 197704 &lt;interactive&gt;: out of memory 
Oh, maybe I'm using the wrong tool for the job then. I'm basically looking for the most efficient method of going from a binary file to a vector. Is reading it manually via the `Get` monad my best shot at this?
Get is a precision instrument for decoding typed serialized material, including complex Haskell types, so it won't be as fast as can be for this. If you can use a strict bytestring then you can immediately get a Storable vector of Word8s to fiddle with using something like this: https://github.com/snoyberg/mono-traversable/blob/master/src/Data/ByteVector.hs#L16 or some similar implementation. Then, if you want to be really depraved, you can use for example unsafeCast :: (Storable a, Storable b) =&gt; MVector s a -&gt; MVector s b writing something like toInt16s :: ByteString -&gt; S.Vector Int16 toInt16s bs = runST (do mv &lt;- thaw (toByteVector bs) freeze (unsafeCast mv :: MVector Int16)) (I didn't typecheck this so it's probably a little off) I think you can see how that would go ... and might easily go wrong. All of the above was inside Data.Vector.Storable and Data.Vector.Storable.Mutable. There are many obvious ways of using the standard combinators to go from there to one of the other vector types. You should only use Data.Vector.Storable or (much more conveniently) Data.Vector.Unboxed and the corresponding mutable modules. 
I should have linked http://hackage.haskell.org/package/vector-0.10.12.2/docs/Data-Vector-Storable.html#g:34 the first several functions there are the ones to use, including also the `convert` function to take you to another sort of Vector, e.g. unboxed or Data.Vector.Vector. You don't need the `Mutable` module. (Note the remark about the size of the memory block, and also the `unsafeThaw` and `unsafeFreeze` functions; I didn't use them above because I would have been typing `unsafe` a few too many times in rapid succession.) I meant to say at the end there that `Data.Vector` is for complex Haskell types, e.g. you can make a `Data.Vector.Vector (Data.Vector.Unboxed.Vector Int)` But certainly not the other way 'round. Edit: nkpart's `byteStringToVector` function puts all these steps together into one go. The `mmap` idea is also in http://hackage.haskell.org/package/vector-mmap (but hasn't been updated lately, so it might be broken.)
They way I see it you don't want doors contain rooms, rather you want them to contain "references" to rooms, aka lenses: data Light = Light | Dark data Room = Room { roomName :: String , roomItems :: [Lens GameMap Item] , roomDark :: Light , roomDoors :: [Lens GameMap Door] } data Item = Item { itemName :: String , makesLight :: Light } data Locked = Locked | Unlocked data Door = Door { doorFrom :: Lens GameMap Room , doorTo :: Lens GameMap Room , doorLocked :: Locked } data GameMap = GameState { yard :: Room kitchen :: Room livingroom :: Room basement :: Room door-kitchen-yard :: Door door-kitchen-livingroom :: Door door-yard-livingroom :: Door door-livingroom-basement :: Door flashlight :: Item key :: Item } 
Actually it is. Not everything is valid, but especially the Greek letters are quite useful as parameters in math stuff, as they tend to stand out from the typeface. Alternate keyboard layouts support Greek letters ... but I mostly copy them from Wikipedia ;-)
&gt; I think it's interesting how the use of SMT solvers really helps gloss over some of those boring "Hey look how arithmetic works exactly as you'd expect" lemmas. [There might be hope for that in Idris](https://groups.google.com/d/msg/idris-lang/YRg7p14LVGg/DCR_gB_6otIJ)
Ok ... [you convinced me](https://github.com/fhaust/dtw/blob/master/src/Data/DTW.hs#L77). Is it ok to use `FlexibleInstances` and `UndecidableInstances` in this case? OTOH: why is there (still) no `Indexable` typeclass in Haskell?
I've used that method before too. It's simple and it works. If there are parts of the state that are inherently read-only and cannot be changed dynamically - like the graph of the rooms, for example - you can do that with "tie the knot".
Thank you for all this information. I've just tested out a demo using mmap'd IO, and it seems to run fast enough for now. I plan on using more and more Haskell at work for data processing, so the more info I can glean, the better. Cheers :-)
&gt;Please enable JavaScript to view the comments powered by Disqus. Huehuehue :-)
 &gt;I asked him if he thought Elm was ready for production. It's not, and its creators admit this, thus the 0.x version numbers. This is good, because it's allowed them to make changes and improvements without worrying about backwards compatibility. Give it a year or two and I think it will be amazing. 
&gt; And the third problem, and for me the nail in the coffin, is it's a real pain to install. If you thought cabal hell was hell, I can tell you there's another layer of problems lying in wait. Honestly, I couldn't even say to anyone, "You should try GHCJS." Getting started is asking too much. It's a valid point, but not necessarily a fair one - it hasn't even been officially released yet. It's getting better, but I understand that is hardly an argument :) I'm still betting on GHCJS though, but sane installation is a showstopper. &gt; I've heard people say Elm is great for game-like things, but little attention has been given to regular HTML apps. That's not true. The elm-html library is, give or take, a perfect substitute for Om/React. &gt; And the FRP approach may prove a bit of a learning curve for some, but I found it very natural. There's a certain way that I've been structuring my ClojureScript apps, and Elm feels like it's taking the same route, but has baked it into the language. I heartily approve. Absolutely true, imo - I *love* FRP for what might be seen as generally *non*-reactive programming. Of course, a web app is reactive - but not in the aggressively real-time way a video game is. However, the ability to declare how things change over time has been revolutionary to me in terms of structuring multi-page applications. For me, I'm eagerly awaiting: a stable GHCJS &amp; installation strategy; `reactive-banana` 0.9, which will hopefully address the remaining problems I have with FRP; and Backpack, which will hopefully let me use `Aeson` on the client and server, but swap out the implementation for native `JSON.parse` calls.
I also posted my experience using PureScript for a fairly large project [here](http://kevinmahoney.co.uk/articles/Jan-2015-editor-update/)
Nice, I was just studying the tutorial of [hplayground](https://github.com/agocorona/hplayground) and found it really interesting to try later. I wonder if post author tried this framework.
Have you looked at Fay?
I haven't. I'm afraid I dropped Haste as soon as I hit the interop wall. (Not that I want to inter-operate with JavaScript. But I must, and when I must, I don't want it to be any more painful than necessary...)
I agree. It's pretty amazing even in its 0.x state. Although I meant the final paragraph as a jokey ending, there is a serious point there. "Production ready," is a very subtle term when your baseline is JavaScript. You're choosing between, "language may break for improvements," and, "language is - and always will be - broken." In the land of the browser there is always pain, but also the chance to choose which kind of pain you can live with.
I know you want to promote Haskell and all that, but [js_of_ocaml](http://ocsigen.org/js_of_ocaml/) deserves to be in this list. Edit : I realize the front page is doing a quite poor job at advertisement, so just a list of feature we have directly in js_of_ocaml to makes you salivate. :p - The compiler produces efficient *and* small code. - Use any OCaml module ever, yes, even some weird stuff like an http libraries (with clever functorisation). [You can even compile the top level](https://ocsigen.github.io/js_of_ocaml/#version=4.02.0). - FRP with the [React](http://erratique.ch/software/react) library (has nothing to do with react.js). - Combinators library to produce Html with Tyxml (and it's proven valid by the type system). A "Virtual Dom" library is available in Eliom too. You can also combine with FRP to have a reactive Dom that auto-updates on signals. It's awesome. - Concurrency with the Lwt library.
Oh, I just mean if I have started with a philosophical foundation which does not accept CT (e.g. intuitionism), then I should have no problem with using a solver. FYI, solvers have nothing to do with refinement types; I brought it up since liquid haskell uses a solver.
I've written some implementations of data structures from Okasaki's book in Idris. [Here](https://github.com/timjb/idris-pfds/blob/master/RealTimeQueue.idr) is my implementation of lazy queues that are indexed by their size. It uses Idris' `Lazy` data type for proper amortization.
&gt; least desirable way to solve the problem Well, it will make type errors "interesting". But, the _1, _2 etc. lenses can also make type errors "interesting" in a different way.
&gt; I'm still betting on GHCJS though, but sane installation is a showstopper. Depending on when I have tried to install GHCJS, I have had a hard time doing it. Every time after it is installed I am wowed that almost any package I install works just like if I had installed it through cabal normally.
I only wrote small programs in it and there's a certain lack of JS libraries, but Idris seemed pretty good. Performance might be lacking (its JS RTS is as simple as it gets), but OTOH it's the most programmer-friendly dependent programming language I've used.
The [original Template Haskell paper](http://research.microsoft.com/pubs/67015/meta-haskell.pdf) suggested that Template Haskell could replace uses of CPP including selecting code for different platforms.
This might help with ghcjs installation: https://github.com/ryantrinkle/ghcjs-setup
LiquidHaskell works with GHC 7.8.3 and 7.8.4, and should just be a `cabal install` away. Please open a ticket at https://github.com/ucsd-progsys/liquidhaskell if you can't install it!
I think you're working towards reinventing free the (or operational) monad + interpreter pattern. http://apfelmus.nfshost.com/articles/operational-monad.html is a decent introduction to them. (That link talks about using the `operational` library - one example of that approach.)
I'm not sure what the author's issue with Haste interop is. Here's a little code that reads a list from JS, reverses it, and sticks it in an alert. {-# LANGUAGE OverloadedStrings #-} import Haste import Haste.Foreign jsList :: IO [Int] jsList = ffi "(function() { return [1, 2, 3]; }) // arbitrary JS here" main = do list &lt;- jsList alert . show $ reverse list 
Thanks for the link! There seem to be a few differences, though: * the operational pattern relies on GADTs, while my post doesn't * the operational pattern uses programs of fixed length, while in my post the length can depend on the initial value of *a* passed to the program, and on the values of *q* received from the handler * the operational pattern can be used to implement any monad, and we know that not all monads commute, while the goal of my post is to make everything commute (though I haven't figured that out yet) I'm not sure if these differences are important. Maybe the two approaches really are the same. Still thinking...
I wish they had called it `identity` instead of `id` because the latter would be so useful for local variables and properties.
&gt; the operational pattern uses programs of fixed length That's not true. In every monad the sequence of actions can depend on values.
To also track the size of the tail (one less than the list itself.) This book keeping is tucked into the `tl` and would be tricky with pattern matching. You could use raw pattern matching if you indexed the list with its size (as in the Idris implementation above) but here we're just using plain Haskell lists. Does that make sense? Thanks!
Thanks! I'm feeling the pinch about mobile devices myself. Any suggestions? I quite like the Rust book style: http://doc.rust-lang.org/book/variable-bindings.html 
UPDATE: So it turns out that our project must also run through ubuntu via putty. Im not sure how much this affects our project but it pretty much removes the idea of using external libraries and packages right? If anyone could reply to help out with some ideas that are nearly completely text based i would be greatful, but Im already thankful for all the help you guys have given me. Thanks again!
I've had a lot of success and joy with Fay. I have been able to replace all javascript on my sites with Fay. And as I understand, FPComplete wrote their entire online IDE in Fay.... so it's ready for web apps, too :) 
&gt; By updating the tables. For instance, if you dig a hole in the ground from the yard to the basement you can just: No, I mean the contents of the rooms if they had any. Would the passages not refer to the old rooms if you changed the yard room's contents. Say digging a hole creates a mound of dirt in the other room after connecting it. &gt; But I think the Named type lends itself better to handcrafting the database since the ids are meaningful to a human and because it allows you to have top-level bindings to game entities Ah, I see.
That looks pretty slick. It's a wrapper around an invocation of the Nix package manager. If you don't have Nix already, it'll install it, and then load a sandboxed shell with ghcjs and nodejs. I just set up a nixos VM for basically exactly this use. I'll have to give it a shot.
Really loving this series.
How long ago and for what platform was a ghcjs install attempted? I know it was a headache to install as little as 4 months ago, but I tried earlier this week and had no problems at all: cabal update cabal install cabal-install # for version 1.22 git clone https://github.com/ghcjs/ghcjs-prim.git git clone https://github.com/ghcjs/ghcjs.git cabal install ./ghcjs ./ghcjs-prim ghcjs-boot 
https://ghc.haskell.org/trac/ghc/ticket/10063#ticket
Wow! That seems to be exactly what I wanted. Many thanks :-)
One tool that already goes some of the way is HLint. It can be used to warn about uses of partial functions, etc. Its rule databases also rather directly use the observation that Haskell is already rather good language / syntax for specifying refactorings. However, of course, HLint does not support automatically appl I realize that the language feature proposal is really diving down into one specific detail of this problem, but I think it's an important one. With this feature in the language, it would mean that Haskell could be used to specify a re-factoring which fixes breakages due to AMP. To me, classes are the most brittle things to change. For example, we can't rearrange the numeric hierarchy without huge amounts of manual labor.
This talk resonated a lot. It was posted as a part of a batch of others 18 days ago, but thought I'd re-post this talk specifically in case anyone missed it (I had).
i was going to run that ghcjs setup script, (on OS X), it said it would take several hours. the homepage said nix is cross platform. is there an alternative you're suggesting? i don't know much about nix, beyond your blog.
the talk was really great. as someone who reads your blog, I'm interested in your thoughts on the talk too.
Parsec. And all the other parsing libraries are very illuminating.
My one other desperate must-have for GHCJS is GHCi support. I need it for my editor support and quickly exploring code.
This is what I usually do (assuming negative numbers are not used): instance Eq Expr where (==) = eq (-1) [] [] where eq _ r r' (Var i) (Var i') = case (lookup i r, lookup i' r') of (Nothing, Nothing) -&gt; i == i' (Just v, Just v') -&gt; v == v' _ -&gt; False eq n r r' (App f a) (App f' a') = eq n r r' f f' &amp;&amp; eq n r r' a a' eq n r r' (Lam i e) (Lam i' e') = eq (n-1) ((i,n):r) ((i',n):r') e e' eq _ _ _ _ _ = False 
Oh cool. Very swank formulation of the 'banana split' equality!
It sounds like `opengl` just doesn't sit at a very good space in terms of abstraction, which isn't surprising to me considering the size of the API they're targeting (I'm thinking also of python's `boto` for AWS). I think a smaller example might be more convincing. If the advice is "expose an Internals module with your datatypes if you think it might be useful at all, or when asked" then it's hard to disagree. I also wonder what a defense of the sort of process that is getting in his way might look like. Sounds like `opengl` is a pathological case, but might there be something to be said for the slow process of discussion and collaboration re. a library's abstractions? Or maybe there's value in forcing ekmett to just take a vacation for a bit :) And since `Internals` modules are likely to be "all bets off" anyway, why not some sort of noisy language pragma that allows users access to certain internals of modules they're importing? I'm sure there's some boring technical reasons why this would be very difficult. Anyway, I'm excited to see what else results from his graphics work in addition to cool demos!
I have used both Fay and Haste to build web visualization tools. I liked Fay, but personally found Haste to be at least as good as Fay in every way (and better in many), so I'll only explain my experience with Haste. Haste is essentially full Haskell. If I recall correctly, I was able to take `Data.Map` and compile it in Haste without any adjustments. I think that's pretty cool! Yes, I did need to make myself a small library for manipulating the DOM (perhaps there are standard libraries available, but I was mainly using SVG anyway and so had my own needs), but Haskell's capabilities for abstraction make this fairly easy and painless. Contrary to the author's remark, I found Javascript interop easy, and was able to process complicated JSON data structures that were served by a Haskell server. I *heartily* recommend using Haste for Haskell in the browser! (I will note that I don't normally develop web apps, so perhaps I don't have a good baseline understanding of the alternatives. Maybe my tolerance for pain is too high?)
Eagerly [awaiting](https://www.youtube.com/channel/UCUCpgCWjaniUkX88wZrK_Ig) the video for the [Typeclasses vs. the World](http://www.meetup.com/Boston-Haskell/events/219074467/) talk.
The repo is just `nix-shell -I . -p haskellPackages_ghcjs.ghc haskellPackages_ghcjs.ghc.ghc.parent.cabalInstall nodejs` with a preamble to install nix and nixpkgs if they're not already installed.
Yeah, OpenGL is the worst offender of this problem that I've encountered. I think you're right on target that it's in large part due to scale: covering everything with a nice API would be extremely difficult. This leads to a worst of both worlds where the differences from the standard lose us the benefit of existing documentation, and we still don't have a great Haskell-specific API. That said, the Raw package is clearly a good move to right the ship, so I view what Ed says here as a useful postmortem as we move on into a better future.
Like this one? https://www.fpcomplete.com/hoogle
AFAIK there were implementation difficulties, see answer to http://stackoverflow.com/questions/4113207/frp-reactive-how-to-use-filtere
Note that the definition of `makeq` and `rot` is still using `l`/`r` for the front/back part instead of `f`/`b` as referred to in the text.
The main problem I have with the `OpenGL` package has nothing to do with its size. It is that it is advertised that `OpenGLRaw` is the escape hatch you should use when it doesn't suit your needs, but it doesn't provide you the data you'd need to climb out the hatch.
Par for the course ;)
FRP is a very young field. I wrote a [comment](http://www.reddit.com/r/haskell/comments/2umqfr/a_brief_and_partial_review_of_haskell_in_the/co9ugcv) about this just yesterday, in relation to how they did FRP in Elm. Reactive has some very hairy issues, mainly the "unamb" function which relies in the current implementation on parallel multiple function evaluation which is hard to implement correctly and is usually computationally intensive. There were also some bugs in the lower level library they used that caused leaks on various platforms. From what I gather, the up and coming technology for Push-Pull FRP is N-ary FRP with signal functions rather than signals as the primitive values. The early theses on the subjects are ["Towards Safe and Efficient Functional Reactive Programming"](http://www.cs.swan.ac.uk/~csnas/thesis/index.html) and ["TimeFlies: Push-Pull Signal-Function Functional Reactive Programming"](http://www.cs.rit.edu/~mtf/student-resources/20124_amsden_msthesis.pdf). Both are eye opening on the subject, but are still early research and have some issues, inefficiencies and problems which might be addressed in the future.
It looks like this needs at least GHC 7.8.4, did not work for 7.8.3 Nope, seems cabal install cabal-install brings in a version of transformers that conflicts. So did rm ~/.ghc cabal install alex happy after installing cabal-install. Easy to do in docker.
Ah great, didn't know that exists. I used Hoogle/Hayoo combo, but it's less convenient.
Yes, but because it's a completely different revision of nixpkgs some fairly fundamental SHAs have changed, which will result in a LOT of compilation. Edit: Sorry, I'm talking nonsense - it *used* to be the case that it was a separate `nixpkgs`, but I see Ryan has updated that now.
Yea, that's the only thing that makes me nervous right now.
I recently not once seen a statement that "FRP part is mostly figured out by now". For example, here's [the comment](http://www.reddit.com/r/haskell/comments/2kyr7x/a_functional_gui_library_built_from_ground_up/clq1iiq) from the author of reactive-banana, and there's another one I have recently seen in a youtube talk by author of sodium... I wonder why your opinions differ? Is it only "unamb" or are there some other issues?
Yeah, I see. There seems to be quite a bit of work on effect handlers in Haskell, which I can only understand now because I've badly reinvented part of it!
That depends on what you call FRP. In the comment I linked I've brought up the issue of timesteps which is one of the main reasons I've dug deeper into the subject because the way it is done in the popular libraries doesn't feel satisfactory. The computation inside the FRP network should not, imho, be affected by the timestep as it is with most of the systems. Moreover, there are safety issues you can alleviate by making the system rely on signal functions rather than the signals themselves concerning the method of evaluation and better protection from loops and timing concerns. I am not an expert on the subject, but saying that FRP as whole is figured out is plain wrong. A certain class of FRP might be figured out, but not the whole concept.
Per the [readme](https://github.com/ghcjs/ghcjs), adding `----constraint=transformers==0.3.0.` would have also worked .
&gt; the operational pattern relies on GADTs, while my post doesn't That's just an implementation technique (and a very nice one, at that). It doesn't mean that /u/apfelmus is doing something different than what you are trying to do.
Oops. Thanks, fixed!
&gt;Strangely, nobody seems to take the other side of the debate -- except to take a cheap shot at Scala now and again! /u/edwardkmett, that's *incredibly* unfair, considering you're the one taking cheap shots at Scala every time it comes up. Anyone got slides from this? I try to collect Ed slides.
It's a monad cotutorial. I guess `duplicate` works for that, too. 
No, it doesn't matter in this version. It was late and I decided to leave it alone. :)
What're your remaining issues with FRP?
Indeed. What I was considering is how `OpenGL` got itself into its current state.
They aren't really issues with FRP, it's just an implementation detail in `reactive-banana` - see https://www.reddit.com/r/haskell/comments/2umqfr/a_brief_and_partial_review_of_haskell_in_the/coas0yr.
I was making a joke about that very fact.
I always assumed "vacation" was just your term for "starting a new project".
I feel like this relates to something the Clojure people keep saying. They don't want interfaces consisting of opaque functions – they want raw, simple data to be the interface. If your thing is internally powered by a HashMap, then expose that HashMap. Don't try to impose limits on what your users are allowed to do, because if you try to guess what they want to do, you'll always guess wrong.
Thanks!
https://www.youtube.com/watch?v=Agu6jipKfYw
They've had to somewhat refine the naive approach to this, see Stuart Sierra's talk on the challenges they faced wit that they're currently considering as a way to go forward... https://www.youtube.com/watch?v=13cmHf_kt-Q 
Neil just a couple weeks ago started talking to the haskell.org committee about ways to make it easier for him to maintain and work on, so I think he's also feeling the pain there, and steps are actively being taken.
i got a "Template Haskell + profiled compiler" bug on ghcjs-boot. so, I guess I'll nuke ~/.ghc and try again, disabling "profile: False"?
are there any packages on hackage that haste can't compile? 
The Haskell.org committee's responsibilities include * setting the policy on what the haskell.org domain name, and its subdomains, may be used for * setting the policy on what the servers owned by haskell.org may be used for *determining how haskell.org funds are spent (see https://wiki.haskell.org/Haskell.org_committee) Actual infrastructure work is performed by volunteers who are loosely coordinated to the #haskell-infrastructures admin team. Hoogle is and has been Neil's project. We've been working to provide him the resources to serve and administer the new hoogle in the best way possible. It isn't just maintaining, but moving forward with what will be some awesome new features. Both the committee and the admin team basically do all the work associated on a voluntary basis, as best we can, with time we can carve out from elsewhere. There's always work to be done -- from becoming a contributor to a resource like `hackage` or `hoogle` to stepping up to help administer and cleanup the wiki or offering expert assistance in design or management of mediawiki quirks, etc. Our resources are only as good as _we_ make them, and the more that people get involved, the better things can be! You can reach the committee at committe@h.o, and the admin team at admin@. There is also a #haskell-infrastructure channel where you're invited to hop in and get involved, or just idle a bit to learn what sort of work is underway and how you could help.
Wow, great to hear that there's another new Haskell team being started!
Yep, that all makes sense -- I think implementing new languages / etc. is a good place to try and integrate Haskell to that ecosystem. (I once had plans to implement an EVM using LLVM's JIT, which still sounds like a lot of fun.)
Rather than cheap, I find Edward's shots at Scala fairly opulent and sometimes downright extravagant, to be honest :-P
We're looking into the account issue now. It's something strange with cookies in the mediawiki setup not being recognized (even though it created them itself!).
Just fixed that -- we had an incomplete cert chain for some of our sites. So it worked fine unless one of the designated providers for one of the "upstream" certs went inaccessible, in which case the browser didn't believe our downstream cert. Now we provide the full chain so it shouldn't reoccur.
Sounds like FB is seeing success with Haskell. Awesome news!
Thanks!
I'm a pretty inexperienced dev who's still in college but I'll certainly drop my resume in. 
Very cool! I'm assuming these aren't positions that can be done remotely? 
Before I set out to update my haskell emacs situation, I had no idea that inferior-mode was even replaced or flymake was deprecated years ago in favor of flycheck. Hopefully this little guide/list helps people see what they should think about replacing in their own configs.
Ah, ok. Thanks for the update.
&gt; if "exe=`setxkbmap -query | grep -c il`" == "1" then &gt; "exe=`setxkbmap -layout us` &amp;&amp; exec $exe" else &gt; "exe=`setxkbmap -layout il` &amp;&amp; exec $exe" Since what you've written is already Bash-like, you may as well write it as such: if [ `setxkbmap -query | grep -c il` = 1 ]; then setxkbmap -layout us; else setxkbmap -layout il; fi and then call it from Haskell with: spawn "if [ `setxkbmap -query | grep -c il` = 1 ]; then setxkbmap -layout us; else setxkbmap -layout il; fi" 
Interesting. I'm not sure, as I've never compiled with profiing enabled. I will say if you swing by the IRC channel when luite is available, he'd probably be more than willing to help you through it. I guess there *are* still a few wrinkles. I wonder if anyone has considered conducting a hardware survey to see when such failures happen?
Any interest in intern level candidates?
I understand your frustration. Here's my haskell emacs config: https://github.com/LukeHoersten/emacs.d/blob/master/elisp/haskell-init.el Definitely not simple. I'd like to get it to the point where there's some good sane defaults for haskell-mode but stuff is still getting merged-in and sorted-out before that can happen. I'm just excited that most of it is not self installed hacks but real packages through normal channels. That's a big step.
Would you help people considering immigration to get a job visa? Any help on relocation?
GHC doesn't cross-compile user programs, if that is what you want. But I believe what you can do is to run native GHC on each platform to compile a user program. Then the result binary would be standalone executable if you compile statically. With some effort, GHC itself can be cross-compiled to run natively on exotic platforms, but this is still a work in progress. 
Facebook hires large numbers of interns. If you're interested in learning more about our internship programmes, [please read on](https://www.facebook.com/careers/university).
We do both, yes.
Why can't you forget outputs? It seems like that should be fine, and there should be a combinator `terminal :: p a o` at the very least. Secondly, I presume you meant categorical (sequential) composition to be implied, but what about parallel composition? Why is `(***) :: p a b -&gt; p a' b' -&gt; p (a `x` a') (b `x` b')` not allowed? This construction would be present in a monoidal category.
And having read up on n-ary FRP, I don't understand why you can't just make it an Arrow.
I wrote [the turtle tutorial](https://hackage.haskell.org/package/turtle-1.0.1/docs/Turtle-Tutorial.html) with the express purpose of getting people writing useful Haskell programs in a short amount of time. Check it out if you want a low-barrier way to introduce people to the language.
It is not only beginners who need examples and tutorials. A composable library may not be very intuitive, yet a single example opens the whole thing up and makes it possible to use the haddocks effectively. The fact is you need both things and we are often missing the examples in Haskell.
Yeah this is the way to go. Part of my senior design project was Haskell on a Raspberry Pi. I just installed ghc + hugs + emacs on Raspbian and compiled on the device. I believe older RPis had too little memory to build ghc from source, and there was [advice](http://www.raspberrypi.org/forums/viewtopic.php?f=34&amp;t=6655) out there to build on a desktop using QEMU and then put the binary on the RPi.
I'd like to add that [Steve Purcell](https://github.com/haskell/haskell-mode/commits?author=purcell) has since summer of 2014 really been driving a lot of haskell-mode support, being very responsive to people and contributing a number of cleanups and fixes.
[ghci-ng](https://github.com/chrisdone/ghci-ng) looks like a fork of ghci with extra functionality inserted. It's a bit of a stretch to then claim it's part of the standard haskell toolchain and thus in a different category to other tools like ghc-mod right? More generally, there seems to be a split about whether ghc-mod is essential, leaving me confused over the best setup. I'd be interested in whether the author tried it and decided that ghci-ng was better (and others' opinions). 
Sorry to disagree, but I've been building and testing a `linux-amd64` to `linux-armhf` cross-compiler for a while and know of others cross compiling from OSX to iOS. Its not as easy as it should be, but it very definitely is possible. I for one am aworking on making cross-compiling easier and more robust. 
This is pretty much how I deal with exceptions these days. You can also use this approach with the `exceptions` package and the extensible exceptions in GHC. The module [`Control.Exception.Lens`](http://hackage.haskell.org/package/lens-4.7/docs/Control-Exception-Lens.html) provides a bunch of combinators for working with the types in `Control.Exception` in a similar fashion. It predates the `makeClassyPrism` code a bit, but the naming convention is the same, and there we add the various `AsFoo` instances for `SomeException` as well. One nice side-effect of that is that combinators like `Control.Exception.catch` which almost universally required `ScopedTypeVariable`-style type annotations can generally be used with just a prism: e.g. catching _IOException x print rather than catch x $ \(e :: IOException) -&gt; print e
if only this came I'm a couple years after I finish my PhD in these subjects with a Haskell based thesis haha. 
&gt; Sorry to disagree, Don't be! I just want to cross compile from linux to windows/mac, I don't care as much about being right. Could you give me a short list of instructions as to whats needed for this or any tutorials you followed? 
Previous version of ghci-ng have been merged into main ghci in the past and, as I understand it, that will continue to happen. Ghci-ng is a bit of a staging ground. You can use most of the features without -ng. I've used mod-ghc for years and it just doesn't keep up with new features etc. It's slow and I have to keep `pkill ghc-mod` bound for when I hit stability issues. Ghci (and the additional features -ng adds) are much closer to being in the fold than mod-ghc. I haven't tried hdevtools and scion was super unstable and hard to keep working. Another benefit is being able to use one ghci process for all the features (instead of ghc-mod for on-the-fly checking and ghci for repl, etc).
I disagree. Learning what Monads are is exactly what gets in the way. You don't need to know what a Monad is to use a Writer or Reader or State. In fact knowing what a Monad is doesn't mean you can use any of the aforementioned Monads and often creates more confusion and frustration when learning Haskell. The vast majority of the programming world needs examples and tutorials, don't expect them to rise to your ideal.
I still think Hackage needs a comments section. If the community can't rise to the occasion and write docs then they need to be crowd sourced and curated.
Oops sorry didn't mean to cut anyone out! Thanks to everyone who's contributed.
I find that documentation that focuses on examples rather than explanations to be less useful. But as far as getting documentation from GHCi, I've discovered this: http://hackage.haskell.org/package/haskell-docs
Hm, everything I've seen has suggested that these alternatives are mostly a subset of what ghc-mod offers, so I'll have to look into it again to reevaluate. `company` is super flexible, so I'm sure it can be made to work with whatever source. You'd also want to take advantage of what `company-ghc` offers.
First stop is the GHC wiki : https://ghc.haskell.org/trac/ghc/wiki/Building/CrossCompiling My notes cleaned up a little follows. To build a GHC cross-compiler, you need a couple of things: * A working native GHC. * A cross-gcc (yes, GNU gcc configured as a cross compiler for whatever you are targeting. For targeting Windows from Linux, you could use the mingw-w64 cross-compiler if your distro has them (Debian Testing has gcc-mingw-w64-i686 and gcc-mingw-w64-x86-64 for targeting 32 and 64 bit Windows respectively. Similarly, when cross-compiling to linux-arm and linux-aarch64 I used gcc-arm-linux-gnueabihf and gcc-4.9-aarch64-linux-gnu also in Debian Testing). Obviously for Mac, you'd need a Linux to Mac gcc cross, but I'm not aware of a pre-rolled one. Once you have the gcc-cross sorted out, you need to grab the GHC sources. I usually work with GHC for git, but thats mainly because I'm working on fixing cross-compile issues. The ghc-7.10rc2 source tarball would probably be a good place to start. From ghc git HEAD I do the following in the following: perl boot sed 's/#.* = quick-cross/BuildFlavour = quick-cross/' mk/build.mk.sample &gt; mk/build.mk ./configure --target=$target --prefix=$prefix make make install where `$target` is something like `aarch64-linux-gnu` or `arm-linux-gnueabihf` and `$prefix` is where ever you want to install your compiler. The above may be a little different if you are building from a source tarball. I have the same username in #ghc on the Freenode IRC channel if you have further questions. 
I wonder if my basic Haskell experience counts… 
or even: &gt; f :: Ops -&gt; ByteString -&gt; ByteString &gt; f Ops{..} xs = doFrobThing xs 
All of these points are impressively accurate. The first one especially is a problem but unfortunately not one lwe can do anything about since it's inherent in the powerful language that Haskell is. More examples in the documentation certainly woud be nice.
I've been to their office and talked to some of them. My impression is that it's a big startup with a good culture. Open plan offices, lots of sweets and a relaxed environment. 
Can you talk more about what the second team will be doing?
True! To clarify, this isn't the time I think it would take to go straight there, but rather working up the chain of typeclass -&gt; functor -&gt; applicative -&gt; monad, as done in the homework. Also, not just "can get it to work", but to reach a certain level of understanding that I was happy with.
Open plan offices? Yuk
I believe Facebook is actually in the middle of building the largest open plan office ever. http://www.forbes.com/sites/kevinkruse/2012/08/25/facebook-unveils-new-campus-will-workers-be-sick-stressed-and-dissatisfied/ 
You can almost always find exactly those examples by clicking the "source" link. The library is written in its self. As are any libraries or programs that depend on it. Both attoparsec and aeson use applicative style (though the later more so). I'd say we usually have something better then documentation in many ways. Especially if what you want is to pick up the little tricks people use to make code cleaner. Every library built on top of another is documentation for the other.
Good to know. Thanks!
You can say it as much as you like but it ain't true. The trouble is that understanding those source libraries requires a much deeper familiarity with the language than have beginners. My sense is that the main reason is the use of automatic documentation tools. Somebody writes a library and they document the purpose of each function so that it shows up nicely and you're done. Except you aren't really, at least if you care about the users of your library. The problem is that people need to see how to use those functions in context. It really doesn't help to say "the foobarM function is used to attach the snafuMonad instance to a gigaWidget value constructor. Beginners don't know what that means. They can't learn anything from it. I've said this before but someone in the haskell community with good writing skills needs to write an O'Reilly-style cookbook just like those available for many other languages. Users should be able to browse it, read it from the beginning and/or simply use properly tested snippets to actually get the job done. Once that happens and people just start using such things, understanding will just creep in.
Actually a language extension to access module internals would be amazing. Sometimes I wouldn't mind the OO ability to override a method in a class. I had a situation where a library author wrote a partial function. The rest of the library was fine but the that one function prevented me from using it. A simple way to write my own using their internals would have saved a lot of headaches.
It took me a while to figure out that the side effects free semantics make `count x` just as good as `length . filter (==x)`. So as you say: it *IS* in the library, indirectly.
sadly, no. This is the new normal.
Excited! I recommend this guy ( https://github.com/TerrorJack ) for an intern who is proficient in Haskell....:)
I agree with /u/badr in that they should not be expanded by default, but also... &gt; Often the instance declarations are interesting There's no way to view the source for an instance! I guess you can indirectly click on the type that has the instance, then view the source for the type declaration and hope that the instance declaration you're looking for is nearby... but that's really not great. Unless you meant, I just realised, that the simple fact that, for example, `instance Monoid Ordering` exists is interesting. I just think it'd be helpful to be able to go from there straight to the source.
&gt; Previous version of ghci-ng have been merged into main ghci in the past and, as I understand it, that will continue to happen. That's definitely the intent; consider `ghci-ng` a kind of incubator for new features for GHCi, which allows for shorter implement&lt;-&gt;get-user-feedback cycles than the yearly GHC release cycle, while still being easy enough to install to reach a large enough userbase. We'll probably start merging features into upstream GHC as soon as /u/chrisdoner considers the new features stable enough for that (and I'm hoping that'll be in time for GHC 7.12...) 
I'm starting to go sour on free sweets, honestly.
The repository contains a Dockerfile, so simply `cd` to your cloned repository, then `docker build -t ghcjs .`. It takes quite some time but then you end up with a `ghcjs` image. Run the image with `docker run -t -i ghcjs bash` and you can try the Hello World example.
Working remote is the future. :-P
Its not about Haskell, but PureScript (which is very similar) has (something like) that here: https://leanpub.com/purescript especially chapter 7 and after
&gt; There's no way to view the source for an instance! Yes, this is extremely annoying. 
As I recall it, haste can't compile packages using template-haskell.
Comparatively Fog Creek seem to get this right http://www.joelonsoftware.com/items/2008/12/29.html
Actually I'd disagree with you there. If there's one thing wish I could tell my past self when learning Haskell, it's don't ignore the instance lists. In fact, when dealing with a new data type, look at them first. Often, when there's functionality that seems like it should exist but doesn't, it's in the instance lists. Which can actually be nice (oh, right, you do X by using the Monoid instance. I already know how to use Monoids! I don't have to learn anything new to use this part of the library).
&gt; Does anyone know if there is anything similar for Haskell Well yes, pretty much all Haskell is written in this style. Are you looking for a specific blog post that you can direct your colleagues to? &gt; Category Theory, of which Haskell is essentially a programming language implementation As an aside, this is not really a meaningful statement.
*you can’t view function docs from the interactive shell (and that bug has been open for seven years)*. Can the employer for /u/chrisdoner give him a day off so he can implement it (-: ?
Looking forward to that!
I find that the call to Haskell must come from inside. I was pushed to learn it for a couple of years and never understood the beauty of it. Then one day I just went back to it and got *trapped* by the grace of it all. Just show these people some simple algorithm written in Haskell (something common like `fibonacci` or something just a little harder); if they are fascinated by it they'll learned it, otherwise they won't. 
An Ocharles write up on this would be a dream come true :)
The second team is the Haxl team, which you can read about [what we've open sourced](https://code.facebook.com/posts/302060973291128/open-sourcing-haxl-a-library-for-haskell/) and [in our github repo!](https://github.com/facebook/Haxl) The open source work hasn't been updated in a while, but there's been a flurry of activity internally built upon this stuff.
have a look on (most of) these: https://wiki.haskell.org/Research_papers/Functional_pearls start at the very first: http://web.cecs.pdx.edu/~mpj/snakecube/ this has nice graphics, a great problem and does not need any CT ;)
I for one find it hard to believe that someone completely new to programming can write production-level Javascript in 10 hours :) Can you hire someone new to programming and take only one or two days to make them production ready? My point is that Haskell isn't harder to learn than other languages... it's that knowing other languages doesn't really help you like you expect it to. Someone who is skilled at Java might be able to write production Javascript after 10 hours. But how long did it take that person to get to that level of proficiency in Java, if that was their first language? Saying Haskell is harder than Javascript in this case is a little silly... you're already 95% of the way to Javascript. That's like looking at two 100 meter dash tracks, but on one of them, you are allowed to start 95 meters in. Both tracks are just as long, still... but it might not seem that way to you if from your past accomplishments you only have to run 5 meters on one of them. It's easier for *you* with your background... but not objectively easier as a whole. 
I feel it might be valuable here to link to the "Parallelism for free" section of my monoids tutorial: http://community.haskell.org/~ertes/hangman/#parallelism-for-free I think it is more mind-blowing than the JS version. =)
Maybe for some, for me I suffer from a lot of loneliness and isolation. Did it for several years, can't say I'm in a rush to get back to it.
Is telecomuting from Europe a possibility?
What's the analogy you are referring to?
I'd like to openly admit that I acknowledge the fact I'm not a very good blogger or writer. That being said, I'm trying to get better at pushing the information from my mind to other people. All improvement suggestions very welcome and appreciated.
http://www.reddit.com/r/haskell/comments/2useoq/haskell_opportunities_at_facebook/cobb5ag
To which email address do I direct correspondance?
I suppose it might be possible to enlarge the space of n-ary FRP arrows to include arbitrary Haskell arrows but allow them no denotation. You'd need to somehow express beh :: (a ~&gt; b) -&gt; (B a ~&gt; B b) ev :: (a ~&gt; b) -&gt; (E a ~&gt; E b) which is only valid if `a` and `b` are *not* signal vectors. There is some room in design space in that direction that I haven't given any honest thought. Generally, though, that's the problem. It's not that you can't inject arbitrary functions into the space of n-ary FRP arrows—it's that you can do it in two fundamental ways and `arr` doesn't let us consider the difference. Furthermore, it's not clear that you *want* to be able to use `Arrow` for n-ary FRP. A lot of `Arrow` ends up being built around tuple manipulation on either side of your arrow. That would be fine except we can't use Haskell tuples to talk about bundles of different kinds of behaviors and events so something like `arr fst :: Arrow p =&gt; p (a, b) a` really needs to be its own method terminate :: GArrow x o p =&gt; p a o dropL :: GArrow x o p =&gt; p (a `x` b) a dropL = unitRight . second terminate so that we can do whatever tricks we need to behind the scenes in order to achieve that reordering in our general monoid.
Haskell is inconsistent (general recursion) so that's not really a problem here. When it comes to dependently-typed programming, you do have such results.
Do they go by a particular name?
Essentially any valid n-ary FRP arrow must be of the form K1 t1 * K2 t2 * ... * Km tm ~&gt; L1 y1 * L2 y2 * ... * Lm ym where the `ti` and `yi` are arbitrary kind `*` types (which probably aren't `Void`) and the `Ki` and `Li` are empty valued "tags" data B a data E a So an example arrow looks like `B a * B b ~&gt; E c * B d` and it means that we're transforming a pair of simultaneously executing behaviors into an event stream and a new simultaneous [0] behavior. There is no meaning to an arrow Int ~&gt; () since without the tags `B` and `E` we can't talk about the time behavior of this arrow. There's also no meaning to (B a, B b) ~&gt; (E c, B d) since `(,)` tuples would require we summon up real values of `B a` and `E c` but none exist. All of the tags and monoid products in the type exist purely as typing signals for the machinery actually occurring in the arrow alone—they have no honest Hask representation. If we extended `(~&gt;)` to allow us to carry arbitrary Hask arrows they would be totally vestigial. The only meaningful thing we could do is somehow transform them into arrows of the proper form which is what `beh` and `ev` do. [0] Maybe not genuinely simultaneous since we still need to have notions of infinitesimal delays in order to have causal loops.
It showed up because while I'd read "Simple, Easy!" it took me several times going through it before I made it to the finish line originally. On the other hand, "Simpler, Easier" go to the guts of a system more quickly and perhaps was pedagogically more approachable for that reason. I thought it was worth throwing it back into the loop, then, in case anyone would benefit from reading as a stepping stone to reading the full "Simple, Easy" paper.
I agree, but my point was not about writing production-level code, but rather about whether your code LOOKS like production-level code. This is important because it lets you learn by reading. At 20hrs I had never written a do, &lt;*&gt;, or &gt;&gt;=, but maybe a different teaching plan would get you there faster.
Right, I get the general form of the objection, but I don't believe it in this case. One also might say "Essentially any valid SQL query arrow must be of the form Column t1 * Column t2 * ... * Column tm ~&gt; Column y1 * Column y2 * ... * Column ym ... There is no meaning to a query arrow Int ~&gt; () since without the tag `Column` we can't talk about the DBMS behavior of this arrow." However, Opaleye *does* manage to work perfectly well with an `Arrow` instance. We have a monoidal category semantics for relational queries which embeds inside the `QueryArr` type. In fact its range is exactly that written above, i.e. when every component of the tuples is wrapped in a `Column`. However, there is no harm in allowing arbitrary Haskell functions to be lifted into `QueryArr`. You simply can't run a `QueryArr` if it contains values that are not wrapped by `Column`. There's just no way of running a value of type `QueryArr Int ()`. But this condition isn't enforced by the `QueryArr` type, it is enforced by the mechanism which runs `QueryArr`s. That's why I'm sceptical of claims that "`Arrow` is too powerful and we need more `GArrow`s in Haskell".
In fact my point of view is neatly explained in this Functional Pearl http://www.cse.chalmers.se/~joels/writing/bb.pdf (Maybe this paper is only *analogous* to what I am trying to say, rather than precisely it, but it's very close.)
Frankly, open-plan vs. not open-plan wouldn't even be in my top 10 of things to consider when choosing where to work. I had my own office at MSR and I now work in open-plan, and there are upsides and downsides to both. Most people can adapt and be equally productive in either.
&gt; In fact, when dealing with a new data type, look at them first. It would be nice if the documentation for a datatype (or the instance itself) would call more attention to the important ones, and explain them if they aren't unique. (Functor needs no explanation; but most others need some clarification.) In particular, things like DiffTime's Num instance (more enphasis) or various Monoid/Applicative/Monad instances (which semantics).
[Lenses](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html) are a really useful and instructive functional pearl.
+10. There's research in cognitive psychology and even software engineering that people do need both abstract descriptions and concrete examples. Experts are more efficient with abstractions but beginners need both the most. The impact of having both is dramatic. See for example the paper [Effects of Using Examples on Structural Model Comprehension](http://gsd.uwaterloo.ca/sites/default/files/2014-Zayan-Effects-of-Using-Examples-on-Structural-Model-Comprehension.pdf). 
That's right. People acquire understanding in layers. "Knowing what a monad is" is quite deep if you take into account all the category theory background you need to learn first. That's a deep layer. And, it does get in the way. Instead, forget about the monads and learn how to use a few examples of monadic API to get hang of the monadic programming style. That's it. Then understand that monad laws are what makes the whole thing well behaved and allows to reason about the properties of composition. That's sufficient for most people. 
yes yes... fixed.
Cryptic comment: this is one reason to prefer `.` to `$`.
I'd suggest having less reliance on "and then this should be obvious to you" kinds of phrases. You're attempting to teach an idea, and no, its not obvious if I've never seen this before (I haven't) - I can't glance at a piece of code and see how it works immediately. The wording ends up turning me off since I know that the rest of the article builds on this idea, its not obvious, I'm leaving. In terms of actual content: the `Whats going on` section starts referring to `hoist` and its arguments, when I don't see where they've shown up previously. `acc` and such don't have any previous mention, so I'm lost. (as a newcomer to this idea). It would be clearer to have something like: "the general form of this is hoist, which has a type signature: .....". Thank you for writing - I hope you keep at it. This did end up teaching me something, although to really integrate it into what I can do will take longer... 
I don't think that's accurate. Just [look](http://www.mew.org/~kazu/proj/ghc-mod/en/emacs.html) at `M-t` in ghc-mod by itself. That said, "find uses" is something ghci-ng offers that I don't think ghc-mod does. As for speed, I can't say that I really notice ghc-mod working; it's pretty well instant. The merging of the REPL with this sidekick information-providing process is the big win with ghci-ng, and I think is the way forward. It would be great if development could merge, but perhaps mutual stealing of features will be just as good for users!
Well, I woundn't mind count :: (a -&gt; Bool) -&gt; [a] -&gt; Int count p = length . filter p to be predefined in a standardlib (or even `count :: Foldable f =&gt; (a -&gt; Bool) -&gt; f a -&gt; Int`)
Oh my, I've only now spotted a mistake you're talking about. What a shame. I've been experimenting and changing the idea and order of samples a few times and that's why the `state narrower` concept somehow stuck there. I'm going to fix this ASAP. I found your mention of *this should be obvious* a bit surprising; I agree I could actually expand on that. Thanks for all the suggestions, that's exactly the feedback I expected to get here. EDIT: The changes are live.
&gt; Ouch! I think that hit the bullseye. I think his follow-up sentence is a much better characterization. &gt; It’s not that they don’t care, but they care about other things more. In short, we've got too many things to do and not enough manpower to do them. So what do we do? We focus on features. In general I'd rather have the capability to do a new thing in Haskell than have an older widely used but poorly documented thing get better documentation. I'd rather have Ed Kmett make us a new OpenGL library than document lens better (even though at times I have really wished lens was documented better). Will it always be this way? Probably not. But right now Haskell is small enough that we still need a LOT of useful functionality. Furthermore, when I wish for better lens documentation, what I'm really wishing for is something that answers the question that I have right now. Too often we run into that problem and we conflate it with poor documentation. I'm not saying that's the case every time, but I think it is somewhat common. In general it's very hard to anticipate what questions people will have and put the answers precisely where they want them. And very often the original authors of a library are not the best people to do this because they by definition understand the library very well and don't have a very good idea of what is unclear to beginners. If you take this as an axiom, then it becomes much clearer that asking for better documentation is equivalent to asking for more manpower--not an increased focus on documentation by the current library authors.
&gt; is there an editor that does give you a satisfactory haskell indentation experience? I don't know ... I haven't seriously tried other editors yet. Mainly because they don't seem to implement all the nice features from haskell-mode and cie (and you need to be quite a good editor to match the emacs feature set in general). I believe (and hope) emacs will be supersede one day but it might not me tomorrow (from my point of view of course) 
These 2 stackoverflow answers might give you an insight into what is going on here: http://stackoverflow.com/questions/13426417/how-do-i-re-write-a-haskell-function-of-two-argument-to-point-free-style http://stackoverflow.com/questions/12409572/pointfree-composition-with-multiple-variables
And 20 hours into learning your first programming language you were still probably struggling over for statements, pointers, or what that darned "public static void" thing was for. At that point your code didn't "look like production-level code" either. If your language was Java, you had no clue what abstract factories or singletons were. If your language was C, your code didn't look anything like the code you see in the Linux kernel. I like your post, and I think it highlights some things that we as a community should be aware of about how to go about teaching people. But mstksg has a good point. I don't think you can expect to be writing production level code after studying Haskell for a month. Just look at this proposed [learning curve](http://s3.amazonaws.com/viking_education/web_development/blog/coding_is_hard_confidence_competence.png) from a recent [post](http://www.vikingcodeschool.com/posts/why-learning-to-code-is-so-damn-hard) about learning to code. Right now I would guess you're between the cliff of confusion and the desert of despair. Actually I don't think the learning curves of individuals can be characterized that precisely. But I do think there is at least a kernel of truth there.
I don't know, it seems like we keep missing the point of what you're saying. I'll try an alternative response: Learning by examples doesn't necessarily result with misguided cut and pasting, not in my experience anyway. Sure it starts that way, but if there is desire to learn it will progress past that point. To add a bit philosophical opinion into the mix: Humans are pattern recognition machines. Show them a few examples and then introduce the pattern - it will be obvious. Show them the pattern directly and you're skipping the pattern recognition engine which activates that "Oh, I get it!" click. If you're careful enough choosing the examples to be different enough, what they have in common will be exactly the intuition of the abstraction 
For static charts/plots/graphs the library [Chart](http://hackage.haskell.org/package/Chart) is really nice. Otherwise diagrams+an FRP library is probably your best bet. Slightly outside of Haskell I have been playing with [Elm](http://elm-lang.org/) recently and am incredibly impressed with its ease of creating interactive applications. Plus since it compiles to Javascript the ease of deployment is miles ahead of Haskell.
I've seen both results; both understanding and misguided copy+paste+modify / monkey-see-monkey-do / cargo-cult programming. I do think the later is more common, when examples are used exclusively.
[Löb and Möb](https://github.com/quchen/articles/blob/master/loeb-moeb.md). &gt; mind-expanding Yup. &gt; elegant Definitely! &gt; simple I'll just be going now The mind-expandiness part: *the entire evaluation strategy for Excel in a single line.*
[this](https://peaker.github.io/lamdu/) comes to mind but I dont think it is really what you are looking for.
&gt; I don't think you can expect to be writing production level code after studying Haskell for a month. Me neither, but you make a good point.
Specifically for domains like crypto, speed is often less important than safety and correctness. The power of Haskell's type system can make it impossible to write incorrect or unsafe programs in a variety of ways, especially when compared to C or go. And of course its memory model prevents a variety of overflow attacks, but this a less unique selling point.
I didn't read this paper very carefully, but it seems like the course described is very similar to what you get by taking the [Structure and Interpretation of Classical Mechanics](http://mitpress.mit.edu/sites/default/files/titles/content/sicm/book.html) course, omitting all the symbolic algebra and doing the numerical parts in Haskell instead of Scheme.
I understand that design, but I think it then leads to the need for ev, beh, a non (,) product, and explicit combinators for things like swap. It's not enough to merely restrict the eliminators, you also need to ensure that the operations are explicit enough.
Interesting that the gloss example seems to be the simplest!
I still don't see it. I think I'll just have to remain stumped.
Whatever; they're the same thing if you have univalence.
If the OP could demonstrate that Haskell's type system would have caught a few bugs their colleagues produced, that would certainly be a great selling point. That said, I keep hearing that garbage collection makes timing attacks on crypto code too easy, but I know too little about that field to make any sort of qualified statement.
Once you've defined a visual model to write Haskell or aid in writing Haskell, the actual implementation is a matter of slogging through the implementation of the model and host of other features of an IDE.
This is why it would need to be curated.
It might be easier to write code that is vulnerable to timing attacks due to laziness, but it's also possible to design algorithms with strictness and branching properties that mitigate timing attacks. I'm not sure that GC specifically makes timing attacks too easy: even if one branch does allocate more than another, the behavior of GC in a complex program may be unguessable. (I'm not saying that your statement is *false*, I'm saying that I don't think it's conclusive either way and needs to be tested more rigorously.) Also to put it into perspective, far more exploits are reported on memory handling, NULL pointers, mishandling of inputs, and so on than on timing attacks. Which is to say that while timing attacks aren't unimportant, a language that can potentially eliminate whole classes of common vulnerability is certainly worth investigating. Also, Haskell has [CPSA](https://hackage.haskell.org/package/cpsa), which can be used to analyze potential side channel vulnerabilities. There are also use cases where such timing attacks are just a dangling vulnerability, e.g., offline key verification and other cases where timings aren't relevant or can't be recovered. As always, anyone implementing cryptography needs to understand their threat model and mitigation strategies. Haskell won't automatically be safer or more secure, but in the right hands I think it could *potentially* be both.
I recommend `gloss` too. There is a simple collection of examples in gloss-examples. One, 'boids' has an associated tutorial about 'flocking' algorithms http://syntacticsalt.com/2011/03/10/functional-flocks/ with some rather blurry videos to give you an idea. 
Thanks for the clarification.
Thank you, will do.
Free carnitas, on the other hand...
Perfect, thank you. If/when the time comes I can turn the whole article into a series of Friday afternoon team tech talks that should accomplish what I'm trying to.
Yes, very good points, whatever demo I come up with needs to be relevant. The context here is that there is a feature we may need to add to our current product to enable us to go after the enterprise market, and which is non-trivial to develop. If we decide to pursue this market (not yet a given), then we have three options for this: 1. Partner with a third party and integrate their service into our app. 2. Build it ourselves. 3. Acquire it. We're evaluating all three, but for #3 there is only one option we know of, a small startup of Haskellers that has built what we need in Haskell, but which hasn't been able to raise funding yet, so the acquisition of both their tech and team would be affordable. There are a number of other synergies that make this option attractive, but a big challenge will be the harmonious union of two distinct engineering cultures, in which each one is open to and learns from the other and the whole is able to effectively choose the right tool for any given job. This is what I'm trying to think through how to facilitate, as part of evaluating this option. Hence the reason for this post.
Thanks, agree in principle, but [business considerations may force an acceleration of the process](https://www.reddit.com/r/haskell/comments/2uuygb/can_your_programming_language_do_this_haskell/cocay8m). So I'm trying to find a preponderance of "hooks", so to speak, which will have a higher liklihood of making it click.
Except they are used in context to build the library its self usually, and always the ones built on top of it. Most haskell libraries are a DAG of functions, building on top of each other. Its exactly that context you get form the source.
Great to hear Haxl is having success internally. Is there any chance the internal work will make it out to the open source repo? or is it mostly work that wouldn't go into the current Haxl repo?
Maybe, but it isn't clear that GC is a problem. On the other hand, it's painfully clear that the other things I mentioned *are* a problem.
btw, my ghc 7.8.3 seems to interpret any identifier that starts with an underscore as a "type hole". even with -fno-defer-type-errors. I couldn't run the examples on control.lens.exception. I couldn't even rename it, for the same reason (no alpha equivalence??). so I had to write my own assertionError rather than _AssertionError. manually, using cast, likely less general than your ones. I can't see any compiler options that should cause this. have you seen this before?
+1 for Elm. It's about to come out with a new interface for Http and WebSockets, so you can use Haskell for the backend and Elm for a fronted, just serve your data by http 
Looking for `_AssertionError` probably failed because the identifier is `_AssertionFailed`. GHC 7.8 takes any identifier _that is not in scope_ and treats it like a hole. No extension is used for this as it changes nothing about what programs are accepted by the compiler, merely about error reporting. It is effectively the same as any other missing identifier error, except it gives you a ton of information about what that variables are that are in scope that could be used "to fill the hole". If you have an `_AssertionFailed` identifier in scope then looked for the right name it'd work just fine. All this did was take the already somewhat baroque lens type errors and make them even worse when folks typo. ;)
I have also been working on a visual representation of Haskell. I am focusing first on just generating the visuals from code (not making a full IDE). My reasoning for this is that more time is spent reading code than writing code [1]. The symbols can be made with [diagrams](http://projects.haskell.org/diagrams/), but I think laying out the nodes and routing the lines might require implementing something like [graphviz](http://www.graphviz.org/). The diagrams wiki does list [graph drawing as a possible project](https://wiki.haskell.org/Diagrams/Projects#Contrib_module_for_graph_drawing). The visual language is still in the design phase, but I am open to collaboration. [1] [Quantifying Program Comprehension with Interaction Data](http://www.inf.usi.ch/phd/minelli/downloads/Mine2014b/Mine2014b.pdf)
Strange that they don't follow the precedence of `$`, `=&lt;&lt;` and `&lt;$&gt;`.
GC sounds like a major problem to me.. Secret keys were already detected by listening to changes in the [background noise of the device](http://www.tau.ac.il/~tromer/acoustic/). GC pauses sound like they'd leak a lot more bits of information than that.
I'm really happy they don't, because the 3 operators you mentioned have different precedences, so cannot be chained. The chaining like unix pipes without parenthesis is a killer feature.
Because OP may want to... for whatever reason.
Will it finally allow circular websockets use? Right now you can't have the signal that you send back to the socket depend on the signal you're getting from the socket, because Elm doesn't let you have circular signal graphs. But of course transit time means that it's not a cycle in a problematic sense. Last time I tried using Elm's websockets I had to hack around it using Javascript (and who wants that?).
&gt; Sometimes multiple choices are just redundant: map and fmap, Monad.mapM and Traversable.mapM, List.foldl and Foldable.foldl, etc. Often, they provide substantively different ways to approach a problem. Too much choice is bad, but so is no choice at all. In this case they don't provide a different way to approach a problem, they're kinda redundant aliases. For all those pairs (map/fmap, Monad.mapM/Traversable.mapM, List.foldl/Foldable.foldl), the first item is the less capable one, i.e. once you have `fmap`, you can forget about `map`, as `fmap` operates on anything `map` can operate on. Personally I'm looking forward to Prelude where `map` is simply a synonym of the more powerful `fmap` (as well as having the Foldable/Traversable versions, which the FTP is going to fix)
Well, until we have more evidence, we should assume GC leaks your secret keys :-)
I could add this to `Control.Foldl`, too: count :: Eq a =&gt; a -&gt; Fold a Int Funnily enough, the `foldl` library has this for `ByteString`s and `Text`, but not for generic elements.
 foo x = x &amp; someField &amp; lift &gt;&gt;= useFieldInLiftedMonad get &gt;&gt;= foo &amp; someLens %~ someFunction &amp; furtherProcessing May require a bit of tweaking the operators, but that exposes the different kinds of lifting/effects happening at every stage.
&gt; In this case they don't provide a different way to approach a problem, they're kinda redundant aliases. That is what I said.
&gt; Ability to work within a team and help prioritize work. Ability to work individually and/or remotely. Does that mean that this is a remote job, or must the applicant be in or willing to relocate to Washington DC?
Luminal's main office is in the DC metro area, but remote is fine. I live in New Zealand!
Can't get much more remote than that, I'll grant ;)
While that's true, I don't think assuming univalence is always a good thing. I'm not sure I'm clear on the computational, and more specifically performance, impacts of assuming and applying univalence.
Awesome! You're doing better than I am; I'm still trying to get mine written.. :S
To be fair, I could be wrong. I think it'd be a pretty novel idea to pull it off, though. If you're ever in the space of AFRP I'd like to see what you can come up with there. Haha, the whole thing started by how I really wish n-ary AFRP was easier to encode! I think it's a great model and I'd love to be proven wrong about the difficulty!
For an expert, it's less useful, sure. But for a beginner who's never set foot in Haskell (or your library), looking at a list of type signatures is just frightening. A lot of people learn by means of examples ("concrete first, then abstract") so it's a good way to introduce beginners to new material. Of course there should always be *both*, because when I'm well-acquainted with a library, I'd rather not have to sift through a tutorial full of basic examples to find the information I need.
I haven't really dug in enough to understand or figure out where you're going with this, but at the surface it reminds me of operads. I'd in particular recommend picking up Spivak's [The Operad of Wiring Diagrams](http://arxiv.org/abs/1305.0297).
Pretty dated by now, but Conal Elliott gave an interesting talk that is somewhat related titled [Tangible Functional Programming](https://www.youtube.com/watch?v=faJ8N0giqzw). It's not an IDE, but the concept seems similar to what you posted with noflojs.
I guess what I'm saying is that they're "isomorphic" anyway. In math, we talk about *the* free monoid, so I feel just fine talking about *the* free monoid in Haskell -- even though there are technically other datatypes which are also free monoids -- especially given the prominent role of `[]` in Haskell. Anyway, univalence isn't actually a thing in Haskell, since types aren't values and can't predicate over values.
To me n-ary AFRP just looks like the API of reactive banana wrapped in an arrow, so I may well have completely missed the point.
I think i tried it with -l, -L , -i and -I but i'll double check tomorrow! Thank you!
OK, is there any way to verify if it's a JHC bug? Thank you for your input! 
There is an interesting duality here. You can use parallel arrows to model separate dataflows (product types) or pattern-matching cases (control flows/sum types) but not both at the same time - so you'd need "boxes" around sections of your diagram to decide which is which. Overall, the theory of string diagrams in category theory provides us with lots of nice extensions: in dataflow, you can represent first-class functions by using linked arrows, like this: &lt;---- ! A ! A -&gt; B --o-&gt; B Another extension is to add "boxes" around some operations to represent uses of functors and `fmap`ped functions. As it happens, one can extend this notation to cover both pointed functors and monads. (However, most functors and monads do not have a sensible notion of parallel flows, so some parts of the representation are slightly clunky. It's still interesting, though.)
Didn't someone write an Arrow quasiquoter similar to this idea? EDIT: They did. [Needle](http://scrambledeggsontoast.github.io/2014/09/28/needle-announce/).
And I wrote a version¹ called [hypodermic](http://www.reddit.com/r/haskell/comments/2hpzpu/announcing_needle_asciified_arrow_notation/ckvh09k?context=3). ¹ where "wrote a version" means I screwed around with ASCII box characters
&gt; univalence actually isn't a thing in Haskell, since you types aren't values and can't predicate over values Oh, sure, but I mean even in a larger context. E.g., Idris is dependently typed, but taking univalence as an axiom allows you to prove _|_ / makes the system inconsistent, IIRC. When you very much care about the performance of your programs in addition to the correctness, univalence may not be tenable. Contrariwise, I understand that when you start wanting type equality, particularly higher inductive types, univalence is the weakest axiom that gives you anything useful. So, I'm not sure (yet) that we need to bring univalence into out programming; I think knowing the monoid abstraction is a good thing for programmers. But, maybe I'm just lagging in my understanding. 2-3 years ago, I didn't understand how dependent types could even be a useful thing for real programs. I purchased the first edition of the HoTT book, but I'll admit that I really haven't been engaging with HoTT for a while.
We in the games and film industries use them often. Maya - and it's precursor PowerAnimator - have had [the Hypergraph](http://3.bp.blogspot.com/_aAP_s_DnGtg/TPvUkr29BMI/AAAAAAAAABU/sd4ZmX7kIog/s1600/toon_outline_hypergraph.gif) for 20 years now, and a lot of technical artists spend a lot of time in there. It's a bit more about objects and relationships between properties, but there's a definitely flow of data, and in that particular example you can even see expression nodes, which hold arbitrarily complex code. The Hypergraph can get [pretty hairy](https://circecharacterworks.files.wordpress.com/2011/11/hypergraph.png). Recently Maya has included a [Node Editor](http://around-the-corner.typepad.com/.a/6a0163057a21c8970d019aff53297e970b-pi), which pulls a few separate dialogs - including the Hypergraph - together to create a more modern feel. I think you can even create custom nodes that present an interface to hidden nodes underneath, so you can create the visual version of the Facade Pattern, which is especially helpful for artists who don't want to deal with the deeper technical stuff. Shake was a compositor that was all node based, too. Apple bought it soon after it was released, made it Apple-only (it started out as PC-only), then a few years later shut it down. It looked like [this](http://s3.amazonaws.com/pbblogassets/uploads/2013/02/Shake.jpg) to work in, and sometimes [like this](http://www.fxguide.com/wp-content/uploads/2010/10/misfits/tree.jpg?62f061) (that menora at the top is basically a fold operation, compositing in the same kind of thing over and over). Working with trees like this is why I feel a node-based editor really needs to be able to collapse a bunch of nodes into a single 'black box,' exposing to the outside only the things you intend [typically] to have tweaked. Build a graph, then collapse it to a box and name it. Then you can reuse it in other collapsible graphs, too.
Quickcheck and Parsec are pretty good "look how simple this can be" tools. If you're looking for something to show off more general features, something like [Monad Transformers Step by Step](http://www.cs.virginia.edu/~wh5a/personal/Transformers.pdf) is a pretty good example of how easy it is to mess with ASTs in Haskell.
Do you have a further set of development goals for this client?
Maybe if there were an update documentation link that led to a pull requestage it would be more obvious.
Wow, I've never checked off so many requirements on a job posting before. Also, this needs to be repeated and echoed: Two Haskell job postings in one day! - /u/tomejaguar
I'm advising Darryl on it (in type theory, two heads are better than one); I also helped in the initial work on the presupposition formalism that he uses. I'm not directly working on the code for LE, but I'm very excited to see it develop!
I've been meaning to give https://github.com/begriffs/haskell-vim-now a shot, but I've been hesitant because it seems a bit *too* opinionated. Looking through the source though will probably give you some ideas on a good setup. 
You might want to check this out: https://github.com/begriffs/haskell-vim-now I personally don't use it, because it's a bit too opinionated for my tastes, but looking through the source ought to give you some ideas on a nice setup.
Hey, I'm in the DC metro area. I'll send a resume your way in the next day or two.
I read this on my phone during lunch. As far as writing quality, I was able to follow through, more than loosely. I found it very interesting which kept me reading. By the end when I was seeing all the Typeclasses and such, one of the first things I thouht of was ConstraintKinds or TypeFamilies. That and one of the recent record posts where you can make types with QuasiQuoting and TemplateHaskell. 
After having read the article, I have to agree with you here. Ironically, I felt like context (pun intended) was missing often from some of the code snippets. In particular, the jump from the problem statement to the solution with a constraint that the reader know *why* the signature needs to look the way it does was a bit bewildering. I'd instead have expected the article to explain that, rather than assume that a concept I'm unfamiliar with should be intuitive to me. That said, I appreciate the effort, and I could see these posts being useful for when I need a quick fix to a problem whose constituent parts I already understand. In other words, while this article wasn't entirely useful to me as is, I don't think the OP should stop writing. 
You could try [Kronos Haskell](http://www.kronosnotebook.com/haskell). People have mentioned the Chart and Diagrams libraries, which Kronos includes and will render in the same document as your code.
&gt; In the first part, we learn a subset of the Haskell programming language. We are particularly interested in types, functions, and higher-order functions. We introduce a number of the types and functions provided by the standard Prelude, and we focus on how to write our own functions. In previous offerings of the course, we used the first five chapters of Hutton’s book [5]. We are less interested in type classes, but we need to be aware of them to understand the types of some functions, and to have any chance of understanding error messages. This reminds me of three things that I feel are connected: 1. [Racket](http://racket-lang.org/)'s support for [simpler, teaching-focused sublanguages](http://docs.racket-lang.org/htdp-langs/index.html) within the same Lisp system. 2. The [Helium](https://hackage.haskell.org/package/helium) Haskell compiler, designed for similar ends. 3. The long-standing debate on whether the Prelude should be monomorphic or polymorphic. (See some example recent discussions [here](http://www.reddit.com/r/haskell/comments/2hzqii/neil_mitchells_haskell_blog_why/) and [here](http://www.reddit.com/r/haskell/comments/2if0fu/on_concerns_about_haskells_prelude_favoring/).) I think that there may well be value in splitting the Prelude into *two* or more different modules/sublanguages along these lines.
Something like IBM's [VisualAge](http://en.wikipedia.org/wiki/IBM_VisualAge)
Sorry to say, but Hoogle 4 sucks in this aspect. Updating it took about 2 hours, required a mix of Windows .bat and Linux .sh scripts, failed regularly (often leaving the service down) and was just a huge pain. It also had sufficiently manual steps that it really did take me 2 hours sitting there. The number of nights I have 2 solid hours is now approximating zero :( Fortunately, I'm writing Hoogle v5, hosted at http://hoogle.haskell.org, which answers all your queries correctly. It's updated at 8pm GMT every night by cron, entirely automatically, and takes 3 minutes (literally, you can see the time it restarted at the bottom). I've got 2 more bullet points to address, then I'm going to pull the switch.
Just so it's clear, this is entirely my fault, and not the committees. The Haskell.org committee has done a great job helping me with Hoogle v5, for which I'm incredibly grateful. I do care the current one is out of date, but unfortunately caring does not provide coding time :( - that said, see my comment in this thread, it's all going to be awesome shortly :)
https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation
So I take it from your comment that not having a PhD degree is still ok? :D (Then again, /r/dizzeehaskell probably has a much larger chance of getting in, since he has/will get one shortly. Good luck to him! :D ) Edit: Wrong user name. D:
(Avoid) (success at all costs), not (avoid success) (at all costs). Success is fine. 
Wow, that looks almost exactly like Blender's "Node Editor". I wonder which came first and whether there's an original inspiration they're both imitating? Either way, I've never really used the Node Editor in Blender much (I'm no artist, just enjoy tinkering with stuff) but when I've poked at it to see how a more sophisticated .blend file worked it seemed pretty intuitive.
There's a bug in Hackage preventing the latest ghci-ng from being uploaded so you need to get the source from github and cabal install it instead. This is just a temporary problem and will eventually be fixed or ghci-ng will be merged with ghci. 
Glad to see some Haskell work in the DC metro area! Mostly soulless contractors around here ;) Do you go to any meetups?
How do people notice that there is a new blog post? I could not find a feed.
Avoid $ success at all costs
That's an extremely bad attitude to take to any job post. Especially back to a company that wasn't interested in you at the time. 
welcome to the job market. pretty normal to get a null response. 
Well, this is Haskell so it should translate as "avoid (purely) monetary success at all costs". We should achieve fame, victory, and influence successes as well!
Having never worked in HR, I don't know the load of applications they get in. However, I really do think it is "bad form" for companies not to at least *acknowlege receipt* of your application. That said, it seems to be rather normal, and calling a specific company out on it may discourage other companies from dealing with you.
The alternative to packages like `transformers-compat` is that everyone just freezes and avoids using new features from `transformers` for many many years. In the meantime I can support code that works on both ancient platforms and the newest. Keep in mind, users _can't_ upgrade the `transformers` package in a sane way. It not only ships with GHC as part of the platform, it is a boot library used by `ghc` itself. If they upgrade `transformers`, then they can't use things like `doctest` which depends on `ghc`, which in turn depends on `transformers`.
&gt; However, I really do think it is "bad form" for companies not to at least acknowlege receipt of your application. I agree with you. And I definitely think that it's good to tell the company that they are missing out on potential re-applicants by not even *acknowledging* that you had applied. Though, saying: &gt;Poor form. 0/10, would not apply again. Probably won't change any minds. In fact, as a recruiter, I'd probably just ignore this person entirely, because they're most likely just as toxic on a team or in person too.
I submit that this is a suboptimal way to build a team. Dividing responsibilities between developers and test people creates a two-tier system, where the devs and the test people each believe that the devs are on the upper tier. On top of that, it allows developers a get-out-of-jail-free card in the form of not giving so much thought to how their code could be structured to be testable. (Yes, it can be made to sort of work in practice, but I would never structure a team this way myself.)
&gt; The alternative to packages like transformers-compat is that everyone just freezes and avoids using new features from transformers for many many years. [False dichotomy](https://yourlogicalfallacyis.com/black-or-white). There's several options besides that. In particular, transformers-compat could be broken into 2 or 3 packages with tighter bounds on transformers, each having a different, but consistent API and don't violate the PVP. I don't know how much extra work that would be or if it is really worth it. It does seem like the current practice is causing a few problems that might justify the extra work. Conditionals like this certainly wouldn't always violate the spirit of the PVP if all instances exported from the package are consistent -- they always added instances removed from imported modules, or always removed instances added in imported modules. That's not what is happening here.
&gt;&gt; Poor form. 0/10, would not apply again. &gt; ignore this person entirely, because they're most likely just as toxic on a team or in person Really? Meme usage and exaggeration / hyperbole (arguably, in service to the meme) are toxic? I think it's an attempt at humor to defuse a harsh critique. I'd be more concerned about their idealism (harsh critique of a practice many of us accept) and lack of venue awareness (memes and humor aren't worth the karma on /r/haskell that they are on /r/funny) than any "toxicity". A few years under the grindstone will shave off that idealism, and some soft skills training will clean up the venue awareness. They might work in a long-term position. ;)
&gt; As I read the original post, the issue isn't with transfomers-compat but with the (unnamed) "several packages" that define instances of their type classes for ExceptT only when built against versions of transformers that define ExceptT `transformers-compat` also only [defines some instances](https://github.com/ekmett/transformers-compat/blob/master/0.3/Data/Functor/Classes.hs#L330) (not `ExceptT`, but `Lift`, `Backward`, and `Reverse`) only when built against versions of `transformers` that define those classes. So, it is an example of the more general problem.
Does Neil like playing compiler?
Blender premiered in 1995. PowerAnimator - which became Maya, and IIRC had the Hypergraph from the beginning - was used to create effects for Terminator 2, which was released in 1991. I think node-based stuff is much older than both. We've had things like flowcharts for a long time. Thinking in graphs of information flowing between nodes is a pretty old concept. Oh, and apparently [this](http://www.gametutor.com/live/tutorials/unity/creating-a-node-based-editor/) is now a thing. As I said - pretty common in my industry.
You understood it as I intended :-) 
&gt;A few years under the grindstone will shave off that idealism, and some soft skills training will clean up the venue awareness. They might work in a long-term position. ;) Perhaps. =) I may have read their response too harshly, and you may have read their response too lightly, but the main point I'm trying to get across is, nobody can make assumptions or judgements on a person's character with a more diplomatic, tactful approach.
To clarify here as well, I did not mean that transformers-compat is bad. It's the most practical solution to the problem. Rather it's the packages that **don't** use transformers-compat that cause issues for anyone who wants backwards compatibility.
I guess you can do case select v of Nothing -&gt; False; Just x -&gt; x == b &lt;=&gt; case select v of Nothing -&gt; Nothing == Just b; Just x -&gt; Just x == Just b &lt;=&gt; case select v of Nothing -&gt; select v == Just b; Just x -&gt; select v == Just b &lt;=&gt; select v == Just b 
Someone flagged this comment up and we forwarded it onto the folks responsible for jobs@. Regardless of how normal it is, a bit more courtesy is needed. 
Is Gnuplot actually installed?
I meant specifically the former, because &lt;interactive&gt;: pgnuplot: readProcessWithExitCode: does not exist (No such file or directory) sounds very much like it can't find the executable. I don't know where it expects to find it though ...
~~or~~ ~~case select v of Just b -&gt; opt1; Nothing -&gt; opt2~~ ~~if you need b in opt1~~
It still feels 'too clever' to do that first jump ... by which I mean how would I write a dumb program that just blindly runs though a set of rules in order to do the transformation. Maybe writing a piece of software like this is just non trivial and you have to perform a kind of search on the terms to see if anything comes up that you can simplify.
Probably my mistake. In my first post, I didn't say anything about any particular package, only about PVP violations. In Edward's reply he mentioned `transformers-compat` so I *went looking* for these types of conditional instances -&gt; PVP violations *in that package*. Sorry if I got things off-track.
Yes, I do :)
It would probably be nicest with an interface such as that provided by Repa or Accelerate.
The beautiful idea I'm reaching for here is that it may not need any new interface. It could accelerate existing existing code unchanged.
There are some flags you have to pass into gnuplot to make it stay, but it's been a while since I've used gnuplot so you'll have to [look at the docs](http://www.gnuplot.info/faq/faq.html#x1-820007.4) and play around a bit. You'll have to see if the library provides a means to do that as well.
It only defines instances when it is forced to supply the data types itself or when those instances were added in later versions of transformers.
&gt; Given that, you should be able to run arbitrary Haskell on the GPU, its just going to be way slower unless you are running the same function over many thousands of elements and there in benefiting from the parallelism. Not really. Branching on GPU's is _incredibly slow_. Like, _super, duper, extremely slow_. Compiling arbitrary programs to GPUs will result in a program that will not make adequate use of parallelism to compensate for the loss of compute speed and of branching.
Does any one came with good names or build CMS for the regular user this days?
Uh... "typically"? I think one of us is confused about what that word means. Also I'll just point out that *verification* of the *logic* implemented by these circuits is a pretty important part of their design using *Verilog*. In fact, Verilog was originally a modelling language, not a synthesis language.
That doesn't do the same thing: it shadows `b` instead of checking for equality. And it doesn't gain you anything anyway; `b` is *already* in scope in `opt1` in Neil's version.
What do you expect? Naming things is still an unsolved problem.
I think the full loas time could even be reduced a bit further by setting up a cookieless domain for assets. Currently no caching is implemented on the application layer, all dynamic content (html) is based on some db query/queries. Good point regarding the demo site. We plan to have one up amd running soon. We also want to put some screenshots on the site. Yet it is really not that hard to get a LambdaCms site up and running, if you have a modern Haskell env available, have a look at the tutorials in the Documentation section.
This is a fun idea, but it doesn't work out too well in practice. There are a few reasons for this. As already mentioned while hsail might be Turing complete some parts (like branching) are going to be slow. The other problem is memory. GPUs don't have random access to memory. Even the APUs don't have great support for random access. Either you're restricted to a subset of memory or access becomes a major bottle neck. GPUs are designed to be vector processors. They can process a lot of uniform sequential data very quickly. They are absolutely terrible at doing anything else. Unfortunately Haskell's data model isn't uniform or sequential. Lists are linked lists, primitive types are often boxed, and structures aren't inlined. This is going to completely break the GPU model. This isn't to say that it'd be impossible to make a good Haskell -&gt; Hsail compiler. It's just not going to be easy, and you'd probably be disappointed with the results. A better option might be a eDSL that can produce hsail code and translate Haskell data structures into something more hsail friendly. On another note I'm not sure what using hsail gains you over opencl. Is it just better support for AMD specific cards? 
Excellent, but could someone explaining me how to implement the operator (^+^) vector addition, Vec -&gt; Vec -&gt; Vec. How do I type it on GHCI ?? I am new on Haskell and FP.
I dont think the name is that bad, we picked it to mean something for programmers; then we chose a logo to make fun of it. :) CMSes for non-programmers are build a lot, many are not open source but online services. WordPress is a very popular open source CMS that attracts a wide variety of users, is there anything specific that you miss in WordPress? 
Last sentence is a pretty bold statement. All research I've seen says the exact opposite.
I've just put in on Hacker News: https://news.ycombinator.com/item?id=9013637 There is a small discussion there as well. (it was on the front page for a couple of hours)
it might be worth nothing that neither `getLine` nor `hGetLine handle` are functions :) 
I'd say it depends. I see two usecases for a CMS, that are better served by a different CMSes: 1. Some CMSes want to be easy for a non-programmer to set-up. This comes at a cost: usually they feature an [EAV-model](http://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model) in the db, tend to be slow, are usually written in a dynamic language, and are not very secure. For instance: WordPress, Drupal, Joomla. 2. Some CMSes are used programmers to (quickly) make websites for their clients. Their clients do not want to be able to tweak everything, just they want to be able to manage the content w/o involving the programmers. These clients are often more demanding of their CMS when in comes to performance and security. For instance: RefineryCMS, and to some extend Django.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Entity–attribute–value model**](https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value%20model): [](#sfw) --- &gt; &gt;__Entity–attribute–value model__ (__EAV__) is a [data model](https://en.wikipedia.org/wiki/Data_model) to describe entities where the number of attributes (properties, parameters) that can be used to describe them is potentially vast, but the number that will actually apply to a given entity is relatively modest. In mathematics, this model is known as a [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix). EAV is also known as __object–attribute–value model__, __vertical database model__ and __open schema__. &gt; --- ^Interesting: [^Diseases ^Database](https://en.wikipedia.org/wiki/Diseases_Database) ^| [^Magento](https://en.wikipedia.org/wiki/Magento) ^| [^Database ^model](https://en.wikipedia.org/wiki/Database_model) ^| [^Embedded ^database](https://en.wikipedia.org/wiki/Embedded_database) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+coe04w9) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+coe04w9)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Right, this is tantamount to "really really slow" for most programs, unless they're carefully designed to avoid this.
 addv :: Vec -&gt; Vec -&gt; Vec addv a b = zipWith (+) a b (it should be this, I did not try it)
No polymorphism?
Thanks, I actually really appreciate that. Normally I wouldn't give it a second thought, but this happened right after some really shitty behaviour by some other companies, and was kinda the straw that broke the camel's back.
Actually, I had a great attitude towards the job post. It was well written, boasted "A real member of our engineering team will read it" and lined up pretty well with my skillset! The focus of my bad attitude is the company itself, with whom I no longer wish to engage in the the future. Interested in me or not, If I spend 30 mins writing and editing a cover letter/resume, they can spend 30 seconds on a copy-paste "no thanks".
The majority has been application level stuff on top of the repo that won't see light outside of internal work. We've already pushed a bunch of improvements upstream in GHC that have stemmed from this work, though. As for things in the repo itself, there are plenty of pieces that haven't been high enough priority to build out just yet. Pull requests are always welcome.
All this equational reasoning is sweet and smart, but can you always assume that the definition (not just the type signature) of all symbols is known? I wonder how often we would not be able to get hints like that, even if we had such a tool.
With a normal language on a normal GPU I totally agree. On an HSA GPU with Haskell you don't think we could figure out how to GPU accelerate this? [ i * 999 | i &lt;- [1..1000000] ]
No. We took a stock `gdiff` and defined a mediator between `gdiff`'s world view and unrestricted Haskell, that can cope with monadic actions. Then the output of `patch` can be monadic, e.g. `IO` actions (or some other, interpretable into `IO`). The resulting technique is pretty much compositional, leaf diffs create atomic actions, and at nodes the actions get combined. All this is totally at the discretion of the programmer.
&gt; Does any one came with good names You mean names unlike Wordpress or TYPO3 or Twitter or Excel or pretty much any other software?
Sure, we already do know how, but doing a massively parallel operation like that on a linked list is going to be pretty slow on a GPU. Write it in Accelerate using its arrays instead: A.map (* 999) $ fromList (Z:.1000000) [1..1000000] HSA doesn't change the fact that GPUs are only really good at sequential data-access SIMD computation (and MIMD/branching can only be really emulated by massive loss of parallelism). Haskell is still a language with a mostly nonsequential data model (linked lists like your example) and plenty of branching.
Timing attacks. Different code paths exhibit different allocation behavior, so cause a statistically different number of GC's to occur. By measuring event times when communicating with a crypto implementation, you can thus leak which code paths were taken internally. If secret bits correlate with these code paths, you're leaking secret bits. Example timing attack for reference: Password verifier which takes more time if the mismatch of the password is after more characters: feed it different first characters until it takes slightly longer, and then you know the first character. Then repeat for the second character onwards. 
I've been successfully running the binary GHC distribution (the deb7 one) on F20 and F21 since its release.
Now Ubuntu needs to catch up 
Probably "bells-and-whistles", which was used in the previous sentence. It's actually a cute abbreviation of the phrase, I might have to remember that one.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Visual programming language**](https://en.wikipedia.org/wiki/Visual%20programming%20language): [](#sfw) --- &gt;In [computing](https://en.wikipedia.org/wiki/Computing), a __visual programming language__ (__VPL__) is any [programming language](https://en.wikipedia.org/wiki/Programming_language) that lets users create [programs](https://en.wikipedia.org/wiki/Computer_program) by manipulating program elements graphically rather than by specifying them textually. A VPL allows programming with visual expressions, spatial arrangements of text and graphic symbols, used either as elements of [syntax](https://en.wikipedia.org/wiki/Syntax) or [secondary notation](https://en.wikipedia.org/wiki/Secondary_notation). For example, many VPLs (known as *dataflow* or *diagrammatic programming*) are based on the idea of "boxes and arrows", where boxes or other screen objects are treated as entities, connected by arrows, lines or arcs which represent relations. &gt;==== &gt;[**Image**](https://i.imgur.com/iHb0zC0.png) [^(i)](https://commons.wikimedia.org/wiki/File:Scratch_2.0_Screen_Hello_World.png) - *An implementation of a "Hello, world!" program in the Scratch programming language* --- ^Interesting: [^Microsoft ^Robotics ^Developer ^Studio](https://en.wikipedia.org/wiki/Microsoft_Robotics_Developer_Studio) ^| [^Visual ^Logic](https://en.wikipedia.org/wiki/Visual_Logic) ^| [^Visual ^Basic](https://en.wikipedia.org/wiki/Visual_Basic) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+coe5p5x) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+coe5p5x)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I second that - it would be nice to know what it actually does. 
Whatever --- it's not helpful to people who are not deeply experienced.
What's lib3? In case you meant libc, % ldd /opt/ghc783/lib/ghc-7.8.3/bin/ghc | grep libc libc.so.6 =&gt; /lib/libc.so.6 (0xb2bed000) And gmp: % ldd /opt/ghc783/lib/ghc-7.8.3/bin/ghc | grep libgmp libgmp.so.10 =&gt; /lib/sse2/libgmp.so.10 (0xb7659000) Perhaps you tried the CentOS binary; that's a dinosaur for sure.
You could do something like case select v of Nothing -&gt; False; Just x -&gt; x == b &lt;=&gt; case Just b of Nothing -&gt; _; Just b -&gt; case select v of Nothing -&gt; False; Just x -&gt; x == b If you fill in the first `Nothing` case correctly, you get essentially the definition of `(==)` for `Maybe a`. (You might have to assume symmetry of `(==)`.)
A lot of code is needed to facilitste that an non-programmer can do "pretty much everything" by merely clicking on a web interface, this makes the CMS "heavy". As plugins need to be installable and configurable by clicking, the CMS needs to chage its behavior at runtime (this adds security risks). Since the content types are not known in advance (as plugins can add content types), the db schema is usually EAV (see my other post). Finally it is my experience that "pretty much everything" is not enough for serious websites, a programmer needs to do work customize it. These customizations are much harder to implement (and harder to maintain, keep up with upgrades of the underlying system) when built on top of a "heavy" CMS. A mouth full. I hope it does not sound like a rant.. :). I think they serve different usecases.
&gt; It usually ends up being premature to write back to a candidate that they've been rejected, as that's often times not the case. Sure, but it's nice at least to have "Thank you for applying for the position. We will be in touch if we decide to take things further."
If you are using a fully featured IDE, then yes, you can get all definitions of everything - I was using such an IDE when I did this reasoning (although the IDE only supports "go to definition", not "inline here"). However, I didn't even use that, I just know what the definitions are for all the symbols I was working with in this instance. In addition, sometimes you don't want to use the "real" definition of a symbol, but equivalent definitions with better properties for what you are working with.
I think that FAQ you've linked me to is more directed at the standalone gnuplot, not the gnuplot within hascal. I'm actually able to use gnuplot directly from my cmd window[ without an issue.](http://i.imgur.com/Tauy5Wk.pnghttp://i.imgur.com/Tauy5Wk.png) My problem lies more within using [gnuplot from within ghci](http://i.imgur.com/JJPoZWu.png)
Agreed. That's why I'm following up with the person who processes the resumes.
Okay, but no problem. I like learning how people address the same kinds of problems. I suppose there are ways to plot from ghci that it works fine for some. I like being able to create and destroy windows with the current data I'm working on, to investigate functions and data sets quickly. I really also would like to be able to zoom, pan and get values from graphs inside figures. To me, just looking at images written to disk feels very limiting, and it feels like it slows down my understanding of the problem I'm working on, and overall progress, by an order of magnitude. That said, I'm not sure how well emacs or IHaskell handle such things. I have been thinking about looking into the latter.
Not at all, that was informative!
Given that you start your question with "I'm familiar with Haskell", I will answer it as if you ask for a comparison between LambdaCms and other Haskell CMSes. The others that I know of are: [Clckwkrs](https://github.com/clckwrks/clckwrks) and [HsCMS](https://github.com/Tehnix/HsCMS). Then there are static content generators, like [Hakyll](https://github.com/jaspervdj/hakyll). I'll compare these systems to LambdaCms, based on what I know. If I miss something or state something wrong, please point it out. Let me start with that all these projects, IMHO, serve different usecases. * Hakyll is the most bare bones one. A pretty technical person is needed to setup and manage the content on a Hakyll-site. By providing programmer's-tooling (mostly command line tools and config files, used with Git), the project delivers quite a complete feature set in a relatively small code base. I'd say it is also the most mature CMS of the bunch (most stars on Github, and I guess the largest installed base). * HsCMS is a CMS that I expect to power the author's (Christian) [website](http://codetalk.io/#!/1). Beyond that I dont know how much it is used. It is complete [Yesod](http://yesodweb.com) application that allows content management. Being a complete application by itself it is not very extensible: one could fork the code base, but not create "extensions" that may be shared with other HsCMS users. Like LambdaCms, it is quite a young project. It needs a programmer to setup a site, but once set up an non-programmer could take over. * Clckwrks is built on top of Happstack. It is supported by [SeeReason Partners](http://seereason.com), who use it for the company's website. It aims to be CMS that can be setup by end users, and thus needs to be able to load plugins at runtime; [web-plugins](https://github.com/clckwrks/web-plugins) has been created by the Clckwkrs team to accomplish this. In the future Clckwrk may hopefully reach their envisioned level of feature completeness such that a non-programmer can start publishing online with it; currently it is not there yet. To summarize: * Hakyll is a static content generator, like Nanoc and Jekyll * HsCMS is a "private CMS" (tailored for a personal site) * Clckwrks is aiming to become a "heavy CMS" (see my other comment for what I mean by heavy), like WordPress/Drupal/Joomla/TYPO3. LambdaCms is set of Yesod subsites (currently just 2) a programmer can use to quickly create an "admin interface". You'll need to make your own content types, for which we encourage you to also make Yesod subsites (more specifically LambdaCms *extentions*); this is easy as we provide [a template](https://github.com/lambdacms/extension-boilerplate). These *extensions* may be shared among LambdaCms users, and we will try to create a curated bunch of official ones over time. When it comes to theming the content of a LambdaCms-site, you will have to simple write the routes/controllers/HTML/CSS/JS as if you are making a Yesod site. Just most of the models will already be there for you, as they are usually provided by the extensions. LambdaCms/Yesod/Haskell is comparable to RefineryCMS/Rails/Ruby. 
I think either solution mostly addresses the desire to see the output quickly. What neither offers are the features that everybody wants from an image preview: resizing, zooming, and panning, much less selecting values. Diagrams does support selection, so it should be do-able. The other ones seem like lower hanging fruit to me as it's just a matter of writing a little GLFW program. I'll get around to it at some point, but I'm with you in hoping somebody does it soon!
Missed this post earlier, but there's a [HaskellDC](http://www.meetup.com/Haskell-DC/) meetup group.
The thesis has version two of Visual Haskell: http://ptolemy.eecs.berkeley.edu/~johnr/papers/thesis.html
Firstly, here's a simple **workaround**: Overwrite `pgnuplot.exe` with `gnuplot.exe`. Or you can [make a link](https://technet.microsoft.com/en-us/library/cc753194.aspx). Now, `pgnuplot.exe` appears to be deprecated and has been removed in version 5 of gnuplot. As far as I know, `gnuplot.exe` seems to do everything `pgnuplot.exe` does so I'm not entirely sure what the distinction is (historical reasons?). I booted into Windows today to dig into this problem for a bit. Still haven't figured out everything yet but so far the problem seems to be: - ~~The Haskell library doesn't invoke `pgnuplot.exe` correctly: it needs to be invoked with the [`-persist` flag](http://www.gnuplot.info/faq/faq.html#x1-820007.4) in order for the window to stay.~~ - Having said that, that alone wasn't enough to solve the problem; I'm still investigating why. It works if I invoke it in Command Prompt (`pgnuplot -persist myplot.gp`) but it still fails in GHCI. *Update:* It seems that `pgnuplot` doesn't like it when you try to pipe in commands while simultaneously feeding in files as input arguments. I get this obscure error message in doing so: &gt; Can't find the gnuplot window After inspecting `src/win/pgnuplot.c`, it seems that when `pgnuplot.exe` launches `gnuplot.exe`, it expects a window to appear. However, due to the presence of filenames in the arguments, `gnuplot.exe` assumed it was batch-mode and thus did not show any windows. It's as if the author of gnuplot altered the behavior of `gnuplot.exe` some time ago but never updated `pgnuplot.exe` in the process, perhaps because it was slated to be removed anyway.
I recommend using [hvr's PPA](https://launchpad.net/~hvr/+archive/ubuntu/ghc) instead of waiting for Ubuntu.
Wow! Thanks for this progress. You definitely seem to know what you're talking about way more than I do. What exactly is the -persist flag doing? and how do I properly use it in ghci syntax? / is this a stupid question? Edit: Woah woah woah! The workaround you've proposed works! I'm able to successfully create plots with plotFunc from within ghci. Thanks a bunch for this help! :D
haskell + docker = hacker?
Not only do you need enumerability, but also decidable equality (in Haskell speak you need an instance of `Eq`). It is not true that all representations are like that, for instance `Int -&gt; Int` is not an instance of `Eq`.
That was once my dream, but I eventually decided *anything* consist is better than laying code out myself. On top of that, I have a lot better things to do with my time than work out where a space should go - especially when other people have already put work into that. Thus, these days I just use `chris-done` style and move on.
Nice stuff Luke, I'm still limping along in an `emacs` setup that doesn't really do anything - I've been waiting for a blog post to come along like this and motivate me to sort it out. Perfect! /u/changetip 1 coffee private
The Bitcoin tip for 1 coffee (6,575 bits/$1.50) has been collected by *LukeHoersten*. [ChangeTip info](https://www.changetip.com/tip-online/reddit) | [ChangeTip video](https://www.youtube.com/watch?v=_AnfKpypMNw) | /r/Bitcoin
&gt; the idea that you can either have easy for programmers or one-click friendly but not both sounds absurd to me. It seems everyone is agreeing that you can't have both of those *easily", so at least that much is not absurd. But if clckwrks is initially more focused on usability by developers, and provides only the groundwork for one-click as a starting point for future work, then that definitely presents a picture that is somewhat different than what /u/cies010 was describing. So that's interesting.
I have serious doubts that having multiple malloc implementations in the same process is a sensible thing to do.
How should we deal with C libraries that allocate memory themselves but don't provide a finaliser and ask the user to free memory using *free* like [libusb_get_pollfds](http://libusb.sourceforge.net/api-1.0/group__poll.html#ga54c27dcf8a95d2a3a03cfb7dd37eae63)?
No, you can't do that. You're effectively calling `dlsym(RTLD_NEXT,"free")` which might *not* be the same malloc that libusb was using.
Hmm, you are right. Probably the best way is to file a bug against the library.
If i may reply to #2: In the case where candidates have applied for a specific position, and the candidate is dropped out of the running for that position, it is not inappropriate nor immature to let the candidate know this, and that you may consider them for further positions. If in the future you pick that candidate out of the applicant pool for another position, they would surely be *more* receptive to the offer after having such a positive HR experience, than less. As /u/bss03 pointed out, it's probably naive to expect companies to do this by default. I greatly respect and am very appreciative that you're taking what amounts to poor humour (and venue choice) on my part and actually improving the process for others. Thank You. Good luck with the hiring process, I hope you find some truly kick-ass people to work with.
As far as I understand, valgrind *replaces* malloc with its own version. So, when you're executing a C program using valgrind, you'll still only have one (valgrinds) version of malloc.
I meant an actual wrapper library, e.g. gcc -o libwrapper_libusb.so -shared wrapper_libusb.c -lusb ghc ... -lwrapper_libusb 
I think it depends on the scope of the event. Stuff like "me and my two friends from FP501 are organizing a meetup in Anderson Hall room 206 on Friday! Free pizza!", then absolutely not. On the other hand, if it's something like the Going Native conferences for C++, then yes, I think people all over the world would be interested in something like that.
Just a nit: if by "has type classes" you mean "guarantees coherence", then Rust has type classes (or at least purports to), not implicits. (No HKT yet, though.)
I don't mind much (even if they don't concern me geographically), as it's only a few, but I guess it would become too noisy if all local Haskell-related meetups from all over the world decided to broadcast here. Is there some service where I can subscribe to Haskell events filtered by geographical criteria?
If you want one-click installs then you build against web-plugins-dynamic, which requires GHC. But if you want static builds, then you build against web-plugins, which does not. If you are compiled against the static version of web-plugins, then any of the functions which attempt to load or unload modules will always fail. There is no fundamental reason why acid-state can not scale beyond a single node. It mostly does not exist because nobody has written a successful enough acid-state based Haskell web application to require more than one node. Of course, many people are unwilling to adopt a database that does not yet implement a feature they will probably never need, but might! (Seriously -- how many wordpress sites are using more than one database server). There is a movement in the Chicago area to make two major additions to acid-state this starting this year. 1) add an S3 logging backend - meaning that writes are as durable as S3 2) Add replication and sharding. acid-state does have some primitive scalability at the moment with the Remote API. That allows applications to connect to an acid-state instance on a remote server. This would allow you to add multiple front end machines if that is where your bottleneck is. 
I've been awaiting the fruits of Hoppinger's labour for some time now. Very cool to see this released, congrats guys! (En ajeto!)
note: the proposals on the associated ghc ticket is NOT to replace the current provided malloc, but to provide an additional `alignedMalloc` (or the like) that uses posix_memalign. [edit: actually, either provide both as separate things, or have the current malloc interfaces call the aligned one on supported platforms]. Yuras does raise some valid concerns, but there are definitely use cases where its eg valuable to guarantee that memory has been allocated in a page aligned fashion! edit edit: reading a bit further, the POSIX standard does specify that `free` is the function used to deallocate `posix_memalign` allocated memory. so theres no issue of correctness induced by making that change on supported platforms http://pubs.opengroup.org/onlinepubs/009695399/functions/posix_memalign.html
Yes, valgrind replaces the system malloc.
On Windows, DLLs have their own heap, so that memory which was allocated with `malloc` by code in one DLL has to be deallocated with `free` by the same DLL. In the PCRE example, this means deallocating with `pcre_free`, not `free`. Whether this means that C-`malloc` and Haskell-`malloc` are, in fact, different, depends on implementation details.
I'm sorry, how is that an error? I thought c_sin was a way to rename the C-function in the haskell file. 
My post is unclear because of typos, if nothing else. I meant that &gt; Why: context reduction, no instance for: &gt;Jhc.Class.Real.Fractional Jhc.Type.C.CDouble is a very different error from &gt; jhc: user error (C code did not compile.) and that /u/slacket only provided a diagnosis for the former.
Fair enough. The reason I lumped rust in with the rest was that I'd recalled being tagged in https://github.com/rust-lang/rfcs/issues/493#issuecomment-67034420 where the discussion seemed to be rather firmly heading the other way at the time.
Yeah (sorry about that... I figured if people had already summoned Bob Harper, then what the heck, and maybe you might want to chime in), that was a rather unrepresentative discussion though: there was basically no chance at that point that the core team would've agreed to considering any changes of that magnitude, which I guess is why people didn't feel a pressing need to argue the other side.
&gt; "only send list comprehension to the GPU if there are zero branches and it has more than 100k elements" that would still be a win. &gt; GPU now uses the exact same page tables/cache etc as the CPU PCI buses relatively slow, significant advantages of GPUs are faster memory, wider buses and manually controlled caches. Just google "matrix multiplication CUDA" and compare results for a naive approach with a smart shared cache usage. It's not like this is a dead end, but you should understand that "take conventional code, [partially] run it on GPU, enjoy 100x performance boost" isn't a new one. A lot of smart people put significant effort to get it going and still employing vector processors requires careful design and a lot of benchmarking. You can find a GPU interface for almost every language and still very few projects actually using them.
which IDE?
- why is your whole page in bold face? - wait, screw that. why is ever line of your page in `&lt;b&gt;`tags? - why is your code magenta? - why do you use yellow for headings, text and GHCi? - why does your page layout and colors look like a five year ate too much M&amp;Ms and puked? Overall, if I didn't knew Haskell, after reading your article I wouldn't even try it. I would think that it is a language only used by lunatics. On the one hand, you explain your clients recursion, on the other hand, you tell them to "Copy, Paste", and "File, Save As" the code. In the same sentence, you mention "working directory". And your article is _riddled_ with typos. You clearly have no actual idea what your audience is. Apparently someone who uses the console for everything, but can't handle a damn editor. Your article can't be read without additional resources. Heck, it's not even that article, same goes for the QuickCheck article. Also, why do you start posting those links __now?__ The article is old and doesn't provide significant input. It seems like you want to place your bunch of half-hearted programming introductions better in Google's page rank. But that's all. __In this post, you have been introduced to some nuisances of your page.__
Yep, it replaces malloc/free, new/delete, new[]/delete[] and any additional functions defined by the --alloc-fns argument if I recall correctly.
Thanks for the patience! Please give it a go and let us know what you think of it. cheers.
An in-house work one. Nothing publicly available on it, I'm afraid. 
It's not a great idea, but unfortunately it's the reality. Every linked unit loaded in a process that returns any heap allocations should also export a mechanism for deallocations. If any so/dll/executable frees memory allocated by any other so/dll/executable then they have invoked very, _very_ undefined behaviour.
This also happened to me. Sent in an application, received a response asking for clarifying details. Replied to request, no response. Followed up a bit later, no response. It didn't necessarily sour me on the whole company as I knew it was pretty common for people to drop the ball on responding to (well anything, but in particular) potential hires. In my mind it was more like "their loss" than "what jerks," as I know how hard it is to find quality developers, and I think it was literally the only resume I sent out in the last 3 years.
It also runs all of the instructions in a virtual machine.
Agreed! This is uncalled-for meanspiritedness. We want people to be better programmers, but we should want to help people better teachers too, not heap scorn on them because they are still working on it...
Then you give me hope. =)
Static Site come with 2 major disadvantages. 1. Terrible update from mobile story 2. A significant amount of user training is required in git, markdown etc. Dynamic sites come with a reality of large API surface areas that have constant security vulnerabilities. A Dynamic Site engine in Haskell offers the possibility of the best of both worlds so I see a real need for these projects in a production ready state. 
Meanwhile, you can use [petersen's copr repo](https://copr.fedoraproject.org/coprs/petersen/ghc-7.8.4/) to get ghc-7.8.4 on Fedora 20 &amp; 21.
I think the conclusion was that it was the GC, see the comments here https://github.com/JuliaLang/julia/pull/8699#issuecomment-66639357, and it should be faster in Julia 0.4. ?
You are going to have a tremendously difficult time trying to dive into writing a non-trivial project in Haskell without some kind of foundational comprehension. I've run a lot of people through learning Haskell, what you're about to do is something I've not seen work well. What I'm suggesting will take less time than the thrashing that results from attempting to write a program in Haskell without knowing Haskell. I say this, because I spent *five years* thrashing with Haskell before I got anywhere because like you I thought I knew better.
A huge amount of research has been done on various kinds of parallel computing models for Haskell, so you may want to look around more. One thing that might be interesting is [Reduceron](http://www.cs.york.ac.uk/fp/reduceron/). While most of the actual implementation work for Reduceron has been on traditional FGPUs, the underlying idea is to take advantage of massively parallel CPUs under conditions of memory starvation. The execution model (as far as I understand it) is to implement direct graph reduction in parallel, rather than compiling down to imperative assembler code that does the graph reduction. Perhaps a port of Reduceron (or more likely, this [fork](https://github.com/tommythorn/Reduceron)) to HSA would be along the lines of what you are looking for.
I've been running GHC 7.8.3 on Fedora for ages via [Petersen-GHC](https://copr.fedoraproject.org/coprs/petersen/ghc-7.8/)
Stop me if you've heard this one. There are two hard problems in computer science: Cache invalidation, naming things, and off-by-one errors.
thanks for such a late follow-up!
I think LambdaCms is (while still in alpha) very well positioned for scaling (out). It uses a relational db, which can be served from a separate node. Application nodes may be added when needed, and are really fast (2-10ms/req). And as it is "just a Yesod app" all your typical web-app caching-fu just works (if you ever need caching).
&gt; LambdaCMS: functioning, needs more extensions to be usable Maybe we (I'm one of the authors of LambdaCms) need to make more clear that LambdaCms does not create default templates for your content types. You have to write the front-end code yourself. And example of this is [lambdacms.org-base](https://github.com/lambdacms/lambdacms.org-base). This repo contains the code for the `lambdacms.org` site. Github shows that ~90% is CSS code, and only ~8% is Haskell code! From these numbers we understand that this "base" app nicely includes all admin interface functionality and content types from the *extensions*, while merely adding some theming over it.
I will make a simple extension of LambdaCMS for demo purposes and include it.
The normal haskell user probably is familiar with git, markdown, etc. Nevertheless, there is sometimes a need for dynamic sites. 
There is the Haskforce plugin for intelliJ https://github.com/carymrobbins/intellij-haskforce
The binary names are admittedly confusing, I mostly just didn't want them to become extremely verbose. The difference between `centos65` and `deb7` is that one is a GLIBC 2.12/GMP 4.x build, and the other is a GLIBC2.13/GMP 5.x build respectively. I've been meaning to change this nomenclature because the end result of this is that about 99.9% of users need the `deb7` tarball - even on Fedora, which sports GMP 5 and GLIBC 2.13+ vs downstream RHEL (a la CentOS) - and few need the `centos65` one. We may not even ship CentOS 6.5 binaries with 7.10 anymore, so I'll probably change the naming convention here too.
I think it is nice to have 'global' Haskell community announcements. There are people who might even travel to other country just to see something interesting... ;) Here in London we usually have a few guests from continental Europe. Vienna is a quite nice place: I could imagine people going there for a visit then staying for a meetup before flying home. Making it global usually helps to source speakers from other countries. And as a community we are still in `the early days of the new era` so showing that we are active is a good thing.
"stop me if you've heard something" is the best addition to that joke I've seen 
Awesome! I'll check it out. Thank you!
Thanks, I'll be kept busy reading up further on these. You mention that you'd argue row-polymorphism and lenses to be orthogonal. I'm not sure what you mean by that. Are you saying that they are independent of each other? In which case I agree. `'this_notation` would be syntactic sugar for a lens made from more primitive record operations and is not an attempt to unify lenses and row-polymorphism in some weird voodoo magic interpretation. If you are saying they are difficult to use together or that they are incompatible, then I don't see why that would be the case. 
You might be interested in [my total library](https://hackage.haskell.org/package/total), which codifies the trick for doing exhaustive pattern matching using `Prism`s. Also, there is no need for any new header file and notification system. Just exporting the list of lenses and prisms suffices as a header file, and the notifications when they change would just be type errors.
When I began HsCMS, the focus, with regards to haskell and web development, lied mostly in frameworks, and not finished solutions. I had fallen in love with Yesod, so strived to either find a CMS using that or create my own basic one. You're absolutely right that HsCMS is primarily for my own usage. After getting the basic done (it was also a project to learn haskell/yesod), I started looking for ways to generalise it though. Mostly centered around layouts though, since most of the functionality would either be done by myself, or not absolutely necessary. I much better like your approach of using subsites, since HsCMS doesn't really lend itself much to extending it. Hopefully I'll get around to contributing to it :)
Yes, agreed. *Emphatically.*
In my opinion, cms/blog engine should be designed with completely separate front end and back end. Visitors should be presented with static site, thus solving security and scalability concerns. Backend web interface should allow editing and organizing the content, and be accessible only to the author(s)/maintainer(s). Having web-based editing interface will make it usable by non-programmers, or from a mobile device.
Maybe because it's using foldl? Try running it with product' = foldr (*) 1
This is a well-known issue. You can read more about it under the section "Accumulating thunks" in this blog post http://www.well-typed.com/blog/90/
I just realized that product of those numbers can get really big. Maybe amount of memory being used is used to store the result? I switch product with sum but it did not really helped to memory usage. So I am still not sure if calculation takes O(1) space but result is huge or calculation takes O(n) space.
As a brief summary, `product` doesn't evaluate the product until you actually need it (here, when ghci wants to print the output); instead, it just gives something along these lines: (((((1 * 1) * 2) * 3) * ..) * 1000000) Each stage of the calculation is a thunk - an unevaluated computation. By calling `product [1..1000000]` in ghci, first a large chain of thunks is generated, then ghci tries to print the result, causing the chain to be reduced eventually down to the final solution. There's an alternative function, `Data.List.foldl'`, which causes the function to be evaluated at every step rather than forming thunks. For large lists, instead of `product`, I'd recommend `foldl' (*) 1`, as in /u/tomejaguar's other comment. On a side note, full props to you, OP, for looking up the definition!
There's a new edition of that, by the way. (Which presumably uses a more up-to-date software package.) I don't think it's free to read online, though.
How goes the effort to transition to foldl' ?
You are right.
`clckwrks` should be in nixpkgs soon. I have most of the work done already, I just need to submit it. I've been using nix for all my development recently so this is a natural outcome :) In the long run, I am hoping to base the clckwrks dynamic plugin capability around nix. This could potentially help solve a number of issues. For example, if a non-programmer is just installing standard plugins from the 'clckwrks store', then nix can just download precompile binaries -- they will not actually need GHC installed on the server[1]. That not only saves time, but also makes it harder for an attacker to inject code -- which would already be pretty darn hard. Additionally, if you are running multiple clckwrks instances on a machine, upgrading a plugin for one instance won't break anything else. To the non-programmer, this integration would be completely invisible -- no knowledge of nix or Haskell required. Nix currently lacks windows compatibility -- but I guess I don't care.. It also means that the initial install is potentially a pretty big download. But it could be worth it. The non-programmer focus for now has been pretty low level. For example, if a non-programmer is going to be running the clckwrks, then nothing they would need to change should be required to be set via command-line flags. Accordingly most (and in theory, all) clckwrks flags are only things that would be set by the packaging system. And, if not explicitly set, they have reasonable defaults. That allows packaging systems like Debian to specify where things like jquery should be found. Additionally, the plugin system is designed so that new plugins can be loaded into the server with out having to modify any code. However, that feature is only at proof of concept stage at the moment -- you currently have to compile some stuff by hand. Perhaps I should change the timeline to focus more on the non-programmer experience now though. One reason I avoided that in the past is that 'cabal sandboxes' were coming, but had not arrived -- and I thought that might be the basis of the plugin system. But now that cabal sandboxes are here and now that I have a bunch of nix experience, there is nothing in the pipeline that I am waiting for. - jeremy [1] depending on how dynamic code loading works. I believe the hackish hs-plugins method does not require GHC, but using the GHC-API does.
&gt; I just realized that product of those numbers can get really big. Maybe amount of memory being used is used to store the result Actually, regarding the product calculation you are right. `log(n!) = O(n log n)`, by Sterling's formula or similar, so we can never hope for the product calculation to be performed in small space!
I really liked this whole series - its a very well written example of how to go from some starter concepts through more complexity a step at a time. I liked that it didn't dive straight into the more complex layers (and typeclasses and monadic polymorphism) too early, since it let me get a grasp on what was happening before it got too abstract. Thanks for writing this!
Do you mean open sourced applications? Because from my experience proprietary applications are bad examples. Probably that is just me doing it wrong though :) I'd say my applications can be used to teach people how *not* to develop in Haskell
&gt; Nix currently lacks windows compatibility For what it's worth, as a Windows-based developer I've found Nix extremely easy to use: I just spun up a NixOS virtual machine. The download was reasonably small, installation didn't take long, and it works great with VirtualBox and PuTTY.
Absolutely. They're a huge improvement versus completely nondeterministic builds.
I'm thinking of things like building menus, defining routes, adding images, videos, etc. I've looked into a few static site generators in the past, but never found one that I would dare hand over to a non-technical client.
Sounds interesting, is this project publicly available so we could take a look?
I've wanted something like hawk! from a glance at the examples and cabal file, it's based on lists, not some stream like pipes right?
Everyone seems to be happy with deterministic builds and Nix, but to be honest, I don't like it. Personally, I'd prefer semantic versioning where "semantic" means a type-checker driven package manager which actually parses your code and verifies the resulting AST against the smv change (or makes the smv increment automatically). It can even generate a list of differences between the two versions so you have a clear picture of what changed. And this exists: https://github.com/elm-lang/elm-package
&gt; is this related to how some database libraries statically prevent connections from escaping some transaction type Yes. It's also related to how `ST` prevents it's references from leaking. It's a way of using the type system to statically eliminate those types of bugs, but there's some trade-offs around delayed-allocation and early/prompt-deallocation/finalization. There are also some tricky bits around overlapping but not nested regions. With indexed monads, (I think) you can get all the advantages with no (or fewer) disadvantages.
I can see how this would solve problems with "bad" dependencies on other "modules", but I'm not quite sure how this might solve the problem with depending on specific versions of the toolchain.
that's the joke! Good ole Yesod!
I really liked this. I recently started to ask myself how one could put effects inside a Mealy machine. This series is very handy!
I just `cabal install feed` with GHC 7.10rc2 and it worked ... what's broken there?
The article, as I recall (it's down at the moment), is about the *toolchain*, i.e. `Cabal`, more than the usual versioning-is-hard topic. The problem, again as I recall, was that a particular permutation of the build environment was failing in a way that people hadn't pushed on. Deterministic builds, combined with a relatively exhaustive build server (and devoted distribution maintainers!), should help detect and work with that situation, at least in the specific conditions that the deterministic system uses. Incidentally, I like that the [nixpkgs common Haskell configuration](https://github.com/NixOS/nixpkgs/blob/master/pkgs/development/haskell-modules/configuration-common.nix) includes links to bug reports, many submitted by Peter Simons, one of the nixpkgs maintainers.
We looked at your work before starting LambdaCms, and I must say it is really neat. Anyone who needs a basic blog/site can use it as a starting point and have a much simpler solution (mostly subsite-free Yesod app) then LambdaCms can ever be. But we were aiming for something like RefineryCMS (Rails), so decided to start from scratch. Feel free to join in, currently there are a lot of rough edges to LambdaCms. I think once you start a site with LambdaCms, you will easily make some contributions along the way. Otherwise there's always the `TODO.md` :)
No you wouldn't. If GHC could handle multiple versions, each module could have a single fixed version requirement for each dependency. In any particular scope, the dependency set is then strictly deterministic.
&gt; Visitors should be presented with static site, thus solving security and scalability concerns. So the static site is basically a cache then? This means every dynamic element should be move into JavaScript (think ping-backs, comments, recent news/activity, ads, user specific content), right? I think I'd rather have --per element-- the liberty of building stuff "per request" or going down the caching route. I'd feel wing-clipped without that freedom in any CMS project of decent size.
I can't wait to watch the code size grow.
Came here with the same thought, but then found [http://www.yesodweb.com](http://www.yesodweb.com) serves the same response.
Ok, I found a way around the easter egg :) https://github.com/yesodweb/yesodweb.com-content/blob/master/blog/2015/02/brittle-haskell-toolchain.md Enjoy!
Does hawk call out to the sandbox to do its interpreting of commands, or is there some way to squeeze all of Haskell into an executable?
This may be naive, but seems like a type returned from a function should be ok if it matches the type in the caller. If not, then no. As long as mismatched types never meet then libraries could use them internally. 
Part of being an algebraic data type means constructors are disjoint and injective. Nothing never equals Just x and Just x = Just y if and only if x =y.
&gt; runhaskell exec -- hawk Do you mean `ghc -e`? $ ghc -e [1..4] [1,2,3,4]
I'm very interested in having a discussion of the relative merits of Nix and Docker. I am aware that there are some people in the Haskell community who are very happy with Nix, but I'm not convinced that it has more traction than other solutions. The advantage of Docker as I see it is that it has a very large following in general *outside* of the Haskell community, plus seems to be quickly getting a following inside the Haskell community. So to me, "move with the community" is not as clear-cut as you imply. All that aside, the main benefit I see from Docker is that it doesn't require any modification to your workflow. If you're familiar with Ubuntu, then just use Ubuntu inside Docker. If you're familiar with Fedora, you can use it instead. And this could simply be my lack of experience with Nix, but I feel more comfortable being able to validate a self-contained filesystem than a Nix hash. Every time I've tried using Nix in the past, it's felt like I'd need to change more of how I work than I wanted to, whereas Docker was an incredibly easy tool to adopt for both building and deploying code. (That's not to say there aren't problems with Docker, and I'd be happy to discuss those too, but overall it felt much closer to the mark.) __EDIT__ I didn't notice your comments on Docker's "weirdness" at first. Frankly, I don't find any of that weird, I find it straightforward. I very much like the idea of keeping my processes in their own network stack and explicitly binding ports, and having a contained file system that is isolated from other things I'm running on the same host. The privileges story could be improved, however. I'm not convinced that Docker is the best implementation of the idea possible.
Gah, that's annoying/embarrassing. I'm looking into what happened.
Are `product` and `sum` candidates for inclusion in [this FBUT](https://github.com/quchen/articles/blob/master/fbut.md)? I'd make a PR if it were thought appropriate. Just a little section that mentions they use _O(n)_ space, and can be reimplemented with `foldl'` if necessary.
How many previous versions is are fully bw-compatible? I think usually only the previous patch-level versions (as there no features should have been added, only bugs fixed). So this means very narrow version constraints are handled by default in this system -- do I understand this correctly? Which means it gets harder to find a build strategy that works (given that all dependencies are having these narrow constraints) 
Hmmm, that's weird. I wonder if it's because of the music towards the end of the presentation. Was the other video we posted available to you (Pi-Forall: How to use and implement a dependently-typed language)? 
Docker has the wider adoption outside the Haskell community, yes. So, clearly the solution is to what I do at my work: install `Nix` in your debian-based Docker container ;) Anyway, thanks for your comments!
When someone says deterministic build in this context, do they mean the same thing as the Tor and Debian projects mean, where given the same source code, compiler and switches, etc you'll get the same byte-for-byte binary every time? Or do they mean something else?
Nope, that should look like math :-) Do you perhaps have JavaScript turned off? Or maybe something is blocking the MathJax URL? It's hard to say from here unfortunately!
That's good news, but I was unaware of any such plans. Do you have a link with more details?
https://github.com/haskell/cabal/issues/948
True, but I consider that a culture problem, not a tooling problem. Enabled by the tooling, certainly -- version considerations **can** be pawned off on the API user, so naturally they are -- but you won't find such a casual attitude toward breaking changes in any other language.
If we only could do away with backwards incompatibility altogether, so that any later version of anything is a drop-in replacement for any earlier version...
Why don't you like deterministic builds? I am not trolling, I am really curious about in which concrete use cases it does not work for you. Also, elm's package manager mechanically checks the semantic version, yes, but do you believe that in practice there are so many accidental violations of semantic versioning that this has a large impact? (Semantic Versioning ~ Package Versioning Policy)
But then cartazio says &gt; We CAN do bug fix releases as hackage trustees. But if you want that, please open a ticket on the trustees issue tracker So I guess that's the process: file a ticket on the trustees issue tracker. I think this would be my preferred way of fixing these issues (apart from just having a responsive maintainer). It's quick, and everyone in the community benefits.
are you using wine just to try windows-compatibility? I don't think this is viable - if you really want to make sure it works on windows you should use a Windows-VM. That would remove all the wine-hacking too.
I was using a Windows VM for testing, but I wanted a way to be able to quickly compile to both platforms without using 30 GB. With Wine, you can compile to both with a single shell script.
Docker by itself isn't a solution to any problem, it's a tool we've used in some customer-specific solutions which has worked out really nicely. We're working on polishing it so it would be more generally usable, at which point we'll be able to share it with the community. What Docker provides is an easy way to distributed a fully contained development environment. We're able to package together GHC, cabal-install, Haskell libraries, system libraries, executables, etc, in a reproducible manner that an entire team is able to take advantage of. I can tell you that, in the places we've deployed this kind of solution, it's been a huge improvement. It's interesting that you see no Docker adoption in the Haskell community, I've seen quite a few posts on this subreddit about Docker.
No, I meant the work of figuring out what the Haskell in your string means, then running it. Is that relying on Haskell being installed? Would the compiled binary work without Haskell being installed?
[**@podmostom**](https://twitter.com/podmostom/) &gt; [2015-02-10 06:17:15 UTC](https://twitter.com/podmostom/status/565031724250566656) &gt; @TailCallingDev it's nothing. &gt; Watch online — http://memorici.de/posts/universe/2015-02-03-read-articles.html#robotsOnHaskell ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
It is the two 5 second clips of "Highway to the Danger Zone" that causes the GEMA violation. If you re-uploaded a version that muted those two segments the video would play just fine in Germany. Alternately there should be some way to claim fair-use.
I am surprised but /u/hagda is right, it builds with version 7.10.0.20150123. How is that possible if it requires time&lt;1.5 and I only have 1.5.0.1 shipped with ghc, which is also the one installed in the sandbox? Edit: nevermind, caught it install `time-1.1.2.4` into the sandbox. But the issue remains for non-sandboxed installations, because it would conflict with the existing version, right?
If you would like to do a write-up about using indexed monads to make code using the FFI more memory safe I would be interested in reading it.
Well, if it was sufficiently easy to change ownership, you could just take the package ownership right back. Consider for example, Debian, which probably has more package maintainers than hackage does. There is a strong ownership of Debian packages; one maintainer or team per package. But any developer can upload anyone else's package too. They can even take over maintenance of a package without consulting its current maintainer. That works socially for a few reasons -- If someone hijacks a package, you can easily revert the change. And note the connotation of the word "hijack" -- the onus is on the person who uses this power to upload any package in all of Debian to ensure they're clearly acting in good faith and not maliciously. There's strong social pressure, backed by sanctions if necessary. (Or consider a wiki, which has many of the same properties.) Hackage's current model strikes me as much less flexible and less likely to generate useful social structures than Debian or a wiki's model. (One problem with Debian's approach is that all prospective maintainers need to go through a process to be vetted. If I could apply for a hackage account under a pseudonym today and then abuse that account tomorrow to upload malicious versions of every package, that would not be acceptable.)
I'm working on [Sweetroll](https://github.com/myfreeweb/sweetroll), the CMS that powers [my website](https://unrelenting.technology). Uses Git for storage, pandoc for markup, [micropub](https://indiewebcamp.com/micropub) for posting. So, it's not a general-purpose CMS, it's specifically aimed at personal websites. But I guess someone here might be interested :)
There is this little game https://github.com/ocharles/netwire-classics/blob/master/asteroids/Asteroids.hs that is a little bit outdated, uses some advanced concepts like arrows and FRP and is very difficult to understand for a beginner like me. If someone could make a tutorial explaining the code, it would be great. It is only 500 LOC. I have stumbled at everything trying to follow the code; from installing the dependencies to understanding arrow's recursive bindings. If I had already understood it, I would make a tutorial of it. 
Once I saw what you were doing, I expected this to end in total failure, so congratulations are definitely in order! Thanks for sharing how things went.
Hackage had this model: anyone could upload any package. But unlike debian, all users will immediately get that package (once they `cabal update`). Since you can even do compile time IO using template haskell, many people (rightly, IMO) thought this wasn't the right state of things. However, hackage is a community resource, so while you own your package, you do not own its place on hackage, and we should try to have a system where broken things are fixed as quickly as possible without inconveniencing active maintainers.
How does ruby's and clojure's package managers handle the case where two of your dependencies require different versions of another package? I know npm ignores this completely and hopes for the best, which will push any errors into the runtime instead.
Just nodding. Docker deeply ties into Linux specific features and thus presents a clean abstraction. Nix is more fine gained and more deterministic, but it is a really leaky abstractions. They could really benefit from some cross pollination.
Google NYC had open-plan but it was less obnoxious than most. I had a window at my back. If I hadn't worked for a psychopath (with a 5-year history of using phony performance issues to tease out his reports' health issues, then fucking with them) I could have survived that environment for the long term. I don't like open-plan, but have come to accept it as the trend of the time. It'll be 20 years before the first generation to live entirely in open-plan offices starts having, um, cardiac manifestations in large numbers. That said, there's a right way to do it and a wrong way and Google NYC was one of the few environments that was at least, for some people, close to the right way. Noise is one thing. Visibility from behind is atrocious, and in some cases, I think constitutes explicit age/health-status discrimination. In 2015, most companies use open-plan because it's cheap and stylish, with no ill intent... but when that trend got started two decades ago, age discrimination was definitely part of the reasoning. 
There are a few (very few) environments where open plan is the right layout. A trading floor is one. Sometimes, you need a message (like, "our quotefeeds are 5 seconds stale", because if you try to do arb with stale data you lose your shirt) to get out quickly and seconds matter. However, that's a big part of why traders make so much money: the stress of the job gets to you, and by 40 even if you call yourself a "trader", you're basically a manager because no one can take that much second-by-second responsibility after 15 years. Most software should not be done in an open-plan office.
Your desired semantics is not my desired semantics so that's futile.
Thank you so much, You've cleared all the doubts I had.
The links were formatted a little weird in the web reader. Here they are again: * **The GHC 7.10 Prelude** https://ghc.haskell.org/trac/ghc/wiki/Prelude710 * **The 7.10 Prelude should remain list based** https://ghc.haskell.org/trac/ghc/wiki/Prelude710/List * **The 7.10 Prelude should be generalized** https://ghc.haskell.org/trac/ghc/wiki/Prelude710/FTP
Here's my response: I think the FTP strikes a good balance between modernizing the Prelude without breaking existing code. The fact that the current Prelude privileges lists is ugly compared to other modern languages that make using the correct data structures easy. Having a privileged data structure means that it is overused and lists are used because they are convenient even when they should not be. It's quick to point out how easy that is to overcome by hiding the Prelude functions or importing qualified. But it still creates an extra burden for choosing the right data structure and this burden is on everyone, not just those caring the most. This leads to overusing lists in the whole ecosystem and anyone who is building software with Haskell suffers from it. There are some good arguments in the Plan List page, but mostly addressing them would require breaking more code than Plan FTP will (or has, since a lot of the adjustment has already been done). Perhaps they can be done later.
No, AMP is the applicative monad stuff. Both FTP and BBP are the Foldable/Traversable generalisations.
Ok, then the comment was even more wrong in referring to "FTP (and BBP)" since they're the same thing :-)
I have to say I don't even like `Foldable` and `Traversable`. They are certainly far from canonical. I think I'd prefer people just to write specialised `Fold`s and `Traversal`s for their types, perhaps more than one for any given type.
As a novice, can I ask why we couldn't adopt a generalization that works for Text and ByteString later? Is it because people get tired of change? As in: "We can't change the prelude again, we just changed it a few years ago!" Or is it because wide adoption of FTP would make some of those other generalizations you mentioned more difficult to adopt?
Comic Sans is the correct font for official Haskell announcements.
Something like [`MonoTraversable`](http://haddocks.fpcomplete.com/fp/7.8/20140916-162/mono-traversable/Data-MonoTraversable.html) maybe?
* **The Survey** https://goo.gl/forms/XP1W2JdfpX
It turns out that such generalization attempts don't really work all that well, leak information into the types about all sorts of your intermediate results, and are quite hard to reason about. https://ghc.haskell.org/trac/ghc/wiki/Prelude710/FTP#monomorphic `mono-traversable` is great for what it covers, but it gives up a lot to cover what it does.
Is there any *sane* reason for `product` (and `sum`) to use `foldl` instead of `foldl'`?
And when you need to change a type with `fmap` or `traverse`? fmapDefault f = runIdentity . traverse (Identity . f) would no longer be capable of defining `Functor`.
Too soon.
Have you had a chance to look at [Halcyon](https://halcyon.sh/) yet? It’s easier to get started with than Nix, and more light-weight than Docker.
Burn those bridges!
I wrote this back on the blog post itself, but I'll say it here too: it's on my list of things to look at next, I've just been inundated for the past few weeks.
Pulling Traversable and Foldable into the Prelude sounds like a terrific idea. I hate explaining to new engineers that the defaults are bad for production code. That said, I see no reason for urgency. If the GHC team can make a better change by holding off until 7.12, then that sounds fine to me.
What do you mean "far from canonical"? As far as abstractions go I feel like they're pretty time-tested by this point...
Is there _actual_ evidence for that? I hear it stated all the time, but in my experience spinning people at IMVU, people immediately understand the concept of type classes, but struggle more with why they have to use fmap instead of map, etc. 
Personally, I wouldn't mind the change because I've got to the point where I have a decent enough understanding of typeclasses and the rest of Haskell so it doesn't make too much of a difference to me. I think the education side, which you pointed out, is more important. I worry that some people will become too frustrated by Haskell to plough through the learning curve, even with mentors and good documentation (both of which are lacking). Having someone there to patiently explain things to you is a lot of help, but it is not available at all times to all people. Even if it is, some people will still decide that it's not worth the time they are putting in. &gt;a more positive, welcoming, and encouraging community I really hope this happens. There's certainly a long way to go though :(
Looking forward to your comments. While Halcyon isn’t optimised for Stackage yet, I expect specifying a Stackage snapshot with [`HALCYON_CABAL_REPO`](https://halcyon.sh/reference/#halcyon_cabal_repo) should just work. When you have some time, we should talk about Stackage LTS.
I would say 90% of the really important class of semantic changes, fixes to subtle semantic bugs, falls in the category of semantic changes without type changes though. 
That's neat! I have enough fun getting things working under real Windows, so I'm impressed you managed it under Wine :-). I find it funny that in my head I had always counted the seemingly large number of libraries that make up the Gtk+ stack as a point against it, whereas Qt is released in one big piece. However, it looks like appearances were deceptive from at least one angle! I've never used it but Qt ships with a tool called [windeployqt](http://doc.qt.io/qt-5/windows-deployment.html) which might help you to get all the needed libraries together for distribution. As you hint at, the next step to reduce sizes further would probably be to recompile Qt with some features disabled. For example, I believe it's possible to disable the ICU dependency altogether with some impact to the finer points of i18n. Also, if you didn't care about native widget styles for Qt Quick Controls then the dependency on Qt5Widgets.dll can be removed with a little tweaking. I have it on my personal TODO list to add a Cabal flag to HsQML for that (N.B. I'm the author). Qt Quick does require a 3D accelerated back-end of some kind, but it can be either native OpenGL or via D3D with ANGLE. It used to be a compile-time choice, but I think Qt 5.4 can dynamically pick which back-end to use at runtime. I've always stuck to OpenGL pass-through with VirtualBox but it does have a WDDM driver.
[You can do this](http://www.reddit.com/r/haskell/comments/1o7lpm/proposal_for_applicative_donotation/ccpi6t6), though it's a bit awkward and can generate inefficient code.
I've found it helps a ton while exploring new abstractions when the documentation lays out examples of how the complicated/abstract type signature simplifies to concrete types or abstractions I'm already familiar with. Perhaps something that could help make this change easier for newcomers to follow would be to have ghci spit out these simplified type signatures on :i or the like. This might help people, for instance, see how `(.)` coincides with `fmap` for the `(-&gt;) r` functor. And similarly, it might help people who are new to Foldable to see how the generalized fold functions map to the `foldr` and `foldl` (when specialized to `[a]`) they're already familiar with.
Bugfixes only require a bump to the minor version, so it will do the right thing in that case. 
 &gt;If the GHC team can make a better change by holding off until 7.12, then that sounds fine to me. that's a big "if" though... or are there concrete ideas on the table? 
Of course it wouldn't, `omap` is strictly less powerful than `fmap`. But you still can define `omap` with `otraverse`. Whether it's possible to roll the `f` and `o` versions into one in the type system (present or in general) is another question. This use of `Element` type families also smells like it could use some more specific form of restrained polymorphism, to be able to more precisely say things such as "`Vector ASCII` can be converted to `Vector ISO8859_1` in place, but to turn it into `Vector UTF8` we'd need to create a new one, better use a different function for that, but we won't complain if you use the latter function to do the former". But that's all pie in the sky, and (at least with this example) way more systems language than Haskell will ever be.
Sadly, it appears links can't be stickied, only self posts.
That matches my experience helping beginners on Stack Overflow, as well. The current situation is the worst of both worlds, because `Foldable` and `Traversable` exist in `base` but so do the monomorphic list functions for no clear reason. We're not simplifying anything, and adding the additional hurdle of people wondering what the difference is and why we have seemingly-redundant functions.
Will `Data.Traversable.for` find its way into the Prelude under the FTP/BBP? I couldn't find any mention of this in any of the proposal documents. More generally, I think it would be a good idea for the Prelude to export the applicative counterpart of any monadic combinator it already contains, if such a counterpart exists. In fact, in the long run, we might want to remove the duplicative monadic versions from the Prelude and relegate them to other modules in `base`. We might not be able to eliminate them entirely because of efficiency concerns in some special cases, but there's no reason to confuse beginning users with these details. Also, in a truly ideal world, we would have a naming convention where the monadic version's name is the applicative name with "M" tacked on to the end. So (`for`, `forM`), (`traverse`, `traverseM`), etc. This would require renaming `mapM` to `traverseM`, `sequence` to `sequenceM`, and `sequenceA` to `sequence`, which might not be feasible. Well, one can wish.
Note that in the current Foldable/Traversable proposal, Data.List does not export the list-specialized versions, it exports the generalized versions.
I don't think it's a problem with Foldable so much as it's a problem with using typeclasses to overload (what should be) non-methods for efficiency concerns.
Ah I guess once this rolls down too far we can add a self post and sticky it to the top instead!
The version we discussed when condensing the roadmap down to two different plans was to delay implementation of FTP for a release, not hold off indefinitely. Note: that it is referring to the "7.10" prelude, not the Prelude for all time. That said, the argumentation on the "Plan List" page makes a much broader case and is mix of "drop or neuter FTP" and "delay a release". The spectrum of potential outcomes is quite broad. Make of that what you will. By way of contrast, the "Plan Foldable" approach defines a single concrete plan with a working implementation, measurable trade-offs and impact.
It is a consequence of a combination of a free theorem, the monoid homomorphism laws, and the `Monoid` laws, so it is _almost_ vacuously true, but it is still a useful reasoning tool.
Just like with the AMP, the FTP doesn't try to "do all the things" and fix all the irregularities in our code. In an even more perfect world, there wouldn't be monadic versions of the `Applicative` combinators here at all. Alas, in the [absence of a fix](https://ghc.haskell.org/trac/ghc/wiki/Prelude710/FTP#traversable) for the need for `mapM`, we're not able to eliminate `mapM` and `sequence` from the class. If such a fix were forthcoming, or we as a community were to decide that a more concise design outweighed such concerns, we could eventually deprecate the redefinition of `mapM`, `sequence`, `sequenceA` in the class, and transition to where we eventually could let `sequence` and `mapM` work with mere `Applicative` effects. However, even that would require a fix for a currently intractable problem, and multiple releases to deprecate all the things needed to get even that far. And that wouldn't get you your chosen regular naming convention, but it would let you shed a lot of the suffixes that are floating around. We've already gone the other way on many decisions, generalizing many `fooM` combinators to work with mere `Applicative` effects where possible. There is no point served in giving them too-large a constraint. As a rule we're trying to evolve things in a manner that doesn't break existing code, though.
I would prefer a more minimal class. I would also like to not introduce rampant performance regressions in code that previously used the Prelude. I would also like to not incur an asymptotic performance regression in code that previously used Foldable. We have many many competing concerns at play.
&gt; but I think so. turns out that my use of those confuses the compiler ... `dtw-0.9.2` has instances for the three standard vectors.
I get what you are saying, but I'm skeptical of this idea of a typeclass with no laws of its own. I'd like to hear a real defense of that idea other than "it's convenient for some ad hoc overloading". Here's a typeclass I just made up: class W00t f where w00t :: (Int -&gt; Int) -&gt; f Int -&gt; f Int `W00t` has no laws of its own, but trust me, it's super useful, see: class W00t f =&gt; Functor f w00t = fmap fmap :: (a -&gt; b) -&gt; f a -&gt; f b See, `W00t` has a very strong law in conjuction with `Functor`, namely that `w00t = fmap`. Look at all that reasoning power! And since there are a bunch of places where I currently just need the `w00t` operation rather than `fmap`, it would be a shame to have to take a `Functor` rather than a `W00t`. How is `Foldable` materially different than `W00t`? _EDIT: This is a total tangent, and I'm not seriously suggesting removing `Foldable` or changing the proposal of the OP. Just discussing this for fun._
That should work if you take care to use new names for similar functions with identical types but new semantics (as you should anyway in a public API).
Yes, I understand. I think FTP is a very reasonable tradeoff given these constraints. Appreciate all the work y'all have been doing. :)
Hmm, found this on the wiki: &gt; There are two clear paths for how to evolve Data.List from here. &gt; 1.) [...] &gt; 2.) Concoct some form of {-# WEAK #-} pragma or enable users to export type restricted versions of another combinator and then apply this pragma to these members of Data.List. This could revert those combinators to monomorphic form, but requires a more controversial language extension that has some potentially thorny implementation issues to work through, and we've elected not to presume they can be resolved. That seems to be referring to what I'm talking about above. Is there anywhere I can read more on this?
I like the proposal (if implementable of course) but I'd apply the feature differently. I think the `Prelude` should export the general functions, and only `Data.List` should constrain them. 
Actually I was going to write the next blog post about that. Haskell is good enough in manipulating values, so we don't need this typeclasses most of the time. The gist shows that I'm not alone in that opinion. 
Well I specified a "law" for Foldable, but its not one that you can observe directly within Haskell. "A fold over a Foldable type touches every single `a` in the type exactly once." This is necessarily how it works when it interacts with `Traversable`. In the absence of a `Traversable` instance you can break it, and it is a genuine law. I just don't know a good way to uniformly state it in the language of Haskell equations, because we don't have a way of saying "get every `a` in this type" outside of a traversable instance :-)
So I was playing around with this, and it looks like you can get ghci to do this right now. :set -XTypeFamilies :t fmap :: (Functor f, f ~ []) =&gt; (a -&gt; b) -&gt; f a -&gt; f b &gt; :: (a -&gt; b) -&gt; [a] -&gt; [b] :t fmap :: (Functor f, f ~ ((-&gt;) r)) =&gt; (a -&gt; b) -&gt; f a -&gt; f b &gt; :: (a -&gt; b) -&gt; (r -&gt; a) -&gt; r -&gt; b So it's verbose and you have to specify the full type of the expression you'd like to simplify, but it's something that can be built on. 
1990... is it a reference to the monomorphic catastrophe?
The previous concrete plan, before folks started offering up language solutions, had been to follow #1. It requires nothing we don't know how to do and the long term state requires no buy in from new Haskell implementors regarding effectively mandatory language extensions. At this point in time nobody has crafted a fully coherent plan around #2. It would require gathering consensus that such a Haskell Report affecting language extension was a good idea for the language as a whole. This would definitely require an active Haskell Prime committee to hammer out. The main point of the item in the wiki is that we can still have room for discussion on this particular item even after adopting FTP.
What is the alternative? Using monomorphic versions with different names of what is semantically the same function just to regain the performance you lost by throwing them out of the typeclass?
Well the userParser and blogPostParser are simply what you would be putting into fromJSON Why go to all the trouble yet reinvent the same machinery? When i want to quickly read json without much ceremony i use aeson-lens: res &lt;- parseJsonBody :: Handler (Result Value) case res of Success x -&gt; do let chartId = maybe 0 (decryptInt k) $ x ^? key "chartId" . _String origPage = fromMaybe False $ x ^? key "footer_orig_page" . _Bool locN = fromMaybe False $ x ^? key "footer_loc_name" . _Bool indexName = fromMaybe False $ x ^? key "footer_index_name" . _Bool pageNo = fromMaybe False $ x ^? key "footer_page_number" . _Bool zeroes = fromMaybe False $ x ^? key "footer_leading_zeroes" . _Bool pagePrefix = fromMaybe "Page" $ x ^? key "footer_page_prefix" . _String 
From what I read in the list discussion the current (vague) plans are to get there eventually after some deprecations.
There remains room for discussion about how to evolve `Data.List` post 7.10. 1.) We could leave them generalized indefinitely in a sort of awkward intermediate state. 2.) We could deprecate the re-exports of the generalized members and ultimately remove them entirely. This has the consequence that it leaves there no good place for such monomorphic combinator to live. 3a.) We could try to derive a language extension that would let folks re-export a combinator with a more restricted signature, and have the combinators in `Data.List` use that means to restrict the `Prelude` combinators down to list signatures. This opens an interesting can of worms about how we should deal with multiple such restrictions, and requires work on the Haskell Prime language standard itself, as it would mean that typechecking the Haskell report would start to require a language extension. 3b.) Another approach would be to add a `QUALIFIED` pragma, where a definition only comes into scope if you import the module qualified or the combinator explicitly. This would address the same kind of scenario as 3a and it has similar Report-affecting concerns. 3c.) Another approach would be to allow us to flag exports as `WEAK`, where a `WEAK` export loses to another export if they both export the same name, and then write the monomorphic combinators in Data.List and export them weakly. As with the other 3x options, it also requires a lot of buy-in in that we're asking for a language extension to be required to compile what falls within the Haskell Report. It also introduces the same sort of ugly "where did this name come from" concerns that we get with extension methods in C#. 4.) Folks could advocate that the minority use of `Data.List` should trump the majority usecase. I'm actually personally fairly neutral on the direction we go here, though, I confess I don't particularly like 3c, and I think we should eventually stop practicing 1, and that I don't like the transition cost of 4. I have a mild preference for option #2 simply because it is a path I can see a clear roadmap to accomplish, requires no new language extensions of every new Haskell implementation, and has an easily described end-state. At that point in time it becomes a question of whether folks value having those monomorphic combinators enough that they are willing to fight for a language change to make transitioning to them viable or make everyone else pay a large transition cost. See: https://ghc.haskell.org/trac/ghc/wiki/Prelude710/FTP#exports 
For example because of this https://www.haskell.org/pipermail/ghc-devs/2014-October/006787.html The whole thread was triggered by statistics package depending on aeson
There are some very good reasons to [avoid type classes](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html)—at least, sometimes. A few quick benefits: - Your code is more explicit about what's happening (although it is also longer.) - You can easily have multiple implementations of a typeclass for a single type without having to resort to `newtype` anywhere. - You can combine these in richer ways that would be difficult without the aid of elaborate typeclass extensions^1. - You can write 'orphan instances' without fear of problems. In general, orphan instances are inadvisable because there's no way of explicitly importing or suppressing the import of a typeclass, or no way of selecting "which instance" of a typeclass to use. On the other hand, it's easy to suppress export or import of a function, and easy to choose "which function" to use. 1: Sometimes, manual dictionary implementations can implement things that would be _literally impossible_ to express with typeclasses, but that isn't likely to show up in an example with `ToJSON` and `FromJSON`. The example of this in the above blog post is class Iso a b where fw :: a -&gt; b bw :: b -&gt; a instance (Iso a b, Iso b c) =&gt; Iso a c where fw = fw . fw bw = bw . bw which cannot be resolved by GHC, regardless of which extensions you enable. The value-level equivalent, on the other hand, is easily written and has no such problems: data Iso a b = Iso { fw :: a -&gt; b, bw :: b -&gt; a } combine :: Iso a b -&gt; Iso b c -&gt; Iso a c combine ab bc = Iso { fw = fw bc . fw ab , bw = bw ab . bw bc } 
Let me ask the converse to get a better handle on this: what is it about Text and ByteString that prevents the use of FTP? Is it because of Unicode, ie. that a string is actually a sequence of code points, and not a sequence of characters like most people want to treat it?
Can you give a small example of what your input and output would look like?
No.
Hmm, maybe I've been misunderstood. I mean bitemyapp teaches loads of people Haskell and he claims it's easier with functions generalised to Foldable and Traversable.
I mean that there are all sorts of orders you can fold or traverse the elements of a datastructure. Why should only one of these be promoted above all the others to the level of the typeclass implementation? Without specifying that the fold or traversal has to be compatible with some sort of concatenation operation I just don't see the use of the typeclass above and beyond a collection of functions. As /u/pchiusano says, it seems more for ad-hoc overloading that genuine generic programming.
The issue is that Text and ByteString only allow you to get out Char and Word8's respectively. Let us ignore Foldable/Traversable for a moment, and just consider something simpler: `Functor`. class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b We all know this class. Here the class is only talking about 'f'. The a's and b's are left to float alone. This class can be defined entirely within the confines of Haskell 98. It can't handle `fmap` for `Text` and `ByteString`, either. So folks define monomorphic `map` functions for those cases and bury it in the module for their data type. Of course, now, you can't change out the contents of a ByteString for tuples or Maybes or any other data type other than Char, now. But this restriction is specific to your `map` combinator and the type of contents it allows you to use. Now try to define a class that generalizes over all of these cases. You could write class Functorish s a | s -&gt; a where fmapish :: (a -&gt; a) -&gt; s -&gt; s instance Functorish Text Char instance Functorish [a] a instance Functorish ByteString Word8 ... But now fmapish isn't as powerful as fmap, it can't change the types involved. It is also actually quite a bit easier to screw up writing, because parametricity isn't ensuring that every 'a' in the structure gets touched and converted to a 'b'. In fact, `fmapish id = id` and `fmapish f . fmapish g = fmapish (f . g)` don't even convey that property. It used to hold via a free theorem and a consequence of a law, but now it is yet another law that you have to figure out how to awkwardly state. Once you are done with all of that you now have an `fmap`-like thing that can't even do all the things we could do with `fmap`! So we roll up our sleeves and we generalize. class Functorish s t a b | s -&gt; a, t -&gt; b, s b -&gt; t, t a -&gt; s where fmapish :: (a -&gt; b) -&gt; s -&gt; t and we ask users to supply us only instances that obey nice structural rules that kind of follow the lens laws. But now we get all sorts of type inference woes. So we rip off our sleeves and flex. In the end, in lens we have a 2-typeclass, 3-instance solution that finally solves all those inference issues for _just_ the case of isomorphisms, which is quite similar to this one. It isn't pretty. We haven't bothered to write such a solution for Each. Why? Because it requires users to write a bunch of instances to solve a comparatively rare inference problem. But along the way, even if we have the idealized form of this solution we get problems. We leak all the details of what types you chose in the interim! Once your "`Functor`" can care about what types 'a' and 'b' are, then you have to care about all of the types we use. This means that tons of previously canonical constructions become open for reinterpretation. You can't just pick the obvious definition and know all the others are equivalent any more. They cease to be! Now, I stuck with a monomorphic `Functor` here. The same issues arise when you talk about monomorphic versions of `Foldable` and `Traversable`. An uneasy compromise that handles about 80% of the cases is to use `Foldable`/`Traversable` for most things, and to depend on something like `mono-traversable` for the remaining completely monomorphic containers. You can up that percentage a bit, by using `lens` rather than `mono-traversable`, because we have more traversals that can support type changing assignments up to whatever the limits of the containers are, but we still wind up leaking a lot of information about intermediate types, and the whole approach is predicated on the fact that we can almost always fuse multiple traversals into one single traversal to avoid needing to pick several different 'a's and 'b's within the same combinator. You get the same kinds of issues when folks offer up restricted monads. You can implement `liftM3` with several different signatures none of which imply the other depending on what intermediate types I run through. Almost all of the laws relating Applicative and Monad start to break down, laws like liftM3 f x y z = f &lt;$&gt; x &lt;*&gt; y &lt;*&gt; z cease to hold. You get completely different types on either side.
We have have `DeriveFoldable` and `DeriveTraversable` extensions in GHC based on the notion that we can just traverse left to right by convention recursively through whatever data type we want. It gives a `foldMap` that is properly compatible with `traverse`. If you need a more exotic instance you can write it by hand. We can do the same things for deriving `Data`. The derived `Data` instance isn't perfect for _every_ case, especially when you want to ensure some form of encapsulation, but it handles the vast majority of them. The power of `traverse` is that we can know that it is parametric in the last argument. So once you know you have `Traversable` you gain access to lots of repeated uses of a particular `Traversal` at different types. No matter what intermediate state you need to store in there you can know it can hold it. That parametricity carries many benefits. e.g. while the `Traversable` laws and the `Monad` laws don't mention each other, they relate through parametricity and the fact that both imply a canonical definition of `fmap`. This enables libraries like `bound` to work. If the only thing you know about a type 'f' that you consume and reproduce by the end of your combinator is that it is `Traversable` you are given no way to change the shape of what is given. This lets you drive things like the `ad` package, which flush and refill your data type with AD variables, and use `Traversable` constraints over the passing raw lists to ensure that this is sane and to line up the sizes of structures. The low price of this functionality is we have to pick an order. If you don't like that order we have tools like `Reverse` and `Backwards` to help you pick another one.
That may well be it, although it's a bit of an unsatisfactory reason. If anything, such a name likely only confuses the notion of "return" in Haskell even more by making it appear like an imperative return.
I'd be curious to see if such a proposal could get through the planning stages and into a released compiler. It is the sort of thing that invites a lot of feuding between different prescriptive camp of Haskell users. The "default superclass" proposals run into many of the same problems. Should they enable us to split up class hierarchies retroactively without breaking consumer code? The horns of this dilemma seem to hoist each such proposal: * If so then one camp gets upset that the methods are being defined in the wrong place, and that instances are being manufactured out of nowhere, etc. * If not they they aren't useful for retroactively factoring out the equivalent of the Applicative/Monad-Proposal.
&gt; "but don't worry about that right now." &gt; [...] &gt;beginners do not need to understand how every function in the Prelude is implemented. I really, _really_ dislike this pedagogic approach. It's one reason [Java is such a bad teaching language](http://wayback.archive.org/web/20131015065640/http://lfborjas.com/2011/03/26/teaching.html). You've got to [explain all kinds of seemingly magical constructs like classes, objects, packages, etc.](http://programmers.stackexchange.com/questions/135925/how-do-i-explain-a-hello-world-program-for-beginners) before beginners feel truly comfortable. Personally, I think the best way to teach anything is to involve as little unexplained "magic" as possible. Ideally, none; my philosophy is that beginners should (in principle) be able to understand _everything_ they type with the concepts they've been given. Having mysterious typeclasses like Foldable or Traversable in function signatures makes that difficult, and I think it's better to start them with plain lists and introduce the generalizations once they have a firm grasp of the basics. Many people learning Haskell have never seen a functional language before! So, I'm in favor of FTP, but is there a reason we can't have a separate "intro" or "simplified" module newbies can use before they migrate to the full language? I'd hate to see the Burning Bridges Proposal become something that pulls up the ladder on beginners.
Given that OP's (blog post: "avoid type classes") untenable position has mellowed as long as type classes have "mathematical" laws associated with them. It's moot. But I think this paper addresses [Scrap your boilerplate with class: extensible generic functions](http://research.microsoft.com/pubs/67439/gmap3.pdf) addresses OP's post. OP's code in light of FromJSON/ToJSON seems like a naive and verbose approach to marshaling and unmarshaling data. However, it's also a matter of personal taste. If one aspires to or wants to write code like that, it's one's call.
In a sense you are both right. If you go back to the original "Burning Bridges" thread, Wren's original email chain included a lot of things. Removing `fail` from Monad, making Applicative a superclass of Monad, Foldable/Traversable in the Prelude etc. The AMP thread was one of a half dozen threads that actually spun out of the burning bridges thread, a week later, when David wrote up a coherent plan from end to end. We've since coöpted the name to just consist of the Foldable / Traversable changes, though.
This assumes that `Foldable f` implies `forall a. Monoid (f a)`. I don't mind that requirement at all, but that's not how the Foldable class is currently implemented, or even documented.
Considering we are talking about importing a library, that means you have a bug. The library author is the one who chooses that semantics. If your type can't unify with the one they export, then you do have to change your code.
A work's already being done on directions related to both your proposals in [the "record" project](http://nikita-volkov.github.io/record/). With it you currently can encode the following (extensible anonymous records): showWithLabel :: (Show s) =&gt; { label :: String, content :: s |* rest} -&gt; String as: showWithLabel :: (Show s, FieldLens r r' String b', FieldLens r r' s s') =&gt; r -&gt; String And here are the ideas concerning the implementation of anonymous tagged unions in a similar fashion: http://www.reddit.com/r/haskell/comments/2svayz/i_think_ive_nailed_it_ive_solved_the_records/cnui1r8 
This fails for `Set`, `Maybe`, `Map k`, `IntMap`, `(a,b)`, etc. Most `Foldable` instances actually fail your law. `foldMap` respects `Monoid` homomorphisms, but `toList` is not itself a `Monoid` homomorphism. Neither `Foldable` nor `Traversable` tell you anything about how you can build or change the shape of a structure, merely how you can tear it down or swap out its contents once you have one.
We can create a law-abiding `Functor` that stores a fixed data type by mimicking Coyoneda: data CoYoByteString a = CYBS (ByteString -&gt; a) ByteString runCoYo :: (CoYoByteString ByteString) -&gt; ByteString Would it be possible to maintain the characteristics of `ByteString` using such an approach?
Of course you're right. My brain wasn't turned on.
`FromJSON` and `ToJSON` are exactly the kind of type classes which _don't_ have mathematical laws associated with them, which means this is exactly a situation where you might consider an alternative approach. The choice of approach is fundamentally an engineering decision: typeclasses are convenient and often idiomatic, but also less expressive^1 and often require extra workarounds (such as various typeclass extensions) that a manual approach would sidestep entirely. As you said, a person can choose for themselves whether they want to use or avoid typeclasses. But the choice _can_ be an informed one. Dismissing different approaches to writing code as "naïve" out-of-hand is to make an unnecessarily emotional value judgement on what is, at its core, an engineering issue. 1: Which is to say, it is possible to write code with manual dictionary-passing that is impossible to express as a typeclass, as my comment above demonstrates.
So you're saying mapM is iterative, while traverse is coiterative? And what does smashing the stack mean?
You can use `foldr` to do the same thing: foldr And T I generally recommend `foldr` for this sort of thing because it tends to efficiently preserve laziness, where as `foldl` does not. The general rule of thumb is to use `foldl'` when you want your result to be strict and use `foldr` when you want your result to be lazy. `foldl` (non-strict) is pretty much useless for anything other than implementing `reverse`.
I agree in Functor and Traversable, but the argument isn't nearly as strong with Foldable.
And yet without `Foldable` as a superclass of `Traversable` we'd wind up with the same sort of rampant code duplication situation we had between `Applicative` and `Monad` for years, except now spawned by 30+ combinators, not 2-3. You'd wind up with situations where you can't replace `traverse` with `traverse_`, because the class doesn't exist. Or you'd wind up with folks needing to reintroduce the very concept that you just ripped out of the language. And then to make `Foldable` a superclass of `Traversable`, we wind up converging on some version of the current design, `f` in `Traversable` is kind `* -&gt; *`, so in its superclass `f` would have to follow the same scheme. The argument for doing something radically _different_ for just the `Foldable` case that then introduces a ton of extensions, and fosters a great deal of code duplication doesn't hold up.
Poor design decisions also happened on Haskell. 
My thought is that when in doubt, we should move in the direction of simplicity and pragmatism. This ensures that Haskell is understandable, useful, and maintainable. The problem in this case is that both proposals claim to be the more pragmatic. On one hand, those in favor of FTP claim that it is simpler, because it generalizes certain functions and hence removes special cases. On the other hand, those against FTP think it makes learning the language harder and breaks comparability. They also claim that the generalization is not very useful, since the functions in question are usually only used with lists anyways. I personally favor FTP. As a community, it is important that we remain open to useful changes in the language, even if they break compatibility; otherwise we run the danger of remaining shackled to the past and stagnating. The key word here is "useful". Is FTP useful? I think it is. People who want to use folds and traversals in the same way as they have been doing can so, with little additional effort. Foldable and Traversable are fairly fundamental typeclasses (IMO, as basic as Monad or Functor); putting them in the Prelude makes them easier to use and less exotic. That's good engineering.
We often can implement (&lt;*&gt;) to be more efficient than the monadic version. If they wind up with wildly different answers then that is probably just flat out wrong.
Yeah. It's a shame. That said, even boxed vectors show a measurable performance improvement under some workloads.
I have voted Plain List. My submission: &gt; My only concern is teaching. Bullet points are listed in https://ghc.haskell.org/trac/ghc/wiki/Prelude710/List under the exosystem subheading, I won't repeat them. I think this is an overriding concern that should have stopped FTP on its tracks at the very beginning. I wonder why it didn't happen. I also happen to like this proposal: "We could support restricting type signatures in export lists, so that when both a specific and general version are imported they do not clash". It will also allow to explain I/O to newbies without using the M-word. I would love to hear criticism.
Well, what do you mean by "efficiency"? Just spacetime asymptotics? In the Haxl paper, section 4, we have Blocked (Done (+1)) &lt;*&gt; Blocked (Done 1) = Blocked (Done 2) Blocked (Done (+1)) `ap` Blocked (Done 1) = Blocked (Blocked (Done 2)) I think that you're right that they shouldn't be _wildly_ different; there should be some reasonable equivalence relation here. The "result" (i.e. 2) is the same but not necessarily the "effect".
Fwiw, there's also a better formatted [GHC Blog entry](https://ghc.haskell.org/trac/ghc/blog/FoldableTraversableDebate) version of that email announcement
Couldn't REWRITE rules be used instead?
I think learning a sperate teaching language when you want to learn Haskell is even worse and more frustrating than "Don't worry about this for now".
Language pragmas are magic though, they could change the reexports too, somehow?
Then you'd get different answers depending on if GHC managed to figure out your type or inline just enough. Given that GHC has changed its inlining behavior almost randomly over the last several releases, I'd be disinclined to trust any code that resulted. We live with this today with `realToFrac`. I do not have any code anywhere in production that relies on the awful semantics-changing rewrite rules we have around that combinator.
...and hopefully, future proposals will be referred to by more neutral designations... :-)
So now we have modules and combinators all over the place getting different types or different combinators based on local pragmas. You have all sorts of hairy corner cases around what happens when one chunk of code turns them on and another turns them off. If you make the decision just locally, then the dozen or so packages that all re-export Control.Monad willy-nilly have to agree on all their imports. Or code that imports them all to define instances will just start breaking. If you make things deeply magic now types they export vary based on a pragma at the use site. This mucks up stuff in linking, typechecking, pretty much the entire compiler. When the full gauntlet of issues that such a pragma would have to run was raised last week, it was pretty quickly pulled from the serious proposals list. A much simpler proposal is to say that we could deprecate and ultimately remove the re-exports. They are available through Prelude anyways, no expressive power is lost, and then eventually alternate `Prelude` designs could become a more viable way to move forward.
 instance FromJSON User where parseJSON = userParser instance ToJSON User where toJSON = serializeUser I wouldn't really call this "using aeson without the classes." I'd just call it "forgetting to register the instances."
I suppose. Some people like reading the actual source code, though, and I'm afraid those will be thoroughly confused by the magical dictionary passing.
Disclaimer: I have a clue and some practice teaching newbies. I don't know how on earth you've got negative karma on that. So this h&gt; :i length class Foldable (t :: * -&gt; *) where ... length :: t a -&gt; Int ... is easier to explain than this??? h&gt; :i length length :: [a] -&gt; Int -- Defined in ‘GHC.List’ You have to explain: - 1) **typeclasses** (!!!!!!!!) [no, they are not especially easy] - 2) what is Foldable and 100 shiny new kludges (all fairly reasonable questions) - 3) why the fsck half of functions on Data.List are not about lists - 3a) why the fsck half of them are left not generalised - 4) why some of them are class members - 5) why, say, Text is not Foldable - 5a) why'd they introduce Foldable to stdlib then - … edit: typos
I had a look at Keter again, and must say they offer indeed very similar propositions. Keter does not hook into git for deployments, does not facilitates Buildback based serverside builds, and does not containerize every deploy. Besides that remarkably similar indeed. The Keter program itself replaces Nginx in the Dokku setup. I wonder how the two compare in a benchmark. Keter has a tiny codebase compared to Nginx (which has a humongous installed base). Very interesting all of this. We like Dokku for being lang-agnostic. As we also deploy interpreted langs, we use containers to scope the OS deps with a deploy.
I also think there should be a very straightforward way to see the simplified, down-to-concrete-type signature. - ghci is an option, already too complexe IMO - why not including examples with concrete type signature in the function's haddock? - another more fancy idea would be to have a parser that takes a set of functions working on typeclass and instantiate them all to concrete types. This way you could generate a Prelude with lists instead of the current typeclasses. Can that be a feature of Haddock? The idea would be, for each typeclass-based function, to add a dropbox in the documentation of that function that lets you choose a possible concrete type instantiation. Then Haddock would open a box with the instantiated function. All in all, the idea is that Haddock would let the user play a bit with the function within the documentation. 
I use `pure` and `fmap` by habit, but I tend to have to switch to `return` and `liftM` because I can't be bothered adding the constraints necessary. AMP will fix that. Yay.
Look at the tab/indentation settings (Settings/Perferences/Tab Settings). Change the tab width until it matches your old indentation. Then convert tabs to spaces (Edit/Blank Operations/TAB to space). And never ever use *tab characters* in the future! (Settings/Preferences/Tab Settings/Replace by space). Save preferences.
I'm completely disappointed by all this things. What is the poll about? What are we discussing here? Whether we want to import Foldable qualified or have it in Prelude? That is a question of taste. The same for generalized vs specialized functions. A change based of taste is a random walk in a design space. I was pointed to this email: http://osdir.com/ml/libraries@haskell.org/2013-05/msg00280.html Basically it describes a roadmap that includes both AMP and FTP and other changes. Now I see that there is an idea behind the FTP. It is still not clear enough for me, but I'm starting to see some sense. I think we should discuss the roadmap and then let the core committee to do their work to find the best way to achieve the goals. Probably that already was done, but looks like the majority of the community just missed that (including me, and that is my fault totally). If it is too late to discuss the goals, then let core committee continue their work. But why voting on this particular step in the roadmap? Voting is a bad way to design things. I found the justification for Plan/FTP unsatisfactory. I want to see: We want to achieve X eventually, but we have Y, so we are doing Z to work around it. Every change should either be part of the final goal, or open a door for such thing. I fail to recognize how removing specialized functions is a final goal because I don't see what is wrong with them, so I assume this change lets us make some other change, but it is not expressed in the Wiki. 
I hate the name. Makes it impossible to explain it to programmers coming from imperative languages. I usually rename it to "inject" when explaining.
And I think that core committee made amazing work to minimize the impact of the changes. And I believe they are much better in language design them me. So I was not actively participating in the latest FTP discussion. But if you are asking for my opinion, then I'm expressing my disappointment :)
How about: import &lt;module&gt; shadowing to import *module* but hide things previous in the import list that occur in *module*. So `import Data.Foldable shadowing` hides `Prelude`'s `foldr`, `foldl` etc. and gives you `Data.Foldable`'s instead. This doesn't seem to burn any bridges.
My favourite, ultimate solution to this problem: don't align things, indent them instead. They can't become "un-indented". But yes, as others have said, if you're aligning use spaces. Tabs are exclusively for indentation.
Yes, exactly this. When I first saw BBP I was very confused, since I've watched Haskeller's for years argue that current monomorphic prelude is better for learning and was chosen that way for that reason and I have been convinced by those arguments.
I'm normally against those, but I'm fine to use them when importing modules from the report, because those aren't subject to change in conforming compilers... then this...
Interesting answer: https://gist.github.com/markhibberd/d6e4869cebb0dd2c09c5 Edit: I like this answer (and mine was in a similar vein) because it highlights two points that I feel are very important: production Haskell, and that the people who proposed FTP as it is found in a release candidate _today_ have put a lot of thought and work in it. The first point is about what we should value: is it about learning Haskell at home or university and then leaving it on the side (I saw that with numerous student at my own university: what they learned was not production Haskell, it was, to them, some kind of nice or boring academic/useless stuff) or learning what matters in the long run, when you have put code to production and must evolve and maintain it ? The second point is important to me because it feels like a lack of respect. We're here debatting and repeating and offering new proposals while significant work has already been done. Included in that work is weighting the pros and cons of the proposal, detailing the proposal itself, giving a migration path, implementing the proposal, ...
Something like the following would work if you really wanted it. data CoYoByteString a = CYBS (Word8 -&gt; a) ByteString deriving Functor coyo :: ByteString -&gt; CoYoByteString Word8 coyo = CYBS id runCoYo :: CoYoByteString Word8 -&gt; ByteString runCoYo (CBYS f bs) = ByteString.map f bs
Fair enough. Out of curiosity: as one of the main maintainers-of-libraries in the Haskell universe, would explicitly-typed exports be more broadly useful when making interface changes? Or does it 'just happen' to help with this one particular case?
I wasn't there, but I did a little digging: `Monad` was introduced in Haskell 1.3 in 1996, and featured `return` then. However, earlier papers from 1992 by Wadler don't use `return`, instead using `unitM`. So somewhere in between this change was made.
Sorry, but actually the opposite: https://gist.github.com/danclien/42e54b94498f941abfff#file-main-hs-L75 Looks like I have to write this blog post anyway (I hate blogging)
I'm confused. In the OP you talked about things not lining up anymore, which implied alignment to me. Indents work with any font (even variable-width ones, though that can look funny for other reasons).
The poll is to see how the community feels about two ways to proceed with 7.10, which on the verge of shipping. The poll is *not* about the future of Foldable in the Prelude - just about if it should be done, in the way the FTP proposal planned, in 7.10. Nor is it an attempt to "design by democracy".
There's an argument to be made that, for learning in a structured environment, it would be better to start with a mostly-monomorphic Prelude while learning the basics, then introduce type classes (and the full Prelude) on top of that foundation. But doing that already entails using a custom "beginner" Prelude, so it doesn't really have much bearing on modifying the standard one. As it stands, all that generic stuff is already there in `base` and in my experience most people learning Haskell have more trouble with apparent inconsistencies or redundancies than they do with not-yet-understood abstractions and cryptic inferred types, in part because the latter is mostly unavoidable anyway (due to classes like `Num` and `Monad`).
&gt; Well I specified a "law" for Foldable, but its not one that you can observe directly within Haskell. "A fold over a Foldable type touches every single a in the type exactly once." I'm not so sure about this. Now you are talking about the behavior of an operation in terms of the internal structure of a `Foldable` rather than algebraic properties. But when I am writing `Foldable`-generic code, any reasoning about internal structure is meaningless. All I can rely on are algebraic properties, and `Foldable` doesn't contribute any of them on its own. That is, if I write: foo :: Foldable f =&gt; f a -&gt; ... `Foldable` contributes nothing to any reasoning I can do (note the absence of any other constraints on `f`). What does help me are free theorems and the monoid laws. But I could get these just by working with a `Reducer` type or some such. The only thing `Foldable` is giving me is some overloading of the name `foldMap`, which might be convenient, but isn't very compelling.
This does not seem realistic to me. Any discipline that you study as a novice is going to have fundamentals that you learn right away, and details that you accept for now and investigate later. Haskell has tons of magic for newcomers. Consider "do" notation. It has precise semantics and I would expect an advanced Haskell user to be able to desugar it by hand. However, I would never expect a beginner to understand the monad operators before writing their first program with "do" notation. All that would teach them is that Haskell is a ridiculous, impractical language.
Not if the font is such that not all characters are the same size then your previously nice indention gets thrown out, which is what happened in this case :/
&gt; The poll is to see how the community feels The community feels... different. Is there any difference between 20/80, 50/50 or 80/20? &gt; The poll is not about the future of Foldable Yes, I know. And that makes me disappointed. It the committee is sure that the FTP is the right thing, then they should do that. Otherwise they should not. It is obvious. &gt; Nor is it an attempt to "design by democracy" The FTP is already applied, and reverting it is a design decision. If the committee is going to make is based on the poll, then it is exactly the "design by democracy". If they just want to know whether there is an agreement in the community, then they already know -- there is no agreement. I'd not complain if they go ahead and do either way, but they are asking for my opinion. And I'm asking for more details, nothing more. I'm asking why they are asking. What is wrong with the initial plan. Why the whole roadmap is not important enough to make a poll, but this particular change -- is. I'm full of questions, and I don't have answers.
Type classes work, of course. The problems start when you have two instances for a single type, or want to build instances dynamically, and you really start to appreciate the clarity and flexibility that explicit dictionary passing provides. Type class =&gt; coherence and confluence; they really work best when laws force instances to be unique or isomorphic. If you don't want coherence and confluence, explicit dictionaries are better, and the `ImplicitParams` extension can still give you good local syntax without introducing ambiguity.
* I have seen no evidence whatsoever that `Foldable` and `Traversable` would be significantly more difficult to teach in general than the way things are now. Even explaining the difference between `data` and `newtype` seems more challenging. * To the extent that generality does make things harder to learn, it's already a lost cause because of unavoidable type classes like `Num` and `Monad`. * Haskell is never going to be an "easy" language to learn and I don't think the needs of *complete* beginners should be a major concern in making decisions. Frankly, I don't think the argument "but it's easier/harder to teach" should carry any weight unless coming from someone who spends a lot of time teaching Haskell. Argue for what benefits you, or argue based on evidence, but don't argue for what you think will benefit some group of hypothetical people.
&gt; I think we should discuss the roadmap and then let the core committee to do their work to find the best way to achieve the goals. Probably that already was done, but looks like the majority of the community just missed that (including me, and that is my fault totally). Yes, this is exactly what happened. There has been very broad general support on e.g. the libraries mailing list for moving `Foldable` and `Traversable` into the Prelude. After the core committee was formed, it spent a lot of time figuring out how to make this happen with minimal disruption. Now, after multiple release candidates and many library maintainers updating their code in preparation for the change, people who weren't even trying to pay attention and didn't join the discussion are unhappy that changes they didn't know about were made without their input. The fundamental justification, and motivation, is that unnecessarily specialized functions shouldn't be the default in `Prelude` when more general forms exist in `base`. Everything else follows from how to achieve that with minimum breakage of existing code. Meanwhile, many if not most of the counterproposals (aside from "don't do anything, keep the status quo") would be more disruptive and break more code than the proposed changes.
But why tie our hands to the report so tightly when being robust to change only requires an IDE key press to add the import lists via dump minimal imports or such?
Would really like to go to this, but a flight would be $1300. Are there any events of this kind going on in the United States?
I admit I don't spend a whole lot of time teaching Haskell. Since I'm just a regular bloke with no special powers, and it's just a hobby for me, I use books written by others. Do I need to throw away books I have and wait while new ones are written? If I teach with old books but insert caveats everywhere about the stuff that has become obsolete, will it work? Won't people just walk away from the course if I keep telling them that types of all basic functions are given incorrectly in all the books and almost everywhere online? I have no idea. I'm afraid I won't be able to explain this stuff to people. Should I give up teaching Haskell altogether and leave it to the professionals?
&gt; I would never expect a beginner to understand the monad operators before writing their first program with "do" notation. Why not? Imo, asking a beginner to use "do" notation without explaining it is a recipe for confusion and an incorrect quasi-imperative mental model. I think the "just pretend it's C" approach does a disservice to newcomers, and personally I never really felt comfortable with the notation until I understood it myself. You can't gloss over the fundamental weirdness of lazy functional programming! If I were teaching the language to beginning programmers, I'd avoid IO and "do" notation entirely at first. Pure code only. Then, after they understood that, I'd introduce state and sequencing with pure code that passes around intermediate results explicitly. Then, I'd show how you can abstract the pattern of passing around of intermediate state into a set of sequencing operators and functions that return a (result, state) pair. Once they understood that, I'd point out how the new sequencing operators correspond to &gt;&gt;= and &gt;&gt; for the State monad. Finally, I'd show how "do" notation desugars into the sequencing operators and the IO monad is like a State monad that passes around an invisible "RealWorld" parameter. No magic.
This is an interesting discussion, because it points to a notion of "observability" that is hard to pin down. As Edward points out we don't actually want to speak about _genuine internal structure_ but rather about "morally internal structure," or "observable structure". But to do so we need to define that. And given our current tools we have to do that _externally_ to the language itself. But in your example don't be so dismissive -- that signature tells you something already, just something so obvious you think you already know it -- the `f a` you are given is only used as an "ordered collection of a".
&gt; Do I need to throw away books I have and wait while new ones are written? This is simply one of the limitations of print. We shouldn't avoid improving something just because there is a book written about it. Anyway, the function types are being *generalized*. A great deal of book examples will work without any changes.
As opposed to all the other minor changes that make old books out of date, including ones far more likely to result in code that doesn't compile? Why are `Foldable` and `Traversable` such a big deal, when code examples that include type signatures (and I'd hope most would, in beginner books) written for the list-specific versions will work exactly the same with the more general versions? Just tell people that more general-purpose equivalents of the functions are now present by default but they can keep using lists. They'll probably just shrug and move on or, if they're quick learners, ask you to explain why `map` and `fmap` are still different. (It's always fun to explain to a beginner that they're confused about something that was done specifically to "help" beginners.) This whole discussion is the worst kind of bikeshedding. Lots of people with strong opinions about a minor change they probably wouldn't even have noticed until months after the fact.
&gt; But I get that information just from the type of `foldMap`... so without any loss of reasoning ability, I could simply accept an extra argument of that type in lieu of a `Foldable`. All I seem to get is a little convenience. Indeed, and furthermore `Foldable` only lets something of the form `f a` contain an "ordered collection of `a`". What if instead I have something of the form `f a b` or `t`? 
&gt; A great deal of book examples will work without any changes. The only possible breakage (as far as example code goes) should be ambiguous types with typeclass constraints due to producing and consuming a value within an expression, e.g. `show . read :: String -&gt; String`. Now, `Traversable` neither consumes nor produces values, and there's no way to produce a polymorphic value with only a `Foldable` constraint. So you'd need to have a `Foldable` function consuming a polymorphic value of the form `f a`. The only likely way to have that in a beginner example is with `Monad`, which basically amounts to "examples that use `do` blocks like a list comprehension" which I doubt is a popular idiom to teach beginners. I think the main use case that will break is when `OverloadStrings` is being used and the now-generalized functions are applied to string literals.
Thanks, I've also thought of writing something like this, it's definitely needed. However, it should expose compileM, or something like it. TH is not usable in all situations. And we need to be able to pass options. Oh, and maybe you should pass 'utf8' by default. And you shouldn't use ByteString.pack on a String, you should encode as utf8 first. Also, non-operator non-overloaded matching functions would be much appreciated. E.g. 'matches :: Regex -&gt; string -&gt; Bool' and 'groups :: Regex -&gt; string -&gt; [(ByteString, [ByteString])]' -- (complete match, group matches). Also from experience, a 'groupRanges :: Regex -&gt; string -&gt; [(Int, Int)]' is occasionally necessary (e.g. use regex to insert color codes). Also, adding an 'escape' function would be nice, e.g. -- | Escape a string so the regex matches it literally. escape :: String -&gt; String escape "" = "" escape (c : cs) | c `elem` "\\^$.[|()?*+{" = '\\' : c : escape cs | otherwise = c : escape cs
Ah I was wondering whether it came from Wadler. Thanks for doing the archaeology!
[This huge paper](http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/history.pdf) helped a lot :)
Awesome! I needed this for a quick script the other day. Great work!
Well, I guess this proposal won't help you then. What if I have something in the form of a man-eating tiger? I guess I have bigger problems then :-P
Exclusively for indentation that is exclusively using tabs. A standard much harder to enforce than banning all tabs.
I'm missing something? Are those related to FTP in a way I don't see? PS also, there are curly brackets.
&gt; "This is a list. There is this thing you can do to it called a fold..." Yes, that's exactly how you teach javascript ;) But for Haskell you'd want to teach them to look at the types
Thank you, that's what I should have written. It was just a shower thought. I guess the better approach might be to use `Vector Word8` instead of `ByteString`. But of course there are interoperability questions...
However, that doesn't solve the problem that the types of many things were made much more polymorphic in the libraries, too.
&gt; there are efficiency issues It would be good to see an example of the efficiency issues. They've been talked about but I still don't understand them.
This is exactly it. You could write a peasant multiplier for any monoid: -- peasant multiplication rep :: Monoid m =&gt; Int -&gt; m -&gt; m rep y0 x0 | y0 &lt;= 0 = mempty | otherwise = f x0 y0 where f x y | even y = f (mappend x x) (quot y 2) | y == 1 = x | otherwise = g (mappend x x) (quot (y - 1) 2) x g x y z | even y = g (mappend x x) (quot y 2) z | y == 1 = mappend x z | otherwise = g (mappend x x) (quot (y - 1) 2) (mappend x z) This uses the same algorithm as we use in haskell to perform (\^). Now we can take that and make the more efficient Repeat: data Repeat a = Repeat !Int a instance Foldable Repeat where foldMap f (Repeat n a) = rep n (f a) instance Functor Repeat where fmap f (Repeat n a) = Repeat n (f a) Here we replicate `a` n times using merely `O(log n)` `mappend` calls. This is both `Foldable` and `Functor`, but can't be extended to `Traversable` without changing the `Foldable` to be the obvious boring Foldable you'd get with `DeriveFoldable`. This is the same sort of situation you see with `ZipList`. It is `Applicative`, but not able to be extended to a `Monad`. We can also make more interesting types that exploit this property and which are, surprisingly, actually `Traversable`! data Table a = Table { count :: {-# UNPACK #-} !Int -- an opaque recognizer , runTable :: forall r. Monoid r =&gt; (a -&gt; r) -&gt; r } instance Functor Table where fmap f (Table i m) = Table i $ \k -&gt; m (k.f) instance Foldable Table where foldMap f (Table _ m) = m f foldr f z (Table _ m) = m (Endo . f) `appEndo` z instance Monoid (Table a) where mempty = Table 0 $ \_ -&gt; mempty mappend (Table i m) (Table j n) = Table (i + j) $ \k -&gt; m k `mappend` n k newtype Ap f a = Ap { runAp :: f a } instance (Applicative f, Monoid a) =&gt; Monoid (Ap f a) where mempty = Ap (pure mempty) mappend (Ap m) (Ap n) = Ap (liftA2 mappend m n) instance Traversable Table where -- this reassembles the result with sharing traverse f = runAp . foldMap (Ap . fmap pure . f) instance Applicative Table where pure a = Table 1 $ \k -&gt; k a Table n as &lt;*&gt; Table m bs = Table (n * m) $ \k -&gt; as $ \f -&gt; bs (k . f) Table n as &lt;* Table m _ = Table (n * m) $ \k -&gt; as (rep m . k) Table n _ *&gt; Table m bs = Table (n * m) $ rep n . bs instance Monad Table where return a = Table 1 $ \k -&gt; k a as &gt;&gt;= f = bag $ \k -&gt; runTable as $ \a -&gt; runTable (f a) k (&gt;&gt;) = (*&gt;) fail _ = empty Here our `Traversable` instance is tricky and manages to reconstruct the new, modified table while even sharing the intermediate applicative values so it preserves sharing. Also the `(&lt;*)` and `(*&gt;)` implementations are able to take advantage of `rep` to make those operations speed up asymptotically as well. In one case it repeats the monoidal result per element, and in the other it repeats the monoidal summary at the end. 
HTTP services should not use String, and should use less lists. But in general code, lists can be - and should be - used quite often. The list type in its simplicity represents one of the most fundamental semantics. One of the biggest values of Haskell is its capability of semantic expressiveness and simplicity in its types. That makes software more readable and maintainable.
The only caveat is that, if memory serves me, there's no good way to override a `.ghci` file--in particular, a `.ghci` file in the current working directory will be run *before* one in the user home directory or equivalents. Which seems backwards to me, but oh well. In any case, the result is that it becomes awkward to switch *back* on a situational basis and you'll get headaches when loading a module that imports the Prelude if there are name clashes. You can disable any use of any `.ghci` file at all with a command line switch, but that's it.
Even if we're willing to have broad sweeping change to semantics there still remain two pretenders to the throne of what precisely the ideal operational behavior of `sum` would. * `foldl' (+) 0` wins hands down on all benchmarks for lists, but `foldl'` didn't exist in the Haskell 98 report, so it wasn't available to be used at the time. * `getSum . foldMap Sum` which which there are containers where it wins by an arbitrary asymptotic margin. It also simultaneously handles somewhat marginal side-cases like lazy Peano arithmetic. Which of these is better pretty much entirely depends on shape of the container at hand.
When this concern was first raised, I took the time to go through example by example through four different Haskell books and didn't find a single line of code that broke. I stopped even looking after that. There are examples where `:type foo` segments now give more general signatures, but that is the sum total of the book-related impact.
What do you mean by the post-AMP landscape?
https://wiki.haskell.org/Functor-Applicative-Monad_Proposal
Could you please give an example? If you have time, of course.
Funfact: i ordered the very same quadcopter some time ago, but never thought about "hacking" it. That is, I replaced the "charger" with a real one. The stock one is nothing more than a resistor plus a led and offers no protection whatsoever against overloading. Nice talk btw ;-)
I'd like to get more people involved in hacking them as they're just super cool! When I get some time, I'm going to put more effort into controlling them with an external camera as I think that has the potential to work quite well.
While semantically for finite structures, `foldr`, `foldl`, and `foldMap` are of equivalent power, clearly depending on the structure one sort of fold is more efficient than another. So depending on the combination of your structure and the sort of operation you desire, you want all three at various points (at least).
Ideally these properties should let us reason about program transformations in some way. So assume you have some other operation -- any other operation -- that claims to take an `f a` and yield some quantity of `a` -- call it a `Bag a`, and that this other operation is parametric in `a`. The idea is that this operation is in some sense _initial_ with regards to any such other operation. 
Sure, but I don't see why that means they all have to belong to a typeclass. In fact, if the choice between them is so critical it seems to me that the typeclass is actually hiding something important.
Ah, I see. That is certainly a valid framing for the semantics of `return`, although it doesn't have much bearing on the choice of name. In the end, there is no platonically correct name which the language designers can be expected to have discovered. While `return` certainly presents opportunities for confusion, it does make a degree of sense in the proper context.
&gt; Typeclasses are nice though, when well engineered, and they help us reason, and they solve problems. I agree. If we track back all the way to [the top of the thread](http://www.reddit.com/r/haskell/comments/2vfczx/ghc_710_prelude_we_need_your_opinion/coh5ji4) what I am saying is that I'm unsure that `Foldable` meets the conditions to be "well engineered". I'm somewhat more secure on `Traversable`. Anyway, I think all that can be said on that has been said, and I'm certainly not claiming I think I know an objective truth on this issue.
Your feedback was very much appreciated, there is no hate. =)
The main complication with [this variant](https://ghc.haskell.org/trac/ghc/wiki/Prelude710/FTP#exports) is that it drags in a language extension into modules that affect what would be a future language report.
FTFY: on top of prce-light
This is `Functor` and `Foldable`, but gets stuck at `Traversable`, not enough information is available.
The deprecation is intended to notify people that things have changed, and will break next time around. But I do see the issue with folks pulling `sort` (or any non-generalized function) from `Data.List`. If it were just me, I'd say that currently maintained code can be migrated to import `sort` and friends from a new location whose name I really don't care about so that the unmaintained code can continue on unhindered by progress, secure in its usage of hallowed names. But, as you know, I'm more of a rip off the band-aid kind of person when it comes to these things. I just don't see `Data.OldList` making anybody happy.
I suspect it will be a big deal. I don't really know, as I have too little experience. We will see.
It may not make anybody happy, but it is a one line change we could make in a cabal file, available today, that gives access to monomorphic versions of those combinators now rather than in two years, for free. =)
While the inferred types become more general, any sample code that they wrote with an explicit type signature still typechecks at that signature.
I know examples will work (compile and run). Typing `:t foldr` in REPL may or may not work for the people though. I suspect it will scare people away (it did scare me a bit when I first tried 7.10).
I can imagine it's pretty complicated, yes, but I like this proposal anyway (from my restricted and incomplete perspective).
We've linked the policy on [taking over packages](https://wiki.haskell.org/Taking_over_a_package) on the hackage front page, along with a link to a new page describing what the [hackage trustees](https://wiki.haskell.org/Hackage_trustees) are all about.
This isn't correct. Only if the maintainer has added other people to the maintainer group can those people upload bug fix releases. In general, the hackage trustees cannot upload new versions, just edit the .cabal file metadata (this is deliberate).
I've been thinking I'd like to see the community make that shift...
The `function`/`return` part has more to do with the laziness than how Haskell's `return` works though. The `M` is more like the Haskell `return` in that example.
Or if you could exchange "metaphor-free" with [zero analogy](https://web.archive.org/web/20120117061438/http://unknownparallel.com/monads.php).
I've had problems where I can load a module in GHCi and think I'm done, but then when I push it to github all of a sudden travis is [all like](http://xkcd.com/1483/) "you need to enable a language extension."
So.. that's nice (and I appreciate the link! I might not have found this otherwise since it's not live anymore outside the archive)... ... but it seems to spend a lot of time saying things like, "Monads are...". And when you're starting out, and you just want to understand a basic sequence of IO operations... it's a bit much. And it isn't really necessary--at least, not during your first week. It might not even be necessary during the first month or two. When you're just starting out, all you really need is to understand how certain operators behave. And you don't need to shove *that* particular page full of nomenclature in order to make sense of it. In fact, making someone think about that stuff at the very beginning is *exactly* the kind of thing that delays understanding. 
Also, I'd like to add that there is really good [picture oriented explanation](http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html) that also doesn't use analogies.
&gt; This is exactly how so many monad tutorials come about. Really? They come about because someone wanted to do it without analogies? Without referring to other languages? ("It's like an abstract class in C++..." No! Stop that!) And *with* a simple discussion on function definitions that use patterns? And about how &gt;&gt;= is just another function defined with patterns? Just the meat needed to understand simple-but-practical definitions of 'main', and no verbiage that you *don't* need to understand it? I mean... I don't mean for that to sound nasty but... I would really really love to see a Monad tutorial that matches that description. I looked, and I didn't find it. Because usually, they devolve into an analogy that isn't quite right. And they're usually pretty light on examples. I'm talking about fixing those bugs (partly by just not having analogies---and I'm not even sure I want the pipe operator to be mentioned, except maybe with a qualifier: "this is just a loose analogy") Also, some of those tutorials fall into the trap of using a lot of new vocabulary terms that you don't really need in order to understand a practical sub-set of syntax and semantics that you find in a simple 'main' that does a little IO. I'm stressing this because, at the beginning, the mechanics that you need to know just to make sense of why IO routines work in do-notation actually are fairly simple. And if we just limited the *initial* discussion to what's needed to understand that, then it might be easier to avoid getting discouraged earlier on. Maybe there would be fewer false starts. If there's a tutorial that already succeeds at that, then I'd love to see it.
I'm going to read through it, but it loses points in the first paragraph: &gt; They are closely related to the monads of category theory, but are not exactly the same because Haskell doesn't enforce the identities satisfied by categorical monads. Why does anyone need this in week 1? I'm talking about a tutorial that never once uses the words: &gt; "A monad is... " ... and one that talks in a straightforward way about the mechanics of how to read and understand code that uses monads. That means our vocabulary includes words like "type", "function", "operator", "pattern-match", "chain", and then eventually "lambda", "do-notation", "de-sugar"... ... and at least a couple/few examples of each, and examples showing intersections of those things... ... and very very little else. (e.g. I think we can get away without using the word "Monad", except to see it as an identifier that follows the keyword 'class'.) This is related to a point Feynman made about understanding things: you can know lots of words for a thing without understanding its nature. Focusing on how things behave, and not so much what we call them, is what leads to understanding. Once we start to understand behavior, *then* we can put a label on it. 
I think there might be only 2 "optimal" methods for teaching monads, though I'm not sure which one would be better. * Mention the 2 functions (return and bind) that define them, and explain the type signatures. Say that anything that can support those methods is a monad. Later define the monad laws to better explain it (they are better shown with do notation imo). In the end you can understand monads just by looking at the type signatures and figuring out what they entail. That "Functors, Applicatives and Monads in pictures" post would belong to this method (just follow the type signature, and imagine why said function might be useful). * Explain what pure functions are, and how you can compose them with the "(.)" operator. Explain what the "id" function is and how it works. Then post an example where you have a composition of various functions, but you want something else done to them (logging, state, access the terminal/file system, etc). Then explain how you could do that by changing "(.)" and "id" to "(&lt;=&lt;)" and "return" to support that functionality. I like this explanation because you don't need to explain the monad laws to understand monads, since they implicitly carry over from the analogy of composition of functions. Anything else confuses people much more IMO.
I like to explain `bind` by calling it `flatMap`.
I think you should read the post in its entirety before passing judgement
Thank you for the link! At a glance, the examples here seem pretty good. There's still a lot of fluff (e.g. cartoons and "cute" language). But... it's closer. It may be that a Khan-style approach is better: that enables you to do examples in ghci, for example. 
Glad I could help. (: Now pair that with the associative law: (m &gt;&gt;= f) &gt;&gt;= g ...is-equivalent-to: m &gt;&gt;= (\x -&gt; f x &gt;&gt;= g) ... and now revisit the way that do-notation expands. 
&gt; I would really really love to see a Monad tutorial that matches that description. Then write one
If you want to introduce things via `(.)`, I suggest using `(&lt;=&lt;)` instead of `(&gt;&gt;=)`. First, the "order of application" matches up better, second the argument types match up better, which allows the associated laws / identities to match up better. Of course each of these composition-like operators is *exactly* composition in some category (or magmoid), but you don't have to mention that if you don't want to. Then (because of equational reasoning) you can treat `(&gt;&gt;=)` as "sugar" for `(&lt;=&lt;)` and finally`do` as sugar for `(&gt;&gt;=)` and `return`. I actually *would* talk about composability first, because all the programmers I've dealt with has existing experience that gave them an intuition for what composability is, even if they (like me) weren't that good at applying it to their program designs. I **probably** *would* actually also mention monoids, too. Many programmers already know a few monoids, even if they don't call them that. All of those internal examples prodive great grounding for the associativity and identity laws. Then, the monad laws (presented with `(&lt;=&lt;)`) are "earily similar" to the monoid laws. I *would not* say that `(&gt;&gt;=)` is implemented via pattern-matching (which, BTW, is no kind of overloading) since it certainly doesn't have to be. It might be for some examples, but I'd leave that as part of the hidden implementation details of a specific monad, not something that is true of all monads. By the end, you'll be able to very quickly explain (and have it stick) why "monads are just monoids in the category of endofunctors", which is largely trivia, but falls out of the properties we want for monads to be used in the ways we use them. But, again, you could ignore this, if you think your audience will be turned off by learning too much.
**You want the [Haskell Wikibook](https://en.wikibooks.org/wiki/Haskell)**. It just explains functional concepts, builds them up bit by bit and then eventually into the workings of monads. No analogies, no weirdness, just makes sense. And if you think it isn't *everything* it should be, *you* can improve it for all the readers into the future! But seriously, it's already about the best, although the whole elementary track goes together, so it's not just a monad tutorial per se.
Heading to sleep so be warned, but a few of my reasons: - Haskell is better at reducing complexity - equational reasoning - turning runtime errors into compile time (imagine doing everything in C++ with stl, except sanely) - referential transparency - pervasive composability - much better code reuse
&gt; I would not say that (&gt;&gt;=) is implemented via pattern-matching (which, BTW, is no kind of overloading) since it certainly doesn't have to be. It might be for some examples, but I'd leave that as part of the hidden implementation details of a specific monad, not something that is true of all monads. This makes me think that I still don't understand it enough to seriously think about making Yet Another Simplified Haskell Tutorial. I'll keep iterating; thank you for the pointers! As for avoiding implementation details... point taken. (At least for week 1. People accustomed to looking at generated assembly code ultimately want to be able to look at everything, and we understand that there's a border between "portable" and "implementation-specific/version-specific/build-config-specific/do-not-count-on-this-being-here-next-month-even-if-nothing-changes-in-your-project".) 
&gt; why should I give up a good chunk of my time to learn Haskell than giving up a good chunk of my time to learn more about say C++ (OpenGL, Unreal etc) or C# (QT, Unity) It's your choice, really. Learning Haskell mostly gives you a new perspective on how to approach programming problems. There's nothing you can implement in Haskell you can't in other mainstream languages. Just be warned, that once you get hooked by Haskell, you may have a hard time going back to your previous languages, and risk being unsatisfied if you are still required to program in those for your dayjob... ;-)
Yep, that is exactly what I meant to convey.