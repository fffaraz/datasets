&gt; R can be coded in a functional-ish style (trust me, I try) You are then saying R is like Python or Javascript then? Since they can be functional-ish in style? I am just saying you can program functional in R since it is a Functional Language It is not Haskel or Rex but you can write R scripts that are purely Functional programs, which you can not do in Python or Java-script or an other First Class Function in their language but are not Functional.
I think this is a good idea although the function should really be renamed to `charToInt`. To keep the original mapping from `String` to `Int`, possibly the following is a more concise way to go about it. stringToInt str@(x:_) = if length str == 1 &amp;&amp; isDigit x then digitToInt x else 0 Otherwise, I'd use a case expression over guards - StringToInt str = case str of "1" -&gt; 1 "2" -&gt; 2 "3" -&gt; 3 "4" -&gt; 4 "5" -&gt; 5 "6" -&gt; 6 "7" -&gt; 7 "8" -&gt; 8 "9" -&gt; 9 _ -&gt; 0 
Nah, the GHC compiler's error messages are really bad compared to other compilers, don't feel bad.
&gt; If the latest version of a package does not suit my needs, that's an important alarm bell, and I need to take action. In some cases, that requires finding a fork or alternative package to do the same thing. In some cases, that involves talking to the author to make them aware of my particular use case. In either case, doing this sooner rather than later makes it easier to find a good solution. You are missing the fact, that sometimes, the new version break existing code. Example Diagram 1.2 =&gt; 1.3 breaks everything as the main type as changed it's arity. It's probably a good idea to upgrade, but sometimes, it's not you priority. You have something which works, "why fix something which ain't broken" ? Also, sometimes you don't want to upgrade at all, you are just forced to do so. The other day, I deleted my sandbox for whatever reason. I forgot to do `cabal freeze` before doing so and I didn't put any version number in my cabal file (I know it's bad). I couldn't rebuild it because diagrams 1.3 has been push and was incompatible with my code. Of course I didn't now at that time what was the problem with diagrams 1.3. At that particular time, I needed stuff to work, not to spend time upgrading. That's the day(night) I switched to `stack`: LTS at that time was still using diagrams 1.2 and I was able to build my project and get the job done. Of course, since that I upgraded to 1.3 (and went back partially to cabal :-)). 
Arrows, man. Beautiful stuff
Could you elaborate why?
&gt; You have something which works, "why fix something which ain't broken" ? Agree entirely - there are forces that push you to upgrade (e.g. the paragraph above), and forces that encourage you to not upgrade (e.g. breaking your code, risk of regressions). I'm not arguing people should be forced to always upgrade, and in your case, picking a Stackage version with diagrams 1.2 would be been a sensible thing to do. I typically start with the latest versions when starting a new project, upgrade when there is something new I know about that I want, and failing that upgrade every 3 months or so, as suits my schedule (for work stuff). The argument in this post is that Stackage LTS, which gives you is the ability to upgrade "a bit", isn't very useful.
Two-part question: 1. Is the FFI as described in this (and other) papers available in any currently released version of haste? The example on github uses a function called ffi whereas the thesis mentions a host function. The example on github also doesn't seem to show the arbitrary marshaling of haskell records. 2. Do the same caveats about performance mentioned with regard to the ffi function apply to the Foreign.FFI module mentioned in the paper? If so, how big is the tradeoff?
If nightly-7.8 was a subset of nightly-7.10 in terms of packages, but where they had the same package the versions matched, that might be a workable solution. Having three simultaneous versions might be too much - I like to support back to 7.2 or 7.4, but most people don't.
One more point: Packages are never **removed** from LTS streams, AFAIK. They could be removed from nightly, to have it rolling.
The release notes are here: http://downloads.haskell.org/~ghc/7.10.3/docs/html/users_guide/release-7-10-3.html
The interface is the same between the paper and the current version of Haste, but the names (unfortunately) differ slightly between current Haste and the paper. The `ffi` function of [Haste.Foreign](http://haste-lang.org/docs/haddock/0.5.2/Haste-Foreign.html) is the exact equivalent of `host` from the paper. The example on GitHub hasn't been updated since before generic marshalling was added, so it doesn't show that feature off. I'm not sure which particular caveat you're referring to with 2), but the performance characteristics described in the paper apply to the version shipped with Haste. If you're only marshalling Ints and Doubles, you're better off using the vanilla FFI (the performance hit for Haste.Foreign ranges from nonexistent to about 15%). For anything else, Haste.Foreign should be substantially faster.
Curious if this requires a specific version of llvm on ARM, as [ghc 7.10.2 did](https://ghc.haskell.org/trac/ghc/ticket/10863). There I needed llvm 3.5.2-2 and not llvm 3.5-10 which (at the time) was the default on debian stable. If there is such a dependence, it seems like it would be a good idea to note that on the download website. 
Some people seem to have misread my terrible phrasing to suggest people should upgrade to the latest nightly every day. That's not what I was going for. Instead, I suggest people pick a fixed nightly, and when they chose to upgrade (once a month, once a year), they upgrade to the latest nightly.
In other words you are suggesting to rename "LTS" to "Fixed Nightly" :) 
Which of those is it for you? Either way, congratulations! I was just looking into this exact question today and happened to spot this thesis. Haven't read beyond the introduction yet, but look forward to it.
The standard FORTRAN numerical libraries are extremely well optimized for individual operations, but take some thinking on the part of the programmer to avoid things like excessive allocations and traversals. Eigen [uses](http://eigen.tuxfamily.org/dox/TopicLazyEvaluation.html) lazy evaluation to achieve optimizations pretty much like Haskell does already, and I find it makes the code more pleasant to write. If I was choosing a linear algebra library for Haskell I would go for one with a clean abstraction and intelligent optimization, even if it meant I didn't get the raw speed of BLAS/LAPACK. In general the kind of stuff I do it takes longer to write than to run.
It's not the same. Upgrading from say [`LTS-3.2`to `LTS-3.17`](https://www.stackage.org/diff/lts-3.2/lts-3.17) doesn't look as big as from [`nightly-2015-08-24` to `nightly-2015-12-7`](https://www.stackage.org/diff/nightly-2015-08-24/nightly-2015-12-07) (nightlies are from corresponding days).
I use it for building web applications. I do this full-time as my job. I assume you have experince build web apps since you listed PHP as something you're familiar with. What I do is mostly the same kind of thing you'd be doing in any other language: sucking in data from somewhere, cleaning it up, presenting it to the user, letting them give feedback. The difference between doing this in haskell vs. doing it in ruby or PHP is that I can lean on the type checker. This doesn't mean that stuff just magically works though. You still have to think about how you model things, just like you do when using other languages. The type checker catches a lot of mistakes, but not necessarily everything. In the small applications I have worked on (under 10000 lines of code), it has been effective enough that I almost never write tests. To be clear, I do not believe that using Haskell means you never need to test code. Anyone who says that is a charlatan. In my particular situation, I have not had much use for them though. My experiences with refactoring in Haskell have been significantly better than in other languages. If you change the data model (in such a way that the change shows up at the type level), the type checker shows you the places that are affected by the change. Mostly, I just enjoy using haskell more than other languages. I find that when I enjoy working on something, I tend to do things better. (I know that this is completely subjective, but I thought it was worth mentioning). In short, a lot people do the same things with haskell that they do with other languages. Haskell's type system gives us a different way model things, but the problems we are ultimately trying to solve are often a lot like what you are used to seeing. 
The lack of higher rank types.
I see the point of LTS mostly in allowing people to share work on backporting security and other fixes once something is truly old. Say your customer wants you to run that two year old Haskell project you haven't looked at in the meantime at all. They are not willing to pay for all the effort of upgrading your code base to work with the latest dependencies and GHC since they are still running the same OS,... as they used to and you only need a tiny fix (e.g. change of a hard-coded email address). The community as a whole can easily keep an LTS version working through minor breakage, it can not keep dozens of nightlies working as easily if everyone uses a different nightly.
Yes, but that's a special case solution. A better more general solution would be to decentralize the problem and allow anyone to define curated collections.
but `Map String String` is too restricted
Is [Trac #3242](https://ghc.haskell.org/trac/ghc/ticket/3242) included in this? It's not in the release notes.
So what's up for your full Ph.D. thesis then?
Here's another argument against LTS, as a library author. If there is some other LTS accepted package has an upper bound on your library, then your newer releases will not be accepted to LTS, until that other package increases their upper limit. So, a package that you've never even heard of can stop LTS from ever including the newer versions of your library, making everybody else suffer. I find this to be the biggest challenge adopting LTS. As Lamport quipped once, "A distributed system is one in which the failure of a computer you didn't even know existed can render your own computer unusable." I'm afraid LTS is playing that role: A package you didn't even know existed prevents you from using the latest version of a library that you need.
I updated my post and code to also give the traditional syntax.
Yeah, when not doing point-free style but doing OO-style left-to-right application, I do use `&amp;`.
Ha, yeah, especially since after Haskell was invented in 1990, OO and method syntax really, really took off.
I put in a mention of `interpolate` and `unindent` in my post for Day 9.
Yeah, these things are not all equivalent. I'll try to go back and clarify when I find the time.
I'm using Haskell at work to manage the stock in a warehouse and help optimizing (manually) how we layout rackings, shelves and find a good arrangement to store the boxes. It can generate 'cli' reports but also 2D map of the warehouse in svg (using `diagrams`) or 3D model compatible with Blender. I also use it as a scripting language to do whatever I need, like generating labels for incoming delivery from packing lists (excel spreadsheet) , reconciliate bank between bank account and accounting system etc , generate cashflow report (csv) etc ... I use to write script like that in Ruby and R. It's much more fun to write in Haskell but more seriously it's much easier to modify or find problems when I use them again 6 months later. I still use R, because some things are much quicker to write in R, however R scripts are really hard to maintain (or at least the one I wrote). I typical example is to join to file, one containing a list of box with their dimensions with another file being the number of box received to compute the total volume. In R, you'll do something like dim &lt;- read.csv('dimensions') boxes &lt;- read.csv('boxes') merged &lt;- merge(dim, boxes) merged$volume &lt;- merged$number*merged$width*merged$length*merged$height print(sum(merged$volume)) Et voila. Basically, the first 2 lines, read the two files, the next one `merge ...` is where the magic append. It does a join between the two array using the field in common. Then we just computes the volume for each row and sum 'em all. The read and merge bits is brillant but also where the problem starts. If the format of the files changes, you could have really different behavior. First, the join can not work at all, or join on field you don't want. Second, if one column is missing (let's say 'height') you have no idea in which file it should have been initially. This example is trivial, but when you start merging 3,4 .. 5 tables it gets very tricky. With Haskell, that types of problem don't happen. Ok, I could have more discipline, write more documentation and more robust R, but then It's not quicker than written Haskell and in the long term Haskell is much more robust. 
I am very interested in the use of Haskell to make a functional [scene graph](https://en.wikipedia.org/wiki/Scene_graph) on top of OpenGL (other backends are possible in the future, of course). You can think of a scene graph as a graph or tree, where nodes can represent drawable primitives, transformations, event handlers, groups of other nodes, etc. Using this structure allows you to cleanly and efficiently render complex scenes, as you get principled interaction between objects in your scene. See [this page](http://archive.gamedev.net/archive/reference/programming/features/scenegraph/index.html) for more info on why scene graphs are useful. One strategy for dealing with scene graphs is to think of your scene graph renderer as a **compiler**. As many of us know, Haskell is really awesome for building compilers! The process goes like this: 1. The user specifies a hierarchy of scene nodes. 2. You compile this initial representation down to intermediate trees, while caching information like bounding boxes, transformations, etc. 3. Finally, you create a data structure that holds what is essentially a list of drawing functions for each individual primitive. Think of raw OpenGL calls as the "machine language" of your scene graph compiler. When you have that final list of drawing instructions, you can do really cool optimizations, like sort the instructions by shader type so you minimize the number of shader loads, or sort by material type so you don't reload resources, etc. There are a few interesting problems that I still need to solve: * How do you **incrementally** update the intermediate representations efficiently? For example, if you have 5000 nodes in your scene graph, and the user changes the color of one triangle from red to green, how can you modify the intermediate trees and drawing instruction data structure without recompiling the entire scene graph? * What is the best way for users to dynamically modify the scene? I've experimented with building the graph using FRP (my current implementation) and also IO callbacks. I have some other strategies to try later on. Basically, how can we make a semantically dynamic tree structure in Haskell without having to litter every node with IORefs for each piece of data? I have some *experimental* code [on github](https://github.com/jdreaver/iris), but the two examples work (a silly 2D scene and a 3D colored cube scene, both with working mouse interaction). The reason I am making this library is because I eventually want to use Haskell at work, where I make GUIs for scientific applications. Also, I have some personal projects that could really benefit from OpenGL visualization, and I want a clean library to make visualizations and plots. Lastly, while this might sound silly, I think that the ability to make flashy programs in Haskell will help increase adoption.
Well, it's technically from Chalmers - I got it from a postdoc in my office who used it for his thesis - but it's not an official Chalmers template. AFAIK the only official templates we have as MS Office ones. I'll ask him to shed some light over the template's origins tomorrow.
Having said that, I don't really use stackage anyway. It seems that I have a knack to chose library which are not on stackage and have to pretty most always use the solver.
Scratch your own itch, then release it to the community.
You can test this by using the `{-# LANGUAGE NoImplicitPrelude #-}` pragma and not importing anything. I don't have the space to check this right now but I'd guess you'd more or less be left with the lambda calculus with a classed hindley-milner type system. It'd be nothing more than a typechecker though because because without importing IO, Show, String etc. you'd not be able to set input/get feedback on what's going on. (If I'm wrong about any of this please say, again I don't have the space to check this moment)
Let's say I have a project on lts-3.8, how do I switch to the latest lts-3? Change the stack.yaml file and issue a new build? And after that how do I nuke the local lts-3.8 packages ?
Right, which is exactly what Stack supports already.
Platform too: https://mail.haskell.org/pipermail/haskell-cafe/2015-December/122432.html
One role LTS could serve is to help us coordinate around which major versions deserve backports of minor fixes.
Ah. I thought I had read at one point (not in the paper) that we'd be trading performance for convenience by using the simpler ffi variant. That said, 15% for the simple case (Ints and Doubles) and an improvement for anything else sounds fantastic! Also, I hadn't gotten to the performance considerations bit of the paper, so I'll have to read through that. 
I seem to understand what you're saying. It's the same as using C++ or other strict language, so you can catch type errors at compile time. Am I correct? Also, what are you using for backend programming with Haskell, I mean libraries :). Also would you mind sharing the most mindblowing haskell learning moments?
I am looking forward to this as well. It is included in the 7.10.3 plan: https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-7.10.3 
&gt; Why would you say it is less extensible or flexible? Visual studio lets you link in plugins written in the language of your choice that can use the API provided by windows or whatever other library. Emacs lets you run scripts written in emacs-lisp. Emacs gives you a language specifically tailored for scripting the IDE, along with a set of option which include _all_ of the methods of customization available for VS. Emacs also has a very mature package management system with a mature set of plugins and scripts. VS package management is terrible. Emacs is cross-platform which gives it major points for flexibility. And, of course, Emacs and its plugins/means are open source which means anyone can modify the core, fork it, examine it, etc. If emacs has a bug or strange behavior, you get online and resolve it, usually within a few days even if it requires changes to the core.The fact that there are so many more scripts, extensions, plugins for emacs as compared to VS pretty much proves the point. &gt; Also Visual Studio starts up way faster than emacs does on this tablet, at least. It takes a fraction of a second for me to start Emacs, it takes a good 10 seconds to start Visual Studio for me, and that is how its been on pretty much all computers and versions I've used. 
As people make counter arguments, there does seem value in periodic checkpoints, just not upgrading those periodic checkpoints by minor increments. I guess having a Stackage monthly/quarterly (just an alias for the nightly on the first of the month) might be valuable.
Why not stick with exactly the same package set as they were using back then? Ideally a specific nightly, e.g. 2012-10-10. If they only want an email changing, doing any package upgrading seems very dangerous.
Thats neat. Really drives my interest towards trying Haskell a bit more. Thanks! :)
Ah, drats! I was hoping to finally link `sdl2`, as it's been broken for me for some time. Thanks a bunch. Guess it's waiting until 8.0 then. :) Has the patch to fix that already been applies in the 8.0 trunk?
The world changes around us. Old package sets might not work with exactly the same packages as back then. That is why people provide backports of important fixes. We need LTS first before that starts happening, so no, the fact that we haven't seen much of that is not a very good indication that it won't happen. Take e.g. the major protocol exploits hitting SSL/TLS every couple of months. You might very well have to update something that previously only spoke, say, SSLv3 to something that speaks at least TLS 1.0 even if your host OS and your own dependencies should remain as close to the original as possible.
the whole tale is in the linked trac bug, above. Short version is that it compiled and linked but then the executable would crash. Ericd linked [this bug](https://ghc.haskell.org/trac/ghc/ticket/9920) as relevant too. 
&gt; Has the patch to fix that already been applies in the 8.0 trunk? [Yes.](https://git.haskell.org/ghc.git/commitdiff/acce37f38bc3867f86cf717694915746bb2f278e)
&gt;It's a horrible representation of strings because it doesn't use Unicode characters It isn't for that, hence the name. &gt;and isn't an unboxed vector of word8s isomorphic to bytestring? Yes, except that Bytestring provides a nice interface to do common tasks while vector is too general to do that. &gt;On top of this the vector library has that sweet sweet fusion built into it. Doesn't Bytestring?
First, a warning and an exhortation: trying to "understand monads" is useless. Please don't even try. You'll learn way faster if you don't think they're important and waste your time on trying to force them into your head. It'd be like insisting you can't learn java until you understand iterators, without knowing what loops are first. Except even more extreme, since it's another step up the abstraction hierarchy. Haskell just has dimensions of abstraction capacity that most other languages don't. Given all that.. From a programmer's point of view, a Haskell program is made of declarations. Declarations can be lots of different things, but the ones you should look at first are function definitions(with their associated type signatures), data declarations, class definitions, and instances. All of those have lots of room for subtlety, but most of most programs is in the function definitions, so they probably deserve the most focus. Within functions, you quickly run into patterns and expressions. They are what drive the evaluation of Haskell programs. There's a lot to say about them, so I'm not going to do you the disservice of trying to summarize it all and leaving something out. The last and biggest thing to learn is how Haskell's type system works. Once you thoroughly understand how to read polymorphic type signatures, you're set. This requires a firm understanding of how pattern matching applies to higher-kinded type variables, why it matters that type constructors are injective, etc. This is by far the toughest thing to learn in Haskell, but it's also the place where you'll learn the most. Just remember why the type system is there. It's not about forcing you to work the compiler's way, it's about figuring out how to shove as much work as you can onto the compiler, reducing as much as possible the size of the minimal amount of code you have to keep in your head to be productive. Well, and the more types dictate behavior of a function, the less room a function has to be wrong. Overall, Haskell is a relatively simple language. The core pieces are far simpler than those of Ruby, for instance. (If you can learn Ruby, you can learn Haskell. I'm not so sure the converse is also true.) The obstacle everyone runs into is that the pieces are actually different from mainstream languages. That's really not true of any language I learned before Haskell. 
Thanks a lot, I read both tutorials. Here is a boiled down short script of something equivalent to what I have to do: http://lpaste.net/146818 The goal is to append to every line with "Start" the label after the next "End: ": Sample input (stdin): Line 1 Start Line 2 Start Line 3 End: Some Label Line 4 Line 6 Start Line 7 End: Other Label Sample output: Line 1 Start: Some Label Line 2 Start: Some Label Line 3 End: Some Label Line 4 Line 6 Start: Other Label Line 7 End: Other Label On seeing "Start", I basically read lines till the next "End", and then output all the lines in one go, rewriting the "Start" lines. I am doing it like I wrote in the parent post, with `over lines id` and squeezing in a pipe after. I could not get it to work with `individually`. Do you have hints as to if this is possible, and if yes, how, and why it is better than the current solution? 
Well then, that certainly makes me inclined to agree with your points about LTS. That sounds quite extreme. I think LTS 4.0 will be the next GHC version, right? Like not even 7.10.3 but, well, 8.0? Even if it were 7.10.3, are we actually saying that LTS will never have significant (non-minor) updates for *any* package until a new GHC version gets to LTS? That seems insane. To be clear, this doesn't mean I have any problem with LTS in principle, but if we have LTS, then the point-versions of LTS should be more inclusive of more package updates or the major LTS versions should not be tied only to GHC versions. Perhaps a two-point level makes sense, like lts-3.5.4 type of thing, where the first point allows significant point-release updates and new packages.
I'm using Haskell to parse and analyze buckets and buckets of SQL.
Note that the haskell platform as of 7.10.2 ships with msys2 tools, just as minghc does and just as stack does. Those tools aren't added to the path by default, and the platform as of yet hasn't shipped with "switcher scripts" to make it super-easy to use them though. Thanks to a patch to cabal (not yet shipped), in the future we should be able to run build-type: configure scripts on windows _without_ needing those msys-tools directly in the path, as setting the proper extra-build-dir and extra-lib-dir in a user's cabal configuration will be sufficient. So this feature will allow an extremely seamless experience for building things like `network` on Windows, and that experience will hold true for `cabal` users and `stack` users alike. Thanks to everyone who has worked on MinGHC for showing the way forward on this problem and demonstrating the sort of progress that was possible.
Already registered! 
wow, this sound really interesting! I always want to do some cool project like this, but somehow never get it done, only ideas :) So good luck to you, keep it going, share somewhere if you'll feel like it, would be interesting to read! (blog posts maybe)
GHCJS is aiming for maximum GHC compatibility, whereas Haste is aiming for maximum GHC compatibility *unless* it significantly impacts performance or code size. More concretely, GHCJS supports weak references and GHC style preemptive multitasking, whereas Haste requires you to use a slightly different API for preemptive multitasking and will not support weak references until the ECMAScript standard gets the primitives to support them natively. Haste also currently has some compatibility warts that GHCJS doesn't have - no Template Haskell support and worse support for pointer-based code (which primarily affects `ByteString` and friends) - which are caused by a lack of time rather than by any real difference between the compilers, and are getting fixed Real Soon Now (tm). If your application needs to be as fast and small as possible, use Haste. If it needs weak references or really fine grained multitasking, use GHCJS. If it needs Template Haskell or `ByteString`s, use GHCJS until the next major Haste release (it's supposed to be fixed by then).
They aren't competing that much, though a combination nix and sytemd containers make docker somewhat less useful. On a modern systemd all the clone and unshare calls can be configured in a normal service file, meaning you'd not even need sytemd-nspawn. Building a docker image from nix is pretty simple (e.g. [1]) so if at some point some neat cloud orchestration stuff (kubernetes without requiring 5 Google instances?) comes along then nix won't be in the way. [1] https://garbas.si/2015/building-aci-images-with-nix.html https://gist.github.com/teh/f3d8532bfef8fb25fe1f 
Thanks for the support! :) I keep extensive personal notes on the project, and I have a blog, so I will flesh out some notes into blog posts once some of the core ideas are more solid.
Yeah, the only reason I care is because of all of the tooling that _uses_ docker, e.g. kubernetes. Thanks!
I just tried this actually. Both times with -O2, and GHC 7.10.2. "memoize3 version" &gt; time ./day9 Just 251 21.33 real 18.30 user 2.78 sys "no memoize" &gt; time ./day9 Just 251 0.14 real 0.12 user 0.00 sys
Oh right, C++ also has `multimap`, which I'd used before and was getting confused with `multiset`. Maybe that's what got me turned around for the rest of the post.
Could a DSL could be made for OpenGL scenegraphs and shaders ?
That was generous. Thanks!
This. There's not much point calling it long-term _support_ if there isn't any support!
Awesome! I will wait for https://ghcformacosx.github.io/ .
Let Joseph Abrahamson get you even more excited about the [Hindley-Milner type system](http://programmers.stackexchange.com/questions/279316/what-exactly-makes-the-haskell-type-system-so-revered-vs-say-java/279362#279362)!
One more strong vote of interest here to stoke your fires! OpenGL through Haskell is a big interest of mine, but I keep pushing it off to learn more of the hundreds of other, lower-hanging fruits in the meantime :)
Thanks!
It mean the type of JSON object value only be `String`
My primary uses all involve IO at some point. Sending/receiving data over the network, saving/loading data from disk, etc. If I want to send Unicode text over the network, it eventually has to be encoded/decoded (typically as utf-8) and then handed to some kernel function. I am not sure about vector, but under the hood, `ByteString` uses `ForeignPtr` so that data is easy to hand off to the OS. I think of `ByteString` as being closer to `CString` than vector. It is something that is typically at a boundary between Haskell and the outside world. 
This is my first attempt of making a blog post. If you are learning Haskell (as I am), then maybe this will be of use. I'm happy to here any suggestions or corrections. 
&gt;They could be removed from nightly, to have it rolling. Can anyone confirm this? 
A perfectly good Reddit app for Android already exists: /r/redditnow
By "remark I made on day 9" you surely mean "... on day 8"?
I do believe this is already possible with `stack ghci --install-ghc`! :D Unless by "0" you mean not even having stack?
Actually I don't exactly agree with having any builder in the "bytestring" package. I'm a strong proponent of a clear isolation of concerns and packages don't make an exception in that regard for me. I believe that packages must have a clear and simple purpose. I'm preparing a post, where I'll explain my position in details.
Yep, I second this. I've been thinking about scene graphs in Haskell as well, but I've been unable to come up with a nice way to mutate them. 
Kinda off topic, but I just so happened to have done exercise from [Haskell WikiBook Traversable](https://en.m.wikibooks.org/wiki/Haskell/Traversable) tonight and got `transpose = getZipList . traverse ZipList`. Though this definition is sorter, it requires `Traversable []` instance along with `ZipList` from `Data.Applicative`.
I recently started building a full stack Haskell web app. A RESTish Haskell server (wai-routes + persistent + postgres), and a JS (React) + Haskell (ghcjs) client. It's been quite pleasant so far!
I'd vote for simply deprecating the current builder from the "bytestring" package. That is, of course, unless it gets proven that it has an edge over the one we're discussing here in any case.
Yeah, I often use that as well! :) Even though, how would you use constraints with that? :)
I recommend stack instead. It's in brewcask if you don't want to download it manually.
You must be the first person that likes JNI! 
great!
 data Dict :: Constraint -&gt; * -&gt; * where Dict :: c a =&gt; a -&gt; Dict c a data FancyGADT :: * -&gt; * where FancyInt :: Int -&gt; FancyGADT Int foo :: Some (Compose (Dict Show) FancyGADT) Something like that, I think should do the trick.
Awesome! I keep looking for a library like [`graphics-drawingcombinators`](https://hackage.haskell.org/package/graphics-drawingcombinators) or [`gloss`](https://hackage.haskell.org/package/gloss) but for 3D graphics. My half-baked idea for lightweight dynamic modification is to expose two functions: translate :: V3 Float -&gt; Scene -&gt; Scene translateIO :: IO (V3 Float) -&gt; Scene -&gt; Scene The latter one pulls a new translation vector on each render.
Yes, a package is removed from nightly if it prevents upgrades of other packages for "too long".
I haven't been benchmarking Haste extensively against PureScript, but "normal" programs produced by PureScript will generally be smaller than those produced by Haste, and probably faster as well, since it avoids a *lot* of thunking and evaluation by being strict rather than lazy. I've seen [microbenchmarks](http://qiita.com/philopon/items/1451f6b8c5ccec41479d) where Haste comes out on top of PureScript by a huge margin (and this was before the 0.5.3 performance overhaul, so the current difference should be even larger), but it's important to note that this kind of code is what Haste does best: tight loops with lots of opportunities for inlining and unboxing. If your code looks like this and has the appropriate strictness annotations, Haste will probably beat PureScript by a fair margin thanks to GHC's optimizer and Haste's comparatively shallow compilation scheme.
This is a very nice expose. I would have done it a little differently with dummy implementations for unwritten functions, so I can start type-checking at once. I don't dare do refactorings without the type checker.
If you're going to count territories don't forget [Jervis Bay](https://en.wikipedia.org/wiki/Jervis_Bay_Territory).
For example: https://github.com/fpco/stackage/issues/981#issuecomment-163122405 (temporary removal of Agda)
The problem with using `show` is `show "test"` would embed the quotes. You would expect: let s = "hello" in [i|${s} world|] to equal `"hello world"`, but instead it would equal `"\"hello\" world"`.
I've used GHCJS + `reflex-dom` once. It's really cool, but I still wouldn't recommend it to a beginner at this point. If you're interested in front-end FRP, elm is a much more gentle introduction.
I open download `stack` manually, and replace the binary `/Applications/ghc-7.10.2.app/Contents/bin/stack`.
[Lambdaya](https://hackage.haskell.org/package/Lambdaya) is my attempt to see how haskell works for embedded projects. It is native haskell code to controll redpitaya (osciloscope/functional generator/logic analyser ..) I am now integrating networking so that one can control device over the network using same code as natively on device. Main problem I focus is how this monad transformer stacks, custom OO like typeclass and pipes works together. I keep getting feeling that code is just too imperative so it has been re-factored multiple times and so far I am progressing slowly but I am trying to put effort into this consistently.
This worked, and reduced the runtime significantly, however the memoize still caused the program to be about half a second slower compared to the same code without it. The list of cities isn't very big so maybe this optimization just isn't worth the overhead.
and making it explicitly clear whether certain sets fit a GHC version, and potentially have some higher bar for testing and reliability potentially… right?
There is also the aspect that e.g. Linux distro packaging teams want a blessed set of packages to work with and it should be the same set for all the distros or your program won't work on all of them.
Exploratory may not be the right word -- what would you call this sort of top down coding? I imagined the pseudocode as a map, and I was just poking at the frontiers, gradually expanding what was defined based on how I used it previously
It's very cool. I've never tried to translate existing algorithms and it seems like it would be particularly resistant to my usual technique.
Actually the first case. those key are create by application customer. application has an feature allowing customer to customize the `User`. e.g. they may add an field to `User` named license which is `List&lt;String&gt;`. 
Good point.
thanks! Trying both of them is on my TODO list.
The standard text editor. Nice. :)
Vectors can be (reasonably) easily shared with C as well.
I understand that Debian now uses Stackage LTS when deciding what version to package from hackage.
Sure. You can build an EDSL for shading. The trick becomes trying to figure out how to balance functionality vs. portability. Building a scene graph has similar trade-offs. You want enough complexity to model the scene you are interested in, while having enough simplicity to be able to do optimizations and efficient evaluation.
Might it be a good idea to start two projects, one for coordinating the writing of file parsers and one for coordinating the writing of API clients? It would be fantastic to have [Awesome](https://github.com/sindresorhus/awesome)-style lists for file parsers and API clients that are missing from the ecosystem. Such projects could also provide beginners some guidance -- there are probably a lot of people that are pretty good with haskell and have interest in writing a FOSS project, but could use advice on specific things that are needed. It would stink to get most of the way through your first real project and then find that it's included in some sub-module of some other existing project.
 data RetryF next where WithRetry :: RetryT m a -&gt; (a -&gt; next) -&gt; RetryF next One problem here is that `RetryT m a` there binds not just `a` but also `m` as an existential. You could modify RetryF to take `m` as an extra type parameter and switch to `FreeT (RetryF m) m` as your monad. This would capture the extra level of dependence on `m` that you have. That said, you have the general problem of 'how do you undo things in a monad you know nothing about?' in the original example you don't have this problem, the computation is pure.
Ugh, yes, thanks, will push the fix along with tomorrow's post. Integer indexing is so hard.
Do you have links about the `Some` type? I’m interested in that. :)
I like the &gt;"it's someone elses problem" approach your paper refers to in one section.
&gt; You could modify RetryF to take m as an extra type parameter and switch to FreeT (RetryF m) m as your monad. This would capture the extra level of dependence on m that you have. Ahh, yes, of course! I had tried to parameterize my GADT on `m` but couldn't figure out the syntax in the use site type signatures. Thank you for the solution. I just tried this for the `RetryT` example and it works great. &gt; That said, you have the general problem of 'how do you undo things in a monad you know nothing about?' in the original example you don't have this problem, the computation is pure. That's an excellent point. I'm not using `RetryT`, though; I only used it as an example to demonstrate the type problem I was having. In the actual DSL I'm using, I need a `WithHandle` command that acts like `withFile` does for `IO`. There's nothing to retry, just a resource that needs to be cleaned up when exceptions occur in the bracketed block. Anyway, I'll go implement this solution in my DSL now, should work fine despite the slightly more ugly type signatures. Thank you so much for the help, and for all the wonderful libraries (including `free`)!
The only problem I have with Servant is the missing standard support for authentication. Is there any example for a servant client with authentication?
It looks like they're actually in the middle of adding it, from their github.
I think servant is a type-level DSL for HTTP so it can use both client and server side or anywhere need to define HTTP API
I'm pretty sure you're remembering a different set of benchmarks. I remember something where Warp outperformed everything years ago, but it was a different website. Anyway, here's the [source to the Yesod version](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Haskell/yesod/bench/src/yesod.hs) if you want to take crack at it.
Obviously we'll just have to use GHCJS and PhoneGap.
Wow this is surprising. I remember seeing this a while back after I started learning Haskell (like Framework Roundup Round 4 or similar), and seeing Yesod and other frameworks towards the top of the list encouraged me to continue learning Haskell. Now it looks like they only have Yesod. Perhaps whoever wrote it did not put much effort into the Haskell version of this round of the benchmark?
Wai was definitely on the top for a while, like top 5.
Maybe because: The tests were run with: GHC 7.4.1 Yesod 1.1.9.2 ( https://github.com/TechEmpower/FrameworkBenchmarks/tree/master/frameworks/Haskell/yesod )
I believe one conclusion I've heard some people draw from that is that those benchmarks are actually more about things like JSON parsing/encoding performance than about the web framework. It would certainly be nice to have a well-optimized Snap entry, but I just don't have the time. I imagine that's a large part of it.
There's a couple of things at work here: * We don't do tests on our i7 hardware anymore. That really compacted the top-end as there was a lot less hardware to take advantage of (this actually has a lot more to do with the fact that the i7 setup used gigabit ethernet instead of the Peak hardware's 10Gb). Haskell looked a lot better here because of that. * Yesod has always been considerably slower than the raw WAI implementation, and there's only a couple tests implemented for WAI. Raw WAI does pretty well in the tests we have implementations for. * The Haskell implementations haven't been touched in quite a while. They're all built with GHC 7.8.3, for example. Other frameworks have been added and updated since then.
The README's never been updated. It's actually currently GHC 7.8.3 and Yesod 1.4.
[Web frameworks benchmark: WAI #3](https://redd.it/346px3)
Backscratcher!? Backscratcher!
I'm working on exactly that as we speak! I'll reply to this thread when I have it working (probably by this weekend). (edit: to be more clear, it's not quite "exactly that," as I'm not doing this for FFI; but I am attempting to use `managed` or `ResourceT` with my `WithHandle` command in my particular DSL. Assuming I'm successful, it should be useful to you, in any case.) (edit2: actually, I think a working `WithHandle` obviates the need for `ResourceT`, if I understand the interface of that monad correctly. I misremembered because I was considering using it when I realized that I could probably implement `with` semantics directly in my DSL. `managed` might still be useful but because it's not parameterized on `MonadIO`, I'm not sure it will stack. We'll see.)
OK, here is a working implementation of 'WithHandle' in my DSL. It's a DSL for performing GPIO on UNIXes. Currently it only supports the Linux sysfs interface and a pure "mock" interface for testing, but the idea is that it could include interpreters for Intel's MRAA, OpenBSD's `gpioctl`, FreeBSD's `gpioctl` (which is, of course, slightly different than OpenBSD's), etc. Here is the Free monad: https://github.com/dhess/gpio/blob/master/src/System/GPIO/Free.hs Here is the "mock" interpreter: https://github.com/dhess/gpio/blob/master/src/System/GPIO/Mock.hs And here is the Linux sysfs interpreter: https://github.com/dhess/gpio/blob/master/src/System/GPIO/Linux/Sysfs.hs Here is an example program which uses the Linux sysfs interpreter (the program itself could be run by any interpreter that can wrap `MonadIO`, however): https://github.com/dhess/gpio/blob/master/examples/Sysfs.hs I'm just getting started and there is still quite a bit of missing functionality (e.g., edge-triggered IO), but it's enough to do the basics. I'm going to try integrating `managed` into my programs this weekend, though as I mentioned above, I'm not sure this will work due to its dependence on `IO`.
Unsafe FFI calls won't work for MySQL as you can end up with a dead lock if some query blocks on locked tables where those tables are locked by a Haskell thread on the same Haskell capability.
People in Haskell-land decided they had better things to do than to constantly invest time in a benchmark I assume. Other communities made different choices.
It'll be in the next version unless we encounter a major problem. In the meantime, you can use something [like this](https://github.com/alpmestan/haskellx-2015/blob/master/code/Auth.hs) with servant-0.4. It shows how one can restrict some part of an API with basic auth. [This repo](https://github.com/arianvp/servant-jwt-example), on the other hand, shows how one will be able to use the JWT (JSON Web Tokens) based auth in servant 0.5.
Most of the reasons have been named in this thread. * Old(er) GHC (7.8.4 and not 7.10.x) * Comparatively under performing database bindings (a lot more work has gone into that for other platforms) * Most commonly used JSON paser/building in Haskell (Aeson) --while being a joy to use-- is not as fast as an optimized C implementation * Yesod not being optimized for speed * Not too many Haskell entries (raw WAI has not been submitted in all categories) * Haskell as a community not too interested in winning this game I want to add two: * The benchmark is not measuring things Haskell is good at: "disk usage of a deployment of app with full stack" (just a binary in Haskell), and "memory consumption during benchmark" (company running the benchmarks is a Java shop, so they might no be too interested) * Quite some new entrants have arrived, that use compiled languages (Go, C, C++); they are all competing in the top tier
I think a pure Haskell implementation of the MySQL client protocol would probably be the way to go if you wanted to avoid all those issues completely.
Probably. But that would involve a non trivial amount of engineering. Plus the MySQL protocol is documented pretty badly.
Ur does seem very interesting! The creator of Ur also posted a Haskell-vs-Ur challenge on the Haskell mailing list a few years ago which unfortunately was largely dismissed (https://mail.haskell.org/pipermail/haskell-cafe/2011-July/094096.html). My impression is that it is not a subjective or superficial difference; the things Ur allows can't be done in Haskell without significant effort/hacks.
Haste has been providing great results for a while now, still community adoption seems lacking. Maybe creating some Gitter channels for the related repos might help? In order to get more devs on board, this thesis is an important resource, providing guidance for example to the use of `Haste.App`, and elaborating about comparisons and design choices. Maybe you could try to convert it to HTML via Pandoc, if it does not take much time, in order to have some useful, linkable documentation available on the web
It seems to me there is easy room for improvement: - use HEAD aeson for JSON encoding - use json-stream for JSON decoding - use blaze-html/lucid for template generation Could be a nice way to write a servant tutorial :)
Canvas with GHCJS/Haste/UHC?
A company. But I've done consulting work in haskell as well, which was an equally good experience.
I really don't get to use Haskell much, so this isn't sour grapes on behalf of my beloved language: Any time I've ever seen a benchmark like this that gets popular, it eventually ends up getting gamed by participants with excessively-specific optimizations. Over time it ceases to be a valid measure of anything real. This benchmark has probably just generally jumped the shark. Which is a pity because I really like the work that has been put into it, it's one of the best of its kind, and all the best to those who created it, but that's just the way it goes with these things. At this point all the measurements are going to be a complicated interaction between the "true" speed of the framework (to the extent that such a thing is even definable) and the amount of effort put in by the relevant community. So, I'm not saying I don't trust the Haskell results specifically, I'm saying I don't trust this benchmark in general much anymore. In a way it's far more useful for this sort of benchmark to be written solely by outsiders, without the relevant communities coming in and tuning them with expert-level knowledge, because the thing more people care about is what you'll get without expert-level knowledge. In fact it would be interesting to run a benchmark where the rules are "I have an implementation, it is the simplest I could possibly make given that I'm a newbie, I have all warnings turned up to max and this is warning-free, no you're not allowed to see it, and the only way to optimize it is either to improve your framework's performance in general OR add diagnostics to the next version of your framework that _detect_ what you think I may have done "wrong" and add more warnings. The idea there being that if someone's using your framework "wrong", but the framework doesn't complain, well... it's a more realistic test that way! And it benefits everyone, not just the benchmark, if you improve your detection of "wrong usage".
I remember wai being in the top-5 last benchmark. This suggests the overhead of yesod is quite high. 
While many are quick to dismiss the importance of benchmarks (at least when they aren't favorable), I would still argue, that as a community, we should care more about what Haskell shows to the public. Some people that migrate to Haskell needs that extra push and stuff like "it has super good performance" is quite a push. Furthermore, for the people trying to sell Haskell to their manager or boss, being able to show results of Haskell frameworks beating a lot of other frameworks, is a very good argument for the non-techy boss. What I'm saying is, that I personally think that dismissing these as "not important" can end up hurting the community and adoption of Haskell. Of course, it still requires people to be willing to optimize the setups, so this is just me voicing my disagreement with the current stance people are taking on this. If I were a bit more advance in Haskell, I'd try and do it myself (hopefully some day).
Just updated it. https://github.com/TechEmpower/FrameworkBenchmarks/pull/1798
I don't think it's so much a case of other frameworks rising above Haskell as it's more frameworks have been added since that are faster. I don't think we had any C or C++ frameworks when the Haskell ones were originally added, for example. And again, the Haskell implementations haven't had any real changes in over a year. That's a lot of time to make improvements.
I think the biggest obstacle to optimisation is theoretical. Almost everything that we as functional programmers think about depends on the concept of composition (you can't even make a category without it), and optimisation *doesn't compose*. I can have a perfectly optimised `f : a -&gt; b` and a perfectly optimised `f' : b -&gt; c`, but yet their composition can be terrible. In fact, even if f is perfect and *composed with itself* it can still be a stupidly bad algorithm! For example, `removeBiggestElement : (Ord a) =&gt; [a] -&gt; [a]` composed with itself. Without something as basic as composition in the mathematical toolbox, I'm not particularly optimistic.
I think for a realistic shot at a sufficiently smart compiler, we need to start with a total functional language. Is there any total functional language with less than full dependant typing? I want something where one doesn't have to write proofs of termination.
Use Idris, turn off the totality checker?
It would be the opposite of what /u/sfultong is asking for.
Simply Typed Lambda Calculus, of course. As to "practical for actual programming but not turing-complete" what comes to mind is datalog (also with extensions), though that's logic programming. If your stuff doesn't fit in EXPTIME it's probably not worth being computed. Datalog gets its termination basically from being very amendable to termination checking, though. The basic thing gets its recursion to terminate by assuming a closed world (meaning that you can enumerate all facts that can be derived from a program, even if recursive), then keeps that by restricting extensions that could lead to inconsistiencies, most prominently you can't have a (mutually) recursive path that goes through a not.
This idea sounds very much like the one behind Red, albeit Red is not purely functional.
It looks like the data is gone before round 9. Someone linked this writeup of previous results though where wai was #3 http://www.infoq.com/news/2015/04/web-frameworks-benchmark-2015?utm_source=infoqEmail&amp;utm_medium=WeeklyNL_EditorialContentOperationsInfrastructure&amp;utm_campaign=04282015news
What's happened to Zero?
[Goodhart's law](https://en.wikipedia.org/wiki/Goodhart's_law): "When a measure becomes a target, it ceases to be a good measure." One interesting option is to benchmark things that are meant to be pedagogical (e.g. TodoMVC). Then optimizing too much would have to be balanced with clarity and simplicity, which is the balance that we want. 
Haven't read the whole thing, but I'm seeing a lot of monads-are-burritos type prose: &gt; Indeed, you can think of ‘Fix’ as defining a program that runs until ‘f’ decides to terminate. In turn, you can think of ‘f’ as an instruction set for the program. The whole shebang of ‘Fix f’ may only terminate if ‘f’ contains a terminating instruction. Another one I see often is "functors are like containers". At least to me, this type of analogy is totally useless. They either imply additional structure, or else miss some other nuance of the abstraction. Why not just talk about what `Fix f` means in terms of `Fix`, `f`, and nothing else?
Yeah to be clear I don't think one *should* think of e.g. `Fix` strictly as a running program parameterized by some instruction set `f`. It's more of a nice example that adds some flavour to the article, I think. People have different opinions on the value of analogy and that's cool. I'm ok with it, obviously.
&gt; MSP allows us to generate code in runtime, link it to the program in a way that the generated code runs in the current execution environment(i.e. the generated code can refer to names in enclosing scope, pretty much like how closures would do). This sounds powerful! As far as I'm aware, MetaOCaml works by constructing the "next stage" syntactically, like a kind of "runtime macros" (or "Template Haskell", as we call it)? My sense is that the holy grail would be a function `compile :: (a -&gt; b) -&gt; (a -&gt; b)` (or maybe it would be in `IO`, I dunno) which would specialize (or partially evaluate, same thing?) the input function according to the stuff stored *in its closure*. So if I write `let f = \x -&gt; x * 3 + 4 in compile f`, that would be a no-op in every sense, because nothing is closed over (you could have written this function in C). But if I write `let f = (+ 4) . (* 3) in compile f`, now we have the function composition function `(.)` partially applied to two arguments, stored in the resulting closure -- the `compile` function would take this and, treating everything in the closure as (from its perspective) a compile-time constant, produce something like the first example, with no closed-over terms remaining. Now this is a trivial example that any of today's compilers could inline and optimize at compile time anyways, but hopefully you can smell the larger possibilities. (Any of the component functions could have come from somewhere else the compiler can't see, for instance as arguments in a HOF, and it would still work.) This would be much more fluid and compositional, letting you create the to-be-optimized programs *as* functions in the host language, instead of the manual quoting, plumbing, and orchestration that (I assume) is necessary in a language with explicit staging. Obviously this couldn't work as-is using the runtime representations of an existing compiler like GHC -- you'd at a minimum have to do something like also store the Core for each function alongside the machine code, so that the `compile` function has something reasonable to work with. But is there any deeper theoretical reason why something like this couldn't, in principle, be possible? 
If you talk about free, could you please take a look at the `streaming` library (https://github.com/michaelt/streaming) I find it quite interesting and intuitive.
System F and System F-omega are also total (all corners of the [lambda cube](https://en.wikipedia.org/wiki/Lambda_cube) are).
Hm. Well, firstly, the way you're caclulating a last digit might be a bit silly. You're literally fully rendering the number to a String and iterating through all of the characters of the string. Why not directly find the last digit with ``(`mod` 10)``? also, `(x ^)` is very different than `(^ x)`. The first one is `(^) x`, with x as the base, and the third one has `x` as the exponent :) Secondly, ghci runs everything unoptimized and uncompiled...so using GHCi to be an indicator of how fast something is might not be the best :) Try compiling it on a file with -O2 and running it and see if it's viable.
Is this "recursion is like `goto`" attitude commonplace? That's news to me. I &lt;3 recursive functions and think once you've grokked them, they're nice to read.
 f .: g = \x y -&gt; f (g x y) I don't get this obsession with writing unreadable code :)
Interesting how you converted the haskell version to look like the python version. Is this haskell version idiomatic?
I think the trick is to have whatever folder has your `.cabal` file open in the left-hand "Project Folders" pane, so that it knows where to run the cabal commands. Either that, or you might need to run `cabal build` once for your project on the command line to get ghc-mod to typecheck it.
Just FYI ghc-mod doesn't even look at any build outputs so `cabal build` will do absolutely nothing.
What is the motivating example for this? - a quick summary would be much appreciated.
That was a really interesting read. I'm convinced this is the best that an automated system could do, and kind of wonder why it hasn't been built already. But does lambda calculus normalization actually equal optimization? There are a bunch of different sorting algorithms, all of which are "equal", yet totally not equal when it comes to actual performance. Isn't forcing normalization here choosing a "blessed" sorting algorithm? And yet the whole point of having multiple sorting algorithms is precisely that there is no "blessed" one. Maybe I specifically chose a sorting algorithm that performs horribly on random data but excellently on "nearly sorted" data because I want to use it for some particle system or something. How could I ever "prove" to the compiler that I "expect" the particles are "usually" in "almost" the same order that they were "last frame"? And yet, that sort of metaknowledge allows me to choose a specific algorithm which can have enormous real-world performance impact.
As I understand it (and I *definitely* don't fully understand it) it can be weighted by the cost of atoms and seeks the optimally best normalized structure from that. I don't know about combining atoms (would probably make it undecidable though) or anything else. As for why it hasn't been done, that was written a year ago. It takes a very long time to go from idea to production level implementation (which is more or less what you need to actually operate on Haskell as defined by what GHC can compile)
[i wonder if rejigging is a technical term](https://github.com/ghc/ghc/commit/6746549772c5cc0ac66c0fce562f297f4d4b80a2#diff-b824c33754b91de696715a82d5048982R502)
Yeah unsafePerformIO in template Haskell can achieve the same affect. People talk about them so much because they are very practical. Though personally I'd rather have my schema arise from my types, not my types from my schema.
I've always had the intuition that whole-program monomorphization was generally tractable. Is it not?
What's Ur's gimmick, by the way? Linear types? Dependent types?
It seems like the motivating example is promoting GADTs to the type level. So just as you could say -- unary nats data Nat = Z | S Nat -- singly-typed (homogenous) list data List a = LNil | LCons a (List a) -- heterogeneous list parameterized by a list of types data HList :: (List *) -&gt; * where HNil :: HList 'LNil HCons :: a -&gt; HList as -&gt; HList ('LCons a as) -- singly typed lists with known length data Vector :: Nat -&gt; a -&gt; * where VNil :: Vector 'Z a VCons :: a -&gt; Vector n a -&gt; Vector ('S n) a (single quotes denoting promoting a constructor to the type level) You can now lift "Vector" to the type level too, allowing -- heterogenously typed vectors parameterized by -- a fixed-length vector of types data HVec :: (Vector n *) -&gt; * where HVNil :: HVec 'VNil HVCons :: a -&gt; HVec as -&gt; HVec ('VCons a as) 
I think .NET programming encourages using Visual Studio quite a bit to the advantage of the programmer. If you've ever used type providers for programming, the initial awe of working with web resources or a sql database doesn't wear off.
Can we now "prove" that the square root of two is irrational, in Haskell? I say "prove" because, well, quodlibet :: forall a. a quodlibet = quodlibet
It's a sort of ugly type f : (x : Int) -&gt; (if x == 0 then Char else String) or something like that.
I'm fooling around with a Servant clone which takes advantage of more kind-level stuff. You can sort of see that as a feature or a disadvantage of Servant as it is that the "Api language" is untyped (all of kind `*`). One place I ran into the lack of kind equalities is that there's no semantics for promoting GADTs which would be useful for specifying HTTP header semantics. I literally have this comment in the code: -- This is a GHC 8 feature. It won't work today because we -- cannot promote a GADT the way that we'd like to. Otherwise, -- we'd let each name specify its "direction". -- -- For instance, we'd have -- -- data HeaderName (d :: Direction) where -- Name :: Symbol -&gt; HeaderName d -- CacheControl :: HeaderName d -- Accept :: HeaderName Request -- Allow :: HeaderName Response -- -- etc. Edit: In case anyone is interested in poking around, here's a plug: https://github.com/tel/serv
Ah, fair enough. Still, we're tiptoeing on the edge of things that are actually being implemented.
I'm not 100% sure on how it would be implemented, but, it would be more like f :: PI Int *Type* -&gt; Int -&gt; *Type*. But not actually that. Because it's new. It's weird and new and practically beyond what I'm accustomed to using, so I can't give any solid useful advice about it, but in principle PI types allow types to be a function of values, so we can have a type function which operates on the type instantiation of 0 and results in Char, and on the type instantiation of 1 and results in String. This is 'possible' (sort of) in current Haskell, but requires far more boilerplate.
In my experience, any time anything ever goes wrong with Haskell and it's not a type error, it's an error code 1. I've never gotten any other kind of error code. Whether this is because I've made the exact same mistake every time, or because Error code 1 is radically overused.... I'm not going to comment. In general, use stack, it "just works".
Fine, ex falso quodlibet. But what are the alternatives? What are the requirements for a self-consistent type system? That some programs NOT have a type? What would they be? Meta-programs, or program specifications, inaccessible to formal proof by some incompleteness theorem? 
I may very well be wrong here; I haven't seen anything but microbenchmarks comparing PureScript to any of the lazy Haskell to JS variants, so it may very well be that it is actually slower for most cases. Both Haste and GHCJS build on top of GHC and so get free access to all of the awesome optimization work that's going on there, which should give both an edge against PureScript. Laziness is *really* costly in JS though, so I would not be surprised to see PureScript coming out on top when it comes to overly lazy programs.
Actually you can't cast `undefined` to *any* type in Haskell right now, only to types of kind *. As for what will happen when * :: *, I haven't the foggiest.
It's Pi, as in the capital Greek letter, and is another vertex on the lambda cube, introducing dependent types. You can read `Pi x.M` as `M` being a type that depends on the value `x`.
&gt; bikeshed war Could you explain what that means?
Capital Sigma (subscript i) introduces addition (Sum (Either)) of things that depend on the index 'i'. Capital Pi (subscript i) introduces multiplication (Product (Tuple)) of things that depend on the index 'i'. The notation of 'tel' above: f : (x : Int) -&gt; (if x == 0 then Char else String) Here you see the output type depends on the index 'x'. And f knows how to be both (so it a Product). The new thing is that it depends on a value, instead of of a type.
That would be a great feature.
Excessive(?) arguing over the *colour* of the bikeshed. In programming language design it usually means arguing over naming and syntax.
And in this case, what to name *
With * :: * (and some additional things) you get Girard's paradox, which means you can make looping programs. But since Haskell already can loop, this is not a big deal. 
Thank you.
Wow, it's amazing to think that this is now possible in Haskell! It's a bit syntactically ugly, but still, the power is there. For some reason, the words of Viserys Targaryen spring to mind: "When they write the history of my reign, they will say it began tonight."
I'm using it, though I'm not doing professional development with haskell. Here's my [configuration for neovim](https://gist.github.com/emptyflask/78c611c73b02d27ad960).
I'm using it. Equipped with [`neomake`](https://github.com/benekastah/neomake), [`vim-hdevtools`](https://github.com/bitc/vim-hdevtools) and [`haskell-vim`](https://github.com/neovimhaskell/haskell-vim) it's pretty awesome (asynchronous checking and linting thanks to `neomake`!) Also, the integrated terminal emulator is useful. I often use a horizontally split window with a terminal on right.
I'm also using neovim with neomake for hlint! It's really awesome!
Lol I meant your version of things. Ambiguity! 
Thanks. How do manage to make neomake to work ?
I get an error message when I run `:Neomake`. Error detected while processing function &lt;SNR&gt;10_NeomakeCommand..neomake#Make..neomake#MakeJob..&lt;SNR&gt;37_JobStart: Probably a problem in my installation :-( 
I don't think it is possible now. The proof of irrationality of sqrt(2) requires quite a bit of technology (do a case split on even/odd, there is no infinite descending sequence of naturals, cancellation by 2...). Maybe there is a way to encode all of that complexity in GHC's type system, but I don't think this patch has suddenly made this possible.
As /u/mstksg pointed out, you can use mod 10 to get the last digit. Also, I'm not exactly sure about the maths here, but it might be that you can get your result using (2^200 `mod` 10) ^ (2^300 `mod` 10) `mod` 10 ... or some variation thereof. Modular arithmetic is the key word here. I'm actually pretty confident that a * b mod c can also be computed as (a mod c) * (b mod c) mod c. This would indicate that at least (2^200 `mod` 10) ^ (2^300) `mod` 10 would work. (I'm always implicitly assuming a low priority for infix mod applications.) Edit: I just tried the last way I suggested, and the ^(2^300) still kills it. Even two-digit exponents there take a long time to compute. But first, lets find out why applying modulo to the first argument should work. We're gonna look at (a * b) mod c and proove that it's actuall ((a mod c) * b) mod c - of course you can flip the operands of * and apply the rule again to get ((a mod c) * (b mod c) mod c. a*b can be expressed as (a' + a'' * c) * b with 0 &lt;= a' &lt; c. (I'm assuming natural numbers and zero here, but it works with any integer). If we multiply the term in parenthesis, we get (a' * b + a'' * c * b) - taking the module, we get (a' * b + a'' * c * b) mod c. We can obviously spread out the modulo over the sum (like a+b mod c == (a mod c + b mod c) mod c ), and we get ((a' * b mod c) + (a'' * c * b mod c) mod c. Since anything times c mod c is zero, we get: (a' * b) mod c mod c, or a' * b mod c. a' has been defined exactly like the a mod c to begin with, so there we have it: a * b mod c = (a mod c) * b mod c. ^ is just repeated multiplication, so we can apply the mod c to at least the first operand. Provably works. Now... That's evidently still too much to compute for your hardware, even though the mathematical problem isn't all that bad. The built-in integer (\^) probably uses theorems like x\^(2*k) = (x\^2)\^k, and x\^(1+2k) = x * (x\^2)\^k which make large exponents a lot easier to compute. If we want to speed up our exponentiation for modular arithmetic, we can do that with a homebrew version. Implement (\^') such that whenever we simplify a exponential term, we apply our modulo on the left side and afterwards. x\^' (2k) mod c = (x\^2 mod c)\^'k mod c, and similarly for uneven exponents. PS: There could be unescaped \^'s somewhere around here. I didn't intend to use \^ as a markup syntax element.
Is KitchenSink really a thing? 
Umm, no, it's denotes "whatever it takes to make this typecheck". I removed it now anyway. 
I think I kinda understand this, but can I have an stateful widget contain another stateful widget?
Bear in mind that `_|_` is not just "error", "undefined" and the like, it also includes non-termination. For example, let f x = if x then () else f x in f False This is bottom, even though GHC's loop detector doesn't detect it. If you try it in GHCi it will hang, waiting for a result that never comes. A language which doesn't have bottom values, therefore, is a language in which all functions terminate. Such languages exist (I think Agda is one) and they are not Turing-complete.
I am. Even wrote a plugin to take advantage of the term functionality and have navigatable and automatically updating compilation and test errors with Simon Hengel's [sensei](https://github.com/hspec/sensei) (plugin [here](https://github.com/jkarni/sensei-neovim)), but getting it to work on a variety of differently configured projects (nix, cabal, stack...) was a little annoying.
I really like this package https://hackage.haskell.org/package/hmatrix which sits atop BLAS and LAPACK especially the Static module which catches e.g. mis-matched matrix multiplication at compile time. I thought numpy also handled higher-ranked structures with a nice way of slicing. We have repa for this but then we have to convert from one type to another. Someone has pointed out that this can be avoided and I hope to follow up on this soon.
 (f .: g) x y = f (g x y)
Better title might be "haskell was useless"
So much thank you. This solves a problem that I've spent a fair amount of time trying to get past.
From what I remember spacemacs doesn't work, or doesn't work well in daemon/client mode.
Why people repeat once and once and once ... the same wrong conclusion "Haskell is useless" from one quick sketch and not conclude "C, C++, C#, Java, ... are unsafe"? Why? (I've my own hypothesis but I prefer not to share...). In that context, Haskell is as useless as unsafe are C, C++, C#, Java, ... to use... What do you prefer? Not to move at all or move to a total disaster... ;P
I wonder if there is a solution like the following: height (l,h,r) x | x &lt; l = 0 | x &gt; r = 0 | otherwise = h build = foldr (maxf . height) (const 0) maxf f g x = max (f x) (g x) clean = map head . groupBy ((==) `on` snd) skyline i = clean $ zip ps $ flip map ps $ build i where ps = sort $ i &gt;&gt;= \(l,_,r) -&gt; [l,r] That doesn't depend on adding arbitrary offsets like so: skyline i = clean $ zip (map floor ps) $ flip map ps $ build i where ps = sort $ i &gt;&gt;= \(l,_,r) -&gt; [l,l+0.5,r,r+0.5] 
Right, but that is basically because of this issue. There is something that looks like a proof system in Haskell, for a particular logic, if you squint a bit. I'm very interested in the way changes to the type system make this logic more expressive.
Repost of a link, that was posted first 3 years ago, of a 4 year old reupload of a video, that was originally recorded 8 years ago. It checks out, "Haskell is useless" is the dank meme of the Haskell language. 
I think it's telling that Erik and Simon who both respect and admire each other's work are a constant source of inspiration for each other, but those who end up consuming their works (C#, Haskell) insist on drawing this hard dichotomy where each individual's contributions are foolish and wrong-minded. The entire point of the video was "We're all trying to help each other meet at the crossroads of safety and productivity" and somehow the only takeaway by the vast majority of people is that "Haskell is useless" or "C++/C# are unsafe"
Wouldn't this still import instances?
Yes. It's possible to nest foldp calls...
I think I can help, but I don't have a good understanding of what you are trying to accomplish. What is the aim of `ckr &amp; ckrsKeyMetadata . _Just .~ (^.kmKeyId)` ?
Folks that are used to an editor that opens instantaneously develop workflows that rely on an editor that opens instantaneously. Folks that are used to an editor that opens very slowly develop a workflow that doesn't involve closing out of the editor very much. I open vim dozens of times whenever I'm working on something, so the slow startup time with emacs/spacemacs is a pretty big blocker. It's enough that I'm trying to learn a whole new editor, but my entire CLI workflow has to go too?
I think this should work: ckr ^. ckrsKeyMetadata &amp; _Just %~ view kmKeyId Alternatively, you can just `fmap` into the Maybe container: ckr ^. ckrsKeyMetadata &lt;&amp;&gt; view kmKeyId The result of both has type `Maybe String`: Just "text" == (Just (0,"text"), 0) ^. _1 &lt;&amp;&gt; view _2 In case you need to use a different prism: Just "text" == (Left (0,"text"),0) ^? (_1._Left) &lt;&amp;&gt; view _2 P.S. Another cool trick that I've recently encountered is using the `Foldable` instances of `Maybe` and `Either`. The following are all `False`: elem 2 $ Nothing elem 3 $ Just 2 any even $ Just 5 any id $ Left True elem 3 $ Right 2 The idea is that `Maybe` and `Either` are containers of 0 or 1 value. `Bool`-valued folds return `False` for `Nothing` and `Left _`.
Thank you! Between yours and @newestHaskeller's answers, I've got plenty to think about. I'm just glad that I am now unblocked.
Monomorphisation is mostly doable for Haskell, but not everywhere since Haskell has polymorphic recursion and higher ranked types. 
Hot damn those are gorgeous 
We have just such a function in our system, `compile :: a -&gt; a`. The idea is not new, check out Andrew Appel's Reopening Closures paper. 
Thank you!
Are HKTs incompatible with monomorphization? And if so do you know if there are workarounds? 
The natural way to make a producer of lines is something like rechunked :: Monad m =&gt; FreeT (Producer ByteString m) m r -&gt; Producer ByteString m r rechunked = folds (&lt;&gt;) mempty id then rechunked . view PB.lines :: Monad m =&gt; Producer ByteString m r -&gt; Producer ByteString m r This of course will accumulate indefinitely long lines. I think `over lines id` may look like it works at first, because it breaks lines; but it doesn't glue lines together where there is a bytestring break in the middle of them, as can happen with eg `fromHandle` - i.e. it doesn't accumulate, which is what you need.
It sounds like you're saying Haskell is only for mathematicians
Here is a follow up on that video: [https://www.youtube.com/watch?v=xmjvOLlCdFU](https://www.youtube.com/watch?v=xmjvOLlCdFU)
Yes. 
Does it include any bindings for less conventional more internal clang code like generating LLVM IR for a C++ call to a function? Not useful for anyone except those wanting to write compilers interfacing with C++ though so I don't expect the answer to be yes. It's really cool regardless, good for editors and all sorts of tooling.
Nice job! I think the colors are a bit off on the first three, the last two are gorgeous.
I also rather like `scanl1` and `scanr1` in this scenario: scanl1 :: Traversable t =&gt; (a -&gt; a -&gt; a) -&gt; t a -&gt; t a scanl1 f = snd . mapAccumL go Nothing where go Nothing b = (Just b, b) go (Just a) b = let a' = f a b in (Just a', a') scanr1 :: Traversable t =&gt; (a -&gt; a -&gt; a) -&gt; t a -&gt; t a scanr1 f = snd . mapAccumR go Nothing where go Nothing b = (Just b, b) go (Just a) b = let a' = f b a in (Just a', a')
Gotta love Haskell :) Still, there must have been a reason for the way that the scans were originally defined. Convenience? It is kind of nice for them to be endomorphic: import Data.List partialSums = scanl (+) 0 x :: [[Int]] x = iterate partialSums [1..10]
This is why inlining is such a powerful optimization: it helps optimize composed code. Inlining is probably the single most important optimization that GHC does even though, by itself, it rarely has more than a moderate effect. Why? Because it makes local optimizations global. It lets the compiler optimize code made of pieces from distant locations as if they were next to each other—by putting them next to each other.
Heh, that's what I do already by placing `source ~/.vim/vimrc` in `~/.vimrc`.
It's pretty similar to how I'd write it, for what that's worth.
License(s) for these?
Could you make versions with transparent logo and without outlines?
il put them on deviant, thanks for the heads up.
I'm late to this post, but I'm glad to learn that you've also discovered this "design pattern", and found some serious use for it. I had reported [a bug in GHC](https://ghc.haskell.org/trac/ghc/ticket/11067) related to type families used as superclasses, and S.P.J. considered outlawing them altogether because he hadn't seen a good use for them. I'm adding your package as an example.
Imagur was encoding down so I moved them to deviant
no license. I just decided to haskellize some nice wallpapers.
il give it a shot later
I don't even really know haskell but wow these are wonderful! Why can't someone make equally beautiful lisp wallpapers?
Could always go crazy and do `unfoldM $ (&lt;$) &lt;*&gt; guard . ("quit" /=) &lt;$&gt; getLine` if you're feeling line stingy.
Use emacs daemon and emacslient. Problem solved.
I was just trying to poke some fun at our friend above "why can't someone.."; /u/Cono52 your initiative and work is remarkable and I thank you for this, like everyone else should
woops, sorry haha , a friend liked them and suggested I should share them. 
Ah..., I can get rid of the `Maybe` by puling the same trick I did with `scanr1` in the original post: scanr1,scanl1 :: Traversable t =&gt; (c -&gt; c -&gt; c) -&gt; t c -&gt; t c scanr1 f = snd . mapAccumR (\a b -&gt; let c = a b in (\b -&gt; f b c, c)) id scanl1 f = snd . mapAccumL (\a b -&gt; let c = a b in (\b -&gt; f c b, c)) id And adapting this technique, rather than losing the *final* element of the accumulator, I can lose the *initial* value of the accumulator, which is probably more useful, as that's necessarily already known: scanr_ :: Traversable t =&gt; (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; t b scanr_ f b = snd . mapAccumR (\g a -&gt; let b = g a in (\a -&gt; f a b, b)) (\a -&gt; f a b) scanl_ :: Traversable t =&gt; (b -&gt; a -&gt; b) -&gt; b -&gt; t a -&gt; t b scanl_ f b = snd . mapAccumL (\g a -&gt; let b = g a in (\a -&gt; f b a, b)) (\a -&gt; f b a) 
&gt; Why can't someone make equally beautiful lisp wallpapers? Because have you _seen_ Lisp's logo? ;)
So you don't care about copyright?
I checked some of the licenses but if you're able to find the others i will gladly take them down and keep these wallpapers to myself :)
Please don't read this. This is a trolling of your Alice in wonderland tale. You have been warned. A better title would be "haskell is more useless than ever" I see that the self filtering mechanism of the haskell community is evolving towards the more and more useless terrain, trying arduously to make the already steeply learning curve for Haskell larger and higher, creating poorly engineered software by newbies that adopt fiercely the first fashionable thing that they just learned, abandoning the functional principles to abuse type safety and calling this functional programming, devoting enormous efforts to trivialities. In a certain way, Haskell has become the caricature of himself, not a playground for fresh ideas, some of them with a good balance of practicity and innovation, but a type safe platform for doing whatever can be done in other languages, but with extra worthless complications. In the other side Haskell is an attraction for aestheticists and mathy people enclosed in their mathematical boxes, that don't want neither can create anything really new and useful, because they can not think out of the box, All that they can produce is nice weblog entries. However not everything is worthless. Nothing is worthless. Everithing in Haskell has some aspect that is admirable (we are on Christmas, you know...). But is the whole picture what makes me sad. I expected ten years ago that Haskell would be widely used in 2015, but I see that Haskell is going in the opposite direction and I see the haskell community happy with that. 
Since some seem to like them I leave for now however in future I will carry out rigorous searches into the licensing of such images if I ever decide to make a desktop wallpaper ever again :|. Here is a youtube link showing some of the effects that are caused by people ignoring licenses https://www.youtube.com/watch?v=t7tA3NNKF0Q
Haskell, the language with the power of Nature and the New Age. I miss a wallpaper of Oprah with a haskell logo in his forehead
I like purple Haskell stuff.
Nice! Thanks for the reference, it was an interesting read. I was honestly expecting someone to point out what obvious thing I had missed that makes this all impossible, so this was a pleasant surprise. In this case I should modify my line of inquiry: what advantages and limitations have you experienced of this approach? I hear a lot more about explicit staging than this (which I had never heard of before) - is there any way in which this is less powerful than explicit staging (a la MetaOCaml)? How come this technique is not more commonly used? One technical difficulty which occurred to me after posting my previous comment is that functions are one thing, but data terms also have a different representation in the compiler (an AST of a literal), vs. at runtime (a graph of pointers, primitives, and whatnot). You'd either need to equip the `compile` function to understand the runtime representation, or translate them back into AST form (similarly to Template Haskell's [`Lift`](https://hackage.haskell.org/package/th-lift-0.7.5/docs/Language-Haskell-TH-Lift.html)). Have you encountered any notable difficulty in doing this? Which approach do you take? (As an aside, did you really mean to write `compile :: a -&gt; a`? I can see how that could work in a lazy language where all values are actually computations, but I thought your dialect of Haskell was strict?)
So what would you like the Haskell community to do?
I have submitted an issue on your behalf. https://github.com/chetant/LibClang/issues/59
Yep, I realized I was brute forcing this, but wanted to see why it wouldn't compile. Still working on the full solution.
Yes, I meant to write `compile :: a -&gt; a`. For atomic types it is just the identity function. The main use is when the argument is a function, but an IO value is also interesting. Restricting compile to just functions is unnecessary, even if it's uninteresting for most types. 
So, exhaustiveness checking hasn't been fixed [yet]? Or was it just not mentioned here? If not, is it going to be? --- What I'm talking about, if it isn't clear: {-# OPTIONS_GHC -Wall #-} {-# LANGUAGE PatternSynonyms #-} module A where data A = A (Maybe Int) pattern A1 = A Nothing pattern A2 x = A (Just x) foo A1 = 0 foo (A2 x) = x + 1 Here, `foo` is clearly exhaustive. But GHC can't see that: Pattern match(es) are non-exhaustive In an equation for ‘foo’: Patterns not matched: _ If A is abstract I can see the reasoning for this, but if not, I would like GHC to be able to ‘see through’ the definitions of pattern synonyms as well. Unless there's a reason I'm not aware of why it doesn't. --- edit: btw, this blog post has the required and provided constraints swapped (or the manual does).
Hmm, well that sounds like bad news for that morte project then. I can think of examples easily enough, but they're pretty obviously contrived (mostly just... arithmetic on big naturals, stuff that at first-glance is obviously designed to behave badly). Do you think the project is overly optimistic then?
Have you tried typing `fd` at the same time ?
It's probably worth noting that you can get things building sooner if you stub things out with undefined or error.
Hmm, yes, like these wallpapers: http://imgur.com/a/DRdNi
I was thinking that too and assume the library probably predated `Proxy`.
&gt; We have to wrap it in a newtype to hide the higher-rank type and go monomorphic Isn't `Ord a =&gt; [a] -&gt; [a]` only rank 1?
How does this relate to [view patterns](https://ghc.haskell.org/trac/ghc/wiki/ViewPatterns)? Are they now obsolete? It seems to me that both extensions aim to do the same thing, i.e. making pattern-matching more readable. However, because view patterns can be arbitrary functions, they seem to be more powerful.
`Vector` has `stream`/`unstream`. Could those be used in place of `toList`/`fromList`? --- `NonEmpty a = a :| [a]` is isomorphic to `(a,[a])`. In case the Traversable container is a list, `NonEmpty` is more informative than a mere tuple. For other `Traversable`s, would it make sense to generalize `NonEmpty`? NonEmpty a = forall t. Traversable t =&gt; a :| t a
My wording was probably poor. The real issue is that we don't have impredicative types http://jozefg.bitbucket.org/posts/2014-12-23-impredicative.html but we can use a higher-rank type instead.
 data Void type family NotRoot2 (a :: Nat) (b :: Nat) :: ((a * a) == (2 * (S b) * (S b))) -&gt; Void -- ...
Neat trick! Just for fun, here it is in point-free: import Control.Monad import Data.Traversable aux = (.) . join . fmap (,) -- aux f g a = let b = g a in (f b,b) aux2 = (=&lt;&lt;) (.) . (.aux) -- aux2 f g a = f (aux g) (g a) snd' = ((snd.).) -- snd' f a b = snd $ f a b scanl_ = snd' . aux2 mapAccumL scanr_ = snd' . aux2 mapAccumR . flip What's the problem? :)
uuuhhhh.... I see. Yes. There's [this](http://ergoemacs.org/emacs/lisp_logo/LISP_logo_big.png), or [this](https://upload.wikimedia.org/wikipedia/commons/7/78/Lisp-logo.jpg). At least you can tell the language has a heritage when you see logos like that.
The two are complementary. You can defined pattern synonyms which include view patterns. For example, we can defined a pattern which only matches even integers as follows. pattern Even :: Int pattern Even &lt;- (even -&gt; True) 
I think it's far too optimistic to think you can always deal with normal forms. To be practical you need some escape hatch to avoid normalizing the bad cases. IMO, being strongly normalizing is a red herring (unless we are talking proofs, then it's essential). I've expressed this view about Morte before. But I think it's a fun and cool experiment, that is definitely worth trying. 
That I agree with. Thanks for the fix.
I'm not sure, just intuition really. Do you have a reason to think it wouldn't help?
The first example in the post reads: &gt; pattern MyJust :: a -&gt; Just a pattern MyJust a = Just a This is a typo; the type signature should be `pattern MyJust :: a -&gt; Maybe a` of course.
View patterns are a special syntax for matching against the result of a function call. They are a very small bit of syntax sugar and require user buy-in. Pattern synonyms are a feature which is much deeper and allows you to, effectively, create constructors decoupled from the type definition. The resulting "constructor" looks just like a real constructor and can be matched in a syntacticly transparent way. It also allows you (in the bidirectional case) to use the synonym as a constructor. View patterns are necessary for some interesting uses of pattern synonyms; after all, you need a pattern to be the synonym of, and view patterns is the feature that allows function calls in patterns. Consider for example [Data.Sequence.ViewL](https://hackage.haskell.org/package/containers-0.5.6.3/docs/Data-Sequence.html#t:ViewL). This is what view patterns were made for. Unfortunately, working with them is a little tedious; I'd rather the views be transparent, so that I could use `:&lt;` to match on a `Seq` directly. With type synonyms and view patterns, I'm pretty sure we can get this (but I don't know right now how).
At the very least, that should be `putStrLn` not `print`, because `print` will print out the quotes surrounding the string and escape any quotes in it as per the show representation of the String, printing `"helloBob \"The Fixer\" Anderson"` for example. I would want to see something more like `main = putStrLn . ("Hello " ++) . (++"!") =&lt;&lt; getLine` or perhaps `main = mapM_ putStr . ("Hello ":) . (:["!\n"]) =&lt;&lt; getLine` if you don't want to encourage the use of concatenation as in (++"!").
That first one is pretty great.
What would it take to get type providers in Haskell?
I think you've just provided a good example of when to use `scanl1` and `scanr1`: λ let partialSums = scanl (+) 0 λ mapM_ print $ take 10 $ iterate partialSums [1..10] [1,2,3,4,5,6,7,8,9,10] [0,1,3,6,10,15,21,28,36,45,55] [0,0,1,4,10,20,35,56,84,120,165,220] [0,0,0,1,5,15,35,70,126,210,330,495,715] [0,0,0,0,1,6,21,56,126,252,462,792,1287,2002] [0,0,0,0,0,1,7,28,84,210,462,924,1716,3003,5005] [0,0,0,0,0,0,1,8,36,120,330,792,1716,3432,6435,11440] [0,0,0,0,0,0,0,1,9,45,165,495,1287,3003,6435,12870,24310] [0,0,0,0,0,0,0,0,1,10,55,220,715,2002,5005,11440,24310,48620] [0,0,0,0,0,0,0,0,0,1,11,66,286,1001,3003,8008,19448,43758,92378] λ let partialSums1 = scanl1 (+) λ mapM_ print $ take 10 $ iterate partialSums1 [1..10] [1,2,3,4,5,6,7,8,9,10] [1,3,6,10,15,21,28,36,45,55] [1,4,10,20,35,56,84,120,165,220] [1,5,15,35,70,126,210,330,495,715] [1,6,21,56,126,252,462,792,1287,2002] [1,7,28,84,210,462,924,1716,3003,5005] [1,8,36,120,330,792,1716,3432,6435,11440] [1,9,45,165,495,1287,3003,6435,12870,24310] [1,10,55,220,715,2002,5005,11440,24310,48620] [1,11,66,286,1001,3003,8008,19448,43758,92378] [and those can be made endomorphic for `Traversable`s](https://www.reddit.com/r/haskell/comments/3wkdn8/foldable_scans/cxx7ejq) 
Does stack install nix in the docker container? Could it?
Sometimes, it is appropriate for the list to grow: difference = zipWith (-) =&lt;&lt; tail integral = scanl (+) 0 -- [0..20] == integral . difference $ [0..20] 
Except the golfing in `whileM_`, I like it :)
Thanks for the exercise. Here's how I would collect input lines: import Control.Applicative import Control.Monad import Data.Function (fix) unfoldM :: (Monad m, Alternative f) =&gt; m (Maybe a) -&gt; m (f a) unfoldM mx = fix $ \go -&gt; mx &gt;&gt;= maybe (return empty) (\x -&gt; do xs &lt;- go return $ pure x &lt;|&gt; xs) notQuit :: MonadPlus m =&gt; String -&gt; m String notQuit = mfilter ("quit" /=) . return readLinesUntilQuit :: IO [String] readLinesUntilQuit = unfoldM $ notQuit &lt;$&gt; getLine
This is nothing against you, but you response illustrate perfectly a form of the disease of the Haskell community: A need to exhibit gratuitous complications, an obsession for worthless details, lack of focus, dismissal of real world problems, hostility to newcomers in the form of exposing gratuitous intricacies and creating artificial barriers hidden behind a mask of willingness to help
Born in an age when neon lighting was cool and futuristic. 
I can think of one advantage with a total language, namely that more program transformations are valid (both call-by-name and call-by-value ones). But to preserve complexity you still have to be very careful with what transformations you do. Did you have some other advantages in mind? 
`hint` is linked statically by default just like other libraries, so it doesn't require GHC. It's pretty big, I tried this a while ago and I got ~70 MB executables. UPX could get it down to 15 MB though.
I made one as well: http://imgur.com/Cwg0O6V
Oh, you know what, I may have confused hint with the plugins library. I apologize. If hint does not require an installation of ghc, then it's actually perfect.
I also gave spacemacs an honest to god try a month ago and I had to put it down. I intend to use my editor for as much as I can, but in languages other than haskell it does really dumb things, like mixing tabs and spaces. There's always glitchiness and I find myself trying to script around it constantly. I could get around most of that but there are a ton of great modes in emacs in which spacemacs evil style bindings conflict with their operation, like magit for example uses a lot of keys that evil uses, so you have to override everything with custom settings. Also elisp is fragile, every time I update packages I have to cross my fingers and hope nothing breaks. At the same time I'm not willing to use it with its default keybindings, it conflicts with a lot of desktop stuff that I have set up. I'm just going to have to hope neovim overtakes vim and gets plugin support (which it seems to be doing, very gradually).
If you do this, then when I go to build your software I will try to use an LTS release that I already have built everything against, so that I can avoid building the whole world just to get one program. If you don't use the LTS release, then that's less likely to work, and I'll have to build the whole world anyway, sadly. Basically, using LTS will save hundreds of gigs of disk space, hours of cpu time, tons of CO2 emissions, etc..
I have some spare time, but no real idea of what would be involved and an extremely deep seated and irrational fear of offering to do something I'm not absolutely sure I can do successfully.
It would be great if someone could clarify what is actually involved in this. Is it just making haddock build with GHC-8.0, or are there specific extensions (like injective type families) that haddock doesn't currently render correctly. If so, it would be nice to know exactly which ones.
Author here. Happy to answer questions or get feedback. Part one [showed up here](https://www.reddit.com/r/haskell/comments/30xugp/highperformance_log_parsing_in_haskell_part_one/) a while back. 
Thanks for the response - by any chance, did you get a name or anything to refer to, regarding the LaTeX template? 
I guess the task is to make `ghc-head` equal to `master` &gt; https://github.com/haskell/haddock/tree/ghc-head &gt; (This branch is 77 commits ahead, 266 commits behind master) My guess we need to merge master into ghc-head and back.
In short, the task is to make `haddock`'s `master` branch build against GHC's `master` branch. Haddock depends upon the GHC API and therefore must be kept up-to-date as GHC evolves. This is done in the Haddock's `ghc-head` branch which sadly has not been kept up-to-date with Haddock's `master` branch. Essentially what is necessary is to rebase `haddock`'s `ghc-head` onto `master`. This afternoon Matthew Pickering indicated that he has started on the task. Thanks Matthew!
Have you been on http://functionalworks.com/ ? 
Thanks, that cleared up a lot of stuff for me.
Maybe some sort of `{-# COVERING #-}` pragma like the `MINIMAL` one that explains when (combinations of) pattern synonyms fully cover their type?
&gt; code generation is strict what does that mean? 
I've been using neovim for almost a year now. It does crash/segfault sometimes.
[removed]
I like how you made an alt-account just to insult me.
What do you think about compiler magic that says "whenever you infer `(Semiapplicative f, Pointed f) =&gt; c`, replace it with `Applicative f =&gt; c`"? Also, when do you actually want `(Semiapplicative f, Pointed f)` as opposed to `Applicative f`?
[removed]
Depending on what jurisdiction the author is in, he might not need to ask for takedown (e.g. by means of a DMCA notice) before suing you for lost royalties. In some jurisdictions (like Germany), a formal request for takedown might even contain a claim of reimbursement for the money spent on writing the notice (which you have to pay). Do not assume well-intent from third parties. Doing so is naïve and can get you in deep shit.
I'm really happy that you made these wallpapers, they look great, but we live in a real world with laws you have to obey. Just because you made them just for fun doesn't mean that you can ignore copyright law. I'm writing all of this to protect *you,* not for my personal enjoyment or for my great sense of righteousness.
[removed]
This is getting out of hand, they are just some silly wallpapers, i'm just a college student who started Haskell a few months ago, I'm aware of copyright law and soon in some countries they are bringing out a law that you have to have a license to even take a picture of something you own. Its all kinda silly in this particular instance to be going on about a desktop wallpaper like its a huge deal where i might be jailed an sued for whatever, just say the word and il take them down
I'm really sorry for overasking, but I have trouble understanding this concept. Suppose you create a dynamic list of those ON/OFF buttons. That is, instead of "5", you have a value that changes dynamically (say, depends on an input field where you type in). Since, every time you input a different number, you have to re-render the whole view, how do Flare knows that button #2 of the old view is the button #2 of the new view? What if the number increases, does it know that the new button has the default state? If it shrinks to size=4 then grows to size=5, will the 5th button have the last state it had, or the default state (because it is new now?)
&gt; Of course it makes sense that numerical packages have Vector Double in their function parameters; but it means we have to cast our Price into Double and only then pass it into the mean function, and we have to think about casting the result Double back into Price. This doesn't completely solve your issue, but you can substantially reduce the number of casts you have to make with `Data.Coerce.coerce`*: import Data.Coerce (coerce) import Data.Vector (Vector) import qualified Statistics.Sample as S (mean) newtype Amount = MkAmount Double mean :: Vector Double -&gt; Double mean = S.mean amountMean :: Vector Amount -&gt; Amount amountMean = coerce mean ^*Provided ^the ^casts ^you're ^making ^are ^well-roled.
This is a good, performant solution. The problem with the general signature proposed in the gist (`(Foldable f, Fractional a) =&gt; f a -&gt; a`) is that it is much slower than `Vector Double` when the `Vector` is unboxed (as it almost assuredly is in a numerical package). If you just have a small-ish `[Price]` or whatever, maybe this doesn't matter too much, but it's an unacceptable performance hit for the kind of numerical applications these libraries were designed for. Still, having very general signatures like `(Foldable f, Fractional a) =&gt; f a -&gt; a` is quite useful, as they're generally applicable in a lot of cases. `Control.Lens.Iso.under` is a useful example of how to do this, and it might even completely solve your problem (though I'm unsure of this).
I'm assuming that everything TH can generate will be generated before continuing compilation.
Using something like `(Storable a) =&gt; Vector a -&gt; a` would make it more general, and things like that are done for example in `hmatrix`. Further generalizing is complicated since the `Storable` constraint does not allow for a `Foldable` instance to be made. In my opinion, the way to go to solve that is what is done in `SubHask` and its `Concrete` subcategories or something similar.
&gt; data (==) :: forall (a :: Type). a -&gt; a -&gt; Type where &gt; Refl :: x == x Where do the `x`s come from?
I feel there is a flaw here at the very start, when making the basic assumption that newtypes should allow you to perform lots and lots of operations on the wrapped value. I have always seen newtypes as the opposite, no operations by default and a carefully selected set of operations you can perform, namely only those that make sense for the semantics of your new type, which can be very few if you use your numeric type as e.g. an ID (pretty much no arithmetic), a poor man's refined type (e.g. limiting the range from 1 to 6 for representing dice rolls or from 1 to 8 to represent rows on a gameboard) and even where you do want arithmetic you often do not want to allow all operations in unrestricted ways.
It got out of hand as soon as you started taking this idiot seriously, ignore him.
I would say that action would come from the fact that it can act over external sources. A group act is too restrictive as it always need to have an inverse, and this is certainly not the case for IO actions. IO is modelled with monads, and you may have heard the phrase "a monad is just a monoid in the category of endofunctors". Since a monoid is like a pre-group where inverses do not necessarily exist, I think that is the closest you can get to a relation between both concepts. It is worth noting also that we can speak about "monoid actions" and "semigroup actions". Take a look at modern dynamical systems books and you will see this a lot.
[That's exactly what #8779 proposes](https://ghc.haskell.org/trac/ghc/ticket/8779).
In the same situation - if there is any grunt work, I'm more than happy to help out, but I doubt I can do any of the super advanced stuff, if there is any :)
Okay, I'm no expert but I'll try. First, consider the non-church verison. newtype Parser a = Parser { runParser :: ByteString -&gt; Maybe (a, ByteString) } The bind (`&gt;&gt;=`) implementation for it is: Parser f &gt;&gt;= k = Parser $ \b1 -&gt; case f b1 of Nothing -&gt; Nothing Just (a, b2) -&gt; runParser (k a) b2 For readability, I'll refer to its non-newtype version for the rest of this comment. type Parser a = ByteString -&gt; Maybe (a, ByteString) (&gt;&gt;=) :: Parser a -&gt; (a -&gt; Parser b) -&gt; Parser b f &gt;&gt;= k = \b1 -&gt; case f b1 of Nothing -&gt; Nothing Just (a, b2) -&gt; (k a) b2 Note that `&gt;&gt;=` is left associative. That is, ma &gt;&gt;= mb &gt;&gt;= mc &gt;&gt;= md = ((ma &gt;&gt;= mb) &gt;&gt;= mc) &gt;&gt;= md To get an intuition of why that could be slower, first consider what happens if you replace `&gt;&gt;=` with `++`. = ((ma ++ mb) ++ mc) ++ md That's expensive because you traverse the list so far for each `++`. That's not quite what will happen with `&gt;&gt;=` in a Maybe monad, but it's worth mentioning to maybe get a better intuition of what's going on. So back to `&gt;&gt;=`, ma &gt;&gt;= mb &gt;&gt;= mc = (ma &gt;&gt;= mb) &gt;&gt;= mc = (\b1 -&gt; case ma b1 of Nothing -&gt; Nothing Just (a, b2) -&gt; (mb a) b2) &gt;&gt;= mc = \b0 -&gt; case (\b1 -&gt; case ma b1 of ...) b0 of Nothing -&gt; Nothing Just (b, b3) -&gt; (mc b) b3 You end up checking whether the left was `Nothing` or not for *every* `&gt;&gt;=`. Compare that with the church version. newtype Parser a = Parser { runParser :: forall r. ByteString -&gt; (String -&gt; r) -- failure -&gt; (ByteString -&gt; a -&gt; r) -- success -&gt; r } The `&gt;&gt;=` definition here is (assuming I didn't get anything wrong): Parser f &gt;&gt;= k = Parser $ \b1 kf ks -&gt; f b1 kf $ \b2 a -&gt; runParser (k a) b2 kf ks Again, I'll get rid of the newtypes. type Parser a = forall r. ByteString -&gt; (String -&gt; r) -&gt; (ByteString -&gt; a -&gt; r) -&gt; r (&gt;&gt;=) :: Parser a -&gt; (a -&gt; Parser b) -&gt; Parser b f &gt;&gt;= k = \b1 kf ks -&gt; f b1 kf (\b2 a -&gt; (k a) b2 kf ks) Now going back to our earlier expansion: ma &gt;&gt;= mb &gt;&gt;= mc = (ma &gt;&gt;= mb) &gt;&gt;= mc = (\b1 kf ksb -&gt; ma b1 kf (\b2 a -&gt; mb a b2 kf ksb)) &gt;&gt;= mc = \b0 kf ksc -&gt; (\b1 kf ksb -&gt; ma b1 kf (\b2 a -&gt; mb a b2 kf ksb)) b0 kf (\b3 b -&gt; mc b b3 kf ksc) = \b0 kf ksc -&gt; ma b0 kf (\b2 a -&gt; mb a b2 kf (\b3 b -&gt; mc b b3 kf ksc)) That's not very undesrstandable. Here's a (maybe) more understandable version of the final form: getA = ma :: Parser a buildB = mb :: a -&gt; Parser b buildC = mc :: b -&gt; Parser c bs1, bs2, bs3 :: ByteString parsingFailed :: String -&gt; r parsingSucceeded :: c -&gt; r ma &gt;&gt;= mb &gt;&gt;= mc = getA &gt;&gt;= buildB &gt;&gt;= buildC = \bs1 parsingFailed parsingSucceeded -&gt; getA bs1 parsingFailed -- Try to parse A, if that fails, call parsingFailed, otherwise, -- call: (\bs2 a -&gt; buildB bs2 parsingFailed -- Try to parse B, given A. If it fails, call -- parsingFailed, otherwise: (\bs3 b -&gt; -- Try to parse C, given B. If it fails, call -- parsingFailed, otherwise, parsingSucceeded. buildC b bs3 parsingFailed parsingSucceeded)) So `ma &gt;&gt;= mb &gt;&gt;= mc` is a bit cheaper to construct in this case because we don't have to do a `case` comparison on each `&gt;&gt;=`. Instead, it's just a simple function application. ----- Disclaimer: I probably made a mistake or said something wrong up there, so expers, feel free to correct me. But in general, this is the gist of why it can be faster to use the church-like version.
Learn both. They are at opposite extremes to each other and learning both will yield the broadest knowledge of high level languages. After all they are just tools and knowing the benefits they bring will allow you to select the right one for the problem at hand.
Ah I will get it a try! I thought that it might have advantages because you can start streaming before you get the whole file read in memory, but really didn't expect it to be worth it in the end. I will try to produce a new post benchmarking your code then!
Learn Haskell and use it like any other language. You will love Haskell. I love Haskell. Something very different is the Haskell community, which is pathologically inclined to make haskell complicated and useless. They will try to indoctrinate you in complicated nonsenses like in order to write "hello world" [you have to do some .... contortions](https://www.reddit.com/r/haskell/comments/3wk8yy/simon_peyton_jones_haskell_is_useless/cxyh0td?context=3). Compared with that kind of "help", a Java snippet made by an OOP maniac would look like a miracle of clarity and conciseness. Others will mess you in the back with category theory, wich is for a haskell programmer like a hardware designer being messed with boolean logic. Nothing that would make you a better haskell programmer. They don't want to help you. They simply want to show you how superior they are and how much you have to learn to be like them. For them Haskell is as good for their self-esteem as much as there are less people capable to use it. Just don't care. Find a friend who really want to teach haskell to you. 
As you mentioned, I found some math shortcuts, which I updated the post with. 
Yeah, the string approach wasn't the best idea. I eventually found a math shortcut, (since it turned out to be more of a math problem, than a haskell one), where raising the last digit of the base number, to the last digit of the exponent (with exceptions and other details I updated the post with) will give you a number whose last digit is the same as the last digit of the original base and exponent.
I never quite understood why "Storable does not allow Foldable instances". Could you expand on this plz? It's extremely relevant to my current work.
This program isn't correct, but I suppose this is part of a larger one. This one should work: module Main where import Data.Aeson import qualified Data.ByteString.Lazy as B main :: IO () main = do jsonDate &lt;- decode &lt;$&gt; B.readFile "/tmp/foo" print (jsonDate :: Maybe Value) 
Can you just clarify whether you mean that it would be a performance hit because the machine would have to convert the unboxed vector to a different data structure before doing the fold operation on it? 
I wrote tryhaskell 6 years ago firstly aimed at programmers, then slightly aimed at non-programmers. I've learned/realised a lot since then about what excites non-programmers and what is merely unnecessary friction. If I were writing a Haskell teaching tool today I'd do something different. As a community I think more people have been thinking about better teaching materials. Yesterday my partner who has zero technical background whatsoever tried out tryhaskell. She found it quite fun and "addictive", but stumbled on the slides that were too formal and wordy (e.g. talking about the syntax of "let var = exp in body" -- a big no-no). The more slides that looked like lectures and less like interrogative/socratic interactions/engagements, the more she lost confidence. Things like syntactic sugar were not clear (like the fact that `'a'` and `"a"` are different, but `"ab" is 'a' : 'b' : []` is downright `wat`-ish), nor the use of operators like `(:)`. Or why the function `fst` was written like that, and `toUpper` is not just `to upper` (to which I answer: programmers). Also, starting out with numbers to demonstrate functions (like `map (+1) [1..5]`) bored her. It seems that even Learn-You-A-Haskell starts out like this: http://learnyouahaskell.com/starting-out The REPL was pretty intuitive, and I think [playgrounds](http://haskellformac.com/) or [a spreadsheet-like UI](http://chrisdone.com/haskell-spreadsheet-prototype/) would also work nicely. Syntax wasn't a big problem, like balancing "" and [] and () and such, but still I'd think an editor like Lamdu would prove ultimately to be the most comfortable learning experience for first-time programmers.
I would argue that, with this thesis, Haste is better documented than GHCJS, yet somehow there is a gap between what is offered and its adoption. Having some channels for the community maybe could help to let some questions emerge. Personally, i can't help but wondering: if GHCJS will manage to keep its promises, will there still be an use case for Haste? If the answer is no, why should i invest in learning how to use it?
I'm in first year on CS degree in University, I have 5 classes this semester, 3 are math, 1 is programming (functional programming) and the other is useless. But as you see the first language I've learned is Haskell, and I love it, it's challenging first but then it's smooth sailing.
I know this is kind of off topic, but can someone help me updating the GHCI without having to download the file to install the update on OS X? It would be great to have something like in Linux 'sudo apt-get update'
That was my impression too - kind of a crippled ad-hoc Functor with a slightly more hipster name. I could be wrong though.
Is there some reason you chose to parse the month names with a Map lookup instead of a parser? Or was it just to show another way as opposed to the HTTP method parsing of the previous version?
[`ncurses`](https://hackage.haskell.org/package/ncurses)
Learn both, and then some. You cannot master programming by learning only one. Every language you learn will teach you something new, and that something is going to be a useful tool on your belt.
Edit: doh, I see /u/Faucelme has said exactly what I wanted. Oh well, good to see more people playing with `IterT`! `untilJust` is also part of `Control.Monad.Trans.Iter`. The nice thing here is that you can then use even more combinators to fine-tune the retrying strategy: main = do putStrLn "Password?" res &lt;- retract $ cutoff 3 $ untilJust checkPassword case res of Nothing -&gt; putStrLn "No correct password after 3 attempts!" Just _ -&gt; putStrLn "Welcome!" checkPassword = runMaybeT (expectSecret &lt;|&gt; deny) where expectSecret = do password &lt;- liftIO getLine guard (password == "secret") fail = do liftIO (putStrLn "Wrong password!") mzero I've also factored `checkPassword` out and used `runMaybeT` for a slightly nicer implementation to address the oddity that /u/theonlycosmonaut commented on.
Any class restriction stops instances. The definition of Functor can be seen as: class Functor f where fmap :: forall a b. (a -&gt; b) -&gt; f a -&gt; f b Since, for example, storable vectors have the `Storable` restriction in `a` and `b`, they cannot satify this `fmap`. The same is valable for other classes.
You can find bindings for a lot of GUI engines like GTK, Qt, wxWidgets, etc... If you want how people are exploring functional ways of doing GUIs, take a look at `reactive-banana`, which is a FRP framework for working with other engines. 
... is there a Haskell implementation that algorithm anywhere?
I dunno, but with that whole immutable streams thing, it sounds like there should be
just hoogled it, there's a fst package: https://hackage.haskell.org/package/fst but it depends on base == 4.* :|
Or: toMaybe f a = guard (f a) $&gt; a
I think the concepts needed to get to a productive understanding are not any worse or more abstract than any other language. The stumbling block to haskell for new programmers will be the error messages, which read as "precise" for advanced developers, but also as extremely long and jargon-y to beginners. 
The one you provided is literally just `b -&gt; [a]` on a church encoded list forall r . (r -&gt; a -&gt; r) -&gt; (r -&gt; b -&gt; r) -- flip b -&gt; (r -&gt; a -&gt; r) -&gt; r -&gt; r -- flip b -&gt; (a -&gt; r -&gt; r) -&gt; r -&gt; r -- church decode b -&gt; [a] so yeah, it's a little silly in a language with laziness and stream fusion. It's more interesting in one with (less) laziness and less opportunity to cut out intermediate data structures, though. You can expand this model by replacing each Reducer with a Fold data Fold i o where Fold :: s -&gt; (i -&gt; s -&gt; s) -&gt; (s -&gt; o) type Transducer a b = forall r . Fold b r -&gt; Fold a r and now you have access to "fold-time state" allowing you to write `take` (note that the Clojure version requires spinning up an `atom` in the middle of the "transduction"). To really go feature-for-feature with Clojure you need to add an early termination feature. Clojurists will try to say that there's something clever going on here, but it really just means augmenting the `Fold` to use `Either` to recognize a "finished" fold from an "unfinished" one. If you want to go the final leg of the journey, jam all that together then add a monad in the middle for effects. I'd like to see someone reimplement [`tesser`](https://github.com/aphyr/tesser) atop this technology.
`Functor f`, actually. [Defined in Data.Functor](https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-Functor.html).
Ah, that's why I didn't find it, I looked in `Control.Applicative`. Thanks!
I like the top YouTube comment &gt; Three gods in one place? Is this real life? 
I'd had issues with my prompt size getting miscalculated when I used ANSI escapes in my GHCi prompt; specifically when I went back in my history to a command wider than the terminal, my prompt would be the wrong size, leading to issues with editing the command. I found this ticket while googling, which pointed out the user error - instead of doing -- show loaded modules in gray on line above a "λ" prompt in cyan :set prompt "\ESC[37m%s\n\ESC[0;36mλ \ESC[m" -- use a "⋮" prompt in cyan for :{ ... :} :set prompt2 "\ESC[0;36m⋮ \ESC[m" I should clearly delineate each ansi escape with SOH and STX escapes so that they're ignored when calculating the width of the prompt: -- show loaded modules in gray on line above a "λ" prompt in cyan :set prompt "\SOH\ESC[37m\STX%s\n\SOH\ESC[0;36m\STXλ \SOH\ESC[m\STX" -- use a "⋮" prompt in cyan for :{ ... :} :set prompt2 "\SOH\ESC[0;36m\STX⋮ \SOH\ESC[m\STX" 
&gt; ...And are everithing but programmers willing to do something useful with the language out of mental masturbation and exhibitionism. Nah, there's some fairly practical Haskell software in the wild. Pandoc, for example, is pretty amazing, and it's not mental masturbation by any metric. &gt; Your response-conference-master-thesis illustrates some flaws of the haskell community: the willingness to introduce questions not asked for. Believe me: Whoever ask for "hello world" do not ask about how a monad works. No, but when you state that you want to learn a programming language, "give me exactly what I need to do the exact thing I'm trying to do here, and absolutely no other information" is a lousy attitude. People who ask for "Hello, world" will get the code snippet I posted. People who ask how it works will get a suitable answer. But I get the impression that my style of writing somehow aggravates you; if that is so, then maybe the root of the problem is really a cultural one, that is, proper punctuation and mildly nontrivial sentence structures scare you and make you all defensive. If that is so, then I have the slight suspicion that you will not enjoy Haskell much, but by all means, give it a try and see for yourself.
I have been programming in Haskell since ten years ago at least. I know CT and all the stuff. You do not aggravate me, my friend. It is the willingness of the haskell community to make Haskell difficult at all costs what aggravates me. If I weren't an haskeller I wouldn't give a damn
Idk, there are certainly a few haskellers who endulge in intellectual wankery, but that's not the general impression I get from the community. People are frighteningly smart sometimes, but most of them are among the most helpful and friendly people I've ever met on the internet, and the general tone of discussion is very civilized. Idk, just hang out on ##php and #haskell on freenode, throw in a controversial or even mildly offensive opinion, and taste the difference.
This sounds like a pretty good mechanism for learning more about monad transformers!
This is essential reading for library writers. Thank you for sharing it
That library doesn't exist, and it cannot possibly, at least not without cheating and violating assumptions left and right. `State` is not a monad that provides mutable variables; it is a monad that threads extra arguments through chained computations, i.e., it abstracts away function arguments and tupled return values. Of course you could wrap something like Redis state in a monadic API, and that would actually be a viable approach, but you would have to go through `IO` at some point, and the only way of doing that is to already be in`IO`, or else you will have to cheat and use `unsafePerformIO` (which you don't want, trust me). This means that your monadic Redis API will probably have a type variable with a `MonadIO` constraint on it somewhere, or just use `IO` directly.
Oh I see, fold using an associative binary. Very nice. I guess my only concern would be that lookups in a map are log n, whereas this is linear. But then again, that same argument could be made against my HTTP method parsing. Edit: it would be worth benchmarking your version, which I like a little more from a consistency perspective. It's quite possible that even over multi-GB files the performance difference is negligible. If I get some time I'll take a look. 
Considering the mention of monad transformers, I doubt the goal is to recreate exactly the State type except hide IO in the operations by bad magic. Having something vaguely like `DistantStateT s m a = DST {distantGet :: DistantStateT s m a, distantPut :: s -&gt; DistantStateT s m (), runDistantState :: IO (m a)}` that's an instance of MonadState would be fine, and probably handy even. 
counter examples are always great. One of my favorite topology book is Steen and Seeberg's Counter examples in topology. Arguably it's even more important there, because there are so many slightly different definitions that are (through lens of euclidean space) intuitively so similar.
Thanks a lot for this. It was very informative.
The usual approach isn't brute force :)
I don't think `Const k a = Const k` is a good counterexample for an apply, since you can just take the first or second `Const a &lt;*&gt; Const b = Const a` or vice-versa. It's unreasonable imo to make this instance but it is a possible instance. More reasonable would be to give it a Semigroup constraint for Apply and a Monoid constraint for Applicative/Monad.
Thanks for writing this up. Very useful.
This might be interesting to you: http://michaelxavier.net/posts/2014-04-27-Cool-Idea-Free-Monads-for-Testing-Redis-Calls.html
While I do appreciate the series, I wonder whether either the source of the C program should be released (and maybe improved by people with knowledge on high-performant C) or the whole comparison with the C program should be excluded. It doesn't feel honest.
Yes, runtime specialization works (our compiler+rts does that too), but that's that's a lot of heavy machinery.
It's not clear to me what makes your code "high performance." What techniques are you applying to make your log-parsing code "high performance"?
The C program can certainly be optimized (obviously, as a good C program should not be slower than a good Haskell program). I decided to include its performance as I actually used it as a baseline when I wrote the actualy program. It should be seen as a sort of milestone in the series, and not as a kind of "look Haskell is faster than C!" strawman. I think the post reeks of my satisfaction at beating it though ... I can't release it (it is not mine). I just checked it and realized it does something terrible for parsing the dates (it parses the tokens, `sprintf` them with a known format and runs `strptime`). I gave the command line that generates the file listing at the beginning of the series, so it is completely possible to write a competing C implementation if you feel so inclined. I would be happy to update the material.
It seems like you wrote that: decode fmap B.readFile "/tmp/foo" It should be: fmap decode (B.readFile "/tmp/foo") People use `&lt;$&gt;` because it works well with the Applicative operators (especially `&lt;*&gt;`). Then I got used to it and feel it is clearer than `fmap` in many cases. I understand that the operator soup is confusing to beginners though.
Author here. This is still a bit of a work in progress, and I'm not sure I've found the best counterexamples in all cases. I'd be interested to hear comments or suggestions to fill in any gaps.
&gt;Another example is given by `Set`, in which the type argument appears covariantly, but where the type class laws fail to hold. Can anyone ELI5?
Perhaps, the discussion at [How to implement actions in ST-monad with my own underlying representation (similarly to STRef or STArray) using simple techniques?](http://stackoverflow.com/q/28768904/94687) can be useful when thinking over such things.
It does work now. It seems I was misinterpreting &lt;$&gt; in every example I came across so of course it didn't work. Thanks a lot. Is there someplace where I could read about use of &lt;$&gt;? I don't really know what to look for.
Look into your favorite learning resource for "applicative functors" or "applicative" ([for example LYAH](http://learnyouahaskell.com/functors-applicative-functors-and-monoids)). Good luck!
 Prelude Data.Set&gt; fmap even [1, 2, 3, 4, 5] [False,True,False,True,False] Prelude Data.Set&gt; Data.Set.map even (fromList [1, 2, 3, 4, 5]) fromList [False,True] `fmap` is supposed to preserve shape, but mapping a `Set` does not. (I believe you couldn't even write `instance Functor Set` to do this because `fmap` has no constraints while `Data.Set.map` has to require an `Ord` constraint, but even if you could, the laws wouldn't hold.)
[This SO post](http://stackoverflow.com/questions/19177125/sets-functors-and-eq-confusion) goes into some detail, but I'm actually going to remove that comment from the blog post, because I think it's more confusing than helpful.
&gt; To really go feature-for-feature with Clojure you need to add an early termination feature. You also need a feature to allow for local state in the transducer http://clojure.org/transducers#toc10 &gt; Some transducers (such as take, partition, etc) require state during the reduction process. This state is created each time the transducible process applies the transducer. For example, consider the dedupe transducer that collapses a series of duplicate values into a single value. This transducer must remember the previous value to determine whether the current value should be passed on I'm not sure how this is accomplished with a simple fold or church encoding in the types used so far in the haskell representation of transducers.
&gt; but even if you could [write an `instance Functor Set`], the laws wouldn't hold. Are you sure? I think they would, provided all the types involved had law-abiding `Ord` instances. (The laws for `Ord` should include at least `x == y` implies `f x == f y` for all `f`.)
I would claim that the `Functor` laws are not broken by your counterexample; instead, the `Ord` laws are broken: specifically, your proposed `Eq` instance for `t2` does not satisfy the law "for all `f`, `x == y` implies `f x == f y`". If you impose this law -- and perhaps the usual laws on `Ord` like transitivity and stuff -- I would bet the `Functor` laws for `Set` are upheld just fine.
The `Fold` representation allows for that. You can slip extra state into the hidden `s` variable inside of the `Fold` and treat it purely like normal.
Or a higher level API: [brick](http://hackage.haskell.org/package/brick). Jonathan does an amazing job documenting his libraries and providing examples.
I'm not sure I can fully explain why it can't be a `Functor` though. Yes, the `Set.map` function does not make `Set` a `Functor` for type reasons, but that doesn't mean it can't be a functor otherwise. I think there are more instructive counterexamples, but I'll be glad to add it back if there is a really compelling explanation.
Do you mean &gt; Any Alt can be used to construct a Semigroup (but not necessarily a Monoid) using &lt;|&gt;. ? I'm just getting at the fact that `&lt;|&gt;` forms a semigroup, since it is associative, not that the `Semigroup` instance for `f a` is necessarily that particular one (I think `Maybe` is a counterexample here, yes?)
&gt; Some people, when confronted with this example, propose that the problem is that Eq should require that x == y if and only if no function can be written that distinguishes x and y, but others insist that this condition is excessively stringent, and that the laws for Eq should just be that == is an equivalence relation. Well, whether `Eq` can be any equivalence relation or has to separate points, the `Ord` docs state that `Ord` must define a total order: &gt; The Ord class is used for totally ordered datatypes.
Well, yes, if you have a `run` function that introduces IO, then it can work lawfully without bleeding IO into all methods. It's just that IO needs to come in somewhere.
As a matter of illustration, this is how it would be in [hplayground](https://github.com/agocorona/hplayground): import Haste.HPlay.View main= runBody $ do button "Ask if it's alive" `pass` OnClick r &lt;- ajax GET "/alive" [] wprint r I will finish the porting of it to GHCJS in a few weeks
Is there a reason to prefer that `.travis.yml` over what is generated by [multi-travis-ghc](https://github.com/hvr/multi-ghc-travis) Regarding stack/stackage. I personally don't use the services but would like to know what I need to do to make life easy for those that do. 
Yeah, it seems so much easier to use that and not think about platform compatibility or anything.
There's an overview of the GUI situation at http://www.haskellforall.com/2015/08/state-of-haskell-ecosystem-august-2015.html#standalone-gui-applications
Yeah, but now we're talking about more thought than I felt was warranted for a single line example that was left intentionally incomplete. I'm finding the more I think about this the more I want it.
&gt; Is there a reason to prefer that .travis.yml over what is generated by multi-travis-ghc? No particular reason. It's just using different tools and providing slightly different guarantees. Arguably multi-travis-ghc is a better choice because it forces you to build against "hackage HEAD" rather than the stabler platform that is stackage. There are some nuances to how each approach does caching that make the stackage approach arguably better, but that is merely a slight advantage at best. &gt; what [do] I need to do to make life easy for those that [use stack/stackage]? Stackage is a service that provides a set of package-versions that are known to build together and (with a few manually-acknowledged exceptions) pass their test suites. Stack is a tool that can (and does by default) make use of the Stackage service. There are two ways to make life easier for those who use stack/stackage. * for users of your package: make sure your stuff builds against the stackage snapshots that your users are interested in. The stackage service only maintains the latest LTS snapshot, but some people use older snapshots (such as the LTS 2 series for ghc 7.8 support). * for contributors to your package: you can include and maintain a `stack.yaml` file in your repository; this is basically a glorified `cabal.freeze` file that tells stack which dependencies to use. This allows contributors to merely clone the repo, run `stack test`, and stack will take care of dependencies and things so that your contributors can focus on the edits they want to make to your project. So ultimately it all depends on how you want to cater to your users and contributors, and what their needs and desires are. Stack and Stackage are meant to provide convenience to *you*, so really you shouldn't have to do much to cater to that crowd. If you ever feel like stack/stackage are getting in your way or adding an extra maintenance burden on you, then let's talk, because that's the opposite of what it is supposed to be.
I feel like I already get all this through Nix. Plus I can use nixops to deploy to the cloud. And can use hydra for CI. Plus nix can automatically use a build farm, and those assets get shared among all developers automatically. That is to say.. if someone commits a new change to git, and builds it on their dev box -- it will automatically get built on the build farm and shared with all other developers. Nifty stuff. I know that stack works fine with Nix, but I am less clear what additional benefits I am getting over nix + cabal sandbox.
16:55 &gt; We started thinking that it was Traversable that we should be targeting, Foldable seemed to be more constrained than we needed. This doesn't make sense to me, Foldable is more general than Traversable.
If I really want my packages to be supported, then don't I also need to get them added to some build-constraints.yaml file on github? Also, is the stack.yaml something that I can autogenerate? And then I need to regenerate it sometimes when I update the .cabal file? As a Nix user I supposedly benefit by having my packages in stackage because nix automatically imports the stackage lts releases. It is my feeling that if my package is not on github and stackage, then it might as well not exist. People will assume that the package is obsolete, or not well maintained otherwise. 
I'm skeptical if the type-level `TShow` could be as expressive as the term-level `show`. The awkward part comes when you have to deal with nested types. For example, how would this work? type instance TShow (Maybe a) = ??? You'd need to come up with the `Symbol` for `Maybe` as well as the `Symbol` for `a`, and then find some way to smash them together. AFAIK, you can't concatenate type-level `Symbol`s (as they're not really strings).
wow, there's even a user guide! https://github.com/jtdaugherty/brick/blob/master/docs/guide.rst
&gt; If I really want my packages to be supported, then don't I also need to get them added to some build-constraints.yaml file on github? Doh, yes. Sorry I forgot to mention this. Another way of supporting users of your packages is for your packages to be a part of stackage. People who use `stack` can still use your stuff whether it's on stackage or not, it's just more convenient for them if it's on there. See the (recently updated!) [stackage doc for maintainers](https://github.com/fpco/stackage/blob/master/MAINTAINERS.md). &gt; Also, is the stack.yaml something that I can autogenerate? Yes! `stack init` usually does the trick. &gt; And then I need to regenerate it sometimes when I update the .cabal file? Situational; it depends on what's changing in your project. Usually all you'll want/need to do to keep your `stack.yaml` current is bump the `resolver` field from e.g. `lts-3.14` to `lts-3.15`. &gt; It is my feeling that if my package is not on github and stackage, then it might as well not exist. Being on stackage is a signal that your package is maintained. Package authors should think of stackage as a service that recommends dependency versions for you to support, and alerts you when your package doesn't support them. If you want to support more than what's recommended, great. If you don't want to support (at least) the stackage-recommended set of dependencies, then it's up to you to either try and negotiate with the stackage curators or just not be part of stackage. There's some give and take, but ultimately we're all just trying to negotiate the proper support of packages in the Haskell ecosystem. As a stackage curator myself, I hope that stackage is an attractive and convenient service for both package maintainers and their users.
The other part of this is that a reasonable Set a can only exist if Ord a, so not all functions would be fmapable. If you take the view that decidable equality should exist for all types and should be "sensible" then a less efficient set implementation that does not rely on ordering should have a lawful functor instance.
I'd be curious to see if you can incorporate [this technique](http://neilmitchell.blogspot.co.uk/2015/09/detecting-space-leaks.html?m=1) for detecting space leaks in CI.
... because we'd like to be informed when passing a Quantity where a Price was expected!
Part of the appeal of stack is automatic dependency version selection and automatic dependency management. I presume there are some simple scripts one could write using nix + cabal sandbox to accomplish the latter. As for the former, using just cabal for dependency version selection is fine as long as the authors of your dependencies maintain good version bounds. Stackage shields the user from packages with bad version bounds. If a cabal user's install goes wrong, they must hand-pick dependencies that work together by specifying constraints. With stackage, the hand-picking and specifying constraints for these special cases has already been done for any package that is part of stackage. Stack and Stackage are for people who want to say "I want to use libraries A, B, C, X, Y, Z, and I want them to be reasonably up-to-date, and I want them to just work without having to think about it." If you'd rather do a bit more of the hand-picking yourself then maybe stack isn't for you, which is fine. If you want to support users that use stack then hopefully this sheds a little light on their mindset and why it is convenient for them if your packages are part of stackage. Of course Stack does more things beyond the scope of how it uses the Stackage service, I just focused on that aspect since that's what you're asking about. p.s. I'd love to see stack support the nix-based workflow. I have no idea what the current state of that is, but I think nix is an excellent tool for this job.
Since the top-voted comment warns that such a "library doesn't exist, and it cannot possibly, at least not without cheating and violating assumptions left and right", I should probably mention that my implementation does not read and write to the disk on every `get` and `put`. Rather, it provides a function withPersistentState :: (Read s, Show s, Eq s) =&gt; FilePath -&gt; s -&gt; State s a -&gt; IO a which reads from the disk, lets the `State` computation transform it, and writes the result back to the disk. A `put` followed by a `get` always returns the value which was `put`, even if some external process modified the file in the interim.
Indeed, that's why I gave it a poly-kinded type. 
A limited form of something like this (available only in error messages) has been implemented in GHC as part of the [support for user-defined type errors](https://phabricator.haskell.org/D1236).
&gt; fmap is supposed to preserve shape, but mapping a Set does not. Is `fmap` supposed just to follow the functor law ? I don't see any breach of the law there.
Is the `Ord` constraint just an implementation details ? In theory only the `Eq` constraint is necessary, isn't it ?
My understanding is that either `Ord` or `Hashable` or something else extra is required if you want a practically efficient implementation, but `Eq` is the minimum required for a mathematical notion of a set.
That takes a `Maybe a`, though, not an `a`. You need a `return`: toMaybe p = mfilter p . return
[This stack overflow question](http://stackoverflow.com/questions/7220436/good-examples-of-not-a-functor-functor-applicative-monad) has some more nice examples.
The title goes back to part one, which talks about how we moved to Haskell from Python for log parsing and some of the huge performance benefits we saw from that. The gist of all this being, "if you've been using a dynamically-typed scripting language to parse logs, but want to see high performance and less memory usage while retaining the advantages of writing high-level code, consider Haskell. Now here's how you do that..." If this post was written for a Haskell audience (like the folks on this Reddit) I would probably have just called it "Log Parsing in Haskell" and it would have been a lot shorter ;-) I actually have looked into a couple of things that have the potential to improve performance, including an implementation using stream-fustion and another using conduit. As of yet though, neither of those have yielded any measurable performance benefit. I have a branch with the conduit-based parser that I'd like to keep banging on though. I'm going to do some profiling and see where I need to focus, but even if that doesn't make it any faster, I'm tempted to keep it because it allowed me remove code and makes some things much easier to express. 
If you want to get something out of the set, it seems like you need `Ord` or something like it.
Yes. (This is the trick used in the works on categorical semantics of GADTs: by restricting yourself to a subcategory where all arrows are equalities, you can easily talk about equalities between types in a categorical way.)
Well, I think Functors are *supposed * to preserve the shape and that's the intention of the functor law `fmap f . fmap g = fmap (f .g )`. If f is inversible and g = f^-1, then you get `fmap f-1 . fmap f` = id. This mean that we can *reverse* `fmap f` by calling `fmap f-1`. To be able to be reversed ,`fmap f` needs to not lose any information and at least keep the same number of elements. Now, for a set, not keeping the order doesn't lose any information so I don't see how the `Ord` constraint breaks the law. I can have `fmap (*(-1)) [1,2,3] = [-3,-2,-1]`, but then `fmap (*(-1)) [-3,-2,-1] = [1,2,3]`, so everything is fine. Moreover, the order within a set is an internal details and is only *exposed* when converting it to a list (to display for example). Your example is a bit different and indeed changes the number of elements. However, your function is not inversible, so there is no way you can come to the original list anyway. You are example is probably breaking the *intent* of the law but not the law itself. 
Got it. Thanks for your explanation and the write-ups. I've found that stream-based parsing, like your conduit-based one or, in my case, pipes, a lot easier to modify, refactor, and think about. 
&gt; p.s. I'd love to see stack support the nix-based workflow. I have no idea what the current state of that is, but I think nix is an excellent tool for this job. From the nix side of things, the stackage lts package sets are already available and fully integrated into the nix experience. Additionally, stack is reported to 'just work'. If you have packages already installed via nix, then the 'right thing' happens when using stack. But, given that nix already solves a lot of the problems that stack aims to solve, and that it already has built-in support for stackage lts, I am less clear what additional benefits I get by using stack instead of cabal sandbox.
Good explanation, though it would be even better if you explained the "appears covariantly" part. I especially like the "you're not five" part - it's frustrating to see otherwise intelligent people ask for things in such a self-infantilising manner.
Is there a sensible way to refactor that as a library author, without breaking client code? Perhaps a typeclass that gives you proxies but doesn't promote `Proxy a` to `Proxy (Proxy a)`?
Renamed, and fixed, thanks.
"Transducers" are starting to sound like pipes/conduits/enumeratees/whathaveyou. At least, the common case for writing pipes code seems to be the "foreach" style which is basically (a -&gt; [b]) with effects interleaved.
I agree, and I'm not sure anyone's elucided the differences. Though Rich denied transducers were Signal Functions (from stream processing loterature)... he was kind of insistient it must be of the form (x -&gt; a -&gt; x) -&gt; (x -&gt; b -&gt; x). i think one can achieve the spirit of transducers with signal functions, folds, and different type signatures, whether he would accept this is a different story. Basically he defined the problem to only be acceptable ina dynamic language
I'm a little tipsy and pretty far from a computer. But I'm going to take a shot and say that because he's appending the xs back in that separating it like that helps with memoization. 
There was a related discussion about this on the cafe mailing list a while ago [1]. The consensus seemed to be that System.Random wasn't to be relied upon for any reliable randomness in this manner. [1] - https://mail.haskell.org/pipermail/beginners/2015-October/015996.html
The short answer is: it doesn't have to be. The sequence of bits generated by `map (\x -&gt; fst $ randomR (0,1) (mkStdGen x)) [0..maxSeed]` doesn't have to have good statistical properties, other than being roughly 50% 0's and 50% 1's. Only the pseudorandom sequence produced by iteratively obtaining the next stgGen from the previous is supposed to have the "random" properties one would expect.
## Boring answer You are 1. using a very weak seed (only a few bits), 2. compressing the result down to a single bit, and 3. misusing the generator. The generator is not really designed to be used by instantiating it fresh over and over again with sequential seeds, so the results you're seeing can be chalked up to "bad luck" in the specific parameters of your runs. Interesting, perhaps, but not of significance. ## Slightly more interesting answer An `StdGen` has an internal state of two 32-bit integers, let's call them `s1` and `s2`. When you `mkStdGen` with a "low" but non-negative value (i.e. a value smaller than a 32-bit integer) the second integer, `s2` will be set to zero initially. (The first integer, `s1`, will be set to the value you fed into `mkStdGen`.)\[1] One of the checks that happen to be used inside the actual generator is "how many times larger than 53668 is `s1`?"\[2] That number is called `k` and is used in the magic formula that determines, in your case, both the next state of the machine and which random number is produced: s1' = 40014 * (s1 - k * 53668) - k * 12211 As you can see, as long as `s1` is 0 times larger than 53668, a lot of this can be reduced to zero. The generated number is basically just s1' = 40014 * s1 and this will be an even number, regardless of what `s1` is. (To get an odd number, `k` has to be odd.) The `randomR (0,1)` call is basically checking "is the generated number odd or even?"\[3] Since your number is always even, `randomR (0,1)` will always return zero. There is a mechanism in place to resolve some of the bias that occurs when modding down a large range to a smaller range, but it doesn't activate in your case because the range of integers is considered "large enough" for it not to matter. (A correct assumption if the generated number is distributed uniformly, which it is with normal usage of the generator. Your pathological usage does not result in a uniform distribution of numbers.) What surprises me is that the `s2` state integer is completely ignored for the entire generator if you start with a seed smaller than 32 bits. I have no idea why this is. \[1]: https://hackage.haskell.org/package/random-1.1/docs/src/System-Random.html#mkStdGen32 \[2]: https://hackage.haskell.org/package/random-1.1/docs/src/System-Random.html#stdNext \[3]: https://hackage.haskell.org/package/random-1.1/docs/src/System-Random.html#randomIvalInteger ---- Edit: Just now I see that `s2` is actually set to 1, and not ignored at all. I believe roughly the same reasoning still applies though, since the generated number `z = s1'' - s2''` and both are even the first go around, still giving an even number.
Let's take a look at `mkStdGen`, defined here (I found it using hoogle): http://hackage.haskell.org/package/random-1.1/docs/src/System-Random.html#mkStdGen {- | The function 'mkStdGen' provides an alternative way of producing an initial generator, by mapping an 'Int' into a generator. Again, distinct arguments should be likely to produce distinct generators. -} mkStdGen :: Int -&gt; StdGen -- why not Integer ? mkStdGen s = mkStdGen32 $ fromIntegral s {- From ["System.Random\#LEcuyer"]: "The integer variables s1 and s2 ... must be initialized to values in the range [1, 2147483562] and [1, 2147483398] respectively." -} mkStdGen32 :: Int32 -&gt; StdGen mkStdGen32 sMaybeNegative = StdGen (s1+1) (s2+1) where -- We want a non-negative number, but we can't just take the abs -- of sMaybeNegative as -minBound == minBound. s = sMaybeNegative .&amp;. maxBound (q, s1) = s `divMod` 2147483562 s2 = q `mod` 2147483398 Inside `mkStdGen32`, we see that for the range of numbers you're using the `divMod` will just return `(0, yourInputNumber)`, and the so `s2 = 0`, this means that for most of the range your considering `mkStdGen32 n = StdGen (n+1) 1`. Next, let's take a look at the `stdNext` function: stdNext :: StdGen -&gt; (Int, StdGen) -- Returns values in the range stdRange stdNext (StdGen s1 s2) = (fromIntegral z', StdGen s1'' s2'') where z' = if z &lt; 1 then z + 2147483562 else z z = s1'' - s2'' k = s1 `quot` 53668 s1' = 40014 * (s1 - k * 53668) - k * 12211 s1'' = if s1' &lt; 0 then s1' + 2147483563 else s1' k' = s2 `quot` 52774 s2' = 40692 * (s2 - k' * 52774) - k' * 3791 s2'' = if s2' &lt; 0 then s2' + 2147483399 else s2' I put some very similar code into a file and tried it out in ghci: *Main&gt; stdNext (StdGen 1 1) (2147482884,StdGen 40014 40692) *Main&gt; stdNext (StdGen 2 1) (39336,StdGen 80028 40692) *Main&gt; stdNext (StdGen 3 1) (79350,StdGen 120042 40692) Notice how the last argument to `StdGen` in the output keeps coming up as 40692. But you're not directly calling `stdNext`, so let's look at how the integral types define `randomR`: instance Random Int where randomR = randomIvalIntegral; random = randomBounded -- The two integer functions below take an [inclusive,inclusive] range. randomIvalIntegral :: (RandomGen g, Integral a) =&gt; (a, a) -&gt; g -&gt; (a, g) randomIvalIntegral (l,h) = randomIvalInteger (toInteger l, toInteger h) And finally that leads us to: randomIvalInteger :: (RandomGen g, Num a) =&gt; (Integer, Integer) -&gt; g -&gt; (a, g) randomIvalInteger (l,h) rng | l &gt; h = randomIvalInteger (h,l) rng | otherwise = case (f 1 0 rng) of (v, rng') -&gt; (fromInteger (l + v `mod` k), rng') where (genlo, genhi) = genRange rng b = fromIntegral genhi - fromIntegral genlo + 1 -- Probabilities of the most likely and least likely result -- will differ at most by a factor of (1 +- 1/q). Assuming the RandomGen -- is uniform, of course -- On average, log q / log b more random values will be generated -- than the minimum q = 1000 k = h - l + 1 magtgt = k * q -- generate random values until we exceed the target magnitude f mag v g | mag &gt;= magtgt = (v, g) | otherwise = v' `seq`f (mag*b) v' g' where (x,g') = next g v' = (v * b + (fromIntegral x - fromIntegral genlo)) The first thing we compute is `(f 1 0 rng)`, which using one of our example values will be: `f 1 0 (StdGen 1 1)`. Let's see how that unfolds (doing some substitutions): f 1 0 (StdGen 1 1) = f (1*b) v' (StdGen 40014 40692) where (2147482884,StdGen 40014 40692) = next (StdGen 1 1) v' = (0 * b + (fromIntegral 2147482884 - fromIntegral genlo)) We can simplify more, because ` genRange (mkStdGen 0) = (1,2147483562)`, so we know `genlo` and `genhi`: f 1 0 (StdGen 1 1) = f (1*2147483562) 2147482883 (StdGen 40014 40692) where (2147482884,StdGen 40014 40692) = next (StdGen 1 1) 2147482883 = (0 * 2147483562 + 2147482883) Notice that now when we make the next call to `f` we will hit the first branch because `1*2147483562 &gt; magtgt` (recall, that this all started with `randomR (0,1)` so inside this function `l = 0, h = 1`, `magtgt = (h - l + 1) * 1000 = 2000`. So, then we return `v` and the random generator, and `v = 2147482883`. So for small ranges, we are really just returning the `v` from `f 1 0 (StdGen ...)`. And if we run through it with `StdGen 2 1`, we would get `39336 - 1` for `v`. Similarly for `StdGen 3 1`, it would calculate `v = 79349`. Finally, the value we return is: l + v `mod` k In each of the cases we have considered, we are getting: 0 + 2147482883 `mod` 2 0 + 39335 `mod` 2 0 + 79349 `mod` 2 Each of those evaluates to 1 because it's odd. I believe your original question is equivalent to why does `f 0 1 gen` frequently give odd values on the first iteration? I believe that has to do with the way `stdNext` is defined, but otherwise I don't really know the answer. **tl;dr: It seems that it has to do with an interplay between the small range given to `randomR` and `stdNext` giving out even numbers in the range given to `mkStdGen`.**
&gt; There should be one-- and preferably only one --obvious way to do it. This doesn't fit well with Haskell.
&gt; since the generated number z = s1'' - s2'' and both are even the first go around, still giving an even number. I wrote my response before seeing yours, but that explains the part I hadn't figured out.
Reading your comment I realise I had misread `magtgt` too. For some reason I thought it was `k*b`, and I'm not even dyslexic! Oh well, that explains it all then. Our comments very nicely complement each other. :)
"Use `Maybe` for errors instead of exceptions" is probably bad as a general rule. Sometimes it makes sense, sometimes it doesn't, and sometimes it's impossible to do. (Async exceptions, for example.) I'd also take issue with the "practicality rarely beats purity" stuff. Purity *is* practical in the long run in most cases, and when it's not I prefer practicality.
cool thanks
I'll add to that: Look for the nice documentation in the Graphics.UI.FLTK.LowLevel.FLTKHS module. It explains what this library is about and how to learn to use it. I haven't actually used fltkhs myself. I just intended to have a quick look at the docs, but ended up reading the whole thing because I liked it so much.
I just came to this very similar solution: https://github.com/cies/htoml/blob/master/.travis.yml Differences: * I do not commit my `stack.yaml`, but generate it with `stack solver`. Works great if you want it to work with multiple StackageSets/GHCs. * I run steps more explicitly (build, test, haddock, benchmarks). Question: with Circle-CI is seems to be possible to communicate the passing/failing of specific builds (i.e.: LTS-2, LTS-3, nightly) in badges. Anyone an idea how to do this with Travis? 
For me it comes down to the question whether I can (and want to) handle the error case in pure code. Sometimes there's nothing I can possibly do without user input, or just no good way to recover from an error at all. That's when I use exceptions. If a possible caller of my function might be able to provide a default value or so, I use `Maybe`.
"Factor out your recursion schemes", maybe?
Also, as a more general maxim: "Name it."
Well, "same output" in that type's Eq sense. The function might still be bijective under the "true" equality. 
That would work, but it would be insanely slow (try it out!). Similar situation to the naive Fibonacci implementation which takes exponential time.
If it isn't, it should be. IMO here's the true way of designing programming languages (or anything really): when designing something, try to answer this question as best as you can: "if I were an Apple designer, what would I do?"
I just used `error` for something that should never be handled in pure code. But I made a mistake, and put the `error` in a completely wrong place. Everything type-checked, of course, because it is `error`! I spent &gt;20 minutes debugging this, and realized a few extra seconds to make it a `Maybe` (even if I use `fromJust`) would have saved me some time :)
Should be: "Name it with a name that's intelligible and doesn't mean something else already." Eg. 'nub' vs. 'removeDups', 'return' vs. 'pure', 'null' vs 'isEmpty', 'contains' vs. 'elem' or 'member', 'Mappable' vs. 'Functor', etc.
I hope you meant that as e.g. `Functor` is better than `Mappable` since the former is already a term of trade in maths (of which CS, PLT, etc. are already a subfield of) and not the other way round because of an anti-intellectual bias against math which would be sadly coder monkey-ish.
Well, depending on use case, the cheapest hardware to support your task really might be to just buy more RAM. A million queries per second on something in RAM with a persisting log to disc is easy, and in fact downright sluggish. Getting the same thing even from a rack of SSDs would cost thousands if not tens of thousands of dollars, which would buy hundreds if not thousands of gigabytes of ram. So it's a question of database size vs access rate. Finding the exact critical point of IOPS/GB of requirements where RAM makes more sense no matter how big the database would take a moderate amount of market research and even then will change over time.
I actually think Go is the epitome of this philosophy in a programming language. Haskell is the opposite; you have enough power that you can do whatever you want.
&gt; anti-intellectual bias against math Maybe it's rather beginner/outsider friendliness? Wasn't 'Functor' name the reason why map was separated from fmap? IMO the error messages would've been easily understandable by beginners if 'Functor' was called 'Mappable'. http://stackoverflow.com/a/6827776 After all, there's just '(+) :: Num a -&gt; a -&gt; a' instead of '(&amp;+) :: Num a -&gt; a -&gt; a' and '(+) :: Rational -&gt; Rational -&gt; Rational'.
Patterns are better than general recursion?
Such lack of imagination, wow
"That which can be abstracted over, should be abstracted over, even recursion."?
So then do we need another name for Applicative? Monoid? Monad? Heavens forbid we accidentally teach people something when we're trying to teach people something. Also, you yourself seem to subconsciously think Functor is correct, since you listed the correct option second in all the other cases, and you also listed Functor second.
https://www.useronboard.com/how-applemusic-onboards-new-users/
&gt; So then do we need another name for Applicative? Monoid? Monad? `Applicative` is fine IMO, no idea for `Monoid` and `Monad`. &gt; Also, you yourself seem to subconsciously think Functor is correct, since you listed the correct option second in all the other cases, and you also listed Functor second. No: `'contains' vs. 'elem' or 'member'` also puts the correct option first, but I actually removed `'Functor' vs. 'Mappable'` before re-adding it, but IMO a healthy amount of "code-monkey-ish" attitude is good for a language.
This is how I put it. ""
How is a name more beginner-friendly if it is made-up, as opposed to one that already exists and means the same thing? With a preexisting word, you at least have a chance that if you've encountered the concept before, the thing you are learning about will be immediately connected to it in your mind.
First, I think it's important to separate Apple designers from Apple engineers. I may be wrong but I think it's safe to assume the people responsible for the design of the App store and the settings menu are not the same people designing Swift and the APIs. Generics, sure, but beside that, I think the moment you start allowing people to use `map` and `filter` instead of `for`, you're introducing tradeoffs (at least when combined with mutability and other language features). Heck, Go even removes `while` in favour of making `for` do everything. However, I do agree with your bullet points, and I do think having _one_ obvious way to do things is a good aspiration, even if there are other less-obvious sometimes-better ways, when the alternative is having no obvious ways and a lot of confusing alternatives (lazy/strict * text/bytestring - we're not great at communicating when these should be used).
Isn't it how the `-able` suffix work? Eg. `edit` and `editable`, `think` and `thinkable`.
&gt; First, I think it's important to separate Apple designers from Apple engineers. You mean, Apple artists and UX experts from Apple engineers? http://ilyabirman.net/meanwhile/all/everyone-designer/ &gt; Generics, sure, but beside that, I think the moment you start allowing people to use map and filter instead of for, you're introducing tradeoffs (at least when combined with mutability and other language features). What kinds of trade-offs?
 Pointed =&gt; Pointable Applicative =&gt; ApplicablePointable Monad =&gt; BindableApplicable Semigroup =&gt; Appendable Monoid =&gt; AppendableWithUnitable I'm not very good at this. :(
It's not really fitting with the style of the code zens if you don't have *something* or even *several* that go too far.
&gt; If it isn't, it should be. Probably but you need to remove all syntaxic sugar then, because the purpose of syntaxic sugar is exactly to propose a different way to write something which could be written anyway. 
You're missing the point. This is not to be taken to the literal extreme. There's always more than one way to implement something, in any language (i.e. if a language is Turing-complete, you can always implement an interpreter of another language and do it in that language). Or even this: func(func(a), b); or: var x = func(a); func(x, b); I think the right way to interpret that is that for every thing to be done there ideally should be exactly one (or as few as possible) obvious (think "low-hanging fruit") way to implement it. I don't think syntactic sugar necessarily violates that, if it makes doing something significantly easier, THAT becomes the obvious way to do it. Eg. start &gt;&gt; getSomething &gt;&gt;= \a -&gt; getSomething &gt;&gt;= \b -&gt; doSomething a b &gt;&gt;= \c -&gt; finish c vs. do start a &lt;- getSomething b &lt;- getSomething doSomething a b finish c
He uses it to compile regexes, so I'm not sure about the reverse part?
I was thinking that it's generated ("compiled") from the corpus you want to search in, and then you can feed it various regexes and have it match. The normal order of things when you talk about compiled regexes is to first compile the regex and then feed it various corpuses to search in.
You mean the `n` in `O(n)` is the number of coin types ?
If I were seriously considering doing this sort of work in Haskell I'd use pipes.
the zen of python encapsulates a lot about what's wrong with python and its community for me. This idea that python is some sort of beautiful minimalist nirvana... that type of complacent self-satisfaction is misguided; Python appears to be a pinnacle of design because most people using it have only been exposed to Java, C, C++98 and python. Haskell shouldn't fall into the same trap. 
wxwidgets is quite easy to use via the nix package manager... 
Apple would release Swift. So either you don't believe this or you're wasting your time here.
Self-plug: [Threepenny-GUI][1] also uses HTML under the hood, but packages everything as a Haskell library that can be used with ordinary GHC. [1]: https://wiki.haskell.org/Threepenny-gui
note the use of the word Zen, in both python and haskell i believe imho it is the authors intention to provide and indication of "the way" rather than a set of rigid rules which will always have exceptions.
So IIUC the PRNG behaves like a Brownian bridge, i.e. it's constrained on one end. Does it mean we should allow some "burn-in" sample to be thrown away, like the MCMC folks do? 
n = ceil(lg(N)), where N is the value passed to cnt. For example, if N=10, n=4. fridofrido's solution is O(N), while OP's is O(n) = O(lg(N)).
I'm not sure. I think `reflex` and `frpnow` are also FRP. maybe also `euphoria`, `Yampa` and `netwire`? Maybe [this](https://www.reddit.com/r/haskell/comments/3fr5ij/frp_systems_discussion/) discussion will help shed more light.
C++ functors never get this kind of treatment..
The feed got removed when I updated my blog software. I should move it back to Hakyll really. I'll see if I can add it back.
I don't mind `Set`'s domain being a subcategory of `Hask`. We can have a different `Functor` class that allows that.
&gt; If the monad is hard to explain, it's a bad idea. If the monad passes the laws then supplying it is a good idea, regardless of how hard it is to explain, because then it is a non-trivial result and you learned something.
&gt; There should be one-- and preferably only one --obvious way to do it. We very much take the opposite stance on the syntax of the language and not only let but encourage many approaches to flourish: * `let` vs. `where` * `case` vs. pattern matching in function definitions * lambdas vs. explicit declarations * type families vs. functional dependencies. For instances, when there is one and only one way to do something we definitely encourage the existence of the instance for that case, however.
I'm a little annoyed that some answers here blame the seed or the use of the RNG. A good RNG would not have a strong correlation between your input seed and any of the outputs or any other generator initialized with a different seed. This should be true even when the seeds have low Hamming distance. *StdGen is a bad RNG* Lets make sure that is well understood at the on-set. StdGen should not be used in any case where "it matters". This has caused issues with property checking in both QuickCheck and Cryptol. Motivated by this weakness, a replacement generator was developed and is on hackage under 'tf-random'. Alternatively, you could use one of the cryptographically secure RNGs available on hackage.
Yes I used a generating function, the coefficient is computed by multiplying coefficients of a roughly 1200 degree polynomial with some binomial coefficients, at most 1200 / 200 multiplications are done. But I believe the cost of multiplying is O(n lg n)? (Again n = lg(integer)).
I am not working on APIs but I use ODEs and stochastic modelling all the time in Haskell. I am not convinced that repa has the best interface. I really like the static interface in hmatrix but I haven't thought very hard about whether this can be extended to a repa Mk II.
&gt; Ultimately, I think this is just a bad prng and we shouldn't rely on it. You can remove the "I think" from that sentence ;-).
You have a lot to learn young panda.
It looks like most C++ people prefer "function objects" now.
I totally forgot about that. The best algorithm known is actually worse than that, but only just (O(n log n * 2^3log*n ). It's conjectured to be the lower bound though. For practically sized numbers (E.G. those less than a googol), it's actually O(n^2 ). I actually have no idea if Haskell switches multiplication algorithms when the Integers get sufficiently large.
Point taken. Even though I kind of agree that is how things should be, it doesn't really apply to Haskell, as they are (unfortunatly) lots of point whereas the *best* to way to write something is still subject to debate getSomething &gt;&gt;= doSomething vs do something &lt;- getSomething doSomething something `map` vs `fmap` (for list). `fmap` vs `&lt;$&gt;` vs `liftA` vs `liftM` ? (,) &lt;$&gt; (+1) &lt;*&gt; (*10) vs liftA2 (+1) (*10) vs f x = (x+1, x*10) Point free vs pointed. etc ... 
Admittedly I don't have time to do a in-depth review. The idea was sparked when I learned that [Wagon](https://www.wagonhq.com/) a SQL editor, build on electron uses [Haskell](https://www.wagonhq.com/blog/square-tech-talk). &gt; there's not yet a way to tell GHCJS to produce output that uses both node and browser JS Can GHCJS output es6? Electron supports most of es6 just like the newest version of Node.js.
Short answer: because `type String = [Char]`, and you can't promote `Char`s to the type level like other data types because `Char` is a primitive data type (likewise, you can't promote `Int`, `Float`, `Double`, `Word`, etc.). As a compromise, GHC introduced `Symbol`. Long answer: I don't believe there's any there's any theoretical reason preventing one from promoting `String` to the kind level, as evidenced by this [open Trac issue](https://ghc.haskell.org/trac/ghc/ticket/9649). It seems to mostly be an issue of finding an implementation for GHC that isn't awkward. Until then, if you really want to use something close to type-level lists of `Char`s, you can use this trick which uses an encoding of kind `[Nat]` (adapted from [here](https://gist.github.com/sdiehl/b70d25b920a97f3002e8)): `TypeLevelString.hs`: {-# LANGUAGE QuasiQuotes #-} {-# LANGUAGE TemplateHaskell #-} {-# LANGUAGE ScopedTypeVariables #-} module TypeLevelString ( tstr, ) where import Text.Read import Data.Char import Language.Haskell.TH import Language.Haskell.TH.Quote stype :: String -&gt; Q Type stype str = do case readEither str of Left err -&gt; fail (show err) Right (n :: String) -&gt; do let chars = fmap (fromIntegral . ord) n let tcons x y = AppT (AppT PromotedConsT x) y let tnil = PromotedNilT return $ foldr tcons tnil (fmap (LitT . NumTyLit) chars) tstr :: QuasiQuoter tstr = QuasiQuoter undefined undefined stype undefined `Example.hs`: {-# LANGUAGE DataKinds #-} {-# LANGUAGE PolyKinds #-} {-# LANGUAGE QuasiQuotes #-} {-# LANGUAGE TemplateHaskell #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE UndecidableInstances #-} import TypeLevelString import Data.Char (chr) import Data.Singletons.Prelude import GHC.TypeLits import Language.Haskell.TH import Language.Haskell.TH.Syntax type family (++) (as :: [k]) (bs :: [k]) :: [k] where (++) a '[] = a (++) '[] b = b (++) (a ': as) bs = a ': (as ++ bs) type Foo = [tstr|"Foo"|] type Bar = [tstr|"Bar"|] type FooBar = Foo ++ Bar main :: IO () main = print . map (chr . fromIntegral) $ fromSing (sing :: Sing FooBar)
That's exactly why `Mappable` works, because more coders have a chance of knowing `map` than they do functor. Not that I'm saying it's a good idea, but I can't pretend I find it totally unreasonable.
We could have called `Monad` `Collapsible` and explained it in terms of `join`.
At least, in the abstract.
`Data.List` does finally have an `uncons` function in `base-4.8`, and `Data.Maybe` has `listToMaybe` (which is the same thing as `headMay`), but yeah, I think it would be nice to have `headMay`, `initMay`, etc. right in `Data.List`.
I agree. If you read Haskell code you'll find people have so many styles! I would say that Haskell code is tremendously variable in the way it looks. Each library has many different approaches!
maybe read is in Data.Text.Read I believe
To be fair the C++ community just made up a word that happened to already be in use.
The fact that they called it `Functor` was literally the thing that led me to 8 years of productive exploration of category theory in the context of programming. If you make up a name people can only learn what you tell them. If you use a name with a rich history and *lots of literature that you haven't read yet* people can tell _you_ stuff about it. I use names like "Kan extension", and "monad" and "closed monoidal category" because in the process I can learn a lot more about the way everything I write fits together and results I learn while programming Haskell immediately transfer to other domains. Moreover, people can correct you when you are wrong. There becomes objective truth to be had. That ability to transfer knowledge without transferring false intuition requires a certain level of precision about the bits and pieces you are working with. If I thought every category was like "Set" or "Hask" I wouldn't be able to use the intuitions I gained from exploring math in Haskell to talk about the real numbers or quantum computation or probabilistic programming or vector spaces or topology without seeing everything as a very different thing and having to relearn everything for each domain. I don't have time to stop and start over every time I'm exposed to something new, without the ability to transfer some of what I've learned before, so I look for good generalities that transfer and good questions to ask about a new domain to get a sense of what works and what doesn't work there. Category theory gives me a library of such generalities and questions that has had the smartest mathematicians in the world beating on it for 70+ years across an incredibly wide array of domains. We just don't have anything of equivalent durability/robustness in the computer science ecosystem.
I sort of meant that different approaches are the same, although they will look different if you are not a category theorist. (See my edit)
What are your approach to partial functions in general? For example, defining a factorial function, would you define `factorialSafe :: Int -&gt; Maybe Int` or `factorialUnsafe :: Int -&gt; Int`? or Both? (edit: I'm talking about the fact that factorial is not defined for negative number. But the question of its definition for large int is also interesting)
`random` is [on Github](https://github.com/haskell/random/issues). Anybody can submit a pull request clarifying the documentation as desired, or just submit a documentation issue. Looking at the issue tracker, there's [some suggestion that a new implementation is on the way to improve the generation quality](https://github.com/haskell/random/issues/30#issuecomment-152789142).
&gt; Why not just head? 1. this means that you don't need to `import qualified Safe as S` 2. `safe` provides tailMay, tailDef, tailNote, and tailSafe, where tailDef takes a default and tailSafe uses a default default (i.e. it returns [] for the tail of an empty list) and tailNote blows up like normal tail, but uses a custom message.
Could live in a different package, and deprecate the old one.
That's a fair point, and thanks for pointing out that ticket.
`uncons` appears to have the same conceptual flaw as pattern matching - it provides the head _and_ the tail.
Two things argue in favor of 5 over an-undergrad or a-self-taught-programmer. First, undergrads and self-taught programmers are highly variable along a huge pile of dimensions. With 5, what I'm asking for is squarely below where you might have aimed in every dimension. Second, I have no idea whether this was the initial inspiration, but 5 is also an oblique Groucho Marx reference: &gt; A child of five would understand this! &gt; Send someone to fetch a child of five.
Well, it comes down to less typing and naming your abstractions. If you had to use your version more than 3 times, you might well define a little helper that encapsulates this pattern, and in that case it's better for readers familiar with `safe` if the helper is already defined there. (Besides, that's linear non-explosion. `4n` variants for `n` original functions.)
You might want to try Stack instead. http://docs.haskellstack.org/en/stable/install_and_upgrade.html#centos-red-hat-amazon-linux
Worth applying as a college sophomore? I don't have a resume
Thanks this is good to know. I'm fairly new to EC2 and Linux in general, so it's hard for me to tell the difference between the images. I didn't realize Amazon Linux was bloated. I'll try spinning up Ubuntu and centOS next.
Actually I have the feeling that this is still a partial function. Word has no type guarantee about overflow and "underflow" (i.e. wrapping above 0), they are always positive, but it does not mean that it is always what I want. Prelude&gt; let a = 5 :: Word Prelude&gt; let factorial n = product ([1..n] :: [Word]) Prelude&gt; factorial a 120 Prelude&gt; factorial (a - 2 * a) ^C^C^.... -- Damned. Prelude&gt; a - 2 * a 18446744073709551611 So Word is not a `safe` type. I want an unsigned type which implements part of the Num class, (+, *, all other have no meaning in the context of Unsigned). and `(-)` should be `(-) :: Word -&gt; Word -&gt; Maybe Word`. I wonder if the `Num` can be splitted in many subclasses (such as in "PureScript"). Unfortunately we can't (for good reasons) overload the (-) of the Num instance with a different signature. Actually I stopped using unsigned in all my C++ development for that reason, I prefer an assert inside each function that an unsigned which will wrap silently. The situation in Haskell is a bit better because there is no implicit cast between signed `Int` and unsigned `Word`, but `Word` can still "underflow" without notice. There is a few package which works on that issue, such as "naturals" or "naturals-numbers", But I don't really like their approach of throwing exception (because runtime) or returning a special value (Because this propagate and quickly become a nightmare to debug ;) (Edit: a bit of clarification, my issue is about the fact that Word wrap above 0)
Stack can download various versions of GHC as needed to build a project. It also chooses "Stackage snapshots" which are additional version constraints of packages which are known to work together. With Stack you don't have to work with cabal directly. With cabal you have to create a new sandbox per project (or risk cabal hell when sharing packages between projects). This requires rebuilding packages over and over for different projects. It always takes a long time and people joke about heating their room with the CPU when building a new haskell project. Stack on the other hand allows projects to share built packages safely, so you don't end up rebuilding them.
Actually I was thinking more about the definition domain of factorial which is not defined for negative number (actually, well, it is, but...). I did a clarification in the question.
Perhaps you could define your requirements a bit more clearly, I'd love to help you out, if I can
Stack is a build tool, an executable. Platform is a batteries-included distribution of Haskell (meaning: compiler, packages etc.), but I understand the need for it has been somewhat undermined by the recent (last year or so) improvements in compiler and tooling. Stack not only creates isolated builds but can also download and install a GHC with package dependencies that are "best" for your project, completely independently of any other system-wide installations you might already have. In short, it's a good thing. 
If I were an Apple designer, I'd make a laptop with only one single port on it, including the power port. I don't want to think like Apple designers. I want to think like the Apple marketers, who manage to convince people to buy it anyway.
Wow, didn't know of the `*Note` functions. Great!
You can get an RHEL image, too. That is good if you are developing something that you plan to install in an enterprise environment.
I doubt there is a general, platform-independent way to do this. Even without Haskell (when just building another C library or application I mean) there isn't a platform independent way to discover C dependencies to the best of my knowledge, and certainly not one used by all libraries. Does your C dependency use pkg-config? That is the closest to a standard way I am aware of to solve this problem besides just using well-known directories. How does your Makefile or Travis Script discover the location and name of your C dependency? You could probably put some code into Setup.hs instead of using a Makefile.
Your point about the "uncanny valley" struck home with me, being reasonably acquainted with both Haskell and Scala. When applying Haskell patterns in Scala I often end up hitting a wall at some point. The same can be said about the other direction. I think that an equally important discussion is, what can Haskellers learn from Scala about Haskell's weaknesses, and conversely?
If it's a small C library (simple to build), and the license allows it, I think the best way is to include the source in the package and let Cabal build it. This is basically the only way which has any chance to work on Windows without issues. And external libs are sometimes mission impossible to make work on Windows, in my experience (no amount of extra-lib-dirs, cabal file editing, etc etc will help)
The package head comes from is `base`. That's a tough one to deprecate.
I would expect that the complexity would depend on both the total sought and the number of denominations. Is that wrong?
I suspect the benefit is more ontological than practical, for Haskell look at Data.Aviary.Birds
Yes -- please post tickets! But I've added a link to your code in my own issues tracker so that I can address some of your concerns. Glad you're having fun with it.
Only if you use the partial combinators (e.g: `?!`).
You might be able to find some old work on compiling functional languages via combinator calculi like BCKW and SKI, where you'd reduce terms in your higher level language into applications of your combinator basis, and then compile the primitive combinators somehow. Generally these approaches are no longer used in most (all?) practical settings. One relationship between the SKI basis and modern practical Haskell usage still exists however, in the form of the Applicative type class. The instance of Applicative for functions has pure = K and (&lt;*&gt;) = S. That instance sees somewhat limited usage though.
I dont know if this is helpful, but I believe that the Applicative instance of (-&gt;) a is SK calculus: K = pure S = &lt;*&gt;
Well, both functions involve a boolean test. So, one way to use it is to validate once, and remember the result for all future operations. This makes sense to do if the test is expensive. Alternatively, you might want to be able to chain several different tests by `fmap`ing into an `Either` or a `Maybe`. In that case, you need to somehow create the first `Either`/`Maybe`. That is what these functions are for.
Yeah, I've seen it this way too. I prefer the `mfilter` approach. Or, the `bool` way for uniformity with `toEither`.
https://en.m.wikipedia.org/wiki/To_Mock_a_Mockingbird
This is hilarious. 
&gt; toMaybe f = guard . f &gt;&gt;= ($&gt;) toMaybe = (($&gt;) . guard =&lt;&lt;) [pointfree.io](http://pointfree.io)
how does it work with other stores, like Redis?
Waiting for times when stack will support installing any `GHC` including `ghc-HEAD` if I want to. There is a way to build ghc from git and and even build a distro `tar.bz2` from it but `stack` currently refuses to use it.
You can't have that signature, but you can have something like `delay :: c -&gt; (a -&gt; IO b) -&gt; (a -&gt; IO b)`, but really, just `delay :: c -&gt; IO a -&gt; IO a` is all you need. The `IO` part is needed because delaying execution is an effect, so you can't do it in pure code. `Control.Concurrent` has some functions for delaying threads, e.g. [`threadDelay`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Control-Concurrent.html#v:threadDelay).
I don't use emacs so I don't know what backups it's touching, but one solution is to simply update your file-watching predicate from `const True` to filter out the events that touch the backup files. Alternatively, there are utilities such as [steeloverseer](https://github.com/mitchellwrosen/steeloverseer) for this kind of thing. To use, either do sos -p ".*\.py$" -c "rsync -arPvz --exclude .git --exclude *.pyc src/ dst/" or else make an `.sosrc` file containing - pattern: .*\.py$ command: rsync -arPvz --exclude .git --exclude *.pyc src/ dst/ and then just run sos I realize it's a lot more fun to write your own solution, but eh, it's an option :)
&gt; --skip-ghc-check I try with: `compiler: ghc-7.11.20151215` &gt; No information found for ghc-7.11.20151215. &gt; Supported versions for OS key 'linux64': GhcVersion 7.8.4, GhcVersion 7.10.1, GhcVersion 7.10.2, GhcVersion 7.10.3 And I have `ghc-7.11.20151215.tar.xz` in my `~/.stack/programs/x86_64-linux`.
Can you give an example for usage of (c -&gt; IO() ) with &gt;&gt; I could not work it out.
So, that will work because (IO a) will not be evaluated until we extract the data out of it. So delay will be something like delay c m= do threadDelay c v &lt;- m return v I think I got the essence of it. I will check Control.Concurrent and try to work it out.
It can been even smaller: delay c m = threadDelay c &gt;&gt; m
This seems like exactly what I need. Thanks you for detailed implementation and explanation. I should probably go over this first to understand all the concepts: http://community.haskell.org/~simonmar/par-tutorial.pdf
[Been there](https://www.reddit.com/r/programming/comments/24g6al/i_have_officially_failed_at_programming/ch75f9e?context=1).
Wow! It's amazing what they did for Idris! We get a gist of that in Atom, for example, but not that good.
Thanks, I didn't know about this one, will add a link to it! So many pretty-printers out there.
Ontological? Don't you mean ornithological?
Thank you all for the information. Now I know what my next research/article is going to be focused on: understanding and comparing Cabal/Stack/Platform from the perspective of a complete n00b.
Not yet, but Docker on AWS is one of my next projects.
 v &lt;- m return v is just m :) 
The recent integration of nix in stack tries to solve that very problem if I understand correctly.
Wouldn’t it serve your purposes even better if you simply excluded the emacs backup copies from the rsynced content?
It's funny, my perception as a Linux beginner was that Ubuntu would be the more bloated AMI, and Amazon's AMI would be the stripped-down image. I don't know what gave me that impression, but it looks like I need to spend a bit of time trying out the various flavors. Thanks for the offer, if I'm having trouble I'll take you up on it.
What do you mean? Where may I read more about this?
This discussion is really pushing me to learn more about the various flavors of Linux and how they relate to Haskell. Thanks!
When you create a project using Stack, a .cabal file (containing module dependencies, compiler flags, author info etc.) is generated (along with a few other things), similarly to when you run `cabal init`. So it's best to know what a Cabal file looks like before starting with Stack. 
yes thats a much better description! I was clearly winging it before 
There's also been ongoing work to put annotations into [wl-pprint-extras](https://github.com/ekmett/wl-pprint-extras/pull/11) since that talk, but I don't yet see it in the Haddock on Hackage.
obviously, then different stores are not "abstracted over", which is what the OP wanted
That already exists! https://github.com/gregwebs/ghc-docker-dev. I use it to build ghc, I then make another docker image that doesn't include all the compilation dependencies. Would nightly builds of GHC mean you wouldn't even need to build it yourself? 
Awesome! I do want to actually build GHC in my case, as I'd like to hack on it when the feeling takes me.
I'm pretty keen to make the changes to `wl-pprint-extras` to line up with your work on `annotated-wl-pprint` but w/ the extra `wl-pprint-extras` argument as well for point annotations and the monad. I've just been pulled in too many directions lately to make the patch myself. Nathan W. Filardo has done a fair bit of work towards making it happen though.
The most stripped down of most distributions would be either Arch Linux or Gentoo. Pretty minimal in terms of setup. I can see your reasoning when it comes to their image. However, think of a new windows computer from a vendor vs one you build yourself, their version has lots of extra software on it you don't need.
Nice problem. My solution so far: -- ghc --make main.hs -threaded module Main (main) where import Control.Concurrent import Control.Monad import Data.IORef import Data.Time mkActionDelay :: Int -&gt; IO (IO () -&gt; IO ()) mkActionDelay millis = do actionChan &lt;- newMVar [] -- a list with actions to perform later (in reverse order) lastTrigger &lt;- newIORef Nothing -- when was the last action added? -- worker thread: run actions when they are old enough, wait, start over again let worker = do mbTime &lt;- readIORef lastTrigger -- get time of when last action was added case mbTime of Just triggertime -&gt; do now &lt;- getCurrentTime let delta = diffUTCTime now triggertime when (delta &gt; (0.001 * fromIntegral millis)) $ do -- when last action is older than X: writeIORef lastTrigger Nothing -- reset time of last action to Nothing actions &lt;- takeMVar actionChan -- remove all actions from list putMVar actionChan [] sequence_ (reverse actions) -- run all actions Nothing -&gt; return () threadDelay (millis * 1000) -- wait and start over again worker forkIO worker return $ \action -&gt; do now &lt;- getCurrentTime writeIORef lastTrigger (Just now) actions &lt;- takeMVar actionChan putMVar actionChan (action:actions) main = do trigger &lt;- mkActionDelay 500 let test = do s &lt;- getLine case s of "q" -&gt; return () s -&gt; trigger (putStrLn ("# " ++ s)) &gt;&gt; test test 
Looks pretty good. Both Elm and PureScript are attractive light-weight solutions, and they have extensible records built-in. Wunderbar! &gt; The function composition operator is not (.), but (&lt;&lt;&lt;) [Oh](http://3.bp.blogspot.com/-2HfyfBHex30/UTFViwlClLI/AAAAAAAABYE/SyvyRmH10HM/s1600/vm.gif).
&gt; Oh. Yeah :-\ I think they made it that way because of using `.` to access members of records. I guess I can't blame them for not wanting `.` to have three different meanings: - record accesor (`fooRecord.barMember`) - function composition (`map f . filter g . something`) - separator between submodules (`Data.Text`)
Yeah, we did discuss using a whitespace rule to differentiate `(.)`-the-operator from `.`-the-record/module accessor, but it's still an open issue. There are some advantages to what we have now, in that there's `(&gt;&gt;&gt;)` also, so the direction of composition is made clear. I think Phil and I are so used to `(&lt;&lt;&lt;)` that it's hard for us to get worked up about it, and in fact I end up habitually trying that over `(.)` when I write Haskell now.
You're welcome!
Well, we're going to get `f ∘ g` which is even better. ;)
Well, better in the sense that it looks exactly like the mathematical composition symbol. :) But there are also problems with Unicode. It can be a pain to input the symbols or search for them using current programming environments. `emacs` has a `TeX` input mode, which replaces something like `\to` with the unicode equivalent `→`. But this does not work for searching. If I'd choose to use Unicode, then I'd also consider using `g ◁ f` and `f ▷ g` for function composition, and `f ◀ x` and `x ▶ f` for function application. The direction signals the dataflow, composition is hollow (similar to ∘), they are only 1 character wide, and visually symmetrical.
Couldnt char be represented as a type level nat? I guess this symbol business just seems horribly hacky and really theoretically unfounded. 
I'm not a fan of unicode in source code anymore, mainly due to the fact that there are way so many codepoints that are nearly or completely identical, but are in fact different. Maybe if you had a cut down set of unicode characters that were all easily visually distinguishable.
I'm very excited for overloaded record fields. I started using lenses in my programs simply as a workaround for the record problem, but that use case always felt like overkill. Also, stack is such a huge improvement over cabal-install. One thing on my wishlist for stack (or cabal) is prebuilt packages with non-Haskell dependencies, just like `conda` for Python. (Or, something that works like the Python wheel format). Thanks for this article Stephen! It's a really nice summary of Haskell advancements this year.
Yes, this occurred to me too. Also, I think npm comes with node 99% of the time anyway, so listing them separately will possibly have the effect of making it seem worse than it is. And of course, there isn't *necessarily* a strictly linear relationship between the number of command line tools there are and how painful dealing with dependencies etc is.
Point is, we can pick any other name we like. The choice in Prelude is the default one in some sense, but it's not baked in. That said, I never found `&lt;&lt;&lt;` to be an issue. I'm interested to hear why people don't like it.
My only complaint is that it's "too big". I use [`&lt;&lt;` in Neon](http://pursuit.purescript.org/packages/purescript-neon/0.1.0/docs/Neon.Operator#v:(&lt;&lt;\)). 
I would like to lose a &lt; and have `&gt;&gt;` and `&lt;&lt;`. Originally the JS bit shift operators used those, but that's no longer the case, and it might be nice to forgo the extra noise. Truthfully I don't think about it that much anymore, though.
This is mostly very good but I do think it has been a bit unfair to pulp. I'll give an example to illustrate. Let's suppose we have a client-side web application which uses web workers. The web worker API means that web workers have to be served in a separate JS file, under a separate url. In JS, you can create them with something like `var worker = new Worker("/js/worker.js");`. It's a pulp project, so we have all our source code under `src`, and our tests under `test`. We have two "entry-point" PureScript modules under `src`: one which gets embedded in our HTML using a script tag, and which creates the worker; and the other one, which is the worker. We've called these modules `Main` and `Worker.Main` respectively. Additionally, we have some benchmarking code using *shameless plug* [purescript-benchotron](https://github.com/hdgarrood/purescript-benchotron), which we keep separate by using a `benchmark` directory for it. The main benchmark module is called `Benchmark.Main`. Of course, we have tests under `test` too, and the main test module is called `Test.Main`. Pulp handles this situation without breaking a sweat: * To build the application: `pulp build --main Main --to /js/main.js &amp;&amp; pulp build --main Worker.Main --to /js/worker.js` * To run the tests (on node.js): `pulp test` * To run the tests (in a browser): `pulp build --main Test.Main --include test --to test.js`, and then open an HTML page with a script tag pointing to `test.js`. * To run the benchmarks (on node.js): `pulp run --main Benchmark.Main --include benchmark` * To run the benchmarks (in a browser): `pulp browserify --main Benchmark.Main --include benchmark --to benchmark.js` and open an HTML page with a script tag pointing to `benchmark.js`. A small note on the last example: `browserify` is necessary whenever you use `require()` to load JavaScript dependencies (and want your code to run in the browser). Benchotron uses the "benchmark" package on npm, so that's why the last example uses `pulp browserify`.
Oh, and also, if you didn't want to type these all out every time, you would normally put them in the "scripts" section of your package.json file. You probably already have a package.json file anyway, because that's the easiest way of specifying npm dependencies.
What is the call-by-push-value remark referring to? Is something new in 2015 or just an off handed comment? we use a special purpose functional language with cbpv in productiom (hacking ghc is quite a bit more ambitious), but I doubt that's what he's talking about. I would be very interested to know if someone else is working on it too.
&gt; on the machines I've tried (OSX 10.9.5 and Ubuntu 12), try it on windows, then.
It's fine, but `.` is just obviously better (shorter, fewer key presses, mirrors math).
Great writeup! I'm a bit surprised though to read your claim that dynamic typing might be a local optima for data science. There's been some innovative work on typing for numerical systems in the last year. My favorite so far is [subhask](https://github.com/mikeizbicki/subhask). Unfortunately, it's still very immature as a library, but with more community effort could develop into a powerful new tool for typed numerical programming.
[Merry Christmas!](https://github.com/purescript/purescript/pull/1727)
One possibility is to add this feature yourself and submit a pull request. Why wait?
I thought I heard something about the 8.0 record field changes not being the full set of changes. Thanks for the info! And, upon further inspection, it appears you are the person who wrote the OverloadedRecordFields extension. Thanks so much for your work!
I thought there was significant progress made with ghc on ARM this year. For the first time (that I know of), there was an arm ghc binary up on the main ghc page, and we ended up with a working ghci on arm which hasn't been the case normally. V8.0 is supposed to bundle llvm with ghc, which should help eliminate the llvm-ghc version mismatch problems too. Here's hoping for a template haskell cross compiling solution in 2016. 
There's at least one more Javascript topic to cover: ghcjs
I like `&lt;&lt;&lt;` and I like `&gt;&gt;&gt;` even better but won't beat that dead horse :-). And I never liked `.`, not even in math class.
&gt; My only (non-technical) concern is that my applications are increasingly becoming impenetrable to those who are not deeply immersed in the lore of GHC extensions. This, sadly, is what's keeping me back from diving headlong into Servant. It's tough to say because it's not exactly an actionable criticism for the library developers; just an unfortunate truth for many people. That said I'm very much looking forward to seeing its progress and what people make with it.
Many ways of doing this. There's the cannonical acid-state. Or, if you don't want to have long term persistence, there are a lot of other ways. Keeping an IORef in a reader monad or using a client session (from the package clientsession) are two good options.
Did people say the same thing about all the template haskell etc. in Yesod?
I also avoid using TH too heavily because it makes you rely on documentation to figure out what code actually ends up being compiled. And the generated code isn't documentable. Obviously in some cases that's fine (generating Aeson instances), but I'm not a fan of too much magic.
Acid-state is actually covered in the crash course. If you're not using that, you're definitely making things harder on yourself than you need to. Clientstate has excellent tutorials if you are familiar with monad transformers, and using a Reader for an IORef requires monad transformers as well. Practically, you can take a ReaderT a m b value, and run it with an IORef. When operating in a ReaderT a monad, you can ask for the value that you run it with with `ask`, and that will give you the IORef, which you can lift IO actions to run on. A trivial example of a ReaderT would be something like printR = ReaderT print printThrice a = flip runReaderT a $ do printR liftIO . print =&lt;&lt; ask printR This prints out a specific value three times. So, the use of `printR` is an example of some already defined reader action, and the `liftIO . print =&lt;&lt; ask` part is one way you do the same thing without having a pre-constructed operation. It uses `ask` to get the value of `a` and passes it to a print, which is then lifted so that it can be used in the `ReaderT r` monad. If you're not comfortable with this, you might want to use a different framework (or work on some monad transformer tutorials, you don't need to know how to build them for now, just to use them and sort-of understand the error messages), because the easiest way to add capabilities to a HappStack (cookies, authentication, databases, etc) is with a monad transformer stack. 
TH prevents libs like Yesod from being usable in cross compilation right now. 
No windows support though
PureScript is a very interesting sweetspot. It is as elegant as Haskell, has a very nice and clean translation to JavaScript and is performant enough for most real-world uses. I'm still a little afraid to work with two languages simultaneously and end up duplicating most of my work, so I tend to insist in staying Haskell only and go through GHCJS. But that is clearly not the way people are going for today.
I was curious what a hello world looked like in servant, and it's pretty pretty, I think: import Data.Proxy import Data.Text import Network.Wai.Handler.Warp import Servant import System.Environment type Hello = Get Text server :: Server Hello server = return "Hello, world!" proxy :: Proxy Hello proxy = Proxy main :: IO () main = do env &lt;- getEnvironment let port = maybe 8080 read $ lookup "PORT" env run port $ serve proxy server From : https://github.com/mietek/hello-servant
What would these operators be for?
I have a working implementation using your mvars as a lock just like your example. But my version do not have composibility that yours have. https://github.com/huseyinyilmaz/cscp/blob/e2556ae53dd7ccf82e31d264de109440a48bfcd3/src/Copy.hs#L16-L33 Thanks for the help. That was very educational for me.
Right, I don't mean the Prelude versions should be monomorphic to `[]`, I just mean the functions in Data.List should be because that's what that module's for. I'm aware of the downsides of `[]`. We're just in a funny state where Data.List has a bunch of functions that work on Foldable, so `Data.List.sum :: Foldable f =&gt; ...` I guess my main point is that 'pedagogy' isn't just for newbies. It might be for every one of us in various circumstances.
I think [the servant template](https://github.com/commercialhaskell/stack-templates/blob/master/servant.hsfiles#L57-L95) for Stack is probably a better example since it actually has a route. It doesn't quite have the wall of extensions the OP described, which is good!
To be fair you write the most confusing code in the community.
In general it is fairly different with a recursive descent LL-style parser to report multiple errors. With an classic yacc/bison LR-style parser you report the rightmost parse, so you you can easily define an error production and then start parsing when there is one unambiguous set of reductions across the entire grammar. You can know that no matter what happened before that the new production is the only thing grammar wide that could even try to continue on there, and so you can safely report errors using it. However, in recursive descent you don't get to know much about the entire grammar, so any recovery construction is more ad hoc.
&gt; Also, I think npm comes with node 99% of the time anyway, so listing them separately will possibly have the effect of making it seem worse than it is. I had two reasons for listing them separately. 1. On Arch Linux, npm and node come from different packages. 2. They are different things. node is a runtime system(?), and npm is a package manager. To some extent, you do have to be aware of what both of them are and how they work. It's not strictly necessary, as paf31 points out, but it is usually helpful to have that knowledge. &gt; There isn't necessarily a strictly linear relationship between the number of command line tools there are and how painful dealing with dependencies etc is. This is definitely a good point.
Is your language open source? I'd be very interested to see.
&gt;This, sadly, is what's keeping me back from diving headlong into Servant Having to turn on extensions? Why would that hold you back?
Why not make use of Nix? Stack has support of building Haskell projects in Nix environment, and Nix solves the problem of managing non-Haskell dependencies and various environments encompassing them.
Even better yet, Cabal is obtaining Nix-style features. Will be interesting how those compare to Stack on Nix.
 &gt; Your solution is sacrificing the ability to return an accumulator, in exchange for the ability to terminate early. The index *is* the accumulated result. Edit: Coming back to this, I think I may have missed what you were referring to. You were suggesting that you can't use 'b' as a result when 'p' holds? i.e. 'b' is the accumulator? When folding right, 'b' is not an accumulator, it is rather what comes next. Hence why you don't want to use 'b' as the result when terminating early. 
If you wanted to return the index and the element then you use a tuple, e.g. indexOf p = foldr (\a b -&gt; if (p a) then (0, Just a) else (fst b + 1, snd b)) (0, Nothing) *edit: formatting (x 5)* 
&gt; The primary criterion I would use for considering the next generation of dependently typed languages is when the first self-hosting optimizing compiler emerges. what is a "self-hosting optimizing compiler"?
As someone new to haskell I keep forgetting about the lazyness properties from time to time. Thats a good reminder.
The simple solution is using MVar or IORef (they are almost the same things). You would do something like this (half copie/past from happstack site and I did not use `lift` where I should ) : main = do ref &lt;- newMVar "Test" serve Nothing (myApp ref) myApp ref = msum [ dir "echo" $ echo ref , dir "query" $ queryParams ref ] echo ref = do refValue &lt;- readMVar ref ok $ template "home page" $ do H.h1 refValue queryParams ref = do refValue &lt;- readMVar ref putMVar ref "queryParams was called" ok $ template "home page" $ do H.h1 refValue
[removed]
By the way, the word "function call" is a bit misleading here with respect to what functions are in Haskell. Something of type `IO ()`, like `putStrLn "hello"`, is not a function...it's an IO action. You don't want to delay a function call, you want to introduce delay in an IO action :) Functions in Haskell are things with types like `a -&gt; b`, such as `Int -&gt; Bool`, `String -&gt; IO ()`, etc. And in haskell, the way to "defer a function call" is to just pass the function itself, without applying an argument! :)
Cabal is supporting nix? I knew cabal is supporting installing multiple packages at once, using hashing and sharing in a nix like style. But what /u/gleberp is talking about is the recent change that allow you to run stack inside a nix environment entirely and get nix to supply your native dependencies. Is cabal supporting the second scenario? 
Unfortunately not. The short version is that the finer grained type information let's us statically evaluate most runtime and memory usage, allowing more ambitious whole-program optimizations. Unfortunately it also means we have to manually tune a lot of rewrite rule parameters to get reasonable compile times - it really demands a general machine learning optimizer that just isn't there yet. I'm hoping to write some declass stuff on it soon, but as sdiehl says, it's a long and thankless job. Having someone else working on it would be a good excuse to elevate priority ;)
I just chipped in! However I suspect that in these dark selfish times, many more people would donate if a "minimum donation" were matched by some sweet lambda-themed merch, e.g. a tshirt. 
Haskell is still in an exploratory stage. It is like the automobile industry at the beginning of the past century. There were cars with electric, steam, gas motors. Small garages produced cars for enthusiasts, but hardly your grand-grand-grandfather would change his carriage horse for a car to transport the stuff of his business. Lisp is a stable language in the sense that his community has an established way to doing things. I would say that it is a dead language. Haskell is too dynamic, has too much of everything: too much of good and bad ideas going on. Too much turmoil. It is at the beginning of a revolution. There is gold somewhere in Haskell, and the gold seekers are looking for it. And are noisy. They feel like pioneers and they happily share knowledge and tools, but as soon as someone find a vein and reclaim the spot, the good mood of the others changes towards him and the noise shift towards another unexplored yard. Everyone want his gold, so there is no common and simple way to do things. There is no glory in making things easy. There is much hype to attract attention. "Hey I found it!!!!" "No, I found It!!! they are polivariadic endofunctors over a n-subshchema indexed monad... This solves a crucial problem of humanity called folding and traversing and getters and setters!!!!" Haskell now is not about programming but about epic. Everyone want to be Buffalo Bill or at least a soldier in fort-apache. with the uncertainty and the limitations, would you use a carriage horse or an steam car at the beginning of the past century for your business? If you choose the second,probably you would need to change the dangerous and contaminating engine of monad transformers for another lighter and better and you would have some extra hindrances. but you would be in line with the future.
I would edit this sentence: &gt; If you just want to transform each element of a collection, but you don’t want to change the type of collection at all, you probably want a map. While not wrong, it does not clearly express the constraints of map/fmap. At first reading, I thought you were saying the type of the output must equal that of the given collection, which is clearly not true. Jeremy Gibbons has written up these constraints very well and I think you would do well to reference his work.
This very post gave me the chance to ask this question I always wanted to: Do Lispers manipulate functions (a.k.a. higher-order functions) as much as Haskellers? Perhaps they are more interested in marcos? I don't know, and I want to. Edit: I hope this question qualifies as a "spectrum" question.
;) Too many distractions; like: http://cs224d.stanford.edu/ where it would be cool to have some haskell library (with examples).
It is truly sad that you have nothing better to do in your life than wage a crusade against something as insignificant as a programming language. It's akin to doing the same for a hammer. If you disliked hammers, would you go online and spend hours arguing with people, trying to convince them that hammers suck? 
Screwdrivers are clearly superior. Nails are outdated tech anyway. ^/s
I for now ran with the MVar solution, thank you
thanks
Purely functional programming requires different kind of thinking about programming process. If you at first introduced a whole array of imperative and effectful techniques to newcomers, they would turn haskell into another imperative language with the use of monads, and iorefs, etc.. And they might never grasp the essence of functional programming, what denotational semantic feels like or how to use type system to build amazingly general and mathematically pure abstracts. Besides... Divine Mathematical Wisdom is far from boring... 
This is how I would approach an early terminating transforming loop: till :: (a -&gt; Bool) -&gt; [a] -&gt; [a] till _ [] = [] till p (x:xs) = if p x then x : till p xs else [] mapTill :: (a -&gt; b) -&gt; (a -&gt; Bool) -&gt; [a] -&gt; [b] mapTill f p = map f . till p 
But you don't. I use it and I have no idea what any of the extensions are.
I just read about Flow; I don't think another balkanization of notation is a sensible way of "writing more understandable Haskell". If anything, using your library forces a compatibility split upon the reader. Granted, Haskell is a platform for language experimentation and your work is a valuable addition to this discussion, but you should advertise Flow as such, I think.. Also, please don't take my criticism negatively. 
I think that one problem with your tutorial is too much GHCI early on. Beginners don't get to compile an actual executable until fairly late into the tutorial. I think tutorial should mostly use real programs instead of GHCI: &gt; still indulging more on the pure side of things IMO people aren't missing impurity, they're missing familiar control structures (early return, early loop exit), familiar ways to debug (eg. inserting printf anywhere) and are lost in "scary" names (`return` that's not really `return`? `null` for `isEmpty`? `cons`, `snoc` for `append` and `prepend`? not to mention scary operators). I think `return` and `break` are big ones. I've decided to try to implement a typical beginner-y program (enter numbers, then show which ones are even): module Main where import Control.Exception (Exception, throwIO, catch) import Data.List (intercalate) import Data.Char (toLower) import Safe (readMay) data BadInput = BadInput String deriving (Show) instance Exception BadInput main = do let getNumbers = do putStrLn "Enter a number (or done to finish)." string &lt;- getLine if map toLower string == "done" then pure [] else case readMay string of Just number -&gt; do remaining &lt;- getNumbers pure (number:remaining) Nothing -&gt; throwIO (BadInput string) catch (do numbers &lt;- getNumbers putStrLn ("Even numbers: " ++ intercalate ", " (map show (filter even numbers)) ++ ".")) (\(BadInput string) -&gt; putStrLn ("Not a number or \"done\": " ++ show string)) This is honestly the best I could come up with (I'm a beginner myself). Without the use of exceptions there'd be even more nesting. I skimmed the docs of `Control.Monad`, and found nothing that would help me. Basically, where a `return` would be in an imperative language, there has to be a level of nesting. Python version: def main(): numbers = [] while True: print("Enter a number (or done to finish).") string = input() if string.lower() == "done": break try: numbers.append(int(string)) except ValueError: print('Not a number or "done": ' + repr(string)) return print ("Even numbers: " + ', '.join([str(x) for x in numbers if x % 2 == 0])) main() Shorter and much less nesting thanks to `return` and `break`. And, the early return loop example from the OP's tutorial still looks quite "scary" and relies on understanding of monads, Either and Maybe: indexOf' list element = let test acc e | element == e = Left acc | otherwise = Right (acc + 1) in case foldM test 0 list of Left i -&gt; Just i Right _ -&gt; Nothing Why can't it be: foldlSome :: Foldable f =&gt; (r -&gt; a -&gt; FinishOrContinue r) -&gt; r -&gt; f a -&gt; r indexOf' list value = foldlSome test 0 list where test index element | element == value = Finish index | otherwise = Continue (index + 1) You can't really revert to familiar imperative style in Haskell. Aside from the obvious lack of `return` and `break`, eg. [mutable `Vector` API](https://hackage.haskell.org/package/vector-0.11.0.0/docs/Data-Vector-Mutable.html) doesn't even have `append`. And the naming of mutable reference API and the way you use it certainly don't help: (i.e. `a &lt;- newIORef 2 :: Int` vs `int a = 2;`, `modifyIORef a (+ 1)` vs `a += 1`, `do aValue &lt;- readIORef a; func aValue` vs `func(a);`). &gt; Most of these concepts are intertwined, so perhaps they make little sense if considered in isolation. This has traditionally given Haskell a bad reputation for displaying a steep learning curve and being "too abstract". Maybe because it's true? Eg. `Either` is used everywhere, for early loop return, for error reporting, etc. Why can't there be more specialized types? data Result e a = Fail e | Ok a data FinishOrContinue a = Finish a | Continue a
&gt; is Lisp in some sense still more powerful than Haskell? My only reddit comment to ever get gilded was an answer to this very question, in a [thread](https://www.reddit.com/r/haskell/comments/29yilg/most_powerful_highest_level_programming_language/) about the same Paul Graham article. Here it is again: &gt; I think Lisp might be more powerful than Haskell, in the sense that given equivalent libraries and the same (large enough) task to solve in both languages, the Lisp version can probably be made shorter and easier to read than the Haskell version. &gt; &gt; However, the Haskell version will be easier to modify. &gt; &gt; The reason for the succinctness and readability of Lisp programs is the usual one: instead of writing the solution in Lisp, you can write the solution in a macro-based custom dialect which is perfectly adapted to the problem at hand. Importantly, this dialect can be anything: it doesn't have to satisfy a type checker, and it can have its own custom non-prefix syntax (like LOOP). Haskell doesn't have this luxury. &gt; &gt; When the program needs to be modified, this lack of limitation becomes a disadvantage for Lisp. To the Lisp compiler, macros are black boxes which are free to do anything. In particular, any tree of s-expressions is a valid input and a valid output. So when you change something in the custom dialect, the compiler will happily pass the old, now-incorrect programs to the new macro, and there will be a large number of failures, and those failures will be in the generated code, which will be hard to trace back to the original code in the custom dialect. &gt; &gt; In Haskell, we also get to write our final solution in a dialect which is close to our problem domain: a custom combinator library, such as a custom monad, applicative, or category. Since this custom dialect must fit into Haskell's type system and use Haskell's existing syntax, it might not be the most succinct and readable dialect in which the solution could be written, but it can still be quite good. Critically, the compiler understands our dialect: thanks to the precise types, it knows exactly which inputs are acceptable to each combinator, so the type errors are triggered by expressions in the dialect instead of some generated code.
I would have begun by teaching the recursive IO implementation, reassuring readers that everything which they are used to write with `while` and `for` loops can be written in Haskell as well using this idiom. Only then would I introduce the other forms, as various shortcuts for commonly-encountered loop-like transformations, and I would encourage readers to implement their own loop-like constructs whenever they notice that they're writing multiple recursive loops which have the same shape. That's just my hunch about how readers would learn best. If someone has some experience comparing different teaching approaches I'd be quite curious to hear your comments.
Nothing says you can't use your own types, in fact you're encouraged to do so, but only if these are effectively needed. Your `Result e a` is identical to `Either a b`, for example. And, let me poke some fun here: doesn't any language "rely on the understanding" of something? Or do you think we are born with innate knowledge of for loops and list comprehensions, and why something like `"Even numbers: " + ', '.join([str(x) for x in numbers if x % 2 == 0])` produces a string and appends a list of string representations of results ? Also, I don't understand why you mention mutable variables in this context. 
Where I wage a crusade against Haskell? Is how things are going for Haskell, which in 2015 should have widespread adoption what make me to react against. My advice in the comment is to use Haskell anyway... I fight and will ever fight narrow minded elitism and artificial complications and bad decisions in the Haskell community until Haskell gain widespread adoption. That is my goal and my commitment.
A couple of years ago I did a [talk](https://github.com/deech/LambdaJam2013Talk/blob/master/Talk.pdf) on translating imperative idioms to Haskell to ease the transition from other languages. Doesn't assume any Haskell experience.
- Meta-platform blah blah, second-generation UNIX microkernel blah blah, functional package manager blah blah. - You can build an awesome website/game with it! Wikipedia/CryEngine are made with it! Which is a more compelling argument? What I mean is that C++/PHP/C# have much more visible success stories than Haskell, not that Haskell has no success stories. &gt; seL4 microkernel Never heard of it. &gt; Darcs Aren't even Haskell people switching from it to git these days? &gt; Nix Apt-get and yum/dnf (and soon Ubuntu Snappy) are surely more successful.
Just a quick note, your qsort implementation treats the list like a set. You're missing an equals case (LTE or GTE) in either of your filters
Honestly, you sound like the one that is narrow minded. Read your comments, man. IO in Haskell uses monads. A lot of people prefer to know what is going on behind the scenes. And they probably should know too. That's not to say that you can't show them how to do IO without teaching monads, but if you're going to learn, you may as well *learn* and not just parrot. Also, just because something requires you to put in effort does not mean that it is elitist. If only the "elite" can put effort into something, then the world is fucked haha.
I would probably add a paragraph or two to discuss why it is often better to use map, fold or filter instead of a general for loop of the kind we find in iterative languages. It allows us to reason about the code much more quickly when we are looking for an error. A map will never change the number of elements and one element will never influence the transformation of another. A filter will never increase the length of a list or insert an element that was not in the original list. This makes it much easier to exclude portions of the code using these functions from consideration when looking for a bug that shows one of those symptoms when compared to excluding a for loop as the culprit. That is true in any language, not just in Haskell.
Good topic for Reflecting on Haskell in 2016...
I am not a package maintainer but reading that list of things to check it feels like there should be some automated tooling to aggregate all the TODOs for Stackage and possibly other sources (e.g. people doing experimental builds of packages on GHC HEAD or new release candidates or with updated dependencies or non-building Hackage documentation) for a given package or even a given maintainer. It could also include a date for when the issue was first discovered so Hackage trustees can see when they might need to step in with abandoned packages and perhaps even an automated list of reverse dependencies affected by a given blocking issue.
None of what I proposed "takes" anything "intellectual" away from Haskell, except the `Functor` -&gt; `Mappable` proposal which I myself had doubts about. &gt; I tried showing you something, but you don't want to listen. Same here. I tried showing you that lowering the entry barrier matters. You don't want to listen. I tried showing you what normal human goal-oriented thinking is. You didn't want to listen. &gt; Listen, what's your angle? Are you raising an "argument by crowd", invoking some imaginary "people who come to Haskell" and regret they left Java or C# (?!? show us the relevant data, please). Well, the article agrees with me pretty much: &gt; Throw in all this business with endofunctors and burritos and it’s pretty clear that a lot of newcomers get frustrated because all this theoretical stuff gets in the way of writing algorithms that they already know how to write. In other languages, these newcomers are experts and they are not at all used to feeling lost. Why are you criticizing me, not the article author?
"normal", "human". We're all robots here instead. What about giving an actual contribution, rather than whining about "the state of things"? The "lack of success"? 
You're right, sorry. Read too quickly :D
An unsafe binding to `require` can be coded in about 5 lines using the FFI. Usually, we've moved `require` calls into the JS modules, to make things slightly safer.
The philosophy throughout the PureScript community is to have small pieces that each do one thing well. This is why Pulp does not contain any mechanism for remembering commands like this - there is already a perfectly good solution, and we want to keep pulp as simple as possible. You could read the entire source code of pulp in an afternoon and I really want it to stay that way. I honestly think the only reason you want pulp to be more like stack is because you're already used to it. If you think about it, "just use stack" is nowhere near sufficient information for a new Haskeller to achieve this stuff. You need to learn what a cabal file is, what stack.yaml is, you have a add a new section to your cabal file for each target, you have to remember to add each module to your cabal file otherwise it will break after you push code to hackage... I think you've just become sufficiently fluent with stack that it fades into the background. If you work with pulp for a little while the exact same thing will happen. You are correct that gulp is probably a better choice if you also want to use webpack, though, at least for the time being.
I use both. There are a few axes to consider. Intellectual temperament: Some people cling tenaciously for years to the first wild idea they encounter, others embrace new ideas so rapidly that they never get anything done. There is a happy medium that few people attain. Lispers lean the first way; Scheme could be considered an exercise to explore closures. Get over it! Haskellers lean the second way; Haskell is the most fertile hot bed for new ideas of any language today, but sometimes it is time to just get things done. Syntax: With editor support and syntax coloring to tame parentheses, Lisps can have a very clean appearance. This is somewhat offset by editor auto-completion and the fact that lispers are often ferocious touch typists; function names tend to read like German compound nouns. Haskell has a denser, more expressive syntax, but each new idea adds cruft, making Haskell look increasingly like Perl line noise. It should be a source of extreme embarrassment that one cannot write a complete one-page parser for Haskell, and that such a parser isn't a core part of the language. Pure/impure, static/dynamic, lazy/strict: The imperative parts of Lisps are stones jamming the gears of abstraction. One should only judge the use of types after one has mastered them; it is then impossible to comprehend how anyone can say "I don't know what type this value will be; my compiler doesn't even know!" as if that is a good thing. Lazy is not an arbitrary choice, but part of Haskell's power. For instance, macros are a band-aid over weaknesses of Lisps; in Haskell one writes many of the same constructions as ordinary functions. That said, it's easier to play pool if one doesn't have to call one's shots. One can't appreciate this debate until one can feel viscerally how dynamic programmers see static programs as corpses, and static programmers see dynamic programs as about to die at any moment. Once one has internalized static typing, one sees that type theory is the language of abstraction, a huge win for Haskell. The future of Lisp will originate in Haskell. Reflect on the history of group theory in mathematics: At first, people separately studied permutations, matrices, symmetries, then gradually these topics received a uniform treatment as abstract groups. It is still sacrosanct even in Haskell that putting two symbols next to each other somehow relates to function application. All of the new ideas for combining adjacent symbols involve wretched operators, and are still in their infancy (e.g. one needs monad transformers to combine monads, this too will pass). When all this matures, we'll be able to look back on (f x y) function application as the most primitive of permutation groups, as we enjoy the equivalent of arbitrarily structured abstract groups. When everything settles down, and we've weened ourselves from Perl line noise to specify how to combine adjacent symbols, that language will look like Lisp. With parentheses. 
That's a great idea!
Not really helpful for your case but I find the behaviour in e.g. C++ parsers where a failed type parse is replaced by int and you get a whole bunch of followup errors related to that replacement more annoying than helpful. Depending on the language you want to parse it might not be very useful to attempt this.
If you're looking for some sort of official support, I'd start with the community mailing list. If you're looking for design ideas or help I'd use cafe. I was wondering if some of the online custom merchandise services can be setup to donate a portion of the cost. That could greatly simplify things.
Why not singletons? - No throwaway classes for computation that also clutter code with their constraints. We can just write functions with a `Sing x` argument or a `SingI x` constraint, and that's all we ever need. - `reflectName = show . fromSing`. Similarly, there's no need for other `Reflect` classes. - Runtime modification/generation of API-s included for almost free. - Proper type-level functions could offset the relatively lower flexibility compared to `servant`. For example, the user could plug in functions to customize API type interpretation. API-s could also have poly-kinded parameters which users could plug in with custom kinds, and we could demand type functions or kind class instances with which we could interpret them. With `servant`, there is a need for classes for interpretation, since it has an open API universe. With a closed universe, I think singletons are a better choice. (that said, the reason I haven't written this myself is because I'm entirely ignorant of web dev and web API-s, so my comments can be freely taken with a grain of salt)
We were talking about running some sort of contest for an unofficial t-shirt design we could use for just such a purpose :-) But nobody had the time to actually get such a thing in gear. If you wanted to step in to help organize that, it would be great!
I don't think they reasonably could. See the list of differences at the bottom.
Article author here. &gt; And, the early return loop example from the OP's tutorial still looks quite "scary" and relies on understanding of monads, Either and Maybe Thanks for the feedback. I agree. It looks like crap. I'm going to update the post to suggest a tail-recursive loop instead. indexOf' list element = let step l index = case l of [] -&gt; Nothing (x:xs) -&gt; if x == element then Just index else step xs (index + 1) in step list 0 On reflection, I think this is one of the most important things that a new Haskeller can learn. Once you can mindlessly slam out tail recursive loops without thinking about it, you're past needing `continue` and `break`.
You've gone full-on ridiculous at this point. Do you think Facebook would let someone that doesn't know what a monad is, alter Haxl code? Have you even seen the documentation? https://github.com/facebook/Haxl/blob/master/example/facebook/readme.md It's not something that you can just parrot. Haskell is not a difficult language, it just takes a small amount of time and effort. I think anyone is capable of learning it in 3 months time, even if they have a full-time job. You sound like a lazy POS right now.
Not compelling vs Python... Still requires mental gymnastics to decipher. Easy and natural: def indexOf(list, element): for i in range(0, len(list)): if list[i] == element: return i return None Maybe take more advantage of Haskell syntax? I don't know. I think this one is a bit better: indexOf' list element = step list 0 where step [] _ = Nothing step (x:xs) index | x == element = Just index | otherwise = step xs (index + 1)
`takeWhile`?
That's the spirit!
I'm much more likely to donate if I get credits for it on the haskell.org site. This is the way to do it: https://gnupg.org/donate/kudos.html Not a coincidence that I donate annually to GnuPG.
Hello! This is certainly getting air time earlier than I expected—Serv is (despite, perhaps, the README which I was bulking up recently) a very young idea I had while working with Servant at work lately. Servant itself is a far more developed codebase and ecosystem and Serv is merely me trying to address a few questions or alternative design ideas I had regarding it. Perhaps the biggest single difference between Serv and Servant is that Serv embraces a closed-universe API type (which is why I punnily call it "kinder") which leaves its API types less open to extension and more open to analysis. This tradeoff is a somewhat fundamental, but largely stylistic difference. That said! I am extremely happy for feedback on all fronts and would love to discuss what directions this somewhat more kind-happy system can go in.
Most likely because I'm not as familiar with Singletons. I *loathe* the Reflect classes though so I am definitely going to examine this approach more carefully! Thanks!
It is exactly that. They could conceivably share some code in that some of the interpretation classes are very similar, but Serv aims to take full advantage of being a closed-universe whereas Servant is doing the same with their open-universe design. That said, see Andras' comment for something that could be done to take more advantage of the closed universe which Serv does not currently take advantage of.
It'd be pretty reasonable to do in Servant. There's not a big trick. I mostly use it so that server implementations can access the current path context (useful for generating tailored CORS responses).
I agree, but my aim with this post isn't to demonstrate ways in which Haskell is better than Python. My goal is to offer some very specific unblocking advice for a someone who has already decided to try this Haskell thing out, but is having trouble writing the loops they need to write in order to express algorithms that they already know very well.
`indexOf list element = length (takeWhile (/= element))` is pretty natural, unless you consider `(/= element)` difficult.
I completely agree with the sentiment of this post, but in practice I find the extra layer of wrapping and unwrapping too tedious to use. The kinds of invariants described here (eg the value should contain no `Foo` constructors) are straightforward to express and verify in a refinement type system like LiquidHaskell, which crucially does not force you to define a separate Haskell type for each invariant. Of course, I'm biased here :)
[removed]
I was thinking of something as simple as reproducing the haskell.org header on black T's . Not original, I admit, but it should suffice to keep the [narwhals](https://wiki.haskell.org/wikiupload/8/85/NarleyYeeaaahh.jpg) at bay this time ..
Lens would *greatly* benefit from newtype's "hiding the definition": type synonym expansion is what makes lens type errors wholly unhelpful. On the other hand, using `type` makes lenses independent of lens libraries, which will have to do for the time being. &gt; but as far as I know the semantics of Lens are entirely described by the type signature The same way, the universe sans gravity is described by the [Lagrangian of the Standard Model](http://i.imgur.com/jXP7So9.png), but there is much need to abstract over it nevertheless. I don't think many people would see the lens laws follow freely from that type.
I share your frustration with the bolted-on feeling of LH and other automatic verifiers. I wish Haskell had built-in support for writing expressive contacts. That would greatly benefit the tooling situation IMO: verifiers like LH could check them, haddock could include them in the docs, you could derive test suites with a QC-like tool (you can actually do this with LH using our `target` library), etc. As to GHC, sadly we have not tried to apply LH to their type synonyms yet. I think it's a great fit, and it's been on our to-do list for a while, but we haven't gotten around to it yet. 
&gt; ...Jeremy Gibbons has written up these constraints... link? I'd like to check that out
&gt; Some shift happened with the dawn of the current internet. Everyone with access to WebMD is now a doctor. Some shift happened when printed books became available. Everyone with a catalogue of diseases is now a doctor. &gt; It may be a bit traditional to think that "something worth learning will take time", but I definitely don't think it is narrow minded elitism. There are also things that aren't worth learning that take time. How do you know that that's the case? Why, if learning it doesn't seem yield any meaningful results for some time, you stop and look around and ask: did learning this help anyone else? If the answer is no, the only sensible course of action is to stop. Yes, there's a chance that it's actually worth learning, but without such a heuristic, we would constantly waste time. For example, you're learning Haskell and you're frustrated, and there's still a long way to go before any I/O. Then you look around. - Wikipedia, WordPress, Drupal - PHP. - AAA games, Photoshop, Firefox, Google Chrome, Qt, Maya - C++. - Microsoft Office, Microsoft EDGE - C++/C#. - Linux kernel, git - C. - Blender - C/C++/Python. - C++ - an industry standard - TIOBE top 3 - Visual Studio - visual GUI design. - C# - made by Microsoft - TIOBE top 5 - Visual Studio, MonoDevelop - visual GUI design. - Java - made by Oracle - TIOBE top 1 - Eclipse, IntelliJ IDEA, NetBeans - visual GUI design. See it now? Haskell experience should be as pain-free and let people write something meaningful as early as possible. As funfunctional said: &gt; Everyone that don´t know Haskell think that is a place where white flying unicorns transport princesses over a lake of pure waters. Haskell should look as close to this as possible. IMO it certainly has the foundations.
*I think a consensus coming out of the functional language community is that limitations on power are important in making reliable software* This is true, but not all "limitations on power" are created equal. You want a language where the limitations have been chosen very wisely to improve safety without limiting the power to create abstraction, and where you have freedom to opt out of those limitations when it's necessary to do something that is not provably safe. Lisp-family languages have essentially no limitations at all. Haskell, Idris, Rust, and a few other new languages have wisely chosen limitations, but offer escape hatches to be used only when absolutely necessary. Some other languages have poorly chosen limitations that limit expressiveness without providing safety--Java and Go come to mind.
It's in https://github.com/tel/serv/blob/master/src/Serv/Internal/Server.hs
&gt; newtype Lens s t a b = L { unwrap :: forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t } There are good reasons to use a newtype here. It makes the type abstract, so you can change the representation later. If we were making only a library that only deals with lenses, I would advocate a `newtype` wrapper. The reason that the Lens library doesn't do this actually doesn't have to do with the fact that the semantics of Lens are entirely described by the type signature. In fact, the semantics of Lens is not entirely described by the type signature. If `l` is a lens we have invariants that `l Identity === Identity` and `Compose . fmap (l g) . (l f) === l (Compose . lmap g . l)`. This invariant is something that could be (in part) be enforced by a `newtype` and smart constructors. The problem is that once you have lenses, and traversals, etc. then you need to punch a hole in the newtype so that we can have different constraints: newtype Lensy f s t a b = L { unwrap :: (a -&gt; f b) -&gt; s -&gt; f t } type Lens s t a b = forall f. Functor f =&gt; Lensy f s t a b type Traversal s t a b = forall f. Applicative f =&gt; Lensy f s t a b In this way a `Lens s t a b` is a "subtype" of `Traversal s t a b` in that every value `foo :: Lens s t a b` also has type `foo :: Traversal s t a b`. Another reason is that when you compose lenses and traversals you get a value which has the optimal constraint. i.e. a lens composed with a lens is a lens, but a lens composed with a traversal or a traversal composed with a lens is a traversal. The `Lensy` `newtype` is a point in the design space of a lens library that could be worth exploring. However, you do lose the ability to use `(.)` to compose lensies. I think some people might actually prefer that lensies had their own composition operator. If you do want to use `(.)` then you need to break apart the newtype to expose the `-&gt;`. And once you add profuctors into the mix with type Optical p s t a b = p a b -&gt; p s t there is no place left to put a `newtype` wrapper to do anything useful.
Haskell Curry was an avid bird watcher. He documented observations of over 150,000 bird species. That was the motivation for the bird metaphor in Smullyan's book. 
Right, I was specifically talking about the non-lazy, finite cases. `[]` in that context has a different intention.
I do return 405 for missing methods. More or less at the moment Serv is status code unaware. It was tempting to try to encode those possibilities, but the power/weight ratio felt bad.
If you do this, I'm totally picking up Serv.
I've wanted this forever. The problem with `list-fusion-probe` is that it doesn't work with `Text` or `Vector`, and with vectors in particular I want to know if an expression is fusing, or if I need to drop into a mutable vector.
&gt; Linked lists are almost never what you want Besides specific use cases such as that one - the real point is that the type `[]` is *not* about "linked lists". You almost never care about "linked lists" or any other under-the-hood compiler implementation detail. You almost always *do* want lists. Haskell is about denotational semantics. The type `[]` is one of the most simple, fundamental, and important types semantically. If you have a specific performance problem you need to optimize away, then by all means, use the various powerful tools that Haskell+GHC provide for that, such as `Data.Vector` - but be prepared to pay the price in complexity. Don't fall into the trap of premature optimization.
I guess all I was trying to do was point out that lazy right folds naturally allow early termination. You don't need monads as per the post originally had. From the Haskell wiki on fold: &gt; One important thing to note in the presence of lazy, or normal-order evaluation, is that foldr will immediately return the application of f to the recursive case of folding over the rest of the list. Thus, if f is able to produce some part of its result without reference to the recursive case, and the rest of the result is never demanded, then the recursion will stop. . &gt; I did not mean for it to come off as negative criticism. I did not infer it that way, and to be honest, the code I posted was far from perfect. I was just trying to illustrate a point. Just for fun, here is *another* version which improves on mine: indexOf p = foldr (\a b -&gt; if (p a) then (Just (a, 0)) else (fmap.fmap) (+1) b) Nothing indexOf ((==) 2) [1..] &gt; Just (2, 1) indexOf ((==) 0) [1..] &gt; ⊥ indexOf ((==) 0) [1,2,3,4,5] &gt; Nothing
I seem to share very much your understanding for the need of such a Haskell library--which would make it possible to program such device as "graphviz". I've written about this at &lt;http://mathoverflow.net/a/203099/13991&gt;: &gt; One thing I've been missing from **graphviz** is being able to do more programming (at least, being more flexible in changing the shapes for abstract types of nodes and arrows). &gt; &gt; Then, I knew there is the powerful **diagrams** library in **Haskell**, which however didn't give me straightforwardly the feature of arranging the nodes automatically, as graphviz would do. (Although this must be implementable, of course, but going for a simpler solution, I'd like to use the existing graphviz code.) I'd like to be closer to the simple graphviz usage model. There, I have pointed to 2 Haskell libraries I have found: &gt; There is a [graphviz](https://hackage.haskell.org/package/graphviz) library, which allows to program on top of graphviz in different styles, say: &gt; &gt; * using a graph type; &gt; * in a graphviz monad which closely follows the way you write graphviz files, but is programmable and type-checked! Look at [a good example with explanations](https://github.com/haroldcarr/learn-haskell-coq-ml-etc/blob/master/haskell/paper/haroldcarr/graphviz/2014-02-28-using-graphviz-via-haskell/2014-02-28-using-graphviz-via-haskell.org#creating-dot-graphs-from-a-datagraphviz-haskell-representation). &gt; * ... (more?) &gt; &gt; A short example of the monadic notation from [the documentation](http://hackage.haskell.org/package/graphviz/docs/Data-GraphViz-Types-Monadic.html): &gt; &gt; digraph (Str "G") $ do &gt; &gt; cluster (Int 0) $ do &gt; graphAttrs [style filled, color LightGray] &gt; nodeAttrs [style filled, color White] &gt; "a0" --&gt; "a1" &gt; "a1" --&gt; "a2" &gt; "a2" --&gt; "a3" &gt; graphAttrs [textLabel "process #1"] &gt; &gt; cluster (Int 1) $ do &gt; nodeAttrs [style filled] &gt; "b0" --&gt; "b1" &gt; "b1" --&gt; "b2" &gt; "b2" --&gt; "b3" &gt; graphAttrs [textLabel "process #2", color Blue] &gt; &gt; "start" --&gt; "a0" &gt; "start" --&gt; "b0" &gt; "a1" --&gt; "b3" &gt; "b2" --&gt; "a3" &gt; "a3" --&gt; "end" &gt; "b3" --&gt; "end" &gt; &gt; node "start" [shape MDiamond] &gt; node "end" [shape MSquare] &gt; &gt; Some overview of the relations between the existing Haskell graph packages and graphviz: [[Haskell-cafe] Generic Graph Class](https://mail.haskell.org/pipermail/haskell-cafe/2009-June/063402.html). &gt; An alternative to "graphviz" Haskell package mentioned in [haskell-cafe](https://groups.google.com/d/msg/haskell-cafe/ZfZaw2E9a18/xZ0OeHCGzVgJ) is [dotgen](http://hackage.haskell.org/package/dotgen). &gt; &gt; In [a follow-up](https://groups.google.com/d/msg/haskell-cafe/ZfZaw2E9a18/9P-dazcd0FsJ) to the post mentioning `dotgen`, the author of graphviz gives some comparison between them (and other similar Haskell libs). (Then, I had some concerns regarding Haskell IDs vs string IDs for the nodes, which had been resolved by an advice from the author of graphviz.) What are the important differences between your `haphviz` and those ones? (Speaking about a Graph class reminds me very much some postings of the author of the Graphviz library; so, you must be thinking in the same direction, at least, in parts, and might find good stuff in each other's work.) 
Oh I see, like [this](https://babeljs.io/docs/usage/require/)? That's interesting. Why do you want that?
If there is a better tool that already does this, or a better workflow that makes it unnecessary I am all ears! I maintain several dozen packages and sometimes find that I have local changes that somehow never made it onto hackage. I now have scripts which call this utility for all the packages I maintain making it easy to spot changes I have not uploaded to hackage yet.
This is my first attempt at pushing singletons into the system. I had to do it all manually because `Symbol` is a bit funny, but I wanted to go ahead and throw this out there to see if this was the kind of thing you were expecting when talking about using `singletons`. https://github.com/tel/serv/blob/feat-singletons/src/Serv/Internal/Header/Name.hs
TIL (as a result of your post) about [another](https://hackage.haskell.org/package/diagrams-graphviz-1.3.0.0/docs/Diagrams-TwoD-GraphViz.html) point in the design space that makes me very happy. [`diagrams-graphviz`](https://hackage.haskell.org/package/diagrams-graphviz) supposedly uses `graphviz` to layout the graph and `diagrams` to draw it.
When it is in a shape you like better, feel free to post here again. It is already looking like a nice library.
&gt; There are also things that aren't worth learning that take time. You're right, but I think we need to have the same definition of "worth" in order to properly discuss this. I spent 6 months learning Java and I don't think it was "worth" it, to be honest. I find the language to be incredibly cumbersome and So I'm sure it depends upon what your end goal is. If you want to build the next big startup, learn something like Meteor, don't bother with something like Haskell. &gt;For example, you're learning Haskell and you're frustrated Ah, so we both agree that the problem is a lack of *good* learning resources for Haskell. I'm glad we can agree on this. 
There's a compelling reason not to `newtype` lenses, which is that libraries can provide lenses of their own without depending on the `lens` library
By "name it", what I meant was that instead of creating patterns, write the abstraction as Haskell code, making it addressable from inside the language (the "naming" part). This applies to any kind of patterns, not just recursion patterns. 
What better way to unleash your inner C programmer than making off-by-1 errors!
Great! It's initial release seems to have been after my post, in 2015-09-18, that's why I missed it.
Looks like the fork is light-years ahead of the original.
What Haskell really suffers from is the inability to make types abstract at module boundaries. In Standard ML, you can make a type synonym inside a module, and then use opaque signature ascription to make it look to third party code like a newtype with its constructor hidden.
&gt; I've written about this at http://mathoverflow.net/a/203099/13991: Cool. Thanks for sharing this! &gt; There is a graphviz library, which allows to program on top of graphviz in different styles, say: &gt; &gt; * using a graph type; &gt; * in a graphviz monad which closely follows the way you write graphviz files, but is programmable and type-checked! Look at a good example with explanations. &gt; * ... (more?) This is the reason for Haphviz. Yes graphviz (the library) exists but: * It does way too much at the same time. Up to the point where you need "..." to describe it. * The monadic style programming was not a part of the library from the start. It was patched on later. * Its monadic module doesn't abstract away the identifiers. * It has a useless name. It's not recognizable by its name. I had to use parentheses to refer to it (call me nitpicky.) Dotgen also exists, but it was too basic in my opinion &gt; What are the important differences between your haphviz and those ones? (Speaking about a Graph class reminds me very much some postings of the author of the Graphviz library; so, you must be thinking in the same direction, at least, in parts, and might find good stuff in each other's work.) In short, Haphviz exists because: * I wanted to make the library do all the work. No manual identifier fiddling, no messing with the arrows, etc. Also, you may want two nodes with the same label and STILL not have to fiddle with identifiers. * I wanted a library that was designed with the monadic style in mind. * I wanted sensible defaults for every function. By default you want one attribute for a node: a label, and nothing else. By default you want a label for a node and not have its identifier be its label. * I wanted fewer dependencies. (Granted, this is probably a non-problem in hindsight.) 
So you don't think that benefiting from a free service and from the intellectual and manual work that many people gave away for free is reason enough to donate?
That said, IMO, Lisp's actual implementations are terrible. Every single one of them. Common Lisp is old and has a lot of awful ugliness (funcall to call a lambda, really?). Clojure is full of OOP idiosyncrasies due to Java. Scheme, possibly the best of them, doesn't respect purity, has verbose lambdas and isn't even curried by default - all of which are essential for real-world productivity. I'd say in any real-world scenario Haskell is much powerful than Lisp. If you just used the untyped subset of Haskell (which is basically Lisp) and was a robot that never made a single type error, then maybe Lisp is more productive for you. Also, please don't be deluded with Graham's "macro" stuff. He is right that macros are great, but you can express any Lisp macro in a lightweight Haskell DSL. The power is the same sans a very small constant factor.
Clear to whom? Emoji (and to a lesser extent, operator symbols) are far more universal. As someone who lives in a non-native language environment, pictograms are *enormously* helpful. God bless everyone who uses them liberally!
An excellent snippet to scare newcomers away...
Unfortunately, using `TypeLit`-s with singletons is a bit of a pain currently. We can get `Integer` and `String` values from the corresponding `TypeLit`-s, but they *don't even lift* in the opposite direction. If we use `Symbol`, we can't do anything on the term level since `Symbol ~ Void`. If we use `String`, we can't do anything on the type level. Ideally, `TypeLit`-s wouldn't exist, and we would just lift `Integer` and `[Char]` directly. This issue can be worked around the following way: abstract over the intended `Symbol` occurrences with a polykinded type parameter: import Data.Singletons.TH $(singletons [d| data Foo sym = Foo sym | Bar Bool deriving (Show) |]) Now we can write ``Foo "foo" :: Foo Symbol`, and `fromSing` happily converts `Sing (x :: Foo Symbol)` to `Foo String`. Here's an [example](http://lpaste.net/147509) for your headers. I also included an example at the end for using a kind class to let users plug in kinds other than `Symbol`. Similarly, you could abstract over the `Symbol`-s in your types in `Internal.Api` (and elsewhere), and use the singletons. 
It has slightly different features, but `mega-sdist` from [cabal-src](https://github.com/yesodweb/cabal-src) tells you which packages in a megarepo need to be uploaded to Hackage, which need version bumps, and has flags to set Git tags and run tests.
http://www.cs.ox.ac.uk/jeremy.gibbons/publications/uitbaf.pdf http://www.cs.ox.ac.uk/jeremy.gibbons/publications/iterator-msfp.pdf
&gt; IIUC you are saying that there are some infix operators that are so ubiquitous and well-understood that using their names doesn't make a difference. No, they're saying that the names are *more* confusing than the operators.
Additionally, we could of course *invent* unique names that are unambiguous and use those instead of the operators... but those names would as much mystery meat as the operators, from the point of view of the submitted article!
I think one thing that you really need for named rather than infix functions is variadic application. I don't want `compose (compose f g) h`, I want `compose f g h`. Natural this "desugaring" - if you can call it that - only applies to associative binary operations.
Haskell is still on the way towards more dependent type features. So while you can still do this particular example in base, it's just not so easy (it basically amounts to rebuilding a chunk of the refined package). I'm not aware of any plans to directly incorporate refined types into Haskell, but there's always Liquid Haskell, which is based around adding refinement types to Haskell. Some reasons as to why we don't, and might not ever have it in base? Well for one, Refined *cannot* even be a Functor, because we cannot lift arbitrary functions to run on it - they might violate the refinement, so we would need constraints on the function lifted, and then it's just not a functor. We'd need a new constrained Functor type, and then all the code that exists and works on functors *still* won't work on the new type. We could say that a `Functor` instance is really a `FunctorC None` instance where None is the trivial constraint that doesn't constrain, but this would probably be the largest breaking change yet in Haskell history, maybe even more than all of the breaking changes made so far put together. This would be like (but not as flexible as) adopting Control.Category into prelude, and then basing Functor off *that*.
It says you're missing a dependency. Coincidentally I installed ghc-mod today. You need to install "happy" and "haskell-src-exts" before installing "ghc-mod". I guess that those two packages will include "template-haskell" which you seem to be missing.
This has now been [resolved](https://github.com/chetant/LibClang/issues/59#issuecomment-166146248).
The evaluation between the two is similar, but the article hardly makes an ironclad argument. It's just an opinion piece and, even if you agree with the reasoning, you can reach different conclusions if you have different priorities. And the priorities between consumer products (which the article essentially assumes), professional tools and *a language* are drastically different. If a tool is designed for intense, long-term use, why optimize for the initial O(1) learning step? The problem with text is that you have to *read* it. I'm sure this is familiar to anyone driving on US roads where half the signs are text rather than pictures: "No turn on red.", "U-turn yield to right turn." and [so on][signs] (and [on][more-signs]). These signs might be clearer *at first* but are a real pain on an ongoing basis. Ultimately, it's the same thing as math notation. People love to complain about it, but have you ever tried rewriting all the notation in a math text to text? It bloats the whole thing up to double the size and, worse yet, *forces you to read everything*. With special notation I can get the gist at a glance and can easily skip around and look at the important bits, all without having to read too deeply. Operators do a few things really well. They're perfect for plumbing and naturally break code up into distinct parts. They let you see the *shape* of some code at a glance, without having to read them. For a lot of things—including lenses—these tradeoffs are worth the up-front cost of learning what the operators *are*. Using lens operators makes it easy to see the important parts at a glance: what's being operated on, what the lens is and what the operation is. Sure, you have to learn them, but it's worth it. This is not to say operators can't have problems, of course, but it's not nearly as clear-cut as people make it out to be. You'll note that a lot of UIs still rely on icons quite a bit and have some auxiliary method for making them clear (like a tooltip). In mathematics, this takes the shape of introducing your notation at the beginning of your text—sometimes even explaining common, conventional notation. If we do anything with operators, I think it should be along these lines, perhaps in tooling. (No reason we couldn't have tooltips based on Haddock, for example.) [signs]: http://www.trafficsign.us/r10.html [more-signs]: http://www.trafficsign.us/r4.html
indeed, a good notation helps one manage complexity. 
Perhaps a tool that can convert a block of code back and forth from operator-heavy to long form would address your issue?
I think you're understating how much laziness matters here. It's not that you can occasionally "get away" with using `[]`, it's that `[]` becomes a *control structure* for sequential computations. It's also incredibly simple, predictable and easy to work with. Often, `[]` just ends up as glue between two parts of your code and the actual list is never in memory completely. In that case you still have a bit of overhead (which is why list fusion is useful), but it's not a big deal. `[]` might not be a great data structure, but it makes a great interface, and that's how it's usually used. Put another way: I think `[]` is almost always what you want *at first*, until you try to use it to store bulk data and run into performance problems. Something like 90% of the uses of `[]` in my code can't be replaced with `Vector` or similar (with the exception of `String`).
Oh, what sort of company is it and what's the use case? I'm always curious about how people use custom languages in production. Also, it took me a bit to realize cbpv stood for "call-by-push-value". I googled it but didn't get the [results I was expecting…](https://www.google.com/search?rlz=1C1CHFX_enUS658US658&amp;espv=2&amp;q=cbpv&amp;oq=cbpv&amp;gs_l=serp.3..0l3j0i30l5j0i10i30j0i30.3013.3921.0.4156.4.4.0.0.0.0.395.918.0j3j0j1.4.0....0...1c.1.64.serp..0.4.916.tX4ZVULu0uI)
Tremendous! Thank you
It *is* different. For instance, if you define an abstract type `Foo` that is internally implemented as an `Int`, you can have an `IORef Foo` that only your module sees as an `IORef Int`. Also, you could define `fooize :: IORef Int -&gt; IO (IORef Foo)`, which internally is just `return`, but would be externally unimplementable as such. With `newtype`, you cannot do this. But, even if you don't care for this particular trick, making types abstract at module boundaries automatically, without having to plumb newtype wrappers and unwrappers, is simply too convenient not to want to have.
It's easier to Google for a name, though since Hoogle can search for symbols, for Haskell at least that's a bogus argument. If you need to read code aloud, e.g. `fmap` works better than "less-than, dollar, greater-than". Actually, I tend to read symbols as their equivalent names anyway. Finally, I'm guessing that at least some programmers have a hear-it-in-their-head thinking style. Telling a mathematician that a single glyph that most people don't know how to pronounce is less readable than a name is clearly a doomed conversation where you'd probably end up being stoned to death for heresy, and computer science obviously has a strong mathematical aspect. We should be grateful that unicode hadn't caught on before the earliest versions of Haskell were developed, or else no doubt every single keyword and library identifier would be a single obscure glyph. I wonder what the pile-of-poo glyph would represent? 
I felt like if I do not extract data from monad explicitly, it will not get evaluated. So I was trying to get the m monad evaluated. But it seems like that was not the case. Thanks for the comment.
A lot of operations in Haskell tend to be *very* generic, to the point that naming appropriately could be quite difficult and less informative.
Probably a stale compiled version of Setup.hs, try: `$ rm -r dist/setup`. That is if you're even building from source or are you using `cabal install ghc-mod`?
Also, many of the `pipes` operators obey the same relations that algebraic operators do. For example, take these two arithmetic laws: (x + y) * z = (x * z) + (y * z) 0 * z = 0 If you substitute `(+)` with `(&gt;=&gt;)`, substitute `0` with `return` and substitute `*` with `(~&gt;)` you get the equivalent `pipes` laws: (x &gt;=&gt; y) ~&gt; z = (x ~&gt; z) &gt;=&gt; (y ~&gt; z) return ~&gt; z = return ... and many other arithmetic laws hold, too (i.e. `(&gt;=&gt;)` and `(~&gt;)` are both associative and have the appropriate identities). This is why I use the rule of thumb that operators are okay if they are associative, because then users can more easily internalize their meaning by analogy to arithmetic operators. However, there are still operators in `pipes` that are not associative (such as `(-&gt;&gt;)`), which I can legitimately see being a problem by my own rule and some people have argued that those should at least have named equivalents.
You might also consider stm which has better support for transactions. 
[Potentially related thread](https://www.reddit.com/r/haskell/comments/1i0uiq/do_naming_practices_hurt_haskell/)
The Article does actually mention that. Definitely worth reading ;-)
I recently had trouble with installing ghc-mod using Stack. Is there a reason why ghc-mod isn't listed on Stackage lts-3.17? 
Totally off topic, but I had the opportunity to work at Gunma university (non CS role) and I had no idea there was Haskell work being done there....interesting. Thank for the link too.
&gt; The only reasonable argument I've heard against using operators There is also the one about operators being harder to Google but Haskell has plenty of special purpose search engines which makes looking them up as easy as a normal function.
&gt; Just because Naturals and Integers are isomorphic doesn't mean they're the same. I am not sure that the Vladimir Voevodsky would agree. ;)
Math notation is actually a really messy thing but the use of operators is probably the least of its problems. I would argue that its problem is the lack of an accepted, standard way, to digitally type it out, one that can be used in search engines to look up the meaning of a given piece of notation. This is made worse by the lack of consistency, even inside a given field.
&gt; A good UI is transparent and intuitive. It should be simple enough to deduce its function I disagree. That is just one type of good UI, the one that should be used for applications used mostly by casual or new users. A totally different type of UI that requires more up-front learning but wastes less efficiency on discoverability is suitable for programs used by long-term users over extended periods of time.
No problem, good to hear that you found it interesting. NB: I'm not the author. 
You could use a lazy pattern to avoid the use of fst/snd in the else branch: indexOf p = foldr (\a ~(i,mb) -&gt; if (p a) then (0, Just a) else (i + 1, mb)) (0, Nothing) Or, you could zip the input list with the index: indexOf p xs = foldr test Nothing (zip [0 ..] xs) where test (i,a) b | p a = Just (i, a) | otherwise = b I like the second one a little bit more, as you only get an index out when the predicate was satisfied :) (Edit to remove vim space characters)
Reading the OP's blog as well as the inline-c code, it's not meant to protect you completely from the low-level languages. Why bother? So, let's make access easy and seamless as possible when a programmer needs such access. It's vague from your description what your use case may be. But reading in between the lines, why not use Haskell to express your threaded computation?
TIL lazy pattern matching! :) Thanks for pointing that out.
I wish Johan had any scolarships or grants for this. I'd definitely consider a PhD working with it. Highly interesting stuff!
I'm really looking forward to this. Although I may be being a bit ambitious with my proposal :) 
I clarified the post. :-)
Hmm, the ability to be able to swap out (or display-on-hover) the text name of an operator in an IDE would be a very nice feature. Hell, it's Haskell, just swap the things out with a simple search-and-replace (easiest implementation ever :) ).
Does Haskell's non-strict vs. Lisp's strict semantics contribute to this?
&gt; In Lisp a procedure tends to accept many options which configure its behaviour. Python is a bit like this as well. I suppose named arguments and default argument values tend to encourage that style.
 Paragraph expected four function definitions, three encountered: &gt; In the Haskell code, we use **four** different functions which do one task: &gt; &gt; take ∷ Int -&gt; [a] -&gt; [a] &gt; filter ∷ (a -&gt; Bool) -&gt; [a] -&gt; [a] &gt; drop ∷ Int -&gt; [a] -&gt; [a] ;-)
You can read more about it here: http://www.tweag.io/blog/stack-nix-portable-reproducible-builds
&gt; Problem: get all elements greater than 5, then just the even ones of that set. I believe the code actually gets all elements up to the first element that is &gt;= 5.
In a way. If there was no Stream Fusion, the laziness would have at least ensured the following: &gt; we don’t traverse the whole list and generate a new list each time, each item is generated on demand I suspect that it was a major reason for the language to become lazy back when no Stream Fusion was discovered yet.
So are you saying the main difference is an emphasis on composability in Haskell vs configurability in Lisp? I'd say another massive difference is the extremely powerful type system vs weak type system.
Well, consider that besides simple data structures and functions, usually the logic you will use on the server side will be quite different from the one you would use on the client side anyway. The main overlap is the API structure, serialisation and deserialisation. I think that there are some ways to reuse data definitions in both Purescript and Haskell. Do you use GHCJS in production? With which user interface library?
Are you good with everything up to that point? Because you need to be good with pretty much everything up to that point to be ready for that too. It's "only" adding more recursion schemes to the category theory level implementation of the Ph.D. Haskell programmer. Edit: Here's the best intro to recursion scheme's I've read so far: http://blog.sumtypeofway.com/an-introduction-to-recursion-schemes/
ad interim: found one https://raw.githubusercontent.com/NorfairKing/haphviz/5e41d62f74612b08e36d8eaa7b7d04c4fa2a8656/test/Text/Dot/Types/Arbitrary.hs which works.
Well, Common Lisp built-in library has some strange functionality in it. But I like these configurable functions over overloading like in C++ or Java and also over having a lot of functions for same functionality with name suffixes indicating which variant you want (Haskell where overloading is not allowed) because when configurations can be combined without restrictions there is often exponential growth in number of names (see in Haskell: sort, sortBy, sortWith, unstableSort, unstableSortBy). Although if you want to pass somewhere just one variant of the function, you need to wrap it in lambda to select the variant. Nothing in CL (except some choices in built-in library which you don't have to use directly) prevents to use coding style with small composable functions maybe except lack of lazyness by default which is helpful for that sometimes. 
It is in the repository. I'm not sure what the problem is. Where did you get that error?
I did a "stack unpack haphviz", and "stack test". It might be my illiteracy with stack. I'm just beginning to use it. OTOH, when I browse the master branch on github https://github.com/NorfairKing/haphviz/tree/master/src/Text/Dot/Types I only see the file Internal.hs I had to edit other parts of the .cabal file to get the test to build. But now it runs. haphviz-0.1.1.5: test (suite: haphviz-test) Progress: 1/2 Dot is a Monoid left identity right identity associativity Finished in 2.2338 seconds 3 examples, 0 failures
This is a great idea! Though the space of possible mistakes in `pure-io` is significantly smaller than that of regular installations. Nonetheless, a very interesting question and proposal.
Indeed, this is the reason. We had to roll our own type-level integers in `Numeric.NumType.DK.Integers` from the `numtype-dk` package.
I've tried the "hello world" `inline-c` examples out, and this post is helpful in filling in some more details, thanks! I am curious what others think about this: I've been hoping for the `inline-c` docs to include a simple Haskell-`data` to C-`struct` and/or Haskell-`storable` roundtrip datastructure, and a use of `malloc` and `free` to pass around instantiations of these to both C functions and Haskell functions without leaking memory, that "takes place" across *different* `C.block` snippets - whenever I am tempted to try `inline-c` out in existing projects, each time I have admitted to myself that just writing in C would take significantly less time than figuring out how to bridge data structures via `inline-c` in Haskell and C by myself. But if I was spoon-fed this information in the Github readme, rather than reading through the `inline-c-nag` examples that I can't try out myself (no access to NAG), I'd be grateful. Eventually though, I'll be patient and disciplined and figure it out myself, because it's such a cool library. And then maybe I can submit a PR to the docs myself :)
You might consider whether replacing IP addresses is sufficient privacy protection. Neither haskell.org nor tryhaskell.org warns about logging; who knows what people have used them for.
Especially because tryhaskell.org states "Try typing in your name". Now we have a list of people's names.. if they've used their surname too that'd be problematic. 
You forgot the colons: `increment:By:AndReturnPreviousValueInStatefulContext:`, called as `increment: x By: y AndReturnPreviousValueInStatefulContext: c`.
Using list generators (perhaps macros?), some lisps (CL?) can be lazy. Have used lisps extensively and *never* relied on anything like this but I know it is a minor thing among (fringe?) lispers.
I like that. I've not liked my way of phrasing it for a while.
If you like, but I was trying to invoke the idea of throwing over so hard that it rams into the wall.
So, actual information beyond the default commands is very sparse. How do we elicit users to try out more language features?
Lispers don't dislike small, composable functions. It's just that you usually leverage some kind of interface already using these composed functions, remove-if-not is an example of such an interface (and so is loop).
Good point. It seems difficult to anonymize that.
It's pretty easy in Haskell actually. Function application takes highest precedence so you always know that the leftmost variable name is a function being called. Since Haskell functions can only take a single argument, it's pretty easy to intuitively work out what's happening from left to right, as long as you keep in mind that a function's result can be a function.
A minor nit pick: ghc doesn't use stream fusion, it uses foldr/build fusion. It doesn't make any difference for the functions used in the post though. 
I really need to check out hoed - I had no idea it existed! I would prefer it did something more native than spawning a Web server, but I can understand why they went for that solution since haskell's GUI story isn't exactly the best either. But debugging haskell programs has to be one of the hardest thing to teach students in my experience. You have to rely on intuition about certain things - intuition that takes a while to acquire - and then otherwise test stuff in ghci, or use the rather bad trace functions. Perhaps hoed, or something similar, can eventually ship with either ghc or the platform. 
And tool tips are essential, but how do you express aliases in haddock?
I kinda feel like talking about CL or emacs lisp at this point is kind of just cruel -- modern lisps like Racket and Clojure not only have way more users now, they're quite different in a lot of ways from the old lisps.
The last commit was 16 days ago so it's not dead at least, although the fay-lang.org domain is now defunct.
Hmmm, so how is that different from [`Debug.Trace`](https://hackage.haskell.org/package/base-4.8.1.0/docs/Debug-Trace.html)? It has a nice variety of functions (both monadic and not). I use that occasionally. And there's no need for the `Observable` typeclass.
Python has no meaningful typing built-in, but it is so insanely flexible (able to introspect its own opcodes at runtime and all) that you can do everything. I did spend months trying to figure out a mathematical model for Python because it's part of my job. My team is building a huge distributed data storage/analysis software (high energy astronomy and cosmology), in Python for portability and make it easy for users to script. However we want our software to be solid, and we don't want to spend our time on stuff that we know can be abstracted. Since Python does not have automatic currying, using operads made sense, it's more general. Currying is still nice, but less mandatory now.
Lisp is a family of languages and CL isn't representative of all Lisps. In fact, pretty much all Clojure code is written using single purpose composable functions. This philosophy even extends to libraries. Clojure community strongly discourages omnibus libraries in favor of focused libraries that solve a specific problem. Seems like the article is about a philosophical difference between Haskell and Common Lisp as opposed to Lisp in general.
What's the difference? I've only heard the first name so far.
Partial function application is trivial in Haskell: fs = map f1 = (* 2) f2 = (^ 2) fsf1 = fs f1 fsf2 = fs f2 main :: IO () main = do print $ fsf1 [0, 1, 2, 3] -- prints [0, 2, 4, 6] print $ fsf2 [0, 1, 2, 3] -- prints [0, 1, 4, 9] print $ fsf1 [2, 4, 6, 8] -- prints [4, 8, 12, 16] print $ fsf2 [2, 4, 6, 8] -- prints [4, 16, 36, 64] And still very easy, equivalent Lisp example in Clojure: (defn fs [f s] (map f s)) (defn f1 [x] (* 2 x)) (defn f2 [x] (* x x)) (def fsf1 (partial fs f1)) (def fsf2 (partial fs f2)) (println (fsf1 [0, 1, 2, 3])) ;; prints (0, 2, 4, 6) (println (fsf2 [0, 1, 2, 3])) ;; prints (0, 1, 4, 9) (println (fsf1 [2, 4, 6, 8])) ;; prints (4, 8, 12, 16) (println (fsf2 [2, 4, 6, 8])) ;; prints (4, 16, 36, 64) Courtesy of Rosetta Code.
Haskell doesn't have a stack, so I'd actually suggest against writing code that looks like it is tail recursive in Haskell without it being strict. Even further, don't write such code at all unless the profiler says you have a problem there. From the few tests I've ran, I found that if I enable optimization I get better performance from something that looks like your first snippet than from the second. 
I agree PureScript is much more production ready at this point. If I didn't care that much about sharing code, or cared about reusing existing web components, I think I'd not tend to GHCJS. But for what I want to do, sharing code is essential, and reusing stuff from the web isn't. To be all honest, I think this discussion shouldn't even be happening. We could be much better than we are today. I have a feeling that the whole functional programming scene is still a huge mess, and things aren't tending the way I'd like them to... fast forward to 2020 what do we have, 10 fancy FP languages in production and nobody can use code from each other for no good reason? This is the world we live in and it isn't pretty. I wish we could do differently from what happened with the mainstream languages. I guess we won't. ^(I hope Morte takes off.)
"Tail recursion" is perhaps misleading; he likely means "manual recursion", i.e. functions where you make the recursive call yourself. It's possible to replace most explicit recursion by calling combinators like `fold`, `unfold`, `map`, `filter`, `traverse`, etc. Using these has the benefit that you can't, say, forget to consider a base case. F# is one language that gently punishes you for using explicit recursion by having you define recursive functions via `letrec` instead of `let`.
Hint: what would the type of converge be? 
That is very cool! Thanks! I used [ghc-vis](http://felsin9.de/nnis/ghc-vis/) for similar purposes. 
The cata/ana/hylomorphisms are from this paper: [Functional Programming with Bananas, Lenses, Envelopes and Barbed Wire](http://eprints.eemcs.utwente.nl/7281/01/db-utwente-40501F46.pdf). Paramorphisms come from Lambert Meerten's 1992 paper ["Paramorphisms"](http://eprints.eemcs.utwente.nl/7281/01/db-utwente-40501F46.pdf). There's isn't much benefit to going full barrel on recursion schemes as your example does; it's only a little exercise in obfuscation because recursion schemes are little-known, even among Haskell programmers. 
Is there a particular reason you use converge :: (a -&gt; a -&gt; a) -&gt; [a -&gt; a] -&gt; a -&gt; Maybe a instead of converge :: (b -&gt; b -&gt; b) -&gt; [a -&gt; b] -&gt; a -&gt; Maybe b or even converge :: (c -&gt; b -&gt; c) -&gt; c -&gt; [a -&gt; b] -&gt; a -&gt; c (using foldl instead of foldl1 in that last case with the base case as extra parameter) ?
Thank you for doing the blogs, and for featuring HOOD! You've inspired the FPG at KU to re-visit the idea, and see if we can update it. 
No reason; just an oversight.
Stream fusion and foldr/build fusion handle slightly different sets of functions. Stream fusion struggles with list comprehensions while foldr/build cannot fuse functions with accumulating parameters. 
The spine is the part of the vector that's not the elements. When talking about containers, "spine strict" means that the shape of the container is fully evaluated, as opposed to fully lazy (e.g. lists) or fully strict.
We mention this briefly on the `servant` paper (when discussing `Happstack` and `Yesod`). I remember having to check twice, since I was a little surprised you could do it, I guess because the `Yesod` documentation doesn't emphasize the point. At the time the fact that you had to associate one content-type with each type seemed annoying, but in practice I don't think it really matters. Moreover, it's even possible to tag the content-type with a phantom type (with, say `Tagged`), and have a constraint in `ToContent` that looks like [`MimeRender`](https://hackage.haskell.org/package/servant-0.4.4.5/docs/Servant-API-ContentTypes.html#t:MimeRender). *EDIT*: Or you could simply have a newtype wrapper for each content-type. I was thinking doing it with `Tagged` meant you could write the same handler for multiple content-types, but there wouldn't be type inference for the tag anyhow, so it doesn't really help - `fmap`-ing is just as easy as annotating the type.
Surprised no benefit
I'm not sure this is accomplishing #2. First, shouldn't it be (flip ($) x) instead of ($ x) ? Flipping apply will store x and use it with each function in fs. Additionally, once x is applied to all fs, it will return a list of results, to be folded with fold1', but this looks like it will call finalF for every result in fs, instead of finalF with all results of fs. So converge needs a type that has recieves a function with an arbitrary number of arguments. I'm not sure how to achieve this in Haskell: converge :: (a -&gt; b -&gt; c ... -&gt; z) -&gt; [a1 -&gt; a, a2 -&gt; b, a3 -&gt; c ... ] -&gt; Maybe b1
Can't maps an filters be expressed in terms of a fold? You mention fold**r** fusion - does the directionality make a difference?
OTOH your keynote kind of said "welcome to LambdaJam, the dials are adjusted to 11, you will not be sitting down!". But, anyway, maybe we can work the schedule a bit better next year.
Two things: 1. `(-3)` is the number "3 below zero", not a function that subtracts 3. However `subtract 3` would do 2. I don't think you really want to converge a _list_ so badly. I'm pretty certain that the all functions used are static. If it's the case then both of these will do: (+) &lt;$&gt; (+2) &lt;*&gt; subtract 3 liftA2 (+) (+2) (subtract 3)
I agree, the Common Lisp kitchen-sink batteries-included culture is one of many different cultures in the broader "Lisp" world.
I'm confused, why is it not a function that subtracts 3? I thought it was basically `\n -&gt; n - 3`.
Perhaps you should put Haskell away and stare at -3 for a while. Yeah of course it's negative 3, just somehow the designers of Haskell decided that negative numbers are more useful than sections and let this syntax inconsistency in.
Unfortunately it leaves you with two zeros. We played with that, and with just having `Pos (n :: Nat)` and `NegOneMinus (n :: Nat)` but felt that it was more confusing. So we ended up with the current version where there are explicit `... Neg3, Neg2, Neg1, Zero, Pos1, Pos2, ...` within a certain distance of zero (iirc about 10) and then something like `Pos10Plus (n :: Nat)` and `Neg10Minus (n :: Nat)` which you basically never see because there are no practical quantities whose dimensions in the SI basis are that high. Looking forward to the day when GHC's integers (and rationals, which are even harder to emulate, but necessary for upcoming work on fixed point quantities) are promotable.
A not-so-well-known use of infix operators is left partial application. [(+2), (3-)]
The output will be the same, the running time will be different (i.e. it'll terminate later). At `[1..]` it never finishes. :(
Looks really cool. Too bad it's gtk. All of those "cross-platform" UI libs are hell to get working on Windows.
I think the same argument could be made with clojure, but it has fewer super-gross examples like LOOP. I'm not interested in the philosophy debate, really, just pointing out that CL and elisp are pretty archaic at this point.
You should make the type of the arguments to `&lt;+&gt;` a Monoid so you can use `&lt;&gt;` instead and get things like `mconcat` for free.
&gt; All Except wxWidgets and Qt, but Qt bindings are outdated and wxWidgets bindings don't provide a Web browser component. I think Haskell community chose the worst toolkit to focus on. Oh, "open" source...
Well, the module names look rather descriptive, so what I would do in this case is look for a module that looks like it would fit my use case, and read its documentation, top down, following thebtypes and functions that it mentions as far as I have to in order to understand it. Unlike this one, most libraries have an obvious 'first stop' module, typically the root of the library's local namespace tree (e.g. Parsec has Text.Parsec), and that module, by convention, contains the 'getting started' / 'how to use this library' documentation. Some libraries, however, are barely documented at all. Sometimes, that is a sign of subpar code quality, or the project being in an immature ir abandoned state, but sometimes, the types alone tell you everything you need to know. 'Following the types' is a very useful skill to develop when dealing with Haskell.
Follow the types. Often, they tell you all you need to know. However, many libraries on Hackage are in a poor or immature state, and the lack of documentation reflects this. It's the price we pay for a low-threshold package repository; if you want a curated list of known-to-work libraries, look into Stackage (or use Stack for building, as it defaults to using Stackage).
Usually functions in Haskell do only one thing. But in math, `-` is used for two separate functions: negate :: (Num a) =&gt; a -&gt; a flip subtract :: (Num a) =&gt; a -&gt; a -&gt; a So if you need to write the number "3 below zero", you could do it as `(negate 3)` or `(-3)`. Also `(3-)` works like you could expect, i.e. it's a curried binary function. But for a curried function that subtracts 3, you need `(subtract 3)`, because `(-3)` already means "the number 3 below zero". Other languages sometimes solve this with different symbols, e.g. 4 - 3 = 4 + ~3 = 1 but haskell doesn't, and so we get 4 - 3 = 4 + (-3) = 1 `4 + -3` is a syntax error.
Type-level integers/rationals shouldn't be too hard, at least to the level that type-level naturals are currently supported. I think the main thing lacking is a good design for how to write literals (since we don't have `fromInteger` or the `Num` class at the type level). It would be nice if we could have both `42 :: Integer` and `42 :: Nat` in types. One possibility might be to define some (return kind indexed) type families: type family FromNat (k :: *) (n :: Nat) :: k type instance FromNat Nat n = n type instance FromNat Integer n = ... type instance FromNat Rational n = ... type family FromInteger (k :: *) (i :: Integer) :: k type instance FromInteger Integer i = i type instance FromInteger Rational i = ... type family FromRational (k :: *) (r :: Rational) :: k type instance FromRational Rational r = r Then GHC could translate a numeric literal in a type expression into an application of the appropriate type family (where I'm using suffixes to indicate numeric literals of the underlying kinds): 42 ~&gt; FromNat k 42n -1 ~&gt; FromInteger k -1i 2.5 ~&gt; FromRational k 2.5r Note that this would allow users to define their own instances of the type families to make use of overloaded numeric literals, for example: data PNat = Zero | Suc PNat type instance FromNat PNat n = FromPNat n type family FromPNat n where FromPNat 0 = Zero FromPNat n = Suc (FromPNat (n-1)) But perhaps moving the type language further away from the term language isn't such a good idea, considering the general direction of travel towards dependent types...
I've tried to get both of those working before and failed miserably. Honestly, pretty much anything that binds to a C library takes a substantial amount of work on Windows. Surely there has to be a better way.
https://github.com/jonnydedwards/snow might help with that. Won't be 100% safe though
Very cool! Do the 'bogus counter-examples' produced mean that it may report a failure when there is none, or that the particular counterexamples reported aren't actually meaningful?
can you help elaborate on using this for zero-copy conversions between *vector* and *hmatrix* types?
That's some line up! I'm sure it will be great, wish I could go.
`flip ($) x` is actually identical to `$ x`. If you write an operator in brackets like `($)`, it behaves like a regular function. If you don't, it behaves like an infix operator - its 1st argument will be on its left, its 2nd argument to its right. Syntax like `(+ 3)` (where you put the argument to the right of the operator) is shorthand for `(\x -&gt; x + 3)`, or `(flip (+) 3)`. &gt;Additionally, once x is applied to all fs, it will return a list of results, to be folded with fold1', but this looks like it will call finalF for every result in fs, instead of finalF with all results of fs. That is true, yes. A regular list doesn't store the number of elements it has on the type level. There's multiple ways to solve this. The easiest would be to pass a combining function of type `[b] -&gt; c` that takes a list of arguments. The downside is that you won't know what that number will be statically. convergeList :: ([b] -&gt; c) -&gt; [a -&gt; b] -&gt; a -&gt; c Another way would consist in creating a converge-function for each possible list-length, like so: converge2 :: (b -&gt; b -&gt; c) -&gt; (a -&gt; b, a -&gt; b) -&gt; a -&gt; c converge3 :: (b -&gt; b -&gt; b -&gt; c) -&gt; (a -&gt; b, a -&gt; b, a -&gt; b) -&gt; a -&gt; c converge4 :: (b -&gt; b -&gt; b -&gt; b -&gt; c) -&gt; (a -&gt; b, a -&gt; b, a -&gt; b, a -&gt; b) -&gt; a -&gt; c ... This might seem silly, but if you only ever have small lists, it will suffice. [Type-level numbers](https://www.fpcomplete.com/user/konn/prove-your-haskell-for-great-safety/dependent-types-in-haskell) are a more advanced technique: {-# LANGUAGE DataKinds, GADTs #-} -- DataKinds lifts Nat from a type into its own kind, with Z and S Nat as its sole inhabitants. data Nat = Z | S Nat -- * represents any type. Nat represents Z or S Nat, but on the type-level. data Vec :: Nat -&gt; * -&gt; * where Nil :: Vec Z a (:#) :: a -&gt; Vec n a -&gt; Vec (S n) a infixr 5 :# instance Functor (Vec n) where fmap _ Nil = Nil fmap f (x :# xs) = f x :# fmap f xs convergeNat :: (Vec b n -&gt; c) -&gt; Vec (a -&gt; b) n -&gt; a -&gt; c convergeNat finalF fs x = finalF $ fmap ($ x) fs -- no more fold. --example finalF --the length is specified via type-level Peano numbers. addThree :: Vec (S (S (S Z))) Int -&gt; Int addThree (x :# y :# z) = x+y+z -- example use: convergeNat addThree ((+1) :# (*2) :# (+5)) 7 On the one hand, you can take an arbitrary amount of arguments and can't go wrong in handling them anymore. On the other: 1. It's ugly. 2. You have to know the size of the list/vector statically. You can' just take a regular list an convert it to a fixed-size vector.
Well... they're known here, but my personal experience is that, out there in the real world, even monad transformers are pretty hoity-toity.
I would say that Haskell does already a good job in promoting sane abstractions. Some tech areas go through alternate phases of fragmentation and convergence, like in a genetic algorithm. Solutions are spawned in many directions, until a winner emerges. You need to be in the right mood in order to enjoy the fragmentation phase, and i think that the average Haskell dev likes convergence more than a fan of alternatives ... still, we are not there at the moment, in my opinion. Javascript makes me think of C. It might remain the underlying implementation language for a long time, thus whatever the higher level language will be, the interoperability story will remain important. Also, if web standards had been properly modularised, like Unix was, this problem would be way less severe
&gt; So do you see much use in going through the material in blog post WarDaft mentions? It's a pretty good blog post on the topic, so sure! There's nothing wrong with recursion schemes as such; they're just a very niche topic. That said, they're not *that* esoteric. All they express is a fixed pattern of recursion you can use if its fits your goals. Roughly: y-combinator = simple loop catamorphism = fold anamorphism = unfold hylomorphism = unfold, followed by fold paramorphism = fold that keeps its argument (like scan, I guess) There's a [post enumerating them](http://comonad.com/reader/2009/recursion-schemes/).
Oh, I adore Haskell paper titles. &gt; Conor McBride - I got plenty o’ nuttin’ &gt; Jennifer Paykin and Steve Zdancewic - Linear lambda-mu is CP (more or less)
Curious: In America, do they distinguish between Non Profit's and Charities? I noticed in the wording that this meant haskell.org is now a Non Profit, but perhaps not necessarily a charity. For example, in Canada, you can be a Non Profit but not Charity. If one donates to a Charity, you get a tax rebate. If you donate to a Non Profit, you don't get much. It's very, very difficult to become a charity in Canada, whereas relatively easy to become a Non Profit. Why become a Non Profit in Canada if you can't give tax rebates? Many granting organizations require one only be a Non Profit. Anyway, just asking out of curiosity.
In america there are 29 types of 501(c) organizations which are tax-except, non-profits. That means the org itself is tax-exempt. I believe 501(c)(3) may be the only type where contributions that people make to the org are tax deductible from the contributors taxes. To be a 501(c)(3) the org must meeting the following requirement (among others): 501(c)(3) exemptions apply to corporations organized and operated exclusively for religious, charitable, scientific, literary, or educational purposes, or for testing for public safety, or to foster national or international amateur sports competition, or for the prevention of cruelty to children or animals. Like Canada it is difficult to be a 501(c)(3), which is why things like SPI exist. Now, whether haskell.org is 'charitable' I can not say. Perhaps it qualified under the scientific and/or educational purposes. But, in the spirit of your question the short answer is, "yes" -- you get the tax rebate and 501(c)(3) status is hard to obtain. https://en.wikipedia.org/wiki/501(c)_organization
electron/haskell integration soon pls haskell gods.
Could you do it syntactically by banning `42 :: Integer` in favor of `+42 :: Integer`? (Redefining `TypeOperators` to require a space if it doesn't already?)
[quotFile](https://hackage.haskell.org/package/template-haskell-2.10.0.0/docs/Language-Haskell-TH-Quote.html) takes any quasiquoter and makes it read from a specified file instead. So if you define (in a separate source file) `hsx_f = quoteFile hsx`, then you can just do `[hsx_f| "XMLfile.xml" |]`.
http://electron.atom.io/ It's pretty much a prepacked chromium web browser with the browsing capability's stripped, so it can function as a sort of GUI toolkit. Not too sure about how the back end works but I'm pretty sure you can just use any old server/language. It also has thing like notifications, shortcuts, a windows installer, and updates builtin, although I haven't looked into those much. Wagon already use it with haskell, there is just no easy skeleton package done yet. If you've ever heard of node-webkit this is basically the evolution. It drives tons of great applications, including the Atom text editor. Perhaps if we got a system with purescript frontend and haskell backend in a ready to use package like how cljs-electron works for clojure/clojurescript. 
Probably! I know that Yesod uses external template files which are interpreted at compile time, so whatever the tricks which the `hsx` quasiquoter uses to transform a markup string into code, it should be possible to write an alternalte `[hsxFromFile|myfile.hsx|]` quasiquoter which reads that string from a file and passes it to the code underlying `hsx`. Of course, that doesn't mean that this behaviour will be supported out of the box by the library which provides `hsx`, and if the underlying code isn't exported, we might even have to patch the original library. But it should be possible. Googling quickly leads me to the eponymous [hsx](https://hackage.haskell.org/package/hsx) package, which... doesn't seem to be what you're talking about, since it seems to already support, and even require, the markup to be in a separate file? Googling on, I find [hsx2hs](https://hackage.haskell.org/package/hsx2hs-0.13.4/docs/Language-Haskell-HSX-QQ.html), which does have an `hsx` quasiquoter (that is, it implements the `[hsx|...|]` syntax) and specifically mentions that it's the same syntax as the `trhsx` program from the previous package. So while I'm pretty sure it should be possible to write an alternative `hsxFromFile` quasiquoter, I must ask: have you tried this `trhsx` program and does it suit your needs?
/u/keyks, please ignore the following comment. You not finding the documentation good enough is useful feedback, it just happens to also be frustrating feedback in the context of prior pieces of similar feedback. &gt; I didn't find any documentation, tutorials or examples. Oh, come on. I know that we have a few problems with documentation, such as hackage docs occasionally not being built, and people who don't feel compelled to improve the existing documentation because they find the types to be clear enough (guilty!). But this particular package's documentation *has* been built properly, and the documentation *does* contain English explanations on top of the types, except for self-explanatory things like `getFeedAuthor` whose English explanation would be the useless "gets the feed's author". The library's author clearly spent some valuable time writing this documentation, including realizing that sometimes it's better to leave the documentation out than to simply repeat the function's name. So it's frustrating to see that "only api documentation" is now synonymous with "no documentation", but at least the message is clear: users don't care about api documentation, so we can stop wasting our time improving it, we should write more examples and tutorials instead. Sorry for the tone of this comment. I usually spend a lot of time making sure my comments are positive and helpful, so it's very frustrating to see the work of other people, who have clearly also spent a lot of time trying to be helpful, being dismissed as if it didn't exist.
Yes, I suppose so, given a bit of lexer hacking. (Though if we wanted to support rationals we'd need yet another syntactic distinction.)
The documentation is in your link. If you click a module name, you'll see the documentation for that module. 
very nice, would love to see more examples, is there any way to reason about vectors using this? 
isn't part of lts-3 is in Stackage Nighlty , and planned for LTS-4
Have you tried [msys2](https://msys2.github.io/)? It provides the Arch linux package manager but on windows. I haven't had much opportunity to use it with cabal, but I have used it for several libraries with the rust toolchain (cargo). With cargo I edited one global config file for each library I installed and after that I could use that library with any rust project. I'm not sure exactly how the details translate to cabal, but I'm 90% certain that it should be just as easy with cabal. Someone should figure out the details and make a short PSA about it.
We had wx bindings before we had functional gtk bindings (get it?). So I think it comes down to maintainership. Duncan et al were willing to maintain the gtk bindings. I'm not sure what happened with the wx bindings, but I assume they are less maintained.
Is it faster than ninja? The site contains no comparisons between the two.
I think that's how threepenny gui works: https://hackage.haskell.org/package/threepenny-gui
Looks like the equivalent type is something a little like converge :: ([x] -&gt; z) -&gt; [a -&gt; x] -&gt; (a -&gt; z) which we can implement as converge fin axes = fin . sequence axes where `sequence :: Applicative f =&gt; [f x] -&gt; f [x]` has specialized `f` to `(a -&gt; _)` Multi-arity stuff doesn't work great in Haskell, but what's going on here is sort of a standard "piping" combination. We can talk about it as arising from smaller pieces we could find in `Control.Arrow`. The first bit, where we distribute the `a` to all of the `a -&gt; x` functions is ["fanout" `(&amp;&amp;&amp;)`](https://hackage.haskell.org/package/base-4.8.1.0/docs/Control-Arrow.html#v:-38--38--38-) and the "converge" bit is just post-composition (`(.)` or [`(&gt;&gt;&gt;)`](https://hackage.haskell.org/package/base-4.8.1.0/docs/Control-Arrow.html#v:-62--62--62-)). Generally these kinds of "piping combinations" can be read out as arrow combinators.
&gt; shake is not a build system, it's a build system construction library. I also really like it for small shell scripts with file dependencies, the vararg "cmd" function is nice to use. I wonder how well it combines with turtle (a colleague of mine likes that one).
Would there be any disadvantages of backing this directly into Yesod? It seems like a super cool approach to getting some of the cool features of Servant :)
No, 1 would be passed in to (+2) and show, and those results to replicate. So one argument spread on a list of functions, and the list of results applied to one function.
It's actually kind of dumb. If you don't have exponentials, that is if your cannot curry easily, then a n-ary function cannot easily become unary. But it's alright, just compose a n-ary function with n functions. You form a tree and don't pretend you're in a monoidal closed category
They will be "meaningful" in the sense that they will represent "undetermined" values of black-boxed types. But they might be bogus since the functions that manipulate them will be arbitrarily chosen by the SMT solver, which will not correspond to the meaning of the Haskell function that got black-boxed.
This is a *vast* improvement. https://github.com/tel/serv/blob/feat-singletons/src/Serv/Internal/SServer.hs#L32
It's not that easy. You've got to guess when you've reached the last argument, but many functions are variadic, so even introspection does not work. Then, lambdas aren't as fancy as usual Python functions, politeness requires that you fill the metadata such as the doc string and the name. Then comes the fact that it's a bitch to parallelize because you cannot pickle (serialize) functions that don't already exist in another process (Python passes just the name over the pipe, you cannot even pickle closures). Yeah.
We could reimburse expenses but we couldn't figure out how to pay someone for work. So for instance if we decided we wanted to pick a GSoC student and pay them to continue their work directly we didn't really have that option before. There have been some other discussions about how to restructure things between haskell.org and IHG that would involve payment for work funneling through the committee and we had no way to handle this before. In addition, for the last 3 years or so, we've had a lot of anxiety over whether or not certain parts of filing for our GSoC P.O. were happening or not. We had no insight into what was happening. Given that our income remains mostly comprised of that, getting kicked out of the program would devastate us. Getting rid of the middle man gets rid of any finger pointing in the process. So ultimately, it means that we can pay things faster and that we get a lot more flexibility in how we work. It also makes it easier to do things like set up a haskell.org store and offer random haskell merchandise for donations and the like. It wasn't clear how or if we could do this before.
Why not just put each page or page fragment in a separate Haskell module? {-# LANGUAGE AllThePragmas #-} module App.Pages.SomePage import AppPrelude somePage = [hsx| &lt;html&gt; &lt;head&gt; &lt;/head&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; |] It is not clear that you need anything special for hsx? Though if the proposed `hsxFromFile` suggestion is what you are looking for I can add it to the official `hsx2hs` source. It does look attractive. `hsx` does have an external preprocessor `trhsx` that you can use. It has a special mode where the file it is reading can actually be a valid xml document. That is mostly just intended to make it possible to work with the documents in a plain old xml editor. I am not sure that it is really what you want. 
I can try to break down your requirements, as it turns out that what you're looking for is split among a number of different sources; a good example on how to write a Storable instance corresponding to a C struct can be found in the original Vector tutorial on the haskell.org wiki. I wouldn't count on the possibility of binding `malloc` and `free` on the Haskell side, as the Hs run-time system takes care of garbage collecting and needs no explicit hints. As you know, `inline-c` requires one to represent the C-Hs type map in a Context, where the Hs-side `newtype`s need to be instances of Storable. This lets us use lexical scoping on the Haskell side, and constructs such as `bracket` from Control.Exception . Say goodbye to memory leaks! Hope this helps. Ping me if you'd like to have a more detailed discussion
Don't spend time on this just use stack..
Of course
/u/adamgundry - I was just reading your units-of-measure paper and was wondering whether your plugin works in IHaskell. I'm thinking of trying it out with my toy Cosmology code, and would prefer to do it using jupyter notebooks.
**Imperative Johnny visiting Functional Space.** Johnny : "Woah, what are those strange creatures, Mr. IO?" IO : "They are the citizens of this land. They make Functional Space what it is today, a bussling metropolis of composability, functions of every kind working with each other in perfect harmony." Johnny : "Gee, Mr. IO, it looks so complicated!" IO : "Don't worry Johny, you'll soon get to interface and meet the representatives of this city, and you'll feel like you've met everyone else." Johnny : [LOL] IO : [LOL] IO : "Johnny, NOOOOOOOOOOOOO!!!!!" CUT TO: Functional Space - Downtown - minutes later Siren monads everywhere. Blood trickles down the sidewalk as Johnny lays dying, half of his body crushed like a pancake. Officer functor : "What happened here?" IO : "I.. I don't know... one second we were talking and then... I told him he couldn't change state like that, but he didn't listen!" Officer functor : "Well, he shouldn't have crossed Fmap boulevard like that. Monads are chaining by really fast around here." IO : [CRIES] 
Heh, this is how I feel in Python now. Nothing solid I can lean against, Cthulhu around any corner...
&gt; In Lisp a procedure tends to accept many options which configure its behaviour. Meh. That's one of the Lisp traditions, particularly from the Common Lisp branch. But it's not the only one. It's a bit like saying "in Haskell a function needs to have a counterpart in category theory." Edit: I see [yogthos already addressed this point](https://www.reddit.com/r/haskell/comments/3xoozt/a_philosophical_difference_between_haskell_and/cy71nsi). I'll leave my comment up because it's worth repeating. 
&gt; cruel I would say out of touch.
My first impression was that the syntax was very clean for a typed language... by that point I had been pretty fed up with people using duck typing for random horrible shit -- and a bit confused trying to fit things into OOP.
It's hardly ever composable, so it does not.
Maybe something like [gobject-introspection](https://wiki.haskell.org/GObjectIntrospection). Do not let the name fool you, this could be used for non-gobject libraries too, in order to stop writing bindings again and again :)
Haha I'm actually impressed how consistent it is for being a research language. R is terrible in comparison! I guess I just don't do a lot of math... or do too much Haskell. Because I think of negative numbers in real life as representing subtraction too.
Is there a standard/widely-used Haskell localization library? I do want to eventually have the user interface elements (like Reset button, default window title and end program message) translated, but I don't know if I should force some specific localization approach...
I'm picturing this as a scene out of Rick &amp; Morty...
So converge [(+2), show] replicate 1 = replicate (1+2) (show 1) = replicate 3 "1" = ["1", "1", "1"] ?
Lazy seem to be just one use of it. When you have reified the graph as a value, you can imagine distributing the computation, caching, pre computing, have 'reactive' version of variables..
I was aware of the integer-gmp package, but I had completely glossed over GHC.Integer.GMP.Internals. This looks promising but the "[this] should only be used directly with the utmost care" warning is a bit concerning. I'll look into it, cheers! edit: Moreover, looking at some of the functions (like testPrimeInteger), these seem to be direct and literal translations of the GMP functions!
We try to make sure that GHC and applictions will generally build with both `integer-gmp` and `integer-simple`. This matters for ensuring it is possible to build GHC itself in a BSD environment. That said, while most of the combinators in there are perfectly safe -- if not so portable, you can get into illegal situations by using the data constructors directly. [Edit: I originally said `integer-pure`.]
You could consider the Python blocks to be laying on the floor while the programmer walks over it with neither shoes nor slippers.
`integer-pure`? Don't you rather mean `integer-simple`? :-)
[naif question] What's the difference in proving/disproving power between a tool such as QuickCheck and SMT verification ?
Paging /u/erikd. See [haskell-big-integer-experiment](https://github.com/erikd//haskell-big-integer-experiment/).
&gt; There's one GUI library everyone had on their computer since long time - HTML. http://i1.kym-cdn.com/photos/images/masonry/000/906/550/37d.jpg
I just learned there is such a thing as a Tardis monad. To be honest, just reading the doc page is giving me a bit of a headache. &gt; You can retrieve the current value of the state, and you can set its value, affecting any future attempts to retrieve it. "Well, sounds perfectly normal; imperative languages do that all the time." &gt; The Reverse State monad transformer is just the opposite: it features a backwards-traveling state. You can retrieve the current value of the state, and you can set its value, affecting any past attempts to retrieve it. "Wait what" [](/ajbaffle) 
Great presentation! However I understand that due to time constraints @mkscrg couldn't present the last part "highs and lows of Haskell in a startup". I think many here would be very much interested in knowing the opinions of the Wagon team ..
Doug, it's on our list to add HaTeX/siunitx support and a Texy instance, followed by an IHaskellDisplay instance. Would this support work for you if you had to enable a package flag to use it? (The other options would be taking all those dependencies outright, or orphan instances. We're trying to decide but we really want to avoid taking the dependencies outright since there are so many applications that don't need them.)
Considering it's syntactically correct Haskell, with the string enclosed in double quotes and all... 
So if I understand correctly this would be similar to passing a pointer to function A then altering the value at that address before function A uses it?
&gt; @mkscrg What's that? Edit: Oh, the handle of the guy. Nevermind.
&gt; Years ago, I had a hell of a time just trying to export an Integer as a list of words. You could do that with my [hashabler](https://hackage.haskell.org/package/hashabler) package directly, or check out its internals or the internals of [hashable](https://hackage.haskell.org/package/hashable) (from which most of the Hashable instances were copied/modified). 
https://www.youtube.com/watch?v=APrpB-i4d_E
[removed]
What package are you looking at? Is it by chance `gogol-compute`? I understood part of the `gogol-` and `amazonka-` series of packages are auto-generated (don't know how, though), so it might look more intimidating than it should be. /u/tdammers , I don't want to start a quarrel here, but "follow the types" is rather an anti-explanation to a newcomer. It only makes sense once you are a rather sophisticated user.
Yeah, it's gogol-compute. And yes, following the types is proving difficult. I've seen a few things I 'get' but most of it is way above me right now.
QuickCheck (and its descendants) are extremely powerful tools that can run very smart tests on large swaths of Haskell code. But they are inherently not exhaustive: By default QuickCheck runs 100 tests: The chances it'll actually hit a failing case, while greatly improved by smart-generators, is still small. Especially if the input space is large and the bug is in a hard-to-reach corner case. SBV and other theorem-proving based techniques are exhaustive: If they say something is True, then you can rest assured that it is true for all possible input combinations. Having said that, they apply to a much smaller subset of the language. For instance SBV mainly understands numeric types (Int, Word, Integer, Float, Double, etc.) and there's limited support for tuples and fixed-size lists. Most importantly, SBV cannot deal with recursive definitions, unless they are "symbolically-terminating." (See Section 7.4 of this paper for details: http://sites.google.com/site/leventerkok/cryptol_PLPV09.pdf. The paper is set in the context of the language Cryptol, but the same comments apply to Haskell/SBV as well.) Support for algebraic-data types is also limited, mainly restricted to enumerations. At the risk of oversimplifying, the main difference is that QuickCheck is much more widely applicable but not exhaustive, while SBV applies to a smaller set of programs/properties, but the results it'll produce will be conclusive. For a fun example, you can try this: {-# OPTIONS_GHC -fplugin=Data.SBV.Plugin #-} module Test where import Data.SBV.Plugin {-# ANN hardToTest theorem #-} hardToTest :: Int -&gt; Bool hardToTest x | x == 0xdeadbeef = False | True = True SBV will give you a counter-example in no time: $ ghci Test.hs GHCi, version 7.10.3: http://www.haskell.org/ghc/ :? for help [1 of 1] Compiling Test ( Test.hs, interpreted ) [SBV] Test.hs:8:1-10 Proving "hardToTest", using Z3. [Z3] Falsifiable. Counter-example: x = 3735928559 :: Int64 Ok, modules loaded: Test. As you can imagine, it'll be very hard for a testing based mechanism to hit this "bug." This doesn't mean you shouldn't be using QuickCheck of course. But if your problem is suitable enough for SBV, then you can get better assurance. You can of course always mix and match both techniques as you see fit in the same framework as well.
This is great. Now you can ask a list of people and map over them to find the good prospects while guaranteeing you don't get any answer more complicated than yes/no!
 *** Exception: SuperRejectionException "No" 
Yeah that. =) Second guessed myself and went looking on hackage, and got confused by the fact that it doesn't show up there.
Probably not a library worth using for your goals then. Hackage has some incredible gems, but it also has mediocre libraries, and it's not always easy to tell the difference at a glance. That said, Atom and RSS aren't very complex, so writing your own feed reader library based on the existing Haskell XML and HTTP packages shouldn't be an unsurmountable task.
"Follow the types" wasn't meant as a full explanation; that would be beyond the scope of a reddit comment. It should be enough to bootstrap some thinking and googling and reading though.
Probably not a library worth using for your goals then. Hackage has some incredible gems, but it also has mediocre libraries, and it's not always easy to tell the difference at a glance. That said, Atom and RSS aren't very complex, so writing your own feed reader library based on the existing Haskell XML and HTTP packages shouldn't be an unsurmountable task.
Are you replying to me, or some other thread? I was responding to a thread about the `monadiccp` library that has nothing to do with Atom or RSS.
Version 0.10.0.4 is a very old version please consider upgrading. Links to the latest version are [here](https://github.com/leksah/leksah/wiki/download).
Odd it shows up for me. Link is to https://github.com/leksah/leksah/wiki/download
TY!
That looks like functional programming with a potentially clunky GUI (well... it's got a higher dimensional structure anyway). I can see it getting really gross with a lot of small composable functions.
Doesn't make much sense to me
I haven't seen a single GUI application which composes with other GUI applications. To call *this* superior as a haskell programmer is absurd. &gt; Yes, text terminal is spawn of Satan, it's elitist and evil - terrible user interface. Just because the average windows user/developer is not willing to invest a few minutes into reading a software manual, doesn't mean the system is flawed. And how will some JS library fix that? 
Brilliant, thanks. I think the key insight was that the final variadic function can just be partially applied with as many arguments as needed in a fold. I tried making a converge that takes functions with arguments of any type, but I think there's probably a better abstraction for that, or just creating a custom type. 
I see, thanks. Interesting example with peano numbers. So yeah, I realized a variadic function in Haskell can be just a partially applied fold, especially if the functions in the list all have the same type. I think your custom type addresses this. For a converge with same function types, it turns out a lot more straight forward: converge :: (b -&gt; c -&gt; c) -&gt; c -&gt; [a -&gt; b] -&gt; a -&gt; c converge f i fs = foldr f i . zipWith id fs . repeat &gt; converge (+) 0 [(+2), subtract 3] 5 9 
Yeah, I'm realizing there's probably a better abstraction that ends being a combination of a monad or type class with a fold or fmap. Someone mentioned the fanout (&amp;&amp;&amp;), which seems really close. 
I already had "happy" "haskell-src-exts" "template-haskell", I still got the error.
As others have said, this library is documented, it's just not obvious from which part to take it... The main entry point is the `Text.Feed` hierarchy (my clue was the generic name compared to the rest), other modules are only there if you have specialized needs. Once you get that, you really don't need much to use this library : you look in `Text.Feed.Import` and you'll find a `parseFeedFromFile :: FilePath -&gt; IO Feed` (which assume utf-8 as said by the documentation) or a `parseFeedString :: String -&gt; Maybe Feed` if you have another encoding initially. Then you look at `Text.Feed.Query` (I'm still just going by the name) and you have all the necessary functions, which name and type tells you everything you need. This is a different strategy than a library with an helpful tutorial but really here it is appropriate and fast to get running though the description of the module should really tells one to start from the `Text.Feed` hierarchy for basic needs.... In less than 5 minutes of looking at it, I could already write this small program that lists the items in a feed from a file : import System.Environment (getArgs) import Data.Maybe (mapMaybe) import Text.Feed.Import (parseFeedFromFile) import Text.Feed.Query main = do [fileName] &lt;- getArgs feed &lt;- parseFeedFromFile fileName let items = getFeedItems feed itemTitles = mapMaybe getItemTitle items feedTitle = getFeedTitle feed putStrLn (feedTitle ++ " :") putStr (unlines . map ("- " ++) $ itemTitles) 
Thanks 
How does this relate to Haskell? I do find it interesting though.
I'm just playing - not using any of these packages in the "real world" [*] - so I don't have a strong input, but I would be okay with this support/instances needing a package flag. [*] or as real as an Astronomy department is ;-) 
You're probably right, but it's a nice move when someone has read misinformed articles about Haskell and comes to you saying "Monads are just how Haskell imitates imperative languages, you don't need it in X since X is imperative anyway", if anything, keeping them awake that night is a proper punishment :)
Interesting project. How would you program such a machine?
Sure? After all, they look like quotation marks (`“”`), not string literal double quotes (`""`).
Restricted to "more complex packages" like the poster asked about, I think this is pretty accurate (although the bit about self-esteem seems a needless dig).
It's only superficially similar, obviously Haskell's intentions are very different from yours. As well, not to be unkind, but it seems to me your post is really self promotional in nature than anything else.
Types will get you further than you'd expect, but there are definitely plenty of packages that need docs badly, some of which lack them. As I speculated in another thread (quite possibly the one you refer to), I know for me a part of the barrier to writing more docs than I do is the sense that they'll inevitably go out of date. Part of why I prefer Haskell is that I want help knowing that the various parts of my output are in sync.
Yeah I might end up doing that. I do feel like I'm reinventing the wheel. The thing is I have zero chance of convincing anybody else here to learn Haskell. But I could have a more minimal config file where each program is expressed like `step1 = choose_genomes infile 3.4e-10` and parsing is done with `words`. I didn't think of that until watching [this Skills Matter talk](https://skillsmatter.com/skillscasts/6548-defining-your-own-build-system-with-shake). (BTW, I made a throwaway login to access it. User "bobross" and password "happytrees") The patterns are absolute filenames entered in the config file, like `/home/jefdaj/shake-dummy/data/in.txt` (after variable expansion).
Not really. The paper is talking about eliminating the possibility of untrustworthy _permanent_ storage. It doesn't relate at all to runtime state of a program.
You should give your colleagues early access to the [Haskell programming from first principles](http://haskellbook.com/) book before you write them off as unwilling to learn Haskell. My hypothesis is that most people's dissatisfaction with Haskell is really just dissatisfaction with poor pedagogy.
No, I found it in my downloads and "eh, why not"? :P
Thanks for the awesome work - I have yet to go through all of them but I've already seen a bunch of cool packages that I need to try out. 
What is topcoder? I googled it, but I'm very confused. It's like stackoverflow but you need an account to see the answers?
Thanks for a great series this year. I've really enjoyed Ollie's work on this in years past, but enjoyed the different tone and feel you brought. Some things I particularly liked: * The emphasis on testing, and the effort you put in to bring a TDD approach. * Focusing on practical matters; for people who feel Haskell is too academic, it is useful to have literature to refer them to that is focused on real-world problems. * Hearing your thoughts on design and abstraction, etc.; as a relative Haskell newbie, it can sometimes be a challenge to know how much abstraction is too much. It's nice to have others' thoughts on such matters to learn from. I think your module of the week - or similar - is a great suggestion. It seems a shame that we have to wait 11 months for another series like this. Especially so when, as you noted, the nature of the 24 days of X format leaves you very little time to explore any one topic in much depth, and takes you away from family at a time when I'm sure they'd appreciate you being available more. That said, I was very impressed with how much you managed to fit in. Thank you once again for your work (and ocharles for years past), you have accomplished a lot of good for the Haskell community! *Edit: typos*
According to Wikipedia [TopCoder](https://en.wikipedia.org/wiki/TopCoder) is a site that lets programmers build their reputations and get prizes by winning programming contests. It makes money by licensing the results of the contests. Royalties for licensed software are paid to the developers. As for how much is paid and whether or not it's "fair", I have no idea.
Great job!
For a lot of bioinformaticians though, it's not that so much as it's *another* language to learn that is tangentially related to getting things done. Most of the people I've worked with are more interested in figuring out biology or statistics, and the tools with which they work are of minor interest.
However, in this specific case it is not tangentially related: the build system in question is natively configured in Haskell so it makes the most sense to expose a Haskell interface to end users.
Rewriting the entire thing in Haskell would be painful, and there's not much in the way of a toolchain that produces code that can stand-alone on bare metal. You'd probably have to hack into the runtime. An OS in Haskell [has been done before, but it used a customized version of GHC](http://programatica.cs.pdx.edu/House/) which is quite out of date at this point. I'd suggest a simpler task to start with: rewrite parts, or write new code, in Haskell and link it into the existing C code. Here's [an article on writing a Linux kernel module in Haskell which might give you some ideas](https://wiki.haskell.org/Kernel_Modules), though that also uses the older, modified GHC mentioned above. Another possibility [might be to use JHC](http://repetae.net/computer/jhc/) instead of GHC, since it compiles to C code which includes the runtime. I don't think it needs anything more than malloc, and JHC has good support for working with C code. Personally, I'd start with the file system since functional programming would really shine there. Some other resources you might find helpful: http://stackoverflow.com/questions/3862954/systems-programming-in-haskell https://github.com/GaloisInc/HaLVM
&gt; But since f is an arbitrary functor, f b -&gt; f t and b -&gt; t are isomorphic. I think this is a mistake 
When it comes to programming there is usually an implicit state value or dependency, and implicit state mutations. Since these are implicitly made, used, or done, it is very difficult to trace where problems lie, edit the system, or simply do unit tests. In the case of x86, there's exploits that directly rely on the implicit state collisions, like the infamous usage of the interrupt system to overwrite part of the protected memory that can let someone get Ring -2 Access from Ring 0.
It took some thought, but I think it's right: Firstly, there's clearly a morphism from `(b -&gt; t)` to `(f b -&gt; f t)`: `fmap`. Now, the other direction. Let's suppose we have some function: g :: forall f. Functor f =&gt; f b -&gt; f t Because `f` is an arbitrary Functor, we can choose any Functor we like, so here let's use `Identity`: g' :: b -&gt; t g' = runIdentity . g . Identity
I don't think that's what arbitrary means here. Arbitrary means it has to be true for every Functor, not for some Functor, hence the `forall f` And it's clearly not true for something like `Const a b = a` 
Many thanks! They were useful and a good read.
Good to know! Could we have this as a commented-out hint in each new `stack.yaml` as well? I'm not sure I'll remember this post in a couple of months.
But that's not true
Seems plausible to me. Why do you think not?
`data Const a b = Const a`. Easy enough to get `(a -&gt; b) -&gt; Const c a -&gt; Const c b` (`fmap f c = c`) but how would go the other way? 
It doesn't mean what you think % ghci -XRankNTypes &gt; let f = (\(Const c) -&gt; Const c) :: Const c Int -&gt; Const c Bool &gt; let g = (\(Const c) -&gt; Const c) :: forall f. Functor f =&gt; f Int -&gt; f Bool .... Could not deduce (f ~ Const a0) from the context (Functor f) .... 
No, the mapping would be (forall f. Functor f =&gt; f a -&gt; f b) -&gt; (a -&gt; b) not forall f. ((Functor f =&gt; f a -&gt; f b) -&gt; (a -&gt; b)) /u/peargreen correctly says `forall f. Functor f =&gt; f b -&gt; f t` means “I'm a function that works on any functor”. Well, a function `Const c a -&gt; Const c b` *doesn't* work on any Functor, and therefore is not a valid argument for the mapping.
What? Of course not, `Const a` is just one inhabitant of `Functor`. Let's turn the tables around. If there is a morphism from `forall f. Functor f =&gt; f a -&gt; f b` to `a -&gt; b`, what is it? 
&gt; What? Of course not, Const a is just one inhabitant of Functor. Right, and that's why your purported counterexample isn't actually a counterexample :) [/u/AdituV wrote the mapping earlier](https://www.reddit.com/r/haskell/comments/3y4kvn/lens_over_tea_5_prisms/cyaqi91). % ghci -XRankNTypes &gt; import Control.Monad.Identity &gt; let g = (\f -&gt; runIdentity . f . Identity) :: (forall f. Functor f =&gt; f a -&gt; f b) -&gt; (a -&gt; b) 
Oh I see, word. 
Congrats on a 1.0.0! Stack has been a real shot in the arm for haskell tooling! 
Yay! Do say if there's anything else in the post that seems wrong, tho – even if it turns out to be correct after all, there's still an opportunity to improve the post by [clarifying it](https://github.com/neongreen/artyom.me/commit/e8af1eaca3e92e24d15f1849524cc4b33939a6e1) to make it more immediately obvious.
:P
[removed]
Actually, Stack builds on top of Cabal and cabal-install. It would have been impossible to get Stack to where it is today without it.
Oh come on, I'm just saying out loud what everyone's been thinking anyway... is it not true that Cabal has failed to deliver? If Stack was able to effectively replace cabal and be even better at it in such a short time, doesn't this tell us something about Cabal development?
This is not true: For all functors `f` and `*`-kinded types `b` and `t`, the type `f b -&gt; f t` is isomorphic to the type `b -&gt; t`. This is true: For all `*`-kinded types `b` and `t`, the type `forall f. Functor f =&gt; f b -&gt; f t` is isomorphic to the type `b -&gt; t`. See the difference? In the untrue case, challenger (you) gets to pick a `Const` functor. Claimant loses. In the true case, it's the *claimant* that gets to pick the functor. Claimant picks `Identity`. Claimant wins.
Ɐ is an upside down Christmas tree anyways.
Stack only builds on Cabal, not on cabal-install (which isn't to say it doesn't consider lessons from cabal-install and the benefit of history and hindsight)
I agree completely, but it's hard to change that mindset. A lot of them seem to be perfectly happy with building up technical debt because they don't understand the internals enough to care about it. I think we need to be more effective at communicating and educating about the problem. Thing is that a lot of us come from the biology side and are lacking quite a bit in understanding what constitutes good maintainable software.
You can interpret `forall f .` as a type-lambda, so we could also write ex :: (f :: Type) -&gt; Functor f -&gt; f A -&gt; f B where `A` and `B` are concrete types and `Functor` is something like data Functor f = Functor { fmap :: (a b :: Type) -&gt; (a -&gt; b) -&gt; f a -&gt; f b } which is clearly equivalent to what Haskell's typeclasses do, if more verbose. Now, here's an implementation of `ex` ex fType functorImpl fa = fmap functorImpl z fa what is `z`? It must be some other function we have of type `A -&gt; B`. If you follow all the arguments carefully this is forced. Now we convert `ex` into `z` by using a trivial `functorImpl` z' = runIdentity . ex Identity (Functor { fmap f (Identity a) = Identity (f a) }) . Identity
The error you are getting is: Foo.hs:10:12: Couldn't match type ‘[Char]’ with ‘Response’ Expected type: ServerPartT IO Response Actual type: ServerPartT IO [Char] This is happening because your two different routes have different types. The `serveDirectory` route returns a `Response` but the `hello` route returns a `String` (aka, `[Char]`). Either of those routes are fine by themselves. If we look at the type of `simpleHTTP`: simpleHTTP :: ToMessage a =&gt; Conf -&gt; ServerPartT IO a -&gt; IO () We see that a server part can return anything of type `a` that has a `ToMessage` instance. Both `String` and `Response` have a `ToMessage` instance. But, when you try to put them together in a list you would have a heterogenous list of `String` and `Response` values. But, Haskell lists have to be homogeneous -- all values in the list must have the same type. To solution here is to simple convert the `String` value into a `Response` using `toResponse`: module Main where import Control.Monad import Happstack.Server main :: IO () main = simpleHTTP nullConf $ msum [ dir "static" $ serveDirectory DisableBrowsing [] "www" , dir "hello" $ path $ \a -&gt; ok $ toResponse ("hello, " ++ a) ] You may find that Happstack mailing list, IRC channel, or stackoverflow are a better place for this type of question. You can locate those resources via the links on the [community page of the happstack.com website](http://www.happstack.com/page/view-page-slug/4/community).
The most surprising figure seems to be the dominance of tabs in Go, which are the least popular choice almost everywhere else.
I think it uses the dependency solver from cabal install.
In practice it doesn't matter. You set your editor to run gofmt on save, you do as you like while editing, your editor mode takes care of everything, it doesn't matter. You literally spend no time worrying about it. There's valid reasons to not use Go, depending on your use case, but that never really comes up as one.
It took a little while to get used to seeing things jump around when I hit save, but now I sort of enjoy being a little sloppy and just letting gofmt take care of it. I haven't done any tests, but I think it actually makes me a tiny bit faster.
I had some problems recently on OSX El Capitan and used OpenBLAS. There seems to be a windows package.
&gt; Is stack better than cabal-install in every way? Or are there times when cabal-install is a better choice? Yes. For example at the moment, if you use extra-packages and do `stack build --executable-profiling`. Stack recompiles everything (all dependencies), which is fine but can take a while. If you later do `stack build` (you forget to the ` --executable-profiling`) it recompiles everything (which can take a while). Ok you realize after 2 secondes that you don't need to recompiles the non-profiling version so `Ctr-C` to interupt it and retype `stack build --executable-profiling`) ... too late. You have to rebuild everything again, which is extremely annyoing when the build take 20mn. Ok, it's probably a bug (there is a [ticket](https://github.com/commercialhaskell/stack/issues/1102)for it) and the more people are using stack, the more they'll find bug and will start complaining about it ;-). Also, the problem with stackage is it forces you to bump all version of package you are using. What if you need to upgrade package A to version 2, but this pushed package B as well which unfortunately as a regression or is incompatible with your code. For example I upgraded to `diagrams-1.3`. to get version 1.3 on stackage you need to move from `lts-2.*` to `lts-3.*` which upgrade base 4.7 to 4.8. My code wouldn't compile with `base-4.8` even though everything would compile fine using cabal and diagrams-1.3. So sometime, cabal is better than stack (but stack works better most of the time). 
Maybe that's not a problem with the analyser, but with the practise of using half-indents...
It's still a mistery for me but does`--profile` implies `-O2` as well ?
The Haskell thing is very surprising! I feel like 4 spaces is more common in the code I read. I guess this settles it, /u/andrewthad :P
It technically does, but only in rare situations. So saying it's "building on it" is a bit unfair. It's a nice extra included because "why the heck not", but I don't think it's an integral part of the Stack experience.
It not just defaults to tabs, it doesn't have any other mode implemented. This is a deliberate design decision.
100% 2 spaces for Ruby? I'm suspicious...
If your analyser wants to analyse what people actually use it is most definitely a problem if it can't do that. It doesn't matter what anyone considers a good or bad practice, the analyser just fails at reflecting reality in its output.