We've been known to do internships here at McGraw Hill Financial. I believe all of the openings we have for this summer are already taken, however.
Both binary and source installs of GHC use the same place as platform, so calling homebrew/macports "most other installation methods" seems rather exaggerated.
My recommendation is to avoid constrained monads. You can implement a decent probability monad sans constraints, by switching perspective a bit. newtype P a = P [(Double, a)] -- or Log Double makes a perfectly good probability monad, just use an occasional normalize :: Ord a =&gt; P a -&gt; P a to collapse down the values to a canonical form every once in a while to keep it from growing too big.
I haven't watch the video yet, but I feel like overloaded fields would dovetail naturally with row polymorphism.
I really can't stand Python anymore - comment based type annotations are so verbose too that I just end up doing a Haskell style type annotation in a comment above. Like you, I do not handle the discipline of wrangling complex dynamically typed programs well at all. I write really shitty code in Python TBH. My Haskell code has, so far (from my intuition and what others have said), been really clean and high quality; which is funny because making the compiler handle mundane crap has actually allowed me to program in a way that's much more fun for me and allows me to express far clearer.
That sounds like bullshit. Most Clojure users simply don't find types to be worth the tradeoff in the long run. This is the same crap that got paraded around in the old Ruby vs. Java discussions, and it's still worthless.
Well like I said, you don't hear this from people who have programmed enough Haskell to understand your point about monads, but from people who are rationalizing not learning it. There's enough FUD out that it's pretty easy for frustrated beginners to go cherry-picking anecdotes from the internet to rationalize that the Haskell language itself must be flawed rather than considering that their past experience in imperative language doesn't necessarily grant them immediate understanding of this new field.
Not all Clojurians, just a handful of them. It's in my twitter history.
EKG can be used to monitor the RAM usage of a haskell process, along with other things. acid-state, if it crashes due to excessive RAM usage, will not jeopardize the content of your data. Every acid operation that executes in the Update monad that alters the state is considered durable only after that event has been logged to disk. If it crashes on your EC2 instance let's say, just increase the RAM usage and start it back up. Any remote client that needs the acidServer will auto-reconnect forever if it goes down. So they just hang, which is what you'd expect. 
Another nonspecific thing that helps sometimes is don't use the Haskell Platform--just install ghc and cabal (and alex, happy, doctest, etc. as you need them), and then do all the rest in sandboxes.
The Ord is for the keys, not the values. And, yes, you can implement as a list of tuples, but it's not as efficient, as mentioned [here](http://hackage.haskell.org/package/probability-0.2.2/docs/Numeric-Probability-Distribution.html).
Thanks. I saw that post; it looked like fun. I was more looking for "best practices" advice here, though, not some way to make it work. I'm not saying that it isn't neat that you got it to work, but I'm getting the sense that I'm better off just not going there.
Thanks, that's more-or-less what I thought. Interestingly enough, I've started doing a lot of F# at work, and I don't run into this problem there with F#'s monad-equivalent. I'm going to have to do some more reading to understand the differences in the type systems that let that happen.
What locale are you using, or how are you building?
My sense is that if I did so (unless a significant fraction of /r/haskell people are hiring) that doing so would do more for people's personal curiosity than it would serve this forum. If anyone is hiring and sends me their information, I would be glad to send any information privately. 
Yeah, math REUs are hard. And I also think I've seen enough of academia at this point that I'd much rather do some hands on work with haskell and see what that's like. Thanks for your advice. 
Isn't this CPS interface the least common bound of the streaming APIs?
this proof is invalid as it doesn't address the eliminator. For example you missed the term \x -&gt; (\y -&gt; y) x as well as the term \x -&gt; (\f -&gt; f x) (\y -&gt; x) You can use the lemma that every term is equivalent to a normal form, but proving that for system-F is non trivial and comparable to the proof of the abstraction theorem itself (which actually implies strong normalization) The free theorem for `forall a. a -&gt; a` as interpreted on the semantics of realizabiltiy as untyped terms is: for any relation on terms $R$ if `f : forall a. a -&gt; a' then for any $(x,y) \in R$ we have $(f x,f y) \in R$. That this implies identity is trivial: for any term `x` we simply define $R = \{(x,x) \}$. Plug and play gives you the result. An even simpler proof (and simpler than the "proof by typing rules" approach) is the "singleton argument"--we simply extend system F with singletons, and use the untyped releasability semantics for types such that `v : T` is interpreted as the same object as `v : Sing T V` and such that universals are interpreted as intersections. Then, you just instantiated the forall with the singleton, easy. Proving by pattern matching over *every possible type derivation* every time does not scale at all. On the other hand, the Girard-Reynolds isomorphism gives strong evidence that *every* fact about System F terms derivable via the type is derivable by free theorem (that is, the relational semantics is in some vague sense "complete"). 
thanks - I will
&gt; The fact that us lowly users never enter into your thought process at all is quite telling. Actually I care about users quite a lot, which is why I work very hard to keep APIs stable and avoid breaking their code. &gt; The fact that a totally neutral third party could write a totally new clone of wai/snap-core and you would refuse it no matter what while calling them an evil yesod shill says it all. Again, you're not reading what I'm writing at all, and I'm tired of explaining simple common-sense things to you, so this will be my last post in the thread. &gt; I think it is even more interesting how often you and mightybyte invent nonsense like this to try to distract from reality. I have nothing to do with yesod or wai or warp. The direction is entirely made up in your head. It may surprise you, but the universe doesn't revolve around you, and you're not the first person (and likely won't be the last) to push for this. &gt; The only grief I have been giving anyone is in response to the deliberate avoidance of a simple, straightforward answer. I've been as simple and straightforward as possible with you, and you continue to choose to misinterpret everything I say as "I hate the evil Yesod people". I don't, and they're not evil. I can't put it any more plainly than that. Goodbye.
 data[] = {1.0, 2.0, 3.0, 5.0, 1.0} ops[] = { const, (+), (*), (-), (/) } f(x,y,z) = data[y] `op[x]` data[z] so f(3,3,4) = (op[3]) (data[3]) (data[4]) = 5.0 - 1.0 = 4.0 Draw call 1: new_data[0] = f(1,0,1) = 1.0 + 2.0 = 3.0 new_data[1] = f(0,2,0) = const 2.0 1.0 = 2.0 new_data[2] = f(3,3,4) = 5.0 - 1.0 = 4.0 Swap buffers (`data` &lt;-&gt; `new_data`), draw call 2: new_data[0] = f(0,0,0) = const 3.0 3.0 = 3.0 new_data[1] = f(2,1,2) = 2.0 * 4.0 = 8.0 Swap buffers, draw call 3: new_data[0] = f(1,0,1) = 3.0 + 8.0 = 11.0 Return `new_data[0]`, result is 11.
The x inside the list comprehension comes from the infinite list?
yes this is a bit strange, I think you are looking for something like this: let solve x = head $ [ y | y&lt;-[0..], y*y==x ] Maybe the author wanted to filter the list of integer-roots. You could get this list (easy but slow stuff following) like this: let roots = [ (x,y) | x&lt;-[0..], y&lt;-[0..x], y*y==x ] Then you can get the answer by: let solve x = head . map snd . filter ((==x) . fst) $ roots Of course a better idea would be to write roots as let roots = [ (x*x,x) | x&lt;-[0..] ] but this is getting further and further away from the guide...
Yes your first idea was what comes to my mind at first, assuming the author only wants integer roots, and to make sure this doesn't run on forever I think it should be let solve x = head $ [ y | y&lt;-[0..], y*y==x, y &lt;= x ] what do you think?
Again, agreed. The second point is a shoe-in; Unity would be a good example of that. Curious what better languages you are referring to, especially in context of game dev and what the definition for better is in any case. 
Um, by definition it gives less freedom (rope to hang yourself with) than Haskell. 
seems fine too - of course normally we would try to write this in such a way that it will return `Maybe Int` instead of an error if it cannot find the integer root... Anyway: I think you got the list-comprehension thing and this is the hole point isn't it? :D BTW: instead of adding "y &lt;= x" you could just say "y&lt;-[0..x]" - the other thing is not the same as you can check by removing the head part and letting both versions run...(HINT: one of those will still enter a infinite loop)
got it, thanks a lot!
great, thanks!
Whereabouts are you? Fynder: http://fynder.io are considering doing some internships in London. We're built in Haskell and clojurescript/Om and doing some interesting real time web stuff. Shoot me a line on ben@ that domain.
isn't it already implemented? also, in what ways is it harder? the stuff I've seen used instances with special type classes that relate types to type-level strings literals to handle naming. this seems like a straight forward addition to the class system.
First of all thank you for replys What I want is to read an OWL ontology and have a structure representing the ontology, for instance: Class{ name :: String, .... } However, it's not so simple as it seems, because we can represent the same "information" in different ways.. That is why I wanted to know if it was possible to make a Haskell OWL parser from the XMLSchema. 
Exciting stuff. I just hope we're not going to expose users to a type family called `FldTy`. Good name for a GHC-internal type, not such a good name for a user-facing type that will appear in type errors. I will hereby donate 4 letters from my daily letter allowance so we can call this `FieldType`.
`import Unsafe.Coerce` [*^^queue ^^maniacal ^^laughter*](http://youtu.be/MSXOIf1YHmo?t=8s)
I don't think that's true. It gives you more freedom to give yourself less freedom.
(Haskell has libraries for parsing XML &amp;&amp; you want to parse XML) =&gt; You can do that in Haskell.
 ghc-pkg distrust zvxr *contramaniacal laughter*
*[cue](http://www.oxforddictionaries.com/definition/english/cue) ^I'm ^so ^sorry
:'-(
Oh no! My evil plan has failed :-( Should have realized that earlier, after all standalone deriving would allow you to get around any data type encapsulation this way.
Slightly off-topic: I really like this sort of "pearls/pills". They are relatively short, they convey a single idea/trick (so they still teach you something) and they are quite accessible.Chris, keep 'em come!
You're not the first one to request this! Once the basic code is merged into HEAD, I plan to fine-tune the library design and make some bikeshedding changes like this. I think the original motivation for very short names was making types/error messages smaller, but simplicity is probably more important here.
OverloadedRecordFields should be included in 7.10, God and SPJ willing, but it doesn't include full-blown row polymorphism. The difficulty is that row polymorphism really gives you structural types for records (e.g. `Record { name :: String | r }`) rather than the existing nominal types. It's not clear how to integrate these types with the current records system. ORF retains the current system, just adding constraint predicates that say a field belongs to a type.
BM's answer could be summarized as: The GTK display is complex and hard to test for portability, maybe there'll an option for this. No news since then. See the [thread on vim-dev](https://groups.google.com/forum/#!topic/vim_dev/0sETSAwe5Wo). @ibotty: I don't think it makes sense to send the patch to neovim. I strongly doubt they will keep the *gvim* code, full of ugly optimization and portability hacks, and probably conflicting with their new architecture. They won't hesitate to purge it, since, if I'm not mistaken, they [removed features](https://github.com/neovim/neovim/issues/460) before knowing what they were or how they would re-implement them. Anyway the patch is in the the link of the previous paragraph, feel free to use it.
http://en.wikipedia.org/wiki/Effect_system
Much the same as any other language. You take your big application and decompose it, define interfaces between the components, tackle them one by one. FP gives you greater flexibility for how those pieces compose. You can have isolated abstract message passing if you like, but there are many other methods which can be both more powerful and better reflect the true nature of the relationship between your parts. Better, there's nothing weird about these patterns. They're the same old monoid, functor, applicative, alternative, monad, arrow, etc you might encounter in the simpler examples. You can also do the opposite of this top-down approach and simply build your functionality upward from simpler functions. This often leads to the embedded DSL approach which is *incredibly* powerful in Haskell. UIs are a little challenging as there are fewer UI bindings and people like experimenting with better ways of executing them—but that's still just one piece. A lot of work is being done here though using things like FRP. I'm no expert, though, so I suggest you just dive in. If you're writing a web application, you can do your same old MVC pattern in Haskell. Happstack, Yesod, Snap, Scotty are all frameworks which will help direct you that way. A big problem with a full app is that you usually want to collect the various "effectful environments" of the subparts of your program together to execute in the large. Two major methods for solving this are monad transformer stacks and free monad interpreters. The former is a method of composing "effectful environments" neatly and usually results in applications which look like big stacks of effects from the top, but each individual subpart demands only the effects it actually uses. The latter is a method of keeping your code as pure as possible for as long as possible. It's very similar to the GOF command pattern, but much more lightweight and safe. This might also be considered a form of "message passing" as the pure (domain driven, perhaps) parts of your application just ship descriptions of the effects they'd like to invoke and the "master" program chooses to execute them however it chooses. As a final note, I spoke as though those two methods are mutually exclusive, but that's hardly the case. Each can be used next to one another—they're simply two methods of composing effectful subparts.
You probably realise this, but your question is very vague. You can build entire applications with functional programming, just like you can build them with with object-oriented programming and procedural programming. If you haven't found a single application, you can't have been looking very far. [This is the first result on DDG for "haskell applications".](http://www.haskell.org/haskellwiki/Applications_and_libraries#Haskell_applications_and_libraries)
This can be expanded, see: http://comonad.com/reader/2011/searching-infinity/ newtype Secret a = Secret { runSecret :: forall m. Monad m =&gt; m a } Now the end user can only plumb your monadic effects around, but they don't get access to the choice of monad -- even if you give them all the pieces, constructor and all! Parametricity forces them to use only the parts of the `Monad` they could if it were the trivial `Identity` monad, anyways. [Edit: this doesn't work]
Am I the only one who is a little disappointed that you didn't make `newtype` wrappers for `Celsius` and `Fahrenheit` and define an `Iso` between them?
Yep.
I guess I'm missing something, then. If you don't export `unSecret`, why does this need to be a monad in the first place? Just export Secret and whatever functions you want to allow people to use, and then they can't actually call `unSecret` or do other things you don't like. Is the monad actually important here?
I'm not even quite sure if we are hiring. 
&gt; whatever functions you want to allow people to use If you make it a monad, you don't need to export any functions for working with underlying values, just functions for unwrapping it (if you want to permit that).
Ah, that's a good point. Thanks!
That's far more fair. "A handful" of any given sampling of people will have odd ideas. That's just a given. Way more reasonable than implying that that's the norm among Clojurists.
Yes, I slipped up there. But the point stands that most Clojure users have tried Haskell as well (since they are much closer to each other than say Java), and for one reason or another they found Haskell's type system to not be worth the tradeoffs. Maybe it's a domain issue, maybe they prefer Macros over Monads for DSLs, or maybe they just like the tooling better (that's my reason).
Consider data Foo = Foo { x :: String, y :: String, z :: Int } With normal record fields, I can have [ x, y ] :: [ Foo -&gt; String ] \t -&gt; (x t, z t) :: Foo -&gt; (String, Int) With OverloadedRecordFields inferring \t -&gt; x t :: t { x :: a } =&gt; t -&gt; a \t -&gt; y t :: t { y :: a } =&gt; t -&gt; a will it successfully infer [ \t -&gt; x t, \t -&gt; y t ] :: t { x :: a, y :: a } =&gt; [ t -&gt; a ] and \t -&gt; (x t, z t) :: t { x :: a, z :: b } =&gt; t -&gt; (a,b)
This is good advice - try to build everything inside a sandbox. 
&gt; And Haskell, despite being older than me, [...] Dangit, I'm old.
Yes. (Up to some issues with pretty-printing the inferred types, at present.)
You can take a look at the [Real World Haskell](http://book.realworldhaskell.org/) book (freely available online), some of the chapters have applications or libraries written in Haskell, and they're more complex than typical functional programming textbook examples. 
I haven't used it, but it's probably not possible to use hackage libs.
Facebook, either London or Menlo Park. It's too late for this year, but next year is good. You could either work with the Haxl team building our Haskell-based DSL, or with the spam-fighting team who will be using it. 
I think largely it's the media, insisting that you have to be a genius to use it. So many people come into learning Haskell with the attitude that they'll just never be smart enough to use it, because that's how people portray it. There's a learning curve, but it's no more intellectually exclusive than any other language out there.
That is the one big benefit of the Identity approach. This version has the benefit that you can force your users to carry whatever effects you want around, not just Identity. Neither fully subsumes the other.
huh. maybe it's just an extension?
Why would you do this?
So am I, and I'm in my 20s.. :( 
You are not older than Haskell, then.
I think you might be asking how can you make an interactive program without explicit mutable state. Well the typical way in Haskell is to use mutable state, through IORefs, MVars, and TVars etc. 
Indeed, I misread your message (I'm not familiar with the w/* abbreviations); sorry for interpreting your message in a fairly diminutive way. I think that when I say "possible", I implicitly assumed "without breaking type soundness", which is why I wasn't thinking of unsound strong update. 
Wouldn't Functor be enough?
Why can't a user just choose m=Identity and then runIdentity to extract the secret?
I'm very interested in seeing why you think it's incomplete and not really the right way to go about things. It's certainly a meta-proof, but it's a nice little constructive one. And I'm not sure you need (or really, should want) anything more than that, since the whole claim is that there's only one *program*.
Not if you want to take two `Secret a`s and combine them to form a new one (e.g. `(+)`, `(++)`); for that you'd need `Applicative`. And if you want to condition effects on the contents of the secret, you need `Monad`.
good developing environment, GUI support, there is some stuff in haskell that never gets used because its not commonly mentioned, libraries and their documentation, alternative functional algorithms, data structures, cabal sucks
Libraries. Haskell would be better if it compiled to the JVM and could use all Java's libraries.
Dimensional Typing, anyone?
I don't think it's "the media", this is the public perception about the language, and it's not easy to change. I also don't think saying there is no difficulty at all helps much. No, you don't have to be a genius to use it, but the learning curve is, indeed, steep. Especially considering what's now idiomatic in Haskell, which is today much more typeclass-heavy and GHC-extensions-heavy than it was even 5 years ago. A newcomer to Haskell has a lot to learn before being able to understand a lot of the code that is used out there, and that's definitely a barrier. And I'm not even talking about lens. 
Currently Haskell can use all C's libraries - modulo an impedance mismatch (functional =&gt; imperative). Using Java would just swap the impedance mismatch (functional =&gt; oo). 
Can some one ELIABP (explain it like I'm a Basic programmer)? As far as I understand it, if I have a data structure of type a then newtype Secret a = Secret { unSecret :: Identity a } wraps the data structure in the identity monad. But I don't understand why do text &lt;- getSecret foo does not count as extracting the value? 
This seems pretty related to some work I did last year: you can find a gentle introduction to this approach in [A Duality of Sorts](http://www.cs.ox.ac.uk/people/nicolas.wu/papers/Duality.pdf), and a more theoretical piece in [Sorting with Bialgebras and Distributive Laws](http://www.cs.ox.ac.uk/people/nicolas.wu/papers/Sorting.pdf). Enjoy!
&gt; Startups using Haskell report that it’s easier to hire Haskellers if you control for quality. You might get fewer applicants, but they’re be uniformly higher in quality. It’s the modern version of the Python paradox. b00thead will chime in here.
No, cattle are programmers, not the core haskellers but industrial types (code monkeys). Someday, maybe, Haskell will be very popular not because it is good language, but because it has this electric fence. Electric fences are good for industry, because they lower the cost of maintenance. You may call me a troll, but the industrial world is slowly gravitating towards more sophisticated tools that will allow better control of less educated (cheaper) code monkeys. 
On a side note, this post is now the top google result for "abuse parametricity", for me at least.
Because we don't know any concrete information about m. We can't choose it to be anything. Notice the forall appears on the right side of the `=`. The only thing we can do with our m is whatever our Monad constraint gives us.
getSecret :: Foo -&gt; Secret Text
Pretty much a rewrite, except some 4000 words shorter.
Very cool! I was just toying around with some material I was using for an introductory presentation and happened upon this while reading the paper I mentioned in the post. It's actually really neat to see a paper dedicated to a similar topic!
runIdentity requires that we have concrete information about m. Specifically, it requires that we know m is Identity. However, we have thrown away all such knowledge by applying the `Secret` constructor. When we pull our `m` out of our `Secret` constructor, the only thing we know about it is that it implements the Monad typeclass. That's all we have to go on.
Something like this can be useful if you have a RPC library in which all of the operations got through IO, and yet we want to tag some of them as "merely observational" (think taking a screenshot vs. firing the missiles). Users of the library won't be able to build a "fake" observational operation out of "fire the missiles".
I find this incredibly useful. Please continue :) 
Oh, I see. Sorry, I must have been talking out of my butt. I missed this distinction. I am now as confused as you are :P.
I like the idea of homoiconicity (you dont have to wait years for a committee to improve syntax) + small, but cross platform core/runtime and platform independent libraries with central repo. I'll draw you deficiencies of popular languages: 1. Common Lisp/Scheme/Clojure - not portable core (ARM, Android), huge initialization times, there are some efforts to achieve portability, but they are just one-man hacks. 2. Haskell - the same picture as above 3. Java - truly cross platform, but a nightmare to write in 4. C/C++ - the same as Java. 5. Red/Rebol3/Rust - brilliant, but immature 6. Prolog - all *GPL or proprietary, not cross platform 7. Idris - my replacement for Haskell, waiting for GMP removal 8. Python/Ruby - slooow, not truly cross platform, GIL! 9. Javascript - dead language to write in, but good as a target (asm.js) 10. Pharo Smalltalk - single core only, VM eats battery on Android, non OpenGL GUI (slow). 11. Scala - breaks backward compatibility too often ... 
It is a little bit more You need that p a b ~ a -&gt; f b where f is Applicative, technically. Which means you can shove all the information about the Applicative to the right hand side. 
haskell has never had a breakthrough application built with it that screams to the world javascript had the web. objective-c, the iphone. java: the backroom show the world something built with haskell that demands their attention and "mainstream adoption" will not be a problem...
Oh I see. For some reasons I thought it was much older.
Haskell is the perfect language for implementing the first generation of compilers for dependently typed languages like Idris and Agda.. EDIT: This is a joke people. I am simply jesting that the only thing Haskell is good for is implementing a better language... 
I appear to have been overzealous. This works for the trick in my post, making the user plumb your effects for you, but not for the use case here. You need the quantification over `m` to lift over their entire calculation. e.g. modulus :: Real a =&gt; (forall f. Monad f =&gt; (a -&gt; f b) -&gt; f c) -&gt; (a -&gt; b) -&gt; a In the function `forall f. Monad f =&gt; (a -&gt; f b) -&gt; f c` you can't abuse the choice of `f`.
No, it isn't. An existential would be: data Exists a = forall m. Monad m =&gt; Exists {runExists :: m a} The difference: Secret :: (forall m. Monad m =&gt; m a) -&gt; Secret a runSecret :: forall m. Monad m =&gt; Secret a -&gt; m a Exists :: forall m. Monad m =&gt; m a -&gt; Exists a runExists :: Exists a -&gt; (exists m. Monad m =&gt; m a) Note how `runExists` can't even be typed properly in Haskell, while `runSecret` is not even Rank2. 
Because do text &lt;- getSecret foo doesn't let you use the value outside of the body of the do expression. So you can return it, but then you'll get it wrapped in a `Secret `again, and you can't print it or observe it in any way because the `Secret` monad doesn't allow you to do that. Just like `line &lt;- getLine` doesn't mean that you're "extracting" anything. Or rather, it does, but only inside its monad, just like here.
No, that's not correct. The type that edwardkmett gave says that `runSecret s :: forall m. Monad m =&gt; m a`, which means that we the caller can chose `m`. So we can extract an `a` without a problem. Your example below with existential types isn't quite analogous. You need to consider where the `forall` is: if it's to the left of the constructor, it's an existential type, but if it's to the right of the constructor, then it's *not* an existential type. In edwardkmett's example, it's to the right, so it's not existential. It's a very subtle difference, but it's very very important.
I imagine the subtle part is adding the new row kind and specifically adding the row-unifier/subsumption to the inferencer, which would diverge from the existing behavior of records. Sounds like a pretty heroic effort to get that into GHC will still maintaining backward compatibility.
A minor issue but there's some confusion over whether it's `insert` or `treeInsert`, as both are used throughout.
Terminology overload. Where most other languages have a handful of relatively independent concepts like loops, function, structs, interfaces, haskell has all of those *and* dozens more all the existing documentation absolutely depends on you to know by heart already, monads, monoids, functors, what have you - terminology *no one* else uses anywhere. Sure, the clearly defined concepts make it concise to talk about things very unambiguous, the same way chinese alphabet is easy to learn.
no, you need something that makes someone a billionaire, something your mom will eventually care about i am talking about something non-programmers will use. every other breakout tool was somehow attached to something that was meaningful in the greater world. this can happen in weird and wonderful ways...look at how unity has made mobile developers consider c#. people like mobile games, therefore, developers care about c# because they care about unity AND, conversely, if haskell is not chosen by people who create meaningful things, that might tell you something 
Very few people can explain the advantages of Haskell to people that don't know Haskell (or even programming). Most people can feel comfortable using python (from knowing nothing about it) after a day of reading, for Haskell it's probably a week. And probably poor mobile/arm support...
Oh, thank you! I'll update that right away.
yes. if there is one nasty secret of nearly anything succesful that contains lots of software: shared mutable state is something people want and need
Hi, I'm the maintainer of rdf4h. As Doug points out, neither rdf4h or Swish have support for OWL ontologies, though this is something I would really like to see happen. There are excellent python and Java libraries for the semantic web out there. Implementing an OWL parser in Haskell would be a great start. Then where? A SPARQL web server, a tractable SPARQL reasoner? If you have the time and inclination, I'd be happy to support this effort!
come on, haskell has a learning curve beyond python. there may be something to be gained from climbing that curve, but to wave it off is just wrong
&gt; import Control.Comonad.Identity What package provides that? GHC based on similarity suggested Control.**Monad**.Identity
Not if you give the user multiple Secret values to work with..
Good thing Haskell allows this and makes it explicit. 
I'm not denying a *learning curve*, I'm denying intellectual exclusivity. What I mean is that, if someone puts the time in to learn the language, they'll learn it. It might take more time to learn and use properly but you don't have to be a genius to use it, which is a common misconception.
Oh indeed. Once Idris is self-hosting, Haskell can just fade away into the annals of time.
Well, there are several of those. Succinct, clear code, referential transparency to prove whatever you want about your code, a strong static type system allowing easy refactoring, and probably most importantly, easy segmentation of program behaviors for ease of working with a team. Not to mention the helpful community!
Does having a method walk :: Traversable f =&gt; p a b -&gt; p (f a) (f b) really imply that p a b ~ a -&gt; f b for applicative f? I find this claim quite surprising.
http://hackage.haskell.org/package/comonad-4.0.1/docs/Control-Comonad-Identity.html I'm not seeing where this is using the comonad methods though.
This is my fault, I was extracting quotes out of context from people who weren't asked or advised that they would become broadcast in a public IRC channel.
Sorry :( I made the post because the quotations I twittered some people liked and wanted a comprehensive blog post with more context.
Having actually *used* java's libraries, this is laughable. Most of them are worthless crap. Maybe 10% are crap, but worthwhile in that using them requires slightly less hacking than rewriting them from scratch would. Less than 1% are actually good. That 1% is nearly covered by Haskell libraries. Bouncycastle is the only exception I can think of. In short, when people say they need the JVM for libraries, I wonder if they have some sort of advanced Stockholm syndrome.
&gt; It is curious that no one has ever raised this point against GoF design patterns, "They" do. See also the whole "patterns are a flaw in your language" discussion, which the Haskell community may have picked up with occasional glee but still originates from the OO community itself, as near as I can tell.
That looks good; worth a blog post if you didn't do so already. I would say that I mostly agree with everything here; I guess C#/F# are missing because you are not a Windows user; neither am I. However I do like C# and F# even more so; they work fine under Mono for all I want to do. Now that F* is open source, it's even more interesting. I have been playing around with picolisp which seems to not have the non portable core issue nor the huge init time. A is however a one-man project mostly but not a hack by any means. I don't agree Java is a nightmare to write in; it was (I wrote in it for over 15 years), but it's quite nice now. I miss stuff from C# when writing Java, but less and less so with every new version. Had to check for the Prolog license; I really like Mercury, but indeed you are right, it's GPL and very *very* hard to port; probably for the same reason Haskell is; the code is so optimized to make Mercury fast that porting it is hard and it doesn't have many people working on it. I do really like it though. Idris is my current pet favorite; you can actually write practical stuff in it and I'm a formal verification and type freak by education, so Idris &amp; F* are, to me, a few of the only really big steps in the right direction. Nice list. I should do something with Rust, read too much about it now to not try it. Thanks for that list.
Is there something specific that makes parsing these OWL ontologies hard? Or just never got around the time of implementing it?
Oh, ok I thought a snippet for beginners to test would consist of base package usage. :)
I don't see why it would really have much to do with records specifically tho.
then show us! build something the world wants to rip out of your hands that shows everyone why this tool is valuable. until then...talk is cheap. rip on Go all you want, but it has very quickly crossed the "we use it in production" boundary and is may cross the oh-wow boundary with tools like docker. the point i am trying to make is that at some point the world will respond to the constant stream of "haskell is better" talk with "put up or shut up!". lisp went through incessant overhyping and failed to deliver...
I'm not sure what you are saying, but it sounds like you're saying that you're expecting to learn Haskell as quickly as you learn Python. This will simply never be the case when your previous experience is with Java.
Are you guys interested in hiring new members of the team in general, or only interns (I don't know how large the team is)? I graduate in December and am starting to seriously look for job opportunities.
I think Haskell actually is going to get some serious mainstream adoption in the next 5 years--not like Ruby on Rails, but perhaps one day like Golang, but in a less "new hotness" manner. (Note: I have nothing against Golang.) As for the main impediment, I think the reason is probably because Haskell is like stepping into an alternate world with lots of new terminology and concepts. It takes a lot of effort to make yourself feel at home as a result. That's time that people have to take out of work or their home life with families and friends in order to pick up Haskell. Naturally that's a tough sell. With that said, I think the problem will solve itself, in the sense that as the language slowly gains more traction, so will more people gradually be exposed to more of the ideas and jargon of the ecosystem, and they will eventually be subconsciously less scared of its unfamiliarity as a result. Through some sort of technical osmosis, the problem of unfamiliarity will be solved in time.
What makes you say that? It is certainly really difficult to learn declarative programming when all experience you have is with imperative programming, but I don't think it's any more difficult than learning imperative programming in the first place.
I don't think anything in that post is relevant to what I am saying. Wai was originally meant to be a simple interface for interoperability. It was limited in how far it achieved that for several reasons. One is the explicit dependence on conduit, which /u/snoyberg is now attempting to remove, which I think is great. I might have done it differently, and it seems that you would have, too. But my opinion isn't that important here. Yours is. Please make explicit suggestions. You say that API stability is another issue? Fine. Suggest an explicit policy. The fact that the package name was originally used by /u/tibbe for something else is totally irrelevant. As is whatever back-and-forth there was between various people in the distant past. Please stay on topic.
A lot of things have concurrency. Ada on the lower-level end, all the way through golang, Scala, Erlang and Clojure. I'm not sure why anyone would go for Haskell if they don't already know it.
&gt; Most people can feel comfortable using python (from knowing nothing about it) after a day of reading Where "knowing nothing about it" means "several months to years of experience in very similar languages."
Lenses are really cool, it's worth effort. But don't worry, you can do just fine without them.
This has been proven in the increasing number of universities where Haskell is now used in the introductory programming course.
Which doesn't make anyone any money.
The real beauty in Haskell is for me the locality of the logic, no other language can I *read* a snippet of and know exactly what it promises to do , not to do and what it promises nothing about. This lets me read less code as I write new and focus more on the actual problem I'm solving.
You might want to look at State, TChans, MVars, TMVars, TVars, IORefs, STRefs, etc. Haskell has something like half a dozen variations on shared mutable state.
Potential to make a great application is there, but why has nobody made such a thing that is well known to people? The majority of programmers are not seeing an example of success with Haskell, which is probably making them ignore it. People want well explained benefits that they will get from using it, and preferably an example they can see of someone before them getting those benefits. Btw, I like Haskell a lot, but this is what I think about its lack of adoption.
There are plenty of cultural reasons why Haskell adoption has been slow, but there are also some technical reasons that we should keep in mind as well: * Gigantic binaries. Disk may be cheap these days, but if you're in charge of deciding what goes onto the Ubuntu iso, a 100KB binary looks a lot more attractive than one that's 10MB. Yesod seems to be the worst offender I've come across; I have a simple web-app that's around 50-60MB. I don't know what the problem is, but I wouldn't be surprised if it's a bug in or a limitation of GHC rather than Yesod. * Various platform issues, such as requiring a version of libgmp that isn't even available in the standard Ubuntu repositories anymore. (Ghc binaries are now available linked against a more modern gmp, so the situation seems to have improved lately.) * Lack of a proper stack trace when something goes wrong. * Lack of low-level control over memory layout. A lot of things get boxed that should be trivially unboxable. * Cabal requires a lot of micro-management to get it to do what you want. It would be nice if it were a bit more automatic. (My least favorite example is profiling support. If you realize you need it after you've installed a bunch of libraries without using -p, you basically have to re-install libraries one-by-one.) In one sense, I think Haskell is a victim of its own success; since the language makes code re-use very easy, we have many small libraries instead of a handful of large, complicated frameworks, which means that any time cabal doesn't work flawlessly, we have a much bigger mess to clean up. * Lack of any kind of cross-platform binary format. If we could compile to the JVM or the .net/mono CLR, that might make Haskell more attractive in certain applications. For instance, I've been toying with Unity3D lately, trying to port a reputation system algorithm to C# so I can stick it up on the Unity asset store. It would be great if it were easy for big, cross-platform frameworks like Unity to interoperate with 3rd party Haskell code. * People often complain about IDE support. I use a pretty plain text editor most of the time, but many developers have tools they're comfortable with and if Haskell isn't well-supported by those tools, they'll be frustrated. edit: I will throw in one more that I forgot to mention: compile times. Ghc is by far the slowest compiler I have ever used. It's not something that bothers me all that much (I accept slow compiles as the cost of all the crazy optimizations that ghc does), but I can imagine it could be a little off-putting to someone trying out the language for the first time.
There are a lot of popular languages that haven't been anywhere close to the mobile market, with examples including PHP, Python, Ruby and C#. I don't think mobile is a make-or-break deal.
The JVM is OO. Interoperating with an OO system from Haskell in a typesafe way is going to be impracticable.
well this not being logan's run, you still have to contend with all of the imperatively-minded programmers that are walking around today doing important things. if you want to suggest some future in which this is not the case...okay...but every new college grad is probably mindful of the fact that 2 billion devices in people's hands today are developed with objective-c and java...and the job opportunities related to this. so am i to presume i must wait for today's mobile technology to vanish first before nirvana is reached?
This I think has some truth to it. I made several loops around Lisp, Erlang and Scala to get accustomed to FP before I felt comfortable enough to tackle Haskell, which in the end was just what I wanted from all of those languages and more.
One of the many improvements it makes over Haskell :)
You are of course correct in that! If you learn programming to earn money you should probably stick with PHP, Java or something along those lines. I just don't see how that translates to Haskell being more difficult to learn than e.g. PHP.
No, not at all. If you want a straight mutable variable with the same semantics as in most other languages, you want an IORef. Most of those deal with concurrency concerns, which do not map straight onto variables, but either onto a combination of variables + a language feature + (often) a library or are simply inexpressible. For example, a MVar can be used 1. As synchronized mutable variables, 2. As channels, with takeMVar and putMVar as receive and send, and 3. As a binary semaphore MVar (), with takeMVar and putMVar as wait and signal. An STMVar is similar, but uses Software Transactional Memory so you can perform groups of accesses atomically and (i believe) locklessly (and automagically have them be retried if they failed). How do you do that in C?
How well does this scale with large records? Say 30+ fields?
It can be overwhelming but you just have to embrace it as part of having a community which keeps coming up with new ideas. The fact that there seems to be a new idea or pattern every month is actually rather remarkable. We live in interesting times.
Well, there's the parsing part, and then the "what should it parse to?" and "what can I do with it"? Hopefully the structures used by rdf4h or Swish are sufficient for the former, but if you actually want to make use of OWL reasoning then that requires work. Swish has some basic reasoning capabilities; I've not looked at rdf4h enough to comment on its abilities.
Kind of depends on how you see it. There are variables, class-instance fields, class-static fields. All of them accessible from different scopes.
So, I'll give you my experience in the domain of web programming. I might be a bit harsh, but I believe in constructive criticism. First, I feel that web programming is a very good vector to popularise a programming langue. (HN/Reddit/Social network effects). It is easier to produce a buzz about web programming than say, finance oriented programming. Web programming in Haskell is quite an enjoyable experience. My friend had chosen to use node. I work with him now, and I have to work in node for now. While programming in javavscript has _a lot_ of drawbacks. The fact is the overall environment is more friendly. Adding a library is very easy, the cabal-install problem is real in Haskell. The major problem is _not_ about cabal-install but more about enforcing good library numbering usage. There is not notion of package reliabity (alpha,beta or stable) in hackage. Node (and clojure) also have some of these problem, but the reality is that I experienced very few of them in node while quite often in Haskell. The goal is to develop and deploy with minimal problems. For this, I want an easy way to say: "use library a, b and c in stable release". If I launch a cabal-install in 6 months I want my package to build without error. It is possible, but you have to do a lot of this work manually, maintaining a list of packages compatible between them. Another aspect, the objects/abstractions you have to deal with are far less impressive at first sight. Generally all libraries show you a nice tutorial, most of a time a simple no brainer copy/paste and that'll do the work. In haskell, the type are generally enough, but you have to produce more cognitive effort on how to use them. On the other hand, types are a great documentation that completely lack in node (and also clojure). Also, you shouldn't pass over the "hipster" effect, and when you look theses sites: - http://actionherojs.com - https://www.npmjs.org - http://nodejs.org and also most node related blog, their design feel younger (you could say the same thing about clojure as well). When you look at the design of haddocks generated documents you feel like ten years ago. The design is not responsive for example, it is not flat, it is not hype. So the lack of designer in our community provide a feeling that haskell is a language for "old bearded white male wearing Linux tee shirts" (you get the idea). So even before trying to look at haskell you might have a bad feeling about what will follow. Another critique I find hard to take: in node while everything seems clumsy, coded with feet, the result is that it always work easily even if you don't understand everything. In Haskell on the other hand, if you don't understand a specific abstraction, or how a library should be used, you are completely stopped. The reality, is that I am more productive in node and even more in clojure than I am in Haskell. Of course, this is true for small projects only. But the community of node (and clojure) don't find it problematic. They just make "small" programs that communicate together. The fact that the programming language is more bug prone, promote a "small is beautiful" effect. They also focus on simpler to understand concept, of course far less abstract but easier to grasp for most people. A good example is to compare the standard blog platforms proposed by both communities: - Haskell: hakyll (I use it, it is great, but not so easy) - Node: ghost (no brainer, everybody could use it) So to promote Haskell my _very biased advice_ are: - do some design - deal with easier to understand concepts - make a stable/unstable/experimental hackage, with the following property: `cabal install all-stable-package` compile without error. nb: an example of a greatly designed functional language website is http://www.scala-lang.org
Is this the same thing as existential types?
Well so the proposal that I'm familiar with is like this: We introduce new syntactic sugar, namely, `r { name :: String }`. Behind the scenes, we have a type class `HasField` which is kind of magical like this: class HasField r "f" a where f :: r -&gt; a I say magical because you need to some how tie together the type-level string (or whatever you use) and the actual method name, so that your instances can work out. One instance, for the accessor `name` would be instance HasName SomeType "name" String where name (SomeTime ...) = ... Then the desugaring is straight forward, eg: r { name :: String } -&gt; Bool desugars to HasName r "name" String =&gt; r -&gt; Bool
&gt; but I don't think it's any more difficult than learning imperative programming in the first place. I don't have any evidence but this is my belief. &gt; It is certainly really difficult to learn declarative programming when all experience you have is with imperative programming However this I disagree with. You are discounting the commonly agreed upon definition of learning a language, which does not involve a paradigm shift.
Unfortunately what everyone really means by reason is financial reason. People like corporations follow the money.
From a software engineering standpoint, there are a lot of libraries that make the process as in other languages. From the point of a new user this might not be obvious, because Haskell comes with cheap batteries included. [2] There are a lot of libraries (and patterns) that deal with configuration, distribution, data persistence, serialization, network/process communication, etc. However you'll have to search for them, or be in the appropriate club [1]. They rarely come in a bundled set, unless you look in the web department (yesod, snap, etc). As a user that doesn't use those, you'll most likely use monad transformers to layer them as the application requires. Those may seem complicated, and times annoying because all the IO lifting; but a natural step in the direction of writing large/complex software. You can study open source applications written in Haskell like XMonad, Leskah, Yi, git-annex, pandoc, etc. in order to learn these software design patterns. Hope that answers your question without going towards more specific Haskell concepts/idioms that would be confusing to explain without you knowing them (at least passingly). [1] Wasting hours to build your own, go to #haskell "Why don't you use X? It would solve the problem" [2] see Python
I wonder if strict by default would be less of an issue with better compiler support. Something akin to JIT in-lining based on usage patterns.
&gt; In my mind you'd want a model to effectively be a data structure endowed with an algebra of actions on it, [...] That's certainly how I tend to think of it. Of course, the problem then becomes: how can we capture this pattern and package it up as a framework? Part of the problem with focusing on patterns like MVC is that they're more a sort of ideology or conceptual ontology than they are the sort of thing that can be packaged up as a nice collection of combinators. This is why all the various MVC frameworks out there don't really give you MVC in its entirety, they only give one particular instantiation of the theme. (This is especially an issue for simple patterns like MVC. For more complex patterns like PAC there's more structure behind the ideology, so there's more that can be codified and shipped as a library.) The fact that most of the work goes into the box labeled "model" is inescapable I think. For example, if we just think about a data structure with actions and views on that structure, then of course most of the work is going to go into defining what the data structure actually is. We can't have a model without sitting down and figuring out what our data are, how they relate to each other, etc— but all those details are application specific. So an MVC framework can either leave the definition of the model entirely up to the user, or else it can restrict itself to only work for a specific class of models in order to be able to provide a set of combinators for defining models in that class. The common idea that the model is the database is one example of such restrictions. 
Good point. I'd still argue that Haskell lets large teams to work on pieces of software together in a way that disallows anyone messing up anyone else's stuff, but there's less to offer to someone seeking only financial gain.
What would be the other many improvements?
While I agree with your point, &gt; Ruby [RubyMotion](http://www.rubymotion.com/) &gt; C# C# is common in modern Windows Phone development, and there's also [Xamarin](https://xamarin.com/) for other mobile platforms. Yeesh, even Visual Basic, if the docs are true. I can't say anything for Python or PHP.
Science, man.
Because it's a build tool that happens to have a download helper (fetching dependencies is kinda nice), not a package manager.
The paradigm shift is what most people consider hard when learning Haskell though, so it sounds like you're the one discounting it.
^(I'd love to speak a five-year old who codes in Haskell, let alone wonder about software distribution.) Source packages are always right, in every situation (or else, they're buggy). Binary packages are built from source, but are only correct in some situations. Binary packages may be specially made for a certain operating system, a specific compiler, a specific version of a compiler (.hi files)... and don't forget that Cabal has configuration flags, which each can be turned on and off. For every combination, you'd have to make a binary package. That results in a lot of binary packages, and some combinations can't be built by the same machine (it's kind of hard to make a Windows package on a Linux machine). You might also have to do that for multiple versions of the source package. If you'd really want to do this, it's easier to make a binary release of a collection of packages (a bit like Haskell Platform). Otherwise, it'll save everyone a lot of headaches by just distributing source packages.
Because they would have to be different on different platforms, or when compiled with different versions of GHC, or when using different dependencies. Maybe I compile and get binary #1, but then next week a dependency has updated and I get binary #2. That's a lot of work. If you want to use binary packages, you have to freeze changes. Debian and Ubuntu do this. You can install Haskell packages using apt-get on Debian but you probably won't find the package you want and it might be out of date. That is the cost of binary packages.
And variables *suck* for shared state as soon as you put in even the tiniest amount of concurrency. That's why we have those other things for various situations. They actually work, as opposed to just bare variables.
Good idea! Think it's portable enough for me to try? I considered making one a while ago, but wasn't sure how to handle libraries. With sandboxes I think it wouldn't matter?
To be honest, I think the actual question is "Why don't we just ditch cabal and switch to [Nix](http://nixos.org/nix/) completely?".
Is this related to linear types? I ask because I've only recently learned of them. "Linear types can change the world!" Wadler, 1990: "Values belonging to a linear type must be used exactly once: like the world, they cannot be duplicated or destroyed. Such values require no reference counting or garbage collection, and safely admit destructive array update."
Until you want concurrency, and then there's a equally byzantine array of state models and abstractions in imperative languages. Do you want mutexs, transactions, futures, channels? This complexity is not something Haskell created, it is something that exists and needs to be dealt with. Pretty much the only thing everyone agrees on is that trying to use plain mutable variables in concurrent code is a recipe for disaster.
&gt; coded with feet I am going to use that phrase much more often from now on.
What trouble did you have getting automatic translations for Status? Either of these seem to work fine for me $(deriveJSON defaultOptions ''Status) instance ToJSON a =&gt; ToJSON (Status a) instance FromJSON a =&gt; FromJSON (Status a) (the default instances obviously requiring that Status derive Generic). The GHC Generics way doesn't look like it can use a Generic1 instance, but it seems you don't need that to get Aeson instances (maybe if you were trying to get automatic support for a type with nested polymorphism).
I rarely see those and typically would want to factor them into smaller logical groupings. If you do have a giant flat record you're copying around a bunch it might slow it down but I still don't think it's very likely that repeated copying of ~250 bytes is going to be the major component in your slowdown.
That corresponds to just IORefs. Variables do not replace any of those other examples.
The amount of monad tutorials is decreasing now I think because monads are occurring in other languages, so the torch has been passed to them. People have heard of monads before they've used Haskell now.
It began development in the 80s, if that's what you were remembering. 
Though `Data.Functor.Identity` (from base) is all that's used there, Control.Comonad.Identity re-exports it, together with material that's not being used yet, but might be -- so it makes sense as policy if you're into comonads etc.
I remember an article about the newest IO manager saying that it allowed an SDN controller to be written that was faster than any other available ones anywhere, but I don't know if anything came of that.
I agree about Hackage needing better tagging, and that it would be better to adopt a convention over configuration approach ala Rails. I'd also love to see some enforcement to ensure quality over quantity. Two tags I would love to see (improved) are: **Supported Operating System**: There are many times when I cabal-install a package but it won't build on Windows. It would be a bit of work, but when a package is uploaded it would be amazing for a script to automatically build it (ex: in a VPN) and update the tags accordingly (ie: it build on Mac, Linux, but not Windows). My Haskell skills are a bit too weak right now but I'm considering this as my first real Haskell project. I also think that the additional clarity could help with improving the quality of Hackage. By being able to filter packages by their build success open source contributers can more easily help to get packages working on their operating system of choice. Right now its a random guess. **Versioning**: Everything you said plus a script to check module exports for breaking changes against the previous version.
This could be done without affecting the GC on values which are known to not be pointers (i.e. unboxed, where all contents are also unboxed). ie, as of 7.8 automatically unboxed: record Foo { a :: Int, b :: Int, c :: Int } I would be very very keen to know what kind of corner case performance increases this would yield.
You can't just paste together a bunch of code scraped from the web and hack together something that works. You have to *understand* what you're writing with Haskell. Look at how most people learn new languages and tools these days: 1. read an article/blog post about the new shiny, decide it sounds worth trying 2. install a package for the language 3. read a tutorial or getting started guide on the web 4. paste together bits of code found via Google searches for similar stuff you want to do 5. ask n00b questions on Stack Overflow 6. hack on something until it kinda works Nowhere does this process involve reading an entire book that is pedagogically designed to take you through a graded learning path. For many languages, this process works sufficiently well that a user can get early satisfaction from producing something that works. Especially in dynamic languages, the user is rewarded at a very early stage, and is incentivised to continue learning. Even if the code is ugly, buggy, non-idiomatic, a newcomer can stumble along sufficiently well that the effort/reward curve is in their favour. The road to learning Haskell to the point where you can do something useful with it is significantly more demanding than other languages. Some is due to the inherent differences in the language, some due to the learning resources available. Fortunately, improving the latter can (to a certain extent) overcome the former.
I hear this a lot, but Haskell is certainly harder for me to grok than my first language was (Java, only a few years ago). I don't think it's unfair to say that Haskell is harder to become productive in than other mainstream languages. The high levels of abstraction and large number of concepts in the common libraries is a barrier popular languages don't have. I mean, I feel similarly confused when I use highly generic and abstract code in other languages. But in those it's very rare and quite avoidable; in Haskell it's the norm.
Yup, something like that.
&gt; I think Haskell actually is going to get some serious mainstream adoption in the next 5 years--not like Ruby on Rails, but perhaps one day like Golang, but in a less "new hotness" manner. (Note: I have nothing against Golang.) I'm curious why you think Haskell will get more adoption. Go has received a ton of adoption in a short time, it seems to me like Clojure is getting adopted faster by new programmers. &gt; As for the main impediment, I think the reason is probably because Haskell is like stepping into an alternate world with lots of new terminology and concepts. It takes a lot of effort to make yourself feel at home as a result. That's time that people have to take out of work or their home life with families and friends in order to pick up Haskell. Naturally that's a tough sell. That would be my #1 reason. Learning Haskell reminds me of the first time I played with LISP/Scheme - it's a totally different thing to wrap your head around. But with Haskell even after getting a base handle on Monads and currying and I find 'common' things like Lenses that just look like it will take another 2 books and few months of reading to get my head around. &gt; Through some sort of technical osmosis, the problem of unfamiliarity will be solved in time. It hasn't happened yet. As things go on more people will see Haskell but the world isn't static. Haskell has to compete against the traditional languages (C/C++/Java/Python) and the hotness (Ruby/JS) and the newcomers (Clojure/Go/Dart/Scala) and the other 'others' (OCaml, Erlang), and languages that haven't been made yet. Haskell is neat, but as someone new to the Haskell world that seems like the rose colored glasses that come with a true believer. To me that sounds closer to "year of the Linux desktop". I don't think Haskell will die but I certainly don't get the sense of "any minute now". I don't see enough mindshare for that kind of snowballing. I would like to see it, but I don't think it's there. I'd be worried things will get worse as new languages continue to appear.
I know many people dislike Objective-C (not in your comment), but I actually think Objective-C is a very nice language and that it was sad it was so poorly known. It's nice that the popularity of the iPhone gave it such a big boost. But compared to Haskell learning Objective-C is trivial. If you have experience in a C like language it's not hard to pick up. It really is C with objects and some Smalltalk. If the only language you could use on an iPhone was Haskell (assume no performance differences/etc.) I don't think we would have seen the app explosion. There would be many, but nothing like today. People complained that Obj-C was an unfair burden to have to make iPhone apps but the truth is you could get the basics in a few days and quickly jump into real app programming. If you've never seen (or really used) a functional language (which I would say is easily the majority of programmers) Haskell is *really* difficult. If you've used other functional languages, it's still hard. If Java is English then Objective-C is German or Spanish and Haskell is Chinese or Japanese.
&gt; I think largely it's the media, insisting that you have to be a genius to use it. As a new Haskeller, it's not. The idea that Haskell is difficult is *correct*. It took me ton of reading in my spare time to be able to mentally parse basic Haskell and understand simple programs well. Go didn't take *nearly* that long. Clojure was easier and I wasn't used to LISPs. I imagine I could get a rough understanding of Dart or something else much MUCH faster than it took me to grasp that much of Haskell. &gt; There's a learning curve, but it's no more intellectually exclusive than any other language out there. Intellectually exclusive? No. Initial cognitive barrier? Vastly higher than other languages, even without the paradigm shift.
I've never heard of either of those. That's the problem. You need something that makes people *want* to try Haskell that isn't an academic project (I'm going to include compilers in that because most people don't write compilers). If it turned out that Twitter or Instagram or WhatsApp or something like that had been written in Haskell you'd get new users who see a big benefit. Look at [this question on Quora](http://www.quora.com/Haskell-in-Industry/What-is-the-largest-commercial-program-written-in-Haskell). I don't see anything in that answer that I'd expect your average programmer to have run across. In fact having GHC be the second largest is a bad sign. Java (with libraries) is a *huge* codebase, but there are still tons of other huge pieces of Java software.
I'd also recommend [Beginning Haskell](http://www.apress.com/9781430262503). So far I've found it to be much better written than Real World Haskell.
How many jobs are there for shell scripting? Even if there were, we already have shell scripting. Why is Haskell better than current shell script systems in a way that makes it worth the effort to replace them? Then ask yourself: how many jobs are there for webdev? For apps? For backend systems? &gt; Haskell isn't really an "application language". Haskell is all about libraries. With enough good libraries, the application part becomes trivial and not really worth thinking about. That comes off incredibly hand-wavy, in the way that dot-com business plans were hand-wavy.
Haskell provides a great platform for language research and is great for certain tasks. Certain features or design patterns get a lot of attention, putting pressure on more mainstream languages to adopt them. It's like evolution, but features can cross species spontaneously instead of requiring thousands of years of random mutations (well, not in every case...). I don't see a reason Haskell needs to be more than that.
The Java standard library plus some of the Apache commons is enough to cover and incredible portion of the code you'd ever need. The fact is you can almost always find a Java library. Many big companies or services provide Java libraries or bindings to their services. It's usually a first-class platform. That is *not* the case with Haskell. 
Nope. It's more like the idea of "tainted" values. A user of your library can't `unSecret` a `Secret` so whatever computation uses the `Secret`ed value gets `Secret`ed as well.
The hanzi composition laws are rather complicated though.
&gt; That works only because old values will never point to new values and that simplifies the dependencies between values greatly. That's not really true due to laziness. We still have to have a write barrier, even for pure code, for this reason.
Some library specific optimizations do this. A good example is array recycling (I can't find the original paper atm, but you might enjoy [this](http://gereeter.blogspot.com/2014/01/extending-array-recycling-with-delayed.html)).
&gt; all in the name of some apparently-necessary abstract goal... Lenses solve a very practical goal: setting and modifying values deep within a record is very verbose in a language like Haskell. To replicate an example from an [old stackoverflow](https://stackoverflow.com/questions/7365425/is-there-a-haskell-idiom-for-updating-a-nested-data-structure), suppose you had data BBTeam = BBTeam { teamname :: String, manager :: Coach, players :: [BBPlayer] } deriving (Show) data Coach = Coach { coachname :: String, favcussword :: String, diet :: Diet } deriving (Show) data Diet = Diet { dietname :: String, steaks :: Integer, eggs :: Integer } deriving (Show) data BBPlayer = BBPlayer { playername :: String, hits :: Integer, era :: Double } deriving (Show) Would you rather see addManStk team = team { manager = (manager team) { diet = (diet (manager team)) { steaks = steaks (diet (manager team)) + 1 } } } Or addManStk = over (manager . diet . steaks) (+ 1) Lens is boilerplate reduction, pure and simple.
I would compare learning Haskell to learning how to program to begin with. It is a literal paradigm shift -- from imperative and object oriented to purely functional, recursive programming with a whole slew of features you didn't know existed until now. Learning how to program in your first language is hard. Learning how to effectively use Haskell is comparably hard. Learning a new language is typically much easier because at least you can ground yourself by almost directly translating code snippets to your language of choice. In Haskell, this is sometimes impossible, and even if it isn't, the concepts by nature take a little bit of time to sink in. I don't agree with the blanket statement "haskell is difficult." I have a much harder time writing correct programs in untyped languages like javascript, or languages much closer to memory like C++. I understand and agree that Haskell is more difficult for imperative programmers to pick up and learn, but I do not agree that it is objectively any harder to use than something else once your bearings are there. I would argue the opposite: Because we focus so much energy on generalizing concepts, it becomes easy to think about entire classes of problems and think about them individually when the time comes. This, at least for me, lessens the mental burden required when programming -- I don't have to keep track of every little individual thing I learn (I'm looking at you, Java!) in order to be productive. And to top it off, I'd say at least 75% of my potential bugs are caught at compile time, and that doesn't even include the ones Haskell's purity catches just by its own existence. That's time I don't have to spend bug hunting (which is *way* harder than programming!), so it leads to higher productivity once your roots are in place. So yes, I agree that *learning* Haskell is difficult; I'll never deny that. But a) I think it's worth learning, for a multitude of reasons, and b) once you reach that first glimpse of a plateau in the learning curve, I do not think that Haskell is any more difficult to use than any other programming language out there.
The forest of extensions.
&gt;Why is Haskell better than current shell script systems in a way that makes it worth the effort to replace them? I didn't say Haskell was a replacement for shell scripting. You took my words way too literally. &gt;That comes off incredibly hand-wavy, in the way that dot-com business plans were hand-wavy. I guess you missed the point. What I'm trying to say is that everyone is looking for a "killer app". *Haskell itself* is the killer app! If you want to know more about the concept of developing libraries instead of monolithic applications, I suggest you check out the [Haskell Cast interview with Don Stewart](http://www.haskellcast.com/episode/002-don-stewart-on-real-world-haskell/).
&gt; Lenses solve a very practical goal: setting and modifying values deep within a record is very verbose in a language like Haskell. so Lenses don't solve a *programming* problem, they solve a *haskell* problem. just like Iteratee IO, Text, and Vectors...haskell to fix haskell. these have been valuable contributions, but lets not elevate them above using haskell to fix itself. why should a prospective coder pay the cognitive penalty of learning all of these patches? so you can still use the mistake-ridden Prelude? hopefully one of the haskell reboots like Idris will put these issues to sleep
In particular, Haskell was first proposed as an idea in the fall of '87, and the committee to design it first met in January of '88. They worked for two years before publishing the Haskell 1.0 report in 1990. GHC wasn't started until 1992, although there were other Haskell compilers at the time.
We don't have any sort of size-known-at-compile-time unboxable arrays, do we? Tuples can serve the same purpose for very small lengths , but they aren't so user friendly (`lens` helps here). We have unboxed vectors, but because their lengths can be chosen at compile time they can't really be unboxed as a whole (AFAIK).
C# runs on ios and android though Mono with xamarin[1] [1] https://xamarin.com/
Learning your first programming language is probably an apt comparison. I agree with the rest of your statement. I meant mostly hat *learning* Haskell is difficult. Once I got the basic bits it wasn't hard to assemble a nice working program. I've been very impressed with the type system and the bugs it can catch. Even if I never write anything more than a toy program in Haskell, I'm glad I put in the time. I like trying languages that very different from what I'm used to so I can see the ideas that weren't available in the languages I'm used to (e.g. lambdas in Java 7 and below) or aren't as common (immutable data, monads) and cool things that are possible (QuickCheck).
That's a good point, I forgot about that. It would be nice to be able to know how common various languages are. I'd be really curious to know the frequency of non-Objective-C languages on iOS for complex/popular/good apps.
I thought it was funny :)
But it's also the case that these solve problems that arise as a result of solutions that yield other benefits.
I can barely keep up with Haskell :: Reddit these days. Perhaps mainstream isn't that far away.
I've just been learning rust, and you can totally to this (explicitly). Example: fn mutter( x : ~int ) -&gt; ~int { let mut y = x; // This makes a mut pointer to x's box *y = 24; // We change what was in x's box // But note, if we tried to do // return x; // We'd get a type error, since we "used it up" // by assigning to y. return y; } fn main() { let a = box 42; let b = mutter( a ); // We don't need to know that there // was mutation used under the hood. println!("b:{}", b); // I'm not sure, but I think we've only ever // allocated that one box on the heap. } 
&gt;If Haskell was really so innately superior, I would expect it to have a much bigger mindshare. Then your expectations would not align with reality. Superiority has never guaranteed popularity in any human enterprise. Just look at PHP!
Not that I know anyone I want to experiment on in particular, but I'd like to see if this holds when you learn Haskell first, or if it's just about unlearning everything you know. I feel like it might be fairly intuitive if all you knew was high school math and no programming.
I think it's really a CL trope, rather than an OO one. I also think it's true, if we spell "flaw" as "constraint".
I agree wholeheartedly that the default definition of String is flawed. Linked list is pretty much always the wrong choice for representation of a string. I guess I just didn't equate String with the language. There are alternate preludes after all. However I suppose you are technically correct, since String is in the Haskell 98 report.
I think phrasing it in the negative like they have creates a false impression that it is struggling. A much better title would have been, "Here is how Haskell is gaining mainstream adoption."
It's not his job to show you. Who cares if clay_davis_sheeit uses Haskell. I mean really. Those of us productive with Haskell are just fine and do not need to prove it to the world just to satisfy internet trolls.
The list example can scale up by making the limit a parameter {-# LANGUAGE DataKinds #-} {-# LANGUAGE KindSignatures #-} {-# LANGUAGE StandaloneDeriving #-} {-# LANGUAGE GADTs #-} module Test where data Nat = S Nat | Z data AtMostFalse (n :: Nat) where Nil :: AtMostFalse n CFalse :: AtMostFalse n -&gt; AtMostFalse (S n) CTrue :: AtMostFalse n -&gt; AtMostFalse n deriving instance Show (AtMostFalse n)
I say Haskell is not mainstream because it combines the two things people hate the most: types and functional programming. People are fine with types (java, c) and they are fine with functional (JavaScript to some extend, ruby). But the two combine and people go nuts. Now this is just an initial scare. I say, after jumping into it, you won't understand how anything else can be used for programming.
So is there a Haskell Pinyin?
As a beginner, I think it would be better if the error messages were more helpful.
`read someString : someList` means the same as `(read someString) : someList`. Therefore in your example the result of `read numberString` is prepended to the list (stack) `xs`. The first three cases of `foldingFunction` match when at least two operands are on top of the stack and the next input string is equal to an operator. The fall-through case, so to speak, assumes that in all other cases the next valid input can only be a string containing an integer literal.
I think maybe you misunderstand the stack trace issue. What I think if is if something goes wrong in IO and an exception gets thrown, I'd be afraid I couldn't track down where that came from and the state. As an aside, exceptions in Haskell have always represented a frustrating, pervasive partiality to me. The exception handling mechanisms are sub par in Haskell but I honestly wish it just didn't have them. Go eschews exceptions, but I would find their approach eminently more usable in a language with a half decent type system like Haskell.
I've heard pots of FUD about the performance penalty of those traces. Any experience you can share?
I've got a lot of hope for cabal. In the last year or so I've seen really useful strides being made on hackage and cabal. We should give those guys credit for listening and making things better for us all.
In short, the context implies that `read numberString : xs` have type `[Double]`: words :: String -&gt; [String] foldl foldingFunction [] :: [String] -&gt; [Double] head :: [Double] -&gt; Double foldl :: (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; a foldl :: ([Double] -&gt; String -&gt; [Double]) -&gt; [Double] -&gt; [String] -&gt; [Double] foldingFunction :: [Double] -&gt; String -&gt; [Double] foldingFunction (xs :: [Double]) (numberString :: String) :: [Double] (read numberString : xs) :: [Double] read numberString :: Double 
The reason npm modules install so much 'better' than cabal ones is because no compilation occurs - your errors are deferred until runtime. This makes it looks like your module installed properly but really there are bugs hiding until you run your program. Good luck depending an specific version of a library with dependencies for a long period of time - at some point a dependency will update silently breaking everything. Sometimes the bugs aren't found until the product is actually in front of users. Sadly, I speak from personal experience about of this - while cabal hell is annoying, it's no where near as bad as npm hell. So long as you use sandboxes for *everything*, you should be fine. Edit: grammar
&gt; The fact that the package name was originally used by /u/tibbe for something else is totally irrelevant. That was only brought up because /u/haskelnoob made it sound as if &gt; WAI predates yesod and snap both even though that kind of "WAI" has [no direct relation](http://www.philosophypages.com/lg/e06c.htm) to the "WAI" that is discussed now. IMO the historical context is important to be aware of as well, even if the result may be more important.
In the UK, many universities (and most of the best - Oxford, Cambridge, Imperial, Edinburgh, Glasgow...) teach Haskell (or another ML-like functional language) as the introductory programming language for their computer science course. I can assure you people starting from the position of no programming knowledge find haskell just as easy to learn as $imperative_language. The ones who struggle more are those with imperative experience.
You want to tell me that if you have less experience with programming, you will be able to learn better. This is generally not true. When you know about programming you have concepts in your head especially logic, types variables, scopes, operators, parameters and many others which make you learn faster. Generally, the more languages you know, the faster you learn.
Any chance you could write a blog post about that? Nix is getting a lot of attention recently and it would be interesting to a lot of people to hear about its drawbacks.
You beat me to it! All I can offer is a late, but emphatic +1
QuickCheck and strong types are nice. They prevent the vast majority of bugs. Some bugs *still* will slip through. Those are then significantly harder to runtime-debug (due to lack of stack traces &amp; debugger) than runtime debugging in other languages. This isn't inherent.
Yes, this is exactly what I'm talking about. Xmonad is another good example. It's often called an application but it's actually a library.
I used this in my (I fear mostly bitrotten by now) [pianola](http://hackage.haskell.org/package/pianola-0.1.1) package. [Pianola.Internal](http://hackage.haskell.org/package/pianola-0.1.1/docs/Pianola-Internal.html) defines the **Nullipotent** newtype whose constructor is supposed to be hidden from clients. [Pianola.Pianola] (http://hackage.haskell.org/package/pianola-0.1.1/docs/Pianola-Pianola.html) defines a **Glance** type. A Glance is a kind of selector that searches over a remote data structure, downloading representations of parts of it during the search process (for example, requesting an screenshot). The interactions with the remote structure go through IO, but I wanted to limit the kind of IO operations a Glance could perform: it should only be allowed to perform "observational" interactions, not something like firing the missiles. I use the Nullipotent newtype for this. Notice that in [Pianola.Model.Swing](http://hackage.haskell.org/package/pianola-0.1.1/docs/Pianola-Model-Swing.html) the **_capture** method returns a Nullipotent. Perhaps I should rewrite Pianola to use lenses/folds/monadic folds for search, instead of my own weird only-for-this-package combinators.
They solve an immutability problem, which itself is quite a huge benefit. For example, consider that multiple deep record updates in Haskell are trivially transactional and persistent (new top-level record guarantees you can only ever see the whole update or none at all, unlike nested mutable updates). Also, they do much more than that. Gabriel Gonzalez wrote an excellent blog post showing how they gain you a lot more than just the ability to do simple nested records updates: http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html This is far nicer than what you can express in imperative languages: &gt; units.traversed.(around target 1.0).health -= 3 
&gt; Cabal requires a lot of micro-management to get it to do what you want. I'm I the only one who doesn't really suffer cabal-hell? Sometimes I have to nuke `~/.ghc` and recompile the whole world but I just go and get a cup of tea and come back twenty minutes later. It happens about once every two or three months so I really don't care.
&gt; Even C and Javascript, two languages with no notion of namespace at all, somehow manage to keep their field names from bleeding. Sure but the difference is obvious there. In C struct access can't be polymorphic and in Javascript it can fail at runtime. Surely it's frustrating that simpler languages have this feature, but it's exactly *because* they're simpler that they can.
&gt; A lot of this is because GHC statically links libraries in your executables. There are cases to make for and against dynamic linking, and it is provided as an option to the compiler, making it possible to significantly reduce file size. That's the usual explanation, but I'm skeptical that even a web framework like Yesod could pull in enough dependencies to justify 50-60 meg binaries. A typical install of Windows 95 took 52.6MB; can anyone plausibly claim that a simple Yesod app is as complicated as Windows 95? Perhaps static linking alleviates the problem to some extent, but still, I can't help but think that there's something else that's gone wrong, like maybe we're linking in the same libraries several (or several hundred) times. (Is there some way to have the linker tell you how much space is taken up by each library that's linked in? It would be interesting to see what libs are space hogs.) &gt; When you test a really small subset of your program, you don't need stack traces to figure out which part of your program went wrong – because it was the part you just tested! That's the ideal, but in practice not everyone tests every sub-component completely before throwing a large, complicated application together. You can always say, "well don't do that then," but I would rather just have stack traces that work. &gt; There are GHC extensions to control such things, like the {- UNPACK -} pragma with strictness annotations. ...except that the restrictions on what can actually be unpacked mean that you're stuck with boxing. There are cases where you'd like GHC to be able to unpack types with multiple constructors simply by allocating enough space for the biggest constructor (like a C union), but it won't do it. Simillarly, you'd kind of like to be able to stuff almost any Haskell type into an unboxed vector without complaint, but it doesn't work that way. Maybe there are good reasons for that, but it can be kind of frustrating if you need a very specific memory layout for performance reasons.
Linear types let you do that sort of sharing analysis.
This might be part of the problem actually, there's less incentive pick up haskell if everyone's only hiring people with years of experience.
In this thread: lots of detractors assuming Haskellers want to work to make Haskell mainstream, and that we "need to do this or that" before other people will start using Haskell. Personally I really don't mind. I want to publicise Haskell so that people who are wondering about the benefits of Haskell in production (like I was two years ago) have access to that information. Beyond informing them about reality, I don't care if anyone likes Haskell or not. 
Yeah, my wording there wasn't too great. I mean that (AFAIK) there's always at least one level of indirection to a `vector` or `array` from a data structure - `data V = V (Vector Int) Int` would contain a pointer to the `Vector`. But it seems as though, if this vector's length was known at compile time, that level of indirection wouldn't be required. But I don't know if that's even possible in standard Haskell or not.
Perhaps we're talking at cross-purposes here: psygnisfive, you're describing the OverloadedRecordFields extension that I've implemented. Row polymorphism, as described by freyrs3, is a separate approach.
Though I'm on a hiatus at the moment due to intensifying studies, I am slowly writing a book that aims to close that hole. It's got friendly writing like Learn You a Haskell, but it means to jump directly into toy applications and have toy examples on the side to reinforce points.
&gt; I find 'common' things like Lenses that just look like it will take another 2 books and few months of reading to get my head around. It depends on how you go about them. I started toying with them from a pragmatist point of view, which meant that I pretty much just copied other peoples examples with them. With time, I learned to cobble together the most common operations I needed myself. I still have no idea how most of the `lens` library works, but I know how to do the few things I often do. And I love it.
I agree. That's been a complaint for a long, long time and they are slowly but surely getting better! Which kind of error message do you have most problems with?
I think one smaller problem that often gets overlooked is not only that Haskell expresses a lot of different concepts – but that many of the *wrong* ones are the ones chosen by default. Such as partial `head`, exceptions in general, the `String` type, specialised functions in the `Prelude`, almost random-looking `Prelude` content and so on. People learn to use strings, they learn to use partial functions, they learn to use `mapM` from the `Prelude` instead of `Traversable`... and then we wonder why they aren't using `Maybe` more, or why they complain when they see 15 different ways of doing the same thing. It would be helpful if 1. We taught the right things from the get-go, and didn't tip-toe around the more difficult things that have to be learned anyway, and 2. The defaults in the compiler would be the sensible things, not the wrong things. I think point 1 is really important. I understand the inclination to avoid the more difficult things, because we don't want our students to get frustrated with us. But viewed as a whole, the student will instead get frustrated later, when they try to learn the correct thing by themselves. I'd much rather have them frustrated when there's a teacher nearby than when they are on their own. Substituting "teacher" for "book" or "tutorial" and "student" for "reader" when reading the above is encouraged.
I know you can do some seriously awesome unpacking with types, but my wizard hat isn't high enough to tell you about details in that case. You could very well be right. In either case, it's not part of standard Haskell; it would be a GHC thing.
Websites that don't allow you to resize the text on a mobile device however, are *far* from beautiful.
Well I've never used Yesod, so maybe that's insulated me from some problems. On the other hand I accidently `apt-get upgrade`d from 7.4 to 7.6 last week and I didn't notice any problems other than having to recompile all my packages. Maybe I'm just lucky with the packages I tend to use.
I think you'd be surprised how disorientating it can be to people who aren't familiar with the concepts - it's just one more ball to juggle internally when you're trying to learn something to help contribute to it.
You should restore the first revision of the script, as the current does not allow to pass GHC parameters with whitespace. In the first revision, just quote *all* parameters, including `$@`, to be on the safe side.
How did it take that long to get greenlit? [According to their blog](http://joyridelabs.de/blog/?p=1557) they stopped all development in Dec 2012.
Voting on Greenlight is only the first step (and the only public step) of the process. Besides, Valve will soon [kill off Greenlight](http://www.examiner.com/article/valve-to-do-away-with-steam-s-greenlight).
If I have interpreted this correctly, it's still a full game, it's just that they won't be releasing more updates and official levels and such.
How did you learn Java? Were there plenty of examples for you to copy, paste and modify for your own purposes? Or did you read a bunch of articles on the theory of objects? I guess the former. Lots and lots of clear examples would help Haskell beginners too.
Usually I can't figure out the ones I get when I'm trying to pass things back from functions or when I use 'return' (I'm not quite sure what return actually does). I don't know what t0 or m0 are supposed to mean; I just know I get them when I'm not using type signatures.
Of course it would help. But many Haskellers seem to think there is no need for tutorials or examples. ["Just look at the types, it is all you need!"](https://www.fpcomplete.com/user/chowells79/types-not-tutorials) If seasoned haskellers don't need tutorials or examples, great for them. I (and many others) apparently do. For now I decided to leave Haskell for Scala, which is sad, but the only way for me to use functional programming techniques I've learned with Haskell AND be productive at the same time.
&gt; And if you want to condition effects on the contents of the secret, you need Monad. I don't think this applies, because there are no effects to speak of. So `Applicative` should be enough. Certainly if Simon Marlows idea to only require `Applicative` for do-notation if possible makes it into GHC. 
I haven't tried it, but there's "windows phone application using F#" template in visual studio.
but the actual purpose is to address the fact that when it comes to records, haskell is by no means "better"
Takes me back to the days when I was first learning Haskell. I interpreted any type error from GHC as "Something's wrong but I'm not going to tell you what. Go back and look at your code.". Nowadays I can understand the errors a little bit better but it does take some getting used to.
sorry, just waving off the real world and claiming to be "self-evidently" better is just lisp all over again
Nah, I think it is fine as far as it goes.
Huh, you're right - GHC 7.6.3 says -O conflicts with --interactive. Is this new behaviour, or has my memory failed me? I guess it's the latter, I can't find anything that agrees with what I've said above...
in the future it will be `ExceptT IO` from the new transformers. i hope adoption of that exception pattern gets bigger then.
Haskell isn't the only answer for all users and all choices of fitness functions. It just is the best answer for many choices of fitness functions. =) I still write maybe 1% of my code in Agda and I still write maybe 1% of my code in Racket. &gt; and people who don't learn/use Haskell are just scared to learn something new? I don't think that everyone who doesn't learn Haskell is scared to learn something new. Perhaps they don't have time or perhaps we have failed to make a compelling case that it is worth their time. Perhaps /u/shapr banned them from the channel in their more impressionable youth or a Haskell user killed their dog and they have a grudge. ;) There are lots of reasons not to write Haskell. ^^I ^^just ^^don't ^^think ^^they ^^are ^^very ^^good.
of course teams can still clobber each other's work with haskell...just start removing pragmas and wait for the kaboom
WhatsApp was written in Erlang, but I don't know whether that attracted a lot of new Erlang users ...
I think it was an obvious rhetorical flourish to see that I was addressing the greater community. sorry, in english "you" is polymorphic
This 1000x is the same way I feel. I want to love to use Haskell but the situation you described just make it hard.
I wasn't suggesting that the actual values are copied - if that was the case it is unlikely any real-world Haskell program would ever finish within reasonable time. While your explanation of how purity is useful for GC is correct, I cannot follow the argument of "no, GHC doesn't do a potentially better optimisation to be able to do other optimisations". If you can in-place update the record, there is no need to GC anything.
It doesn't scale well - that is precisely why I'm asking the question. If you want to try it out, a fast loop in the strict State monad that updates record fields performs linearly worse as you increase the number of fields in the record.
I agree, but I don't see how it wouldn't work for pointer values: If you can statically see that your in-place update will update everything needed, you know that you don't have to GC anymore.
Excellent work tibbe, going to get straight to work adding this into a production setup!
I do not know about linear types yet. My question was rather about a GHC optimisation transparent to the user - would linear types be something the user has to deal with explicitly?
It's no accident that PHP became successful. PHP has an *insanely low* barrier to entry, that's why it go so much adoption. It was *superior* to getting started with web programming in other languages such as Perl or Java for novices.
Yes, for vectors that would be even more useful! I haven't finished reading the whole article yet, but already a question: Does this kind of fusion really permit rewriting an update to an already fully allocated, pure vector to be rewritten to a destructive update (given the old one isn't used of course) like I am asking? As far as I understand, vector fusion ends as soon as you have the fully allocated vector (e.g. after printing all its elements). The last time I tried this, this didn't work (e.g. using a fold over `Data.Vector.//`), and honestly, I cannot imagine how something expressible entirely within the Haskell language (vector fusion) can hav access to information that I believe only the compiler can have (value liveness analysis).
It would be great if you could a pointer to something that explains if/how GHC can actually do that.
That's excellent news. It's just what's needed. I hope that you can make sure to add it to the Haskell Wiki books section when it's ready. I don't want to miss it.
Hope you enjoy it:) This is our first real experience with Haskell as a language for web development. Appreciate any comments about code and etc. 
That's very neat.
The real problem is it's just not very user-friendly in a lot of ways. You should be prepared to put in a lot of work to support the packages you need and figure out how to make them work. (It *is* user-friendly in other ways though, I can have transactions and a declarative configuration, etc.) I've spent a lot of time getting my NixOS environment up-to parity with my old server (I guarantee you don't appreciate how much software people have packaged for you - as a data point, I maintain almost 40 packages now, including some core ones, and I've used NixOS for like ~3 months). **This is a fundamental aspect of the difference between Cabal/Nix**, in some ways. They have very different sets of goals and priorities. Nix has a robust dependency management solution, but it's a very different from what Cabal does, and Cabal is robust in other ways. The problem is Nix doesn't actually do 'dependency resolution' - you as the author of Nix packages do. What I mean by that is when you package software and say it depends on 'foo', it will by default implicitly depend on the latest version of foo (for the most part, bear with me) - so if I make a point release of `foo`, then its dependants will see that upgrade as well. But that's not dependency resolution! What if `foo` depends on `bar` and I make a major upgrade of `bar`, and that breaks `foo`? What if there's not a new version of `foo`? Well, the Nix solution is to add two versions of `bar`, manually - the old version and the new major version. `foo` can then be (again) manually changed to use the old version, then you can upgrade `foo` later and it can use the new one. The key is you can have two versions of `foo` (and they might even be the *same* version with different settings!) This isn't constraint solving though, it just makes distro maintainers solve the constraints for you. Which is OK - a system like Nix has much different design/user constraints than Cabal (it *is* a package manager! Cabal is not a package manager!), and for maintainers to do this isn't unusual anyway. You need to carefully review upstream dependencies as a maintainer and coordinate dealing with them - it's just part of the job. Nix is pretty kick-ass because it makes doing all this safe, I can safely say that as I maintain 40+ packages. But it's tedious, manual stuff. On the other hand, Cabal will automatically select the proper package set given the right constraints. If I say `bar` depends on `foo &gt;= 2.1 &amp;&amp; &lt; 2.5`, I don't *as a dependant of that package* need to necessarily intervene when `foo` may get upgraded. Providing they follow the PVP basically, if you upgrade `foo-2.5.1` to `foo-2.5.2`, then `bar` will implicitly allow it and it will be included in the install set once the maintainer uploads it and users see it - the key is `bar` needs to do nothing for this to work. If `bar` also depend on `baz` and it has `foo &gt;= 2.2 &amp;&amp; &lt; 2.3`, then the constraints can be narrowed to allow `foo-2.2.1` - and in the future, if there is a `foo-2.2.2` (say a security update), it will still allow this upgrade seamlessly. Again, the key difference is the *solver* is doing the work of selecting the right set of packages automatically! (This highlights why the PVP is so important, by the way - if everyone follows it and puts upper/lower bounds, then you get a fully transitive guarantee that every package can deal cleanly with PVP-compliant upgrades within those bounds, and old packages with old compilers will not retroactively break - because the old software can still solve those constraints and satisfy them. Consider this for security upgrades - the fact I can always be sure a minor PVP-compliant point release fixing a security upgrade will never break anything - even for code I have not upgraded or touched otherwise (like production systems) - is extremely useful as a user of Cabal. *By definition* such PVP-compliant upgrades can be included in the install set seamlessly with no explicit work for any person depending it.) At the end of the day, Nix and Cabal have two very different end goals. One is a development tool for Haskell software. The other is a full blown package management system with users and a Linux distro. Maintaining a Haskell package is *way* different than maintaining a Nix package, and comes with a lot of different constraints (how punny) and responsibilities most importantly. We don't want Nix for Cabal - we want the *ideas* of Nix from Cabal - the most important thing here that Nix has is that it allows *two versions of `foo` to be installed, even if they may be the same version*. That is because Nix tracks dependencies more precisely and encodes this into the package in a unique way. Right now, Cabal will destructively update packages if you install the same version twice. This has the nasty side effect that reinstalls will break things as we all know. Today, cabal is at least smart enough to tell you that will happen. Tomorrow we want it to handle it seamlessly by allowing such oddities.
 foo (Baz a b) = ... How would one know in this case that Baz is a pattern synonym and NOT a datatype? Or maybe it does not really matter? 
Yes, users have to deal with linear types explicitly. It's impossible in general to determine if a pointer aliases another. There are some other type systems that would make it possible to have very fine-grained alias analysis (e.g region types) but they're not possible to completely hide from the user AFAI understand.
Thanks for that in-depth explanation. Nix sounds like it has a similar approach to Debian with regard to compatibility: the group of maintainers as a whole take responsibility for packages working together. Cabal has a more unilateral approach which is made workable via the PVP. 
The most vicious kind of exceptions are asyncronous exceptions, since then it's not even the code you call that throws the exception. However, `ThreadKilled` can't be anything else and resources should be cleaned up when the thread ends. (For example `System.Timeout.timeout` throws `ThreadKilled` to the thread, if the timeout expires.) Pressing Control-C in a console program throws `UserInterrupt` and not being able to stop a program would be very unfriendly. I don't think exceptions can be removed. However, they shouldn't be used when not necessary. *Edit:* typo
I'm not sure about the gory details myself.
That's right. Maintaining a set of distribution packages is also difficult because in-the-wild software is ridiculously hostile and stupid, it's true. It's just part of the job to work and scrutinize such things closely, and doing it manually. We even patch Haskell software and GHC sometimes, etc. Note that whether you maintain a Nix package or a Haskell package, you *still* have to do some work and coordination to make sure everything is kosher and will install right. You are still a maintainer. The PVP doesn't magically free you of all work or interaction with others - but what it *does* do is it strictly codifies the guidelines for dependencies and reduces the amount of human intervention necessary to make sure the tools work. IOWs, we move the harder problems *out* of the human hands, and at least give rules to the human process.
I thought I had used it with `-O` before, but I could be remembering wrong.
If you have any function of type `Foo -&gt; Secret Bar` (e.g. provided in the same API), you need `Monad` to use it. You're right, though, that if no such functions exist except artificially, you don't get any more out of `Monad` than you would out of `Applicative`.
And honestly I find Cabal's solver is *much* more capable than that of most other package constraint solvers. It also has one other very crucial, important difference - it *tells you* when things will break horribly and you will overwrite existing packages with reinstalls, should that occur. Yes, it's a shortcoming of the tool it can't deal with this scenario (and it occurs for various reasons). But it at least stops you from shooting yourself in the face (every other one has this same problem and just lets you do it), and in the large majority of cases it still does the right thing given the information it has.
I guess you mean not a *data constructor*. I'm not sure to what extent it might matter, especially if you can ~~match on it~~ construct with it!
So can one use pattern synonyms to create new values and pass them around? let x = Baz a b y = foo x 
At least for my problem, that solution took the running time from effectively infinity (for a list-based probability monad) to 'instant'. That said you could probably get similar results with just list monad + a properly written 'optimize' that was effectively `Map.toList . Map.fromList` (except that `fromList` would need to accumulate identical entries instead of choosing one)
That will help me scrap some half baked code that was in production to do the same. Thanks !
GHC does not do this in any kind of generality. It might, as you mentioned, unbox the record's components to never allocate it in the first place. If you're updating one field of a record in a loop, inlining may expose something of the form myfield (r { myfield = newvalue }) which can be simplified to `newvalue`, which likewise avoids constructing an intermediate record. The full optimization you're talking about can't be expressed in Core, so it would have to take place at a lower level of the pipeline -- probably Cmm. You'd need a liveness and an escape analysis, and the extra mutations might require minor RTS changes. I don't know much about the optimization that goes on below the Core level, so I can't comment on how difficult the analyses or the transformation itself would be to implement.
I wrote almost the same thing a few years ago as a single file CGI: [Lambda Viewer](http://www.projectultimatum.org/cgi-bin/lambda). It also does a pictorial rendering using dot. I put [the source code here](http://codepad.org/OxSlVOOP), and you may find it interesting to compare to yours. 
[There's a bug](http://bro-app.herokuapp.com/reduce/?expr=%28\f.+%28\x.+f+%28x+x%29%29+%28\x.+f+%28x+x%29%29%29). There should be parentheses surrounding the `x x`s, but they are not displayed. Beta reduction is applied correctly. Edit: The link doesn't work. I guess that's a problem too. The expression is `(\f. (\x. f (x x)) (\x. f (x x)))`, a typical fixpoint combinator. 
I think row polymorphism comes at the problem from the other side. You're saying that named types of records have specific fields (i.e. Cat has just "name"), and you use a typeclass to extract fields from records. The row polymorphism technique says that records themselves are first class objects which have arbitrary fields, and particular types are constrained to only hold records which have the correct fields. For example: data Cat = Cat { name :: String } data Bug = Bug { species :: String } someRecord :: { name :: String, species :: String } -- unnamed record type someRecord = { name = "Bob", species = "Ant" } aCat :: Cat aCat = Cat someRecord -- aCat = Cat { name = "Bob" | ... } aBug :: Bug aBug = Bug someRecord -- aBug = Bug { species = "Ant" | ... } Here we can construct cats and bugs from *the same record* because those records meet the constraints of the Cat &amp; Bug constructors.
&gt; Thus, GHC probably can and does, under some conditions, split a record into a mutable array or something no less efficient. Why would updating a tree in the State monad be as efficient as a mutable array?
Actually another thought just occurred to me: if you are using Fay and only rendering text, wouldn't it be reasonable to build everything on the client side? 
Imagine that learning languages is like learning to drive something. Haskell is like a motorcycle, Java is like a van, and python is like a car. It's probably no more difficult to learn how to drive a van than to learn how to drive a motorcycle, especially from scratch. However, having spent your entire life driving vans, you're going to find driving a car much easier than learning to driving a motorcycle. 
As I said in my comment, cabal is not in cause here. It is a great tool. We use the "cabal hell" terminology simply because some cabal-install command doesn't work. But this is a good thing, because cabal work only with unstable packages. A great amelioration for production ready usage would be to have a default set of packages which are considered stable and cabal would only use them. For more advanced people, you would have to use a "cabal-install --unstable" or "cabal-install --experimental" command. Typically, there would be a stable set of package for each stable version of GHC. I imagine we could do this by changing how we submit package on hackage. For example, a VM should be dedicated to try to install each packages in some preference order (may be by popularity). If some package fail to install, it is not stable otherwise it is added to the stable set of package.
...what about http://hackage.haskell.org/package/runghc ?
I would disagree. Haskell certainly does have the benefit. A large part of the barrier is also unlearning all the damage OO did to programmer's minds, in particular the fact that they are used to using mutable state everywhere is hard to unlearn but also on of the core reasons why OO programs are so hard to reason about.
Another bug (different one, I think? it's fully parenthesized) is shown by expanding their third example. Click the outermost expression twice to get ((λy.(λz.zy))((λx.xxx)(λx.xxx)))(λy.xxx) ((λz.z((λx.xxx)(λx.xxx)))(λy.xxx)) ((λz.z((λx.xxx)(λx.xxx))(λx.xxx))(λy.xxx)) First step looks okay to me, but I think the second step should result in ((λy.xxx)((λx.xxx)(λx.xxx))) ...or could be my mental parser that's broken. =)
Isn't it great that you can [verify the result here](http://www.projectultimatum.org/cgi-bin/lambda?term=%28%5Cy.%5Cz.z+y%29+%28%28%5Cx.x+x+x%29+%28%5Cx.x+x+x%29%29+%28%5Cy.x+x+x%29)? :=)
Presumably one would look in the context for a binding of Baz, just like GHC does.
Then yes, haskell is not superior to other languages. It is superior once you learn it, but it has a higher barrier to entry. And it is going to stay that way, because lowering the barrier of entry will inherently cause other parts of it to get worse as well, which is a silly tradeoff. Barrier of entry is a good metric to help you become popular, but a really shitty one if you want to write good software.
I'm really glad to have read this as I wasn't aware of everything pattern synonyms could do! The as-binding examples has now sort of freaked me out. That's definitely something to look out for when reading code. I'm also looking forward to expensive pattern synonyms driving home the point that anticipating performance by looking at the code is impossible. We will need haskell-mode or ghc-mod to provide a command to reveal the core produced for a region of Haskell.
I don't know if that logic makes sense. I'm sure it would compel library authors to stop using them but for unmantained libraries and in the interim it seems like it would just crash everybody's code, wouldn't it?
You might be interested in the Ask-Elle programming tutor.
You aren't the only one ;)
I can assure you Java got big in enterprise long before mobile Java was a thing.
 pattern December day = Date { month :: 1, day = day } Looks like a typo (section: More complicated dates).
I agree with you. It's just that usually "much easier" isn't an argument that sticks.
Wow, very cool. Thanks for the taking time to write this out!
I see. Thanks!
Yay! This is awesome. How is the performance compared to freenet (especially in the area of concurrency, where freenet does especially bad)?
&gt; I'd love to have a bundled implementation of FMS (Freenet Message System, a spam-free, pseudonymous forum system built on top of Freenet) Spam-free? How does that work?
I'm glad to hear that, there has been a great deal of focus on the type system but there are other important directions to focus on. The classic example is the pattern synonym that allows you to match on the first and last element of some structure: headLast (a :&lt; _ :&gt; b) = (a, b) I wanted to expand the possibilities a bit with this tutorial.
Yes that is a typo :) thanks 
Yi is still pretty bad, but we need more people working on it.
I'm using STM for almost everything, and it's pretty sweet. It feels much snappier than Freenet. When visiting a Freesite with the original client there is this "download in progress" page before the site finally shows up, you know? The one with the progress bar, which goes to ~120 percent and then parts of the site show up. I don't need this, the page is just there shortly after you click it. I'd like to improve concurrency some more, though. Currently I'm stuck on this one: http://stackoverflow.com/questions/23395590/concurrent-file-reads-writes-in-haskell 
I'm learning Haskell right now. Is there a document that mentions all these right things that should be learned?
I just told you that I am seeing this from a productivity standpoint. I am trying to actually do something with this language because it is a tool for solving my problems. Let's go on with you car analogy. I am trying to be productive with the vehicles. My task is to buy a 60" LCD TV and take it home. So I take your van or car and can load it behind the seats and I don't see any further problems. You keep telling me how great the motorcycle is and the problem you can solve are all equivalent to the problems I can solve with other kinds of vehicles. You also keep telling me that you already transported a 60" LCD TV home without any problems, but you don't tell me how and no one cares about telling me how. I don't see how to do this and since I don't know it and no one gives me reasonable hints about it, my opinion is that I postpone learning how to transport my LCD TV until someone finally appears who has mercy with me and explains it to me.
Why is this not a link post?
The "Documentation" link on the web page links to the Hackage page, which only documents the API. From what I've seen the documentation is lacking. I think I looked at this library myself (I'm a beginner BTW) a while ago and gave up because I found no documentation. Finding the API documentation has rekindled my interest however.
Sounds like you're doing the monolithic-prealloc that freenet does :(
Is this implementation protocol-compatible with Freenet, or just a similar idea.
The data formats are compatible, but the protocol is not.
Being a re-implementation, is it able to communicate with nodes running the original Freenet software? If not, is that a goal? (This is not something I'm actually looking for, just curious).
Both problems are solved with the extension in question.
I explain this in the readme on GitHub, in short: It does have interop with Freenet nodes, but it's a bit tendious to set up.
&gt;it's kind of hard to make a Windows package on a Linux machine I don't know about haskell/ghc but I know of one project who could only get their windows package to compile a linux box, mingw is a lot easier to setup and automate on a linux box as well.
Hm, I tried this but mine seems off: http://i.imgur.com/qauu9GQ.png
You've list all credibility here because what you've posted has nothing to do with reality. 
Do elaborate 
Did you try not using it? I'd think that disk speed would not be a large issue compared to network speed with a decent structure. 
The docs are either lacking or they suck. You're just wasting people's time.
You're right that the only way that this or any game framework will take off is if there are pioneers making it happen. That being said, why Helm? I have not been convinced one way or another that Helm is the right way to do things. I also have no notion of what the developer's goals are with the project, and without a good amount of examples or documentation, I have no incentive to learn about Helm over any of the other myriad of "functional" game platforms out there. Not to mention that I've *tried* to install it and run it and it still doesn't build (mostly due to errors that I'm not interested in tracking down when I hit 'cabal install --only-dependencies')
At the risk of turning this into a support thread... I retried to install everything and told myself to go through with it. There are a few things you need to do to get it running... 1. Install the not-on-hackage SDL2 bindings located [here](https://github.com/Lemmih/hsSDL2) (and make sure you have SDL2 installed) 2. cabal install gtk2hs-buildtools Then, when I ran the simple helm-demo-platformer, it did something super funky to my screen that caused me to reboot my machine. Not totally sure what was up here, but it may be due to the super experimental SDL2 bindings.
GHC uses [some bignum library](http://www.mega-nerd.com/erikd/Blog/CodeHacking/Haskell/integer_pt1.html), usually [gmp](https://gmplib.org/). I usually use [`Int`](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#t:Int), which doesn't have unlimited size, instead of [`Integer`](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#t:Integer). There are also smaller sizes such as [`Int8`](http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-Int.html#t:Int8) and [`Int16`](http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-Int.html#t:Int16), and C-compatible types like [`CInt`](http://hackage.haskell.org/package/base-4.7.0.0/docs/Foreign-C-Types.html#t:CInt) and [`CUint`](http://hackage.haskell.org/package/base-4.7.0.0/docs/Foreign-C-Types.html#t:CUInt).
What now? Easy: Implement gnunet in idris. With full correctness proof, of course, against formalised tinfoil-grade threat models.
I have experience with this. I created Ready Lisp for the Common Lisp community and recently started building Try Haskell using the same idea. We should collaborate!
Make sure you aren't using the HaskellLig font. Perhaps you could try adjusting the magic 8xxx numbers. I am using Emacs on Ubuntu, you may need something else on Aquamacs.
I've definitely done that with C.
&gt;That's the usual explanation, but I'm skeptical that even a web framework like Yesod could pull in enough dependencies to justify 50-60 meg binaries. A typical install of Windows 95 took 52.6MB; can anyone plausibly claim that a simple Yesod app is as complicated as Windows 95? I think the problem is that since the vast majority of executables are built with dynamic linking that we have just become used to small file sizes. I just looked at my Hearthstone install on windows. The exe is ~10MB and it comes bundled with ~25MB of DLLs. Then after that who knows how many MB of system DLLs it pulls in to run as well. Also if you think about it, on Windows just to be able to run C++ applications you need to have installed ~100MB of the C++ runtime.
Thanks! I figure using "Int" is probably a good idea for size-related reasons.
Shot you an email :)
Such a shame that they didn't get more purchases to drive development. I bought a the first episode when it came out, but I few others did :(
Fantastic! This is something I wish I could have done (I had a few false starts quite a while ago, starting from the protocol side). If I ever find myself with some free time (not any time soon, unfortunately) I might contribute!
Installed codex-0.0.1.7 and now it seems to work fine :) 
&gt; [You may need to jump a few hoops to install the Cairo bindings](https://github.com/switchface/helm#installing-and-building) No kidding! Helm looks interesting, but installation difficulties are going to be a major hurdle for adoption. I still haven't managed to install it, but on OS X at least, I can already pinpoint several mistakes in the installation instructions: &gt; Mac OS X &gt; [...] &gt; $ brew install SDL &gt; [...] &gt; Helm is now installed, but there is currently a mischevious issue with SDL preventing you from using Helm to make games without a C wrapper. Checkout [issue 7](https://github.com/switchface/helm/issues/7). According to that very issue, I should install SDL2 instead of SDL. &gt; $ brew install cairo --without-x For some reason brew then runs `./configure [...] --with-x`, ignoring the `--without-x` argument. I'm currently installing cairo from source with `--without-x`, to see if it helps. *edit*: it does. Also, brew installs cairo's dependencies in "keg only" mode, meaning the pkg-config files and everything else are in non-standard locations, leading `cabal install cairo` to complain that it cannot find `cairo-pdf` and others. The solution I found is to add a large number of `/usr/local/Cellar/*/*/lib/pkgconfig` folders to my `PKG_CONFIG_PATH`. **edit**: I think I'm done with the cairo hoops... and now I am discovering that the pango hoops are even worse. :(
You can think of `Integer` as the following definition. (simplified of course) data Integer = Small !Int | Big !ByteArray As long as you're within the range of values covered by Int, that's what it's represented by. There's some runtime checking to determine if an operation will cross over this boundary and needs to switch to the larger representation, but otherwise the performance is not *that* much worse. If you just need to track the size of a list or block of memory, Int is your friend. If you need to process big numbers (mathematics/statistics) then before grabbing for Float or Double, use Integer. (p.s. this is [the real definition](http://git.haskell.org/packages/integer-gmp.git/blob/HEAD:/GHC/Integer/Type.lhs#l108))
It's always been possible to make pattern matching expensive, due to lazy evaluation.
How about using some form of `mmap`? It seems like there are several `mmap` packages on hackage — if they don't work, you could create your own using the FFI. That way you can have multiple threads write to separate specific locations in the file without without having to worry about file-global position.
Thanks. Works on Debian. Was going to add some `unset GHC_PACKAGE_PATH` stuff, except now I can stop fussing with paths at all! EDIT: Do you ever find yourself needing to install multiple packages together? If so you could use `$(echo "$@" | tr -s ' ' | tr ' ' '-')` as the sandbox dir.
I also found more success with Cairo with `brew install cairo --universal`. Additionally, `brew link cairo` (you may need `--force`, as well) will link things in the expected locations.
Not that I know of, no. It's something you get to know with experience. And I'm sure people have differing opinions about it too. As a rule of thumb: learn efficient data structures (Text instead of String), and learn general functions (mapM from Traversable, not Prelude).
Is this just a bump? The guide section still seems to be "wip" with regard to forms, and v0.6 still isn't on hackage*. That said, I do, in general, like the API. I have a half-finished game sitting on my hard drive that uses this (with branches that use other libraries such as free-game and gloss). This has been my favorite so far, though I hope that at some point cairo (and by extension gtk) isn't required. I'm also hoping that the new SDL2 bindings aren't a pain to install on windows... `* I expect that this is waiting on the "other" SDL2 bindings (hsSDL2 instead of sdl2) bindings to hit hackage.`
&gt; (p.s. this is the real definition) ...why not simply link to the nicely rendered [Haddock docs on Hackage](http://hackage.haskell.org/package/integer-gmp-0.5.1.0/docs/GHC-Integer-GMP-Internals.html) I spent time improving for GHC 7.8? :-) PS: If the docs are not clear enough, patches are welcome!
Yes, they use block sizes not supported by AES in some places. edit: I added a readme explaining why the cbits are needed: https://github.com/waldheinz/ads/tree/master/cbits
No, ORF doesn't support anonymous record types - `someRecord` will not be accepted. It solves the basic problem (reusing record field names) but doesn't allow more general extensible records.
I think he is referring to a section with things like components overview + code samples/ commented examples.
And the advent of SSDs.
Lazy evilaughter.
apologies, my google-foo is apparently not good enough to find this page.
My personal rule of thumb: always use `Integer` unless you really need `Int`. Integer is fast enough for most common use cases and doesn't overflow, which means one class of bug less to be concerned about. 
Not so much the parser, which is defined using `parsec`, though you could obviously write a more ad-hoc parser for the client-side, or interface with something like [jison](http://zaach.github.io/jison/).
Very cool. Congrats on your first Haskell web app!
Haskell itself is terrible when it comes to most things. Haskell is a very "bare bones" language. The power of Haskell comes from its libraries. You can't disregard them.
This is great news. I bought the full version of the game and think it deserved more recognition than it got. Plus it's written in Haskell!
I'm not sure what you mean by extensible records, but it definitely allows row polymorphism. Two types with the same field will be accepted by `r { name :: String } -&gt; Bool`, regardless of what other fields they have, whether it's zero or a million, which is what row polymorphism does.
I get your point, but doing so also comes with a price: I have no words for what Freenet does at it's protocol / messaging layer: I've been there, I don't want to go there any more. Here are some random thoughts: * I only wanted the "good" parts of Freenet, and I don't think the protocol is something to like * it is really just an inefficient version of TCP on top of UDP, so why not use TCP? * a request having the ability to run Freenet over Tor comes up every few weeks, and as it stands this means TCP. I can't judge if it makes *sense* to run Freenet / ADS over Tor, but people want to do this. * because of the existing [interop](https://github.com/waldheinz/ads#interop-with-freenet), data exchange with Freenet users is effortless for anyone wanting to give ADS a try. Sure, it would be great if I wasn't the *only* person having a companion setup running, but having this ugliness only in some isolated places seems preferable over doing it everywhere * there was a GSoC project which aimed at giving Freenet a plugin architecture for using different transports. To my knowledge, this was never merged. But maybe this is a better starting point to simplify and distribute the Freenet &lt;-&gt; ADS data exchange.
The reason `Integer` typically hurts performance is that it cannot be unboxed, adding indirection and GC pressure (in addition to the additional branch to check for overflow).
*ceased
I won't defend their protocol choices, but there's not reason you can't use UDP protocols over Tor, it's just less efficient.
It's very nice app:) I really like the idea of rendering terms as graphs
Ooh, that interop looks pretty nifty.
That is clever ;)
We decided to not parenthesize an application that is not a redex, but your example shows that it was a bad idea. Thanks!
Yes, that's clearly a bug. Thank you for report;)
The problem is that other people may not know that you should nuke ~/.ghc. This is just one of the ten thousand little things that you have to know and have experience with in order to be productive. This is by no means exclusive to Haskell, the problem is much worse in the C/C++ world. I have to fill my brain with thousands of trivia that have nothing to do with what I'm trying to accomplish. I want to program, not sysadmin. This is one of the reasons for the popularity of languages like Ruby. I install Ruby, which works flawlessly. I type "gem install &lt;packages&gt;" and 99% of the time it just works even if the package needs to compile C code or whatever. Documentation is good, with examples that get you up and running quickly. Everything *around* the programming experience is so much better that I'm more than willing to put up with Ruby.
Dynamic / SomeException works fine so long as your rep can handle infinite sums. Something like Data.Profunctor.Choice, however, only gives you binary sums, and building to infinite sums by splitting the space in half over and over can take a long time. Ultimately tricks like class Profunctor p =&gt; Walk p where walk :: Traversable p =&gt; p a b -&gt; p (f a) (f b) Are abusing representability behind the scenes. That may be fine, but it is worth noting at least. 
Have you tried Typescript? It looks a bit too much like Javascript, but it is typesafe and easily integrated with most libraries out there (including AngularJS) ;)
My experience with Elm is that it's more of a replacement of Angular than a Haskell to Javascript compiler. It's got an API for input, output, communicating with the server, but its Javascript integration is more black-box than code translation.
&gt; looks a bit too much like Javascript I'd like to avoid this if possible...I'm unfortunately bothered by silly things like syntactic ugliness. :)
None. You can always ffi of course, there's no problem with that. But angular expects some OO monstrosity and I haven't seen any libraries abstracting it. And this has little to do with the compiler. 
TypeScript is really nice. This is the kind of project helping microsoft's reputation. It fits nicely in the js world, solves a lot of problems linked to the language, and mostly introduce new syntax from future standard. It is also really easy to debug and has ready-made interface declaration for most js librairies. I find ts is the best existing solution for a compile-to-js framework because of this power-to-weight ratio.
'Fraid I don't have anything very helpful to contribute. I have a small question: why define (.*.) and (.\^.) using guards instead of patterns? You have: a .*. b | Infinity==a || Infinity==b = Infinity | otherwise = Scalar $ (real a) + (real b) Not that it matters but I'd have expected: Infinity .*. _ = Infinity _ .*. Infinity = Infinity (Real a) .*. (Real b) = Scalar $ a + b Anyway, good luck with this :-).
`return` is an exceptionally poor name for that function. This seems to really confuse people. They should have called it `unit`, or now that we have applicatives, `pure`. It doesn't return anything. It just puts a value into a monad.
Your definition of Scalar is... are you really sure it's what you want? What would be the problem with `Scalar a = Scalar a | Infinity` and putting a `Real` constraint in the `a` where you want it? If you don't actually need the parametricity, it's better, and you won't need the rankN types. Also, I wouldn't use record syntax on a sum type if I were you: pattern matching is better.
Is this not similar to `containers` works?
The Hash Map data structure described (from reading the slides) is the same data structure used in unordered-containers.
At the very least Typescript demonstrates nicely one of the big challenges of using a typed language to integrate with existing Javascript libraries—you have to write a typed adapter layer! I spent some time reading through the React&lt;-&gt;Typescript library and it's sort of a mess. It's not obvious to me that all of these libraries can have much of a nice typing formalization.
Purescript is actually not like the others. It is in the CoffeScript league. It is a syntax sugar that generates js code (like CoffeScript). While others try to port ghc runtime into javascript which makes it quite hard to operate with js world and requires some sort of FFI. EDIT: I'm confusing PureScript with [LiveScript](http://livescript.net/).
Ah that's interesting, thanks.
&gt; you have to write a typed adapter layer! This! Promises/A style promises are a major pain to type properly due to implicit flattening. It makes it near impossible to write polymorphic async code in a typed JS language like TypeScript. I'm looking eagerly toward generators to solve this problem.
I wasted fairly a bit time on this thing, it's faster than ghc-mod, but doesn't work well with sandbox, and doesn't have windows implementation. I switched back to ghc mod .
IMO, this is a weakness of the compiler struct { bool tag; union { long small; char[] *big; } payload; } (which is roughly the C equivalent of the Haskell definition) can be easily inlined. A very clever compiler would even avoid wasting space for the variable `tag` by combining it with the tag for the enclosing structure. This optimization even resolve the only major complexity related to inlining sum types: handling GC information.
&gt; the list seems endless This list is not endless. It is quite well documented, including some categorization, over here: http://www.haskell.org/haskellwiki/The_JavaScript_Problem &gt; I wonder how doable it is with one of these Haskell-flavored ones. I think these are not very well suited for integration with Angular, as their approache to stuff like interactivity/ajax/hooking-into-html are rather different.
The particular implementation used for Elm's Array library is called [Relaxed Radix Balanced Trees](http://infoscience.epfl.ch/record/169879/files/RMTrees.pdf%E2%80%8E), which is pretty new and a bit fancier than what Zach described in his talk. As I understand, the benefit there is that appending arrays is logarithmic. I'd be surprised if Haskell was doing anything much different than this though, even if it does not used the relaxed-radix idea. I am also no expert on data structures, so maybe there's more cool stuff that can be done. I'd be very curious to learn more! :)
Not Angular, but you might be interested in this PureScript project that I've had on the back burner for a while: https://github.com/purescript-contrib/scrap-your-markup/ https://github.com/paf31/lambdaconf/tree/master/reactive It's a little DSL for building reactive UIs. See the `tests/Example3.purs` file for a working todo list app.
Are you sure you aren't thinking of LiveScript? [PureScript](https://github.com/purescript/purescript) claims to be "A small *strongly, statically* typed language which compiles to Javascript"
Well it's not new that the Dutch lose out to the British. But seriously, what do you think is so terribly worthy in uniqueness typing? I think it's just a clumsy version of the State monad. The implicit threading of state like put 5 modify (+1) get is better than threading it manually.
Well, there has been no traffic on the mailing list other than calls for proposals for conferences at all this year.
I had never even heard of Clean before reading [Awkward Squad](http://research.microsoft.com/en-us/um/people/simonpj/papers/marktoberdorf/mark.pdf) yesterday where SPJ briefly mentions that Clean's IO system is less hacky than GHC's because it ensure single-threadedness with its type system rather than a compiler implementation detail
Uniqueness is capable of saying things you can't nicely say with just state. You can, for instance, talk about quantum computations where observing something 'uses it up' so you can enforce the use of unitary operators throughout with simple uniqueness. In Haskell the encoding of linearity/uniqueness for that is vastly heavier.
Looks like you are right. There are so many of js generators nowadays. 
The Renderable typeclass should probably be a record.
Just install from source. [This branch](https://github.com/schell/hdevtools/tree/cabal-support-ghc7.8) even has support for cabal sandboxes and ghc 7.8. I've used it for a few weeks now and have not had any major issues. Sometimes some weird error creeps up and I have to restart the hdevtools server, but thats done in less time than ghc-mod needs to tell me the type of something. Just clone it, create a cabal sandbox and install it with a custom build location thats in your path.
Go on, and do you mean the Render typeclass?
When I saw a presentation about mozilla's Rust language, I remember being surprised by the advanced types it is using. One of the types was very similar to uniqueness types (or my understanding of it), I think it was called "owned pointers".
Same :)
I think Peaker is referring to [the existential typeclass antipattern](http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/).
The use of the typeclass seems unnecessary.
He's actually wrong about that (if you ask me). Using uniqueness types for IO is very hacky, because token passing doesn't make sense as a model for IO. It gets the job done, but it is semantically unprincipled. Hiding it behind an abstract type is actually better, because you could imagine that the implementation is something that does make sense. Uniqueness types are better for array usage, I'd say. But IO in general is not a good use case.
There's only one function, so that record would be equivalent to `IO ()` -- I think they're using the typeclass here to support polymorphic actions, which can make application code much nicer to write.
+1 PureScript
I'm curious: what's your background? Most of the languages I use (Ruby, Python, Haskell) have unbounded integers built-in.
Yes - talk to me offline
I think someone should write, at least where there is a consensus. Anyway, thanks for the two tips. Regarding the Prelude, I think it would be great if it didn't have so many functions imported from the other modules by default. There would be a cleaner separation.
Yeah, those render instances just smack of "abuse of monad".
Argh I accidentally deleted my comment. Let me try to recover it: First, the goal seems to be monoidal composition of effects (a.k.a. `EndoKleisli`). The monoidal restriction could be relaxed if Kleisli composition was used. (Upon further review, what is wanted seems to be a way to combine `(a -&gt; IO ()) -&gt; (b -&gt; IO ()) -&gt; c -&gt; IO ())`, which Kleisli composition would not provide. However, `(&gt;&gt;)` or `(*&gt;)` is a perfectly suitable monoidal product for values of type `IO ()`) Second, rendering does not depend on the results of previous computation and therefore can be done in an Applicative context instead of a Monad context. Generally, using `(&gt;&gt;)` but never `(&gt;&gt;=)` (or their do-notation equivalents) is a smell for this. Concretely, instance Render Game where render (Game b p1 p2 s) = do render b render p1 render p2 render s can be replaced with instance Render Game where render (Game b p1 p2 s) = render b *&gt; render p1 *&gt; render p2 *&gt; render s or equivalently, instance Render Game where render (Game b p1 p2 s) = sequenceA_ [render b, render p1, render p2, render s] where sequenceA_ = foldr (*&gt;) (pure ()) -- available from Data.Foldable which is just the obvious monoid for applicative sequencing.
The worst problem isn't that the Prelude re-exports functions. The worst part is that some of the functions that exist in the Prelude, such as mapM :: Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b] have the same name as their more general counterparts, in this case mapM :: (Traversable t, Monad m) =&gt; (a -&gt; m b) -&gt; t a -&gt; m (t b) A Traversable is something you can loop through, such as a list, and the later version works for all Traversables, while the one in the Prelude works only for lists. I don't mind that there are two different functions. It irritates me that they have the same name.
Ah, okay. I didn't about this issue and I agree that this name shadowing is terrible. Can't the Prelude be cleaned up at some point?
Ok, that's another thing that's good to know.
If you claim the token represents a region of memory and not "the world" then it may be a bit more principled :-)
There is no interpretation of the token that makes programs like: forever $ putStrLn "hello" principled. Unless that program doesn't actually print anything (but then you're ruling out programs that you might want).
What decisions are you looking to make about the project?
I do not see existentials used in this blog post. 
I prefer explicit records to type classes when it's not a pain. Type classes aren't meant to group methods together, they're meant to allow overloading without lots of dictionary passing. There's no need for actual overloading here, it'd be just as easy or even easier with records and more flexible.
In fact typescript has some form of type inference. Of course in some cases it falls back to dynamic typing.
&gt; Hell, why not implement Tor and I2P as well? Couldn't hurt. Because Tor and I2P work in a fundamentally different way. Freenet is a distributed store, and tor and i2p are anonymization systems for normal traffic.
I agree that it would be more flexible with records. For this example, I don't how records would be simpler, but I'm not exactly sure what you are envisioning. Could you give an example?
I'm surprised no one has mentioned the GHC LLVM backend. With LLVM IR created by GHC, you can convert to many other languages including JavaScript through [emcscripten](https://github.com/kripken/emscripten/wiki). Although I'm not sure about the quality of the translation of rendering-based code, emcscripten has been used to translate both [Unity games](http://beta.unity3d.com/jonas/AngryBots/) and [Unreal Engine](https://www.youtube.com/watch?v=BV32Cs_CMqo). I haven't seen anyone use emcscripten with GHC, but I would think it would work.
PureScript is a lot nicer than TypeScript with similarly predictable and compact output. I would strongly encourage you to check out PureScript before any other alternatives. PureScript is *not* just a syntax, it's an impressive programming language that has many of Haskell's strengths. - has used: Typescript, Fay, Haste, etc. Also, there's work happening on PureScript + ReactJS bindings, so you might hear something about that in the next month.
I think I read somewhere it allows for 'safe' destructive/in-place updates of data, which seems interesting...
Here is said monoid for applicative sequencing: http://hackage.haskell.org/package/reducers-3.10.2/docs/Data-Semigroup-Apply.html
Well you can interpret it fine if you don't worry about purity, I think. Instead you have a language with graph reduction semantics and effects, and linear tokens are just a tool to help programmers tame the resultant confusion.
Thanks! And that's a great point! That is actually a huge deal, but I was having a hard time making it sound exciting as it is important :)
It's' under src/Example3.purs in the second link for anyone else wanting to find it: https://github.com/paf31/lambdaconf/blob/master/reactive/src/Example3.purs
He means this: data Render = Render { render :: IO () } renderGame :: Game -&gt; Render renderGame (Game ball p1 p2) = Render $ do render $ renderBall ball render $ renderPlayer p1 render $ renderPlayer p2 renderBall :: Ball -&gt; Render renderBall = ... renderPlayer :: Player -&gt; Render renderPlayer = ...
Just me being curious and noobish... Seems that could be further reduced to: (Real a) .*. (Real b) = Scalar $ a + b _ .*. _ = Infinity Did I miss something?
the simplest solution that mostly works is the one i wrote up 6 months ago https://gist.github.com/cartazio/7131371 :) (note well, there are a few rare libs where my directions don't work and you'll see some weird error about objective see header files, the work around for those rare libs is add --ghc-options="-pgma clang -pgmc clang" to the cabal install invocation. Otherwise, my direcitons work like a dream and are used by industrial grade haskell mac users planet wide :) ) edit: just to be clear, my directions are also linked to from the haskell platform mac webpage www.haskell.org/platform/mac.html :) 
Does [dependency freezing](http://blog.johantibell.com/2014/04/announcing-cabal-120.html) introduced in cabal 1.20 help with the "stable/unstable/experimental hackage" problem? It seems at least to be a more accurate, though a la carte version of what you want. Sure you'd have to manually check if your packages work with newer versions, but you'd always have a specific set of dependencies that *always* work.
&gt; Python * [Kivy's python-for-android](https://github.com/kivy/python-for-android). * [SL4A's python-for-android](https://code.google.com/p/python-for-android/) [PyQt](http://www.riverbankcomputing.com/software/pyqt/intro), [Pyside](http://qt-project.org/wiki/PySide)(though it seems to have stalled after Qt 4.8), and [PyOtherside](http://thp.io/2011/pyotherside/) have also been deployed to mobile platforms. I think a stronger argument might(?) be that "a lot of popular **functional** programming languages..."
I used that branch, doesn't work with cabal test dependencies, the author is not very responsive, many pull requests didn't merged, painful to use.
So you're saying that to print things out, I use IO? But that wouldn't confuse anyone!
Right, its really just a way to enforce ordering in a non-strict context, not about purity at all. And linearity checks just make sure you really _are_ enforcing ordering. Its an interesting point in the design space I think, but yes one conceptually very far from purity.
Hmm. I'm going to require some explanation here...
Oleg has a version http://okmij.org/ftp/tagless-final/course/index.html#linear And so does Jeff Polakow http://www.haskell.org/pipermail/haskell-cafe/2013-March/107215.html But they basically proceed by tracking an environment explicitly. It is messy.
I just want to say that I think I stated my point most clearly in [this particular email](https://groups.google.com/forum/#!msg/elm-discuss/1acyOfxvasA/LHxZamXkP7UJ), somewhat deeper in the thread. The whole thread is about precision and pedagogy, and I feel like the title I gave that thread in the context of /r/haskell misrepresents what I was trying to get at. It's about using the word "monad" in a precise and helpful way. P.S. /u/alexander_b, I tend to think of the mailing list as a somewhat private place to test out and discuss ideas. I think it's great to share this general idea on /r/haskell where it is quite relevant, but if my name is going on it like that, I'd prefer to present my ideas on my own terms.
&gt;I feel like the title I gave that thread in the context of /r/haskell misrepresents what I was trying to get at. Indeed. When I saw the title I thought "Oh no, it's that debate about `Monoid` all over again", but then I opened the link and saw it was actually about Elm.
you could just as easily say "IO type". that it's a monad is useful and important for generalizations, but that's an observation you make about the type, not a thing imposed on it. we don't say "to collect values, you use the List monad", we just say you use lists. why not say something similar for IO, and avoid confusing people unnecessarily by bringing up funky, tangential properties?
That's what I'm trying to get at! I've been thinking about this a lot recently, so hopefully I can write it all out in a clear and well-thought out way at some point :)
Thanks! `brew link --overwrite pango` was the solution to my problems. After that, `cabal install pango` worked flawlessly. I can't say the same of helm, however. The version on hackage fails because of the above "mischevious issue with SDL preventing you from using Helm to make games without a C wrapper", while the github version fails because it is looking for an "SDL2" package which it didn't list in its cabal dependencies.
Thanks, those SDL2 bindings were the last piece of the puzzle for me! To provide another datapoint: I am running this on OS X 10.6, and the color demo did *not* crash on me. The platformer demo didn't crash either, but it didn't accept any input, and since it was fullscreen and grabbed control of the keyboard, it was hard to return to the OS. *edit1*: trying the examples which come with SDL2, it looks like they don't support keyboard input nor quitting either. *edit2*: if I install the hackage version instead (and install SDL, and revert the github code to tag 0.5.0 in order to find the os-x version of the colors demo), I can run a version of the colors demo which does quit (with a segfault) when I close the window, and hangs after a few seconds otherwise. I can't say I'm very impressed with helm so far.
Debate about monoid? What's there to debate?
Weird, I am using Ubuntu Mono and also am using Emacs under Ubuntu.
Back in 2009, a blog post led to a lot of ink being spilled over whether `Monoid` should have been given a friendlier name such as `Appendable`. You can find the rather large Haskell-Cafe thread about it [here](http://www.haskell.org/pipermail/haskell-cafe/2009-January/053539.html). 
The point is well taken, but I think highlighting algebraic properties is not devoid of significance. Use `IO` to print things, oh, and you'll probably want to use all these other functions like `mapM` that are defined in terms of some properties that `IO` happens to have.
&gt; Well, the Nix solution is to add two versions of bar, manually - the old version and the new major version. foo can then be (again) manually changed to use the old version, then you can upgrade foo later and it can use the new one. The key is you can have two versions of foo (and they might even be the same version with different settings!) That's... ugly. I think that Gentoo mostly got things right in "how to be uber flexible without having two packages of the same software", but Nix got things right in "how to make upgrade as reliable as a fresh install". I want both. After decades there's still not a single solution in this problem space. I don't even know if we are closer this time. But, &gt; We don't want Nix for Cabal - we want the ideas of Nix from Cabal - the most important thing here that Nix has is that it allows two versions of foo to be installed, even if they may be the same version. Hey! Npm does *this* right. Node.js packages uses [semver](http://semver.org/) to track whether it's backwards compatible or not. Npm solves the constraints in an optimal way: if A needs B 1.2.x and C needs B 1.3.x then you get both installed, no need for manually making b-1.2 and b-1.3 packages (like some distros do). I think that package managers like nix or dpkg need to integrate directly with "language ecosystem" managers like cabal, npm or rubygems. My issue is that sometimes a rubygem may depend on a native package - and you need to install this manually before installing the gem (with a different command for different OSes). The usual solution is to repackage the rubygem package (or cabal, etc) into native packages, and people make scripts etc to make this easier. But this still need to be done manually, so in practice the gems your OS provides aren't up to date, and developers still prefer to handle with the real thing directly. But there's nothing preventing the OS to call rubygem (or cabal, etc) itself, and do this automatically for every rubygem package - no need for having a separate package maintainer for Debian, one for Red Hat, etc. It should just work - there shouldn't be a need for a third party maintainer to babysit a package that was already created and shipped (either through hackage, or the rubygem site, etc). I compare this situation with init scripts - the common approach is to make a slightly different init script for each distro and there was weird quirks because the software itself didn't ship with its init management (and if it did, it would be ignored). Systemd kind of standardized this. Well when talking about those issues it's hard to ignore that all software sucks.
Not quite - you would want `(Scalar a) .*. (Scalar b)`. But, in the greater sense, yes, you are right, that would be more minimal.
You are probably right. I am not a Haskell expert. I didn't know if `Real` had a way of expressing infinities, so I used the type annotation. [Different tasks call for different conventions.](http://imgs.xkcd.com/comics/donald_knuth.png)
Guards were easier to read, and produced less code.
Maybe the reason we talk about IO *only* as a monad is because we primarily interact with it using its monadic structure (and maybe sometimes as a functor or applicative). Almost every other type has some intrinsic interface which is useful by itself, but IO doesn't. You don't have access to IO's constructor, and IO values by themselves do nothing really. IO values are just slugs, that don't have any interesting properties by themselves. In fact, the only reason we can do anything useful with IO values at all is because of the magic in the compiler exposed through IO's monad instance. It allows you to pretend the IO action isn't just a useless slug, and actually holds a value. Then, you tell the compiler something along the lines of "If, hypothetically, this weren't just a dumb slug and magically I was able to get a value out, here's what I'd do with it". Then the compiler says "And thennnn..." and you say "Oh yeah, and then I jam it right back into some other IO value". That's getting a little rambly, but I guess the point is that the special magic sauce of IO is the special cased implementation of its monad instance. That's what makes IO types useful at all (plus the main function being special cased to kick off evaluation). Using IO values without using the monad instance is very limited usefulness. So, it makes sense that the two are always talked about together.
As someone who uses numeric-prelude (and thus, write constraints in terms of, e.g. (Algebra.Additive.C a)), I have to disagree with the premise that being specific leads to confusion. It seems like focusing on "the additive group" and other such algebraic properties wouldn't have lead to the yucky numeric hierarchy in the default prelude. Why should I have have to implement multiplication in, for example, a vector space? Likewise, focusing on monads I think leads to cleaner abstractions.
I think that this book is better than Real World Haskell, but the author fails to clearly separate teaching Haskell from building an application (at least for part 1 of the book, maybe it'll pick up in a later part). The book begins by stating that *we'll be building a web store to sell time machines*, and 4 chapters in and I'm still not sure if that's actually happening. All the code is organized into modules by chapter and chapter section rather than functionality. So import statements look like **import Chapter2.MoreModules.range**. Also, the code which is used to demonstrate Haskells language features and the code used to build the store is mixed together adding further confusion. If you're going to read this book then you'll have to work hard to figure out how the application fits together yourself because little guidance is provided by the author. I don't think that this will be the canonical Haskell book.
If it's OK in java to say "To input things from the user, use java.util.Scanner *class*", is it OK in Haskell to say "To input things from the user, use IO *monad*"?
The link refers to exactly the same post. Did you mean [this one](https://groups.google.com/d/msg/elm-discuss/1acyOfxvasA/LHxZamXkP7UJ)? 
That's really nice.
This could clear up a lot of confusion. If a new Haskell user hears "You need to use the `IO` monad" their next thought is "What on earth is a monad?". If they hear "You need to use the `IO` type" then they'll just think "OK show me how to do that". Looking forward to reading your blog post.
Yes.
Using "monad" would be more similar to the saying ""To input things from the user, use java.util.Scanner iterator". But even that's not a particularly good correspondance because java.util.Scanner is one way of getting input and IO represents the entire concept of input-output.
C++, Java? I would say that native fixed size integers are more of a default than unbounded ones :) 
From a pedagogical point of view, yes, agree. Calling monadic types monads just because they happen to be, well, monads, kind of makes them appear magical and special, and it takes quite some brain twisting to realize that monads are really just another abstraction that happens to come with a bit of syntax sugar built into Haskell. IO is a monad, but that doesn't answer the question "how does pure code do I/O"; the real answer is "it doesn't, it merely *describes* I/O actions". We could make IO non-monadic without sacrificing its workings, and retrofit it into a monad once we realize the isomorphism.
It seems the above comes across as snarky, even though the intended effect was pretty much the opposite. Oh well.
This sounds awesome, I was looking for something like that. I'll try and let you know if I find problems.
I can only suggest everybody visiting Oleg's [lecture notes](http://okmij.org/ftp/tagless-final/course/lecture.pdf) on the initial-final tagless correspondence. They are highly inspiring and a pleasure to read!
Using "the java.util.Scanner class" is more like using "the IO type", while, as /u/tomejaguar said, using "the IO monad" is more like using "the java.util.Scanner iterator" or similar, which highlights its properties, not only what it is.
You might try cabal-cargs[1] for the configuration of hdevtools. It's kind of a shameless plug, because I've written it, but I'm also using it with projects having multiple sections (library, executable, test-suite), and it works quite well for me. [1] https://github.com/dan-t/cabal-cargs
&gt; That's what makes IO types useful at all (plus the main function being special cased to kick off evaluation). I disagree with the stuff outside the parentheses. With a reasonable IO monad (I'm not sure that still true with the great runtime additions; is Haskell's `IO` really a monad?), you could perfectly implement the language by defining IO as a perfectly pure AST describing the operations to be performed, and then write a small interpreter in an impure language that gets this value from `main` and actually performs the execution (it would need to run the Haskell runtime). The fact that IO may be special-cased for efficiency reasons and what not is not in fact relevant to the way users should think about IO, I think.
I liked your reply -- although I found the original point convincing enough to actually consider using it for Haskell as well.
An existential of the form data AnyRender = forall r. Render r =&gt; AnyRender r is the main missing piece to the author's "how do I treat these heterogeneous types homogeneously?" question. While the author chose another approach by basically punting on the original problem and only using the typeclass to share the `render` identifier, with explicit sequencing of IO actions rather than folding/traversing a list, the design has all the other smells of the existential typeclass antipattern. Whether these smells are enough to justify moving away from the typeclass approach is another question. I'm not sure they do. The resulting type is just (equivalent to) `IO ()` after all, which is not very interesting.
Or just use Haste that has no problems with parsec 
The approach presented in this article seems to be too narrow. A wide class of games (applications) has more than two 'objects', and the count of objects can be unpredictable on start of the development. Neither a single aggregating type (like Game in the article) nor an existential type is considered a good solution. So the question is: what is a good one? It's always beneficial to think in types in Haskell, but much better is reasoning about domain in terms of it's nature. In this case you need a good idea, what your domain exactly is, but not an idea how does it work. Reasoning this way gives you a freedom to work out a composable code based on properties and laws of the domain. Like an author of 'Thinking in Types', I encountered this problem in my game 'The Amoeba World' and found some solutions of it. Even more: using this experience, I started a cycle of [articles](https://docs.google.com/document/d/1Gypzxp13aMpzD7o5XAmgy5zRrchQKrEbJArm6hDmwqk/edit?usp=sharing) (careful: Russian!) wich desribes a hi-level design and arhitecture in Functional Programming. It seems, we have a lack of such materials. I hope, I'll translate it someday to English.
I wonder if the OP "idletoad" is the same toad that worked on the Freenet codebase all the time.
&gt; user That seems a bit harsh ;-)
&gt; "how to be uber flexible without having two packages of the same software" You say Gentoo is better, but... &gt; if A needs B 1.2.x and C needs B 1.3.x then you get both installed, no need for manually making b-1.2 and b-1.3 packages (like some distros do). Cabal can already do that, this isn't unusual. In fact everything can. I didn't explain myself well enough. The trick is what if `A` and `C` depend *on the same version of B but in different ways*? What if they both depend exactly on B 1.2 but they require it to be built different ways? What if A needs B built with an extra feature but C does not care, and I already have B installed? What happens if I *overwrite* B to satisfy A? Just the exact same version of B with this extra feature? Does C break? The question is 'it depends', but in reality just about every package manager *on earth* has this problem - including Node. The problem is the *destructive upgrade of existing dependencies*, or, in other words, that your package manager isn't purely functional. :) So the real core of the question is, given A and C depend on the same version of B - what happens if B changes, and what does that imply for A and C? No package manager other than Nix has a graceful way of dealing with this AFAIK, because this change in the dependency is captured precisely by Nix's cryptographic hash. This is the ultimate sort of identifier of the exact dependencies (including that extra feature) - relying on anything else other than an exact specification of the dependencies leads to pain. In Nix, If I installed 'B-1.2', it would have an hash identifying it. Now, let's say I reinstall 'B-1.2' with an extra feature on zlib or something - now *the dependency hash has changed*, because the inputs to B itself have changed! Thus this difference manifests at the package level as a hash discrepancy, and the two packages are not the 'same' even though they have the same name, same version. This isn't a hypothetical situation and in practice occurs all the time for us. The real way this manifests is the deadly *cannot reinstall without breaking* errors. What happens here is that dependant packages essentially force a recompilation of upstream packages in a way that changes the ABI. But doing that 'breaks' upstream packages, again, because their ABIs have change too as a result. Why does this occur? The reason this is so much more difficult for us is because we actually have an ABI we compile against, with cross module optimisation - the last bit is what makes the difference. Interpreted languages don't care about things down the dependency chain possibly forcing *upwards* recompilation like we do. Furthermore, most languages do *not* care about cross module optimisation. C has a stable ABI, so an 'upgrade' of a package in-place can be considered possible in a lot of instances. Node/Ruby/Perl obviously it doesn't matter. For GHC it makes a very big difference - recompiling B may force recompilation of A and C, simply because their dependent interface files may change. &gt; But there's nothing preventing the OS to call rubygem (or cabal, etc) itself, and do this automatically for every rubygem package - no need for having a separate package maintainer for Debian, one for Red Hat, etc. It should just work - there shouldn't be a need for a third party maintainer to babysit a package that was already created and shipped (either through hackage, or the rubygem site, etc). The problem is every language - and every Linux distro - has vastly different policies on how to manage dependencies and what exactly packages like this mean to them. On top of that, every language has a different kind of dependency resolution that must be dealt with specifically. Semver vs what Hackage does, already for example, are two different approaches. And Haskell has a lot of complex machinery to detect when we *do* need to recompile things - and it works! So we'd have to invent all this anyway, the question is how any Distro maintainer chooses to leverage that.
Well you could perfectly define the monad instance on that AST (binds corresponds to substituting `Return` constructors with subtrees). But thanks for your answer, I think it helps me understand what habitue meant. Namely, the main point being &gt; Using IO values without using the monad instance is very limited usefulness We do agree on that. Instead I got stuck on &gt; the special magic sauce of IO is the special cased implementation of its monad instance Which I disagree with because you could have a perfectly reasonable IO implementation with a traditional (pure) data-structure where bind is defined by substitution, and not a "special case [of monad]".
 But with laziness, you can define infinity as a member of `Nat`, rather than `Maybe Nat`.
While that is correct, it comes from an example where I am doing this at the type level. GHC will prevent you from constructing infinite types :-( I did not yet add the suggestion to lift pattern synonyms to the type level :-)
I must use a different IO than you do. I mostly interact with it via functions that produce IO actions. IO would be pretty darn useless if the only interface to it was the monadic one.
But you lost something when you encode it that way. With the current set up, only when using the view pattern is Infinity indistinguishable from a Nat, but the internals might take shortcuts by inspecting the actual maybe. For example, the show instance might be a special case, since it wouldn't terminate otherwise.
&gt; `... | pred@(Just (Succ a) -&gt; Just a)` Is this right? It looks syntactically like a view pattern but obviously `Just (Succ a)` isn't intended to be a view function. I get what you are trying to do but I don't know how to write it. Anyways wouldn't it be easy enough to define an auxiliary function `maybePred :: UNat -&gt; Maybe UNat` and write pattern S pred &lt;- (maybePred -&gt; Just pred) I think you are basically asking for or-patterns (e.g. [#3919](https://ghc.haskell.org/trac/ghc/ticket/3919)) in pattern synonyms. Any reason not to ask for or-patterns generally, as an orthogonal feature to pattern synonyms?
Good points! Because of "-&gt;" being taken in the pattern language already, I'll switch the arrow around: ... | pred@(Just a &lt;- Just (Succ a)) But wait, these look like *pattern guards*. Fortunately pattern guards are not allowed **inside** patterns IIRC. I'll check. Regarding or-patterns, yes that would be interesting to have but they are probably ugly without the pretty abstraction that pattern synonyms offer. EDIT: Looks like this could work: * patsyns do not allow "|" in 7.8 * patsyns do not allow @-patterns in 7.8 * @-patterns in Haskell do not allow "&lt;-" (which I propose as the replacement syntax) * also "|" is not allowed in patterns (this might pave the way to or-patterns in GHC)
&gt;I can't tell the direction your irony points to. However, reading it made the pedagogic argument against do-notation strike me as reasonable for the first time ever since I began learning Haskell. Um, thank you? :-) And you're very right.
OK, but can you make it clear on the wiki that this `Just a &lt;- Just (Succ a)` is a second bit of newly-invented pattern syntax, so that future readers won't be baffled like I was?
Looks good, thanks !
Um, if we are going to start talking about abstract pattern matching, should we go all the way and talk about prisms and maybe affine traversals (though if we throw in affine traversals, we ought to talk about lenses as well).
Yeah, I never got Helm working either, for the same reason.
This is a great feature. And I'll use it. I would nevertheless find interesting to have a big list of compatible packages, à la stackage. FPcomplete provide such sets actually, but with fewer packages I would like. But this is already a good enough system to me. But, my reflexion is more about absolute beginners that want to pass from learning to making something useful for the first time. 
The implementation of the IO monad you are talking about is called the "Free monad interpretation" of the IO monad, where the actual implementation in GHC is closer to the "State monad interpretation" of the IO monad.
You can, and people do create mirrors of IO that the compiler doesn't treat specially. see [Control.Monad.Prompt](http://hackage.haskell.org/package/MonadPrompt-1.0.0.2/docs/Control-Monad-Prompt.html). But the point is that yes, Haskell builds that pure AST, but then it isn't useful unless it gets executed with side-effects where the side-effect slugs were in the tree. And if you think about it, a normal compiler for any other language also has a pure AST in its memory describing the effects the program wants to create, and then it injects correct trap codes etc in the output. Haskell isn't really remarkable in that respect, except that it has lazy evaluation so it needs to jump through some hoops to ensure effects happen in the right order.
And thus, &gt;&gt; is isomorphic to ;
No worries, perhaps it's good that this got out the door because I was fairly hesitant to actually write up a blog post on this before :) It may be that the list should evolve and I should be okay with that, so I was more surprised than anything else. So I guess, thanks for posting this then :)
Yes. And if you want to print multiple things, you can use the `Applicative` instance for `IO`: putStr "hello, " *&gt; putStrLn "world!" No monads required! You can, of course, sequence things using the `Category` and `Arrow` instances as well -- though those particular instances do rely on the fact that `IO` has a `Monad` instance. Given all the type classes `IO` belongs to -- focusing on just one seems not quite right... Really there is an `IO` type, and then a bunch of useful classes: - `Functor` -- for modifying the pure value that results from executing the IO action - `Applicative` -- for sequencing `IO` actions - `Monad` -- for sequencing `IO` actions where you need to use intermediate values - etc In related news, I am not really that happy with my explanation here either. There is still some sloppy terminology going on.
So someone with no understanding of category theory I wish that monad's had a better name. Yeah, they are explicitly clear to category theorist's but to everyone else it's jibberish. If I had a dollar for every time someone tried to describe a monad to me from first principles while I was learning it I'd have a lot of money. Simply stating the mathematical definition is very different from just giving a practical explanation. I think that it would have helped everyone who isn't into category theory if there would have been some less-precise but somewhat explicit name even if the name missed some subtle cases at least it would have given normal programmers a head start. Of course, now that I know this stuff I don't care as much but it certainly would have saved me a lot of time. TL;DR If your goal is wide-spread adoption of Haskell we really should have easier to understand terms for normal people at the cost of precision.
Yeah, knowing what words not to use does not tell us which ones we *should* use! We're [experimenting](https://groups.google.com/forum/#!topic/elm-discuss/j9Da2UIA5xo) with different terminology for Elm, but I'm not totally certain yet. Also, I'm generally a fan of having the full functionality in the library with explicit names. So people don't have to know anything general to get started. I think at least a couple Haskell FRP libraries were hard for me to pick up because I couldn't find a `lift` function. "Oh, it has an Applicative instance, so that absolutely essential function is in some other library." This way you really can be specific about your library without stopping anyone from being general if they want.
As the arguments in this thread show, perhaps it is not that implausible. After all, at the very least there seems to be consensus that it is not a good idea to immediately talk about monads when presenting IO to newbies. When it comes to Haskell, I think the main issue is that monads as a pattern are common enough, and appear transparently enough, that it doesn't take long before beginners meet them face-to-face in the wild, and therefore not using the term "monad" to refer to things like `State` would be of limited help.
&gt;If I had a dollar for every time someone tried to describe a monad to me from first principles while I was learning it I'd have a lot of money. That is a legitimate problem; however, perhaps the solution is not changing the name, but avoiding describing them to beginners from first principles. 
&gt; If I had a dollar for every time someone tried to describe a monad to me from first principles while I was learning it I'd have a lot of money. But how would a different name help with that? The name has nothing to do with how hard it is to understand; only with how scary and foreign-sounding it comes off as.
I think this could be significant. Monads are regarded as a Big Deal, in no small part because IO is an instance of it. If you stop emphasizing that IO is a monad - in contexts where it isn't very relevant - then maybe you could dampen the whole monad-fear that is so prevalent for beginners and on-the-fencers (people that are curious about Haskell but think it is too theoretical and jargon-y). I mean, people get by fine with learning what the map function does without knowing that list is a Functor, or that Maybe is a Functor, or that function composition is a Functor ...
I just found it interesting. As for evolving, I just referenced your Elm thesis in my master thesis, so your name is next to the greats. I guess you should get accustomed to being a PLT "celebrity", whatever that entails. ;-)
The actual argument of the author, deeper down in the thread, is more fine-grained than that. He does not claim that you should *hide* the fact that say `Reader` is a monad, but rather than, terminologically-speaking, saying "the `Reader` monad" is bad because it suggests it is *only* a monad, and you should rather say "`Reader`", or "the `Reader` type", and then mention that it is a monad (in the documentation, or when you start talking about its usage).
Unidirectional pattern synonyms are more like affine folds than affine traversals, aren't they? (Incidentally, maybe it's my bias towards prisms, but the unidirectional pattern synonyms are not really to my taste. `f X = Y` really looks like it should match just a single value...)
&gt; How would a different name help with that? Names can provide context for what something represents. I know it's horrible to think about it like that for mathematicians but it does help. You wouldn't write variables in imperative languages a, b, c you'd give them some context that you'd expect someone to reasonably understand. Monad, monoid, functor are not terms normal programmers will understand -- there is no context associated to them for people who haven't studied a lot of math and are just starting with Haskell. Could you imagine how horrible medicine would be for doctors first learning if medical terms had no context. Instead of derm use wackspurt, cardial is now ooopop, etc... Its hard to discern what meaning those could have. It's reasonable for someone to understand basic Latin/Greek roots that give context to terms -- why should programming languages be exempt?
&gt;Names can provide context for what something represents. I know it's horrible to think about it like that for mathematicians but it does help. The difficulty is that it is also desirable to have names which do not lead to flawed intuitions, which are harmful in the long run - that is, when one discovers that integers are "appendable", or that functions are "containers". &gt;Could you imagine how horrible medicine would be for doctors first learning if medical terms had no context. Instead of derm use wackspurt, cardial is now ooopop, etc... Its hard to discern what meaning those could have. It's reasonable for someone to understand basic Latin/Greek roots that give context to terms -- why should programming languages be exempt? There are lots of Latin and Greek roots only students of ancient languages are aware of. If we know, or can guess, what cardial means it is because doctors use the term, and not the other way around. 
I have written widgets in Purescript and Elm that run inside of angular directives. Using the linker stuff in angular you can tie basically any javascript control structure. My goal is to slowly replace angular with purescript at the back end and then elm for animation and contained interactivity. Though, if someone wrote a really nice frp module for purescript that would be amazing. 
That is an interesting perspective. Even though it is hard to police language, specially in the case of easy metonyms which aren't technically incorrect, attempting to speak in this fashion might be an interesting experiment.
Why not put this on the sidebar?
I'm not sure whether evangelising is a particularly good idea on that thread. Given the top comment it seems most people are a bit fed up of Haskell being evangelised. 
and haskellers are tired of people who dont know haskell spreading misconceptions about haskell!
The top post is a bit over the top but they're are some good but over exaggerated points: * String vs Text * laziness and IO * Haskell being difficult to learn (you can argue why this is - different to the norm, no examples in documentation, scary etc) * cabal problems If you evangelise too heavily you end up with posts like that one where people are fed up with it and just rant. I kind of agree with them on the last bit: &gt; all of this would be moot if someone built something outrageously awesome in haskell that demands our attention We should show people how fantastic Haskell is rather than just talking about purity and that sort of thing. Let's write blog posts [like this](http://engineering.imvu.com/2014/03/24/what-its-like-to-use-haskell/) where we explain how Haskell helped us achieve something really whilst acknowledging its downsides. Lets write useful software like git-annex and pandoc that a lot of people don't even know were written in some strange FP language. Lets show people how some aspect of Haskell stopped a bug going into production that Java wouldn't have caught. I think that is a better way to educate people about Haskell than getting into an argument on reddit :)
&gt; While the author chose another approach by basically punting on the original problem and only using the typeclass to share the render identifier I think this is unfair characterization of the author's solution. The author explained that true heterogenous list if overkill for this problem. A simple solution will suffix, assuming the problem is as simple as the author described it. &gt; with explicit sequencing of IO actions rather than folding/traversing a list Are saying this is bad? If so, this seems like argument against all do notation that does not use bind. Although folding is equivalent, I would not say it is easier to read. &gt; the design has all the other smells of the existential typeclass antipattern. I don't know what you mean here. What are the smells that he shares specifically? &gt; The resulting type is just (equivalent to) IO () after all, which is not very interesting. Of course, but what interface should you present clients of the code. Being able to just call render on a type seems like a simple solution to a simple problem. 
In the general case of making a drawing library, drawing command monoid, e.g. diagrams seems like the best solution. For something as simple as the author is presenting, that seems like overkill. 
see /u/augustss answer and the diagrams package http://projects.haskell.org/diagrams/
Case in point: the striking regularity of SO questions along the lines of "why isn't Haskell doing automatic memoization for me?"
I don't think it's overkill. A simple type synonym DrawCmd = IO () would suffice. 
I like to think I'm a 'Haskeller' and I'm not terribly bothered about what other people think of Haskell.
Lawvere's book is highly recommended then!
the best thing about this post are the references included at the end. i've actually come across this post prior to it being posted here, and i found it useful for the links it handed out.
I wish I could explain why homotopy type theory and especially univalence are important to CS. It's really frustrating. The whole thing started by a gaggle of mathematicians-mathematicians, to be specific homotopy theorists, suddenly seeing the light and becoming constructivists, and they're bringing a metric fuck-ton of strange vocabulary and attached concepts with them. Worst of all: I can't even quickly discount all that stuff for being formalist and thus inherently speculative. [This is just far beyond the usual greek](http://www.cse.chalmers.se/~coquand/mod1.pdf).
Further: &gt;To be honest, I don’t like set theory. It’s artificial – the axioms aren’t obviously true, but rather the product of a search for a paradox-free foundation for mathematics. I'll assume he's discussing ZFC with the axiom of choice (versus alternative axiomatic set theories). While it is true that there isn't a known way to show that ZFC is consistent, that doesn't discount the theory. The writer may not be comfortable about the axiom schema of specification - it isn't the easiest to grasp and I think it was formed to avoid Russell's paradox. But just because the axiom's implications may be difficult to grasp does not make it less of an axiom (though one is free to reject an axiom as well). &gt;So, immediately, category theory has an advantage over set theory in that it’s a less artificial construction, given that it stems directly from work in algebraic topology. I don't know what this means. Set theory is constructed entirely from axioms that are mostly acceptable (some mathematicians reject the axiom of choice - but category theory relies on the axiom of choice). There is a very large body of work of set theory from Cantor through the 1970's. It is a field that has matured so much that mathematics work has moved elsewhere. I don't know much category theory (or Haskell for that matter, I'm learning) but I did find this post's Applications of Category Theory section quite interesting.
I think the general criticism is that ZFC, with or without AC, describes a relatively arbitrary structure, the set. Most mathematicians have a really fantastic grasp of the set and use it constantly, but it's still a fairly complex object to try to rest all of mathematics atop. So constructivists and category theoreticians rebuilt them from simpler feeling axioms. In doing so they natively built a much richer world where ZFC is just a single, fruitful player. It's all philosophy to argue whether ZFC is bad, but there is a fundamental "phase distinction" between ZFC and CT in that the former tends to try to lay out the properties of a thing and then supposes that they're useful while the latter describes more a perspective on meaning in mathematics and then boots everything else up out of it. The basic idea is that math is about studying structure, that structure implies the existence of a rich net of homomorphisms (see "the celebrated" Yoneda lemma) and that foundations of mathematics can arise from noting the importance of this network of homomorphism instead of trying to describe one particular, nice structure directly.
MOOC didn't include much essential math courses, so i think there must be some intrinsic difficulties in it.
I'll be giving a talk at lamdajam/chicago on this topic! (Not that the half hour format will let me get far). Don't be scared off by things like what you linked though -- the computational models stuff is very tricky active research. its perfectly possible to understand the interesting things about HoTT without ever learning what a cubical set (or even a simplicial set) is. (and the cubical sets stuff is in fact only one approach [that may not end up being "the" approach] to how to get a computational, constructive model of HoTT)
This McLarty paper is a fun read on related topics: http://www.cwru.edu/artsci/phil/NumbersCanBeJustWhattheyHaveTo.pdf
shout: https://github.com/xpika/live-source
I've been reading the linked Mac Lane historical as well—http://www.cwru.edu/artsci/phil/BJPSMacLane.pdf
Yeah. It's kind of dumb. But blog posts like this are more for the author to write them than for any audience to read them. 
???
Funny you say this. It was Haskell that made me *enjoy* these properties. That is, before Haskell, my experience with static typing was with those languages that didn't have a sane type inference implementation. As for functional programming, my limited experience with such languages included the parenthesis-heavy s-expresion languages like lisp, and (what I think is) ugly syntax of erlang. Elixir convinced me that functional programming could look nice, but I didn't really like the whole ruby-style do/end blocks. In short, Haskell was the first language to make me appreciate both. I find it difficult to write Python (beautiful, but dynamic) now, because of it.
&gt; `listToChain :: forall (t::Nat) (t1::Nat) b. [(b, Direction)] -&gt; Chain t t1 b` Your type signature is saying that for _all_ naturals `t` and `t1`, `listToChain` can produce a value for those nats. That is, the _callers_ get to choose what they want the return type to be. That doesn't really match my intuition for your problem, since ultimately what decides `t` and `t1` is the combination of `Direction` values in your input. Is that correct? If so, you want an existential or a higher-rank continuation. Something like data SomeChain b = forall t t1. SomeChain (Chain t t1 b)` listToSomeChain :: [(b, Direction)] -&gt; SomeChain b or listToChain :: [(b, Direction)] -&gt; (forall t t1. Chain t t1 -&gt; r) -&gt; r I'll leave actually implementing them up to you unless you encounter trouble with them :)
I hardly know anything about anything, but I do believe that I've heard that category theory does *not* rely on the axiom of choice.
I'm guessing the [staticlibs](https://wiki.archlinux.org/index.php/PKGBUILD#options) option for makepkg would be cleaner. You can also set this in `/etc/makepkg.conf` by removing the "!" from staticlibs in OPTIONS. I can't find the reference at the moment, but I think the default until like a year ago was to build static libs. *update:* Not the blog post I was originally looking for, but the [arch mailing list](https://mailman.archlinux.org/pipermail/arch-dev-public/2013-May/025006.html) seems to indicate this was changed back in May of last year (2013).
Conal Elliot has been working on [compiling Haskell to hardware](http://conal.net/blog/posts/haskell-to-hardware-via-cccs). I don't know what the status on that is, though.
Python is my language of choice for pretty much everything, and I also have experience with Java and Matlab.
6 days later and no response. All the more evidence that this guy is a troll. Plus his karma is -400 or so.
That's the thing; I /am/ a newbie. I have yet to actually use a monad in Haskell. 
Sorry. I was making a post while I was inputting my PIN Number on the ATM Machine.
Perhaps I didn't express myself that well :) Many people, when they hear of a new concept (especially one with a scary name), will become a bit apprehensive, and often put too much emphasis on that concept since it's foreign. It's the classic case with the IO monad - people hear "monad", get scared, and then put all their focus on "what the hell is a monad", even though that's not particularly important when learning to do IO in Haskell. But you use `numeric-prelude`, so you are probably used to seeing new/foreign concepts, but also knowing not to over-focus on them when trying to understand something new. It's just a difference in the way people approach new concepts and knowledge, and your use of `numeric-prelude` likely puts you more towards one end of the scale than most people. EDIT: I'm also interested in you saying you've yet to actually use a monad. What do you mean by this, exactly? Because if you've written a program that takes user input and performs calculations on it, you've used the monadic interface to `IO` (not strictly necessary if you aren't taking input). Or do you mean something else?
John Darlington's team at Imperial College London built special purpose computers based for graph reduction within the ALICE project running from the late 70s to the mid-80s. A related project was the functional language NPL which I think introduced set-comprehension notation. There was quite a lot of activity at the time designing and building language-biased computers and architectures - famously the Fifth Generation initiative in Japan for Prolog - but commodity chips and OSes are now so successful (and the engineering to build them so huge) that there is no room so such research any more.
you can sequence actions just fine with only the Applicative instance. 
https://en.wikipedia.org/wiki/Lisp_machine
Duplicate of [a 5 minutes earlier post](http://www.reddit.com/r/haskell/comments/24oplw/ezyang_the_cost_of_weak_pointers_and_finalizers/)
Nice work. There seems to be more discussion on this over at HN. https://news.ycombinator.com/item?id=7692831
No documentation. I cannot use it, I cannot contribute, I cannot even judge whether it's what I want. This is arguably fine for a hacky WIP branch, but a no-go for a library release.
Existential can be created by CPSing universals and this way we don't have to introduce more keywords.
The readme on [github project page](https://github.com/bitemyapp/bloodhound/) is pretty good, actually. Better than many other projects on Hackage. The Hackage docs seem to be largely missing though.
I wish that it'd be possible to re-use Github's README.md for a package's top-level page on Hackage. IMO .cabal/haddock format is horrible for writing any lengthier pieces of documentation. I think this explains the tendency of many projects having better examples on their GH readme than in Hackage docs.
My general understanding is that it's a can of worms and isn't worth the massive complication it brings with it.
Here's a quick DSL I made. It's a bit opaque; I didn't really think about how it works, but just played with the types until it did what I wanted: now a f = f . ($ a) later = (.) apply = ($ id) Your example using this DSL (assuming your 5-argument function is called `foo` and the two parameters you want to partially apply it to are called `b` and `e`: apply (later . now b . later . later . now e) foo I don't think I would use such a thing in practice, but it was a fun exercise. Edit: Of course, this doesn't really answer your question, since I wouldn't consider this idiomatic. To answer your question directly, I would just do what dave4420 suggested.
&gt; Not that the half hour format will let me get far Why not? Forget about the actual maths, do a pie in the sky talk! Just pretend you'd be at a TED conference. Once people's appetite has been wetted, they'll slurp up the rest on their own. E.g. what does univalence mean for aggressive program optimisation and abstraction capabilities?
As /u/acow pointed out - it does matter. A pattern synonym can look just like a constructor but actually be hiding an expensive calculation.
good to hear!
The other option (getting away from existentials), is to constrain the return type so that listToChain can fail. For example, lets say you knew that the input list had exactly one up and one down somewhere in it. -- sing = Singleton; a runtime-inspectable witness of the given Nat data SingNat :: Nat -&gt; * where SingZ :: SingNat Zero SingS :: SingNat a -&gt; SingNat (Succ a) listToChain :: SingNat up -&gt; SingNat down -&gt; [(a, Direction)] -&gt; Maybe (Chain up down a) listToChain SingZ SingZ [] = return Nil listToChain up down ((x, Center) : xs) = Add x &lt;$&gt; listToChain up down xs listToChain (SingS up) down ((x, Up) : xs) = AddUp x &lt;$&gt; listToChain up down xs listToChain up (SingS down) ((x, Down) : xs) = AddDn x &lt;$&gt; listToChain up down xs listToChain (SingS up) (SingS down) ((x, UpDown) : xs) = AddUD x &lt;$&gt; listToChain up down xs listToChain _ _ _ = Nothing type TOne = Succ Zero singOne :: SingNat TOne singOne = SingS SingZ test :: Maybe (Chain TOne TOne Int) test = listToChain singOne singOne [(1, Center), (2, Up), (3, Center), (4,Down), (5,Center)] -- test = Just (Add 1 $ AddUp 2 $ Add 3 $ AddDown 4 $ Add 5 $ Nil) 
&gt; Lots of questions either get ignored totally or are given one line non-answers now. This is certainly the case right now. I know lots of times my basic questions(mostly about Cabal) are ignored or lost while other people discussing something else. I don't know what changes but I'm also not happy with this. This was not like that when I first started(about 1,5 years ago).
What's wrong with StackOverflow, again? It's made for questions and answers. I'm not saying IRC is a bad or old technology, but it's pretty obvious that when a new optimized option comes up, old ones are slowly replaced.
to be honest I don't hang around IRC much anymore (exactly to that reason) - there are the mailinglists of course but I would go to SO - at least you can expect some real answers there. But I am interessted in a "beginner/medicore" haskell room as well and I would try to participate from time to time. For me it's hard to find connecting points after the LYAH stuff in my ascend to haskell enlightment - and I would really love to meet some like-minded people and discuss this stuff.
I hope it is just a coincidence. In particular, I think a large part of it may just be the loss of signal in the noise. A lot of us do try to answer questions when we're around, but it can be hard to see one question in the scrollback amidst 1300 other people talking. That said, shapr used to rule the channel with a "be nice or else" ban-hammer that was a large part of how the "old culture" was enforced, but we've been less proactive in instilling that ethic in the last few years.
I think it depends very much on when you ask. People are at work or sleeping, and it's easy to miss a question. Also, people simply don't respond when they don't have a good answer, just try again an hour later.
This is probably the most convincing video I have seen for dependant types. Can idris or agda do type inference yet? (I know they can't in general)
I don't agree with StackOverflow being the answer, but I agree however that the IRC channel shouldn't be seen as a helpdesk line.
Wouldn't the shipping of haskell-platform be advised for distros?
Probably only a per-capability one is needed?
I don't see this. Please note that I'm not accusing you of being wrong or lying, but merely stating that this is not my experience. I almost always get an answer. Both when I ask something very "newbie"-like, and when I ask something more complicated (in that it requires the chan members to read my lengthy pasted code, or I need a lot of back and forth to describe what I mean). The few times when I don't I ask on a more specific channel, like #haskell-game or #ghc. Note that #haskell isn't generally suited for complicated questions like described above. In such cases, consider the haskell-café mailing list, or, if you suspect the question is very "newbie-like" , the haskell-beginners mailing list.
What exactly was using dependent types in this video? It just looked like type checking and the relf function to me. Could you not do this in Haskell?
You couldn't do this in haskell. (=) is a type whose parameters are values, not other types. Essentially, 1 = 1 is inhabited but 1 = 2 isn't.
On #nothaskell, we try to be slightly more helpful when we can. We're still a small channel, and try to be as welcoming to newcomers as possible. We also have a code of conduct.
My experience is similar. I think it's a matter of timing more than anything else. Another thought: sometimes somebody will pop into #haskell with a question about a specific library. It is entirely plausible that nobody watching the channel can answer the question off the top of their head. As the number of packages grows, the number of people who can answer pretty much any Haskell-related question drops to zero.
It would be cool if there would be some kind of syntax sugar like function _ b _ _ e 
I've had a similar experience as well as a newbie. I was learning another esoteric language a while ago and I was able to get a lot of great and really patient help from their channel, not nearly as much with #haskell. Sometimes my stuff will go through, but it's maybe a 50/50 nowadays. I certainly still appreciate every bit of help I get.
What do you expect from a media format that is channeling all inputs into one stream? It works relatively well when there are not that many participants. But once the number of chatters starts to grow most of non interesting noob questions drown quickly and disappear in the fast stream. That's why there are no IRC chats for large languages like java or c#. The format would not work at all. And i think haskell is approaching the size of active community where chat rooms would stop being viable venues to quickly get help. Time to switch to Stackoverflow, Google groups etc. 
Yes. I filed a ticket about it https://ghc.haskell.org/trac/ghc/ticket/9075#ticket Actually it needs to be per-capability per-generation.
I have no doubt that people have very different experiences in #haskell. Some people have loved it. Others... well, I have suggested people go there, and had them come back and tell me that everyone in #haskell is stuck up and self-centered and they'll never speak to anyone there again! It's probably a function of: 1. Time of day. The experienced Haskell community tends to be more European than most other programming groups, so time mismatches are possible. At certain times, the channel has a higher than average number of newer people who are more interested in getting their own answers and haven't yet assumed their share of ownership of the overall community. 2. Type of question. Questions that are interesting or easy will tend to get answered, because there's a greater reward per effort for doing so. This isn't something that's going to change easily, so as long as there are many questions being asked at a time, some questions are going to appear to get ignored. In those cases, I think SO might be a better forum. 3. Communication skills. Sadly, some people just lack the ability to ask clear questions, and in a streaming chat room, most listeners are more likely to ignore the question than risk being rude and calling out that the question isn't written well. One person to whom I suggested #haskell absolutely hated the experience, and I realized it's because he can't seem to ask a clear question without imparting 30 minutes of context and everything he's tried so far, BEFORE he feels comfortable stating a question. No one is likely to sit around for that. This is a personality thing, and I wish I knew the answer.
Is there a good listing of #haskell-foo channels? Posting that in the sidebar might be better, since people can more accurately get to the right place to ask their questions. As for the second bit, #python and a few others are still really good and really active, even with less need to proselytize. Hopefully haskell moves towards that direction, rather then in the direction of #c or #java which are not as welcoming.
These sorts of questions are much harder to get an answer to on most channels, in my experience. Package management and environment configuration are a finicky, ugly, environmentally dependant, and not terribly interesting question, and people are less likely to want to get involved.
it's unfortunate that no one here who shares your opinion has anything constructive to add. the response to this post essentially echoes my complaint that the haskell community is too far up its own categorical ass to notice it. the core language of haskell requires no understanding of category theory, yet this subreddit and the haskell community at large insist they're category theorists. when i first learned haskell, i viewed it as an extremely practical language, but this subreddit has taught me anything but that. there was a comment the other day in /r/programming that mentioned their favorite programming language in college was haskell. for career and other reasons, they were pulled away from it. they then mentioned that they don't even recognize the language as it is today with all the extensions and talk about higher order libraries like the all too popular lens library. like i mentioned, the references are very good in the post, so it's clear the poster has at least an inkling of what it means for category theory to be important. does their post contain errors? yes. point them out and correct them instead of bitching and moaning or don't comment at all. is the poster trying to gain something from this post? if it's something other than trying to better themselves by writing about an interesting topic or trying to spread knowledge to others, i'd like it pointed out to me. some of the references included are the only instances i've seen the references mentioned in a blog post other than the original sources, so it seems to me that people who actually know category theory don't actually understand its importance.
Awesome video! A quick question: when writing the `VerifiedSemigroup` instance at around 11:00, you're specifying a lot of "None" equations. Why wouldn't semigroupOpIsAssociative (Some _) (Some _) (Some _) = ... semigroupOpIsAssociative _a _b _c = refl be sufficient? How is this different from the explicit version used in the video, semigroupOpIsAssociative (Some _) (Some _) (Some _) = someSomeSome semigroupOpIsAssociative (Some _) (Some _) None = refl semigroupOpIsAssociative (Some _) None _c = refl semigroupOpIsAssociative None _b _c = refl ? (I took the liberty of naming unbound patterns, assuming that syntax carries over from Haskell, for readability.)
Perhaps Idris is nice enough to evaluate stuff for you when doing type checking but will not enumerate the search space and evaluate? 
Popularity happened. There wasn't 1300+ users in 2007. // truly yours, CO 
If you're a night-owl in the Americas, #haskell.au is a decent option. Some really nice people there. I second Stack Overflow for anything that requires more than a moment's explanation though.
&gt; I hope, I'll translate it someday to English. Do it. There's a severe deficiency of such material 
dons is no longer active. that's why.
That probably says something more about tiobe than about Haskell.
Apparently they changed their algorithm at the end of 2013, and it did seem that Haskell came out lower since then.
This is what I was going to say, there was some research into this in the past. Unless somehow things change and functional hardware is a disruptive innovation for the new conditions, I don't see this taking off anytime soon, except possibly for specialized hardware for specific applications, maybe using compilation to FPGA or something like it. 
Stack Overflow is **not** an optimized IRC. It's an optimized *option* for question and answers. I'm saying this because: - It's an active Q/A website - Most of the time you get a good and final answer within an hour (for most non-localized problems). - A huge number of times you end up [solving your problem by *almost* asking a question](http://blog.jerryorr.com/2014/04/solve-your-problem-by-almost-asking.html) on StackOverflow. - You help the next person that is going to have your problem, by publicly asking on a website that is very easily indexed by search engines. Chats on the other hand were not exactly build/optimized for question/answer format, but rather for... chatting. :)
Will give this channel a shot, thanks for letting us know about it!
I really prefer r/&lt;language&gt;, it's much more friendly and responsive than Stack Overflow. Stack Overflow is the worst, for questions beyond 101 intro programming. When I ask how to do X behavior in Y language, the answers are awful things like "Wrong forum" or "Don't do that." I do remember #haskell being very friendly and helpful. If it is changing, I'm glad to hear #nothaskell is starting over.
I've started #haskell-beginners on Freenode for anyone that wants to answer or ask questions. It'll be mostly myself and the other friendly folk from #haskell.au. Figured it would be more fair to the #haskell.au people if I didn't import people into their channel :)
Indeed, this solution is simpler. However, would you explain why we cannot use `Nat` instead of `SingNat`?
 It's fairly absurd that *Ladder Logic* makes that list.
Avoid success at all costs!
Yeah, there are certain types of questions (even certain things which some beginners might ask) which you can really only expect a couple of people to know the answer to. The most common thing I see of that sort are questions regarding how to get Cabal to do something which it probably can do, but which is a bit nonstandard in some way. Such questions are frustrating for me in particular to answer, because I've only ever had to write very simple .cabal files, and the Cabal documentation can be a little hard to locate things in. If one of the Cabal devs is on, I'll try to ping them, but it's often a bit tricky to get someone help on that. (Whereas if the question is about a library on Hackage, I can often search its Haddock documentation and rely on type signatures to answer many questions a beginner might have even if I'm unfamiliar with the library.)
&gt;but I'm also not happy with this. Maybe just nobody could help you? No point in being upset
Excellent video, definitely needs rehearsing though, content to "uhm" ratio was 1:1 
This is actually a really cool solution. It's a shame it's so verbose though.
Re-ask your question in intervals of ~20 minutes if it gets ignored. Usually it's just a traffic/visibility thing rather than people actually not wanting to answer it. Let's face it, IRC is a cruddy medium.
Right. The gist is that you're composing a bunch of functions that will ultimately modify some other function.
Nat doesn't carry an index around with it at the type level. SingNat is indexed by a particular Nat. Note that you can use [the Singleton package](https://hackage.haskell.org/package/singletons) to automatically generate the singleton type for a datakind, as well as to automatically instantiate a witness when the type of the singleton is known.
Wow, that's really cool stuff! Thanks for taking the time to explain. Idris really seems like a worthy successor to Haskell.
I would guess that it isn't that clever. Agda certainly isn't! Fallthrough patterns would have to get expanded by the compiler for the result type to reduce appropriately, and it isn't always obvious how to do that.
I actually really enjoy helping people get to the bottom of Cabal issues, and I try to hop on them ASAP whenever I'm on IRC.
Would have made sense if he used forall notation. I thought that is where this was going.
The general proof of the adjoint functor theorem isn't very important, although its far more handy than needing all the particular variants. If you want to be constructive you can typically derive the adjoint functors you want in the concrete without choice. Also categorical equivalence in the general sense is sort of overrated too. You can get to substitute notions while requiring less. Also note that you can do category theory without any underlying set theory -- you only need hom objects to be some sort of monoidal category. Historically category theory was developed, for convenience, over traditional set-theoretic foundations, but it doesn't _have_ to be. 
I have no idea what you're talking about. How does the response to this post indicate anything about the haskell community's mathematical posterior or the presence or lack thereof? The bulk of response I see is a nonresponse with some negative comments downvoted, and a few stray assertions leading to some off-topic digressions that have some fairly interesting material buried in them. Par for the course as far as I'm concerned. Neither most prominent members of this subreddit nor most prominent members of the haskell community insist they are category theorists. If one looks at current FP research, there is a fragment that uses category theory, but a relatively small one compared to that which just uses lots of dense dependent type theory. In fact, of the top 50 posts on the haskell reddit, this is literally the only one actually about category theory! Nor do I have any idea what you mean by "people who actually know category theory don't actually understand its importance". I don't even know what you're trying to mean by this.
Well... I don't know if it will mean anything for the former anytime soon! And for the latter, I don't know what this might mean for anything but transport of proofs... I do intend to go high level, but I also don't want to make airy promises...
We are having trouble getting this to work consistently. I get that (8500 . 8800) is to change just that character range, but otherwise I don't understand how this works. For me this mostly works, but some symbols, such as ::, &amp;&amp;, ||, == don't work. I did put the range upper bounds to 8900. 
Real talk: I write fewer types in Agda than I do in Haskell. The types I write in Agda are also easier to understand.
Probably life. That's what usually happens to people.
while claiming you have no idea what i'm talking about, you certainly have a lot to say about it. you at least have some idea about what i meant. :) you are partly correct in your critique. in general, it is my opinion that the haskell community can tend to be more focused on theoretical matters than practical matters, even in claiming that haskell isn't *just* a research language, and that's where i'm coming from. i focused on category theory in my above comment, because that is what's being discussed here in this thread. although, in my mind, i sorta lumped all the type theory talk that often goes on with category theory in my head. even with these mistakes on my part, i think there's a valid critique on the community's focus on more theoretical matters than practical matters. &gt; Nor do I have any idea what you mean by "people who actually know category theory don't actually understand its importance". I don't even know what you're trying to mean by this. well i am not sure where the confusion lies because you didn't really expand on why you're confused. you're also quoting out of context, and i think, within my comment's and this entire thread's context, it is clear what i meant and that i don't actually believe the generality holds (referring to the exact portion you quoted). to expand on what i did mean: even within posts or blogs talking about category theory in an accurate and knowledgeable manner, i have not seen some of the references that are posted in this blog post. my comment was in response to the fact that everyone here harping on whoever wrote this post for being inaccurate failed to see that they actually do understand that category theory is important by the mere presence of the links to category theory applications. this is in contrast to the many category theory posts, whose authors actually know category theory, that fail to mention applications of category theory and its usefulness. basically, i agree with the sentiment below that you don't have to have a thorough knowledge of a subject to know it is important, so the negative response was a bit overblown. a person knowledgeable in category theory could scroll right to the bottom and learn something, and a person not knowledgeable wouldn't be thrown off enough by the inaccuracies and could learn something by following the quoted references or links. so in the end, i don't know why people were treating this post like poison, and i made the, possibly incorrect, assumption that this reaction was coming from a high-brow or ivory tower type of attitude.
What's a good book on cat theory?
Overall, the API feels very, very verbose. Perhaps unnecessarily verbose. It looks like it's designed to be easy to grok for trivially sized examples, but would unwieldy for large or complex problem domains. Lots of long names and [unnecessary Newtypes everywhere](https://github.com/bitemyapp/bloodhound/blob/master/Database/Bloodhound/Types.hs#L229-L270). Is there any reason for so very many newtypes? data Query = ... | QueryMoreLikeThisQuery MoreLikeThisQuery | QueryMoreLikeThisFieldQuery MoreLikeThisFieldQuery Symptom of problem: So much newtype wrapping! let filter = RangeFilter (FieldName "age") (Right (RangeLtGt (LessThan 100000.0) (GreaterThan 1000.0))) RangeExecutionIndex False Why not design for easier to read? let filter = RangeFilter "age" [GT 1000.0, LTE 100000.0] RangeExecutionIndex False DSLs generally serve 2 purposes. * To use knowledge of a domain to set type constrains, ensuring that invalid programs are harder or impossible to write. * To use knowledge of a domain to create a subset of the host language that is simpler/easier to read/write. You've done really well at the first. The second, I'm not too sure of.
How so? PLC programming is an in demand skill in the industry I currently work in (Water industry). I could imagine there are quite a few sectors where it is important too. 
I don't know any no, but I do have a math background so I'd prefer a math book and not a generalization/programming book
i have a math background as well and it was conceptual mathematics that launched my interest in category theory, a subject i all but ignored while in graduate school. it's written by one of the top category theorists and a mathematician. also, i understand what you meant by no programming books, but i don't understand what you mean by generalization because that's sort of the point of category theory. :)
Awesome video. Does anyone have an up-to-date comparison of Agda and Idris? I want to learn at least one of them now...
This isn't exactly an architecture specifically for functional programming, but the Mill is pretty interesting as an example of an architecture that operates in a way that a functional programmer might find intuitive. IIRC, it doesn't use registers and has a "belt" kind of like a stack machine, except that instead of popping things off the stack, they kind of fall out of scope at the bottom. You can only write to the head of the belt (or to regular memory), the other belt entries are immutable, I believe. One feature that seems like it may have been inspired by Maybe or option types is the idea of metadata about the contents of belt locations. A belt location can contain data, or it can have metadata that says that there's nothing there (like a Nothing value) or the metadata can say that the contents are garbage and you should throw whatever the mill's idea of an exception is if it turns out the final result of a computation is garbage (like trying to read undefined). Having those two other states allows them vectorize loops that aren't otherwise easily vectorizable. One of the clever ways they use the garbage state is in implementing strcpy (and similar kinds of functions). They read a whole vector of bytes in from the source buffer, do some stuff to figure out if there's a NULL character and thus not all the bytes are supposed to be copied to the destination, and then write the good values into the destination buffer. An interesting thing happens if the mill reads a vector from the source buffer that's longer than the input buffer and (to make it more exciting) also the extra bits happen to overhang into another page of memory that the current process isn't allowed to access. The mill does not trigger a page fault. Instead, it simply marks the metadata for those vector elements as being in the garbage state. Any operations that would normally read those vector elements would also be flagged as garbage (sort of like how a Nothing value propagates in the Maybe monad). If the strcpy function is written correctly and the string is properly terminated, we'll never read those bogus values and everything is just fine. http://millcomputing.com/docs/
What do you expect from a search engine based ranking for a language that has excellent search engines that cover just the language's documentation?
I was just on there last night asking dumb noob questions and making dumb noob statements and a lot of people chimed in to help me and explain. There was the occasional joke about things I say, which I actually welcome (if they're playful enough), but probably many others don't.
I meant vulgarisation books sorry :) If I'm going to read a math book might as well read a naughty one so that it's useful in my studies
Did you just come up with that? It's spookily similar to the HoleyMonoid http://hackage.haskell.org/package/HoleyMonoid-0.1/docs/src/Data-HoleyMonoid.html
How come? I'd expect to write the top-level types in both, and that's it?
Another neat example of dependent types is demonstrated with another video that Brian put up a few weeks back where he made a [type-safe printf function](https://www.youtube.com/watch?v=fVBck2Zngjo) in Idris as well. It clearly shows how the type of the function is determined by the value passed to it (in this case, a format string).
I'm a fan of `time` but I'm interested to see attempted improvements to the API. My quick responses to the list of issues: &gt; UTCTime is represented as number of day since a date (sometimes in 19th century), plus a time difference in seconds from the beginning of the day. This is probably the worst representation to settle to as main type as it neither a good computer representation nor a good human representation. Why does it matter to the human what the underlying representation is? This doesn't affect the API at all. &gt; Every time I need to use the time API, i need to look at the documentation. With the number of time I used the time API, I feel like I shouldn’t need to anymore. Sure it got easier, but it’s not as trivial at I want it to be. The number of functions, and the number of types make it difficult. YMMV. It is indeed a bit puzzling. &gt; C time format string for parsing and printing. Each time I need to format time, does pretty much mean I need to consult the documentation (again), as there’s almost 50 different formatters, that are represented with single letter (that for some of them doesn’t have any link to what they represent). Yes, let's have typesafe parsing and formatting please. &gt; Need to add the old-locale package when doing formatting. Why is this old, if it’s still in use and doesn’t have a replacement? Good question. 
Yes, I have used ElasticSearch before. Yes, I admit that it's a complex API full of implicit sum types. &gt; I chose not to attempt to ignore or bury the complexity with the assumption that porcelain could happen later after use-patterns were better understood. &gt; If I drop all the explicit-ness and proper speccing of how ES actually works then there wasn't much point to this library. I'm not in any way suggesting that you ignore, bury, or drop any proper speccing of the underlying API. I'm merely commenting that the DSL part of the design seems to be very verbose in talking about those specs. I'm not even talking about porcelain, I'm talking about encoding efficiency. that is all. This being said, I actually really like the way you have thought about explicitly modeling the underlying API. I'll be very interested to use the library in the future. 
It doesn't matter to a human, it does matter for the machine though. This representation is not good for performance, since you need to do an expensive computation to convert it from what the system usually use (a number of seconds, or sub-units thereof). So UTCTime is a bad computer representation. And since no one use a calendar that count the number of days since 18xx, a bad human representation too. And, while we're at it, not a good intermediate representation either, as converting from seconds is faster anyway with hourglass (according to the apple-to-orange bench "utc-to-date"). So not a good time representation to choose as main type. Hourglass for that matter use the simplest type that the computer can manipulate quickly and store easily. just like the system API give anyway.
&gt; Why does it matter to the human what the underlying representation is? This doesn't affect the API at all. It does, because the underlying type representation needs to be sematically correct. If it isn't, then while you can work hard to try to make the API semantically correct, the type system is no longer helping you. You are almost certainly going to get things wrong for something as complex as time. The `DateTime` type in `hourglass` looks equivalent to `UTCTime` in `time`, so that's fine. (And I'm not sure how it's an improvement.) But `Date` is semantically wrong; so I hope at least the constructor is hidden to ensure that only valid values are constructed. A semantically correct type representation of Gregorian time would be unnecessarily complex. I think the choice of `UTCTime` as the basic type and then providing `toGregorian` and `fromGregorian` functions with the obvious type signatures was a great design choice by Ashley. I'm not sure why Vincent finds that difficult to use. &gt;&gt; Every time I need to use the time API, i need to look at the documentation. Two comments: First, I am sure I would need to consult the documentation for `hourglass` as much as for `time`, because time is complex. Second, that said, the documentation for the `time` library definitely could use improvement. Though it's quite usable once you realize that the lists of type class instances are the most important part of the documentation. &gt;&gt; C time format string for parsing and printing. &gt; Yes, let's have typesafe parsing and formatting please. Agreed. C-style time formatting is pretty standard and used by most programming languages, but that doesn't mean I have to like it. &gt;&gt; Need to add the old-locale package when doing formatting. Agreed that this is a very annoying dependency. &gt; Why is this old, if it’s still in use and doesn’t have a replacement? Because no one has written a replacement yet. Because the topic of locales is huge and difficult, with a very low ratio of benefit to work. The `hourglass` library appears to be stuck in the en.us locale. I would love to see `Data.Time.Format` split out of `time` into a different package to get rid of the `old-locale` dependency. And to be able to use `hourglass` formatting easily with values from the `time` library. Perhaps we can get just the amount of locale semantics needed just for formatting time for the most popular locales, to start with.
&gt; &gt; Why does it matter to the human what the underlying representation is? This doesn't affect the API at all. &gt; &gt; It does, because the underlying type representation needs to be sematically correct. Well sure. I guess I'm saying something tautologous, that is, I don't see why the current representation of `UTCTime` has any effect on the user from an API point of view, compared to any other representation which is functionally equivalent. 
In my opinion `r/haskell`, SO, the mailing lists, and `#haskell` are each optimized for a different kind of communication. In particular, specific technical questions with clear answers are great on SO. Google prioritizes them in searches, so it is the most useful for making information easily available also for others in the future. EDIT: Added the mailing lists. Hmm, how could I have forgotten about that? Times are changing...
Those kinds of questions do seem boring when they take up a lot of the bandwidth. On the other hand, it's really important for all users (not just beginners) to have a resource for quick help with that. And I must admit that I have profited quite a bit from what I have learned from those exchanges on `#haskell`.
With the Spineless Tagless G-Machine, GRIN, and other implementation techniques that work well on general purpose hardware a lot of the pressure to mimic graph reduction in actual hardware is removed. Custom silicon is expensive, FPGAs are slow, and the gains have to be enormous for it to win. The Reduceron approach needs a pretty wide access pattern for the heap making memory both non-standard and expensive. I'm curious if variants on the design can establish themselves as a competitive execution strategy, but I'd consider it quite far from given that there is actually a win to be had here given the constant factors it has to overcome against more traditional architectures. It doesn't just have to be faster for the same number of gates. It has to be multiple orders of magnitude faster, because it'll be running on FPGAs or worse manufacturing processes.
PostScript actually used to be the #1 programming language if you count machine-generated LOC. Compiled assembly is only generated by software companies, whereas significant percentages of the printer drivers on everyone's desktop were generating huge amounts of PostScript all day long. Nowadays proprietary printer protocols are the order of the day. And come to think of it, who prints on dead trees anymore anyway? So sad, PostScript was so lovely.
Are you sure you ´get´ the cleaver thing of the Gregorian calendar? To me, precis and correct for every use case trump easy of use for a single use case. The Modified Julian Day was established by Smithsonian in 1957.Why UTCTime choose that and why Erlang choose to use year 0 can be debated but is quite easy to transfer a historic date/time between platforms that are based on the Gregorian calendar. I don't agree for a bit that the UTCTime representation is bad with its usage of Days (as Gregorian calendar) and Seconds since midnight. Dates and Times are Hard in real life, don't implement a homebrew system if you don't understand why the current system is the why it is. To me your library seems to be a reimplementation of System.Time.
DateTime is not equivalent to UTCTime at all. They are equivalent in the sense that you can convert one into another (and back) without losing any information, but yet one doesn't offer the human benefits. I found it difficult because 'FromGregorian'/'toGregorian' is only part of the story. If I want to get the time of the day (i.e. h:m:s) what functions do I need to use ? I will have to get the utctime accessor to the difftime utctDayTime, and then divide by 60/3600 to get h:m:s. Or I could have use 'utcToLocalTime' with a 0 timezone (from another LocalTime module, why is it local anyway ?), then call 'localTimeOfDay'. Using hourglass, I can now do that in one call 'timeGetTimeOfDay' for any 'Time' type without having to do any conversion myself. I can do that from a number of seconds since unix epoch, a number of seconds since Windows epoch, or a Date, or any other 'Time' type that are not built-in the library. This kind of N functions scheme to get to what I want (that are not easy to remember), is pervasive to time. And this is the reason of hourglass. Yes, hourglass is EN locale only. It wouldn't be very hard to add some user-defined conversion function from EN locale to another to hourglass, but until that need arrive...
Yay! I like (in theory, haven't played with it yet) the idea of being able to create the objects within a monad rather than having to specify type class instances. Great job, Kay!
Avoid: success at all costs.
I wouldn't accuse Vincent of not understanding time semantics, but you are correct that his library doesn't seem to implement them fully. Semantically, a time value in `hourglass` does not represent a moment in time; it represents a response from the system using the C API for time, with other units of time then bolted on top of that. That was previously the common approach in most programming languages, and it is a known source of subtle bugs. After a higher-level library was first implemented in Smalltalk and then the `time` library in Haskell, such higher-level libraries are now available for many other popular programming languages.
Can you make an example of this "wikipedia syndrome", on Stack Overflow?
[`time-lens`](http://hackage.haskell.org/package/time-lens-0.4.0.1/docs/Data-Time-Lens.html) mostly solves that issue of `time`.
Could you please push it to github, this is a great idea, and a lot of people will interest in this package and would contribute to it.
I'm not sure I'm following exactly what you mean here (if you can explain it in more details it would be greatly appreciated). But I agree that you can lose the "localness" information of the local time if you use certain api instead of another, but I didn't found a nice solution to handle this; If anyone got ideas.
&gt; I'm a fan of time but I'm interested to see attempted improvements to the API. What do you make of [`thyme`](http://hackage.haskell.org/package/thyme) then?
Fear not, I have generates just the module for you! https://github.com/maxpow4h/flip-plus/blob/master/Control/FlipPlus.hs Simply hold shift and press the number of the argument you want to apply. So for your example, you would use the @% operator. Unfortunately due to RAM constraints, it only supports up to 6 arguments. 
Why isn't it obvious how to do that? The arguments come from a data type with a finite number of constructors. The type checker could just enumerate all of them. Of course this would be inefficient, but it shouldn't be very hard.
&gt; ...is available on hackage, and the code on github. IMHO, `hourglass` being developed on GitHub is already an improvement over the `time` library, which seems to have a less open/transparent development model. You can't even browse the commit history without cloning the Darcs repo locally.
When you have to hide the API of something behind something else, I have to disagree that it's solved :). Otherwise, yes, it has somewhat the same effect as to what I did with hourglass.
I remembered HoleyMonoid and tried to come up with something similar, although I didn't base this implementation on HoleyMonoid.
Idris has a lot more features than Agda. I think Agda has better IDE and it has a smaller set of language features, and every quantifier is explicit. The end result is that in Agda you write a lot of uniform looking code, and it's more designed for small formalisations and experiments in type theory, but can most certainly be used for programming as well. Idris has been designed from the beginning as a programming language first, and a theorem prover a distant second. Agda tries to cover both use cases with a fairly minimalistic design.
&gt; DateTime is not equivalent to UTCTime at all. They are equivalent in the sense that you can convert one into another (and back) without losing any information, but yet one doesn't offer the human benefits. If they are equivalent in that sense, it is good enough for me. &gt; I found it difficult because 'FromGregorian'/'toGregorian' is only part of the story. If I want to get the time of the day (i.e. h:m:s) what functions do I need to use? Once you realize that a moment in time is not the same thing semantically as a wall clock time at a particular location, and that to convert makes no sense without specifying the time zone, it's obvious how to do it. The `hourglass` library mixes up the two concepts, then provides a type class method to distinguish between them - as pointed out by /u/roche. That is analogous to how in PHP strings and integers are represented by the same type, and then you have a function to decide at run time (if you're lucky) which one you have. &gt; Yes, hourglass is EN locale only. It wouldn't be very hard to add some user-defined conversion function from EN locale to another to hourglass, but until that need arrive... That would be great! EDIT: I am *not* saying that this is like PHP. It's just an analogy.
`time-lens` has a lot of the same semantic problems that `hourglass` and the many other "easier" of "simpler" or "friendlier" time libraries on Hackage have. I would love to see a lens library that implements the semantics of the `time` library. So, for example, instead of just iterating a UTCTime over days, which is ambiguous, you could choose one of several more precise operations: view it through a Day lens to iterate over the UTC day, or view it through a LocalTime lens composed with a Day lens and iterate over that, or view it through a Gregorian day-of-month lens and iterate over that, etc.
`thyme` doesn't change the API very much. It's about speed.
Here's what I mean exactly. With time and time-lens, I get a compile time error when there's no timezone information and I'm trying to access it: Prelude Data.Time Data.Time.Lens Control.Applicative&gt; getL timeZone &lt;$&gt; getCurrentTime &lt;interactive&gt;:10:6: No instance for (HasTimeZone UTCTime) arising from a use of ‘timeZone’ In the first argument of ‘getL’, namely ‘timeZone’ In the first argument of ‘(&lt;$&gt;)’, namely ‘getL timeZone’ In the expression: getL timeZone &lt;$&gt; getCurrentTime and if there is one, there's no way for the operation to fail: Prelude Data.Time Data.Time.Lens Control.Applicative&gt; getL timeZone &lt;$&gt; getZonedTime EEST With hourglass, this compile-time information moves to run-time, so a bad operation will compile but result in `Nothing` Prelude Data.Hourglass System.Hourglass Control.Applicative&gt; timeGetTimezone &lt;$&gt; timeCurrent Nothing and a definitely valid operation will nevertheless return a `Maybe` Prelude Data.Hourglass System.Hourglass Control.Applicative&gt; timeGetTimezone &lt;$&gt; (localTime &lt;$&gt; timezoneCurrent &lt;*&gt; timeCurrent) Just -4445 
Haha very nice. Now we just need emacs/vim plugins that make them human readable.
I wonder if hourglass just needs an extra `HasTimeZone` typeclass to address this.
Isn't this exactly `HoleyMonoid` specialized to `Endo` with `later == HM.later id`? EDIT: Nope. Doing it with `HoleyMonoid` itself for functions of arbitrary arity would require some type trickery.
&gt; instead of just iterating a UTCTime over days, which is ambiguous Is it? Using just the `time` library: Prelude Data.Time&gt; :t getCurrentTime getCurrentTime :: IO UTCTime Prelude Data.Time&gt; getCurrentTime 2014-05-05 12:22:58.625162 UTC So, despite the perceived ambiguity, `time` can also associate a day (namely, 2014-05-05) with a `UTCTime`. Of course, nothing prevents you from converting your `UTCTime` to `LocalTime` and dealing with the notion of day you seem to be more comfortable with.
&gt; They are equivalent in the sense that you can convert one into another (and back) without losing any information They are not equivalent in that sense. Converting from UTCTime to DateTime is [lossy](http://www.reddit.com/r/haskell/comments/24rcf0/hourglass_a_new_simpler_time_library/ch9zrax).
Does the `thyme` library - which keeps the semantics of the `time` library but is faster - address this concern?
You're right of course; I'm just bitter about having been forced to learn it without any context whatsoever on extremely dated software.
&gt; Using some sort of foldM seems an overkill when the number of arguments is small. Why? That sounds like exactly the right solution.
You are right that a `UTCTime` has a `Day`; it is actually one of the fields in its representation. (I wouldn't give too much weight to the formatting output, though. Parsing and formatting is a weak point of the `time` library imho.) What I mean is that `time-lens` uses the same lenses to mean several subtly different things in different contexts, with related but different behaviors, then relies on explaining those different behaviors in the human-readable documentation. That is the less-semantic approach to time, and it is what the `time` library allows us to get away from. I would rather have the lenses themselves make it clear exactly what they are going to do. I would also like to have `''makeLenses` for the various types of `Data.Time`, so you don't have to skip over the various types and thus obscure what you are doing.
New type theories, coming out of a defense department near you!
Yeah, I meant more how to do it cleverly without creating an exponential number of synthetic cases. 
That's great, but won't it make the discoveries classified, like many are in cryptography?
That makes sense. Often, though, when the language you are parsing is line-oriented, it makes more sense to apply `T.lines` to the input and then map your parser over the lines. You don't need `attoparsec` just to skip blank lines.
You'll need an efficient way to search the input space for values for which you have an output. But come to think of it, once you have that, you actually implemented (a domain restricted form of) `foo :: T -&gt; T` !
Well, functional reactive programming is essentially that, but more.
The DoD funds an *insane* amount of public and private research - you'd really be amazed at the kinds of projects that get a cut of their money from them, and the kinds of things they do. They are literally interested *in everything*. HTT is such a new field that I doubt they want to classify everything, in the hopes of laying waste to current mathematics, or something. The reasoning is much likely more simple: HTT promises a foundational, constructive approach to the core of current mathematics (being constructive, it's in theory quite nice for a computer, since proofs are actual constructive objects, like we're used to in dependent type theory). Something like that is potentially very appealing to them - they could use it in the future to design improved systems, and they have had long-standing, publicly known interest in things like formal verification for infrastructure. In other words, they'd have a lot more to gain by letting it flourish. $7.5mil to them is a tiny investment in a handful of smart people to help that happen. I see no particular reason why research on HTT would need to be classified.
It should still be only a polynomial number of cases. But doing this cleverly in general is indeed hard. For instance, suppose you want to implement a decision procedure for equality. In peudo-Agda data DecEq {A} (x y : A) : Set where yes : x == y -&gt; DecEq x y no : not (x == y) -&gt; DecEq x y data MyType : Set where foo bar baz : MyType decide : (x y : MyType} -&gt; DecEq x y decide foo foo = yes refl decide bar bar = yes refl decide baz baz= yes refl decide _ _ = no absurd if you expand the last case, then the type checker can indeed see that `foo /= bar`. But if you want to see that without expanding, the typechecker would not only have to keep track of equalities, but also of inequalities.
Oleg himself works (or at least used to work) for a Navy establishment. I don't understand why defense departments would fund public research though. Research surely benefits them, but doesn't it benefit their enemies just as much?
Don't exaggerate; you can still get plenty of help in #haskell too. I have noticed a distinct change in the people there, but it's still pretty helpful most of the times I'm paying attention.
Scala does this and it's nice in restricted cases, but in the presence of type variables and lambdas they can be pretty ambiguous.
Another point of view: http://langpop.com Haskell appears high on github, reddit and other discussion sites (but not on lambda-the-ultimate!?) 
is the cause of this a slow `realToFrac`?
That depends on who your 'enemies' are for one thing, alongside about a million other things - including what your actual goals are. Are we worried about North Korea or random insurgents using Homotopy type theory to fight us? Or are we worried about China using HTT to build some kind of supermath that could destroy us? I mean, if you really think about the angle here, not only does that seem ridiculous, but the question becomes *what do they gain by classifying it*? I know people like to point fingers about Roswell and area 51, but I'm not under the impression they literally frollick around classifying information on a whim when unneeded. Maybe I don't understand all the true implications of Homotopy - but for some reason this seems a little far fetched. What's less far-fetched is that, say, 20 years from now, they could maybe use some of that research when designing something. Or, in 20 years, the research will be rich enough they *do* have something worth classifying for their advantage. Or in 20 years enough has happened and run its own course that money just *paid for itself*. All of these have happened, all of them worth much more than $7 million, potentially. The original question was, will it make the discoveries classified? Under this project I think the answer is decidedly no - $7 million dollars is a lot of money, but it's not enough money to revolutionize the Queen of Sciences in, like, 5 years or whatever. I'd imagine if they did classify all $7mil of that work, they'd gain *nothing* - it wouldn't have had enough of an impact to pay off its investment for them, they'd just have to keep sinking money into it to push it further in Classified-World. In other words it would just be a money sink for them. They have more to win by letting that $7million of research go to the public to let it gestate faster, and it's a longer-term but ultimately less-costly proposition for them. It's an investment like anything else, and $7mil is pennies if in 20 years (a short time-scale) you get some good results from the public sector of researchers - which isn't very unreasonable a proposition to consider, when you have longer timescales.* * It's already pennies to an organization with an annual budget of like nine gazillion dollars, but you get my drift.
I agree with what you've said, but if it doesn't have defense implications now, why not fund it from a science budget, and defense funding can come later for parts that *are* going to be secretive.
I personally prefer /r/haskell for serious discussion. Key features include: A) No limit on comment length B) Threaded conversations C) Upvotes and downvotes that improve signal to noise IRC only has (A), Stack Overflow only has (C), and Twitter only has half of (C) (Upvotes). This is why I get upset when people try to shut down beginners who ask questions here, because I think this is the best format for answering questions.
I am very happy to hear this. I have been using HsQML a lot recently and I have already taken a sneak peek from darcs at Qt5 support. I currently do a lot of C++ Qt development and I am happy to transition that QML knowledge to my Haskell projects. I've been working on a TorChat clone in Haskell using HsQML (https://github.com/creichert/hstorchat) and it has been solid. I would be happy to take any criticism on the code. I admit it's fairly imperative at this point. I am still implementing most of the client communication. I also have a list of suggestions and I would like to make some contributions in the future. Cool Project. Thanks again! EDIT: Already done porting to Qt5: Haskell is great: https://github.com/creichert/hstorchat/commit/edadd796ce9c4c1a24a4c08f6cf30f7e62c0db4e One question. Is `initialWindowState` in `Graphics.QML.Engine`? I see it in the Morris demo but I can't find it in the documentation and it does not compile for me.
The trick is to write your `sum` function to aggregate a list of values, typically by using `foldr` or `foldl'`: sum :: [Int] -&gt; Int sum = foldl' (+) 0 Then you can take a list of arguments wrapped in an applicative functor: xs :: Applicative f =&gt; [f Int] ... you can use `Data.Foldable.sequenceA` to combine them into a single functor value: sequenceA xs :: Applicative f =&gt; f [Int] ... and then you can `fmap` your function over that to combine the result: fmap sum (sequenceA xs) :: Applicative f =&gt; f Int
And the internal representation used for time (`Integer`).
doesn't a fragment live on in pdfs?
ELI5 Homotopy Type Theory?
Because their time-scale is different than yours, and again their 'enemies' are not what you think. 20 years from today is enough to warrant investments now - that *is* good enough from a defense implication. The question is how they go about doing it. As for the question of budget, well, that seems obvious to me - the DOD has a budget of about nine-gazillion annually. They can basically put as much money as they want on the DOD tab with little ramifications. There aren't many institutions *at all* that can compete with such generous and flexible grants/investments for decidedly fringe research, let's be completely honest. They fund a *lot* of research. It's really not a matter of 'if' it has defense implications, as far as they are concerned. It is a matter of *when* it does. And that makes it worthy of an investment, just not one to classify.
Hmm.. do you think we could move to an `Int64` without too much protest? 292 billion years should be enough for anybody...
I often have to annotate values in Haskell due to people's type class shenanigans ruining inference. Fwiw, that sort of thing happens in Agda too, but instead of annotating a term, one can just give the missing parameters explicitly in curly braces.
Doing Haskell in the real world.
I have an account on [hub.darcs.net](http://hub.darcs.net/komadori) which I use to keep the source code for all my projects. Do you think it's practical to keep a git mirror of a darcs respository?
They suggest that the ranking boils down to hits for "$LANGUAGE programming" on google and a few other places. (I tried it with google alone, and it seems about right.) It is thus just a question of a community-wide commitment to using the somewhat repulsive expression "haskell programming" wherever possible. Haskell programming
On the Unix time, it's described like this in Wikipedia (with addentum of lack of leap seconds). I wasn't particularly striking for pedantry when writing this line UTCTime doesn't account for anything. It *allow* recording a leap seconds (difftime to 86401, just like DateTime allow setting seconds=60), but everything in time just do conversion with nominal diff time. So every time you call getCurrentTime, it will get the *POSIX* time, and convert it using divMod 86400 to get the number of days plus the remaining number of seconds. Likewise for the calendar function. It doesn't account for leap seconds _anywhere_ in the normal path. Anyone using getCurrentTime without loading a timezone file (contains leap seconds usually) or using the TAI module (which requires the non-provided LeapSecondsTable), and adjusting the day/DiffTime would have the exact same result that hourglass is giving. Likewise hourglass could be modified to, given a leap second table, give you a UTCDateTime if you wanted.
There's no ELI5, but try [this book](http://homotopytypetheory.org/book/) or [these lectures](http://www.cs.cmu.edu/~rwh/courses/hott/), which are probably easier to get into from a CS POV.
I use [darcs-to-git](https://github.com/purcell/darcs-to-git) to maintain [a github mirror](https://github.com/gelisam/hint) of the [hint](https://hackage.haskell.org/package/hint) library. It works fine for transferring new darcs patches to git, but I haven't found a good workflow for transferring git commits back to darcs.
[I asked this very question](http://www.reddit.com/r/compsci/comments/19skgi/what_is_homotopy_type_theory_and_are_there_any/) and got some very good answers.
&gt; Someday all will know it as such And by the time this happens, the inscription will be outdated, because `join` will have been incorporated into the `Monad` class.
I am very excited about this! Using QML feels surprisingly similar to writing Haskell code, in that you can use abstraction and order-idenpendent declarations. Very different from any other UI-building system I have ever used, even Haskell's UI systems. I only wish GHC supported as many platforms as QML!
You can try passing `-ddump-ds` to ghc, and probably add `-dsuppress-all` and `-dsuppress-uniques` to hide some uninteresting outputs that you don't need. It's not going to be pretty though as it desugars everything including the typeclass dictionary passing mechanism, but it should still be fairly readable.
FYI, without the realToFrac call, time is 10x slower on my system than hourglass.
That's an interesting idea. Although one quick fix, would be to just remove the timeGetTimezone from the Timeable type class, and just allow the LocalTime to report the timezone. (as currently the LocalTime type is the only types to record local time.). The only reason it was here, was to provide an escape hatch to be able to call conveniently timePrint. HasTimeZone could be useful to support multiples local types. not sure that's needed yet though.
Glad to hear it!
Yeah, keep a mirror is totally fine, since more people are using github, and it is easier to fork your code. Thanks!
This looks great! If I can figure out how to get OpenGL to play nicely with it, I'd like to use this to provide a UI shell for a bunch of programs.
That's okay. Why doesn't it make sense to manufacture special purpose hardware?
Why the base constraint VerifiedSemigroup on VerifiedMonoid?
Fair enough, but where's the equivalent of GitHub's pull-requests and issue tracking? Where can I see open/rejected feature-proposals, submitted bugfixes (mostly so I can avoid wasting time sending in fixes for which there are already 10 other emails waiting in the maintainers INBOX?), and so on?
Why did they pick such a weird syntax? Talk about confusing a haskeller (lambdas are fat arrows, type declaration is : but cons is :: )
I sent a patch to time maintainer Ashley Yakeley to do some minor changes for Safe Haskell. After a couple of weeks he took my changes and I was suprised they made it into the version that shipped with GHC 7.8 as it was already in rc at that time. Github is fine for projects that use it but I don't think it's fair to say a project is less open just because it doesn't use Git and hand its data over to Github.
It's doublethink. Spending money on fundamental science is a waste because the promise of benefits in the future is not a certainty. But spending money on science for military advantage is worth the risk. This is how you sell science to a population that thinks money should be spent carefully, and that war is a good way to bring peace. I'm truly convinced that there are people joining military boards out of goodwill to use their influence for things like this. 
&gt; doesn't it benefit their enemies just as much? I don't think so. Knowledge doesn't just exist on paper; it needs people to understand it. I think grants like this are less about producing HTT papers than about producing HTT theorists. Notice that the grant went to U.S. university, and not, say, INRIA. If HTT turns out to be fruitful, a Carnegie Mellon postgrad can be snapped up much more easily by the DoD than by, say, Russia. The grant might even create a research culture that is sufficiently attractive as to deprive "enemies" of potential native researchers by bringing them to the U.S. Also, language barriers are powerful: when was the last time you read a research paper in Chinese?
But the postgrad working on HoTT who gets funding from this project may very well be French, Russian, Chinese, Argentinan, whatever, and take their knowledge out of the country when they leave, being snapped up by their own country's defense industry.
Unfortunately there are way too many bad questions being asked on SO, as nasty as the close brigade is, it isn't efficient enough to keep up. I would agree it has caused them to go over the line and start closing questions they don't recognize properly.
Anything by [SPJ](https://www.youtube.com/watch?v=7NPBrWDzO2A).
Unless you invent something really radical (and I don't know what it is) you need to do memory loads&amp;stores, arithmetic, etc. Today's processors do that really well. I'm not sure what a special purpose architecture can add.
Take a piece of string and some legos. Tie the string down so that both ends are stuck in place, but it's still kinda loose. Now build some lego stuff. Sometimes you can move the string up and around the lego stuff, sometimes you can't. Homotopy Type Theory is kinda like that. And then the 5-year-old would get distracted playing with Legos, and say "look, look, homa mopey tie feary!" [Obligatory (today's) relevant xkcd](http://xkcd.com/1364/)
[Image](http://imgs.xkcd.com/comics/like_im_five.png) **Title:** Like I'm Five **Title-text:** 'Am I taking care of you? I have a thesis to write!' 'My parents are at their house; you visited last--' 'No, no, explain like you're five.' [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=1364#Explanation) **Stats:** This comic has been referenced 14 time(s), representing 0.0738% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcdcomic/)/[kerfuffle](http://www.reddit.com/r/self/comments/1xdwba/the_history_of_the_rxkcd_kerfuffle/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me)
That's a rather clever hack for generating these operators!
&gt; You can't even browse the commit history without cloning the Darcs repo locally. ...actually you can: https://git.haskell.org/darcs-mirrors/time.git (it's automatically mirrored from the upstream Darcs repo hourly)
How about John Carmack talking about Haskell? http://www.youtube.com/watch?v=1PhArSujR_A
I don't think that's changed. I've consistently had my questions answered by asking in the window of 1 - 3pm, Mountain Standard Time. They have not been particularly great or probing questions. They were mostly on the stupid/oblivious side, and with each answer my misconceptions have been sharply but gently corrected. I've found that some consistency helps as well. Ask again if you need to, but don't be obnoxious about it. Things tend to get washed away in the text of a lively conversation, and simply aren't noticed by most.
Well, I don't know about the full reasoning behind lambda fat-arrow syntax, but I think the other is somewhat trivial: In dependently typed languages, you are dealing with types way more often than consing lists - so it's just quicker to type one `:` - it saves a keystroke in a place that you'll be working with often. I believe Edwin Brady commented on this in a presentation about Idris at one point, but I don't have a link off hand. :(
Thanks for your comment. The problem with the format I was parsing was that there is some "state" associated between lines, so I couldn't simply map over the lines. I had to switch between two parsers periodically (header vs content), even though the individual parsers were very much line-based.
Because the defense budget is *ridiculously huge*. It's far easier to fund research through the DoD than it is to convince congress to budget more money for science.
defense departments fund public research all the time. there is some "leakage", but the research ecosystem and environment are really what the intangible things are. Not only do they build up researchers, they build up the system by which research happens --- something that can't be as easily copied and exported. everyone in the world might be able to use the innovations, but not everyone can consistently innovate.
This might be the key. An artefact of the political process.
Encourage the maintainer to move to darcs hub.
&gt; fail :: String -&gt; m a That felt surprisingly depressing to read.
If you can encode your language as a structure in a constraint logic programming language, you can then write a type checker and use it as a generator of programs given you have a suitable search strategy. In general program synthesis is an active research topic, and something a lot of people are thinking about and working on.
Start by understanding dependent types and homotopy. Then, add homotopy to dependent types by making equality only hold up to homotopy, and declaring equality to be equivalent to equivalence^1 . Then, start using equalities in your type formation rules to construct types with non-trivial homotopy. Then build mathematics. So, what you probably want first is ELI5 dependent types and ELI5 homotopy. 1: There are many equivalent notions of equivalence for this, but your intuition of an isomorphism from any other area of math should be roughly close enough. I assume most five-year-olds have learned what an isomorphism is.
It has nothing or little to do with the military. Research in the USA is channeled through the defense budget. The military in the US is the social branch of the government, but for certain demographies, funding the military sounds better than funding universities directly. Besides, you end up saying things like "the military invented the internet" which keeps the funds coming. 
Agreed. If only there was a SO where questions could be answered! 
I don't get this. How can there be to many questions? I only find answers to trivial questions there and all specific questions I ask are closed. The reverse is also true. When I search for specific problems I find closed questions that exactly asks what I need to know. Clearly a site where everything that is on topic for IRC is on topic to be asked is needed.
Also it is a security measure to discover things before any other and make it public. Just before some evil power find it , classify the discovery and make some evil use of it thank to the anticipation and classification. it is a bit like finding and publishing security holes before any evil code is available. "We know it and have the patch for everyone" is the message for possible evildoers.
Could EKG use per capability counters that are lock and synchronize free?
&gt; So, what you probably want first is ELI5 dependent types and ELI5 homotopy. Yes, please.
Safe Haskell should be up there. Yesod has some ideas in this detection regarding type safe URLs, but I don't think it is fully utilizing the type system to sanitize input. The powerful parser libraries are a good step up from regular expressions used in most other languages in order to sanitize input strings. I think ezyang blogged about using the garbage collector to effectively limit mutation in a thread in connection with safe Haskell. He was looking for people interested in that sort of resource control. You should look it up.
The library seems to conflate everything into a singular "Timeable" class, and I don't think it's a good design. A decent date/time library has to have the following datatypes, each of them distinct (in brackets: examples from `time`): * instant (can be implemented as date+time+UTC) (`UTCTime`, `PosixTime`) * date+time – no timezone (`LocalTime`) * date (`Day`) * timezone (`TimeZone` – sorta) * interval (difference between 2 instants) (`DiffTime`) And optionally: * date+time+timezone (`ZonedTime`) * time (`TimeOfDay`) * time+timezone * period (a certain amount of date/time units, treated non-linearly) (there are functions in `Data.Time.Calendar` for some of that functionality) `time` isn't perfect, but it has most of these. `java.time` is an example of an almost-perfect data/time library (they finally got it right at the third try). As for `hourglass`... There are several types of instants (`CTime`, `Elapsed`, `ElapsedP`), but the only candidate for date+time (`DateTime`) behaves like an instant, which may lead to mistakes (for example, user inputs her local time and the programmer forgets to convert it and treats it as an instant, causing the program to be hours off). Also, `LocalTime` supports the same API too, leading to type-unsafe things like `timeGetTimezone :: Timeable t =&gt; t -&gt; Maybe TimezoneOffset`. No real timezones and no DST. No info about what happens with dates before 1582. Sorry for sounding harsh, but correct handling of dates, Unicode, and internationalisation are pet peeves of mine. They are all things that seems easy first, but often end up being buggy and/or culturally insensitive.
The MURI program is for basic science, and as far as a know it doesn't involve classifying anything. I am funded by a MURI, and our program officers are civilian Ph.D.'s. There are very little strings attached, they just want people doing important science in areas the DOD thinks is important. source: I am a grad student currently supported by a MURI from the army to work on basic science of artificial cells. 
&gt; Clearly a site where everything that is on topic for IRC is on topic to be asked is needed. I apologize if I implied otherwise. StackOverflow has a specific niche, and they biggest problem is explaining that niche to new users. &gt; How can there be to many questions? Because having unanswered questions is the easiest way to find people who need help. However when you have 1.75 million unanswered questions that is no longer such a good guide. &gt; I only find answers to trivial questions there and all specific questions I ask are closed. The good thing about community moderation is you get a lot of people helping out. The bad thing is that the results aren't always the best. A lot of questions you don't have to know the language to know if it is a bad question or not. However too many people take that as "I don't need to know the language" and get close happy.
It isn't harmless in the large though. It means the _type_ of basic research that gets funding tends to be that which can demonstrate some potential application (even if fairly airy or bogus) to either A) keeping secrets B) making things explode C) making things that are hard to explode. So writ large this means certain realms of even relatively cheap pure research don't get as much funding as one would like because they have a hard time making such a case.
Homotopy is the notion of paths between paths. It's really hard to explain without pictures, but I'll try. Let's imagine that you live on the surface of a giant donut. Your house is on one part of the donut, and your friend's house is across the donut hole on the other side. There are a few different ways that you can walk from your house to your friend's house. Let's say there are two different paths between your houses that take you around the right side of the donut hole, and another way that takes you around the left side of the donut hole. Now let's imagine we made a chain of people all holding hands, standing on a path from your house to your friend's house. Then, let's have everyone walk from that path to one of the other paths. If you try that between the two paths around the right side of the donut hole, they'll be able to make it to the other path no problem. But if you try to have them walk from the path around the left side to one of the paths around the right side, some of them will fall through the hole. The pairs of paths that don't cause people to fall through the hole are called "homotopic" to each other, or you can say that there exists a homotopy between them. The homotopy is the path that the people follow to get from one path to the other path. It then gets complicated, because you can have higher homotopies, which are paths between homotopies. This becomes hard to visualize pretty quickly. (Looking back at what I've written, I expect this explanation to be more confusing than helpful, but I'll post it anyway just in case.)
Thanks for the feedback. I think most types you describe are present in hourglass: * DateTime+Timezone is "LocalTime DateTime" * you can get intervals in Seconds (timeDiff, timeDiffP), but not a type by itself * periods are handled through TimeDiff. I don't think it's fair to say that because the programmer can use the wrong type, it's the fault of the API, for localized time. One could input the wrong timezone in ZonedTime (say the UTC timezone instead of the local timezone), and it would be hours off too. To input LocalTime with hourglass, you would have to use the LocalTime type (which is a parametrized time's ZonedTime). For timeGetTimezone, it might be going for removal, as other pointed out earlier. Also, time (by itself) doesn't handle timezone any more than hourglass. And I really really don't care what happens before 1582. :-)
Interesting to compare to the same index posting, but when it looked more favourable. I like that the robustness was questioned a bit last time. http://redd.it/uuqv2
Dependent types are types that can depend on values. There are two main kinds of dependent types to learn: pi-types and sigma-types. They can be understood by analogy to simpler types: function types and pair types. A pi-type is like a function type where the codomain can depend on the value passed to the function. So, rather than a function from A -&gt; B, you have a function from (x : A) -&gt; B(x), where B : A -&gt; U, and the values of type U are themselves types. An example will make this a bit clearer. Let's say we want a function f from (x : Bool) -&gt; B(x), where B(True) = String and B(False) = Int. That means that if you pass True to f, you'll get back a String, and if you pass False to f, you'll get back an Int. Then f can be defined such that f(True) = "helloworld" and f(False) = 5. A sigma-type is similar, but with pairs instead of functions. With a regular pair you can have values of type (A, B). With a sigma-type (dependent pair) you get values of type ((x : A), B(x)). So using the same example above, you could have the values (True, "helloworld") and (False, 5) be of the same type.
Need? It depends on how much of a breaking change we want to make. Currently, `DiffTime` has the following properties: * `Num` instance as seconds * uniform resolution of picoseconds for any size time interval * exact arithmetic, no floating point ugliness `Int64` isn't big enough to support that; it would only give us up to about 3 months. `Double` would change the semantics. We could implement a custom type with two words that would support the same semantics and be much much faster than what we have now, but it would never match the speed of `Double`. `Data.Fixed` is in dire need of optimization even if we stick with `Integer`. What do think would be the best direction to go?
Idris is aimed at programmers. It's almost impossible to run a program written in Agda. Both are fun, though.
What have you tried?
&gt;I don't think it's fair to say that because the programmer can use the wrong type, it's the fault of the API Isn't that the point of static typing? Anyway, why I think the library doesn't do its job correctly: * user inputs local dates: 2014-01-01 11:00:00 and 2014-05-01 11:00:00. Claiming there are (31+28+31+30)×24×3600 seconds between those two timestamps is incorrect – there isn't sufficient info to calculate a time difference. Therefore, since those 2 values would be stored as DateTimes, `timeDiff (a::DateTime) (b::DateTime)` should not compile at all. * TimeDiff is broken; it conflates intervals and periods. There is no meaningful way to add one month to an instant of 146262465 seconds after Unix epoch. Therefore, `timeAdd (Elapsed 146262465) (mempty{timeDiffMonths=1})` should not compile. * let's take the following snippet: &amp;nbsp; let t = timeConvert $ Elapsed 15126362 :: DateTime putStrLn $ show t let tz = TimezoneOffset 300 let l = localTime tz t putStrLn $ show l let u = localTimeUnwrap l putStrLn $ show u putStrLn $ show (u == t) -- this will print False Why does it compile? Conceptually, `u` and `t` are different things: `t` is a precise moment in time, `u` is a sequence of numbers that may or may not be a human-readabable identifier of a moment in time given a particular timezone. Or, using `time` terminology: `PosixTime/UTCTime` vs `LocalTime`. The final "False" suggests that those objects represent different points in time – while the only thing I did was converting between different types. * Also, the API is confusing. Is `localTime` converting UTC time to local time with a timezone, or is it appending a timezone to a local time without a timezone? Since DateTime has this dual use, reading the type signature doesn't help – and the docs don't explain that either. As for time library design, I think [`java.time`](http://docs.oracle.com/javase/8/docs/api/java/time/package-summary.html) is one of the best available implementations – it neatly separates all the concepts, makes them quite easy to use, has almost all possible useful capabilities, and has almost perfect, grammatically correct i18n.
**Not a complete solution,** but this should help get you started. Pattern matching on your algebraic data type `Expression` is the first step, module Main where data Expression = Constant Integer | Variable String | Sum Expression Expression | Difference Expression Expression | Product Expression Expression deriving (Show) main = do print $ expressionToString (Constant 5) print $ expressionToString (Sum (Constant 6) (Constant 7)) print $ expressionToString (Sum (Constant 2) (Sum (Constant 3) (Constant 5))) expressionToString :: Expression -&gt; String expressionToString (Constant n) = show n expressionToString (Variable s) = s expressionToString (Sum n k ) = expressionToString n ++ " + " ++ expressionToString k 
Homotopy Type Theory is theoretical dependently-typed programming language which can also be used as a foundation for mathematics. It's based on a typed lambda calculus, just like Haskell. Unlike Haskell's core language, though, HoTT is based on a language called Martin-Lof Type Theory. Very similar, not the same. It has dependent types. That means values can appear in the type signatures of other values. So you might define not just a list of integers, but a list of integers that has exactly 5 elements: `Vect 5 Int`. HoTT is a strong functional programming language. That means that every term is guaranteed to normalize (up to belief in a higher power). In particular, the language admits no general recursion. It also bans a certain ill-behaved subset of inductive types (all inductive types must be "strictly positive"... a rough approximation of well-behaved). Unlike in Haskell, where `undefined` (aka "bottom") inhabits every type, HoTT has truly empty types. No objects may be constructed of an empty type. Far from being useless these allow the following: By the Curry-Howard correspondence, types may be interpreted as propositions. The correspondence says that if an object of a given type may be constructed, the type represents a true proposition. (The object serves as a proof). This allows us to define fanciful types. For instance, I might define `Prime p` to be a type which is inhabited only when `p` is a prime number. Or I might define `Nonzero n` to be inhabited only when `n` is non-zero. This allows me to define a safe division function: `div : (n : Int) -&gt; (d : Int) -&gt; Nonzero d -&gt; Int` Which takes two `Int` arguments (named `n` and `d`) and a *proof term* of type `Nonzero d` guaranteeing that `d` is in fact nonzero. Then, I get the result of the division. If, on the other hand, I attempt to call `div 1 0`, I will see the type of the remainder is `Nonzero 0 -&gt; Int`. But I design my `Nonzero n` type so that you can't actually construct an element of `Nonzero 0`. And so, I will never be able to supply the third parameter, and division by zero is impossible. Identity types are a major idea. For every type `A` and objects `a` and `b` of type `A`, we have a type `a = b`. This type is inhabited when `a` equals `b` and empty otherwise. The constructor is `refl`, which will let you build a proof of `a = b` for any `a` and `b` which are "obviously equal". (That is, judgmentally equal). So our proof that `1 + 1 = 2` is simply `refl`, since the compiler knows how addition is defined. That is dependent types in a nutshell. That all applies to Idris and Agda as well as HoTT. The thing that makes HoTT HoTT is that it expands on a question long held by dependent type theorists. What are the inhabitants of `a = b`? The inductive definition of `=` says that there's only one constructor, namely `refl` that we mentioned above. However, it became apparent that you can't actually prove that `refl` is the only proof of `a = a`! More precisely, the statement that couldn't be proven is called the Uniqueness of Identity Proofs or UIP axiom. It says for all types `A`, elements `a` of type `A`, and for every pair of proofs `p` and `q` of type `a = a`, we must have `p = q`. (And since `refl : a = a`, that would mean that every proof `q` must be equal to `refl`). Hoffman and Streicher gave an explanation for why this was impossible to prove using their groupoid interpretation. In this, every type corresponds not to a set, but to a groupoid. And while `refl` is the only inhabitant *guaranteed* to have type `a = a` for every type `A`, it might not be the only one. This is because equality corresponds to isomorphisms in a groupoid, and in addition to the `id` function being an isomorphism, you could potentially have many others. Voevodsky and Awodey took this interpretation to the next step. Rather than go onto the obvious "what about 2-groupoids?", they decided to make a bold next step and go straight for ∞-groupoids. So tl;dr, equality was given a geometric interpretation. A proof that `a = b` becomes a path through a kind of topological space (a groupoid). `refl` corresponds to the trivial path, where you start at a point and don't go anywhere. But other paths might exist, too. The paths are defined up to homotopy. If you know what that is, cool. If you don't, don't worry too much about it. It let's you kind of wiggle paths around and you still consider them equal. It's very important to topologists. Voevodsky also proposed a new axiom to make type theory more pleasant to work with. It's called univalence. The precise statement is a little obnoxious, but it basically says that isomorphic types are equal. (The catch is that two things can be equal in more than one way now). Isomorphism is a "weak" sense of equality that we tend to love. However, it becomes a PITA to work with because, for every property P we care about, we need to show that P "respects" the kinds of isomorphism we're using. With univalence, though, we can promote isomorphism to equality, and by definition, every property respects equality. So the idea for mathematicians is that you can study homotopy and topology synthetically. It gets you some nice properties, because all the functions you can define are automatically continuous and functorial over paths. It also allows you to do category theory in a cleaner way, since you don't need to worry about the distinction between isomorphism and equality. Why do programmers care? Because dependent types are amazing, and HoTT is a big push to get them the attention they deserve. The type systems we use today, even the powerful ones like Haskell, are incapable of expressing basic constraints on programs. (Even Haskell is still prone to buffer overflow). But with dependent types, we can start blending formal verification into our programs. 
(Not an expert by any means, but here goes...) Homotopy comes from trying to look for "holes" in objects. If we have a ball bearing and we pick any two points within it we have the interesting property that we can always draw a path between those two points. This indicates that there are no "holes" (of a certain kind) in the ball bearing. This is pretty obvious—ball bearings are solid. Now if we have a donut instead, we can do the same thing—for any two points we can connect them with a path. But this falls short because we'd like our test to tell us that there *is* a hole in a donut. The trick is to convert our test into a higher-dimensional analogue. Instead of trying to draw a path between two points, we pick two paths and try to draw a "higher-dimensional" path between them. This ends up being a two-dimensional "sheet". As a simple example, consider the paths (a) and (b). ______ (a) ______ / \ + + \ ______ (b) ______ / We can talk about the "path between the paths (a) and (b)" as the "sheet" between the lines—a disc. So if we apply this new test to a donut we'll see that while we can draw a path connecting any two points, given two of those lines we can't always draw a "sheet" connecting the paths. If we really formalize this we can figure out the structure of these sheets and paths and ask direct questions about whether or not a donut has a hole in it. So given these two ideas, 1-dimensional hole searching and 2-dimensional hole searching we, as mathematicians, want to generalize this process. We build 3-dimensional hole searching by drawing "spaces" between "sheets" between "paths". And 4-dimensional searches by drawing "hyperspaces" between "spaces" between "sheets" between paths. Each higher dimensional method is sensitive to "higher-dimensional holes". Ultimately, the collection of all of these tests ends up talking about the exact ways you can *connect* different things in the space of interest. And that's homotopy—it's the method of connecting objects of various dimensionalities, all in a sequence like I described. In particular, you call a "path" a "homotopy between points", a "space" a "homotopy between paths", and so on. This ends up being interesting for HoTT because we'd like to make equivalence proofs more rich. Instead of just having every equivalence proof `p : x = y` end up taking the value `refl` (which is the only way to generate proofs in normal type theory) we'd like to find a natural structure for them. So instead, we take identity to have a structure like homotopy. To prove two "points" are equal we must find a "path" between them. In most spaces (like the ball bearing) there are many such paths instead of *just* `refl`. Then we become interested in the question as to whether two "paths" (two proofs of identity, two values of type `a = b`) are the same, but we've already got a generalization of "path" handy from homotopy—we just look for "paths between paths", our 2-dimensional homotopies, our "spaces". Simple types (all the ones you might normally define in Agda) look like ball bearings, this is because we already know that `refl` is the only value of identification for these types and in ball bearings we can draw sheets between any two paths (in other words, all of the paths are `refl` and thus we can find equivalence between each proof). I've skipped over *lots* of detail, but the idea of a hierarchy of equivalences is a major component of homotopy and a big part of what HoTT takes advantage of.
Cause compared to the defense budget, the science budget is a complete joke.
There is a massive amount of research funded by the DoD into things like slowing bodily functions down, in order to extract soldiers to hospitals. So no, not all the research of the DoD is blowing things up. And while certain areas of pure research struggle to get funding, I am OK with the idea that most funding goes to more applicable areas like biology.
This [5 part series by Ralf Lämmel](http://channel9.msdn.com/tags/ralf-laemmel) on the expression problem and a demonstration of how it can be solved in Haskell. The first part is just an overview of the expression problem and how you might try to sovle it in other languages - another episode closely follows Wadler's original paper, [The essence of functional programming](http://homepages.inf.ed.ac.uk/wadler/papers/essence/essence.ps), on implementing a basic interpreter and incrementally improving it to support additional behaviors via monads and monad transformers.
I would look at SPJ's STM talk.
What are the actual properties of HoTT which makes it better for verified programming? I've often heard that it makes proofs shorter and that it introduces real quotient types, but are there any other "practical" applications yet discovered?
Your right-sided path is still homotopic to the left-sided path. You can just continuously translate it around the edge of the donut until it's on the other side. If I understand correctly, the two classes of paths on a donut that are not homotopic are the ones that form closed loops. You can make a path that goes all the way around the donut, being concentric on the hole, or you can make a loop that goes through the hole like a bracelet.
The endpoints of the path are fixed.
You are on the right track. It's a sort of higher-order continuous function. A continuous function of continuous functions. The idea is, if I have two paths that are given by continuous functions, a homotopy gives me a mapping from some continuous interval to a continuous function which draws a path somewhere "between" these two "outer" paths. As I vary my input, the paths drawn by the output function of my homotopy should move farther from one outer path towards the other outer path.
[Wikipedia's definition of path-homotopy](https://en.wikipedia.org/wiki/Path_(topology\)#Homotopy_of_paths) indicates that it is.
Do you have any specific interests?
I prefer [A taste of Haskell ](http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-tutorial/index.htm) [[video]](http://www.youtube.com/watch?v=jLj1QV11o9g) or [A Pragmatic Case for Static Typing with Brian Hurt](http://vimeo.com/72870631)
If nothing else, univalence can be seen as the missing piece from Martin-Lof's type theory. If you have univalence and the ability to introduce path constructors, you get the "proper" notion of equality of types (two types are equal when they are isomorphic). Univalence might have practical applications since you can use it to transport results between different representations of the same type. For instance, in most DTLs, the fundamental data type is not the integer, but the natural number: 0, 1, 2, ... . The current trend is that natural numbers are defined in unary using two constructors: Z : Nat for zero and S : Nat → Nat for the successor. This representation is, of course wildly inefficient. (Every natural number is essentially a linked list without any data attached, the number itself encoded in the length!) Most implementations special-case naturals, since they are so important, giving them arbitrary precision two's complement representations as a compile-time optimization. However, as with any special case, it only works with naturals. The moral is this: there are often two different representations needed for a data type: one optimized for theorem-proving and another optimized for runtime. The former will usually rely on some clever coding, but that coding ends up infesting the proofs. But with univalence, we can prove a result for the naive implementation then transport it to the optimized version. This nets us a lot of productivity. You also get functional extensionality: if f and g are functions and f(x) = g(x) for all x, then we have f = g. For instance, the functions `\x -&gt; x + 0` and `\x -&gt; 0 + x` agree on all inputs. But they aren't equal in Agda or Coq, because the equality is not "obvious" (they don't reduce judgmentally to the same normal form). Missing functional extensionality has been the "elephant in the room" for a long time. There were three solutions I had heard of up until HoTT. The first was to declare `funext` as an axiom. The problem with this, as with adding any axiom to type theory, is that some terms get "stuck". It breaks canonicity in the language, which is a bad thing, because we could write a program whose type is Nat, but when we ran it, it would error out, rather than returning a natural :( The second solution was to introduce equality reflection. While any judgmental equality was automatically promoted to propositional equality (that's exactly what `refl` does), equality reflection says that any propositional equality may be turned into a judgmental one. The issue with this approach is that type checking becomes undecidable (because to verify something is well-typed, you might potentially need to prove something, thus solve the halting problem). Some people still prefer this approach. I think they are nuts. The last popular solution was pioneered by McBride in his observational type theory or OTT. From what I understand, OTT treats equality in a very different way, shoving equality proofs into a new little universe. I believe it uses the fact that ordinary programs can't inspect the structure of the equality proofs, so in effect, it shields the compiler from those "stuck" programs. HoTT replaces the `funext` axiom with the univalence axiom. `funext` then becomes a theorem, proven with the use of univalence and the interval type, I believe. This may seem like it doesn't solve the problem. But Voevodsky's idea was that, if univalence can be justified in some way, it will result in a much better type theory overall. Currently, there have been two major attempts to reconcile univalence: 2-dimensional type theory by Licata and Harper, and the Cubical Sets model by Bezen, Coquand, and Huber. I don't know much about the former, but the latter has a toy implementation on Github that shows you can program with the univalence axiom without the programs "crashing" on it. Another small win is, as you said, you get quotient types. Quotient types are great for when you have multiple representations for the same data, but you want to treat them as equal. The most familiar example, although not heavily used in programming, are rational numbers: 1/2 = 2/4, even though as ordered pairs, (1,2) does not equal (2, 4). We glue points together, and this is achieved through higher inductive types. A more practical example of a quotient type would be finite sets. You might want to represent a finite set as a list. However, in a set, order doesn't matter, and in a list, it does. So what we do is we "glue together" all lists which are simply permutations of each other. Now, any functions that give some result on [1,2,3] has to give the same result on [3,2,1] because the function must respect the (imposed) equality [1,2,3] = [3,2,1]. Another important consequence is that equality will work properly for *co*data as well. In Agda, we often have to settle for a weak form of equality usually call "bisimulation", where we show that unfolding a piece of (potentially infinite) codata any amount will reveal the same data. Just like functional extensionality, we intuitively feel that bisimulation should imply equality.... but it doesn't! But, almost surely, when we have univalence, we'll also get bisimulation implies equality for free.
[The slides for the first talk](http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-tutorial/TasteOfHaskell.pdf)
Of course you can code the function itself, you just iterate through the infinitely many valid input-output pairs until you find the value that matches your input. (Assuming you can test for equality.) I don't think that you can complain about that implenentation if you start out with the assertion "you can generate infinitely many" anythings.
It's difficult as threads might migrate between cores. Lock striping seems to be plenty fast so unless we're seeing performance problems I prefer to keep things correct in face of thread migration.
I believe the OS used a pair of int64.
It took me a few minutes to understand why forM_Maybe :: Monad m =&gt; Maybe a -&gt; (a -&gt; m b) -&gt; m () forM_Maybe Nothing _ = return () forM_Maybe (Just x) f = f x &gt;&gt; return () is bad, but now I understand that because of the function `f` that is passed in, a chain of `&gt;&gt; return ()` is built up. The function being passes as `f` is \(c, t') -&gt; do when (idx `mod` 100000 == 0) $ putStrLn $ "Character #" ++ show idx ++ ": " ++ show c printChars (idx + 1) t' which as you can see is recursively calling `printChars`; hence the `&gt;&gt; return ()` makes the whole thing non-tail recursive, which is what the article is all about. *Of course*, no one would write a recursive `printChars` like that now-a-days. Instead we just write: import Data.Text.Lens import Control.Lens import qualified Data.Text.Lazy as T import Control.Monad (when) printChars :: Int -&gt; T.Text -&gt; IO () printChars offset = itraverseOf_ text (\i c -&gt; do when ((i+offset) `mod` 100000 == 0) $ putStrLn $ "Character #" ++ show (i+offset) ++ ": " ++ show c) main :: IO () main = printChars 1 $ T.replicate 5000000 $ T.singleton 'x' Which thankfully doesn't overflow the stack. Though I'm not entirely certain how much that is due to edwardk's cleverness.
Wouldn't it be easier to build simpler wrapper libraries in haskell? It seems that all the challenges you mentioned would be even more work with a brand new language. 
Does the naive way still overflow the stack when you turn on `-O2`? Is there a rewrite rule we can put into Foldable that will make it Just Work?
That hasn't been my experience /: Haskell is too giant for me to deal and have control over with it the way I would want/need. The "my own language" approach is actually being much easier than I thought. Haskell really facilitates the work required to do that kind of stuff. Pattern matching, recursive tree datatypes, parsec... everything you need to make a language is right there!
Ooh! That last property about bisimulation is a killer. I didn't realize that was part of the game!
&gt; Of course, no one would write a recursive printChars like that now-a-days. Not true at all, my completely unfounded intuition is that less than 0.5% of Haskell programmers would reach for itraverseOf_ in this case.
Its a great idea. But we haven't got serious work mixing univalence and codata yet, so I guess its got a bit to go. On the other hand, the sorts of models people are building of HoTT over simplicial structures seem like they admit infinite things really naturally, much more so than with a term-reduction style approach?
To be fair, I'm not sure if that last one is guaranteed. The HoTT people haven't seem to have done much with coinduction. However, I'd be extremely surprised if it was not a consequence. (I know OTT, for instance, has both funext and bisimulational equality, for instance).
Sorry. My use of "Of course" was meant to be tongue-in-cheek, but I didn't get that across in my writing. Reaching for `itraverseOf_`, or something equivalent, will be natural in a few years, but it will take time.
To double check how much is due to edwardk's cleverness, I tried subverting him by rewriting most of `itraverseOf_` myself, but it still runs fine: import Data.Text.Lens import Control.Lens import qualified Data.Text.Lazy as T import Control.Monad (when) import Data.Functor.Constant import Control.Applicative import Data.Monoid newtype MonoidApplicative f = MonoidApplicative {runMonoidApplicative :: f ()} instance Applicative f =&gt; Monoid (MonoidApplicative f) where mempty = MonoidApplicative (pure ()) MonoidApplicative x `mappend` MonoidApplicative y = MonoidApplicative (x *&gt; y) mytraverseOf_ l f s = runMonoidApplicative . getConstant $ l (Indexed (\i a -&gt; Constant . MonoidApplicative $ () &lt;$ f i a)) s printChars :: Int -&gt; T.Text -&gt; IO () printChars offset = mytraverseOf_ text (\i c -&gt; do when ((i+offset) `mod` 100000 == 0) $ putStrLn $ "Character #" ++ show (i+offset) ++ ": " ++ show c) main :: IO () main = printChars 1 $ T.replicate 5000000 $ T.singleton 'x' I'm even explicitly using `() &lt;$ f i a` which is one of the things that edwardk cleverly avoids.
I am definitely pushing to get a stable release into Hackage soon (it would be my first). I will definitely keep in touch with some of my suggestions. HsQML has a ton of potential and I think as I use the library more I will get a feel for higher-level abstractions. An HsQML mailing-list may not be a terrible idea :). I am also hoping to get the app running on my RaspberryPi which I think would be pretty cool. I was a bit confused about `EngineConfig` in 0.3. This makes sense and the documentation is pretty clear and pairs well with the examples. Some documentation on RW property mutators would be a big help as well. It was not a problem porting at all. The code is actually more readable. I am especially excited about NOTIFY properties. If you ever have more time to write blog posts I would certainly appreciate that as well.
Since types and values are unified you can write a : Type a = Int -&gt; Int which means that `(-&gt;)` as syntax is a little overloaded. You write a lot more type-level `(-&gt;)` than value level lambdas, so I think it's good to push the syntax that way.
huh? this thing you're rambling on about seems a bit...magical? i can't even figure out what you are describing...a new programming language that magically provides transparent optimizations for android games? 
`refl` is the standard abbreviation for the constructor of an equality type. That is, if `a` is any object, then `refl` is a proof of `a = a`. It's short for reflexivity. And it asserts that every object is equal to itself. The other two important properties of equality are symmetry (that `a = b` implies `b = a`) and transitivity (that `a = b` and `b = c` implies `a = c`). But both of these other facts can be proven using pattern-matching. (Or path induction, as it's called in the HoTT book).
I *think* you get the point, but just to be clear: my `printChars` is not in any way an example of "good coding" or how I'd recommend people implement this function. It was just a simple example demonstrating the stack overflow. I linked to some code that was sent in a conduit pull request that demonstrated the same bug, and recently was sent some code for review that had the exact same `forM_` problem, so I thought it was worthwhile writing up an article explaining the problem, as it seems to catch people.
 $ ghc --make -O2 foo.hs &amp;&amp; ./foo [1 of 1] Compiling Main ( foo.hs, foo.o ) Linking foo ... Character #100000: 'x' Character #200000: 'x' Character #300000: 'x' Character #400000: 'x' Character #500000: 'x' Character #600000: 'x' Character #700000: 'x' Character #800000: 'x' Character #900000: 'x' Character #1000000: 'x' Stack space overflow: current size 8388608 bytes. Use `+RTS -Ksize -RTS' to increase it. As for rewrite rules... I guess it might be possible. But I don't like the thought of relying on a rule firing to ensure I don't get a stack overflow, especially when there are much simpler ways of avoiding it by just changing the code to use `maybe`.
your term "sum-like function" sounds a lot like the normal way the monoidal/semigroup operation is typically presented
:(
I read that post. I wouldn't have guessed that this was your solution, though. A very welcomed surprise. 
Actually, I've always thought of Haskell as being a pretty small language (especially compared to a language like C++). It doesn't really have all that many corner cases and it's pretty uniform (juxtaposition means application, etc). Anyway, it seems like the main difference between writing your own language in Haskell and making a framework in Haskell is that if you write your own language, you need to do everything that you need to do to make a Haskell framework but you *also* need a parser/compiler/interpreter etc. So I'm not sure how that would make things easier... Haskell is really good at EDSLs (and you can even have your own rewrite rules to create new kinds of fusion), so it might be easier to take that sort of approach. Also, I'm sure we could help you out if you tell us some of the issues that you run into.
ou should probably stop right now and consider that every decent Ygame engine is the product of thousands of man years. I know that everybody has the urge to do it my way™ ... but in the end most (not all) will fail because they underestimate the work involved. My advice would be to either support a project like LamdaCube ... Or to try to do a binding to a "real" engine. I'd recommend Unity for two reasons. First, someone already figured out how to call Haskell from the Unity C# code (mobile, no link). Second, I observed that a guy I know created a little game with it in like two weeks. That is a game running on multiple playforms and with ok 3d graphics. You would spend several times as much time figuring out how to do proper OpenGL code if you'd started from scratch (been there ... done that ... never was successful).
That is, I know how to write proper 3.x OpenGL code and even worked on a commercial project involving 3d rendering. But writing a whole 3d-/gameengine alone and from scratch is something I wouldn't dare again.
I only know a little about writing compilers. When you say things like: &gt; In other words, I want the compiler to be smart enough to figure out when you are mapping a function to a 1024x1024 list and do it using GPU buffers instead. without adding any details on how it might be implemented or what the pluses and minus might be to such a feature does not clarify what your current skill level is or what you know or what you do not know. &gt; So, Haskellers, do you think I'm on the right way, or would you this any different? What advice do you have for me? If I was told this plan by any random person I meet at a programming meetup I would think the project is very ambitious and that most such projects end up partially completed and great learning experiences or taking years to finish. One test to see if you are on the right track might be to estimate the difficulty of implementing the some subset of the features you are interested in as an esdl in Haskell. Implement it and see if it matched your expectations. &gt; Yes, there are neat ways to do GPU in HS (LambdaCube/REPA/etc), but this involves a lot of particular knowledge on those libraries. What I need is that things like (map black_and_white colors) :: [Color] are compiled to run in a buffer inside a WebGL, which is obviously far from what I can expect from any Haskell compiler, at least by now, and probably never, considering this would be a semantically wrong translation. Not clear what: (map black_and_white colors) :: [Color] would do in your proposed language. [Accelerate](http://hackage.haskell.org/package/accelerate) allows you to write some code that looks like normal Haskell, but runs on the GPU. dotp :: Acc (Vector Float) -&gt; Acc (Vector Float) -&gt; Acc (Scalar Float) dotp xs ys = fold (+) 0 (zipWith (*) xs ys) I have not used this library much, outside of testing basic functionality, so I do not know if this extends to more complex cases. It is not clear that a Haskell edsl would not fit your needs. I can not tell from your post what your requirements are nor am I an expert in Haskell edsls. 
http://www.infoq.com/presentations/scala-idris It's buried in this talk (which should totally be watched!)
This was awesome.
I think...what you are proposing is more or less identical/isomorphic to having something like data Event' = EventType1 | EventType2 type Event = Maybe Event' and just using the applicative and alternative instances for Maybe? I could be reading it wrong. FRP has someone similar concepts which solve similar problems, but they don't match exactly. In FRP, an Event stream is a stream of discrete events over time. Your `Alternative` instance, for example, is normally represented by merging two event streams: merge :: Event a -&gt; Event a -&gt; Event a where it takes two event streams and merges them into one...the combined one contains events from both streams as they come. You can easily define an Event combinator that takes an event stream and returns a new one that only has the first occuring event, and then that's your Alternative thing. onlyFirst :: Event a -&gt; Event a But your Event model actually corresponds more closely to the idea of an inhibitable Behavior in FRP. A Behavior is a time-varying value (not as "discrete" as an Event, which are blips), and so your Event might be more like a Behavior (Maybe a), where sometimes the event is on and sometimes it is off. You might have a combinator like hold :: Event a -&gt; Behavior (Maybe a) that is `Nothing` at first, then `Just a` as soon as the first event is received. So you might `hold` on all of your input events, creating Behaviors that are either "on" or "off" depending on if their associated events have happened or not. Your Alternative instance then is just a `hold` implemented such that only the first event is held and the rest are ignored, on the merged event stream. Your Applicative instance is pretty straightforward too...well, depending on whether or not inhibition is built into your type. Behaviors typically implement Applicative, so you can do something like behavior1 :: Behavior (Maybe a) behavior2 :: Behavior (Maybe b) f :: a -&gt; b liftA2 f &lt;$&gt; behavior1 &lt;*&gt; behavior2 :: Behavior (Maybe c) (the extra liftA2 is awkward, but sometimes some libraries have inhibition "built in" to the Behavior type, so a Behavior a is implicitly a Maybe a...so you could just do `f &lt;$&gt; behavior1 &lt;*&gt; behavior2`.) In that way, your Behavior now only is Just a when both input events have fired :) (You can turn that back into an Event stream, because most libraries have a combinator that fires an event as soon as a predicate is true, but I think you'd really want a behavior, considering your usage case) Disclaimer -- I am no expert on all things FRP, and FRP itself is under constant change and debate on what it means to be FRP. My experience comes from using AFRP and the Netwire library, and scanning over some documentation from other libraries to make sure nothing I said here deviated too much. ---------- Also, some style points --- I noticed you have a lot of `const x &lt;$&gt; y` sprinkled around. Just wanted to let you know that this might be more concisely written as `x &lt;$ y`. You also have a lot of do ... x &lt;- a return (f x) which might be more concisely written as do ... f &lt;$&gt; a I also noticed a few `f $ g $ h $ x`, which might be a little more cleanly written as `f . g . h $ x` :)
Thanks. I've created a github mirror using darcs-to-git here: https://github.com/komadori/HsQML I'll try and keep it up-to-date.
This problem is severe. Even the `forM_Maybe` version overflows stack if you put an innocuous `return ()` on the end of `printChars`: printChars :: Int -&gt; T.Text -&gt; IO () printChars idx t = forM_Maybe (T.uncons t) $ \(c, t') -&gt; do when (idx `mod` 100000 == 0) $ putStrLn $ "Character #" ++ show idx ++ ": " ++ show c printChars (idx + 1) t' return () I don't think the answer is a knee-jerk change to the combinators. We need to think hard about how to explain the operational semantics here so people can understand memory usage clearly. 
The "Outsiders call it the Tower of the Elephant’s Tusk" line was a very nice touch.
I came from a C++ background. I was attracted to Haskell because it seemed to have all the benefits (in terms of readability, conciseness, etc) of high-level languages like Python, but it was statically typed and compiled to machine code. It made me curious. But it sounds like the OP isn't after a conversion to Haskell specifically.
&gt; In our case, we ended up going with 100% Haste, avoiding javascript frameworks altogether. Do you use any frp for UI work? I saw haste has some lite frp lib, but didn't have chance to use it yet. 
&gt; Isn't that the point of static typing? Not in this case. It's like saying that the type system should prevent a user to enter an integer that represent meters in the type representing Seconds. No type system can stop you doing that, and ultimately that's the user prerogative to use the right type for certain things. If you're not using a LocalTime type with hourglass, you're not using a local time, but "global time". Ok, thanks for detailing some of this. I would go back to the drawing board with some types. May I point out that `localTimeUnwrap` is an escape hatch, so if you use it, it's very likely you would get the wrong thing as you point it out. From my point of view, it mean that this escape hatch should probably be hidden better than in plain sight :-). timeAdd with an elapsed and a month, work, provided you convert your time to a calendar time before adding a "month" which doesn't have a clear seconds value, which is exactly what hourglass is doing. I disagree with your analysis of timeDiff, I think it's perfectly legal to ask the number of seconds between two Date (which happens to means two date in the "UTC timezone"). Note again that `DateTime` is *NOT* a local date in hourglass, however `LocalTime DateTime` is (which is the time equivalent of ZonedTime) I agree with the confusion of localTime; at the moment, `LocalTime` and `localTime` do something very different, and ultimately that's very confusing. I would look at making this better. I'll see from java.time if there's anything worth transposing to hourglass too, thanks for the link.
For the unenlightened among us, care to share the wisdom? EDIT: Well duh, that should've been obvious. Now I feel dumb.
Elephants tusks = Ivory, so paraphrasing, outsiders call it the Ivory tower.
Another way to phrase "ivory tower".
Great summary, but one small point: &gt; Voevodsky and Awodey took this interpretation to the next step. Michael Warren should really be mentioned here as well -- he often gets forgotten somehow because he hasn't been as visible in many ways as Steve Awodey, but he was very much a co-originator of the homotopical interpretation.
&gt;No type system can stop you doing that, and ultimately that's the user prerogative to use the right type for certain things. If you're not using a LocalTime type with hourglass, you're not using a local time, but "global time". Of course it's impossible to make a type system that catches everything, but here the pitfall is too big. In Hourglass there's nothing preventing me from parsing a string "2014-05-06 12:55" without a timezone and then using it as a timestamp. &gt;timeAdd with an elapsed and a month, work, provided you convert your time to a calendar time before adding a "month" which doesn't have a clear seconds value, which is exactly what hourglass is doing. This is a meaningless/dangerous operation. For example, if we have a certain timestamp that represents February 28 in one timezone and March 1 in another, and add a month, we get either March 28 or April 1 – dates differing by a day become dates differing by 3 days. We can only add months to values that have an explicit date element; mere timestamp doesn't have one. Converting it automagically behind the scenes is asking for bugs. java.time has Duration for second-based, physical time intervals (which interprets days as exactly 24h long), and Period for calendar-based date periods (which uses years, months and days only). You can't add a Period to an instant without converting it first into a ZonedDateTime using a timezone. &gt;I disagree with your analysis of timeDiff, I think it's perfectly legal to ask the number of seconds between two Date (which happens to means two date in the "UTC timezone") This doesn't take into account DST. Therefore, difference between two dates doesn't have to be a multiply of 24h. For calculating differences between two dates (without time), there could be a function called dateDiff that returns the number of *calendar* days, but this is not the same unit as *SI* days. An SI day is defined to be 86400 seconds long, calendar days can be of various lengths. &gt;I'll see from java.time if there's anything worth transposing to hourglass too, thanks for the link. That would be great. IMHO, a good clone of java.time would the have functions like the following: -- conversions: Instant -&gt; ZoneId -&gt; ZonedDateTime ZonedDateTime -&gt; Instant ZonedDateTime -&gt; ZoneId ZonedDateTime -&gt; LocalDateTime LocalDateTime -&gt; LocalTime LocalDateTime -&gt; LocalDate LocalDateTime -&gt; ZoneId -&gt; ZonedDateTime LocalDate -&gt; LocalTime -&gt; LocalDateTime -- additions/subtractions: ZonedDateTime -&gt; Period -&gt; ZonedDateTime LocalDate -&gt; Period -&gt; LocalDateTime -- here I'm not sure LocalDateTime -&gt; Period -&gt; LocalDateTime Instant -&gt; Duration -&gt; Instant ZonedDateTime -&gt; Duration -&gt; ZonedDateTime -- differences: LocalDateTime -&gt; LocalDateTime -&gt; Period LocalDate -&gt; LocalDate -&gt; Period Instant -&gt; Instant -&gt; Duration -- other: midnight :: LocalDate -&gt; LocalDateTime noon :: LocalDate -&gt; LocalDateTime now :: IO Instant getTimeZone :: IO ZoneId Note that java.time is less strict at the compile time when it comes to Period/Duration distinctions, but it will throw exceptions at runtime. I don't consider that part to be a good API. 
HoTT researcher here: I've not heard any suggestion that this grant would put any restrictions on how we publish work funded by it. Indeed, a lot of us in the field lean towards the radical end of the spectrum in terms of preference for open access publishing (hence, in part, why we didn't go to a commercial publisher for the HoTT book), and would be horrified if it had any such strings attached.
That's a fantastic operator naming convention! Keyboard-layout-specific, of course, but that could hardly be helped I suppose.
`t a -&gt; (a -&gt; m ()) -&gt; m ()` is basically just `foldMap`.
Still, you were left wondering *for a couple of weeks* till you got any feedback on your patch. So if there was anybody else who had the same idea in the same time-window, he would've reinvented the wheel preparing a patch and submitting it via Email. IMHO, if the patch-queue isn't visible to contributors it *is* less open, as you don't see if patches from other contributors are being silently ignored, rejected with some reason, or how contributions are handled in general. I was under the impression, that due to network effect, most packages on Hackage were hosted on GitHub. However, I don't care much if a project is hosted on GitHub, BitBucket, DarcsHub, or something similar, as long as its patch/issue tracker is publicly visible and allows to track how and why the package evolved.
Good to know, but that doesn't address my primary concern about patch/issue tracking.
A question for you, since you seem to understand this stuff. Elaborating on the homotopy/equality type correspondence, can you give me a motivating example? I'm thinking about how for example the fundamental group of S_2 is trivial, but S_1 (or any space with a single hole in it) is Z. If the inhabitants of type a = b can be varied and we associate them with homotopies, is there a trivial example of a non-trivial equality type like S_1 in algebraic topology? Something with more than just `refl`? EDIT: roconnor mentioned in another post that univalence means that types equal up to isomorphism are equal, which means that for example a type like Bool could be equal to itself in two different ways (one that maps True to True and False to False, and the other that maps True to False and False to True). I understand this but intuitively it seems like these maps would be, uhh, diffeomorphic for lack of a better term, but I say intuitively because I really don't understand how one defines a diffeomorphism in deptype land.
&gt; Unfortunately due to RAM constraints, it only supports up to 6 arguments. I read that and thought "What‽ How could partial function application be limited by RAM?" Then I read the source and became enlightened. Very nice. 
No.
It's not bad in normal code, just a trick used in lens. Since `MonoidApplicative` can be made entirely an internal type (it's called Traversed in Control.Lens.Internal.Fold) then we can fudge its Monoid instance and save an `fmap (const ())` for each step in the traversal then just throw the (totally bunk) wrapped value away at the end with `void`.
Can't the sigma type be encoded via a pi type, in the same way you can church encode a pair via lambdas in the lambda calculus? 
Cargo cult monadolatry. 
For hardcore C++ you could consider [Bartosz Milewski's Programming Cafe](http://bartoszmilewski.com/), in the spirit of "interesting ideas from functional programming" as opposed to "aggressive advocacy"; let Bartosz explain things and let the reader independently notice that what takes a ream of C++ code is three lines in Haskell.... here's a [specific example](http://bartoszmilewski.com/2014/02/26/c17-i-see-a-monad-in-your-future/) of the sort of post I mean. To be clear, the word "hardcore" is not redundant... that's not for "casual" C++ users.
One of the most basic things you should be able to do with proper row-variable polymorphism is add or remove a field, like this: dropX :: {x :: a | r} -&gt; {r}
I did find ["Why Haskell is Great - 10 minutes"](https://www.youtube.com/watch?v=RqvCNb7fKsg), which seemed so very promising...
Nothing wrong with it. It's just that `() &lt;$` is the applicative version of `&gt;&gt; return ()` which is what was causing all the trouble before, but it isn't a problem in this context.
Right, your actual example that you link to is a little more compelling. Though in your particular case, and not knowing anything about conduit myself, I would replace `maybe (return ())` with `maybe (return s)` and that way you won't even be tempted to use `F.mapM_`. :D 
Not really unfortunately. I'm looking for something quite simple though. Thanks
Diagrams seems to be really well organised, with lots of information on how to get involved and general documentation. Thanks
I can confirm: $ /nix/store/hz2h1zcmpywp9f30wdq48mbp91pd11wh-ghc-7.6.3-wrapper/bin/ghc --make -O2 foo.hs &amp;&amp; ./foo [1 of 1] Compiling Main ( foo.hs, foo.o ) Linking foo ... Character #100000: 'x' … Character #1000000: 'x' Stack space overflow: current size 8388608 bytes. Use `+RTS -Ksize -RTS' to increase it. versus $ /nix/store/rddla85zgryvw9jc0kx6v85anw28rfkr-ghc-7.8.2-wrapper/bin/ghc --make -O2 foo.hs &amp;&amp; ./foo Character #100000: 'x' … Character #5000000: 'x' 
The default stack size was massively increased in 7.8: http://www.haskell.org/ghc/docs/7.8.1/html/users_guide/release-7-8-1.html I don't know why the supposedly good `forM_Maybe'` would crash for you on 7.6 though.
What xss vulnerability do you see in the scotty docs?
These days it's possible to implement new primops in Cmm without modifying the compiler. So it's much easier to experiment with implementing ideas like this.
I got the non-recursive cases now, but dont know how to tell haskell to read x-expression and divide it into smaller expressions. pseudo-code: expressionToString (Data Expression) = expressionToString (Data Expression)-1 
Isn't water an element rather than an industry? Is it patented yet?
&gt; Something with more than just refl? The circle becomes a data type with higher inductive types. It consists of just one "point constructor" named `base`. It also has a "path constructor", which introduces a new path (proof of equality) named `loop`. This `loop` has type `base = base`. The path constructors introduce new paths/proofs of equality to a data type which is not judgmentally the same as `refl`. However, there are some extra steps to go through to show it isn't *propositionally* equal. For instance, another higher inductive type is the interval. It has (point) constructors `zero` and `one` and a path constructor (I've seen it called `seg`). But this space is contractible, and so it's possible to prove that `seg = refl`, even though the compiler can't figure it out if left to its own devices. The other non-trivial equality proofs / paths come from the univalence axiom. Just like you mentioned, `Bool` has two isomorphisms. When you hit either isomorphism with univalence, you get a path. It turns out one of those paths corresponds to `refl` (namely, the path corresponding to the identity function). And again, you can do a small bit of theorem proving to show these paths are not equal. The notion of diffeomorphism doesn't really come up. Hell, even topological notions like "homotopy" don't really mean the things they do in HoTT. All the topology lives inside the model. When you work with the type theory, it's all invisible to you, and you just worry about typechecking. That's the beauty of it. You don't worry about defining groupoids or paths or fibrations or anything. You just pretend you're working with types, equality proofs, and dependent function types. 
Okay, how in the world do you know addresses into your /nix/store for hitting particular ghc versions? That seems so inconvenient.
Hmm. I guess that came off as more critical than I intended. &gt; The author explained that true heterogenous list if overkill for this problem. Punting on solving a problem that isn't worth solving is a fine decision! &gt; Are saying this is bad? Nope. Seems reasonable. &gt; What are the smells that he shares specifically? Using typeclasses where a type would do and types where a value would do. &gt; Of course, but what interface should you present clients of the code. I was saying that the solution to the existential typeclass problem would involve a type that was literally just `IO ()` and wouldn't be very interesting. The solution is more interesting when the typeclass is more complex. &gt; Being able to just call render on a type seems like a simple solution to a simple problem. Yep. Sharing the `render` identifier rather than writing a bunch of `renderGame`, `renderFoo`, etc. methods seems nice. My experience with the existential typeclass antipattern is... sometimes you just want to share an identifier. And that seems fine.
[Github Repo](https://github.com/purescript-contrib/purescript-react)
Some people, when confronted with a problem, say "I know, I'll write a compiler from scratch and solve the problem in that new language." Now they have two problems. I can't say that I recommend this solution. Very rarely is it an effective use of time. It's worked out a few times, but those are very much the exception to the rule. If it works out, usually you'll have written a language that's much different from existing languages - for example, Erlang was written because there weren't any languages with the kind of concurrency model they required. I've personally seen a few people try this, and I haven't yet seen a case where it's been a clear win. Usually, it ends up with having sunk a lot of work into the compiler for relatively little benefit. It's a great idea if you're interested in learning PLT and this is a fun side project, but I'd caution against taking this as a serious approach for solving a real problem. &gt; it is nowhere near trivial running Haskell on the browser, creating 3D games for Android and so on. But, more than that, this will have a focus on games I'd also be vary wary if you want to write a one-man compiler for writing mobile games. You'll want to make executables that are 1) fast and 2) have a fairly small memory footprint, because you'll be running them on underpowered systems with little memory. Do you honestly think that you'll make something competitive with compilers that have probably had nearly a man-century of work on them?
&gt; We need to think hard about how to explain the operational semantics here so people can understand memory usage clearly Also, it is important to explain the purpose of folds. They are recursion schemes and thus should be a *replacement* for explicit recursion. It shouldn't be natural to make recursive calls in the body of forM_ and I think it also should not be encouraged (e.g. by optimizing combinators for this kind of situation). The reason forM_Maybe works is just that the Maybe Functor has such a ridiculously degenerate fold. I think when doing explicit recursion, pattern-matching is perfectly acceptable. And in order to avoid explicit recursion in the current example, one should of course traverse the text, be it with itraverseOf_ or just simple (mapM_ print . zip [1..] . T.unpack). However, locally replacing combinators with more general versions for the fun of it, even though the code specifically deals with a concrete type like Maybe, does not seem like a good idea; I wonder why people would think it is; it feels like a real misunderstanding to me. After all folds are very fundamental to functional programming and everybody should learn them properly and know what they are about and are not about. Am I wrong? (Sorry if I come across a little rude; I read the post this morning and something bothered me about it all day long, no harm ment:) EDIT: mispelled itraverseOf_
I don't think it's a consequence. You get coinductive "extensionality" (with or without univalence) if and only if your coinductive types are homotopy-terminal algebras. I formalised some of this stuff in HoTT [here](http://github.com/pcapriotti/agda-base) (see [`mext`](http://paolocapriotti.com/agda-base/container.m.extensionality.html), for example). If you don't have the eta law for coinductive types at least in its propositional form, then I think it's consistent that bisimilarity doesn't imply equality. For example, the discrete simplicial set consisting of the natural numbers plus two infinities should satisfy all the rules of nu X.1+X, but clearly violates extensionality (the two infinities are bisimilar but not equal).
To clarify the original problem: these are cases where a fold isn't an option. I know the simplified example I gave in the article *can* (and *should*) be handled by a fold, but the kind of activity I've seen this bug pop up in is, for example, traversing a directory structure, where there's a function: nextFile :: DirStream -&gt; IO (Maybe FilePath) I would probably say that explicit pattern matching or the `maybe` combinators are the *right* way to do this. But I've also seen a number of people using `forM_` since it leads to shorter/easier to read code. So this article is meant to (1) warn people of the dangers of doing this with recursive functions, and (2) mention that there's a possible workaround in mono-traversable if we change type signatures.
Well, I only address the nix store directly when I want to play with something not in my default environment. For more serious work I would write a nix expression parametrized by a ghc package. I include the nix store path in my post so that, in principle, other nix users could compare my hash to theirs to start to understand why they might get different results from me. And, in principle, the hash can be used later to reproduce the build, though it isn't quite so simple in practice at the moment. (also, the haskell-text dependency was taken from my environment, so this is a pretty lousy example of using nix to reproducible results.) $ nix-build '&lt;nixpkgs&gt;' -A pkgs.ghc.ghc782 /nix/store/rddla85zgryvw9jc0kx6v85anw28rfkr-ghc-7.8.2-wrapper $ `nix-build '&lt;nixpkgs&gt;' -A pkgs.ghc.ghc782`/bin/ghc --version The Glorious Glasgow Haskell Compilation System, version 7.8.2 $ `nix-build '&lt;nixpkgs&gt;' -A pkgs.ghc.ghc763`/bin/ghc --version The Glorious Glasgow Haskell Compilation System, version 7.6.3 
Touché :-)
Either way it's O(n), which seems like "you're not doing it right". More interesting, I think, would be an efficient resizing snoc for mutable arrays, like C++STL `vector&lt;T&gt;`. This can grow the array by some multiple of its current size, for amortized O(1) snoc. Perhaps the 'unconstructed' thunks can be initailized to blackholes, to be overwritten with an actual thunk when they are allocated?
Ok. Your Expression type isn't a list. It's a tree, and Haskell comes with a marvellous facility for decomposing expressions called pattern matching. Lets say I have a worthless tree, with neither data in the trunk nor the leaves. data Tree = Tip | Bin Tree Tree Lets say I want to print it. To follow my own advice, I will deal with the base case, then the recursive case showTree :: Tree -&gt; String showTree (Tip) = "Tip" showTree (Bin l r) = "(" ++ showTree l ++ "," ++ showTree r ++ ")" The important thing to note is the second equation. It matches the Bin case and binds l and r to the two arguments to the constructor. They have type Tree and showTree has type Tree -&gt; String So you can apply showTree and get a string back. You just join the strings up, and you are done. I am sure with a little imagination on your part you can do figure out how this applies to your own expression type.
Ah, those docs. The examples in the haddocks use text, which sets content type to text/plain. The only active static analysis project I know of is Liquid Haskell: http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/about (link currently down) For taint checking in particular, I think making proper newtypes would be about as easy as using a static analysis tool. Tracking "taint" on inputs isn't even the right thing to do, if you have multiple outputs with different escaping requirements (e.g. SQL vs HTML). For an example, postgresql-simple newtypes ByteString as [Query](http://hackage.haskell.org/package/postgresql-simple-0.4.2.1/docs/Database-PostgreSQL-Simple-Types.html#t:Query) to guard against SQL injection.
&gt;fmap works on Either something What if I want to fmap over a Left value? What if I have a type that has many values inside it? Do I need a different fmap for every one of those values?
Hm, I see. So the current case would suggest that Foldable types ~~with a fixed maximum number of~~ at most one element should be treated specially (and probably only generally Foldable with a newtype wrapper). I wonder if that's what these `affine traversals' are about, that pop up sometimes in the recent lens discussions. 
I had thought about this stack size increase and preformed two naive tests. 1) I changed 5000000 to 1 billion and decreased the print rate by a factor of 10. main = printChars 1 $ T.replicate 5000000 $ T.singleton 'x' All version had the same response, they died with the message killed at 118M. No stack overflow message. 2) I used criterion to look at the performance characteristics on the default sizes. All reports similar means(all within a std dev from each other) and similar std devs. Anything else I should try?
the term reduction approach admits co-data perfectly well. You just have to be willing to talk about co-term reduction.
&gt; &gt; fmap works on Either something &gt;What if I want to fmap over a Left value? What if I have a type that has many values inside it? Do I need a different fmap for every one of those values? You can't fmap over the left value, because of how the Functor class is defined. Basically, the functor class is class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b Now that looks a little odd, because Either is of kind * -&gt; * -&gt; * (it has two type variables), whereas Functors have to be of kind * -&gt; * (it has one type variable). The reason we can define a Functor instance for Either is that we've partially applied the first argument: instance Functor (Either l) where fmap :: (a -&gt; b) -&gt; (Either l) a -&gt; (Either l) b Since this works via partial application, fmap has to transform the last argument. Fortunately, though, there's a structure very similar to a Functor that gives us what we want: class Bifunctor p where bimap :: (a -&gt; b) -&gt; (c -&gt; d) -&gt; p a c -&gt; p b d first :: (a -&gt; b) -&gt; p a c -&gt; p b c second :: (b -&gt; c) -&gt; p a b -&gt; p a c Now, with Bifunctor, we can map over either the left or right, as we prefer.
&gt; Do I need a different fmap for every one of those values? "Sort of". There's only one `fmap` per type, so in that sense there can be no "different `fmap`" for a type... there is, by definition, just the one. You _could_ newtype your way to a different type that has a different `Functor` instance. I don't know of anything off the top of my head that does exactly that, but [Data.Monoid does something very similar with Monoid](http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-Monoid.html#v:Sum) over some types that have multiple plausible `Monoid` instances. ([Do check out the source to see how it's done](http://hackage.haskell.org/package/base-4.7.0.0/docs/src/Data-Monoid.html#Sum); it's just a handful of lines.) Probably nobody does this because if you've got "multiple `fmap`s", rather than a newtype, you probably want to provide some sort of lens to the various bits, which could be anything from a standard record accessor up to the lenses you've probably seen /r/haskell talk about before. (If not, ask away and somebody'll explain.) The `Either` instance is special in that I'd personally say the `Functor` instance for `Either` is "`Either` in its capacity as an error type", rather than "`Either` as a type in general". When using `Either` as an error type, `fmap` operates over the "correct" value and leaves the "error" value alone. Considered abstractly as just an `Either a b` with no further context about handling errors, there's no compelling reason to favor either for the `fmap` and I would probably not implement a `Functor` for that "in general" as a result, personally. YMMV.
You may not even be able to do it through `Functor` in the `Either` case. `fmap` for `Either` must have the type: fmap :: (a -&gt; b) -&gt; Either x a -&gt; Either x b So you can't really map over a Left value. The closest you can do is making a function `Either a b -&gt; Either b a` and sandwiching the `Maybe` inbetween. The point of `Functor` is that, whenever you have a type constructor `f`, there is exactly one sensible instance for `Functor`, if there is any sensible instance at all: the type just constrains it that way. That means that now, you can activate the `DeriveFunctor` extension in GHC and it will be able to make the `Functor` instance just like it also makes `Eq` or `Show`. So you get `fmap` for your datatypes for free. This is not the only thing that makes functors useful, but it's neat. The reason that is like that is the polymorphism. Let's look at the type of `fmap` again: fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b It must work for any `a` and `b`, and it is allowed to change the type of the functor. That means that if we have `data Foo = Foo a a` we *can't* make a fmap that applies the function to only the first argument: if it did that, the type of the first and the second won't match. It also shows up in the `Either` example: `Either x` can't be an instance of `Functor` mapping on the left because fmap must work for any type, while `Left` will always have an argument of type `x`. Something similar happens with "type with many values in it": you must map on all of them, or otherwise you can't implement `fmap`.
Thanks for trying it out and providing the command to change the stack size. With that stack size I now get an overflow error for all of the functions above on ghc 7.8.2. ghc -O2 Main.hs --fforce-recomp -rtsopts ./Main +RTS -K8m Here is the gist: https://gist.github.com/Davorak/9e5def76c17c20d24bcc If something is different in my development environment that is causing this disparity I want to find out what it is. I have tried the two function provided on the school of Haskell site and I see the difference there, but I have yet to replicate on my linux vm or my mac.
Works for me: $ cat foo.hs &amp;&amp; /nix/store/rddla85zgryvw9jc0kx6v85anw28rfkr-ghc-7.8.2-wrapper/bin/ghc --make -fforce-recomp -O2 -rtsopts foo.hs &amp;&amp; ./foo +RTS -K8m import qualified Data.Text.Lazy as T import qualified Data.Foldable as F import Control.Monad (when) forM_Maybe :: Monad m =&gt; Maybe a -&gt; (a -&gt; m ()) -&gt; m () forM_Maybe Nothing _ = return () forM_Maybe (Just x) f = f x printChars :: Int -&gt; T.Text -&gt; IO () printChars idx t = forM_Maybe (T.uncons t) $ \(c, t') -&gt; do when (idx `mod` 100000 == 0) $ putStrLn $ "Character #" ++ show idx ++ ": " ++ show c printChars (idx + 1) t' main :: IO () main = printChars 1 $ T.replicate 5000000 $ T.singleton 'x' [1 of 1] Compiling Main ( foo.hs, foo.o ) Linking foo ... Character #100000: 'x' … Character #5000000: 'x'
Could boxed Arrays perhaps carry an optional bit vector specifying which cells are not yet initialized (which can be discarded when all are initialized)? For initialized arrays, it would only cost an extra bit test per array.
Thank you for your patience I have spotted my mistake. I was not properly recusing in in my alternative functions.
&gt; you must map on all of them, or otherwise you can't implement fmap. That seems so weird to me. I tried: data Goo a b = Goo a b a b deriving(Show) main = print $ fmap addOne (Goo "Hello" 2 "world" 4) addOne :: Int -&gt; Int addOne x = x + 1 instance Functor (Goo a) where fmap f (Goo a b c d) = Goo a (f b) c (f d) ...and that compiled and worked. But: instance Functor (Goo a) where fmap f (Goo a b c d) = Goo a b c (f d) -- f is not applied to b value ...makes the compiler output an error message.
By the way, instead of: main = print $ fmap addOne (Goo "Hello" 2 "world" 4) addOne :: Int -&gt; Int addOne x = x + 1 You can do: main = print $ fmap (+1) (Goo "Hello" 2 "world" 4) 
&gt;&gt;you must map on all of them, or otherwise you can't implement fmap. &gt;That seems so weird to me. More explicitly, you must choose one of the type parameters, put it in the last position in the declaration and map over all occurrences of it using the same function, and leave the occurrences of the other parameters unchanged. or leave them unchanged. In your first `fmap` for `Goo`, you chose `b` and left `a` untouched. From that angle, the situation with `Goo` is very similar to that with `Either`; the only difference is that there are more places for you to apply the function to. As for the second example, you will see why it doesn't work if you try to writing down the types involved in the `fmap`. Either you will give a too restrictive type for `f` or you won't be able to give a type to the result.
Yeah, in the case where `fun :: Y -&gt; Y` this issue doesn't directly show up. Generally with things like definitions of `Functor` and `Monad` special cases should be ignored and the most general behavior is what should guide you. The dissonance about whether or not `b` and `d` are fixed and whether they're both of type `R` is exactly why this situation, where `Functor Goo` is defined incorrectly, doesn't work out. In correct definitions `b` and `d` necessarily have the same type and you know it to be the second type following `Goo`. Nothing goes wrong and types check out.
As a side note, Bifunctor will probably be part of the Base package next release. http://comments.gmane.org/gmane.comp.lang.haskell.libraries/21930
How can you "generate" input-output pairs without coding the function? If you have all the pairs, you have the function: just do a lookup. Now, it get interesting if the function is not computable.
&gt; Why do we have them? To make sure fmap doesn't sneakily do anything behind the scenes or change anything we didn't expect. They're not enforced by the compiler (asking the compiler to prove a theorem before it compiles your code isn't fair, and would slow compilation down - the programmer should check). This means you can cheat, but that's a bad plan because your code can give unexpected results. I think that rather, having theorem proving in Haskell would make types more complicated. The trouble isn't slowing compilation, but making the code too complex for mere mortals. I would choose having the compiler enforce fmap invariants over "fast compilation", if that was the only concern.
I like Rich Hickey's "Simple Made Easy", which is basically just a modern take on "Out of the Tarpit".
As another side note, you can also use `EitherR` from [the `errors` package](http://hackage.haskell.org/package/base-4.6.0.1/docs/Data-Either.html#t:Either) to swap the type variables and get the `Functor` instance for the other type variable.
Thanks. Any chance for a readme? The home page is great, but it'd be nice to have it duplicated (or an abridged version) along side the code.
I ended up writing a generalized version of `HoleyMonoid`... `HoleyCategory`! But it turns out that `HoleyCategory (-&gt;)` still isn't the same as my partial application DSL.
Maybe I'm spoiled, but I tend to like links to content, not links to pictures of content that mostly lack context clues for where that content came from. For the uninitiated, this came from [LYAH chapter 9](http://learnyouahaskell.com/input-and-output#files-and-streams), which covers IO.
not exactly a haskell problem - see http://en.wikipedia.org/wiki/Interpolation (a bit longer answer: you cannot find *THE* definition of a function because there is not only one function that will have theses values at those points - but you could try to interpolate the values for new points or try to approximate it by some simple function) maybe you could write a bit more on the background of this question (homework?) 
I'm sorry, but I can't look at this list without thinking ["That's numberwang!"](https://www.youtube.com/watch?v=qjOZtWZ56lc)
Well there's an infinite number of closed forms expressions one could write down that satisfy this. 
Sure, here's an example of printing all of the files in a directory tree: import Control.Exception (bracket) import qualified Data.Foldable as F import Data.Streaming.Filesystem (closeDirStream, openDirStream, readDirStream) import System.Environment (getArgs) import System.FilePath ((&lt;/&gt;)) printFiles :: FilePath -&gt; IO () printFiles dir = bracket (openDirStream dir) closeDirStream loop where loop ds = do mfp &lt;- readDirStream ds F.forM_ mfp $ \fp' -&gt; do let fp = dir &lt;/&gt; fp' ftype &lt;- getFileType fp case ftype of FTFile -&gt; putStrLn fp FTFileSym -&gt; putStrLn fp FTDirectory -&gt; printFiles fp _ -&gt; return () loop ds main :: IO () main = getArgs &gt;&gt;= mapM_ printFiles This is actually pretty similar to the code someone sent me the other day. You'd have to have a pretty deep structure to cause a stack overflow, but this implementation *is* less efficient than using explicit pattern matching on `mfp`.
Put another way, I don't think the problem as stated (i.e., without any more constraints) would ever likely give a satisfactory solution. Using your example, I could correctly reply with this: `f 0 = 0` `f 1 = 2` `f 2 = 1` `f 3 = -1` `-- snip` `f 30 = -14` `f _ = 0` For any given set of known input/output pairs.
The problem, of course, is that we need to make sure the recursion of `printChars` uses tail calls. That is, it's necessary that `forM_Maybe` performs the desired tail call (if we want to write in this style), but doing so not sufficient to eliminate all stack overflows resulting from the use of `(&gt;&gt; return ())`. We could push the problem further away by using `forM_Text` to capture the recursion in `printChars` and ensuring that `forM_Text` performs the desired tail call. Though, again, this won't eliminate all stack overflows, since we could `forM_` over some `f Text` where we call `printChars` on each `Text` but then call `return()` afterwards. Thus necessitating a tail-calling `forM_f`, etc all the way up the call stack. Still, this is an excellent point to bring up. Details like these are very tricky if you don't have a good internal model of how HOFs and TCO work.
You could probably find the smallest one, though, if not analytically then perhaps genetically.
Copy all of that, paste it into some file. Done, there's your function f. Now seriously, doesn't the Yoneda lemma say something about understanding x if you know all of the ways you can get x? Additionally, you could graph it and see what equations can emulate it (this is what scientists do when trying to figure out equations to explain some physical phenomenon). Spreadsheet programs tends to have such tools.
https://research.microsoft.com/en-us/um/people/sumitg/pubs/synthesis.html
Best answer so far, thanks!
[EASTL's vector](https://github.com/questor/eastl/blob/master/vector.h) costs a fixed `sizeof(void*) * 2` more bytes than a standard heap allocated array if you know the size you will be filling the array to. Yes, if `snoc` is your only operation for creating/growing dynamic arrays, you waste O(n) space on average, but real implementations almost always include "reserve" and "resize" operations which allow you to exactly specify the size you want allocated.
&gt; The problem, of course, is that we need to make sure the recursion of `printChars` uses tail calls. Right. The intriguing thing is that they should be tail calls not with respect to the usual evaluation order, but with respect to `(&gt;&gt;)`! Thus in `printForever = print "Hello" &gt;&gt; printForever`, `printForever` is in "tail position" for this purpose although strictly speaking it is not in tail position in the expression.
&gt; (e.g. Bool is isomorphic to Bool in 2 different ways) Does this mean the two different mappings: 0 -&gt; 0 1 -&gt; 1 or 0 -&gt; 1 1 -&gt; 0 ?
Perfect example, thanks. This is reasonable-looking recursive code. The recursive call to `loop` is in "tail"[1] position so we'd expect everything to work fine, but `forM_` does not actually perform a tail call for us. I still find it rather mysterious that *ignoring a value* should consume stack space. [1] cf. http://www.reddit.com/r/haskell/comments/24t0sj/foldablemapm_maybe_and_recursive_functions_school/chbmngh 
Lagrange polynomials.
smallest as in... least number of haskell characters? what an arbitrary metric :) also, I'm pretty sure such a problem is impossible as a genetic algorithm because the search space (characters in a Haskell program, and function values) is probably as discontinuous as it gets. 
Yeah, it's very frustrating that there's no way to ignore a value without consuming stack space. I'd imagine we could achieve that somehow by including `void` in the `Monad` typeclass (or `Functor` for that matter), but I'm not even sure if that's true. I actually thought that a specialized definition of `&lt;$` for `IO` might be what's missing, but haven't figured out a way to do that yet.
Not sure about the rest, but because of how big SAP is in large enterprise environments, ABAP actually ends up being a pretty major language. It's just completely insular, so SAP-heads know nothing outside the SAP world, and nobody else really has much of a clue what the SAP world is doing. Source: I worked as an ABAP developer for a while a few years back.
very insightful. nice work :p
Thanks ... and thanks for introducing me to the concept ;)
https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects#Arrays It seems like they already do, as in a card table. 
I'm a bit confused of what you want todo. Do you wanna process mail from a network port that speek smtp to its clients? Ie you don't wanna send mail but recv mail?
Yup. I don't want to send mail. Just receive mail, parse it, and store it in postgres. But I need to do this concurrently so that while a mail is being processed, the server is still open to receiving new mails.
Checkout https://github.com/alexbiehl/postie. It uses the pipes library to stream the incoming mime messages to user code. It's quite rough though. Pull Requests addressing cleaner connection handling, AUTH command implementation appreciated. I wanted to write a streaming mime parser, but went with mime-package in the end. 
There are several ways (mentioned elsewhere in this thread) to do that, but what you have to keep in mind is that a Left value is typically viewed as an error. I'm sure someone has come up with a reason they need to write a bunch of code that maps over a series of applications that all fail, but it shouldn't surprise one that this isn't the optimized (for usage) case.
So you would likte to rewrite [insert well known smtpd here] in haskell? Depending on what you actually want to do (or must do) you can cheat a bit. Would it be possible for you to use an existing smtpd and then 'snatch' the msg into you own server written in haskell ? (Take a look at the perl thing http://smtpd.github.io/qpsmtpd/. )
I'd reconsider your architecture. How about using Postfix as MTA, and a custom 'mailbox_command' for delivery, which is some Haskell program which does... anything you like.
Interaction is valuable, but we should try and reduce the need to answer the same questions over and over as well. Let's try and get this populated? http://en.wikibooks.org/wiki/Haskell/Resolving_Cabal_Hell
For me the symbol does change, but it takes up a single character rather than 2.
procmail. you can pipe messages to a haskell prog
Why not make the Action Table a type class instead? Then you can have multiple implementations based on type
That sounds like it would be the Existential Typeclass Antipattern http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/ 
Here is the architecture of a large haskell code base that is over 20 years old and used by pretty much anyone who uses haskell: http://www.aosabook.org/en/ghc.html
Kripkenstein would like a word.
Are there well–recognized collections of patterns and anti–patterns for Haskell? If so, where?
In all seriousness, that's not a very helpful answer to the questions most people are asking. Compilers are trivially pipelines, and I don't think people struggle with data coming in one end, getting chewed on, and output coming out the other. (Even if that isn't exactly how GHC happens to work, conceptually I think the point stands.) I think people are asking about programs that must _interact_ with IO somehow.
Perhaps for a function like this yes, but trying to do so in general will hit up against some hard problems in information theory pretty quickly.
Are you saying, for example, STArray s Int (STRef s DataStructure) Doesn't `STRef` introduce an extra layer of indirection/boxing? And again, what do you do about the 'unallocated' elements, to avoid the GC wasting time there? EDIT: Oh, I see... I think you meant `STRef s (STArray s Int DataStructure)`, which doesn't seem as bad to me. But natively supporting `growArray#` or something like it on mutable arrays still might be useful.
link back up now. #heartbleed etc. :)
I've found Data.Sequence to provide good performance for every application for which I've needed `snoc` in a pure context, although in cases where lookup is very common but combining is rare enough I suppose an optimization of `concatArray :: [Array] -&gt; Array` would be quite welcome.
One general thread is that Haskell has less need for patterns and antipatterns due to patterns being more easily embedded in general libraries. So, on one hand you can go use some nice libraries and use them to drive your patterns. There are a few lists of good libraries around. On the other hand, there isn't a good set of documents which outline these "patterns" and "antipatterns" abstractly. As another point, the "existential antipattern" is a pretty catchy name but it doesn't go without getting some flack. Existentials are great features for certain cases, but they are often turned to at times where the strength-to-weight ratio is out of whack. Though, I suppose you could say the same about standard antipatterns in any language, so maybe it's just my "Haskell needs none of these *pattern*-things" bias creeping in.
In the [Cmm primops source](https://github.com/ghc/ghc/blob/master/rts/PrimOps.cmm) a new array is initialized with a simple for loop. LLVM could vectorize that later on, though I wouldn't bet on that. 
procmail is overkill for that task. every mta has that capability build in (e.g. milter).
I would write it as 0 &lt;-&gt; 0 1 &lt;-&gt; 1 and 0 &lt;-&gt; 1 1 &lt;-&gt; 0 but otherwise, yes.
try to write a lmtp daemon instead. that's easy to hook into virtually every serious mta in the world and you won't start a new process for every mail received (as some alternatives will).
Yep ... i actually was aware of that. But I left it in for brevity ;)
What type should the dispatch be based on? 
You could avoid this by making Interface a typeclass too, not sure why one would do that though. 
Haskell needs more real world and awesome applications. Many, many people in the industry are convinced that it's not productive or possible to do in Haskell.
Thanks I'll have a proper read when I have progressed with learning Haskell. I enjoyed [Jekor's code deconstructed videos](https://www.youtube.com/watch?v=FEFETKhhq8w&amp;list=PLxj9UAX4Em-Lz5btngxMVZxf_B44GETVz), especially the one on XMonad. Though I don't have enough experience to get the full benefit I am sure.
&gt; What I think I can't have, is a result-type for the elimination that depends on the pair's values. Is this the expressiveness you mean? Yes. This is what makes it a sigma type.
"This was our paradox: no course of action could be determined by a rule, because every course of action can be made out to accord with the rule." (How do you know that the definition of f is f x = floor(tan(x*x-3)) and not f x | x &lt;= 30 = floor $ tan (x * x -3) | otherwise = 4 ?)
Oh, I was fixated on the additional laws, and wondered how the semigroup laws enter. Thanks for the clarification. Maybe I was fooled by the Haskell way of declaring these principal constraints in the class definition while imposing constraints in the instance is only necessary for the methods defined therein. 
Sounds very interesting. It would be nice if there was a book like "Parallel and Concurrent Programming in Haskell" but focused on these advanced type system topics.
You want better? Feel free to start writing a replacement yourself. If cabal is bad enough, I'm sure you'll get some help.
The community *is* working on cabal. Releases keep coming and things keep getting better. One thing you should understand: cross module inlining means that the problem cabal is trying to solve is much harder than the one npm or pip or gem have to solve. I have to say that I haven't had a cabal problem in a long while. ghcjs is pretty bleeding edge from what I understand. I can't speak about fay or haste as I have no interest in either. Feel free to get on github and join the mailing list. EDIT: one thing I must take exception to is language like 'getting anything to work in Haskell is a pain'. I am offering a free hangout / helpout to anyone looking to understand how to get things working with cabal on Linux.
You may be interested in [a previous comment I made about npm and cabal](http://www.reddit.com/r/haskell/comments/24ddu3/z/ch6pg2y) - npm solves a much easier problem and only seems to work better, when it reality there are bugs waiting until you run your program. At least cabal is upfront with you if things are going to go wrong! 
If you install everything - and I mean *everything*, including binaries you want on your PATH like happy - in a sandbox, you should be fine. I haven't had a cabal hell experience since I started doing this when sandboxes came out. 
Is the key behaviour of Constructible captured by instance Constructible t where wrap :: a -&gt; t a null :: t a Of course, your interface allows a few other options like putting an alternative to the left, completely throwing away the input, etc, but those seem unwanted. EDIT: oops, null was original listed with the wrong type above.
Do you think Cabal will do this by default eventually?
I don't believe so. How would this map to construct?
 construct f = wrap &lt;$&gt; f &lt;|&gt; pure null Basically, from a "f a" you can only get "f (t a)" by either fmapping something of type (forall a . a -&gt; t a), or having some null values of type (forall a . t a) - and then you can stick those "f (t a)" together however you like with the alternative bar.
Putting this [comment of mine](https://pay.reddit.com/r/haskell/comments/244num/would_someone_kindly_provide_a_certified_tutorial/ch3q4gj) from one of your previous threads into a script will get you the first part. I haven't gotten around to the GHCJS bit, still (I've been busy), but I'll put some time into it.
I hope it will - I want it to go even further and never install stuff globally. `cabal init` should make a sandbox as well. If you try to `cabal install` outside a sandbox it should do something like [this script I wrote](http://www.reddit.com/r/haskell/comments/24ac3h/z/ch6pqrt). I think doing these will make everything a lot easier, especially for beginners. 
What didn't work?
Yes, I plan to move the documentation on my web-site into the repository and extend it into a proper manual.
I actually originally created this because `ghc-mod` and some other development tool (I can't remember what) wouldn't install together :p The downsize is I have a very, very large `bin-sandboxes` folder.