Also, when you mean the `sendfile` system call, is it a Haskell system call or the C system call?
Why are you so concerned with efficiency? The RPi can only read from the SD card at like 15MB/s, and any USB HDD would share a single USB 2.0 bus with the slow 100Mb/s Ethernet controller. You're not going to transfer data much faster than 10MB/s and you'd have to do something fairly spectacularly inefficient to be CPU/memory limited there. Just use the web application framework you're already using, you'll likely max out the I/O without much special effort.
I think the main point is not to avoid annotations within function declarations, but rather to allow things like lambdas and typeclasses.
Pretty sure he means this: http://man7.org/linux/man-pages/man2/sendfile.2.html
Correct. The simple-sendfile package provides a Haskell binding to that, and that's what we use in warp.
U mean like `sortBy` with an alternative implementation for `Compare`? 
Great posts! I believe you have some typos in the `parseTmIf` function in both Token parsing sections in the B language. You have some `&lt;*` that should be `&lt;*&gt;`
Yes I realize that, which is why I'm including the constant factors since that's the only thing you can mess with on this problem.
Great! Funny thing is that ByteString.pack is also O(n) so if you have to convert then you're not helping asymptotics, but the constant factor might be better.
For `String` no. But for `Text` or `ByteString` you could compare the beginning and the end of the string in the same pass and get n/2 worst case. 
Thanks! I've been using Haskell for a long time, and I never realized this. I guess because a lot of functions like `head`, `last` and `length` *are* O(1) I just assumed that indexing was also. 
A simple example stolen from [this paper](http://research.microsoft.com/en-us/um/people/simonpj/papers/gadt/implication_constraints.pdf) : data T :: *-&gt;* where T1 :: Int -&gt; T Bool T2 :: [a] -&gt; T a f1 (T1 n) = n&gt;0 What should the type of f1 be ? The following type signatures are both valid and "most general" in a sense, and neither is an instance of the other: f1 :: forall a. T a -&gt; Bool f1 :: forall a. T a -&gt; a We conclude that f1 has no principal type. Jacques Garrigue has a similar example for OCaml on slide 3/20 of [this presentation](http://gallium.inria.fr/~remy/gadts/Garrigue-Remy:gadts@aplas-slides2013.pdf).
Now that is useful!
Couldn't it also be `T Bool -&gt; Bool`? Any examples for total functions? This just seems like a corner case because you omit the T2 case. It's kind of like saying, more bad things happen when you do something bad. 
Thanks for this! I have several codebases that have several of these ideas partly implemented, like trying to differentiate async exceptions. Having this in one library will make it much easier to apply these practices consistently. The icing on the cake is that its using the typeclasses from `exceptions`, which is great because some of my larger codebases sometimes use the IO-based exception functions and `exceptions` in other places. This is going straight into my custom prelude.
He's complaining about you including the "O", not about you including the constant factors. It's fine to care about constant factors -- you just can't use big-O notation at those times.
&gt; For `String` no. I disagree (though I haven't benchmarked). /u/codeonwort, it's possible to model this problem with a nondeterministic pushdown automaton as follows: * There are 3 states (1, 2, and 3) * 1 is the start state * 3 is the accept state * There are 5 possible transitions: * From state 1, push the current token onto the stack and advance the input stream; go to state 1 * From state 1, push the current token onto the stack and advance the input stream; go to state 2 * From state 1, advance the input stream and go to state 2 * From state 2, if the token on top of the stack equals the current input token, pop from the stack and advance input; go to state 2 * From state 2, if both the stack and the input stream are empty, go to state 3 This can be simulated. Since basically every transition advances the input stream, you just need one copy; you need a set of stack-state pairs (more usefully, in this case, a map from tops of stacks to pairs, or some kind of trie). If your set ever becomes empty, you can reject; if you ever have state 3 in your set (or equivalently, you reach the end of input with an empty stack in state 2), you can accept. The non-deterministic algorithm works in _n_ steps (as opposed to _2n_ of your original), but the simulation "wastes" some effort going down false paths. The worst case is a string of the same letter repeated many times, and it is at least quadratic in this case (I don't _think it's exponential, but it might be). There may be some way to optimize this using ideas from [this](http://matt.might.net/articles/parsing-with-derivatives/) post.
This would have been literally perfact for me about a year ago. Still might rewrite to use it even now.
Hmm. I didn't think it was technically necessary to omit constant coefficients, etc. in big O notation. I'd love to see conclusively if that's true.
Yes
It would be easier to come up with an alternative way, to avoid making those +by methods. No?
That's also not total though.
Hmm. Well that certainly seems like coarse-grained "recovery" to me. It's pretty much the kind of thing I was thinking of as the only thing you can reasonably due in response to a program bug. (Kind of analogous to GUI applications with a top-level catch-all block which throws up a dialog saying NullPointerException or whatever...) Could this be structured as some kind of supervisor task which watches the request tasks and sounds out the generic "Hoogle couldn't perform your query, sorry" message when one of them fails? (My intuition would be that "fine-grained" error handling (recovery of recoverable errors) is performed in-task, while "coarse-grained" handling is performed at the task boundary (doing something about threads which failed due to unrecoverable errors)... though I'm not sure how meaningful this distinction is in a language like Haskell where all tasks have a single shared (mutable) heap anyways.) 
Unfortunately it does not wok for some reason. Decided to use sdl2 package instead. And it works just fine. I guess the SDL package is a bit out of sync with homebrew.
Agreed. We have places where we catch everything but async exceptions (I wish this package existed back when we wrote that code) and we definitely don't want to catch these two. They are often thrown during shutdown if you don't kill your threads in the exact right order. Catching them and restarting the thread (which is what we were doing) would have resulted in infinite loops.
True, but start [here](https://docs.google.com/forms/d/1xZP8LeXQz4AK931rl7dK62BBmz-w7_iVPyriBI5sUjQ/viewform) to vote.
Yes, exactly the same issue. Mutable cells should never be polymorphic (as that lets you choose one type when you write to them, another type when you read). The value restriction prevents mutable cells from generalizing in `let` to become polymorphic. `newIORef`, by virtue of having to be bound to a lambda, prevents `let` generalization of the created IORef (which is why Haskell does not need the value restriction). Once you allow `unsafePerformIO`, you can have `let` generalize your `newIORef` result, to become polymorphic and then you can `unsafeCoerce`.
Doesn't polymorphic recusion lead to non-principal types? https://en.wikipedia.org/wiki/Polymorphic_recursion
Thanks, that explains it perfectly!
I'm glad you liked it! Thanks heaps for pointing that out, it should be fixed now. If I get time I'm hoping to split that post (and bits of the post about parsing and printing the I language) into mostly self-contained intros to `parsers` and `ansi-wl-pprint` and then the content specific to the languages themselves. Hoepfully the intro posts will be useful on their own, and having that split will also open the door to setting the parsing and printing pieces as exercises before I actually get to the content.
Have you looked into Template Haskell? You could also try and make your own little code generator that runs before you compile your application. Just define a type for a column and write a function that generates Haskell code from a value of that type. Then just stitch those together into a Haskell source file and compile it with the rest of your app. I know this sounds kind of dirty, but I've actually done something similar to this in a [library](https://github.com/frontrowed/stratosphere) of mine and I am very pleased with the results. The code generator takes a JSON file with a specific schema as input and produces a Haskell module as output. Whenever I update the JSON files I just run the generator again and commit the output to git. For example, [this JSON file](https://github.com/frontrowed/stratosphere/blob/master/gen/models/resources/ec2-instance.json) gets turned into [this Haskell module](https://github.com/frontrowed/stratosphere/blob/master/library-gen/Stratosphere/Resources/EC2Instance.hs). This approach is also used in amazonka, but the author of that library has a much more type-safe code generator.
For one thing, consider using [`IntMap`](https://hackage.haskell.org/package/containers-0.5.7.1/docs/Data-IntMap.html) ;) It's used the same as `Map Int v`, but obviously optimized for `Int` keys. Another thing, consider using a parametric column type. newtype Column a = Column (TVar (IntMap (TVar a))) This means that all your other `*Column` types can be collapsed into one, with common functions shared between them. `NullableIntColumn` is `Column (Maybe Int)`, for example. It also doesn't make much sense to have a different map for each row. This makes no guarantees about whether columns are in sync or not. I think it'd make more sense to get rid of `Column` altogether and structure it like so: newtype Table a = Table (TVar (IntMap (TVar a))) type ExampleTable = Table ExampleRow data ExampleRow = ExampleRow { rowA :: Maybe Int, rowB :: Float } createNewTable :: STM (Table a) createNewTable = Table &lt;$&gt; newTVar Map.empty You can see how this reduced the boilerplate down to just one `newTVar` call per table, and it's polymorphic on the type of table. As for the rest of the records problem, you'll be a lot happier using the [`lens`](https://hackage.haskell.org/package/lens) library. It's got a steep learning curve if you're trying to understand the internals. But likely all you need to know is how to use it to make records cleaner to work with, which is easy. [Here's a simple tutorial](https://hackage.haskell.org/package/lens-tutorial-1.0.1/docs/Control-Lens-Tutorial.html).
Is there any reason that libraries like those don't use [`MonadRef`](https://hackage.haskell.org/package/ref-fd-0.4.0.1/docs/Control-Monad-Ref.html)? Or is `MonadRef` just not popular enough to warrant use? I just can't help but feel that these libraries based references would be better served allowing the user to decide on what style of references to use.
in the Warp `sendFile` interface what's the "hook action" of type IO() and the other argument of type `ByteString -&gt; IO ()` ?
What is good about this?
all the stuff
This seems to be the most convincing use of comonads I've seen, which traditionally I've found hard to motivate.
I think the question was pretty clear... In the context of the problems that coeffects solve, are the solutions that we already have better or worse? I.E. Why do we want coeffects?
Thomas Petricek presented this in a talk at the NY Haskell meetup last week. One small thing he mentioned that others (including myself) may not have known about is the [Implicit Parameters extension](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#implicit-parameters) in GHC.
This discussion seems to be confounding the exception/error distinction (which this library explicitly is _not_ discussing) and the sync/async distinction. The purpose of the sync/async distinction is that async exceptions are a signal from external to your current subroutine that this thread needs to die. Trying to stay alive despite that message is buggy behavior (consider `timeout` and `Control.Concurrent.Async.cancel`). By contrast, in the case of `BlockedIndefinitelyOnMVar`, there is absolutely no external authority demanding the death of your thread. There's an exception being raised because you're misusing an `MVar`, true, but that's coming from actions you're taking in your own thread. It pleases no external entity to force-kill the entire thread because you have a programmer bug. All of the responses here seem to be saying "you shouldn't recover from MVar exceptions because they're errors." Maybe I'm misunderstanding those comments, but such logic is contrary to the way exceptions are intended to be used: the ability to recover from any synchronously generated exception, whether a programmer error or otherwise, is a good thing.
I broke it. let dyn x = ?fst + ?snd in (let ?fst = ?other in dyn 0) + (let ?snd = ?other in dyn 0) gives types @?fst:num,?other:‘a,?snd:num⊢let dyn=… in …+…:num I expect `?other:num`. If I set `?fst=1, ?snd=2, ?other=3`, I get 9 as expected, but if I set `?other="hello"` I get `NaN`. 
Could be interesting to detect pure/return via their type though, rather than name. If you have a function of type: (a -&gt; m a) with up to Monad constraints on the m, then you can consider it a "return". For example, here: ($) return :: Monad m =&gt; a -&gt; m a No special casing needed, it's the type of return so it must *be* return. This would currently break down due to fail: const (fail "boo!") :: Monad m =&gt; a -&gt; m a But that will be fixed once fail is taken the hell out of Monad. 
I was going to say the same thing; I've rarely seen such an accessible web page made for a popular language nevermind a research language, and a simple explanation of type theory notation to boot! This choice: &gt; Short is good! You can always come back. I'm practical! Show me more examples. Love theory! Give me all the equations. Show me all! Time is not an issue. Is particularly innovative for a material presentation, I've often considered a book like that for Haskell; one tailored to different backgrounds and learning styles.
Actually I found it rather difficult to read. I was never able to understand in which order I must read the two columns and the ability to expand / collapse details break my linear mental flow. So I guess we are not equal when confronted to an UX ;)
http://www.issihosts.com/haveged/
Definitely, I think the "I need some context to achieve my goal" for e.g. time is very intuitive for regular developers.
This is an amazing exposition. Can someone explain the connection to structuring with Implicit Parameters + phantom types in Haskell? There seems to be a lot of overlap but I can't quite place my finger on it.
Won't hold up to criticism from all programmers, but it's nice publicity. Gets people talking; it's easy to forget that functional programming is still new to many people. I like the guy presenting, he has the enthusiasm of SPJ and the style of David Attenborough.
Since you bring up Midori, I assume you've seen [this](http://joeduffyblog.com/2016/02/07/the-error-model/) post by one of the developers. For others reading (/u/snoyberg maybe?), I previously summarized the article in a [comment](https://www.reddit.com/r/haskell/comments/4nv96r/four_months_with_haskell/d4a9kfa?context=3); I reproduce the summary here: --- Personally, I like the error model described [here](http://joeduffyblog.com/2016/02/07/the-error-model/) (long read; skip to the conclusion): * Sometimes, return codes are used (`Either` and `Maybe` are just improved versions thereof) * Other times, for recoverable errors, _checked_ exceptions are used * Unchecked exceptions are uncatchable and terminate the process (so are referred to as "panic" rather than "exception" to avoid the implication that catching is possible) * To avoid checked exceptions and return codes but still handle a swath of failure cases, the function can take a handler object as a parameter (possibly in a `Reader` monad)
http://blog.huoc.org/system-f.html mentions at least: simple typed lambda calculus: λx → x can be typed as `int -&gt; int` or `bool -&gt; bool`, but as there is no polymorphic variant, there isn't principal type. system f: -- | if-then-else choose : ∀ a. bool → a → a → a choose p x y = if p then x else y id : ∀a. a -&gt; a id x = x expr = λb. choose b id expr : ∀ a. bool → (a → a) → (a → a) -- that what -- or alternatively expr : bool → (∀a. a → a) → (∀a. a → a) Neither type subsume other one, so there we got an expression without principal type again. Second example is what you can do with `RankNTypes` in Haskell: λ Prelude &gt; :set -fprint-explicit-foralls λ Prelude &gt; let choose b x y = if b then x else y λ Prelude &gt; :t \b -&gt; choose b id \b -&gt; choose b id :: forall a. Bool -&gt; (a -&gt; a) -&gt; a -&gt; a λ Prelude &gt; :set -XRankNTypes λ Prelude &gt; let expr :: Bool -&gt; (forall a1. a1 -&gt; a1) -&gt; (forall a. a -&gt; a); expr b x = choose b id x λ Prelude &gt; :t expr expr :: forall a. Bool -&gt; (forall a1. a1 -&gt; a1) -&gt; a -&gt; a Note, GHC would refuse to type-check `expr` if it's not eta-expanded!
When you're already using a data structure, where random access is O(1), why not design your testing algorithm specifically around that single operation, to gain most performance from that behaviour, like so: import qualified Data.ByteString.Char8 as B isPalindrome :: B.ByteString -&gt; Bool isPalindrome input = (not . B.null) input &amp;&amp; palindrome' 0 (B.length input - 1) where palindrome' x y | x &gt;= y = True | otherwise = (input `B.index` x == input `B.index` y) &amp;&amp; palindrome' (x+1) (y-1) Haven't done any comparison, but please do let me know 
&gt; But that will be fixed once fail is taken the hell out of Monad. Amen to that.
I've slowly been creeping into functional programming from a strong C background (It still is my go to, and probably will remain so), but the functional mindset for programming has helped me much. Since diving into functional languages like Haskell and ATS, I'm much more conscious of how I design certain things, how I manage variables, and how I go about writing most functions. This has been a great help especially to multi-threaded programming. I don't like this example because I feel it is feeble. It skips over a big deal of Haskell's modus operandi by glancing at the functional aspects by instead focusing on terse code (which is still readable, I must say), and the example he chose strikes me as rather trivial. If I had seen this before really diving into functional programming, my response would have been "so, who cares," because it seems more to rely on built in features rather than the programming methodology. Saying your going to sum the squares of even elements in a list (array, vector, whatever) in one line showcases built-ins to me because this is easy in MATLAB as well for one line sum(ns(rem(ns,2)==0).^2) Then we just read it from inside out. rem(ns,2)==0 Form a logical vector where the elements divisible by two are marked TRUE ns(rem(ns,2)==0) Index into ns to get the elements we want ns(rem(ns,2)==0).^2 Square each element individually sum(ns(rem(ns,2)==0).^2) Sum each of those. This does the same thing by relying on various built in features and syntactic sugar really. In relation to the Haskell example, you don't see the more functional pieces (namely, the map function &amp; **HOW** filter_evens or sum works) unless you know how to see them already. Both showcase terse code, which can be found all over anyhow. What would have been nice to seen for a quick example of haskell is how to build up to something like the filer_evens function or that sum function. On their own, those appear superficially trivial, but building up to them shows the difference of functional programming pretty clearly, impresses upon the viewer how refinement based programming mentalities are pretty straightforward in functional styles as opposed to imperative. From there, it's easier to extrapolate to something like a multi-threaded process, or how easily one can build up complex things in functional style. All this said, I feel my words are colored by my biases by coming from really a hardware design level up. But if we're going to showcase Haskell and other functional languages, I feel it is imperative to highlight the mentality behind it, how it makes for clean and easy design, and why people should care.
Though arguably that's still 1. Coupling the various separate pieces of logic, and 2. Actually writing "Haskell code" except within JavaScript.
LINQ was actually influenced by Haskell (Erik Meijer).
I'd suggest to advertise the concurrency/parallelism support and how one can saturate a GPU with Haskell to do various things. Currently, and this may change, machine learning seems to concentrate a lot on targeting GPUs. So why not write blog posts and articles for general developers.
If you want to write functional code in JS, sure. I believe the point of the video was to demonstrate the conciseness of FP in general, with Haskell being the example language shown. I'm also not sure JS would have those functional features if it not (at least partly) due to Haskell, anyway. 
There does seem to be a lot in *flat* coeffects that can be achieved with typeclasses plus something like [reflection](https://hackage.haskell.org/package/reflection). I don't see how structural coeffects would work though.
No, obvious examples that take implementations as parameters are fmap, fold, orderBy, ...
I'm reminded of [this](https://arbital.com/p/bayes_rule/?l=1zq) project, which attempts to build documents explaining mathematical concepts automatically tailored to reader level.
Hearing him explain what his one-liner does makes me wish the pipeline operator was idiomatic Haskell. To explain his expression, he has to read from right to left. It'd be the same with a pointfree version too. sum (map (^ 2) (filter even ns)) sum . map (^ 2) . filter even Why flip everything around like that? Write expressions that read left to right like people are used to. ns |&gt; filter even |&gt; map (^ 2) |&gt; sum
I want to caution that if we only advertise the exotic things that Haskell can do then people will think it's an exotic language unsuitable for normal programming. Sometimes it's important to emphasize that Haskell can do ordinary things just like other programming languages.
I got the checker stuck in an infinite loop with the dataflow language using the y combinator: let y f = let z x = f (x x) in z z in fun n -&gt; y (fun m -&gt; n + prev m / 2) Had to kill my browser.
This looks really nice, thanks for putting in the time and effort to think this through and come up with a library. I would be really interested to hear if Simon Marlow has seen this and has expressed any opinions about it.
It's hard for me to find links in Reddit text, so here they are in easier form! * [Stefan's blog post with details](http://stefanj.me/funblocks/) * [Link to try it out](https://code.world/blocks)
not true - a few years back Erik was much more visible on Channel9 and stuff - I think it's fair to assume that many interested C#/.net programmers know him from either LINQ or Rx (btw: he even had an Haskell-tutorial series there that later got ported to Edx)
Would `f1` really typecheck? Would this be valid?: f1 :: forall a. T a -&gt; Bool f1 (T1 n) = n &gt; 0 val :: T Int val = T2 ([1,2]) res :: Bool res = f1 val Based on types alone it should typecheck, but it'd obviously fail since the T2 case is not being handled. Idem with this: f1 :: forall a. T a -&gt; a f1 (T1 n) = n &gt; 0 val :: T Int val = T2 ([1,2]) res :: Int res = f1 val Couldn't the typechecker realize this and assume the only possible type is `f1 :: T Bool -&gt; Bool`?
When it comes to deadlock detection, I think the distinction between "my bugs" and "other people's bugs" is not clear. Verifying that code is deadlock free requires non-local reasoning so it's not clear where responsibility for the bug begins and ends.
I've seen `&amp;` used for that. Can't remember the reasoning, though. I'd have a function with that type (but maybe with `|&gt;` as the name/symbol) in my personal utility library (which I keep thinking I should make). Likewise the S combinator from SKI calculus (`s f g x = f x (g x)`) which I keep needing, but can't find a good name for.
I won't speak for Simon on it, but I did email him to ask some advice on some design points. He did give me a few other pointers on the library.
`&amp;` is the name for the operator in `Data.Function`. And the S combinator has two names in `base`, `ap` and `&lt;*&gt;`. 
Very good luck and Thank-you. 
I think it'd be fun to have a tutorial that writes this first without any library funations, then shows how to rewrite it with eg. custom mapSquared and filterEven functions, then how to factor those into map and even, then how those are both folds, a then finally using Mu combinators to factor recursion out of the fold, so that you only define recursion once for multiple different types of data. This might even exist somewhere.
Ah, thanks. The reader monad gets me every time. And Hoogle doesn't suggest it, of course. Then again, it didn't even find `&amp;` from `Data.Function`, so maybe I expect too much of it.
Not really. Difference is bottom-up, top-down approach. f = filter even &gt;&gt;&gt; map (^2) &gt;&gt;&gt; sum Take even numbers; square even numbers; sum squared numbers. f = sum . map (^2) . filter even Sum square of even numbers. Or: *f* is sum. Sum of what? Squared numbers. What numbers? Even numbers.
What about people that actually use right to left languages?
Try it with the streams example and see what happens.
If only there was a section like "Why languages need coeffects?" which discussed issues like this, or some longer article, like say, even, a phd thesis or icfp paper, or both, which addressed this in more detail. But nope, I guess the best a poor reddit user can do is ask for what such things _would_ say if only they were made available for us.
There's one linguistic issue that I wonder about, namely where the fellow says "filters out the even numbers". "Filtering out X", whenever I've heard the term used, has meant that the filter catches all the X and lets everything else pass--but the argument to filter returns true for the items that are let through, not those that are not. Folks who teach Haskell: how do you describe filter to your students? Do you avoid saying "filter[s] out"? If I ever teach Haskell, or even if not when I just discuss Haskell, I'd like to describe code as clearly as possible.
This is what Michael had to [say](https://groups.google.com/d/msg/yesodweb/uh2utnMB3xo/yznzPJh7CwAJ) the last time it was discussed. IMO, these numbers can definitely be improved if someone has the time. 
The top-down approach fits well with Haskell, in my opinion, because of its laziness. Suppose you have `f [1,2,3]` and you want to know its value. What's going to happen (and I must say conceptually, because GHC is a really clever compiler) is somewhat like this: `sum` asks whether the list it was passed is empty or not. In order to find out, it asks `map` whether it results in an empty list, which it cannot answer by itself, so it asks `filter`. `filter` will then look for an element that satisfies `even`, and if there is one, it returns a reference to that element and the filtered list after that. So actually, it is not really flipped around when you consider laziness. Luckily, you don't normally have to think about those things when you write programs in Haskell, and actually I think both styles are useful. For example, there's a 'pattern' where you have some data type with a bunch of fields that may need to be forward compatible. Letting the client code use the data type as a record is fragile, so instead, you export a bunch of setter functions. That is one situation where '`&amp;` chaining' works well: defaultThing &amp; withName "foo" &amp; withBla bar &amp; withBaz [qux] It depends :)
I can't even find Yesod on the benchmark anymore.
I don't know about combinators, arrows, monad transformers and all that fancy jazz that haskell people always talk about even though I have a good grasp of the basic language constructs. A tutorial that gives a motivation for using higher level abstractions and also explaining them (basically what you said) would be great. Maybe you can write one? Please?
I agree, but I wouldn't say "sum the squares of the even numbers in a sequence" is a particularly ordinary thing to do. If you can come up with a real-world example where that operation makes sense, by all means phrase it as a real-world example. I'm absolutely certain you know this already, I'm just pointing it out for other people.
Yeah, the name `filter` is ambiguous. When teaching it, I usually describe it as “a filter that *admits* only the things we say we’re interested in”, like a coffee filter. We don’t call it a “grounds filter”. :) But in that way I think another term would be more illustrative, such as `select`, `keep`, or `where`: select even ns keep even ns ns `where` even Because even nowadays I screw up my `filter` conditions! 
How would I apply recursion using these blocks? It doesn't seem that I can define a block in terms of itself :(
I couldn't watch the video immediately so I had to look it up: ----- **Quipper** is an embedded, scalable functional programming language for quantum computing. It provides, among other things: - A high-level circuit description language. This includes gate-by-gate descriptions of circuit fragments, as well as powerful operators for assembling and manipulating circuits. - A monadic semantics, allowing for a mixture of procedural and declarative programming styles. - Built-in facilities for automatic synthesis of reversible quantum circuits, including from classical code. - Support for hierarchical circuits. - Extensible quantum data types. - Programmable circuit transformers. - Support for three execution phases: compile time, circuit generation time, and circuit execution time. A dynamic lifting operation to allow circuit generation to be parametric on values generated at circuit execution time. - Extensive libraries of quantum functions, including: libraries for quantum integer and fixed-point arithmetic; the Quantum Fourier transform; an efficient Qram implementation; libraries for simulation of pseudo-classical circuits, Stabilizer circuits, and arbitrary circuits; libraries for exact and approximate decomposition of circuits into specific gate sets. http://www.mathstat.dal.ca/~selinger/quipper/
This version is extremely early, functions and recursion aren't there yet , but will be covered soon enough. 
Excited to see to see the end result!
Yesod probably failed to build. It does that a lot, and we had to rush the last round out so I didn't notice it. Looking at the results, I don't see Spock either, so the script to install `stack` probably got broken. Unfortunately, I'm not in a position to fix these things anymore.
btw, Adam has a presentation on infoq about implementing Tor on top of HaLvM: [Tor in Haskell &amp; Other Unikernel Tricks](https://www.infoq.com/presentations/tor-haskell)
Does anyone care to post an example zero-to-halvm-ec2 script? I imagine we could do some code (shell script) golf and get this in around a dozen reasonable lines such as: sudo apt-get install ghc haskell-stack haskell-cabal wget $halvm ... install halvm ... cabal update &amp;&amp; cabal install ec2-unikernel echo $hello_world_HaLVM &gt; demo.hs halvm-ghc demo.hs ec2-tool-thing demo
would Unchecked exceptions in this model be allowed to run "cleanup finalizers", before terminating the process?
I hate using verbs, so I'd probably call `filter` `only`. As in `only even xs`.
This is such an annoying thing. "What are you programming your project in?" "[Some functional language]". "Oh, functional, right?" "Yeah." "Why don't you just use [some OOP language (usually Java)]? It has map, fold, etc. too. No reason to use such an esoteric language like that!" And then I just shut up because any possible explanation of how my preference is for an actually functional language, and how map, fold, etc. don't make a functional language, is just going to make me sound like a functional elitist to them. 
&gt; I would not expect "wai" to be beat by go, for example. Out of interest, why not? Go itself, as well as its `http` library, were built by Google, the biggest web company ever, specifically for them to build servers with. Seems like it'd be embarrassing for them to be beat by what's essentially been a research project for the last 20 years.
I learned a lot from this article like `existentially quantified types`, `reification` and `skolemization`. I have seen some examples of these in other places before, but I either didn't bother to look carefully at what is being done or didn't understand enough to know the nuances/theory behind it. 
(Y)
Any particular reason for the dislike of verbs? I’ve been thinking about this recently while designing a standard library for a language, and I’d be curious to hear your reasoning. In my mind, there’s a mapping between concepts in natural language and concepts in programming. Nouns are values, verbs are functions, adverbs are higher-order functions (`map` = `listwise`, `zipWith` = `pairwise`, &amp;c.). Of course, I came up with this independently, thinking I was clever, then found out that the APL/J people have been talking that way for years. :P 
I was referring to the linked tool from adam...
What? Erlang doesn't perform significantly better than Haskell in any of the tests, and the only test where there is a significant difference it's [Haskell that's faster by a factor of 3](https://www.techempower.com/benchmarks/#section=data-r12&amp;hw=peak&amp;test=json&amp;l=28). Further, all this talk about parallel garbage collection seems odd given that Java, which also uses a parallel garbage collector, has an entry in the top 5 of all but one of the tests. Oracle even refers to Java's parallel garbage collector as the "throughput collector".
Im using haskell [scotty] for the web frontend on a website serving billions of pages per day and it has great performance. It took a lot of tweaking to get it working well. Probably the techempower.com guys didnt understand how to tweak haskell for high web performance.
FP Complete’s Hoogle database is more comprehensive, and [it finds `&amp;` without issue](https://www.stackage.org/lts-6.5/hoogle?q=%26). :)
Ok, looking good. But please help me to **link** my project for using this library! I downloaded this library to `./vendor/haskell-opencv` and created empty sandbox in my project: `cabal sandbox init`, and `cabal add-source /vendor/haskell-opencv`. Then I copied cabal.config to `./`, and configure and install: `cabal configure`, `cabal install --only-dependencies -j9`. Ok. Now I try to build my project, "imglearner": `cabal build imglearner`, and it fails with link error: Linking dist/build/imglearner/imglearner ... /usr/lib/gcc/x86_64-pc-linux-gnu/5.3.0/../../../../x86_64-pc-linux-gnu/bin/ld: /home/dima/datamarket/.cabal-sandbox/lib/x86_64-linux-ghc-7.10.3/opencv-0.0.0-A3LWDYfNc6jCHV6EnYa8nI/libHSopencv-0.0.0-A3LWDYfNc6jCHV6EnYa8nI.a(Internal.o): undefined reference to symbol '__gxx_personality_v0@@CXXABI_1.3' /usr/lib64/libstdc++.so.6: error adding symbols: DSO missing from command line How to fix this link error??
- https://en.wikipedia.org/wiki/Discordian_calendar - https://en.wikipedia.org/wiki/Discordianism
I also try build with `cabal build imglearner --with-ld=/usr/bin/g++ --with-gcc=/usr/bin/g++` with the same result :(
Try to add `-fno-exception` to `CC-Options`: same result, link error.
&gt; Note that Java is in the top 5, not number 1. &gt; Any language that uses garbage collection will naturally perform worse than an language with explicit memory allocations and deletions. Please just look at the actual results. Java is number 1 in one of the six tests, while it holds the number 2 spot in two others. One of those number 2 spots is behind another GC language. Out of the six tests, four of the number 1 spots are held by GC languages.
I agree. Java &amp; C# are bloatware, and they bolt on paradigms. As a maths grad we work with set theory, functions &amp; compositions of functions. Its been around for centuries. OO is new by comparision.
[I just wrote one.](https://gist.github.com/evincarofautumn/e6b8688a8413d45c6010abc9f3f25029) Didn’t get into implementing `filter` &amp; `map` in terms of folds, nor general recursion combinators. But let me know what you think of it. :)
The same could be said about Javascript (with Google's V8). But nobody's implementing performance-oriented applications in JS.
&gt; Out of interest, why not? Because Go used to be much slower than Haskell, and I suppose it still carries that poor performance stigmata with it ...
source on the GPU thing? haskell's GPU support is still quite flaky an limited to one or two slow DSLs afaik.
Do you have any mobile friendly or app plans?
Check out some of the previous discussion about this. https://www.reddit.com/r/haskell/comments/1bqsvi/web_woes_for_haskell/ The executive summary is that these benchmarks are mostly tests of database bindings and JSON serialization. Actual web framework overhead doesn't play a major role.
Wrong Haskell.
Hi, thanks for the paper. Hazelnut is a structured editor, that that tries to syntactically and semantically leave the program in a proper state, which sounds very useful. Currently this project is much less theoretical, and is more focused on providing a friendly user experience to younger students, so that they can more easily understand some of the underlying concepts of programming. Alternatively you can take a look at [Viskell](https://github.com/viskell/viskell) which tries to explore functional programming in a visual way, and [Lambdu](https://github.com/lamdu/lamdu) which is a interactive 'next generation' editor. I think Lambdu is much closer to Hazelnut than this project.
Thanks for the explanation. Lamdu is without a b.
The poster did not think that there is more than one Haskell.
I made the Spock one. It's pretty recent, it didn't exist when round 12 was made. I also fixed the Yesod build. So next round should be good for them.
or "ns' evens' squares' sum" i think bottom-up and declarative are different. left-to-right composition still read as "the what" (not "the how", i.e. indices, allocation, ordering, etc). 
It doesn't describe the concept of Monoids, as it does not require associativity on the composition operator. It describes the notion of an [identity element](https://en.wikipedia.org/wiki/Identity_element), in the same way you can describe a semigroup as a monoid without an identity. There is a very high probability I'll get to monoids soon though :)
Why Data.Type.Equality instead of (a ~ Char)?
&gt; I would not expect "wai" to be beat by go, for example. &gt; &gt; &gt; &gt; Out of interest, why not? Go itself, as well as its http library, were built by Google, the biggest web company ever, specifically for them to build servers with. Seems like it'd be embarrassing for them to be beat by what's essentially been a research project for the last 20 years. As a Haskeller who writes Go for pay, it's well within Haskell's reach to beat Go. I think Haskell getting beat is purely about immaturity of database bindings. Not necessarily lack of age, but lack of the use/improve in the real world cycle (as much as it pains me to say that). If we want Haskell to be represented fairly, we'll have to put some significant work into popular database bindings such as mysql-simple rather than our technologically superior brethren postgresql ;)
When did this `Data.Type.Equality` stuff sneak in? The ability to write function to accept anything BUT some type seems very interesting. Seems like a more satisfying translation of negation in the Curry-Howard sense than the (-&gt; Void) stuff.
How could a person contribute to that? Looks like /u/bos is the author of `mysql-simple`, and I generally get the impression that his libraries are very high quality and performant.
Sure, but the goal of the article is to help the reader understand the value of the identity laws, and to see a basic example of algebraically modelling programming concepts, so I considered mentioning Monoids not to be worth it. You're correct in stating that all examples are monoids, I'm merely defending the approach from a didactic perspective. I plan on writing on associativity soon. Interestingly, in the applications section I mention the javascript build system gulp with the pipe operator, which actually fails to be associative, though admittedly it's something I consider a major design flaw. As far as I'm aware it correctly implements the identity operation though.
GHC tweaks, runtime tweaks, kernel tweaks, etc. These tweaks will be different for every hardware setup and codebase. As far as I know, there isnt a general all-purpose guide to doing this. 
`mysql-simple` is based on `mysql` which has significant issues with concurrency. There are unmerged PRs to address this but they've been sitting there for quite some time. If you work with `mysql-simple` I would suggest trying hard to become a co-maintainer of `mysql` so you can get some of that work merged. If that doesn't work: fork and let `mysql` die off.
Have you been able to get that tool to work with long running processes or daemons? I found it wasnt very relevant to working with web stuff.
&gt;How could a person contribute to that? Looks like /u/bos is the author of `mysql-simple`, and I generally get the impression that his libraries are very high quality and performant. I think that is generally true, but the benchmarks I did showed MySQL-simple being much slower than Go's MySQL library. This means that either MySQL-simple or GCC can be improved. I feel like Haskell can be much faster than Go and that profiling and optimizing MySQL-simple is the right path.
Yes a simulation would likely work fine, but if you are tuning garbage collecting, you will likely want to do it over a longer period. How much of a simulation are you prepared to run?
And faster, too. I should start using Stackage Hoogle instead.
&gt; 164388 requests in 5.01s, 19.91MB read Non-2xx or 3xx responses: 164388 Requests/sec Pretty easy to be fast if you just error out on every request. 
That's the patched version of MySQL-simple I use in my repo.
Looking at how long the PR's have been there it is tempting to fork and add the PR's that belong then releasing it to hackage after having contacted the current maintainer. That's probably seen as hostile, but it wouldn't be meant that way.
I assume this is a bit out of date, but does/did Haskell actually provide linear types? I hadn't heard anything about them. (Also, what are MADTs?)
This was *really* helpful. Thanks so much.
GHC 8's `singletons` looks really weird. It's sort of hard to tell from the haddocks (source hyperlinks keep bringing me to the wrong location), but I think `Sing` is defined as something like data family Sing (a :: k) data instance Sing (a :: Bool) where STrue :: Sing 'True SFalse :: Sing 'False which is sort of an "overlapping data family", except with an explicit kind. I guess that's allowed?
My mistake, it's been quite some time since I last touched this.
Check out Control.Lens.Plated
That looks awesome, but I can't find any resources that describe it, or show clear examples. WOuld you mind enlightening me a bit with an example that shows the motivation behind `Plated`?
Awesome, thank you!
Lens Plated is just a lens API based on Uniplate/biplate so you will find a lot more documentation there. The motivation was to transform Syntax Trees generically by matching and transforming only particular subtrees that may be deeply nested.
I haven't but I believe the Sparkle project has a method for packaging Haskell as jar files that might be worth checking out. http://www.tweag.io/blog/haskell-meets-large-scale-distributed-analytics
Great stuff! I've tried to adapt blockly to functional programming before, and I quickly ran into problems with a general way to allow let blocks within other blocks. I'm stoked to see more efforts in that direction!
Something similar is on its way: https://github.com/rahulmutt/ghcvm
How does it overlap? 
A bigger problem with IfCxt is that it requires TH to generate the instances, and it only generates them for types that have instances for your class available at the point where the TH is invoked (at least I don't see any other way it could work).
Counterexample: I guess not if the contents of the glasses cause chemical reactions. The identity glass still has its properties, if I got it right.
I think the proper term is "kind-indexed data family". This concept was one of the biggest hurdles I had for understanding singletons (not that I understand the library all that well...) The [Dependently Typed Programming with Singletons](https://www.cis.upenn.edu/~eir/papers/2012/singletons/paper.pdf) paper explains the following: &gt; The singletons library uses a kind-indexed data family, named Sing, to provide a common name for all singleton types. [...] A data family is a family of datatype definitions. Each instance in the family has its own set of data constructors, but the family shares one type constructor. The applicable data constructors for a particular datatype are determined by the parameters to the data family. Kind-indexed type and data families are a new addition to GHC, introduced with datatype promotion [Yorgey et al. 2012]. A **kind-indexed type family can branch on the kind of its argument, not just the type**, and the constructors of a kind-indexed data family are determined by the kind arguments as well as the type arguments to the data constructor. 
(For those who may not get the joke: ML is both an acronym for Machine Learning, and a strict functional programming language.)
It overlaps in the sense that data instance Sing (a :: Bool) precludes the existence of data instance Sing 'True data instance Sing 'False which would otherwise be valid.
This is not a direct answer, but here are some alternatives to calling Haskell from Java: 1) making the Haskell code consume from a queue and publish results to another queue (JeroMQ and ZeroMQ libraries spring to mind for this and have very low overhead) 2) making the Haskell code a RESTful service (Servant and JAX-RS)
Sexism from stallwarts begets sexism abroad. Show us your dual prose, vagabond, why no feminine missives to the brutish imperative? Shall I troll the subreddit C, to compel from them their sinful stew they wallow? Their bugged work wrought from too tired sore hands for yet another case to test, a misstep that grounds the sea, sky and land, that sloughs the flesh from our over irradiated sick? Nay. They mourn not for the victims of their undeeds, unthought, untracked faults. For just yesterday the riders of Tesla found an unfit end and what of it? No cries, not even for Rust that these eyes spotted. Forever lost they are, forever defending at best an art, a trade, a guess, when in their reach salvation lay. Her arms open to them, but for the love of metal they stay.
The hell they aren't. If you're running an app with a high connection count and minimal memory usage Javascript is really damn fast 
&gt; More mindshare means more minds working on the most common set of problems in the industry today, and webdev is where most budding developers cut their teeth and many remain there for their entire career. ... and webdev can be very boring. Heavy-desktop entreprise software was boring as hell back in the day, now people are paying programmers to develop the same boring web stuff all over again. If the people that work in this domain want to develop it in a cool language, more power to them, please share the good stuff with strong free software contributions. If a company wants to subsidize building an ecosystem to support web programming (like Google did for Go), again, excellent! But please don't expect me to contribute to this world of boring software for boring needs. I like Haskell, it's a beautiful language, and can be used to write interesting programs. If sharing code to help build beautiful software means remaining non-mainstream, so be it. (It *might* be possible to make web development interesting with just the right approach, but I'm skeptical. It may also be that people find a way to spend more of their time doing fun type-level stuff instead of useful domain-specific stuff.)
I'm also curious.
Reminds me of this argument about Linux adoption: &gt; The oh-so-common threats of "Linux will never take over the desktop unless it does such-and-such" are simply irrelevant: The Linux community isn't trying to take over the desktop. They really don't care if it gets good enough to make it onto your desktop, so long as it stays good enough to remain on theirs. I think the real advantage behind Haskell to the wider world of programmers is as a test bed for fancy new ideas that slowly percolate into other languages. Just like with Lisp there will always be a handful of us who find programming in Haskell to be so much more pleasant that most other languages will feel like a chore, but I don't see the need to proselytize this view. As you say, if someone wants to make it more useable for a certain group of people, more power to them. But we shouldn't treat that as a necessary step in the evolution of Haskell.
There's some implementation improvements that could really help the language in theory, but it would require working on RTS and I'm not in that league of Haskell/C programmers yet. One improvement would be a smarter incrementalish GC strategy for Gen 1 objects. As I understand the GC's implementation right now, it's just a solid Generational parallel GC, so it's great at dealing with an onslaught of short-lived objects, but you're kind of screwed if your code doesn't call for that allocation pattern. I'm sure though that the reason the GC currently doesn't deal with this elegantly is because of the amount of complexity the GHC devs will have to bite in order to implement it such that end users would actually see a positive difference. And then there's concurrent GC...
I'm currently writing a `servant` (plus `hasql`) entry for the TechEmpower benchmarks, and the results are pretty encouraging. ([This thread](https://github.com/haskell-servant/servant/issues/529) has links to four of the benchmarks so far, and is getting updates in real-time.) *UPDATE*: http://termbin.com/wgo9 has the same fortune test with `flask` and `spock` (and a few other frameworks - the yesod code seems to have bit-rotted). http://termbin.com/f0jb has `servant`. Both `spock` and `servant` have several times higher throughput than `flask`. I tested `phoenix` as well, since /u/jaetteintemer described in this thread as super fast, and `spock` and `servant` also easily outperformed it. Incidentally, `phoenix`, in round 12, got absolutely trounced, but it could well be because the code or configuration wasn't good. 
I thought warp was supposed to be really quick. I remember seeing some benchmarks that put it well above node. 
I really don't think performance is a legitimate general-case criticism of Haskell webdev. Haskell crushes the likes of Ruby and Python which are obviously much more widely used. Now there are applications that need top-notch performance, but those are few and far between. The OP here is about broad mindshare, not squeezing the last ounces of performance out. 
The issue with `foldl` isn't that it's intrinsically slow, but rather that it's not strict in the accumulator. If the accumulator isn't evaluated as you go then you just keep building up a bigger and bigger thunk. This growing thunk can exhaust your available memory and even when it is finally forced it generates less efficient code than if you had forced it after each iteration of the loop. This is why people recommend using `foldl'` (from `Data.List`) instead of `foldl`, since it (sometimes) fixes the problem by being WHNF strict in the accumulator. If your accumulator is a strict data structure (i.e. no lazy fields) then WHNF strict is the same as fully strict in the accumulator.
I've been thinking about this on and off, and I can't convince myself there's a problem. I'd like to see it happen.
Don't think this answers OP's question. When the laziness of `foldl` is the problem, `foldr . flip` fixes it. But OP's whole premise is wrong because `foldr . flip` is not equivalent to `foldl`.
"Web dev" is an increasingly irrelevant description of development today despite the fact that there's almost always some sort of "web tech" involved. All of the interesting stuff happens outside the narrow area that web tech benchmarks tend to measure, so there are plenty of areas in modern development for Haskell to shine even if the raw HTTP handling isn't up to par with the hottest HTTP servers. Realistically, there's no way to compete with JavaScript in overall developer mindshare, so the best chance to attract a bunch of developers is to come up with some new "must-have" back-end tech that plays to Haskell's strengths. Come for the tool, stay for the language.
In raw CPU speed it's nothing special, but it's extremely good at low latency, high concurrency workloads and scales horizontally quite nicely. 
The fundamental problem is that it takes `O(length(xs))` space just to state, to declare, to enunciate the computation of homomorphically applying a function to every element of the list. This is unavoidable. There are only three ways this can play out: **We do the bad thing.** We build up that whole expression as thunks on the heap, and once we've finished building it we try to force it. Doesn't matter whether we've built the thing with a left-fold or a right-fold; the arrangement of deck chairs doesn't change where this ship is headed. **We don't do it.** This is where lazy right-folds shine. Since the recursion is only ever forced if the folding function demands it, using a function which discards the recursive call means we needn't bother building up that expression in the first place let alone evaluating it. Of course, this only happens if our folding function actually does discard the tail of the list at some point. **We amortize the cost.** This is where strict left-folds shine. They don't reduce the total amount of work to be done, but they do let us break things up into smaller chunks rather than forcing the whole thing at once. Thus, the amount of space taken for the whole computation is the maximum of the space taken by each chunk (plus a small constant for the left-fold itself). **We amortize the cost.** If the function you're folding with happens to be lazy in its arguments and to produce a data constructor (which, naturally must also be lazy in the folding function's arguments), then you can use a lazy right-fold to build up the whole datum— but amortizing the construction of it by waiting until whatever consumer demands each sub-datum before actually building it. Of course, this is a terrible idea if that folding function needs to hold onto a bunch or resources until it is finally forced. In the end, if you need the entire datum at once, then you might as well build it strictly; and if you don't, you should prefer some way of streaming it to ensure timely release of resources. This is where all the "iteratees", "streams", "pipes", etc came from. There's nothing special about lists here. Doing a fold over any data structure has these exact same choices.
Agreed. For most apps network latency accounts for the majority of actual performance concerns. The advantage from tightening the bolts on server code is normally negligible when compared to reducing the number of HTTP requests.
Whoops I only tried it with a couple things
Yes. Furthermore: * You *can* get top-notch performance in Haskell if needed using optimizations. The total cost of optimized Haskell is likely to be far lower than many of those other languages. * What code was used for Haskell as opposed to the other languages? Often these benchmark sites depend more on how much effort is put into that particular benchmark site by that language community than the actual capability of the language. * Among the top results are several variation on Ur, which is a Haskell spin-off that might even be considered a Haskell dialect. I enjoy doing full-statck web programming professionally in Haskell, but I'm sure I would enjoy Ur as well. * Nowadays, for serious full-stack web programming, the only real consequence of a small difference in framework performance is a small difference in hardware cost. For many of those languages, even if there would be a small additional hardware cost for Haskell, that would be far offset by reduced cost of development and maintenance. EDIT: Yes elsewhere in this thread /u/jkarni reports that the yesod entry in this benchmark is bit-rotted.
Actually, there is a correct implementation in terms of `foldr`: foldl k z xs = foldr (λv fn z. fn (k z v)) id xs z Which by itself is slower than the regular definition (it has to allocate O(n) thunks, I think), but can participate in list fusion. Newer versions of GHC (&gt;= 7.10 I think) include [an analysis](https://www.joachim-breitner.de/publications/CallArity-TFP.pdf) which allows it to remove unnecessary allocation, so that `foldl` should be "pretty fast", in the sense that performance should match your definition, while additional preserving semantics.
Interesting, I had never heard about garbage-collection vs. safety, I'd be glad if you could elaborate a bit more.
Safety as in explicit memory allocation/deallocation. In Haskell it is harder to segfault due to double free, or run out of memory for forgetting to free.
please don't get me wrong but I think that would be a good thing (if you want to write Haskell for bucks) - those Web devs usually care for usability (docs, tools, time to market, ...) which are exactly the things lacking so once you get enough of em you either have solved the problem or they help you solving it (remember atwoods law? I wish there was something for it in an actually good language)
&gt; an analysis For common cases there has been some analysis done in versions of GHC going way back, at least to the GHC 6 series. The linked paper describes an additional improvement. I'm quite sure it would not be hard to prove that getting it exactly right in all cases would be equivalent to solving the halting problem. 
Raw speed is important, but given that GHC uses GC, what's even more important is mean time to respond. It's not very useful when it takes 100ms for 5% of the requests while it's 800ms for 95%. I haven't compared GHC to BEAM (Erlang/OTP), but it's something where HTTP servers running on the Erlang VM (BEAM) usually fare better than those with many more kilo-requests-per-seconds than Erlang. Erlang is able to deliver more consistent response times with less fluctuation while still being very good in the i/o department. I mean, I would concentrate on this if I were looking for places to outshine other web stacks.
I see thanks !
That's a FAQ, check the answer here: https://github.com/rahulmutt/ghcvm/issues/3
Of all big player languages, which ones have huge amounts of advertising dollars behind them? And which ones don't? I don't mean to say that certain languages are illegitimately popular. But all languages go through a period of history where they are the new thing with rough, shitty edges. Some languages power through it with the help of business deals and mindshare capture, and *then* become usable. Other languages are so immediately fun and useful for their niche that they sneak in the corporate world through the back door. I don't feel like Haskell has either of those options open to it. It is a enormously successful research language with a hardcore sect that uses it in business. Until someone with clout says, "This is the new future," I think it will stay where it is.
I think you've understood your initial confusion, but yeah there are two things different about folding from the left and folding from the right, regardless of `foldl` versus `foldl'`. The first is "where do the parentheses go". With `+` it's hard to see the difference, but try these two: (10 / (8 / (4 / (2)))) -- foldr1 (/) [10, 8, 4, 2] ((((10) / 8) / 4) / 2) -- foldl1 (/) [10, 8, 4, 2] Second is laziness. With foldr you can process part of an infinite list, no problem. foldr f z [a, b, c, d, e] = a `f` (b `f` (c `f` (d `f` (e `f` z)))) --&gt; evaluated as --&gt; **thunk** --&gt; a `f` **thunk** --&gt; a `f` (b `f` **thunk**) With foldr, if `f` ever decides it's "done", it can throw away the rest of the list. Just stop unwrapping the thunks by ignoring the second argument. foldl f z [a, b, c, d, e] = ((((z `f` a) `f` b) `f` c) `f` d) `f` e With foldl, it can't. The only way to return from foldl is to run the computation on the whole list. That means the list can't be infinite.
I know BazQux, but it also has major Haskell parts and unfortunately not something enough people will recognize in order for it to be a good success story for Ur/Web.
Sure, but given that (frontend) web development often has shallow "depth" and a lot of "width", the expected productivity gains from beautiful systems are small compared to the sheer amount of work required to support the latest norms, interface with the latest APIs, etc. To provide a seamless experience you basically have to run at the speed of the ecosystem around you, with little power to restructure it to allow beautiful approaches to work. (For example you may think of developing automated bindings for those APIs, but this requires their producers to agree on a precise high-level description of their interface, eg. the Typescript type annotations efforts, and in practice those are always slightly wrong and require a lot of manual adjustment. The Typescript example shows that the situation can improve if you capture enough developer mindshare but it remains, by design, a fundamentally thankless task.)
I have no clue about either ruby or rails - having said this: I was under the impression that rails really boosted ruby quite a bit can you elaborate or point me to some thing explaining how rails "destroyed ruby"? --- for Haskell (ok for me) I would wish to get to the point where you can compete with say the rails community
Despite the fact that for a while 90% of Ruby developers were Ruby on Rails developers, the Ruby development team basically disregarded that entirely. For example the company I work for developed a patch for Ruby that provided Ruby on Rails with basically unbounded memory savings (it made the GC safe for Copy-on-Write) at a time that memory was the number one cost bottleneck for Ruby on Rails applications. We submitted the patch to Ruby but it was never accepted because in the general case (i.e. non-web environments) it had a small performance overhead. So my company launched a fork of Ruby called Ruby Enterprise Edition (was tongue-in-cheek) that had this patch and a couple of others that together dramatically improved Ruby performance for web applications. I believe it was at least as popular as MRI itself in web dev for years. In return though, the Ruby community absolutely flourished under the success of Rails. Its core developers contributed things like Bundler, a world class packaging tool that's inspired basically all modern packagers today (including for example Stack), also RSpec and the renaissance of TDD (or BDD). The result is that whatever task you have, there's a good production-ready actively supported and well documented package for it on Rubygems. At least there was in the haydays of Rails, it's slightly diminished in popularity now, even though it's still breaking through in the enterprise.
&gt; In return though, the Ruby community absolutely flourished under the success of Rails ... sounds like I actually would like this for Haskell ;)
no offense but this sounds like you have some beef with the rails guys but what makes you think that the exact same thing would happen to Haskell?
No, but you can model linear computations using a custom Category: data Linear f a b where Prim :: f a b -&gt; Linear f a b Id :: Linear f a b Then :: Linear f a b -&gt; Linear b c -&gt; Linear a c Associate :: Linear ((a, b), c) (a, (b, c)) Unassociate :: Linear (a, (b, c)) ((a, (b, c)) Swap :: Linear (a, b) (b, a) instance Category (Linear f) where id = Id (.) = flip Then The idea is that you also define a custom `f` for describing the operations which produce and consume your resources, and then `Linear f` is a DSL in which you can rearrange your resources but not duplicate them. You would also need to write an interpreter which does the actual resource-producing and -consuming. The main disadvantage of this approach is that its point free syntax makes it harder to follow the flow of the resources than if you could give them meaningful variable names. I have a [project](https://github.com/gelisam/category-syntax#readme) which should fix that (assuming I finish it one day; I've been busy with so many other Haskell things...).
Now I understand what newbies feel when they read our posts full of technical jargon! I understand the simpler words, and so I can get the gist of it, but as a non-native English speaker, the subtleties evade me.
I work at a company that does web dev in PHP and Haskell. Nobody complains about Haskell's performance. They complain about how cryptic and difficult the language seems to be and how the tools and runtime instrumentation aren't as good as what they're used to. I'm really looking forward to doing the GHC 8 upgrade. It will be a huge help.
I watched it before but didn't think carefully how many bytes will `String` take! I thought "list cons and boxing will cause 5 bytes per character" is a generous assumption, but 32 bytes per character? Meanwhile I'm a man of no learning ability :( I reminded of my previous question and `ByteString` again solved the problem. Now it uses less than 10 mb.
I cannot recommend Learn You A Haskell For Great Good enough for beginners.
Not quite what you asked for, but perhaps this concept will spark some ideas: If I were teaching Haskell, I would use the [Haskell Wikibook](https://en.wikibooks.org/wiki/Haskell) have the student critique the lessons and *edit* to improve the book. Probably not right in your case, but just for perspective… I find that editing the learning material and thinking about how to teach a concept to others is the best way to master a topic.
A rather bold generalization for such a big and diverse group.
I don't know enough to suggest a timeline for the curriculum, but you might find the following useful: Chris Smith has experience teaching Haskell to younger students, and you can find his blog [here](https://cdsmith.wordpress.com/). Specifically he has a [CodeWorld](https://code.world/) project to teach Haskell (somewhat modified, read the differences [here](https://cdsmith.wordpress.com/2014/08/26/on-codeworld-and-haskell/) ). Click on help for some example programs. There is also a configuration close to haskell, which you can see in this [example](https://code.world/haskell#Prj5yE1YhdXlU3S6XLHdzzg). You probably know of it, but [Write you a scheme](https://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours) is a tutorial for writing a scheme interpreter with Haskell. It covers: * Parsing * Error handling * Read Eval Print Loop * Mutable variables * Functions * IO With that said, I'm not sure how far someone with no programming experience can get in two weeks. I have my doubts about him being able to write his own language (maybe he'll follow along though). In university, most people struggle to understand the basics of C after an introductory one semester course. 
It's a common quality across all of humanity.
How often are you writing CPU bound web applications?
This sounds like a pretty strong bias. I have never in my life seen anyone prefer php over anything because it's *less cryptic.* PHP rivals JavaScript in how ad hoc and nonsensical it can be.
Ur is intended to exist as a language specification independently of Ur/Web, but there are no existing "Ur-only" compilers.
That's absolutely true, but not quite what I meant to get at. Put another way: We're all very satisfied with Haskell's performance. We automatically go to Haskell any time PHP's performance becomes a significant blocker. We choose to build new projects in PHP sometimes. Performance is rarely the tradeoff that tips the scales.
I would advise your current schedule, but also maybe with a project involved. Maybe something like generating an image or a choose your own adventure game. Simple fairly easy program. I remember doing basic graphics on an Apple 2e at that age. Do remember most of lesrning is doing. Concepts are great, but keep them to a minimum and applied practice to a maximum. Make sure your cmaper writes a lot of code.
Haskell programming by Christopher Allen is way better and worth the money. Real World Haskell is also okay but a bit dated. LYAH left me not really able to build anything and skimped on a lot of important theory. It's a fun book but not practical.
&gt; Haskell programming by Christopher Allen and Julie Moronuki
I think that we need something that would increase the productivity maintainability and reusability of web and cloud developments dramatically by making available for average developers the promises of Haskell in terms of algebraic and monadic composability without forcing them to be Haskell experts, so they could do more with less haskell code in less time and generating faster code than with any other language, and could combine code pieces brainlessly as long as they type match. Out of that, there is no hope. Industry thinks in terms of effort and it can not think otherwise.
I would like to suggest using the `text` library, which is much more efficient with memory and computation
The point of your comment is what, exactly? If you're not interested in web development, that's fine. No one is asking you to contribute if you don't want to. Unless your intent here is to imply that the Haskell community would be better off if it didn't allow for web development.
Nice! Glad to see my ideas challenged 
I see, thanks for the explanation!
You can scrub key data, you just have to do a bit more work. Use the FFI to allocate pinned memory, store the key in it, and wipe the key when you are finished with it. Add a finalizer so the GC will catch anything you miss, and there you are.
&gt; but there are no existing "Ur-only" compilers. Which is a shame, I suspect there would be far more interest in Ur if use of the language were not dependent on the framework. Anyway, for better or worse Haskell and OCaml are the only MLs in town.
Seems to be a very polarizing book. It gets a lot of praise, but it also gets a lot of [flak](http://bitemyapp.com/posts/2014-12-31-functional-education.html#learn-you-a-haskell) from well-respected sources such as Chris Allen.
I'm not sure whether it counts but [Bartosz Milewski's Category Theory series](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/) is also great
Regarding "maths", don't try to "hide" monoids, but don't turn it into a maths lesson either. A monoid is, as far as that kid is concerned, the name of a programming concept. Don't underestimate how quickly kids can learn things within their subject of interest, and how quickly they can go off on a tangent (or worse, lose interest) if you bring up a different subject.
I think that at the very least you might want to create a custom, monomorphic prelude. maybe something similar to [purescript-preface](https://github.com/paf31/purescript-preface). One that they'll use throughout the curriculum. that might also means a built-in parser which may only need assembling the right pieces in the right place. I hope [this](https://gist.github.com/soupi/d4ff0727ccb739045fad6cdf533ca7dd) might be of some use or inspiration for that. Make it simple, stick to the basics, provide as much as possible (dev-env wise, for example), and may the force be with you.
The parallel haskell book seems amazing. Did y'all read it ? I'm curious about prerequisites.
This has really progressed nicely. Looks like a great alternative to Haskell Book if you're less academically inclined &gt;In terms of naming language features when we use them, we often find people get incredibly confused when you throw a multitude of jargon at them. Haskell has a lot of names for things, and we’ve witnessed many times where students have explaimed things like “Oh, THAT’s all ((feature)) is, I totally know how THAT works!” We’d rather you got an understanding of what you’re doing so that when we explain what things are, you have a practical base for them. This is definitely my style 
Thanks for pointing me towards CodeWorld. Focusing on things like graphics, simple games, etc. are usually great approaches, and if designing a language doesn't work out (if the camper loses interest, etc.), I will totally turn to it. Smith's blog is also a great read - thanks for pointing me in that direction. I had seen Write You a Scheme, and I was considering using it to guide my work with this camper. My problem is that Write You a Scheme essentially requires you to become well-versed in Lisp, as well as Haskell. It's a super cool project, and it'd be nice to have the book as a guide (hell, I might even print it out to have it as a reference), but I think it might be a bit much. I'm with you that this is very ambitious, and I'm worried. But, I don't want to discourage any child from working on something because it's hard. When I was that age, that would have angered me more than anything else, to have an adult say a project was out of my reach.
[Haskell wikibook](https://en.wikibooks.org/wiki/Haskell) should also be mentioned.
Thanks much for your recommendations. I do like the idea of teaching by having the campers design games, as in Realm of Racket. Most often, that's exactly what our campers do - make games during their two weeks at camp. I'll keep Realm of Racket in the back of my mind, but right now I'm looking for ways to help this particular camper achieve his goal of making his own programming language. Elm might be a great idea - any simplifications would be highly helpful, and I'm sure it would also be great if this camper decided to make a game or something of that nature.
Okay, first of all, thank you for that blog post - even though my task is a little more intense than just teaching a kid Haskell (designing a programming language and making a basic interpreter), it's greatly reassuring to see that Haskell concepts are accessible to an average 10-year-old. I'm a firm believer that children are capable of more than we imagine - I've seen it myself - I was just worried about the time constraint. I only have two weeks with this camper, and I want him to be able to go home having made an awesome project. Now, I have some hope that we can get going quicker than I thought, provided I can give this camper the attention he needs. Second, I hope that, if I have any questions as I'm going about my work with this camper, I can turn to you and your co-author, since you've both been recommended in this thread.
I'm working on building my own parser, so that if he absolutely can't get past a certain point, I can jump in with a bit of my own code. I don't know about changing the Prelude - from what I've read, that's a common approach, but something about it feels wrong. I got this kid hooked on Haskell, and I want to be truthful and teach him Haskell, not some hacked version of it. Thanks for linking the transpiler - I'm going to try to read through all of the suggestions in this thread, as you say, for much-needed inspiration.
have you seen a [`Apply`](http://hackage.haskell.org/package/singletons-2.2/docs/Data-Singletons.html#t:Apply) trick used in `singletons`? Check `singetons-2.1` for pre-GHC-8.0 version
&gt; Avoid {success {at all costs}} Did I get it right?
Looks like you got Haskell and Scheme mixed up. (avoid (success (at (all (costs))))) Haskell looks like this: avoid . success . at . all $ costs
Here is the link: http://haskellbook.com/ Go get it, it's really worth it.
Yes, and it is technically cheating because of that. However, when we discussed the issue internally we decided to give the Haskell tests (among others in a similar situation) a pass. The issue has a number of considerations, but ultimately it comes down to a question of how the language deals with memory allocation and erring on the side of standard practices. When the tests were initially devised it was done from a Java point of view\*, and the JSON test was designed to capture the common workflow of making an object and then serializing it to JSON. In a real application you obviously wouldn't be repeatedly serving a serialized version of the same object, because the data would be different for each request. Thus, the requirement that a new object be instantiated for each request, because in Java you'd use `new` to make a new object on every request. Haskell, in contrast, doesn't have a `new` keyword. We don't control allocation that explicitly. So how do we ensure that a new object is instantiated on every request? Since the value is constant it will always only be evaluated (and thus, allocate) once. We could potentially introduce a workaround to force an allocation every request, but at that point we have to go back to why that requirement was introduced in the first place: to better model a real-world workflow. The question at that point becomes, "What better represents real code in a real application?" For Haskell, the answer is the code that doesn't have workarounds to force extra allocations. \*Standard disclaimer: TE does not give Java any preferential treatment, and nearly every framework submission in the benchmarks was contributed by a third party.
I wonder why Gabriel decided to name the module `List.Transformer` instead of something more traditional like `Data.List.Transformer` or `Control.Monad.Trans.List`.
The book is over 5 years old. I wouldn't hold my breath.
How desperate are we to capture the human demographic?
Hmm, I wonder if it would be worth composing exercises to accompany the book...
[removed]
It probably will, as people insists on recommending it despite it's big flaws. If I had the time I'd rather contribute to the wikibook instead.
/u/tekmo I've long been thinking about simplifying the "list-t" API myself, but you've beat me to it. :) Good job!
The "problem" with Haskell is that it has different ideas, idioms and mindset than mainstream languages, so for many people it often feels like learning programming all over again (though the more you learn the more you feel the best practices are similar). For the same reason you'll probably want to learn about OOP before writing servlets in Java, You'll want to learn some FP before writing web servers in Haskell. However, it's not so bad. One of my first Haskell programs was the software for my blog and there are [tutorial](http://adit.io/posts/2013-04-15-making-a-website-with-haskell.html)s to get started quickly with that. If you really want to do it, I guess it might be possible, but I don't know of tutorials directed specifically for that audience.
I started on this last night and am finding it very good. I know only basic Haskell, and feel that my math skills are generally weak (and untested for a long time!) and yet it's still explained at a level I understand.
And I'm assuming it retains its performance characteristics?
And I'm assuming it retains its performance characteristics?
Use `singletons`' defunctionalization symbols. {-# language TemplateHaskell, ScopedTypeVariables #-} import Data.Singletons.TH type S a b = (a, b) genDefunSymbols [''S] It gets you a new `data` symbol for each partial application of a type synonym/family, which can be applied using `Apply f x` of `f @@ x`. foo :: SSym0 @@ Int @@ Int -- applied to 0 args foo = (0, 0) bar :: SSym1 Int @@ Int -- applied to one arg bar = (0, 0) You can refer to the kind of type level functions with `TyFun a b -&gt; *`, for a function from `a` to `b`. data Foo (f :: TyFun * * -&gt; *) where ... `TyFun` can be made a lot cleaner with GHC 8, where you can define a kind synonym: type a ~&gt; b = TyFun a b -&gt; * infixr 0 ~&gt; 
you can read *pure* as *without side-effects* - it enables/helps out with things like [referential transparancy](https://wiki.haskell.org/Referential_transparency), optimizations, reasoning about your code, ...
Mind linking to any such discussions? This is the first time I hear about it!
Here is a sample implementation of Scheme in Python: http://norvig.com/lispy.html I used it as a sample to do same in C#, as a matter of exercice: https://github.com/lissen/Lissen 
`TyFun` is made clearer in `singletons-2.2`, which requires GHC &gt;= 8.0
Thanks, that's nice.
the answer to the last question is probably just functional programming with side-effects ;)
Thanks Gabriel, still a noob to Haskell here but this builds a bridge for people like me.
Okay, both of those are awesome, and I will definitely suggest those (building a calculator and building a text game domain-specific language) to my camper! I also think that both are simple enough that the camper should be able to understand what is needed without much intervention, which is one of the goals. Thank you, /u/gilmi and /u/pridefulpropensity!
My sense is the community is the Haskell community is growing at a decent clip and language q&amp;a is getting distributed across an increasingly larger group of people. I'm already finding responses to intermediate level Haskell questions on stack overflow seem to get answered more quickly than 6 months ago. Between irc, stack overflow and Reddit, I'm relatively confident I can get unstuck from any particular problem and I'm only a beginner-intermediate haskeller.
Quite excited about this. Really wish it success.
It's not possible to call Haskell full stack until GHCJS is supported.
In most situations people would use `$` so the bracketing is already invisible.
Because JavaScript, is essential that GHCJS becomes a first class citizen. I'm using it with react-native, and I'm getting a gazillion more ready made modules than I would with the ARM backend.
You're welcome!
Not by humans.
I'd love to have this feature too, but for slightly different reasons. With newtype you can have a huge RHS, but typechecking will only use the LHS when doing unification etc. This is a big efficiency win when the newtype RHS is thousands of tokens big. But it only works for kind *, as you point out. I've yet to come up with a satisfactory solution for other kinds. The type family "solution" doesn't really solve my problem, because it will still unfold during type checking (somewhat avoidable using injectivity annotations, but not controllably so).
If a human(oid) exists then, they'll at least want to direct computers, explain their will. no?
I actually think this is a bad thing
Probably not. In 1000 years computers will be able to guess what we want by reading our thoughts (via neurointerfaces). You've picked a time span *too* huge. Programming in 150 years would be more interesting to discuss.
Programming in ten years may also be interesting to discuss ;)
With sticks and stones. 
interesting, reading our minds would still, imo, be programming. but how about 150 yrs, what are your thoughts then?
&gt; In 1000 years computers will be able to guess what we want by reading our thoughts (via neurointerfaces) My thoughts are contradictory and nebulous on the best of days. I would rather type.
&gt; reading our minds would still, imo, be programming You're not explicitly giving commands to the computer, so it is not programming. &gt; but how about 150 yrs, what are your thoughts then? I expect that programming will be computer-assisted. You write some incomplete and/or incorrect code and the compiler tells you how you can proceed writing it and/or fix errors. I also think that the code won't be represented as text — more like syntax trees or even graphs.
That’s not very constructive. We should discuss technical matters, improvements and shortcomings, and not merely state the final vote.
How does this compare to [list-t](https://hackage.haskell.org/package/list-t)?
For this library they behave differently. `(&lt;|&gt;)` is used to append, but `(&lt;&gt;)` does element-wise `mappend` (a.k.a. cartesian product). In other words: select [a, b] &lt;&gt; select [c, d] = select [a &lt;&gt; c, a &lt;&gt; d, b &lt;&gt; c, b &lt;&gt; d]
The language also only supports general CRUD-like semantics, because the language and its implementation deeply depends on the rollback abilities of the `transaction` monad. Without this guarantee, Ur would have to implement things like 'real' garbage collection, where today it can get by without it (and is probably one of the reasons it's so fast). This is problematic for certain classes of application. Lots of dynamic web applications expect things like work queue integration to do things like push/pull changes from other systems... But in Ur/Web, you have to do everything through the database, because every program in Ur/Web has to be side-effect free. So in practice, anything that isn't literally "The website displaying the data in database" has to be written in another language, and use the SQL server as a mechanism for coordinating work (or some other things, but that's the gist). It's unclear if you could even fix this with a standalone version of Ur, and have it interoperate with Ur/Web, without radically rewriting the entire implementation of the Ur/Web system. That said - Ur is very kick ass despite all that.
Looking at your other comment, the monoid instances also seem different. Is that right? Why?
It doesn't use either. Instead it just accepts requests from an incoming socket and puts them on an incoming work queue, which is handled by a thread pool that gradually dequeues things. It's kind of closer to the way you would organize a high-performance server in Windows instead of Linux, actually. I'm not entirely sure how Ur/Web would utilize `sendfile`, because the general use case for this call is pushing out large amounts of static data, like static files, out to a socket. Ur/Web can package static files into executables, but it puts them in the binary itself and just writes the blob out to the socket. I'm not sure if having the kernel do this with `sendfile` (by going right from the disk cache or whatever to the network socket) is any better than just a direct `send` or whatnot with a given, static memory buffer. And anyway, if you want to accelerate serving of static site content and use `sendfile` - just leave that to nginx. It's very good at it, and will take care of many other important features that Ur shouldn't care about, like if you need GeoIP, or SSL, etc. Personally, I think a lot of the C code could be substantially improved inside Ur/Web to have deeper integration with high-performance system facilities and improved libraries. But at the end of the day, it still obviously kicks tons of ass despite that. So it's not clear if all that would really make most applications meaningfully faster, since a lot of it is entirely dependent on the database. Then again, it would still be fun. :)
The real question is whether, in any universe or time, it can take a form that is not isomorphic to lambda calculus. (see Phil Wadler's conclusion, for example, in http://homepages.inf.ed.ac.uk/wadler/papers/propositions-as-types/propositions-as-types.pdf) As for the agency (who/what 'does' it) that is another matter. Might the notion of 'doing' be a bit too imperative ? 
I don't disagree, but there's no way to explicitly force allocations in Haskell. GHC will recognize monadic code that is actually pure and optimize accordingly, so hiding the object behind an IO action does not necessarily prevent it from being pulled out and serialized once. And any workaround that manages to thwart the optimizer is in danger of being thwarted itself in a future version of GHC. That would put us in an even worse situation where upgrading the compiler can cause a non-cheating implementation to suddenly become a cheating one with no one realizing. Again, I agree that the choice we made is not great, but from our point of view there wasn't a better alternative.
Very against the lambda not being in brackets. Why is it part of the same proposal as the `do`?
The high-level reason is that I believe that all type classes related to collections operations should be for type parameters of kind `* -&gt; *` because it greatly simplifies constraints (and a collection is intrinsically something of kind `* -&gt; *`, not kind `*`). For example, suppose that I had written the `select` utility to use `Monoid` instead of `Alternative` to build the output. Then the type would be: select :: (Foldable f, Monoid (g a)) =&gt; f a -&gt; g a Besides being ugly, that requires the use of `FlexibleContexts`. If you use `(&lt;|&gt;)` instead of `(&lt;&gt;)` then the constraint gets much simpler. Also, suppose that the `list-t` library wanted to add a `MonadCons` instance for my `ListT` implementation for whatever reason and I wanted to define it like this: instance MonadCons ListT where cons x xs = pure x &lt;&gt; xs That would be forbidden because the `MonadCons` class only has a `MonadPlus` super-class. You can't even add a `Monoid` superclass to `MonadCons` even if you wanted to because it's the wrong kind. You can't write a constraint like: class (forall a . Monoid (m a)) =&gt; MonadCons m where ... Well, you can if you're really determined, but it requires a whole lot of type system abuse. However, if you use `(&lt;|&gt;)` or `mplus` to add on the element then everything just works. Also, suppose that you write a function like this: example xs ys zs = fmap odd (xs &lt;&gt; ys) &lt;&gt; zs The inferred constraint if you use `(&lt;&gt;)` is: example :: (Integral a, Functor f, Monoid (f Bool), Monoid (f a)) =&gt; f a -&gt; f a -&gt; f Bool -&gt; f Bool ... but if you use `(&lt;|&gt;)` instead of `(&lt;&gt;)` then the constraint simplifies down to: example :: (Integral a, Alternative f) =&gt; f a -&gt; f a -&gt; f Bool -&gt; f Bool Also, with the `Alternative` class the *only* thing you can do is concatenate collections (there's no way to write the cartesian product because the whole class is higher-kinded) so it's meaning is always unambiguous. However, with the `Monoid` class there are two possible behaviors (append or cartesian product). Given that the `Alternative` class *has* to be append, I prefer to give the `Monoid` class the other behavior. From the end user's perspective, if you use `Alternative` you always know exactly what you are getting so there is no disadvantage to using `Alternative`. The other reason that it's nice to give the `Monoid` class the other behavior is that it works really nicely when chaining monoid instances as described [in this post](http://www.haskellforall.com/2014/07/equational-reasoning-at-scale.html).
&gt; Looking at `f \x -&gt; x \y -&gt; y` makes my brain stall Poorly formatted code tends to do that. I am personally in favour of the change, but I would only ever use the feature when the parentheses would otherwise span several lines. So I would write f (\x -&gt; x) (\y -&gt; y) and f (\x -&gt; x (\y -&gt; y)) but in the second case, if the body of the lambdas were longer, I would not write f (\x -&gt; a_much_longer_implementation_1 x (\y -&gt; a_much_longer_implementation_2 y)) instead I currently make use of `$` f $ \x -&gt; a_much_longer_implementation_1 x $ \y -&gt; a_much_longer_implementation_2 y and with ArgumentDo, I'd write it like this instead: f \x -&gt; a_much_longer_implementation_1 x \y -&gt; a_much_longer_implementation_2 y As you can see, the indentation clarifies that these are nested lambdas, not two separate lambdas.
`do` on the LHS? What does `f do { x &lt;- m ; k x } = ...` mean? But on the RHS: it always struck me as odd not being able to use do/lambda without parens/$ despite the lack of ambiguity. Having recently started using Purescript, it works quite nicely there.
We'll probadly have that in 50 years, let alone 150.
I don't find your last example more readable than the second last. `$` actually stands out for me - I read it as separation between two parts *before* $ *after*. Also, I would rather write something like f (\x -&gt; g x (\y -&gt; h y)) put call of `g` to next line or even use `where` with named sub expression.
So construction operations can be done in terms of the type class operations and [this post](http://www.haskellforall.com/2014/11/how-to-build-library-agnostic-streaming.html) goes into more detail about that. The short summary is that you can use `MonadPlus` and `MonadIO` to build the `ListT` It's only for consumption that you need to pick an implementation. So what you could do is provide variations on `runListT`/`fold`/`foldM` for a stream fusion representation and encourage users to program over `MonadPlus`/`MonadIO` so that they can switch easily between libraries.
&gt; isomorphic to lambda calculus This is the kind of answer I was looking forward to! Is there any extant research, or philosophies, that propose non-lambda calculus flavored computing? I would guess that quantum computing is a different game. If so, might there be even more substrates to run calculations on that would diverge from the turing machines we're used to working with?
Related: https://github.com/aelve/haskell-issues/issues/34
Yea I think we'd need dollar sections to be another syntactic thing. But if somehow a change was made to make `$` a syntax level feature, there's nothing stopping that same change from including dollar sections, so I doubt an extra `DollarSections` extension would be necessary.
`yield` is taken by `pipes`.
Well played. For the uninitiated, it is a reference to a series of novels (Dune) by Frank Herbert. https://en.wikipedia.org/wiki/Dune_(novel)
I like how concise and clear your writing is. Since I am already familiar with functional programming, I don't know how whether an imperative programmer would find it clear as well. But I've had colleagues ask me for a shorter introduction than our typical book-length material, and I couldn't think of anything; now I'll have yours in mind, and I'll be sure to tell you whether it worked out for them. One question though: in which way is this related to Sherlock Holmes? Another question: why did you change your style for the square root puzzle section? All of a sudden, you're throwing a lot of code to a reader who hasn't seen much Haskell yet. I hope they will figure it out anyway, but you haven't given them the incentive to even bother reading through the code. What if this really was a Sherlock Holmes story, and you could only find the solution to the mystery by deciphering the code? Just thinking out loud :)
`msplit` is equivalent `next` except using a type class I believe the CPS implementation is faster when using type-class operations but slower if you are recursing manually over the spine of the data structure. Also, if you restrict yourself to standard type classes you can easily switch between both implementations as [described here](http://www.haskellforall.com/2014/11/how-to-build-library-agnostic-streaming.html)
Nicely worded, where did SPJ say that?
&gt; It would have to be baked into the language in Haskell2020 to get me to use it. And the first step towards integrating it into the language is implementing it as an extension, so people can try it out and give a more informed opinion on whether it should make it into the language.
Fair enough.
This proposal is not going to fix that.
Or maybe our efforts to destroy the planet will have succeeded. That way no human will be programming, too.
&gt;&gt; All comments and criticisms welcome.&gt; &gt; Guess not. I didn't down vote you, if that is what you imply. &gt; how you use recursion/higher order function instead of for loops. How original. When I was new to Fp it was indeed a new idea that you can do loops with recursion. It looks stupidly apparent in hindsight. But at that point, FP looked like a fancy car that you can only drive on a race track. So I have tried with this articles to change that impression for someone like me when I was new to FP...
Slightly related on the issue of slow folds: The `-flate-dmd-anal` flag can do amazing things with the performance of pure folds. I discovered this while writing [benchmarks](https://github.com/Gabriel439/Haskell-Foldl-Library/blob/master/bench/benchmarks.hs) for /u/Tekmo's `foldl` library: No extra flags (some lines removed; the first two results use functions from the `foldl` package): ~/s/Haskell-Foldl-Library (master) $ stack bench benchmarking [1..10000 :: Int]/sum/fold sum time 82.46 μs (81.82 μs .. 83.59 μs) benchmarking [1..10000 :: Int]/sum/foldM (generalize sum) time 50.85 μs (50.75 μs .. 50.96 μs) benchmarking [1..10000 :: Int]/sum/Prelude.sum time 463.6 μs (460.9 μs .. 465.5 μs) benchmarking [1..10000 :: Int]/sum/Data.List.foldl' (+) 0 time 122.6 μs (122.2 μs .. 123.2 μs) With `-flate-dmd-anal`: ~/s/Haskell-Foldl-Library (master) $ stack clean ~/s/Haskell-Foldl-Library (master) $ stack bench --ghc-options -flate-dmd-anal benchmarking [1..10000 :: Int]/sum/fold sum time 55.08 μs (54.97 μs .. 55.20 μs) benchmarking [1..10000 :: Int]/sum/foldM (generalize sum) time 50.69 μs (50.36 μs .. 51.32 μs) benchmarking [1..10000 :: Int]/sum/Prelude.sum time 55.31 μs (55.02 μs .. 55.76 μs) benchmarking [1..10000 :: Int]/sum/Data.List.foldl' (+) 0 time 55.09 μs (54.99 μs .. 55.20 μs) [Sadly using the flag seems to entail some compatibility issues in the `.hi` format.](https://ghc.haskell.org/trac/ghc/wiki/LateDmd)
I wish this was true for performance, it would make things _much_ simpler. Unfortunately, the advantage of stream fusion only comes when we represent a stream as a state machine, so having streams in terms of `MonadPlus` and `MonadIO` would not get us any performance advantage. I've thought about some kind of abstract representation for this state machine before that could be captured in a typeclass (`MonadStream`?) and then instances defined for multiple streaming data representations, but I've always comes to the conclusion that such a generic _and_ performant representation would be too difficult for people to use in general.
Real World Haskell is truly amazing. I'm currently reading it and I love the idea of explaining by building small projects, it really is fascinating to read. I completely recommend it, not for beginners beginners though !
Ultimately I think getting a `Stream`-like type into `base` is the best case scenario. And this may work for simple data sources (though I'm not sure about the presence of `m` in both positive and negative position there), but data transformers (functions of type `Stream -&gt; Stream`) will likely fail on this.
Is there any way to get around that? Because I personally love that ($) is just another function like almost everything else in Haskell. IMO the less syntax there is the better, I would even say that "if then else" was a mistake.
It would help. I suppose the idea is that the `runST $ do { ... }` idiom will be deprecated in favor of `runST do { ... }` -- which needs no special typing rules to be correct -- and at some time in the future when most projects have had a chance to convert, the special typing rules for `$` could be removed.
As I said in [my other comment](https://www.reddit.com/r/haskell/comments/4r70pl/argumentdo_proposal/d4z404y), I'd be really happy with `in` being the keyword of choice. Sometimes we can have our cake and eat it too =)
I would agree with you if it weren't for all the hoops GHC jumps through to make `$` work. It just doesn't make sense to have the language do so much work to specialize the semantics of an arbitrary function, when it could have just been a syntax.
But I really do wonder why GHC has to do all that work. Surely the type signature of `$` isn't so bad that it breaks everything.
But it does. As was already mentioned, it breaks with `runST` and `ApplicativeDo`'s desugaring of `pure` and `return`. There's a *lot* more nonsense going on behind the scenes to help it work with a variety of other special cases. As another example, it works with both lifted and unlifted types while still displaying the type as `(a -&gt; b) -&gt; a -&gt; b` for beginners, even though that's technically inaccurate.
I mean I know it does I just wish it didn't haha. I personally think `ApplicativeDo` is weird, because if you wrote a function that did the exact same thing as `pure` but wasn't `pure`, it wouldn't work. IMO two things with the same definition and type should work the same way. Maybe something like `ado ... in` instead of `do` for applicative do, idk. Also I sort of see why it breaks with `runST`, I again just wish there was some way to improve Haskell's type system so that it worked without any magic. So yes I being a bit hand wavy and not giving much in the way of concrete solutions, hopefully someone more knowledgeable than me can come up with a good solution that doesn't feel as dirty.
Umm... link? Edit: nm. https://www.futurelearn.com/courses/functional-programming-haskell
This is a proof of concept, and it can be done! ( https://github.com/commercialhaskell/stack/issues/1485 ) Those re-packs are semi-automatic, and I work on an even more automatic way which everyone could apply whenever they change resolver. The bigger plan is that it take the upstream ghcjs and use the exact versions from your `stack.yaml` to have all versions matching. 
But if you use specialized versions of the stack to accomodate different effects in different parts of the code, you need different instances isn't? Unless you want to define a single and monolithic stack for all the application, that, in a medium size project will include state effects for dozens of different states, which implies using a single monadic stack with dozens or hundreds of elements trough all the code. Unless you use non functional techniques which destroy composability and are in general bad software practices, like global state. Even if global state is obscured by calling it "application data" or configuration changes, when it is not data, but state data, and overusing the database for it. Unless, finally, you use a form of extensible effects monad. That is for me the only alternative that could allow Haskell programming in a functional style out of the laboratory, in the wild. This is for me the summary of my point in this discussion.
That all makes sense; cheers for the explanation. Was deliberately not implementing that benchmark so as to avoid publishing misleading results ever discussed? Or at least commenting the code for future investigators?
What will it look like when *anyone* can make a thinking machine understand their will? We already know what that looks like, since it's what human interactions look like. It's fuzzy, confusing, and often unsatisfying, since one rarely knows their *own* will, and knowing somebody *else's* will doesn't mean you will agree with it! In such a world, programming would involve overcoming philosophical disagreements, giving rise to the need for civic/government involvement. But, is programming more akin to building pyramids or forging iron? In 1000 years, will we look back and wonder, "How did they do that? It must have been aliens!!" Or would a computer from that era look to us as the Saturn V must appear to an Iron Age Viking?
There's also [this issue](https://github.com/ghcjs/ghcjs/issues/481) on github which gets some comments from time to time, but no resolution for now...
I don't know, really. I just think that it's better to stay on the conservative side with language extensions, unless it's universally clear what cognitive/compiler performance advantages they will bring.
Extra credit if you can pull this off _without_ using TH ! ;)
I feel like this would ruin how small and simple this library is.
Agreed; it's an important feature in a production-ready web services library but my guess is that this is moreso intended to highlight a simple example of what is possible using Generics than a new web application framework.
We want zero tokens, they want one token, and you herald two!? 
Looks like what typeclasses would provide. You would implement a type plus an instance for each plugin. The core program would call functions in the typeclass to manipulate your types. Your types can also take other types as arguments, or have instances based on other instances, which implements plugin "extension". You can also try to structure it inside-out. Instead of defining what plugins must be able to do, you can also export your core library directly. For example, do you want to visualize some data? Don't bother making the core lib call `toNiceData`. Just give me those `drawFooShape` functions. Other than that, I don't have much advice because I know absolutely nothing specific about what you want. The mention of the "visitor pattern" sounds like your core program calls functions in plugins to interact with them, so the typeclass one might be more suitable. The "link" part is the most confusing for me. What are you trying to link? Are those plugins so wired in that you can't get them out? OOP probably won't have helped either. Do you need to store data from the core into plugins or the other way around? I don't know what specific trouble you are trying to say.
Do you mean *runtime* plugins, in the sense that you compile your "core" program once and then dynamically load libraries ? If so, it's a bit annoying to do in Haskell. If you can recompile, then it's exactly the same as you would write in an OO language. You can define a record type that describes all the functions that you expect a plugin to implement. You can keep its state opaque, just like you would do in an OO language too. 
I intended to recompile with the new plugins. I don't want them to load dynamically. I just want them to depend on the Core program at compile time, and the core program to depend on the plugins at run-time. Basically, I want a function on the form, loadPlugin :: (Plugin plugin) =&gt; plugin -&gt; IO () which auto-magically handled setting up new views and signals. Or something like it. IO could be replaced by some monad stack. But it also seems like I'd have to modify my main function to be able to load the plugins, if I used this method.
I think you need to be more specific as to what you expect of your plugin architrcture. Do you want dynamic loading for instance? Is it just at matter of separate compilation (plugins live in separate libraries from the core)? You can take a look at some of the database access packages on hackage. Some of them support different backends, and maybe you can copy their approach if they qualify for your notion of 'plugon'.
Even if that is not what I'm looking for, it looks pretty interesting :) Thanks for the link!
&gt;But for what it does, how is it a "no-go"? New to software development? :) In this profession, the "barely manages to not completely fail on the first try" is the greatest enemy of the proper solution.
It's not "small and simple" if it's not complete, it's simply incomplete.
Nope, but I've been in software development long enough to know that when a manager says "we don't have time to do this right, just hack something together and we'll fix it later" that later is never going to come. I'd rather ways to do things wrong didn't exist in the first place so I couldn't be forced into using them by false deadlines set by people who will get the kudos for the apparent rapid implementation speed but will never face the consequences of the technical debt it creates.
Just what you would do with you OO language, `Plugin` is a record that holds all the "methods" you would want to call : data Plugin = Plugin { alterSomething :: Int -&gt; IO () , doSomething1 :: Foo -&gt; IO Bar , doSomething2 :: IO Baz , getVersion :: Version } In order to do that you would probably need to do something like that in your plugin: initPlugin :: IO Plugin initPlugin = do mv &lt;- newMVar initialState return $ Plugin (alter mv) (do1 mv) (do2 mv) "4.28" alter :: MVar InternalState -&gt; Int -&gt; IO ()
Suppose I were to fix the request method issue. Would there be any other things you would like to see fixed?
I asked SPJ about this as a tangent to another question. At the time I was asking about adding newtypes for kind # and he felt there'd be no particular issue with adding them in kind #. I need them to improve the type safety of a bunch of custom C-- code. I've also had usecases for them in kinds that end in `#` or `*`, e.g. `k1 -&gt; k2 -&gt; #`, or `k1 -&gt; k2 -&gt; k3 -&gt; *`, like you mention here, etc. as the Coercible instances you can derive for them are more powerful, and as you note they can be passed partially applied in a few additional situations. When I asked him about those he seemed more dubious as to their merit, but seemed able to be convinced. The conversation moved on to other things though, so no commitments were made. Anyways, I for one would love to have this functionality. Right now I wind up having to fake it by using class associated types without bodies and using reflection-like machinery to generate instances that violate the assumption that certain kinds are closed. But the code I need to use this for isn't the sort of thing that would meaningfully convince Simon it is a good idea for Haskell. ;)
Haha yea honestly I don't have a really practical use case for these. It's just something that's come up once or twice when trying to have some fun with overly-polymorphic stuff.
I tried Idris and ATS for a few days each, but that is hardly enough to make a me an expert. My conclusion is that I like it more in modern languages with good tooling that are not quite the extreme bleeding edge of PL research. On the theme of predicting the future, the thing to keep in mind about long term predictions and such, is that they are pretty what we have today except extrapolated to dizzying heights. There is no need to wait 50 years for inexact programming - that pretty much already exists today! Nvidia is really working hard on making those silicon brains right now. I actually haven't seen typed programming extrapolated like this - call it Singularitized - so I felt like giving it a try. Turning things on their head and taking things past their limits often reveals interesting new things. It does not mean it won't happen either :) But more seriously programming languages, unlike computer hardware, and unlike [optimization algorithms](http://www.math.uwaterloo.ca/~hwolkowi//henry/teaching/f14/602.f14/602miscfiles/UF_Entrepreneurship_19November2014.pdf) are hardly in an exponential trend. Deep learning is having its own spring right now and it might be exponentially improving, though the algorithms used have softer benchmarks so it is harder to judge their rate of improvement. ...Let me make another speculative prediction. If deep learning keeps getting better and better at this pace, around five years down the line and even less, there are probably going to be dedicated programming languages with automatic differentiation capabilities built into them capable of using GPU (and later neurochips) natively. One called Stalingrad actually exists, but I am thinking higher profile and much more useful. That does not sound like much, but having the soft kind of optimization as done by DL seamlessly integrated and actually having users that regularly use it in lieu of where they would use regular programming would change the world. Only then, at that point in the future can we start to consider bridging soft differentiable optimization with hard optimization which is MIP (mixed integer programming) - and regular programming for that matter. Those super type systems are not going to be made by humans.
Snap also has [snaplets](http://snapframework.com/docs/tutorials/snaplets-tutorial), which might also provide some inspiration.
Wasn't there a runtime plugin system in the makes for the clckwrks cms? I forgot it's name and on the phone now
I was also going to suggest type classes (I found how yesod uses them to be quite "plugin like") But if you need runtime plugin mgmt then this is not the way to go.
 :t \sx -&gt; runListT [putStrLn s | s &lt;- sx] Monad m =&gt; ListT m String -&gt; m () If you look at the type, you can see that it does not do IO.
Your citizens demand Intermediate Haskell: from monad transformers to the moon in 15 weeks. 
&gt; I disagree. Language standards should be conservative, language extensions should be liberal. Unfortunately there's only one "Haskell" that's relevant for any type of real usage, namely GHC/Haskell. That means that any *many* extensions become de-facto parts of the Haskell language -- if you're using Haskell for anything remotely practical. Additionally, many(?) of the extensions you've cherry-picked lead to *clear wins* and had better community support. What were the criteria for the extensions you picked out, incidentally? Why is MPTC (for instance) not on the list? &gt; Should we get rid of them all? False dichotomy. Not accepting ArgumentDo doesn't mean we need to get rid of any existing extensions, so I'm not even sure why you're bringing it up...? &gt; I use most of them daily and they make my code easier to read. ... in your opinon. There's no agreement as to whether ArgumentDo makes code easier to read. (I certainly don't agree with that.)
Very nice article! &gt; mtl, the “monad transformer library”, is a library providing typeclasses (MonadReader, MonadError) and concrete implementations (Reader, ReaderT, Except, ExceptT) for a handful of helpful monads. Afaik the concrete implementations are part of the `transformers` library. Mtl only has the instances and depends on transformers. &gt; ExtractT You mean ExceptT :) Do you have any experience with the `exceptions` library and how it compares to the one you're using?
&gt; Learning Haskell Thanks, I'll take this advice as I read it's a very good book. I've tried to take the time to learn Haskell in the past, but other projects and time and work get in the way. It seems a bit expensive though. I'll have to put in the budget soon.
Compare/contrast with servant?
Nah, it works. The core program doesn't need to have a list of types. It just needs to know that something is `Plugin a =&gt; a`. Yesod uses this heavily, for better or worse. The simplest example is the `Yesod` typeclass itself. The library can work with any datatype as the foundation of the app, as long as that datatype instantiates `Yesod`. Similarly, any helper code that works with the library can be agnostic to the concrete type in use. Really, the two techniques are quite similar. The core program either defines (1) a datatype or (2) a typeclass which enumerates all the values/actions it needs from a plugin. Then, to create a plugin, either you build a value of the built-in type, or you build a value of your own type, which instantiates the typeclass. I imagine there are pros and cons to both techniques. Typeclasses give you the flexibility to define your plugin type however you want, adding any extra records you might want. Conversely, (judging by my experience building Yesod apps,) I would guess that using a concrete type might make all your types a lot more palatable.
Two possible solutions: - An `Extension base plugin` type representing `base` extended with `plugin`. A typeclass may be used to provide an uniform interface for extensions. - A `Combine p1 p2` type representing the combination of two plugins. Then it should just implement the original plugin typeclass. Type operators can be used to reduce the syntax noise. There's an example of the combining approach, namely servant. You combine API types into a new API, and equally easily combine server implementations of the APIs into a new one matching the new API. The core, servant-server, translates the implementation into a wai application. See it here: http://hackage.haskell.org/package/servant
* It's a lot simpler - servant specifies services as an abstract type, and then uses a type family to get a type for the server, etc. Here the API spec is just a regular type with regular values. * Because of this lack of indirection, it's possible to derive Routers for records. For me, this is hugely more ergonomic than servant's :&lt;|&gt; trees, which require you to implement things in an opaque order and throw massive unreadable error messages if you don't. This is why I wrote Solga in the first place. e.g, you can write: data MyAPI = MyAPI { rng :: "rng" /&gt; Get Int , echo :: "echo" /&gt; Capture Text :&gt; Get Text } deriving (Generic) instance Router MyAPI myAPI :: MyAPI myAPI = MyAPI { rng = brief (getStdRandom random) , echo = brief $ \txt -&gt; return txt } -- serve myAPI :: Application * Solga makes routing decisions purely based on the type. First it looks at the type of your API and figures out the route that matches, and only then does it look at the implementation and perform any IO. This means that it will never for example authenticate a request if it knows that it won't match anyway. 
The only thing I can think of is that BangPatterns is an extension so if you want to avoid that for whatever reason you have to use seq. I agree that bang patterns are more readable.
Immense efforts were made in the past to get a (pure) `Stream` type like this into base, as a part of a stream fusion plan for lists. This was to replace the build-foldr fusion we still have, but it had mixed results, of course. It succeeded for text and vector where restreaming in the event of a failure of fusion is from an unboxed array, and operations like the monadic bind are not so important. The present case is much closer to Haskell lists than to vector, so I wonder why people are so confident that a build-foldr system would not be better? 
I developed a plugin architecture for web applications which supports adding plugins either at compile time or via dynamic loading. Though, the dynamic loading portion is only proof of concept. The repo is here: https://github.com/clckwrks/web-plugins A plugin is defined by the Plugin record: data Plugin url theme n hook config st = Plugin { pluginName :: PluginName , pluginInit :: Plugins theme n hook config st -&gt; IO (Maybe Text) , pluginDepends :: [PluginName] -- ^ plugins which much be initialized before this one can be , pluginToPathInfo :: url -&gt; Text , pluginPostHook :: hook } Obviously, this is very web server centric -- and that is to be expected since I only care about loading web server related plugins. The type parameters for `Plugin` will be fixed for a particular application. In the example, it is: type ExamplePlugin url = Plugin url Theme (ServerPart Response) (IO ()) () () In clckwrks, plugins have types like: Plugin AuthURL Theme (ClckT ClckURL (ServerPartT IO) Response) (ClckT ClckURL IO ()) ClckwrksConfig ClckPluginsSt If we want to do try, dynamic loading of plugins that the application did not know about at compile time, then we need things to be even more standardize. So, each plugin also needs to have a function with a known name and type. So, in the include example, all plugins will provide a function: plugin :: ExamplePlugins -&gt; Text -&gt; IO (Maybe Text) plugin plugins baseURI = ... Or in clckwrks we have: type ClckPlugins = Plugins Theme (ClckT ClckURL (ServerPartT IO) Response) (ClckT ClckURL IO ()) ClckwrksConfig ClckPluginsSt plugin :: ExamplePlugins -&gt; Text -&gt; IO (Maybe Text) plugin plugins baseURI = ... Now we can load a plugin using `initPlugin`: -- | initialize a plugin initPlugin :: (Typeable url) =&gt; Plugins theme n hook config st -- ^ 'Plugins' handle -&gt; Text -- ^ base URI to prepend to generated URLs -&gt; Plugin url theme n hook config st -- ^ 'Plugin' to initialize -&gt; IO (Maybe Text) -- ^ possible error message Here is an example main function: main :: IO () main = withPlugins () () $ \plugins -&gt; do initPlugin plugins "" clckPlugin initPlugin plugins "" myPlugin setTheme plugins (Just theme) hooks &lt;- getPostHooks plugins sequence_ hooks simpleHTTP nullConf (route plugins) I am glazing over the details a bit, but I gotta run!
I generally prefer bang patterns in my code, but if you are going into someone else's code that doesn't already use bang patterns anywhere seq is less new extensions. 
I think you are describing [HATEOAS](https://en.wikipedia.org/wiki/HATEOAS), which is not the same thing as REST. 
I wonder how useful static analysis could be for detecting space leaks. The first three seem like they could be found quite easily. The last two are definitely tricky to find, but as long as there are not too many false positives finding some space leaks automatically is better than having to search for them manually.
There only seems to be a couple of minor things really blocking a release. Just seems to be a lack of participation that's the issue.
I realise many of these arguments can be applied to dynamic vs static typing as well, so maybe these arguments suck...
Because if each readFile is deep within a function you might not realize you needed to catch it at the time you write this block of code. And the compiler won't tell you.
Some basic feedback about the repo would be that there need to be tests, and ideally also evidence they have been run (Travis-CI, Circle-CI and others are all great).
While you definitely have some valid points, I tend to disagree. At least as long as dynamic analysis consists of manually playing around with stack sizes and looking at stack traces it feels unnecessarily complicated. Compare that to a static analysis telling you “Did you mean to add a bang pattern to `i`?”. But I guess you could wrap the dynamic analysis to get to something close to it, so it’s more about the presentation than about static vs dynamic analysis.
+1. The whole reason for checked exceptions right there.
I think you're right in pointing out that using `:&lt;|&gt;` can get unwieldy. And this is a very appealing way of tackling that issue. It seems like it could (with GHC 8, or with GHC &lt; 8 and TH) be implemented as sugar on top of `servant`. The `servant` API type would be generated from the Generic `Rep`, and then another generic function maps the value-level products to `:&lt;|&gt;`. I'm not sure I understand the last point. What exactly is the difference to `servant`?
I use [bazqux](https://bazqux.com/) all the time. It is the best Google Reader replacement of which I know. It *happens* to be implemented in Ur/Web, but I didn't know that until long after I switched.
Yeah. For one, `servant` needs the field/case name info at the type level. So, it'd need GHC8's Generics. 
I'm not really following this, but: * The type of `build` seems very much _not_ to allow any monadic side effects: `build :: forall a. (forall b. (a -&gt; b -&gt; b) -&gt; b -&gt; b) -&gt; [a]` * I've experimented with stream fusion where there is no underlying in-memory representation, whether it be heap based or unboxed. In fact, `vector` itself provides boxed vectors as well, where the pointers are packed but the values are normal heap objects. I don't know exactly what you're referring to, but stream fusion certainly extends beyond unboxed arrays.
The boxed vectors in `vectors` in vector are pretty temperamental, though, and the "spine" of the sequence is an unboxed array (of pointers), not a potentially infinite Haskell recursive item. There are several church encodings available, e.g. the one in `logict` or say forall x. (a -&gt; x -&gt; x) -&gt; x -&gt; (m x -&gt; x) -&gt; x then given cons :: Monad m =&gt; a -&gt; ListT m a -&gt; ListT m a cons a ls = ListT (return (Cons a ls)) nil :: Monad m =&gt; ListT m a nil = ListT (return Nil) wrap :: Monad m =&gt; m (ListT m a) -&gt; ListT m a wrap = ListT . join . next we have build :: Monad m =&gt; (forall x. (a -&gt; x -&gt; x) -&gt; x -&gt; (m x -&gt; x) -&gt; x) -&gt; ListT m a build psi = psi cons nil wrap 
That Wikipedia page, however, describes it as a constraint of REST, implying that a REST implementation must implement it to actually be considered REST. 
You can't cache a GET anyway, because someone else may have done something that modifies the database and hence the response.
I can't agree. I suspect when *you* say "the typechecker is broken", what you mean is "the (old) typechecker has a hard time with the `runST $ do { ... }` idiom". But in fact, `runST $ do { ... }` simply *does not* typecheck, and the fact that it's usable in current GHCs is, frankly, broken. So removing the hack which lets it type-check *is* in a sense fixing the typechecker -- by removing broken, incorrect (albeit useful) behavior.
Yeah, I guess you can use them in arguments and returns manually, but you can't stick polymorphic things in them. So to be transparent you'd have to specialize it all away in the compiler or something. Maybe easier than that: it'd be nice if we really had a family of strict, boxed pairs, analagous to the family of lazy pairs exposed by `base`. Of course, I have no idea how to actually do this off hand (syntax etc)! But it'd be nice.
I'm fairly new to Haskell and haven't heard of checked exceptions; is https://www.reddit.com/r/haskell/comments/3g41au/follow_up_safe_lightweight_checked_exceptions/ a good place to start, or do you have any other recommendations?
Links to some of the literature, please? I'd like to see what the CS angle is on this topic that I haven't cited yet.
The second is less efficient since the int can't be passed around unboxed.
&gt; the only case I ever use ($) for Really? I use `($)` all the time! One common situation in which I use it is when I wish to describe a function as a composition of simpler functions, like this: foo :: Z -&gt; Y -&gt; X -&gt; Int foo z y = long_expression1 z . long_expression2 y . long_expression3 but I can't because the order of the arguments which is convenient to expose is not the same as the order of the arguments which is convenient for the implementation, so I write it like this: bar :: X -&gt; Y -&gt; Z -&gt; Int bar x y z = long_expression1 z $ long_expression2 y $ long_expression3 x I guess I could simply define `bar x y z = foo z y x where foo = ...` and then I can stick to function composition, but that feels like pushing point freeness for its own sake, decreasing readability instead of increasing it.
Hi friend! Very sexy release :D
With minimal amounts of boilerplate, it should definitely be possible. Something like this maybe: data Foobar = Foobar { foobarID :: Int, foobarBaz :: Text } foobars = [ Foobar 1 "Number One" , Foobar 2 "This is Two" , Foobar 3 "Three little pigs" ] instance RestGettable Foobar Int where restGet = \id -&gt; case (filter ((== id) . foobarID) foobars) of [] -&gt; fail "Not found" (x:xs) -&gt; return x main = do let api = makeAPI [ getRoute (undefined :: Foobar) ] runAPI api The only truly ugly thing here is the use of `undefined` just to select the right type... 
&gt; To elaborate on what everyone is doing wrong: all REST libraries I'm aware of let you build URIs (sometimes even in a type-safe manner) but document the resulting URI (e.g. with Swagger) and this is exactly wrong. Yes! This needs to be said more often and more vigilantly. The "map route patterns to backend queries" approach is not RESTful; "map paths to resources, verbs to actions on resources, and implement said actions transparently as queries" is. [This framework](http://python-eve.org/) seems to be doing the right thing, FWIW.
On mobile and can't watch the video. Is this for letting coders make VR, or for coding within VR? 
It's for both: letting coders make VR by coding within VR
Yeah I guess I don't quite understand the difference between the two, I should've at least checked it out on a few more things before assuming this was their relationship
Now let's see the type errors in VR
I have another construction you can add to the options. There is a lazier reverse that at least returns the spine of the list in the infinite case. It is done by zipping the original list with a version obtained by lazily reversing itself using irrefutable pattern matches. So technically, you could foldr over the more lazily reversed list with a flipped function. Then so long as you didn't need to look at any of the values your foldl could terminate on infinite lists. (You can fuse the intermediate list construction away `build`-style.) I don't recommend this version in practice, but it is an interesting intellectual exercise.
Needs the big red 3D source engine style **ERROR** message. https://i.imgur.com/Ceoqvu1.jpg
I think everybody who work with Haskell did choose to do it, and put the dedicated effort into it. However, I know lot of people using regular "mainstream" platform (Java, Js etc..) who just use them because this is their job, without any passion or interest into it. So, as an average, you would find more people dedicated to Haskell than to Java/etc.. However, I would not extrapolate this cultural context to the IQ of people choosing Haskell vs Java/etc.
I think you could make that argument for any non-mainstream language. If someone has taken the time to explore languages outside the Java/C#/Ruby/Python/PHP/JavaScript mainstream, they are probably a better programmer, because they are curious. But what that non-mainstream language is doesn't matter quite as much. If someone said they had written a full program in Haskell, or Clojure/Factor/whatever, I'd take that as a sign of craftsmanship.
I use both of these styles all the time, and I very much enjoy them for easily-readable, compositional code (bonus points if, like me, you are a religious user of `where` blocks for particular subexpressions). Sometimes you even mix your dollars and dots!
How about an explicit introducer: (\| foo . elem _ ['a' ... 'z']) . bar foo . (\| elem _ ['a' ... 'z']) . bar foo . (\| elem _ ['a' ... 'z'] . bar) \| foo . elem _ ['a' ... 'z'] . bar (chosen because I think that \| is not legal syntax anywhere) This wouldn't make it perfect, but there would certainly be situations where it would be cleaner than manually writing the lambda arguments. If there are multiple _, they should be handled in lexical order. Or perhaps we allow _1, _2 for anonymous stuff, so \| _2 + _1 is \x y -&gt; y + x
I really like Haskell, but I'm the worst programmer I know. I'm good at math though.
Accurate.
It's the [Python paradox](http://paulgraham.com/pypar.html) updated for 2016. It's not that Haskell makes people smarter, or that only smart people can use it, but it's the trend among smart and curious people right now.
This doesn't work for most of the existing monad transformers. There is typically at least one member of each of the classes that can't be defined with just composing in `lift`. `State` is the exception, not the rule here. This is in part because the effects in the mtl contain enough information to characterize them via laws rather than just give you the ability to say, `ask` and not manipulate the environment with `local` or `tell` but not be able to `listen` or `pass`. Finally, such an instance would get in the way of having any other instance on a type that happens to take of kind `(* -&gt; *) -&gt; * -&gt; *`. e.g. despite how we treat it in the `free` package, newtype Free f a = Free (f (Free f a)) | Pure a isn't really a monad transformer. Or you might have newtype Whatever s a = Whatever (MyState -&gt; (a, MyState) which under polykinds could allow (s :: * -&gt; *) as one of the possible kind signatures. You might want an instance of MonadState MyState Whatever, but if you turned on polykinds when defining that module and worked parametrically in `s` you'd be in for a lot of pain.
I'm a terrible programmer, which is why I love Haskell. Haskell helps me look less stupid by catching my mistakes at compile time.
There's a negative association that can come from this as well: those who struggle with Haskell and give up on it thinking "it's only for smart people." The category theory discussions exacerbate this, in my opinion, and makes this not a harmless opinion to advance. Personally, I don't like focusing on intelligence, which people often think of as fixed. I'm more interested in motivation and the fluid nature of capabilities. Maybe there's some demonstration of someone's motivation if, in the face of large difficulties, they continue to pursue the esoteric or opaque. It doesn't mean much about intelligence to me and I think it's not productive or useful to spend a lot of time talking about intelligence or pat ourselves on the backs about our intelligence because of the activities we engage in.
Haha, well I *am* actually planning on packing up the code editors into a 2D IDE, so they'll get that too!
Thanks, it makes sense now. I didn't notice the overlapping. I'm still in a stage where those kind of errors surprise me, often. Anyway, that DefaultSignature extension looks very interesting. I guess I have some use for it :) 
Awesome! Too late to submit to FARM this year, but you should definitely try for next year: http://functional-art.org/
I wonder how many common patterns could be added to an optional pack for hlint...
I think learning Haskell (or pure, typeful, functional programming in general) can make you a better developer. Which is another way to account for a correlation that's got less presuppositions about the quality of people Haskell "attracts" or whatever. But I've seen enough Haskell code to know that you can learn to write it without learning the things that I think are important to learn from "the Haskell way". Like the saying goes, you can write COBOL in any language...
Actually, 10 years is younger than I have any experience teaching to. I'll be trying that for the first time this fall, but using a block-based version of my own environment. But kids are all different, and it sounds like it may work for this particular student. I'd be much more cautious with a less self-selecting audience.
I'll reiterate the concern. The problem isn't so much that the task is hard, as that the task is very ill-defined. A lot of programmers go through a stage where they decide to design a programming language, and it's amazing. But at that point, they have enough experience programming to have developed some ideas they want to try, or at least some solid idea of what is actually needed from a programming language. Your student, from the sounds of it, will not only struggle with the implementation, but will have no idea what it means to design a programming language, how to start thinking about what the result might look like, or how to decide whether an idea is good or bad. The risk is that if you push things that far, you'll end up doing a lot of dictation: telling the student what to write or type next. This end up being a lie: you aren't teaching at all. This happens SO often in all the "program a video game in a day" activities that have popped out of the woodwork. Even at its best, it's just a recruiting activity to encourage more students to give computer programming a try, and hope that they will go back and really learn later. Sounds like your student is already motivated. Part of your role as a teacher is to keep students engaged by letting them work on what excites them, and it sounds like you've got that down. But another part is to recognize when they are not ready and won't learn anything with their chosen approach, and redirect their energy in a more productive direction.
Turns out it was being compiled with threading support. However, while that was an accident (`stack` defaults apparently), I actually will need to utilize all cores eventually for this program -- but hopefully not just in reading the input data stream. The CSV file is just a local file -- no weird network stuff happening here.
Are you sure you're version is correct? It seems that it can still build up thunk for a while on this line: ```haskell f i (A : ts) = f (i + 1) ts ``` 
What ML MOOC?
Thank you!!! Yesss, I will for sure!! Have been wanting to go forever. /u/yaxu was a huge influence on me getting into Haskell in the first place : )
&gt;This is all on FreeBSD. No idea why that should matter, but I'm sort of grasping at straws here. I'm equally confused, but I have a suspicion this will turn out to be a bug due to friends but getting as much use with Haskell. I could be totally wrong. If so, I hope someone tells me how :) 
Yeahh, and auto-updates and support and all the nice Steam social features (that I'll probably be leaning on for the upcoming multiplayer!) It just didn't seem right to have the first (afaik??) major 3D (and certainly VR) Haskell release be closed source : )
The closest thing I can think of to what is being discussed here is data refinement. For that, I would recommend de roever and engelhardts book. Also, the handbook of theoretical computer science refers to tons of the classic papers in this area
Haskell seems vastly more prone to space leaks than strict languages are. Space leaks are an absolute bitch to diagnose and fix even on the best of days; and Haskell doesn't even have its own version of `valgrind`. &gt;Alex and Happy have been running well with these bugs for years and years! This makes me question the Haskell community's definition of "running well". I've seen people warmly recommend HXT even back when it needed gigabytes of memory to parse relatively small XML documents. (I do not know if the situation has improved.)
^Most rational thing I've ever read on Reddit. 
I would next check that if you pass a lower capabilities arg it consumes correspondingly fewer cores. Then i'd check GC stats to see if this time is all burned in concurrent gc. 
Which is exactly the reason why the first example fails, but -of course- you know that.
Higher IQ? No, I don't think so. I think it has more to do with the *personality* type. If you have time to learn Haskell, you are most likely *personally* motivated to learn it, and that is huge. It's the difference between intrinsic and extrinsic motivation. We want employees to care on a personal level, but we can't *force* it. I like programming in Haskell just for the sake of it. I think most people who codes in Haskell generally just like programming, but that's a wild guess. Maybe I'm just projecting.
Not sure if you are kidding. But same here, and I have been just told (repeatedly) by a someone with phd that I display traits of a bad programmer...and I don't necessarily disagree with them. :)
I think I'm an average programmer in terms of skill and ability. I work with legit good programmers and they are beyond my skill level. I'm frequently reminded of this by them when I review their solutions to problems. I may not be the smartest programmer, but I know an elegant solution when I see it. On the other hand, I love programming and I've been interested in it since teen years. So I attempt to compensate for my skill and ability with experience, but ultimately I'm nothing special when it comes to programming. In fact, I would go so far as to say it's being completely average that attracts me to languages like Haskell. I learned years ago that I'm not smart enough to maintain code in languages like Python or Common Lisp. I need the crutches that static types provide. 
This is a common misconception. The real problem is that avoiding, detecting, and fixing space leaks in Haskell requires techniques and intuitions that are somewhat different than those required for traditional imperative languages. The differences are subtle but significant. So a common scenario is: An experienced developer in an imperative language who is not aware of those differences comes to Haskell. They think that they are following best practices to avoid space leaks. When they later face the stark reality that they have serious space leaks, they blame it on Haskell. EDIT: As a result of that difference, and the fact that until now far less people have done practical software engineering in pure functional languages than in imperative languages, the standard tools and techniques available are far less mature. So in that sense, it is fair to blame Haskell. But Neil is fixing that now. :)
Hmmm. what's his definition of a "bad programmer"? Why does a phd qualify someone to judge this?
I mean, there's no right or wrong here, I'm not showing two semantically same functions. I'm just saying that bang patterns work quite differently than `seq`. In our case, first function is actually always strict on its first argument, because when it returns, it returns the first argument (so `seq (f x)` forces `x` in any case). So maybe that wasn't the best example. Here's a better one: f' :: Int -&gt; [T] -&gt; Maybe Int f' i [] = Just i f' i (A : ts) = f' (i + 1) ts f' !i (B : ts) = f' (i + 2) ts f' i (C : ts) = Nothing This is only strict in first argument when `[]` or `(A : ts)` patterns don't match the second argument. Example: λ&gt; isJust (f' undefined []) True -- not strict in first arg λ&gt; isJust (f' undefined [A]) True -- not strict in first arg λ&gt; isJust (f' undefined [B]) *** Exception: Prelude.undefined -- ops λ&gt; isJust (f' undefined [C]) *** Exception: Prelude.undefined -- ops, tried to match (!i, B : ts) on the way In this version it's possible to accumulate `i`: λ&gt; isJust (f' undefined (replicate 10 A)) True (we know it accumulated a thunk because it has to traverse the whole list before returning `Just`)
Still time left for Compose :: Melbourne if you're looking for somewhere to show this off. It looks Awesome!
I think Haskell attracts people more likely to do programming things based on principle or ideals (can be good, or bad), stubborn enough to put up with friction of bad docs, bad tooling, lack of resources in order to achieve it (good, Haskell isn't a bandwagon you jump on, it's a hill you have to climb, with people at the top throwing books at you). People more attracted to novel things (can yield good results, and indicate passion) rather than particularly practical means (can lead to rabbit hole work that never gets done). They might be more demanding people (can mean reliable, or difficult to work with). Said people are also possibly the kind of person who would stick vehemently with your company, or just up and leave your company because of something that does/doesn't appeal to their ideals. Couldn't comment on higher skills or intelligence, that's a bit vague.
There is one difference, I think. Haskell is a lot more different to Java and Python than Java and Python are different to each other. It's not just "learns another language" but "learns a radically different language."
The worst programmer I ever worked with had a PhD. He worked for me for almost a year. I tried really hard to work with him, teach him, find his strengths, but he only caused harm to our code base. I couldn't fire him because my boss didn't believe someone with a PhD could be bad. I asked for him to be moved to another project, and the lead on that project asked for the same, until he was finally fired after more than 2 years at the company.
High intelligence is not always a necessary trait for a programmer. If it comes coupled with arrogance it can be the worst thing.
That could depend on `CC.stdout`. If it's in turn relying on IO's instance of Applicative, it could be the same problem!
Space leaks don't really go away with experience, you just stop having the most obvious one. These aren't the kind of space leaks that blow the stack or use O(n) memory when you should use O(1), but those that add a constant factor tax to everything. Here's an [example from cassava](https://github.com/hvr/cassava/issues/17), which wastes hundreds of MB in lots of small temporary closures (without any long buildups of thunks like in the example in this post.) The solution in this case was to make every function strict, including `fmap` and all `Applicative` combinators. I would go as far as saying that if you using `fmap` in your program you're probably leaking some memory unnecessarily.
&gt; The real problem is that avoiding, detecting, and fixing space leaks in Haskell requires techniques and intuitions that are somewhat different than those required for traditional imperative languages. The differences are subtle but significant. No, the problem is that debugging space leaks requires *exactly the same techniques* as reasoning about imperative programs: you have to reason very carefully about evaluation order and how thunks are aliased. This is a real shame because the main selling point of functional programming is that you shouldn't have to worry about aliasing.
&gt; &gt; I learned years ago that I'm not smart enough to maintain code in languages like Python or Common Lisp. I need the crutches that static types provide. &gt; Static types can be nice, but isn't the test suite much more important? In case of C++/Java style static types, I'd say yes. But in case of Haskell's strong-static type, it has been said that it seriously reduces the need for "boring" tests. Also when refactoring (and /u/dagit mentiones "maintain code in languages like Python or Common Lisp", which often entails tom some sort of refactoring), these "boring" tests really slow you down; where Haskell's strong type system really saves your ass. 
I also noticed that `stack.yaml` contains a bunch of `../some_dir` dependencies, doesn't this mean that this won't be compileable for anyone who simply clones this repo?
I think Haskell has a very different trick up its sleeves. It doesn't look fun to the outsiders nor does it look very "get the job done", but all throughout the IT world the language and its users are respected and feared like dark magic. I think this is another kind of hype that attracts a special kind of programmers.
This is interesting. Would this have arisen if there were a version of `V.fromList` that strictly evaluated things before constructing the boxed vector? The main troubles in [these](https://github.com/hvr/cassava/commit/f6641c5048cae492fe18fe073171771477ab4175) [two](https://github.com/hvr/cassava/commit/3be2878559efc27b1440a0753b0a48cad53c8136) commits seem to have to do with `fromList` and are managing the mix of strictness and laziness you get with boxed vectors, e.g. by making the list given by `sepBy` stricter in the elements with `sepBy'`. But maybe a stricter `fromList` would come too late.
If GHC could be made to warn about obvious leaks and/or potential leaks (optional warning), then the whole "haskell leaks memory if you write pretty code" complaint would become less of an issue. Also, having warnings will make them more visible and quite possibly lead to improvements in all libs and maybe even in GHC itself. I love the aesthetics of Haskell, but the semantics are a tough sell compared to strictly evaluated languages or even Rust's resource handling nanny that reminds you if you forgot something.
Working with Haskell you need more grit and stubbornness than intelligence. The learning curve is steep and the payoff is not obvious. In my case, learning Lisp macros was easier than finally understanding Haskell monads :-)
All language communities have sub-groups of people who are very impressed with their own intelligence. Without being able to objectively justify why.
Nice write up! Are Node and NPM only used for installing deps and the build script; or is GHCJS also somehow dependant on it?
Would this be useful in combination with Yesod? Does it have a way to allow typed-URL-building?
Not in this subreddit :)
Wow, Amazing ! The feature set is really big already ! I love how simple it is. here are a few thoughts: I think that "simple typesafe routing" is quite an unfortunate description :/ Many people probably skipped this thread because "typesafe routing" is nothing new. For instance, tekmo's [auto generate service api endpoints](http://www.haskellforall.com/2016/07/auto-generate-service-api-endpoints.html) post ([reddit](https://pay.reddit.com/r/haskell/comments/4raxz9/autogenerate_service_api_endpoints_from_records/?st=iqasscfp&amp;sh=7c46b5c9) here) was IMO more descriptive for a similar concept. I also have the impression that a few important things are still missing for the lib to be 100% usable as-is. For instance, ability to dispatch according to the presence of headers or, ability to pass header values to handler seems not to be built-in. Also, content type handling seems a bit week (compared to servant) In the same time, I also have the impression that those features shouldn't be hard to add. Also, some features like file upload seems to be more mature than in servant. As mentioned before, a test suite would have been great. All in all, I'm very happy to see this lib ! I was really wondering why nobody made such a library yet. I'm really looking forward to see swagger support. I'll soon give it a try ! Thanks a lot for sharing ! /u/vahokif : do you use it in production for some projects ? Do you have any plans for it ?
&gt; * likely to do programming things based on principle or ideals &gt; * more attracted to novel things rather than particularly practical means &gt; * would stick vehemently with your company, or just up and leave your company because of something that does/doesn't appeal to their ideals I can definitely see myself reflected there :D Although that last one, while I agree with its general sentiment, sounds a bit extreme, IMHO. I wouldn't quit because a company wants me to work with language X, but I'd quit if a big portion of my feedback regarding the technology (and potentially the development process itself) is dismissed.
Also IQ doesn't measure creative intelligence, which I'd argue is important for pretty much all real-world programming; so IQ and programming skill might not even correlate.
It was minimumBy.
I always figure that when I work with Haskell I get to have this long dialogue with the compiler in which it constantly tells me how I'm wrong with the disembodied, infinitely patient voice of Simon Peyton Jones, until it finally doesn't say anything and the result works.
&gt; Haskell isn't a bandwagon you jump on, it's a hill you have to climb, with people at the top throwing books at you Scarily accurate image. Yet, all higher education is an embodiment of this ..
While I applaud Processing for making programming so engaging via its quick visual feedback, and therefore I applaud your effort as well, I wonder why you chose to mimick the original Processing language so closely? Processing happens to be an imperative language, but I don't feel that this is an important part of its appeal: the important bit is that it does not take a lot of effort to get something cool on the screen, and as [gloss](http://hackage.haskell.org/package/gloss) demonstrates, a pure API can achieve that goal as well.
I don't agree that is a "memory leak". The chances that this kind of constant factor would be the bottleneck for requirements acceptance of an end-user application are extremely small. Cassava happens to be a widely used (and I might add very high quality) general library, so that kind of low-level optimization makes sense. But most people are not writing such libraries. Manually modifying the operational semantics from the default in that way is almost always a massive premature optimization. Elegant idiomatic semantically clear Haskell code is not expected to be fully memory-optimized Haskell code, just as in any other language. The fact is that idiomatic Haskell code written by an experienced Haskell developer is well within reasonable memory and performance bounds just as often as unoptimized code written in any other practical programming language by an experienced developer in that language.
[removed]
No the techniques are quite different. If you debug space leaks by mentally "compiling" the Haskell into imperative code and tracing the steps, you are going about it wrong. It is true that space leaks come from interactions with the operational semantics chosen by the compiler for your program. But techniques like Neil's can help abstract those kinds of operational dependencies into formally verifiable properties of your code.
"Perhaps you intended to use DataKinds?"
That's not what Dunning and Kruger showed: &gt; The pop-sci version of Dunning-Kruger is that, the less someone knows about a subject, the more they think they know. The actual claim Dunning and Kruger make is much weaker. [...] In two of the four cases, there’s an obvious positive correlation between perceived skill and actual skill, which is the opposite of the pop-sci conception of Dunning-Kruger. A plausible explanation of why perceived skill is compressed, especially at the low end, is that few people want to rate themselves as below average or as the absolute best. In the other two cases, the correlation is very close to zero. Source: http://danluu.com/dunning-kruger/
Only in the same sense that all C++ projects are research projects given C++11 and C++ 14.
And consumes them.
I think this link is an entry point : [design patterns in haskell](http://blog.ezyang.com/2010/05/design-patterns-in-haskel/) . It makes a bridge between Gang of Four design patterns and Haskell mechanisms that can achieve a similar effect. EDIT : [here](http://blog.octo.com/design-patterns-saison-2/) is another ressource but in French. The author discusses Iterator, Strategy, Visitor, Interpretor and Command patterns in Haskell with some code snippet.
Very interesting! This more or less fixes the issue when using the first version, but the Conduit version still suffers from the same issue. Notably, even in the first case, the program's throughput is still quite a bit slower (about 30%) when compiled with threading support and using `-qg1` compared to not compiling with threading support. --- Update: Combining `-qg1` and `-A32m` helps out the Conduit version quite a lot. `-A2m` wasn't enough for some reason. Larger values don't seem to help. It caps out at about 23 MiB/s throughput on average, whereas when compiled without threading it's a bit over 30 MiB/s. It does slightly outperform the simple lazy Text version though, which is capping out at about 20 MiB/s for me when compiled with threading. I should try a `pipes` version of this.
This is interesting, OK I'm using lists as it seems more natural to express expressions of arbitrary length without the need of parentheses generally. 
&gt; This language extension allows us to take a string literal such as "Some text." and use it for arguments that require type String, Text, or [Char] (a ['l','i','s','t'] or array of characters). String and [Char] are the same type, and you don't need OverloadedStrings to use string literals as values in String.
Well the implementation was already accepted long before we were aware of the issue, so we would have to have removed existing implementations instead of just rejecting new ones. And keep in mind that this affected languages other than Haskell, so such a precedent would have forced us to investigate most of the implementations of the entire suite. We simply didn't have the manpower for that. As for commenting the code, I think that's a sensible thing to do.
Taking the time to learn a non-mainstream programming language indicates that the person is passionate about programming. That's the [Python paradox](http://paulgraham.com/pypar.html), thus named because Python was non-mainstream at the time it was coined. And this explanation rings true to me, because it matches my behavior: I'm super passionate about programming, I do it at work all day and then I come back home and I do it all weekend too, and over the years I've learned many non-mainstream languages for fun, including really obscure ones like Pizza and Icon and CeasarJ. Does picking Haskell as this non-mainstream language indicate a passion for correctness? This does not sound true to me, because that's not at all how I picked up Haskell. I was a Ruby fan, and the authors of what was at the time the most influential book on the language had written that programmers should learn one language per year: year one was Ruby, year two was Haskell... and there never was a third year, they just stopped updating their list. So every time I stumbled upon that list I was reminded that I still hadn't managed to learn Haskell yet, which is why I kept trying to learn it for the many years it unfortunately required to learn the language with the material which was available at the time. I think what is more relevant is not why I decided to pick up Haskell in the first place, but why among all those non-mainstream languages I have tried it is Haskell which I have chosen as my default language, the one whose community I identify with. I don't think that's because of correctness, I think it is Haskell which taught me that correctness really is that important, not the other way around. Rather, I think it is the precise types that hooked me in, they (and Agda's even more precise types, which I had also learned along the way) allow me to model the problem much more clearly, it allows me to understand what I'm doing in a way which was previously locked inside my head because I had no way to express it in code. So no, I don't think Haskell programmers are likely to have picked up Haskell because they were already intrinsically concerned about correctness before they picked up the language. I do think that programmers who end up sticking with Haskell might have already been intrinsically modeling the data in their heads in a way which matches Haskell's precise types, even before they encountered a language which allowed them to express it. But I'm a sample of one, so I'd be quite curious to see if others on this sub feel the same way!
A quick follow up to you and /u/alien_at_work I implemented a version of this idea in `hpack`. Take a look at the [WIP PR](https://github.com/sol/hpack/pull/112) and please chime in if you have any thoughts! It seems to me that it all works out, and doing it this way is totally backwards compatible with `hpack`, `stack`, and `cabal`... and doesn't require touching `Cabal`!
A loyal of them aren't going to make sense just due to immutability. The decorator pattern, however, takes on a life of its own. Monad transformers are literally just one example.
Yes, [I think something like that is a good idea](https://www.reddit.com/r/haskell/comments/4fqoke/thoughts_on_using_as_syntactic_sugar_for/d2co9nx?context=1), modulo some oddities with `let` and `where`.
I disagree. Even in volunteer-powered projects, a good contribution structure can help a lot, and make everyone more efficient -- instead of less as you seem to assume. My intuition is that this is especially true for language evolution which is a topic fraught with bikeshedding opportunities (sometimes necessarily), and where the socratic process of asking for clarifications on an informal proposed design can be very long and time-consuming. Exhibiting specific standards for a proposed change or contribution upfront can help a lot to delegate these efforts away from a small core of maintainers/gatekeepers, to the larger contributor community, and that is very important.
I don't think that's true: a good process saves time, even in the relative short term. Right now it's hard for people to contribute *and* it's hard for the organizers to solicit contribution *and* (I argue with no direct experience) hard for organizers to *accept* contributions. Or, at least, harder than it should be. "Good", is, of course, a bit of a weasel-word here. And I don't have any concrete suggestions on what that would look like or even what we *want* from a "good" process. But I'm confident that we can adopt a process that's more efficient and less time-consuming for *everyone* involved.
This is incorrect, only programming on hardware will be forbidden. Now if you'll excuse me, I must set my mind in motion by will alone... [sips Sapho juice]
You quickly run into the problem of dictionaries between languages: words don't translate one-to-one. You can try writing prose using a dictionary to look words up but you'll end up with stilted writing that is, at times, actively wrong. The same is true for what you're proposing but *worse* because OOP patterns don't capture universal notions. They're solutions to specific problems that crop up in OOP and often neither particularly natural nor elegant. Some patterns don't make sense outside the context of Java-style OOP, some don't solve problems applicable to Haskell and most of the rest map to multiple different idioms and constructs in Haskell, depending on what you're trying to accomplish. Just saying "X pattern is Y in Haskell" is misleading at best. I've seen this confuse a lot of people. They take an approach they know (ie a specific pattern) and ask how to do it in Haskell. The answer is usually: "you wouldn't, what are you trying to accomplish?" Sometimes this comes off as hostile¹, but it's inevitable—the Haskell approaches that corresponds to different uses of the "same" patter often differ wildly. Sometimes you just wouldn't write code in the first place that boxes you into the pattern. This is even trickier to appreciate: it requires a high-level understanding of what a *whole Haskell program* looks like and will seem unnatural if you're used to languages where you *do* organize programs in a way that forces you to use that pattern. (In Haskell, this often comes up when somebody used to an imperative language wants to mutate a variable—the answer is far more often "we wouldn't" than "reach for ST".) My point is that this is really the wrong way to organize an article. The domains are too different for patterns to meaningfully translate. If you want something similar, structure it like a cookbook instead. Have a list of a bunch of specific tasks you want to accomplish and then compare how you would do it. In Java this might rely on a specific pattern, in Haskell it'll be something different but, crucially, the focus is on *what you're accomplishing* rather than the pattern involved. You should also make sure that your examples are high-level tasks not tied to the language; it doesn't make sense to compare how you would use subtyping or inheritance since that only makes sense in Java, for example; instead, talk about actual, concrete tasks. **footnotes** ¹ Sometimes the person who asked the question gets really angry at this sort of answer. On IRC, at least, this often seems to come from people who were trying to demonstrate how Haskell sucks by asking a question they thought was unanswerable rather than genuinely looking for an answer.
Also tests are a lot easier and less boring thanks to property based testing.
The copyrights for `happy` and `alex` are from 1993 and 1995; people have been using these programs for over 20 years. The principles nmitchell is working with are by now known best practices.
Yeah, I've been procrastinating about publishing it but then I saw Tekmo's post and didn't want to wait too long since people were obviously thinking along similar lines. :D We're using it in production at a stealth mode startup and it's been working great. We started off using servant but it really started to annoy me, so I wrote Solga and ported everything over. The included Routers are the ones we've needed so far, so I'm sure there are a lot of useful ones that could be added! PRs are welcome. :P
Agreed. So OP - can you come up with a few concrete examples of, for example, when the "repository pattern" might come up? "Concrete example" means an application that does something. Let's see how modeling that application in OOP leads to the repository pattern, and then we'll talk about how we we would model that same application (in a similar "spirit") in Haskell.
As to my goals, as someone who has used strict Haskell for 5+ years, I consider space leaks the main significant drawback of laziness - in every other way I prefer laziness. This is my attempt to remove the only advantage of default strictness.
See https://www.reddit.com/r/haskell/comments/4r02sz/does_foldl_have_to_be_slow/d4xh498
For the database example, I'd probably define a DSL with different interpreters/compilers for different databases. The code would look quite different from Java and the repository pattern.
There are really two options here. Either the template engines each extend their API to return a list of files to be pushed or the server parses the generated HTML In both cases you would at least want a way to opt out for given files (eg a large ghcgs generated js file).
Corporate language backing is not just unnecessary, I think it's pretty harmful.
You can't deny how much more productive Rust's language development has been than Haskell's. Rust has a lot of manpower pushing the language forward that Haskell just doesn't have. I don't see any inherent downside in Mozilla's influence on Rust, and Facebook has already had a positive impact on GHC, driving the development of ApplicativeDo and fixing several GC bugs.
The problems that the so called design patterns solve, are almost always exclusive to OOP. Sometimes there is some genuine algorithmic idea in there that has broader applicabiltity, but that seems mostly accidental. Of course you could call a lot of things design pattern, but for the way the expression is commonly used, I stand by that observation.
Someone that has written a non-trivial program in Haskell is someone that chose to wear the "hair shirt" of purity for some reason. I do see that as some additional level of craftmanship. Unfortunately, I'm not a craftsman of anything besides code, so I don't have a good analogy to draw on, but it's like undertaking a project and using a rare tool in favor of a commonly used one. Haskell isn't entirely unique, but there's less than a dozen general purpose (i.e. pac-man complete) pure languages out there. And, some of those chose to use uniqueness types instead of monads to model references/state. It is definitely a "rare tool". Sure, that "rare tool" might actually not be useful all the time, but you've grown as a craftsman in learning to use it. You might even decide that using that "rare tool" (or a specific "tool set") produces markedly better output, and switch to only producing "artisanal" products or preferring those "artisanal" methods when producing your own products.
Hey u/lukexi, congrats! Cool release. Two questions - 1) looks like you went with an entity/component system - was this the first architecture you tried, or did you go through a couple iterations? 2) did the parallel matrix operations (or any other parallel optimization) turn out to be very valuable?
Right, if C is a 5 lb. sledge, and C++ is a 10 lb. sledge, then Java is a ballpeen, and C# is a claw hammer... Then, Python is a Drimmel kit. And, Haskell is a food truck. (Monads = burritos)
&gt; You can't deny how much more productive Rust's language development has been than Haskell's. I can. I don't measure productivity in lines of code in the compile or coders using the language. I measure productivity in Ph.D. theses containing it. ;)
Thanks Schell!! The ECS was the first thing that felt powerful enough and easy enough to understand to not get in the way. Still a lot of work left to do to reap more performance benefits from it and make it easier to add components freely (still too much boilerplate). And I'll probably be just simplifying it down more to an "EC" and have the systems be just invisible entities with a known set of components. That will be part of moving even more of the engine into the runtime-editable space : ). The parallel matrix experiments left in the codebase aren't used since no, they didn't help : ). The concurrency architecture that did turn out to be a *huge* win was just splitting out the VR thread into one that just receives a scene description, and handles grabbing the headset pose data and rendering the newest received scene description to headset in a tight loop. All the rest of the logic (physics, user code, hot code updates, etc. etc.) live in a separate thread. The big upshot of this, besides providing nice concurrency, is that the render thread becomes much much harder to disturb, which is basically priority number 0 in VR development (as missed frames cause a huge lurch attached to your face : )). If the logic thread hits a blip and takes too long to generate a scene description, or even crashes entirely, you can still walk around the scene smoothly with the last received geometry. Have been locked in my house for a looong while getting this finished haha, but would love to meet up and talk some time!! Email is the same, this username at me.com : )
(that all lives in https://github.com/lukexi/rumpus/blob/master/src/Rumpus/Main.hs by the way!)
Uh ... Yeah, sorry about that. That was me.
Would it use Comic Sans in accessibility mode?
I don't really think this is true. Rust has a lot more manpower behind it, and you can tell. They have a lot going on to discuss changes to the language and ratify proposals. With Haskell, we're having trouble just getting people to turn ideas into proposals. I think this is largely because no one has the time to write thoughtful proposals for these things. And this isn't due to a lack of problems compared to Rust; Haskell has several problems that need addressing which are simply being ignored for now (Strings being chief among them).
Awesome, thanks for the run down. I'm going to have some fun poking around in the code. I would love to catch up soon - I'll hit you up :)
Slightly OT but IMO the repository pattern in OOP (among with some more) is not necessarily about switching databases/persistence but about reducing coupling. If you for example look at things like DDD/Onion-architecture you'll see that the infrastructure stuff like database-access is pushed outwards (away from the core/domain-logic) and the the OO solution for this is interfaces which soon lead to the repository-pattern (most people I met used a form of this even if they never heard of the name before) Now the DDD stuff and especially the Onion-architecture seems to be a good fit for FP/Haskell too IMO - but interfaces might or might not be the best fit for the *glue* (at least if you translate interfaces into type-classes) For me most of the time records are fine but it surely depends. If I'm on F#/.net I'll use interfaces (why not - the fellow C# dev will thank me) and I guess the same is true for Scala. When I'm on Haskell I'll try not to over-generalize (yes I know - this might rub people the wrong way) when I write my stupid little private apps and just use records. If you look at the way yesod handles stuff on the other hand, type-classes and monads are used for some of the same stuff so take my opinion with a grain of salt.
&gt;In university, most people struggle to understand the basics of C after an introductory one semester course. Most people in those courses have no place being at university. Seriously, the standards for getting in are way too low and it's a giant waste of money. C is not difficult to understand. 
It's reverse Dunning-Kruger a.k.a. Impostor syndrome.
I've certainly met highly intelligent programmers I wouldn't want to work with on a day-to-day basis, even if only over email. I've not met any highly intelligent programmers I wouldn't want to work with at a more distant level. That is, I've never met a highly intelligent programmer and thought 'wow that's code I never want to see again'. 
Intelligence (which is measured by IQ, there is only one type of intelligence, and creativity is part of it, and is measured by IQ) definitely correlates with programming skill. The correlation is *almost definitely* causative.
'Worse is better' is still true, though. It doesn't mean 'bad is good'. It means that sometimes the technically worse solution to a problem is nonetheless the practical solution. It doesn't mean "it's okay to have massive bugs, because worse is better". It doesn't mean "lol who cares about security holes, because worse is better". It means that for example choosing the algorithm with the best computational complexity that works brilliantly on really large inputs (e.g. a million characters of text) might be a really poor choice if by far the most common usage is for small amounts of text where the algorithm is much slower than a naive and worse-scaling solution. The naive algorithm is technically worse. It doesn't scale properly, and scaling is important. But it is practically better.
I wanted to write something up in response to Stephen Diehl's recent post on this subject, since it really bugged me, but I never got around to it. So here we go. This is definitely a problematic generalization, for at least two reasons: 1. From a cultural perspective, I think it's not a great idea to just assume everyone involved in Haskell is "smart". That's a classic property of an elitist group, and regardless of how true it is, it's something that I guarantee will cause problems in the culture of the Haskell community. 2. It's straight-up not true in some very practical ways. There are a lot of developers who I would rather work with who actually have experience dealing with the kind of stuff that practical software development involves. For example: if I'm implementing software as a service, I'd rather teach Haskell to a non-Haskell programmer who knows about solving those problems than a Haskell developer who has never gone anywhere near them. Writing clear code, migrating and optimizing databases, implementing integration tests, and making software that is easier to operate (has good logging &amp; metrics, etc) are really more complicated than a lot of people think -- or, at the very least, experience here makes a big difference. I know of at least one big Haskell SaaS obviously written by people who knew Haskell very well, but didn't know how to write operable service software, and it was an absolute nightmare -- much worse than my personal experiences with teams that used dynamically typed programming languages. So, yeah... can we end this harmful meme?
I think an important point, here, is that there is already *a* process, however haphazard. The hope is that it could be a *better* process.
That looks very good. I didn't know about hpack before today, I'll have to take a look. Can stack use it transparently?
It certainly is what worse is better is about. It's not about what is easy to implement, and Go is not a good example. Go is in fact a bad example, because it genuinely restricts everyday programming out of desire for easy of implementation, which totally goes against worse is better. 
I use languages that I trust. The compiler is my assistant, and it's supposed to make sure that I don't shoot myself in the foot (as much as possible). So I love Rust, F#, and OCaml. But Haskell comes with too many strings attached, and I can't trust it not to eventually blow up in my face.
OP is already using persistent, which is a already DSL for querying various SQL/NoSQL databases. I don't really know what he is after with the factory pattern ...
Most of the OOP patterns are about using interfaces as a substitute for function types, so you can generate most of them by just performing the reverse substitution, or in the more complicated cases making a record with multiple functions in it or a typeclass depending on whether you want static or dynamic polymorphism. Since they're so much easier to write in FP, I feel like they're just a part of a more natural design process and don't need much calling out as patterns, though.
Test suite and Swagger generation added.
I think you don't get the point of patterns if you are looking for concrete example.
Well than, I guess monad is not a design pattern than (and almost any concept in FP)?
Not an article with the whole scope, but a concrete solution for your decoupling solution: https://www.schoolofhaskell.com/user/meiersi/the-service-pattern
There is [a good quora answer to a question not unlike yours](https://www.quora.com/Are-design-patterns-also-applicable-to-functional-programming-If-they-are-what-are-examples-in-Haskell/answer/Bartosz-Milewski?srid=iX6w) that outlines the difference between a design pattern and a library, and why it's hard to find design patterns in Haskell.
As I said, it is likely I don't understand it. I didn't know of its existence before you asked that question. What is missing from `persistent` that prevents it from qualifying as implementing the repository pattern ?
I think patterns were invented because of some repeating problem - not other way around.
I keep seeing "codygman" everywhere. I'm reading [this thread](https://github.com/NixOS/nixpkgs/issues/7696) and then come to /r/haskell for a break and boom you're here again ^_^
Where did you get that from? Wikipedia seems to agree with me: https://en.wikipedia.org/wiki/Intelligence_quotient#Relationship_to_Intelligence
[`Monad`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#t:Monad) is a type class. It might be similar to an OOP pattern, but in Haskell you can simply write (or derive) an instance for `Monad` and then use the existing machinery for monads.
I would be hesitant to call a monad design pattern, as it is a much more precise concept than most of the things you usually call design pattern. It would be a little like calling classical mechanics a design pattern for modelling physical interaction. But as I already said you can call lots of things design pattern, including a monad (and it has been done occasionally, although I don't think that is helpful, as explained above). But then you are using the term in a non-standard way. It is probably possible to find patterns in Haskell that work around flaws in the language/ecosystem in a similar way like the OOP design patterns do for OOP languages, but to my knowledge nobody calls them design patterns.
Very good write-up. I hope the first parts of this write-up becomes supported in `slack` - both ghcjs and a ready-made template for work like this. The first part could then be replaced by a `stack` command.
Yes!
But regardless if it expects it or not, don't you still have to test the Nothing case?
&gt; I believe you can do this with both cabal and stack. With stack I think the --ghc-options flag does what you want. The question is I'm not sure if add these flag to my project will effect all my dependencies since they are not recompiled anyway, I know I can effect my own module if I do a `cabal clean` and recompile my project.
What, no. GUI generation is the main problems solved by object orientation. Just around now people are creating other tools (like FRP) for that that are approximately as good as OOP. Yes, those patterns are mostly solution to Java problems. But not all OOP languages are the same, and none of them are the reason OOP took over the world.
There are dozens of us!
Agreed, `forM_` forces everything to be collected in memory. The [pipes tutorial](http://hackage.haskell.org/package/pipes-4.2.0/docs/Pipes-Tutorial.html) explains this in the first few paragraphs if the OP is interested.
I clearly know the trade off, I just want to know how can I do it ;)
Yes, I have. Thanks!
&gt; Intelligence (which is measured by IQ, there is only one type of intelligence, and creativity is part of it, and is measured by IQ) You seem to have a lot of confidence in the validity of IQ testing. The idea that such a subtle phenomenon as intelligence can be rendered as a single number seems daft to me, as a lay person, and lots of psychologists seem to agree. [EDIT] For instance: http://www.sciencedirect.com/science/article/pii/S0896627312005843.
My point is that persistent *seems* like and implementation of the repository pattern, based on what I have gathered. Persistent abstracts the query mechanism, persistence, and underlying database type. What is missing for it to qualify as a repository ?
&gt;&gt; Is there any tutorial / article that draws parallel between OOP and FP patterns for those specific problems that are applicable in both cases &gt; Do you really think I have asked for 1 : 1 translation of OOP practices to Haskell? Yes, to me it seems you are asking for a 1 : 1 translation of at least of some OOP practices to Haskell. Based on the answers, I don't think I am the only one to believe this ...
But, even if you write an instance, monad laws [are not enforced](http://stackoverflow.com/questions/37124471/are-monad-laws-enforced-in-haskell). Part of the implementation is a library (the instance), but the developer has to prove that laws apply.
&gt; patterns that are applicable in Haskell. Gabriel Gonzales has some articles, such as : * http://www.haskellforall.com/2014/11/how-to-build-library-agnostic-streaming.html * http://www.haskellforall.com/2014/04/model-view-controller-haskell-style.html * http://www.haskellforall.com/2014/04/scalable-program-architectures.html * http://www.haskellforall.com/2014/03/how-to-model-handles-with-pipes.html AFAIK there is no cookbook-style compendium :/
I'm amazed at how many people here don't understand purpose of repository pattern.
While that sounds nice and all, if none of them end up adding something to the actual compiler code, then for all intents and purposes they might as well not exist (from a GHC user point of view).
I believe -O3 is where performance may degrade. -O2 may have no benefit over -O1 but it tries harder. EDIT: Oops. I must have been thinking about GCC and passing optimizations through GHC to GCC.
I see what you mean now. Thanks
I don't think GHC even has an `-O3` flag, but I might be wrong.
I don't take a GHC user point of view. I prefer a sum total of all human knowledge point of view. 😉
It's like using a custom Prelude: it's something you commit to right away and make part of your core coding style. (Or you don't, of course.) I don't use a custom Prelude in my code or my libraries, but I'd be pretty tempted to do it if I was starting a large new Haskell project somewhere, especially if it was internal. The same goes for automatically enabling certain syntactic extensions throughout the whole project.
amazing ! thanks :)
[removed]
Welcome, and rest assured that Haskell is an amazing choice to learn FP. I don't know of a good JS -&gt; Haskell tutorial, but a suggestion I give is start using things like http://ramdajs.com/ in your day to day JS to get more acquainted with concepts that are very common in Haskell. I would also recommend http://haskellbook.com/ as a beginner friendly book.
Do I have to add "not all" to my every sentence? Do I really need to write "Is there a tutorial that draws a parallel between OOP and FP patterns, but only those that solve the same problem in both languages, not all of OOP patterns"? Seriously, do you really expect me to communicate like this? Do you understand the meaning of words "alternative" and "parallel"?
I have clearly listed Repo, Factory and IoC in my original post and we know what type of problems they are solving. Why do I have to make an example app for you? What even counts as a concrete example for you? What you were asking here was completely absurd.
Don't sweat the lack of JS -&gt; Haskell tutorials. Haskell is a *much* bigger jump than other langs that you typically see those types of tutorials for. Even if such a tutorial existed, you would miss out on some of the profound insights Haskell can provide you when you approach it on it's own terms. It is for that reason that you should learn it. Two years of JS is perfect: you have an idea of what programming's about, but you're not completely indoctrinated into imperative programming as the only way to do computation. Once you internalize pure FP and what it values, you can ~~always be let down~~ apply those ideas in other languages, leading to cleaner programs with fewer bugs.
&gt; it doesn't allow you to have 2 or more easily interchangeable implementations of your repository with different data sources (SQL, text file, NoSQL, etc...) Yes it does, that is exactly what persistent is for. There are mysql, postgresql, mongodb, sqlite, odbc, zookeeper, etc. backends. &gt; Than we could also have a factory function that takes configuration file and depending on our configuration it gives us back fileRepo or sqlRepo. pool &lt;- case dbconfig of Postgres -&gt; createPostgresqlPool "connection string" Sqlite -&gt; createSqlitePool "/tmp/foo.sqlite" 4 runSqlConn query pool This is how I would deal with that problem in Haskell. I hope this answer this specific question. In the case of `persistent` the author went for a typeclass-based approach. The record solution you suggested would work just as well, but would be a bit more verbose.
I would echo this. Purescript or Elm might be nice intros since both have much better documentation. I'd prefer Purescript if you want something as close to haskell as possible and Elm if you want something the uses haskell concepts but abstracted/named slightly differently. FWIW, Purescript by Example actually helped me understand haskell better than haskell did.
[The Haskell Book](http://haskellbook.com/) is the best learning resource in my opinion. It does a fantastic job of guiding you to an understanding of pure functional programming with high level concepts like monads. But be warned; Haskell is unlike most programming languages. It's a total paradigm shift in ways you won't expect. It's gonna take a while to be comfortable enough with it to translate its ideas back to JS or anything else. But I think the ideas it instills are absolutely worth using.
Yes, [it clamps to the range 0 &lt;= n &lt;= 2](https://github.com/ghc/ghc/blob/586d55815401c54f4687d053fb033e53865e0bf1/compiler/main/DynFlags.hs#L2086). It shouldn't have any other effect (grep for `optLevel` and you can see all the places it touches things).
Yeah, it's a really well done book. I think the biggest downside to moving to Haskell from Purescript is that it really can spoil you for pleasant record notation and row-level polymorphism. Haskell records just feel painful now, even with the new GHC 8 extensions.
These days I recommend learning Elm first: it's a strongly-typed and purely-functional language, just like Haskell, but it's a much simpler language than Haskell, and it's designed to be easier to learn. It's also a frontend language whose target audience is JavaScript programmers, so that's a bonus for you. The idea is that to learn Haskell, in addition to learning how to describe your data with precise types and how to cope with the lack of unconstrained side-effects, like you'll have to do in Elm, you also have to learn about laziness, type classes, specific type classes like functors and monads, and you're likely to encounter libraries which are very abstract. Those things are all very useful, but taken all together this can all be very intimidating. So I now recommend starting with Elm, which only has a subset of those things. The syntax is also pretty much the same as in Haskell (they recommend configuring your editor to use the same syntax highlighter as for Haskell), so learning Elm is more like learning a subset of Haskell than like learning a separate language.
Go is the epitome of Worse is Better done poorly. Go was designed poorly, and just because they did Unix right that doesn't mean that they did Go right.
Ooh, that is a nice tip. I get the following error: &gt;ByteCodeLink: can't find label During interactive linking, GHCi couldn't find the following symbol: SDL_GetPlatform This may be due to you not asking GHCi to load extra object files, archives or DLLs needed by your current session. I don't know how to solve this. I have all necessary SDL2 DLLs in the directory with the *.hs file. When I compile it to an .exe, it all works, so it seems that I have a working setup.
easy heroku deploys
Once https://phabricator.haskell.org/D1696 gets into release SDL2 should work out of the box in GHCi and GHC-mod. In the meantime try editing cabal file for SDL2 libary: extra-ghci-libraries: SDL2 extra-lib-dirs: path-to-where-SDL2.dll-is-located
¯\\\_(ツ)_/¯ I guess *you* need to write the essay then, since you know the material better than I!
How is that trolling?
I think I will. We need to kill this reflex Haskell community has when OOP is mentioned.
I really like Elm. Only thing that don't like is having to buy in to totally different set of packages and APIs. Elm is same like Haskell... but different... but still same...
Nice article! I wasn't aware of that [`Interp.hs`](https://phabricator.haskell.org/diffusion/GHC/browse/master/utils/ext-core/Language/Core/Interp.hs;78c209010058cd7669781de92068b64dd32caaea) file from 2008! There is also [`StgInterp.lhs`](https://phabricator.haskell.org/diffusion/GHC/change/master/ghc/compiler/ghci/StgInterp.lhs;bca9dd54c2b39638cb4638aaccf6015a104a1df5) by Simon Marlow from 2000. I feel like there should be an archive of such code archeology somewhere so that we can use them for reference. &gt; There are several good examples of plugins such as the Herbie plugin that “improves the numerical stability of Haskell programs”, or this plugin which enables to make functions (previously annotated) strict. There is also the [HERMIT](https://hackage.haskell.org/package/hermit-1.0.0.0) tool which is for doing equational reasoning and proofs. It might be relevant to what you're working on, if only to provide inspiration. &gt; The above is more complicated that I thought it would be. Welcome to the GHC API! ヽ (＾▽＾) ﾉ &gt; but it is also a lot of fun really trying to understand what happens inside GHC That's what keeps me coming back. &gt; In fact, every Name contains a Unique identifier that is “used in many places in GHC for fast ordering and equality tests”. This means that if we are not careful we could end up with different CoreExprs being equal during an equality check! To solve this we need to generate the Uniques from a UniqSupply value, complicating things even more. [..] I am not sure if there are easier/better ways to deal with Core but so far as I know there are not many projects that attempt to generate Core from scratch I have the same problem too. I'm interested in making a proof checker for Core.
&gt;That wikipedia paragraph is unsourced crap. Technically, so are your assertions.
What I want is actually a little more than than. I want a staticly verified ordered list **that is also promotable to the type level**. Then I could write things like `ToList`, `FromList`, and `Insert` as closed type families. Unfortunately, using `Sing` in `OListCons` does not work because data families cannot be promoted. I'm currently looking at what happens if I give up on polymorphism and just do it as: OListCons :: SNat x -&gt; LiftGeq upper ('Middle x) -&gt; LiftEq ('Middle x) upper -&gt; OList lower upper Even though this will promote, it's not great because (a) it's not polymorphic in the list item and ordering and (b) the singletonized nats are kind of weird to use in type families.
I'm trying to write `ToList` as a type family since I'm only interested in using `OList` on the type level. This means I cannot have a singleton value in `OListCons` because data families do not promote.
&gt; there's no means to extend said polymorphism like type-classes. I know, the lack of type classes is so annoying! Which is why I'm not encouraging Haskell programmers to switch to Elm, I'm only encouraging non-Haskell programmers to learn Elm first, so they don't have to learn about type classes yet :) Surprisingly though, there is a situation in which I found Elm's lack of abstraction facilities to be an advantage for me: when I [wrote a game in Elm in 48h](https://www.reddit.com/r/elm/comments/4f9dio/tetroidvania_my_entry_for_ludum_dare_35/), for a 48h game jam. In this situation, the fact that creating abstractions is difficult meant I couldn't simplify my code by creating mini-libraries like I do in Haskell. I'm sure creating these mini-libraries is beneficial in the long run, but in the short run, like in 48h, it turns out they take more time to create than they save :)
&gt; amortized O(1) cons/uncons and O(log(k)) lookup, while still having the same memory footprint as lists. Hmm, how does this compare to [Data.Sequence](https://hackage.haskell.org/package/containers-0.5.7.1/docs/Data-Sequence.html)? It claims the same costs for those operations. Does Data.Sequence have a prohibitive extra cost in terms of memory?
This one probably* has somewhat better constant factors, *much* simpler implementation, ~~and smaller memory footprint~~ (edit: the binary version has larger memory footprint than Data.Sequence, but the ternary and quaternary versions have smaller footprint) On the other hand, it only supports efficient modification of one end of the sequence, while Data.Sequence (finger trees) can efficiently modify both ends (and can also split efficiently)
Seems interesting! Out of curiosity: Is there a reason you chose a somewhat different naming scheme compared to other libs, e.g. * lookup vs. index * mbLookup vs. lookup 
I don't think we have a totally uniform naming scheme, but you are right, `lookup` is maybe not the best name (for lists it is something completely different), and it may be a good idea to rename it to `index` before people start to use it. However, I would then probably rename `mbLookup` to `mbIndex` for internal consistency.
I can't find the source. I'm getting a "HTTP 404 error getting http://code.haskell.org/~bkomuves/projects/nested-sequence/_darcs/inventory" How does the insert work on this data type?
I tried running ghci but apparantly that fails as well, giving the error &gt;(my project dir)\.stack-work\dist\2672c1f3\build\cbits\helpers.o: unknown symbol `TTF_RenderUNICODE_Solid' linking extra libraries/objects failed Apparently I've installed SDL2_TTF incorrectly? Weird thing is that it all works with ghc...
Thx. Don't get me wrong: I didn't mean to critisize, your naming scheme seems consistent in itself. Was just curious.
[removed]
Indeed. Vector is easy, I can predict the results without running it: Vector will beat this for lookup and memory footprint, and this will beat vector for consing. They are very different data structures. Think of this as a singly linked list, but with faster lookup. Data.Sequence: Well, this is somewhat embarrassing. I just ran some very simple micro-benchmarks, and it seems that Data.Sequence is a tiny bit better for both memory footprint and speed of building (but they are almost the same). Though lookup is somewhat faster with this library (I estimate something like 30%-50% faster - I don't know yet how to isolate and measure it, so take this with a grain of salt). I thought this will have smaller memory footprint, because the size is not stored in the nodes, but the finger tree in Data.Sequence has nodes with more children, which offsets that. It would be interesting to write a ternary version of this library (this corresponds to binary) and compare with that, too. EDIT: meantime I implemented the ternary version too: it is a little bit faster, and memory footprint is reduced from 3 extra words per element to 2 extra words per element. That's now better than Data.Sequence (which have on average 2.5 extra words per element). Next up: 4-way version :)
I've made very simple games in both [Elm](https://github.com/soupi/ld32) and [Pure](https://github.com/soupi/ld34)[Script](https://github.com/soupi/ld35). Both haven't been a problem the first time around when you start from scratch because both can give you at least the same experience starting out (you don't have to abstract everything in PureScript and you can always refactor when you want!). But trying to take what i had after the jam and make something reusable out of it it and failing is why i stopped using Elm. Your game is pretty great, btw :)
[removed]
There's an announcement [here](https://haskell-lang.org/announcements) and some discussion over on [Hacker News](https://news.ycombinator.com/item?id=12054690) as well.
The reason for also starting a new Haskell [subreddit](https://www.reddit.com/r/haskell_lang) is quite nonsensical. &gt;"/r/haskell has become a place of constant flamewars. We need a clean break. A new subreddit provides a fresh start allowing to mold a new community based on better principles. Everyone who wants to be part of the new community is invited to join the new Haskell movement. Troublemaker will hopefully stay behind." I don't exactly notice any "flamewars" on /r/haskell.
There must be a way to determine the http method while keeping a type-directed approach... Oh! How about using the monad in which the result is returned? -- Identity mean GET add :: Double -&gt; Double -&gt; Identity Double -- IO mean POST create :: FilePath -&gt; String -&gt; IO () -- ...Delete means DELETE? newtype Delete a = Delete (IO a) delete :: FilePath -&gt; Delete () Hmm, having to implement the deletion code inside a newtype'd IO wrappers just so the http method can be detected doesn't sound ideal. Oh! Instead of a newtype, what if we ask the user to define an effect system in which creation, modification and deletion operations are separate effects? Then they can have their own Delete monad which is much better than a wrapper around IO, because the type system will make sure that they are using the semantically correct http method! ... Now that I'm taking a closer look, I realize that /u/Tekmo isn't deriving the types of the parameters based on the types of the handlers, but rather on the type of a Command type which is received by a single global handler. Which means that every route must return a value of the same type? That doesn't sound right. So by using a Handlers type containing the handlers for all the endpoints, we'd fix two problems at once: we'd have a natural place in the type (the chosen monad) from which to determine the http method, and we'd allow different endpoints to return values of different types! 
Yes, I think a new subreddit is very unwise. The rest of it I think I support though.
The basic Servant stuff lives in a `Handler` monad, which is `ExceptT ServantErr IO`. This means that it can throw `ServantErr` to indicate webserver related failures (like 404, 401, etc), and do `IO`. In order to do `IO` in a monad transformer stack, you use the function `liftIO`. This is a function that takes any `IO` action and embeds it in a stack capable of doing `IO`. I'm going to guess that `connectSqlite3` has the type `String -&gt; IO Connection` (or similar). You'd do `conn &lt;- liftIO (connectSqlite3 "db.sqlite")` to get the connection out. You'll want to do the same for `quickQuery'` and `disconnect`. Alternatively, you can put them all in the same block like: getItems = do items &lt;- liftIO $ do conn &lt;- connectSqlite3 "db.sqlite" res &lt;- quickQuery' conn ... disconnect conn return (map convRow res) return items This pattern is actually redundant! In the same way that we can transform `foo x = bar x` to `foo = bar`, we can transform a block like: foo &lt;- bar return foo into simply bar So you should be able to reduce the original expression even more.
Even if you could qualify /r/Haskell's discussions as flamewars, it's not as if a new subreddit will be immune to this. The same discussions will be had there.
When it becomes too much again, we'll simply create /r/haskell_lang_lang. Problem solved.
EDIT: Looks like it's probably a false alarm and there was just some confusion regarding the release of this news. The reason for this change is completely an attempt by FP Complete to control the haskell community. The only sponsors are FP Complete and Commercial haskell. Commercial haskell is just a group created by FP Complete and consists of literally any company that has expressed mild interest in haskell. /u/snoyberg is also the creator of the /r/haskell_lang sub and top mod. I'm not sure about the other mods, but none are current mods of /r/haskell. The goal *might* actually be to make haskell more user friendly, but it seems much more likely that they just want to get more control over the community considering the extremely flimsy reasoning they give for the split. EDIT: Also /u/snoyberg, is this you https://news.ycombinator.com/threads?id=Rabble_Of_One That account was created 3 hours ago and had this to say about /r/haskell (and submitted the link to HN). &gt;Because /r/haskell has become a place of constant flamewars. We need a clean break. A new subreddit provides a fresh start allowing to mold a new community based on better principles. Everyone who wants to be part of the new community is invited to join the new Haskell movement. Troublemaker will hopefully stay behind We all know this is a load of crap and that /r/haskell is one of the least cancerous subreddits on this entire site. EDIT 2: More from Rabble_Of_One: For starters just recently FP Complete open-sourced a new cool high-performance serialization library. Then a few people started giving Michael shit for no reason. See https://www.reddit.com/user/snoyberg?after=t1_d3ncybv This is the related drama: https://www.reddit.com/r/haskell/comments/4l3y9f/store_a_new_and_efficient_binary_serialization/d3lbs8m?context=5 /u/snoyberg might not be trying to control the community, he might just be mad people are arguing with him and decided to make a new subreddit where his friends are the mods.
You are correct, however that warning is much easier to tweak, both technically and politically. For now, we would need to disable orphan warnings on the new module. Moving forward, something that has already been discussed is that orphan status could be at the package level rather than the module level. Irrespective of this new thing, that change would be helpful to free up code layout for everyone.
lol :D people simply don't read what I write, and don't know what is purpose of basic patterns in programming and than they make funny comments. Read my code again and you'll notice function named `getTopXStudents` with file example implementation of `\x -&gt; readFileLines x file`. I was just giving quick example of what repository is, which is decoupling your code from underlying db access library and it's DSL, and making higher abstraction level. In this concrete example I want to get topX students. How? I don't care, DSL should be hidden behind repo. You say this, but than you show completely oposite in your response leading me to believe you don't really understand underlying matter and what is abstraction. Pick apart my example some more. I can't wait -.-
I didn't think too hard about this, hence the qualification with probably, but lets see. Take singleton types (used to enforce constraints usually out of reach for the type system) as an example, a convoluted solution to a genuine lack in the language. Their usage can be eased a lot by using the singletons package, but that's doesn't disqualify them in my mind. Another pattern that I can think of is using Template Haskell to enforce constraints for static data. Take regexs as an example used like this match :: String -&gt; Regex -&gt; Bool multiLangSalut :: Either RegexCompileError Regex multiLangSalut = compile "h(a|e)llo+" The value `multiLangSalut` could be evaluated at compile time, if GHC would do such things. And if this were done it shouldn't be an `Either` since the left case would be useless. Sure you could use the underlying representation to build a value manually but it would be good practice to keep that hidden and a painful endeavor anyway, think `Regex [Letter 'h', Choice [Regex [Letter 'a'], Regex [Letter 'e']], Letter 'l', ...]`. Also your limited in what kind of constraints can be enforced purely by representation choice, while a smart construct can check basically anything. `compile` is therefore more suited for value construction at run time. Now you can define a companion function say `mkRegex :: String -&gt; Q Exp` that can be used like `$(mkRegex "h(e|a)llo+") :: Regex` and that calls `compile` internally and throws an error if the left case. See the path package for an example of this. But besides regexs and paths this pattern can be used for datetime, SQL, word width aligned pointers or basically anything that requires a smart constructor or an external syntax. Imagine you have some rarely taken error-handling code path which is supposed to dump to some log, triggered by a non-deterministic bug in production. In some dynamic languages you can even have a syntax error lurking there, but in Haskell say you have a typo in the log path, the outcome is the same, crucial data needed for debugging is lost. Now one could say tough luck, that should have been tested, but you can say that of everything, though granted this particular example should be covered by a decent test suite. But I think the term design pattern is ill-defined at best, therefore I really don't want to add to its proliferation.
That handle looks like this one: https://www.reddit.com/user/Mob_Of_One
something something fixpoint to the rescue
~~/u/Mob_Of_One mods /r/haskellbook where his flair is "Author of the Haskell Book". The author of "The Haskell Book" is in part is Chris Allen (http://haskellbook.com/authors.html) aka /u/bitemyapp who is also an /r/haskell_lang mod.~~
Whichever you find more useful.
Exactly. This is why I said earlier that this is a tragedy for the Haskell community.
haskell.org is the "official" source, as far as there is one.
Yeah I'm really annoying don't go by my comment alone.
To aid mobile users, I'll link small subreddits not yet linked in the comments /r/haskell_lang: Discussion of the [Haskell programming language](http://haskell-lang.org). Common links: * [The Haskell build tool Stack](http://haskellstack.com) * [Commercial Haskell Group](http://commercialhaskell.com) * [Stackage](https://www.stackage.org) * [Glasgow Haskell Compiler](https://www.haskell.org/ghc/) --- ^I ^am ^a ^bot ^| [^Mail ^BotOwner](http://reddit.com/message/compose/?to=DarkMio&amp;subject=SmallSubBot%20Report) ^| ^To ^aid ^mobile ^users, ^I'll ^link ^small ^subreddits ^not ^yet ^linked ^in ^the ^comments ^| ^[Code](https://github.com/DarkMio/Massdrop-Reddit-Bot) ^| [^Ban](https://www.reddit.com/message/compose/?to=SmallSubBot&amp;subject=SmallSubBot%20Report&amp;message=ban%20/r/Subreddit) ^- [^Help](https://www.reddit.com/r/MassdropBot/wiki/index#wiki_banning_a_bot)
I think a new home page is great. The rest is problematic. IMO, there is a schism between the "computer science research" community and the "practical, even commercial" community. While the language has long been led by the computer science research interests (naturally), IMO, the practical community isn't as well-served in the language's adolescence. For practical use, including new learners, stability of the language *and its libraries* is of great value. If we can achieve stability, which I have proposed includes long-term support for older versions of the language and its libraries, then we can heal the schism. I wouldn't call /r/haskell a site of flamewars, but there is clearly (IMO) a bias toward the experimental, as reflected in the generally cool reaction to NIH ideas. I experienced this myself when I wrote my patch for GHC Trac ticket 2615, which allowed GHCi and TH to run on Gentoo Linux (and other distributions (e.g., Ubuntu), using linker scripts in place of shared libraries). It took a long time to get the patch in, resulting in some distributions backporting it to GHC 6.12. (Note: My patch wasn't rejected, but it was overlooked for GHC 6.12, meaning it wasn't included until GHC 7.0. This made for a rockier experience for newcomers trying GHCi under many Linux distributions.) I hope the more practical focus of the new site will lead to greater adoption of Haskell, rather than fragmentation, but this will depend on how both communities react to each other. I call on everyone to assume the others are acting in good faith, even if they have different goals and perspectives.
I hope no one really expects to me to respond to this kind of ridiculous rhetoric, the reasons for why a new site were launched are public record and have been discussed extensively many times in the past. This is not an "FP Complete vs Community" issue, this is a large part of the community realizing that there is no way to improve haskell.org with how it is being run right now. And no, Rabble_Of_One is not me. The only non-snoyberg-named accounts I have are either ancient, or accounts I use to discuss non-programming content. If I have something to say, I'll say it under my own name, and say it proudly. So to summarize: I've said everything I can to explain the reasons for what I've done. I have no hidden agenda. I'm unaware of anyone else involved in this project - whether at FP Complete or not - who has a hidden agenda. If you (and others) want to continually make up these false narratives, I guess have a good time. But you're wrong, and if you continue doing it after seeing this response you're also dishonest.
To aid mobile users, I'll link small subreddits not yet linked in the comments /r/haskell_lang: Discussion of the [Haskell programming language](http://haskell-lang.org). Common links: * [The Haskell build tool Stack](http://haskellstack.com) * [Commercial Haskell Group](http://commercialhaskell.com) * [Stackage](https://www.stackage.org) * [Glasgow Haskell Compiler](https://www.haskell.org/ghc/) --- ^I ^am ^a ^bot ^| [^Mail ^BotOwner](http://reddit.com/message/compose/?to=DarkMio&amp;subject=SmallSubBot%20Report) ^| ^To ^aid ^mobile ^users, ^I'll ^link ^small ^subreddits ^not ^yet ^linked ^in ^the ^comments ^| ^[Code](https://github.com/DarkMio/Massdrop-Reddit-Bot) ^| [^Ban](https://www.reddit.com/message/compose/?to=SmallSubBot&amp;subject=SmallSubBot%20Report&amp;message=ban%20/r/Subreddit) ^- [^Help](https://www.reddit.com/r/MassdropBot/wiki/index#wiki_banning_a_bot)
&gt; The reason for this change is completely an attempt by FP Complete to control the haskell community. It's not all dramatic as that. We're just a team who want Haskell to be easy; for newbies and clients. That usually starts with a link. Go here, download this, use this reference. Months of effort have been wasted discussing and arguing about that link with the current holders of haskell.org. &gt; The goal might actually be to make haskell more user friendly, but it seems much more likely that they just want to get more control over the community considering the extremely flimsy reasoning they give for the split. If the explanation is curt, that's intentional (and part exhaustion, see above re months of arguing); our energy is being redirected on work being done. Bringing things together like stackage, stack, hoogle, intero and this home page is a natural consequence of drive to make Haskell easier under a cohesive experience. The subreddit isn't a big deal. 
I apologize for accusing you personally I jumped the gun on that. However, there's no denying how closely tied the new website/subreddit are tied to you and FP Complete. Couldn't we have had like a meta post about toxicity in the subreddit and maybe increased the number of mods? I don't really know much about the website, so there might be good reasons for breaking there. But the subreddit and irc are still fine and it seems strange that this split is on the heels of you getting into fights with people in comment sections.
I just feel like this should have been resolved in a way that doesn't look like it will divide our small community. It just feels like you all went behind our backs' and took matters into your own hands without much thought about the members of the community themselves. I would have greatly preferred to see more open discussion and a real ratifiable plan as opposed to this ambush.
Use package manager in your distro instead.
You should really use [Stack](http://docs.haskellstack.org/en/stable/README/) for that instead.
&gt; Couldn't we have had like a meta post about toxicity in the subreddit and maybe increased the number of mods? I have no idea where this is coming from, I never made any claims about the subreddit being toxic, or said we need to distance from it. My statements on this have been really clear: * https://www.reddit.com/r/haskell_lang/comments/4rv4uu/is_this_the_official_haskell_subreddit_or_is/d54cpja * https://www.reddit.com/r/haskell_lang/comments/4rug4n/why_a_new_subreddit/d54cdfu * https://www.reddit.com/r/haskell/comments/4ghznv/improved_hpcaballess_wwwhaskellorg_in_the_works/d2jhwm6 We've kept the arguments that led to this new website out of the announcements. I originally kept them in to give context, but others wanted to just focus on the future instead of the past. I'm honoring those wishes and not turning this into a he-said-she-said. When people (like you) make vague and ominous statements like "you getting into fights with people in comment sections," I really regret that decision and wish I'd pointed concretely to the core arguments that led to this.
Reddit noob here; I don't know if you were notified that I edited my above comment. I think it answers your original question.
I don't think it's that clear from the site itself or any of the announcements. I think the site looks very official so many paranoid crazy people like myself think that this is supposed to be the new home for haskell and not just a resource for learning haskell better. Especially when it has an accompanying subreddit and irc. Maybe some changes to the site are necessary to set it apart from the current haskell.org. And perhaps something in the /r/haskell_lang sidebar that says the purpose is for the site only. I really like the idea of making haskell more user friendly and accessible, so I apologize that I get spooked so easily.
Ain't me though, I'm going to ask them why they used a handle like mine.
I don't feel like this changes much for me as a haskell user. Moving forward, when meeting people interested in haskell, you are free to direct them to whichever site you prefer. hopefully both sites will get better in time and we'll get better documentation for Haskell.
I think they made a conscious decision to emulate a name I'd used on Reddit, but I don't know who they are. I have no problem speaking as myself when I want to, even if it draws heat. This isn't something I'd do and you'd admit that if you weren't trying to cast aspersions. Stuff like: &gt;Troublemaker will hopefully stay behind Makes me think they aren't a native English speaker. I'm a lot more anal about punctuation than that too.
I don't have time to grab links, but this simply isn't true. There was a large discussion on Reddit a month or so ago about this planned effort. Nothing was done in secret. If you personally didn't hear about it until now, I'm sorry you feel like this is a surprise.
While I agree all of this is unfortunate, this is not "an ambush". Work on this site has started a while ago, and the problems have been brewing for a while (visible examples are the rant about contributing to GHC, and the whole platform in the download page situation). And one of these problems is precisely the lack of public discussion concerning many decisions that are taken in informal settings and impact everyone!
&gt; you'd admit that if you were trying to cast aspersions. I rescinded my comment.
C'mon people, don't downvote /u/JimShitty, they're asking a legitimate question.
Without comment, I'll link to https://github.com/haskell-infra/hl/pull/130
Please calm down. This is just an alternative site for presenting the language. I don't see how this introduce any sort of fragmentation.
&gt; If we can achieve stability, which I have proposed includes long-term support for older versions of the language and its libraries, then we can heal the schism. Can you clarify "support"? Building on newer operating systems? Backporting bugfixes? Takes work. How does it help both sides of the "schism". 
I don't know what to tell you then. The [previous discussion](https://www.reddit.com/r/haskell/comments/4ghznv/improved_hpcaballess_wwwhaskellorg_in_the_works/) had 150 comments, so it seemed widely known enough. If you'd like to Monday-night-quarterback how to roll out a new site, be my guest. I have no qualms about how we did it: we made it known that it was happening, I discussed with members of the haskell.org committee, had public discussion, developed it in public, and announced when we thought there was something worth having. I don't feel bad about any part of this process.
fwiw, had no problems with `stack` on Windows, with: setx STACK_ROOT=c:\stack_root http://docs.haskellstack.org/en/stable/install_and_upgrade/#windows 
Except for this part: &gt; aren't fatal (program runs even if they are present) 
For an example of things on haskell.org which are emphatically *not* documentation, see the page on [Zygohistomorphic prepromorphisms](https://wiki.haskell.org/Zygohistomorphic_prepromorphisms). Haskell desperately needs better docs. Rust is absolutely an example to emulate, though getting there took a full-time employee of Mozilla (/u/steveklabnik1) working only on documentation several years on top of the already massive community contributions.
Its elitism is a tragedy. Preferring coolness over engineering is a tragedy. Language and library design via annealing is a tragedy. And diversity is probably the only way to push the community forward IMO. I have no idea what they want to achieve with the new site. But if the new subreddit will be a bit less eager to downvote, then it is probably OK.
Author of the second article here. In addition to those GoF patterns, I also tried to take a more historical approach on design patterns. A long time ago, functions became a pattern for assembly programmer, as well as objects for C programmers. You could also think of the whole "dependency injection"/IoC business in Java, which makes up for the absence of lazy evaluation and first order functions. TL;DR: Integrating design patterns in the next generation programming languages (as in "providing new features in the core language to make them vanish") is a great way to increase expressiveness.
[Yes, although building a cross-compiler isn't much fun at all](https://ghc.haskell.org/trac/ghc/wiki/Building/CrossCompiling).
'Fraid not.
There never has been one official site that has everything you need. I have had to use many sites, blogs, documentations, PDFs, research papers, books, tutorials, etc, while learning Haskell. I believe one of the goals of the new haskell-lang.org is to focus on a single path for getting started with Haskell to avoid confusion. This goal seems to align with what you want, so I would suggest starting with haskell-lang.org and a tool called "Stack" which is described on haskell-lang.org. But again, no single site can lead you to Haskell mastery. I would suggest choosing a book if you are looking for a single source to get started with: http://learnyouahaskell.com/ (this is free and how I learned) http://haskellbook.com/ (this is not free, but covers more topics)
Ocaml is a great tool, but the Haskell situation isn't so bad as to drive one away if one has some patience.
Looks nice! It would be cool if you could provide a small input/output in the readme?
if/then/else is used as an example of laziness but a strict `if` is usable if you have a convenient syntax to suspend computations (just be explicitly lazy about the two arguments).
You have asked legitimate questions which, as a community, we can respond to better without being vague or confrontational. As stated in another comment, a charitable interpretation is that within this particular thread people are using their votes to aggressively curate the discussion and not to deride specific individuals. - Haskell.org : Official, less opinionated (and therefore considered less up-to-date by some) - Haskell-Lang.org : Brought to us by several known names in the community, more opinionated about guiding people into the language (specifically encourages using Stack/Stackage) If you want to just dive into reading and coding, the links suggested by /u/bitemyapp are more than enough to get started with. Since he is involved in the creation of the Haskell-Lang site, you will likely find it easier to refer to (again, because it omits some information which isn't necessary to getting started and encourages the use of certain tools/libraries).
I would suggest not creating a new IRC channel. You have #haskell-lang listed as being for "general help and queries" but this goal seems well covered by the existing #haskell and #haskell-beginners IRC channels, both of which I've had nothing but good experience with. We don't need another IRC channel. See: https://haskell-lang.org/irc As I understand, a major goal for haskell-lang.org was to list fewer options for beginners, and thus help beginners avoid "analysis-paralysis" and just get to writing some code. Creating and listing a new general purpose IRC channel does not serve this goal, but will be a point of confusion for beginners, something you have worked hard to avoid.
I've done some work to make Desktop/UI apps [easier](http://hackage.haskell.org/package/fltkhs) to build in Haskell. I even gave a [talk](https://www.youtube.com/watch?v=5hoQLovZBxQ) on it relatively recently. Beyond that, pointing out that the project exists whenever someone like you asks and spamming updates on /r/haskell I have no idea how to make people aware that it exists or how to elicit help/feedback. I tried haskell-cafe without much luck and the wiki seems to be pretty outdated. I'm aware that there's hurt on both sides but I'm willing to contribute with to this alternate website or a new wiki or whatever if it will help people evaluate currently available options quickly and easily. 
[Yes: it is very much possible to offer a purely functional interface](https://existentialtype.wordpress.com/2011/05/01/of-course-ml-has-monads/). The standard library is perfectly ok with providing the user with and using impure features but it's something you can abstain from if you want to. And given that lately quite a bit of work has been invested in [making monadic programs look like usual ones](https://github.com/janestreet/ppx_let), the transition from one to the other should arguably be pretty painless.
Stack has already taken over. This is what they call a cleanup operation.
I am sorry you are getting downvoted. This is no way to make a newcomer feel welcome. I agree with your concern. For what it's worth, haskell-lang.org is established in part to address your very concern, and it sets forth a simple, clear way to get started. Meanwhile the old haskell.org throws three choices up and says "umm, you pick" to newcomers, the exact people who are not equipped to decide which of these to pick from.
&gt; https://msdn.microsoft.com/en-us/library/kx37x362.aspx Oh yes. I'll make sure to tell my friends who want to learn C# to visit "MSDN dot microsoft dot Com slash en dash us slash library slash kx37x362 dot aspx". Really official, rolls off the tongue, and doesn't even mention C# anywhere in the URL
Welcome! :-) You're not being attacked. You've been asking many short repetitive questions on various topics in a short space of time with a spammy (in english) username on an unusually highly-charged thread which is attracting a lot of voting. The downvotes are not personal. For your general haskell questions, you'll get much better help in #haskell or #haskell-beginners, on stack overflow, or even in a new reddit post.
&gt; I'm interested in making a proof checker for Core. Just curious: what kind of proof checker? Have you tried Liquid Haskell yet, or are you thinking of something different, like Catch? Actually while LH is currently automatic (maybe that's exactly what you *don't* want), there is [some work](https://twitter.com/nikivazou/status/727271163915702272) into actually making it a proof assistant. It is unfortunately still a very rough project and evolving rapidly, however, so there's lots of pointy corners and trial and error to deal with.
nice name lol
&gt; Compared to other languages, will it be harder for me to understand my own code x months/years from now? (Assume for the sake of argument that I don't write any comments.) It really depends. You can write really nice, really succinct haskell code and completely unreadable symbol soup. Comments aren't that useful in Haskell in my experience.
 newtype Outbox a = Outbox { send :: a -&gt; IO () } newtype Inbox a = Inbox { recv ∷ IO a } Both of these newtypes are of kind `* -&gt; *`. If you try to write Functor instances for both of them, you'll see why we need Contravariant functors.
Functions going to a given type is probably the simplest contravariant functor: newtype F a = F (a -&gt; Int) or more generally: newtype F2 b a = F2 (a -&gt; b) Another example is actors, as [explained here](https://ocharles.org.uk/blog/guest-posts/2013-12-21-24-days-of-hackage-contravariant.html) In mathematics, contravariant functors appear very often. Maybe in practical programming they appear somewhat less often.
When I learned C# I can tell you that I had no idea that existed.
There are a number of reasons to use nameless approache, the primary being that alpha conversion isn't necessary. The main issue is that if you substitute an open term (i.e. a term w/ free variables) under a binder, you might accidentally capture variables. For example, if you substitute the pair `(x,y)`, which has two free variables, `x` and `y`, in place of the variable `p` in the function `\x -&gt; \y -&gt; p`, then you'll get back `\x -&gt; \y -&gt; (x,y)`, but this is not the right result. You should instead get back `\z -&gt; \w -&gt; (x,y)`. The solution I used in my recent rewrite of SimpleFP was a locally nameless approach, where you have two kinds of variables, free and bound. Bound variables are de Bruijn indices, and free variables are names. Then you have a special function that converts free variables to de Bruijn indices in the correct way. Then, substitution is easier. You take a scope, free up its variable (with a fresh name that doesn't appear in the scope), and just substitute along that name. You don't risk capture because free variables are formally distinct from bound variables, so you're safe. BTW you should come to the Haskell channel on IRC. Ping me, I'm augur there.
&gt;I would check out this guide: &lt;https://github.com/bitemyapp/learnhaskell/&gt;. They guide is written by the guy who probably wrote a lot of the new site you said is a tragedy ;)
 The author told me that one of the reasons he chose Haskell was language-c. He also wrote a blog post about contributing: http://jamey.thesharps.us/2016/07/translating-c-to-rust-and-how-you-can.html
&gt; I disagree, but I probably have a very different kind of resource in mind. I would love to hear your thoughts on what an effective Haskell cookbook/wiki would entail. I'd also love to hear your reasoning for not wanting to build off of HaskellWiki as I think that would be ones best shot at implementing a central documentation hub that the Haskell community *as a whole* can get behind - because it already has that role (theoretically) and has established brand recognition in the community already. Politically, it would be the smarter and more credible means to the end goal. Plus, initial efforts would basically amount to just writing solid wiki pages on certain topics, which could be done pretty non-invasivly in HaskellWiki as is. Making the technology behind HaskellWiki more effective can be focused on once the content is clearly valuable enough to warrant such changes. &gt; https://en.wikibooks.org/wiki/Haskell seemed alright styling-wise, what have you made of that? The haskell wikibook has quite a bit of informative content, but its delivered in a tutorial/book-like manner, so I can't reach for its contents efficiently in a wiki-like fashion. Compare that with Real World Haskell, where each of its chapters is focused on pretty self-contained and well-defined topics. It's useful because you can find what you want without knowing what you're actually looking for, specifically. But even then, Real World Haskell is still very much in book-form as well. The alternative would be a wiki that provides content that's something like [this](https://wiki.archlinux.org/index.php/General_recommendations), [this](https://wiki.archlinux.org/index.php/Xmonad), and [this](https://wiki.archlinux.org/index.php/Security) (but for Haskell, of course) where general FP and software engineering topics have dedicated pages detailing the common and best practices in Haskell with regards to those topics and what relevant libraries address the problems encountered in those topics/domains. Furthermore, individual but popular Haskell libraries have dedicated pages detailing their purpose, usage, examples, tips/tricks, troubleshooting, and further resources. If I'm solving a problem in a certain domain and I can't figure out what library I should be using, I should at least be able to search for my domain in the wiki and find a page addressing what the Haskell ecosystem has to offer with regards to that domain and potentially turn me on to some new libraries that actually focus on the problem I'm currently trying to solve. The Haskell wikibook doesn't really provide that service. The HaskellWiki *tries* to provide that service, but is very sparse and inconsistent in its content.
Right, by nameless I meant locally nameless. I didn't realize that nameless had another meaning.
Then Haskell is not a lazy language because you can have strictness annotations on your datatype declarations...
&gt; IMO, there is a schism between the "computer science research" community and the "practical, even commercial" community. Implying that computer scientists and other researchers have no interest in "practical" results. &gt; If we can achieve stability, which I have proposed includes long-term support for older versions of the language and its libraries, then we can heal the schism. Great, feel free to pick a version of the Haskell Platform and provide long-term support for it. &gt; My patch wasn't rejected, but it was overlooked for GHC 6.12, meaning it wasn't included until GHC 7.0. I can't tell if this complaint is meant seriously... "my patch took slightly too long to review, so it wasn't included in my preferred version of the compiler, but only the next one!" Review times are dictated by the availability of people who will do the work. A schism in the Haskell community can only bworsen things, not improve them. 
I thought perhaps you were talking about a classic DeBruijn representation where even the free names are just represented by indices: If you are looking at a subterm under *k* binders, indices lower than *k* refer to bound variables and indices greater than *k* refer to free ones. At the risk of telling you something you may already know, the tricky/annoying-to-implement part of the DeBruijn representation is that when you are implementing `subst e k m` (substitute *e* for the the free variable *k* in term *m*) and *m* happens to be a lambda: λ.m′ you now need to shift all the free variables in *e* by 1 (ie, `subst e k (λ.m′) = λ.subst (shift 1 e) k m′`). (Well... in any case that's the part that I always found annoying to implement.) With locally-nameless you don't need to do any shifting of *e* since free and bound variables have distinct representations. 
[C.md](https://github.com/jameysharp/corrode/blob/master/src/Language/Rust/Corrode/C.md) seems to be a literate markdown file. How does that work? Was this implemented in GHC? I only know of [the proposal](https://ghc.haskell.org/trac/ghc/wiki/LiterateMarkdown), not that it is in a current release.
&gt; I would love to hear your thoughts on what an effective Haskell cookbook/wiki would entail. It's a complicated conversation made more complicated by non-overlapping priorities and notions as to what's effective for learners and practitioners. What I call a "cookbook" or at least intend to make is not a grab-bag of examples that'll fall out of date and never be organized into a coherent curriculum. IMO, if you want to play it fast and loose: a wiki is strictly worse than a git repository of example code because at least you can build the latter and manage changes via pull requests validated by CI. Further, it's easier for users to fork a git repo and extend it with their own stuff if they want.
When I did .NET for a living, I remember MSDN having a lot of good information but being nigh impossible to navigate without third party references.
Purely on the technical side: IMO, a modern web-based tutorial should be a bit of both; long-form text and easy-to interact code: I really love this format: https://probmods.org/ , pretty much like a notebook-style website.
Please define the terms you're using otherwise the discussion is pretty much useless: we are simply going to alternate Yes and No all the time...
This could come up in practice if you use the structure like a stack and you push and pop at the `2^n-1` to `2^n` boundary. That is amortization, but it's a specific case of it. You can make up your own definition, but then you aren't doing an apples to apples comparison to other libraries that use the correct one.
http://hackage.haskell.org/package/nested-sequence-0.1/src/
That's the same with any immutable data structure, including finger trees.
From your link: &gt; purely functional if they guarantee the (weak) equivalence of call-by-name, call-by-value and call-by-need evaluation strategies. Haskell does not satisfy this criteria and is therefore not purely functional (non termination is an effect and can be observed or not depending on the reduction strategy picked). On an unrelated note: Why do you suddenly feel the need to start attacking me rather than putting your arguments forward in a civil manner?
This blog post gives an example of a useful case: http://www.yesodweb.com/blog/2015/07/yesod-table Full discloser: I wrote that post. It may not seem compelling unless you have done web development.
This is why laziness matters. I would recommend reading "purely functional data-structures" and the [finger tree paper](http://www.staff.city.ac.uk/~ross/papers/FingerTree.pdf).
You suggested that haskell-lang.org should explain itself. I cited the part of the announcement that explains the motivation for the site. **EDIT:** Did you mean something else with "Shouldn't that other website explain itself?"?
Ok, I can see how laziness may help in some specific cases like the push/pop example, but I don't see how it helps in general if you keep repeating worst-case scenarios and at the same time inspecting parts of the result in a way it forces evaluation of the spine?
When you force the execution of the spine, it's because you are doing an `O(log n)` operation already. As I say, I don't remember exactly how the amortization works, but it'll be in the paper. It's why finger trees were so novel when they were new. Also, this is an easy mistake to make. The scala finger tree implementation was incorrect for the same reason at some point, though I think it's been fixed now.
Jamey explained it in his blog post (linked in another comment, [relinked here](http://jamey.thesharps.us/2016/07/translating-c-to-rust-and-how-you-can.html?m=1)). Basically you just tell GHC what pre-processor to use to get the Haskell out. 
Who are the haskell community and who the FP complete community?
Ok, let's consider an imperative vector. We'll double the capacity when it becomes full and we'll half the capacity when it dips below a quarter full. At any point in time, we have to do at least `O(n)` `O(1)` operations to change it's size twice and this rebuilding costs us `O(n)`, so we can distribute the building cost and we get `O(1)` amortized per operation. This is how we do amortization, but back in lazy functional land, we need to consider lookups as well as inserts and deletes since they'll force the thunks (otherwise, we'll just end up with `O(1)` for every operation no matter what data structure we're using).
Brb, registering learnyouakx37x362.com 
Most of my interactions with .NET have been with a 5-10 foot pole and I've found the MSDN sources to be a huge pain (and the first few times I wasn't *sure* I was even in the right place). However, maybe F# makes a better comparison– I'm sure there are resources on MSDN somewhere, but when I first had to learn a little I found https://fsharpforfunandprofit.com and hardly ever left. If I were to suggest where someone start with F#, I would absolutely direct them there first.
That is assuming that there are no bugs. That's verification.
Yes, I too always felt that haskell.org was not explaining what to choose adequately. 
My issue is still the following: You have an *immutable* data structure, you keep repeating the worst case operation, the running time will be always worst case. I see that in *some* real-world program this won't happen, and that with laziness there are more programs in which this won't happen, but I cannot imagine a definition of "real-world program" which guarantees that this doesn't happen. So it seems to me that you will never have a better amortized running time that worst case, if you really want to stick with a definition of amortized which allows repeating the same operation on the same data structure. I simply don't think that's a good definition for purely functional algorithms. I accept that there are definitions of "amortized" inbetween, for which the strict version of this library has worse amortized cost than the lazy version. I don't think this is true if you want this very broad definition (because then both will be O(log(n))).
Yes you can. I suggest using the tool "stack" for the most streamlined experience. I myself build on win, Linux and Mac using "stack build" command.
See the [cabal file](https://github.com/jameysharp/corrode/blob/master/corrode.cabal#L17), which tells GHC to use [markdown-unlit](https://hackage.haskell.org/package/markdown-unlit) to process `C.lhs`, which is a symlink to `C.md`.
That's why there is /r/haskellquestions
I think there's something wrong with the examples on the index page: do let c0 = inTCtx 1 c1 &lt;- resume f ct1 _ &lt;- resume g ct2 Where do `ct1` and `ct2` come frome? What happens with `c0` and `c1`? I believe the next example contains similar mistakes. You should also include the example of the email server, the rest of the documentation is rather abstract. **EDIT:** The indentation is somewhat unconventional too. **EDIT2:** Sorry for being so negative above. This looks like a really nifty abstraction, only I currently would have a hard time figuring out how to use it.
Shameless plug for a post that I wrote on this subject: * [How to make your Haskell code more readable to non-Haskell programmers](http://www.haskellforall.com/2015/09/how-to-make-your-haskell-code-more.html)
I have seen at least two other libraries using the fooMay naming scheme for functions that return Maybe. [Safe](https://hackage.haskell.org/package/safe-0.3.9/docs/Safe.html) and [MonoTraversable](https://hackage.haskell.org/package/mono-traversable-1.0.0/docs/Data-MonoTraversable.html) For example, both define headMay. My preference is for consistency across Haskell libraries, so please consider indexMay over mbIndex. That aside, thank you for contributing your library!
Oops :) I edited the metadata. But, do you know how to indent code on the Cabal description? Good idea about adding a practical example. I'll do that.
Yes! Haskell can do that : ) Thanks so much Kyrenn, can't wait to have you aboard : )) (big fan of Starbound!!) Well, the GC often SEEMED to be a problem, but in every case so far it turned out to just be a symptom of some memory issue elsewhere. The only remaining special provisions for GC are the `-H512m -A4M` flags to the RTS and I haven't touched those in a while because it's been running beautifully : ). VR is an especially tricky case with 11ms per frame @ 90FPS so that's pretty exciting! Feel free to send me an email any time at this username at me dot com if you get into it and have questions : )
I think type-level tracking of scope is great for DSLs and meta-theory, and an awful idea for implementing PLs.
&gt; But, do you know how to indent code on the Cabal description? It seems to work when you format it like this: &gt;runT (f 1 &gt;&gt;= g) &gt;where &gt; f :: Int -&gt; T M a &gt; g :: a -&gt; T M b BTW I tried to clone the code [from your repo](https://sealgram.com/git/haskell/interruptible/) with the instructions on top but got ~/src $ git clone http://haskell.git.sealgram.com:8888/git/haskell/interruptible/ Nach »interruptible« wird geklont fatal: unable to access 'http://haskell.git.sealgram.com:8888/git/haskell/interruptible/': The requested URL returned error: 500 
I think your question might be a bit misguided. There is no "main reason" of having an id function...just like there's no "main reason" of having the number 11 in a language. It's just that someone got tired of writing `\x -&gt; x` some day in a lot of random and insignificant places and just aliased `id = \x -&gt; x` so it would be more convenient. Looking for neat examples of `id` is possibly missing the point, and I don't think it'd help you understand it any more. I can only see it making someone more confused and misleading people. It's just a common alias for a function that people commonly use in random places. For example, i might want to compose all functions in a list: composeAll :: [a -&gt; a] -&gt; (a -&gt; a) composeAll = foldr (.) (\x -&gt; x) The "empty list"/accumulator case is just the function that returns its input, so `(\x -&gt; x)` is what we'd use. We'd use it even if we didn't have `id` defined. `id` just makes it more readable. In any situation where you work with higher-order functions, there'll sometimes be random, unconnected, coincidental, happenstance situations where you might just want to pass in `\x -&gt; x` as an input. These situations aren't related in any way. You might sometimes want to pass in `\x -&gt; x * 2`, you sometimes might want to pass in `\x -&gt; foo x`, you sometimes might want to pass in `\x -&gt; not x`...`\x -&gt; x` is just another function that you might sometimes want to use, for all different reasons with no unifying theme :) You might as well search a code base for uses of the number 11 and find a unifying connection between them. you'll be disappointed when you find out the theme is "when you arbitrarily need something bigger than 10 but less than 12" :)
I don't know if you are the author of the postgresql-simple gist, but there are generic default instances for ToRow and FromRow; this does need to be documented, see [#171](https://github.com/lpsmith/postgresql-simple/issues/171).
In `WAI`, you've got these web `Application` things. What's a middleware for an application? It's something that gets access the requests/responses and does something with it in a way that can be composed and layered. In the types, it's an `Application -&gt; Application`! A common middleware is request/response logging.Sometimes, you want to `logStdout :: Application -&gt; Application`, and sometimes you want to be `logStdoutDev :: Application -&gt; Application`, and sometimes you don't want to log at all: `id :: Application -&gt; Application`.
I do translate that as "pushing every wanted feature as an extension and seeing what stands". Personally, I think it's great. But I understand someone thinking it's a tragedy.
I would say that if two pieces of code are equally readable and all else is equal, then the more succinct version is almost always better. The tricky part is that this is rarely the case. You have to gauge the readability, maintainability, etc. But you probably should do that in the context of Haskell developers rather than developers at large.
I think you're misunderstanding it. The main goal of pretty much any story for binders isn't about saving memory, it's primarily about ensuring that we don't write unhygienic programs or program transformations, often with a secondary goal of providing fast/easy alpha-equivalence. Using explicit names it's all too easy to break hygiene: allowing a variable to be used outside the scope in which it's bound, allowing binders to capture things they shouldn't, etc. As a particular example, correctly implementing capture-avoiding substitution is far trickier than expected. So the primary goal of any variable binding story (de Bruijn levels, de Bruijn indices, locally nameless, HOAS,...) is to get rid of these problems.
&gt;you can build the latter and manage changes via pull requests validated by CI. Why someone hasn't built something that combines a Wiki and a git repo into a web-based literate-programming environment is beyond me.
Isn't this a joke? seriously? this reminds me the nodejs/iojs split, but that's a matter of core runtime disagreement, what do we gain by setting up a new site like this??? The fundamental problem is this site is NOT COMPLEMENTARY to official site from any point of view.
&gt; The fundamental problem is this site is NOT COMPLEMENTARY to official site from any point of view. ~~I don't think it's intended to be complementary to haskell.org. It's intended to be an alternative that guides newcomers directly to `stack` and beginner-friendly documentation.~~ **EDIT:** See chrisdoner's [much better explanation of the purpose of the site](https://www.reddit.com/r/haskell/comments/4ruqbl/new_haskell_community_nexus_site_launched/d55c9bb).
&gt; I have no idea what the drama is between you and haskell.org &gt; And now, with this attempt at dividing our community to fit your side of the drama, you're extending this drama to places that were previously safe from it. Maybe you should read up on the background of this "drama" before you make such accusations. You'll easily find some examples in snoyberg's comment history.
Thank you for your answer ! I agree, my question may be a little misguided. I will try to explain my logic. We have `length` function so we can get the amount of elements in a data structure. We have `map` function so we can apply function to something that can be mapped over. So why do we have `id` ? What tasks does it help us accomplish ? Maybe "reason" is not the best word here ! To avoid that kind of confusion i made 2 questions: 1) why do have `id` function ? 2) what are examples of its applications ? (that might not necessarily imply the reason why we have it in language)
If it not call "haskell-lang.org" instead of other name like "friendly-haskell", it world less controversy.
Here's two aphorisms for you to weigh up: 1. It's harder to debug than to write code, therefore if you write the cleverest code you can, you will not be able to debug it. 2. The more you say, the harder it is to understand your meaning.
org-mode! But yes, more development in that direction regardless of language/environment would be much appreciated.
That would have been a good idea in hindsight. The name `haskell-lang.org` arrogates to itself the claim to be the official site for Haskell. Maybe the creators would be open to rethink this and may find a better name that better conveys the purpose of their mission, namely to provide a resource targeted mainly at newcomers. 
Wow ! thats pretty cool stuff here 8)
&gt; Months of effort have been wasted discussing and arguing about that link with the current holders of haskell.org. Wait... is the mere disagreement over a few &lt;a&gt;-tags on the download page the reason haskell-lang.org was made necessary?
No, by link I meant haskell.org. The user experience is quite different. Getting Started gives you a one liner script to get stack and a trivial Hello World script. From there you can get GHC, GHCi, Haddock and Hoogle (merged on master, will be in the next release) and Intero integration. The Packages page links (just for one or two at the moment but the aim is more) to a page each (see e.g. http-client) which makes use of stack's ability to run and setup a script in one go, showing a plethora of runnable examples. /intero is already a page but yet to be integrated, but that's another component that assumes all this basic infra is in place. Stackage is going to be integrated right into the main site. In summary it's not just a few links but we've done a lot of hard work on tooling that fundamentally departs from the status quo and are using that as a foundation for a smoother for beginners and experts alike experience. We've just outgrown the scope of what haskell.org is prepared to handle.
I'll give you what I hope is an objective view on what's happening right now. Haskell.org is run by a committee. It's been around a long time, as have some of the members on the committee. The committee is all-volunteer and yet tasked with making tough decisions. Recently, they have been moving very slowly on changing one page in particular: the [downloads page](https://www.haskell.org/downloads). Obviously that's an incredibly important page for the language community! And unfortunately, it is a victim of "design by commitee". It gives you *three* entirely separate methods of getting a Haskell environment on your machine, for the unfortunate reason that the separate proponents of the three cannot find it in their hearts to let any one method be the standard. FP Complete is a Haskell-based company that has gone through a few iterations. Earlier iterations could be reasonably argued to have been at odds with Haskell's community focus. This has not been true for many years. Today they employ prolific Haskellers who contribute to all manner of useful community tools, and they do so in a free/libre/open manner. One [prolific Haskeller](https://hackage.haskell.org/user/MichaelSnoyman) in particular works for FP Complete. He has also been involved in disputes with the Haskell.org committee, acting as a sort of "accidental spokesperson" for a non-trivial section of the community that feels excluded from the committee's decision-making. Tempers have occasionally flared. FP Complete wrote stack, one of the three options listed on the downloads page. Everyone at FPC, as well as plenty of people outside that company, think that it's the only method that should be listed. (I'll break my attempt at objectivity long enough to say they are correct. The other methods should only be used if you know what you are doing — and maybe not even then.) FP Complete has just launched a new website, haskell-lang.org, which is *not* run by the haskell.org committee. Just look at their version of ["Getting started"](https://haskell-lang.org/get-started). They could not get the result they wanted from the committee, so they have broken off and staked out their own domain. Doing so has given them the freedom to implement the better "technical solution" to the downloads page. The irony is that nothing has really changed. Both domains are run by a benevolent autocracy. Perhaps one day a more satisfying democratic system will eventually be developed. Perhaps also, one day, the rift will get patched over. But who can say? Maybe that will never happen. Diversity is both a blessing and a curse, so who can say which would be better.
[Image](http://imgs.xkcd.com/comics/standards.png) [Mobile](https://m.xkcd.com/927/) **Title:** Standards **Title-text:** Fortunately, the charging one has been solved now that we've all standardized on mini\-USB\. Or is it micro\-USB? Shit\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/927#Explanation) **Stats:** This comic has been referenced 3175 times, representing 2.7017% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d55d0ji)
If Stack suits your workflow, then it makes perfect sense for *you* to use `stack`. But you can't deny there's a significant amount of users who seem to be prefer Cabal which suits *their* workflows. Lemme play devil's advocate: Let everyone use whatever tool they deem better for *their* workflows, rather than imposing the tool you prefer based on *your* workflows. That's what all the "beef" is about. Live and let live. Don't promote one tool at the expense of the other.
Or haddock documentation from stackage. 
Fair enough. Just for the record, personally I often switch between `stack` and `cabal` myself. Because each tool (or software in general) is "bad" albeit in different ways. And I try to use the one tool which is the lesser evil in a given context. I tend to recommend `stack` to newcomers myself while mentioning they also need to learn about `cabal` at some point if they're planning to use Haskell for serious development.
That's a great idea!
Indeed. There is a literate programming system for C as well called CWEBx
Nice write-up. I used your original post on servant and persistent to get started with those tools. Thanks for writing this and building the starter pack.
It almost sounds like the Rust team intend to make a tool for this? &gt;Whatever this venue is, it would track the progression through all of the phases. Let’s call this venue the “Problem Tracker”. I also really like the phases they divided it up into: 1. Problem consensus 2. RFC drafting 3. RFC PR(s) 4. FCP 5. RFC merged I really think the Haskell community/GHC development could learn from this, if they ever wish for it to also garner widespread contribution adoption (if that is even a word?). Also: &gt;Idea: move away from video meetings for decision-making, instead reaching decisions entirely in the associated comment threads. Would also be cool. My biggest gribe as a I-wish-I-could-contribute-to-GHC person is that the process is clearly layed out, preferably centralised (instead of mailing list, trac, phabricator, wiki....) and open to everyone so they can participate or just view on the sidelines if they are simply interested without any strong opinions of their own. Seems like a far away dream though...
Thanks. Edited it again. The repo instructions are wrong (didn't notice it either). You can clone the repository URL from the package description: &gt; git clone https://sealgram.com/git/haskell/interruptible 
I was first overcome with Haskell fever about 3-4 years ago, and have been writing or otherwise spending time learning Haskell almost daily ever since, on my free time (I have a big [hobby project](https://github.com/jasonstolaruk/CurryMUD) that I'm working on). At the same time, I've been a Java/Android developer at work, but a couple months ago I got put on a project developing a web app in JS/TypeScript, using the React/Redux framework. I hadn't written or read a line of JS prior to this, so I had to dive in and learn the language as quickly as possible. The learning process has been quite interesting as I've discovered that although Haskell and JS are in many ways quite different, they share some key similarities in their functional aspects. Given this, I think my knowledge of Haskell has been a great asset. It's helped me to pick up JS quicker than I otherwise would have. In particular, my coworkers talked about how it was a bit of a struggle for them to learn React/Redux, while for me it clicked immediately. I was a bit surprised by all of this. If you follow this reddit, you'll notice people regularly say that learning Haskell ultimately helped them to become a better programmer in other languages/in general. I was never able to relate to this much, as I never really found ways to apply my Haskell skills/knowledge to my Java/Droid programming. But I see things a bit differently now that I'm writing JS. So, to the op - I suspect your knowledge of JS will only help you as you endeavor to learn Haskell. But having said that, I wouldn't necessarily try to find JS -&gt; Haskell tutorials (do such things even exist?). Get a Haskell book and read it, and (this is important) write a lot of Haskell code! Try to think of a couple toy projects that you can see through from start to finish. Haskell has a very steep learning curve, but it truly pays off if you can make it over to the other side of the curve. Don't give up. Don't expect things to click right away, expect that at certain points you're going to need help from the community. When you are stuck or just curious about something, come back here or to the #haskell IRC channel and ask questions. Good luck to you!
Don't you notice a hint of circular reasoning here? X is bad because X is bad. There are a few people who get off their ++++s and try to improve things, and then there's the passive user/consumer, who can only complain about big macs getting smaller over time. It's open source, do something about it! &lt;/rant&gt;
A more familiar form is ($) = id @(_ -&gt; _)
This doesn't make sense to me... You usually can *only* have either a covariant or contravariant Functor instance for a given type but not both (the only exceptions are the type constructors who ignore their type argument like `Const`). They functionally behave in different ways, since they are exact opposites. As /u/mn-haskell-guy said, you can see the covariant `fmap` as working on outputs and the contravariant `contramap` as working on inputs, so they are not interchangeable. As far as the general mathematical concept of covariance vs contravariance goes, the functors will ultimately map to different categories. Those categories are closely related since they are opposite categories, but they can be very different in important ways: for example Set, the category of sets and functions, and its opposite category (Set^op ) are not even isomorphic to each other.
Thank you for stating this. I'm unaware of anyone on the haskell_lang team who either said this or anything like this. It's definitely neither my words nor opinion.
&gt; I have no hidden agenda. Oh come on. Private control over what obviously is intended to appear as an official website absolutely has dubious overtones. Not that that bothers me. HaskellPlatform is crap, not offering stack as a newbie download is ridiculous, and if the community is incapable of making sensible decisions for the website, then I for one welcome our new corporate overlords. Almost reminds me of how Stallman's aloof hardlining caused the creation of Clang/LLVM.
Very interesting idea but I can't understand it. I have a few questions that might help me. What is the type of `dag1`? Why is the right-hand-side in `a &lt;- []` a list? When you write `Roll` do you mean the `Rec` constructor in your definition? What are the types of `let_`, `node` and `Place`?
(-) and negate could actually be defined, with the negation of a regex being a regex that matches iff the original regex doesn't.
&gt; Using a strict data structure won't help you... it's the lazy IO (tying side effects to function evaluation) that's the problem. FWIW, I believe the strict-bytestring-flavored IO functions aren't lazy...
Great job, /u/bgamari. Your work here is much appreciated.
I would also like to stay on community infrastructure and looked into using our existing Phabricator infrastructure for this reason. Unfortunately Phabricator lacks some features (e.g. the ability to render Markdown or ReStructuredText files) which make it rather unappealing. Given how much pressure there has been to move various aspects of GHC development to Github and the fact that the Rust process seems to work reasonably well, I thought I'd give it a try. That being said, suggestions for alternatives are of course welcome.
Seconded. Let's hope this doesn't lead to louder requests to move from Phab to Github.
I am not sure it helps distros since they generally prefer not to bundle anything, but sure it will be good for binary tarballs, etc.
&gt; Meaning my specification part got read and processed, but the nodes part didn't. Now I don't understand this. Evidently, using span forces evaluation of everything up to "NODE_COORD_SECTION" but I don't get why the nodes aren't evaluated, since that lambda function I mapped over them has to actually split up each string in order to get it in the proper form. How can that huge expression be a thunk? An in-depth explanation is welcome(but of course not expected), since I'm a beginner. Here's, as far my knowledge goes, a line-by-line explanation of what gets evaluated when. tspFile &lt;- openFile path ReadMode Does not evaluate anything, just opens the file handle. (specs, nodes) &lt;- span ((not . (=="NODE_COORD_SECTION"))) . lines &lt;$&gt; hGetContents tspFile Whether this evaluates anything depends on the internal implementation of `span`. If `span` is defined in a maximally lazy fashion, it could return a tuple constructor before even looking at the arguments you give it. Based on your report, it looks like `span` probably does evaluate the matching part of the list *before* evaluating to the tuple constructor. let parsedData = ((map (\spec -&gt; let (keyword, value) = span (not . (==' ')) spec in (keyword, drop 3 value)) specs) , (map (\node -&gt; let vals = words node in (read (vals!!0), read (vals!!1), read (vals!!2))) $ tail (if last nodes == "EOF" then init nodes else nodes))) Shockingly, this evaluates **nothing**. Using `let` to bind a name to an expression, even a *huge* one like this, does not cause any evaluation to happen. It only creates a 100% unevaluated thunk, and binds it to the name `parsedData`. hClose tspFile Closes the file handle. From this point on, if any thunk that depends on the file contents (as a result of `hGetContents`) triggers a read from a file, you'll get the error you encountered. return parsedData This too evaluates nothing. The name `parsedData` was bound to an unevaluated thunk, which is then returned back to whoever called `readTSP`. If that parent function starts evaluating it (such as by printing it), that's when you'll get that error. As others have explained, you should use `Text`. [`Data.Text.IO.readFile`](http://hackage.haskell.org/package/text-1.2.2.1/docs/Data-Text-IO.html#v:readFile) reads the file simply and strictly, and then [here](http://hackage.haskell.org/package/text-1.2.2.1/docs/Data-Text.html#g:19) are `lines` and `words` functions. 
How about gitlab??
At *minimum* GitLab is worth a look—ideally, but not necessarily, self-hosted. It offers a set of features very similar to GitHub with an open source core. Some of its design is better than GitHub, some is worth, but they're mostly very comparable. I've been experimenting with GitLab for one of my [own projects][cow] recently and have been reasonably happy—happy enough to start moving away from GitHub. I did also get some sweet GitLab socks, so maybe I'm just easy to bribe :P. [cow]: https://gitlab.com/tikhon/cow
Credit where credit is due and no credit where none is due: we both know that's directly motivated by not being steamrolled by FPC. It's the same way GCC is now scrambling to keep up with all the stuff LLVM has. HP was crap for ages and nobody lifted a finger to fix it, now suddenly "oh hey look it's great now! Give it a go!"
Thank-you. 
That's an unproductive and rude thing to say. And even then its besides the point -- if the platform is better, shouldn't you just be happy? (The actual history is as follows: It's due to complaints that led to the minimal installers [especially MinGHC] and attempting to resolve them within the platform. The wheels for two elements of this plan were set in motion before stack was even released [those elements being the minimal installers and the improved windows situation] and the third element -- the addition of stack -- was proposed jointly and collegially by Mark (who then maintained the platform) and Michael Snoyman.) I don't see what you gain out of spreading this sort of mean and gratuitous negativity.
It's definitely pretty confusing...and I think it makes it harder to read and glance over, and probably for sure less beginner-friendly. `Const` is poly-kinded, so its kind is: Const :: * -&gt; a -&gt; * Meaning the second parameter can have any kind: ghci&gt; Const 1 :: Const Int Double Const 1 ghci&gt; Const 1 :: Const Int Either Const 1 ghci&gt; Const 1 :: Const Int Monad Const 1 ghci&gt; Const 1 :: Const Int (Ord Bool) Const 1 So the second parameter can be `Double`, `Either`, `Monad`...really, anything that exists at the type level. Typeclasses, constraints, you name it. The haddocks are basically treating `Const` as if it had an implicit *kind* parameter, as well. Kind of as if it were data Const :: k -&gt; * -&gt; (a :: k) -&gt; * -- not legal haskell With an implicit parameter describing the kind of the second argument. So `Const Int Double` would really be `Const * Int Double` (because `Double` has kind `*`), `Const Int Either` would be `Const (* -&gt; * -&gt; *) Int Either`, `Const Int Monad` would be `Const ((* -&gt; *) -&gt; Constraint) Int Monad`, `Const Int (Ord Bool)` would be `Const Constraint Int (Ord Bool)`, etc. So when it says that `Const * m` is a Functor, it means that it's a Functor when the "second parameter" is of kind `*`. This means you can have: fmap f :: Const Int Bool -&gt; Const Int String but not: fmap f :: Const Int Monad -&gt; Const Int (-&gt;)
I'm using the `RebindableSyntax` language extension to replace monadic do-notation with my own custom bind and return. The right hand sides of each line are the parents of the given node. You are correct, I did mean the second Rec constructor (fixed now). The types are indeed a bit odd - the correct type for well-formed DAG should be `dag1 :: forall a. Rec DAGF a a` (I called this `DAG` in the code). Unfortunately I don't have a working ghc at the moment so I can't give you the types of `let_` and `node`. `Place` has type `b -&gt; Rec p a b`. If we simplify `p a = f`, we get a new type `Rec f b = Place b | Roll (f (Rec f b))`. You should think of this as `b` wrapped by some unknown number of layers of `f` (possibly none). So `Rec f b = b | f b | f (f b) | ...`.
I just learned vimscript for this project, and it's far from well engineered or done. Right now it's communicating with a REPL, does type-at-point, and installs intero if it's not available.
I have made a proposal to display `*` as `Type` in GHCi ([`#12030`](https://ghc.haskell.org/trac/ghc/ticket/12030)), which I believe is clearer. Your question concerns Haddock but the same reasoning holds. class Functor (f :: * -&gt; *) where becomes class Functor (f :: Type -&gt; Type) where and the kind of `Const` would be &gt;&gt;&gt; :kind Const Const :: Type -&gt; k -&gt; Type ---- In the syntax of the proposal for *visual kind application* ([`#12045`](https://ghc.haskell.org/trac/ghc/ticket/12045)) the documentation would read Functor (Const @Type m) ---- How it would look &gt;&gt;&gt; :kind Const Const :: Type -&gt; k -&gt; Type Const @Type :: Type -&gt; Type -&gt; Type Const @(Type -&gt; Type) :: Type -&gt; (Type -&gt; Type) -&gt; Type Const @(Type -&gt; Type -&gt; Type) :: Type -&gt; (Type -&gt; Type -&gt; Type) -&gt; Type Const @Type Int :: Type -&gt; Type Const @(Type -&gt; Type) Int :: (Type -&gt; Type) -&gt; Type Const @(Type -&gt; Type -&gt; Type) Int :: (Type -&gt; Type -&gt; Type) -&gt; Type Const @Type Int Bool :: Type Const @(Type -&gt; Type) Int Maybe :: Type Const @(Type -&gt; Type -&gt; Type) Int Either :: Type Const @Bool :: Type -&gt; Bool -&gt; Type Const @Bool Int :: Bool -&gt; Type Const @Bool Int 'False :: Type
It is actually valid syntax with `TypeInType` GHCi, version 8.0.1: http://www.haskell.org/ghc/ :? for help &gt;&gt;&gt; :set -XTypeInType &gt;&gt;&gt; :set -XGADTSyntax &gt;&gt;&gt; :set -XRankNTypes &gt;&gt;&gt; import Data.Kind &gt;&gt;&gt; data C :: forall k. Type -&gt; k -&gt; Type where C :: a -&gt; C a b **Edit:** Missing argument to `fmap`, valid: fmap @(Const Int) @Bool @String :: (Bool -&gt; String) -&gt; (Const Int Bool -&gt; Const Int String) *In*valid: fmap :: (Monad -&gt; (-&gt;)) -&gt; (Const Int Monad -&gt; Const Int (-&gt;))
Your answer seems to contradict mstksg's answer.
Hi, you have a mistake at &gt; max :: Ord a -&gt; a -&gt; a -&gt; a should be &gt; max :: Ord a =&gt; a -&gt; a -&gt; a It's very clear from the context but these kinds of little mistakes can terribly confuse beginners. 
That's not how everybody is telling the story though...
The 6E6 dollar question then is: Now that we can do the same, why don't we?
I think the absence of shepherds puts the onus back on the proposer to move through the process. I think this will mean the new process isn't much better than the old one. The main thing that makes Rust's process good IMO is the shepherds. Also, I really can't stand phabricator. I hope you don't use that for proposals as well.
What would `x - y` mean?
Neato. There is also https://github.com/myfreeweb/intero.nvim 
Like Ben already said, GitLab would only add to the confusion and maintainance by adding yet another tool to the mix of tools we're already using. Which is, primarily Trac+Phabricator, with a few outsourced GitHub repos. It would make more sense to use Phabricator if GitHub is considered too proprietary, since we have already expertise with extending/customizing Phabricator to our needs (thanks to /u/aseipp taking one for the team by diving into Phab's PHP codebase). However, one of the features that make GitHub especially useful here is to be able to prettily render diffs/PRs of markdown as well as comment diffs inline, which is quite valuable for reviewing and commenting on text-proposals. Does GitLab offer this as well?
Fwiw, the Git repo would have been mirrored to http://git.haskell.org anyway. And it makes sense to also look into hooking up github-backup
What about making an issue for open discussion points, referencing them in the main pull request and closing the issue when done?
We use Gitlab for snowdrift.coop. It's an instance that Gitlab Inc. donated and hosts for us, as I'm sure they would do for GHC. I think it works great. Its feature set is not 1:1 with Github, but the only thing I *really* miss right now is the auto-checklists in Github's markdown. That's pretty minor. All the major points are handled just fine. I second tikhon's concern about placing community infrastructure on a closed-source platform. Nobody denies the value Github brings to open source work — least of all me — but when such an excellent alternative exists that doesn't critically undermine the principles it supposedly stands for, it would be a shame to dismiss it too readily.
I believe so, yes.
&gt; if the platform is better, shouldn't you just be happy? Sure I'm glad that HP is apparently good now. My unproductive commentary is largely eye-rolling over the virtue-signalling happening by both sides: FPC *does* have obvious political gain by making this move, and the sudden improvements to HP and haskell.org were motivated by the threat of being superseded by FPC's move. This feigned altruism by both sides is annoying; these moves were not altruistic, they were in self-interest, and that *does not bother me*: the result is tangible technical improvement. Cool!
Others already gave lovely examples with details. I'll add what I'm most familiar with. I've been using [hasql](https://hackage.haskell.org/package/hasql) which makes use of contravariant functors for encoding query parameters. I think of it as having the parameter tuple's type (output, to be fed to the query) stay the same across contramap calls. Likewise with decoding the result as a functor, the result type (input, received from the query) stays the same across fmap calls.
What if they define: instance Eq Int where ... instance dEqInt1 &lt;: Eq Int where ... --(the introduced named instance) -&gt; instance candidate instance dEqInt2 :: Eq int Where ... -- not a candidate instance (but can be given explicitly) Which instance will be used in the next code example? test :: Int -&gt; Int -&gt; Bool test i1 i2 = i == 2
Use more specific names like `blogPostTimestamp`, `commentId`, etc. This also has the other positive effect of making your code more understandable.
Sure. This page explains it a bit: https://hackage.haskell.org/package/Chart-1.1/docs/Control-Lens-TH.html "Classy lenses" generate a bunch of type classes in the background. `makeFields` (instead of `makeLenses`) might be the first thing to try: http://stackoverflow.com/questions/25585650/whats-the-difference-between-makelenses-and-makefields I'm not really qualified to comment on whether there's a run-time impact unfortunately.
To be honest I find that example surprisingly unimpressive. It's very low-level, it uses a lot of code to accomplish very little, and it doesn't demonstrate that it would be easy to create something useful. Those aren't criticisms of FLTKHS, just of your use of that example.
That's actually an interesting solution (and one I hadn't considered) However, I'm not sure if I really want to add more stuff to the pipeline of building. Haskell tooling in itself is somewhat rickety... 
With this approach, how do updates work? And can I use lenses with this for sane updates? (I have some deeply data because of the domain) if I can't, is there anyway to do lens-like updates? Also, this is with GHC 8 right? Does stack have support for it yet? Thanks
I'm thinking something like JSON (via [aeson](https://hackage.haskell.org/package/aeson-0.11.2.0/docs/Data-Aeson.html) )would be appropriate too. It's perfectly reasonable to go to an interchange format when communicating over a network, and JSON is natural for lots of web APIs.
You're absolutely right. I probably shouldn't have linked that example without including some context. I define [easy to learn](http://www.youtube.com/watch?v=5hoQLovZBxQ&amp;t=4m4s) as emulating the [C++ API](https://github.com/IngwiePhoenix/FLTK/blob/master/test/tile.cxx#L27) in order to piggy back on the already extensive existing C++ documentation. I have also outlined my [design motivation](https://github.com/deech/fltkhs-demos#why-is-the-demo-code-so-un-haskelly) in the [demos](https://github.com/deech/fltkhs-demos#why-is-the-demo-code-so-un-haskelly) package. And lastly I have documentation on how to [get started](http://hackage.haskell.org/package/fltkhs-0.4.0.9/docs/Graphics-UI-FLTK-LowLevel-FLTKHS.html#g:12) with the API. So, in a nutshell, the bindings are meant to be low-level and weren't designed to shield the user from the underlying imperative model. The code I linked isn't meant to show off Haskell the language but more show how to stand up a UI using idioms that are already in place and baked into the underlying C++ API.
I see what you did there
&gt; this is a serious problem... it's sort of embarrassing to advocate for Haskell for production software and then have to explain that this problem exists. While I applaud the various efforts to support new record styles that some people might find more comfortable, that is a huge exaggeration. This is really a minor issue. We write large production application web apps for the enterprise. We use the traditional record syntax, with the traditional style of prepending a short abbreviation of the data type name to each field name. It works fine. We spend our time solving real problems, not things like this.
&gt; the traditional style of prepending a short abbreviation of the data type name to each field name. It works fine. can confirm. the prefix convention works fine with all our production haskell code. It's really such a non-issue in production that none of us at work have even really bothered exploring the new GHC8 stuff for records despite running some stuff on 8 now.
&gt; Except that more than one record may have a `blogPostId` field But far fewer records will have that field than will have the name `id`. &gt; that name doesn't clearly say to me "this is the id of this record" If `blogPostId` doesn't say to you "this is the id of this record", then pick a different but not ambiguous name that does. To me, the name `blogPostId` communicates exactly that. `id` certainly doesn't--it conflicts with the very commonly used function `id :: a -&gt; a`. I'm not saying what naming scheme you should use, just that you should use one with names that are more unique than the ones the OP mentioned. There are [several](https://www.reddit.com/r/haskell/comments/4s5i79/whats_the_current_agreed_upon_solution_to_records/d56q72j) [approaches](https://www.reddit.com/r/haskell/comments/4s5i79/whats_the_current_agreed_upon_solution_to_records/d56ouei) mentioned in this thread. Pick one that makes the most sense to you.
&gt; In my opinion, this is a serious problem that I immediately ran into in a sort of big, spectacular way the first time I tried to write serious software in Haskell. This isn't a serious problem with Haskell, it is a serious problem with your naming convention. Patient: Doctor, it hurts when I use ambiguous names! Doctor: Well then don't use ambiguous names. In **all languages everywhere** name uniqueness should be roughly proportional to the size of the scope where they are used. Data type field names essentially have global scope, so it's reasonable to expect that they should be pretty unique. I think the reason people like /u/Bollu have this problem is because they are thinking about it from an OO mindset. But Haskell record names are not like the class/struct fields in OO/imperative languages. In those languages field names have a very small scope because they are always used "after the dot" behind another name that already restricts your scope to just that class. But in Haskell these names are actually global functions. And you should treat them as such. &gt; I feel like it's sort of embarrassing to advocate for Haskell for production software and then have to explain that this problem exists. I have worked with hundreds of thousands of lines of production Haskell and this has never been a problem for me.
You do have to love typing additional prefixes though ;) luckily, the language lets you cut corners elsewhere, e.g. by writing custom getters/setters without having to import `lens`
I would use short qualified module name as a prefix. Only small problem with this is that I can't import value constructor unqualified and record fields qualified. 
&gt; While I applaud the various efforts to support new record styles that some people might find more comfortable, that is a huge exaggeration. It's not so exaggerated; I routinely have to explain to clients that, yes, you literally have to prefix your field names, meanwhile trying to maintain that Haskell is a step _forward_, rather than a step _sideways_. &gt; We write large production application web apps for the enterprise. Facebook use PHP. The fact that it _can_ be done is uncontested, the point of having different programming languages is to make the _way_ things are done be improved. &gt; We use the traditional record syntax, with the traditional style of prepending a short abbreviation of the data type name to each field name. It works fine. That's called a workaround i.e. a means of overcoming a problem. The problem is that you cannot access and set fields of records simply by their name. What your workaround brings is redundancy and noise. Redundancy in that you always have to read and type your prefix (which is either short and unpredictable, or long and takes up keystrokes; because prefixes are all the same they are resistant to autocompletion), and also harmful for the use of things like deriving and reflection, because now you have to take the prefix into account; a prefix which exists only to workaround a problem which does not exist in other languages. I use the prefix workaround, but by no means am I happy about it, nor was I happy to discover its necessity when first learning Haskell. Nor, presumably, were the authors of TRex or the people behind the `OverloadedRecordFields` initiative. 
If the author were complaining that you can't do mutation in Haskell, I'd agree with your `Doctor, ...` analogy, because purity is at the core of Haskell's values, but the fact is we're talking about one of Haskell's warts, which is its implementation of records. Having a shitty record field access story isn't, as far as I'm aware, one of the core values of Haskell or an aim of the authors of the language. Other languages like Elm and PureScript have a much better story. 
I regard it as a really minor nuisance. You can either use modules as you suggest or prefix labels with some type specific letters (like C in the 70s). 
What happened to the Spock benchmarks? How can I help to make them fly? ;-)
I go with data SomeRecordType = SomeRecordType { srt_fieldName :: Int } Gets the job done form me.
Switching to `hasql` is probably a large part of it. You can take the code for the DB logic from the servant entry. Also it may be using an old version of spock (LTS 5.4). I don't know why it didn't complete certain tests though.
A regex that only matches iff x does and y doesn't.
Phabricator has plans to support bidirectional communication with GitHub, using their tool called 'Nuance', including mirroring diffs and tickets (this tool will, incidentally, hopefully also handle support requests through systems like HTML forms and Email, which would be useful for haskell.org). In theory, this tool could also allow the fabled acceptance of GitHub pull requests. (But you did not hear this from me, and furthermore, the devil is in the details as usual - we'd need a few particular things but I think that's doable.) Phacility was actually working on this quite a bit earlier this year, but it got shelved and they're currently prioritizing other work. They seem like they're willing to explore that and support it, though.
&gt; Having a shitty record field access story isn't, as far as I'm aware, one of the core values of Haskell We could just all agree that it *is* one of the core values.
I mean, I've written major pieces of production infrastructure in Tcl of all languages. It worked -- quite well, in fact. That doesn't mean that the language didn't have some large issues. It just meant we were able to work around them and/or got so used to them that we were sort of numb to the workaround.
No, it doesn't. DRF allows no more polymorphism than classic records, as it requires selector names to be unambiguous. We have plans for polymorphic getters/setters, but they didn't make GHC 8.0. The GHC wiki has more details: https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields
What about cyclic module dependencies, do you overcome those by using parameterised data types or do they just not come up that often?
Also, if you're just doing this for shits and giggles, you could try modifying `acme-http` to be a competitor to `rapidoid` :) http://hackage.haskell.org/package/acme-http
&gt; Are there another good implementations of type inference algorithm that would be easy to understand? The *[Typing Haskell in Haskell](https://web.cecs.pdx.edu/~mpj/thih/)* paper presents an implementation of type inference for Haskell 98 and is generated from a Literate Haskell file.
How do I import the code from that blog post? Talk is cheap, show me the code. Not snippets in the blog but a usable standard library that makes things monadic, as in Haskell.
Agreed, PureScript nips this one in the bud. I feel like the apologists in this thread haven't seen it, otherwise I don't understand how you can defend something that is demonstrably fixable.
Looking at the inference algorithm's code, I have the impression that the problem comes from generalisation being too eager. I (re)posted a write-up by Oleg of a [really nice solution](https://www.reddit.com/r/types/comments/4q9ol7/efficient_and_insightful_generalization/) to this precise issue. The code is in OCaml but you shouldn't have any problem reading it.
This will answer your questions: * [OverloadedRecordFields proposal](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields) Short answer: no lens-like updates just yet, but they will be available in a future version of GHC
Can't you stick them in different modules (in which you can use short names) and then use qualified names in the modules where you have ambiguities?
I believe I'm seeing a pattern here, maybe next on that roadmap: Trying to work with GHC devs, giving up and creating their own Haskell compiler?
The people who work on cabal and the website overlap more than the people who work on ghc. Fpcomplete have convincing examples of how difficult it was to get changes into either, and their ideas (e.g stackage) were dismissed as bad. Now that they price their ideas work quite well, they are taken more seriously, but even then they can't get the website maintainers to recommend new users to use stack.
Yes, but that is not without cost. If you have n things to be named and you are assigning them m names where m &lt; n, then the name does not uniquely identify the thing. That has implications for type inference and means that the programmer has to look at more context to understand something. One might argue that that is a small thing, but then I'd argue that adding prefixes for disambiguation is a similarly small thing. To be clear, I think it's great if people want to try to apply more sophistication to this problem. I just don't think that this is a serious problem with Haskell.
I'll mostly try to work out what's going on rather than suggesting alternatives, but in your second fix you could use [Control.Exception.evaluate](http://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Exception-Base.html#g:7) instead of `putStrLn`. Looking over the code suggests none of the file contents should be evaluated before `hClose`, not even demanding the first character: let is lazy, and the binding `(specs, nodes) &lt;- span ...` should only evaluate far enough to be sure that span will produce a pair. The documentation for span says "`span p xs` is equivalent to `(takeWhile p xs, dropWhile p xs)`", so the pair should be produced without demanding any lines from the file. Except, the documentation is wrong - even though both that comment and GHC's implementation are taken directly from the Haskell Report! Testing in ghci shows span needs to be able to evaluate the predicate on one list item before the pair is produced: Prelude&gt; span id undefined *** Exception: Prelude.undefined Prelude&gt; span id (undefined:undefined) *** Exception: Prelude.undefined Prelude&gt; span id (True:undefined) ([True*** Exception: Prelude.undefined (This is the behavior expected from the Report's implementation, after I re-read parts of the Report on let, case, and do). So, it makes sense that one character is read before `hClose`. Running you program myself (on GHC 7.4.1 under Windows 10) doesn't reproduce your problem, but instrumenting the code like this shows one character read before `hClose`: import System.IO import Control.Applicative import System.IO.Unsafe import Data.IORef traceList :: IORef Int -&gt; [a] -&gt; [a] traceList ref l = zipWith (\ix a -&gt; unsafePerformIO (writeIORef ref ix &gt;&gt; return a)) [0..] l type Node = (Float,Float,Float) readTSP :: FilePath -&gt; IO ([(String,String)],[Node]) readTSP path = do readIx &lt;- newIORef 0 tspFile &lt;- openFile path ReadMode (specs, nodes) &lt;- span ((not . (=="NODE_COORD_SECTION"))) . lines . traceList readIx &lt;$&gt; hGetContents tspFile let parsedData = ((map (\spec -&gt; let (keyword, value) = span (not . (==' ')) spec in (keyword, drop 3 value)) specs) , (map (\node -&gt; let vals = words node in (read (vals!!0), read (vals!!1), read (vals!!2))) $ tail (if last nodes == "EOF" then init nodes else nodes))) ix &lt;- readIORef readIx putStrLn $ "Used "++show ix++" characters before hClose" hClose tspFile return parsedData main = readTSP "gr96.tsp" &gt;&gt;= print Hopefully I could have used the gchi debugger instead of modifying code, but I haven't learned it very well. The only explanation for getting more characters than that is that the list returned from `hGetContents` must be implemented with buffering, and continues to return buffered characters after `hClose` rather than immediately raising the exception. Your output shows it got as far as the `NODE_COORD_SECTION` line, but I can't tell where it stopped, because the second component of your pair is if last nodes == "EOF" then init nodes else nodes and evaluating the if condition will hit the exception if the file was truncated at any point. You could try something like what I did with `unsafePerformIO`, or replacing the expression with something lazier like this dropTrailingEOF [] = [] dropTrailingEOF ["EOF"] = [] dropTrailingEOF (x:xs) = x:dropTrailingEOF I'm still a bit confused because that Stack overflow answer says the exception was introduced in 7.10.1, and I think the relevant buffer size is the one [here](https://github.com/ghc/ghc/blob/master/libraries/base/GHC/IO/FD.hs#L119) or [here](https://github.com/ghc/ghc/blob/master/libraries/base/GHC/IO/Handle/Internals.hs#L434), which [were set](https://github.com/ghc/ghc/commit/798805f7e841f381469a0c1d750970575d8e6fda) before 7.10.1 and look big enough to hold your whole file. EDIT: If I define span' p xs = (takeWhile p xs, dropWhile p xs) and replace `span` with `span'` in your code, it does reach `hClose` before reading any characters, and fails like the file was entirely truncated.
&gt; The people who work on cabal and the website overlap more than the people who work on ghc. You sure about that? Comparing - https://github.com/haskell-infra/hl/graphs/contributors - https://github.com/haskell/cabal/graphs/contributors - https://github.com/ghc/ghc/graphs/contributors doesn't seem to support your claim. And I don't get the last item either. https://www.haskell.org/downloads does recommend Stack as one of 3 options. What FP Complete wants is rather to suppress all other options instead, which I can understand from their point of view. But I can also understand why the Haskell committee doesn't want to censor the other options they're investing time and effort into. This clearly has all the signs of a power struggle.
Yes, that's possible, but there are many situations where it isn't practical - primarily, due to mutually recursive datatypes, which come up a lot, e.g. in database schemas. By standardizing throughout (nearly) all code, template haskell stuff doesn't need to be changed each time, and developers don't have to spend extra time thinking about which approach to use in each case (and refactoring when they get it wrong).
I've tried that approach, but ultimately decided it was better to just go with the full thing instead of abbreviating. In theory, there could still be conflicts, but in practice I don't run into them much at all. On the other hand, with abbreviations, I did run into them somewhat regularly. I added the leading underscore for better lens support.
I might be wrong about the overlap -- though I wonder if it is possible at all to have a quarrel with spj :) I'd be glad, by the way, if FPComplete came out with a competing compiler, and it drove improvements as great as stack, stackage and the new web site. IIRC, stack was not even the primary recommendation on the web site while they were arguing. And if FPComplete, working with newbies and companies routinely encounter the disasterous results of the other tools -- their input should be considered, and cabal/platform should not be recommended for newbies.
I do not think it is a serious problem -- but I think we all don't really know how serious it is. We've become accustomed to working around it, we don't really know what we're missing out on. For example, perhaps styles of programming where more precise record types are used everywhere (instead of tuples) would have been practical if we had shared record fields. 
One of my friends also happened to be doing some [simple benchmarks](https://github.com/sa1/server_benchmarks) (not the techempower ones, but he's going to build his own). Intrigued by your hasql results on the servant PR, I showed off some warp benchmarks to my friend, and it consistently beat other frameworks. Note: It was a fun experiment, but we learnt quite a bit. e.g the latency numbers reported by wrk don't mean anything. For the next set of benchmarks, we'll use wrk2 by Azul's Gil Tene.
Personally I think all we really need is being able to have multiple records with shared field names. Writing code that's polymorphic over the name of a field is insane.
The fact that you even need to think about this, and can't just do the obvious/naive thing, is embarrassing in a modern programming language.
You're right, it's not actually polymorphic—the effect is similar to that of polymorphic functions, which tricked me. I'm looking forward to Part 2. and 3. though!
&gt; stack seemed to be unable to pull in records? I'm not sure, I tried adding the dependency and it couldn't find it on stackage. I tried adding it to extra-deps and that didn't work as well. `records` seems to have seen its last update in 2012, so I guess it's fair to say that it's currently unmaintained. FWIW I got it to build with `stack` in the following way: $ stack new records-test $ cd records-test $ # Add records to the library build-depends in the cabal file $ stack solver --update-config # Add the necessary extra-deps $ stack build $ # Got a typechecking error, let's try with ghc-7.8 $ stack solver --update-config --resolver lts-2 $ stack build 
A lens is like defining a path through a nested data structure, and you can use that path to either get elements, set them, or modify them. The path is just a function, so you can do all the other things you usually do with functions. I really like [this expository blog post](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html) and [this tutorial](https://hackage.haskell.org/package/lens-tutorial-1.0.1/docs/Control-Lens-Tutorial.html) on lenses.
Because they are done of questionable quality, people see them then base decisions off them...so then you have a million posts on "why haskell is so slow on benchmark rando X" thats all it takes for people in companies to see once, next thing you know it's a mountain of bullshit you always have to argue against. For instance say I'm trying to convince center X of developers to give haskell a go, one guy in the crowd during question time explains "why should we use this, so and so benchmark says its mega slow", while I could say something to the effect that "many companies use it successfully without performance problems" question man has just done some good work convincing people that the already new "paradigm" i'm trying to interest people in, is slow and therefor probably not worth it. Just the fact that performance has greatly improved in one framework by having someone who is knowledgeable is good enough point to see that whomever is putting the benchmark site together doesn't take it seriously. It's very existence spreads misinformation.
For many, what makes `lens` particularly attractive is that it's a breeze to drill down into nested record data types using lenses, either to read or "update" a particular value. `lens` is especially elegant in this regard when working in the `State` monad, but it's still quite nice to work with lenses in other contexts. If you have `IntMap`s or `Map`s inside record data types, the `at` lens is very handy, too. I've found the `_1`, `_2`, etc. lenses to be very nice to use when working with tuples. `both` and `each` can be quite useful, too, when you want to do something with both/all the values in a tuple. Overall, `lens` adds more tools to my bag of tricks and frequently I can find concise, elegant ways to code using `lens`.
[This article was helpful to me](https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/a-little-lens-starter-tutorial)
This is what I do when I'm writing so many records that conflicts become likely
My hunch is that the cool performance comes from just trying to wrap nio [1] and the underlying nio library just being really good and fast. [1] https://docs.oracle.com/javase/7/docs/api/java/nio/package-summary.html
&gt; which SQL library was servant using earlier? As far as I know, `servant` doesn't use any SQL library. It doesn't deal with SQL. You as a user of `servant` choose and use an SQL library. Or do you mean to ask which SQL library the benchmarks were using previously?
Absolutely! But shepherding is not just unstructured helping; I think good shepherding involves a bit of responsibility and "herding cats" sensibility, as well as a certain level of expertise. So I would hope that the set of shepherds is larger than the number of core GHC developers, but I am not altogether sure how to get to a place like that.
Do you find L-values useful in other languages? ``` foo.bar.baz[17].quux.joom = 7; ``` Try doing (the immutable version of) that using haskell's record update syntax.
If you haven't already, you should check out Stack: http://docs.haskellstack.org/en/stable/README/ It seems to solve a lot of problems with managing GHC versions and Haskell packages. I've only used it for one project so far, but I'm really liking it.
I find that a high level comment on the function saying what it does and why is pretty important. When I come back a year later the name often isn't enough to tell me what and why, but a few concise sentences are very helpful to explain why do I even need this weird thing. So I often haddock even non-exported functions, and always haddock modules. However, I usually don't comment anything inside a function, though I will rarely add an internal type declaration if it's especially complicated. If I actually need to dig in to modify or debug (or to write that description I failed to write the first time), then I can almost always follow it pretty easily, no comments needed. I don't really clean up old code, since I'm no smarter now than I was 5 years ago. It's definitely easier to read than the much more verbose Java I have to figure out in my day job.
As far as I can tell, lens is really only interesting if you have nested data structures (often records). And the deeper the nesting, the more interesting it begins to sound. I find lens to be fairly utilitarian but not as mind expanding as other Haskell topics. It is based on a whole pile of solid mathematical foundation for sure -- but it is there to do something pretty mundane. Learn lens won't "make you a better programmer", but it may make you less irritable when working with nested data structures. It does have limitations though. You can modify an element in a (nested) list -- but you can't insert a new element. It is also reminiscent of Perl 5 line noise style coding where there is a special operator for everything made up of mostly punctation symbols. There is a rhyme and reason to the operating naming, but there is still a lot to remember. lens also has an astounding number of build dependencies. Despite these issues, I still think it is a good option if you are working with nested data structures. And, some of the issues are 'non-issues' anyway. For example, while lens has many dependencies, there is a good chance some library you depend on requires lens, so you will probably have to install it anyway. 
Everyone here is saying that lenses are all about nested structures, but they're also fantastic for transforming data. Got a bytes trying you need to treat as Text, do a transformation and convert back to utf-8 bytes? mybs &amp; from utf8 %~ myTextTransformer What if it was a lazy byte string but your transformer only works on strict Text? mylazybs &amp; strict . from utf8 %~ myTextTransformer Libraries like configuration-tools are built around using lenses to make configuration objects which can be nested arbitrarily. Once you start using lens, and exploring the whole library (and ignoring the parts you don't understand until you need them) you'll find yourself using them all the time, even for simple like linger level field access of s record, because it makes things consistent. I also find myself using `to` all the time, as a conduit between things which don't have a lens between them, so I can use lenses everywhere when transforming data. 
I probably shouldn't respond here, but I'm going to set the record straight for both your comments and what Gershom is trying to pull below. There is no FP Complete agenda here, or goal outside of what we've stated. You can say things like "dubious overtones" or "feigned altruism" all you want, but it doesn't change the reality of the situation. The reality is: myself and other individuals - inside and outside FP Complete - have tried for _years_ to improve the situation with Hackage, Haskell Platform, Cabal, and haskell.org. The changes you're now seeing come out in the platform are changes _I originally agitated for_ and spent many hours, days, and weeks hashing out with the maintainers. Stack was released because all efforts to speed up Cabal development were failures. We switched to using Stackage-based package hosting because of the glacial pace of Hackage security. And haskell-lang.org is only being launched because Gershom made unilateral decisions that were detrimental to the content of the site, and undoing those decisions takes far too much time to be worth it. Honestly, this silly community trop of FP Complete trying to amass power is just stupidity. I got the budget approved to work on this site not for _any_ community reason at all: I pointed out that we were fracturing our own internal documentation efforts because there was no solid, central place to put this stuff due to the problems with haskell.org. The fact that we made this open source and publicly available was due to having a team that loves open source, and hoping to collaborative with other great developers in the community. I'm sure most of the typical Reddit commenters are going to continue to attack this as "feigned altruism" or whatever. But if any of you are reading this and actually want to go through a real thought experiment on this, think this through: 1. If FP Complete was just interested in "politics," why would we: * Have collaborated on improving haskell.org, Hackage, etc, for so long before making clean breaks? * And if by politics you mean "look good to the community," every time we've done something like this we've been shit on by this Reddit hivemind approach. Why would we do it to get community brownie points? 2. What exact benefit are you thinking FP Complete is getting from being in control of things like haskell-lang.org? The benefit we care about is that there will be a site with good content that we can send new hires, new customers, and evangelize to the non-Haskell community (and even _that_ is just a hobby, not a business interest). __EDIT__ I based one of my non-blog-post posts on this comment: https://gist.github.com/snoyberg/b486983451fa8e5007de39bec8966edb
&gt; As far as I can tell, lens is really only interesting if you have nested data structures (often records). And the deeper the nesting, the more interesting it begins to sound. The prism stuff is very compelling to me too, and so is working with the state/reader monad. I routinely use lens-aeson to query large json files from ghci.
&gt;about how lack of tooling/infrastructure makes it hard for Rusts shepherds to keep on top of all proposals. They get a lot more proposals than we ever will, probs be fine to at least trial having shepherds if the intent isn't to have this die in the crib.
I've never understood why people keep using algorithm W. Algorithm M is more efficient, easier to formalize and I find it just more natural. Here are some references: [A Generalized Let-Polymorphic Type Inference Algorithm](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.6832), [Compositional Type Checking](https://gergo.erdi.hu/projects/tandoori/Tandoori-Compositional-Typeclass.pdf), [Proofs about a folklore let-polymorphic type inference algorithm](https://ropas.snu.ac.kr/~kwang/paper/98-toplas-leyi.ps.gz).
&gt; The changes you're now seeing come out in the platform are changes I originally agitated for and spent many hours, days, and weeks hashing out with the maintainers. Indeed, most of the changes to the platform are absolutely in accord with the "improving the get haskell experience" proposal jointly authored by yourself and mark roughly a year ago: http://projects.haskell.org/pipermail/haskell-platform/2015-July/003129.html I'm happy we finally got them made in the platform, and I think it is much better for it!
I tend to agree, to an extent. One reason for the planned polymorphic accesors is to allow constraint-based type inference to disambiguate names, even if all the code you write ends up being monomorphic. You can use DuplicateRecordFields already to have shared field names, it's just a bit harder to disambiguate them.
Fair enough. The design has been through several revisions over quite a few years, so no doubt it is hard to keep up with the current status!
You can't insert a new element at an arbitrary position in a list because it is lists are continuous, so what if a list has only 10 elements but you want to insert it at 100th position? In contrast you can insert new elements into `Map`, `IntMap`, and so on, using `Control.Lens.At`.
Serving static files is probably the least important benchmark anyway. In the real world you'd just put that stuff on a CDN. It's their job to win that benchmark! In any case I'm pretty impressed with how well the other benchmarks turned out.
Any time you want to change zero or more sub-parts of a larger value they're useful. It's not that they are revolutionary in any way. You basically know them already. They're kinda like accessors in OO languages. Except they're much, much, much more powerful. If you know them, you'll [miss them in languages where they don't exist](https://github.com/kqr/gists/blob/master/proof-of-concepts/pyfun-lenses.py).
unfortunately that one needs a different intero.
CI support is baked in. pre-commit hooks are possible (would need a support call if your instance is hosted, but I suspect it shouldn't be a problem). They do have some plugins, but not as many as Github does. Webhooks, yes.
[removed]
Facebook posts are not allowed on /r/haskell. If you believe your post should be an exception, please send the mods a modmail with a link to this post for further review *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/haskell) if you have any questions or concerns.*
Yes, of course. But whether msgpack is integral to intero is up to the maintainer (/u/chrisdoner) to say. Given, that it's not in mainline, I doubt it.
I'm not the author of this code, but i think you should definitely should add your feedback in a Github issue, since it will be nice to have clear documentation and examples!
Though this can be considered an anti-pattern as well if done to excess. I had a discussion about this in #haskell; almost all marshalling operations can be turned into "lenses" (technically isos) which gives you a fairly convenient API, but you lose the ability to examine errors.
Seems to be a class to abstract over `Elf32` and `Elf64` which encode things with different types/sizes, but are otherwise similar enough.
Haskell UTF8 source code in the wild! https://github.com/mvv/data-elf/blob/master/src/Data/Elf.hs Specific cases: * https://github.com/mvv/data-elf/blob/master/src/Data/Elf.hs#L248 * https://github.com/mvv/data-elf/blob/master/src/Data/Elf.hs#L259 * https://github.com/mvv/data-elf/blob/master/src/Data/Elf.hs#L485 * https://github.com/mvv/data-elf/blob/master/src/Data/Elf.hs#L611 
Good syntax is a "goal" of Haskell: * Whitespace sensitivity * Juxtaposition as function application * User-defined operators * Do-notation and then: * `-XPatternGuards` * `-XViewPatterns` * `-XPatternSynonyms` * `-XOverloadedStrings` * `-XOverloadedLists` * `-XOverloadedRecordFields` * `-XRebindableSyntax` * `-XPostfixOperators` * `-XTupleSections` * `-XNamedFieldPuns` * `-XRecordWildCards` * `-XQuasiQuotes` * `-XMultiWayIf` * `-XLambdaCase` * `-XUnicodeSyntax` * `-XApplicativeDo` * `-XBangPatterns` * `-XArrows` * `-XDeriveAnyClass` * `-XEmptyCase` * `-XTypeOperators` 
Extra example: &gt; ("{\"a\":[45,5,8]}" :: Text) &amp; key "a" . _Array . traverse . _Number +~ 42 "{\"a\":[87,47,50]}" (Yes, it's still a `Text`)
Do you find message chain a code smell? It is probably OK when you are working with tree-like data structures, but otherwise it is a sign of bad design.
I don't necessarily use them together, hence my "[spoilt] for choice" remark. I start with a couple simple, old fashioned data types. As soon as I use one of them as a field in another data type, I go for `makeClassy` so that I can easily use functions on the inside type. If I find I want to juggle multiple values of the same type, or I specifically want to write code without knowing anything about fields of a record, I go with `vinyl`. There is certainly overlap there, and in a case where I don't know which to use, I'd go with `makeClassy` as it's simple and builds on normal Haskell records.
Yeah I had considered using an almost identical example, but typing it on my iPad was painful. Great example!
I use https://github.com/tonsky/FiraCode ... so no real need for any higher planes of UTF8 (most latin1 is utf8)
Haskell newb here: why is ELF important to Haskell?
Fixed ! Thanks.
Undertow has an extremely large latency though, much worse than even wai has (~ x5 times larger). Batching?
AFAICT that doesn't prevent this: data Foo = Bar {unBar :: Int} | Baz {unBaz :: Int} main = print $ unBaz (Bar {unBar = 0})
&gt; Is that a good thing or a bad thing? Or neither? I hold no position on this myself. I know it is possible, but just haven't seen it yet in the wild. Personally I'm interested in how to code with symbols that do not have a place on my keyboard. I guess you can still use the non-UTF8 alternatives (::, &lt;-, -&gt;, =&gt;, etc.) but then you might break with the coding style.
&gt; Personally I'm interested in how to code with symbols that do not have a place on my keyboard. Extend your keyboard layout, better yet, just let the editor do it. Which is also why it's pointless: If the editor can expand them, you can also save them as plain ASCII and just have the editor *display* those digraphs differently: Ligatures that are exactly as wide as the digraph written in ASCII. Which brings me to another thing that annoys me: Most of those characters look abysmal in practically all fonts. 
I think we are seeing haskell moving into an era of much more serious industrial usage, instead of it being of purely academic interest. Not that the two can't live together, they certainly can, but they each have different requirements. We have amongst others seen this with the new `stack` tool, and the sudden increase in development speed of `cabal-install`, adding many much needed features. One other serious focus that the industrial usage has, is adoption of a language to increase the talent/hiring pool. Of course, this has always been a topic, but again in these recent times we have seen stuff such as the incredible new haskell book, and now this "takeover" of the haskell website to cater more to the needs of newcomers, rather than people already that already knew what they needed/wanted. I'd argue for any newcomer that an opinioated starting page gives a much better start, than starting out with having to research what option of the bunch you should use. We also see this in the ongoing debate with refining the proposal process - we are beginning to not just be a handful of developers (probably more, but you get the point) wanting to contribute to Haskell/GHC, and the current process simply not cutting it in a modern world of modern open-source. Dismissals such as "if they wanted to contribute, they wouldn't mind going through the tedious and unclear process" gets us nowhere nearer to fixing the actual problem that it is costing us man-power that would have liked to contribute, but are cut-off before even starting, because compared to the alternatives, the process is quite convoluted. Having programmed in haskell for a while now, and watched the community closely, I for one welcome the shift and see it as a sign that Haskell is being take more serious, and gaining more adoption. Just my two cents... Edit: Just to add, another thing I see often coming is as an argument against some of the changes, mostly in tooling, is that "this is currently being worked on" or "thing x will solve this" followed by a "and will land in version x" where x is some far off thing. Usually these efforts have been under way for a while, and there is still quite some time before they are complete, which to a user might as well mean they don't exist yet, for all intents and purposes. Some work is just so painfully slow, that I symphatise a lot with the ones developing tooling to solve problems "here and now", because we don't we the time to wait a year or two (at least our clients don't). /rant
&gt; Most of those characters look abysmal in practically all fonts. Yups. &gt; let the editor do it. Sure, yet another style difference between projects :) tabs/spaces, comma at start/end, line up imports, and now ASCII or UTF8 symbols!
In OO it's a violation of the law of Demeter and to be six levels deep in a hierarchy like that almost certainly a sign of serious problems. But why is that? Because the odds of you violating some invariant tends to go up quickly as you reach out of your object like that. In a strongly-typed immutable language, you're just constructing new values. If there was something wrong with you constructing that value that way, it should have been blocked by the type system or the visibility rules in the first place, because the invariants should be maintained there. What is "moral" and what is permissible ought to have a much tighter overlap in such a situation. Incidentally, even in OO, I consider there to be a significant difference between `A.x.y.z` and `A.x().y().z()`, in languages that lack "properties" like Python that turn all property access into potential method calls. If your objects have methods on them that don't maintain the invariants, well, what did you expect to happen?
The first two lines are alright. The third (with the type synonym) is incorrect. This is what I would recommend: import Data.Text (Text) import qualified Data.Text as T import qualified Data.Text.IO as TIO It seems like you found that the [Data.Text.Internal](http://hackage.haskell.org/package/text-1.2.2.1/docs/Data-Text-Internal.html) module exports the `Text` data type. Conveniently, `Text` is re-exported by the [Data.Text](http://hackage.haskell.org/package/text-1.2.2.1/docs/Data-Text.html) module. That's where you should import it from. For what it's worth, I personally like writing my imports like this: import Data.Text (Text) import qualified Data.Text as Text import qualified Data.Text.IO as Text Since `Data.Text` and `Data.Text.IO` don't have any conflicting exports, this works.
Yes, that's the official word, and it's true enough, but in my experience, invariant violation is the bigger problem. Refactoring `x.y.z.t` to be Law of Demeter compliant is a mechanical exercise... _unless_ the chain was violating an invariant and that violation has been fundamentally written into the semantics of the resulting system, and now you've got a _real_ problem. It has been an ongoing source of astonishment to me how often this happens. It has taught me a much deeper appreciation for trying to get the external API right the first time and as minimal as possible, because _every_ last quirk you export is something that will become a critical feature of the system in the future. (I mostly work in OO-land. Though Haskell isn't necessarily immune to "exporting too much".)
Note that there is a lot of prior work on embedding linear languages (like the language Cogent, which Gabi mentioned, and I developed ;) inside Haskell, but all of them have a certain amount of ugly. This Haskell Symposium paper covers a lot of the work. http://functorial.com/Embedding-a-Full-Linear-Lambda-Calculus-in-Haskell/linearlam.pdf
It's usually done with the Compose key, i.e. just an extra key stroke before these '&lt;-', '-&gt;'. E.g. `caps-lock` `-` `&gt;`. Can be done in the browser too, e.g. ←, → .
It is great for research project. But if (part of) the community tries to push Haskell to mainstream, then more systematic way to improve language is necessary.
This is more of a reason to NOT use lenses, imo.
where is the github repo with the code that you used to create these benchmarks?
Yes! Let's introduce weak typing in Haskell!
Well, yes, but usually you would do something simple like `data FileClass = Elf32 | Elf64`. The author seems to be going to a lot of trouble in order to accomplish something.
I view those libraries as outliers. The more standard approach is either to use the word "maybe" spelled in full as part of the name, or not mention the "maybeness" in the name at all. For example, in base you have `maybeRead`, `listToMaybe`, `lookup`. There are no functions in base at all whose name includes "May".
You're right. Those slipped my mind. Matching base would be better.
From a technical perspective the diff is [fairly self-explanatory](https://github.com/purescript/purescript/wiki/Differences-from-Haskell#records). I doubt it could be integrated into to Haskell purely due to community resistance (not backwards-compatible, doesn't bear any relation to the investment in overloaded records in GHC 8+, overloads `.`). Something as innocuous as integrating Nikita's record package [proposed by Simon Marlow](https://mail.haskell.org/pipermail/ghc-devs/2015-January/008049.html), was drowned out in backwards-compatibility-is important-to-me and fizzled into nothing. SPJ's reasonable TRex-like proposal was bikesheded out of popular memory. The _Avoid success at all costs_ benefit is over. Our crappy records are here to stay, I expect. 
He has associated types (e.g: address type is Word32 for Elf32, and Word64 for Elf64). So the types differ based on the FileClass. You could possibly do it with a GADT, but a type-class is more conventional.
...which is why the ligatures should be double-width. If you have a character-cell display, just change the glyphs for `-` and `&gt;` if adjacent, forming a nice arrow. Triple width for `&gt;&gt;=`, etc. Proportional fonts in code are generally not a completely insane idea, but need flexible tab stops. LaTeX's Haskell mode can do that by detecting ASCII alignment.
While finger trees are mentioned in passing near the beginning of the paper with a claim that RRB-trees will be better, there is no significant comparison in the rest of the paper. The focus is on comparing RRB-trees with existing immutable vector implementations in Closure and Scala. So I am not convinced yet that we need this in Haskell.
As someone who is not on the Haskell.org Committee, and was not present at ICFP/Haskell Symposium when that issue was discussed, you are making a claim with an incomplete picture of the situation. I can't speak for Ed, but please stop telling me that I was not involved in conversations where I was present, and you were not. It is insulting and makes me seriously doubt your claims of respect for members of the committee. From Gershom's reply to your comment: &gt;i responded quickly to this ticket because we had just discussed this issue earlier today in a meeting in person of the full committee, so I knew the discussions we had just only conducted. Regarding oligarchies. When a community such as ours has failed to avoid success at all costs, there are inevitable tensions and disagreements between competing opinions and goals. Sometimes these become mutually incompatible and require leadership to resolve a deadlock. Community members acting in good faith recognize that any one person or group's perspective is necessarily limited, and that when such decisions don't go their way, the people involved in the decision are nonetheless attempting to serve the community's broad interests.
Which ICFP was this issue discussed at? Because the PR was opened July 18, 2015, and merged July 23, 2015 (again, after an extensive community discussion that was _opposed_ to the change). ICFP 2015 occurred August 30-September 5, and ICFP 2014 a year earlier. The entire proposal for a revamped Haskell Platform downloads page was [June 24](http://community.galois.com/pipermail/haskell-infrastructure/2015-June/000898.html). And to the point: I've checked with at least two other committee members, and both confirmed that they were not aware of this decision (specifically, pull request #122 being merged). So it's certainly true that I have an incomplete picture of the situation. The cause of that is that, as I've objected in the past, the haskell.org committee behaves secretly and does not properly report to the community what it's doing. But based on all evidence at my disposal, I cannot reconcile what you're saying here. __EDIT__ Gershom's quote of "I responded quickly" is referring to immediately closing pull request #130. It has nothing to do with the claim I'm making here of him unilaterally deciding to merge pull request #122. Is any committee member able to say that this was done with knowledge of the rest of the committee?
&gt; Or do you mean to ask which SQL library the benchmarks were using previously? Yes!
The PR Michael is discussing is an earlier one which had been thought to be entirely mundane and uncontroversial and related to the work discussed at http://community.galois.com/pipermail/haskell-infrastructure/2015-June/000898.html (a thread in which chris done participated)
I challenge anyone to [read the Reddit discussion](https://www.reddit.com/r/haskell/comments/3b1yuk/haskellinfrastructure_fwd_new_haskell_platform/) and come to the conclusion that this change was "entirely mundane." This is revisionist history, tempers flared up over the very fact that Chris posted the link to Reddit! There was a clear attempt to try and sneak the new design onto the site without the broader community noticing, and when they did, the change was merged anyway. Also, it's funny that you say that "Chris Done participated" in the thread, when his feedback was _opposed_ to the change.
Any specific reasons to write the whole thing in one file?
The difference in one happened is discussed at length in pull request #130. You're right, we're missing a shared understanding since you were not involved in the discussions I've referenced here. Chris's proposal looks a lot like _today's_ downloads page, which has all three options "above the fold." Pull request #122 that Gershom merged unilaterally (I'm glad we agree on that now) made the HP section take up the entire first screenful (at least), making the other options almost impossible for a new user to notice. __That__ was the objection, and it's one I clearly enunciate in pull request #130.
&gt; Pull request #122 that Gershom merged unilaterally (I'm glad we agree on that now) We absolutely do not agree on this, and to suggest so is such a dramatic and disrespectful misreading of what I've written in this thread that I am done with this conversation.
Are we hung up on a technicality here? What's the big deal if PR #122 was merged before everyone else on the committee became aware of that specific action, even though everyone on the committee would have been ok with it? Couldn't that PR-merge simply be reverted if it turned out that it was performed without consensus?
I tried pipes not because I needed it, but because it showed me a simple problem and elegant code to solve it, and playing around with it felt nice. Sure, its core was quite intimidating for a beginner back then, but there were corners I could make use of regardless of that. This package does not show me any problem it solves, there is no documentation worth being called so, it’s a single giant source file. It does not interest me (or make me interested) in the slightest, despite the fact that maybe I should one day learn more about ELF. **Really, documentation is *the* most important feature of a library** if you want people to use it. Invest a couple of hours into adding Haddock comments (miniscule compared to how long developing something like this takes), add a couple of catchwords that help me when reading other sources on ELF, and maybe it turns out it’s a marvellously designed library that does ELF better than anything else. Problem is, if there is no documentation, chances are nobody is going to find out about it.
Not for anything serious, but yes. Why do you ask?
And you ultimately made the decision to do so without the committee explicitly saying "go ahead." There was clear controversy in those threads, despite your claims of it being "uncontroversial." You're playing word games, and I hope people can see through them.
I do not believe the committee would have been OK with that action, and that's demonstrated by the fact that after the fact the website was changed away from the PR #122 decision. In other words, this isn't a technicality: I believe that Gershom made a decision that was contrary to community interest, and had the committee actually accepted input from the community and made a decision would not have made the decision they did.
Playing the "disrespect" card here is ridiculous. You clearly admitted that I was right in that the committee was not consulted on this: &gt; so I was not aware of this thread at the time. You're right that I was referencing a different conversation above. The only disrespect here is the fact that you called me out on a false claim, without paying enough attention to what I claimed to realize I was right. And then after I showed that there was no way the claims you were making were correct, you decided to get offended at me. You're demonstrating perfectly why a new website was needed: there's no way to work constructively with the haskell.org committee.
Just for the record: Lenses allow you to use the function-composition dot for this purpose.
I'm fairly sure the Spock benchmark was ported to Hasql in response to [this suggestion](https://www.reddit.com/r/haskell/comments/48wtc9/spock_framework_submission_to_the_techempower_web/d0nju4j), I remember browsing the Hasql-related commits on github. It looks like /u/Rydgel reverted back to postgresql-simple before submitting to TE. Presumably because he didn't see any significant improvement?
I didn't provide any reasoning, but I can if you care. Although I'm biased, I find that particular example incredibly difficult to read. For work in ghci, do whatever's more convenient. For code that others may need to maintain, I'm in the camp that given two isofunctional (?) pieces of code, the best one is the one that is the most likely to be comprehended by the rest of the team.
That sounds nice in theory, but in terms of what mechanisms are available right now, the `eName` approach is the most effective/pragmatic solution. Personally, I would like to see nested modules. Something like: module Lib (Event, Event.Event) where data EventKind submodule Event (Event (..)) where data Event = Event { name :: Text , kind :: EventKind } - module Main where import qualified Lib main = print (Lib.Event { name = "asdf", kind = undefined }) Nested modules are super nice in other languages like SML and Racket.
This is what I use everywhere now. It's only a little bit ugly, avoids collisions, and gives a little bit of context in all use-sites. I was sold on this after using the `gogol-*` libraries, which use this convention in their apis.
How was it?
It was pretty good. It was some time ago now, but I remember liking it as an approach to the problem. I was using it to manage memory that was outside of the GC'd heap to reduce GC pressure. We didn't end up using it, it was just a spike, but I liked the library.
Doesn't drop out for me, but I haven't got it to ping properly yet (probably because it's firewalled). It looks like you need to run the client on a separate host. If I run both on the same system, I get Client: bind: resource busy (Address already in use) I'd expect to need to start the server first, FWIW. MacOS 10.11.5, GHC 7.10.3 
Thanks, I'll give this a look
Same here, with OS X 10.10.3 and ghc 7.10.2. Inspecting `addrinfos` on both sides, I noticed that the `AddrInfo`s were not listed in the same order on the client and on the server: on the server the `AF_INET6` entries were listed first, while on the client they were listed last. So I guess you were on the right track trying to disable your IPv6 configuration, but commenting it out in `/etc/hosts` no longer seems to sufficient to disable it, as I still get `AF_INET6` entries after I comment it out. Anyway, so the problem is that both code samples blindly take the first element of the list returned by `getAddrInfo`, hoping that the same one will be picked on both sides, but like I said the lists are in different orders so one side ends up listening on IPv4 while the other tries to connect on IPv6, so the error message is correct in saying that nobody is listening on the target IPV6 address. I found two workarounds. One is simply to specify the target IP explicitly using addrinfos &lt;- getAddrInfo Nothing (Just "127.0.0.1") (Just "3000") instead of addrinfos &lt;- getAddrInfo Nothing (Just "") (Just "3000") This way the IPv4 format will be recognized and the client will contact the server on IPv4 where it is listening. This solution is quite fragile though, as it relies on the fact that the server's `getAddrInfo` lists the IPV4 address first. A better approach is to keep the `Just ""` and to instead filter out the results on both sides to only keep the IPv4 results, by using let serveraddr = head (filter (\info -&gt; addrFamily info == AF_INET) addrinfos) instead of let serveraddr = head addrinfos Or you can make them both use IPv6 using `AF_INET6`, that works too.
I think the big advantage over finger trees would involve merging and splitting. Both should have a fairly good story for indexing.
There's also a shorter [slides pdf](https://www.azul.com/files/HowNotToMeasureLatency_LLSummit_NYC_12Nov2013.pdf) and the description in the [wrk2 readme](https://github.com/giltene/wrk2). Basically what these latency measurements do is to send a request, wait for the server to respond and then fire the next request. The latency measured is the time between firing the request and the response. Now what happens in these tests is that if a server becomes slow or unresponsive for some reason(GC), the client waits for the server to get over it and respond before it sends another request. This leads to a *single* large outlier value, while all other values are small. In the real world, what would happen is that a bunch of requests would queue up in this time period, and all of those would experience high latency as well. When the input throughput is low, it doesn't matter much, the server is able to get over any hiccups soon, and actual average latency would be low. But when the input throughput is close to the the number of requests a server can handle, slight hiccups lead to increases in actual latency for a whole queue of requests. When the input throughput surpasses what your server can handle, you would see very high actual latencies for all requests. I saw it jump from a few ms to several seconds in a range of 2000 req/s. So, its meaningless to compare techempower's latency numbers for servers that have different requests handling capacity. If undertow can handle 500k req/s and warp can handle 200k req/s, then warp would see actual multi-second latencies at 201k incoming req/s while undertow would see actual multi-second latencies only at 501k incoming req/s. warp's actual latencies at 501k incoming req/s would probably be in the order of several minutes. EDIT: While techempower seems to have begun using wrk2, from how they present the results, I doubt they understood Gil Tene's point.
Unlike many other languages, Haskell let's you specify arbitrary operators and has syntax for using operators as prefix functions and functions as infix operators. If you think about it, there's really nothing terribly different about operators and functions other that one defaults to prefix and the other to infix notation. The list of operators you linked seems to be about build in syntax rather than common library defined operators. Just use Hoogle if you want to know what an operator (or function, there's not much difference) does: http://hoogle.haskell.org/?hoogle=(%3C%2F%2F%3E)&amp;scope=set%3Astackage&amp;=
If you only have a bytestring then I'd say no. Aside from maybe some statistical analysis
If you're confused about the meaning of an operator, the best way is to use GHCi's info command. Like, say there was a weird operator that looked like `#$%$#`. If you wanted to know about it, you'd open GHCi and import the right module (or, if it's used in a file and you don't know which of the imports it comes from, you'd run GHCi on the file like `ghci SomeFile.hs`) and type `:info (#$%$#)`, or `:i (#$%$#)` for short.
Sometimes, infix operators make things more readable. A lot of the time, they don't. Even when they're nice, they can make it harder to learn some library. It really helps to use [hoogle](http://hoogle.haskell.org/?hoogle=%3C%2F%2F%3E) to search for them, since it can search for symbol characters. Some libraries use *a lot* of infix operators, like `lens`, but you'll mostly be seeing the same set of operators in ordinary Haskell.
As lens is commonly used as an example on how to introduce many confusing operators: Edward himself wrote a fieldmap: https://github.com/ekmett/lens/wiki/Operators they are basically just combinations of the things you want to do (with 1 char ~ 1 thing). Hope that won't confuse you even more :)
I'm hoping that this is a role that the GHC Committee members will to some extent take on. While I'm not sure we have the human-power to guarantee a responsive shepherd for each proposal that comes along, I think that with a well-staffed committee we could get most of the benefit.
Thanks for everyone's comments so far. To be clear, we are looking for feedback on this proposal; if you have thoughts you'd like to share please do leave them on the pull request!
+1 for hoogle. When reading articles, you often don't know what module something comes from. Hoogling operators helps a lot.
Wow! This community is pretty awesome. That's a lot of helpful feedback! Thanks guys, I really really appreciate it! :) :)
How about this: https://realworldocaml.org/v1/en/html/records.html I always assumed that everyone already knows about the usual implementation, but has been holding out for extensible records and functions polymorphic on different kinds of records, and they don't want the plain ones all the other languages have.
I tested your code, but more confusion: Repa's `traverse` +RTS -N1 =&gt; 1.273s +RTS -N2 =&gt; 1.912 +RTS -N4 =&gt; 1.505 +RTS -N8 =&gt; 0.813 `unsafeTraverse` +RTS -N1 =&gt; 3.014 +RTS -N2 =&gt; 2.89 +RTS -N4 =&gt; 2.217 +RTS -N8 =&gt; 1.196 `traverse` shows the same issue as my code, `unsafeTraverse` goes fast proportional to the # of cores. But `traverse` is just faster than `unsafeTraverse`. My compile option was: -O2 -rtsopts -threaded -fno-warn-tabs
As a newbie I was overwhelmed by immense pool of mysterious infix operators which seem 'commonly used' by haskellers. But as I write some Haskell code I found that I don't need to code with all that operators. They let you write mathematically beautiful or concise code, but a little more verbose code can do the same thing. Anyway I think I have to get used to them at some time to understand the codes all experienced haskellers wrote :)
Is there a way to detect non unicode characters?
Oh, thanks! I hadn't noticed that before. I'll try record in that case and tell you how it goes
Yep, there's a serious overuse of infix operators in a lot of Haskell code. The worst thing about infix operators from obscure third-party libraries is that they often make code just totally incomprehensible without having to search hoogle or load up ghci. That said, I think Spock's usage is pretty clear and can be inferred (provided you've got some experience with the language -- probably more than the OP has).
Thanks for letting me know!
For those interested, I also have modules to read ELF files in my ViperVM package: https://github.com/hsyl20/ViperVM/blob/master/src/lib/ViperVM/Format/Elf.hs https://github.com/hsyl20/ViperVM/tree/master/src/lib/ViperVM/Format/Elf There is also an app to explore ELF files (e.g., ./ElfWeb /usr/bin/ls) in the browser: https://github.com/hsyl20/ViperVM/blob/master/src/apps/Elf/Main.hs (It shows disassembled code sections for X86-64 arch too). Most of the infrastructure to write ELF data structures is available too but it has not been really tested and a simpler higher level interface is still lacking.
What *did* you end up using, I wonder?
You can model a bit of it with typeclasses, but you have to implement the instances by hand: https://github.com/ekmett/roles/blob/master/src/Data/Roles.hs#L41 Alas, not everything can be handled this way! Consider `StateT s m a`. The role of s depends on the role of m, but the s comes first in the argument list!
There weren't any Servant benchmarks for TechEmpower before, so no library for database access either. :)
I didn't find one that would work for me a few days ago, and wrote a C# program instead ...
Besides the obvious -O0 speedup, GHC also has -fno-code which causes it to, well, not generate actual code. Since optimisation and code generation are the slowest part of compilation you can combine those two to get really fast type-checking if you just wanna find type errors during development. 
[Stackage](https://www.stackage.org/) has a more updated hoogle, with more and updated libraries from my experience.
&gt; GHC also has -fno-code which causes it to, well, not generate actual code The caveat being that it doesn't work with template-haskell, which almost every non-trivial project will contain. 
No news yet about that possible post?
&gt; In general, the interactive development and proof related parts of Agda are more mature than generating executables, so I would recommend to go through one of the conventional Agda tutorials first. I'm going through [Verified Functional Programming in Agda](https://svn.divms.uiowa.edu/repos/clc/projects/agda/book/book.pdf) at the moment (awesome book) so I can see how Agda works and get to know Dependent Types better. &gt; By the way, there is also an Agda subreddit: https://www.reddit.com/r/agda Thank you for make me noticing, I didn't bother to check... &gt; FFI / Postulates [...] I came to the same conclusion about Haskell FFI calls treated as black boxed. Making use of ST Arrays inside Agda would be nonsense, I would be forced to use Haskell nonetheless to manipulate the array every time so better to use Haskell from the start. Actually I'm already making that game in Haskell, I was looking for a way to "incorporate" Agda in the process to include the learning of this language as well. Guess this is where "Verified Functional Programming" come in.
&gt; template-haskell, which almost every non-trivial project will contain The non-triviality bar is set pretty high it seems ..
Really great example for why we are putting up with all the comic-book swearing! It's sometimes hard to explain to outsiders why it's worth it.
That's a shame. But I guess you could set up a system to run without -fno-code only when you are idle, ie thinking and not currently typing. Delayed incomplete pattern warnings are not the end of the world, compared to the lower latency for type checks.
I'd like to take this chance to exhort the writers of tutorials to use explicit imports. When readers are at their most vulnerable we should go out of our way to make code as explicable as possible.
"Equivalent" perhaps. As I get it, informally, two code pieces are equivalent when they do basically the same thing.
It's easy to produce docx using pandoc: use Text.Pandoc.Builder (in pandoc-types) to create your document and writeDocx to transform it into a docx. You can specify a reference.docx if you want to adjust the default styles of the elements pandoc produces. Images are supported, as are tables (as long as they're fairly simple, no rowspans or colspans or fine-grained control over borders): see the Pandoc structure in Text.Pandoc.Definition (in pandoc-types) for an exhaustive list. For manipulating docx using pandoc, you'd have to use readDocx to convert to a Pandoc structure, transform that, and then writeDocx to convert back to docx. So, structural transformations should work fine, but, for example, special styles that are used for document elements will be lost. If you're generating the docx yourself and then manipulating it, things should be okay because you can use a reference.docx to change styles of the elements pandoc produces. Jesse Rosenthal, who wrote the docx reader for pandoc, expressed an interest a while back in factoring out some of the docx specific stuff into a separate docx manipulation library which could have wider scope than pandoc, so you might get in touch with him.
There already is a notion of an empty MVar. If you want to check for emptiness without blocking, there is [tryTakeMVar](https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Concurrent-MVar.html#v:tryTakeMVar).
[Here are the notes](http://gelisam.blogspot.ca/2014/12/how-to-package-up-binaries-for.html) I took last time I had to package up Haskell binaries on multiple platforms.
What's the difference between haskell-mode and intero? Are these competing projects? or complimentary? 
&gt; can I just get Travis to export the binaries it’s producing? Travis can "deploy" binaries to Github releases (https://docs.travis-ci.com/user/deployment/releases). Normally you'll add `on: tags: true` to the `deploy` section of `.travis.yml` and then whenever you want a binary on your project page you just push a tagged commit. Alternatively (or in addition to this) you can also use native Stack integration with Docker and push Docker images to the Docker hub (there's no out of the box functionality for this in Travis but the script is easy to write).
I have a package called [Octane](https://github.com/tfausak/octane) with binary releases. I build binaries for OS X, Linux (64-bit Ubuntu), and Windows (64-bit). I don't do anything special with Stack. I use Travis CI to build OS X and Linux binaries. I use AppVeyor to build Windows binaries. I attach the binaries to GitHub releases with a small package I built called [github-release](https://github.com/tfausak/github-release). You should be able to get what you need from either of those project's `.travis.yml` and `appveyor.yml`. I wrote [a short blog post](http://taylor.fausak.me/2016/05/09/add-files-to-github-releases/) explaining how to use my github-release package. 
We switched to front end development. React, Ghcjs, Purescript, Elm, chose your pick. 
Yes, and I'd like to thank /u/chrisdoner (and other intero devs) for being super-responsive on github. Also, how is intero-devel-reload faster than Yesod devel, technically? PS: it was me who raised similar issues on the Intero github repo, which lead to these issues being created. just posted here in-case someone knew of workarounds. 
Hah, thanks. I should avoid making up words.
The async library looks really cool, but I don't know how it would help me here. Is there a way to query things from async like i said? And since I will have a UI involved I would like to be able to poll stuff from the async tasks. I think since I'm not really "waiting" for tasks to end, but monitoring them over time, async works at too high a level to do what I want.
The async part is nice for running the download threads. The threads themselves can regularly update an MVar that you can query anytime from any other thread.
What is the relationship (if there's one) between the "frontend syntax" (HsSyn) and Template Haskell?
You can `poll` on the `Async a`.
You should also check out: [ghc-mod](https://hackage.haskell.org/package/ghc-mod) - quick type checker [hdevtools](https://hackage.haskell.org/package/hdevtools) - ditto [steeloverseer](https://github.com/schell/steeloverseer) - automatic recompilation and more [halive](https://github.com/lukexi/halive) - live recompiler
Oh I didn't know about poll. Thanks! 
You can put the state in TVars, and the commands into a TChan. Though if the state vars have only one writer you could also use atomicModifyIORef for those, and if you don't need to do any coordination with the chan and can always block on read, then a plain Concurrent.Chan will do.
My first pass would be to take the first 4 bytes and see if they are a BOM in UTF-32 (and if so, what the byte ordering are used for the 32-bit units), a BOM in UTF-16 plus 2 bytes (and if so, whether it is LE or BE), or a BOM in UTF-8 plus 1 byte. If you have other encodings that support the Unicode BOM, I'd test them, too. Then, I'd also see if those four bytes match some magic value that might indicate an encoding or a way to find one by navigating bytes. For example, "&lt;?xm", "&lt;?Xm", "&lt;?xM", or "&lt;?XM" would have me use UTF-8 enough to read the encoding in the XML declaration. Another example, "&lt;!DO" leads me to think it is SGML, probably HTML, and they can have embedded encodings that I can usually find. After those heuristics, I'd go to pure stats. Generate a histogram of bytes in the first 4k (or less) and compare it against known histograms of various encodings. This is fairly accurate, but IE used to use a similar approach and it can definitely go wrong.
 type String = [Char]
/r/haskell
I see, okay. So generally Data.Text, then?
The [libclang](https://github.com/chetant/LibClang/) project does a really good job of bundling. They include Clang and LLVM as submodules and [combine](https://github.com/chetant/LibClang/blob/master/Setup.hs) them at link time using [an AR script](https://github.com/chetant/LibClang/blob/master/Setup.hs#L221) so the user doesn't have to do anything except make LibClang a dependency in Cabal/Stack.
We can do a direct translation of for loops into recursion. I'll just do `isPrime` for example C++: bool isPrime (long n) { for (long i = 2; i &lt;= (sqrt(n)); i++) { if ((n%i) == 0) { return false; } } return true; } So, first we need a function definition: isPrime :: Integer -&gt; Bool isPrime n = ... Integer has taken the place of long and both it and bool have been bumped up a line, sort of like a pre-declaration. Note that this line is optional, but generally good form. I'll omit it from here on for brevity. Next a for loop... isPrime n = forLoop 2 (isqrt n) where ... `where` lets us write convenient local values and functions to break up the problem. Haskell doesn't have a built in Integer square root, so let's define that isqrt n = floor (sqrt (fromIntegral n)) This can also be written as: isqrt = floor . sqrt . fromIntegral Haskell also doesn't have a direct for loop analogue that's precisely suited for this situation, but it's easy to make... forLoop i n | mod n i == 0 = False -- exit the loop with failure | i &lt; n = forLoop (i+1) n -- next iteration | otherwise = True -- we have finished, without finding a factor So now we have: isPrime :: Integer -&gt; Bool isPrime n = forLoop 2 (isqrt n) where isqrt = floor . sqrt . fromIntegral forLoop i n | mod n i == 0 = False | i &lt; n = forLoop (i+1) n | otherwise = True Of course, `isqrt` isn't really specific to primes, so we should move it out, and we should refactor `forLoop` since you have another one of them coming up. forLoop i n | mod n i == 0 = False | i &lt; n = forLoop (i+1) n | otherwise = True ... in general you'll want to be able to *do things* at each iteration so ... forLoop build i n | mod n i == 0 = False | i &lt; n = build i (forLoop (i+1) n) | otherwise = True ... build can now take care of the condition checking and returning of values, since it is given the current state and the result of the next iteration ... forLoop build i n = build i (forLoop build (i+1) n) ... you'll want to be able to do more than just (+1) with each iteration, and we're not using 'n' for anything anymore (we can get back the (+1) functionality by writing `(+1)`) ... forLoop build i iter = build i (forLoop build (iter i)) ... and a little more renaming/tweaking to make it more efficient and a bit more Haskelly... loop state iter build = go state where go s = build s (go (iter s) So now our code looks like so: isqrt = floor . sqrt . fromIntegral loop state iter build = go state where go s = build s (go (iter s)) isPrime n = loop 2 (+1) (\i next -&gt; if i &gt; isqrt n then True else if mod n i == 0 then False else next Where `next` tells it to run the next step of the loop, but `next` is *also a value*. Since it's a value, we can re-write that if as a Boolean expression... isqrt = floor . sqrt . fromIntegral loop state iter build = go state where go s = build s (go (iter s)) isPrime n = loop 2 (+1) (\i next -&gt; i &gt; isqrt n || (mod n i /= 0 &amp;&amp; next)) That's translating. This can also be done in a more traditonal Haskelly way... isqrt = floor . sqrt . fromIntegral isPrime n = null (filter ((==0) . mod n) [2..isqrt n]) This creates a list of potential factors from 2 through `sqrt n`, and filters out the ones that divide n. If the list is empty, there are no divisors, and so it is prime. With full optimizations turned on, these should compile to more or less the same thing. If you get all that but still need help with the next function, just ask. If you don't get all that, again, just ask.
Laziness is one of my favorites! Infinite data structures are awesome! EDIT: Also, friends, I'd consider this an opinion thread, so there's very little reason to downvote dissenting opinions like this.
&gt; Which brings me to my fourth problem, Intero doesn't have a 'stack build' command You're using spacemacs, right? I type space-p-c to run "stack build" from spacemacs. This uses the "projectile" module which comes installed by default with spacemacs.
Although, a workaround is to only put your template haskell into certain modules.
It's the other way around. Once you're lazy you need to be pure or side effects will have a surprising ordering.
I am aware, but every time this is discussed people are reminded that the current implementation is extremely convenient particularly for beginners or in places where the text implementation is of little importance. Because "string" is ubiquitous across programming languages I agree that Haskell should nudge users in the right direction (e.g. using the most appropriate implementation). But "a sequence of chars" is useful and memorable enough when you're just plunking around that having `Char` and `Chars` sorta follows naming convention (e.g. `foo (x:xs) = ...`) and is memorable enough to me to stand on its own in tutorials and elsewhere. Of course, the real issue is changing anything in `base`. Sorry I have no suggestion for easing that pain. :)
Surely it's very convenient, but don't forget about the drawbacks: - Need to go great lengths to get basic debugging tools like stack traces. - Can't time/benchmark functions reliably. - Lazy by default means redundant thunking, thunk entries, allocations etc. - Being able to hide bottom values behind thunks is IMO is a bad practice that is unfortunately done a lot.
[citation needed]
There's no reason that beginners should find `type String = [Char]` more intuitive. In fact, it's a little obscure; most languages treat text as an opaque data type because it represents a complex encoding (of course there are outliers like C, but even Java makes `String` very opaque). For the most part, people aren't using generic List functionality on Strings. Most of what you need to do comes down to more complicated algorithms that perform poorly on `[Char]`. I suppose list traversal can occasionally be useful on Strings, but this isn't something that `Text` can't do. I just don't see any reasonable convenience provided by `[Char]` that can't be trivially provided by `Text`.
That's more because the two go hand in hand; not because purity begets laziness. You can easily have purity without laziness. It's much harder to have laziness without purity.
Idris is pure but strict. Purity doesn't necessitate or make obvious laziness. Laziness goes quite well with purity, but purity doesn't care much whether the language is lazy or strict.
&gt; the current implementation is extremely convenient particularly for beginners or in places where the text implementation is of little importance This is the statement that sparked my comment. I don't believe that `[Char]` adds any reasonable convenience.
Unfortunately i cannot find anything. But you can read the history of developing lazy haskell here: http://www.scs.stanford.edu/~dbg/readings/haskell-history.pdf 
&gt; In September of 1987 a meeting was held at the conference on Functional Programming Languages and Computer Architecture (FPCA '87) in Portland, Oregon, to discuss an unfortunate situation in the functional programming community: there had come into being more than a dozen non-strict, purely functional programming languages, all similar in expressive power and semantic underpinnings. There was a strong consensus at this meeting that more widespread use of this class of functional languages was being hampered by the lack of a common language. It was decided that a committee should be formed to design such a language, providing faster communication of new ideas, a stable foundation for real applications development, and a vehicle through which others would be encouraged to use functional languages. This document describes the result of that committee's efforts: a purely functional programming language called Haskell , named after the logician Haskell B. Curry whose work provides the logical basis for much of ours. [SPJ from the preface to Haskell 98 report](https://www.haskell.org/onlinereport/preface-jfp.html). As far as I understand this, non-strictness came first.
Purity is a requirement for non-strict evaluation, unless you want to worry about potential side effects lurking behind every seemingly innocent expression. Non-strictness contributes greatly to composibility, which is why most pure languages choose non-strict evaluation by default.
I hadn't seen that blog post before; it's really good! Thanks.
hashtache looks promising, and it has the advantage of being used outside Haskell, so there are additional resources people can be pointed to. It does a little better than Heist for control flow, but still not great; I'll have to see how it works out in practice.
That looks like a really good solution, I'll see how much I can rip out of Hakyll and use in my own code.
Do you really have to have 4, especially for non haskeller ?
Not just _changing_ things in `base`, but `Data.Text` isn't even a part of `base`, so it requires an understanding of how to install a dependency before you can even use it! Obviously beginners are going to shy away from that; it's way too much overhead.