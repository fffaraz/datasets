Yes. I work there. One of our core systems is done in Haskell, as is a simulation system developed for a client. Haskell use is increasing. 
You can run a visual interface from the keyboard too, people who use photoshop or engineering CAD programs professionally get very adept at it.
I'll see if I can find out for you tomorrow when we're back at work (and hopefully have resolved the degree issue).
"Zemlya" is Russian for "Earth", so I found the fact that a geological metaphor was used, as opposed to e.g. a culinary one (molasses) or a zoological one (snail's pace), interesting.
I have added a [real-world example to the proposal](https://github.com/ghc-proposals/ghc-proposals/blob/context-fixes/proposals/0000-context-fixes.rst#large-real-world-example). I think it shows that readability can be greatly increased! Let me argue this way. If you have a “real” constant, of course you define progName = "coolProgram' once somewhere, and refer to it. You would not pass it around as arguments because it is more readable. So why should it be more readable this way, just because it is a “constant” only from the point of view of one module?
&gt; while-loop condition that reads a mutable variable Sounds like a bad idea. The compiler actually won't let you write Fortran in Haskell. 
This one is very relevant to Haskell: &gt; HOPS is a graphically interactive program development and program transformation system based on term graphs. http://www.cas.mcmaster.ca/~kahl/HOPS/
that's not true either, though
&gt; I'm not talking about drag-n-drop UI creation why not? We would see then that even there "are" tools to create UI by drag-n-drop real projects end up with manually editing the data files with UI definition. Despite of them being [not really human readable](http://service-architecture.blogspot.fi/2006/11/xml-is-not-human-readable.html), it still happens to be more appropriate than dragging and dropping UI controls and editing their properties though context menu. Maybe it worth thinking why that happens.
It's for building up metadata "on the side". For instance, as I parse a list of catalog items into memory, I could build up a `Set` of all colors the clothes come in in my writer as I'm parsing.
Writer is not suitable for logging, especially when used in conjunction with IO. Writer necessarily accumulates thunks when used with a strict underlying Monad instance like IO. Writer is suitable for lazily streaming outputs, however. I recently did a merged implementation of the Advent of Code problems 12, 23, and 25. In problem 25 you're asked to determine which input value causes a program to loop while outputting an alternating sequence of '0' and '1'. In my implementation I can use a Writer to produce a lazy stream of outputs for a particular program. The program will only evaluate as much as is needed to produce the output. I also am able to replace that Writer layer with a different layer that validates the outputs, allowing me to search for the correct starting value. These are the kinds of situations where writer is appropriate. Typically while using a Writer, the whole point of the computation will be the value being accumulated by the Writer, rather than it being an informational record of what's happening. Here's the bit of code that's able to lazily generate the output sequence thanks to a Writer implementation https://github.com/glguy/advent2016/blob/master/asmprog-final/Main.hs#L58-L60 Here's the newtype'd WriterT layer that allows me to switch my interpreter to generate that lazy list https://github.com/glguy/advent2016/blob/master/asmprog-final/Main.hs#L240-L241
But we are talking about why Haskell *isn't* popular right?
Correct. The stack shipped with the platform is just plain old stack. It'll act just like stack does, defaulting to that directory and all. The platform ships the stack binary, but the behavior of that binary is no different than if one had gotten it from any other means.
So you're saying when I download and install Stack via the Haskell Platform which is basically a GHC distribution, Stack will start download and install its own GHC distribution because it doesn't know there's already one installed?
Kaspersky Lab, Moscow, Russia https://www.kaspersky.com We (OS team http://www.theregister.co.uk/2016/08/23/kasperskyos/) are hiring, but not very actively. I. e. you won't find Haskell vacancies on our website, but you can send resume to me.
I noticed that HP has stopped providing the [exceptions](https://hackage.haskell.org/package/exceptions) library as of GHC 8. What is the reason?
I heard about https://typeable.io (Moscow, Russia) but afaik they aren't hiring at the moment.
For example the compiler puts all the state into one global IORef during compilation - module names, symbols, etc. It's not very Haskelly. Or DynFlags.. that's one big data type which gets passed everywhere.
See the [Hazelgrove](http://hazelgrove.org/) project for instance.
I know someone that is always looking for a sexier kitchen appliance and I'm sure he'd take "aerodynamic toaster" as an excuse to spend a silly amount of money. 
in reverse engineering it is sometimes useful to use graphs: https://www.hex-rays.com/products/ida/pix/5_plain_graph_view.gif 
These may be architectural flaws but I don't see how they'd hinder Haskell adoption – they don't impact the usability of the compiler, do they?
There is also `Given` in the `reflection` package. Just now GHC is discussing how to make the underlying functionality safer by adding a `reify#` primitive.
For me it is only while writing real programs. Same happened with design patterns in OOP, I didn't really get them until I reinvented them myself or saw them used in real world situations. It's much easier to understand a technique/design/whatever when you run into the problem it tries to solve and see how it solves it after attempting to solve it yourself
Yeah, I think so. But I am not sure that one is the same as internal.
The article needs a re-read, there's lots of typos. And that line should probably be deleted for the thing to work.
I don't know about OCaml, but in Agda you can do this: module MyModule (progName : String) where bar : Int -&gt; Either String Int bar 0 = Left (progName ++ ": zero no good") bar n = Right (n + 1) foo : Maybe Int -&gt; Either String Int foo nothing = Left (progName ++ ": no number given") foo (just i) = bar i Then to use it: open MyModule "My Program" doStuff = foo (just 10) or you can even open it in a very local scope like this: doStuff = foo (just 10) where open MyModule "My Program" 
Why does `addSession` return a `Maybe Session`, when it never produces Nothing? One of lines 194 and 196 should not directly refer to `a`. Should line 216 be `Nothing -&gt; d &gt;&gt; status forbidden403`? If so, you can have: authCheck d a = authCheckWithSession d (const a) authCheckWithSession d a = do vaultContents &lt;- liftIO readVault maybe (d &gt;&gt; status forbidden403) return &lt;=&lt; runMaybeT $ do c &lt;- lift (SC.getCookie "SessionId") &gt;&gt;= liftMaybe session &lt;- liftMaybe $ H.lookup (T.fromStrict c) vaultContents curTime &lt;- liftIO getCurrentTime guard $ diffUTCTime (sessionExpiration session) curTime &gt; 0 -- this shouldn't abort, browser should delete it lift $ a session
You could use something like [monad-loops](https://hackage.haskell.org/package/monad-loops-0.4.3/docs/Control-Monad-Loops.html#v:whileM) and then write the body in the list monad?
Is there a way to change this default? Whether or not HP will do that is a different question, but in our environment I definitely don't want stack ignoring our standard GHC and going out to download its own. We need all of our developers to use a consistent GHC build that we choose ourselves and control, not something that stack randomly replaces on various people's machines.
I've been slowly learning Haskell for ten years, coming for a completely imperative background. I can't really identify aha moments, it's been a more or less continuous process, really. I've made progress through a combination of reading books, trying to make real programs, and reading code. The whole thing has improved my coding skills in all languages a lot. I realize this might not be extremely helpful, sorry :-p
Understanding CPS (continuation-passing style) was the last real aha for me. Now it seems obvious that the `a` in `f :: (a -&gt; b) -&gt; b` is the type of `x` in `f (\x -&gt; ...)`. PS: typing code on mobile is not a very pleasant experience. 
&gt; Functional languages are pure No, only pure languages are pure, but then the sentence becomes a tautology :) In my mind, "functional" means we're using map and folds to construct new immutable values instead of using loops to mutate existing mutable objects. If we do so in a normally-imperative language, that program is said to be written "in a functional style". I also put combinator libraries in this style. Similarly, to me, "pure" means that side-effects are tracked. The default, with zero effects, is for "purely-mathematical" code, whose output only depends on their input, and which don't cause other code to behave differently later on. These are the functions for which referential transparency applies, so we can do equational reasoning with them. I guess it would be possible to have a pure imperative language, in which simple arithmetic expressions would be distinguished from the statements surrounding them. But if we bother to distinguish the pure fragment, it makes a lot of sense to make that fragment more expressive than just arithmetic, and the functional style is a really good fit for that. I think of "purely-functional" as a confusingly-worded shorthand for "purely-mathematical and functional".
&gt; People are smart enough to figure out "Oh, building up a huge list and then printing is totally different from printing as I go." Except that's not how we sell Haskell. New Haskellers might have the intelligence to know the difference, but they might still hope that Haskell, with its smart compiler and laziness will emit identical code.
There is also [Unison](http://unisonweb.org/2015-05-07/about.html#post-start) being developed
Lazy evaluation blown my mind: fibs = 1 : 1 : zipWith (+) fibs (tail fibs) And from SICP, you can define all kind of structures only with list and you can define list only using lambdas like: list a b = \m -&gt; m a b head xs = xs (\a b -&gt; a) tail xs = xs (\a b -&gt; b) ex01 = head.tail $ (list 1 (list 2 3)) 
u/fegu, who works there, has said they still use Haskell the sibling comment. I see no reason not to trust that.
But a fitness function does not deliver what humans want. It is not the same as our current model of development where we interview users and try to capture their requirements. Take AlphaGo for example. Unlike chess programs it does not have fleshed out algorithm and strategy coded by humans. It learns on provided input and then makes its own decisions. And while the end result is **generally** what its creators wanted, the details are not in creators control. In other words it is "win me a game", rather than "here's how you should play to win a game". 
[removed]
Encapsulation of objects is only useful because it's so hard to be disciplined about how your code affects things. With purity and encapsulation of state changes, this isn't needed anymore! Types, data and functions can exist separately. All these restrictions make it easier to go wild with your code! Refactoring, generalizing... using old code in different ways for new projects is a breeze. 
My last addition: Trigram, Norway (trigram.no) using Haskell within natural language processing.
Haskell has static typing, a less noisy syntax (subjectively), and there's nothing that lisp has that I am super envious of.
&gt;Surely the future of programming looks different? And if there's some future that looks different I think programming is moving so fast that features are going to be the focus for the near future. Right now I have a bunch of tools that work (well) to program in haskell by typing text, and it's a massive undertaking to replace that. Hell, we don't even have a git GUI that won't crash. Things like Feynman diagrams and dessins d'enfant had to be invented by Feynman and Groethendieck, respectively, so I don't think similar abstractions for programming will be easy to find. 
Well, technically () is all mapM_ ever returns, might you somehow not be running mapM_? Does "mapM_ putStrLn $ snd $ runWriter (tell ["line 1"] &gt;&gt; tell ["line 2"])" work?
Henry Ford once said “If I had asked people what they wanted, they would have said faster horses.” Do you ask paying customers questions about inner workings of the system you build? Or do you make those decisions understanding that's what they are paying you for? As we progress forward we make more and more decisions on behalf of paying customers. And the ultimate goal is to make ALL decisions, just like we make when we build a farm for cows. 
Can you explain what you mean?
Comes from the paper Theorems for Free. Not super intuitive though. I read that paper in my first year into Haskell, and missed the point entirely at the time. After reading the paper, you should be able to tell why fmap is easier to reason about when compared to map, and hence, in general, why we should prefer more polymorphic types. We not only get theorems for free, but it also constrains the number of possible implementations.
Structured recursion schemes can generalize all types of recurrences.
Makes sense. I've changed that section accordingly. &gt; In my mind, "functional" means we're using map and folds to construct new immutable values instead of using loops to mutate existing mutable objects. I was trying to get at the fact that [purity a feature of functional languages](https://wiki.haskell.org/Functional_programming#What_is_functional_programming.3F). Also, I've always interpreted pure to mean side-effect free - in the way you say purely mathematical. The tracking of side-effects seems to still be part of the impure section of Haskell even though it is tracked. &gt; I guess it would be possible to have a pure imperative language, in which simple arithmetic expressions would be distinguished from the statements surrounding them. Wouldn't you then say that it is possible to write pure functional code in an imperative language rather than that it is possible to have a pure imperative language?
I doubt such attempts are ever going to get anywhere, because of the sheer *amount* of how much you need to specify when programming. Seriously, the difference between math proofs and programming is not that math proofs use diagrams. The diagrams tend to be *theorems* or parts thereof that have things proven about them, in 'chickenscratch'... except it's even more chickenscratchy because a mathematician rarely if ever hesitates to use 1 symbol function names and as many 1 symbol operators as they need to invent.
I’m (almost done) reading _Learn You a Haskell for Great Good_ and for me it clicked with`fmap`. The revelation that `List a`, `Maybe a`, `Either String a` and C#'s `Task&lt;a&gt;` all share something that can be abstracted opened my mind to a new level of thinking about types and operations.
In wild dreams and speculations it can be, there is no practical prototype doing even the simplest general intelligence and it isn't for lack of trying.
What about [TNG Technology Consulting](https://www.tngtech.com/) in Munich? They hosted MunicHac but do they actually use Haskell?
Google already created an AI that learned and mastered not one [but many different games](http://www.theverge.com/2015/2/25/8108399/google-ai-deepmind-video-games). I think the chances of us getting AGI sooner are higher than the chances we will see human programming shift away from text based input (the subject of this thread). 
For me it was understanding that I can replace the left hand side of an equal sign with what is on the right hand side, and vice versa. That programming can be done with the very same intuition I know from mathematics and physics was mind blowing to me.
&gt; No, only pure languages are pure, but then the sentence becomes a tautology :) &gt; In my mind, "functional" means we're using map and folds to construct new immutable values instead of using loops to mutate existing mutable objects. Well what you've described is a pure operation (possibly supported by a strong type system, without which higher-order functions quickly become unfeasible), in contrast with an impure one; so it's totally unclear to me the distinction or connection you're trying to make between "pure" and "functional" and how those terms should be used to describe languages.
Not the way they back C#. Their IDE for C# is actually pretty good. 
For me, one of the big "aha" moments was recursion schemes. We can model parent's `Prop` data structure as a functor: data Prop' a = Atom Bool | Not a | Or a a | And a a deriving Functor This is more-or-less the same thing as the original `Prop`, except that we aren't forced to store `Prop'`s all the way down. Here's the cool part though. We can "factor out" the recursiveness of that datatype: ```newtype Fix f = Fix { unwrapFix :: f (Fix f) }``` and we can now reassemble the original `Prop` as: ```type Prop = Fix Prop'``` At first glance this doesn't seem to have bought us anything, but in fact it has! We can write an "`F-algebra`" over this type corresponding to `eval`: eval' :: Prop' Bool -&gt; Bool eval' (Atom b) = b eval' (Not b) = not b eval' (Or b1 b2) = b1 || b2 eval' (And b1 b2) = b1 &amp;&amp; b2 which you'll notice is definitely *not* a recursive function. This intuitively makes some sense if you believe me that we "factored out" our recursion into `Fix`. Which implies the recursiveness of the original `eval` was likely factored out too: cata :: Functor f =&gt; (f a -&gt; a) -&gt; Fix f -&gt; a cata alg f = alg . fmap (cata alg) $ unwrapFix f and so we can reclaim `eval` as follows: eval :: Prop -&gt; Bool eval = cata eval' The real beauty of this approach is when you want to write other functions that tear down your data type. Maybe we want a pretty printer: pretty' :: Prop' String -&gt; String pretty' (Atom b) = show b pretty' (Not b) = "Not " &lt;&gt; b pretty' (Or b1 b2) = "(" &lt;&gt; b1 &lt;&gt; ") || (" &lt;&gt; b2 &lt;&gt; ")" pretty' (And b1 b2) = "(" &lt;&gt; b1 &lt;&gt; ") &amp;&amp; (" &lt;&gt; b2 &lt;&gt; ")" pretty :: Prop -&gt; String pretty = cata pretty' We've gotten only modest savings here by not writing the recursion explicitly, but imagine the case of an abstract syntax tree in a compiler, which might have upwards of 100 different sum terms. `cata` and `Fix` are general-purpose enough that they're [library code](http://hackage.haskell.org/package/data-fix-0.0.3/docs/Data-Fix.html#v:cata). Which means we can now write these F-algebraic folds over any functors we might want, and the library code will take care of the recursion for us for free. Blew my mind, and still does.
Great notes, thank you! Out of curiosity, is there better way of doing the line: c &lt;- lift (SC.getCookie "SessionId") &gt;&gt;= liftMaybe ?
Either way, the shipped binary will be usable in a couple of days i.e. once resolvers pick that compiler up. Changing compiler within predefined resolver will only make mess. The only solution is to set resolver to the compiler version.
Thanks, perfect link! That setting, plus a few more around it on that page, will allow us to use our existing scripts to switch between several standard GHC versions by setting environment variables.
Hmm, I'm not sure what you mean. If I use `system-ghc: true`, which is exactly what we need - are you saying that we need to set something else too, to keep stack consistent? Something called a "resolver"?
Who's marco?
Project Euler was exactly the place where I learned that "memoization" is not a very useful concept. In less than the amount of time it took you to convert your cubic code to quadratic using the so-called "memoization trick", you probably could have done a little more math instead and discovered a completely different approach that gives you constant complexity in a one-liner.
I'm not referring to the tricky cases where you're not sure what optimizations the compiler is doing. I mean the obvious cases. Like when you map a function over multiple copies of the same values, so you know for sure that the calculation will be run over and over again. Or when you bind the result of a calculation to a variable in the `where` clause, so you know for sure that the value will be computed only once.
Lamdu is much more than an initial attempt. It's not complete, but it's very far along. You can create code with it.
Here's another mind-blowing example of laziness: decode_LZW :: [t] -&gt; [Int] -&gt; [t] decode_LZW alphabet xs = concat output where output = map (table !!) xs table = map (:[]) alphabet ++ zipWith (++) output (map (take 1) (tail output)) https://wiki.haskell.org/Toy_compression_implementations
I like the innovation around designing little languages in racket. I came from Scheme (from SICP), then Clojure and Scala. I was always interested in Haskell, but I thought it would take longer to learn, so I put it off. I found myself wasting some time with simple type errors in Clojure, so I started learning Haskell. I also started studying formal methods, and there is some interesting overlap between types and formal methods. What I like most is the community and innovation in libraries. If I want to learn about interesting abstractions to solve a given problem, there is usually a Haskell paper and implementation of it. I am also hoping to eventually get good enough with the advanced parts of the type system to be able to express constraints that tests usually guard against.
Hmm, that's true, sorry for the confusing example. I once wrote [a combinator library in Java based on callbacks and mutable state](https://www.youtube.com/watch?v=85NwzB156Rg&amp;feature=youtu.be&amp;t=20m47s), this would be a better example but it's not nearly as well-known. The gist of it is that I defined a DSL for describing one-time events in terms of other one-time events, for example by stating that this event occurs once all the other events in a list have occurred. This was clearly a declarative combinator library, even though it was implemented using callbacks and mutable state. So I'd say that I was writing in a functional style, and that a language in which this was the predominant style should be called a functional language. Again, this is my understanding of the terms after seeing them in a variety of places, I don't have an authoritative source to quote for any of this.
I'm sure that you will be OK very soon Ed. I know what you feel. Of course this is not a blessing to say the least, but I'm sure that after some time you will contemplate it as a very positive learning experience.
They don't, directly. Indirectly the technical debt makes it harded to optimize some parts of the compiler or to implement some new stuff.
&gt; Have fun writing a while-loop condition that reads a mutable variable. It's pretty easy: whileM_ (readIORef mutableVariable) doSomething https://hackage.haskell.org/package/monad-loops-0.4.3/docs/Control-Monad-Loops.html#v:whileM_
 I think that the author is basically right (lack of composability in presence of impure effects is the problem that has made haskell fail to bring his promises to the real world) and he has made a language (unison) that live up to his claims.
Maybe? Nobody has been able to make it work well though. I've dealt with things like LabVIEW and it's deeply unpleasant. Maybe you'll be the one to make it work well!
One thing I want to add is parsing. In haskell you can completely make your own parser combinators from scratch once you have a clear grasp of monads. And then using do notation you can parse much more easily than monolith libraries in other languages. You can easily understand the code of ReadP and other libraries which in most other language requires hours if not more to find the thing one is looking. I wish more monad tutorials are based on concept of parsing as anyone familiar with parsing will see the immediate monad operations involved. See http://dev.stephendiehl.com/fun/002_parsers.html to make your parser combinator. It's not even a toy implementation(barring performance) and covers all of most used operations in parsing.
It's a very good example in this field. I have used IDA many times for the exact same reason it outputs a good graph. And I can testify this feature is very useful for reverse engineering.
 c &lt;- MaybeT $ SC.getCookie "SessionId"
Thank you so much for these efforts.
[removed]
This was mentioned in the 8.0.1 announcement. `exceptions` was one of a few libraries that were never officially in the platform, but were added as transitive deps of `cgi`. When `cgi` was removed from the platform, those deps were removed too. We need to get a working process together for managing the full platform and discussing adding things to it, etc. But in the short term the focus has been on providing the minimal (now "core") platform instead, and generally making this easier for people that want to get the packaged installer but don't necessarily want a ton of global packages.
Ah I see. Thats pretty nifty. I don't think so. If i annotate it like `runDB c $ deleteWhere ([] :: [Filter Session])` it will compile, but i think that won't delete anything. Well my idea was to have the database act as a backup for the in-memory store in case it went offline suddenly, so every so often it discards whats in the db and writes the memory store to it. And when starting up it reads the db into memory. Do you have a better suggestion?
Pretty sure the empty filter list admits everything. The empty conjunction is True, just as the empty sum is 0 and the empty product is 1. Also https://hackage.haskell.org/package/persistent-2.5/docs/src/Database.Persist.Sql.Orphan.PersistQuery.html#deleteWhereCount . Why would that need that annotation? http://hackage.haskell.org/package/persistent-2.6/docs/Database-Persist-Class.html#v:deleteWhere says that the argument must be [Filter Session]. Paste the error pls?
All the bespoke ways you used to use to transform some data in seeminly complex ways, edge cases, and effects, can most likely be modeled as a simple Traversal
Just tested it, you are correct. Thanks. Here's the error: /scotty-login-session/src/Web/Scotty/Login/Session.hs:155:13: Couldn't match expected type ‘PersistEntityBackend val0’ with actual type ‘SqlBackend’ The type variable ‘val0’ is ambiguous In the second argument of ‘($)’, namely ‘deleteWhere []’ In a stmt of a 'do' block: runDB c $ deleteWhere [] In the expression: do { threadDelay $ floor (syncInterval c) * 1000000; t &lt;- getCurrentTime; vaultContents &lt;- readVault; runDB c $ deleteWhere []; .... } I just put it down to persistent having lots of type magic..
There were a few. I'm a physicist so most of them are math related. 1) First one was what sold me recursion, which was the classic example of sorting a list: quicksort [] = [] quicksort (p:xs) = (quicksort lesser) ++ [p] ++ (quicksort greater) where lesser = filter (&lt; p) xs greater = filter (&gt;= p) xs This is kind of cliché but for me was particularly interesting. I'm kind of dumb and I didn't study computer science, so a lot of things that are natural for you guys only come with a lot of effort for me, and this was the first time I saw a code for a typical algorithm that I could read at first try.e 2) The second one was a code for automatic differentiation I saw in Sigfpe's blog. That's what sold me Algebraic Data Types and typeclasses. Imagine you define the following type: data F a = F {value :: a, diff :: a} and the following instances: data F a = F {value :: a, diff :: a} deriving (Show, Eq) instance (Eq a, Num a) =&gt; Num (F a) where (F x dx) + (F y dy) = F (x + y) (dx + dy) (F x dx) * (F y dy) = F (x * y) (y * dx + x * dy) -- etc... instance (Fractional a, Eq a) =&gt; Fractional (F a) where fromRational q = F (fromRational q) 0 recip (F x dx) = F (recip x) (negate (recip x) * recip x * dx) instance (Floating a, Eq a) =&gt; Floating (F a) where pi = F pi 0 exp (F x dx) = F (exp x) (exp x * dx) log (F x dx) = F (log x) (recip x * dx) sin (F x dx) = F (sin x) (cos x * dx) -- etc... Than you can calculate exactly the value of the derivative of any single variable function you write using the functions defined in `Num`, `Fractional` and `Floating`, but simply doing: diff (f $ F x 1) t where x is the point where you want to evaluate the derivative. This blew my mind. It's so clear and so obvious, and so beautiful. Of course there are way better ways to do this, like defining an AST for the functions you want to represent, etc... but this is very neat. 3) The third one was when I finally understood my first Monad, when I got that probability composition is monadic. Lets loosely represent random variables taking values of type `a` as a new type `P a`. Than lets consider a conditional distribution P(x| y) with X a random variable taking values in `a` and Y taking values in `b`. If you think about this for a moment you'll realize that this is just a function `b -&gt; P a`. Then if you have `y :: P b`, and a conditional distribution of type `b -&gt; P a`, you can calculate the distribution of `x :: P a` with no conditioning on `b`. For this you have to have a function `P b -&gt; (b -&gt; P a) -&gt; P a` which is just `&gt;&gt;=`. For distributions this is just defined by composing P(y) with P(x|y) and marginalizing y away. For sampling from random variables this is just sampling from `y :: P b`, putting the result on your conditional `b -&gt; P a` than sampling from the resulting `P a`. Thus, there are many different ways to reify this "probabilistic bind". You can easily defined `return` too: it's just a deterministic value (`return x` is a "random" variable that has value `x` with probability 1). There are many ways to represent this more concretely and do [actual useful work with probabilistic programming](https://github.com/hakaru-dev/hakaru). 4) Right after that I understood the state monad and monad transformers, which I have been trying to understand for a while, got immediately obvious after I understood that if you combine StateT and the probability monad above you have a Hidden Markov Process. 
Because types, purity and laziness.
[Stilo](http://stilo.com) in Ottawa, Canada. Haskell is the core of the server part of their latest product. 
The way a lot of problems can be intuitively and accurately modeled by ADTs -and it's often rather primitive sum types in my code - and how this keeps my code simple.
Yea Cabal on its own has certainly come a long way and can definitely be used as you're describing. It's just more complicated and cumbersome to use a system like that than it is to use Stack. If you want a sort of middle ground, Nix provides a "curated" package set that's more automated and very aggressively updated. Plus Nix comes with a ton of other benefits, though it's got an awful learning curve
Are there new cool one liners that are enabled with the Generic1 support?
The real world use cases I've found so far are: liftCompare = mzipWith1 (For :: For Ord1) liftCompare cotraverse = gcotraverse1 (For :: For Distributive) cotraverse You can also derive fmap, foldMap and traverse, but ghc has that built in already. Edit: Oh, I almost forgot, also an example from one of my own libraries, unfoldable: unfold = createA1 (For :: For Unfoldable) unfold What is interesting too is that the shape of the type of generic1: (forall d e s. c s =&gt; p d e -&gt; p (s d) (s e)) -&gt; p a b -&gt; p (t a) (t b) was not at all what I had in mind at first, but was dictated by getting the instance of functor composition `(:.:)` to work. And now it turns out that this shape fits the real world cases perfectly!
&gt;IIRC |&gt; is also the built-in/default for (&amp;) in F# Yeah, it got it from OCaml. :) And indeed, custom operators a quite loved in the haskell world, but we can't fix everything with custom operators. Structural syntax changes, like being able to access subrecords with `foo.bar` for instance, need to happen upstream. :/
I think the key point is explicitly addressing the social dimension, leading to adoption by both people whose packages are in a snapshot and users of the snapshot.
&gt; ... doesn't make Haskell bad My point isn't that Haskell is bad. MacIver's "What does it do..." quote is explained when he says, "Haskell is different enough from most languages that everything feels like an achievement when writing it." I shared my own anecdote about how proud I was to accomplish function overloading (which frankly is a simple concept). &gt; Don't do this. You can't tell me what to do! You're not my dad :D. Hahaha, more seriously, it's *my* hobby project and I can write whatever I please.
One design choice that was not explicitly mentioned, but which has the largest impact on me, is that in order for a package to be included in Stackage, the maintainers has to agree to perform timely maintenance. This is why my own packages are not on Stackage: I'm happy to do maintenance, but I can't promise timeliness in a way that the Stackage maintainer's agreement requires.
I think you are looking for Johan Tibell's [ekg](https://hackage.haskell.org/package/ekg) library.
[removed]
I don't think it is the case that if I compile a project today with cabal-install, and someone else compiles it tomorrow without changing anything within the project, they are guaranteed to get the same install plan and thus the same result that I did, because new versions of dependencies might be released which remain within version bounds. With stack, they will get the exact same install plan and thus the same result.
Are you aware of the `stack solver` command? I think I'd probably not describe stack's approach as completely disabling the solver in light of this command. Additionally, in my experience, `stack solver` makes it very easy to depend on a version of some library which is different from the one in the snapshot I'm using. 
In the same space, but not really what I had in mind. The generation of instrumentation data seems to be tightly coupled, based on this example here: https://github.com/tibbe/ekg/blob/13b9b2d80da8b12ec5cfb488ca7db46bbce283d2/examples/Basic.hs#L23-L46 **Edit:** nope - not what I'm looking for. This has only counters, gauges, and labels. I need to broadcast a stream of events and metrics for rich debugging and monitoring.
Well, OOP was vastly over-hyped. All OOP actually brings is is demarcated global variables. This is why people complain about classes getting too large and come up with all kinds of rules (e.g. "single responsibility rule") to prevent it. If your program is just one single, giant class then you've gained nothing over BASIC/C/whatever (with regards to global variables).
First off, with Stack you won't get the same result either *unless* you make sure to pin down the environment not managed by Stack (just one example is the `pkg-config` database). A more principled way if you care about this kind of guarantees is to use a system like Nix. Moreover, you'll get about the same level of reproducible build guarantees in cabal as you currently get with Stack when you combine [`index-state`](http://cabal.readthedocs.io/en/latest/nix-local-build.html#cfg-field-index-state) with freeze files, except that cabal is in the process of getting a much more expressive `constraints:` language than the simplistic one (e.g. Stack has currently no support for qualified per-component constraints) Stack is currently using in its Stackage snapshots.
&gt; It's just more complicated and cumbersome to use a system like that than it is to use Stack Can you elaborate on that? Does this criticism also apply to the new [Nix-style Local Builds](http://cabal.readthedocs.io/en/latest/nix-local-build-overview.html) facilities?
[Siemens Convergence Creators](http://www.convergence-creators.siemens.com/) in Vienna, Austria. Not heavily, but we use it.
Huh, I read this twice and both times I got lost at your "newtype Fix" :( Lots to learn!
It makes me excited when you have mentioned SCIP. unfortunately, I came from oriented object programming .. recently I have read many chapter of real work haskell and now I am reading SICP , it was amazing experience the way it talk about abstraction. I am always interested in reading and expanding my knowledge in thinking with abstraction , functional programming and haskell. if you know a book that you consider it important as SCIP please mention it
That metaphors are useful only when you have too much detail and need a high-level view - when the reverse is true the only option is to learn things from first principles / laws &amp; definitions.
That example was written that way for simplicity. And the original use case is to monitor your program's performance and use of resources in the GHC runtime, which is inherently tightly coupled. But it doesn't need to be tightly coupled. You could have a monitoring thread, for example. And using "distributions", as in the example, you can monitor whatever you want, not just GHC runtime statistics. If you don't need to be inside your application at all to get the data you want to monitor, it could even be a separate application. The point is that it's very lightweight, and yet powerful. It has built-in integration with existing high bandwidth monitoring frameworks such as statsd and Graphite, and also provides a JSON API. See [Johan's latest blog post](http://blog.johantibell.com/2014/05/ekg-04-released.html) (from a year and a half ago), and [Johan's original blog post](http://blog.johantibell.com/2011/12/remotely-monitor-any-haskell.html) (from 5 years ago), and [Ollie Charles' blog post](https://ocharles.org.uk/blog/posts/2012-12-11-24-day-of-hackage-ekg.html) (from 4 years ago). It's mature, but well supported - latest update 3 weeks ago. Perhaps there is something newer and even better. If so, I'd love to hear about it.
I have just installed the full platform (8.0.2, 64-bit, Windows), and when I try `cabal install singletons-2.2` I get a SEGV: [22 of 50] Compiling Data.Singletons.Prelude.Instances ( src\Data\Singletons\Prelude\Instances.hs, dist\build\Data\Singletons\Prelude\Instances.o ) Segmentation fault/access violation in generated code cabal: Leaving directory 'C:\Users\ggreif\AppData\Local\Temp\cabal-tmp-26588\singletons-2.2' cabal: Error: some packages failed to install: singletons-2.2 failed during the building phase. The exception was: ExitFailure 1 Has anybody seen this (reproducible for you)? 
This is neither here nor there really, but this is how it works in concatenative programming languages like Joy. In Joy, `2` denotes the _function_ that takes a stack and returns a stack with 2 on top. Library differences notwithstanding, you can define `f` as `f = "Hello" putStrLn` and then do `f f f f` to print Hello four times.
This has been bugging me for a long time. I just posted a complete proof on the haskell-cafe mailing list: https://mail.haskell.org/pipermail/haskell-cafe/2017-January/126026.html
Well, I assume Heinrich didn't force anyone to use his packages in the first place, so I don't think he has any obligation to accomodate the users. Creating these packages is his hobby AFAIK, not his job.
&gt; Seems like schemas (which I see as a very limited ontology) get you almost all of the benefit. I'm definitely using the word in a very casual way, just talking about schemas. I know there's been a ton of semantic web research related to ontologies but I'm not very familiar with it. My goal is to push the adoption of simple advances we should be using more already (immutability, content-addressable links, schma'ing). Hopefully if I'm missing something big from research someone will tell me=)
[unagi-chan](https://hackage.haskell.org/package/unagi-chan)
&gt; In my opinion, our dependency version language is not expressive enough. I want to be able to distinguish between estimations and known hard incompatibilities. There's a discussion on this issue: https://github.com/haskell/ecosystem-proposals/pull/1
What would you think of using a streaming library? Functions that need to yield instrumentation data could run in a streaming monad for that. import Pipes foo :: MonadIO m =&gt; Producer InstrumentationData m Foo foo = do x &lt;- getSomeFoo yield (instrument foo) return x
Here's an idea for a relatively low-effort way to check your lower bounds: Create a stack configuration, e.g. `stack-lower-bounds.yaml`, that specifies the lower bounds of your dependencies in the `extra-deps` section. You'd probably use a ghc-resolver, e.g. `resolver: ghc-7.8.4`. If you add this configuration to your CI, you'll notice when your package becomes incompatible with this configuration. One limitation of this approach is that AFAIK stack doesn't work with GHCs &lt; 7.8, but that will be less of a problem in the future.
With respect to that repo's approach: You can do it with `nix-env`. Entering the `nix-shell` will also work, but Intero and your build tools can't be told to do this. So you have to do it with `nix-env`, which is a major deal breaker to me. I don't want to pollute my user environment with something that should be sandboxed. But more importantly, you can't have two different projects at the same time this way.
It would be better to use a gender-neutral word rather than "guys"
&gt; Why are macros unsafe? Depending on how they are implemented, they could introduce aliasing/shadowing that didn't exist before macro expansion. In fact, some users prefer these non-hygenic macros. Often they subtly violate the module system as well, unless they are designed concurrently with the module system. This can impact both compatibility and correctness as the implementation of a module changes.
Distributions don't allow you to add custom payloads to the event. The only thing you can push into a distribution is a Double. 
Feel free to use 2 or more dimensions for your symbols. In fact, I often use two myself. I have problems with more because my visual field is mostly two dimensional, so using more dimensions tends either obscure parts or be difficult to internalize or both. Turning provided arguments that only one dimension is necessary for your symbols to describe universal computation. So, I'll stick with ASCII characters visualized in 2d and stored in 1d.
`Fix` is used to move the recursion outside the data definition (for the benefits listed in the post you replied to). It would be easier to understand if for the moment you imagine `Fix` to be equivalent of the `:` operator on lists. Without `Fix`: *Main&gt; :t (And (Not (Atom True)) (Atom False)) (And (Not (Atom True)) (Atom False)) :: Prop' (Prop' (Prop' a)) Building the same expression with `Fix`: *Main&gt; :t (Fix (Atom True)) (Fix (Atom True)) :: Fix Prop' *Main&gt; :t (Fix (Not (Fix (Atom True)))) (Fix (Not (Fix (Atom True)))) :: Fix Prop' *Main&gt; :t (Fix (And (Fix (Not (Fix (Atom True)))) (Fix (Atom False)))) (Fix (And (Fix (Not (Fix (Atom True)))) (Fix (Atom False)))) :: Fix Prop' 
&gt; I can assure you, toasters are not tested in wind tunnels :)) Not even [these ones](https://en.wikipedia.org/wiki/After_Dark_%28software%29#Flying_Toasters)? ;)
If I understand the current situation correctly then you're still depending on the cooperation of the original package author to upload new releases to Hackage as Stackage doesn't allow to apply patches to packages. Or am I misunderstanding what your role as a Stackage maintainer comprises?
That's correct. The package author still has to release new versions. This is how I view my role as a Stackage maintainer for other people's packages: I will fix issues myself and make pull requests. If fixing an issue requires more than rote work, I'll let the package author sort it out. That typically means the package will be dropped from Stackage in the meantime. 
Honestly, it's because I don't understand what's written at http://hackage.haskell.org/package/stm-2.4.4.1/docs/Control-Concurrent-STM-TChan.html#v:newBroadcastTChan And, why do I need to duplicate a channel? I why does `dupChan` docs say the following: &gt; Hence this creates a kind of broadcast channel, where data written by anyone is seen by everyone else. Why is it a "kind of" broadcasting channel and not a real Broadcast channel?
Performance is a concern for all professionals. Haskell differs substantially from other languages in it's strict type system. The strictness of Haskell's type system allows the compiler to make substantially more aggressive optimizations because the compiler can infer at compile time a greater amount of information about any point of your program's computation. Often recursive code written in Haskell is executed in an "imperative-style" loop using constant space because the compiler was able to infer that execution model was equivalent to the recursive form from the types involved in the computation. It's still possible to write inefficient code, blow the stack, or "leak" memory in Haskell but in general we trust the compiler to make aggressive optimizations on our behalf. If we encounter a runtime performance concern, we can profile the code, dump the STG/Core output, and inquire with the rest of that Haskell community as to how to improve performance with respect to that specific performance concern. If you have some example code exemplifying a particular performance concern please share it with us!
&gt; I think this is a great idea, but it seems that if you were to get a really large data set, you'd just run into problems trying to crunch these large data sets by creating copies of these data sets, and then passing these large copies of the original data around to other functions that would create even more copies of it! That's a lot of memory. Since data is never modified, Haskell can intelligently share portions of a data structure. Lists, for example, can share tails. If I create a list in memory: let foo = 1 : 2 : 3 : [] and then I make a new list: let bar = let (x:xs) = foo in x + 1 : xs GHC doesn't make a copy of *all* of `foo`. Instead, it creates a new cell for the first element of `bar` and they share tails: bar = 2 -\ &gt; 2 : 3 : [] foo = 1 -/ When you go to `map` over `foo`, it *does* make a new copy of the list, since they're no longer the same. So bar = 2 -\ &gt; 2 : 3 : [] foo = 1 -/ map (*2) foo = 2 : 4 : 6 : [] When you're using an immutable map, and you do an insert, you *technically* create an all new data structure that has only the new item inserted. But under the hood, the new map shares all of it's structure with the old value. It's only when the value changes that copies are made.
I love the interpreter in 20 lines of code. Concepts, Techniques, and Models of Computer Programming - I haven't finished this, but people call it the follow up book to SICP. Also, people can point you towards great books in type theory. It's natural go from the basic lambda calculus (Scheme) to the simply typed lambda calculus. I am not an expert in this area though, many others here or on Haskell blogs or wikis can provide you with great book suggestions there.
I'm not sure what you're asking. You can just try it out right? Also don't be afraid to look at the implementations of both of the libraries I mentioned; they're quite small, especially the `stm` ones.
&gt; Also, you do realize that you can use non-Stackage packages with stack, right? If not, see https://docs.haskellstack.org/en/stable/yaml_configuration/#extra-deps. They do realise that, as their original comment above mentions `extra-deps`. Apparently, though, the `extra-deps` field is also "undermining the Stackage project in a small way".
&gt; The only thing I pull in with an unqualified wildcard these days is Prelude Modules with the same package are also safe :).
I am also unable to reproduce this.
Absolutely disgusting. A+, may God have mercy on your soul.
It can be concern sometimes, but there are simple ways around it if you know what you're doing. Haskell can sometimes use lots of memory, and sometimes not much, depending on how you order the evaluation of code. It's something you have to be conscious of, but it's easy to get good at it. You have to understand that Haskell is lazily evaluated, uses thunks instead of stack frames, is garbage collected, and will memoize data so that it doesn't need to be recalculated. I won't explain these, because you can probably find better explanations than what I would give you. But you can to understand that Haskell uses a different memory model than C++, so you have to understand it to use it properly. You have to take advantage of laziness and the IO monad to control when data is created and destroyed. The basic premise is that, when you create data, it will fill up memory, and when you use it up and are finished with it, it can be garbage collected. It's good to try to follow a process of creating small bits of data, transforming it, using it up, then moving on to the next small piece of data. When you use if up, it can be garbage collected right away. If you create a large dataset, then leaving it sitting around while you do other things, then com back to it, it will be sitting in memory taking up space the entire time. You have to get a feel over time for how lazy evaluation works, and how to manipulate it. Also, binding IO actions (`x &lt;- someFunc`) performs that action right away, whereas pure functions will be evaluated only when they are needed. You can keep this in mind when figuring out when the program will create or be done with data. Also, your program will memoize data from pure functions and the data will sit around if it may be reused. Use up the data and allow it to be garbage collected. This really isn't something to worry about at the start, as it isn't much of a problem. When you start building bigger projects and it does become a problem, there are always ways around your memory issues. **Tail-call elimination** Haskell has built in tail call elimination. Tail calls are when a function calls itself at the end of its function body. When you write a tail-recursive function in C++, a new stack frame is created for each new call, potentially leading to a stack overflow. /* this will overflow the stack if it recurses too many times */ void funcLoop(...) { /* some computation */ funcLoop(...) } Haskell on the other hand. Will eliminate the tail call. It uses thunks instead of a stack frame. But it will re-use the same thunk for each loop. -- constant memory space, no stack-overflow funcLoop :: ... -&gt; IO () funcloop = do --some computation funcLoop ...
It sounds to me like this person's question isn't based on hearsay, but instead is based on looking at what the code does and trying to understand how it can be fast given what he or she knows about the way C works. I think it's far more productive to explain data-sharing in immutable structures, tail-recursion, and laziness than to dismiss this particular kind of concern as unproductive and call it a rumor.
Ah, I see. I misread.
Oh, I was going for variant vs contra-variant. Circles are a subset of ellipses. A function taking an ellipse can also take a circle. A function working with a helper function that produces ellipses can usually also work with a helper function that restricts itself to producing circles. (This may sound like trivialities. But once you introduce mutability, this becomes a crazy complicated mess.)
I had a lot of ahas when I was reading the [Stream Fusion Paper](http://fun.cs.tufts.edu/stream-fusion.pdf).
https://github.com/hotwirestudios (Dortmund, Germany) We build: - Frontends in Purescript - Backends in Haskell Specialized in mobile apps (iOS/Android).
&gt; should run in constant time You mean space.
Ah, right. Yes, for that you can always introduce an adapter if necessary. I was thinking whether it would be doable without any explicit conversion (which is usually what's touted as an advantage of OOP).
"f-shaped" refers to the shape of the nodes of the tree (the leaves are made with the `Pure` constructor). Pick the `Pair` example: Pair a = MkPair a a Here is an arbitrary example of a `Free Pair Int` value: Roll (MkPair (Roll (MkPair (Pure 0) (Pure 1))) (Pure 2)) Or, laying it out a little more suggestively: Roll (MkPair (Roll (MkPair (Pure 0) (Pure 1))) (Pure 2)) Thanks to the shape of `Pair`, the nesting of the constructors sets up a binary tree. Here is a schematic drawing of it: --*-- | | -*- 2 | | 0 1 
I get this error in Canada: &gt; Sorry! This content is not available in your country yet. &gt; We're working to bring the content you love to more countries as quickly as possible. &gt; Please check back again soon. 
Another way to look at it: start with a basic binary tree data Tree a = Node a | Branch (Tree a) (Tree a) Substitute `Pair`, instead of baking it into the `Branch` constructor. data Tree a = Node a | Branch (Pair (Tree a)) Generalize by allowing _any_ functor data Tree f a = Node a | Branch (f (Tree f a)) Rename the constructors data Free f a = Pure a | Roll (f (Free f a)) And voila, we have Free.
Not sure why that's the case. You could build from source and install the apk :)
[removed]
I have Windows 7 Professional (my workplace machine). Will reinstall tomorrow and report back.
[Fixed!](https://hackage.haskell.org/package/fltkhs-0.5.0.3/docs/Graphics-UI-FLTK-LowLevel-FLTKHS.html#g:8) Thanks for the feedback!
I'm sorry to hear about your loss. It definitely puts the comparative mildness of my own diagnosis in perspective. &gt; Get well soon. I shall. I plan to be around and contributing to our community for a long time to come.
Wouldn't Stackage become the name authority for its snapshots? In a way, it kind of already is. Packages that aren't in the snapshot you're using "don't exist". And package names within snapshots must be unique. 
Oh wow, that's really cool. And a smart way to structure everything. So I understand how it won't create a whole new list in every instance, but then even when the values change, and copies are made, that could still be expensive. In your example, when you map over foo and it does create a new list, is that list stored as a whole new list evaluated with map applied to foo, or is it stored as "map (+2) foo" and every time I call it, it has to recalculate "map (+2) foo"?
I suspect yitz was talking about more general experimentation than just hoping maintainers start doing more manual work. I think better automation might allow finer-grained dependencies (likely not human-readable) depending less on guesswork, and allowing taking advantage of source diffs to ask the maintainer a much smaller set of questions to keep information up to date.
&gt;It's still possible to write inefficient code, blow the stack, or "leak" memory in Haskell but in general we trust the compiler to make aggressive optimizations on our behalf. If we encounter a runtime performance concern, we can profile the code, dump the STG/Core output, and inquire with the rest of that Haskell community as to how to improve performance with respect to that specific performance concern. I see. That's reassuring. I don't have any code examples either, I was just wondering about situations like this that may occur. I have yet to do this, hah.
I think type classes might be enough to solve this problem?
That's crazy. I guess the "stack" model is just one way dealing with function calls. I suppose there's a ghci command that would give you the assembly before it compiles the code into an executable. Maybe I could write a program in C and Haskell, and see how the assemblies differ!
This is exactly what I was wanting to read! Thanks!
In C, just about any recursive call consumes stack space, and stack is tracked separately with a much lower limit than heap space. In Haskell (with a non-ancient GHC) many recursive functions run without using up increasing stack, either by tail-call optimization or by laziness. If you write code that does need to keep more memory with deeper calls, then using recursion is probably saving you from making an explicit data structure so it's not a wasted cost, and you only get a memory error if the whole heap fills up with stack frames (stacks are kept in a linked list of chunks, so there's not a huge amount of pointer-chasing or copying involved). For data sets, most immutable data structures are designed so you can share all but the part of the data that actually changed. Generally that works pretty well, maybe a few times slower than a mutable data structure, while making it cheap to hang onto old versions, and getting the other benefits of immutability like easy passing between threads. If you need to write C++ and design your types around cache lines and caches you'll think it's an unbearable cost, if you've ever considered Python for your application it's probably in the noise. It's also possible to explicitly use mutable data in Haskell, with syntax a bit more cumbersome than imperative languages.
I think ghc -S produces assembly output, but be forwarned that without understanding the execution model the assembly will be mostly incomprehensible (and even with an understanding, difficult to figure out). C was built to translate pretty directly into assembly, haskell was not. I recommend SPJ's implementation of functional programming languages [book](http://research.microsoft.com/en-us/um/people/simonpj/papers/slpj-book-1987/), and the GHC internal [documentation](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/GeneratedCode). Note that in both there is a 'stack', but if you follow along, you'll see that it doesn't necessarily grow with recursion, but rather by the number of thunks entered. It's hard to explain succinctly, but basically, every time a recursive call appears as an argument to a data constructor, the stack becomes 'bypassable'. For example, this expression can be evaluated by GHC to WHNF in constant time and space, and won't grow the stack &gt; f :: [Int] &gt; f = 1:f because the `(:)` constructor becomes a thunk which can be 'bypassed'. This expression cannot be guaranteed to not grow the stack. It may not because of optimization, but either way it evaluates to bottom &gt; f :: [Int] &gt; f = f Partially applied functions somewhat follow the pattern for constructors. This is a basic intuition only, and not a complete one at that.
By the way, if you like "Learn you a Haskell", you should have a look at the new "Haskell from first Principles" book. I like it much better. (Alas, the latter's not available for free online. But if you are really hurting for money, there are ways to download it..)
We still encapsulate in Haskell. Look at eg the Data.Map module, which doesn't export the constructors.
If you like Automatic Differentiation, you gonna love the Brzozowski derivative (https://en.wikipedia.org/wiki/Brzozowski_derivative) approach to matching regular expressions.
How would eg a Boolean be a stream?
Docker's tooling's focus on immutable containers is pretty neat. (But there are probably other tools that do similar things?) Hating on PHP is a long cherished past time. But do keep in mind: PHP made it really, really easy to copy-and-paste a handful of lines of code to add a silly visitor counter to your otherwise static website on dirt-cheap shared hosting. Compared to the alternatives at the time, that deployment option for beginners was awesome. Of course, someone could have added the same approach for embedding source code into html pages that PHP used to some other existing language, and an alternative universe might have seen this: &lt;?Haskell putStrLn "Hello World" ?&gt;
ghci is not always going to compile into the same code as ghc. If you want to test I would compile it to executable code, not bytecode. 
Isn't it the opposite? OOP has hardly any nice theory---that's why people publish papers about FP mostly. But it seems to be bearable to some extent in practice?
What would you like to happen when there are zero readers?
I inhabit both sides of these roles for different packages, sometimes within the same package, and it works fine.
Yes, isn't it neat? I'd count stateful monads, too. Encapsulation in Haskell is a separate tool. It's used when necessary, in stead of just in case. 
Sure, a linear bunch of values can be treated as a 'stream'. But I still don't see how 'everything is a stream'? I was going with an example that's too simple to be a stream. What about with an example that doesn't have the linear ordering of a stream? Eg how is a function like (Set String -&gt; Set String) a stream? Or a Linear Program? (https://en.wikipedia.org/wiki/Linear_programming) Don't get me wrong: (lazy) sequences of values are an interesting and widely applicable concept. I just don't agree that everything should be viewed through that lens. (This reminds me a bit of the common misconception that 'everything is a function' in Haskell.)
Oh neat, this looks very similar to an idea for an always-well-typed editor I have... in my head.
There is Metrix Financial Reporting Solutions in Hamburg, Germany: * [Github](https://github.com/metrix-frs) * [Cached version of website](http://webcache.googleusercontent.com/search?q=cache:U2ex9cXPBwgJ:metrix-financial-reporting-solutions.de/) AFAIK they use Haskell for the backend, Purescript for the frontend.
probably a bounded channel so it doesn't explode
Not sure if [FPComplete](https://www.fpcomplete.com/) counts. AFAIK their only office is in the US but they are basically remote-only with team members in many countries.
The "you're" in my last comment was meant to refer to myself and others responding to OP rather than op himself. 
&gt;I don't understand what "the tree" is The trees are `Free f a` values. &gt;does a monad have to have a corresponding tree? No. Any *functor* `f` has a corresponding `Free f` monad, which is the free monad for that functor. The examples in the message amount to different choices of `f`, which give rise to different monads. &gt;Don't both Monad and Functor only admit types of kind `* -&gt; *`? I can't imagine how there'd be much possibility for different structures or shapes. Yes, but the variations in shape do not require any further flexibility at kind level; it is only a question of which `Functor` you choose. &gt;Can someone point me to some reading material? Though it considers the issue from a rather different angle, [this Gabriel Gonzales post](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html) is a nice introduction. 
A similar question came up recently here: https://www.reddit.com/r/haskell/comments/5ozp3j/haskell_vs_c_benchmarks_for_numericheavy_code/ I don't have as rosy a view as others as far as advocating for haskell performance. For one - on stack vs. heap allocation policies - languages like rust, and arguably modern C++ (which borrows from FP concepts) are going in the opposite direction - away from GC collection, towards better use of stack allocation via RAII. Mike Acton's data-oriented-design concepts also raise performance issues that are not even on the radar for a lot of Haskellers. I'm personally not convinced that GC is needed for abstracted, high level code. A lot of high-level developers take it for granted in the post-java world, but I'm open to the possibility that GC may turn out to be an innovation dead end. Not to say that there aren't substantial tradeoffs in these alternative directions, but I do think the Haskell community has blind spots when it comes to performant code.
Streaming will not leak memory in that case. You will simply discard the results.
Something akin to: class Overload a where type OverloadedOutput a :: * overloaded_f :: a -&gt; OverloadedOutput a instance Overload (Int, Bool) type OverloadedOutput (Int, Bool) = Bool overloaded_f :: (Int, Bool) -&gt; Bool instance Overload (Char, Int) type OverloadedOutput (Char, Int) = Char overloaded_f :: (Char, Int) -&gt; Char
IIRC when you compile stuff instead of using GHCi don't you have an unbounded stack?
STM is a good first choice, and is often faster.
Gabriel Gonzalez's post that you linked to and [Michael Xavier's post](http://michaelxavier.net/posts/2014-04-27-Cool-Idea-Free-Monads-for-Testing-Redis-Calls.html) on Free Monads are the two that felt the most practical to me when I first learned what free monads were.
Should this be a buffered and bounded channel?
The degree requirement has been removed.
Cool. Clearest explanation I've seen. Are you missing an f in the 'allowing any functor' one? | Branch (f (Tree f a))
&gt; while_ ((&lt; 10) &lt;$&gt; readIORef i) $ do That's quite ugly compared to `while (i &lt; 10)` to be honest. Do you think the `&lt;$&gt;` madness is not more painful than in imperative languages? If so I have no more to say.
Fantastic!! I guess I'll grab some coffee and perhaps also grab /u/lolisakirisame and read it... **Edit**: Now it's the top comment here and I feel ashamed...
I think the best thing that Stack has to offer is curated builds. I doubt that this alone is enough since tooling to use the builds is also needed. I don't see how better specification of version bounds or new ways to specify version bounds will relieve us from cabal hell, I suppose it's just my own lack of imagination. When a tool replaces stack that is more useful I will be convinced otherwise. Thank you Michael Snoyman, FP Complete and others. 
Why did you choose FLTK? I have no problems with it but I'm curious what you think of it and why you chose it.
Yep! Fixed. Thanks :)
&gt; I would rarely write such code in Haskell You would rarely write proper code in Haskell? (lol jk) Seriously there's simply no point saying it's possible. Imperative programmers (fortunately) won't buy into it. Then they sometimes go combinator-y function-y and they call it 'advanced features' and that's an entire other story...
I know that Keera Studios makes games in Haskell for the Android platform, they also have some examples on github. Here is their blog: http://keera.co.uk/blog/
An equality can be used as any other value in the optics hierarchy. The simplest example is `id`, which is an Equality. This is useful because it can compose anywhere in a chaining of lenses. For example, you could have something that takes or returns a lens where in certain cases you want it to be a noop.
This thread seems to suggest that `broadcastTChan` has everything covered. From the documents I am unable to understand if the 0-reader case is covered or not. The existence of `broadcast-chan` and `split-channel` seems to suggest that it's not covered (unless they were released *before* `broadcastTChan`was added). Your comment also seems to suggest that `broadcastTChain` doesn't cover the 0-reader case. What's the actual situation?
For context, here is Ben Gamari's comment that alerted me of the new policy: https://github.com/ghc-proposals/ghc-proposals/pull/11#issuecomment-274712095
Does Eta have the more common language extensions though? 
&gt; Not really. Version pinning is available in cabal, too, if that's what you want. It's just not the default. The reason to use stack is the usability of its interface. It's idempotent builds that everyone likes. Which are achieved by version pinning. Also cabal got the freeze feature after stack was introduced I think. Also the curated package sets are worth something. It's more than *just* UI. 
&gt;I don't see how better specification of version bounds or new ways to specify version bounds will relieve us from cabal hell cabal hell usually refers to having your system stuck because one project requires one version that interferes with another. IMO the best thing stack does is give an idempotent build command, not just curated builds. 
The identity function is actually the *only* `Equality` value. Nothing else is polymorphic enough!
&gt; Ignoring the upper bounds sounds like an idea worthy of an experiment too! I'd imagine that it might help discover actual (hard) incompatibilities more quickly. &gt; People change their APIs all the time. The hope is they use PVP or something so you know when this will happen. 
So I'm not an expert on it but it appears to me the situation is: * `TChan.newTChanIO` can space leak with no readers * `TChan.newBroadcastTChanIO` won't space leak but is type-unsafe in that you can `TChan.readTChan` it which will retry, blocking the thread it's called from so you must remember to use `TChan.dupTChan`. * `BroadcastChan.newBroadcastChan` won't space leak and is type safe. You cannot `BroadcastChan.readBChan` it because that's a type error. You're forced to use `BroadcastChan.newBChanListener `. The API is very simple and clean; I've used this one before for logging to multiple places. * I'm not quite sure if `Unagi.newChan` can space leak with 0 readers. `Unagi.Bounded.newChan` definitely won't however. Both are type-safe. I want to try this library out some time because of the performance tuning that was done. My sense is that broadcast channels both of the `BroadcastChan.newBroadcastChan` and `TChan.newBroadcastTChanIO` have a more limited use case than normal channels like `Unagi.newChan` and `TChan.newTChanIO` which happened to be the use case I had. I've never looked into `split-channel` before. Looks neat.
Cool!! This one remember how to calculate frequencies with arrows to do Huffman enconding: freqs = fmap (head &amp;&amp;&amp; length) . group . sort 
This is really awesome, can't wait to get home and pick up some 'newcomer' tickets :)
I see. Still they don't seem to work particularily well together or at least library makers don't care about compatability as much. Hence my prefixing problem.
Haskell doesn't take mutation very well but once we get some linear-ish types that's no longer a problem. I hope it happens soon enough
Sounds like a great first pull request! ;) Edit: [ya snooze, ya lose!](https://github.com/ghc/ghc/pull/16)
The vector package provides mutable arrays under Data.Vector.Mutable. That's a good start for O(1) write access to an array. Once you're done writing to it, they all support freeze functions which create a pen immutable version of it. The mutable functions all run in some primitive monad which can either be IO or ST. With ST you can do local mutation and wrap that up in a pure way. If you want specific examples of that just ask here.
Actually I'm not sure where the limitation to documentation comes from. Ben mentions "small changes", and that doesn't necessarily mean documentation-only.
Indeed, there is a lack of resources and finding your way can be difficult. In my case that was more or less reading types and trying to match them. The quick references for [reflex](https://github.com/reflex-frp/reflex/blob/develop/Quickref.md) and [reflex-dom](https://github.com/reflex-frp/reflex-dom/blob/develop/Quickref.md) helped me a lot. Also, there is /r/reflexfrp, where you can ask more specific people and read the previous discussions.
Of course x)
These are some problems i think i met frequently enough to give a shoot: + Sometime i don't know if i should use `mask` in my library code, or just document it and ask my user to use `mask`. The former would lead to nested mask, the latter often lead to unsafe code. + I have to read the source to know if a library function use `mask` or not. `bracket/...` or any helpers applied above too, Is there something we can use in type level? 
I assume they meant "reproducible" instead of "idempotent".
I'd be interested to read more about setting allocation limits, and how to profile the application in order to asses what a good threshold would be.
Here is an example of mutable-containers: https://gist.github.com/chrisdone/293214d4a64524dc582d22ff7013b8ec
Implement of broadcast-chan here. The main reason for me to implement broadcast-chan, rather than using `TChan.newBroadcastTChanIO` is not type safety, but the fundamental issue that STM is not suited for all workloads. Specifically, `TVar`s suffer from "thundering herd" problems, that is. if a `TVar` is updated *every* thread blocked on that `TVar` will wake up. This *also* applies to `TChan`. Regular `Chan` uses `MVar`s internally, which are single wakeup and guarantee fairness. That is, if a 100 threads are blocked on an `MVar` and something is written to it, then only one of those 100 threads will be woken up. This means that `MVar` and `Chan` based solutions are preferable in scenarios with lots of concurrent accesses. broadcast-chan is really just `newBroadcastTChanIO` reimplemented for `Chan` with a fancy newtype wrapper for safety. I initially proposed it as patch for base, but people wanted it to "prove itself" as library first.
It's kind of sad that *vectors* immutable to mutable optimizations are not "perfect". At this point I chose the mutable way by default because it gives the same good results every time and does not depend on flunky optimizations... That said I need to process about 400.000 Events per second... For every day usage the mutable instances are fine. 
Structural records could had also solved it like so: myModule progName = { foo = ... , bar = ... } 
&gt; That's crazy. I guess the "stack" model is just one way dealing with function calls. The way GHC compiles function calls every call is basically a jump/goto. If you're comfortable with C (and a tad of assembly), then there is a very enlightening paper on (one way of) compiling Haskell to machine code: [Implementing Lazy Functional Languages on Stock Hardware: The Spineless Tagless G-machine](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.3729&amp;rep=rep1&amp;type=pdf) Many details of it are outdated compared to how GHC does things no, but the high level model/overview is still accurate enough to help. Including the absence of a function call stack.
Hmm ... any idea on how to integrate that with the "script interpreter" used by *stack*? Just putting it on the "magic line" does not work neither does this: -- stack --resolver lts-7.0 --install-ghc runghc -- -fobject-code -ghc-arg=-O2 -package txt2bottle
It's a regular immutable map. You're creating a fresh immutable vector.
Your problem there is stack. Guess what the solution might be?
I struggle to understand what is the actual problem that you're trying to solve. This: &gt; None of these provide a solid foundation for personal computing. feels like it's missing a "here's why it's a problem:" section afterwards.
Yes, hence my silly Haskell inside HTML example. Of course, at some point it makes more sense to talk about tooling and language as one when discussing why the trajectories of popularity.
Yes it depends. Most of the time the lazy immutable approach is great. But sometimes programs really do use those tight loops.
&gt; I guess that mutability and unboxed data is important in tightly loops with heavy calculations. You're right, ephemeral data structures (a better name than "mutable" - the data structures Haskell uses are also 'mutable' under the hood!) are best seen as an optimization. And ideally, sound use of such data structures would require the compiler to check that "writes" only happen under exclusive access to the "written" subcomponent, as Rust does.
The vector package has a generic module for doing that. But I wouldn't bother doing anything with lists. It's pretty much always not the right tool for a data structure. 
This is great! I don't even like git, but I never understood the reasoning for not embracing github considering ghc had already switched to git. Github is the main reason I would use git in the first place.
Setting allocation limits: https://www.stackage.org/haddock/lts-7.16/base-4.9.0.0/System-Mem.html#g:2 You can use the allocation counter (via `getAllocationCounter`) to determine the allocation cost of requests, then collect data and decide where to set your limit.
Thanks for the quick reply, I wasn't aware of `System.Mem`.
/u/gbaz1 /u/bgamari I have uninstalled both the platform and stack, reinstalled both, `cabal install singletons`-ed and got the *same crash*. Then I deinstalled again and reinstalled the 32-bit version of the same tools, and this successfully installed `singletons` as well as all the other stuff I need.
For hobby stuff no, for user facing Android apps I'd be uncomfortable.
Ah ... I should mention that I hope to put that into the line of the "stack scripting" line (https://docs.haskellstack.org/en/stable/GUIDE/#script-interpreter). I have some scripts that speed up considerably when compiled, but I want to avoid the compilation by hand part.
Isn't basically the only "good" use of async exceptions to kill a thread? So why not provide "thread finalizers" that run when a thread is killed to clean up? Then instead of complex masking interactions we just need to write allocators to register a finalizer and deallocations to deregister.
Could you please be more explicit about what additional guarantees Rust is providing? Are you talking specifically about a concurrent environment?
If you enable `PolyKinds`, you can give even fewer fucks.
The guarantees are *especially* relevant when dealing with concurrency, but not exclusively so. For instance, with some ephemeral structures such as "growable" vectors, there are write operations that can invalidate previously-created "read" pointers into the vector. Rust can then check that none of these are live across such a write, as a special case of the "exclusive write access" rule.
Did anything come about from the discussions to ship a supported version of LLVM with GHC?
[removed]
the condensed notation makes it a bit more opaque, but (\bc.cbz)z(\wv.w) (\c.czz)(\wv.w) (\wv.w)zz (\v.z)z z apply left to right until you can't anymore (edit: oh man, gdeest did a really great job of this, but leaving this here in case it helps someone visualize)
Your question is not very easy to understand without seeing you *trying*. I'll try anyway. First, if unsure about precedence rules, it may be helpful to group elements with parentheses and remove any syntactic sugar: &gt; (((λa.(λb.(λc.(cb)a)))z)z)(λw(λv.w)) As may be clear now, the lambda-term is of the form: &gt; F A with: &gt; F = ((λa.(λb.(λc.(cb)a)))z)z &gt; A = λw.(λv.w) A nice property of lambda-calculus, called "confluence", is that you can basically reduce it in any order and get the same solution. So you may now focus on either F or A, reduce it all the way down, substitute the result in the original term and proceed. Now, there is not much to do to simplify A, as it does not contain any redex (a term of form: (\x.y)a). As for F, observe that it is of the form: &gt; ((λa.(λb.(λc.(cb)a)))z)z = (G z) z With z applied twice, once to G, then to G z. Focusing on just G z for now, we have: &gt; (λa.(λb.(λc.(cb)a)))z = λb.(λc.(cb)z) With the right-hand side being the result of beta-conversion applied to the left-hand side. Now, beta-conversion is just glorified substitution: (λa.T) z = T[a=z], meaning that any free occurrence of a in T is replaced by z. That's exactly what we did here, by removing the first lambda in the term and replacing every a with z (note that some precautions may be required here to prevent accidental name capture ; that is not the case in your example. Proceeding all the same: &gt; F = (λb.(λc.(cb)z)) z = (λc.(cz)z) Getting back the original term: &gt; F A = (λc.(cz)z)) (λw.(λv.w)) &gt; = ((λw.(λv.w)) z) z &gt; = (λv.z) z &gt; = z Hopefully I didn't mess up somewhere and it helps a little. 
you're good, we got the same normal form but your explanation is way better and more correct 
Hello everyone, Thanks for advertising this, /u/sjakobi! I'd just like to take this opportunity to clarify the motivation here and perhaps set some expectations. While GHC has had a healthy flow of moderate-to-large patches recently, I have long noticed that we seem to lack the smaller patches that many other projects enjoy, especially to documentation. Looking at our development process, I think the reason is pretty clear: submitting small patches with Arcanist is awfully painful. Accepting Pull Requests on GitHub is an attempt to lower this barrier. Of course, this doesn't mean that we will accept all patches which come in via GitHub. We will continue to insist that non-trivial functional changes to GHC are reviewed via Phabricator. To help in this process I will gladly help contributors by moving their pull requests to Differential. However, I do expect that a significant number of documentation patches can be accepted as-is. I hope this will be encouragement for more people to pick up the Phabricator work-flow in time. As always, let me know if I can be of help. Happy Hacking! ~ Ben
&gt; Well, your 'vault' and 'operational' packages are on Stackage! Which is interesting, because I didn't put them there. :-) &gt; Also, have you tried finding co- or backup-maintainers who could take care of the compatibility problems that arise on Stackage? Looking for a co-maintainer has not occurred to me, probably because I didn't expect to find one. But it sounds like a good solution. Since you volunteer: Would you be my co-maintainer for `reactive-banana` and `threepenny-gui`? I'll send you a separate email to hash out the details.
I'd like to add that since there's a whole bunch of functions that compose pretty well, you might find that something requiring explicit loops in C or even C++ becomes just a matter of putting a handful of already-existing functions together. *That's* how most Haskell code looks like.
I would gladly take that responsibility. cc /u/apfelmus
If you find examples helpful, here's a four-function calculator with a Reflex-based web interface: https://jonathanknowles.github.io/haskell-calculator-web/ It parses standard arithmetic expressions such as "(1+2)*(3-4)/(5+6)" and renders the result along with a colourful parse tree (generated with Reflex). Source code: &gt; https://github.com/jonathanknowles/haskell-calculator &gt; https://github.com/jonathanknowles/haskell-calculator/blob/master/source/library/Calculator/WebInterface.hs Let me know if you have any questions about the source. (Admittedly, I'm still learning Reflex, but will be happy to help if I can.)
Here's a paragraph with a bunch of company names from Stephen Diel's [Reflecting on Haskell in 2016](http://www.stephendiehl.com/posts/haskell_2017.html): &gt; Lots of exciting things going on at Barclay’s, Facebook, Target, Ambiata, Tweag, Takt, Zalora, Galois, JP Morgan, Helium, Silk, Lumi Guide, Awake Networks, FrontRow, Clearmatics, Standard Chartered, Digital Asset Holdings and Microsoft
What do you think about ghcjs vs purescript?
Thanks, that's a super helpful point. My original attempts at the README started with describing the problem as I see it in detail, but they kept bogging down so I resolved to rewrite it more as a technical explanation. Sounds like I went too far though:) I'll attempt a shorter version here, and then condense it again for the README later: ### Unix filesystems + Hierarchies are bad when data can fall under more than one category. This is most personal data. For instance, I have a code folder that I would like to organize by language, by project, by the organization I wrote it for, by whether it's active or not, etc. This can't be done well with a hierarchy. [Heroic efforts](http://tmsu.org/) have been made to fix this, but the unstable foundation [shows through](https://github.com/oniony/TMSU/wiki/FAQ#why-does-tmsu-not-detect-file-moves-and-renames). A tagging system that can't handle renames isn't really acceptable. + I'll skip mutability since this is /r/haskell. Mutability should be an optimization for special tasks like video editing, I don't want my core data (code I modify, my writing, etc.) to be mutable. ### Webapp storage Talking about "webapp storage" is hard because it's such an amorphous concept. Each webapp stores data differently, how can you criticize webapp storage as a whole? The answer is to criticize it for exactly that -- it has no structure. Imagine a computer where each program tossed its data to a different part of the world instead of storing it locally, and from then on instead of actually getting to see the data (the underlying SQL or JSON or whatever) you just got to see markup. This is one of the worst data storage systems imaginable, yet it's incredibly popular and becoming more so. Defeating webapps is my top priority, but I don't think it's possible to do this without massively improving non-webapp personal data stores. In the long run I don't want the systems built on Station to be just immutable, tagged filesystems, I want features that will let them outcompete webapps. There are a lot of these, let's just consider one. On Unix filesystems the only way (at the datastore level) to track "who created something" is by local user. For a computer with just one user that's not helpful at all. Station allows tracking where data originated (through the Version's "authors" field), so if you download a piece of data from a friend Station will actually distinguish it from data that originated on your computer. Dang, now I remember why I had trouble condensing this to fit! Maybe it should go under `/docs` and I should just link to it from the relevant section of the README.
Isn't a broadcast channel *by definition* a thundering herd? When I write to a broadcast channel, I want every listener/reader to be woken up, to be able to react to the newly posted message. 
I wasn't aware of `IsList` prior to this post. It is not that big of a deal. It's visual clutter which makes code less readable. Furthermore different names suggest there is a semantic difference. And lastly you can't easily swap out types if you prefix everything. If I can get rid of this annoyance why wouldn't I :)
`SafeT` and `ResourceT` are *some* help at the type level.
Hmm, did anybody consider using async exceptions to enforce real-time deadlines? Sound like a possible application. 
After trying reflex-dom myself i was able to progress in building some toy one page examples, but was discouraged quickly by how complex it all became and how little documentation was for more advanced things I tried a different library: [react-flux](https://hackage.haskell.org/package/react-flux) and liked it very much. It is built on reactjs, has a very thorough, easy to read and follow documentation. I was up and running with it in no time. Most importantly it does not get more complicated as the application grows, and the concepts are much simpler to understand than in reflex-dom. And a huge number of third party reactjs components is readily available to you.
From my point of view, GHCJS is more powerful, because it has most of GHC features and 80% already existing packages are provided, but it tightens you to Haskell ecosystem. On the other hand, Purescript can work on top of nodejs or something else without fuss, strictness by default There is also alternative view and nice article http://mutanatum.com/posts/2017-01-12-Browser-FP-Head-to-Head.html which states that PureScript is a "future of Haskell".
Doesn't work on mobile 
If it's a Haskell position it is important to make this absolutely clear: we've had plenty of »Haskellers wanted to write PHP legacy code« style posts over the years. For example, when I follow your link, I am greeted with a suspicious Java logo.
&gt; It's idempotent builds that everyone likes. Everyone likes idempotent builds for release versions, but not when in development mode when anyway every build is different. &gt; Also cabal got the freeze feature after stack was introduced I think. I think the opposite is true - I remember fighting for that feature before the idea of stack was hatched, and I believe it was delivered before then also. But in any case, even before the official feature, we were able to tease freeze files out of cabal using a simple shell script. &gt; Also the curated package sets are worth something. Definitely. But that is stackage, not stack. You can use stackage with cabal, too, it's just not the default. So it's really just a question of UI.
`unsafeCoerce#` is indeed evil. It's an evil version of the identity function! But yes, I was talking about legitimate functions with that type. Ignoring the annotations needed to make it maximally poly-kinded, `type Equality s t a b = forall p f . p a (f b) -&gt; p s (f t)`. So I can write `newtype P x a b = P {unP :: x}`, and given any `e :: Equality s t a b`, use it something like this (I'm not sure if there are enough type annotations here): f :: a -&gt; a f = unP #. e .# P By parametricity, `f = id`. Since `Equality` allows no constraints, `e` must behave operationally the same for all types, so it can't be anything more or less than the identity function.
I'll look into Reflex, thanks for the hint. I have mixed feelings about Helm. It's the first time I've written any code using FRP, which I found really nice, but there were a number of things I found frustrating... * [Working with images was painful](https://github.com/mrmonday/waver/blob/5d99e87c77f3126a11daf83c706dadfa8260e838/app/Main.hs#L561-L563). Having to list them all in advance and include them all in the game state seems pretty hacky. I don't know if there's a nicer way to use them, but as it is I would have to build my own abstraction if I wanted more images in there. * [No audio support](https://github.com/mrmonday/waver/blob/5d99e87c77f3126a11daf83c706dadfa8260e838/app/Main.hs#L559). I resorted to spawning VLC in the background for the sound track. I decided not to attempt sound effects - it would have taken too long to do under the time constraints of a 48 hour game jam (or led to some incredibly messy code). * The coordinate system. Even now I don't know how it works. Some things seem to place the origin in the top left of the screen, others place it in the center of an object, or in the top left of a parent object (which you only know exists when rendering, so you can't account for it during the update phase). It took me a long time to get collision detection working as a result. I noticed that the `move` function didn't always work too. For example, [in the view credits function moving some text worked just fine](https://github.com/mrmonday/waver/blob/5d99e87c77f3126a11daf83c706dadfa8260e838/app/Main.hs#L470). However, [in the view prelude function move didn't work, so I hard coded some white space to get things to line up](https://github.com/mrmonday/waver/blob/5d99e87c77f3126a11daf83c706dadfa8260e838/app/Main.hs#L461).
For what it's worth[1], functor law 2 does hold. [1] Nothing.
I was looking for exactly this. It seems to be the only real option in Haskell fulfilling this space. Do others know of alternative mutable hashtable implementations?
Slide #35 isn't right about Agda: we do have there the equivalent of superclassing.
Yup, binary installer, OS X, GHC 8.0.1. I just tested on Linux, and it seems to work. So perhaps argument parsing works different on OS X and Linux? That would be... surprising. In any event, Stack isn't the relevant bit here, it's just passing through arguments directly to runghc.
How does this compare to `ansi-wl-pprint`? On first glance these two libraries seem to have major overlap.
http://schema.org/ is pretty successful. It's a collaboration between search engines.
&gt; Sometime i don't know if i should use mask in my library code, or just document it and ask my user to use mask. The former would lead to nested mask, the latter often lead to unsafe code. &gt; If you (as the library author) manage a resource yourself, you'd expose a `withFoo :: (Foo -&gt; IO r) -&gt; IO r`. If you don't, you'd expose both an `acquireFoo :: IO Foo` and a `releaseFoo :: Foo -&gt; IO ()`. Either approach is reasonable, but where does the confusion come in? The decision to use `mask` in the library or not changes the type of the exported function(s).
&gt; ansi-wl-pprint I had not see it before, but taking a quick look at it, there seem to be some similarities (decorating with expressions). As far as differences, `escape-artist` is not intended to be a pretty print library. And as such, I think `ansi-wl-pprint` has to consider more things when designing the API. Also, `escape-artist` uses data constructors to build the structure to render instead of functions. If pretty printing is what is needed, then `escape-artist` would not be the right choice. But with that said, let's make a comparison where pretty printing is not needed. &amp;nbsp; For example, this in `ansi-wl-pprint`: &amp;nbsp; `putDoc $ red (text "Red") &lt;&gt; comma &lt;+&gt; white (text "white") &lt;+&gt; text "and" &lt;+&gt; blue (text "blue") &lt;&gt; char '!' &lt;&gt; linebreak` &amp;nbsp; would be equivalent to this in `escape-artist`: &amp;nbsp; `putEscLn $ FgReg "Red" &lt;&gt; FgDefault ", " &lt;&gt; FgWhite "White" &lt;&gt; FgDefault " and " &lt;&gt; FgBlue "Blue" &lt;&gt; FgDefault "!"` &amp;nbsp; But, what `escape-artist` can do is take ANY data type in the constructors and mix them in the same expression: &amp;nbsp; `putEscLn $ FgReg 7 &lt;&gt; FgDefault ", " &lt;&gt; FgWhite "One" &lt;&gt; FgDefault " and " &lt;&gt; FgBlue 6.5 &lt;&gt; FgDefault "!"` &amp;nbsp; And if you implement `ToEscapable`, you can do the same with your own data types. A very simple example would be like this: &amp;nbsp; data ABC = A deriving (Show, Eq) instance ToEscapable ABC where toEscapable _ = Inherit $ show A &amp;nbsp; Then you can do this: &amp;nbsp; `putEscLn $ FgReg A &lt;&gt; FgDefault ", " &lt;&gt; FgWhite "One" &lt;&gt; FgDefault " and " &lt;&gt; FgBlue 6.5 &lt;&gt; FgDefault "!"` &amp;nbsp; `ToEscapable` simply determines the default "template" that your type receives when adding it to an expression. I have some examples in my README in the Github repo that are more advanced than this simple one.
We actually moved to purescript after starting development with react-flux. The main reason was a large footprint, somewhat slower startup times and the app would not load on IE9. I know IE9 support is not important for most projects. But we have a lot of enterprise customers with very conservative IT departments that still have xp. We are using thermite framework built on top of reactjs. The reason we chose it is because the development process is practically the same we already learned from reactjs and from react-flux. And we get to reuse all the third party components (react-bootstrap) 
Codeblocks with the same background as the rest of the slide are _suboptimal_ for readability. PS: I really don't think the slide form per se is really working for something like that, but that's just my opinion.
For what it’s worth, I find your presentation clearer. When teaching this to coworkers, I’ve also introduced `let` to show how the substitution proceeds: (λabc.cba)zz(λwv.w) (let a = z in λbc.cba)z(λwv.w) (let a = z; b = z in λc.cba)(λwv.w) (let a = z; b = z; c = λwv.w in cba) (let a = z; b = z in (λwv.w)ba) (let a = z in (λwv.w)za) (λwv.w)zz (let w = z in λv.w)z (let w = z; v = z in w) (let w = z in w) z And some people find this version more enlightening. You never know what’s going to make it click. Edit: here’s a higher-level way to think about this: `λabc.cba` is a Church-encoded flipped pair. It stores `a` and `b` in its closure, and you can access these by passing in the appropriate `c`. `λwv.w` deconstructs the pair to get the first element—`b`, because the pair is flipped. So `(λabc.cba)qz` represents `(z, q)`, and `(λabc.cba)qz(λwv.w)` is `fst (z, q)`, which is clearly `z`. 
Thanks for that, fixed. Will have to remember that next time I try to share a youtube link from my phone... :)
Is that safe to use `unsafeFreeze` with `ST` monad?
What makes you say that? I recently interviewed at a bunch of those, and the technical interviews really were about my ability to write Haskell code, and the discussions I have had about the roles made it quite clear that their codebases are, indeed, written in Haskell.
This is cool stuff. Thanks for open sourcing it. Now if someone could finish [Arcanist Lite](https://mail.haskell.org/pipermail/ghc-devs/2015-September/009827.html) so that contributors to GHC don't have to install the right PHP version before talking with Phabricator, these two would improve the toolbelt of most Haskellers.
I disagree; I think that this is a good choice for stack, and one that follows the general principles of tool in general. stack is a tool that uses a *predetermined* set of packages, or a distribution, that is known to work together. When you add an item to `packages`, i.e. an additional package to the `stack.yaml`, you are declaring a package to pull from with a higher precedence than stack's distribution. In other words, when building with stack, it will first download all the packages listed in `packages` to obtain their respective Cabal files. When one of them is required by the package being built, then work out the requirements of that dependency. If any package is not available in (1) the list of packages obtained or (2) the stack distribution, then an error is reported. The reason that stack is failing is due to the fact that the package it requires (a) has not been added to the `packages` section of `stack.yaml` so stack doesn't know where it is located or more importantly which version to use, and also (b) it is not in the stack distribution.
If you want more examples, I made a naive implementation of Hearthstone card search app months ago using Reflex and materialize CSS framework. It needs to download a relatively big js file containing all the card data and parse it, so it might appear to be stuck at loading. In the code I need to import some jquery function via FFI in order to correctly inform materialize to update the UI. * the module relating to reflex [here](https://github.com/Frefreak/hearthstone-cardsearch/blob/master/client/src/Widget.hs) * [demo](http://frefreak.github.io/) Also there is also a trivial json formatter example(the source is tiny because it uses aeson-pretty to do the actual formatting) * related [code](https://github.com/Frefreak/jsonFormatter/blob/master/src/Webview.hs) * [demo](http://carso.nz/tools/jsonFormatter.html) However I'm still at the stage of beginner and the code might not be of good quality. In my experience it takes some time for the types in Reflex to become more *natural* to me. I'd recommend to get more practice and read other people's code for more advanced usage. As mentioned there's a subreddit and irc where you can ask. 
I wonder if/how this would be a problem: say we have: Underline (FgRed (toEscapeable (Just 7))) If you insist on not having the `toEscapeable` there, provide functions like: underline :: (ToEscapeable a) =&gt; a -&gt; Escapeable underline = Underline . toEscapeable and underline (fgRed (Just 7)) I believe you should certainly keep your data types dumb and functions smart. It's much cleaner that way. There's simply no point in threading typeclasses throughout your data like that. As for the `toCompStr` thing, I *would* vote against having it in the API, but say I had newtype Name = Name { getName :: String } It would be impossible for me to render `Name` in `toCompStr` like other strings.
I think that you should expose the types that you are operating on with the caveat that if you are actually using `Map Int String` like an internal data structure rather than extra utility functions for `Map` like your `Data.Foo.Utils` case, then you can instead wrap that in a newtype that you expose. I don't think there's a objectively or morally right or wrong answer. At least in `mtl` `Control.Monad.{State,Reader,...}` all re-export `Control.Monad`, `Control.Monad.Fix`, `Control.Monad.Trans`. Meanwhile in `lens`, `Data.ByteString.Lens` doesn't re-export `ByteString`, nor does `Data.Array.Lens` export `Array`, etc. which doesn't seem to have hurt its adoption.
Looks like it would be. The rule for `unsafeFreeze` and `unsafeThaw` is that you don't reuse the vector you passed to those functions past that point. I think it's reasonable not to use those functions in an example for someone who is new to those packages, though.
Thanks for the kind words. It's been exciting to see all the tooling improvements in the community the past few years, and I hope to continue contributing where I can!
That's unfortunate! For our use case it's hard to beat the flexibility of Jenkins, and it's managed to build up more inertia here than similar platforms in the space. To your last point, the longterm goal would be to split out the Jenkins REST logic into a library that can be applied to different automation tasks in the Jenkins space. That would certainly open the door for alternative backends in Bartlett itself!
Can you provide an example?
Perhaps you could call `mask` within `withFoo` and call release also within `withFoo`, just before ceding control to the callback. Like `bracket` does: bracket before after thing = mask $ \restore -&gt; do a &lt;- before r &lt;- restore (thing a) `onException` after a _ &lt;- after a return r
It *is* very easy. It's basically just `join`/`&gt;&gt;=`, but it's very hard to convey to someone who has no experience with abstractions like these why exactly they are so useful. That's why I keep telling people who try to learn Haskell that they shouldn't worry about it too much and just learn to *use* it before trying to really understand the ins and outs of monads. It's a lot like learning maths, you just need to see and do a lot of examples in order to build some intuition for it.
You're right. I was being lazy towards the end of the slides.
What does "enforce" mean in this domain? I assume something crude like System.Timeout is not what you mean. 
Oh, I was expecting something like Control.Monad.Fear, the Fear Monad.
IMHO, becoming proficient in Haskell *does* imply a reasonable understanding of monads (and a few other concepts, such as functors and applicatives). The big mistake is in trying to teach them upfront, with a lot of dubious analogies and bad category-theoretical explanations, before the beginners have the opportunity to forge much practical / intuitive understanding. 
Is this just an advert for unison.cloud?
What's so wrong with Docker? It does a great job for enough simple use cases. I like a uniform interface for deploying apps built on different tech stacks (e.g. I can write something in Haskell and another in Java, but both are deployed the same).
&gt; ['10', '10', '10'].map(parseInt) It turns into [parseInt('10',0), parseInt('10',1), parseInt('10',2)]
Link broken: https://github.com/Akii/acme-fucks/blob/master/src/Acme/Fucks.hs Base repo link: https://github.com/Akii/acme-fucks
[Here](https://agda.readthedocs.io/en/latest/language/record-types.html#instance-fields)
BroadcastChan is `newBroadcastTChanIO`, but using `MVar`s instead of `TVar`s (and a trivial wrapper for type safety). It has all the same trade-offs that `Chan` vs `TChan` does and is, all it does is fix the space leak you end up with should you attempt to use `Chan` in the same way you'd use a broadcast `TChan`, does that unconfuse you?
Why are people so unwilling to admit that Haskell is difficult to learn? *Programming* is difficult to learn, and Haskell is a very different kind of programming. It should be no exception. We can't pretend that learning Java or Python is any more trivial than learning Haskell. It's hard. I think it's natural that such a paradigm shift is going to remain hard to those familiar with other paradigms. No, you don't *need* to understand monads to use Haskell, but you're going to have a much better time if you do. Anyone who learns Haskell thinking that `IO` is special, rather than a part of some bigger design (monad), is going to assume that `IO` is the only way to do too many things. They will use `IORef` instead of the `State` monad. They'll use `Chan` instead of a streaming monad. Monads are *essential* to being good at Haskell. You don't need to learn all about monads to *start* learning Haskell, but if it's not somewhere on your learning track, be it early or late, you're going to be missing one of the most important concepts the language has to offer. To me, saying you don't need to understand monads is like telling a Java learner they don't need to understand inheritance. You can get away without it, but it's so important to the language and the ecosystem, it would be a mistake to try. EDIT: Now, that said, I agree with the takeaway here: `IO` is not how we should introduce `Monad`. But it does need to be introduced sooner rather than later. Maybe as a more intermediate concept, but definitely not as some abstract nonsense that no one needs to learn.
I don't understand how it works, though.
This was the best explanation of the issue I've seen so far, and also the shortest. Maybe it helps to stop people calling Haskells IO the *IO monad*. The first problem with monads is that the abstraction is just too abstract for most. In simple terms, it's really just a guarantee that `&gt;&gt;=` and `pure` *behave in the least surprising way*, but this isn't easy see from the monad laws alone. Beginners don't need to know this to work with the respective `&gt;&gt;=` and `pure` functions. The abstraction is also too general in the sense that there isn't much you can do with a monad instance alone, except string monadic values together in the most obvious ways via `sequence`, `mapM`, and so forth. It doesn't become interesting until you have concrete type classes like `MonadRandom` or `MonadState`.
It's an interesting thing to study. The kind of things you wake up understanding after a very long period of thick fog. And you don't even know why you now see ... Says something about the brain inner workings IMO.
It was the spookiest monad I could think of.
But monads are easy right? So easy people think they are missing something?
To also be fair, I mentioned these specifically because they were problems for me. Copy and move semantics hit me relatively early last time I tried to write a C++11 program with `shared_ptr` and `unique_ptr`. And what I said about Python is the reason I can't ever really trust lambdas and inner functions to behave in a sane way. You can of course escape these issues by not using them, but that makes them no less part of the languages "core". In that sense monads are not "core" to the language either. You don't need them. A simplified base library could just export `bind :: IO a -&gt; (a -&gt; IO b) -&gt; IO b` and `pure :: a -&gt; IO a`, even forget about do-notation, and be done with it. No need to care about the fact that `(IO, pure/return, bind/join)` forms a monad.
I thought it was going to be a movie.
I realize you can write Haskell without monads and do notation, but it seems more integral to the language in the fact that you're not going to get nearly as far without bumping into the concept. I'm still definitely a Haskell beginner (especially compared to C++ or Python) so I guess I still find monads "harder" (or at least much more abstract). I also feel like I've been grappling with them for quite a bit longer than I had to grapple with either of your examples.
If I recall correctly there was a syntax proposal at some point that would have allowed us to write `Just do it`
The presentation talks about exactly that. Monads are an abstract algebraic property that beginners unnecessarily bump into too early and for the wrong reasons. That is a community related problem. About whether monads are more difficult than the other is maybe a matter of taste. Unlike monads, unintuitive scoping can't be understood, only remembered and hopefully avoided.
It introduces a type to describe interactive computations, which is coincidentally called IO. It doesn't introduce it as an example of a monad, because it's not really relevant at that stage. What's relevant is that you can use a type to describe an interactive computation. I was going to introduce IO later, but having the ability to write interactive programs gives far more possibilities for how to use dependent types in more realistic contexts. For example, reading some input and checking that it's valid before converting it to a more precise type.
&gt; You can write an awful lot of code in C++ without being aware of move semantics at all, and those odd scoping rules in Python are something of an edge-case I would actually disagree here. I mean, if your C++ code is just glue tying different libraries together, then sure. But as soon as you start making your own classes you're very likely to run into these issues. Likewise, in the python case, the SO entry seemed to be a beginner who ran into some surprising behavior.
This. Finally someone who understands.
&gt; For some people Haskell is out of reach. Maybe, but I'd rather believe that this isn't true. There's a simplicity and minimalism in pure functional programming that imperative programming won't ever have. If programmers can understand the unholy mess that is OOP, then why not something simpler. Most were taught functions in school and how to work with them. Regarding calculus, this reminds me of a [great blogpost](https://existentialtype.wordpress.com/2011/04/02/functions-are-values/) by Bob Harper. In the last paragraph he talks about a common misunderstanding of students in calculus, which stems from interpreting functions as "special" rather than just first-class values.
Actually it turns into [parseInt('10', 0, a), parseInt('10', 1, a), parseInt('10', 2, a)] where `a` is the original array, but `parseInt` ignores it since it takes 2 arguments.
I guess I can only talk from personal experience, but I'm working on a C++ codebase that uses (partial) C++11 features, and I know that no-one who I work with knows what move semantics are, and certainly would never write a method with `T&amp;&amp;` in the signature or use `std::move`.
There's http://hackage.haskell.org/package/stm-containers, but that's based on Hash Array Mapped Tries…
Simple /= easy, example: Dependent Types as in Idris or Agda are simpler than the type system of Haskell, yet anyone agrees that they are an advanced feature not so easy to get into. Complex /= difficult, example: graphical user interfaces are way more complex than textual ones, yet they are way more easier and intuitive to use and do not require reading manuals and fighting with parameters. Regarding languages like C++ or Scala, they can be more complex than Haskell, yet you don't need to learn all their features to become productive, and maybe most of their more advanced ones you will never need to use at all in all your programming career, that's not the case with Haskell, in which you need a good grasp of a big part of the language to start writing not trivial programs, and without counting all the extensions that are used daily that are considered standard part of the language. But that's exactly the beauty of all this, you always have to learn something new and every new concept/abstraction is a new tool that will improve you as programmer. I don't think you can say this of C++, where a new feature can be just a specific detail of the language that you will never need to use in any other part of any other project.
Ugh, I remember those OOP tutorials with car/wheel/engine all over the place. Those were weird and not very helpful.
I do not reexport types in my libraries. In addition to /u/mightbyte's argument about increasing the surface area of your API, I have another qualm with reexporting types. When you're browsing haddocks, it's harder to figure out where a type is actually defined. For example, let's say that I'm using functions from `mtl` that operate on a lazy `RWST`. From the docs for the module [Control.Monad.RWS.Lazy](http://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-RWS-Lazy.html#t:RWST), we can see the type `RWST` because it's reexported. But `RWST` isn't actually defined here. From the haddocks themselves, we have no way to figure out where it was actually defined. You have to look at the source and find it was pulled in from `Control.Monad.Trans.RWS.Lazy`. But, what package does that module belong to? If you're familiar with the ecosystem, you know that it's from `transformers`. Especially when I was new, I would run into problems trying to figure out where things were actually defined. Even now, I don't particularly like reexports. I would rather type out: import Data.Map.Strict (Map) than have it be brought in by some other import statement. Also, in your particular case, I would recommend that you consider a newtype: -- The name Session is probably not appropriate in your case. -- Pick a name that indicates what the Map represents. newtype Session = Session { getSession :: Data.Map.Map Int String} Now, your type signatures become: foo :: Int -&gt; Session baz :: Session -&gt; Bool This does a better job telling the user, "hey, there is some opaque Session type that you can create and consume with this library. It's internal representation is not important to the end user though."
Sadly, things like these only help to add more magic to the "monad" even when it explicitly says reader that "there is no magic, trust me"...
so how would you do this correctly, turn a list of strings to int how would you a function on ['10', '10', '10'] and return [10, 10, 10]
&gt; Simple /= easy What makes something simple, is that it's composed of smaller, self-contained, clearly distinguishable components with relations and dependencies between them that are easy to recognize. As I see it, this is the case for pure functions, but not often for state machines. &gt; that's not the case with Haskell, in which you need a good grasp of a big part of the language to start writing not trivial programs This is simply not true. I wrote a very simple SVG renderer after the frist few chapters of LYAH and no prior Haskell experience. I've later written non-trivial programs with mostly the core libraries and no extensions. You don't need monad transformers and advanced type system features to create something in Haskell. I have worked both in C++ and Scala and none of these were any easier to learn. C++ especially has a ton of hidden complexity that you always need to be aware of. Everyone can write some simple toy in C++, but knowing all the intricacies of C++ takes a real expert and years of experience. Most of which are things I can barely use in any other language. I bet that functors will still show up in new programming languages in 20 years, `const_cast` and `reinterpret_cast` probably not so much. Also, the Haskell language itself is really tiny. There is a simple module system, variables, literals (strings and numbers), let expressions, where clauses, case distinctions, if-then-else, and the ability to make your own infix operators. Then data types and type aliases. That's basically it. Neither IO nor basic strictness annotations are actually complicated and once you are there, is there really anything that can't be done?
As /u/TarMil demonstrated, I don't even know how to use this function. I usually reach for lazy.js ...
Functor is such a scary name ... I found this "in picture" tutorial great to explain the concrete usage (and not the theoretical mindblow): "Functors, Applicative and Monads in pictures" http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html
Monads are a circular definition: you need monads in order to make monads practical. They capture only half of the common sense. Observe: `m a -&gt; (a -&gt; m b) -&gt; m b` "If I have a thing that is wrapped, and I have a function that performs an operation and wraps the result, then it shouldn't double-wrap if I apply it to a wrapped input." "Ok. But couldn't you just... not wrap things in the first place?" Don't just show me why you've made wrapping less of a hassle, show me why _not wrapping_ would be worse. And btw, even just translating that formula into the words above is a thing Haskell programmers consistently refuse to do, which only increases the barrier. Especially if m, a and b are abstract instead of concrete examples.They are unable to read the meaning in the notation. 
I find it hard to believe anyone could get very far with Haskell by muddling through without understanding what's going on. By the way, as far as Monad explanations, this is the best approach that I've seen. It's only too bad it's in video form: https://www.youtube.com/watch?v=ZhuHCtR3xq8#t=40m
Here's the issue: http://imgur.com/a/bYmg9 A "TypeError" in JS, figure that! Use GHCJS next time ;)
&gt; There's a simplicity and minimalism in pure functional programming that imperative programming won't ever have. In a sense, the higher level of abstraction makes things "simpler." The same is true about higher math, which makes certain things simpler to accomplish. That's why these things are useful and used. But it's not simpler for the brain. Or I suppose the best way to put it is: the brain required isn't simpler! It's similar to how the Haskell compiler is not simple, it's extraordinarily complex, even in the measurable sense. That's what it takes (for software) to understand the language. You don't need a Turing-complete typechecker to interpret Python, no matter how weird the scoping rules are. &gt; Most were taught functions in school &gt; [link to a post claiming that most students of calculus fail to understand what functions even are] Hm.
&gt; What makes something simple, is that it's composed of smaller, self-contained, clearly distinguishable components with relations and dependencies between them that are easy to recognize. As I see it, this is the case for pure functions, but not often for state machines. And this does not imply in any way that pure functions are always easier to understand than state machines. &gt; This is simply not true. I wrote a very simple SVG renderer after the frist few chapters of LYAH and no prior Haskell experience. I've later written non-trivial programs with mostly the core libraries and no extensions. You don't need monad transformers and advanced type system features to create something in Haskell. That's exactly the opposite of my experience, after reading various books regarding Haskell I started to grasp the language and started to write some non-trivial programs. That was simply not the case with C++ in which I was able to work to some non-trivial programs very quickly, without knowing much of the language, it's intricacies etc... (of course that those programs were actually completely corrects is another thing :-) ) &gt; Everyone can write some simple toy in C++, but knowing all the intricacies of C++ takes a real expert and years of experience. I agree, but no one need to learn all the intricacies of C++, and no one does. &gt; Also, the Haskell language itself is really tiny. There is a simple module system, variables, literals (strings and numbers), let expressions, where clauses, case distinctions, if-then-else, and the ability to make your own infix operators. Then data types and type aliases. That's basically it. Yes, the language, but the concepts around it are another thing, look at lens, pipes, FRP... Also monad as concept is really simple, yet it's its abstractness that makes it so difficult to grasp. C++ as language is more complex, but it's parts/features are not so abstract and difficult to learn, it's just that there are so many of them that it takes years to learn them all. But you don't need so many years of study to become productive. Haskell can be considered simpler, but the concepts around it are more difficult to grasp and more abstract, otherwise there would be not so many monads tutorials.
Almost everything from GHC 7.10.3. The only notable exception being TemplateHaskell.
My example is a bit bad, as it is not necessarily an opaque type, it might be a type that you can feed to functions from another library. Consider `ByteString`: I might not want to manipulate these, but pass them between independent libraries that happen to provide an API based on `ByteString`.
You need to be careful though. You inevitably reexport e.g. typeclass instances, and need to be very conservative (i.e. use minor version bounds) to protect the users of your API against any observable changes in the exposed typeclass instances, as otherwise a consumer depending on `build-depends: package-reexporting-opaque-types == x.y.z.*` may easily get into a situation where that version bound does not accurately describe the depended upon API anymore. See also [this PVP FAQ entry about API-reexports](https://pvp.haskell.org/faq/#what-implications-does-the-pvp-have-when-re-exporting-api-elements).
You wrap parseInt in a lambda so that you control the arguments and can match them correctly to the function you're wrapping. You don't care about the index (2nd arg) or the array (3rd arg), so you can do this: ['10','10','10'].map((x) =&gt; parseInt(x)) Or you can do like /u/tnonee did and use `+x` instead of `parseInt(x)`, but that's (IMO) less obvious and readable due to how it's (ab)using Javascript's type coercion rules. Maybe I'm just paranoid, but I'd be reluctant to rely on the behaviour of `+ '10'` coercing to a number when the same coercion rules also make `- + '10'` return `-10`, and `0 + '10'` return `'010'`.
That’s a valid point. Bummer.
Thanks! If you run into any issues or have any questions, feel free to contact me at the e-mail address on the Hackage page or open an issue on the Github page. Can't wait to try your game! 
Even for things like `ByteString`, my preference is to not reexport it. That's probably because I almost always qualify imports or specify an import list for them. The only time I use an unqualified wildcard import is if I'm really familiar with the module (something like `Control.Monad` or `Data.Hashable`). So, I typically don't end up using the reexports that are provided by other people's libraries. But, I can totally understand why you would want to reexport something like `ByteString`.
Because, when people say "Haskell is hard", they often actually saying one of "Haskell is inherently harder than other languages" or "Haskell is too hard to be worth learning". People believe this pretty strongly too: it came up back in college when talking about why we wouldn't teach Haskell in undergrad courses, for example.
If you want to allay the concerns of the natives here, I'd recommend making time-share fractions explicit for each language. e.g. for my job it's: 90% Haskell, 10% ansible fuckery Is this gig: 10% Haskell, 90% Java 50% Haskell, 50% Java, etc. People understand things fluctuate based on project needs but if you won't offer even a rough magnitude they won't trust you.
This is profoundly arrogant. It's not a cognitive barrier. It's a lack of quality documentation, and a heavy reliance on academic nomenclature. I have seen maybe two Haskell modules, ever, that had an actual English language preamble with code examples illustrating what a module is actually for, and how to use it. Many have short preambles with crap like : this is a class of Monad transformers to bla bla applicative blah blah contravariant bifunctors blah blah. 95% of the available written words on how to do anything assume you know a very specific set of nomenclature, and basically every piece of documentation you can find that is actually readable to a mortal just tells you that you shouldn't try to understand, just use it till it clicks. Which would be great advice, if you can find actual examples of people using it to do something that isn't some weird academic pet project. Most people who are teaching themselves a language are going to learn by practical example, and it's a huge pain to find practical example code for this language. That, combined with a set of nomenclature that the average programmer is not familiar with, is why so many people bounce off this language. And frankly, it's much the same way with math. I was utterly terrible with Algebra until I took physics, and my exposure to Trig and Calculus was about the same - it just didn't make sense with crappy toy problems, so there was no frame of reference for my brain to apply the concept to, and it all went in one ear and out the other. As soon as I realized how to apply principles to problems I could recognize, I understood the concepts almost immediately and proceeded to out perform my peers by a wide margin. That's a "hardware" limitation, it's a lack of approachable instruction.
&gt; And this does not imply in any way that pure functions are always easier to understand than state machines. You're right, they aren't always easier to understand. But they are easier to compose and refactor. &gt; That's exactly the opposite of my experience, I think it took me almost no time getting started with C++ either, thanks to already knowing Java quite well, which is not much different than C++. But Java was my first programming language and it took me a long, long time to get from *Hello World!* to something useful by myself. At least as long, if not longer, than learning Haskell took me. &gt; Haskell can be considered simpler, but the concepts around it are more difficult to grasp and more abstract, otherwise there would be not so many monads tutorials. And many of the monad tutorials get monads wrong, precisely because their authors try to shoehorn an abstract algebraic property into some metaphor. This urge to try and take every abstraction apart is what makes it so difficult for many to get them.
No, but I am also smart enough to not make it any harder for me than it needs to be. It takes a lazy programmer to really appreciate a lazy language.
Right, just often not a good one :-) (E.g. if you want your code to be `-Wall` clean).
I presume that most people coming to Haskell have learned a first language. And since most programmers seem to find Python "easy" and it's ubiquitous, I think it would help the Haskell ecosystem to have a document somewhere that gives examples of how Python code could/would be translated to Haskell in a way that is idiomatic. It wouldn't even necessarily have to improve on the Python code, but of course, some examples where Haskell is (perhaps much) more elegant (I specifically am refraining from using the word "efficient"), "safe" in a way that reduces bugs/errors, and re-usable or more abstract. Who knows, maybe one day I'll write up that document. :)
basically, haskelly should always search local project installation first and than try global?
And Monoid is 'one' plus 'like', which means...well I don't know what that means either.
Like this? http://rosettacode.org/wiki/Haskell
Subhask uses something like this but only with ($), because this extension wasn't available. Essentially you want to be able to apply concrete categories, ones which have faithful functors to Set, which is approximated by the function space category.
How would you implement that unambiguously and without other syntactic overhead?
&gt; My use case for overloaded application is from singletons, where I want users to be able to apply something of type `Sing (x :: k1 -&gt; k2)`. Right now, they must call `applySing` to do the application. &gt; &gt; — /u/goldfirere in [email](https://mail.haskell.org/pipermail/haskell-cafe/2017-January/126072.html) Will this work with this proposal, how can we reference the existential type `x`? applySing :: Sing f -&gt; Sing x -&gt; Sing (f @@ x) instance IsFunction (Sing f) where type Domain (Sing f) = Sing ??? type Codomain (Sing f) = Sing (f @@ ???) same problem with encodings of natural transformations like in [*hask*](https://github.com/ekmett/hask/blob/cd4d30e7911dd7cc2da78383fd833272b1ff9303/src/Hask/Category.hs#L100) or newtype Nat f g = Nat (forall a. f a -&gt; g a) How can they be supported? ---- I am skeptical but I see some interesting cases (data types wrapping functions) newtype Rec a = In (Rec a -&gt; a) instance IsFunction (Rec a) where type Domain (Rec a) = Rec a type Codomain (Rec a) = a applyFunction :: Rec a -&gt; Rec a -&gt; a applyFunction (In f) = f # Expressions data Exp a = Var a | App (Exp a) (Exp a) | Lam (Scope () Exp a) (·) = App I use a minimal notation for application (`·`) but with this proposal I could write instance IsFunction (Exp a) where type Domain (Exp a) = Exp a type Codomain (Exp a) = Exp a applyFunction :: Exp a -&gt; Exp a -&gt; Exp a applyFunction = (·) # From [*reactor*](https://hackage.haskell.org/package/reactor) data Observer a = Observer { (!) :: a -&gt; Task () , handle :: SomeException -&gt; Task () , complete :: Task () } instance IsFunction (Observer a) where type Domain (Observer a) = a type Codomain (Observer a) = Task () applyFunction :: Observer a -&gt; a -&gt; Task () applyFunction = (!) # `Op` from [`Data.Functor.Contravariant`](https://hackage.haskell.org/package/contravariant-1.4/docs/Data-Functor-Contravariant.html) newtype Op b a = Op (a -&gt; b) instance IsFunction (Op b a) where type Domain (Op b a) = a type Codomain (Op b a) = b applyFunction :: Op b a -&gt; (a -&gt; b) applyFunction (Op f) = f # `Endo` from [`Data.Monoid`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Monoid.html) newtype Endo a = Endo (a -&gt; a) instance IsFunction (Endo a) where type Domain (Endo a) = a type Codomain (Endo a) = a applyFunction :: Endo a -&gt; (a -&gt; a) applyFunction (Endo f) = f # `Cont` from [`Control.Monad.Cont`](https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-Cont.html) newtype ContT r m a = ContT ((a -&gt; m r) -&gt; m r) instance IsFunction (ContT r m a) where type Domain (ContT r m a) = a -&gt; m r type Codomain (ContT r m a) = m r applyFunction :: ContT r m a -&gt; ((a -&gt; m r) -&gt; m r) applyFunction (ContT cont) = cont and same with [`Search`](https://hackage.haskell.org/package/search-0.1.0.1/docs/Data-Search.html).
The most important section is still missing: why would we want this feature? What code would be particularly elegant with this? Wouldn't a custom operator do just as well? 
HTTP2 client
Yea I'm just thinking about the simple case like this: foo &lt;$&gt; bar &lt;*&gt; baz If we had this: instance (Applicative f, IsFunction x) =&gt; IsFunction (f x) where ... &gt; foo bar baz I can imagine there would be all kinds of undecidable instance and inference problems. And once you need any syntactic overhead for any of that, it starts to defeat the purpose of not having idiom brackets
Nice APIs for everything! For example,I think I recently read a comment by someone who complained that he couldn't find a nice library for sending email via Gmail…
I'm also interested in which Haskell webserver is faster than, say, nginx. 
I'm trying to come up with an example but I'd imagine you'd wrap up `Applicative` in a newtype and your idiom brackets would essentially become wrapping and unwrapping the newtype, with overloaded function application taking over inside.
[Here's some](https://www.reddit.com/r/haskell/comments/5q51hl/proposal_overloaded_application/dcwjmhi/) I'm more concerned about the impact on type inference, particularly constraint propagation from arguments to functions. Right now if we have the expression "f x" we get constraints `x :: a` and `f :: a -&gt; b` where `a` and `b` are new type variables. With this proposal that is weakened to `x :: a`, `IsFunction f`, `Domain f ~ a`. I think the impact will generally be minimal, but it would be interesting data to see if there's *anywhere* on hackage where enabling this extension would cause ambiguous type errors.
I think that you have to provide logical reasons for developers why to choose haskell. I was never really interested in haskell, because i thought that other programming languages are much easier, but other programming languages are also much "buggier". Haskell, like all other languages, has advantages and disadvantages. The main advantage is its stability (in my opinion), if it compiles it works. Haskellers should sell these easy understandable arguments (e.g. pure fp =&gt; rock solid programming) and not try to convince people that haskell is as easy as other languages, even if this is the case.
DSP
&gt; &gt; You can write an awful lot of code in C++ without being aware of move semantics at all &gt; I would actually disagree here. I occasionally work on a largeish C++ codebase that doesn't even use a C++ compiler. No one is worried about move sematics, I assure you. The code is not valid C, so it's called C++. In general, it is valid C++, but I haven't audited the code base (too large). Unfortunately, the compiler that we are stuck with (because of an odd syscall convention of the OS) predates the C++98 standard, is missing most of the standard library, doesn't handle partial template specialization well, and doesn't follow SFINAE (among other things) which makes it difficult to implement the parts of the standard (C++98 OR C++11) library that I want. I feel there are a lot of *professional* C, C++, Python, and Java programmers that haven't tackled the "weird" parts of their languages. [10 years ago](https://blog.codinghorror.com/why-cant-programmers-program/) most *professional* programmers couldn't write code. Having provided the technical aspects of more than a dozen interviews, I have to let you know this is also true in 2017.
I'm really excited to see efforts like this to improve Haskell documentation! But accepting documentation patches on GitHub will only be effective if people know about it. So, it might be a good idea to change the `source repository` field of the `base` package Cabal config (https://hackage.haskell.org/package/base-4.9.1.0) to point to GitHub. Right now it points to http://git.haskell.org/ghc.git, which has no mention of GitHub.
I'm having trouble finding what the current LTS scope is. How long term is the LTS support? 
There are no existing solutions. Currently the only way to access Ms Sql Server is through hdbc-odbc lib. And it is broken/buggy depending on what platform (windows/linux) you use it and with which specific drivers (freetds, MS driver etc) 
In every degree I've done, 40% has been the pass rate of every exam.
Takt - (http://takt.com) - SF, Seattle, Remote - Hiring!
https://www.stytch.com in Vancouver/Canada They have quite a large Haskell source base.
From what I can tell, this is one of those extensions that can't be used everywhere. (Note that function application is defined using function application.) This means we would still need a way to indicate where it is being used if overloaded application became an official part of some Haskell 20XX. Maybe a way to indicate "in this block of code, the following types may be used as functions"? That would make it clear to the reader that something is happening and still avoid accidentally applying the wrong thing.
There was an experience report on implementing http2 in Warp by Kazuhiko Yamamoto presented at icfp'16: http://icfp16.sigplan.org/event/haskellsymp-2016-papers-experience-report-developing-high-performance-http-2-server-in-haskell https://hackage.haskell.org/package/warp e: argh you said client :( Well, shit, I thought the report was interesting so I'm leaving my comment up
If you wanted to do this, I'd suggest instead just getting rid of your import Data.Map (Map) and adding a line type Map a b = Map.Map a b You could come up with a different name for the type name if you'd like. And then you're halfway to making a newtype, which you may find useful. These sorts of insertions also help with the idea of types as documentation. It also means that you could hide an underlying HashMap, or a change from Lazy to Strict. 
In my opinion, GitHub is pretty lacking for issue tracking. For example, people who haven't contributed to the project before can't even be assigned tickets. I don't mean assign it to themselves, I mean can not have tickets assigned to them. I also find it difficult to keep track of comments in your pull requests and figuring out which ones you already replied to/completed. That being said, I'm curious what benefits of Github there are which would make you switch to git to use it?
I agree that it's not perfect. I prefer the code review system where I work over Github pull requests. But there's a lot of momentum behind Github, and it's overall pretty nice.
Let's figure it out! haskell.org's Hoogle [doesn't know](https://www.haskell.org/hoogle/?hoogle=HaskellCallbackType) about `HaskellCallbackType`, but Stackage's Hoogle [does](https://www.stackage.org/lts-7.16/hoogle?q=HaskellCallbackType). It's in "haskell-gi-base-0.18.4: Foundation for libraries generated by haskell-gi". Okay, so let's check out [haskell-gi](https://github.com/haskell-gi/haskell-gi#readme). "haskell-gi: Generate Haskell bindings for GObject Introspection capable libraries". Okay, so that explains why the types are so huge: they were written by a machine, not a human. Anyway, this clearly comes from the gi-gtk side, not reactive-banana, so the constraint must come from the line `on object signal fire`. Let's look at the type of [`on`](https://www.stackage.org/haddock/lts-7.16/haskell-gi-base-0.18.4/Data-GI-Base-Signals.html#v:on): on :: forall object info m. (GObject object, MonadIO m, SignalInfo info) =&gt; object -&gt; SignalProxy object info -&gt; HaskellCallbackType info -&gt; m SignalHandlerId It mentions `HaskellCallbackType info` right there, it's the type of the third argument. You used `fire` for that argument, and `fire` is the second result of [`newAddHandler`](http://hackage.haskell.org/package/reactive-banana-1.1.0.1/docs/Control-Event-Handler.html#v:newAddHandler): type Handler a = a -&gt; IO () newAddHandler :: IO (AddHandler a, Handler a) So, the `HaskellCallbackType info ~ (a -&gt; IO ())` constraint is quite straightforward: you were asked for an argument of type `HaskellCallbackType info`, but you gave it an argument of type `a -&gt; IO ()`, so it's only possible to call your function when the associated type `HaskellCallbackType info` is `a -&gt; IO ()`. You should be able to simplify the type by choosing a concrete `info` for which this is the case. Let's now look at `propEvent`. First of all, the "Unknown attribute" stuff is clearly text intended to be used as a custom error message, it would be displayed as an error if the constraint was `TypeError "foo"` instead of `FindElement ... "foo"`. Clearly, `FindElement` is a type-level function which cannot be further evaluated until a concrete `propName` is given. `AttributeList object` looks stuck as well. So in order to simplify this type, you should pick a concrete `propName` and a concrete `object` such that the propName is an attribute of the object, whatever that means in gtk+.
That's a housekeeping changes, like updating cabal file etc. If you go to actual folder https://github.com/denisenkom/hsmssql/tree/master/Database/Mssql You'll see nothing was touched for 3 years. 
Totally agree. I'm speaking more about having advisors attached. If you propose a project likely you have need and knowledge, but not time. Leave your contact info as an advisor/mentor/reviewer. This allows those without domain knowledge to leverage ambition and experience.
It's quite possible, but what we have is already pretty noisy. f &lt;$&gt; a &lt;*&gt; b could become applic f a b not to be confused with `liftA2`, since this would have the same advantage as `&lt;*&gt;` in working for all arities.
A number literal like `42` is *not* an integer. It actually has type: 42 :: (Num r) =&gt; r If you read [this](http://hackage.haskell.org/package/base-4.9.1.0/docs/Prelude.html#v:fromInteger) it says &gt; Conversion from an Integer. An integer literal represents the application of the function fromInteger to the appropriate value of type Integer, so such literals have type (Num a) =&gt; a. Our dear `42` is such a literal. Basically it stands for `fromInteger voodoo42` where `voodoo42` is some compiler-internal magic value of type `Integer`. Haskell does this because it (purposefully) omits conversion. For example, if you see this in C++: (1 / 2) * 3.77 You are going to cry out because the first `/` works on `int`s so the result is really zero. Not in Haskell: ghci&gt; (1/2) * 3.77 :: Double 1.885 Hmm... Why? A different process is going on! In fact, all these literals have type `Double` thanks to type inference (*not* conversion), and `/` works on `Double`s there! Come back to the main question. When you call `map` with that: map :: (a -&gt; b) -&gt; [a] -&gt; [b] The compiler happily sees that, well, `r` should be `a -&gt; b`. But `r` has to be of typeclass `Num`, which gives: map 42 :: (Num (a -&gt; b)) =&gt; [a] -&gt; [b] This is very important: there's *nothing* special with function types, i.e. `-&gt;` types, here. They participate in this type inference thing the same way as normal types. Which is pretty valid.... Or maybe not. There isn't an instance for that, so you can't actually use it. Which kind of makes sense. *If* we make an instance, that becomes valid actually -- I'm leaving out a lot other methods here, just done fromInteger so you can actually see something ghci&gt; instance Num b =&gt; Num (a -&gt; b) where fromInteger x = \_ -&gt; fromInteger x -- Now literals are functions too! ghci&gt; 42 3 42 -- Now it works pretty well! ghci&gt; map 42 [1,2,3,4,5] [42,42,42,42,42] But don't, ever, do this in 'real' code. You probably want to learn more about Haskell's type system. I suppose all the Haskell tutorials include a section or even chapter on that. Edit: Something was indeed fishy. It used to say ghci&gt; 42 3 3 Thanks to /u/Darwin226
&gt;why we wouldn't teach Haskell in undergrad courses, for example. wtf
&gt;I occasionally work on a largeish C++ codebase that doesn't even use a C++ compiler. Surely you can agree that your experience here is a bit of an outlier? &gt;Having provided the technical aspects of more than a dozen interviews, I have to let you know this is also true in 2017. Oh I don't disagree but that's not the topic under discussion here. If those people haven't mastered C++/Python then it's no surprise they haven't mastered Haskell either. But if we limit our discussion to intermediate skill level and above, I think the sentiment expressed above is correct: monad's are no more difficult to grasp than some of the complex things a C++/Python programmer will run into.
I think the main mistake in this discussion is trying to order human brains on a scale of intelligence. Every mind works differently, some people find it easier to learn top-down and others bottom-up. It seems to me that Haskell attracts top-down folks more, so the community at large values abstractions over examples. I see the same pattern with my own interactions with others. When they ask me something, I start talking about the issue in the abstract, and by the time I get down to concrete details, they've already stopped listening to me :) That's something I'm trying to fix with myself, so I guess it'd be most constructive if we tried to fix it as a community as well.
Well, I'm not talking about either of the cases you mention. I'm talking about [this](http://stackoverflow.com/questions/3106110/what-are-move-semantics). I learned about it from Meyer's "more effective C++" but it's something I ran into well before that due to bugs. I wouldn't have known the name everyone used for it and I didn't know the rule of 3 but I definitely knew about this gotcha because it had hurt me before.
I think that's an important point. A few days ago a colleague of mine asked me whether it would be a good idea to imitate purely functional programming in PHP. My response was, if you tried to imitate it to the bone, your code would be extremely inefficient (of course FP is still good as a guide for large-scale architectural design of your program). Haskell, thanks to GHC's optimizations makes purely functional programming applicable to real problems.
I'm not familiar with how Haskell releases work, will a point release cause breakage in libraries? Or just possible bugs due to ghc fixes?
I have been thinking about doing a comonadic DSP/filters library. Still playing with ideas at the moment though.
Why is haskell example so long compared to others? Can it be reduced ?
No, go ahead
In general function application goes like `f 1 2 3`, but when an operator consists of symbols come in, the compiler treats it differently, and it actually rewrites `a + b` into `+ a b`. You are allowed to do that in Haskell code if you put it like `(+) a b`.
Something's fishy with that GHCi session. `42 3` should be 42
great example!
Hey! Haskelly uses Stack so all the package dependencies are managed by it. As far as I know Stack first uses the local installation and then defaults to global ones.
If that command works, you'll know that your project is compatible with GHC-8.0.2 and the libraries in the most recent nightly snapshot (module bugs that don't cause the build or testsuite to fail). Stack won't send you an email, but most Continuous Integration services will, if use them to run that command and the command fails.
Yes, I think there is some misunderstanding from OPs side. Nice extension btw. can't wait till it becomes usable on windows ;D
What's so great about fixed point math? metafont has it's own fixed point math implementation, but sometimes its lack of accuracy can be a problem. Double is fast, is implemented (almost) everywhere, and has plenty of accuracy.
Integer math is universally reproducible across operating systems, compilers and CPU architectures. Floating point math isn't (in theory it could be, but practical issues conspire to make it not so). If you want truly deterministic and widely reproducible physics simulation you basically have to use fixed point.
This is one of my peeves about Haskell, by the way. I think infix operations should always be either associative (both ways yield the same result) or fully parenthesized, otherwise they will confuse beginners. Writing `1/2/3` in arithmetic is not okay, why should `f x y` be okay?
Hum, you confusion is with conflating infix operators with functions. `(+)` is an infix operator, so it's perfectly valid to write `1 + 2`, but a function that isn't an infix, like `f`, has to have its parameters proceed it, eg, `f 1 2`. Functions are not necessarily infix operators, but infix operators are functions. Left associativity is a basic property of functions/operators. Whereby: ```1 + 2 + 3 = (1 + 2) + 3``` But not necessarily: ```1 + 2 + 3 = 1 + (2 + 3)``` _Ignoring the part where addition is associative_ (ie, both left and right associative)_._ Next up is your example, why the lambda and the dot? ```λx. x + 4``` That's a lambda expression if I am not wrong, breaking it down we can say that ```λ &lt;The Head, containing bound variables&gt; . &lt;An Expression which may or man not have said bound variables&gt;``` We can express it equivalently as ```f x = x + 4``` And with this, we can explain why function application is left associative. It's a consequence of Haskell using lambda terms as the basis of its syntax. Application is the left-associative juxtaposition of a function and its parameters, where the parameters proceed the function they are being applied to. ```(λxy. x + y) 4 5 = ((λxy. x + y) 4) 5 = (λy. 4 + y) 5 = 4 + 5 = 9``` Though in more idiomatic pseudocode, it looks more like ```map f [1,2,3] = (map f) [1,2,3]``` Which gives us easy partial application of functions. Though if you are wondering why we have `(+2)`, that's an interesting consequence of how parameters are applied to infix operators. I think it's usually referred to as sections, though. Needless to say, `(2/) /= (/2)`, 2 divided by some number is not the same as some number divided by 2. This is allowed because it's sometimes convenient. Especially when you learn that surrounding a function name in back ticks makes that function an impromptu infix operator, then we can very easily partially apply the second arguement to the function. ```(λx. x / 2) = (/2)``` ```(λx. 2 / x) = (2/)```
Anyone interested in Haskell should just read the Haskell Programming from First Principles.
I have a personal hypothesis that it's not that monads are hard, it's just that introduction to monads is usually bundled with introduction to higher-kind polymorphism. I mean, it's not like the other type signatures from the [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia) are any less scary. I remember that seeing functions where every single type is polymorphic, and the polymorphic types are themselves parametrized with other polymorphic types, made it hard for me to imagine what the code actually is meant to do.
To be pedantic, you can implement software floating point which is completely deterministic and reproducible. EDIT: it is also reproducible if it is executed on the same processor, with the same instructions.
I have it in a package here: https://hackage.haskell.org/package/exists but I haven't touched it in a long time. I'd be happy to transfer ownership to anyone who wants it. Also, there is a more general way to formulate this, as I learned (if my memory is correct, which it may not be!) from Ertugrul Söylemez: data Exists f = forall a. Exists (f a) data Given c a = c a =&gt; Given c a type ThePreviousExists c = Exists (Given c) 
That's interesting. I would use such module. And, if you or anybody here pushes that into hackage, I just want to say that I like that Some name more :)
True, but software floating point is prohibitively expensive for many use cases. I recall comparing 2D physics performance between fixed-point and soft-float math on a Nintendo DS a long time ago, and the difference was something like 50x in favor of fixed-point.
I don't think that's true anymore. Floating point is a bit slower, but in fixed point math you have to do extra operations, like rescale after multiplication, etc...
This one is a bit awkward though due to the problems with constraints on datatypes (you need to always include the constraint to be able to use them). EDIT: Oops. Yeah I was confused with DatatypeContexts
~~I think that `Given` is wrong. It requires `DatatypeContexts`, which are not recommended for the reason you mention. A more useful definition would be:~~ data Given c a where Given :: c a =&gt; Given c a ~~Then it would work.~~ EDIT: nothing I said was true. Umbrall are I were thinking of DatatypeContexts, which is not what the data type `Given` uses in the example.
There hasn't been any significant change in CPUs that would make it much faster than back then; advanced branch predictors might narrow the gap somewhat but software floating point will still have to do massive amounts of work compared to fixed point. Rescaling after multiplication is literally free on many CPUs and in practice free on many others thanks to either architectural details with shifters or heavy pipelinining, and fixed-point addition and subtraction are just integer addition and subtraction. Software floating point requires a lot of branching because so many operations have the potential to change the base of your number's representation, and it's especially bad if you want to support a significant subset of IEEE floating point semantics which involve all sorts of wacky crap like denormals, NaNs, exceptions and whatnot. Edit: Actually I just remembered that recently a colleague was experimenting with deterministic online stuff for one of our physics based puzzlers (using modified Box2D) and while soft-float ran ok on a PC, the game was running at like 5FPS on an Android device, but a fixed point implementation ran at a smooth 60FPS for the same scenes. Edit 2: [Here's an implementation of floating-point addition.](https://github.com/llvm-mirror/compiler-rt/blob/master/lib/builtins/fp_add_impl.inc) With fixed-point that would be literally one instruction.
I guess it'd be nice to mention the other side of the coin; namely that instance (Monoid a, a ~ b) =&gt; PairOf a b where thePair = (mempty, mempty) will not allow the definition of *any* other instance without `OverlappingInstances`, therefore one would simple be better off defining a simple function: thePair :: Monoid a =&gt; (a, a) thePair = (mempty, mempty) Whereas with `instance Monoid a =&gt; PairOf a a`, you should still be able to define, say `PairOf Int Char`.
Author here: happy to answer any questions. Let me know your thoughts. 
&gt; I'm not familiar with how Haskell releases work, will a point release cause breakage in libraries? [This issue](https://github.com/fpco/stackage/issues/2203) shows that most "breakage" came from a major version bump in `directory`. It's not clear whether these packages were actually fundamentally incompatible with the new `directory` version, but they had a preventive upper bound to signal that compatibility with future versions of `directory` is unknown and prevent compilation. In contrast, the minor `base` version bump caused hardly any breakage.
Complex, no tutorials, costly in compile times, and also the entire concept is an unwieldy workaround for GHC limitations. That said, I personally like to play with it from time to time, and have spent a considerable amount of time testing its limits.
thx! fixed.
&gt; Surely you can agree that your experience here is a bit of an outlier? I hope so. &gt; I think the sentiment expressed above is correct: monad's are no more difficult to grasp than some of the complex things a C++/Python programmer will run into. I think it's wrong because monads drop up much earlier in your Haskell experience. I don't *need* to know them to write "Hello, World!", as long as I'm fine with cargo-culting. But, for even the most trivial programs, they are part of the translation from source code to binary; if I really want to understand I'll use them. On the other side, for simple C/C++/Python programs, I don't have to understand structure padding / alignment or move semantics or mutable variables in lexical scope to understand the full translation between source and binary / byte code.
SSH client library.
Here is another way to look at Haskell's syntax: Function application is left associative (as explained below by /u/silverCloud7) because that's the only way to figure out how many arguments there are to a function when reading from left to right. For example, in the function `f x y z = x + y + z` we need to recognize `f` first. That tells us that we are looking for three arguments, so we parse the next three arguments and bind them to x, y and z, respectively. If necessary, we need to parenthesize the arguments that contain functions themselves to show that the contained functions need to be performed first on *their* arguments. This style is different than conventional mathematical notation, but it is appropriate when we want our syntax to support partial application ([currying](https://en.wikipedia.org/wiki/Currying)). Alternatively, there are languages like [APL](https://en.wikipedia.org/wiki/APL_%28programming_language%29) where everything is parsed from right to left ([Polish notation](https://en.wikipedia.org/wiki/Polish_notation)). This requires that we use many unusual operator symbols to figure out which things are the operators and which things are the arguments. This has caused many APL learners to criticize the syntax as a "write only" language (as in, you can write it, but no one can read it).
there's a valid function numbery type too-- church numerals zero f x = x succ n f x = f (n f x) so you can define a pretty sane integral instance for "(a -&gt; a) -&gt; a -&gt; a" although I wouldn't recommend it 
I don't think that's correct. What you've written is equivalent. I believe DatatypeContexts puts the context to the left of the type name, e.g. `data c a =&gt; Given c a = Given c a` or some such.
I think if it as a *choice*. We *chose* arrays, *chose* procedures, *chose* `for`-loops, *chose* iterators, and of course, we *chose* monads. There is just no meaning in *any* of these concepts on their own. I learned `goto` well after `for`, that was C++. Should I really have known how much a hassle there is using `goto`? Or can we just pretend that `goto` never existed? I now know non-monadic IO composition existed, now that the slides talked about it albeit briefly. And we sort of reached a paradox here. A rather simple construct of abstraction is used in place of a rather fragile one. Things work differently in C, and a simple thing is made hard. Now people *don't* say, well, monads are useless. They say they *can't understand how it works*. Right where there is so little work to care about. Really. How does the semicolon work? In this sense I agree with the slides. We've put monads into a wrong position, although of course not intentionally. *And* I think we're going to have a hard time fixing it. But it can be done. And I hope we do it.
Derp. You're totally right. Datatype contexts has the syntax you describe. /u/glaebhoerl's original example should work fine.
We should release a Windows friendly version very soon! EDIT: it's out now! 
u/cdsmith, how does this compare with the corresponding part of Idris's [DSL notation](http://docs.idris-lang.org/en/latest/tutorial/syntax.html#dsl-notation)?
Yes of course hard-float is fast, but the point was to get universally reproducible results which requires either fixed-point or soft-float.
he problem is (it's on slide 16): Haskell's IO is *not* imperative at all! The do-notation is syntactic sugar for lambdas/binds, and because of Haskell's lazy evaluation it won't always give you the same thing that doing stuff in order would. 
&gt;with a lot of dubious analogies and bad category-theoretical explanations, before the beginners have the opportunity to forge much practical / intuitive understanding It think you're right that bad category theory and "functors are like containers" analogies are bad. However, the "practical/intuitive" approach to things doesn't always work: in particular, it was only after thinking "huh, do-notation is syntactic sugar for a bunch of lambdas a binds" and "oh, right Haskell is lazy" that I actually understood the behavior of some of the code I wrote. Also I think it's worthwhile to explain the motivation behind monads even if you don't explain the theoretical machinery (which is math, but inevitably done with poor notation to explain to Haskellers).
&gt; I found this "in picture" tutorial great to explain the concrete usage (and not the theoretical mindblow): It's not a mindblow at all if you have a math background. With that, you just need a few examples of functors/why those ones in particular are useful. Monads are different, but then they're still explainable in terms of motivation if not theory. 
You're looking for the `nub` function, to remove duplicates. It wouldn't be very fast, though. `length ts /= length (nub ts)` 
&gt; That's why I keep telling people who try to learn Haskell that they shouldn't worry about it too much and just learn to use it before trying to really understand the ins and outs of monads But that's half the problem! It's ok to say "ignore the machinery" but saying "here's no motivation, go forth and use it!" isn't useful either. For one, they won't know *when* to use, for another it's pedagogically bad form to say "here's this concept I won't tell you why it's useful but learn it." &gt;It's a lot like learning maths, you just need to see and do a lot of examples in order to build some intuition for it. Not exactly? Math needs examples, but it's kind of about abstraction/proof. 
Using a set is definitely not a bad idea. Set uses a binary tree internally, which is a good data structure for checking duplicates. Of course, checking the lengths is not ideal. Another simple solution in O(n^2 ) is hasDuplicates' [] = False hasDuplicates' (x:xs) = x `elem` xs || hasDuplicates' xs or, if you're feeling fancy, implementing a tree yourself for O(n log n) data Uniques a = L | U (Uniques a) a (Uniques a) deriving (Eq, Show) hasDuplicates :: Ord a =&gt; [a] -&gt; Bool hasDuplicates = isNothing . foldM insert L where insert L x = Just $ U L x L insert (U ls a rs) x = case x `compare` a of EQ -&gt; Nothing LT -&gt; do ls' &lt;- insert ls x return $ U ls' a rs GT -&gt; do rs' &lt;- insert rs x return $ U ls a rs' If you can, I'd use the first solution as it has the important property of being obviously correct. O(n^2 ) is worst-case performance, and if your list is large enough for that to be a concern, you should probably not be using a list.
Not necessarily, see the second comment: http://gafferongames.com/networking-for-game-programmers/floating-point-determinism/
&gt; I don't think that's true. It *is* true. People find monoids and monads hard to understand, even though all they do is encode the idea of associativity and zero. The motivation is right there: associativity. This isn't difficult. It's a concept from 2nd grade mathematics! Associativity has a much more precise meaning than *chain together arbitrarily*. &gt; Not true! You misread my post. What I'm saying is that a monad **instance** alone doesn't give you much to work with. Say you have a type signature: f :: Monad m =&gt; ... Now what is it you can really do with values of type `m a` within the scope of `f`? You can use `&gt;&gt;=`, `pure` and all function that these two can build (`fmap`, `ap`, ...). That's not much. It's basically `mapM`, `sequence`, `filterM` and a few others, but there isn't much more to do. It takes a concrete instance, for example `MonadPlus`, to do more with a monadic value than just the most basic sequencing.
If you can add an Ord constraint use this one: http://hackage.haskell.org/package/extra-1.5.1/docs/Data-List-Extra.html#v:nubOrd
Planned in raaz. See https://github.com/raaz-crypto/raaz-ssh. Busy with getting raaz (the primitives library) to shape. Will work on it after that.
A production strength GRPC library
What I mean with this isn't that they should just ignore the abstraction forever, but that they shouldn't worry too much about fully getting it just yet. In my opinion, once you can write code using bind explicitly, you're ready to work with a lot of examples and just generally play around with it. Learning highly abstract things is always a mixture of experience with concrete cases and the abstract part itself. I'm not saying anybody should just never have to worry about the abstract stuff, but that they don't need to have a perfect grasp of it right from the start, and can still *use* abstractions without it. For *when* to use it, for the most part I think that the common monads are pretty self-explanatory for when you might want to use them. You generally know when you need IO, or when you're working with state of some kind. You generally recognize that you keep passing the same parameter into all your functions, and finding Reader from there is just a Google search away. The reason I recommend this is just because it closely mirrors how I learn things. I do like to dig deep into theory again and again but in order for something to really click, I absolutely need to get my hands dirty and apply it, whether it's a theorem, or some abstract structure, or whatever. For me it boils down to finding the right balance between shoveling abstract nonsense at people and being entirely pragmatic. Neither will do on its own, but I do think that just seeing many different instances of an abstraction in different contexts is what beginners need to do the most.
See https://github.com/snoyberg/posa-chapter/blob/master/warp.md In particular, see this graph: https://camo.githubusercontent.com/26181bacb927311dcd40eed24eeae9c06466b175/68747470733a2f2f7261772e6769746875622e636f6d2f736e6f79626572672f706f73612d636861707465722f6d61737465722f62656e63686d61726b2e706e67 Also, ping /u/meta_leap
We can go esoteric, and use `ordNub` or `hashNub` from [Data.Witherable](http://hackage.haskell.org/package/witherable-0.1.3.3/docs/Data-Witherable.html).
In the case of really any database setup, it's *usually* pretty simple: SQL Server is used all over the place in $DAYJOB. Your company isn't going to move off SQL Server just to write some Haskell. Native database drivers also tend to be much less generic than alternatives like ODBC, which allows deeper integration with features offered by the DB (e.g. freetds is multiple versions behind the current TDS spec). Lots of apps are tied to their DB technology fairly deeply, too. (Also note, SQL Server is no small competitor; it's a major enterprise database used by many, many massive companies at huge scale). Replace "SQL Server" with literally any database technology, tweak your $DAYJOB - and the statement is still true. That's basically what it comes down to in a nutshell, in my experience.
I liked this remark about `instance (C a, C b) =&gt; C (a, b)`: &gt; The basic intuition around type class instances can diverge from the actual implementation: it looks like the instance definition is saying, "If we have an instance for MyClass a and an instance for MyClass b, then we can also have an instance for MyClass (a, b)." This is a reasonable intuition, but it's precisely backwards from what Haskell is actually doing. Not surprisingly, this is something that PureScript gets right. In their [differences from Haskell](https://github.com/purescript/purescript/wiki/Differences-from-Haskell/3ee82f0e6e5dd3ace84a6f11b58bd8ce6da3ce9b#arrow-direction) document, they say: &gt; When declaring a type class with a superclass, the arrow is the other way around. This is so that `=&gt;` can always be read as logical implication; in the above case [of `instance Eq a &lt;= Ord a`], an `Ord a` instance implies an `Eq a` instance.
Here is one such comment, taken [from this post](https://typesandkinds.wordpress.com/2016/07/24/dependent-types-in-haskell-progress-report/): &gt;Singletons are an abomination. I hate them. These are gone with my design for dependent types, and I think the resulting language has the niceties we all want in a dependently typed language (modulo termination checking). 
There is no way you can make it "work" on infinite lists.
Not really sure about the performances but the following may do the trick: hasDuplicate :: (Ord a) =&gt; [a] -&gt; Bool hasDuplicate [] = False hasDuplicate lst = any (uncurry (==)) . zip s $ tail s where s = sort lst
You can make it "work" in the sense that *if* there are duplicates, it will terminate. The `length`-based one runs forever even on infinite lists that do contain duplicates; mine only runs forever on infinite lists that do not contain duplicates.
cheers. FWIW the direction of the arrow being same in both the cases always used to trip me up.
Ah good point. I think your assessment is fair.
We're about to open source what we have within a week (although it's still under development and experimental), so buyer beware
Well ... that's exactly what I do: * https://travis-ci.org/fhaust/vector-split * https://github.com/fpco/stackage/blob/master/build-constraints.yaml#L2604-L2605 I am just wondering if I get any notifications that my package on stackage has a problem. Especially as the Travis Job is basically not run regularly.
&gt; I am just wondering if I get any notifications that my package on stackage has a problem. Didn't you get a notification for https://github.com/fpco/stackage/issues/2194?
Thanks!
Mostly exists: https://github.com/glguy/ssh-hans
Can Agda model diamond superclass hierarchies with this method? I'm guessing not because there could be multiple paths up it?
It is effectively a const True that uses bottom as it's 'other' value. The only way it could be useful is to run it on another thread, but then it may just devour all the memory.
The one that only works on finite lists is partial too. When applied to an infinite list or one where there is a `_|_` in the tail after the first duplicate it will always `_|_` out unnecessarily. You don't get to know in advance if you are dealing with a finite input list, or an infinite input list, or even if you are dealing with `1:1:undefined` so you might as well gracefully handle the latter scenarios. Providing an early exit for infinite lists when a duplicate is detected simultaneously makes the result faster to compute and more defined. Nothing gets worse, some scenarios (that you may not care to think about) get better, so there is no reason not to make the improvement.
I will write up what I end up liking later. I think CI should break off into multiple systems - build runners that perform checkouts and builds, packagers that build distributable images, build workflow systems that schedule builds and fire off pipelines and communicate results - and let users mix and match implementations for each.
I think beginner pedagogy could be helped by exposing non-generic `seqIO` and `bindIO` functions.
There should be an instance of Num for functions because it follows the laws. I wish it were in base. The whole approach to type classes now is based on laws. Its inconsistent that there isn't one.
It's close! Definitely check it out.
Have you heard of Datomic? http://www.datomic.com/about.html
So approximate-then-exact membership test, ok.
Cool! Reading about it right now.
https://hackage.haskell.org/package/classy-prelude-1.2.0/docs/ClassyPrelude.html#v:ordNub
Yes of course. Doh!
&gt; Of course, the whole DB isn't loaded in memory; whether it is on memory or disk, that is up to the DB engine. Acid-state loads the whole thing into memory, although you could compress it.
You know, we f***ed it up with `Foldable` `Traversable` instances of `(,)` already and I hope it doesn't happen over and over again. The classic: ghci&gt; minimum (2, 5) 5 
Sadly a Bloom filter doesn't really gain you anything here. You have to insert into the set regardless of the Bloom filter result, and the cost of doing the lookup while doing the insert is about the same as the insert alone. On a related note, it is unfortunate that while `Map` has `insertLookupWithKey` there doesn't appear to be an equivalent for `Set`. e.g. insertLookup :: Ord a =&gt; a -&gt; Set a -&gt; (Bool, Set a) or insertLookup :: Ord a =&gt; a -&gt; Set a -&gt; (Maybe a, Set a) would be convenient here.
I don't want to sound ungrateful but I skimmed and it looks much more complex to learn/use than what I'm proposing, as my scheme is basically one line to load the DB and the rest is just plain Haskell. Am I missing something?
If you haven't seen this talk yet, you should. https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/
Or go magical, https://hackage.haskell.org/package/discrimination-0.2.1/docs/Data-Discrimination.html#v:nub
It reminds me of Redux, in the React world. Have you been inspired by it? 
I don't see how you can avoid MonadIO. All DB code will require it anyway ... I don't know `scotty`, but it uses `wai`, so there certainly is a way to add a middleware there for your HTTP instrumentation needs. As you should have an underlying IO monad for all DB code, you should have access to all sort of message passing systems.
One way that we could introduce this in to APIs is to use NullaryTypeClasses to 'hook' into particular calls. Basically, modern GHC can accept classes with empty heads, which means there can be at most one coherent instance of this class. If libraries had library-level hooks with a nullary type class, then applications using the library can declare hooks to instrument the code. For example, suppose the Warp web server declared a class class WarpInstrumentation where bracketServeRequest :: IO a -&gt; IO a bracketServeRequest action = action Whenever warp went to serve a request, it would now bracket that call with the `bracketServeRequest`. Each relevant warp call would have to be predicated on `WarpInstrumentation` being available. So the warp `run` function would now have the type: run :: WarpInstrumentation =&gt; Port -&gt; Application -&gt; IO () An application that wants to use any warp calls would now have to instantiate the `WarpInstrumentation` type class. Since default implementations were provided, you can simply do instance WarpInstrumentation If you wanted to send your request times to data dog for example, you could customize `bracketServeRequest` in the run time code. Most notably, any other library using warp, will now pick up this instance. instance WarpInstrumentation where bracketServeRequest action = do start &lt;- getCurrentTime res &lt;- action end &lt;- getCurrentTime sendStatToDataDog start end return res You raise a concern about these only being able to pick up actions in `IO`. Ultimately, this is not much of a problem, because, ultimately, only things in IO take time. Remember that any other value is constructed instantaneously, and only forcing it in IO can cause evaluation, which consumes time. For example, the following syntax wouldn't make sense to measure the time of the fibonacci function, where `time :: a -&gt; (Double, a)` is some potential function which measures the amount of time to 'produce' its first argument: let fibs = 0:1:zipWith (+) fibs (tail fibs) nthFib n = fibs !! n millionthFib = nthFib 1000000 in time millionthFb It doesn't work, because the thunk representing millionthFib is constructed pretty much instantaneously. It's only when you pattern match on millionthFb in IO (over-simplifying a ton) that you can actually force the thunk to evaluate to a result, but since this evaluation takes place in IO, using `getCurrentTime` to time it shouldn't be a problem.
&gt; Basically the use-case is to allow a library to give a hook that, by default, has a no-op implementation, right? That's my idea, and it's implementable using `NullaryTypeClasses`. More generally, `NullaryTypeClasses` are a solution to the configuration problem. Indeed the library authors would have to accept a PR opting-in to this approach, but this is an advantage of Haskell -- the code that is being run is exactly what you see. The ability of some languages to execute arbitrary code as you demonstrated above is unsafe, as the behavior of safe (perhaps even verified code) can later change at run-time if a particular module is loaded.
But then you need to know both those facts, and how do you compute them from a simple [a]? If you _do_ know both those facts, then why did you call the function in the first place?
You will know it for specific a though. 
You're missing the fact that any non-trivial data set will not fit into memory. You can't just 'one line to load the DB' when the data set is larger than a few GBs. I wrote a framework on top of a nosql database which is append-only (with option to manually vacuum old entries to reclaim space), doesn't mutate records, supports views (for cached queries or special indexes on the data), is patch based (OT-like API, which mans you get collaborative editing for free), has snapshots to periodically cache the results of a list of patches, supports streaming patches to clients (through websockets, if you're writing a web client). The basic API is fairly simple, though complexity increases if you want to write more complicated queries (the nosql DB supports multiple tables and joins, the API is very powerful, but you're in no way forced to use all of it). There is no predetermined data structure (the underlying storage is based on JSON, for interop simplicity with web clients), it's up to you to add validation to ensure the values conform to a particular structure. I have no idea how the complexity of my framework compares with datomic, since I've never used it.
The package head lists papers, but there is a lot going on in the "internal" modules (high # density!). I suppose [this](https://www.youtube.com/watch?v=cB8DapKQz-I) is a good description, but I haven't seen it.
Can't help wondering why FPComplete would take a term with an established meaning and associated expections as you can see e.g. at https://en.wikipedia.org/wiki/Long-term_support where *long-term* support periods are in the order of years rather than merely months and redefine it to fit their reality. Or is this simply a marketing trick to make Haskell appear more enterprisey by subtle use of [alternative](https://en.wikipedia.org/wiki/Alternative_facts) labeling and hoping people won't be too disappointed when they realize what the actual deal was?
Is this really the **recommended** and **pragmatic** way of writing instrumentable code in Haskell? 
It should work on single .hs files. In case you encounter any issue, please let us know on github! 
I mean, there's more than one way to do things. This is just the personal preference I've come to have. Could you please explain what's wrong with it?
Given merely a value of type `[a]` you don't get to know without looking at it in ways that affect your termination if it is finite or not. &gt; The length-based one runs forever even on infinite lists that do contain duplicates; mine only runs forever on infinite lists that do not contain duplicates. That statement was entirely true, just not complete. It also happens to terminate for any finite list or list that contains `_|_`s as elements or a `_|_` in the spine so long as they occur after the first duplicate. This is the most defined you can make this function without resorting to `unamb` tomfoolery.
No, free monads play quite nicely with transformers. You can use it as your base Monad, or you can even use the free Monad transformer to put it anywhere in the stack. That said, i think you have to be confident in your understanding of the performance of various free Monad approaches before you can consider them for production. Basically, just always use the church encoded one. But even that has non negligible overhead. For reasons like that, I would often recommend not using a free Monad, and instead just using a monadic typeclass. class Monad m =&gt; MyDB m where getUsername :: Id -&gt; m Text Creating alternate implementations of this class is very similar to creating alternate free Monad interpreters. But with this, you can just use a fast monad transformer with no Free overhead.
&gt; There is nothing more annoying than databases. This is a very immature attitude to have. Archiving all transactions forever isn't possible for any serious data task. Purging old transactions and data is a necessity for anyone that handles a substantial amount of data. You can't afford to store everything forever and performance will turn to shit! Also, consider that you have legal requirements in many countries to not store personal data older than e.g 10 years. How will you solve that? Will you switch to a real database after 10 years since your can't support that requirement? 
&gt; You don't need a Turing-complete typechecker to interpret Python, no matter how weird the scoping rules are. Yep, you need a duck type-checker! Quack
Well, it's long term. LTS-6 just got an update this weekend, and the first LTS-6.0 was published 2016-05-25. New LTS doesn't mean that old are "unsupported". I hope that LTS-6 will get updates until GHC-8.2 based LTS will be published (or until "it converges", i.e about everything gets major updates). Stackage maintainers: could you run a check to see what could be updated if LTS-2.23 would be published today? It would be interesting to see.
&gt; It doesn't work, because the thunk representing millionthFib is constructed pretty much instantaneously. This is "solved" with `deepseq`.
It is as total as it can possibly be given its type signature. It has bottoms for list elements that are bottom themselves, and for infinite lists where the elements are from an infinite set, and there are no duplicates in the list. For a type that allows these situations, you cannot possibly avoid these bottoms (because that would require an "isBottom" check, which amounts to the Halting Problem); but they are the *only* bottoms this function has. Compared to the original one, a few other bottoms are notably absent, namely infinite lists that *do* contain duplicates (`length xs` where `xs` is infinite is, unfortunately, bottom), and lists that have bottoms (either as elements, or by being infinite) after the first duplicate. That is, my implementation works fine for the following examples, while the original does not: - `hasDuplicates (0:[0..])` - `hasDuplicates [1,2,3,1,2,3,undefined]` As far as useless goes; even for the bottom case of an infinite list without duplicates, this implementation provides the most useful possible behavior; it won't tell you that there are no duplicates (because, again, Halting Problem), but you can decide that if it doesn't give you an answer within a set time, then you assume that the list has no duplicates, and you can achieve this behavior by running it in a separate thread that you kill after the maximum execution time has been exceeded. The original implementation cannot do that. Another concern is refactoring; if, at some point, you decide that you want the function to return the duplicate rather than just a boolean, or even a list of duplicates found, then you can do this with an almost trivial refactoring, there is no need to touch the structure of the function, and all the useful properties will remain: getDuplicatesWith :: Ord a =&gt; Set a -&gt; [a] -&gt; [a] getDuplicatesWith seen [] = False -- base case: empty lists never contain duplicates getDuplicatesWith seen (x:xs) = (if x `Set.elem` seen then (x:) else id) (getDuplicatesWith Set.insert x seen) xs This implementation will work fine on infinite lists: it will produce output while traversing the infinite list, and if you use it to find the first 10 duplicates in an infinite list, then that's fine if there are at least 10 of them in the list. So IMO it is definitely not useless; it is as useful as it possibly can be, and while it has bottoms, it does not have any additional bottoms on top of the unavoidable ones. 
Heh, I didn't think it through apparently - the Bloom filter does make the check slightly faster, but only for cases where you'd have to do the insert anyway, and only for some of them.
I wouldn't want to replace `f x y` with Lisp's `(f x y)` which has the same problem. I prefer Java's `f(x,y)` which doesn't have the problem, and is more understandable to beginners and mathematicians to boot.
What's wrong with applySing :: Sing f -&gt; Sing x -&gt; Sing (f @@ x) instance IsFunction (Sing f) where type Domain (Sing f) = forall x. Sing x type Codomain (Sing f) = forall x. Sing (f @@ x)
Well if the Bloom says True you have to do the insert, because it could be a false positive. If the Bloom says False you have to do the insert because it is definitely not there. It can save you a lookup in the False case, but since the insert has to happen anyways, nothing is won. =/ Let me give a sense of how you could really use a Bloom filter here: Now if you really want to get fancy with a Bloom filter you could do something where you build just a bloom filter at first, with no set and hope to ride true negatives all the way to the end of the list, then on failure you can skim back through the original list for true matches for just the one element. If you know the input list size a priori you can set the false positive rate to balance the cost of the expected number of second scans against the size of the list. Since inserts are handled one at a time, you can use more advanced filters, such as the d-left and cuckoo filter variants to get asymptotically better space usage, bringing things down to the fluid limit. To handle real lists with unknown size you can use a [scalable Bloom filter](http://gsd.di.uminho.pt/members/cbm/ps/dbloom.pdf). Pick the 'true' false positive rate then keep a chain of bloom filters with exponentially increasing accuracy and start inserting into the next one once the previous ones get 'full'. This still requires falling back on a linear scan for the corresponding match, and you need a really good hash function to avoid spurious excess false collisions, because they are _really_ expensive this way. I'd personally not be comfortable with something less powerful than simple tabulation hashing or the like because I'd really want not just O(1) collisions in expectation, but w.h.p. bounds. With a good enough Bloom filter you can afford to have a _very_ expensive slow path if it gets stuff off the fast path.
That's event sourcing isn't? https://martinfowler.com/eaaDev/EventSourcing.html
Thank you for pointing out the name of this pattern. When I tried to search for it without knowing its name, I found nothing. Ended up doing my own thing. Now I have many solutions to compare to :) What a difference knowing a technique's name makes...
One of the more practical problems with having all state in a single `IORef`, which is then updated by a `handler :: Event -&gt; st -&gt; st` is GC. When you have a big dataset, even if you manage to fit all of it into memory, Haskell's stop-the-world copying garbage collector is gonna be a serious bottleneck. Afaik, the way it works is that it starts at the root of your data graph, walks it and copies everything over into a new chunk of memory, then switches over to it. So if you have a 4GB dataset, every time the GC runs it's gonna have to copy 4GB of memory (minus the unreachable data) into a new location, leading to unacceptable pauses. I've read that for big datasets the pauses can get pretty big (10s of seconds in pathological cases).
Nice, thanks! Now both are as of "nearly 4 years ago", so I'm curious if anyone since has bothered to check whether nginx has improved or ghc/warp may have regressed in the meantime --- **but** definitely impressive and good enough for me *personally*!
Can this approach be extended with binders for sharing subexpressions, e.g. `Let a (a -&gt; Prop' a)` ?
Disclaimer: this is not Stackage maintainers' opinion (and I'm not one anyway). I'd say: - yes, it's ok to stay on LTS 1.x for as long as you wish - no, if you care about security updates: but Stackage doesn't promise you'll get any Stackage only promises that if you succeed to build your system againts snapshot "LTS-x.y", you'd be able to build it for even against LTS-x.y (obviously, as it's frozen configuration), and against LTS-x.(y+n), with some caveats (if you use unqualified imports, orphans etc stuff might break, but those are trivial to fix). TBH, Stackage frontpage should explicitly say that you don't get any (security) updates, even in active snapshots, if the package authors don't act.
It's a good input, I realized I posted the job description with a vague definition: Time spent: 75% time coding in the proprietary Haskell-based language 25% time designing models over the financial datasets Aaaand that what I meant by not your typical Haskell role. Hopes this clears some confusion 
&gt; You're missing the fact that any non-trivial data set will not fit into memory. You can't just 'one line to load the DB' when the data set is larger than a few GBs. I don't know much about DB design so I concede you're right and I'm wrong. Nether less, I just thought that could be solved by storing data on disk, and lazily caching it on memory as the lazy evaluator requests subsets of the whole data. Isn't that possible?
I can't find it right now but someone had posted here in /r/haskell a benchmark of many different web servers, and lamented that Warp had fallen lower in the rankings. That was itself a year or more ago but I don't think anybody is actively trying to make Warp faster anymore (unlike nginx), so it's probably still the case or even moreso. 
I wonder if it could be modified to only load things to memory when the lazy evaluator needs them? That way you could treat very large datasets stored on disk as if they were plain Haskell values, `mmap`-style. That is what I had in mind.
&gt;&gt; Someone with tons of CPU power is still able to DDOS my app. &gt; Yes. No, someone with as much CPU as you will be able to DOS you app, by definition. I was about to object about the belief that having an append-only log-based database made it "simple", but the whole replication part is bonkers.
I don't understand your question. If what you want is to take code that was written without any regard for instrumentation and monkey patch it while running to have instrumentation, I would consider that pretty much impossible with Haskell. (Note that I use the qualifier "pretty much" because there might be some ugly low level ways of accomplishing it, but I would not want to use them.) The whole point of Haskell is that you run the compiler AFTER you write code. If what you want is to be able to change the instrumentation levels without recompiling your code, you can do that with a config file. If what you want is to be able to change the instrumentation levels *while your program is running*, then you can still do that with config files that you've designed to be reloadable on the fly. If what you want is to be able to change the instrumentation *without changing certain parts of your codebase*, then there are all kinds of possible ways to do that based on the specifics of what you're trying to do. But you're going to have to change something somewhere. In your above example, your second bullet point under "disadvantages" does not have to be the case. You can make your `instrumentedscotty` package use the exact same module names as the `scotty` package so that it is a drop-in replacement. But I would say that your first and third bullet points are pretty much unavoidable. You're unavoidably going to have to change something. And that something is unavoidably going to impose some requirements on your environment. The only way to avoid needing to change is to plan ahead and use a library that has this particular concern already built into it and lets you change it with a simple runtime flag / config option.
Can someone provide further details (or links) why those extensions and packages are considered unsafe? Do they rely internally on stuff like unsafePerformIO to be implemented? 
Yeah, I'm not even proposing a change to Haskell, it would be too far reaching. You're right that currying doesn't work well with `f(x)` syntax. But I feel that currying is overrated, it gives an O(1) brevity bonus at the cost of having to know the arity of everything while reading code. (I also feel that way about type inference in many cases.)
What I proposed is that each transaction much be sent with a Proof of Work - i.e., a small enough hash of the last tx's hash (or some similar construction), which takes, say, in average 10 seconds to compute. Connected nodes only accept transactions with that Proof of Work. That way people can't spam those, because sending 100 transactions would cost 1000 seconds of CPU time, for example. The point is that, in order to send a tx, you must burn a few seconds of CPU. Enough that it isn't a problem for the end user, but would be expensive to keep a sustained attack. I've found a post by Vitalik (one of the founders of Ethereum, second largest public blockchain) suggesting exactly that kind of fee-less decentralized application 2 years or so ago, so it seems that I wasn't the first one to have this idea, but nobody implemented anything like that AFAIK.
Whole thing. Maybe the author is right when he says dont attemp to learn monad on its own.
This creates a double indirection, which may or may not be an issue for your application, but it's definitely noticable
I derive a great deal of utility from that viewpoint. YMMV. Have a nice day.
YMMV, but I personally use partial application all the time. E.g. `doSomethingWith myArgument &lt;$&gt; container`
Thanks, I'll read that article.
Hmm ... none that I can see in the github notification panel.
Neither. Almost everyone has their own project-specific `runDB` function. You put the instrumentation in your runDB function. It would just add noise to shove this into a library. Further, I'm not sure you understand what Esqueleto is. Esqueleto is an EDSL, the actual database-talky-walky is Persistent.
Lens does work with safe haskell. Its "trustworthy" by default and can work with a safe flag (http://hackage.haskell.org/package/lens) that makes it fully safe at the cost of some performance (because indeed there are coerces inside that can be promised to be trustworthy but are not inferrable to be fully safe -- they can be replaced but at the cost of performance). Generally, I think few people do the work to mark their packages because its a pain to maintain and test (and work around various versioning issues with deps that may switch from safe to unsafe etc) and there's been very little demand for it. Which may be chicken-and-egg (since so few packages do it, people find it too inconvenient to bother trying to use it, and thus don't ask for it from other packages, etc).? That said, the key thing for SafeHaskell is how it works with sandboxed environments where you run untrusted user code. Its not for verifying _libraries_ -- its for verifying _which libraries you trust end users to call_, which is a more niche case imho...
Actually I understand the difference between Esqueleto and Persistent, but I noticed that you recently took over Esqueleto's maintenance (which I have been tracking), and used that as an example.
&gt; Almost everyone has their own project-specific runDB function. Why would this be such a common thing to do? What am I missing?
I do not use SafeHaskell. Also, I've never met anyone who uses it. My impression has always been that it's supposed to be able to stop you from being able to write anything that's equivalent to `unsafeCoerce`. More specifically, here are the use cases the [GHC User Manual](http://ghc.readthedocs.io/en/8.0.2/safe_haskell.html) provides: &gt; - Enforcing strict type safety at compile time &gt; - Compiling and executing untrusted code Needing to compile and execute code provided by a user is an uncommon requirement (but not unheard of). The mention of "strict type safety" only half true. Safe haskell does not equip haskell with a termination checker, so I can still write this: bad :: Integer -&gt; a bad n = bad (n - 1) So, while `SafeHaskell` stops you from writing programs that segfault, it cannot stop you from writing programs that hang. It also stops you from performing IO in a pure context. This seems like it might be useful in you want to compile and run arbitrary code from untrusted (potentially malicious) users, but since I've never needed to do that, it's not something I ever really consider.
&gt; the recommended and pragmatic way of writing instrumentable code in Haskell? IMHO, you don't need free-monads. You just use a Monad that has logging &amp; instrumenting capabilities such as monad-logger. But monkey-patching is a no-no in Haskell. So if you really want to instrument a library that you do not control (e.g. scotty), &amp; and that library doesn't provide any way to augment it's behavior, you're out of luck. But if it is a stack you're writing, you just use monad transformers e.g. monad-logger.
I'm no longer directly involved in the process, but here's what I know: haskell.org is applying for the Google Summer of Code this year. Niki Vazou and Jasper van der Jeugt have joined the haskell.org committee and have taken up managing the GSoC org application. I think once we are accepted or rejected from GSoC, they'll make a decision about whether to try to organize and find funding for another Summer of Haskell.
[relevant](https://twitter.com/bigdataborat/status/338092796374294528)
I think the problem is that there are two of them. One for `forall a. (Num b =&gt; a -&gt; b)` -- Pointwise lifting. Ont for `forall a. a -&gt; a` -- Peano. Type class instances should either be unique or require a newtype wrapper.
If it suits your needs, the `unification-fd` package is well tested and maintained with a rich api: https://hackage.haskell.org/packages/search?terms=unification
Thanks, that's not a bad idea. This time I'm not writing a type inference algorithm, so I don't need full on first order term unification. But I suppose I can just use a degenerate term structure and get a simple union-find algorithm. Definitely worth considering.
If you have lots of updates you're going to run out of disk space pretty soon.
I personally don't dislike the Lisp syntax any more than C-like syntax (Lisp used to be my favourite language for a short time, before learning Haskell).
&gt; Don't re-align things. Don't move it around. Don't fix formatting errors. Yeah. This would be reason enough for me to reject a patch. If the existing formatting is a real issue, then *first* the project has to determine a style that can be automatically checked, and then I will accept patches that reduce the number of checker errors as long as they don't do something else at the same time. And that's on code I know quite well. If I was just barely maintaining something, I'd be even more hesitant.
The concepts you describe at the start of your post remind me of the data structures distributed consensus protocols like PAXOS or Raft use as a log.
Does Haskelly do type inspection (tooltip) like ghc-mod does?
You can do a lot with `Some`. e.g. wrap a phantomy GADT. 
You can get `Dict` here: http://hackage.haskell.org/package/constraints-0.9/docs/Data-Constraint.html#t:Dict
Why did Church let Turing get away with it?
I hear this argument a lot, but it's misguided imho...Currying does not just give us an O(1) brevity bonus (several other languages allow for convenient partial application without currying). What currying really does is letting us describe a function of **any arity** as a simple `a -&gt; b`. This is essential to a lot of great Haskell idioms, applicatives being a prime example (no pun intended). Other examples include Testable from QuickCheck, and the Monoid instance `instance (Monoid b) =&gt; Monoid (a -&gt; b)` 
I used to try to use Safe Haskell but gave up because it is too much of a pain. [My thoughts on this](https://marc.info/?l=haskell-cafe&amp;m=139369610710605&amp;w=2): "My issue is this: why is there no easy way to trust *any* package, not just packages that are Trustworthy? I should be able to say "I trust this package." It is immaterial whether the package author has raised her hand and said "my pure functions don't launch missiles" when I can examine the code for myself and determine whether the code launches missiles. Indeed, if I use package trust, I need to either examine the code or trust the author--the author's pledge isn't determinative. I see what "Trustworthy" adds when you're not using package trust, but it's just an informational flag if you are using package trust. Despite this Safe Haskell will not recognize the trustworthiness of packages that I have deliberately marked as trusted--merely because the author has not made a pledge." The GHC manual says Safe Haskell can be used to check for good style, but in practice Safe Haskell is unusable for this purpose.
I'm pretty comfortable with almost anything, but I've found I prefer languages with fewer braces. ML-style languages, lisps, Lua and Ruby's do/end, etc. Maybe it's because I touch-type, but I find it easier to type a few letters, or parentheses, than constantly needing to use { and }. They just feel like the wrong way of doing it now. And the prefix notation? Well, that took some adjustment, but the consistency makes it a worthwhile tradeoff.
One thing that (annoyingly) blocks up safe Haskell is the safe coercion system (and therefore also GND). This is considered "unsafe" because a module written without it in mind may use truly unsafe features in a way that would be safe if not for coercions. Ugh. Thankfully, it seems that at least some of the people writing such conditionally-safe code have started using role annotations to sidestep the problem. I hope that in another release or few the GHC folks might mark `Data.Coerce` as trustworthy and make everyone's lives better.
https://ghc.haskell.org/trac/ghc/ticket/8310 was my last attempt to fix the situation. It got "fixed" with a warning that still leaves the task of maintaining safe haskell imports on a best effort basis impossible. I've since stopped going out of my way to support it. I'll take patches from folks I trust that fix it for my code, but I'm not actively writing code in that manner any more. That said, a large portion of my emergency patches over the last few years have been to fix those same fixes where they missed an esoteric combination of dependencies. Why? If I mark a module "Trustworthy" and it happens to infer "Safe" that's a warning that spams my users. And worse, the trusted code base is rendered larger than it needs to be. But managing this fact requires transitive knowledge of all of my dependencies leading to ridiculously complicated, brittle and non-future proof CPP all over my code to manage this transition, and it requires me to manage all combinations of those packages to know precisely how these safety properties change over time. Worse, I've seen folks change the trustworthiness of a module in a patch level release, where I can't even detect it with CPP. So now I have to either make affirmative "Safe" annotations and then downgrade them to "Trustworthy" very selectively, or worse, I have to remove the "Safe" annotations and try to pigeonhole just the "Trustworthy" ones and then scan my _haddocks_ to figure out if I failed and happened to transitively depend on someone who decided to optimize their code's performance with `coerce` or GND. I've taken to never writing explicit Safe annotations, and writing Trustworthy annotations where I can track down both the before and after window where I have to rely upon it for all my dependencies. This cut my workload by about half, but even with that before I gave up entirely I was spending more time trying to manage this nonsense than all other package maintenance tasks put together by a large margin. The current approach just isn't manageable at all.
`lens`'s support for safe haskell is rather brittle and has come and gone repeatedly over the years. It is currently broken in that importing Control.Lens isn't `Trustworthy` or `Safe`. See http://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens.html Building with the last few major releases of GHC and every combination of lens's dependencies to try to figure out what CPP we need to fix it is a rather non-trivial task ill suited to mortal attention spans. We occasionally try to fix it to try to get lambdabot to build, but patches that purport to fix it almost always get it wrong.
Yes, you can use a vacuous term structure to get at the underlying union-find monad; though I haven't looked at what sort of overhead that might have. Re your desiderata: * all instances of the monad interface do path compression. The ranked versions aim to give the inverse Ackermann asymptotics, though they're less battle-tested than the mere path-compression versions. * The package is in maintenance mode rather than active development, but I aim to keep it from bit rotting. * I've tested it enough to preclude any the obvious bugs, but haven't gone about making a (semi)formal proof and don't have a huge test suite that ferrets out less common corner cases.
Common way to solve this problem is to introduce snapshots. I.e. you periodically roll state up to some point in the log and trim the log from the head. If your app crashes/restarts - you'll need to replay events only from the latest snapshot. 
Given what existed before LTS, this can not be a real question
&gt; The current state is handled rather unprofessionally. speechless
And that's coming from the guy who [proposed](https://www.reddit.com/r/haskell/comments/5m678s/suggestion_code_of_conduct_for_haskell_community/) that we adopt a code of conduct to make our community friendly and welcoming…
Is the original presentation anywhere? I'd like to watch it.
&gt; Mongo all the way I say. Acid compliance is for suckers. Best to add /s so nobody is led to take it literally. The implied sarcasm may be lost on some readers. 
I'll try my best. Maybe that some people believe that one programming paradigm solves *all* problems. 
You fail. Because any given person can be avoided. But databases cannot be avoided. *There is no escape.*
&gt; I do understand, what lambda calculus is but I do not recognize the relation with the programming language. Haskell is based on lambda calculus, that's the relation. Do the exercises in the chapter and continue reading the book.
Sorry maybe, I expressed me wrong. I mean should continue reading the book or research more about lambda calculus.
1/2/3 is ok in arithmetic, meaning 1/(2*3). Of course in real formulas you would use a big line to separate denominator from numerator.
We currently do not support type inspection, but we are planning to give it a try, due to the fact that the backend that we use (Intero) supports type inspection. 
http://stackoverflow.com/questions/2398503/query-on-booleans-in-lambda-calculus
Well then, if there is no escape you have two options. Continue to resist and prolong the suffering, or accept the current state of affairs. 
I'm not sure I understand what you're hinting at. Could you elaborate please?
Which is basically `any ((&gt; 1) . length) . group . sort`, or `any ((&gt; 1) . length) . GHC.Exts.groupWith id`.
Hey, thank you for having the confidence to believe in yourself!
Thanks for writing this up! This is some definite black magic that's associated with type classes, but imo is essential knowledge when you start doing advanced work with type classes.
But do I need to understand lambda calculus completely to continue reading?
Thanks much for explaination.
You can compile GHC code with profiling enabled to achieve something similar, but it will be low level like function calls and allocations, not high level like DB queries or HTTP requests.
A Singleton is essentially a way in OO languages to ensure that there is only ever one instance of a class, globally accessible via static methods. This is seen as a bad thing when you start unit testing or want multiple copies of your code running in one program (e.g. with different settings or to use concurrency). `NullaryTypeClasses` seem to have the same disadvantages and offer just as few advantages over simply passing whatever you want to convey to your program as a parameter.
Not directly related but this year we will get 3D XPoint memory. The overhead databases have is getting increasingly unacceptable when non volatile storage is only 4 times slower that DRAM.
[removed]
What optimization flags are you using?
Why would you want it to be configurable as general hooks if it was only being used for instrumentation?
Not really.
thanks
` ghc-options: -threaded -rtsopts -with-rtsopts=-N -O2`
Have already benchmarked. Shocked to see a 200ms penalty for using the monad transformer version!
Because then you could choose which implementation backend to use when compiling the executable, while the library would be blissfully unaware.
When you start layering you end up with a performance penalty. Oleg's extensible effects manages to bypass this to some extent.
Can you provide context as to what 200ms means? 200ms penalty for a billion requests would probably count as "negligible".
No, it's a reference to a prompt monad example. https://mail.haskell.org/pipermail/haskell-cafe/2007-December/035800.html
https://www.youtube.com/watch?v=eis11j_iGMs 
I believe the "real implementation" remark refers only to the implementation of functional references (cf. the second definition of the `Accessor` type in the post), and not to the whole prompt monad example.
Hmm, having read the post, I am thinking that we would use lenses these days.
Thanks for adding some clarifications! Do I get this right that it basically says that if you want to sure to be on an actively supported so-called LTS you have to upgrade to the newest LTS major version if it exists once the previous LTS is older than 3 months? And which in the worst case amounts to upgrading every 3 months to a new major LTS version?
&gt; Continue to resist and prolong the suffering, or accept the current state of affairs and prolong the suffering FTFY 
Keep on reading. I am myself at 16th chapter and it is a very good book. You will get the relation to Lambda Calculus later on even if you don't get it now.
Seems like Andrea Asperti wrote this paper as a response to my previous to [my previous thread](https://www.reddit.com/r/haskell/comments/2zqtfk/why_isnt_anyone_talking_about_optimal_lambda/), which asked why the lack of interest on optimal λ-calculus implementations. He clarified some misunderstanding that his earlier papers "problematic result" meant optimal evaluators can't be efficient, while in fact all that this result says is that shared beta-reduction steps isn't a meaningful way to measure the complexity of a λ-term, in any strategy. He proceeds to dissect the merits of the idea in a very readable way, talks about the state of art and covers some new ideas such as super-optimal evaluators. Great summary of this class of algorithms which could possibly one day be the best way to implement a functional language.
The logic checks out.
A few things: &gt; Monadic functions that are polymorphic on the monad could be transformed into pure functions. Is this sound? I feel there could be meaning lost with regard to strictness. &gt; Functions that work in IO could be auto-lifted to work in any MonadIO. Not necessarily an easy transformation. For example, consider the `bracket` function: bracket :: IO a -&gt; (a -&gt; IO b) -&gt; (a -&gt; IO c) -&gt; IO c a simplistic transform could easily rewrite this for any MonadIO, as you would like bracket' :: MonadIO m =&gt; IO a -&gt; (a -&gt; IO b) -&gt; (a -&gt; IO c) -&gt; m c bracket' create release action = liftIO (bracket create release action) This is indeed a valid embedding of `bracket` in `IO`, but it's neither particularly interesting nor the definition you really wanted which is bracket'' :: MonadIO m =&gt; m a -&gt; (a -&gt; m b) -&gt; (a -&gt; m c) -&gt; m c This second adaptation is much more difficult to construct, because it requires understanding what the monad `m` actually does and there may even be multiple valid definitions of `bracket''`. I think the same could be said about your third transformation. &gt; Functions that work in IO could be instrumented automatically in some way, perhaps with logging statements. I believe this could be done with a module system, independent from any kind of transformation -- you could have a module exporting a particular module signature that itself depends on another module of the same signature.
You may be interested in attribute grammars. There's an introduction to them in the monad reader: https://wiki.haskell.org/The_Monad.Reader/Issue4/Why_Attribute_Grammars_Matter
&gt; &gt; Monadic functions that are polymorphic on the monad could be transformed into pure functions. &gt; Is this sound? I feel there could be meaning lost with regard to strictness. It's just specializing it to the identity monad.
Oh I see. Yes, that makes sense. Certainly, you can do this, then!
Its most likely getting memoized better with the fix! Haskell is quite bad at optimizing primitive recursion. 
I think I figured it out. For getting the n-th fibonacci number, this variant is much faster: fasterFibs :: [Integer] fasterFibs = f 1 1 where f !a !b = a : f b (a+b) so the version in the article is probably running faster because it's forcing more suspensions along the way because of the call to `snoc`.
Could you put most of the data on the C heap, turning as much of it as possible into `Storable`s and using `StablePtr`s for the rest? Then the data will be kept separate from the GC, and the only thing being copied around is the pointer to the database.
&gt; Oh yeah? Name one thing that's more annoying than databases. &gt; You can't do it. Impossible. Printers.
Even doing that it gives me an error. Deprecated: "Use Control.Monad.Trans.Except instead"
I had to overcome a tricky bug caused by the interaction of GHC RTS ticks and the way the w1-therm kernel module responds to read() calls on the virtual files for the sensors. Details: https://github.com/eamsden/w1-therm-haskell/issues/1
&gt; Functions that return `ExceptT e IO r` could be transformed to return `IO (Either e r)` and vice versa. Isn't this what [`runExceptT`](https://hackage.haskell.org/package/transformers-0.5.2.0/docs/Control-Monad-Trans-Except.html#v:runExceptT) and [`ExceptT`](https://hackage.haskell.org/package/transformers-0.5.2.0/docs/Control-Monad-Trans-Except.html#t:ExceptT) do?
I think that's a warning, not an error.
Yes. But if your whole API uses ExceptT, you have to go function by function, applying the transformation mechanically. I think it would be nice if we could transform a whole API in one go. 
Take a look at the https://github.com/vacationlabs/monad-transformer-benchmark Reproducing the benchmark results here: Using the `ScottyT` transformer, but bare Lucid: benchmarking transScottyBareLucid time 161.5 ms (155.5 ms .. 167.4 ms) 0.998 R² (0.996 R² .. 1.000 R²) mean 157.2 ms (155.4 ms .. 160.4 ms) std dev 3.164 ms (1.510 ms .. 4.661 ms) variance introduced by outliers: 12% (moderately inflated) Using the `ScottyT` transformer, along with Lucid's `HtmlT` transformer: benchmarking transScottyTransLucid time 269.2 ms (246.5 ms .. 292.8 ms) 0.998 R² (0.994 R² .. 1.000 R²) mean 263.4 ms (256.2 ms .. 267.0 ms) std dev 6.762 ms (6.966 μs .. 8.284 ms) variance introduced by outliers: 16% (moderately inflated) 
Take a look at the https://github.com/vacationlabs/monad-transformer-benchmark Reproducing the benchmark results here: Using the `ScottyT` transformer, but bare Lucid: benchmarking transScottyBareLucid time 161.5 ms (155.5 ms .. 167.4 ms) 0.998 R² (0.996 R² .. 1.000 R²) mean 157.2 ms (155.4 ms .. 160.4 ms) std dev 3.164 ms (1.510 ms .. 4.661 ms) variance introduced by outliers: 12% (moderately inflated) Using the `ScottyT` transformer, along with Lucid's `HtmlT` transformer: benchmarking transScottyTransLucid time 269.2 ms (246.5 ms .. 292.8 ms) 0.998 R² (0.994 R² .. 1.000 R²) mean 263.4 ms (256.2 ms .. 267.0 ms) std dev 6.762 ms (6.966 μs .. 8.284 ms) variance introduced by outliers: 16% (moderately inflated) 
I think the one most people would call the goto is `extensible-effects`. But I'd urge you to give each of them a glance and see if any of them have a compelling argument for you in particular.
Looks like you have `-Werror` turned on, which turns warnings into errors. Regardless, the *right* solution is to do exactly what the message tells you: use `Control.Monad.Trans.Except` instead of `Control.Monad.Trans.Error`. Due to the weirdness in mtl, this means you should import `Control.Monad.Except`
Importing it alone doesn't seem to fix anything, and I don't have the skills to fix it myself I don't think. I'll just disable -Werror in my IDE ¯\\\_(ツ)_/¯ 
Huh, for some reason I thought `Control.Monad.Error.Class` only exported `MonadError`, as the name would imply, but it appears it also exports `Error`. Try `Control.Monad.Except` instead, which should definitely avoid the warning.
I'm actually quite keen on [freer](https://hackage.haskell.org/package/freer)'s use of type level list syntax for its open union. Looks a bit prettier than [extensible-effects](https://hackage.haskell.org/package/extensible-effects), but it's just a syntactic thing...
I vote for `freer`. It has clean implementation, simple API and good performance.
You've layered monads. I think that performance drop is normal.
It's also closest to Oleg's original implementation.
Both. Even with Reflection Without Remorse, the performance cost is high. It's no longer linear in left associated binds, but the constants are very very high.
I am a little confused about the `Freer.Internal` module - are you meant to use this? The example does, but it seems a little odd... should I just be trying to use `handleRequest` and `handleRequestS`?
Let's break down the things you've mentioned a bit more: * (1) The reason why monadic functions cannot be treated as their pure functions transparently is because without type lambdas, you cannot instantiate m a to a. You can set m to Identity, but Identity a is only *representationally* equal to a; so what you are seeking is a way of automatically inserting the coercions you need. * (3) is similar to (1): `ExceptT e IO r` is representationally equivalent to `IO (Either e r)` but you need to explicitly coerce * (2) involves inserting `liftIO` to move from a concrete type to a more generic one. In this case, there's a "coercion" but it is only one-directional. I'll leave aside (4) for now. If you were asking for cases (1) and (3) to apply in *all* situations, you're essentially asking for "implicit conversions" in your language. This has another, better known name: subtyping! But actually what it is that you are requesting is not that Haskell always treat `Except T e IO r` as `IO (Either e r)` (probably not a good idea, in any case), but you can apply the coercions only at specific points; for example, when "casting" a module that provides values of type ExceptT e IO r into IO (Either e r). This is actually something I have thought about in Backpack, because Backpack requires a subtyping relationship between entities in Haskell, and there are always possibilities for expanding this subtyping relationship. For example, suppose you have a signature that has the type `IO (Either e r)`, but you provide an implementation of type `ExceptT e IO r`. Today, as the types are not the same, Backpack will reject this match; but it is not hard to imagine an alternative design where the match is accepted, and we generate the "adapter" code to coerce from one to another. I'm not aware of any papers which go down this design route (though, I hope anyone more well read can correct me.) This actually would work a bit better with a traditional, non-mixin ML-style module system, because there the matching judgment is the only important thing. With Backpack, we also have to ensure that an appropriate merge of signatures always exists, but it's totally unclear which version of the type we should accept in such a case! 
How is it "normal"? It's 100-200ms! And why is this not better documented? Isn't this the very reason you would use transformers? To lift action from one monad to another? It's shocking that standard, as-intended usage has such a big performance penalty !
Not GP, but if you have the money: http://Haskellbook.com If you don't have that much money, contact their support and they'll work something out with you. If you have no money: https://github.com/bitemyapp/learnhaskell https://en.m.wikibooks.org/wiki/Haskell Also, I never finished this so to speak.... But it may be useful: https://docs.google.com/document/d/1JJnMiRip4HwvAM40ZdOu3ZAx7evrhaz7wUMlYNIwLSQ/edit?usp=drive_web 
Oh, interesting. Doesn't seem to be a `runNatS`. Maybe I should also make an issue about there not being [any docs for `runNat`](https://hackage.haskell.org/package/freer-0.2.4.1/docs/Control-Monad-Freer.html#v:runNat). Any idea where the name comes from?
I am still getting a lot of mileage out of just an MTL approach, where each MTL class just lifts an algebra. E.g, class MonadHttp m where liftHttp :: Free Http a -&gt; m a.
&gt; but you can apply the coercions only at specific points aka [coerce](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Coerce.html#v:coerce)? foo :: (forall m. Monad m =&gt; StateT s m a) -&gt; s -&gt; (a, s) foo = coerce :: StateT s Identity a -&gt; s -&gt; (a, s)
Are you aware of this [modernized version](https://www.reddit.com/r/haskell/comments/5fdh6u/write_you_a_scheme_version_2/) ? 
You may be right. Isolating the benchmark also seems to suggest the same. Have filed an issue with Lucid: https://github.com/chrisdone/lucid/issues/63
thanks so much.
Maybe it's a little odd, I use [tuples](https://hackage.haskell.org/package/data-has) for extensible effect(maybe not that extensible, I just want some reader usually), but works fine for me.
And there you have it. Posting on the Haskell reddit matters.
I'm not aware of any Cordova bindings yet, although it should be possible to write some fairly easily using the [FFI](http://www.purescript.org/learn/ffi/). For React, there are several options: - [purescript-react](https://github.com/purescript-contrib/purescript-react/) - [purescript-pux](https://github.com/alexmingoia/purescript-pux) - [purescript-thermite](https://github.com/paf31/purescript-thermite/) each of which come with examples and documentation.
Cool, looks pretty similar to my eff-transformer package
Interesting. Are there benchmarks showing this?
Injecting the whole free monad, instead of each individual method, is a great idea. My issue with MonadFree was that I couldn't have multiple FreeF f transformers in a stack. Seems pretty simple, but do you have an example?
They probably exist somewhere. I've benchmarked it before myself, but I don't have that on hand anymore. I am working on getting benchmarks for a bunch of Haxl like approaches to try to find the optimal system for Fraxl, so I could add Reflection Without Remorse to those benchmarks.
I really hope so, especially because in addition to just showing the type, it can show extra info. What I love about the ghc-mod extension is that it also shows the fully qualified name / module that a function or type was imported from. It'd be amazing if Haskelly did this!
I'm not haskell_caveman but I've written my opinion on LYAH a few times, I'll link to [the last one i could find](https://www.reddit.com/r/programming/comments/50m99l/new_edition_of_programming_in_haskell_now/d75pyxf/) just to give some concrete opinion of why i do not recommend LYAH. :)
on stackoverflow I opened a question: http://stackoverflow.com/questions/41925449/cabal-missing-packages You will see the output of `ghc-pkg check`. Thanks
For beginners, I would tend to go for well-supported languages with mature ecosystems and active communities. This minimizes the amount of inane work you have to do getting things working on your computer, and maximizes the number of people who can help you out when you have a question. By this metric, Haskell is the obvious winner. I wouldn't sweat language choice too much, however.
Haskell and Purescript both have "parametric polymorphism" and "type classes" and this is the most powerful thing in each language by far (besides ya know the lambda calculus). Haskell does each of these things with much more intuitive syntax in my opinion though Purescript is deployable in a more easily profitable context and has a lot of deployment resources which overlap with JavaScript which is arguably a great thing.
Yes, but this discussion is all about automatically inserting coerce ;)
We had some a couple of years back in the lens channel. Not sure what happened to them, but we were seeing ~20x factors. I had hope for replacing the guts of `machines` with it at the time. Sadly, things got much worse than that if we actually fixed the deque to be O(1) worst case rather than the amortized one used in the paper so that it could be suitable for streaming applications. https://github.com/alang9/deque/blob/master/Data/Deque/Cat.hs provides a thristed Tarjan-Mihaescu deque that has O(1) worst case behavior for all operations. (Note the complete lack of recursion as a trivial proof of bounds)
[removed]
I’m not sure this really answers /u/wildptr’s question, since the question seems to be more about the specific details of abstracting over an arbitrary *pattern* than anything else, which currently isn’t directly possible short of using Template Haskell. For `Maybe`, `isNothing` exists, but even then you end up using `fromJust` in your solution, which feels bad to me. I think a more direct answer might be to use lenses/prisms here, which are probably closer to a “first-class pattern” than much else. That might be a bit heavyweight if you’re not already using `lens`, though.
&gt; I'd recommend Elm for a beginner. It really depends on what that beginner what's to get out of learning a new language... * Know a general purpose functional programming language -&gt; not Elm * Get experience with typed FP -&gt; Any of 'm (Elm, PureScript, Haskell) * Learn the "hard parts" -&gt; PureScript and Haskell * Not get stuck too much in the beginning and/or have a soft learning curve -&gt; Elm * etc.
True, I'm just going by what OP said: &gt; I'm just wondering if one language would be better than the other for beginners based on the design of the language / tooling. 
Of course! I should have thought to look in monad-loops. Thanks :)
Lens might be a good solution for first-class patterns. (or possibly overkill) This problem sounds a little like the [`failover`](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Traversal.html#v:failover) combinator. Adding a failure case to it might look something like this: {-# LANGUAGE RankNTypes #-} import Control.Lens import Data.Functor.Compose import Data.Monoid matchCase :: Functor f =&gt; Getting (Traversed r (Compose ((,) Any) f)) s a -&gt; f () -&gt; s -&gt; (a -&gt; f r) -&gt; f () matchCase l e s afb = case traverseOf_ l (Compose . (,) (Any True) . afb) s of Compose (Any True, t) -&gt; t Compose (Any False, _) -&gt; e z :: Maybe Int z = Just 4 ex :: Either String Int ex = do x &lt;- Right 3 y &lt;- Right 7 matchCase _Just (Left "oh noes") z $ \val -&gt; do a &lt;- Right 5 b &lt;- Right 4 Right () return y With some `-xTemplateHaskell`: data MyPattern a = Successful a | Failure | OtherFailure | AbsolutelyAwfulFailure makePrisms ''MyPattern x, y, z :: MyPattern Int x = Successful 2 y = Failure z = AbsolutelyAwfulFailure example :: IO () example = do putStrLn "testing" matchCase _Successful (putStrLn "x not successful!") x $ \val -&gt; do putStr "x was successful, and its value was: " print val matchCase _Failure (putStrLn "y was not a failure!") y $ \val -&gt; do putStrLn "y was a failure!" print val matchCase _Successful (putStrLn "z was a failure!") z $ \val -&gt; do putStr "z was successful, and its value was: " print val outputs: testing x was successful, and its value was: 2 y was a failure! () z was a failure!
&gt; This isn't that much related to Haskell. You'd have the same problems with compiled C. Probably, but AFAIR, I would have total control on which library I'm linking with or not. My actual problem is I have no idea how and which package is actually linking to `libmysqlclient18`. Is it `persistent-mysql`, `mysql-simple` or `mysql` ? &gt; If you have multiple versions of the library available on the compile system, you may have to start plying around with linker flags and library paths. Any idea how I can do that ?
I'm not sure if this is what you're looking for but there is a research language [1ML](https://people.mpi-sws.org/~rossberg/1ml/) in which modules are first-class values. So you can have an expression like ``if b then module0 else module1``, functions can accept and return modules and so on.
It is related to Haskell, because I'm using stack and I have no idea whatsover, how help stack (or which ever haskell tool) to pickup the right external dependencies. I even don't know how to specify an external dependency in cabal.
Looking at the source it seems that when the map is constructed the new value is the first argument to the function while what's already in the map is the second. I'm guessing this means that you're just appending a singleton list on the left which is basically the same as consing, performance and all.
Yeah /u/ephrion's solution is initially what I was coding towards, but it looks like yours and /u/foBrowsing's solution seems cleaner, especially considering that I was going to rewrite certain functions in terms of lenses anyways. Thanks!
Do you know much about efficient reduction of simply typed lambda calculus? It looks like one of the references in the paper linked might talk about memoization techniques, but I don't have access to it. Also, what's the current thinking on whether strong normalization helps for optimization?
I think the same issues the simply typed lambda calculus have, the normal lambda calculus has too... there are quite complex terms on the STLC... Strong normalization is good for optimization in many cases, for example, if you convert data structures to a church-encoded (recursion-free, strongly normalizing) representation and build algorithms for that representation, then by compiling your code you'll get a program that doesn't create intermediate data structures. For ex., `sum . map (+ 1) . map (* 2) . filter even . enumFromTo 0 $ x` would run in constant space rather than allocating 4 intermediate lists and then summing the last. In general, strongly normalization is good for optimization because it removes the barrier for compile time inlining that recursive definitions bring in.
I much prefer the newer Haskell from first Principles. It comes with exercises, and a bit more formality.
I would expect the performance related issues to be in the Hackage docs of mtl in a big hold warning. 
I'm very interested in understanding why `&gt;&gt;=` has such a performance penalty. Around what time in the talk do you address this issue?
I talk fairly early on about how the way we write mtl code means that we're constantly doing dictionary lookups to figure out how to `&gt;&gt;=` / `return` / `etc` for the particular monad that we're working in.
I don't understand your proposed type, but I generally agree that the current interface leaves something to be desired. The general idea, I think, is that you want the value ultimately stored at a key to be calculated by folding over all the entries in the list with that key. It's unlikely that strictness analysis and such will help anything here, so it probably pays to offer both strict and lazy folds. I suggest you try to write up your idea in more detail and submit an issue to the containers GitHub. Wren Romano and I will have a look and see what we think.
You'd probably have to look at the core on a case by case basis to figure out how often dictionaries are being recreated here.
You can generalize the type to foreverFixM :: Monad m =&gt; (a -&gt; m a) -&gt; a -&gt; m b which subsumes not only the type you've given, but also the more accurate, but still too restrictive `b = Void`.
What about [total](https://hackage.haskell.org/package/total)?
According to what's in the post (and in my experience), the best way is still to try and fail a few times ... there might be some good practices that are emerging, but API design still seems more art than science. As for the practices, some are non controversial (refrain from using type classes), and some are (export lenses instead of field accessors to prepare for backwards compatibility, write more generic type signatures, etc.). API design is very much a taste issue.
I can relate a bit to trying to use dependent types in Haskell and it resulting in a much more complex library, but contend the advantages are often worth it. I'm eagerly awaiting -XDependantTypes after having seen how user friendly they can be in Idris.
The other alternative is just to provide a function which groups all values in a list and then let the user fold over each "group" if needed.
How safe and reliable is reflex? It seems really cool but experimental stability and 1122 downloads total (as of time of writing) make me worried. I have similar worries about react-flux, but was wondering if that was any good.
&gt; Easier setup and ecosystem maintenance on your machine. That's a bold claim. With Stack, this problem is pretty well solved. PureScript doesn't really have anything as nice as Stack. `psc-package` is getting there, but it's still experimental and immature. PureScript by Example also is probably not better than Haskell Programming From First Principles (aka Haskell Book) in terms of teaching someone purely functional programming from nothing.
See the original comment...... you can choose anything for `f`, including useful things.
That's more general and nice to have. But it's still convenient to have a type-specialized version of this kind of thing, to avoid noisy type annotations. The classic idiom for not caring about the result is still unit.
Theoretically speaking, there are four options here, and you've already mentioned #1 (compile to JS, use FFI to run native JS) and #2 (run a JavaScript execution environment in a subprocess). The other two options are #3, write a JavaScript interpreter in Haskell and use that to interpret the JS at runtime, and #4, write bindings for an existing JS interpreter (e.g. v8). AFAIK, there are no good mature implementations for either though. For #3, at least a parser exists that seems to parse most JavaScript programs correctly, so you would "merely" have to write an interpreter for it...
My package [threepenny-gui][1] includes a JavaScript FFI (module `Foreign.JavaScript`), which can be used to call JavaScript from Haskell and vice versa. However, it's a bit of a contraption: The Haskell side implements a web server, to which the JavaScript side has to connect. Then, a WebSocket connection allows for communication between the JavaScript and the Haskell side. Ultimately, your main problem is that you need some way of communication between the After Effects JavaScript runtime and your Haskell program. Some options for tackling this are: * Compiling Haskell to JavaScript makes it possible to execute your Haskell program within the After Effects JS runtime; this makes communication trivial. * WebSocket communication. That's what Threepenny does. * … [1]: https://github.com/HeinrichApfelmus/threepenny-gui
It's pretty bleeding edge, active development. But the latest version is stable. Can't make comment on safety, what do you mean? Most developers use plain github repo, instead of hackage/stackage, because of GHCJS, which is also built from GitHub repo. I have experience with react-flux also. I don't like Flux architecture in general, but bindings are pretty good, especially after 1.3, and you always can do custom stuff.
I just discovered this project from [this comment](https://www.reddit.com/r/haskell/comments/5qen18/my_conception_of_the_ideal_functional_programming/dcyti4z/). I am very interested to know if and how it is used in practice and what experience the people using it made. Thanks in advance!
It certainly depends on your country.
what makes the case ?
I think some people have observed the fact that type classes are often misused and come to the conclusion that type classes are best avoided. I don't agree with this line of thought. It's better to learn how to use type classes effectively than to avoid them. 
Does it control the need to annotate each line in a do block (if the effect type type is polymorphic as with State, Yield etc.)? This is characteristic of `freer` `extensible-effects` etc. I love them all but I find this really depressing. 
Very interesting. This would be an alternative solution to the Haskell "string problem" wouldn't it? One different from a module system.
Are you sure it's possible to control After Effects from JavaScript? Only JavaScript? That sounds unusual. Whichever library they wrote is not magic, they need to communicate with After Effects somehow, probably via a socket or something, in which case they are likely to support more languages than just JavaScript, and you could connect to that socket directly if you can figure out the protocol. Looking up "After Effects SDK", I found something called [ExtendScript](https://cgi.tutsplus.com/tutorials/introduction-to-the-basics-of-after-effects-scripting--ae-22518), is that what you are talking about? If so, calling JavaScript from Haskell won't do you any good, nor calling ExtendScript from Haskell for that matter. That's because the interpreter is inside After Effects, so the script can interact with After Effects by calling functions which that interpreter knows about but e.g. the node.js interpreter doesn't. So you need to compile Haskell to Extend Script, in which case you won't be able to use e.g. GUI libraries. Or, you could try to communicate with the Extend Script side, using sockets for example, if ExtendScript supports them.
Adding to the other answers here, it's not that type classes are bad but they are less straightforward than just using data and functions. There are times when they really help but creating a new typeclass is powerful magic to be used sparingly. So the advice is to start with data and functions.
Utrecht does lots of Haskell stuff: uu.nl
You can define a video transformation DSL, compile it to Javascript, and send that along. 
I recently researched MSc programmes in the area of type theory, dependent types, Coq, Agda, functional programming etc. What I came up with: - US: UPenn (Weirich, Pierce, Eisenberg), Princeton (Appel), MIT (Chlipala) - EU: Chalmers/Gothenburg (Coquand, Hughes, Abel, Danielsson), Utrecht (Swierstra), Freiburg (Thiemann) For the US unis, be aware that both their tuition fees and application requirements are bullshit. Freiburg only makes the list because I'm currently studying there. There are a couple of interesting research groups in the UK (e.g. Nottingham, Strathclyde), but they don't seem to teach much as part of the associated MScs, so they'd be more interesting for a PhD.
There was/is the Yale Haskell Group led by Paul Hudak (one of the language designers). Not sure about the group's status after his passing
You mention that the monad stacks cannot be inlined because they might be recursive or something like that near the beginning of your talk. I do not understand that part - monads are not really recursive in my mind. Could you elaborate a bit on that?
I'm referring to the After Effects Scripting API, available here: (PDF warning) http://download.macromedia.com/pub/developer/aftereffects/scripting/After-Effects-CS6-Scripting-Guide.pdf It'd write a JavaScript wrapper that would do the transformation I need, then invoke that somehow from my Haskell application.
Certainly looks like it could help, but I've never used it.
&gt; Oh yeah? Name one thing that's more annoying than databases. You can't do it. Impossible. More databases
&gt; The documentation makes the claim that the reason database systems don't use relational algebra is because it was made later. That seems odd. Afaik, SQL was based on relational algebra, so SQL couldn't have been the one created first.
Indeed, that was the problem. I've put the solution in the original post. Thanks!
[Here](http://stackoverflow.com/questions/32837278/difference-between-relational-algebra-and-relational-calculus) is a stack overflow answer on the subject. I also noticed on [Quora](https://www.quora.com/What-is-the-difference-or-connection-between-SQL-and-Relational-Algebra) that relational algebra seems to predate SQL, so that further calls into question the claim by the website that databases didn't use RA because it was too new. It seems it was known and vendors chose not to use it.
Can you post where you're seeing that in the documentation? I don't recall reading anything along those lines. He does say that current SQL implementations do not faithfully follow the original conception of relational algebra.
Off-topic, but the Javascript powering this slideshow seems to be consuming *all* keypresses, even those used by the browser. This is quite annoying for those using keyboard-driven browsers. In Conkeror, I couldn't use "q" to close the page, or "Alt-x" to get a commandline, etc. To close the page I had to resort to a terminal command "conkeror -f kill-current-buffer".
I have a strong feeling that the future of programming lies in total functional programming, and I want to build a STLC VM backend for this. I've been looking for something like this, but I haven't found exactly what I want, so I'm building something myself. I'd much prefer to contribute to an existing effort or find other people to collaborate with.
[Here](https://github.com/agentm/project-m36/blob/master/docs/introduction_to_the_relational_algebra.markdown#why-do-existing-databases-not-adhere-to-the-relational-algebra).
Use mtl / mtl-style.
Your name threw me. You are wanting something more general. foldingFromList :: Foldable f, Ord k =&gt; b -&gt; (a -&gt; b -&gt; b) -&gt; f (k, a) -&gt; Map k b foldingFromList nil (&lt;|) = foldr (uncurry myInsert) empty where myInsert k v = alter (Just . (v &lt;|) . fromMaybe nil) k EDIT: Nevermind. :P
That's an excellent question! I'd have to sit and think a lot if I wanted to try to give a real answer, but for starters I can at least tell you that I think "design patterns" don't exist in Haskell in *quite* the same way that they exist in, e.g., Java. To be more specific, I mean that a most of the vital underlying concepts used in building a program might be *implemented in libraries* rather than being language features - for example, apart from do-notation, monads are entirely a standard-library feature implemented with ordinary user-level code.
I think that there is no such things as "object oriented programming" and "functional programming", even if a lot of people want you to think there is ;) Most of the thing you know about "object oriented programming" can be applied in "functional programming", at least in Haskell. OO Classes are types, OO methods are free functions. Subobject are types which composes other types. Encapsulation is done through a module export list. Inheritance, well, do you really need inheritance ? Most of the time you need composition and more polymorphic function, or sum types. You can solve some of the "virtual dispatch" using partially applied functions, for the others, existential type and typeclass can help. It is difficult to give you a correct and detailed answer without a real problem, but I'm sure that the same question asked on an "object oriented programming" subreddit can only lead to a similar answer.
I brought this as an example. The point is that in OOP one always thinks in terms of objects and their interactions and the application or library design is based on this strategy. So, what is the case in functional programming? I know that there are several hybrid programming languages like Scala which mix OOP with FP for those who want to mix different approaches but in terms of pure FP like Haskell how does one design an application? Does one think in terms of functions and somehow model the operations that must be implemented like in pre-OOP programming or is there some other approach? What about types? Since they do not represent objects how should one think of them? Are there any FP design patterns based on practical experience or theory?
I agree with this comment a lot :)
&gt; With Stack, this problem is pretty well solved. Not on Windows, unfortunately. A little bit of breakage here, a little bit there, a little bit in MSYS, all multiplied by the sheer amount of dependencies that Haskell packages often have, and you get basically a dumpster fire. Case in point: I've tried installing `purescript` with stack. The version in `lts-7.16` can't compile the examples. The version in `lts-7.18` won't build because of a bug in `fast-logger`. Latest nightly won't build because of a dependency conflict (~~no idea how can that even happen with stack, but I guess it was because I had to add an extra dep~~ EDIT: I forgot to check if the `purescript` package is in this particular snapshot, turns out it wasn't). (meanwhile, I've tried `pacman -Syu` on MSYS, as the bug in `fast-logger` involved gcc being unable to parse a preprocessor directive, so I've thought that maybe updating it would solve it. turns out my SSL certificates are outdated, and I have to reinstall stack. now I can update the packages, but it didn't help `fast-logger`). I won't even mention that stack complained about missing .cabal files and broken connection to amazonaws from time to time, as that fortunately didn't break anything permanently. I really wish this was the first time something like that happened to me.
/u/tekmo has written a little bit about this type of thing on his blog previously; here are a few links: - [The category design pattern](http://www.haskellforall.com/2012/08/the-category-design-pattern.html) - [The functor design pattern](http://www.haskellforall.com/2012/09/the-functor-design-pattern.html) - [Scalable program architectures](http://www.haskellforall.com/2014/04/scalable-program-architectures.html) - [Model-view-controller, haskell style](http://www.haskellforall.com/2014/04/model-view-controller-haskell-style.html)
Your reply explains how to emulate OOP in Haskell. This is not my question. I am not looking for ways to design an application in Haskell using an OOP philosophy. I am also not trying to find out how to do OOP design patterns in Haskell. I am wondering if functional programming proposes or encourages a special way of designing the structure and logic of an application. OOP is indeed a design strategy, a programming philosophy which can even be followed in other languages like C which do not provide language support for OOP and therefore make the coding harder. My question is whether FP has a design philosophy. If I am supposed to follow an OOP strategy in FP why should I struggle trying to emulate it in a language not designed with this strategy in mind? I can simply use some OOP language or some hybrid language to also have anonymous functions and high order functions. So, which is the FP way of designing an application that the language is made to support and therefore it is more suitable to use?
You are telling me how to implement OOP design patterns in Haskell but this is not my question.
Functional programming has fewer "design patterns" and more "libraries" -- we're a bit better about abstracting out the repetitive patterns in approaches to problems. That's not to say that we don't have our own design patterns (which might someday influence the design of future languages such that we can eliminate them), but it's harder to recognise them in general at the moment. Here is a [talk](http://ulf.wiger.net/weblog/2008/02/29/simon-peyton-jones-composing-contracts-an-adventure-in-financial-engineering/) I highly recommend, given by Simon Peyton Jones, discussing one of the major approaches to functional programming which I would consider a "design pattern" of sorts -- embedded domain-specific languages. At present, it's hard to imagine taking the entirety of that approach and turning it into a reusable library to kill the pattern entirely, though there is no shortage of libraries which can help with various aspects of it.
This sounds like you might have out-of-date PureScript dependencies, although it's hard to tell without seeing the error. We can help on #purescript IRC if need be.
Types types types. Everything starts from the types. Figure out what data types your application needs. Then figure out what operations you need to do on those types. Try to make them pure functions as much as possible. This is where it all starts. It's very similar to OO classes actually, minus the inheritance. There are plenty of other things to talk about. But that would take a lot longer, and this is really the core to it all. This talk by Conal Elliott does a great job at showing this in action: https://www.youtube.com/watch?v=bmKYiUOEo2A
 I am not super experienced in Haskell, but I think these are fairly objective: - Separate data definitions, pure functions and a thin I/O wrapper. - Try to keep your functions as general as possible - If you use a type in a specific way add a newtype or type alias for it - Write type signatures for your top level functions - break your problems into smaller and smaller ones until your functions are easily readable These are super general and I recognize that it's probably not what you want, sorry. About functional design patterns: In my experience most of them are just obviously the right thing to do so I wouldn't really call them patterns as such. OOP design patterns are often covered by first class functions, type classes or currying. Other than that I usually think about functions as transformations of data.
Is there a forum/blog for Haskell on Raspberry Pi for collecting these bits of wisdom?
Thanks, but as I mentioned after editing the post, it turns out the version I've installed with stack must have been outdated, and installing it with npm solved the problem.
I'm not sure if I'll really answer your question, as you seem to want to design in OOP way, but according to your comments are not interested in how to model object in Haksell. Anwy here is my attempts. I remember having a similar problem when I started learning Haskell. I though type safety and polymorphic function was amazing, but the first thing I looked was the support for OOP in Haskell. I know people in here don't like it, but I use to love OOP. Even though it as some drawbacks, it's pretty straightforward to design using it : just look at your business model and create object. And it offers a reasonable level of isolation between different things. In Haskell, you do the same, look at the "world" you need to model and create data for it, or function, or type class, depends (but starting with data is usually good enough). Then you create functions to transform those data and that's it. You'll soon realize that you don't need "object" as such. So the question is, why do you need objects for ? Objects are usually used to represent different things - interface or traits : For example Triangles and Circle, can be "drawn", they responds to "draw" message - closure : You construct an action with some parameters and then can call it. The action will be executed with the parameters passed at construction. - state : A few language support dynamic inheritance. For example a empty buffer, will have a different behavior that a full buffer. - solve dispatching problem : calling "draw" on a circle doesn't do the same that calling in a "triangle". Interface or traits, can be modeled either with record, type class or even just data. closures are native in Haskell, no need for objects for that. In fact most of the traditional OO design pattern are closure in disguise. So that removes quite a lot of them (like factory, commands etc ...) State : Haskell ADT is much much powerful than OOP for that. For example, The classic example of Circle, Triangle etc .. is fine. But what about Rectangle and Square ? Is a rectangle a square or the opposite ? Of course everybody knows that a square is rectangle with the two sides equal to each other, therefore a square IS A rectangle. So in OOP, square should derive from rectangle, but that doesn't work. You need to do the opposite. Have a square (with a length for example) and *extend* it to a rectangle by adding a height. Annoyingly, once a rectangle is rectangle it's stay a rectangle forever, even when I set it's height the same as it's length. Dispatching : Haskell allow "multiple dispatching" pretty much out of the box (using the "Multi parameters" extension). (Multi dispaching is when you need to specialise "draw" depending on the shape but also the display). This remove the needs for "visitor" pattern. In Haskell using Algebric Data Type, you don't have to decide, you can have square and rectangle side by side and automatically transform a rectangle to a square, when it becomes square. About design pattern. So what are design patterns ? I read somewhere (on stackoverflow if I rembember well), that design patterns are abstractions which can't be represented in the language itself. Therefore, the more powerful at expressing abstraction the language is, the less design patter makes sens. They just becomes "model" in the language. Haskell allows to describe really high level or abstract stuff, so needs less design patter. Having said that, there are "techniques", which some even have names (tying the knots, for example) which could be considered as design pattern. Also, "boiler plate", which can be see as a pattern, can usually be removed using Template Haskell or Generics (which is amazing). tl;dr Most of the design pattern (at least gang of four) are there to fix some flaws in C++ which are not really relevant in Haskell. Just start coding and see what happen ;-) 
Unlike SQL, Project:M36 supports [relation-valued attributes](https://github.com/agentm/project-m36/blob/master/docs/tutd_tutorial.markdown#group). You may also be interested in reading [more about NULL in outer joins](https://github.com/agentm/project-m36/blob/master/docs/on_null.markdown#outer-join-ambiguity).
&gt; For example, when one wants to write an application which deals with geometrical entities he can represent them in classes like Triangle, Tetrahedron etc and handle them through some base class like Shape in a generic manner. How does one design a large scale application (not simple examples) with functional programming? One nice thing about Haskell is that you are not forced into the object paradigm for every problem. Some problems are not well suited for this. Your example however is, so an object-like paradigm really seems best. This is no problem for haskell, though. Just go ahead and define your class; data Shape = Shape { shapeDraw :: Diagram, shapeArea :: Double, shapeDoesIntersect :: Point -&gt; Bool } triangle, square :: Shape triangle sideA sideB sideC = Shape (drawTriangle sideA sideB sideC) (triangleArea sideA sideB sideC) ... The best part is that, unlike object oriented programming, these structures could be made to combine algebraically. For example, suppose you wanted to write a method that combined shapes. This is very simple in my interface. In fact you can make `Shape` into a Monoid (Assume `Diagram` is like the `diagrams` library): instance Monoid Shape where mempty = Shape mempty 0 (\_ -&gt; False) mappend a b = Shape (shapeDraw a &lt;&gt; shapeDraw b) (shapeArea a + shapeArea b) (\pt -&gt; shapeDoesIntersect a pt || shapeDoesIntersect b pt) In C++ or Java, I would have to either make the combining method part of each class or create another class to represent combined shapes. The first approach has the disadvantage that I could actually make the implementation different depending on if I called `a.combine(b)` or `b.combine(c)` (This is the same problem python's `__add__` and `__radd__` face). The second approach has the disadvantage that I can differentiate combinations of shapes by inspecting the class at runtime. In contrast, in my Haskell example, if you combine two triangles that themselves exactly make up a larger triangle, *you cannot distinguish between this new triangle created from combining triangles and the same triangle created by a call to `triangle`* Thus, this design pattern is quite universal: make some types to represent your data, and then create an interface to those types. However, Haskell lets you properly construct interfaces to these types instead of awkwardly forcing all interactions into a subject-predicate-object paradigm.
Should be back up now. Our first real load test has discovered a bug. :) We have it [autorestarting](https://github.com/3noch/project-m36.io/commit/07a9cf70042cb60796e9e294ba6e33274abc5322) until the bug is fixed.
Perhaps this is why FP has not taken off in the industry. On the other hand OOP has some very clear ideas explained in the beginning of any material before any particular language syntax. My opinion is that what makes the difference is not syntax and abstract elements but the philosophy behind them and how they are supposed to be used in order to design and build an application effectively. Not for experimenting with some interesting theoretical concepts but actually producing applications that matter.
Then everything that has data is OOP. Every function that takes an argument can be seen as a method of that argument.
Another option in the world of React / PureScript are bindings to React Native (w/o Cordova). For example: * [purescript-reactnative](https://github.com/doolse/purescript-reactnative) * [purescript-rnx](https://github.com/atomicits/purescript-rnx) * there might be few others on GitHub, but most of them are out of date
They're both pretty much the same though. Except PS seems to be a little more modern in that they designed the language without any legacy cruft that haskell is required to keep, like older string types, etc.
Could you show an example of getting a do block to work with annotations? I haven't managed to get a multi-effect do to typecheck with freer yet, and haven't been able to find any examples :(
I think that F# is actually a hybrid language. Is this wrong?
I would also add this video to the list: MuniHac 2016: Beautiful folds are practical, too https://www.youtube.com/watch?v=6a5Ti0r8Q2s
Haskell language server for VSCode. It's based on intero, so many issues with ghc-mod should be avoided, i hope :-) Don't hesitate to give it a try.
&gt; I think that a lot can be said and thought around the concept of the Algebraic Data Types of Haskell which is (as far as I know) unique to FP. ADTs are available in quite a bunch of languages nowadays: https://en.wikipedia.org/wiki/Algebraic_data_type#Programming_languages_with_algebraic_data_types
It's a completely different thing. This sorts type level lists. Basically lists whose elements are types.
Algebraic data types are certainly useful. However, none of what I wrote above has to do with the algebraic structure of data types. The `Shape` type is a simple product type, available in most languages with `struct`s or similar constructions. I think you may mean that the current Haskell prelude encourages defining algebraic interfaces to your data types, and this is certainly true, but is different from algebraic data types. For what it's worth, you can define the same structure (with the indistinguishability property as well) in vanilla C, by making the closures explicit. However, typical C programs do not typically define algebraic combinators, as a matter of style and linguistic convenience. On the other hand, Haskell makes it so brain-dead simple that it's hard to not want to define algebraic structures. struct Shape { Diagram *d; double area; bool(*doesIntersect)(Shape*, double, double); } struct TriangleShape { Shape shape; double sideA, sideB, sideC; } struct CombinedShape { Shape shape; Shape *a, *b; } Shape *mkTriangle(double sideA, double sideB, double sideC) { TriangleShape *s = malloc(sizeof(TriangleShape)); s-&gt;shape.d = mkTriangleDiagram(sideA, sideB, sideC); s-&gt;shape.area = mkTriangleArea(sideA, sideB, sideC); s-&gt;shape.doesIntersect = triangleDoesIntersect; s-&gt;sideA = sideA; s-&gt;sideB = sideB; s-&gt;sideC = sideC; return s; } bool intersectsCombinedShapes(CombinedShape *s, double x, double y) { Shape *a = s-&gt;a, *b = s-&gt;b; return a-&gt;doesIntersect(a, x, y) || b-&gt;doesIntersect(b, x, y); } Shape *combineShapes(Shape *a, Shape *b) { CombinedShape *s = malloc(sizeof(CombinedShape)); s-&gt;shape.d = combineDiagram(a-&gt;d, b-&gt;d); s-&gt;shape.area = a-&gt;area + b-&gt;area; s-&gt;shape.doesIntersect = intersectsCombinedShapes; s-&gt;a = a; s-&gt;b = b; return s; }
This is really cool! I hope it is possible to eventually handle client-side queries _ala_ servant. I've written about [this](https://www.reddit.com/r/haskell/comments/5grixw/missing_packages_that_needs_to_be_written/daux5nn/) before but it would be amazing to have a lens-like api that could be used to safely query a GraphQL server. The package is very young of course but I would love to see some more docs / examples :) 
Thanks for the link, it illustrates the point nicely. 
Interesting idea - though I'm not super sure how graphql queries could be derived from lenses. Would love to see a proposal! For docs we have a small tutorial and 2 examples: * http://haskell-graphql-api.readthedocs.io/en/latest/tutorial/Introduction.html * https://github.com/jml/graphql-api/tree/master/tests/Examples But we know that's not good enough. More as we find time.
That's a great and somewhat complicated question. # YAGNI Since OOP encourages mutation, local state, implicit behavior, "spooky action at a distance," etc, it requires more discipline and structure to create applications. This discipline and structure results in Design Patterns. Haskell/FP say "don't do these things," and the resulting applications are a lot smaller/simpler. So for the same set of basic requirements (eg, the essential complexity of the problem domain), the FP solution will be smaller and simpler. It will also require less structure/design-patterns. Design patterns are not "free" -- they cost LoC to implement, testing, debugging, etc. and choosing the wrong design pattern can make software extremely unpleasant to deal with and modify. Since the extra structure is itself *more code*, you need to have discipline/structure around it too. As a result of all this, 10K lines of OOP might translate into 2K lines of FP. Since the FP project is much smaller, it needs less structure and discipline, and can do without the overhead of those design patterns. Where the essential complexity is the same, FP has much lower incidental complexity. Many common apps don't have *that* much essential complexity, so many common Haskell solutions don't call for advanced structure and design patterns. # What does it buy you? Design patterns serve three main purposes: make code easier to write correctly/verify, make it easier to extend, and make it easier to modify. Haskell's compiler and strong types give you these, provided that you use them. Since GHC does so much of the work that design patterns are for in OOP, they didn't evolve in the same way in Haskell. Many of the things design patterns attempt to solve are problems inherent to OOP/procedural programming. If you don't have those problems in the first place, then you don't need solutions for them. # ... so how do I structure my program? Well, batch mode programs are crazy easy. main = do inputs &lt;- getProgramInputs structuredData &lt;- parseInputs inputs let result = process structuredData somehowDeliverThe result Sometimes you need to stream, or interleave effects and data. Pipes and Conduit are good there. main = runConduit $ getProgramInputs .| parseInputs .| mapC process .| somehowDeliverResult Sometimes you need a long running server: main = do sock &lt;- makeSocket forever $ do request &lt;- receive sock forkIO $ handle request The biggest thing that helps over ordinary IO is `mtl` style type classes that delineate effects. Everything else is sugar.
IMO, Haskellers tend to focus on &gt;the philosophy behind them and how they are supposed to be used in order to design and build an application effectively a great deal. It's just that there isn't a clean, direct correspondence between OOP concepts and Haskell concepts, so you can't go around asking "how do I implement singletons in Haskell" or something.
Yes, that's true. Regardless, you don't see libraries implementing *program structures* in Java to the same *extent* that you see them in Haskell. e.g. [pipes](https://hackage.haskell.org/package/pipes-4.3.2/docs/Pipes-Tutorial.html)
/u/guibou's post here is my favorite one. I do not think guibou advocates emulating OOP in a functional setting; rather, it is that you should ultimately program the same way. While I do learn the idioms of any language I wield, I end up writing the code in a paradigm-agnostic way. GoF design patterns are obviated in a language with first-class functions. My C++ code does what my Haskell code does. Everything is reading in data, performing a transformation, and writing out new data: compilers and algebras.
I submitted an internship application in December, but never got any response. Have you been reviewing them on a rolling basis? Should I resubmit now that the deadline is closer?
LYAH was really helpful for me. I think the best way to use it is to stop at every code example and try to do it yourself before you look at his code. So, for example, when he says "Here is how foldr is implemented:" you try to write your own foldr, perhaps looking at the type signature for a hint.
You're welcome! Yes, hackage/stackage for 3rd party libs. And most people tend to use libs from central repository, which is ok, since you can be sure it works well with other libs - stackage, but sometimes you need something experimental from hackage or plain repo. IMHO, react-flux binding gives you ability to typecheck some calls and do proper dispatching on events and state transition, but you still have a lot of mess with JS. FRP is good for async and handling many events, if you need more of the rendering that you probably won't get much benefit, just different syntax.
When would one practically use a type level list or similar?
**Yes.** It's a feature that most authors don't care about—I suspect most people who upload a package don't even realize the field is there. I think there was even some talk about removing it on one of the mailing lists, but I guess nothing came of it.
That would be really cool--I've been looking for this as well. I use Servant and would really like to use GraphQL in my projects.
Thanks a lot for the info! And I would strongly support such a removal, I'm sure I'm not the first person to have this worry.
kept getting an error when trying to download VS itself. does the plugin do the following: * jump-to-definition in a file which has compile errors? * fast auto-complete with function signatures and a snippet of the accompanying API doc? * show you occurrence of values of type X throughout your app?
I'm kind of dumb, so this example was the first usage of type level lists that I actually saw myself using. I'm always fascinated by type level stuff but had never before understood a use case I might actually want to implement. This is the first. Thanks for the example.
I too submitted an application some time ago.
The old stuff doesn't have to be accessed frequently so you could probably create a way to push the old state to slower, higher capacity storage. Perhaps even use a hybrid cloud storage type approach
I'm from an imperative/OOP background and I had noticed that I could fairly easily emulate OOP in Haskell by having a module per "class", and exposing only a smart constructor function for the data type (i.e. keeping its constructor private), and then exposing some "public methods". I haven't done this as it seems like it's cheating/lazy, that I won't be gaining as much from learning Haskell. The only real benefit I'd get is immutability.
There's my MSE thesis, though that's a lot more complicated than what unification-fd does. (unification-fd came from stripping out most all the theoretically interesting bits of my thesis.) Mea culpa! I've been meaning to write the rest of that series, but life and too many other commitments. The main issue about backtracking shows up when using ST/IO for the variables, since you need to undo the mutations on backtracking. This can generally be done by constructing a trace, à la the [WAM](https://en.wikipedia.org/wiki/Warren_Abstract_Machine), but there's still the inevitable issues about it being ephemeral and non-multithreadable. If you use the IntMap-based variables, all these issues go away. IIRC, I never got around to implementing a monad for automatically tracking traces (since the IntMap version was good enough for us); so that'd take a decent chunk of work (though it could be rolled into the library once the work is done). The other issue of note is one discussed in the logict paper itself and has nothing to do with unification: namely the issue about fair interleaving. LogicT provides a fair interleaving combinator, but it's a lot slower than the unfair interleaving. So depending on how you want to search to go, you need to bear this in mind when writing your unification programs, choosing the right one in each place. So far as I'm aware, this is a fundamental issue in any backtracking-based logic-programming system. All in all, logict is the best option I've seen since it provides one with all the knobs. To do better, you'd want to have a different heuristic than the top-down left-to-right depth-first search that Prolog et al use.
I'm sorry, I disagree. Wrapping your head around the following concepts is practically much tougher in Haskell: * Monads &amp; transformers * `lift` &amp; `liftIO` * `ReaderT` * `&lt;$&gt;` and `&lt;*&gt;` operator * How to do memoization * How to have a list containing different types of elements * How to have a map containing different type of keys &amp; values I can look through my notes and come back and add more stuff here.
Thanks so much for that info!
Not that I know of. There's some random stuff on HaskellWiki and various Github wikis. Might be nice to have someplace that's somewhat organized.
I really like LYAH but it was just plain outdated. In addition to the things you mention. 
Hi /u/vannnns thanks for your extension. Do you know about Haskelly (https://marketplace.visualstudio.com/items?itemName=UCL.haskelly). Is it possible these two projects could collaborate? Thanks again :)
I got into Haskell through it but... I also didn't really learn much haskell at all from it. No cabal, or even mention of hackage. 
&gt;Not saying you shouldn't learn Haskell or Purescript, but if you're specifically looking for a gradual introduction, Elm is the way to go. Conversely, if you know Haskell you can use all three. Elm feels like it's easier to use because it has fewer features - one of the things I like about Haskell is that by the time I was mad at the compiler I could actually implement a solution myself. 
&gt; Easier setup and ecosystem maintenance on your machine. I don't know what the ecosystem of PureScript is but I can hardly imagine it's more mature than Haskell's. And installing/using stack is incredibly easy if you are on linux/Mac. 
&gt; Latest nightly won't build because of a dependency conflict Well yeah that's why there's lts releases. Although I agree stack is not the best on windows. 
Sounds like a very complicated architecture for something that RDBMSs has already solved on commodity hardware. 
 data Exists f = forall a. Exists (f a) data Given c a = c a =&gt; Given c a type AnyShow = Exists (Given Show) Now you can use e.g. `[AnyShow]` for a heterogenous list of types which can be shown.
A real world example would be servant's content types. Any endpoint for a request that you define looks something like this: `Get '[JSON, PlainText] Foo`. The middle part here is a type level list. Servant uses this to enable serving of multiple content types depending on what the client accepts. The same also happens with `ReqBody` to denote what content types the server accepts in a request body.
Combine this with haxl and you got an awesome stack. 
&gt; I've not found any way to express foo ^. bar . (both baz quux), which makes their utility as an API for GraphQL somewhat limited. IIRC, `Lense`s can be "summed" with `&lt;&gt;` from `Data.Monoid`: ghci&gt; ('a','b')^..(_1&lt;&gt;_2) "ab" The result is only a `Fold`, not a valid `Lens`. Also, the ["alongside"](http://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Lens.html#v:alongside) function [can be used](http://stackoverflow.com/questions/26722458/using-a-lens-to-read-multiple-fields/) to create "product lenses" of a sort, but one must first provide an lens from your source type into a tuple, to "prove" that the focuses do not overlap.
I'm sorry, i can't help you for vscode installation issues. About plugin features: - jump to definition is working (to go to errors/warnings and type definition site) - auto-complete works for symbols (for current and imported modules) but i don't provide type signature analyzing to assist typing on the fly. For snippets or linter, you can install dedicated extensions like https://marketplace.visualstudio.com/items?itemName=hoovercj.haskell-linter - not implemented yet
This should be even faster, and language agnostic: fibonacci n = fst (fib n) where fib 0 = (0, 1) fib n = let (a, b) = fib (div n 2) (c, d) = (a * (b * 2 - a), a * a + b * b) in if mod n 2 == 0 then (c, d) else (d, c + d)
The same as the advantage of `List a` over `IntList`.
Hi, thanks for your reply. This plugin is a language server implementation (protocol defined for IDE language services), so technically, it's VSCode agnostic, even if everything is packaged for a vscode extension for now. What i mean is that, functionalities like pining a popup info, changing shortcuts, and so on are VSCode job, and not the extension one. For instance, the shortcut to trigger "showHover" at cursor position by default is defined as followed: { "key": "ctrl+k ctrl+i", "command": "editor.action.showHover", "when": "editorTextFocus" } You can change it in vscode keybindings configuration. For pining the popup, i don't know if the feature is already requested in the vscode repository, but you can check at their repo : https://github.com/Microsoft/vscode/issues For synatx highlight issue, i'm going to check it :-)
&gt; Now I did not get the @-mention from that issue because I don't watch the Stackage repository. That fixed I now get lots of notifications that are not interesting to me. Just change the Watch button in the GitHub project to "Not Watching" which is "Be notified when participating or @mentioned." http://i.imgur.com/YyMabCD.png You probably enabled "Watching" which is "Be notified of all conversations."
Regarding design patterns in general, Peter Norvig has a classic talk on [Design Patterns in Dynamic Languages](http://norvig.com/design-patterns/). He is mostly thinking about Lisp, but the combination of first-class functions, laziness, and parametric types mean that almost everything applies to Haskell. The tl;dr is that one big reason Haskellers don't talk about design patterns as much is that they are better equipped to take a design pattern and turn it into code. Elsewhere, I've got a [worked example of this](https://jml.io/2016/08/anti-patterns.html). In practice, when you build code at scale, you often define data types that correspond to some aspect of your problem domain (e.g. `User`, `Booking`) and a cluster of functions for that data type (e.g. `createUser :: Name -&gt; Email -&gt; User`). These aren't so far removed from object oriented programming.[1] Where you'd use interfaces, in Haskell, you'd most likely use type classes. There are differences, but it's a great start. The classic OO principles of encapsulation and abstraction and polymorphism apply very cleanly to Haskell, as do each of the points of [SOLID](https://en.wikipedia.org/wiki/SOLID_(object-oriented_design)) (whether or not they are _good_ principles is a separate discussion). To your broader point, I feel your frustration. The idioms of writing Haskell code are very different from writing OO code, and it can be hard to know where to begin, or how to proceed. However (with all respect to fellow commenters), no one is going to post a Reddit comment that has a few quick, bullet-proof guidelines for designing 1,000,000+ line apps maintained by dozens of programmers for over half a decade, to pick one definition of "large scale application". [1] Well, at least as often practiced in Java, Python &amp; Go. It's not sending messages to objects in the classic SmallTalk sense. 
[removed]
Throttling seems to be the problem. If you look at the issue, after 50 entries only the handles of people in the first 50 entries are highlighted. `vector-split` gives me one of the last places so I am below that threshold. Filing an issue now. [Done](https://github.com/fpco/stackage/issues/2250)
He says that database management systems existed before the relational algebra, not SQL.
The research buzzwords (Cyber-Physical Systems etc.) don't actually mean that much. I'm loosely associated with Peter Thiemann's [group](http://proglang.informatik.uni-freiburg.de/) on programming languages, but the MSc programme spans all [divisions](http://www.informatik.uni-freiburg.de/divisions) of the department of Computer Science. As far as GPA is concerned, I'm afraid I have no idea what the acceptance rates are like. In general, Freiburg is very strong in AI, robotics and machine learning, so if that's interesting to you, the Master should be great. My own focus is broadly on programming languages, and unfortunately Mr. Thiemann's is the only research group in this area, which is why Freiburg is not a terribly attractive option to me. For more details, check out the [course list](https://campus.uni-freiburg.de/qisserver/pages/cm/exa/curricula/genericRailsSearchUnitsSimple.xhtml?_flowId=searchCourseOfStudyForModuleDescription-flow&amp;_flowExecutionKey=e1s2&amp;noDBAction=y&amp;init=y) (which takes ages to load and I had trouble even finding. Freiburg is definitely better than its website...). If you have any other questions, of course feel free to ask.
/u/dmwit are you considering remote applicants? Or is this a on-site only position?
For now, ff it messes with intero, you should restart vscode too. But as it's a language server, restarting the server should be enough, we'll investigate a way to do so (you can fill an issue to track the evolution)
when I started to discover Haskell in 2016 it takes my attention and make me excited to deepen my knowledge in the functional paradigm and other paradigm as well and the fact that it needs knowledge in mathematics and Categorie Theory make me excited to continue my master degree in a related field (my background is telecom engineering unfortunately ). Actually I am not sure weather Frieburg would be an ideal destination or not so I would like to hear from you based on your experience . Thank you for your help :) 
The point of an experience report is to share an experience that others could learn from. When you were writing the symengine bindings, were there any sort of tricky interactions between the type-level programming and the FFI? Did you learn something about this sort of programming that isn't widely known? If so, you might have material for an experience report. The interpreter sounds like it would be more suited to a "system demonstration" than an experience report. Especially if it has potential as a teaching resource like the recent STG interpreter. You should know that experience reports have a pretty low acceptance rate. I don't mean this to discourage you, just be prepared. Even if your report is rejected, the process of writing it up and getting reviews will probably be good experience. And /r/haskell might appreciate the report regardless. Finally, https://wiki.haskell.org/HaskellSymposium/ExperienceReports seems to have a good bit of advice about writing experience reports.
I am doing so, I'm not advocating the use or creation of such a database, but I am pointing out that it's really not that complex or that much of a downside. 
I already live in the Portland area, and I've been a professional Haskell dev for over a year. I didn't come from the comp-sci education side, though, but started from humble web design and climbed the web-dev ladder (and am mostly self-taught). I would *love* to work at a place like Galois, but I worry everything they do comp-sci wise would be over my head. Honestly, should I bother applying? Thanks!
Yes, this is a program transformation technique, either at the source or the compiler level.
Wow, thanks for doing this! I hope the project gains steam and becomes a full IDE (e.g. refactoring support, add type signatures with a click/key chord, package management features like writing our cabal/stack files based on adding files, etc.).
Yes, you will need to work in one of our physical locations. We have tried to support remote employees before, but our organizational structure makes that very difficult to maintain: the flip side of great flexibility is a greater need to stay connected and communicate often, even with people you are not immediately working with.
Hi! I am not familiar with the symengine code, but I'd say it definitely sounds like it would be in scope for the Haskell symposium. The template instantiation in Rust sounds a little less on topic. I'd certainly encourage you to submit something. My experience has been that writing things down is a very useful experience, which helps me clarify the ideas, and explain the design choices I made. And if you can make the early deadline (in just under a month and a half), you essentially get two tries, with feedback from the PC in-between!
The background you describe might be a tough sell, but the thing that comes to my mind is that [@shitmydadsays](https://twitter.com/shitmydadsays/status/4811790555) quote.
Or change the bounds on hackage. There's no need for a new version if changing package metadata is sufficient.
ohhhh my god of course it's a monoid
I wouldn't worry about writing it as a standard library. Database engines are so unique that it's worth it for the initial libraries to support the entire set of features (or to at least be designed with the full set in mind). Later, someone or you can write an HDBC driver for it using your lower level library Or you can improve odbc support. The Haskell FFI to C is very straightforward, so if you can do what you want in C you will likely be fine there.
Well... And *then* I found out that there probably were to many mentions in that issue... but you already know that :-)
If you're asking me, maybe I spoke unclearly - I meant to be saying the same. I haven't tried it since `-XTypeApplications` came out. I just checked that this does indeed make stuff like put @Int 2 &gt;&gt; put @Int 3 &gt;&gt; get @Int type check while put 2 &gt;&gt; put 3 &gt;&gt; get has ghci inferring three independent states. So you can conveniently have many states writing say &gt;&gt;&gt; let program = get @Int &gt;&gt;= \n -&gt; modify @String (take n) &gt;&gt;&gt; run $ runState @Int (runState @String program "haskell") 4 (((),"hask"),4) ... though frankly I'd just as soon use a tuple and `Control.Lens.zoom` and the like. I had been tending to follow one of the methods /u/Darwin226 mentioned - defining, say specialized `getInt` &amp; `putInt` which isn't so bad, but is irritating. 
You're not wrong of course. But I often feel like Hackage revisions are this nifty trick that saves space on the one hand, but removes information from package versions and makes dependency management a lot more confusing on the other. I sometimes wonder if we should stop doing Hackage revisions and instead allow Hackage trustees to publish new package versions. I believe that the security implications would be quite the same, and I'm not convinced that the space savings that revisions offer are worth it.
There was this thread from last year: https://www.reddit.com/r/haskell/comments/4c9en3/when_to_prefer_a_reader_monad_over_a_function/
&gt; Stack works best if you just follow the instructions on their site. Idk why the pacman stuff is a pain but that's on arch not stack. System repositories are great for binaries (e.g. pandoc) but not libraries since they're often pretty far behind. If you follow what I declared, being _not_ up-to-date is not an issue I mention. Quite the contrary, as Arch is rolling release and the maintainers are quite nice, I found it as a nice solution to stay up-to-date, and it does work in that sense, I just don't have the patience. If I'd grab a binary outside from my package management system, and had to manage it myself, it would be exceptional in that sense, b/c I don't have to do that with any other programming language I use.
This is awesome work, and a great point of reference for building spam filtering in other contexts. Thank you!
You want to modify the `mysql` package's Setup.hs most likely. It is the one calling `mysqlconfig`, the program producing the C compiler and linker flags for `libmysqlclient`.
&gt; I care about not having to do mundane, boring things such as testing whether my package still compiles and then bump the dep ranges on hackage. I don't think that Hackage revisions save you much, if any work here: AFAIK Hackage revisions are only allowed to replace what would otherwise be a sub-minor version bump. And these shouldn't cause you any compatibility checking anyway, provided that you have set the right version bounds in your cabal file. Apart from that, let's hope that something good comes from https://github.com/haskell/ecosystem-proposals/pull/1.
Oh my. Turns out I am, in fact, blind. Thanks.
Ah, thank you for the feedback :)
If a group of functions that are interdependent expect to have some common value shared between them, then `Reader` might be appropriate. foo pos x y = ... bar pos (x + y) ... bar pos z = ... quux pos "hello" ... quux pos cmd = ... foo pos (length cmd) 3 ... In this example, we're passing `pos` around, but that value is expected to remain constant. We can use `Reader` to ensure that we don't accidentally pass the wrong value of the right type in our mutually recursive set of functions. If you might expect wanting to change the value, but in an explicit/scoped way, then `local :: (r -&gt; r) -&gt; Reader r a -&gt; Reader r a` can be used. This makes changing of the environment explicit: foo = ... local (+1) bar ... Now we know that the environment in `bar` for that execution is one greater than the environment in `foo`. If you're already in a monadic context, then adding `ReaderT` to the stack is syntactically easy. I'm a fan of the `MonadReader` type class, which has instances for `ReaderT` *and* `(-&gt;)`, so you can write a function: foo :: MonadReader Int m =&gt; Int -&gt; m Char foo = do pos &lt;- ask pure 'a' and you can use it with any instance of `MonadReader`: runReaderT foo 3 :: IO Char runReader foo 3 :: Char foo 3 :: Char 
Would you release the dataset? Maybe someone with free time can help train better models.
Can you give a example how that can work ? 
Hayoo reveals the following: [(\^?)](http://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Fold.html#v:-94--63-)
Well shows how bad I am are searching through the dependency docs. Really appreciate the answer and the tip regarding Hayoo.
I understand the benefit of `Given`, but I was originally asking about the benefit of `Exists` (and making it a newtype). In particular, we can just have data Any c = forall a . c a =&gt; Any a If you'll look a few comments up, I was asking about a post that said "Yeah GHC should really allow newtype Exists f = forall a. Exists (f a)". 
&gt; Before broadcasting a transaction, the DB creates a small proof-of-work of it - basically, a sufficiently small hash of the App code. Other nodes only accept transactions with enough PoW. This takes time to compute, so you essentially create a "portable" anti-spam measure for a distributed network that replaces the need for fees and an integrated currency. Not quite. This just shifts the cost from a fee deonominated in, for example, bitcoins, to a fee denominated in power costs. If your argument is that the power costs are too small to matter, then it's not a proper denial-of-service protection method in the first place. The point of Bitcoin is that, rather than have everyone inefficiently hash on their CPU, a dedicated party takes care of the proof-of-work hashing (with efficient equipment), and we just trade these proofs using digital signatures. 
[removed]
I think with both Simons not enthused about it, it probably won't make it.
No, hackage revisions can tighten or loosen bounds with a pretty free hand. As noted elsewhere. This is important from preventing older versions of packages from acquiring bad install plans.
For what it's worth, just the regular `a -&gt;` type forms a reader `Monad`, so you don't really need to choose. You can just write your program as functions who take in their common argument as the *last* argument, and then, freely switch between monadic and non-monadic style. So for example, import qualified Data.Map as M import Data.Maybe (fromMaybe) type VarName = String type Context = M.Map VarName Int readVariable :: VarName -&gt; Context -&gt; Maybe Int readVariable varName ctxt = M.lookup varName ctxt add :: VarName -&gt; VarName -&gt; Context -&gt; Int add var1 var2 = do val1 &lt;- fromMaybe 0 &lt;$&gt; readVariable var1 val2 &lt;- fromMaybe 0 &lt;$&gt; readVariable var2 return (val1 + val2) Under the hood `Reader` is exactly this type of course, but you don't need the `newtype` to use it. It's only really offered to form a nice orthogonality with `Writer` and `State`.
Generally, I would suggest that if you see a weird operator and don't know where its from immediately, 9/10 times its lens.
Honestly, I disagree. Most of that stuff (sans memoization) is a lot more intuitive to me than a bunch of ad hoc concepts in OOP, because they all build off the basics. Haskell at it's core is a very internally consistent language where every new concept is a natural extension of previous, simpler ones. Your mileage may vary though. It may just be personal preference when it comes to what's more intuitive to you.
- [Atom](https://atom.io/) + [Plugins](https://atom.io/packages/ide-haskell). - [Intellij](https://www.jetbrains.com/idea/) + [Plugins](http://haskforce.com/). - [Visual Studio Code](https://code.visualstudio.com/) + [Plugins](https://marketplace.visualstudio.com/search?term=haskell&amp;target=VSCode&amp;sortBy=Relevance) I recommend them in that order. As for every other Haskell IDE, no thank you. Ease of use and functionality are of equal importance to me. Keep in mind, Visual Studio + C languages is hard to beat. These are just the best options Haskell has available at the moment.
I agree with most of this. One important note: orphan instances might be a minor wart *for application code*, but they are a **major** wart for libraries. Do not put orphan instances in libraries when neither the class nor the type were defined by you. Two packages defining the same instance will cause *massive* headaches down the line—they'll cause conflicts for anybody who depends on both, even transitively.
&gt; The hypothesis is that txs would cost that much less in real money, in average. The problem with asking clients to hash on their CPUs is that a CPU is literally 1,000 times less efficient than just a barely functioning ASIC. So, in other words, if your DB system ever catches ground, all an attacker needs is to buy some special hardware in order to spam your server. Attackers, who make a living from it, can easily afford to buy special-purpose hardware using their profits, while simple users of your database cannot. This increases the cost for clients to a point where, in my opinion, it would make the whole thing a lot more expensive in the end than just exchanging and verifying digital signatures. The good thing about signature verification is that it's fixed difficulty. With your protocol, you'd have to adjust the difficulty in the presence of an attacker with dedicated hardware. This is basically what the Bitcoin protocol handles for you. Also, it almost certainly wouldn't make sense to pay for each request. If you just require a payment of, say, 1/10th of a cent, paid up front, per 1M requests, then the per-request verification time becomes insignificant (one signature per 1M requests). For what it's worth, I'm in the process of writing [a library](https://hackage.haskell.org/package/bitcoin-payment-channel-1.0.1.0) that can be used for off-chain Bitcoin transactions.
Wow, thanks for the hard data!
Did you experiment with this earlier? I could have sworn at some point (maybe one or two years ago now) I had to remove a URL from an otherwise innocuous paste to have it go through. 
Oh. Well the only difference between newtype and data is that one of them introduces an additional indirection and bottom and the other doesn't. You rarely *want* the extra bottom, and fewer indirections are better for performance. And `Exists f` doesn't need to store an extra dictionary or anything so it *could* be represented as a newtype. `Exists f` can also be used to represent other existential types, like [folds](https://hackage.haskell.org/package/foldl-1.2.3/docs/Control-Foldl.html): data FoldF a b x = FoldF (x -&gt; a -&gt; x) x (x -&gt; b) type Fold a b = Exists (FoldF a b)
"Maxims and Arrows" is borrowed from Nietzsche. It seemed appropriate.
Quite adept. As in, fluent.
How does this work with `-&gt;`?
I really hoped for the carrot operator, shame it was a caret one. :-P
Ho, ok :) As i'm relying on intero to understand haskell source code, if intero can't do it, my extension can't do it neither
It is tested on linux and windows. I'm working on a clear auto diag process to explain why something goes wrong and how to get a clean install
[Here is a list of a few companies that use Haskell - all are outside of the USA](https://www.reddit.com/r/haskell/comments/5pan74/looking_for_haskell_companies_outside_usa/).
And what about inside the USA?
Seems to be most of them using Haskell for finance stuff.
Well, I'm thinking more about the stuff you'll need to know. I know my way around monads and type systems. I'm starting to get GHC extensions. What more would I need to know? I don't have a clue as to what Haskell developers would need because I don't in contact with them.
This sound awesome. I am currently working on an application with ~100 endpoints, so this is sorely needed. Franckly I would be really happy to have a preliminary version on hackage, even without documentation.
I couldn't find where the reference to the actual mysql lib was made and it's indeed in the `mysql` package `Setup.hs` file. It doesn't refere to any version though, which is kind of a good news, meaning I can build it againsts any version of `libmysqlclient`. I managed to solve my problem by just changing the link `libmysqlclient` to the version I needed.
I work at [Ambiata](http://ambiata.com/) (Sydney, Australia) where we do data science in the service of marketing. Working in Haskell in a team of about 15 talented and enthusiastic engineers is awesome. Haskell is very definitely production ready.
This is the only solution that I (as a noob to vim/emacs) have been able to get working first attempt, and without issue.
Incidentally, how are the compile times with such huge types?
Yeah, servant alone is bad enough for that :/
I think the magic OP is referring to is the one with `traverse` (or the slightly less general `sequenceA = traverse id`), which is really the swiss army knife of FP. I don't really have time to write down a worked example, but you can check the first part of [What are types for, or are they only against?](https://youtu.be/3U3lV5VPmOU) or [The essence of the iterator pattern](https://www.cs.ox.ac.uk/jeremy.gibbons/publications/iterator.pdf) 
I was defining a vector class for use in some low-level 3d code. The usual implementation is basically `data V3 = V3 Float Float Float`. But it turns out you can write a lot of code much more simply with `data V3 a = V3 a a a`. The Applicative instance basically falls out of that definition. For example, the Applicative version is easy to make an instance of `Num` with much less boilerplate than the non-parameterized version: instance Num a =&gt; Num (V3 a) where (+) = liftA2 (+) negate = liftA negate -- etc for the other operations fromInteger = pure . fromInteger -- etc for Fractional, Floating, Monoid, ... This lets you write really simple code, that reads a lot like HLSL. It also lets you turn all of the nice support that Haskell has for custom number types onto your vector type. I had a shader with some quaternion code in it, and it wasn't clear to me whether a certain optimization was correct. I whipped up a quick symbolic math library and ran symbolic quaternions through both paths and subtracted the results from each other to find out if they were different. It turns out the two codepaths gave different results, but the error in the optimized version was zero if the input quaternions were normalized. I don't know any other language that lets me mix low-level math with high level abstractions of that math quite so easily, and Applicative is one of the best tools to help make that possible.
Oh, I also use tons of TH, so all modules are bad ;)
Can you give an example of removing type variables using quantification, in order to simplify things for the end-user? I've recently started using type variables a lot, together with classes, because it ties different pieces of my code together in a way that makes me feel confident about the solidity of the model I've created. I prefer this to sum types, since I can see, from the type signature, what I'm working with. When it comes to parsing arbitrary input, though, is there any other way than creating a sum type? As far as I can see, sum types are useful as the return type for a function, because you're asking the function something, rather than telling it something (by specifying a type). As an argument to a function, however, I prefer a class of types to a sum type, as it exposes the type of whatever I'm working with. Does this make sense? **EDIT:** Also, is there a way to easily derive `FromJSON/ToJSON` and `Serialize` instances for my types if they contain existentially quantified types? I usually use `-XDeriveGeneric`, but this [doesn't appear to work with existentials](https://ghc.haskell.org/trac/ghc/ticket/10514).
Sure, I would like to provide a link to dumps somewhere as part of the back-up cronjob. 
It becomes unavailable to look at, and moves into a list of reported pastes. As the admin I have a page to review the list of reported pastes, and I can either mark it as spam, delete it outright, restore it, or disregard it. In your example I'd just restore it.
May I ask, how do you structure your types? Do you prefer mostly flat hierarchy (one type per end-point) and then you just smash them all together with `:&lt;|&gt;`, or maybe you have some nesting? If so, do you use something akin to 'Canonicalize'? Do you use servant authentication (it kind of affects how one lays the types - it's tempting to mark group of endpoints with auth protection, but then it's troublesome to write handlers)?
Awesome. Would it be possible to do the same thing for server handlers as well?
I disagree with the points on strictness and optimization. You should think about strictness as you are writing your code. It is either semantically meaningful or at least better documents the semantics you have in mind. It's not that difficult to get right. Reasoning about efficiency is not a dark art restricted to compiler writers. It's worth taking the time to understand.
Oh man, this is amazing! I'm essentially doing exactly this manually for my APIs already. Can't wait to upgrade :D
A redux is a reducible expression. I'm guessing a beta redux is a beta reducible expression.
I was just about to suggest FEECa
It has the downside of incurring a significantly higher performance cost. data V3 = V3 {-# UNPACK #-} !Float {-# UNPACK #-} !Float {-# UNPACK #-} !Float Has a significantly smaller memory footprint. If you're only working with `V3 Float`, it's unfortunately worth specializing. We see exactly this in the `ad` library, which has to double it's API to provide abstract parameterized classes, and `Double` specific variants: https://hackage.haskell.org/package/ad
I'm guessing the poster meant "redex" for "reducible expression", rather than "redux"?
Some languages even forbid orphans entirely!
There is. Instead of making a new type class and using existential quantification, you can use a record. Just write the class methods as record fields. It'll simplify the code and will support Generics.
It’s when I see things like this that I think that Haskell could do with whole datatype strictness annotations. I suspect that for most use cases data types are all-or-nothing when it comes to strictness. That might be a side effect of my personal programming history though - does anyone have strong counter-examples where mixed strict / non-strict types were just the thing?
Yep, that's the idea. It would be very useful.
Yes, and I was confusing those terms. Replace "redux" in my comment with "redex".
I spent a few hours with somebody in #xmonad once tracking down a bug that ended up being caused by a stupid and clearly wrong strictness annotation on one of the fields of the `XConfig` type. When I asked the author why it was strict, the answer was essentially "because it looks prettier if all the fields have an exclamation point than if one doesn't". Anyway, yeah, strict function fields and strict list fields are kind of dumb, and those types are occasionally usefully mixed with more primitive field types.
I'm not sure I follow exactly what you mean. Can you give an example?
Sorry, yes, I was being mildly humorous. I noticed that others were writing long sets of technologies joined with '+' which I didn't think was very Haskelly. However, I do think all the technologies I included are very effective and work well together. I've now deployed two fully functional sites using that stack.
The thing I like about Yesod is that it has batteries included and you don't need to add anything else to get a fully-fledged site up and running very quickly. However, if you need to customise things, there are lots of entry points for you to get in and do that. Yesod has a very succinct way to define forms, and if you use the Bootstrap support they look great as well. Reflex is very powerful, and although (when I last used it) documentation was still a little lacking, Ryan Trinkle (the author) was amazingly responsive and helpful, which more than made up for it.
What are the problems with `StdGen`? I know its `split` is broken, and I believe that's the main motivation behind `tf-random`. But splitting remains quite niche. Aren't you fine as long as you don't split?
&gt; You need at least (..) to produce a random number (..) A pure function that produces a new number from that seed. (“RNG”) &gt; Better would be to separate out IO as much as possible from the inevitable rest of our program. Except I *still* don't see how. Lemme explain: what I *really* want, and I reckon this is a newbie issue, from any *random* lib is demand (this can be in a monadic or IO context) a "non-monadic/pureish" generator function that I can more easily pass over to that one remote part of my app very-deeply-buried in a long chain of non-monadic calls all over various modules until ultimately reaching there. Whoops now I'd need a simple RNG here. But we've been non-monadic all the way down here, and while I'm getting all kinds of "handler funcs" passed down here from the "app"/`main`-context-of-sorts I can't just enter do notation (or &gt;&gt;/&gt;&gt;= "for a bit") and smoothly get out of it again in the middle of, for lack of a better word, "pure"/non-monadic-ish "normal" function.. and I'm sure as heck not gonna refactor that whole beautifully-clean code path down there into some extra context requiring do or &gt;&gt;/&gt;&gt;= throughout! This article seems more approachable in this respect but I'm still left scratching my head. In fact I make do with the program's start time and a seed and use it with a neat-but-lame-ish "list shuffle algo" that doesn't require any official random/RNG machinery but truth be told it's not *quite* as random as I'd like!
servant-client-0.10 is there for you.
Go for it.
https://www.reddit.com/r/haskell/comments/3x15sm/why_is_the_first_random_value_produced_from_a/ Really bad statistical properties in general.
I'll update the post with a link to `TFGen` where I have mentioned `PureMT`; thanks for the feedback!
I use a "hierarchy" of TVar, MVar for shared state and STRef for imperative algorithms... didn't need the "raw power" of `IORef` just yet.
You can get quite far by just"strict data + lazy functions".
How are they bigger? Maybe I'm missing something, but I thought pointers on a 64-bit system would take 64 bits and floats normally take 32 bits.
Yeah, OK: I’ll admit that a datatype which contains a list or a function probably shouldn’t be making those strict without very good reason :) Thinking about it, recursive datatypes in general would be bad candidates for strictness. for the same reasons presumably. I guess I was thinking of struct-like datatypes of fundamental machine values which often seem to have ! and unboxed compiler magic instructions liberally sprinkled everywhere.
Is `(a -&gt;)` actually a Monad instance? Wow I never knew this.
The sample with the state monad is not really fair: dieRoll :: RandomGen g =&gt; State g Int dieRoll = do gen &lt;- get let (result, newGen) = randomR (1, 6) gen put newGen pure result is the same as `state (randomR (1, 6))`. That's still more boilerplate than the `Rand` version `getRandomR (1, 6)` but less strawman-y.
That's exactly what `MonadRandom` and the `State` approach does; haskell just won't let you hide implicit function arguments or side effects without something like `State` or `Reader`. For most of the blog post, the generator function is, for all intents and purposes, `StdGen`. You need a new generator function every time you want to generate a new value, because otherwise you'd continually get the same value out (due to referential transparency/purity). In most of the examples, `IO` is only needed to get an initial generator. You can use `mkStdGen &lt;seed&gt;` to get a generator function from a `seed` without touching `IO` at all. Don't be afraid of the Monad stuff. It's just being explicit about your program state. It might feel weird, but ultimately it's a good thing. The big idea behind writing this flavor of blog post is to avoid talking unnecessarily about the dirty details of everything, and just provide a kicking off point.
&gt; Have faith. This speaks to me &gt; Always be looking for patterns. Without conscious thought I developed something of an internal checklist of clearly defined, simple patterns and their behavior that I constantly cycle through while developing in Haskell—in the back of my mind anyway. &gt; Persevere in getting an abstraction just right. When you find it, everything will magically fall into place. Not every pattern that matches these checks helps but fortunately it doesn't take long to notice that your square peg won't fit the round hole. When the patterns **do** match I gain too many things to count: immediate and intimate familiarity with the code, laws, an ecosystem, relationship to the ecosystem, *google*ability and *github*ability, papers and tutorials. And the Haskell community is always on the hunt for new patterns. It is for those reasons that learning Haskell has become synonymous with familiarising myself with these patterns. When my problem doesn't fit the patterns either I split my problem up or ask myself if I'm solving the right problem. This has proven to be a great rule of thumb, although there was one case where I spent months trying to cram what I wanted to code into Haskell's type system.
Reader is just read-only state. A very common use case for it is to store program-specific configuration variables in your `Reader`. It's not the only use case, but that might provide some intuition.
Solga http://github.com/chpatrick/solga already did this, it is one of the reasons /u/vahokif made it.
Thank you all for your comments. There is a lot of material posted here which is very interesting and useful.
You need to understand them at the library level. At that point they're just DSLs. You can introduce &lt;$&gt; and &lt;*&gt; for a specific instance, and then show it's actually an abstraction. After that you can use the other Applicative instances pretty intuitively. Tbh it's hard in Haskell because you have to look quite a bit for an introduction which does it that way. But that's the way the official Elm tutorials do it, and it works. The problem is learning resources, not the language. Luckily this is slowly getting better for Haskell. 
Does leksah work on a Mac these days? I was very enthusiastic about it a few years back but it stopped working on my Mac.
I am really excited and hope to be accepted. What is the procedure after getting accepted? Will there be interviews?
You probably mean a "beta redex", which is short for a "beta-reducible expression". It basically is any expression of the form: (\x -&gt; y) z Beta-reduction is similar function application but not exactly the same thing. The key difference is that the function has to be a known lambda expression to be a beta-reduction. To illustrate the difference, consider this expression: \f -&gt; f 1 The `f 1` at the end is a function application, but the above expression is not beta-reducible (and therefore not a "beta redex") because we don't know what function `f` is and therefore we can't reduce any further until somebody provides us with a specific `f`
It depends on how your program uses the random number. If the random number is used to, say, seed a hash function, but ultimately there are *no observable effects at all* to an outsider, then you can use something like [unsafePerformIO](https://hackage.haskell.org/package/base-4.9.1.0/docs/System-IO-Unsafe.html#v:unsafePerformIO) (with great care!) to disguise the effects. This should only be used at the encapsulation boundary. On the other hand, if the result does actually depend on the random number you get, then it would be a very bad idea have a pure function return a result that is fundamentally indeterministic. Not only will it surprise the caller, it means the results of the program can change depending on how the compiler optimizes your code. If you're not a fan of monadic code, you can always just pass in a seed (or ask for an arbitrary random number generator) and then return the new seed afterward.
&gt; Without conscious thought I developed something of an internal checklist of clearly defined, simple patterns and their behavior that I constantly cycle through while developing in Haskell—in the back of my mind anyway. If you ever feel like writing down that internal checklist please share it! It might be quite useful for less experienced Haskellers (like me). :)
At [IMVU](https://imvu.com/next) we use Haskell to: * serve our higher performance rest endpoints (legacy endpoints and non performance sensitive are in php) * distribute eventually consistent configurations to applications across multiple languages * manage a cluster of asset metadata * transform timeseries data via a unix like dsl We are trying to use Haskell more and more everyday. (edit: formatting)
Please explain why. I'm on mobile and am not sure what advantages `tf-random` has, and can't easily look it up.
As I understand it, GHC doesn't actually have a monomorphization step (outside of the special case of applying the monomorphism restriction). I believe a monomorphization step would prevent polymorphic recursion from working, which is why impure/strict ML-based languages generally do not support polymorphic recursion.
This is so cool. Haskell spans an incredibly wide spectrum of power tools.
Yes. I really don't understand where the confusion is. You can factor out the `Any c` into two separate definitions, `Exists f` and `Given c`, but because GHC requires *all* existential datatypes to be `data` rather than `newtype`, this ends up incurring an additional indirection, which it wouldn't if GHC's restriction were slightly more finer-grained and only applied to existentials which actually bind a constraint.
Everyone else here seems to understand what the motivation for this is, but I don't quite get it! Could you add an example of why you would want to use this to the readme?
Supports git like Xcode does. cf: http://community.hfm.io/hc/en-us/articles/213724557-How-do-I-use-Git-with-Haskell-for-Mac-
Yes, good idea. I don't see why it should not work. 
https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#server-side-programming
The readme says: &gt; it can be hard to manage two complex lists of API endpoints (one of types, and one of handlers), making sure to keep them in the same order. I understand what that means, but I'm pretty familiar with Servant. It boils down to this: When you define an API with Servant, you list the endpoints *in order*. type API = FirstEndpoint :&lt;|&gt; SecondEndpoint :&lt;|&gt; {- and so on ... -} If you want to define a server for that API, you have to define the handlers (things that handle requests) in the *same* order. server = firstHandler :&lt;|&gt; secondHandler :&lt;|&gt; {- and so on ... -} This is fine if you don't have many endpoints because you can visually see that they are the same. If you have hundreds of endpoints, you can't do that. The type system will catch any type level problems, but it can't catch everything. By giving each endpoint a name, you can avoid having to keep the API endpoints and server handlers in sync. 
Seller Labs uses Haskell for some internal APIs, websites, and worker processes. You seem curious about web APIs. We're finding that Servant is an amazing tool for this job. It is especially valuable for internal APIs, as we get client generation/documentation for (nearly) free that we would otherwise have to spend time creating. Developing a new endpoint is super fast and safe. It's also pretty fast -- our peak usage has been 200 requests per second, maintaining ~10ms average latency (according to AWS load balancer metrics). 
I think so? The way that `servant` provides is basically `zip (api :: [Endpoint]) (server :: [Handler])`. The way that `servant-named` provides is like having `api :: Map String Endpoint` and `server :: Map String Handler`, then making sure that every key in `api` has a corresponding key in `server`. 
I mean you just kind of can't. At least not safely. That very directly violates the whole "purity" and "referential transparency" thing that the entire language is based on. So no, you cannot and should not even try to do that. Put passing in a seed or even just a random number or two directly through that chain to the place it is used shouldn't be that big of a deal.
If you're partial to the classic editors, then Emacs + [intero](https://github.com/commercialhaskell/intero) or Neovim + [intero-neovim](https://github.com/parsonsmatt/intero-neovim) are both excellent options, although the Neovim plugin is still fairly young/immature. Otherwise, see the other replies in this thread.
Well, there is a [Unicode symbol for it](http://www.fileformat.info/info/unicode/char/1f955/index.htm)...
Honestly I would say for recursive data you should default to lazy. For example `[]`, `Seq` and `Tree` would be much worse if they were strict in the recursive part. 
wow that is impressive. from the readme it says it will switch over to pure haskell. curious why the decision to start with purescript and why the decision to change? Also, why a dedicated client vs. piggybacking on jupyter. I'm not a huge fan of jupyter myself, but nevertheless it probably requires non-trivial investment to maintain competitive feature/usability parity. Was there a particular motivation for going down a different path?
Wouldn't making the method of detecting spam transparent allow spammers to generate spam that won't be detected?
You can use the `Random` methods `randoms :: (RandomGen g) =&gt; g -&gt; [a]` or `randomRs :: (RandomGen g) =&gt; (a, a) -&gt; g -&gt; [a]` and pass the resulting stream to pure functions. Then they can pass a modified stream back up if necessary—at which point it makes sense to switch to `State` or something anyway. This does represent a pain point in Haskell, though: pure code and monadic code have different notations, so adding effects generates a bunch of busywork. Functions can be polymorphic in *which* effect they’re using, but not *whether* they use an effect, hence the proliferation of `map`/`mapM`, `foldx`/`foldxM`, `filter`/`filterM`, &amp;c.—one option, albeit not ideal, is to use `forall m. (Monad m)` and `Identity` everywhere.
Galois is in Portland, they do lots of Haskell
I don't think they are that sophisticated. Mass spammers spam millions of websites. It's way cheaper to move to a new website with less spam filtering capabilities than to try to adapt to a single site's spam filter. They only try to adapt to a spam filtering technique if it becomes very widespread, in a way that moving to other sites isn't paying off enough, or if that website is a important part of their revenue. It's like running from a tiger: you don't need to outrun the tiger, you just need to outrun the other runner. 
Joke's on you: there's bangs [all over the place](http://hackage.haskell.org/package/containers-0.5.9.1/docs/src/Data.Map.Internal.html#Map) in the definition of `Map`, even the lazy one. Same for `Set` and `Seq`. They're typically lazy in the contained elements, but spine-strict (and in `Map`'s case, key-strict as well).
Oops I meant to delete that comment not submit it. My other one was more accurate. 
The order of endpoints in the API type is surely the one used to determine routing precedence. Yes? That isn't documented in your library though.
You're right, I used the wrong adjective there, I didn't mean to imply there was an actual difference in expressive power, is more like `join` and `&gt;&gt;=`.
In my head I imagine GHC applying unpacking in cases of specialised of functions that only call other specialised functions (i.e. they don't then pass the value off to an opaque polymorphic function). I imagine it's significantly more complicated than that though.
No worries, just wanted to clarify that, and yeah it is very similar to `join` and `&gt;&gt;=`.
She has 3 step children (a couple are teenagers), a 6 yr old son, and twin baby boys. She is also a Girl Scout leader. A class in Haskell is quiet and much easier to deal with. &lt;grin&gt;
They use PureScript for the front end. You know PS can compile down to Javascript? So they're using Javascript's Electron framework to make the front end. Probably want to switch to Haskell for front end so they can use bindings to a library like QT.
Well, I'm not arguing for strict data by default. I'm arguing for understanding whether you want strictness or not. 
Floats and double are automatically unpacked if strict, regardless of the machine word size.
Oh, I think I misunderstood your question. You should be able to specify any dependencies that you need with `--package`. Maybe add some info on the command you're running and the error you're seeing. ---- Here's my original answer: AFAIK, the `--package` flag is only available for the `ghc`, `runhaskell`/`runghc` and `ghci`/`repl` commands. For e.g. `stack build` you'll have to specify the `extra-deps` in a `stack.yaml` or make a [custom snapshot](https://docs.haskellstack.org/en/stable/custom_snapshot/).
Can you post the stack.yaml of the surrounding stack project (possibly `~/.stack/global-project/stack.yaml`)?
You can find the stack.yaml by running `stack path --config-location`.
Your example is for clients whereas this library is for the endpoint handlers, but you're right. This seems like a pretty good pattern to adhere to which would definitely obsolete my library! 
You're right it's not helpful for handlers. The pattern I've noticed recently is that the signature of clients and handlers are the same except for the choice of Monad, so you can define mtl style classes to generalize both: class Monad m =&gt; MonadUser m where getUser :: UserName -&gt; m User postUser :: UserName -&gt; m Bool instance MonadUser ClientM where getUser = client (Proxy @ GetUser) postUser = client (Proxy @ PostUser) instance MonadUser Handler where getUser = getUserFromDatabase postUser = insertUserInDatabase server :: Server API server = getUser :&lt;|&gt; postUser I don't know how useful this style is but I thought it was cute.
Is it possible to use Electron + natively compiled Haskell? Or some other HTML + CSS renderer, such that the UI logic is handled by Haskell code compiled by GHC? I think such a minimalist HTML5 + CSS renderer without JS baggage would be useful for many programming languages, especially ones that have mature compilers to JS, so that they can use the exact same code to build webapps and then also release efficient desktop applications.
Facebook, Galois, ThoughtBot. I believe a number of U.S.-based financial institutions use Haskell in production (and Jane Street uses OCaml, which shares a lot of ground with Haskell).
What about websockets?
The example starting [at this point](https://youtu.be/soKl1IslU-I?t=554) seems to be an evolution of [Trees that grow](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/trees-that-grow.pdf).
I'm not sure what you mean, could you give an example ?
the program looks promising , unfortunately it is quite expensive 
Yes, but that's "just" the messaging protocol. IHaskell uses this under the hood I think
What I've done in "big" applications/services is something very similar. Instead of defining one type per endpoint, I'd just regroup them "logically" into groups of at most 5-6 endpoints. And then compose bigger and bigger APIs from there, until reaching the root, as you walk up the module hierarchy. Avoided the need for any such thing too, but it does add a little bit of work.
Does lambda calculus have no precedence rules for application?
About 1 weeks ago i managed to install it on Windows. It worked. But later it stopped working, despite i did not change anything.
Even humble Functors have some `deep magic' in lenses.
I am not sure whether we say that the calculus itself has a precedence rule (or merely any concrete syntax we choose). Without parens, I would pick the Haskell default of left-associative function application (because it makes the most sense for curried functions). So what dramforever wrote needs explicit parens.
Bloomberg uses OCaml in London and New York for some very strategic, mission-critical projects. We have a few pieces of Haskell here and there, too; but nothing I know of in production.
you can fill an issue here to help us finding the bug:https://gitlab.com/vannnns/haskero/issues
How do you then avoid allocating anything? Since even primitives are boxed in Haskell, I don't really see how you could ever be sure no GC will be needed.
I am now confuse. It is left our right associative?
I wonder why nobody attempts to decouple the HTML5+CSS renderer from JS completely :/
Let's say you need types to represent geometric shapes. With sum types: data Shape = Triangle | Rectangle | AnyPolygon [Point] render :: Shape -&gt; [Point] myShapes :: [Shape] With type classes and existential quantification: class Shape a where render :: a -&gt; [Point] myShapes :: [forall a. Shape a =&gt; a] With records, and this can be combined with the first method too: data Shape = Shape { render :: [Point] } myShapes :: [Shape] triangle :: Point -&gt; Point -&gt; Point -&gt; Shape Which of these is the *right* way to do it depends on your use-case, however. They all have their advantages and disadvantages.
This only works If you know the upper limit of allocations in that thread and set the initial size to that or higher. If you can't tell this or the limit varies widely or the maximum is too high, then the trick isn't practical.
You can use my [Variant type](http://hsyl20.fr/home/posts/2016-12-12-control-flow-in-haskell-part-2.html) which can hold values of different types. Then you just have to define some type classes as follows (I use an Int to distinguish the different types in the JSON encoding): {-# LANGUAGE DeriveAnyClass #-} {-# LANGUAGE DeriveGeneric #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE UndecidableInstances #-} import Data.Aeson import GHC.Generics import Haskus.Utils.Variant ------------- Boilerplate -------------- class ToJSON' x where toJSON' :: Int -&gt; x -&gt; Value instance ToJSON' (Variant '[]) where toJSON' = undefined instance ( ToJSON' (Variant ts) , ToJSON t ) =&gt; ToJSON' (Variant (t ': ts)) where toJSON' n v = case headVariant v of Right t -&gt; toJSON (n,t) Left ts -&gt; toJSON' (n+1) ts instance ToJSON' (Variant ts) =&gt; ToJSON (Variant ts) where toJSON = toJSON' 0 ------------- End boilerplate -------------- data Foo = Foo Int Int deriving (Show, Generic, FromJSON, ToJSON) data Bar = Bar Int Int deriving (Show, Generic, FromJSON, ToJSON) data Baz = Baz String deriving (Show, Generic, FromJSON, ToJSON) type T = Variant '[Foo,Bar,Baz] main :: IO () main = do let t0 = setVariant (Foo 42 43) :: T t1 = setVariant (Bar 42 43) :: T t2 = setVariant (Baz "Test") :: T print (encode t0) print (encode t1) print (encode t2) Result: [1 of 1] Compiling Main ( json.hs, json.o ) json.hs:29:1: warning: [-Worphans] Orphan instance: instance ToJSON' (Variant ts) =&gt; ToJSON (Variant ts) To avoid this move the instance declaration to the module of the class or of the typ wrap the type with a newtype and declare the instance on the new type. Linking json ... $&gt; ./json "[0,[42,43]]" "[1,[42,43]]" "[2,\"Test\"]" You can use the same method for the decoding.
But how do you ever know? That's what I'm asking. I guess it _only_ makes sense for short lasting threads.
You profile threads and pick those that have a predictable allocation pattern. Short running threads are an obvious pick but long running threads can also have stable allocation with minimal to no fluctuation. Take for example some of the Unix daemons on your system. There are many that never allocate more than they already did and have a stable memory profile. If you have a thread that fluctuates between 5MB and 2GB, you probably don't want to set the initial size to 2GB unless you're certain of the benefit. If the thread will stay at 2GB and is unlikely to shrink, then given performance requirements and machine specs, it can make sense to set initial size to 2GB.
I had a typo on the old post, so I resubmited, sorry!
The idea is that if it stays below the initial size, then GC will not kick in. As you noted, this is easiest and most likely to be applicable for short running threads. I don't know if GHC's GC has the required knobs to achieve this for long running threads. My experience is with Erlang's VM and this may not be applicable to GHC8's GC without new features.
Did compilation just get slower again? 
/u/dmwit thanks for the reply....guess there's not sense in applying then since I'm not in the US. 
Link is 404 for me.
I think it would have to since this is making the core rep more complex, though it's arguably worth it. I develop with GHC -O0 anyway.
Notice the large benchmark benefits are all in terms of `bytes` - size of the resulting object file I expect. The smaller benefits, and a few of the more notable regressions, are `seconds`.
Those are bytes allocated during execution, not the size of the object file. Many of these test programs are what you would call "microbenchmarks", which perform one specific task and use all of their compute time and memory for that task. In real world programs, e.g.: a web server with multiple operations that it can perform, excess memory usage in one part of a program can greatly increase execution time in another due to cache thrashing and increased GC pause times. I would expect these results to carry over very favorably to real world Haskell programs, where the reduction in memory allocation for these microbenchmarks may translate to the same but for some recursive functions executed within long-running server applications. 
It's allocations. Not file size.
In case someone is wondering, the results from [the wiki](https://ghc.haskell.org/trac/ghc/attachment/wiki/SequentCore/) shows that binary sizes are roughly the same.
Weighing in on this, it is really cool that you're making this. In the last few years, we've seen a lot of IDE projects surface and die out. If you collaborated with the haskero people, the thinking here is that the community would benefit from a better supported product. It's something to consider anyway.
Unless GHC is one of the, judging by the benchmark, 40% of programs that got slower.
Some of these won't make the code better but may instead have a knock-on effect. You have to apply judgment. # Terms The terms aren't as interesting and others can probably think of more interesting examples. [*pointfree*](http://pointfree.io/) does a lot of these things for you so I won't spend more time on them. * `f x = print (replicate x x)` can be `f = print . join replicate` # Data Structures * I will massage structures like data RGB = RGB Word8 Word8 Word8 into data RGB a = RGB a a a deriving (Functor, Foldable, Traversable) of kind `Type -&gt; Type` which has all the instances of [`V3`](https://hackage.haskell.org/package/linear-1.20.5/docs/Linear-V3.html) and can derive many of them. * If something is a [`Product`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Functor-Product.html), [`Sum`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Functor-Sum.html) or [`Compos`-ition](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Functor-Compose.html) of functors you get instances for free data A a = MkA (Maybe a) [a] data B a = MkB₁ (IO (Maybe a)) | MkB₂ [[a]] | MkB₃ a | MkB₄ Int data C a = MkC₁ a | MkC₂ (ZipList a) can be modeled as type A = Product Maybe [] type B = Compose IO Maybe `Sum` Compose [] [] `Sum` Identity `Sum` Const Int type C = Sum Identity ZipList * If something is a `Sum` where one of the summand is `Identity` you can get additional instances `Applicative`, `Alternative` that can't be defined for `Sum`s by using [`Lift`](http://hackage.haskell.org/package/transformers-0.5.2.0/docs/Control-Applicative-Lift.html) data Lift f a = Pure a | Other (f a) type C = Lift ZipList * If your data type is an instance of [`Representable`](https://hackage.haskell.org/package/adjunctions-4.3/docs/Data-Functor-Rep.html#t:Representable) (as `RGB` is) you get `Monad`, `MonadFix`, `MonadZip`, `MonadReader` and `Distributive` for free. All of these shapes are representable functors data V0 a = V0 data V1 a = V1 a data V2 a = V2 a a data V3 a = V3 a a a ... data Stream a = a :&lt; Stream a * See if functions are [`Free`](https://hackage.haskell.org/package/control-monad-free-0.6.1/docs/Control-Monad-Free.html) or [`Cofree`](https://hackage.haskell.org/package/free-4.12.4/docs/Control-Comonad-Cofree.html). data Free f a = Pure a | Free (f (Free f a)) data Cofree f a = a :&lt; f (Cofree f a) If your data structure is an instance of them, you get all their instances for free. Many common data structures are really free/cofree like binary trees (`Free V2`) data Tree a = Leaf a | Branch (Tree a) (Tree a) [`NonEmpty`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List-NonEmpty.html) (`Cofree Maybe`), infinite [`Stream`](https://hackage.haskell.org/package/Stream-0.4.7.2/docs/Data-Stream.html) (`Cofree Identity`), rose trees (`Cofree []`) data NonEmpty a = a :| [a] data Stream a = a :&lt; Stream a data Rose a = a :&lt; [Rose a] # Kinds * Anything with kind `Type -&gt; Type` (like `[]`, `Maybe`, `Either a` …), see if it can be made `Functor`, `Applicative`, `Monad`, `MonadFix`, `Alternative`, `Traversable`, `Foldable`, `Representable` ... ([*Counterexamples of Type Classes*](http://blog.functorial.com/posts/2015-12-06-Counterexamples.html)) * Anything with kind `k -&gt; k -&gt; Type` may be a [`Category`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Category.html) so if I encounter a kind (k -&gt; Type) -&gt; [k] -&gt; (k -&gt; Type) -&gt; Type I massage it to type Cat k = k -&gt; k -&gt; Type [k] -&gt; Cat (k -&gt; Type) and see if it makes sense as a category. * Having a polymorphic kind is a good sign, like [`Product`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Functor-Product.html), [`Sum`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Functor-Sum.html), [right](https://hackage.haskell.org/package/kan-extensions-5.0.1/docs/Data-Functor-Kan-Ran.html) and [left Kan extensions](https://hackage.haskell.org/package/kan-extensions-5.0.1/docs/Data-Functor-Kan-Lan.html), [*n*-ary products](https://hackage.haskell.org/package/generics-sop-0.2.2.0/docs/Generics-SOP-NP.html#t:NP) and [sums](https://hackage.haskell.org/package/generics-sop-0.2.2.0/docs/Generics-SOP-NS.html#t:NS), lifted fixpoints of functors and [free categories](https://www.reddit.com/r/haskell/comments/52f77p/how_would_you_express_the_free_category_in_haskell/) (‘thrists’) Product :: (k -&gt; Type) -&gt; (k -&gt; Type) -&gt; (k -&gt; Type) Sum :: (k -&gt; Type) -&gt; (k -&gt; Type) -&gt; (k -&gt; Type) Lan :: (k -&gt; Type) -&gt; (k -&gt; Type) -&gt; (Type -&gt; Type) Ran :: (k -&gt; Type) -&gt; (k -&gt; Type) -&gt; (Type -&gt; Type) data NP :: (k -&gt; Type) -&gt; ([k] -&gt; Type) where Nil :: NP f '[] (:*) :: f x -&gt; NP f xs -&gt; NP f (x ': xs) data NS :: (k -&gt; Type) -&gt; ([k] -&gt; Type) where Z :: f x -&gt; NS f (x ': xs) S :: NS f xs -&gt; NS f (x ': xs) newtype Fix2 :: ((k -&gt; Type) -&gt; (k -&gt; Type)) -&gt; (k -&gt; Type) where In2 :: f (Fix2 f) a -&gt; Fix2 f a data TList :: Cat k -&gt; Cat k where Nil :: TList f a a Cons :: f a b -&gt; TList f b c -&gt; TList f a c # Types * If a type looks something like `(_ -&gt; _) -&gt; _` I will see if it matches a [continuation](https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-Cont.html) newtype Cont r a = Cont ((a -&gt; r) -&gt; r) it may also be of the form`(_ -&gt; m _) -&gt; m _` in which case it might be newtype ContT r m a = ContT ((a -&gt; m r) -&gt; m r) If our function is polymorphic in the `r` like withFile :: FilePath -&gt; IOMode -&gt; (Handle -&gt; IO r) -&gt; IO r it forms a [`Codensity`](https://hackage.haskell.org/package/kan-extensions-5.0.1/docs/Control-Monad-Codensity.html) monad instead (this is what Gabriel's [*managed*](https://hackage.haskell.org/package/managed-1.0.5/docs/Control-Monad-Managed.html) package provides with a less scary name) and as always you get all the instances and functions for free. newtype Codensity m a = Codensity (forall r. (a -&gt; m r) -&gt; m r) withFile' :: FilePath -&gt; IOMode -&gt; Codensity IO Handle If the `m`'s differ they form a right Kan-extension newtype Ran m₁ m₂ a = Ran (forall r. (a -&gt; m₁ r) -&gt; m₂ r) * The above can be dualised, with left-kan extensions, `Density`, etc. That's all I can think of atm
Ah, thanks!
And one of the speed decrease is fannkuch-redux, which is one of the test that had a -99.99% drop in allocation, which is unexpected and [it's going to be investigated ](https://ghc.haskell.org/trac/ghc/ticket/13225)
Well I did warn you in advance about the whole more verbose thing haha: class (FooF a ~ b, a ~ FooR b) =&gt; Foo a b where type FooF a type FooR b (!) :: a -&gt; Int -&gt; b instance Foo [Int] Int where type FooF [Int] = Int type FooR Int = [Int] (!) = (!!) The speed thing I read [here](https://wiki.haskell.org/Functional_dependencies_vs._type_families#Advantages_of_TypeFamilies).
&gt; Haskell and Stack Can it be used without Stack?
those are apparently now Richard's top priorities for 8.2, after they've punted on Type vs Constraint thing for this release cycle, according to this mail http://mail.haskell.org/pipermail/ghc-devs/2017-February/013712.html
Back almost five years ago when I was successfully applying, being able to solve your standard interview problems in Haskell was basically enough. So looking at Hacker Rank and doing their challenges in Haskell is a good start. (Not sure whether it'll be enough today. But I'd suggest to just give it a go.)
ohh, sry; saw it being high priority and owned by goldfire but didn't notice the explicit bump to 8.4 . Well there's still hope; depending how quickly the other stuff gets merged I guess. ORF stuff seems to have a good chance of getting merged apparently so maybe there's time then.
hi, haskero dev here. I'd be pleased to see how we can imagine a solution with several extensions to facilitate the maintenance. As VSCode support extension dependencies, haskelly could be the main extension, with dependencies on haskero which provides Language server support and on justusadam.language-haskell for snippets and color. Haskero could stick on its specialty : language server (type at, goto def, errors, warnings, find usage, etc.) and the main role of haskelly would be to provide haskell special features like explain in the readme: - build - launching tests And many more asked by our users : - insert type signature above a line - running sample code in current scope - goto definition in hackage - assist typing on the fly for calling a function of know signature - etc. what do you think ? :)
Hi i'm the haskero dev. When you reuse open source code under licence, don't forget to **mention the authors**. e.g.: https://github.com/haskelly-dev/Haskelly/blob/master/src/Providers/InitIntero.ts#L79-L101 https://github.com/haskelly-dev/Haskelly/blob/master/src/Providers/InitIntero.ts#L48-L53 and so on. thanks :)
The script interpreter will now give a more detailed error message for this case - see https://github.com/commercialhaskell/stack/issues/2879 . If you don't want to upgrade to the latest git version of stack, a workaround is to just run the command from the commandline (`stack --install-ghc runghc --package turtle-1.3.1 something.hs` instead of just `./something.hs`)
Wow! This will take some time to digest. I'm already pretty amazed at the Product / Sum / Compose stuff (and slightly baffled too!). Thanks a lot for sharing this!
Also, haskelly was awailable before haskero ;)
Well apparently I now am saying that, looked at injective type families proposals and realized you could do the following: type family List a = b | b -&gt; a class HasList a where fromList :: [a] -&gt; List a (!) :: List a -&gt; Int -&gt; a type instance List Int = [Int] instance HasList Int where fromList = id (!) = (!!) newtype UnitList = UnitList Int type instance List () = UnitList instance HasList () where fromList = UnitList . length (!) (UnitList n) i | i &lt; n = () | otherwise = undefined :t fromList fromList :: HasList a =&gt; [a] -&gt; List a :t fromList [1 :: Int] fromList [1 :: Int] :: [Int] :t (!) (!) :: HasList a =&gt; List a -&gt; Int -&gt; a :t ([1, 2, 3] :: [Int]) ! 0 ([1, 2, 3] :: [Int]) ! 0 :: Int fromList [()] ! 0 () To get the above to compile you need `TypeFamilies` (obviously), and `TypeFamilyDependencies`.
Yes, i suffered from this trouble before. that is to encode all environment into a fixed record, which defeat all purpose to modularize. 
They posted some interesting stuff in the February "Who's Hiring" on Hacker News. I'm too lazy to look it up but if anyone is interested it should be easy to find.
&gt; very-deeply-buried in a long chain of non-monadic calls You may find this library useful: https://hackage.haskell.org/package/hs-di Especially the latest experimental feature, monadic inject: https://github.com/Wizek/hs-di#monadic-inject-injio It allows you use IO in a deeply nested dependency chain without having to change all the intermediate functions from pure to monadic. All this **whithout breaking refferencial transparency**, still allowing the code to be pure and deterministic under the hood. If anything is unclear or you have questions about how to use it, feel free to ask me. It's quite late around here, but I would be happy to write up and post an example later if requested.
n.b. Takt is hosting BayHac 2017. https://wiki.haskell.org/BayHac2017
This leads to violation of global uniqueness of instances, can cause odd failures, and used to be able to violate the type system. It also makes the category of `Constraint` non-thin, which prevents the `Dict` trick for reifying instances not work.
Paired with some smart constructors and pattern aliases and it's nearly usable. (It's 12 kinds of pain to deal with a product of two compositions, one of which contains a sum, and the other contains nested composition.)
Haskell, Vim, Zsh, Arch Linux on an encrypted Btrfs in a Thinkpad... The Arcanists Choice ... in a good way :-)
Hi! Thanks for pointing this out. Right now we only use the standard commands, such us stack build and stack ghci. If you think that this will not work on a multi package project, could you please open an issue on github? It would help us keeping track of the things that need to be fixed! 
I should elaborate... Haskell and Vim both have a steep learning curve... You can (barely) use Vim like any other editor... And you can force imperative programming on Haskell. You learn to keep your functions pure and to keep your fingers on the home row. You learn how to compose functionality... Both by using `.` and by finding out about `*c*hange *a* *b*lock`. You study the *Typeclassopedia* and master move commands that take you to and from your last edits. You dabble in *type level programming* and find out that Vim has unlimited branching undo/redo capabilities. Well... Then you find there is so much stuff on Hackage and so many Vim Plug-ins that it'll take several life times to master them. You earned your entry to the Arcanum... but the journey has just begun.
Thanks for all the hard works! Crypto is a rocket-science tech in my mathless mind.
We're also using reflex on the front end. The biggest pain point for us is the long turn-around time. How do you combat that?
Hi vannnns, Thank you for the work you put into haskero. I'm so sorry for not mentioning you in the code. We used your code and wanted to see, if it works with Haskelly, but then decided to implement an own solution. Unfortunately, we forgot to delete your code, which is the reason why it was still there (we deleted it now). Sorry for that! If we are using your code, we will of course reference it! But we would like to invite you helping us to make Haskelly better and for this reason we created a github organisation and you are very welcome to join us! https://github.com/haskelly-dev/ 
I've been using Vim for 13 years perhaps, but Haskell for 2 tops. Whenever I learn a new language I never use an IDE, just Vim. This forces me to remember things and to not rely on shortcuts.
Huh, I thought injective type families only worked with closed type families. Cool!
which bit order to assume? :)
I don't know the full Takt team, but I know a lot of my friends here are working for them. If you're looking to do Haskell work professionally, this looks like a great option. Best of luck to both applicants and Takt!
https://github.com/haskell/bytestring/issues/110 , actually
Hi, no problem :) Did you see my comment about adding extension dependencies ?
Or emulate. 🙁
I think it can only be inferred for closed type families. But yeah with that extension it looks like you can avoid multi parameter type classes pretty well. Only needing them when you genuinely have an unrestricted relationship between two types / for when you have type functions of more than one argument. So I don't think functional dependencies is ever really needed, and I know the opposite is not true, as there are a fair amount of things that can be done in typefamilies that need undecidable instances or just straight up aren't possible.
This is awesome, will definitely try it!
I already [posted about this project a couple of months ago](https://www.reddit.com/r/haskell/comments/56ailh/herculus_purely_functional_spreadsheet_tools/). The beta version is finally live now and you're invited to try it out and play with it. Herculus is a way of manipulating relational data using a built-in statically typed functional programming language, with a spreadsheet-like user interface. In contrast to, let's say, MySQL, columns can not only have simple types such as `Text` or `Int`, but also complex types like `List (Maybe (Row from table X))`. The goal is to support arbitrary user-defined ADTs as column types in the future. You can use functions that are familiar from Haskell (like [filter](https://app.herculus.io/doc/formulas/#filter), [find](https://app.herculus.io/doc/formulas/#find), [maybe](https://app.herculus.io/doc/formulas/#maybe)) to transform the data in your tables, and you can of course also define your own functions. This makes it incredibly straightforward and fast to perform tasks that traditionally would require multiple queries, or joins. [Read the guide to see what this means](https://app.herculus.io/doc/guides/app/) in an example. I'm interested to hear your thoughts, am curious about what type of applications you imagine you would build with this, and which features you think would be great to have. I'm also still looking for co-founders.
Agreed. Most difficult part of Haskell for me was learning Stack, Cabal, then Nix. Now this? ... But maybe its better!
[removed]
If you add probabilistic models (just a random monad would be fine at first) to that it would be a hell of a tool for business analysts. You would be able to do things like "if my sales vary in this range, inflation varies in this range and interest rates have a distribution like this, what would me my revenue?". Together with this very nice database modeling tool you have, it would be a breeze to do this. Today people use Excel for that and it sucks in both directions: poor database modelling and poor analytical tools. Alternatively people use R for that, or Python, but then you need to be a programmer and you have to model your business process as code, which is not hard but can be difficult for business analysts and programmers alike. 
How does this get around nix stripping the dependency bounds out of my packages? That is the thing that has always stopped me using nix. I'd really like to use nix so I'm interested in this but when it converts working packages into ones with silent errors, or ones that can't be built that is a problem. The only solution I've heard is to make your own entire nix package repository and maintain it separately. Does this provide the automation to make that efficient or something?
Heh, yes I should have looked closer.
It's not a reason, but the computational and storage costs would be greater than many people expect from bitwise operations.
Please use `cpuid` to determine if AVX2 exists and call the correct routine conditionally. I proved this concept to myself, along with detecting if the compilation chain supports the extension, in cipher-aes128 (for aesni) and Vincent adopted this (plus cleaner code) in cipher-aes.