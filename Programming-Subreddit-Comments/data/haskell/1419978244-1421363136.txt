I found the answer to my question in the cabal user guide: https://www.haskell.org/cabal/users-guide/installing-packages.html#creating-a-binary-package I tried it and it indeed creates a package that does not include the source code. 
That's odd, "share-alike" should presuppose "in source form", just like copyleft in software, ala GPL.
Sounds good. Can't wait until the new Stripe stuff is live.
I will definitely look into it! In the meantime, do you have any suggestions for books/articles/blog posts I can read about the topic?
Well, the CC-SA is very general. For instance, you could use CC-SA for artwork (e.g. xkcd). Requiring that artwork be distributed in source form doesn't make much sense. Thus, CC-SA has no such requirement. The FDL is designed specifically for written documentation, so the language is much clearer on the matter. Unfortunately, the FDL also carries the 15 pages of crap :-./. **Edit**: I've managed to reduce it to 5 pages of crap. I'll put a nightly up pretty soon.
&gt; As an aside, recursive data types like this can be somewhat inefficient, since subsequent left-associated binds have to repeatedly traverse the tree, in pretty much the same way that left-associated appends have to repeatedly traverse a list, but just like we have DList there are ways to make Free more efficient. One other way to implement more efficient free monads is to use a [Van Laarhoven free monad](http://r6.ca/blog/20140210T181244Z.html). However this method is limited to those free monads generated by functors suitable for making command interpreters; fortunately this is one of the more common uses of free monads.
You could use mathjax 
Oh, believe me, I tried. Mathjax works fine for very simple formatting. However, for anything beyond the basics, it doesn't work very well.
Never had problems using mathjax, but I have to admit, that my usage might be far from complex :-)
This project is indeed quite awesome. Unfortunately I don't currently have a Git setup around, so allow me to post a couple of nitpicks here. Of course, some of them are quite subjective and I expect friendly disagreement. ;) The page numbers at the start of each bullet point reference https://gitorious.org/lysa/pdfs/source/8407b90d3a8ff468a1353e3074b25899eaf416fc:lysa-141230013149.pdf. - 9f: I'm afraid I don't really see the point of giving an ultra-quick introduction to sets, functions and proofs here. The motivating effect (hopefully) results from the last paragraph containing a bunch of cool theorems, but it is unclear how the aforementioned concepts would relate to them. (And I don't think it *can* be made clear without writing half a maths book first. ;]) - 13: repetition and ordering are treated inconsistently: {0,1} = {1,0} but {0,0,1} isn't even a set. I believe the standard approach is to define {0,0,1} = {0,1}. This maybe also helps with the intuitive notion of a union adding all elements of one set to another set, then throwing the duplicates out again. - 14: I'd say {0} \notin {0} (and the whole concept of sets of sets) needs an explanation. I recently taught a bunch of university freshers some basic set theory and it's remarkable how big of a problem the uniform 'type' of set elements is. - 15 (fn. 2): I find it slightly misleading to say that the distinction between "N := {0,...}" and "N := {1,...}" is "really important". After all, it boils down to one side writing "N \ {0}" and the other "N \union {0}" at various points. (To be clear, {1,...} needs to die a horrible death as far as I'm concerned. ;]) - 16: I'm a big fan of considering functions a special case of relations (with both defined in terms of subsets of the cartesian product of two sets) and introducing the latter before the former. You'll probably need relations somewhere anyway, and the restriction that produces functions [forall x: not exists y, y': y != y' and f(x) = y and f(x) = y'] is a nice dual to injectivity [forall y: not exists x, x': x != x' and f(x) = y and f(x') = y]. Maybe moving the usual properties of binary relations (symmetry, reflexivity, transitivity, closure) here instead of discussing them in passing with the Peano axioms could be useful as well. - 22: I think adding a hint that the exercises are *really really mandatory* would be useful here. Without union/intersection/difference, I'd imagine people would have a hard time later. - 22: Typo in the associativity of union as well as "X \ Y" (should be X \setminus Y).
Am I reading that correctly, that the bang patterns still don't prevent you from taking the fixed point? That seems counterintuitive.
The values in the syntax tree themselves are able to be lazy.
Sounds amazing! Really looking forward to reading it when it's done.
I think the main thing missing from my answer at the moment is a compelling explanation of *why* the typechecker conflates restrictions on rank and qualification in this context.
Me neither, but a MathJax-heavy page can rapidly become ungodly slow even on a modern browser.
For other physics / quantum information people interested in the more abstract side I strongly recommend Bob Coeck's papers "introducing categories to the practicing physicist" and "quantum pictoralism". They are entertaining and surprisingly accessible. They might even be good for you crazy abstract types interested in the physics side!
Right, that was one of my concerns, if it'd be "okay" for Haskell to just "copy" other solutions, or try something new. But I'm not sure how the Flow monad could be used to completely substitute other integration solutions. The main advantage of other integration solutions, like ESBs, is that they provide an abstraction over the whole process of the workflow. The developer only has to describe, declaratively, what components there are, which component connects to each other, what adapters to put on either "end", and some other stuff. They don't have to deal with redoing actions, catching exceptions, etc, they just draw a flow of a message coming from an adapter, going through some components (being transformed or routed in the process), and then going off on another adapter (or maybe being logged or something else). That is a nice abstraction in my mind, and it's the main abstraction I was thinking about when thinking about how Haskell could be applied to this problem. In my mind Haskell could provide such abstractions, making it easy to define such an integration system without all the hassle of XML programming and big clunky GUI editors (and lots of tweaking, unfamiliar errors when deploying, etc). For example, using an Arrow or Monad DSL to define the flow, which ONLY uses components, and it needs an adapter on either end. Then apply FP concepts (mapping, filtering, etc) to simulate other "big enterprise patterns" out there in a much friendlier way. Then just compile it and run. Bam, couldn't be simpler. Seems to me the Flow monad is still too low-level for this kind of stuff. Seems promising, but dunno, I think Haskell could bring so much more in this area.
You have to avoid a higher order representation if you want to analyze everything. It's tedious.
Personally, I started by implementing Hindley-Milner (the collect, then unify variant) and, through that, finally understanding all that Greek in those papers. There's also a reason why Idris' core is called just "TT", because it is, well, Type Theory, Idris dialect. Meaning if you get the gist of dependent types, you're pretty much done (modulo the Greek). The rest is mostly bureaucracy: Learning the exact laws of whatever concrete jurisdiction you're in and learning to enforce/not break them. But they're all similar, and especially in the book you might not be as precise and just do fast and loose but morally correct reasoning. No need to quote paragraphs on "It's bad to murder people, mkey?" If someone wants to tell you that `Type : Type` is sane, or even useful, mentally sort them into an asylum (i.e. the mathematics department), that's a stray formalist. So don't be afraid of it, the Greek is indeed the scariest part. [Here's some nice lectures](https://www.youtube.com/watch?v=ev7AYsLljxk). And if you have some off time, have a look at [computability logic](http://www.cis.upenn.edu/~giorgi/cl.html). Different thing, still developing, but very interesting, especially in cleanness of the semantics. More in the area of Type Theory, [HOTT](http://homotopytypetheory.org/) is hot shit right now. Probably not a good thing to use in a book just now, but might be worthwhile to have a look at to get a broader view on things.
Check out this blog post for an excellent motivation and comparison between set theory and type theory: https://golem.ph.utexas.edu/category/2013/01/from_set_theory_to_type_theory.html
* Are you going to add haskell code in the book or just talking about math? * Who are the audiences? * What else are you going to cover? Can you offer a content list? * What ground knowledge is required to read this? * Last but not least, why will this book help people understand haskell better?
I can't read the pdf https://gitorious.org/lysa/pdfs/raw/8407b90d3a8ff468a1353e3074b25899eaf416fc:lysa-141230013149.pdf
If you're somewhat mathy like me (I'm no Erdos), you could try looking at the Homotopy Type Theory book which is free to download on their website. It's meant to be an introduction for mathematicians by mathematicians, so it can be pretty dense but I'm at least able to understand the intentions if not the proofs. It's very interesting stuff though, it's changing how I'm looking at mathematics.
My thanks to Roman for raising an important point about the practice of separating abstract specification from concrete representation. I think he's giving up much too easily, however, especially considering the disadvantages of exposing (and thus committing to) one's data representation rather than an abstract interface. It's been *very* helpful to me to cleanly separate specification and representation in software design. (In contrast, whenever I hear people talk about FRP in terms like "graph" or "propagation", I know that they're missing the (denotational) essence of the idea and are thinking instead about a particular representation style and usually an unfortunate one.) Rather than giving up on denotational design, I recommend a nuanced perspective. Try giving your library/DSL more than one denotation, perhaps of overlapping subsets of your full vocabulary. Give each "language" subset as simple a denotation/interpretation as you can. Each denotation brings its own notion of equality (i.e., having equal denotations), and you can state and prove laws in terms of these equalities. In particular, look for laws in the form of type class morphisms, guided by the type classes inhabited by your (as simple as possible, while precise &amp; adequate) denotations. Rather than discarding denotation (or implementation-independent specification in general), apply it more finely. More denotations, not fewer.
About this electronic book, with the modern type setting facilities here in, it seems that it ought to be able to conjure some relevant facts about the nature of each expression. Consider that the cursor we use to navigate, this tells the machine our location of attention, it should, I think, be able to conclude about things we should like to know, such as connections to other parts of the text, symbol definitions, inflating expressions to less dense forms. Yes, I see a future in this. - C. Babbage 
 $ wget "https://gitorious.org/lysa/pdfs/raw/8407b90d3a8ff468a1353e3074b25899eaf416fc:lysa-141230013149.pdf"
The article concludes that: &gt; It is certainly useful to think about denotations of your entities in specific, simple contexts (like the evaluation semantics for a DSL); such thought experiments may help you better understand the problem or even find a flaw in your implementation. &gt; But when you are implementing the actual type, your best bet is to create an algebraic data type with all the information you have, so that when you need to extend the interface (or «leak the abstraction»), it won’t cause you too much pain. But this is always what I thought the point of denotational design was! FRP itself is a classic example here. Even if we give a denotation that's just a function "time -&gt; value" our implementation will probably hide a whole _lot_ of information inside of it to enable efficient execution in various contexts. Denotational design is also not a gospel, or a "best practice" or a rulebook or a replacement for UML or whatever. Its just that approach of trying, whenever and as much as possible, to think of the simplest thing that captures the "meaning" of the domain you want to model, and to check the sanity your more complex structures by comparing them against that meaning and making sure they don't violate the spirit of it.
&gt; I'm thinking of an experience similar to the "code editor as proof assistant" model you see in (IIRC) Agda, Coq, etc. There was a neat [demo](https://www.youtube.com/watch?feature=player_detailpage&amp;v=uFwh3Uv8Nrw#t=1695) of this in Idris at 31c3 (watch for 1m30s, 28:15-29:45). It would be really neat to see the same sort of thing in Haskell!
In the multi-denotation story I suggested above, you can also make interesting and useful connections *between* denotations, particularly which are refinements of which.
 &gt; Indeed. I was confused about Roman's conclusion as well. Of course, there will be representation detail, as an algebraic data type or whatever. I guess Roman means to fully expose this representation *to users* and then accept the inability to switch to radically different and more efficient representations. Much too steep a price for me. Thanks to the discipline of separating (denotational) specification from representation, I've explored wildly different implementations in FRP and graphics, while maintaining a single compelling, simple, and precise specification. For instance, code generation and quad-trees of infinite extent and resolution rather than graph traversal. 
Oh, crap :/. If you are on Mac, Linux, or BSD, you can use `curl` or `wget` to get it. I'll talk to the server guy about publishing them properly.
* Yes, I'm planning to add Haskell code. * Pretty much anyone interested in learning abstractish math. * https://gitorious.org/lysa/lysa/source/HEAD:outline.md is a very out-of-date outline. However, it contains a vague idea of where I want the book to go. * Elementary school math * In my experience, learning math helps with programming, and vice versa.
Can you do HTML renders?
[This was the follow-up post on concurrency using free monads](http://www.haskellforall.com/2013/06/from-zero-to-cooperative-threads-in-33.html). [This is the corresponding paper](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.39.8039)
The last question is tricky. Let me ask more specific, what is the basic knowledge in math do you think to learn haskell very well? And is your glorious book going to live up to this standard?
Uhm... I'd really like for that to be available in epub :)
Works for me? &gt; ("foo" :: ByteString) =~ ("f" :: ByteString):: Bool True 
What /u/barsoap has linked to about HOTT (the link contains a PDF of the original treatise on HOTT) and /u/echatav's link would probably give you some idea as to why /u/divip thought it's a good idea for you to incorporate type theory. Basically, if you're going to be doing categories, you might as well do (homotopy) type theory, which gives a "better" natural setting to do categories and leads naturally to higher categories, i.e. categories of categories of ... etc. However, HOTT is pretty much on the frontiers of our mathematical knowledge, and it's not quite clear how much of it you can use without crushing the more elementary concepts under its machinery. /u/divip has pointed out that there is no introductory material so far, and this is simply because this material is cutting-edge work-in-progress. On the other hand, people think there's a lot of promise in HOTT. These people, as /u/barsoap says, belong to mathematics departments... which is kinda true. So, yeah, if you're incorporating type theory, that could be cool, but it can slow down the project tremendously. Or that could be part 2.
As a proponent of type theory and constructive rather than set-theoretic foundations, I'm going to be conservative here and argue that there's a place for teaching simple stuff with naive set-theoretic semantics. _Really_ doing stuff with sets is tricky. But it seems like you're working at a fairly high, informal level. Since your readers may have more familiarity with set theory, and you want to throw a bunch of ideas at them quickly, I don't know if the type theory detour will serve you well. Furthermore, once you hit categories and category-theoretic-constructs, which I understand is the goal, you're better off with sets anyway. Now, you _can_ do categories without sets, and this leads to a much more general theory once you start enriching over various other hom-objects. But the traditional way of presenting a hom-set is very efficient and straightforward, and maps to the typical presentation of categories much more directly. The fact that you're building your categories in an ambient set-theoretic setting shouldn't really hurt here. Most importantly, it seems you're not teaching set-theory as a "foundations" so much as a good language to sketch concepts quickly in. And it certainly, at least for some concepts, some times, can fulfill that role.
Hmm, that is tricky. I would like to say that the programmer needs to be familiar with the concept of functions and types. I'm sure it's much more nuanced than that, though. To answer your second question, the book starts assuming the reader is familiar with arithmetic, and builds up from there. I would like to say that the book will live up to that standard, but I can't be certain until I write it.
In fact, those useful connections can be though of as morphisms in a category (where objects are specific denotations) and the syntactic denotation that Roman describes would be an initial or final object in that category (depending on how you define the morphism direction).
Thank you a lot. I did it.
Thank you too!
You could do this through typeclasses and associated type familes, perhaps? class Vectorize f where type Vectorized f :: * vectorize :: f -&gt; Vectorized f instance Vectorize ([a] -&gt; b) where type Vectorized ([a] -&gt; b) = ([a] -&gt; b) vectorize = id instance Vectorize (a -&gt; b) where type Vectorized (a -&gt; b) = ([a] -&gt; [b]) vectorize = map This will run into `OverlappingInstances` and requires associated type families; I don't even know if this will work. Either way, it's not something that is idiomatic Haskell – you generally always want to know the types involved, and so you would just use `map` directly to "vectorize" an unvectorized function wheneve it was necessary. (tl;dr you probably don't want to do this, but it's probably possible using advanced type system hacks)
This sort of feels like sheaf semantics.
Not bad! ghci&gt; _ ord "test" :: [Int] &lt;interactive&gt;:7:1: Found hole ‘_’ with type: (Char -&gt; Int) -&gt; [Char] -&gt; [Int] Relevant bindings include it :: [Int] (bound at &lt;interactive&gt;:7:1) In the expression: _ In the expression: _ ord "test" :: [Int] In an equation for ‘it’: it = _ ord "test" :: [Int] ghci&gt; :hoogle (Char -&gt; Int) -&gt; [Char] -&gt; [Int] Prelude map :: (a -&gt; b) -&gt; [a] -&gt; [b] Data.List map :: (a -&gt; b) -&gt; [a] -&gt; [b] ... ghci&gt; map ord "test" [116,101,115,116]
Hmm, then I must've done something very wrong, thanks for the input. And which versions of BS/Regex.TDFA are you running?
By default, Haskell does not tag values. But you can do it yourself: data Value a = Scalar a | Vector [a] vectorize :: (a -&gt; b) -&gt; (Value a -&gt; Value b) vectorize f (Scalar x) = Scalar (f x) vectorize f (Vector xs) = Vector (map f xs) instance Functor Value where fmap = vectorize
Yeah, it is rather difficult to get this on an iPad, where I'd actually read it. This kind of material is my kind of bed side reading, plenty of interesting dreams as I guess my memories try to form and make associations :) 
Do you really have a use for it? There isn't much difference between f 1 And map f [1, 2, 3] Sure, it's handy in Python when it's a lot more common to have functions that accept different types of arguments, different numbers of arguments, etc, but in Haskell you usually already know exactly what types you're working with and therefore don't really have to reach for overloading.
First, another typo on page 22, second paragraph, last sentence should change: `...injective function $f : X− &gt; Y$...` to `...injective function $f\colon X\to Y$...` &gt; 13: repetition and ordering are treated inconsistently: {0,1} = {1,0} but {0,0,1} isn't even a set. I believe the standard approach is to define {0,0,1} = {0,1}. This maybe also helps with the intuitive notion of a union adding all elements of one set to another set, then throwing the duplicates out again. Really? I can't see how. The axiom of extensionality tells us a set is an equivalence class of "gadgets" which contain the same objects at least once. Multiplicity doesn't matter.
&gt; I can clearly see that RC is dependent on RL: &gt; &gt; class RegexLike regex source =&gt; RegexContext regex source target where &gt; &gt; And RL, in turn,'s dependent on &gt; &gt; class Extract source =&gt; RegexLike regex source where &gt; &gt; And that — Extract — does indeed have a ByteString instance. So then RegexContext Regex ByteString ByteString should exist, but it doesn't. Wait, that's not how parent typeclasses work. If `Eq a =&gt; Num a` and there is an `instance Eq Foo`, it doesn't automatically mean that `Foo` is also an instance of `Num`. Rather, it means that if you try to implement `instance Num Foo`, the compiler will complain unless it can also find an `instance Eq Foo`. Now, let's look at the problem in question. λ: import Text.Regex.TDFA λ: import Data.ByteString λ: :info (=~) (=~) :: (RegexMaker Regex CompOption ExecOption source, RegexContext Regex source1 target) =&gt; source1 -&gt; source -&gt; target λ: :info ByteString data ByteString [...] instance RegexMaker Regex CompOption ExecOption ByteString instance RegexLike Regex ByteString instance RegexContext Regex ByteString ByteString instance Extract ByteString So it looks like ByteString has the necessary instances after all. And indeed, I can specialize `(=~)` with `source1 = ByteString`, `source = ByteString` and `target = ByteString` without any complaints about missing instances: λ: (=~) :: ByteString -&gt; ByteString -&gt; ByteString &lt;function&gt; And I can call it just fine: λ: let foo = (=~) :: ByteString -&gt; ByteString -&gt; ByteString λ: :set -XOverloadedStrings λ: foo "foo" "foo" "foo" And, in fact, I can't reproduce the missing instance error you got unless I go out of my way to import `Regex` and `RegexContext` but not the `instance RegexContext Regex ByteString ByteString`. The later is defined in `Text.Regex.TDFA.ByteString` and indirectly imported by `Text.Regex.TDFA`, at least in regex-tdfa-1.2.0. You could try importing `Text.Regex.TDFA.ByteString` directly to see if it helps in your version. λ: import Text.Regex.TDFA.Common λ: import Text.Regex.Base.RegexLike λ: import Data.ByteString λ: :set -XFlexibleContexts λ: :set -XOverloadedStrings λ: let f = undefined :: RegexContext Regex a a =&gt; a -&gt; String λ: let s = "foo" :: ByteString λ: :t f s &lt;interactive&gt;:1:1: No instance for (RegexContext Regex ByteString ByteString) arising from a use of ‘f’ In the expression: f s 
Do you mean a poset category, ordered by inclusion of the induced equivalence relations (as sets of pairs)?
I feel like Sheaf doesn't really need to be something that's "clawed up to". It's a really nice simple intuition with just enough formality. Its tragic that it's usually not presented without topology and contravariant leads. Goguen has a good paper on this somewhere...
Vectorizing is common in Matlab and R language for numerical computations. It would also improve the readability. I don't know if it is possible due to Haskell be statically typed.
&gt; Since your readers may have more familiarity with set theory, [...] I don't know if the type theory detour will serve you well. Partially I agree. On the other hand, this attitude supports the status quo of an inferior framework. This is kind of similar to the situation in which Haskell is not used more in industry. I would really be interested in an experiment in which people were educated with type theory instead of set theory, in a playful way, not with academic training.
I do licensing for a living, so if I may offer a suggestion that is what I would generally see as a best practice having looked at literally hundreds of thousands of these. In the code files include a stock header comment identifying yourself as the original author, some way if getting a hold of you and a copyright stamp. Also state briefly what the license us, give a URL to the project and any other info you want to be sure gets passed along. Include this short text in the final form of your doc also, so it appears in print but only takes a few lines. In the root of your project have a file called COPYING which contains the full license text and nothing else. You could also call it license.txt, but tradition with the license you chose is COPYING. The short text should look something like this: Learn Yourself An Algebra http://gitorious.org/whatev Copyright Joe Blow (jblow@abc.com) 2014 This text is licensed under the GNU Free Document License (http://gnu.org/fdl) In this way, even if I only receive a print copy, I know what my license is, where to find the original source, and how to contact you if I have questions or want to talk with the author about alternative licenses. Also, because relicensing is common, make sure you take the name and email of all contributors. It may just come in handy later if you all want to join forces with some other group that was doing there stuff under CC-BY-SA and you will need signoff of all contributors. Alternatively, have the contributors sign a CLA giving you the okay to make that decision unilaterally.
You are distributing the source along with the PDF. You intend for all redistributors to be doing the same, so burying the full text in with the source and not in the final PDF is fine since your whole stated intent is to keep them together. Now maybe technically they only need to make an offer of source, in which case the onus is on them to print out the license and their offer separately and hand it out along with the PDF. Since almost no one will get that right though, the shortened blurb I mentioned above acts as a failsafe for people who didn't get the source.
Free monads are monad computations represented as an abstract syntax tree. You can do the same for monoids. A monoid is a `+` operation that's associative, and a `zero` value that's an identity. You can define `a + b` as `Plus a b`. Now every time you use `+` it builds up a larger syntax tree representing the computation, instead of actually doing the computation. The `+` operation has some laws, for example `a + (b + c) = (a + b) + c` and `a + zero = a`. We could define the data type as: data MonoidAST a = Value a | Plus (MonoidAST a) (MonoidAST a) | Zero But now you'd have different representations for the same computation: Plus (Plus a b) c vs. Plus a (Plus b c) Plus Zero Zero vs. Zero So if we implemented `+` as Plus and zero as Zero then that `+` and `zero` doesn't satisfy the monoid laws. What we want is not only for the underlying computation to satisfy the monoid laws, but also for the syntax tree constructors to satisfy the monoid laws. This means that we have to put the computation in a canonical form, so that for each computation, there is a unique syntax tree representing it. We can do that by making sure the computation always stays right associated: data MonoidAST a = Plus a (MonoidAST a) | Zero Now each expression `x+y+z` has a unique syntax tree representation as `Plus x (Plus y (Plus z Zero))`. This means that the new MonoidAST data type is a *free monoid*. Monads are more complicated than monoids. It turns out we can still do a unique syntax tree representation: data Free f a = Pure a | Roll (f (Free f a))
Personally I find the topological viewpoint of sheaves the most illustrative one. What other picture do you think is better for exposition? Also, not sure why you are mentioning contravariance as something bad; is it inherently more complicated than covariance? If so, just switch to cosheaves :)
How do you define it without open sets though? It's possible without categories but certainly clumsy so I don't see why one shouldn't use its language. Not mathy folk might not like a rigorous definition, but they usually don't demand one. 
It is certainly possible if you use something like Data.Dynamic. The question is more if you would want to do it. For numerical computations what you would probably want to do is to write instances of Num or some similar typeclass (depending on the operations you want to support) that has instances for all types involved.
I like your general framework of denotational design, as well as its concrete applications to FRP and elsewhere. But your insistence on type classes and type class morphisms always puts me off because I don't consider them proper way to do abstractions. Have you thought about what your denotations would look like with ML-style modules or if expressed with dependent types? In other words, maybe we should apply the denotational design to the problem of modules itself, for which type classes are just one (and by far not the best) approach :)
The difference is that in ML bottom is an effect, not a value. In your example, you cannot call `getBottomForList` and store its result in a variable the way you can in Haskell. Once again, ML types do not contain bottom value (not sure why you think they do). They are true data and contain only precisely what you want them to contain, in stark contrast to Haskell, where they contain a lot of stuff you usually don't want. So I don't think I'm conflating anything. If you want laziness, you need a way to refer to unevaluated computations, which is precisely what forces you to have bottom value in the language. In strict languages there is no such requirement and, indeed many strict languages don't have bottom values. Turing completeness is also something thrown around a lot. This has been debunked numerous times, just read any tutorial on Idris or Agda. It's the same kind of myth like "Haskell is not pure because it has IO". As for the "Fast and Loose", yes it can be done, but if you want to be truly correct, not just morally (i.e. checked by type checker), you need to avoid bottoms. That's why languages like Idris, Agda, CoQ and many others are strict, you can't get a decent theorem prover otherwise (to my knowledge). Of course, for Haskell or ML this matters less, as they are foremost programming languages. But even here you can benefit from not having bottoms to be sure you're reasoning is correct and it also makes teaching way easier when you don't have to precede each sentence with caveat "pretend there's no undefined then this is true".
I have a postgres rest framework very similar to it (security through `set role`, wai) that I wanted to release for a while. One thing I do that postgrest doesn't is efficient paging via offset ids. It might make sense to join efforts. I'll have a detailed look in the new year.
Well, you need to refer to unevaluated computations somehow, right? So the easiest way is simply to make the computation itself the member of the type the computation returns. There are of course other approaches to laziness, like CBPV, but there it's implicitly appreciated that laziness is but one mode of evaluation that coexists harmoniously with strictness and you can pass between positive and negative types in the language itself. But I have yet to see fully lazy language without bottom or some such. Re reasoning: that's not what I said. What I said is that proper reasoning (as opposed to intuitive reasoning) is simply impossible in Haskell, period. In ML when you deal with type Nat, you can be sure it contains only natural numbers and when you apply induction principle, you can be sure (as in type-checked sure, not as in "this seems fine" sure) your proposition is valid. In Haskell you can do undefined :: Void, proving the contradiction and exploding the (mathematical) universe. Which means the language has no value for logic (which you might or might not care about, but it's a fact). Of course seasoned Haskellers learned to ignore undefined but you can't deny that you reduced rigorous reasoning to intuition of when to ignore what.
&gt; Furthermore, once you hit categories and category-theoretic-constructs, which I understand is the goal, you're better off with sets anyway. Hm, if you mean informal sets (i.e. set understood as a collection of stuff and you don't care about the ZFC) then maybe. If you mean formal set theory then this is completely false. One of the main motivations for developing HoTT was precisely making category theory (and especially higher category theory) easier to formulate. In HoTT, a type is already infty-groupoid, functions are (infty-)functiorial, .. all simply as a consequence of type-theoretical notions and you get category-theory goodies for free. With sets, not so much. Again, yes, it simple to say "Morphisms from x to y form a set" but what does it really mean in the formal language? And what is the category of all categories? Then you need to deal with small categories and all the usual set-theoretic nonsense. General point of your comment of course stands, sets are better understood in the current mathematical community (informally understood, anyway; I doubt many people here have seen any decently complicated proof expressed in the language of ZFC and first/second-order logic ..it's not pretty, especially compared with the beautiful constructive TT proofs, which are really just programs/functions).
Isn't it so nice to manage to finish a project in the last days of a year? ) That warm feeling of satisfaction... Congrats! It's a very promising project! And thanks for the reference to Hasql. Now I get why you needed so much dynamicity from it.
The straightforward definition type checks for me: dicts :: All c xs =&gt; Proxy c -&gt; Product Proxy xs -&gt; Product (Compose Dict c) xs dicts _ Nil = Nil dicts p (_ :*: xs) = Compose Dict :*: dicts p xs This is on GHC 7.8.3. What code do you have, what is the error, and what is the compiler version? 
Just my two cents, but I've always found that vectorizing in R makes code less readable, because it hides useful type information.
Is `Type : Type` really that bad? I'm no mathematician but I've heard seen papers that discuss/consider using in programming languages.
Thanks for taking this project on. I plan on following it as it grows. Just one typo to mention that I didn't see posted here already: On page 15 in the definition of the set of rational numbers, it needs to be noted that q != 0. That is, you don't want to say that 3/0 is a rational number. Also, is this the best place to talk about typos, or do you have some sort of dedicated email address for this? (And will you send me a check for $2.56 like Knuth does, if I find one?) 
The first chapter is very understandable even for programmers, I'd say, and gives a very good overview of type theory.
Hey /u/conal, if you're still reading: On p2 of [Denotational design with type class morphisms](http://conal.net/papers/type-class-morphisms/type-class-morphisms-long.pdf) you have that the semantic function `[|.|] :: Map k v -&gt; (k -&gt; Maybe v)`, but then later `[|.|]` is applied to `size m` which is of type `Integer`. What's the deal with that? 
I am the author, and that's on the list of things to do. :)
Hey, everyone! I'm the author of this package. Questions, comments, and criticisms are welcome, as well as pull requests to improve the code!
IIRC that would cause a paradox to be possible, in particular [Russel's Paradox](http://en.wikipedia.org/wiki/Russell%27s_paradox)
It does but I recall it was a lot harder to accomplish.
I'm too poor to send checks =P. I do appreciate when people point out typos, though! 
Thank you!
So, the workaround is to just download an archive containing *all* the PDF's. http://i.imgur.com/yyF6wf9.png ^ See the "Download" button on the right - click that, and you can download all the PDF's in either a .zip or a .tar.gz, depending on your OS. There are only 2 PDF's so far, so this isn't too horrible. I'll let you know when we get a proper build system.
When people study languages they try to treat a language as a mere syntax (e.g. "2 + 2" is "the tree with node represented by the string '+' and left child of the node represented by the string '2' and right child of the node represented by the string '2'") and then apply *semantics* to that syntax. The most common semantics is *operational* which is to say we translate the syntax into steps in some kind of abstract machine which flow toward a final result. Another kind of semantics is *denotational* where we choose a denotation domain and then map our syntactic objects into objects of that domain and study the meaning of the language via the proxy of the meaning of the language's denotation. In particular, we write [[a + b]] = [[a]] + [[b]] [[n]] = n (if the string "n" is a decimal number representing the natural number n) which means that our denotational mapping `[[_]]` translates the syntax "2+2" into the mathematical/algebraic combination (2 + 2). This is a little strange since I can't project the idea of a mathematical/algebraic combination without using some kind of syntax to type but it's important to think of the first as being different from the second. In particular, the syntax "2 + 2" alone has no laws, but the math (2 + 2) has the laws of the definition/meaning of (+) and thus we can say that (2+2 = 4). Typically, denotational semantics is best when you can find a nice denotational domain which adequately captures some kind of complex syntactic rules/meaning you would like to have. Other great examples include interpretation of Haskell syntax into the domain of "pointed complete partial orders" which gives a beautiful description of non-strict semantics and the interpretation of many language features into continuation passing style (e.g., this is a great way to formalize what try/catch means... and even to implement it). Generally, denotational semantics gets into a somewhat hairy world of comparative linguistics since ultimately the domain of your denotation can usually be thought of as not much more than a "nicer" language than the one who's syntax you're studying. Thus sort of gets to both Conal's and Roman's points about the difficulty of getting good pay-off for doing denotational design—it only works if there is a faithful translation of (part of) the meaning of your language into a simpler/more lawful domain. Which brings me back around to the beginning. The other way to go forward is usually Plotkin's notion of operational semantics. Here we skip all the nonsense of trying to map a syntax into something else and instead apply meaning to the syntax tree *directly* by talking about how it reduces to certain canonical forms. Sometimes we also provide denotation for those canonical forms by calling them *values*.
Yes
For mac ^$ ^curl ^https://gitorious.org/lysa/pdfs/raw/8407b90d3a8ff468a1353e3074b25899eaf416fc:lysa-141230013149.pdf ^-o ^lysh.pdf
I like how the comment section takes up 9/10ths the page :O
cf. "girard's paradox" http://www.cs.cmu.edu/~kw/scans/hurkens95tlca.pdf
I'm not sure what you're getting at here. In what sense -- in foundations, or in day-to-day practice? In how we write on chalkboards, or in how we formalize things? My point is just this -- if you want to mainly talk about categories, you only need set theory insofar as you want to define a thing called a "hom set". Now, you can do so on set theoretic or type theoretic foundations. Or you can just wave your hands and call it "a bag of stuff that you can tell apart" and that's that. Or you can define it as an object equipped with some functions, and that's high level enough that someone can translate it into type- set- or category- theoretic foundations as they please. But again, if the goal is to talk about categories, just pick the fastest way to get there! Mac Lane, for example, is conducted entirely in terms of set-theoretic foundations. I hardly think this makes it any the poorer as a text, and in most portions of the book you hardly notice, because it is irrelevant. (There are other texts where I feel the naive use of set theory _does_ hold things back on the other hand -- those more directly elucidating computational notions, for example. But in those cases, often the right "next move" is not to present things in a type-theoretic setting but just to use categories, and whether you in turn choose to situate your categories set- type- or category- theoretically in foundations is really besides the point).
~~That will get you that particular version. You would want the entire archive~~ curl -sso lysa.tar.gz https://gitorious.org/lysa/pdfs/archive/HEAD.tar.gz tar -xzvf lysa.tar.gz ~~That will create a folder containing all the PDF's.~~ I should test stuff. I guess the easiest thing to do would be to install [Git](http://git-scm.com/downloads), then just run git clone https://gitorious.org/lysa/pdfs.git Then just browse the `pdfs/` directory. Even better, you can always get the latest version by `cd`ing into the directory, and running git pull 
Alright, I moved the stuff over to GitLab. You can now get a PDF at https://gitlab.com/lysa/pdfs/tree/master 
Alright, I moved the stuff over to GitLab. You can now get a PDF at https://gitlab.com/lysa/pdfs/tree/master 
I really like the concept here, however, to use something like this I would need a much more extensive knowledge of Postgres than I currently have. What resources would you recommend to learn more about schemas, roles, triggers, etc.?
I feel that 'Type : Type' is a perfect assumption in naive type theory. I am not an expert though.
This intuition helps me most when reasoning about freeness. Free structures can't assume anything besides the basic laws of that structure, thus expressions in that free structure can be imagined as e.g. expression trees, only recording but not reducing any of the used operators. It's kind of like the opposite of the compiler who tries to reduce everything to as simple expressions as possible at compile time. On the contrary, a free structure must store all information on what called what in the result, which only then can be given meaning by interpreting it in a certain way. Take the list type `[a]`. It's a manifestation of the *free monoid*: It's some elements of type `a` which are about to be folded by some binary operation. Since we can't lose any structure, we have to store all values (in order of reduction by the way, monoids are not necessarily commutative). The interpretation function for the free monoid is the fold. Give it the operation with which to reduce and a seed value and get the result of the computation. Applying different kinds of monoid instances is trivial in this way, e.g. you can choose to either multiply or sum all elements of a `[Int]`.
But one thing that is a failed promise of all these ESB solutions is that they apparently seem to have the adapters for everithing. The truth is that it is not the case. You probably will have to develop your own adapter. Don´t expect to find an adapter for every ERP OLAP database to your ESB. More than probably you will have to program it in Java for your particular integration. Some if not all of the adapters will be hand coded. Neither will exist ever an `adapterTo system` being `system` anything. No ESB will have it. Neither in Haskell nor in any other language. There are generic adapters that you have to configure, using XML configuration files that are little less workable than Java libraries. In the other side, there are generic adapters almost for anything in haskell iin the form of Haskell libraries: databases, any network protocol, REST Amazon... etc. If you have to specialize it to do your own particular adapter, you can do it with a higher level language far more easy to develop than XML or Java. And with the orchestration/workflow effects of the Flow monad, you can do all the integration fully in Haskell with no event handling hell. Even declaratively, coding event sequences in XML for commercial ESBs is a nightmare. Redoing actions and rollbacks are unavoidable since they are particular for each integration problem. there is no way to generalize, except this rule: "If you are doing something, some weakly integrated component down in the orchestration flow will force you to undo it , fix it and redo it again". That is an effect implemented in the Flow monad. In the other side, Integration is not only about light events. Suppose that it is necessary to perform a synchronization of a big OLAP database with a SQL Database every night. That may last for hours. The ESB must support any failure condition including the failure of the ESB, so that at the end of the night either the databases are synchronized or nothing was changed. That implies logging, state recover, long running transactions.... 
Thanks for this!
Awesome! Link to the related talk would be nice!
I don't think they were made for a talk, judging from the announcement on haskell-cafe: &lt;https://www.haskell.org/pipermail/haskell-cafe/2014-December/117554.html&gt;
http://en.wikipedia.org/wiki/Denotational_semantics
In the other side, Integration is not only about events. Suppose that it is necessary to perform a synchronization of a big OLAP database with a SQL Database every night. That may last for hours. The ESB must support any failure condition including the failure of the ESB, so that at the end of the night either the databases are synchronized or nothing was changed. That implies logging, state recover, long running transactions.... 
The author here said that he wants the code available. Burying the license in the code means that as long as the code is distributed along with the PDF then everything is fine, which is exactly what the author wants.
But that's exactly my point! Type classes are just one of the multitude of abstraction mechanisms and yet anytime I hear about Conal's denotations the first thing everyone mentions is that the central concept around which universe revolves are type class morphisms. I'm glad you confirm my feeling that they are completely unimportant and probably arose just as an accident of being present in Haskell. If Conal was doing research in OCaml, we'd have signatures and functors instead.
I had no idea that the "default" keyword even existed. I'm using this in my code now. Thanks!
Yeah the arbitrary deadline is a nice motivator. :) Thanks for your tireless work on Hasql and all the time you've spent answering my questions on Github.
That Comic Sans though.
Sheaves can be defined over a base category which has a system of covering families which satisfy certain axioms.
The efficient paging sounds great. PostgreSQL never ceases to amaze me, always something new to learn. Postgrest does pagination with range headers and I'm curious if there's a way to make this work with offset ids.
I want to add to this point that there's another way that "complex" denotations can be good. As you state in another comment it's the obligation of the denotational designer to determine the space of adequate and precise denotations and then optimize for simplicity. However, one's notion of "simple" is often more like a threshold and, worse, one that's generated prior to the act of searching. It may be the case that your simplicity threshold is a good motivating force to drive a difficult search. It may also be that your simplicity threshold is (technically) insane and should be adjusted! Sometimes this is just the feeling of the material world pushing back on our desires.
Well, I think a "String-branching tree" or a "Natural-branching tree" is a pretty natural construction in some ways. It certainly feels like the right approach when we think about defining a universe of data structures from w-types or the like :-)
No need to be ashamed, there is a lack of Dict-like techniques, anyway. It is a powerful tool, and no standard practice distilled yet.
*explodes*
I found the best way to think about relational data in general is to start with an old book, one that covers the subject in a pure way without reference to any particular system. Then you can translate the concepts into a nice modern system like PostgreSQL. http://www.amazon.com/Handbook-Relational-Database-Candace-Fleming/dp/0201114348 If you just want to jump in and try stuff out here are some tutorials and docs. Here are some tutorials about triggers http://www.postgresqltutorial.com/postgresql-triggers/ Managing roles (the "official" docs are actually pretty good) http://www.postgresql.org/docs/9.4/static/user-manag.html http://www.postgresql.org/docs/9.4/static/sql-grant.html http://www.postgresqltutorial.com/postgresql-roles/ Creating schemas and using the search path http://www.postgresql.org/docs/9.4/static/ddl-schemas.html
You loose consistency and analytic properties for no useful gain in expressiveness. Having an infinite tower of universes, that is, `Type 1 : Type 2`, `Type 2 : Type 3`, is useful, though, and if you have that you can introduce semantic sugar and get to universe polymorphism... meaning that on the surface syntax you can pretend `Type : Type` and not care about universe levels, but behind the back the compiler is going to make sure that any such uses don't have any loops in them. Not that it's easy to just stumble across such a loop. In fact, it's hard to construct even if you try. ...and not at all all *programming* languages might care about analycity and consistency. Haskell isn't consistent in the first place, for example, which is why they don't worry about introducing `* :: *`, and probably won't enforce totality on dependent types, either. Compile-time non-termination is just another kind of bottom, it seems.
That's kind of a tradition in Haskell land :p
In addition to /u/tel's great reply, the README from Conal's talk at last year's BayHac is an easy-going introduction to many of the concepts: https://github.com/conal/talk-2014-bayhac-denotational-design You can also find the video on the YouTubes.
That's a heckova speech act.
&gt; I would like to see a type theory based informal introduction to mathematics to primary school children. I would like to see the same for adults covering advanced topics too. I would like to see how the additional computational content of proofs could be utilized during this. Ah, and here we agree too! But I think we will have to reformulate what we think of as the "core" of type theory to a degree if we are to get the "right" sort of informality to make this stuff as accessible as we would like it :-)
An even simpler implementation: type Value = [] scalar :: a -&gt; Value a scalar = return vector :: [a] -&gt; Value a vector = id vectorize :: (a -&gt; b) -&gt; (Value a -&gt; Value b) vectorize = fmap As a bonus, if you remove the type signatures it works for any `Monad`
Right. Yes, that is a problem with today's ESBs where you have to code events, adapters, etc in XML and Java. Making adapters for Haskell should just be instancing a specific typeclass "ESBAdapter" or something. So you could use any Haskell library in existance as an adapter, you just need to create the instance and use it. Of course there could be an "esb-commons" library that includes the code for the most common adapters out there. But yes, this is a place where I think Haskell could shine. With normal ESBs the adapters and components are already made for you, and you can't really create new ones without a lot of hassle (and you sure as hell can't really reuse other libraries and use them as components). In Haskell you should easily take custom code or external libraries and use them as adapters, with just a little tweaks. But yeah, this "ESBAdapter" class, and the glue code that uses it have to exist in the first place.
Let's say it is absolute, the simplest denotation is probably going to be some group with caveats. This basically how I approach Doubles, by pretending they're real numbers (approximately). I don't think these views are in as much conflict as the title lead me to believe.
I see it used nonironically in academia "all the time"
He's on record stating it's a "friendly font."
I ended up playing around with this stuff quite a bit. Here's some of the possibly-useful but definitely interesting things I've found you can say with techniques like this: https://gist.github.com/kwf/c03206171c240a8a1179. Thoughts?
 Thanks very much for this feedback! I can see that my use of the term "type class" here suggests that the idea is specific to Haskell, which it is not. As you observe, Haskell type classes provide just one mechanism, and specifics would look different in ML (for instance). I wonder what to call the general idea? Suggestions most welcome (not just from mimblewabe). Let's brainstorm here and see what emerges. It might also help to experiment with what an ML-style analog to type class morphisms would be.
I like this book: http://en.wikibooks.org/wiki/Haskell Making up coding exercises and looking at other peoples code seems to be useful too.
Do you have trouble with the fact the wikibook is incomplete?
Well, yes, that is what I have been trying. However, as alluded to in my original post, the traditional references seem to be failing to explain the concepts adequately/sufficiently. Granted, it could just be I'm too stupid to learn Haskell and I am perfectly willing to acknowledge that possibility.
&gt; At the end of Learn You A Haskell, I'd recommend writing your own parser combinator. Okay, you really went beyond me with that one. Writing my own what?
Well, for me the cheesy part made it more interesting. :)
What I wanted to know is how type errors in Core are translated into errors relating the original source code..
All you need to start on your own is the ability to define a function, a few data structures, and the generally the syntax for types and values. That's actually enough for simple toy projects! Then, just build your knowledge piecemeal, adding one step at a time, and learning enough to know what the next thing you need to learn is for the next thing you want to write. The incomplete portions of the wikibook come towards the more advanced section, where you can really pick and choose as you go. Haskell-the-language is really simple to get up and running with. It's just that there's a large, and growing, body of libraries, techniques, idioms, and all sorts of other stuff that you can keep learning with for a long time to come. However, by the time you need to start grasping that stuff, then all the things that now feel now might be too difficult or advanced will already be much easier for you to approach!
Okay, I get what you're saying except for a few bits. 1. Go all the way the way thru BH:AP-BA? 2. Define "backup"; do you mean "supplement BH:AP-BA's teachings with LYAH"? 3. Please elaborate what you mean by point 2.2. 
2.2 means to write your own versions of the standard functions, so you can see how they work.
&gt; Haskell is now my favorite language I have a hunch I will feel the same , presuming I can understand what the heck I am doing. Thanks.
Ah, thanks.
You're very welcome! And good luck with your book, thanks for sharing your knowledge. I will read it when it's a bit further along and give you some feedback.
I believe type errors like those that are reported to the user are detected in the type inference engine's stage(while the compiler is still dealing with the haskell source code's AST). There's more detail here: http://www.aosabook.org/en/ghc.html They don't say it quite explicitly but it seems evident: &gt; On the whole, the type-check-before-desugar design choice has turned out to be a big win. Yes, it adds lines of code to the type checker, but they are simple lines. It avoids giving two conflicting roles to the same data type, and makes the type inference engine less complex, and easier to modify. Moreover, GHC's type error messages are pretty good. From the section titled, "Type Checking the Source Language" EDIT: formatting
I would say, "Of course I know what a parser is. It's something which takes a mass of some sort and breaks it into smaller specific pieces," but I have been surprised with technical terminology/meaning differences from expectations before.
Intersting, why `xmonad`?
&gt; LYAH is written very well I think. If your goal is to learn Haskell, it's not good. I help LYAH refugees at least a couple times a week. Other teachers in the IRC channel do so at least every other day. I haven't seen you helping anybody learning the basics on IRC recently, so I'm not sure you appreciate just how pedagogically problematic it is. I think if you dug into the process and got some hands on experience teaching people that are struggling, you'd understand where I was coming from. Being cute and friendly doesn't fix broken material. Did you learn Haskell from LYAH?
Thanks. Another thing, I can't find it in Google - does GHC use De Bruijn indices anywhere? (well, since it says it uses a symbol table during variable substitution, and otherwise each variable carries its own name, I think that it doesn't - but then how it tests for alpha equivalence?) edit: actually [found it](https://www.fpcomplete.com/user/edwardk/bound#de-bruijn-indices)
This is basically what the NICTA course is, but a structured means for doing this. This is why I recommend doing the NICTA course after cis194.
&gt; I wanted the library to keep the http-client implementation independent. After discussing a few different approaches, I settled on the Free Monad approach for its flexibility. This has to be one of my favourite sentences from the opening paragraph of any article I've read recently.
Thank you.
The problem is that it this claim (Haskell does not allow bottom-free structures in the way ML does) does not seem obvious or strong to me at all. I'll try to explain why I don't think that the bottoms in ML are that different from the bottoms of Haskell. First, you can refer to unevaluated computations in strict languages. It's not even too hard to do that; you make something of type `unit -&gt; t` instead of `t`. let mutable containsHaskellBottom = (fun () -&gt; bogus ()) Haskell just implicitly does this transformation. Haskell `bogus` is really a function on unit that is called only when its result is needed. You *can't* store the result of `getBottomForList` in a variable for Haskell (well with Haskell lists, you could, because the result would actually be finite, but assuming we are using an ML-style strict list in Haskell). Haskell just implicitly stores the computation rather than the result. We can make the transformation back to ML style semantics explicitly: &gt; x &lt;- let bogus = undefined in bogus `seq` newIORef bogus *** Exception: Prelude.undefined We don't even always have to be explicit about such transformations. For strict functions in Haskell, like `sort`, the property f(⊥) = ⊥ holds the same way it does in things like ML. It's no different. You could even do an inductive proof on the correctness of `sort` without even making it require a strict list data type as input. `sort` is fully strict on its argument and completely evaluates it (at least given a strict `compare` operation). It doesn't even work on infinite lists, it doesn't even make sense to sort an infinite list, so you just treat it like the result of the same ML function that would attempt to construct an infinite list (and never return). Haskell `sort(repeat 1)` is the same as ML `sort(bogus())` or `bogus()`, just like in a strict language. You only ever have to worry about things like co-induction or bisimulation when working with non-strict function. When you embed non-strictness explicitly in ML, you have to do these things too. It's just a different semantics by default. It's true that this isn't trivial, it means that non-strict and partial values are common in Haskell and pop up without you meaning to have them, while in ML they are rare and clunky to create and use. `(fun () -&gt; bogus())` is perfectly fine so long as it never gets called and can be stored in a variable and fed as an argument, just how `seq bogus (newIORef bogus)` will fail in Haskell. Basically the claim seems to be that bottoms only exist in t^1 (an interpretation of `unit -&gt; t`) rather than in t, but these types can reasonably be considered to be the same thing, Haskell just works with this equivalence more obviously and implicitly, although you can banish it if you want. When you explicitly reintroduce the distinction (which exists in Haskell in practice), it strikes me that the paucity of types is not an issue. At least in Glasgow Haskell, you can create strict data structures. They are useful in practice and forced evaluation to normal forms is captured with things like the `NFData` typeclass. Thus you can just require for your inductive proofs that Haskell types written t should be rewritten t^1 except for the fully strict ones, and t^1 treated as a separate type. And doing this kind of explicit transformation of `t` into `unit -&gt; t`, you end up embedding Haskell-like semantics. I think to get a real distinction, we need to look at things like Agda and Idris, where you cannot write things like `getBottomForList` at all in their total subsets, and you can prove this at compile time. I'm not sure this sort of thing can be embedded in ML or Haskell, yet. But they can embed one another's semantics. So the whole "paucity of types" thing seems a little pointless. You can clunkily and verbosely put the lazy things in ML and clunkily and verbosely put the strict and eager things in Haskell. If you can, in practice, express your strict ML structures, functions, and programs in Haskell, and get the exact same semantics, can you really consider this something *missing* from Haskell? It's not asking for Haskell to have something that it lacks, Haskell already has things that don't have bottoms in the way ML does if you make all the right, tedious, explicit transformations. it is about asking for Haskell to have different default preferences, a kind of change in philosophy. It's analogous to saying that ML should allow me to define short circuiting functions and a useful Y combinator. It already does, it's just more verbose because it uses a different style by default.
&gt; It's been a few months since I looked at the text. I'll give it another read thru and possibly get back to you on this point. Feel free! I have to admit I am curious to see what it is that's causing you trouble. If I had to take a guess, I'd go with type classes. That seems like the most advanced early topic, and if you hear class and think "Ah ha, a class like in C++!" that could definitely cause confusion. Typeclasses aren't like classes of objects at all in Haskell. &gt; I am most familiar with C, C++, Ruby, Scheme, and Python. While I get the general gist of others I was mostly a Python person when I started learning. (But I know C and some other imperative languages also.) I'm definitely sure you are capable of learning too!
xmonad is written in Haskell and is probably one of the most well-known / widely used tools written in Haskell (not counting GHC, cabal, etc.). Another tool, by the way, is Pandoc – and you might learn quite a lot by trying to hack on it. Pandoc is written in a rather beginner-friendly style, doesn't use any advanced features or libraries, and its structure is nicely suited for beginners as well (lots of parsers + lots of writers, almost all of them independent from each other). So, my way to learn Haskell would be: * Reimplement almost all of Prelude (excluding IO/exceptions, of course) and `Data.List`. When you get stuck, read the source and try to evaluate functions on paper; if still stuck, ask on #haskell. * Take a bunch of shell scripts and reimplement in Haskell. * Write a quicksort operating on mutable vectors. * Reimplement `grep` or some other tool, and upload it on Hackage (so that you would learn to use cabal, sandboxes, create packages, specify dependencies correctly, break a project into modules, etc.; this side of programming seems to be overlooked by all books and tutorials). * And maybe hack on Pandoc if you haven't got anything better to do at this point and want to learn about parsers. I can help you with diving into Pandoc if you're interested. Now, the most important thing: whenever you think “I wonder what would be the idiomatic solution for &lt;something&gt;” or “I wonder whether &lt;something&gt; is possible in Haskell”, ask on #haskell. I have a feeling that it's much easier to learn something when you're learning it for a specific purpose, rather than learning that it exists and then trying to apply it everywhere. For instance: * If you ask why `String` is so slow, you'll learn about `Text`. * If you ask how you can do logging without awful amounts of boilerplate, you'll learn about `Writer`. * If you ask how to handle global environment of e.g. command-line parameters passed to the program, you'll learn about `Reader`. * If you ask how to combine previous 2 points, you'll learn about monad transformers. (I spent several months using them successfully but not knowing how they work, until I accidentally found out by reimplementing them.) * If you ask how to handle nested record updates, you'll learn about lenses. (Again, knowing how to use them and understanding them are 2 *very* different things. I haven't found any way to understand lenses short of reimplementing them, but maybe it's just me.) And so on, and so forth. *Edit:* you can google all those things as well, right. I'm not suggesting using IRC as a better mentor – merely as a better google. Some things (idioms, recommended libraries, etc.) are really hard to google, while on IRC you would get an instant reply and save your time as well as make someone a bit happier (assuming people like sharing knowledge and helping people save their time – which, I think, they do... at least the ones on #haskell, right). [Also, by “sharing knowledge” I don't mean “patiently explaining everything to beginners” – some people like it as well, but I feel that there's much fewer of them.]
&gt; Some Monads are isomorphic to Free Monads for a given functor (e.g. Identity is isomorphic to Free Empty, I believe). It's interesting to think about when this is true (perhaps its true for all monads? I can't remember). I welcome any corrections to this off-the-cuff remark. Many familiar monads like Reader, Writer, State, Cont are (usually) not isomorphic to free monads. See my explanation [here](http://stackoverflow.com/a/24918234/190376) (and also my answer to [this question](http://stackoverflow.com/q/25827271/190376)). The tl/dr version is that a free monad can never have a non-trivial "invertible" action `m` for which there is an "inverse" action `m'` such that `m' &gt;&gt; m = return ()`. Reader, Writer, State, Cont have such invertible actions for most choices of their parameters. As an analogy, consider the statement "all monoids are isomorphic to free monoids". This is obviously false for the same reason, namely that free monoids never have nontrivial invertible elements, and any nontrivial group is a monoid that does have nontrivial invertible elements.
Have you tried [exercism.io](http://exercism.io/)? Basically, everyone does the same practice problems, then reviews everyone else's code.
I don't much like the language of subtyping, but a type class morphism essentially states how a denotational morphism can be pushed out to morphisms on supertypes, I suppose. Ancestry morphisms, super morphisms.
Haskell is my first programming language, so I don't have your experience with Python to compare it to, but in the main I agree with you. I worry less about the psychic leech problem, a bit more about the other problems, and about at least one other issue that is probably specific to me: I believe I must not be very good at formulating questions, at least not until I already know something a bit (so that I can get at exactly what I'm not understanding), because when I do ask a question I find I often get an answer that doesn't help. I've been lectured by Haskellers in the past for not asking questions well enough for them to answer appropriately; so, OK, I accept that I am not good at asking questions and that IRC is not going to be very helpful for me. It is helpful to have a mentor or a teacher, preferably one who has an idea of where you're at so they can respond to your questions with that background in mind, and I can see why IRC is a very helpful medium for people with some experience who know how to ask questions well. But I share your concerns that it's a problematic way of addressing the concerns of beginners. FWIW, the people in the #haskell-beginners channel do seem to be very nice, and I've seen some really good insights there. From what I've seen, I doubt they are worried about you becoming a psychic leech. (I lurk in there sometimes) Finally, I hope you have a garage band called Psychic Leeches. 
All I can say is that this method has worked great for me. You worry seems almost nonsensical to me. Now maybe things have changed since I taught my self haskell over the summer between high school and college, but I think your long list of hopes is over thinking the problem. 
Also I think your feeling that you will be some how not really learning it if you ask others for help is ... wrong. It's not a binary thing, of independence or infintileism(sp?). 
Haskell historically has had a very friendly beginner-friendly irc channel (and indeed we now have the beginners specific one) so I suspect the "ask on IRC" recommendation is a reflection of the fact that many of us did a great deal of our initial learning in that fashion. Certainly for a chunk of us, when we were learning, even many of the resources you're now tackling didn't exist, and it was more an issue of piecing together a few very basic tutorials than then _pow_ into papers and discussion with fellow developers, with virtually nothing inbetween. So yes I hope the impression given isn't that irc is at all mandatory at this point -- its not -- but nonetheless, along with mailing lists (and now stack overflow) it forms a very good resource. The best thing on IRC (and I don't know if the code review overflow does haskell well? it may?) is often that you find people willing to do a code review for fun and practice, and so learn many things in the process of taking your program to a nicer one.
&gt; I keep seeing something along the lines of "When learning Haskell, if you don't understand something, ask on IRC." OK, here's some other advice: "When learning Haskell, if you don't understand something, keep reading until you understand it.". Does that help?
Here's what worked for me: A big project. I started learning Haskell using Real World Haskell (which I think is excellent). I was learning Prolog in parallel by the way, which worked rather well. A few months in I was given the chance to pick the language for my next project at work and I thought I might as well give Haskell a shot. If something didn't work out performance wise, I could still fall back to C and use the FFI for some parts. Building a medium sized project in Haskell on my own really taught me not only how the language works, how libraries are designed in it, but also how to design and structure software in a functional language. Haskell itself is a remarkably simple language. All the mysticism are really just libraries for the most part. Once you've groked the syntax and can solve the 99 Haskell problems, pick a project and start coding. I'd actually recommend writing a compiler or intepreter for a small language, since Haskell can really play its strengths there.
In the sense used here, a parser takes some sort of text input (for instance a `String`) and extracts some data from it. As a very simple example, consider the `read` function read :: Read a =&gt; String -&gt; a It takes a string and builds an `a` out of it, so it "extracts" data from that string. A more complicated use case arises when writing a compiler. The parse takes the source code as its input and returns what we call an abstract syntax tree, which is just a representation of the code that is nicer to work with in the compiler. It's pretty much the first step of compilation. I'd recommend reading the paper [Monadic Parser Combinators](http://www.cs.nott.ac.uk/~gmh/monparsing.pdf) on the topic of parser combinators. The first few pages should be enough to give you an understanding of what parser combinators are all about, and it's not that hard to read (although you might need to read it a few times to build intuition about what's going on). Even better, you can implement what's outlined in the paper and you've now built your own parser combinator library!
The expectation to be productive in Haskell feeds the frustration in learning how it's different. You're a programmer. You have a side project you're working on? Oh wait, you have twenty projects in different rates of decay in the back your brain you're working on? OK, now forget all the ones that depend on graphics. You're looking for a project that's maybe 1000 lines of python, a project that is easily testable. Something like a Rubik's cube solver. The complete relevant state is very neat and wrapped up, you won't need anything fancy. You'll get a good feel for what it's like to model a somewhat complex thing and manipulate it with (Cube -&gt; Stuff -&gt; Cube), (Face -&gt; Stuff -&gt; Face) type functions. A Rubik's cube solver is fun, there's no pressure about getting it out the door. -- Don't get down on yourself if it takes a couple years to really get the hang of things. Lots of people take breaks, put Haskell down and pick it up later, repeatedly. A good deal of IDE functionality can be had in emacs and **very much worth the effort to get it configured correctly**. If you're feeling industrious, [use this project starter](http://yannesposito.com/Scratch/en/blog/Holy-Haskell-Starter/). You should be able to poke around the skeleton that produces to see how a regulation project is laid out. $ cd your-project $ cabal help $ cabal repl will load your project's library into ghci, this is convenient, but integration with emacs / ghc-mod / haskell-interactive-mode is approaching FP Complete's facilities. $ your-editor ./src/Main.hs Fame and fortune await. 
Great blogpost. I haven't done any type-level programming so far but after reading this I can't wait to do so :)
I can't remember where I saw this and it might be old information On x86 excess precision is used unless -msse is specified. This is specified to be a bug in the manual. On x86-64 everything goes through sse so there is no excess precision unless it is specified via -fexcess-precision There is vague information here: https://downloads.haskell.org/~ghc/7.8.3/docs/html/users_guide/options-optimise.html#options-f
I think I am talking about a totally different topic, but Prelude&gt; 0.1+(0.2+0.3) 0.6 Prelude&gt; 0.1+0.2+0.3 0.6000000000000001 also happens.
That could still be deterministic, it's just addition in IEEE 754 is not commutative. Edit: Whoops - as noted below I have once again mixed up associativity and commutivity. 
It shows that Doubles can't form a semigroup (and thus, monoid) under addition. Associativity isn't met.
I find you are right, but.. Why doesn't ghc make it commutative? Why would ghc rather choose the IEEE 754 standard?
It's not associative either, which is what I think is going on here.
~~infintileism~~ ~~inifintisim~~ ~~inifantim~~ helplessness
Type checking is done on the original AST. There is a later Core type checker that can be turned on or off to ensure optimizations are type preserving.
&gt; when I do ask a question I find I often get an answer that doesn't help Keep in mind that answers from Haskell people can be quite difficult to comprehend at first, because you are dealing with foreign concepts. It might not be immediately obvious how the answer applies to your situation. Don't be afraid to ask for clarification. "Sorry, I don't quite see how this applies to my situation. Do you care to elaborate a bit on what you mean?" goes a long way.
It is, but not associative
The decimal value `0.1` has no precise representation in binary - translating to binary gives an infinitely recurring series of digits after the point. Floating point can't handle that (though e.g. rational number representations can) so rounding error is inevitable even without any operations being done. If having a rounding error scheme that preserved commutativity and associativity for operations where that's expected was practical, probably IEEE754 would have adopted that scheme. That said, IEEE754 specifies that `NaN` isn't even equal to itself, so maybe I'm wrong. 
We have `Rational` values. Those have all sorts of nice properties, except speed. They are kinda bad at that part.
You can get associative addition with limited precision as long as it is limited fixed precision, e.g. a fixed point number, when you have a floating decimal point for the mantissa it doesn't work.
You win.
As someone with lots of really smart friends, I've got to say, learning stuff by asking people about it is one the fastest and most fun ways to learn! Having said that, there are tons of material about Haskell online and it's perfectly possible to learn solely from them, computer experimentation and old fashioned thinking. I think the IRC advice is not meant to suggest human contact is *necessary* to learn to Haskell, rather giving that advice is meant to create the expectation (probably correctly) that the Haskell community is friendly.
I found a small error [here](https://gist.github.com/aaronlevin/4fc1fcfdce947a41567b#file-01-naive-functor-hs-L50). The comment says that the name gets updated but that doesn't seem to be the case.
It would be possible to set up that kind of polymorphism in Haskell, but I really don't see the gain. I think you'd end up fighting the language in the same way as if you tried to set up a functor/applicative/monad scheme in MATLAB. Besides, I tend to find that pushing the vectorisation responsibility to the functions leaves you with some points of confusion. You have to know things like "is the function I want to use vectorized?", "what if I pass an array of arrays. What happens?" and so on. In Haskell every function is implicitly vectorized for free through fmap over not only lists, but any functor datatype. This includes arrays, maps, sets as well as other things that are less collection like such as Async actions, STM actions and so on.
I find it helpful to look at these things through the lens of *universal algebra*. I think it reveals the essence of what's going on, it gives you an established terminology with which you can easier identify related work, and also guides you to natural generalisations. Let me explain a bit more. Your `data Crudable` is the set of *operations*, the `type family {Read,Create}Data` assigns a *parameter* to each operation, and `type family CrudBase` assigns an *arity* to each operation. The `data CrudF5` has an implicit forall quantifier in it, i.e. an existential, that packs up an operation with its parameter and children (`next`). `Free CrudF5` is the *term algebra*, which given some variable set creates trees with operations and their parameters in the nodes, which branch by their arity, and have variables in their leaves. Simultaneous substitution is given by the bind operator. So, for example, a round about way of doing lists of integers would be to take the set of operations to be nil and cons, nil doesn't have any parameter (the unit type) and has arity zero (the empty type). Cons on the other hand has an integer as parameter and arity one. The ground terms, i.e. the free monad at the empty type, gives you a type isomorphic to the type of lists of integers. It might be a bit weird to think of datatypes, such as lists, in these terms, but it might be helpful in getting a grip on the terminology. Closer to your stuff is the work on *algebraic effects and handlers*, which uses the terminology in the context of effects. See, for example, Bauer's [post](http://math.andrej.com/2010/09/27/programming-with-effects-i-theory/). Your smart constructors, `{create, read}5` are what they call *generic* operations, by the way. Once the connection to universal algebra becomes clear, it's natural to ask: what happens when we go from single-sorted signatures (such as the examples above), to multi-sorted ones (such as mutual datatypes, length indexed lists, etc)? A hint might be to look at the literature of, so called, *containers* by Abbot et al in the context of dependent type theory. If you do, you will notice that what you have is essentially an encoding of containers in Haskell and that working with free monads gets easier still if one has proper dependent types. Your `data Crud5` is the *extension* of a container, by the way. Once that becomes clear, the answer to the multi-sorted question can be found by looking at *indexed containers* aka *interaction structures* (which are in fact older than their trivially indexed counterparts).
Right, that's true. But that gets rid of the flexibility of floating point wouldn't it?
Exposing representation isn't the goal here. It's just a common observation that representations and implementation details sometimes survive deep changes in domain. You assume that domain is known and never change, like abstract functions over time presented in Fran form a deeply learned field. But more interaction you program have with people less likely your model sustain changes. Example: you had logistics system for oil. Is was perfectly safe to assume oil volume is a monoid - few barrels could be mixed into one big tank. Next day sales walks into your office ans says that the system sold to a milk company. Milk get spoiled over time, and mixing volumes produced different at dates could be a waste. Your basic assumption gone together with all the design while some representation details (like 'tank' table in database) cold survive. I'm not insisting you're wrong saying that abstract models are useful. I just think that your reasoning ignores significant part of the problem.
Yes -- good point! This is why I think its important to stick to the basics for a period. In my opinion, some of the trickiest things about learning FP are already posed sans- extensions, sans-typeclasses, etc., and if you then worry about all the other stuff at the same time it'll feel overwhelming.
I can't tell if you're being sarcastic or not. I do realize that of the HTTP clients available, most of them are built on the library http-client. I should remove the dash in my post.
I can't thank you enough for all your work on educating us plebeians in Haskell. Glad to contribute back, however small!
Good catch. Leftover copy pasta. In a phone but will update soon.
You reasoning that `i ` must be `id` is correct when you use "Fast and Loose" reasoning, which requires some side conditions in Haskell land to be valid. e.g. `i` could be `const (fix id)`. Regarding `b`. Take `f` to be `\b -&gt; if b then a else a'` for some arbitrary `a` an `a'` of some arbitrary type `A`. Then if (b_Bool True False) then a else a' == {def of f} f (b_Bool True False) == {free theorem of b} b_A (f True) (f False) == {def of f} b_A (if True then a else a') (if False then a else a') == {reduce if statements} b_A a a' Now `(b_Bool True False)` has type `Bool`, so it is either `True` or `False` (remember we are using Fast and Loose reasoning). If `(b_Bool True False) == True` then b_A a a' == {reverse of previous lemma} if (b_Bool True False) then a else a' == {assuming (b_Bool True False) == True} if True then a else a' == {reduce if statement} a Similarly if `(b_Bool True False) == False` then `b_A a a' == a'`. Thus if `(b_Bool True False)` is `True` then `b_A` is `flip const` and if `(b_Bool True False)` is `False` then `b_A` is `const`.
Since you didn't specify a formal setting I'll choose my own: ordinary sets and functions. First, you can use the same `f = const x` to deduce that `b x x = x`. Now, assume `b True False = True`; we'll prove that `b = const`. (If `b True False = False`, then by choosing `f = not` it follows that `b False True = True`. Then replace `b` by `flip b` and reduce to the following argument.) For any `x` and `y`, if `x = y` then `b x y = b x x = x = const x y`. Otherwise, let `f z = (z == x)`. Then (b x y == x) = f (b x y) = b (f x) (f y) = b True False = True so again `b x y = x = const x y`.
Ah I see! Very nice. Thank you.
It depends what you want to use it for. "bird track" style literate haskell is common for small examples and expository material. latex style literate haskell is used extremely widely by people authoring papers. However, the rise of haddock for documentation has meant most library authors tend to not like the literate style, since literate text doesn't get reflected in haddocks. For a long time GHC had a large number of literate modules. My impression is that those are being phased out in favor of exposing the comments haddock-style. That said, you should expect support for lhs files to continue indefinitely -- the only issue is when you may or may not want to use that style of coding. My own personal guideline is I use lhs for things I consider "an article I want to typecheck" (and i've used it for slide decks as well!) and regular haskell files otherwise. 
Very nice. In essence the same proof as roconnor's I think.
Maybe, but what's annoying is : for the sake of adding a couple of signatures to subdefinition, you need to alter every single top level signature in a non pure Haskell compatible way.
Are you comfortable with using Maybe, Either, IO and List? With the bind operator &gt;&gt;=? Unless you are, I would not recommend trying to grok "what a monad is" because it's not going to make sense to you. You learn the general concept by learning the specific instances, not the other way around. Trying to learn "what a monad is" without being comfortable with some monad instances is like asking "what is a filesystem" without knowing anything about computers. You are going to get an answer, but it's not going to tell you much.
It's a thing that takes multiple things that operate on one another in an ordered sequence, and turns it into one thing that operates on one thing. I have a feeling that if you want further understanding you would actually have to build one. Using a monad and watching what it does is different than how the computer understands and can reason about monads, which is really why they are special. 
Yes and no. Literate Haskell has never had anything close to the power and flexibility that Knuth imagined and implemented in CWEB. So, to call it `Literate Haskell` is a major overstatement. The literate modes, however, are still very useful for writing papers and documentation because they allow you to actually run your code and make sure it works. [The Happstack Book](http://happstack.com/docs/crashcourse/index.html) is written entirely in Literate Haskell + pandoc + shake. Being able to test that the book is still accurate by running all the examples is awesome. Since the exact same file is used for compilation and for generating the html/mobi/epub/pdf, I know there can not be any differences between the code I ran and what is in the book.
I agree. Many people who are experts in Haskell learned in a time when there was little documentation, but a very friendly IRC channel. At once point #haskell was considered to be one of Haskell's greatest strengths because it was highly informative and welcoming. People would enter to the channel to try to troll and leave a true believer. I suspect the IRC channel recommendation comes from people who learn during that time period. These days, there are a lot more guides and books. Additionally, there are places like stackoverflow/stackexchange. AFAIK, the IRC channel is still a friendly place. But it is no longer the only option for getting help.
&gt; but that sounds more like what it does than what it is. And the elder monk said nothing, and the student was enlightened. ;)
The challenge with "monads" is that they aren't the class of thing you might be thinking they are. First let's start with a list and a queue. These are different things, but they share something in common: we can both put things on them and take those things off. In particular, we can define `push :: a -&gt; f a -&gt; f a` whether `f` is List or Queue. Similarly, we can define `pop :: f a -&gt; Maybe (a , f a)` for them both. Finally, we can state our expectations around these functions: namely, we should be able to pop everything we push and nothing more. Together, these operations and expectations form a *thing*. Perhaps you call it an interface (it's also known as a an algebra). In particular, we can say that both list and queue both have instances of the PushPop interface. But what I just did there was abstract again: it's nice to refer to "kinds" of interfaces together. I want to be able to say that push and pop on lists is similar to push and pop on queues and I do so by saying they're both the same kind of thing. This is a "classification" of interfaces. The last thing to note is that it is possible to write a function which works over *any* instance of a class of interfaces: e.g `ex = \f -&gt; pop (push 3 f)`. We can even consider what we think that function does despite not knowing the type of f . Due to our laws we know that if we push something on to a PushPopable then we must be able to at least eventually get it back out: thus `ex a` is never Nothing. So finally we're in the right place to talk about Monad: Monad is a classification of interfaces. A Monad interface contains functions `return :: a -&gt; m a` and `(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b` which follow the monad laws. This allows us to write functions generic in the particular choice of monad. Which is important to note: there are many types which instantiate Monad! These are sometimes called "monads" themselves but this is just an informal convention. Just like knowing list instantiates PushPopable lets us know a bit about how lists behave, knowing that lists instaniates Monad lets us know a little something else and different. In particular, Monad tells is that there's a way of "layering" or "sequencing" the types which instantiate it. For list this has to do with the fact that we can nest for loops and collect the results from the inner one. If that doesn't make sense directly: that's ok. PushPopable only makes the tiniest amount of sense on its own. To truly understand its meaning you have to study the things which instant hate it (list and queue at least). The same holds with Monad. Try to understand each type instantiating monad one by one. They are each interesting in their own right. The fact that they instantiate monad is just one small piece of their value.
I was first thinking along these lines to, but the trouble is that the step, "let f z = (z == x)" presumes that the type of `x` has decidable equality, which may be a problem. My solution was to create a function `f :: Bool -&gt; A` instead of `f :: A -&gt; Bool`. Using a function from `Bool` to `A` neatly avoids any worries about decidable equality.
My understanding is that the same floating point expression in Haskell can yield different values depending on the rounding mode of the CPU, something that a FFI call could twiddle with. So that is another issue.
Oh thanks! This solves my problem.
&gt; It's not the default though. Why not?
What's the difference between using a free monad to accomplish an interpreter independent DSL versus something like finally tagless?
Wow, types of the # kind still have headers? I thought they were unboxed, but they still have a header. Does that mean that to load an Int# into registers, the CPU has to 1) follow the link from the header to entry code 2) load the entry code 3) execute the entry code by incrementing the pointer 4) finally get the actual machine int?
In Haskell, we have not just concrete data types, like integers (`Integer`) and strings (`String`), but also types parametrized over another, like lists *of* integers (`[Integer]`) or nullable strings (`Maybe String`). (In languages like Java, C++, and Rust, these parametrized types are sometimes referred to by the term "generics"). These parametrized types are very important to the language. The basic ones every Haskell needs to be familiar with are: * `Maybe a` (nullable `a`s) * `[a]` (lists of `a`s) * `IO a` (imperative operations which return `a`s) * `Either e a` (results of type `a` which might instead return an error object of type `e`) (Hopefully I haven't forgotten any important ones). From here on, whenever you see `m`, just imagine I wrote one of the above examples instead. So perhaps imagine `m a -&gt; m b` means `Maybe a -&gt; Maybe b`. A monad, roughly speaking, is a parametrized type where you can do three things: * You can lift a "boring" function `a -&gt; b` to an interesting one `m a -&gt; m b`. * You can take a "boring" value of type `a` and wrap it in the monad to get a value of type `m a`. * You can take a double-wrapped value `m (m a)` and compress it to get a value of `m a`. This kind of description is a mathematical one. Do not expect to understand it raw! Instead, work through examples you are familiar with. See what the three properties mean in concrete examples, and you will soon become familiar with them. I'll get you started. The first property applied to `Maybe` says this: Suppose you have a `Person` data structure. Every `Person` has a first name (which is a `String`). Therefore, you can access the first name with a function `firstName : Person -&gt; String`. But now what if you are accessing a `Person` record from a database... maybe someone enters a non-existent person id. To accommodate for the possibility that the person might not exist, you need to work with `Maybe Person`'s instead. When you grab a result from the database, you will either get `Nothing` (which is like null in other languages) or you will get `Just person` with `person` being of type `Person`. But then what about when you want the first name? It seems sensible that you should be able to ask for the first name on the result even if it turns up null... you just want it so that the first name of a null result is also null. What to do? Well, you lift it. The function `personName : Person -&gt; String` is our "boring" function. We want to lift it: `fmap personName : Maybe Person -&gt; Maybe Name`. If you pass in a non-null value, `Just person`, you get back `Just (personName person)`. On the other hand, if you pass in `Nothing`, you get `Nothing` back. Perfect. What about wrapping a person? If `person` has type `Person`, we can just wrap it with one layer of `Just` to get `Just person :: Maybe Person`. So wrapping works. What about compressing a double-wrapped object? In other words, can we turn a `Maybe (Maybe Person)` into a `Maybe Person`? At first glance, it's not clear what this question even means. In most programming languages, something is either nullable or it's not. You can't have a "nullable nullable person". But if you think about it, the notion makes perfect sense (and is lacking from those other languages). Think about it a second before going on. Ready? Well, If `person` is an object of type `Person`, then `Just person` has type `Maybe Person`. To get a `Maybe (Maybe Person)`, we would need to have another wrapper: `Just (Just person)` has type `Maybe (Maybe Person)`. But other values of `Maybe (Maybe Person)` are `Just Nothing` and `Nothing` all by itself. So a `Maybe (Maybe Person)` is a result which might be null... or it might be a nullable person (which in turn might be null or just a regular person). In effect, it allows us to distinguish two levels of failure: `Nothing` vs `Just Nothing` (and all over values are `Just (Just person)` for some object `person`). Multiple levels of failure are very common. In an application, maybe you want to get the initials of a person. To get their first initial, you might use the following strategy: use `personName : Person -&gt; String` to get their name, then use a function `firstChar` to get the first character of the name. However, `firstChar` won't be `String -&gt; Char`, because not all strings (namely the empty string) have a first character! Oops. So we can use `String -&gt; Maybe Char` instead, which returns `Just c` when `c` is the first letter of the input and `Nothing` in the erroneous case that the string was empty. We can compose these two to get `firstChar . personName :: Person -&gt; Maybe Char`. We might define `initial = firstChar . personName` to save on typing. But now, let's lift this new function to work on `Maybe Person`'s from the database: `fmap initial :: Maybe Person -&gt; Maybe (Maybe Char)`. Oh look! A double-wrapped character! Of course, much of the time, we don't *care* about preserving several layers of error-tracking. Maybe to us, an error's and error. So we want to compress the two levels of `Maybe` into just one. We do this with `join`. The function `join :: Maybe (Maybe a) -&gt; Maybe a`, when applied to either `Nothing` and `Just Nothing` just gives back `Nothing`. When applied to a `Just (Just x)`, it gives back `Just x`. It strips off the extra information. And we are done. When I first learned this, it seemed like it paid unnecessarily close attention to details I didn't care about. But even small programs are full of these little details. Most of the time, they are easy enough to get right without really thinking about them. It's just that when you *forget* about them, that's when bugs crop up. And they can be hard-to-spot, nasty ones. Each of the other parametrized types work in similar ways: With lists, mapping just means "apply the function to each element in the list". Wrapping means "make a 1-element list out of this thing". Compressing means "concatenate all the lists in this list of lists". For `Either e a`, all the operations work like in `Maybe` but it preserves the error object. For `IO`, lifting a function `f :: a -&gt; b` gives you a function which takes an `IO a` action and gives you back another one which just calls `f` when it's done. Wrapping a value means "give me an IO action which performs no side effects and just returns this value". Compressing means taking an IO action which returns an IO action when you run it... and just cut out the middle man: run the action, then run its resulting action". Hope that helps.
http://codon.com/refactoring-ruby-with-monads It's not Haskell oriented but explains them better than anything else I've read
On x86 you get excess precision (and therefore non-determinism) unless `-msse2` or `-msse4.2` are used. Note that these options only apply to the current compilation; pre-compiled code (including base, where a lot of the floating-point primitives are implemented) will be using excess precision. I don't think the LLVM backend respects the `-msse` options. Historical note: the C backend used to use `-ffloat-store` to get determinism, which was a huge performance loss so most people used `-fexcess-precision` to turn it off. The native x86 backend has always used excess precision by default, `-msse2` was added later. On x86-64 SSE is always used for floating point, so all operations are at the correct precision and are deterministic.
&gt; What is the black king in chess? This is a strange question, and the most satisfactory way to deal with it seems to be to sidestep it slightly. What more can one do than point to a chessboard and explain the rules of the game, perhaps paying particular attention to the black king as one does so? What matters about the black king is not its existence, or its intrinsic nature, but the role that it plays in the game. —Tim Gowers, *A Very Short Introduction to Mathematics* I can't help but think of this quote every time someone asks, "What is a monad?"—which is the wrong question. 
There are no proposals to abandon it that I'm aware of. I use a StackOverflow-markdown-to-lhs translator all the time for my own projects, it's so much easier to comment stuff in and out, and visually separate code from comments. Documented here: http://stackoverflow.com/questions/12666723/i-taught-ghci-to-compile-my-stackoverflow-posts-can-i-make-it-slicker (Edit: disclaimer: I'm the author of that SO post.)
Hi ! Thanks for the tip ! I din't know about that subreddit .. Great ! There goes my next pom :P !
Such a good analogy. I'll steal that.
Can you actually run haskell on the fly, without pre-compiling a binary?
Not entirely: http://bitemyapp.com/posts/2014-11-22-literate-url-shortener.html http://hackage.haskell.org/package/bloodhound-0.5.0.1/docs/Database-Bloodhound-Client.html I'm still figuring out what works well and what I'm happy with. I think it's pretty contextual. Depends on the medium and what you're trying to accomplish. Little of what I write day-to-day merits literate style.
The Utrech Haskell Compiler (UHC) is built using Shuffle; which looks like literate programming on steroids. I haven't used it yet, but it looks impressive. http://foswiki.cs.uu.nl/foswiki/Ehc/ShuffleDocumentation 
&gt; For a long time GHC had a large number of literate modules. My impression is that those are being phased out in favor of exposing the comments haddock-style. I think they have been completely removed (i.e. converted to .hs) now.
I love literate Haskell for tutorials. It allows me to write tutorials that can be compiled and included in my test suite. This is the most reliable approach I've found to keeping tutorial docs up to date. Here is one of the tutorials I've included in a test suite: https://github.com/snapframework/heist/blob/master/test/suite/Heist/Tutorial/CompiledSplices.lhs And here is the markup output we get from pandoc. http://snapframework.com/docs/tutorials/compiled-splices
I hope your book turns out well... I never had the benefit of the more recent resources that people seem to like (NICTA etc), and I sort of just had to muddle my way through learning haskell, since as you point out, most of the available books aren't really suitable, and the literature that the haskell community itself has produced is in general kind of inane (IO as a state monad nonsense, awful LYAH)—and most haskell-oriented literature is pretty insular and isolated from modern PL theory and mathematics, so you start to think as a beginner that typeclassopedia represents the reality of algebra, or that type classes and laziness are essential aspects of functional programming in general (tldr: they are not), or that intensional/expression-oriented approaches like monads or *internal* effect systems were/are the state of the art for effects, or that they were invented in haskell, or that it is a feature rather than a bug to conflate doing with being, etc. I'd love to see more literature that just tells people how to do real stuff well in Haskell. there's a big gap between "learning Haskell" and being able to write real production code, I have found, and that kind of need doesn't seem to be well-served by current resources (at least, judging by the number of people I see who think that the "maybe monad" is a suitable way to implement error handling). I really like marlow's concurrency book; I am hoping that the two in-progress Haskell book projects I am aware of can at least reach and hopefully exceed its quality and usefulness. We need less propaganda, and more practical instruction. looking forward to reading your book when done! 
That doesn't help. I understand what a list is. I understand what a pair is. I do not understand what a monad is.
This makes as much sense as any other I have seen so far.
I'm confused. Do you like literate programming?
Ah, that's a very good point, you're right.
It's the wrong question in the sense that asking it suggests you're not in the right frame of mind to make good progress in Haskell. The next paragraph in Gowers's book begins: &gt; The abstract method in mathematics, as it is sometimes called, is what results when one takes a similar attitude to mathematical objects. This attitude can be encapsulated in the following slogan: a mathematical object *is* what it *does*. ... Haskell encourages the same attitude. So the right question is something like, "What can I do with monads? What role do they play in Haskell?" And to be sure you're thinking sufficiently abstractly, you should be looking for the things you can do with *all* monads. That commonality is what a monad is, and that's all. The larger point here is that, as a Haskell programmer, you will benefit greatly from cultivating a more abstract way of thinking. That's what mathematicians try to do, too, which is why Haskellers get along so well with them. (Gowers's book is short and costs about $10 on Amazon, by the way. I encourage you to read it.)
Interesting, then why do we call lists "lists" and not, say, "random access sequences"? Or, by application of Gowers's reasoning, should not all types in Haskell be verbs?
Names have nothing to do with it, and it's not a noun-verb distinction. You need to ask what you, the programmer, can do with things (and what you will know about the results), not what the things themselves do. When you're dealing with simple objects that have close analogues in the real world, you can get away with thinking about them in ways that will begin to fail as things get more complex and more abstract. The attitude Gowers and I are advocating is not some sort of blessed right-mindedness; it's just a way of thinking that often works when others leave you in a muddle.
Just Idly Curious, but is there a place where you aired your complaints, or do you just air them in conversation?
Speaking of lists, they can implement the "Monad" interface in Haskell, just like they can implement other interfaces like "Iterable" in languages like Java. You need two functions: - One function that constructs a list out of a single value (returning a list with a single element). - Another function that flattens a list of lists (this function is called "join" in Haskell; the more commonly used &gt;&gt;= operator is a really a combination of "join" and "fmap"). You must also ensure that certain "reasonable" laws are preserved (much like one must respect the contract of methods like "equals" and "hashCode" in Java). For example: if you want to flatten a list of list of lists (List&lt;List&lt;List&lt;SomeClass&gt;&gt;&gt; in Java) you can flatten the outer "list of list" first, or the inner "list of list" first. For the List type to be a valid instance of Monad, you should always arrive at the same result in both cases. In other words: "join" must be an associative operation. And what do you gain with all this? The ability to compose functions that return "listy" results in a "sane" and consistent manner.
Could you elaborate on what you mean by "conflate doing with being"? Is this a reference to thunks, or something else?
That's one facet of it yes... There's been a lot of good research lately on polarity and how it manifests in the fine structure of a proof/type theory. Here's something approachable from Conor McBride &amp; Sam Lindley: http://homepages.inf.ed.ac.uk/slindley/papers/frankly-draft-march2014.pdf Also check out "call by push value" and focussing (key names: Paul Levy, Noam Zeilberger, Robert Harper, Dan Licata)
A monad is like a burrito! https://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/
Sometime ago I wrote [this explanation](http://slepnev.blogspot.pt/2014/06/yet-another-monad-tutorial.html), which was [quite well received on proggit](http://www.reddit.com/r/programming/comments/284mdx/yet_another_lousy_monad_tutorial/). It starts with five parallel examples, so you can see the commonality easier, and uses a pseudo-Java syntax for monads, so you don't get scared by Haskell.
This doesn't affect purity maybe, but the result is unwelcome.
To a computational researcher, 0.1 + 0.2 = 0.3 is a good result. I wonder why languages like MATLAB can do this, but Haskell choose not to do this job elegantly. It just closes the door to people who concern precision issues. http://www.reddit.com/r/haskell/comments/1k97l2/is_haskell_a_good_language_to_do_numerical/
Re-reading hoping it will click might be a "waste" of time. I find my time is better used if instead of reading the same theory over and over again, to just fire up the ghci and play with the concepts until you can make sense of it. 
This is a very good point: "A lot of Haskell's source code is pretty easy to follow". Don't be afraid to just dive in. 
Is it possible to define my own Data instances? 
Well: (&gt;&gt;=) :: Maybe a -&gt; (a -&gt; Maybe b) -&gt; Maybe b Nothing &gt;&gt;= function = Nothing Just something &gt;&gt;= function = function something and (&gt;&gt;=) :: Either e a -&gt; (a -&gt; Either e b) -&gt; Either e b Left error &gt;&gt;= function = Left error Right something &gt;&gt;= function = function something and -- not an efficient implementation, but should be correct (&gt;&gt;=) :: [a] -&gt; (a -&gt; [b]) -&gt; [b] [] &gt;&gt;= function = [] (x:xs) &gt;&gt;= function = function x ++ (xs &gt;&gt;= function) and (&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b -- implementation very special and not useful to show We define the function -- Just a monadic version of succ minc :: Monad m =&gt; Int -&gt; m Int minc x = return (x+1) and with it we can see that Prelude&gt; Nothing &gt;&gt;= minc &gt;&gt;= minc Nothing Prelude&gt; Just 5 &gt;&gt;= minc &gt;&gt;= minc Just 7 so &gt;&gt;= for Maybe values will propagate Nothing and otherwise just chain monadic function calls. You can try it for Either e a – it will do the same thing except propagate Left values. For lists, it's interesting. We start with the helper function posneg :: Int -&gt; [Int] posneg x = [-x, x] It might not be immediately obvious at first, but this function is also monadic, because list is a monad. `Int -&gt; [Int]` is very similar to `Int -&gt; m Int`. In any case, here we see what &gt;&gt;= does for lists: Prelude&gt; [] &gt;&gt;= posneg [] Prelude&gt; [13] &gt;&gt;= posneg [-13,13] Prelude&gt; [1, 2, 3] &gt;&gt;= posneg [-1,1,-2,2,-3,3] it takes the lists you get when you apply the function to each element of the previous list, and it makes a new list out of those. For IO, we don't need to dig far to find an example. We define a helper function greeter :: String -&gt; IO String greeter name = putStrLn ("Hello, " ++ name ++ "!") this function also has a very similar type signature to the earlier ones. In any case, Prelude&gt; getLine &gt;&gt;= greeter john Hello, john! Here we see that &gt;&gt;= for IO combines the two IO actions. It asks the user for a string, and then prints that string. Play around with these! If you play with them for 15 minutes every day, I am confident you will understand how they work in a months time, and then you have learned to use four monads! After that, there are many, many more monads to learn, but these first four are in my opinion among the most useful!
&gt;since intermediate stuff can often just be learnt on the job. Though that bit would be easier with more examples and better documentation :)
MATLAB just displays it a bit more nicely. Try putting it in exact or hex display mode and you can see that it displays numbers which are different as the same number in the default display mode. You can't represent arbitrary floating point numbers exactly with a finite amount of memory. There's no better way than IEEE 754. 
The abstract and introduction of Phil Wadler's original [monad paper](http://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf) gives a similar but more comprehensive description. I also think that definitions of what a monad "is" miss the point, especially when they focus on operational semantics. A monad in Haskell *is* a type constructor of kind `* -&gt; *` equipped with functions `return` and `(&gt;&gt;=)` and laws describing their behavior. It's more interesting to focus on what you can *do* with particular instances of Monad and what similarities allow these things to be generalized by the Monad typeclass.
I agree strongly with this sentiment. I'm partial to not even deigning to call `IO` and `[]` "monads" since that confuses the issue making it seem as though monad is some secret to understanding a whole class of types instead of more or less the opposite. Monad is a class, an abstraction over a particular kind of interface. All of the interesting stuff lives in the implementer.
&gt; That's not what the article is saying at all. In fact, the article isn't even about Haskell, not really. It's about human nature, it's a really important article in the history of our community, and it hurts my feelings to hear you say such negative things about it. My apologies for the way I came across. My statement was meant as a summarization of perceptions relayed to me, not my personal view.
&gt; You might not be aware of it, but the subject of explaining monads is a sensitive one in this community, which might be why your question is being downvoted. This would be an unintuitive reason to downvote a question.
Thanks for the link to the paper.
Between this and other questions I asked within the last two days, I think I now have the outline of an answer to what is probably a better question: what is meant by "monad"? For example, to use your analogy, if I were to ask, "What is meant by 'musical instrument'?", one might explain the idea of an instrument and then the idea of music and then say, "a musical instrument, therefore is an instrument which makes music".
So there is a concept that is difficult, and it takes some time to get if you study it too early, but if you study it at the right time it is no trouble to get at all. Now should we tell beginners "this concept is easy" and then they discover it is not and they get discouraged. Or should we say "take your time, and build up the appropriate knowledge and _then_ it will be easy". I tend to think the most important advice for new learners is that they can _take their time_.
LYAH's English is great. I read somewhere that it was intended to be in the same style as _why's Poignant Guide to Ruby, and I think it really captures that spirit. I personally learned basic Haskell from it. It was fun to read the descriptions of the syntax, the important types and typeclasses, and to go at my own pace. I still go back and re-read parts from time to time. Is it everybody's cup of tea? No. Realise that if you think that a particular Haskell tutorial (e.g. the UPenn course) is bad, you'll notice the 'refugees' from that tutorial and confirm your bias. The same goes for LYAH or anything, really. Of course I absorbed Haskell knowledge from many other sources and formed my own opinions. I think everybody should be exposed to multiple Haskell tutorials. It's not productive to recommend some but put down others.
Please don't waste your time trying to make sense of that definition. Are you comfortable with type classes? Have you tried reading the sections in LYAH or RWH about `Monad`? If so, where are you stuck?
Correction: Brent's last name is "Yorgey."
A parser combinator library is just a set of functions that work together to parse something. The reason people came up with parser combinators is that the old way--lex and yacc and bison and all those things--is just really cumbersome and forces you to deal with multiple languages (a declarative language to define whatever you're parsing, then C or whatever to run the parser, etc.). The parser combinator approach is relatively much simpler--everything is written in Haskell and the combinator functions end up looking like a mini declarative language for defining the parser. It's really neat.
My motivation for PostgREST was different from the other libraries, in that mine is not really a library, it's an app. People who don't program in Haskell at all can run it as a server which looks at a Postgres db and then serves an API corresponding to the schemas/tables/views inside. Haskell just happens to be the language that powers it.
That's a good intuition -- we say something forms a monad if it has certain operations and laws, just as we might describe other abstract algebraic structures like rings. So if someone says "you can use a ring to count things" well, maybe _some_ rings, but probably not the boolean ring. But conversely if someone says "rings capture logic" and then you look at the real numbers, then that's probably not a good intuition for them either. So a Ring, like other structures, really is best thought of as a collection of objects and operations satisfying certain relationships. And like with the abstract method in general, the payoff really comes when you start to transport constructions and intuitions across different instances.
Sorry that's one area of Postgres that I don't know much about. Maybe you can get Amazon RDS Postgres to do the read replication for you rather than trying to configure a cluster yourself. https://aws.amazon.com/about-aws/whats-new/2014/11/10/amazon-rds-postgresql-read-replicas/
The definition I quote actually makes far more sense than anything else I have read all week.
&gt; Now should we tell beginners "this concept is easy" and then they discover it is not and they get discouraged. Of course not. My concern is the use of a term or idea which people do not know enough about to be able to describe and the seemingly heavy reliance upon the concept which relates to that term. An analogy would be a brain surgeon not being able to tell someone what a nervous system is.
&gt;Why is plain English always the loser when confronted against it? Best way to learn datatypes like IO, Reader, State, Writer, [], Maybe, and Either and their associated Monad instances is to just write code. Jabbering on Reddit doesn't help anybody. Doing the work does.
You can define a custom `Data` instance, but with the details of `pretty` hidden you are pretty much out of luck with regards to actually constructing the `fold` parts of the instance. You can only `gunfoldl`. 
&gt; Why is plain English always the loser when confronted against it? Because monads aren't a concept that would exist in English? Here's what a monad is, it's any tuple of functions of types (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b return :: a -&gt; m a obeying the laws return a &gt;&gt;= f ≡ f a -- left identity m &gt;&gt;= return ≡ m -- right identity (m &gt;&gt;= f) &gt;&gt;= g ≡ m &gt;&gt;= (\x -&gt; f x &gt;&gt;= g) -- associativity ...you can rearrange all that algebraically (as you can arrange `x-y=0` to `x=y`) but in the end this is the *one and only* definition. The barrier to understanding isn't the word, it's not the definition, it's getting to grips with all the different things involved (higher-order functions, polymorphism, higher kinds) and doing the usual yoga (that is, work through examples) until you have built proper intuition. No amount of explaining will replace that, same as you don't explain to your kid how to balance on a bicycle. It's maths. *Constructive* maths, on top of that. God forbid that programmers should learn some, they're only using it for their jobs day in day out.
Different analogies help different people. I think it's OK to point people to the various monad tutorials, as long as you suggest they read more than just one or two. Burritos might work him, and computational contexts her. Monads clicked for me when I finally de-sugared them and related classes to interfaces. Do notation was obscuring exactly what I needed to see in order to understand (which I think is a whole 'nother problem with teaching Haskell). Edit: extra word.
Okay. Then, fine. Haskell is clearly not for me. All I will say is this: if the people who supposedly know a topic best cannot explain *what* they know about it to others and they actively dislike someone attempting to get a straightforward answer in a forum they feel comfortable with, there is something fundamentally wrong.
By the way, if I am wrong in my description of monads, someone should change the Haskell wiki entry because, as far as I can tell, it essentially says the same thing. [Link to entry](https://www.haskell.org/haskellwiki/Monad)
This man is right. I am currently studying and got used to read and read and read. So the first attempt when confronted with Haskell was to read all I can get on Haskell and then try to make some programs. But it was so frustrating. I read about lenses, arrows, monads, ... and i got nothing done. It took me weeks to learn and I could not write any useful program. But I still tried! And then I remembered my young days, when I played with Basic, Assembly, C to hack some calculator. When I was young, I did not want to know some abstract concepts, I just had a goal and tried to reach it by just reading what was necessary to know. I just copied code and changed some parts to see how it behaves. With this approach I started to write my code. It was not the best code, it was not the most elegant. But it worked, and that was important. So I changed my approach to Haskell and let me tell you something: I now can write usefull programs. This is a really good feeling. And by just using lensen, monads, arrows ... I begin to understand them! Edit: And btw. my first two usefull programs were sth I could get done easily in python or bash, but i did it with Haskell and it was not hard at all!!!
If you learn and use a few examples, you'll start picking up an intuition for the general case. Or at least, that's how it worked out for me. The intuition for the knowledge won't come without using the knowledge, but that's true of any technical topic.
The DFAs are implemented using my very much still in development Automata library, located in the same [repo](http://github.com/terrelln/automata). If you check out the library I would greatly appreciate any feedback you might have.
This made me even more confused :( The more I read about Haskell the more I believe you must first finish a math major in order to be able to understand even the examples and blog-posts about Haskell :\ I like the "what is meant by *monad*?" question. But is there not a way to answer this without using a concept from mathematics (like rings)? The linked article from this post makes an analogy with musical instruments. And that made perfect sense to me. I can relate to the "Now what?!?" sentiment. I understood by reading LYAHFGG *what* monads are. But that did not help. I have read an re-read that chapter and other blog posts about monads thinking that I missed some secret sauce somewhere. They are all sprinkled with math concepts here and there which are equally abstract. The analogy with musical instruments suddenly made it click. I still don't know how to use or write monads, but at least now I know *why* I don't understand monads in general. You could probably make the same analogy with spoken languages. Understanding *what* languages are does not automatically make you fluent in all of them.
I'm not sure whether I understand your comment correctly. The axiom of extensionality implies that `{0,1} = {0,0,1}` as well as `{0,1} = {1,0}`. This is what I was arguing for above, but the text, I believe, disallows `{0,0,1}`.
Monads _are_ a concept from mathematics. The analogy I gave is a bit troublesome in that unless you know math and know what a ring is, the analogy won't help much. But there is only so much that I can describe a mathematical concept by analogy to nonmathematical things, sorry :-(
Oh... that makes sense then. I thought Monads were a concept originating from Haskell... Well... sooner or later it will click for me too I'm sure. Can't wait for that to happen. I love these "click" moments :)
Yeah, there's the minor matter of undecidability too :) But hey, that never stopped anyone right? :) I left out the other arithmetic operators just to keep things simple -- you're right, as things stand it would be decidable. Perhaps I should add a `Mult` constructor just to make that point :) (The actual compiler where I am implementing this of course has a much larger expression language still..) As regards your second example, sure. This is just one optimization of many, and indeed there is a strong interaction between the inliner and the symbolic evaluator. 
I am much embarrassed, and have corrected the error. Thank you.
I think [barbie](http://i.imgur.com/9uLoO9H.png?1) says it better. To expand, I think the explanation above won't be very helpful because: - The general idea of `monad` is a clichéd term with a long history of usage in haskell, in computing and in mathematics. As a cliché, it has lost any central and agreed meaning that can form a basis for encapsulation of general principles. - The narrower definition of a monad as coded in haskell is a [type class](http://haskell.org/haskellwiki/Typeclassopedia) and each and every type class in haskell fits together like a jigsaw. This is Barbie's point of view - you need to first understand functors, monoids, category theory and applicative functors to understand monads. - Monads are less of a big deal than outsiders expect. I can't recall ever inventing my own monad (outside of free monads and monad transformers). And I don't feel competent enough to inflict my idea of a monad on others (hell can be other peoples monads). Even after two years haskell learnings, I honestly still don't know what a monad is really. It doesn't seem to matter.
I am so excited! 
That terminology makes sense but it may confuse beginners into thinking that a monad is uniquely determined by the type. When you are still learning that may set off your understanding on the wrong track. I certainly remember finding that very confusing. It's probably a good idea to explain early on that the relationship between the monad and the type is similar to the relationship between a comparison operator and the type. There's often only one useful one, but that is a choice that we as programmers make; its comparison operator is not an inherent property of the type.
no.
You seem to be confusing monads and values that have a monadic type. A value of type `IO a` is not an IO monad. `IO :: * -&gt; *` is *the* `IO` monad.
Instead of rings you could equally well take comparison: the Ord typeclass in Haskell, IComparable in C#. You have a type, and a comparison operator on that type. The comparison operator should satisfy laws: if a &lt;= b and b &lt;= c then a &lt;= c. This situation is analogous to monads: you have two operators (called `return` and `&gt;&gt;=`), and they have to satisfy some laws. I wouldn't say that monads are particularly mathematical compared to other things you use all the time when programming. They are less mathematical and more computer sciency than sets, integers, etc. The main problem is that people expect that they can learn monads by reading some abstract description of them. Suppose you have never heard of comparison and I told you that comparison is an operator of type `(&lt;=) :: a -&gt; a -&gt; Bool` with the laws `forall a,b,c. (a &lt;= b &amp;&amp; b &lt;= c) =&gt; (a &lt;= c)` and `forall a,b. (a == b) &lt;=&gt; (a &lt;= b &amp;&amp; b &lt;= a)` and `forall a,b. a &lt;= b || b &lt;= a`, do you think you'd understand it? I certainly wouldn't. You could maybe understand the laws in a symbolic way, but you wouldn't have any intuition about comparison and you wouldn't know what it's useful for. Would a skateboard analogy help? Or a pizza analogy maybe? I doubt it. The way to understand comparison is to look at examples: integer comparison, lexical string comparison, etc. The same goes for monads. There is no easy shortcut. You have to build up intuition by looking at several different monads and writing programs that use them. Then you'll start to understand the general concept, and why the laws are what they are. Monads are easy, you just need to put in the work.
Soon we will reach The 'The "What Are Monads?" Fallacy' Fallacy, in which multiple people explain why people shouldn't explain Monads.
Because it doesn't actually say anything concrete.
I wholeheartedly agree with you! The funny thing is how close you're to commit the fallacy in this post yourself. Maybe it's simply impossible to avoid it? :) &gt; a type class with a couple of functions and three laws Come on! A monad is not a type class, it is... [a monad](http://en.wikipedia.org/wiki/Monad_%28category_theory%29) I agree with you that you should simply learn how to use a Either, an IO, etc and then dig deeper and try to understand the reasoning/logic behind. Once you know how to use them, there is whole new world waiting to be discovered, if one cares enough to do so. Unless you are an academic studying languages, I think this is the best approach to learn them - use them, then find out how they work. Once you know how to use various monads, I strongly suggest to learn the basics of Category Theory, try e.g. [Category Theory for Scientists](http://math.mit.edu/~dspivak/teaching/sp13/CT4S--static.pdf) by D. Spivak. Otherwise, I'm afraid it is not possible. What do you think?
&gt; When they finally understand monads, the traditional thing to do is to write a blog post explaining them. This blog post is often misguided and, in the worst cases, actively hurt the understanding of others. I just can't agree more. Many blog posts on monad make weird analogies or give a very limited definition. Say, monad is not a magical box (abstraction itself is the box) nor a sugar for sequential programming. It's rare to see anyone saying that monad is just a construct, or, at least, an interface.
That makes perfect sense. I only recently started to look into haskell, and the term "monad" keeps popping up all the time, and it looks like it's a powerful thing to understand. Same goes for functors and applicatives (those are still kind of wobbly in my head as well). I don't expect it to be easy to "get it". And I am sure I will get there eventually. But I have to agree to the *"What Are Monads?" Fallacy*. While reading several blog-posts, I have already been thinking: "Give me examples for god's sake...". Until now I actually did not know that `Maybe` and `Either` both are monads. I will look into the implementation of both, but I doubt that two examples will be enough to see the pattern.
The book should ask you questions and tell you when the answers are correct. 
I've learned a lot in the #ocaml channel for quite some years. And not only related to OCaml, but functional programming in general. There's some folk lore in functional programming that is just recently became mainstream, and by staying in the channel you kind of absorb it (I don't know if it's still active).
The entire point was that you don't need to make a new language with different semantics! This is all moot anyway because GHC's TypeLits mean you have real inductive data types that you can check at compile time and don't depend on a certain semantic interpretation (via type level nats), so real inductive data types are going to come in the future as library authors start to use this feature, but to try to sum things up very briefly: The distinction only exists at run time. If you explicitly embed ML semantics into Haskell by explicitly encoding how the ML program is evaluated, it behaves the same. You can tell (unit -&gt; t) apart from t again, despite your claim that this can never be recovered. You don't have to change Haskell in any way, you are just changing your -&gt; to something more like the ML -&gt; by making it have the side effect of heating up your computer (and arbitrary IO). Although you'd have to write down something other than -&gt;, this is not forbidden. The claim being made that Haskell has a paucity of types is entirely about runtime behavior, not types. If it was about types, you'd be able to demonstrate that programs that don't convince the type checker in ML are able to convince it in Haskell.
I'm going to write a blog post about how we're on a downward spiral of indefinitely nesting more and more levels of "The _ Fallacy".
If only this were true. It is less of an issue, perhaps, but it is still an issue. Look at all these blog posts that open with a bunch of imports at the top, instead of having them at the bottom where they'd look better and make more sense (or having them quoted at a first use of a feature imported from them, etc). Not to mention things like order mattering on pattern matches, and so forth.
Well it shouldn't be hard to just not output those imports etc. at all, lhs+tex can do that, too, why can't the markdown version. As to pattern matches: No please *don't*. If the order matters then the order matters and you shouldn't confuse people by having your text use another one. If it really becomes nasty, refactor the code to not need fallthrough.
Now we only need a parser combinator library that works nicely with custom token types, not just Char. 
 fix (\x -&gt; the x fallacy)
I prefer fix (flip the fallacy)
If it's covered in the first couple of hours of every category theory course, why is it that even in this thread a good definition of Monads cannot be found?
I think I was just scared of using `tokenPrim` directly (looks like parsec internal to me), but now that I look at that again, I don't think it's possible to make custom token types simpler. Also, I'd like to have error reporting like `trifecta` :)
You should trust your community to not be idiots, and trust them to be capable of building accumulative knowledge independently and collaboratively. Even if people have to sift through hundreds of blog posts to get their 'ah-ha' moment, that's still something that is valuable to learn how to do. Even if they have to ask hundreds of developers what something is, until they make that connection, that is still something that is valuable to learn how to do. There are plenty of moments in time where people have to determine whether they have a shitty understanding of something, or a good understanding of something - and it doesn't depend on someone giving them a bright red check on a paper to validate a one time understanding. Shockingly, there WILL be moments in time where NO ONE has a good understanding of something, and together, people have to develop that understanding in order to make significant progress. There are plenty of people in the world who pretend they understand something and they are only vaguely 80% sure they understand something - and that 80% typically is the easy part of understanding that no one really has to try hard anymore to understand because it so heavily forms the base of thought for the community. And I honestly am pretty sure that rule goes for 99% of concepts, because it's actually really really really really really really hard to be able to convey an idea from one mind to another, assuming the first mind's idea is dependent contextually on their mental environment, (which is established uniquely assuming people learn using different orderings of infinite knowledge) as is the second mind's idea contextually dependent on their mental environment in the same way. I think it's better to let people learn, and to learn how to learn, and to learn how to know for themselves, than to chop off their thinking and say "this is impossible for you to understand". People are not idiots. They know when they understand something and they know when they don't, and it doesn't depend on anything outside of them aside from the knowledge they have collected over their individual journey. The people who don't care are the ones who give up, and eventually their influence on the community is reduced, if not removed entirely. tl:dr; this is why most developers do not use haskell. There is such a high barrier to intellectual entry over stupid things that once understood, foster a sort of earned arrogance in the community. 
I was a bit scared by tokenPrim too but it turns out that you only ever use it at a very low level and then work with (and define your own) combinators. In my wee BBCode parser, I ended up using it only twice.
`IO :: * -&gt; *` is the IO type constructor. The dependent triple `(IO :: * -&gt; *, return :: a -&gt; IO a, (&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b)` is a bit closer to what the "`IO` Monad" is. More technically we might say that such a triple above is a monad if and only if it also satisfies the three monad laws. `Monad` is a typeclass and thus allows us to speak generally about triples like the above where the first entry is quantified over, e.g. `forall (m :: * -&gt; *) . (m, return :: a -&gt; m a, (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b)`. Even more specifically, `Monad` can be thought of as a type constructor of its own which takes types of kind `* -&gt; *` to types of dictionaries containing `(return, (&gt;&gt;=))`. This is accurate with the added detail that `Monad m =&gt; ...` implicitly passes the dictionary if its identity can be solved for uniquely, so really the prior type is something more like `forall m . Monad m =&gt; ...`.
Thanks to all the authors and users who commented. This certainly cleared a lot up for me and I think it's extremely helpful to have this on record for the community. As I suspected, it seems all the projects have significant differences in aims and implementation even if they may look similar on the surface. Thanks again guys!
&gt; What made monads hard for me was that it was three concepts rolled into one: ad-hoc polymorphism, parametric polymorphism, and higher-order functions. And don't forget higher-kinded polymorphism, a feature that many languages don't even dream of having.
&gt; My take away from the articles I've read is that monads seem to be a "must-have" understanding for haskell. Understanding individual monads (IO, State, Either) is "must-have", but understanding what "monad" means is not. Does that make sense?
Yes. It makes sense, and helps a lot. Thank you!
Hey all! I'm really excited to announce the first release of Kronos Haskell, which is just [IHaskell](http://www.github.com/gibiansky/IHaskell) packaged together with [IPython](http://www.github.com/ipython/IPython) as a Mac app totally independent of any external environment. It comes with its own GHC, Cabal, package databases, etc, so the installation should consist literally only downloading the zip file, unzipping it into `/Applications`, and double clicking on it to run it. If you'd like to see some of the cool things it can do, I recommend typing in the examples on the website (apologies – for now they're images...). In particular, the last example on [the website](http://www.kronosnotebook.com/haskell) demonstrating interactive widgets for Parsec is my favorite :) It includes a number of built-in packages and is capable of installing (through a dialog) many others – should work on most pure haskell packages, though maybe not on ones that require compiling C code or linking to C libraries. You can export created notebooks to HTML and do all the other nice stuff IPython usually lets you. If you have any issues, please let me know. This is still a work in progress! As an aside, please don't spread the link around too far (post it on other social networks) – I know that people are going to find problems, and I'd like to make sure that I have an opportunity to fix them. I know it's a bit of a silly request, since once on the internet it's available everywhere, but still, I'd appreciate it :)
Please [contribute new programs](http://benchmarksgame.alioth.debian.org/play.html#contribute) that do represent contemporary fast Haskell ! The latest greatest "The Glorious Glasgow Haskell Compilation System, [version 7.8.4](http://benchmarksgame.alioth.debian.org/u64q/haskell.html#about)" is installed and waiting.
I've no idea what's a terrible design, because apparently the design cannot be expressed in words in the English language. That is terrible.
If by "couple of hours" you mean less than 8 hours, I'm surprised since the category theory courses I've heard of don't cover monads so quickly. They typically seem to cover the definition and many examples of: categories, functors, natural transformations, limits and colimits, adjoint functors; as well as the Yoneda embedding and lemma--- all before discussing monads. 
&gt; [...] in which multiple people explain people shouldn't explain that you shouldn't explain Monads. FTFY
&gt; Would you be so dismissive of the RSA algorithm because someone couldn't explain it to you over lunch? But you can, in fact explaining public/private algorithms via the 'lock analogy' is a common thing. It turns out that asking this question about monads will also spark a multi hour IRC debate.
Is there any chance of a Windows version in the future?
Will do; thank you. The McBride &amp; Lindley paper is fascinating as well as amusing (a usual combination for things associated with McBride...). Getty and I talked about CBPV last summer during my Galois internship, but I haven't looked at it in a while — thanks for reminding me!
The "lock analogy" is just that... it's an analogy. It doesn't tell you anything about how RSA actually works. To understand RSA, you need to know about modular arithmetic, the Euler totient function, and how to efficiently compute modular exponentiation. And once you have those, you have enough to implement it, but not enough to understand why the algorithm works. For that, you need Fermat's Little Theorem and the Chinese Remainder Theorem as well. All in all, it's not all that complicated (RSA is a pretty simple algorithm when you put it in perspective). But unless you've seen some of the material before, it takes time to digest it all. The same is true of monads. Except it is almost guaranteed for most programmers that they are completely lacking the background to understand them. That's not to disparage people -- it just means that if you really want to understand "what a monad is", it's going to take a while. You first have to develop and understanding an intuition of what categories are, what functors are, and what adjunctions are. The pedagogical progression in Haskell sucks at the moment, and many people aren't interested enough to stick it out. If you feel math is dumb, then you certainly won't fit in with the Haskell community. But if you're willing to sit and be confused for a while, you might find you come to understand something new and interesting.
I certainly hope not. We use it extensively where I work in conjunction with lhs2TeX to: a) to communicate e.g. how algorithms work and b) to produce nicely formatted code and results to give to regulators. I also use it to write blog articles which I process using BlogLiterately.
When it was posted the other day on /r/programming people were comparing it to Yesod.
Yep, it's huge. It has to build a ton of libraries that ghcjs needs, ghcjs itself, and then all the base libraries for ghcjs. It takes even longer on OS X. The good news is, now that it's moved upstream, binary packages will eventually be available for more of it, possibly all of it.
Yes, although it's somewhat unlikely in the near future. The reason for this is two-fold. First of all, IHaskell itself relies heavily on the `unix` package, which (as I understand it) does not exist on Windows. The `unix` package is necessary to use things like `hDupTo` and making pipes to deal with stealing `stderr`, `stdout`, and `stdin` from the GHC API process and exposing it to the notebook (in a browser). Second, and perhaps more relevant, is that for the time being IHaskell and Kronos are really just a one-person project. I don't have a Windows machine and know very little about development on Windows. So although I'd really love for IHaskell to work on Windows, I don't have a huge amount of resources, time, or expertise to devote to it... Of course, I would be more than happy to work with someone with more Windows experience and motivation than me to make it happen sooner :)
Hah, good observation :) The real reason for the difference in pricing is a combination of a few things. First of all, IHaskell is my fun side project, which I like working on and promoting and helping people with, and so Kronos Haskell is just an extension of that; the Python version was a stepping stone to getting Kronos Haskell working, so it only made sense to package that in case anyone wanted to use it. I figured that it would be fun to go ahead and use this as an opportunity to learn a bit about how to sell a product, since it seemed like I had one. Second of all, there are way more Python programmers than Haskell programmers, so the market is of course larger. (That said, the IPython setup is easier than the IHaskell setup, so perhaps I am providing more value on the Haskell side...) Finally, I'm somewhat interested in helping further Haskell as a language and ecosystem, and hopefully some beginners will find IHaskell a good way to get into it – but for that it has to be free, I think. I'm hoping that IHaskell can become one of the faster ways to go from "sounds interesting, I'd like to try it" to playing with `Chart`, `diagrams`, `parsec`, etc. All of that said, the Python pricing is still an experiment. If it turns out that it's not something that people want to pay for, it might also become free. We'll see where that goes :)
re: being unnecessarily harsh Thing is, careless people bandy about the non-explanation that "monad is a type class, it is an abstraction." The effect if not the intention is to silence the newcomer, especially when accompanied with a swift kick to "just write some haskell, yo." For evidence of both, see the rest of the comments. If brent yorgey had said what /u/gelisam did, then the community wouldn't have fallen 6 years behind. Kudos to gelisam for his courage and moral honesty. 
You know, I thought it was funny that while the author was critical of metaphors like "monads are burritos", they leaned heavily on "the monad fallacy is like a musical instrument fallacy". I think they were spot on in this, but it was still amusing.
Let's see... for return, any pure fact can be inserted into a context that causes it to deduce a fallacy. For bind, Given a fallacy based on 'a', and an rhetorical argument taking 'a' as a premise and producing a fallacy based on 'b', we should expect to produce a fallacy based on 'b'. The math checks out.
It's mostly because the question had been asked so many times, and elicited so many bad answers, and generated posts like monad tutorial fallacy, that people see a post like yours and have a knee-jerk reaction that you haven't looked for yourself enough first. I don't think this way (I'm still a newbie myself in some ways), and tons of people in this community are helpful and friendly. But if you're being downvoted, I speculate it is because of reason above I gave.
For an alternative like `a &lt;|&gt; b`, Parsec will return the result of a if it matches, or result of b otherwise. The maximum munch rule means that if both a and b match, the result of `a &lt;|&gt; b` should be the result of a if a consumed more input than b, and the result of b otherwise.
&gt; Now if you find the way that they explain things too complicated, then I would suggest that you learn the things that are the preconditions to the way they explain things. As an analogy, if one is taking a biology class and does not know what a parietal lobe is, knowing to ask questions about the nervous is an unreasonable expectation. Conversely, when learning Haskell, if one asks, "What's a monad?" and is given an explanation of "It rather complicated and, depending upon where you are in the learning, you might want to look at monads as '\_\_\_', '\_\_\_', or '___', but keep in mind you will need to alter this perception over time because, while it is a crude first order approximation, it is *crude* and it is indeed a *first order* approximation. As you go along, you will find they are significantly different," my hunch is people would feel a heck of a lot better about the explanations. As noted in a recent blog post (emphasis in original): &gt;Also replies like, “it might not be the Haskell texts that is the problem”, are totally unconstructive and play into the “elitist” caricature of functional programmers. I don’t care if you invented Haskell, wrote GHC, wrote 10 books about Haskell, if you can’t be helpful then *just don’t say anything*. Go get a cappuccino and chill. Learning Haskell is hard, especially for people that aren’t accustomed to thinking in terms of mathematical functions. Which is, incidentally, *most programmers*. Most programmers are used to learning mostly similar programming languages, swapping out syntaxes as they go. They are not accustomed to a new programming language involving a total reconfiguration of their brain. It’s as much about *setting expectations* as it is anything else. and (again, emphasis in original) &gt; If you deliver your explanation of IO to a programmer and they can’t tell me what adding unsafePerformIO or returning into IO (cf. evaluate in Criterion) will do to a Haskell program, then your explanation has **failed**. These are two key points which I think people seem to forget very often, not just here, but in many arenas of life. At the same time, when one is told, "Hey, welcome. We're a friendly and patient bunch. No, don't worry about your 'hope-hope-hope' chain of concerns about IRC. It's cool," and then conversations turn into "***JESUS H. CHRIST, YOU FUCKING TWIT!!!*** We've explained it six ways to Sunday already. Stop asking!" it gets very off putting.
&gt; Come on! A monad is not a type class, it is... a monad In Haskell `Monad` is indeed a type class. Learning the full generality of monad in category theory is probably not that helpful for functional programming.
Sounds like most of those issues wouldn't be a problem for a Linux port, right? But I imagine that if Kronos Haskell is just IHaskell packaged with a bunch of stuff as an easy install, it might not be as necessary and/or useful on a platform like Linux where canonical package managers are a thing. I see that NixOS (which looks like it's shaping up to be the Distro Of Choice for Haskellers these days) currently [ships IHaskell](http://hydra.nixos.org/build/17774228), so that's good!
&gt; separate map and mapM I see `mapM` as quite a different beast from `map`: it bolts a notion of order onto the map. Unifying the two is bolting more structure than necessary to `map` which can be completely independent of execution/evaluation order.
As a random data point: I feel like from my math background I've commonly heard "over" used in abstract algebra when talking about groups/rings/etc formed by pairing operations with an existing set. So based on that, I would have expected "over" to be the common term used for constructing monads too.
For an arbitrary monad it has left to right evaluation order in the monad sense of evaluation order, but mapM on the identity monad is the same as map no?
That was my logic as well. In my experience, the most problems in installation come on Macs – on Linux, package managers do a good enough job that a GUI app like Kronos makes less sense. That said, I may want to make an ihaskell package for Ubuntu some day, and I've seen that NixOS does already have it.
Would it be possible for a piece of Haskell code to produce LaTeX output? (similarly as it is possible to produce graphics). And, an even wilder idea: Would it be possible to program custom copy-pasting? So for example you have some data-structure with some special visualization/pretty-printing, but you want to copy and paste the underlying structure instead?
My posts have been to the end of attempting to direct you into productive learning channels. I'm sorry if they've seemed otherwise. But recall that learning is a two-way street, and if you feel that is is appropriate to ask people to "teach better, teach better" then perhaps, out of fairness, you should also consider advice that directs you towards better ways to learn?
Well, at least Mac has brew, macports, stuff like that which is pretty widely used. I think Windows is the OS where bundled installers are still the most useful by far -- at least, last I checked, though I've heard rumblings about Microsoft-managed package managers in Windows's future...
&gt;manually handle pull requests I'm not sure I understand what you mean there, would you mind elaborating?
Very cool! I'm digging all this quasiquote-external-languages stuff (like [language-c-quote](https://hackage.haskell.org/package/language-c-quote)). It seems much more practical to shift the FFI layer to "wrappers around the bits you need / your particular function written in the external language", away from "complete wrappers of external libraries" (which need to be heavily maintained). It strikes me that the API for this could be made a lot smaller / more convenient by using [polyvariadic functions](http://chris-taylor.github.io/blog/2013/03/01/how-haskell-printf-works/). Seems like a very appropriate usage.
Once the build cache catches up it should be super quick on linux (just download some binaries). [Hydra is still borked for OS X](https://github.com/NixOS/nixpkgs/issues/2689) so it's going to take hours if you use OS X as everything will have to be recompiled.
That's awesome news - I've really been looking forward to BBP. 
Merely speculation, but maybe [OverloadedRecordFields](http://www.reddit.com/r/haskell/comments/2pnjdk/is_overloadedrecordfields_getting_it_into_7101/)?
Heh, my error, I read the newest version looking for the errors you mentioned. I assumed you were arguing {0, 0, 1} *can't* be a set, contrary to what the newer version stated.
Isn't Monad just a data? ( or datatype, i hope i am not using words too loosely)
So can a beginner without knowing the guts of "what is a monad?".
mapM with the `Identity` monad is the same, because `Identity` has `&gt;&gt;=` that doesn't specify evaluation order, and the RHS's function is non-strict on its argument. If the underlying language is strict, then is it still possible to have an order-free `&gt;&gt;=` and `map`? ---- As a separate question: If we define `map` as a special case of `mapM`, should we also define other structures as special cases of more general structures? i.e: should we have: type Maybe = Either () type Maybe = MaybeT Identity type List = CoFree Maybe etc? I think `map` and `mapM` are like `Maybe` and `MaybeT`. You want to define both because you don't want to "pollute" the simple with the complex, and because the complex can actually re-use the simple in its encoding, and it doesn't require any code duplication at all. I think `sequence . map f` is beautiful -- basically saying "impose a notion of ordering on the map's result".
I note that canonical FizzBuzz skips printing the numbers for multiples of 3 and 5. This DFA intersection method prints all the numbers instead, and "fixing" this is not simple with this method.
We might not get *all* of it, but Richard is extremely dedicated to this (and has quite a following!), so I would guess that we're going to see *some* new features that get us towards this goal. Perhaps being GADT-like in kind arguments, for example.
Where do they live?
Any correct definition of monads in Haskell must reference the concepts of "typeclass" and "higher-kinded type". If you don't understand these concepts in isolation, you have no hope of understanding monads properly, because Monad is a higher-kinded typeclass. As a rule of thumb, if you don't have a 100% understanding of [this paragraph from Wikipedia](http://en.wikipedia.org/wiki/Type_class#Higher-kinded_polymorphism), you're going to have difficulties. Hope that helps.
You missed one RESTful library, which to the best of my knowledge should be at the bottom of your list * https://github.com/ozataman/restful-snap
That advice, however, seems to be, roughly speaking, "It's not something you can know factually; it's something you have to know intuitively." No matter how well that works for others, it does not work for me.
&gt; Haskell still uses the GNU MP library - written in C ... I believe there's an effort to reinvent that wheel but I don't really see why except a kind of religious embarrassment. I remember now - it's not embarrassment , it's the license for the GNU MP library which can cause problems for some people deploying Haskell projects. 
In registers, on the stack, or in fields of objects on the heap (e.g. unboxed constructor fields).
I should add there are some objects that we normally call "unlifted" rather than unboxed that *are* represented by heap objects, such as MVar#.
Yup, the notebook interface is similar to the one Mathematica uses, although it's really using IPython. It's not just Mathematica though! Similar interfaces have existed for a while in Sage, Maxima, R via Knittr, Maple, etc... So it's about time we got one for Haskell :)
Argh! Was really eager to see that video at SkillsMatter (even signed up and everything!) only to be told "Because of its privacy settings, this video cannot be played here." :(
Exactly. :)
That was one awesome reply, thank you!
Seems like a good way to fund the critical Haskell community projects. Very interesting approach I haven't seen before. Would anyone in Haskell community be interested in setting up one or two Haskell related projects on snowdrift to see the level of funding that could be achieved this way? 
Nice article! You say: &gt; could we achieve this without Type Families and Data Kinds, using a regular GADT? The answer is no. However, couldn't you do something like this: data CrudableC d b where ProductC :: CrudableC ProductData Product OrderC :: CrudableC OrderData Order data CrudF next where Create :: CrudableC d b -&gt; d -&gt; (b -&gt; next) -&gt; CrudF next This basically "inlines" the type families in the GADT.
Agreed about almost everything, this should be carved into stone. When beginning to learn Haskell (especially if its your first FP language) you feel like entering a promised land. But after having basic type and category theory under your belt, you realize it's more like a cult. People are worshiping typeclasses to no end and willing to jump into fire for laziness... a reality check with PFPL would definitely go a long way for them. Haskell is an amazing language, but if we could just get to work and stop singing praises on monads, that'd be great. &gt; ...bug to conflate doing with being... Isn't "bug" a bit too strong a word? My understanding is that this is an ongoing research area and while it's becoming clear that having microscopic positive/negative types and customizable evaluation strategy will be nice, there's not a single language out there doing this today (I only know of Frank and I'm not sure what state that's in). Thus conflating doing with being seems like a perfectly reasonable thing to do right now.
Oh thanks I didn't even know that existed. 
Well, it obviously depends on the specific implementation, but you can read up on GHC's by searching for the terms "tagless spineless g-machine".
Nice technique. I'll have to play around with this when I get back tonight. Not sure if there are any limitations, with the exception of representing endpoints like `Products` where there is only a notion of `Read` and not the other verbs.
So the nice thing about Haskell is the whole compiler pipeline is transparent, you can look at all the intermediate forms all the way down to assembly. ghc -ddump-simpl -ddump-stg -ddump-cmm -ddump-asm compile.hs Output: https://gist.github.com/anonymous/f3ca9af198ed61b55bb4 &gt; However, this function also might be partially applied, meaning the runtime has to wrap everything into some sort of closure object, and include a reference to f in there too. It does, if you look at the Cmm there will be an object called ``f_closure`` that holds the entry code for the f function. If the function is partially applied then a PAP (Partial Application) object is created that itself holds a reference to the entry code for the ``f_closure`` object and invokes it when the PAP's arguments match the arity of the ``f``. If you call ``f`` directly with two arguments than GHC will usually use a fast call and jump directly into the body of the function with the two arguments in two register.
You might find my blog post [Understanding the RealWorld](http://www.well-typed.com/blog/95) helpful. I talk about how partially applied functions are represented and how they are called. 
There are some interesting readings listed at the bottom of http://lambda.jstolarek.com/2013/06/getting-friendly-with-stg/ From my memory of reading these papers, GHC actually creates three functions, on for each level of application, i.e. it actually uses different functions for when it sees `f` statically called with zero, one or two arguments.
This is so amazing! I'm just so excited to share it with everyone I know, and I can already think of a few people who will be happy to have a tool like this, especially for learning :) Just one note, the parser example doesn't seem to be working in my installation (screenshot http://i.imgur.com/SbCRIRf.png)
Mystery solved! There was a FIFO lying around called test as well. #NeverNameThingsTestEver
'arise during development' - definitely. For instance I was recently trying to compile some hackage package and was greeted by LLVM errors ..
I'm not entirely happy with this explanation, but I can't put my finger on why.
Because... &gt; Main Idea #1: Monoids, functors, applicatives, and monads are all different algebras Algebra has a technical term, and while monoids are algebras, the rest are not. &gt; Main Idea #2: Monoids, functors, applicatives, and monads are ideas about computation, and are not just specific to Haskell None of these ideas are about computation. The endomorphisms of an object in a category form a monoid regardless of whether those endomorphisms are computable functions (like in Haskell), arbitrary functions (like in traditional mathematics), or anything else you might make a category out of. &gt; Main Idea #3: Functors transform values inside containers, while applicatives and monads combine values inside containers It is true that container data structures in programming often form functors, that is just one particular way functors come about. It also makes it sound like applicatives and monads are not functors... which they are. &gt; Main Idea #5: You can put values inside a container but you can’t always take them out Again there's this word "container". Except it sounds like the author confused functors and monads. An arbitrary functor doesn't necessary need a map `a -&gt; f a`. &gt; Main Idea #6: Applicatives and monads offer an elegant way to manage side effects and state I don't think monads are especially elegant for managing these things. Monad transformers in particular are just terrible to work with. &gt; Main Idea #9: Monoids are a general algebra for combining things in a certain way This is a really generic statement. It would hold of any algebraic structure. 
Here they come
&gt; If I remember, it depends on whether the arity is known statically. Yes, absolutely. I'm talking about OP's *particular* `f` here, not making a general statement!
controlling the export of an orphan instance would only make things worse. This is like treating symptoms while ignoring the underlying cause.
&gt; PAP's arguments **math** the arity s/math/match/?
Could you explain ? AFAIK the main problem with Orphan instances, is that they "leak". For example , I might need `Monoid Int` or `Num (a -&gt; a)` etc ... and I don't want them to leak. Not exporting them would solve the problem, wouldn't it ? 
For any given typeclass and any given type, your program should contain at most one instance of that class for that type. Since an instance can only be defined when the type and the typeclass are both in scope, it is easy for the compiler to verify that an instance destined in the same module as the type or the typeclass does not clash with another instance (the module containing that other instance would not be able to avoid importing our original instance). Being able to prevent an instance from being exported would seem to make it harder for the compiler to do this (as you could now import the type and the typeclass without importing the instance, so how can it know you have conflicting instances?) How do you think being able to suppress instances from being exported would improve the situation? Why is it a problem that instances “leak”?
I'm partial to [exploding mini black holes](http://www.abc.net.au/science/articles/2009/10/28/2726708.htm) myself.
Nice post. One side issue though - I thought the era of non-GADT expression AST's was long past. Why litter your code with distracting "`throwError`" cruft for no reason?
Yes, fixed.
Thanks!
Oh, that's how I do merges anyway :-/.
The main problem the term "orphan instance" exists is, that (as others have pointed out already) you need to make sure instances are globally unique in a given program. Otherwise you can subvert internal invariants and break abstractions. As a simple example, consider if it was possible to have two different `Ord` instances for a given key type (let's say the two `Ord` instances are dual to each other). And you construct a `Data.Map.Map` with one instance, and then operate on the resulting `Map` while the other dual `Ord`-instance is in scope. One could argue, the problem in this case is that `Map`-construction does not capture the `Ord`-instance used at construction.
Yes, and the problem would be solved, by the user, hidding the one he doesn't want, and/or using it's own without exporting it. I still don't understand how controlling export makes things worse (whereas it seems to help).
it solves an inconvenience by introducing a serious unsoundness... i dont think the benefit justifies the cost 
[There's a Frank prototype available.](http://hackage.haskell.org/package/Frank)
The `sig` syntax is a little weird. Think of it as data Send x where Send :: x -&gt; [Send x] Unit In other words, it's generating a "constructor" of some sort called send which takes arguments in x but returns unit.
According to the HN jobs post, the chosen candidate would be replacing him.
the PDF comes from here: [github.com/takenobu-hs/haskell-ghc-illustrated](https://github.com/takenobu-hs/haskell-ghc-illustrated) but these is no source??? 
Great work! If I could ask one favor, it would be great to get this as a docker image based on top of the official haskell docker image, or some other base that FP Complete has blessed. If not an image, a documented setup, similarly to how the haskell docker image is supposed to be used for development. 
There's a ___lot___ of `IO`. 
Orphan instances are a misfeature and the general desire is to get rid of or restrict them altogether. Indeed we would prefer to have newtypes and deriving. cf http://blog.ezyang.com/2014/09/open-type-families-are-not-modular/
Haskell relies on instance coherence. You can't have two different modules linked into the same program, sharing `Map k` values and having different ideas of what `Ord k` is.
I don't want to solve problems myself, I want the compiler to help me. That is why I program in Haskell.
Hmm inbuilt effect handling and no mention of disciple in "Related work"? (Yes I know it's a draft)
If all you need is some data from a file, you can write a function `loadFakerData :: IO FakerData` which loads the file, and then the other functions can all have type `generateData :: FakerData -&gt; String` (where `generateData` is things like `name`, `author`, etc...) Note that since you do random generation, you may want `FakerData` to contain the `StdGen` for generating random values. A slightly more advanced but probably much cleaner solution is to use a `State` monad. (If you don't need the random number generator to be updated, this can be a `Reader`) You can also use a newtype over a `State` monad: newtype Faker a = State FakerData a with all your functions being in this monad name :: Faker String author :: Faker String apartmentNumber :: Faker Int ... Then, you provide a `runFaker` function of type runFaker :: Faker a -&gt; IO a runFaker faker = do myData &lt;- loadMyData stdGen &lt;- randomGen -- however you want to make it return $ runState (FakerData myData stdGen ...) faker which loads data, creates a random number generator, and then creates the fake data. This would allow the user to write code like this: main = putStrLn $ runFaker $ do n &lt;- name apt &lt;- apartment c &lt;- country return $ unwords ["Hello, I am", name, "from", c, "at address", apt] 
Awesome! Thank you, I'll try this tomorrow. It is really what I looked for, because now every time when I generate some fake data – it loads file, parses it and only then provide me what I want – not a good way for case you described in the end
[`MonadRandom`](http://hackage.haskell.org/package/MonadRandom-0.3.0.1) for randomness and `ReaderT` for configuration could be a possible solution: ``` type FakerT g m a = RandT g (ReaderT FakerData m) a ```
Thank you for all advices and kind words! I'll try to get rid of IO where it is not needed. I wrote Giml for this library because I really wanted to start making faker and it was too hard for me to understand YAML or JSON parsing for that time and libraries for them were too heavy – many types, many functions, many monads and many dependencies). So I've decided to make simple small language for large amount of simple data, which can be parsed without heavy dependencies and provide clear and simple data. Here is the manifest, and links to parsers at the end of file https://github.com/gazay/giml
What do you mean every monad transformer stack is not a monad? Compositions of monads are not necessarily monads. But transformer stacks over a base monad all tend to be monads. If you look in e.g. the `transfomers` library, you will see that e.g. `MaybeT` over any monad `m` is also given a monad instance, etc.
also Giml shows pretty good speed in comparing with YAML and TOML parsers in Ruby (but not JSON). Don't know how to benchmark in Haskell yet :)
Try [Criterion](https://hackage.haskell.org/package/criterion) for benchmarking in Haskell.
Very neat! Parsers are great initial projects. Haskell parsers tend to be written in either Parsec (good error messages) or Attoparsec (great speed) and operate atop the binary string types for Haskell, ByteString (binary data, no linguistic interpretation) or Text (fast human-language strings). Since you've already written a parser by hand, looking into how to do it with something like Parsec/Attoparsec can be very educational. Further, getting away from the `String` type will probably increase speed dramatically.
How do orphan instances help, then?
The problem is that once you define such an instance, if you can hide it, now I need to know an awful lot about the provenance of the instances used at every step along the way. Consider `Ord`. You make up a local `Ord` instance that sorts one way and "hide" it. Now you have a problem with every `Set` that leaks from your hidden scope out into the outside world whereupon it is used with another instance. You put things in with one order and I took them out with another.
I actually can't think of any shit libraries I have to use that export problematic orphan instances.
Oh! That's good to know. I kind of had the impression Frank was on an indefinite backburner. 
He's the one who posted this link here. Indeed, he's been the lead programmer and co-founder. He's as responsible for the whole project as anyone. He's taking on a full-time day job now and will continue to do development on the site in his free time. He will also use some of his income to help pay for the new lead programmer to take his place. Basically, he just doesn't want the progress to slow or to rely on him when he has limited time, so he wants to find a full-time replacement while he continues to help where he can.
Right, and he's not leaving either, just won't have full-time anymore.
Hey, let me see if I can help a bit. I come from a C#/JavaScript background and did 6 months in Scala (mostly imperative/OO style with some monads in there not by that name) and just read LYAHFGG. I think the easiest way to get at it from that background is starting with map and list. So we start with some data that is in a container (or "context"). For a value in a list, the container is that list and the other stuff in it. `map` lets you apply a function to each value in the container at once (most langs call it `map` but C#'s LINQ calls it `Select`). So say you have a list of `[1, 2, 3]` called `nums` and a function that takes an Int and doubles it called `doubler`. Now: `map doubler nums` gives you `[2, 4, 6]`. That works well and in Haskell one would say a list was a functor (and you could use the general fmap function instead of the specific one for lists called map but they do the same thing). There's also the empty case which doesn't use the function. `map doubler []` gives `[]`. It makes sense in that there's no value inside the container to map over. Now, what if instead of a function that takes an Int and returns another Int, you had a function which took in an Int and returned a list instead? The example from LYAHFGG gives back the positive and negative versions which we could call `posNeg`. So `posNeg 3` returns `[3, -3]`. Now if we map over our nums `map posNeg nums` we get `[[1, -1], [2, -2], [3, -3]]`. That works and doesn't have any errors but what if we didn't want any of those sublists in there? It is an extra layer of containers/contexts. We could flatten the list after (Haskell calls this `concat`). So `concat (map posNeg nums)` gives us `[1, -1, 2, -2, 3, -3]`. But there is a function to do that for us. In C# calls this `SelectMany` and Scala calls it `FlatMap`. For lists Haskell has `concatMap`. So `concatMap posNeg nums` also returns `[1, -1, 2, -2, 3, -3]`. It turns out `concatMap` is the monadic bind (`&gt;&gt;=`) for lists. Since often you'd chain a bunch of stuff with monads, the ordering of the params is different but otherwise it does the same thing. So `[1, 2, 3] &gt;&gt;= posNeg` also gives `[1, -1, 2, -2, 3, -3]`. Now we make `rep` a function to repeat a num its own num of times: `let rep x = replicate x x`. So `rep 3` gives `[3 3 3]`. Then `nums &gt;&gt;= rep` returns `[1, 2, 2, 3, 3, 3]`. Now we can chain them: `nums &gt;&gt;= rep &gt;&gt;= posNeg` to get [1, -1, 2, -2, 2, -2, 3, -3, 3, -3, 3, -3]`. Each step takes something from the container, applies the function which gives back a new container, and flattens it all out. Now, the Maybe type is another big example. C# has the nullable type. So you could have `int?` that is either a number or null and has `.HasValue` and `.Value` properties. So if you have a function (say `doubler` again) that works with an int. If you have a nullable int and want to double it if possible, you'll check if it isn't null and if so double it. Otherwise you'll return null yourself. In Scala and Haskell to make this easier, you'd map. Haskell has the Maybe type which is similar to nullable. For ints it could be Nothing, Just 1, Just 5, etc. In Haskell `map` is specific to lists but `fmap` works for other things as well (functor). Now, `fmap doubler (Just 5)` is `Just 10`. So in this case the container or "context" is the maybe-ness of the value. When mapping over it, we take the value out, pass it to the function and put it back in. Again there is the "empty" case with no value per se: `fmap doubler Nothing` is `Nothing`. In a way Maybe is like a list that is either empty or has a single value. To progress to the next step again, say we have a `safeFraction` function. It divides 1 by another number and gives a Maybe num back. If you pass 0 (divide 1 by 0) it gives back Nothing but otherwise it does the divide. So `safeFraction 1` is `Just 1.0` and `safeFraction 0` is `Nothing`. Now what if we want to map that function over an existing Maybe? `fmap safeFraction (Just 1)` gives us `Just (Just 1.0)`. Now we've run into that problem again of two many layers of containers or "contexts". `(Just 1) &gt;&gt;= safeFraction` gives `Just 1.0`. And `Nothing &gt;&gt;= safeFraction` gives us `Nothing`. You can keep chaining as much as you want: `Just 2 &gt;&gt;= safeFraction &gt;&gt;= safeFraction` is `Just 2.0` since that's the same as `(1 / (1 / 2)`. IO in Haskell is monads in that the container/context is the fact that it's going to go out and do something to get the return value. In C# this would be a Task and in JS would usually be called a promise. It isn't a value yet but a thing saying "hey a value will come eventually from the outside world". In those other languages you tend to use the concept more for async while Haskell uses it for any interaction in the outside world (though due to laziness it probably won't happen immediately in Haskell anyway). So if we have a function `makeUpper` that takes a string and makes it uppercase. We can call it on a String but not `getLine` because `getLine` returns IO String (container that will eventually have a string in it). As with lists and Maybe, we can map over it. `fmap makeUpper getLine` from the repl brings up prompt and if you type `foo` it prints `"FOO"`. But if you `:t fmap makeUpper getLine` that shows the result is actually still IO [Char] ([Char] is the same as String). Ok so we can map over IO. Now what if we have a function called `appendInput` which takes a string, asks the user for more text and returns the user input appended to it? You could define it like `let appendInput x = fmap (x ++) getLine`. Now `appendInput "foo"` prompts the user and if they type `bar` it gives back an IO String of `"foobar"`. So far so good but now what if we want to get user input and then append to it from user input again? If we try `fmap appendInput getLine` that ends up giving an `IO (IO [Char])`. From the repl it doesn't even ask for the second input yet. But instead we can do `getLine &gt;&gt;= appendInput` which asks for input twice and if we type `one` and then `two` we get `IO [Char]` of `"onetwo"`. So to recap functors let you map over stuff inside some kind of context with functions that know nothing of that context. It takes the value out and changes it and puts the result back in. But if we have a function that adds additional context itself (for Maybe decides whether the output is Maybe or Nothing, for list returns its own list of multiple values, for IO does its own IO) we can use Monad's bind `&gt;&gt;=` to chain them together without creating additional layers of context every time. Hope this helps.. I know it's long but helped to make things more concrete in my mind as well. Edit: Also I'd urge you to read LYAHFGG from the beginning if you haven't already. I like the way it goes step by step building constructs itself and then showing how they already exist in Haskell. Trying to jump in to the Monads chapter directly would be futile I think. Having said that, one thing it definitely doesn't do is show you a large program. I'd think trying to make a web app api or some other project would help a lot to get real world experience. I was able to get used to the idea of using map and flatMap over the Option type in Scala without knowing what functors or monads were.
Right. Some time back I tried the "full time job with Snowdrift on the side" thing, and it wasn't very good for either Snowdrift or my stress level. So this time I plan to pass off the major responsibilities and get myself out of the loop for development progress. I still plan on being involved with Snowdrift generally, will certainly be available when the new lead has any questions, and may pick up a ticket here and there.
...okay, that took me a minute. I blame growing up on the Pacific coast. Very nice.
Hm, I thought monad transformer stacks *are* compositions of monads? The definitions in the `transformers` library sure look like composition: m &gt;&gt;= k = ErrorT $ do a &lt;- runErrorT m case a of Left l -&gt; return (Left l) Right r -&gt; runErrorT (k r) Yes, of course they *tend to be* monads, but there are no rigorous guarantees for an arbitrary stack. [Counter-examples exist.](http://stackoverflow.com/questions/13034229/concrete-example-showing-that-monads-are-not-closed-under-composition-with-proo) Whereas for applicative functors, the guarantee is there, and composition of applicatives is always an applicative. That's why I wondered where that fine line between being and not being closed under composition may be drawn.
I guess this means that the answer to my question is "There is no easy solution and everybody has stopped trying to find one". Am I right ?
Ha, McBride never disappoints. "Do be do be do"/Frank was a jaw-dropper, too.
I adore the use of **infers**, **checks**, **does**, and **matches**, rather than [a random assortment of these](http://unicode-table.com/en/sets/arrows-symbols/).
I've been working on languages which support recontextualization of some sorts of effectful process for over twenty years. In the mid 00s, after the idiom bracket clue, I began wondering how to treat the identity monad as not deserving of special treatment (and other monads as not deserving of a gratuitously imperative-style notation; I used to program in ML, you know). Also, around about then, I saw a lot of Paul Blain Levy's tutorials on CBPV, and his "a value is; a computation does" slogan reminded me of the joke whose punchline is "Do Be Do Be Do", the authorship of which Kurt Vonnegut so vigorously denies. Early models happened in Agda, around about 2005, leading to a spurious directory called "sinatra" in the darcs repo. I first proposed some typing rules in a talk in Tallinn in December 2007. I got the chance to hack something up one amazing fortnight in Paris (was it 2012? it seems like a lifetime ago) when I was visiting INRIA and pretending I couldn't access the internet. That's the thing in the repo that runs, with a bunch of examples including Shivers/Turon rollback parsing. Sam (who's been into effects-and-handlers since Matija Pretnar became Plotkin's student) and I have been batting it about a bit, trying to get it the way we want it. There are quite a few things we're still scratching our heads about, and some fairly radical changes we have in mind since the previous implementation. E.g., I no longer see any need to distinguish between the notion of "handler" and the (not necessarily pure) notion of "function": they're both forms of contextualization, and that's the general idea I want to capture. Most of 2014 was wiped out for me by circumstances beyond my control. I'm trying to get back to working on things I care about, and Frank is certainly part of that picture. I've been blogging a bit in the last few days, trying to scribble down where I'm at on a whole bunch of fronts. Hopefully, I'll be able to set out more of the prospectus as part of that process.
Super exciting to hear! Sorry, I didn't mean to imply that I thought you were out of the picture there. I just really started looking into this area of research during 2014 so I think from my limited perspective it looked like a dead zone. I've been following your recent blogging and have found it really intriguing—as almost always is the case with your work. I'm excited to see where Frank goes.
&gt; I've been blogging a bit in the last few days I'd be quite curious to read that. Where do you blog? I couldn't find anything new on the strictlypositive.org homepage.
I've had a dormant blog [here](http://pigworker.wordpress.com/) for a while, but I got busy a few days ago, thanks to a bit of time at home. I'm a slow updater of web pages, so referrals have largely been via twitter thus far.
I presume this will become future release notes - https://git.haskell.org/ghc.git/blob/refs/heads/ghc-7.10:/docs/users_guide/7.10.1-notes.xml surely incomplete but still: Language: - There is a new extension, StaticPointers, which allows you to create pointers to expressions which remain valid across processes. This is useful for referencing higher-order values in distributed systems. The pointers are created with a new keyword static as in x = static ("abc" ++ "123") :: StaticPtr String. All processes which dereference x get the same result, that is, the body of the static form. - Added support for binary integer literals - Simplified rules for implicit quantification. In previous versions of GHC, it was possible to use the =&gt; arrow to quantify over type variables in data and type declarations without a forall quantifier. For example, data Fun = Fun (Ord a =&gt; a -&gt; b) was identical to data Fun = Fun (forall a b. Ord a =&gt; a -&gt; b), while data Fun = Fun (a -&gt; b) caused a not-in-scope error. This implicit quantification is now deprecated, and variables in higher-rank constructors should be quantified with forall regardless of whether a class context is present or not. GHC 7.10 raises a warning (controlled by -fwarn-context-quantification, enabled by default) and GHC 7.12 will raise an error. See examples in GHC documentation. The change also applies to Template Haskell splices such as [t|Ord a =&gt; a|], which should be written as [t|forall a. Ord a =&gt; a|] - Instance contexts inferred while processing deriving directives attached to data and newtype declarations now forbid equality constraints. This is a regression in obscure cases, but it will yield better error messages in more common cases. Users caught by the regression can simply use standalone deriving, where you specify the context yourself. Compiler: - GHC now checks that all the language extensions required for the inferred type signatures are explicitly enabled. This means that if any of the type signatures inferred in your program requires some language extension you will need to enable it. The motivation is that adding a missing type signature inferred by GHC should yield a program that typechecks. Previously this was not the case. This is a breaking change. Code that used to compile in the past might fail with an error message requiring some particular language extension (most likely -XTypeFamilies, -XGADTs or -XFlexibleContexts) - -fwarn-tabs warning flag is turned on by default with this release of GHC. It can be suppressed either by using GHC_OPTIONS pragma or by specifying -fno-warn-tabs flag. - A new warning flag, -fwarn-trustworthy-safe has been added and is turned on with -Wall. It warns when a module that is compiled with -XTrustworthy is actually infered as an -XSafe module. This lets the module author know that they can tighten their Safe Haskell bounds if desired. - The -fwarn-safe and -fwarn-unsafe that warn if a module was infered as Safe or Unsafe have been improved to work with all Safe Haskell module types. Previously, they only worked for unmarked modules where the compiler was infering the modules Safe Haskell type. They now work even for modules marked as -XTrustworthy or -XUnsafe. This is useful either to have GHC check your assumptions, or to generate a list of reasons easily why a module is regarded as Unsafe. For many use cases, the new -fwarn-trustworthy-safe flag is better suited than either of these two. - -ddump-simpl-phases and -ddump-core-pipeline flags have been removed - Many more options have learned to respect the -ddump-to-file. For example you can use -ddump-to-file with -ddump-splices to produce a .dump-splices file for each file that uses Template Haskell. This should be much easier to understand on a larger project than having everything being dumped to stdout. - Compiler plugins (with the -fplugin flag) may now modify the behaviour of the constraint solver, to add new functionality to GHC's typechecker. - A new warning flag, -fwarn-missing-exported-sigs has been added. The behavior is similar to -fwarn-missing-signatures but GHC will only flag exported values. This flag takes precedence over -fwarn-missing-signatures so it can be used in conjunction with -Wall. Template Haskell: - Added support for generating LINE pragma declarations - The type Pred (which stores a type constraint) is now a synonym for Type, in order to work with the ConstraintKinds extension. This is a breaking change and may require some rewriting of Template Haskell code. - Pattern splices now work. - reifyInstances now treats unbound type variables as univerally quantified, allowing lookup of, say, the instance for Eq [a]. - More kind annotations appear in reified types, in order to disambiguate types that would otherwise be ambiguous in the presence of PolyKinds. In particular, all reified TyVarBndrs are now KindedTVs. (This does not affect Template Haskell quotations, just calls to reify.) - Various features unsupported in quotations were previously silently ignored. These now cause errors. - Lift instances were added for many more types: all of the IntXX and WordXX types, Ratio a, (), Float, and Double. - All Template Haskell datatypes now have Generic and Ord instances. - Ppr instances were added for Lit and Loc - Two new declaration forms are now supported: standalone-deriving declarations and generic method signatures (written using default in a class). This means an expansion to the Dec type. - Template Haskell is now more pedantic about splicing in bogus variable names, like those containing whitespace. If you use bogus names in your Template Haskell code, this may break your program. I'm sick of copypasting and cleaning further, and they're getting even more pedantic furhter on - linker, build, pkg system, libraries ...
See, for example, Neil Mitchell's tweet earlier today about criterion now [failing to build on Hackage](https://twitter.com/ndm_haskell/status/551790981507719168?s=09)
Have you looked at Ben's PhD thesis? Not sure if it has what you are looking for but I'm sure it has more than code samples / feature lists. :) Am on phone but it is linked from DDC Haskell wiki page.
Thank you for the hint!
Thank you, I think that this is what I had hoped to find. :)
All I see there is some functional shortcircuiting using haskells weird switch(params) like function definition shorthands.
Hopefully nothing bad happens near the 2.0 release, as I fear people in the US might not take it seriously...
&gt;Very neat! Parsers are great initial projects. Or final projects, depending on your preference *please don't hurt me*
Sugoi desu ne!!! Haskell is one step closer to being a mainstream development environment because of this.
This is indeed the intention! Before we see general industry acceptance we will need the tooling to suite. 
Awesome. How integrated with git is this? Any chance of a mercurial version?
It's certainly possible to detect the revision control system. The interface that we would have to generalize for that would be: 1. Set up the hook and detect it is set. 2. Find all files in the repository. 3. Find staged-for-commit files in the repository. Factoring this out would provide an agnostic system. We don't use mecurial, but if you'd like some support adding this please open a ticket and we can spec out some types for you.
This is intended to be run client-side. GitHub does not allow push hooks to be run (due to you needing to run code on the server). You could always set this up as a web based hook with a bunch of plumbing if you had the inclination. The intention is to use it as a client-side, instant feedback, though.
FYI this is still not fixed.
What can we do to make Hackage more reliable?
From the point of view of the blog post -- it distracts from the main message as lots of people would not be familiar with the GADT syntax. From the point of view of the actual compiler I'm working on -- GADTs for toy expression languages are fine, but for an actual expression language with binders, polymorphism, etc. -- not at all easy. 
Does this mean there's no hope for Hackage's novel approach, and we should just go back to using the old uninteresting but proven pinned-version-distribution scheme?
&gt; Does this mean there's no hope for Hackage's novel approach No need to give up hope (yet) as it's being worked on, see [How we might abolish Cabal Hell, part 1](http://www.well-typed.com/blog/99/)
This is a temporary glitch introduced by yesterday's Hackage server deployment, see [hackage-server#305](https://github.com/haskell/hackage-server/issues/305) for details. It'll hopefully be fixed asap. It's a bit frustrating to me to see that tweet after I've spent quite a bit of time [hacking up some tooling](https://github.com/hvr/hackage-matrix-builder) (which will eventually be integrated with/into `hackage-server` at some point) and using that to restrict incorrectly specified version-constraints over the last couple of days, to make sure *all* combinations of ghc 7.x versions crossed by *all* package versions result in valid install-plans for several popular packages low in the dependency-graph... So without that glitch, that tweet would have never happened...
...and I'd like to add, that it's still very important to specify proper version bounds. I've been spending most time on tracking down and fixing up packages whose author's omission of bounds caused indirect build-failures further down the dependency graph for *several* other packages. :-/ Fixing up missing upper version-bounds retroactively for 50 versions of one package is quite tedious... it's significantly less work to use major-version upper bounds, and relax them for a few recent point-releases on demand. I guess those bounds are also useful to guide Stackage or other distributions that create vetted version-cross-sections (i.e. an element of the cartesian product of the package-versions) of Hackage. As otherwise they'd have a *much* larger configuration space where most cross-sections would be in fact invalid install-plans.
&gt; You could always set this up as a web based hook with a bunch of plumbing if you had the inclination Last time I checked, GitHub only supports post-push web-based hooks... but no hooks that can reject a push operation
How does all this work compare to [A dissection of L](http://assert-false.net/arnaud/papers/A%20dissection%20of%20L.pdf) (PDF)?
&gt; I guess those bounds are also useful to guide Stackage or other distributions that create vetted version-cross-sections (i.e. an element of the cartesian product of the package-versions) of Hackage. As otherwise they'd have a much larger configuration space where most cross-sections would be in fact invalid install-plans. In the case of Stackage we take the latest versions available of packages, pulled from Hackage, and see if they build and pass tests. Pre-emptive upper bounds usually prevent this process from going ahead, so we have to ping the authors to [bump their upper bounds](https://github.com/fpco/stackage/issues/291), who will either not get back to us and hold back the progress (we might [open a PR](https://github.com/batterseapower/parallel-io/pull/9) for it), or can [fix their package](https://github.com/tibbe/hashable/commit/f27312bd16b303cccf5ff8f95bcd1a3ed64cd107). Finally, once a build plan can actually be produced, we can do a complete build and run unit tests to see if everything *really* works. In the opposite case, with no upper bounds the package will either build and pass tests or it won't, if it doesn't we [add an upper bound to the dependency](https://github.com/fpco/stackage/blob/master/build-constraints.yaml#L648-L649) in Stackage and notify the authors to fix their package. Most often upper bounds are just pre-emptive ones which prevent otherwise successful builds and passing tests. So while they aid your work, they are the thing we spend most of our time on with daily Stackage maintenance.
Wonderful! I've been toying with writing an ML-like compiler in Haskell, using 'modern' principles (especially w.r.t. writing type-safe and type-preserving compiler passes, cfr http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.1175 and other related literature), but never got too far... Looking forward to reading all of this!
really nice! but I would have one wish: now I have can have [http://www.stackage.org/snapshot/*lts-***1.0**/cabal.config?global=true](http://www.stackage.org/snapshot/lts-1.0/cabal.config?global=true) which points me to towards the LTS release 1.0 (for use in global config) could I also have a relative link towards the latest and greatest LTS release at the time? [http://www.stackage.org/snapshot/**lts**/cabal.config?global=true](http://www.stackage.org/snapshot/lts-1.0/cabal.config?global=true) doesn't seem to work thank you! 
Sure, here you go! http://www.stackage.org/lts/cabal.config?global=true Yes, the URL structure for this is a little bit confusing, since there are essentially two ways of getting to a snapshot: * Through it's globally unique slug, via the /snapshot/ URL. * Via its LTS or Nightly designation, via the /lts/ and /nightly/ URLs. There's also a request to provide a command-line utility to automate some of this stuff, which you can [see on Github](https://github.com/fpco/stackage/issues/396).
Thank you for taking the time to answer so thoroughly. This is extremely helpful. 
again, than you
We need to be precise here, because there are three sets of packages under discussion: Set #1: The sets of packages on Hackage, that can specify version ranges Set #2: The unstable sets of packages on Stackage, which have their versions pinned but they are updated frequently Set #3: The stable set of packages on Stack (i.e. the LTS release), which have their versions pinned but they are updated infrequently. Set #3 is analogous to the Debian stable set of packages: it will work very reliably, but if you need a more recent version of a package you're out of luck until the next stable release. Set #2 is analogous to the Debian testing set of packages: it is less reliable, but you get a reasonably current view of Hackage. The disadvantage is that it's difficult to test an **older** version of a package. Set #1 is analogous to programming without a package distribution. It gives you much more flexibility to test older or newer versions in different combinations Generally, application writers like sets #2 and #3 when they first start out because they just want things to work without testing. However, as time goes on and they find they need unusual versions of certain packages they begin to wish that set #1 works. A lot of us are just asking that people don't stop adding upper version bounds because when you need set #1 **you really need it**.
You were going to give a talk about why Haskell's approach to type classes is an important point in the design space (and one which you prefer) and what the alternatives sacrifice, but it seems like that never happened... Is there a single post/thread/video anywhere that makes your argument? I've only been able to find scattered bits and pieces.
We, as a community, really really hope you'll have the best of success in getting past whatever may be troubling you. We love your work, and we're eager to see much much more. Best of luck!
I legitimately have no clue how Conor actually does peer reviewed papers. Aren't many paper reviews for things like conferences done blind, i.e. without the knowledge of who the author was during the review? I feel like I could spot Conor and his work from 10 miles away with my eyes closed.
&gt; 2. The dependency solver selected an invalid plan, and the build failed I'm mostly concerned fixing those on Hackage by `.cabal` edits. As those are ones I can actually fix in good faith by examining the compile error. Relaxing bounds otoh is something the author needs to decide IMO whether it's safe to do. 
Yay, C optimizations work properly.
You're right, it's funny. But the author isn't critical of the metaphors. The author is saying that when Brent was critical of the metaphors, the problem wasn't really the metaphors themselves, but the whole idea that you need to understand what the metaphors are trying to explain. Furthermore, what the author is explaining in the post *is* very worthy of understanding, so using metaphors for that is perfectly OK. &lt;/straight-man&gt;
Do you use the haskell distribution packages from Debian? If you even need anything packaged do not hesitate to send an RFP bug to debian-haskell@lists.debian.org . I would like the package sets to be a bit more complete for production use. #3 is very important to me for distributing software but I usually work within #1. I use Debian Stable in production and Continuous integration but it's not always possible to use the distribution packages, as you mention. It is still very important to me that the packages are all tested and vetted the best they can. I use Stackage in various places as well but I primarily use Hackage for development and I almost never have issues working on projects by myself. When I am working with a team, however, more version issues inevitably crop up. I can't always expect my team members to just patch these issues themselves and it is often a waste of time. 
Fantastic work. I very much look forward to reading this.
Where is nix in all this? I get the impression it builds on top of cabal but does the dependency resolutions itself instead of letting Cabal do it. Can nix co-exist in a world where Stackage becomes the dominant stable Haskell package repo?
I think that practice of peer review occurs more predominantly in other fields? It's certainly employed at least some of the time.
Hmm, indeed, the [Wikipedia article on the subject](http://en.wikipedia.org/wiki/Peer_review#Different_styles_of_review) explicitly says so: &gt; In "double-blind" review,[35] which is more common in the humanities than in the hard sciences, the identity of the authors is concealed from the reviewers, and vice versa, lest the knowledge of authorship or concern about disapprobation from the author bias their review. And the sentence after that echoes /u/aseipp's concerns: &gt; Critics of the double-blind review process point out that, despite any editorial effort to ensure anonymity, the process often fails to do so, since certain approaches, methods, writing styles, notations, etc., point to a certain group of people in a research stream, and even to a particular person.
&gt; Can nix co-exist in a world where Stackage becomes the dominant stable Haskell package repo? Probably, Stackage builds off of Hackage and uses Cabal configurations to enforce version restrictions http://www.stackage.org/lts/cabal.config (in the Inclusive build, at least). I don't think this issue is a zero-sum choice. Of course it would be nice to have one tool fix all of these issues but several people have already pointed out why each component has a use-case: http://www.reddit.com/r/haskell/comments/2re3pw/neil_mitchell_on_twitter_tried_installing/cnf0ob8 http://www.reddit.com/r/haskell/comments/2re3pw/neil_mitchell_on_twitter_tried_installing/cnf5974 
Thanks! Looking forward to the talk.
There's also [indentation](http://hackage.haskell.org/package/indentation).
According to the acid-state website, throwing an error should abort the transaction. Are saying that it doesn't and that the event is made durable despite the error? http://acid-state.seize.it/Error%20Scenarios 
I tend to think this would be well-served by drawing from some of the machinery from QuickCheck. In particular, I'd like you'd expose a bunch of `Gen a` values, and maybe some full `Arbitrary` instances.
I use the Haskell Platform provided with the distribution, but for everything downstream of the Haskell Platform I install from Hackage using `cabal`. The reason I do things this way is that I want to make sure all my packages build against a well-tested release of the Haskell Platform. I'll make sure to post any issues that arise, but overall I haven't had many issues. I may try using distribution packages for things outside the Haskell Platform and see how well that works.
You're probably right. :)
[No](http://validator.w3.org/feed/check.cgi?url=http://dev.stephendiehl.com/fun/). But maybe if we nag hard enough...
&gt; It also makes it sound like applicatives and monads are not functors... which they are. Well, in a math sense a monad a tuple, of which one element is a functor... It may or many not be confusing to say "a monad is a functor" - I think a lot of confusion winds up arising from the fact that we identify a type (or type constructor) with "properties it has when we pair it with other things."
Can you prove [`TRMonad X a`](http://www.reddit.com/r/haskell/comments/2r9dnx/monoids_functors_applicatives_and_monads_10_main/cnffdzc) implies `Monad a` for any particular `X`? Or better, classify which `X` have that property.
Replace it with microservices. It does way too much.
&gt; respect the needs of other package distributions and try to cooperate with them instead of competing. Wut? How does a) bumping upper bounds, and b) adding upper bounds not "respect the needs of other package distributions" etc..?
Why can't the upper bounds in #1 be specified as a cabal.config file? The problem is that the upper bounds are bundled with a source code repository, and is not a separate piece of metadata. That is why it keeps the Stackage developers busy sending all of those PRs, emailing maintainers etc. The upper bound is a piece of mutable state that is stored in an immutable data structure (the source tarball). It is simply wrong as the data is not a function of the source tarball, but a function of the (mutable) set of dependencies it has. Thus systems such as Stackage that can produce cabal.config files is a step in the right direction, making this mutable data be calculated and updated globally. Granted, the cabal.config file that Stackage *actually* produces pins the versions, but I don't think there is anything inherent in the Stackage ecosystem that makes it impossible to just export the upper bounds as a cabal.config file. Why exactly do you want it in the cabal files? 
I suggest having a look at [bitcoinj](http://bitcoinj.github.io/). That one is probably better reference material. My java code on the other hand is a result of several sleepless nights before the final deadline ... so I wouldn't vouch for anything there ;)
The initial cabal file generated by `cabal init` infers a list of `build-depends` of the form "`pkgfoo == x.y.*`" where `x.y` is the major version of the currently "default available" package version of `pkgfoo` in the pkg database (as queriable via `cabal list --installed`) And the reason for defaulting to such a major-version-range is based on the [Package versioning policy](https://www.haskell.org/haskellwiki/Package_versioning_policy)
just only posted already http://www.reddit.com/r/haskell/comments/2regln/write_you_a_haskell/
You're correct, you couldn't synchronously reject commits. You'd have to incorporate into some kind of CI.
just reading this now. *whoa* &gt; This article considers whether ’tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a C of troubles
Neat, I am especially interested in the front-end aspects (type-checking especially) since SPJ's book pretty well covers the back-end translation from a functional Core language to [name your low-level backend]. Can you compare your approaches to the backend with his book? It looks like you are focusing on the STG-machine.
May want to wait for: https://github.com/anchor/git-vogue/issues/59
&gt; Incidentally, do you know if choice of functor always uniquely determines bind and return? It does not. A given Functor instance can give rise to multiple Monad instances. For some simple examples, consider a version of Writer which flips the order of concatenation or the [reverse state monad](http://lukepalmer.wordpress.com/2008/08/10/mindfuck-the-reverse-state-monad/). (Of course, Haskell restricts you to defining a single instance for a given type, so you would have to use a newtype to realize the alternative instances.) However, bind and return must still be related to fmap by fmap f xs = xs &gt;&gt;= return . f
Better fit for /r/programmerhumor but yes... Dat feel.
Yeah, I feel like the way we usually talk about these things works fine as communication between people who grok them, but can prevent understanding in people who've not yet ~~acheived enlightment~~ noticed the subtlety.
I was writing some Idris for fun just now and was very upset at the lack of double-semicolon, which meant definitions like: myfunc :: Show a =&gt; Int -&gt; Int don't quite align: myfunc : Show a =&gt; Int -&gt; Int a travesty.
I know I could Google this but I'd probably get some over complicated weird-word paragraph; what's Idris?
Uniqueness and existence are fairly sophisticated notions. It's hard enough for most interesting things to show it can be done at all (that is, showing existence). You have to be very comfortable with the notion in the first place to appreciate uniqueness. Of course, once you know something is unique, you are very happy, because you know you can pattern-match on it!
Here ya go: https://hackage.haskell.org/package/idris &gt; Idris is a general purpose language with full dependent types.
To clarify, this transaction will not be serialized: updateEx = put (error "error!") But this one will: updateEx2 = put (MyDataConstructor (error "nested error!"))
In cases like this I'd just unindent the arrows 1 space: myfunc : Show a =&gt; Int -&gt; Int Although auto-formatters don't tend to know that I like the right edge (of the arrows) to line up instead of the left.
Oh man this paper is awesome, thanks for the link! I wish there was an Haskell implementation of this :) Is there?
The [OCaml](http://gallium.inria.fr/~fpottier/inferno/inferno.tar.gz.) implementation is really clean and well-documented, so I guess doing a Haskell port would not be too much work. You would need to add monadic layers here and there though, it uses transactional state.
I think you should use ST for the union-find structure so that the final function (that maps a ML term to a System-F term) is as pure as possible. There may be a name generation component that escapes, but that is independent. One issue you may have is that the code uses ML functors to abstract over the details of the language and type system (the constraints/elaboration bit is meant to be reusable). It may work with type-classes instead, but if you want to use the technique for one fixed language you can also specialize the code directly.
`M-x align-regexp` makes it happen every time.
Doesn't it mean that cabal tried to install the dependency but failed (upper in the logs), then it continues and indeed fail to install the requested package ? This wouldn't be a failure of cabal or hackage, just that someone uploaded a broken package (the dependency, not the requested package). I see that behavior from time to time.
Some CS conferences are experimenting with double-blind reviewing these days, though the majority seem to still use the reviewer anonymous but authors known combination (the worst combination). As a PhD student I lived with a geophysicist and apparently in that field all reviews and authors have their names attached, which stops the proliferation of crap reviews.
No, I've seen that before, but this was different. Reinstalling just the package that failed (either), it complained it couldn't find stm-2.4.3, and it didn't seem to have tried to install that package. All in all, very weird.
POPL [uses double-blind review](http://popl.mpi-sws.org/PrinciplesofPOPL.pdf), not sure about ICFP.
I'd distinguish two things: *package set* and *delivery mechanism*. Stackage Nightly and LTS Haskell each provides a series of package sets. stackage.org is one way of delivering those package sets, via a set of cabal.config constraints (or via a custom remote-repo). But there's no reason why stackage.org should be the *only* delivery mechanism. Just like Debian and Fedora both ship haskell-platform packages, they could also provide lts-haskell or stackage-nightly releases. Those would just be alternate delivery mechanisms. Similarly, I've discussed with Duncan many times over the past few months the idea of having a "stackage" or "lts-haskell" tag on Hackage, and having that logic built into cabal. I've reached out to Linux distro maintainers before, and I still feel the same way: it would be great if we could all converge on a consistent set of packages, instead of each distribution coming up with its own individual and incompatible set of packages.
for non-strict fields only?
Attaching the arrow to the result rather than the parameter seems really weird to me. The contravariant parameters are much more worthy of emphasis. Not to mention name-based alignments are problematic at renames, and even if renames are handled correctly, they cause spurious conflicts when collaborating. So prefer: myFunc :: Show a =&gt; Int -&gt; Int Unfortunately, this seems incompatible with haddock :(
That was kind of where I was thinking you were going. It'd never be an internal mechanism to write `Branching m =&gt; (a -&gt; m b) -&gt; (m a -&gt; m b)` because you need to constrain `a` somehow... but meta-Haskelly it ought to be possible. Something like `Binary` ought to be sufficient?
I'm currently using myFunc :: Show a =&gt; Int -&gt; Int Which aligns nicely regardless of name length, plus it always lines up the arrows nicely :)
`Session` was a bad choice, imo - and doesn't really add a huge amount to the system. The idea of `Session` is to accumulate a value over the life of a `Wire`, *regardless of whether or not it inhibits*. That means if a wire inhibits and then resumes, it will suddenly see a state change from the point when it began inhibiting. That said, the problem you're having isn't really related to `netwire` at all - you've simply got your `do` notation wrong: loopWire wire session value = do (step, newSession) = stepSession session (result, newWire) = stepWire wire step (Right value) case result of Left _ -&gt; putStrLn "Signal inhibited." Right v -&gt; do putStrLn ("Value: " ++ show v) loopWire newWire newSession v Within a `do` block, you have to use `&lt;-` for assignments: loopWire wire session value = do (step, newSession) &lt;- stepSession session (result, newWire) &lt;- stepWire wire step (Right value) case result of Left _ -&gt; putStrLn "Signal inhibited." Right v -&gt; do putStrLn ("Value: " ++ show v) loopWire newWire newSession v In regards to your questions.... 1. Your final approach looks correct. If you find the type signatures too much, you can always introduce your own type aliases. E.g., `type MyWire a b = Wire (Timed NominalDiffTime ()) () IO a b`. 2. I'm not sure how you're supposed to guess it, but I always use `Timed NominalDiffTime` as you've written. However, in my wires, I leave `s` polymorphic and add `HasTime t s` to my context (as GHC already infers). 3. What problem did you have? 4. That really depends on how you form your system. Personally, I don't have side effects in my `Wire`s - so infact I don't even pick a `Monad` instance and just leave it polymorphic. Thus I usually have types such as: `worldCamera :: (Applicative m, HasTime t s, MonadFix m, Monoid e) =&gt; BSP Float -&gt; Wire s e m [SDL.Event] (M44 CFloat)`. 
I piss myself everytime, EVERYTIME
&gt; the worst combination Would you care to explain why?
I never noticed that my own argument to align list literals so all the commas line up is applicable to this case too. Good point! :)
`:: Show a` Is `::` related to `Show a`? I'd say `Show a` is related to `=&gt;`. `=&gt;` always comes with classes but `::` doesn't necessarily. Same for `=&gt;` and Int etc..
http://dev.stephendiehl.com/fun/rss/atom.xml
**EDIT:** Nevermind about (3) and (4). /u/gelisam explanation covered that. Actually, the `=` was just a typo. In my actual code I used the correct notation (`&lt;-`). Anyway... &gt; 1) Your final approach looks correct. That's good to know. I had th fear that I was overcomplicating things... &gt; 2) (...) in my wires, I leave s polymorphic and add HasTime t s to my context (as GHC already infers). Good catch. I just changed to `loopWire :: (HasTime t s) =&gt; Wire s () IO Int Int -&gt; Session IO s -&gt; Int -&gt; IO ()` (along with the wires, of course) and the types matched :) &gt; 3) What problem did you have? Consider: wireA :: SimpleWire Int Int wireA = mkPure_ $ \a -&gt; if a &lt; 10 then Right (a + 1) else Left () How should supposed to call `testWire`? λ&gt; testWire clockSession_ wireA &lt;interactive&gt;:88:1-8: No instance for (Control.Monad.IO.Class.MonadIO m0) arising from a use of `testWire' The type variable `m0' is ambiguous Possible fix: add a type signature that fixes these type variable(s) Note: there is a potential instance available: instance Control.Monad.IO.Class.MonadIO IO -- Defined in `Control.Monad.IO.Class' In the expression: testWire clockSession_ wireA In an equation for `it': it = testWire clockSession_ wireA &lt;interactive&gt;:88:10-22: No instance for (Applicative m0) arising from a use of `clockSession_' The type variable `m0' is ambiguous Possible fix: add a type signature that fixes these type variable(s) Note: there are several potential instances: instance Monad m =&gt; Applicative (Wire s e m a) -- Defined in `Control.Wire.Core' instance Applicative m =&gt; Applicative (Session m) -- Defined in `Control.Wire.Session' instance Applicative Identity -- Defined in `Data.Functor.Identity' ...plus 16 others In the first argument of `testWire', namely `clockSession_' In the expression: testWire clockSession_ wireA In an equation for `it': it = testWire clockSession_ wireA &lt;interactive&gt;:88:24-28: Couldn't match type `a' with `Int' `a' is a rigid type variable bound by a type expected by the context: Wire (Timed NominalDiffTime ()) () Identity a Int at &lt;interactive&gt;:88:1 Expected type: Wire (Timed NominalDiffTime ()) () Identity a Int Actual type: SimpleWire Int Int In the second argument of `testWire', namely `wireA' In the expression: testWire clockSession_ wireA In an equation for `it': it = testWire clockSession_ wireA &gt; 4) (...) I don't even pick a Monad instance and just leave it polymorphic While this worked nicely for the wires, `loopWire` have problems to infer the type. Still, good catch. Thanks for the response. It already helped a lot.
I installed with cabal "clock" and "formatting" but when I run the last bit of code you wrote that uses the Monotonic time, after ghc compiles and links and I run the exe, I get an output of: &gt; time.exe 14047021413122.65 d Do you know what I did wrong?
Did you install [formatting 6.1.0](http://hackage.haskell.org/package/formatting-6.1.0/docs/Formatting-Clock.html)? =3
Ooops! This was just a type in my post. The actual code uses `&lt;-`
&gt; But this is very rare! Much more frequently, the package would just build correctly, and the base constraint is just another hurdle. In my experience, this is not true: most new versions of base break some of my/our packages, and many more in our dependency chain. In the end, it comes down to what you want to offer your users: a possibility of either a successful compile or a type error they probably don't understand, or a message saying it couldn't resolve the dependencies since the package doesn't support the newest base? I prefer the latter, especially since you can override it with `--allow-newer`.
 14230228015319.42 d TimeSpec {sec = 2182353207576363848, nsec = 0} TimeSpec {sec = 3411844908099961672, nsec = 0} I have no idea what I'm looking at XD Sorry my stupid Windows machine is causing problems.
Reported issue [here: #17](https://github.com/corsis/clock/issues/17). Would appreciate any Windows users looking into that.
Why not use ordinary type-classes for this?
I executed cable install formatting I didn't use a particular version number --- shouldn't it just do the latest version? If not, how is one supposed to know which version?
With `Control.Lens`, you can define: null = hasn't each Which is general already for all these cases, I believe. size = lengthOf each Also works, but is of the wrong performance characteristics for some structures that keep a size. So a specialized subclass (or superclass?) of Each could help with that. `length` should really be one and the same with `size`, no? `map` is already `fmap`, except for a minor caveat with `Set` that cannot be a `Functor`. That caveat might be a feature, because `fmap` preserves laws that `map` doesn't (e.g: maintains the shape/size, whereas Set.map doesn't).
You can read about this proposal here: https://ghc.haskell.org/trac/ghc/ticket/4479 
Can you make an example where using `Set.map` (as if it was `fmap`) would not satisfy the laws of `Functor`?
Neat! It would be helpful if you could spell out a bit more clearly here what exactly mutation analysis is, and what it could be used for.
http://en.wikipedia.org/wiki/Mutation_testing
:D Together we can make the world a better clock package for Haskell! 
It's a technical restriction. `Set.map` requires an `Ord` constraint and therefore isn't suitable as an implementation of `fmap`.
Thanks, I have updated the post.
Right. So maybe for `Set.map` it really has to be `Set.map`. But there are a lot of other `map`s, and we would all benefit from using: null list null map null set null string null bytestring instead of: null list Map.null map Set.null set null string ByteString.null bytestring and so on. Also `Control.Lens` is a huge dependency and not always appropriate for all types of projects.
Thanks for the tip about `executable-profiling`, that fixed an issue I was having.
Personally I think I'd prefer to be explicit. Some people feel differently, and there's already classy-prelude for them.
The forward-instance-declaration actually seems like a very pragmatic 80/20 solution while we're waiting for Backpack (or similar). Any idea if it's actually been proposed or...? (EDIT: I'm guessing Backpack will probably take several years given that type classes still aren't handled yet last time I heard.)
You are working with a rather mythological new user who is using a bleeding edge compiler, doesn't know how to fix upper bounds but does know how to fix the compile errors resulting from a compile with a new compiler if there is no upper bound. Meanwhile those old packages without upper bounds are quite real, quite painful to encounter (think an entire sandbox worth of packages built with a wrong, incredibly old dependency resolution) and cost those who invest most in the language lots of work.
You could always have two repositories, one for push and one for pull and have the CI push checked changes from one to the other.
I don't think my scenario is "mythological" at all, but please explain to me what work is saved by adding upper bounds **on base**.
It is not just that. map f . map g = map (f . g) can be broken if the intermediate state has a non-structural `Ord` instance.
When visiting http://www.yesodweb.com/page/quickstart like at the beginning of the video I don't see anything after the learn more section, though there is a whole other part there in the video.
Well, I don't see a problem with that. As long as it holds the laws for `Functor` I don't see why it wouldn't be a functor. The real problem is the `Eq` and `Ord` problem other described. 
All the work that goes into failed compiles instead of direct dependency resolution failures on old packages and/or the work Haskell trustees have to put in to fix upper bounds after the fact on every single old version of every package (and no, it is not just 'set all of these to one value' since old packages are not necessarily all the same age).
I don't see anyone assigned to the ticket. So, unless you are doing the work (under Simon's mentorship), I wouldn't expect it to be implemented any time soon. There's also the backwards compatibility issue. Right now `(a.b)` in Haskell code means `((.) a b)` not `(b a)`. You might look at Frege, too.
Hm, good catch. It looks like the page has changed a bit since I made that part of the video. The old instructions were split into Stackage/sandboxes, but since now Stackage is more compatible with sandboxes I guess they've been combined together. The new instructions should work fine, though.
The usual things: Have you tried profiling? What did you learn? How are you compiling? `(!!)` is certainly a huge part of your problem as singpolyma points out. Have you looked at how the type `[a]` is implemented (try typing `info []` into ghci)? Often knowing what your data structures look like will give you an intuition for the complexity of different operations. 
Seems like it, I had some vague misconception that [] is not really a linked list but some sort of array like contiguous data structure... Back to the drawing board I guess, but then this particular interface of returning indices makes no sense unless the underlying data structure is an array. In C++ its different since it is an iterator. 
Thank you. This was enlightening and I simply don't have words to thank you enough for this explanation!!! It's a shame that I ccan only give one upvote.
How does it compare to [American Fuzzy Lop](http://lcamtuf.coredump.cx/afl/)?
The Fuzzy Lop is a fuzz testing tool. MuCheck is used for another purpose. That is, How do you compare how effective is your fuzzer compared to say another fuzzer? Mutation analysis can be used to evaluate how good your testing tool is. What you do is, ensure that your fuzzer is run, and all tests are caught, so that according to your fuzzer, the program is bug free. Now, if I manually introduce a defect, and fuzz it again, if the fuzzer is effective, it should lead to a crash or an invalid program state that is detectable. So whether you are able to detect a newly introduced bug provides some indication of the effectiveness of your test suite, in comparison to another tool that was able to detect it. One problem that such a technique has is that the results depend on where the bug was introduced. So we have to rely on a large set of heuristics compute the probability of bugs in various portions of code, and introduce bugs according to that probability. What mutation analysis (that mucheck implements) does is to do the introduction of (first order) faults exhaustively. That is, by introducing an exhaustive set of small defects, and see how many of these were caught, it can -- to some extant -- step outside the problem of having to decide on the defect probability. It utilizes the theory of [coupling hypothesis](http://en.wikipedia.org/wiki/Mutation_testing#Mutation_testing_overview) to state that statistically most of the higher order defects will have been caught by any tool capable of catching the first order defects.
Got it. That is some material you gave me right there. Thanks.
The inferred type for `loopWire` is loopWire :: Show a =&gt; Wire s e IO a a -&gt; Session IO s -&gt; a -&gt; IO () not loopWire :: SimpleWire Int Int -&gt; s -&gt; Int -&gt; IO () But the trouble above is with `testWire` and `testWireM`, which don't fit with the wires you are defining, since they require something with the `a` parameter polymorphic or unspecified, where yours all specify it as Int (something this will always happen with `mkPure_` unless you supply it with a constant function.) Specializing a little bit: testWire clockSession_ :: (forall a. Wire (Timed NominalDiffTime ()) () Identity a Int) -&gt; IO () The `forall a` means that the `a` parameter mustn't be used. By contrast `loopWire` can use the wires you have so far. 
You can just not do any of that work though. That is what I am saying. What will go wrong then?
I didn't. That's what makes it *sweet*
Look, it's very simple. Musical instruments are just organoids in the category of audiofactors. What's the problem?
As far as I know the [AMP](https://www.haskell.org/haskellwiki/Functor-Applicative-Monad_Proposal) will take full effect, thus changing class Monad m where return :: a -&gt; m a (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b (&gt;&gt;) :: m a -&gt; m b -&gt; m b m &gt;&gt; n = m &gt;&gt;= \_ -&gt; n fail :: String -&gt; m a to class Applicative m =&gt; Monad m where join :: m (m a) -&gt; m a return = pure -- rest as above Therefore, any paragraph that states *"a Monad is always an Applicative but due to historical reasons…"* obsolete. Have a look at the [migration guide](https://ghc.haskell.org/trac/ghc/wiki/Migration/7.10) for more information.
Performance aside, this seems like a "lifting" type of transformation. Like, you are lifting the "&lt;" function into the concept of a list. Does that make sense?
Is this something backpack can solve? I'm having trouble finding some "mock code" that shows an example of, say, the Map module defining an API that other modules can program against. Now that I think of it, would that even provide a solution to this particular problem?
My understanding is that `join` was originally going to become part of `Monad`, but due to problems caused by GeneralizedNewtypeDeriving roles it remains outside the class.
Honestly, I think you are being a bit overly paranoid on this particular count. There has been no substantiated, public instances of this occurring, and to be frank, we are much too small of a community to be generally targeted in this fashion. For the moment, the only plausible scenario is that somebody's explicitly targeting you. That concern may or may not be reasonable, but if it is, then cabal/hackage may very well be the least of your concerns. If you want a simple and effective way to mitigate this concern, you could simply set up your own cabal repo on a read-only media, perhaps a server on a local-area network that you control, or a cabal repo in it's own VM. Then you'll be able to go back and audit what you compiled if anything happens. And, hackage supports TLS, so you can even download the packages over TLS to your own repo. It's something that should be fixed, and will eventually be fixed. If you are that concerned about it, then I'd suggest taking the initiative in making it happen. One concern however, is that the cabal-install project is very sensitive about foreign-language dependencies, for better and worse. So adding support for TLS in cabal-install may be a tricky proposition. edit: One more idea, you could pretty easily set up an HTTP proxy or stunnel on your own machine and point cabal-install at it, such that it upgrades the connection to TLS. 
I tried looking at the CMM and other intermediate output. I cant make any sense of it! 
This is a specific case of a law that holds for all types `Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b` by virtue of Haskell's type system (this is called `fmap`s free theorem), but it doesn't hold if we change the type to include an `Eq` constraint. This means we have fewer possible manipulations of `fmap` that are guaranteed to not change the meaning of the code.
&gt; There has been no substantiated, public instances of this occurring, and to be frank, we are much too small of a community to be generally targeted in this fashion. And then we're asking ourselves why haskell is not mainstream. It's probably the lack of native support for zygohistomorphic prepromorphisms, right?
That's an important point, although I find the polemic way of arguing it unhelpful. It's kind of a chicken-egg-problem. There need to be more haskell developers to get issues like this fixed, but those issues might keep away devs from adopting haskell in the first place. I'd like to help out, but I know next to nothing about implementing crypto, so I'm not touching this. edit: word deduplication
OK I now understand how @ works However you realize that this minmax does 2N compares right. The original algorithm is 3n/2 and proven to be optimal. It involves taking two items at a time, and comparing them, so you can be sure which one can potentially be a max and which one a min. I'm wondering if that can be implemented using a fold. Let me try Let me try writing that 
&gt; I very seriously doubt that this particular issue significantly constrains the popularity of Haskell It doesn't but this attitude is a perfect illustration of why (people think) that haskell is unusable for anything except for calculating factorials.
&gt; I like that approach. Are there any blog post describing this method? I guess it won't be too difficult to figure it out by my self, but help is always appreciated. I don't know of any. You might find it easier and/or more convenient to use a local proxy or tunnel. I'm sure you woudn't be breaking new ground, but you could well be the first to write it up, which would be useful.
Woah, I missed your “rest as above” and thought we were going to have a `Monad` class with only `return` and `join`. “What will the tutorial writers say!”
Personally I use Debian packages. Debian provides a great Haskell [ecosystem](http://pkg-haskell.alioth.debian.org/cgi-bin/pet.cgi). If a library I need is not in the official repositories, then I package it. All official Debian packages are GPG signed and checked for a valid signature at install time.
I still think package signing by authors and code reviewers in a web of trust would be ideal. Maybe I should make a page explaining how to do it, perhaps people just need a clear path.
When I heard that the problems with GNTD were being addressed, I perked up, but then when I started reading about roles I was confused and disappointed. I don't understand why GNTD doesn't just do a little code generation and type check it as normal.
Amen to that. This "roles" thing is beyond redundant - it's actually harmful.
Right, I missed that subtlety, thanks. I just instinctively rewrote the function more idiomatically because it seemed like there were some problems in translating from imperative to functional. I'll reread the original. EDIT: can't use a normal sort of fold, as this algorithm involves stepping through the list by twos. [Current brain contents.](http://hastebin.com/hajikuxaqo.hs) Of course, this doesn't really address the question of whether you should be using a list or a vector - I'm just attempting to clean up those huge nested if statements. Does this look any clearer? I've managed to follow the 3n/2 comparisons algorithm, IIRC.
&gt; In Haskell, we do aliasing with the keyword type, which is a bit stupid since we won’t necessarily alias a type (we could alias a function, a typeclass constraint, and so on and so forth). What? I'm fairly certain foo x = x + 1 type Bar = foo won't compile.
That would need a better explaination, yeah. I meant this: type Pred a = a -&gt; Bool Which is a type, of course, but not a “common type” for regular users. Of course you could argue that it’s an operator type, yeah…
A real example would be type Foo = Maybe By doing this instead of type Foo a = Maybe a you can write instance Thingy Foo where which would otherwise result in a compile error.
He decided to hold his breath while finding the bug.
I don't know that I agree with the conclusion: "don't use aliases". Newtypes are cheap but not free either syntactically or at runtime (if you forget to use coerce). Furthermore, sometimes you *want* to be able to confuse two types. It depends upon the story you're trying to tell. The thing to remember is not "don't use aliases" but instead to just keep in mind what they are: aliases, nicknames, documentation. To this degree using the keyword "type" feels a bit misleading despite the savings made on minimality of keywords.
For someone trying to get productive with haskell quickly would you agree that it isn't harmful to say that list suck in terms of performance? The reason I ask is that little white lie could have people writing productive code faster instead of learning where to use vectors vs. List which may not be a very good use of their time. My head space is aimed at reducing corner cases you have to learn about to write real world practical haskell code. Warning: I used Google Voice typing for this so there could be mistakes.
I personally wouldn't worry. I mean, maven has been running a hodge bodge of do-whatever-you-want without https since the dawn of time, and literally piss all has ever actually happened. But I hear tin hats stop them from reading your mind...
hey man, if we can steal it that easily, it's obviously not very secure!
You’re cutting the context of my “don’t use aliases” ;)
&gt; newtype Max a = Max a deriving (Eq, Show) &gt; newtype Min a = Min a deriving (Eq, Show) Using types to distinguish between the min &amp; the max gives me an idea... Here's another way of doing it: import Data.Monoid data Min a = Min a | UpperBound deriving (Eq, Ord, Show) -- x &lt; y =&gt; Min x &lt; Min y &lt; UpperBound data Max a = LowerBound | Max a deriving (Eq, Ord, Show) -- x &lt; y =&gt; LowerBound &lt; Max x &lt; Max y instance Ord a =&gt; Monoid (Min a) where mempty = UpperBound mappend = min instance Ord a =&gt; Monoid (Max a) where mempty = LowerBound mappend = max minAndMax :: Ord a =&gt; [a] -&gt; (Min a, Max a) minAndMax xs = mconcat [(Min x, Max x) | x &lt;- xs] 
That was all an image… You guys are… ;)
`a -&gt; b` is not a function, it's a function type.
Supporting your point by questioning the askers sanity… Thanks for the input, but this isn't 4chan, you know?
One author of the referenced pearl here. Thanks for an interesting post and a very useful trick! First I was a bit confused as to why you'd call this "a much faster version of Axelsson and Claessen's trick" when we didn't even try to implement substitution in the paper. But I suppose it relates to the discussion about transformations under binders at the end of the paper (which is not what I would call the trick of the paper :) ). For clarification, the implementation in your post improves the efficiency of a naive implementation of `rebind` that does not use an environment. Then the `Lam` case will be something like Lam b t e -&gt; lam (hint b) (go t) $ \v -&gt; rebind' (subst1 b v e) f where `subst1 b v e` substitutes `v` for `b` in `e`. Have I understood this correctly?
I would go for: let bin = [0] : [1] : concatMap (\a-&gt; [0:a, 1:a]) bin take 10 bin however a bit different order
&gt; Have I understood this correctly? You have it exactly. &gt; which is not what I would call the trick of the paper :) The core circular substitution trick of the paper is quite elegant. =) The bits I was addressing in this post were: &gt; This restriction is fine in an embedded language front end where terms are constructed from scratch, but it makes the approach unsuitable for introducing new bindings in existing terms. through &gt; Although this version does avoid capturing, the whole approach is a bit fragile, and the need for renaming makes it quite inefficient. at the end of part 4. The main problem with the paper I had is that if I go to do any sort of serious refactoring / substitution on the syntax tree, I wind up having to walk under binders all over the place. If they naively rebind the way mentioned at the end of the paper then the approach scales very badly. The `rebind` trick for performing simultaneous substitution rather drastically accelerates such processes, and makes it possible to use this in 'deeper' embeddings where you tend to do a lot of refactoring / substitution on the syntax tree. Since 90% of the usecases I have for HOAS-style EDSLs are for deep embeddings, this was the barrier to me actually using your approach before. I'm not likely to give up using something like `bound` for most of my name capture needs, but for use cases where I want to expose a HOAS-like syntax to the users for construction, this is very clean!
You need to import `Control.Applicative` for `liftA2`: let digits = [0, 1] next = liftA2 (:) digits binaries = (tail . concat . iterate next) [[]]
Wait... packing/unpacking a newtype constructor isn't free at runtime?
And it’s also my point :).
You can add other useful defaults as well: x &gt;&gt;= f = join (fmap f x) join m = m &gt;&gt;= id
Not *always*. With the canonical example newtype Age = Age { unAge :: Int } we have that `Age` and `unAge` are free, but `map unAge :: [Age] -&gt; [Int]` will still cross the list once despite it being a no-op. This is one place where it's "ok" to use `unsafeCoerce :: [Age] -&gt; [Int]` since it's exactly the case that the runtime representations are identical. More recently GHC has gathered some machinery called `Coercible` which is a typeclass that's automatically generated from `newtypes`. It looks a bit like class Coercible a b where coerce :: a -&gt; b where `coerce` is guaranteed to be free at runtime. We'd also have the (automatically generated) instances instance Coercible Age Int where coerce = unsafeCoerce instance Coercible Int Age where coerce = unsafeCoerce but, more importantly, the instances instance Coercible [Age] [Int] where coerce = unsafeCoerce instance Coercible [Int] [Age] where coerce = unsafeCoerce Since these instances are all autogenerated they essentially delimit places where `unsafeCoerce` is actually safe. The upshot is that while `map unAge :: [Age] -&gt; [Int]` is not free at runtime, it's completely equivalent to `coerce :: [Age] -&gt; [Int]` so, provided you notice this opportunity, you can use `coerce` instead.
&gt; but for use cases where I want to expose a HOAS-like syntax to the users for construction, this is very clean! Yes, indeed!
Like in any other language, you have to understand something about the performance tradeoffs to write efficient code. Saying things like lists are always bad (or always good), is just not true. There's no to easy advice. Vectors are certainly not the answer. They are sometimes the right answer, sometimes not. I rarely use vectors, I tend to use various maps (Map, IntMap, HashMap) more.
Same here. While sometimes a little out of date, this also means I'm sure my program will work with the next release of Debian. Note that Debian maintainers still have no way to be 100% certian of the validity what they download from hackage either in the first place though..
But what you say is just wrong. `a-&gt;b` is the type of functions from `a` to `b` or _are_ all the functions (if you want to look at it with sets and not type theory). That's not splitting hairs but being not totally wrong.
The semantic is that `a -&gt; b` is the **type** of a function that takes an `a` and returns a `b`. That's why it's called a function type. This isn't splitting hairs, there's a fundamental difference between the two. The `-&gt;` doesn't have any meaning in the realm of values (although Haskell uses it as syntax occasionally), where actual functions exist. If you will, `-&gt;` is a type constructor, taking two types as parameters. This is a fundamental part of type theory: You can take types and make more types out of them. In this regard, `-&gt;` is no different from `(,)` or sum types (that's `Either` in Haskell). This property is at the core of what makes types a useful language. To get back to your initial intention: `type` is probably not the best choice of name for type synonyms, but it doesn't work with things that aren't types, and as such it's not too misleading either.
It does with `ConstraintKinds`…
One thing that you can do is download the git repository of a package, and build from there. But before you do, check the tag of the version you want with "git tag --verify $tag". Make sure it's signed. If there's not a signed tag (or perhaps not even a tag), you can contact the author and ask them to improve their release process. This will benefit not only you, but distributions like Debian that package haskell packages and can verify this stuff too. Of course, then authors need to be hooked into the gpg web of trust. Probably keysigning parties at haskell meetups are called for. All my haskell software should be verifiable this way, and I'm well connected in the strong set of the gpg WoT, and happy to keysign with any haskellers I am lucky enough to meet.
Oneliner challenge accepted concatMap (flip replicateM [0,1]) [1..]
 binaries = [] : [ new : old | old &lt;- binaries , new &lt;- [0,1]] &gt;&gt;&gt; take 10 binaries [[],[0],[1],[0,0],[1,0],[0,1],[1,1],[0,0,0],[1,0,0],[0,1,0]] -- edited from the nicer looking binaries = [] : [ new : old | new &lt;- [0,1], old &lt;- binaries ] which of course wrecked the sequencing as matematicaadit notes. 
Damn, I was hoping marketshare would move Happstack to CPAN.
This is great. I like darcs, but github makes it so easy to fork, send pull requests and file issues that I'm much more likely to contribute, or at least will do so sooner.
Constraints are also types, they're just not types of kind `*`, but of kind `Constraint`. See for example [this comment](https://github.com/ghc/ghc/blob/master/compiler/types/TypeRep.hs#L222) from the GHC source.
With all binary strings. I said it wrong.
&gt; all the nice packages integrating ghci with Emacs won't work unless I put all development tools into the VM – something I usually avoid. Why avoid it? You should be able to make a dev image with all those tools that you just clone or whatever, so effort of installing them shouldn't be too much of a problem. You also probably don't need a separate VM for every single project.
I think you get it wrong Prelude&gt; let binaries = [] : [ new : old | new &lt;- [0,1], old &lt;- binaries ] Prelude&gt; take 10 binaries [[],[0],[0,0],[0,0,0],[0,0,0,0],[0,0,0,0,0],[0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0]]
Interesting idea. I never thougth about using subtyping for this purpose. I've heard that integrating subtyping in Haskell's type system will indeed have some drawbacks. If I remember correctly, subtyping is one of the main reasons that type inference in Scala works only sometimes, but I haven't yet found time to look at the phenomenon closely.
This is a position for a company migrating from C# to Haskell as the language of choice (default language for projects, if customer requirements does not dictate otherwise). One will encounter and work on the MS stack as well as Haskell, but the Haskell part is initially substantial and will grow.
Yes, this position is in Sri Lanka for the offshoring company Embla. I work for the client company which is located in Stavanger, Norway. Embla does not think it will be easy to get any Haskell-devs in Sri Lanka, this post is here to try to prove them wrong :)
Given my employer, I think these type of devs are indeed hard to get, if not impossible.
For a Norwegian salary or a Sri Lankan one? ;)
check out Chris Done's structured-haskell-mode
Well two things... The algorithm only makes sense if we have the indices. The use case is to find elements in a sequence that are "least" and "most" for some definition of "&lt;" and manipulate those elements, perhaps remove them from the list, perhaps move them to the front or whatever. Returning indices only makes sense for Vector I agree. Also as I've said in other threads here, the algorithm I use does 3N/2 compares, not 2N as a simple implementation using fold would... 
Duncan and I have been working on it, but slowly. For the most part I'm busy with my Microsoft contracts working on GHC, but right now that's winding down for the fiscal year - as well as the annual release cycle - so I'll have time to do more things in this area. We also want to be sure we don't ignore important research here; the biggest inspiration for me since that comment has been reading about [The Update Framework](http://theupdateframework.com/). Hackage doesn't exactly fit this model 1-to-1 in the sense every piece is applicable, but there's a lot of good research here to learn from (which has a nicely defined threat model as well as many important components to shut them all out).
As some who has used qwertz, dvorak, and colemak in the past. And who is currently using workman. I think the whole "qwerty suck" is a bit exaggerated. It's a little less efficient probably, but not enough for most people to care. 
What are some examples? If it was all the complexity for that small benefit, it would be an arguable trade-off. But once useful things like a "join" method in Monad become problematic (And GNTD for Monad is crucial!) then even without the complexity it's probably not worth it.
For example, deriving `Functor` for newtype T a = T (Foo a) where `Foo` does not have a `Functor` instance. The generated deriving code has to assume that occurrences of the type variable nested inside other type constructors can be treated systematically. BTW, here's what the Mu compiler uses for the `Functor` instance: instance Functor T where fmap = \ x0 -&gt; \ x1 -&gt; T (fmap x0 ((\ T x -&gt; x) x1)) (&lt;$) = \ x0 -&gt; \ x1 -&gt; T ((&lt;$) x0 ((\ T x -&gt; x) x1)) 
Ah. Clear now.. Thanks!
&gt; You’re defining them as “they’re just not types of kind *, but of type Constraint”. Something is wrong with your sentence. Yes he got that wrong. "To be precise, with this flag any type of the **new kind Constraint** can be used as a constraint. ([Source](https://downloads.haskell.org/~ghc/7.4.1/docs/html/users_guide/constraint-kind.html)). Every constraint that you can use with `-XConstraintKinds` is a *type* of kind `Constraint`.
What code does Mu produce for deriving a class with just the single method `join :: m (m a) -&gt; m a`?
For now, classy lenses are the answer here.
It's a hack: join = \ x0 -&gt; T (join (Prelude.fmap (\ T x -&gt; x) ((\ T x -&gt; x) x0))) 
I disagree that typeclasses are the proper solution here. Most pretty-printers don't need coherence of type classes so they'd be better off with an implicit parameter (scala-style). I'm not sure that FromJSON and ToJSON need coherence either. I guess it's a little hard to judge if coherence is needed based on the declaration of a type class, but rather on how it is used.
The Norwegian client company. We have been using Haskell in production for two years now. We did some research in Sri Lanka last year and met 8 offshoring companies there (in person). They had close to zero experience with any functional development.
This guy gets it :)
FromJSON and ToJSON do require coherence if you want to be able to statically ensure that your encoders and decoders match up (in, say, a distributed application). If you're allowed to have multiple implementations of these, then it's hard to guarantee that you're using the right one for a given sender / recipient of the encoded JSON. Regardless of qualms with a particular set of typeclasses, the concrete example chosen doesn't matter. Orphan instances are not something that are easily "designed out" of Haskell, and have a bad reputation for no good reason. Personally, I haven't had them bite me in practice, or see anyone have any significant problems due them. It would be good to have our tooling be more aware of them, but overall they are quite useful and I wouldn't want them removed from the language without having features which essentially fill the role.
&gt; * Time synchronization services regularly update time. &gt; &gt; If you’re on an Ubuntu desktop, time is updated when you first boot up from NTP servers. If you’re on a server, likely there is a daily cron job to update your time, because you don’t tend to reboot servers. That's odd. The way the `ntpd` daemon works, generally, is that it will only hard reset the time when the time error exceeds a certain threshold; the preferred mechanism for maintaining the sync is by speeding up or slowing down the OS's clock, so as to avoid having time jump discontinuously. (See [this item in the FAQ](http://www.ntp.org/ntpfaq/NTP-s-algo.htm#Q-CLOCK-DISCIPLINE).) So either Ubuntu doesn't use `ntpd` by default, or this description isn't quite right. (Or alternatively, `ntpd` is on but the clock is wildly unstable and thus regularly exceeding the time error threshold that `ntpd` uses to decide to reset the clock.)
Opinion: I think Happstack doesn't get enough attention it deservers. It is my go to web framework. It provides more features than Scotty and isn't as opinionated as Snap or Yesod. Yesod having a lot of template haskell and Snap has a requirement on lens for its snaplets.
I like this series of articles about CT alot.
Hmm, I did this a couple of years ago in Hugs except it would never return anything for me I had bind f m = join (fmap f m) join = bind id Is it a new thing that makes this possible or is just GHC (it's been a long time since I thought about this)
&gt; I haven't had them bite me in practice I have. Not too many of them, but rather not knowing what module to import to get an orphan instance. They get documented at the class declaration and at the data type / newtype declaration, but importing both of those modules did not give me the instance I was expected. I can also imagine them being used as the "path of least resistance" during application development and hurting modularity and refactoring when part of the application is trying to become an open library. &gt; the concrete example chosen doesn't matter. *Having* an example does matter. I'm do currently advocate orphan instances as a mistake that should be rectified. Not only do they make coherence harder to guarantee, all the examples I've seen of them use type classes because they are our only ad-hoc overloading and suffer (at least some) from the coherence requirement. &gt; FromJSON and ToJSON do require coherence if you want to be able to statically ensure that your encoders and decoders match up (in, say, a distributed application). Coherence is insufficient for this. They'd have to be combined into a single type class (and you'd probably need a dependent type system) to static ensure they are an "almost" isomorphism.
I occasionally find them important in application code where I need to get two libraries to interact properly, until one library author adds an instance. In that case, I try to keep all my orphan instances in a single file, add OPTIONS_GHC -fno-warn-orphans, and import that file everywhere.
The problem is that I might get something from one library that contains a Set Foo based on one Ord Foo instance, and then want to define my own Ord Foo. What happens when I go to use that Set Foo? Note that I might not even *see* that it's a Set Foo, inside of somewhere...
Do you think that this offshoring approach will deliver better code than the C# you have now?
What about: map (`replicate` 0) [0..] In finite time you can't prove it _doesn't_ contain all the desired results :-P
``` concatMap (`replicateM` [0,1]) [1..] ``` golfed! :)
The typechecker will keep away the worst, and the client company can always refactor the code far more easily :D
The days of old-style waterfall offshoring is gone, today offshoring means a distributed team of equals. Augmenting our group of developers with a new group off shore is primarily a (quite widespread) cost-cutting measure.
That would be perfect. Please apply.
Correct me if I'm wrong, but it seems like this solution might not be very memory and computationally efficient (not that the OP required such). What about adding 0 and 1 to the last "step". Something like: concat $ iterate ((:) &lt;$&gt; [0, 1] &lt;*&gt;) [[]] Edit: better example
If the problem is really just in the algorithm for solving for dependencies, throw a SAT or ILP solver at it.
Golf, you say? (`replicateM`[0,1])=&lt;&lt;[1..]
Can't think of anyone better to talk about HoTT than Dan! Looking forward to this :)
&gt; An interesting position given your work on structured editing In a text editor, if I see a name, having to TDNR in my head to know the first thing about the semantics of that piece of code is a bad thing, IMO. It's a big step away from denotational semantics which we should aspire to. &gt; I think the fact I can't use parent :: Text for a person and parent :: ThreadId for a thread and use them in the same scope without resorting to personParent and threadParent copouts, advanced type trickery, or hacks like qualifying them by importing them from modules, is an unfortunate artifact of current programming language implementations. What happens when you refactor that piece of code out to its own function? Does it break type inference? What type is given to the expression f x = parent x? &gt; Avoiding acknowledging that this problem, which the person you're replying to pointed out, is indeed a problem, isn't particularly fair or productive We all agree it is a problem. I just don't like the solution of TDNR and believe it brings more problems than it solves. I'm fine with TDNS (Search) which is what your Facebook example and Lamdu is actually doing. When you *search* for a name, the types can go into play. But once you've created the link -- you've expressed your intent, and it should not be second guessed when the types change, nor should I as a reader have to do that search over and over when I read code. &gt; Pretty much the same problem is true for type-classes: you can't compile type-class-using code without type resolution. Both are type-directed resolution The nice thing about type-classes, though, is that you're supposed to be able to understand the code and use of the class method without knowing any of the instances. You only need to know the semantics attached to that name, and those semantics should hold for all instances. This is exactly what's missing in TDNR.
While I agree that Dan is a great speaker, I must admit that for an ordinary Haskell programmer that's a pretty useless discussion. I didn't understand almost anything...
or to drop the `[]` λ let binaries = tail . fix $ \bs -&gt; [] : [ new : old | old &lt;- bs, new &lt;- [0,1] ] λ take 10 binaries [[0],[1],[0,0],[1,0],[0,1],[1,1],[0,0,0],[1,0,0],[0,1,0],[1,1,0] 
I'm ambivalent about TDNR, but I find the criticism of it that you gave to Duck apply to type-classes equally well, a feature which we all accept the trade-offs for: Recall the contentious debate about Traversable/Foldable in base about being too overloaded, too hard to understand without doing type resolution in your head. “But laws” doesn't change this human factor. I think fundamentally type-classes solve a different problem. Type-classes are meant for generic programming; abstraction over different implementations of the same concept, like writing a sorting algorithm that works on different array types. That's an orthogonal problem to the fact that you can't use the same name for *different concepts*, like `map` as in the function or `map` as in a pictorial geographical map. Using type-classes for this is incorrect. 
Perhaps "sums" would be a better word than "coproducts". It doesn't have that "copro" association, heh.
A pure linked list has a sequential dependency between reading successive list nodes, while a more "chunked" organization lets you make concurrent request for the next few elements (assuming complete location independence it's not necessary that elements be contiguous, just that you can compute the addresses of more than one element without chasing pointers at each step).
Using the same name for different concepts is fine as long as you disambiguate. I think disambiguation based on types is a bad idea in Haskell. Disambiguation via yucky imports is still bad, but better IMO. The best disambiguation, of course, is to just choose (Facebook search style) at use time, and then have the presentation deal with disambiguation of the display, as needed.
Very nice, enjoyed this. And thanks to you, I now found dash :) Where did you get the docset for Yesod?
Agreed.
Could you link to some control theory resources?
Indeed it is, thank you!
What do you think about Keybase? I made an account there sort of on autopilot, but as long as I have a public key floating around I wouldn't mind growing its footprint. I suppose it defeats some amount of the "real person" purpose, but it's significantly easier to do en masse and links all of your online reputations together at the least.
Good idea, done: https://github.com/fpco/mutable-containers#readme
You've just ruined that word for me.
See https://phabricator.haskell.org/rGHCc409b6f30373535b6eed92e55d4695688d32be9e
Yup, that's a very good strategy! Orphans are particularly appropriate in the context of an application, as you can reason locally about all uses of orphans. Even if your application is so large that you accidentally end up with overlapping orphans, it's easy to resolve.
[Fold version](http://pastie.org/9821100) of minmax using /u/Tekmo 's [foldl] (https://hackage.haskell.org/package/foldl) library, and using Maybes rather than minBound and maxBound. On my machine, there's a 3x speedup compared with your go solution, making it faster than the C++ list implementation at least (yay!). My guess is that list versus vector is a red herring - there's a lot of copying going on if you stick to immutables, whatever the data structure. 
For additional humor try hoogling for "google" and "bing".
I have done in the past, but since everyone's background is different, specific recommendations need to be tailored to the person. That said, here are my favorites: - for pure type theory: [Programming in Martin-Löf's Type Theory](http://www.cse.chalmers.se/research/group/logic/book/); I originally didn't really get the point of this book, but in hindsight it's probably one of the better ones out there. At some point, it might be useful to produce a companion or commentary that sort of catalogues the "ah-ha" moments that led me to prefer this book. - for PL foundations: [Practical Foundations for Programming Languages](http://www.cs.cmu.edu/~rwh/plbook/book.pdf) There are plenty of other good books on type theory, some of which may be better or worse suited for each student. I also like [Type Theory and Functional Programming](https://www.cs.kent.ac.uk/people/staff/sjt/TTFP/ttfp.pdf). TAPL is also very popular, and has some good chapters, though honestly I have never found it as useful as the books I've listed above, personally; many people swear by it, though, so it is definitely worth a read. EDIT: I forgot this was about homotopy type theory (wine). If you don't know type theory well, I still recommend PiMLTT; if you've got a cursory understanding, then read the HoTT book; it's got its drawbacks, but it's a good read. I've also heard of people first learning type theory from the HoTT book, but frankly, I don't recommend that.
Note that my library is not buying you much for that example, because: fold (Fold step begin done) xs = done (foldl' step begin xs) So you could just write it to use `Data.List.foldl'` instead.
I think it's a great language for stats/numerical computation (that's one of the things I use it for). Unfortunately, its ecosystem in that area is still rather underdeveloped compared to that of R and Python - hopefully this will improve in the next year or so, given the number of people who are asking this question. It is indeed possible to call Haskell from R[0], and vice-versa[1]. You may also be interested in iHaskell[2], which is a Haskell backend for iPython which shows great promise. [0] http://neilmitchell.blogspot.com.au/2011/10/calling-haskell-from-r.html [1] http://hackage.haskell.org/package/Rlang-QQ-0.1.0.2/docs/RlangQQ.html [2] https://github.com/gibiansky/IHaskell
There are definitely a few, I think I usually see 8 downloads within a few hours of publishing a totally new package with no announcement. Definitely not 77 though; there are packages that have never seen that many downloads total.
I've built a fair number of numerical, statistical, machine learning algorithms in Haskell while in grad school. If you learn how to optimize Haskell code you can make it go reasonably fast and things work out nicely. But I was more or less implementing each thing from scratch. That is not what you want to do. If you're willing to work your way up from BLAS then you can do alright today. There are a fairly reasonable number of stats/numerical libraries in the wings. I know Carter has an advanced numerics library that's in heavy development. This will still leave you a wide chasm to jump to getting to the convenience and availability of things like Numpy and R, though.
Which package is it?
We have our answer!
The general consensus I've found is: language + compiler, yes. ecosystem, no. but people are working on fixing that :) 
I agree that loading compiled code in ghci is way harder than it needs to be. There are a couple of pain-in-the-ass ways to ask it nicely to load your compiled code, but they really shouldn't even be necessary just to load some optimized code in.
Yes, you can introduce `x : _|_` like that. But you have to use the assumption rule to do it, so it will be an assumption. Will you be able to eliminate that assumption? No. (Check the elimination rule for `=&gt;`, which is the only rule that allows you to discharge assumptions.) So if you build a derivation that uses `x : _|_`, it will forever remain an undischarged assumption in your derivation. In other words, you will end up with a "proof" of the proposition `_|_ =&gt; A` (for some `A`), which is indeed formally valid by the principle of [ex falso quodlibet](http://en.wikipedia.org/wiki/Principle_of_explosion).
Correct me if I'm wrong, but the emptiness above the bar next to `(⊥F)` means that my derivation of `x : ⊥` has *no assumptions*, so I can introduce it w/o incurring an assumption.
You need the rule of assumption *below* the formation rule for `_|_` in the derivation tree. Notice the difference between formation rules ("A is a formula") and introduction rules ("x : A"). Formation rules are prior to introduction rules: you must show that A is a syntactically correct formula before introducing a proof of A. But formation rules have no term-level content. Assumptions are the leaves of the derivation tree at the term level; that is, they require prior derivations, consisting of formation rules only, showing they are syntactically well-formed (though those parts of derivations are usually elided, including in Thompson's book), but no prior term-level derivations. Thompson doesn't do a great job explaining the distinction between formation rules and introduction rules, but maybe re-read the discussion following the description of the rule of assumption on page 77.
Only about 20 or so are bots in my experience.
This (especially the third paragraph) helps a lot, thank you.
Simply generating random numbers is already hard, coming from R or Python. Besides dealing with state/monads, there also does not seem to be "one way" as there are different candidate packages from which one might start out (e.g. Data.Random, Statistics.Distribution).
yeah, I think my major misunderstanding was missing the word "assumption" in "rule of assumption" &gt;_&lt;
&gt; Orphans are necessary as soon as we have a circumstance where we have AModule.AClass, AModule.AType, BModule.BClass, BModule.BType. If you want to have AModule.AType . Did you accidentally a word? Or possibly am I not seeing some unicode characters here? I do believe that, based on the canonicity and coherence required of type class instances that they *should* be restricted to occurring in the same "import unit" (not necessarily module, but that's what we have for now) as either the data type definition or the type class definition. That will cause a dependency between the two, but I advocate other refactorings (to prevent circular dependencies) instead of allowing orphan instances. That said, coherence isn't required of all ad-hoc name overloading, and it would be nice for Haskell to grow a good way to do this. (GHC has ImplicitParameters, but those are still a little wonky and not very popular in Haskell.) Scala just goes "all the way" toward implicits and forgets about coherence completely and I don't think that's great either, but something like it would be nice get good ad-hoc overloading without coherence.
I'm team no. I will say that trying to do your statistical computing in haskell is guaranteed spiritual benefits. Sure, you can arrive at you destination sooner if you take the R train, but the journey...
This lack of a unified approach is one of the biggest problems IMHO. There's actually a number of libraries related to analyzing data on hackage, but none of them work together at all.
I remember taking "Programación Funcional" courses in the Universidade da Coruña (UDC) back in the day. We were taught OCaml, Erlang and Coq, and a smidget of Haskell that I forgot completely until I looked at it again. Here are some slides about lambda calculus: http://www.madsgroup.org/~quintela/programacion_funcional/teoria/ And here is a thesis about Coq: http://ruc.udc.es/bitstream/2183/1148/1/PerezVegaGilberto_opt.pdf There seems to be a FP Meetup group in Madrid: http://www.meetup.com/FP-Madrid/
see https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci-compiled.html for common pitfalls.
congratulations. i am really looking forward to some documentation on why you choose which types etc.
Thanks!
I'm glad it's useful, even if your background is somewhat limited. One of the reasons I want to do this podcast is so that I can learn myself, and HoTT is hardly something that I'm good at. You may have noticed that I was fairly quiet for portions of the interview :-) I'm not sure that a "type theory 101" episode would work well in an audio-only format. I think a book or a blackboard is almost totally necessary for the basics. Above, Jon recommends some good books, and various lecture series from OPLSS are also good for getting started. I would be interested in episodes that are more in-depth examinations of basic ideas, like meaning explanations, the relationship to philosophical intuitionism, or a critical examination of the propositions-as-types idea. These can touch on issues that are both basic and foundational, but I still think that a certain level of background will be necessary. 
PiMLTT is exceedingly dry, so if your temperament isn't so suited to that kind of work, it might be better to watch some online lectures or ramp up using the exercises in Software Foundations. I see TAPL as more of an introduction to programming language theory in general, rather than type theory in particular. It's got some type theory, but it also has plenty of things that can't really be considered as part of that tradition.
I happen to know Daniel Gorín (http://glyc.dc.uba.ar/daniel/) and these other scholars from South America come to my mind: http://www.fing.edu.uy/~pardo/ http://www.fceia.unr.edu.ar/~mauro/ Maybe browsing their sites turns up something relevant to you.
s/xmonad/Haskell/
It kind of sounds like he was trying to use the notation and terminology of judgements to formalize the notion of assumption. It's kind of neat—your intuitions about assumptions can be readily encoded into this language. Unfortunately, it sounds like he played fast and loose with this and lost the point that assumptions aren't a normal judgmental rule.
I'm not a very bright guy, but that sounds like a terrible joke.
that were my thoughts as well.
Thanks! 
And the C and Fortran ecosystem are are light years ahead of those. It all depends on *what* you want to do. Invert a matrix with 500 million entries? Lapack or bust. Want to do some quick and dirty floating point calculations? You can use just about anything.
I agree with that. Short answer : If you need something quickly -&gt; R If you have more time and need something more durable -&gt; Maybe Haskell Pro of R : amazing ecosystem, you won't make a mistake by using R Cons of Haskell : you never know if you'll hit performance/lazyness issue and if it does, it really hurts. Long answer: I am myself switching (in theory) from R to haskell for simple data-mining/stats and reporting. I've started in march and 10 months later, I'm still using R. I trie to write new stuff in Haskell, but I haven't had time to rewrite anything already written in R. Still working on the ecosystem ;-) However, I don't regret my choice and will eventually rewrite everything in Haskell. 
Except that the Python, R (and possibly some others) use these.
I see this same issue is my new and unannounced packaged. Check out unagi-streams for the latest example. 
Congrats on the new job!
You still might enjoy to play with the IHaskell notebook. [Kronos](http://www.kronosnotebook.com/haskell) is a bundled Mac App.
I must be easily amused...
is there a difference between "unsafelyPerforming" a foreign call, and (if the library author), making the type pure in the foreign import?
Good observation. With `-fobject-code`, ghci will load compiled modules. You have to be particular to get optimization to work with `cabal`. (I've edited this post--originally I said GHCi wouldn't load optimized modules from the present project.) It looks like you *need* to put `-O` in an `OPTIONS_GHC` pragma. Putting `-O` in the .cabal file doesn't work. You also have to pass `-fobject-code` to ghci, either with `cabal repl --ghc-options="-fobject-code"` or with `ghc-options: -fobject-code` in the .cabal file. {-# OPTIONS_GHC -O #-} module Main where f x = 1 {-# NOINLINE f #-} {-# RULES "f/id" forall x. f x = x #-} -- With optimization, f x = x; otherwise f x = 1. main = print $ f 3 ... $ cabal run 3 $ cabal repl --ghc-options="-fobject-code -O" ... [1 of 1] Compiling Main ( Test.hs, dist\build\test\test-tmp\Main.o ) [flags changed] Ok, modules loaded: Main. &gt; main 3 &gt; f 3 1 Presumably "[flags changed]" fires because `-O` isn't set in ghci; apparently the pragma successfully turns it back on. As we can see, the optimized version of `main` is present. But of course using `f` at the REPL doesn't get optimized; that's reasonable. You can even recompile `Main` with `:r` and the rule will fire. Cool! I thought this wasn't possible. Does anybody know if this is on Stack Overflow? I'm tempted to write up a quick Q&amp;A.
Here, take this: [U+22A5](http://graphemica.com/%E2%8A%A5), it's a Unicode symbol ⊥. It'll work pretty much anywhere with proper Unicode support (probably including your text editor, terminal, even compilers of some functional languages). No html *entities* needed.
I know of something sort of similar, called [exercism.io](http://www.exercism.io/). It uses github for user management and you can use a commandline tool to fetch and submit exercises and solutions. After the solution to an exercise has been submitted, other users have the possibility to comment on the code and suggest improvements. In the case one then comes up with a better solution, one can submit it again and it is displayed as a new *iteration* and is open for discussion again. The idea is that this process repeats until one is completely satisfied with ones code. After one has submitted the first iteration of an exercise one has also the possibility to look at solutions of other people. So there is not really a single example solution for each exercise, but rather a range of solutions of other people which have or are in the process of perfectioning their code. The exercises are translated to a variety of different programming languages, where Haskell is one of the languages with the most exercises available (I think around 60, if I remember correctly).
The source is on Gitlab: https://gitlab.com/lysa/lysa
Codewars has about 150 haskell exercises last time I checked.
Ugh, so many typeclasses!
I'm fluent in Spanish and a Haskell newbie. Maybe we can try explaining functors and monads to each other in Spanish. :)
gracias!
gracias!
I wrote `exhaustive` when working on some JSON parsers at work. They are a bit funky, so we aren't using generics for them - but I was unsatisfied that we had to rely on ourselves to at least consider all possible cases (I know, how unlike me). Using `exhaustive`, we now at least get compile time checks that I have indeed considered all possibilities, which is a good start to keeping things correct. I'm also amazed that with a little bit of trickery (OK, several hours of head scratching), we can turn a single heterogeneous list into an n-ary function - without requiring any annotations from the user!
A wife who is a functional programmer. I envy you )
I think you can do something similar with prisms. See [this comment](http://www.reddit.com/r/haskell/comments/1wgob8/bidirectional_patterns_synonyms_with_or/cf1wsh9) by /u/tomejaguar
Alex Clemmer wrote a [comment](https://lobste.rs/s/7w6p95/msft_open_sources_production_serialization_system_written_partially_in_haskell) on this on lobsters.
Just downloaded it, and it chokes on this code: import Data.Vector (Vector, (!)) import qualified Data.Vector as V import Graphics.Rendering.Chart.Easy errors = V.fromList errors' where errors' = [2.0, 3.7, 7.3, 8.0, 9.2, 11.4, 15.2, 17.9, 21.2, 22.9, 24.6, 26.7, 27.5, 29.7, 32.9, 34.2, 36.5, 39.7, 43.1, 44.5, 47.6, 49.5, 52.2, 54.7, 57.8, 60.6, 61.0, 63.4, 64.4, 65.9, 69.0, 68.6, 70.6, 70.7, 71.6, 74.2, 75.2, 75.8, 74.4, 74.9, 75.5, 74.9, 74.8, 75.7, 76.0, 75.5, 77.8, 76.9, 75.5, 74.5, 74.0, 74.9, 75.5, 77.5] chart = line "errors" [V.toList $ V.imap (,) errors] toRenderable $ execEC $ plot chart {- : can't load .so/.DLL for: libcairo.dylib (dlopen(libcairo.dylib, 9): Library not loaded: /opt/X11/lib/libxcb-shm.0.dylib Referenced from: /Applications/Kronos-Haskell.app/Contents/Resources/ghc-7.8.3/dylibs/libcairo.dylib Reason: image not found) -} I see that the ihaskell GitHub page recommends to `brew install gcc48; cabal install cairo --with-gcc=gcc-4.8`, but clearly this isn't the problem here, since it does seem to come with its own libcairo. **EDIT:** Well, I just needed to install Xquartz. I think the Kronos Haskell homepage ought to mention this...
Apologies for this – this is actually a bug. It wasn't mentioned on the homepage because I didn't realize it was an issue :) It was reported a day or two ago on the IHaskell github, and will be fixed in the next release. Thanks for confirming that installing Xquartz fixes it for now, though!
Yes, I used Prisms to do something similar back in https://ocharles.org.uk/blog/posts/2014-06-10-reversible-serialization.html, though I couldn't get exhaustive. Tom's comment doesn't show exhaustive production, it shows exhaustive consumption - but maybe the same approach can be easily flipped around (these are prisms, after all). [`stack-prism`](http://hackage.haskell.org/package/stack-prism) might end up being significant in this approach. However, I do find that the prism approach tends to work well when you are parsing applicatively - because you mostly consider each field one by one. I wonder if it's harder to do an "all at once" parse, which `exhaustive` makes easy (you use a monad and call the constructor at the end).
Field puns are alive and well. Ask what happened to Haskell 2011 instead.
Interesting. We have been using this internally for a while and I never knew it was Haskell based. It's a protobuf for microsoft and has some interesting features like templating. 
Instead of using generics, you could use TH to create a record type that has a field for each constructor.
[It's up for consideration](https://github.com/ocharles/exhaustive/issues/2#issuecomment-69453792), that's for sure. I'm leaning a little more towards quasiquotes at the moment - the big problem I have with TH is that it would be forced to introduce new symbols, which ideally I'd like to avoid.
I think that's what `stack-prism` is doing - right?
I share your enthusiasm. When I was first introduced to Haskell in graduate school, I immediately realized it could solve all of the current problems I had been having with Java. I dropped everything I was doing and began re-writing my thesis project in Haskell, learning as I went along. I've been using it for about 10 years now, and I still remember vividly just how exciting it was, and still is. Every day I use it, I learn something new, and it just is never ceases to amaze me.
/r/dailyprogrammer might also be interesting.
Awesome! I want to ask your opinion on something-- I've been programming for 2 years and been interested in Comp Sci for the past year, and then in the past 6 months developed a deep enough interest to be considering grad school. Now I've dabbled in C, C++, C#, SML and Python (my best being Python), but as I mentioned in my post my most recent language I'm learning is Haskell. So, with that in mind, I actually have a few questions: Is it worth devoting most of my time outside of class to learn Haskell and to make it my most effective programming language? If I don't go to grad school do you think it will harm my chances since Haskell is probably not going to be used where I get a job? I will be focusing on C++ in my Parallel and Distributed processing course, so it's not like I'll completely ignore other languages, it's just that I really want to be that "Haskell guy" in my department. I know only a few others who have played with it a little, and I'm having so much fun learning it because of the complexity of the language... and am fascinated when those complex nuances allow me to write shorter, more effective, and more powerful code. **In Summary:** Is Haskell worth devoting 10 hours a week to for the next (at least) 20 weeks of my life? Will I without a doubt see the benefits of learning it at the end? Right now I am learning simply because of my fascination with FP and Haskell itself.
 import Control.Applicative import Control.Monad import qualified Data.Map as Map import qualified Data.Vector as Vector main :: IO () main = do -- fmap/&lt;$&gt; can be used to lift pure functions to IO [nrows, ncols] &lt;- map read . words &lt;$&gt; getLine rows &lt;- replicateM nrows (map read . words &lt;$&gt; getLine) -- imap f [x, y, ...] = [f 0 x, f 1 y, ...] let imap f = zipWith f [0 ..] -- indices = [ [((0, 0), x), ((0, 1), y), ...] -- , [((1, 0), z), ((1, 1), w), ...] -- , ... ] let indices = imap (\i -&gt; imap (\j x -&gt; ((i, j), x))) rows let asMap = Map.fromList (concat indices) -- Alternatively, as a vector of vectors let asVectors = Vector.fromList (map Vector.fromList rows) Edit: Comments/Explanation
After some experimentation, I think this won't work properly due to the certificate used by hackage.haskell.org. The cert is issued for `fastly.net` and will not be validated correctly. I couldn't find a mechanism to tell stunnel (or nginx) to simply check the certificate's fingerprint. Getting it to work without validation (or with different domains) was simple enough, but that doesn't help much against MITM attacks. I guess I'll play with this some more, maybe you have a good idea to make it work?
There is work being done on the topic of domain specific error messages: http://www.cs.uu.nl/research/techreps/repo/CS-2014/2014-019.pdf Disclaimer: I am doing my PhD on this project ;)
Dash as really amazing. For those of you on Linux and windows(?) there is also Zeal, which is a qt port that works with dash docsets and apparently gets contributes from the dash team itself under the understanding that it won't do any work to compete with dash on OSX. Zeal is much less polished than dash, but it's still invaluable. I use it daily.
I'd rather ask what happened to Haskell 2014. I haven't even seen it bumped to "Haskell 2015". Got any info about this?
Heh, i started writing a haskell program which does that (plus a bit more). Now I wonder if I should continue though hehe.. Will prolly do so anyway :)
In what way? I just built it and the executable works as intended. Do certain command line parameters require external files or something?
Nice. There is also Cabin that is solving the same problem. https://github.com/nc6/cabin 
If you install pandoc with the `embed_data_files` flag, you'll get a relocatable executable with the data files baked into the binary. Same for pandoc-citeproc. 
I'll release something. I think I have an okay API for this.
Great, that is really exciting! :)
I just subscribed to this sub and I'm currently learning haskell for fun with http://learnyouahaskell.com/. I am also subscribed to /r/dailyprogrammer and thought I would give [this](http://www.reddit.com/r/dailyprogrammer/comments/2rye5u/20150110_challenge_196_hard_precedence_parsing/) a go, but, uh... it might be too damn hard for a beginner. Thanks for pointing this page out. I will be spending a lot of time here!
Alright, [it's up](https://github.com/Gabriel439/Haskell-Total-Library/blob/master/src/Lens/Family/Total.hs). Just let me write a blog post about it and I'll upload it to Hackage.
We should add this to the list of screen casts on yesodweb.com. Max: would you be able to send a PR to the yesodweb.com-content repo?
Yeah sure
Absolutely, I was thinking something like this myself. Perhaps the policy should be to attach upper bounds on releases only, and regularly rebuild the sandbox such that cabal can resolve the newest working version, and suggest that as an upper bound.
If you have anything to do with this project can you please fix the font color on that site? Using #777 and font-weight 300 means that it's basically unreadable on my Chromium/Linux without extreme eye strain. (This is not helped by the already ridiculously "thinness" of Lato.)
Thanks for the tip!
This looks intriguing. What I'd really like to see, though, is an example of what happens when you have a partial pattern match. Also, it seems like the text is describing a case where each constructor is "expressed" via a type parameter; what happens with data types like the following? data Foo = Bar Int | Baz String data Foo a = Bar a | Baz a
Neat! I would love to see how elegant it would look if the language was designed for this.
I've just updated colors and fonts of the default theme provided by GitHub. It should be more readable now. 
Thanks for the clarification, that makes perfect sense.
You're welcome!
Thanks for writing this up and packaging the library. It looks great! You didn't mention overlap so I don't know if you realise that `total` deals with that to some extent too. This won't compile: total :: Either Char Int -&gt; String -- Same as: total = _case -- total = \case &amp; on _Left (\c -&gt; replicate 3 c ) -- Left c -&gt; replicate 3 c &amp; on _Right (\n -&gt; replicate n '!') -- Right n -&gt; replicate n '!' &amp; on _Left (\c -&gt; "Overlap" ) -- Left c -&gt; "Overlap" And well spotted that you need a `Void` in the type of `on` to make the typeclass resolution work. 
Would it be possible to tell cabal (using --prefix or similar) to install binaries and data files into ~/.cabal while still building inside the sandbox?
You can pass ordinary Cabal parameters to the script. The newest version doesn't even use `mv` anymore, and specifies `--bindir=...` instead. In the end, the script is really just a wrapper around Cabal; you can even enable profiling etc by adding the parameter.
 Does not pandoc use cabals getDataDir? I'm almost sure I run it with wrapper *pandoc_datadir=some/path pandoc $@* in one case.
I fixed it. Thanks!
I've just posted (one of) [the relevant blog post](http://www.reddit.com/r/haskell/comments/2s3oaw/lazier_function_definitions_by_merging_partial/).
I agree, documentation without examples is very hard to read. doctest can keep the author honest. 
The newer version sets Cabal's `prefix` instead of just moving the binaries around manually using `mv`. I think that covers what you've mentioned.
The new version uses --prefix. I didn't use it initially because I was afraid more than necessary would be moved to the global package space, which apparently is not the case.
Nice. Thank you for sharing.
I guess it's not talked about because it's straightforward to use but of little theoretic interest. 
This looks fantastic. I hope the project is a big success. Exposing explanations of the type invariants would be awesome for DSLs. (By the way, I didn't see a mention of our old friend the *orphan problem* in the report. It'd be potentially bad if, say, importing a certain module changed the error messages for expressions only involving Prelude types.)
Structure preserving with structure preserving inverse. Preservation of structure is arguably a more fundamental notion than category, although maybe once you've got one you have the other and they're essentially equivalent.
You at least need some kind of groupoid though. What else would isomorphic mean? 
He's wrong, though. The heart of type theory is variable binding, which fits very awkwardly into the algebraic style of category theory. (Eg, the usual semantics of simply typed lambda calculus in CCCs models contexts inaccurately. If you try to be more accurate you end up needing multicategories, which are an exercise in mathematical masochism.)
I'm a little confused - you said you'll release something that would do this, but what you've done is the opposite of `exhaustive` - `exhaustive` is about *producing* data, not consuming it. Was that a misunderstanding above?
Hello, what are the collect and the unify variants of HM? Are any of those the [algorithm W](https://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system#Algorithm_W)? I'm trying to search for this, but the [original paper](http://prooftoys.org/ian-grant/hm/milner-damas.pdf) doesn't even have the word "collect" on it..
I find this little hack useful; hopefully others will as well.
Although it has nothing to deal with the library itself, you should consider picking another name for your library. It’s too close to Happstack.
Nope, it's decidedly not algorithm W. Never really got my mind around that one, but then I didn't even try as the other way is so much clearer (and has better error messages): I'm not sure which paper exactly I got my algorithm from, but the general thing is very easy: 1. Collect all constraints 2. Solve them by figuring out the most general unifier, which is a standard logic algorithm. Algorithm W does both in one. [Helium](http://www.open.ou.nl/bhr/heeren-cp03.pdf) uses the same approach and also modern GHC, though the GHC papers talking about it are rather advanced, dealing with GADTs and whatnot. Hindley-Milner, is this sense, is less of an algorithm but a specific type system semantics. I should probably clean up that code and put it up somewhere. Right now, it's not fit for human consumption.
I'm open for suggestions. :) I'm bad at naming, but how about `ExtStack`? Edit: Or `AutoStack`?
Don't presheaf-derived approaches work decently? In particular doesn't change-of-base give a fairly straightforward account of substitution? Or is there another problem you're referring to here?
But that presumes you know what notion of structure you care about up front. Take an example. Are C and R^2 isomorphic? As groups sure. As rings not even close. The matter is much more subtle when you are dealing with something having as many pointy bits as a type theory. It's like McBride says, to paraphrase, equality looks so simple until you start thinking about it. I've read the same problem comes up with the vying definitions of higher category and higher topos. We have a few different definitions that intuitively seem to agree. We would need to formalize that intuition to make it precise. But then we can ask about the next higher dimension... where we run into coherence issues, and so we're never fully satisfied. 
Haven't logged into my account for a while, so I just saw this. My solution is definitely, noticeably faster than stm-containers for this application. I don't just wrap containers, I use a `Vector (TMVar (Map k v))`, indexed by the key's hash. Right now, there are 4096 separate maps, which results in an extremely low contention probability. This isn't a suitable generalized solution, but for what I'm doing here it's better than all other structures I tested (and I tested a lot).
&gt; But that presumes you know what notion of structure you care about up front. Agreed.
Something very like this that prompted to confirm major/minor bump, and also uploaded docs would meet just about every need.
&gt; Eg, the usual semantics of simply typed lambda calculus in CCCs models contexts inaccurately. If you try to be more accurate you end up needing multicategories [...] Can you expand on this or point me towards a reference?
We released a tool called [bumper](http://hackage.haskell.org/package/bumper) that does this, and also transitively updates other packages. It might be useful to people who like this script, as it supports slightly more sophisticated scenarios.
Actually the guy from "what I wish I knew when learning Haskell" is writing [a series](http://dev.stephendiehl.com/fun) on how to write a Haskell compiler and did [this chapter on Hindley-Milner](http://dev.stephendiehl.com/fun/006_hindley_milner.html). He published it this month, that's just too convenient.. On collecting, he says "Instead of unifying type variables at each level of traversal, we will instead just collect the unifiers inside the Writer and emit them with the uni function.", perhaps he is talking about the same thing you're.
Yes, he is.
* https://www.haskell.org/haskellwiki/H-99:_Ninety-Nine_Haskell_Problems And if you want problem-solution / how-to Haskell examples then: * http://rosettacode.org/wiki/Category:Haskell Each problem there contains a solution in Haskell. Just scroll / search for "haskell" each time. * https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/Simple%20examples
I really like the style of documentation you've adopted here. It's explanatory without being redundant. In particular, I appreciate your graceful handling of the TH problem ("Unfortunately, as this function is used via Template Haskell, the type is not particularly informative. . . it's helpful to look at the type of `con` applications:") and the exported internal types ("The following are implementation details, but exported to improve documentation.") That said, one of the issues discussed here last time around was that the previous version only made sense if each constructor was represented by a type variable, e.g. `data T a b c = A a | B b | C c`. I see (and noticed right away from the examples under `con`--thanks!) that `Construction` is indexed by the constructor number. I assume that means the prior limitation has been eliminated?
&gt; This isn't a suitable generalized solution, but for what I'm doing here it's better than all other structures I tested (and I tested a lot). Don't get me wrong, I'm trying to push it on you, but &gt; Right now, there are 4096 separate maps, which results in an extremely low contention probability. That is a probability of `1 / 4096`. This means that if you have 40 concurrent processes accessing this data structure, on average every 100'th transaction is guaranteed to conflict. That is at least. I'd call such a probability anything but "extremely low". Another evident problem with your approach is the memory consumption. No matter how small the map you're trying to represent is, you'll always allocate for a quite big vector of empty maps. The HAMT algorithm, which is behind "stm-containers" on the other hand, treats the memory very delicately and only allocates for as many items as there are currently in the map. So unlike your solution, it allows to write applications which have lots of maps. PS, if you'll stick to your solution I at least advice you to switch to the "unordered-containers" HashMap, since it's more efficient both memory and performance wise, and you're already utilizing hashing anyway.
Can we use a library? import Data.Numbers.Primes (wheelSieve) primesNo :: Int -&gt; Int primesNo = length . flip takeWhile (wheelSieve 6) . (&gt;=) {-# INLINEABLE primesNo #-} main = print $ primesNo 67108864 IIRC, `length` from prelude is strict enough to avoid unecessary massive thunks. I used `wheelSieve` so that each call to primesNo recalculates the list of primes; this is a trade-off but I think it should work better than the alternative for this particular program. This should save about n / lg n comparisons, and I think the wheel sieve should also be faster than the trial division used in the article.
&gt;This means that if you have 40 concurrent processes accessing this data structure, on average every 100'th transaction is guaranteed to conflict. That's a very reasonable contention probability for 40 concurrent threads. Of course, in reality, the server will usually be run on 1-4 physical processor cores, which means that contention is unlikely even with hundreds of green threads. &gt;Another evident problem with your approach is the memory consumption. A single, global 4k vector of TMVars takes probably less than 1% of the memory consumed by the runtime alone. &gt;The HAMT algorithm, which is behind "stm-containers" on the other hand, treats the memory very delicately and only allocates for as many items as there are currently in the map. My approach takes 4096*TMVar + 1 map entry per item. This is more efficient than `stm-containers` for a large number of entries, and negligible for a small number of entries (trust me, I tested). &gt;switch to the "unordered-containers" HashMap, since it's more efficient both memory and performance wise, and you're already utilizing hashing anyway. I'm pretty sure I tested that as well, and IIRC, it was actually slower for a reasonable number of entries. This is possibly due to the redundant hashing of the key (once for TMVar lookup, once for HashMap lookup). Regardless, over 80% of program time is spent on serializing/deserializing, so changing the store data structure won't help a whole lot.
I could not tell you, I am very new to Haskell, I am compiling it with "ghc -O2 primes.hs". If you have any suggested improvements, I would be most welcoming of them. 
Thanks, I'm glad you like the documentation - it was actually quite enjoyable writing it :) &gt; That said, one of the issues discussed here last time around was that the previous version only made sense if each constructor was represented by a type variable I think this is a misunderstanding as some similar (though orthogonal) work came out around the same time as my `exhaustive` library - /u/Tekmo's `total` library. While `exhaustive` is about producing data (e.g., parsing text or JSON into data), `total` is about pattern matching on data (like a normal `case` statement). It's `total` that has that limitation - `exhaustive` has in fact never had this limitation.
I'm not sure where exactly you're drawing the line for whether something counts as "the same algorithm", but here's a straightforward improvement: -- Use an accumulator so you can use tail recursion primesNo :: Int -&gt; Int primesNo = go 0 where -- N.B., we make sure to be strict in the accumulator go acc 2 = acc + 1 go acc n = (go $! acc + isPrime' n) (n - 1) -- This is a suboptimal implementation, -- but should be optimized away isPrime' n = if isPrime n then 1 else 0 And style-wise, I think it's better to use `compare` rather than a bunch of redundant boolean tests. We're not limited to a few built-in types in Haskell, we have ADTs so feel free to use them liberally— especially when it clarifies what exactly is going on. isPrime :: Int -&gt; Bool isPrime n = case n `compare` 2 of LT -&gt; False EQ -&gt; True GT -&gt; odd n &amp;&amp; go 3 Of course, depending on what exactly you're doing, sometimes using `compare` can end up slower than using redundant boolean operations (due to very low-level issues like branch prediction). If performance is absolutely critical, then feel free to benchmark the bit twiddling stuff and see if it helps. But for for most code, you should aim for maintainability and legibility rather than eeking out a few extra cycles.
Yes, I agree with you entirely, that is why (I hope) I make it clear that I am not drawing general conclusions about the speed of the language compared to others, just in this one particular case with this one particular algorithm. In future I plan on looking at different kinds of algorithms, perhaps one ideally suited to Haskell and show how other languages compare to it. 
Looking at the code for the other languages[1], I think the algorithm with the accumulator is a more fair comparison. [1] https://bjpelc.wordpress.com/2015/01/10/yet-another-language-speed-test-counting-primes-c-c-java-javascript-php-python-and-ruby-2/
How do I install it??? Install script simply fails in so many ways... if there is no global ghc and the user is not root... Anyway, any hints how to install kernel driver? I wonder if the author could add `.travis.yml` into his account and compile it on the test server... this would help a lot to understand the build process I really need it to work to play with `HaNS` :) if it is faster then `PCAP` it would be nice... 
I did something similar, except I used a fairly simple Sieve of Eratosthenes. I used Haskell, C, Common Lisp (SBCL), and python. Not surprisingly, Python was the slowest, followed by Haskell, but the CL version somehow managed to be about 10% faster than the C one, with the algorithm as similar as I could make it.
I am actually quite impressed with Haskell in my case, especially in the lower range where it matches C++. For me, the real surprise is that JavaScript on Node is as fast as it is. All the code for the other languages (for comparison's sake) is here in the previous post: [Yet Another Language Speed Test](https://bjpelc.wordpress.com/2015/01/10/yet-another-language-speed-test-counting-primes-c-c-java-javascript-php-python-and-ruby-2/)
I wrote my own Haskell version. It's basically a direct port of your C version, except for the main loops I used some higher level array operations. Here's my code: import qualified Data.Vector.Unboxed as Vector import Data.Word isPrime :: Word -&gt; Bool isPrime n | n &lt; 2 = False isPrime 2 = True isPrime n | n `rem` 2 == 0 = False isPrime n = Vector.all (\i -&gt; n `rem` i /= 0) . Vector.enumFromStepN 3 2 $! (floor (sqrt (fromIntegral n)) - 1) `quot` 2 main :: IO () main = print . Vector.length . Vector.filter isPrime . Vector.enumFromN 0 $ 67108864 + 1 I built the above with `ghc -O2`, and I built your C version with `gcc -O2 -std=c11 -lm`. Here are the results: # time nice -20 ./c_version; time nice -20 ./Mine pi(67108864) = 3957809 nice -20 ./c_version 182.05s user 0.00s system 100% cpu 3:01.97 total 3957809 nice -20 ./Mine 158.66s user 0.00s system 100% cpu 2:38.60 total Perhaps amazingly, this Haskell version takes only 87% of the time that the C version does, on my machine. Edit: I guess I need to explain some of that math, though. Basically, `Vector.enumFromStepN` is way faster than `Vector.enumFromThenTo` (because the latter uses `fromList` for some reason...), so I needed to compute the length of the vector up front. Edit: Added a "on my machine" qualifier to the performance claim...
Hmm, so now I am wondering if there is a name for the function which recovers a `Fix f` from some `Free f` when `Pure` never arises in the structure, e.g. some function toFix :: Free f a -&gt; Maybe (Fix f) since it seems like `Free` is very similar to `Fix`, except that it introduces the `Pure` constructor. Similarly, you could imagine fromFix :: Fix f -&gt; Free f a
Unfortunately not; Cabal needs to play ball and that functionality hasn't been implemented yet.
Yes, it does. 
[What is supercompilation?](http://stackoverflow.com/questions/9067545/what-is-supercompilation)
Supercompilation isn't there yet, and I'd consider it's status in GHC to be "on hold". A few random thoughts: * A lot of the newer JIT optimisers are doing some of the same kind of things, so potentially it is an optimisation time whose idea has passed... * One problem with supercompilation is that it optimises everything to an insane level, which takes a lot of time, when most of your program is not performance critical. Maybe profile guided optimisation or JIT can reduce the space to cover. * The latest resurgence in supercompilation has tackled optimisation, but you can equally apply it to proof, which might end up being a more suitable domain in the end. * The GHC optimisations are quite complex, supercompilation has the potential to make them quite simple, I think that's the ideal. * Supercompilation was designed to go between languages, so you compile from Haskell to ASM, for instance - that's not something people have looked at as much.
It's quite true. Though obviously we can't all just switch to nix, but we can import the nix concepts into cabal.
Thanks for the report!
This should be fixed now. Thanks for taking the time!
Are the keys somehow encoding all GHC options used etc.? What I mean, is whether building the same package with the same version and the same dependencies but with different options will result in a different key? Only then the keys would be useful for cache identifiers. Otherwise, a complex key must be formed which includes the other distinguishing information.
Oh, I confused this with [Super Optimization](http://en.wikipedia.org/wiki/Superoptimization).
Great, just yesterday I wondered why the existing parser libraries don't do this.
alex and happy would be separate lexer and parser generators, similar to flex and yacc/bison http://hackage.haskell.org/package/alex http://hackage.haskell.org/package/happy
Funny, I just wrote it, it's simpler than that, no? unfix :: Functor f =&gt; Fix f -&gt; (forall x . Free f x) unfix = Free . fmap unfix . out voidfix :: Functor f =&gt; Free f Void -&gt; Fix f voidfix = \case Pure v -&gt; absurd v Free f -&gt; In (fmap voidfix f) The obvious isomorphisms are toFix :: Functor f =&gt; Free f a -&gt; Fix (Sum (Constant a) f) toFix = \case Pure a -&gt; In (InL (Constant a)) Free f -&gt; In (InR (fmap toFix f)) toFree :: Functor f =&gt; Fix (Sum (Constant a) f) -&gt; Free f a toFree = \case In (InL c) -&gt; Pure (getConstant c) In (InR f) -&gt; Free (fmap toFree f) which would be clearer if one used `Control.Monad.Trans.Free.FreeF f a` instead of `Sum (Constant a) f` 
Just to add my experience, everything works on OS X 10.9/GHC 7.8 and 7.10 for me. I did brew install fltk, and then installed the bindings. One minor issue I had is that I only had `Cabal-1.18.1.3` installed on GHC 7.8, and your cabal file requires `Cabal &gt;= 1.20`. Since cabal-install doesn't auto-install it, this was a bit of head-scratching and manual installing. 
Haskell generally doesn't give you a way to directly deal with hardware, where C lets you just manipulate low level internals a lot more. (there are probably Haskell ways to do low level memory stuff but they're not central in the language.) That said, there's no inherent reason you couldn't use Haskell. The main barrier is getting the runtime and compiler to work with these platforms. It's possible, but just hasn't been the focus of the community. 
Successful build on OS X 10.10, GHC 7.8.3, Cabal 1.20.0.6, fltk 1.3.3
Yes. This is what /u/ezyang's GHC work is about, as far as I understand it.
This should work in the common case. You should clean up the TODOs, make it toggleable by a flag, and PR it. You won't get a speed up in all cases, because I believe your "cache" will only be able to support one build of any package at a time, so if you build foo against bar-0.1, and then foo against bar-0.2, you'll lose the cached results for bar-0.1. You can easily fix this if you also reorganize the dist directory to be sorted by package key. Ditto with multiple GHC versions, which you comment on.
I don't know CircleCI well enough to say.
Afaik nix insists you use the most recent version of any package from Hackage. (Not due to any inherent limitation, just that that's all that's packaged.) It would be nice to be able to build against a past snapshot of Hackage, or against Stackage, for example.
Eaton Corp. uses Haskell for embedded systems in automotive control systems. Their technique is to write controller software in an EDSL called [Atom](http://hackage.haskell.org/package/atom) embedded in Haskell that gives high-level assurance of correctness and reliability, and then render that code as low-level object code for the controller. They used to use another Haskell-based EDSL called [ImProve](https://github.com/tomahawkins/improve) for controllers of hybrid hydraulic vehicles. But nothing much seems to have happened with ImProve for the past few years. There are links to more information about these things on Tom Hawkins' [home page](http://tomahawkins.org).
In embedded control software, you have tight hard-real-time requirements - your code must finish in the allocated time and never exceed the allocated memory budget. You cannot provide such guarantees for Haskell in general. Regarding garbage collection. C code still does memory management but it's controlled at the low level. In Haskell, you cannot control the garbage collector so precisely. What is an interesting application of Haskell to embedded systems is perhaps using the streaming libraries which have some guarantees about performing in fixed space (like pipes and conduit). If the processing of the stream is somewhat predictable then Haskell should work well. 
Some doubts about monoid-subclasses. If I understand correctly, a **CancellativeMonoid** is not a group because the "cancellation" operation is a *different* operation from **&lt;&gt;**, correct? In strings, you can always delete something you have appended, but you cannot "delete by appending", so to speak. It's interesting that some type classes have *complexity* restrictions. By the way, how come **FactorialMonoid** doesn't imply **GCDMonoid**? Decomposing into factors seems like a "more powerful" operation than finding a common prefix...
Can you think of a simple way to integrate that workflow into the hackage/cabal ecosystem? Extracting the resolved build plan from cabal and then manually verifying and downloading every package from git can be a drag when you're building a package with well over 100 indirect dependencies. That is not uncommon.
&gt; If I understand correctly, a CancellativeMonoid is not a group because the "cancellation" operation is a different operation from &lt;&gt;, correct? In strings, you can always delete something you have appended, but you cannot "delete by appending", so to speak. Yes, that's exactly correct. &gt; By the way, how come FactorialMonoid doesn't imply GCDMonoid? Decomposing into factors seems like a "more powerful" operation than finding a common prefix... FactorialMonoid+Eq would imply GCDMonoid. You need to be able to compare the factors for equality to see if they're common. 
It really depends what it is you need to represent. Binary floating-point numbers like `Float` or `Double` are great for things like data points from a continuous domain, such as audio samples, seismographic data, water level readings, etc., but they are completely unsuitable for financial calculations. As long as you don't have to deal with sub-cent amounts (or whatever the smallest unit in your currency is), you can just use `Integer`s representing cents. `Rational` is another option, but like /u/yitz notes, the denominators can quickly grow very large, which is usually only a problem with arbitrary denominators, whereas for monetary values, you'll typically only have multiples of 10 in the denominator, so you're probably good there. Rational calculations frequently feature renormalization, which may or may not become a performance problem. I've used it quite successfully to represent durations in music notation; *perfect* for that. Decimals, as in `Data.Decimal` or [`Data.Scientific`](http://hackage.haskell.org/package/scientific-0.3.3.5/docs/Data-Scientific.html) are probably the best option.
Others have given some good answers as to why Haskell isn't currently useful for embedded systems- I think the biggest hurdle is that functional code doesn't run natively anywhere, hence the Haskell runtime system, which, as /u/jringstad mentioned, takes more memory, cpu, etc. A cool project that aims to skip the whole "run Haskell by running C" is the [reduceron](http://www.cs.york.ac.uk/fp/reduceron/) project. The idea is to do graph reduction (which is what "executing" a Haskell program entails) in hardware directly. [github page](https://github.com/tommythorn/Reduceron) where the current project is maintained, but it needs some active input.
Just an additional thought regarding my confusion. I wonder if there's better terminology for what `exhaustive` guarantees. I don't know if "exhaustive" has a technical definition, but I think of the exhaustiveness checker, which for some `a -&gt; b` can tell you if there's a problem on the `a` side. It's a contravariant thing. Whereas `exhaustive` is about a different type of exhaustiveness, something like test coverage. It's about *potentially* producing every variant of `b`, in the `Parser` examples.
Presenter here. I covered the same ground as this talk in a [recent post](http://joelburget.com/react-haskell/) which includes more details and live demos. Some thoughts on the state of react-haskell * I'm building it for another project. Adding features as I need them. That's why I dove deep on animation (and continue to). I'm also starting to think about if / how to do routing and embedding react classes. * The class abstraction is really, really poor at the moment. I previously hadn't used React classes in this project (instead rendering only `DOM.*` elements outside of a class), but decided I needed them when adding animation and using `window.requestAnimationFrame`. I built only the bare minimum acceptable to achieve animation. You can't even render nested classes at the moment. Building a clean semantics for classes and documenting them fully is one of the next things I'm going to do. * In that vein, documentation is pretty poor right now. The post I link at the top is the most up-to-date / comprehensive coverage. I'm always working on the hackage docs and will add more in-depth posts in the future. links: * [github](https://github.com/joelburget/react-haskell) * [hackage](http://hackage.haskell.org/package/react-haskell)
In an effort to improve the efficiency of finding interfaces with Hoogle, here's a [small patch](https://github.com/ndmitchell/hoogle/pull/88) adding keyboard navigation for search results (similiar to that used by [`Rustdoc`](http://static.rust-lang.org/doc/master/std/index.html?search=fmt). The up and down arrows, as well as enter keys are currently implemented. I would like to know users' thoughts: Do these keyboard shortcuts get in the way? Can the bindings be improved? What other papercuts have you found while using Hoogle?
Nope, I updated it and you're right, I just had "old" formulas around indeed =)
Author here - thanks for the feedback :) * I haven't looked at blaze-react yet, planning to. * "`react-haskell` seems to focus on animation" - I worked heavily on animation for a couple weeks over the holidays because I needed it for another project. I don't plan for it to always be so animation focused - the library will become more balanced as I need other functionality and as other people contribute. * React-Haskell's current API for constructing classes is gross and will change in the future. I haven't yet decided exactly what I want classes to look like. * "I imagine there's some implementation reason to use the `ReactKey` type functions" - because I found it cumbersome to write `React state animState signal ()`. Those first three types are related, so I replaced them with one type which maps to the three. Now we can write `React key ()`. * "I don't see how to construct `ReactNode`s for `ReactT`". `ReactNode`s are an implementation detail which I should hide from the docs. You describe the document you want in the `ReactT` monad with `div_`, `class_`, etc. This description lives in a react class which is put on the page with `render`. Sorry about the state of the docs, I'm working to improve them.
Yups.
Interesting, I have just compiled your Haskell code and and done a few timed runs and I am getting pretty much exactly the same times as the code posted by winterkoninkje which is only very slightly quicker than the original code I posted. Quick question, are you on a 32-bit system? 
See my answer to mn-haskell-guy. I'm on a 64-bit system. I'm also confused. It isn't clearly my gcc, because I got a similar result against clang. The best I can figure is that it's my cpu. 
My results are more in line with mn-haskell-guy. ghc 7.8.4 64-bit (compiling with -O2 flag) and gcc --version "gcc (GCC) 4.9.2 20141224". I'm running Arch (Kernel 3.17.6-1 64-bit) on a virtual box with 2 x 2.8GHz i5 cores and 4GB RAM . 
Nevertheless, there are still options where javascript (https://tessel.io/) and python (https://micropython.org/), which langs have also eliminated "place". Whether these options are truly considered "embedded" is another question, but it would be interesting if there was an option in this space for haskell as well
I have successfully compiled Idris to run natively on the Arduino, https://github.com/stepcut/idris-blink There is still more work to make it practical. The first major piece is to implement a garbage collector that is better suited to low memory and realtime requirements. Also, Idris now has uniqueness types which could be useful for things like proving that certain sections of code (perhaps interrupt service routines) do not allocate any memory. 
I think I'm curious to see a run on more AMD hardware. I may look into this more tonight. 
Good timing! Rust has an alpha out now for the 1.0 version of the language. I look forward to replacing low level "cbits" with memory safe "rustbits" in Haskell projects.
Perhaps it is something to do with the nature of the Haskell language that attracts a certain kind of person but so far I am finding the Haskell community one of the most engaging and inquisitive. It is making learning Haskell quite a joy. 
If you want to correctly handle money you must use a fixed point representation and also know about basic accounting rules in your country.
hledger is using Decimal for this, and it's working well.
One thing you _can_ get out of category theory (not everyone who studies it takes this away in the same way, apparently) is a more systematic way of thinking about the relationship of code and data, and particularly about moving between the two. You also get facility with a number of ways of thinking about commuting diagrams, and the idea that certain sorts of operations should "transpose" in certain ways. I don't think it gives any immediate day-to-day benefit, because lots of the basic ideas we can take away have been specialized and translated back into non-categorical language. So that means that what you'll get out of it really depends on what you put into it and how you apply it -- there's no fixed answer here. That said, if you're hitting a point reading various papers and literature and finding _they_ lean on category theory, then that's a very good motivation to dig in. I.e. if you want to understand more deeply any of the work on "initial algebra semantics" (to name something that's been studied for a while) or "effect systems" (which is a more in-vogue topic with roots in category theory) or etc. Also, if you're interested in any sorts of linear type system semantics, or want to understand some more recent work in modeling concurrency, etc. At a certain point you'll hit papers that have a fair amount of that language, and at that point you'll get a sense of how much more background you want in order to follow along more closely.
Cool I had not thought of that possibility, thanks for the update! Let me know if you end up making a new graph.
At the moment I favour attoparsec for tokenizing and parsec for parsing, wrote [about an example of that here](https://www.reddit.com/r/haskell/comments/2r3jww/lexical_analysis_with_parser_combinators/cnd3ddn). This picoparsec is an interesting new development. 
Dead link.
Haskell needs a runtime with a GC (or [something with the same capabilities](https://stackoverflow.com/questions/9952602/does-haskell-require-a-garbage-collector)). In a low level language with no runtime like Rust or C, your program need to keep track of every variable on the heap and free its memory when the variable is not needed anymore. In C you need to literally call `free`, in Rust the type system [keeps track of ownership](http://doc.rust-lang.org/intro.html#ownership) enabling the compiler to insert free calls when they are necessary (but the end result is the same). In Haskell (and other languages implemented with a GC) there is a piece of code responsible to scan your objects to determine which ones aren't needed anymore - a [tracing GC](https://en.wikipedia.org/wiki/Tracing_garbage_collection). Generally speaking, a tracing GC works like this: while your program is running, there is a list of "root" objects (objects you know a priori they are needed, like globals), which can point to other objects and so on; if you start with the root and touch all objects it can reach, any object left untouched is not reachable by your program and can be deallocated. Of course, actual GCs are more complex than that, but they have one limitation: it's hard for data structures not managed by the GC to have a pointer to data managed by the GC. This is because when the GC runs, it may free an object that can't be reached by the GC root, even though it's pointed at by the foreign data structure. Also, for performance, the GC can rearrange the data it manages at any time (rewriting all the pointers of the objects it manages) - that's called a generational GC. This means that either you write some awkward interface between the GC and the code that doesn't use GC, or you manage your entire kernel / embedded system / low level program with your GC. Microsoft had a research project to do the latter - an OS called [Singularity](https://en.wikipedia.org/wiki/Singularity_%28operating_system%29) that relies on GC for managing all its data (it also ran all user programs at kernel level, relying on software to enforce security)
That is interesting, I also got no speedup by explicitly setting the type declarations within the code to `Int32` (using `import Data.Int`), which is what I tried before going the other way and converting C to `long int`. I tried a couple of days ago to use LLVM but mine too was too new for my current GHC. 
No it doesn't, but we generally don't bother maintaining a bunch of versions in nixpkgs for each package. If you need `text-0.1` for some reason, you can just run `cabal2nix cabal://text-0.1 &gt; old_text.nix` and import `old_test_nix` in your project. We do have a couple of ghc/cabal versions though.
Thanks for mentioning this! Try again now. It seems that the behavior of the `keypress` event is less than consistent between browsers. For this reason it seems common to use `keyup` or `keydown`.
Yes, binary floating point is totally unsuitable for financial calculations. Yet, that's what Excel uses. 
&gt; "react-haskell seems to focus on animation" I apologize if that came across as a criticism, by the way--not my intent. Animation is cool. &gt; I found it cumbersome to write `React state animState signal ()` The usual approach is to use a type synonym to fix the variables you use in a given project, like `type MyR a = React MyState MyAState MySignal a`. The deeper reason is to make writing polymorphic code easier. If the type parameters are `animationState signal a`, you can express (and enforce) that a given value doesn't depend on the animation state, for instance, by giving it the type `forall a. React a SomeSignal ()`. &gt; ReactNodes are an implementation detail which I should hide from the docs. You describe the document you want in the ReactT monad with div_, class_, etc. Oh, I see what's going on--there's a whole bunch of `React.*` modules that you haven't exposed in the .cabal file, so they don't appear in the docs. Any particular reason?
Theres also an argument that binding libraries are sub-optimal especially for systems which are moving targets, and instead we should be inlining code directly in the application - https://www.youtube.com/watch?v=pm_WFnWqn20 
The only question is: Do you ever have to run `cabal clean` in order to get a correct build? If no, then we're fine, as this method is equivalent to going into the source package and running `cabal install` after it has been run before, just like you would do if you `git pull`ed. So the correctness of this caching method is equivalent to the correctness of `cabal install` - if that one sticks to its promises (I think `cabal install` tries to guarantee correct installs independent of what is in the `dist/` directory) then the cache will too. @ tibbe, dcoutts, 23Skidoo, you have a better overview on current and past cabal issues. Are you aware of any situation where a `cabal clean` is needed?
Two questions: 1) The problem you mention only leads to cache invalidation when we could avoid it, not wrong builds, right? 2) Correct me if I'm wrong, but won't I get the speedups on all modules in `foo` that don't (transitively) depend on modules changed in `bar`? Or is the sole bumping of the version of `bar` enough to have all modules of `foo` recompiled, no matter if they use code from `bar` or not?
Any code to share that illustrates your reply?
You will get wrong builds only if Haskell's recompilation checker messes up. Which happens pretty rarely, but could lead to some hard to debug sandbox problems. They need to use code from bar, but if they use anything (no matter how trivial, and even if its actual implementation has not changed at all) recompilation is necessary.
I'm not familiar enough with nix to say this with confidence, but won't the kind of caching that I propose (`ghc --make` based) give speedups in cases where nix doesn't? If I understand it right and nix caches at the package level then one package being changed will result in all modules of all packages that depend on it to be recompiled, while my method, working on the module level, would only recompile the modules that were changed. (I asked a relevant question further up in response to ezyang's post.)
That sounds correct to me.
Unfortunately you'll find that the links point to `file:///` URLs due to how the Haddocks were originally generated. That being said, as a proof of concept it gets the point across.
If we only look at the usual "polynomial" types that make up algebraic data types? Then no. On the other hand, if we take a broader view, of Joyal's combinatorial species rather than just the usual polynomial types, then the answer is, surprisingly, yes. We can define so called 'virtual' species, which permit many such operations, as well as a combinatorial version of a logarithm! For more information you'll probably find Brent Yorgey's pearl on the topic of species somewhat more accessible than almost anything else written in that space: http://www.cis.upenn.edu/~byorgey/pub/species-pearl.pdf
JIT can also be in pure runtime -- you just need way more tricks to instrument the code on the fly :-)
This is an interesting question with many answers! The answer you stumbled into is a good one. There is a lovely paper by Marcelo Fiore and Tom Leinster, "Objects of Categories as Complex Numbers" (http://arxiv.org/abs/math/0212377) that provides a generalized version of what you have observed -- we can operate on types as _though_ they had subtraction and division, and as long as our result has neither, then there exists an "honest proof" of the same result. This blog post puts this result to actual useful work even :-) http://chris-taylor.github.io/blog/2013/02/11/the-algebra-of-algebraic-data-types-part-ii/ So thinking in terms of (1 - a) doesn't necessarily make sense. But we can use subtraction and division in passing in the course of other proofs and it works out fine. What about negative types as such? James and Sabry have conducted work (http://www.cs.indiana.edu/~sabry/papers/rational.pdf) on a type system that would treat them explicitly with an interpretation in terms of information flow in the course of reversible computation. This is very pretty stuff (and they give good references to related work) but there are no "real" versions of this system around, and I don't recall even any very complete toy ones. I have also seen it described by /u/byorgey that the theory of combinatorial species (which can be seen as one way of generalizing types) provides an interpretation of negative species, although it is reputedly rather tricky to comprehend. (edit: there's also some related discussion on SO here http://stackoverflow.com/q/9190352/371753)
Excel isn't too surprising. Informatica on the other hand ...
We do lose quite a bit, actually. For example, Rust has parametric polymorphism, but you can't expose this directly to Haskell, as a corresponding C signature will have a concrete instantiated type. Rust has a typeclass-like 'trait' system, but there's no way to expose `fn foo&lt;T: Ord&gt;(x: T, y: T) -&gt; Bool` in Rust as `foo :: Ord t =&gt; t -&gt; t -&gt; Bool` in Haskell. Rust has Haskell-like algebraic data types, but there's no easy way of exposing those in Haskell-land (although clever hackery involving ViewPatterns and PatternSynonyms might get you somewhat closer.) Rust has its own calling convention, and while the ABI isn't stable yet, I can imagine some point in the future adding support directly into GHC for `foreign import rustcall`. But this example shows that we already have a safer alternative to C for low-level code that works today, which is a pretty good start!
[Interesting](http://blog.sigfpe.com/2010/05/optimising-pointer-subtraction-with-2.html) 
I'm in the process of whipping up a library for virtual-dom. I have very little time to work on it though. 
For Haskell to be adopted widely, it needs less scary errors. It feels like shit to be working on something and then just have to stop and try and figure out what that 30 line high type alias expanded error even means. While the clock is ticking. And money is trickling... feels very edgy. And yes, I used Haskell at work briefly in a company which is very used to using 30-year old tech like Cobol. But later I canned it in favour of gouging my eyes out with Java because it felt less risky. Edit: canned Haskell? No way?! Downvote!
On my system is 8.0M. Looks like is not build statically then?
Ahh, I'm not the first one to experience this then. Thanks.
Fixed
I hope it helps. I'm sure this is quite a showstopper for some Fedora users.
I was wondering if it is possible to use the llvm backend to create a kernel module for linux. Output an `.o` file and a header and link against that statically from a kernel module.
float is unsuitable for accounting calculations, Decimal works well for that (Rational is pointless there). For anything more complicated I think float is a pretty good compromise in most cases.
Haskell is as scary as you make it. If you're working in an environment where you can't afford to hit a stumbling block, then keep the types simple. 30-line type aliases can be useful but they are not the only way to write Haskell.
Super helpful for me -- all stuff that I almost knew, but everything came into some focus after reading.
Awesome thank you!
I just wanted to say that the ideas here in this post are pretty damn awesome. I, for one, would kill (well, at least maim) for easy access to the kinds of type information you currently make available in the FP Complete IDE in other contexts.
Is HsCenter's info so much better then what you get from a fully pimped out [emacs as Chris Done has shown](https://github.com/chrisdone/emacs-haskell-config)? I should have a look at HsCenter again!
&gt; Java [...] feels less risky Yups, and it feels like this to many: very common for a mainstream technology. When I have to navigate through 5kLOC+ that is unknown to me, in order to maintain/refactor it a bit -- that's when Haskell feels a lot less risky then Java, to me.
I'm very interested in the distributed computing effort. I cannot justify use of Cloud Haskell for myself at the moment, but in day-to-day coding the language components I still miss while using Haskell universally arise from Erlang. Even without actual distribution, having the ability to manage and create "process-driven" applications is interesting and valuable. I've thought several times about trying to write "OTP in Haskell" atop local threads alone, even. Of course I have no idea where FPComplete is going in these efforts, but I'm glad to see that there is a focus in that space. It would be a very exciting place for Haskell the language and Haskell the community to shine.
Could you write all this down into a proposal to the mailing list or in a GitHub issue. I want to pursue it but I don't quite have the time now. If you write it down as one coherent argument in one place it's much less likely to get lost.
The difference is you get one of these things at compile time before your code goes into production, and the other after things have blown up on a customer.
Da.
We're still analyzing alternatives. At this point, we're comparing Cloud Haskell with lower-level, custom-written alternatives to see how robustness and performance play out. Nothing we have right now is ready for prime time, but I'll share results when we have them.
&gt; For Haskell to be adopted widely, it needs less scary errors. That's probably true, although it's gotten better already, I think. Skolem isn't a word most programmers know; having it in the error message doesn't really help. That's not the only example of jargon in user-visible error messages. Other than that, many Haskell errors are scary just because of their length which is mainly due to &lt;expr&gt; in &lt;expr&gt; in &lt;expr&gt; in equation/case for &lt;identifier&gt;. I think with accurate line / column numbers, most of *that* text could be elided entirely. Figuring out what type aliases to expand (or not) and re-collapse (or not) is tricky. Done right, it will greatly improve the readability of error messages, but I actually think there is lower-hanging fruit to make Haskell errors less scary without losing precision.
And, that's a *big* difference. Before your product/app/feature is in production being "drowned" in errors doesn't feel good, and most of the time you only fix the first one, and 60% of the time you only needed to know the line / column number and you'll see the problem yourself. After your product/app/feature blows up in a client's face, you want a wall of data that will give you all the knowledge you need to prevent such "explosions" in the future. I'm not sure it has a rational justification, but my gut agrees with the sentiment.
Предрекаю скорую гибель от нехватки авторов.
btw, is FRP considered ready for adoption, or is it still in design-space exploration stage?
aaronlevin is talking about scalac errors, while you're talking about runtime errors. While Haskell code has less runtime errors, the ones you get are harder to diagnose: a Scala incomplete match will include a scary but useful stack trace, a Haskell incomplete match will include no such thing (only the line number if you're lucky). Disclaimer: I might be missing some better Haskell debugging tool (I know ghci debugging, that's not great, thanks), but I think the difference is still there (even because tracing the control flow lazy code feels harder, or at least very different).
Note to other commentors: he's referring to runtime errors in scalac, not in Scala programs. I haven't seen a clearly related answer yet. Back to answering: I agree with the feeling (the Scalac team learned my name from my bug reports, so I've lived them), but Scalac is stable enough on many "common" programs (say, what you learn from Odersky et al.'s "Programming in Scala" or PiS). That is, while I, you and authors of fancy libraries live in Scalac hell, many Scala users are fine with the stability. They still probably suffer the language complexity and edge cases, but I guess PiS documents enough of it for many to limit the frustration.
Can't it be both? ;)
I think you will run into performance issues appending elements to the end of your state. I would recommend either prepending and then reversing before interpretation, or using something like Data.Seq.
Oh, haha - glad it was of help :D
You can actually use GHC + patches like the HaLVM does to write kernel modules in Haskell. http://tommd.wordpress.com/2009/09/13/kernel-modules-in-haskell/
Although I don't really have a raging Marxist part of my brain, I agree with /u/jberryman: the content, message, and tone of this post are all *exactly* what I want the Haskell community to be like. Bravo.
From what I remembered Miranda's approach was to try more specific patterns before trying more general ones. This seems to be something of a dual approach with regards to bottoms, which finds a solution strict enough that the differences of pattern ordering doesn't matter. It wouldn't solve the problem of the left most argument being evaluated first. This approach seems to be specifically about that kind of reordering, trying to find a non-strict enough function that the order in which arguments are given does not matter (through concurrent evaluation of its arguments). One problem with this is that you would lose lazy evaluation to some degree, but this may not matter if you can show that your best case is no worse than a constant factor of the lazy version.
well, tbh, I don't want to be an early adopter of FRP while it's still in its infancy... I'd rather its wrinkles to be ironed out before considering it ready for consumption...
previous discussion: http://www.reddit.com/r/haskell/comments/2qxmbm/type_families_make_life_and_free_monads_simpler/
Thanks. Wonder why Reddit didn't find it when submitting.
This gives me a bit more confidence, and I'm looking forward to reading your book... :-)
&gt; Predicting a quick death from the lack of authors
&gt; Lack of Russian speaking haskellers? Or is it because they'd prefer a different site
I've found the opposite. For a while I found myself spending time trying to figure out what part of what I wrote isn't behaving like I expected... only to eventually break down and read the error message closely and find that it tells me exactly what's wrong.
Also, *please* don't make error messages less scary by making them less useful.
&gt; Figuring out what type aliases to expand (or not) and re-collapse (or not) is tricky. It seems to me a big win of comparative ease there might be tooling to better explore synonyms for a particular type.
Both. Well, there are a few on Habr.
The plan is that it will mostly be Java and Javascript, with possible snippets in C++ or others. Most likely no Haskell. We're going after a mainstream audience to teach functional programming by stealth.
A couple of points: * You're presuming that there's a performance problem. If that presumption is wrong, then your suggestion is irrelevant at best. * Let's say there is a performance problem, optimizing code by reading it seems rather irrational when optimization is fundamentally an empirical exercise, i.e., profile your code, figure out the slow bits, and then optimize. As best said by D. Knuth: "Premature optimization is the root of all evil (or at least most of it) in programming"^1 1) http://en.wikiquote.org/wiki/Donald_Knuth 
I don't think you need anything more than what I assume Sublime Text 2's Haskell plugins have to offer, really! Are there any specific features you're looking for that it doesn't already have?
I don't expect him to change anything until there is a problem. I don't see anything wrong with giving him a hint where to look first. EDIT: Thanks for your input though.
The two easy windows IDE are: - http://carymrobbins.github.io/intellij-haskforce/ - http://eclipsefp.github.io/ although it has an older interface, the eclipse version is much more mature. In both cases there are a few installation steps, but they are quite easy. You get type information on cursor, autocompletion, and live compilation. With eclipsefp you also get hlint integration and easy unit testing. The "downside" with those is that you have to work with cabal projects. The only way I know to get the same features for scripts is to install emacs with haskell-mode, ghc-mod, company-ghc. It is a bit harder if you don't know emacs.
Have you tried [Leksah](http://leksah.org/)? There is a 64bit Windows MSI linked to in the [downloads](https://github.com/leksah/leksah/wiki/download). Install the latest [Haskell Platform](https://www.haskell.org/platform/) first. The first time you run Leksah it will pop up a dialog box that might be confusing, but the default options should work fine if you are not sure. Use Package -&gt; New... menu item to make a new Haskell package.
Is SublimeHaskell actually any good? Their issue tracker implies that getting it to work at all right now is pretty iffy.
Thanks, it works now. One thing I'd suggest is to disable scrolling by keypress when in this mode. I think Google has this behavior in its search results, and I think it works quite well.
&gt; suspicion, where "increasing the commercial adoption of haskell" is code for "create evil walled gardens around the fruits of the haskell community, which we make more Enterprisey and pitch to big corps". Even that wouldn't be a bad thing. For one, it increases the market demand for your Haskell skills.
I have lots of experience with React. Very little with Angular and Ember. Just comes down to what I know and want to use. That said, I don't know how you would bind such big and "webby" frameworks.
Excited for this book, but I'm not sure considering all aspects of FRP would lead to a coherent picture, especially for someone also trying to come to grips with FP at the same time. A viewing of Controlling Time and Space: understanding the many formulations of FRP" by Evan Czaplicki https://www.youtube.com/watch?v=Agu6jipKfYw makes it clear there are several forces in opposition here. Still, if it's to get better books like this can only help the situation by further spreading these ideas. 
Yes... This is a tricky assignment. That resource should be very helpful - thanks.
You're right. I'll have some angry hornets delivered immediately. To show I'm serious. :)
Emacs with ghc-mod and company-ghc provides great auto-completion. You may check my configuration and adapt it to your needs: https://github.com/Denommus/emacs-config
Yes i agree that Sublime is fine. It definitely does everything I need it to do. I'm just annoyed with the whole unregistered reminder every now and again. Is Notepad++ an easy install? I'd look into the program right now but I'm probably heading to bed in the next couple minutes.
Are any libraries involved?
&gt; I'm in a computational physics class that is based on the Haskell programming language. Is the course-material online by any chance? I'd be curious to know which packages are used for the exercises...
I think it's the C preprocessor. If you're on *nix box with a compiler, it's probably the command `cpp` and does that you'd expect.
The [mentioned google plus post](https://plus.google.com/u/1/115504368969270249241/posts) has some more comments.
Oh ok I was trying to figure out how C++ could be thought of as a "bizarre string concatenation language"
I can only agree with this. Many of the changes made to ghc and to libraries are not backwards/forwards compatible when they actually could be if they were done a little differently.
&gt; /= FTFY
Was this pain ever resolved in Haskell? 
Vazhny ne bukvy, a dukh!
No, it's ongoing. But the community has learned a lot about the problem by now.
I really don't see the problem, honestly. And I don't get the hate for CPP in general. CPP solves these kinds of problems well, and is sufficiently powerful for what it does. This may belie my sordid past as a C programmer, but frankly complaints about CPP being unreadable or difficult to me say more about the programmer than CPP. Said less charitably, if you have trouble reading CPP it probably means you don't read enough CPP, not that there's some fundamental problem with the language. Someone in that thread suggested that we should identify the most common CPP use-cases and integrate them into the language. I disagree. This mostly has to do with portability and separation of concerns. The entire *point* of CPP is that it's separate from the compiler, and thus not beholden to its whims and changes. I also would like to remind everyone that despite GHC's de facto status as the "standard" Haskell compiler, Haskell is not GHC. I have used CPP in the past to ensure that my files were useable with other compilers (notably Hugs, back in the day). People who develop other experimental Haskell compilers like JHC or whatever probably don't want to have to integrate a whole host of special compile-time language features into their compilers just because some people are allergic to learning a widely deployed and widely understood standard preprocessing language. Assuming that you could cleanly integrate 99% of what CPP is used for into Haskell, you'd need to get it into the standard, wait for the various compiler-writers to implement it across the board, and *then* you could start using the replacement -- while still needing to fallback on CPP for the remaining 1% of cases. Or you could just use a solution that exists today and works very well for what it does. Alternatively -- and this I might actually support -- you could build a more Haskell-friendly macro language that is *separate* from Haskell, which like CPP works as a preprocessor. I do not know what the real benefits of this might be, but the drawback is obvious: everyone has cpp, and your hpp or whatever is going to be an extra dependency (also known as: why I still use make and not your newfangled make replacement that weighs in 200 MB and has three-thousand extra dependencies that invariably are not installed on the machine I want to build my code on). But perhaps someone can give a compelling use case that couldn't be just as easily solved today with CPP + Template Haskell. Until that happens, my view is that all this is griping for griping's sake.
И штанга!
[Sublime Text](http://www.sublimetext.com/) with [SublimeHaskell (hsdev tree)](https://github.com/SublimeHaskell/SublimeHaskell/tree/hsdev) is the best compromise for me. However, the [options recommended by /u/sbergot](https://www.reddit.com/r/haskell/comments/2sh4b3/haskell_idetext_editor_for_windows/cnpfljo) and [FP Haskell Center](https://www.fpcomplete.com/business/fp-haskell-center/) are also good ones!
I'm not sure I totally agree. Or maybe I do, depending on who "we" is in the conversation; people that maintain widely used libraries, particularly ones that are in the Haskell Platform, have a heavy burden to ensure backwards compatibility with new releases. On the other hand, people that don't mind only targeting the latest release of GHC can shrug, change a few lines of code, and release a new version of the package with a bumped version. If people properly specify package constraints in their code, then code for old GHC releases can typically just use the old package version.
There's research into doing type checking and static analysis of code with IFDEFs. Testing is also an issue because you have to enumerate all possible variants and test each individually. There's research on that too in the product-line community and feature-oriented development community. For example, FeatureIDE and work of people like Thomas Thüm, Sven Apel and Christian Kästner. In general, IFDEFs have low barrier of entry and they are perceived "simple"; however, they create difficult and costly problems in maintenance and testing later on. The current trend in embedded software where you have many variants of the controller, for example, is to move all static variability from `IFDEFs` to dynamic variability using `if` whenever possible. Then use constants and rely on the compiler to optimize away dead code and remove the run-time penalty. Partial evaluation of code at compile time helps a lot in this as well.
The established convention by important Haskell packages actually seems to be to support the latest three versions of GHC, rather than only the latest two.
&gt; Until that happens, my view is that all this is griping for griping's sake. (This kind of statements are unnecessary in civilized discourse.) I maintain several libraries you probably use. You are getting worse libraries out of me *today* because I need to waste my time on adding and maintaining CPP code with releases of GHC and other dependencies, instead of improving these libraries. There are several concrete problems with CPP that affects our daily coding: * CPP is a *C* preprocessor and has rules from the *C* language (e.g. to do with lexing) in it. Our use in Haskell is relying on unsupported behavior that only works in GCC's implementation. Our CPP code breaks on Clang (which is what Apple ships on OS X), which is not bug-compatible with GCC. For example, this Haskell code doesn't work on Clang: {-# RULES ... #-} because the second line looks like the start of a CPP macro. We had to patch a bunch of libraries to work around this issue. GHC still doesn't work 100% correctly on OS X (https://ghc.haskell.org/trac/ghc/ticket/7678). * Code that uses CPP bitrots because we can't reliably test all combinations of `#if` statements and thus the compiler can't tell us about something as simple as a missing important on GHC version X that's not needed on X+1. We try to mitigate this by using build bots with versions of GHC, but code still breaks. &gt; I have used CPP in the past to ensure that my files were useable with other compilers (notably Hugs, back in the day). We recently removed support for e.g. Hugs in a bunch of core libraries, because it had bitrotted. :) CPP might once had been accepted by programmers (back in the day with N different Unix versions, at a time when autoconf was thought to be a sane thing). Today programming language users are used to ecosystems where things don't break all the time so they can focus on their tasks at hand.
No everything is on campus. The professor is currently writing the book for the class, so he is refraining to publish any of the content until he's finished with the book. 
I don't think so, cargo is pretty much 'cabal sandbox' + 'cabal freeze'. 
It's not just you. from https://status.haskell.org/ : &gt; Haskell.org is slow Wiki seems to be the culprit
Oh. Didn't know there was a tool like this available. Thanks.
&gt;(This kind of statements are unnecessary in civilized discourse.) They are perfectly reasonable in civilized discourse; complaining about my tone because I'm expressing an opinion you disagree with doesn't advance your point one iota. Getting on to your real arguments, which are not without merit: &gt; CPP is a C preprocessor and has rules from the C language ... Your point about CPP eliding C-style comments and not Haskell-style comments is well taken. However, if the only place you encounter problems with this is when you place an ending pragma at the beginning of the line, it seems this is a fairly superficial drawback. Put another way, is the drawback of forcing your users to install a different preprocessor in order to build your libraries greater than the drawback of not ending your pragmas this way? &gt;Our CPP code breaks on Clang ... Yes, this the unfortunate drawback of using a standard tool that has many implementations. As you noted, we've been dealing with this since time immemorial. Different versions of make, different versions of sh, different compilers, different OSs, etc, etc, etc. They are a huge time sink, admittedly (that's why autoconf was invented). But again, you can avoid this by not formatting your pragmas this way. &gt;we can't reliably test all combinations of #if statements This issue is unrelated to your choice of preprocessor. Any time you preprocess code you will have these kinds of issues. If you aim to support several different environments and compilers, you will always have to test on every permutation of platform. This will not stop being the case if you get rid of CPP and replace its functionality with something else. &gt;Today programming language users are used to ecosystems where things don't break all the time so they can focus on their tasks at hand. I'm sorry /u/tibbe but I think you may be living in a fantasy land. Most programmers who do development in a language that targets several different systems who desire portability wrestle with these issues. Even if the one-stack world of .NET where you need only worry about one OS and one vendor developers need to deal with versioning issues. It's a pain in the ass and I feel your frustration, but developing a Haskell library -- even with CPP and all its warts -- is *significantly* easier than maintaining open source C libraries back in the day, and just about as difficult as maintaining librairies in popular languages today. These are problems *everyone* has. Getting rid of CPP for the most part probably means replacing it with something else, adding a new dependency for your end-users. It's not surprising that you've stopped supporting Hugs, or older versions of GHC, or that OSX has problems. These are the trade-offs we make and there is no panacea.
If we had a Haskell-specific preprocessor, it would be bundled with the compiler, I imagine, so it would be easy to obtain and build.
Bundled with which compiler?
Nice! These are always tricky to write by hand. Perhaps they could be merged into monad-control proper at some point?
I've already seen a few packages that do it that way, but I guess it causes less dependency-pain for downstream packages, if the same major package version supports multiple GHC versions...
&gt; They are perfectly reasonable in civilized discourse; complaining about my tone because I'm expressing an opinion you disagree with doesn't advance your point one iota. You do realize that for all practical purposes it's impossible to write Haskell without incurring dependencies on something /u/tibbe has written right? There are not very many people who have done as much work for the community as he has. You're blowing off his attempt to start a discussion about a problem that is very real for the maintainers of code all of us rely on, and doing so in a patently insulting way. Some of your points might have merit but the way you're communicating them is not going to endear to you our community.
The maintained ones, I suppose...
&gt; Until that happens, my view is that all this is griping for griping's sake. The person making this post is a prominent member of the Haskell community maintaining a lot of core libraries. To say that he just likes complaining does not help anyone, because it is obviously not true. It likely comes from experience actually using these things in anger, and finding about their shortcomings.
I'm well aware of who he is. Why do people assume I just crawled out from under a rock here? Maintaining libraries that need to support several different versions of one compiler -- particularly one in active development that makes breaking changes with some frequency -- is *always* extremely frustrating. If on top of it you need to maintain support for several different compilers that all implement slightly different extensions and interpret the language standard in slightly different ways, that's even *more* frustrating. Some CPP-like tool is essentially necessary to deal with these. CPP is not ideal for a number of reasons -- downthread he pointed some of these out. But CPP has some very strong advantages, which far outweigh its (in my opinion) superficial drawbacks: it is ubiquitous and, apart from some edge cases, very consistent across implementations.
I don't think 808140 is really responding to tibbe, but rather to "someone in that thread" who "suggested that we should identify the most common CPP use-cases and integrate them into the language." I think they are mostly in agreement, in fact. In the initial post, tibbe is not saying to fold preprocessor functionality into the language. He recommends: 1. "We need to reduce [breaking changes in GHC and libraries we depend on] in the future." 1. "The right way to move to evolve an new API is to add new functions and data types, not modify old ones, whenever possible." Thus, tibbe is not recommending the replacement of CPP, but rather the minimization of its need. He does state that CPP is hard to read, which 808140 refutes (correctly, I believe), but primarily he is urging everyone to avoid the need for it. I can only imagine 808140 agrees: "It's a pain in the ass and I feel your frustration, ..."
Definitely. It lives at https://github.com/haskell/cabal/issues/2365
From reading so far, it sounds like the translation to reactive-banana will be relatively straightforward though?
It's very easy to have the branches diverge. If the changes are small and localized, then yes, they are easy to propagate across branches. For large and crosscutting changes, it's much harder.
&gt; We would need separate packages for separate versions? This would be the only solution, unfortunately having a GHC 7.8.1 vs GHC 7.8.2 version of containers would result in impossible to resolve dependency chains.
That's a false statement. Here's his quote: &gt; "Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%." ref: http://c2.com/cgi/wiki?PrematureOptimization
I don't think anyone could have summed up that thread chain any better, well at least as far as I have understood the exchange. I like that the question asked is essentially, can we do better, this I believe is the type of questions that are asked which has drawn all of us in one way or another to this language. I am not in favor of increasing the complexity for the same power though. I am in favor of preserving the time for the participants of our small ecosystem. If /u/tibbe says it is a problem, then it likely is. I don't want the solution to be something that is easily understandable by him; but is unfortunately, takes longer than necessary to understand for someone who is not on his same level.
A trivial and yet especially annoying sort of ifdef is when I need a particular module only for some OS, or build flag, or library version, and not others. If I import the module all the time, ghc -Wall will complain when it's an unused import, but when I ifdef the import, that makes the code more fragile to change, because I might accidentially use that module outside the ifdefed code blocks that currently use it. I'd like a way to tell ghc to not warn about a particular unused module import, with a higher granularity than {-# OPTIONS_GHC -fno-warn-unused-imports #-} Something like "import Some.Module {-# no-warn-unused -#}" or "{-# OPTIONS_GHC -fno-warn-unused-import-of Some.Module -#}"
I don't think we can totally get rid of the need for a preprocessor, and then we might as well use CPP (I'd like to use the cpphs implementation and ship that with ghc). But why not do what we can to minimize the use of CPP? A simple example, a library `Foo` adds a new exported identifier, `bar`. This happens to clash with `bar` I've used in my module which does `import Foo`. This would not be a problem if top level module identifier shadowed imported identifiers, but Haskell currently doesn't do that. (I've suggested this change on the haskell-prime mailing list; and also implemented it. It works great in practice.) Another way that would avoid CPP would be to allow `import Foo hiding(bar)` even if `bar` is not exported from `Foo`. I believe ghc now implements this. Personally, I'm not on a quest to ban CPP, but I'd certainly like to minimize its use, since maintaining code with conditional compilation is a pain.
We already have cpphs, which everyone could use. I just need to convince Malcolm to change the license. :) But he's close enough that I can throw things at him, so I might succeed. :)
[Haskforce](https://carymrobbins.github.io/intellij-haskforce/) is a plugin for [Intellij IDEA](https://www.jetbrains.com/idea/download/) that I have good experience with.
You're taking this much too personally. You're far from the first person to gripe about CPP, and I was responding to the entire thread, not just to you. If we're going to address just *your* points -- that is, the e-mail that started it all -- I would not say it was filled with valid technical complaints regarding CPP. Do you think it was? You mostly were annoyed that GHC was introducing breaking changes regularly (this is unrelated to CPP) and some hand-wavvy vagueries about how CPP introduces bitrot and is difficult to test. In *this* discussion, however -- after your obligatory complaints about how rude I was being -- you *did* raise an important and worthwhile technical shortcoming of CPP, namely the Haskell-vs-CPP parsing issue, particularly as relates to CPP understanding C comments but not Haskell comments. I agree that this is a valid technical concern. One brought up *after* I made my post. However, I disagree that it is an important enough concern to warrant abandoning CPP, for reasons I've already outlined. But who am I? Just some person on the internet. You don't need my validation. Really, what are you after here? Someone other than you submitted this to a public forum, and I find it positively bizarre that you should come here and accuse me of being uncivilized because I pointed out that you were griping. You were. You are *allowed* to gripe, you know.
They can do that already on reddit. Why would one want to limit his / her audience to russian-speaking community? I would understand if ruhaskell was offline, but it isn't.
&gt; I'd like to use the cpphs implementation and ship that with ghc Btw, we're tentatively planning to address that for GHC 7.12 in one way or another. The primary motivation is to become independent of the system `cpp` whose "traditional" semantics we rely on vary depending on the platform/environment in annoying ways. Also, I don't think we can rely on "traditional mode" to be available forever. Another motivation is to carefully reduce some of the conflicts with Haskell's syntax `cpp` has by being able to modify the implementation (`cpp` mangles Haddock comments, has subtle issues with some Haskell operators, and breaks multiline strings... just to name a few issues you encounter when enabling `{-# LANGUAGE CPP #-}`).
It would be compounded significantly by that change though... 
That depends on the library. Not every library cares about the GHC/base version.
It is not an option to have things evolve and not have breaking changes. Not with finite resources at least. There is a maintenance cost to having multiple copies of an API in a code base, and it increases the more code is added. Eventually code is removed or development stops. In the end you can't avoid deprecating old code. The question is whether it is easier to integrate a stream of small changes or many changes in one big lump. From my experience the difficulty of finding the root of a problem increases dramatically the more there is to consider, so small changes are preferred. Personally I would rather see more releases of GHC and less backwards compatibility support. 
How do you avoid caring about the GHC/base version when your upstream library requires you to specify your GHC/base version?
I'm not sure what /u/JPMoresmau had in mind, but I'd like to see a specification of a Haskell-specific preprocessor (in the style of CPP, but tied into Haskell instead of C tokens) and at least one free software implementation. The compilers could decide for themselves to re-implement based on the specification or depend on / bundle an existing implementation. Either that, or some improvement over TH that would allow cabal to communicate versions (and other things we communicate with CPP defines) to the TH and make it less likely to fail in the wrong place (type-safe TH?). That feels a bit more "idiomatic", but it feels like more effort than a CPP-style preprocessor with possibly not that much advantage.
&gt; Which branch will you release to Hackage? Both. 1.1.9 might be uploaded after 1.2.3, but cabal can choose 1.1.9 since 1.2.3 depend on a version of base that you don't have installed, e.g.
Let's say I'm interested in Haskell, Type Theory, Computer Science, and I know a certain amount of it from personal study, but I have no formal education outside of high school. Is it possible for me to be accepted into this or similar position? I know typically you need to have a degree, but I was wondering if there is any other way in general? I know some people receive honorary degrees based on their work - is there a similar way to become a PhD student? If you think about it - what is the university losing by accepting a student that pays for everything himself? Worst case he will fail, best case he is capable enough to get his PhD. Something I've been wondering.
Thanks for the plug. I designed Decimal specifically with financial calculations in mind. In particular it can divide an amount into pieces that are close to equal but are guaranteed to sum to the original amount. E.g. $10.00 divided 3 ways gives two lots of $3.33 and one lot of $3.34.
It still a lot of work for those projects too. We just put up with it because not backporting changes is even worse.
There are (or were) users of GHC who would like a completely GPL-free distribution. That's why we have integer-simple, for example. We only bundle gcc on one platform (Windows). Adding another (L)GPL component takes us a step away from a GPL-free distribution. It's not a deal-breaker, but it's kind of unfortunate. We've talked about writing our own cpp replacement to work around the license issue, the question is whether it's worth the effort.
&gt; Users end up having to use *both* the old "unclean" API *and* the new "clean" API. For a certain value of "users." End users of a Haskell program obviously don't. Step up the chain of "users" one level, and Haskell application developers still don't have to use CPP. It's only when you step up the chain again, and get to *library developers depending on other libraries (including base)*, that you find this issue. Breaking changes for a cleaner API usually have the application developers in mind, not the library developers.
[Users find orphan instances extremely surprising.](http://stackoverflow.com/q/26770247/1186208)
Wrote up a trac ticket for you: https://ghc.haskell.org/trac/ghc/ticket/9990
I think you need to correct this: &gt; I think this is a mistake and that it should instead have been: a `f` b &lt;=&gt; f a b 
Done. Thanks!
I was gonna ... but then I got confused and thought I was thinking of Ada. Oh, it's the same: http://en.wikibooks.org/wiki/Ada_Programming/Operators#Logical_operators
Branch 1.1: type MkAForm x = AForm Handler x Branch 2.0: type MkAForm x = AForm WebApp WebApp x So there's a one-line difference between the branches. What's the problem again? 
No, my suggestion would be 7 `minus` 5 == 2 minus 5 7 == 2 Think of `minus 5` as a function subtracting 5 from it's argument. `subtract` would be a better name here.
 a `seq` b could be first `a` then `b` which would then correspond to seq b a In this case I would perhaps prefer `seq` to be named `after` and flip it so: b `after` a &lt;=&gt; after a b returning the result of `b` after evaluating `a`. 
It almost seems obvious in retrospect. You make a very convincing case.
It is certainly fine for many projects. My hexacopter flies fine with only 2K. But when controlling a lot of RGB pixels or doing audio, it is easy to gobble that space. I remember the first time I tried to make a nice 4096 entry sine wave lookup table... that obviously did not end well ;) This is, of course, encouraging. Since many projects do just fine with only 2K, even if Idris has 32x worse memory usage, it should still be viable for a lot of things on the teensy. 
Haha, alright :)
Ah, yeah, sound or image-data gobbles up memory like nothing. But then that's your actual data that is eating the memory, not just "your code". So you're not just using a lot of memory to accomplish the same thing you could've with much less, but you're actually getting value out of it. So you can either change your algorithm to progressively work on smaller chunks/tiles, or, if you can't do that, use a larger device (which is absolutely warranted in that case.)
can you give a few examples of improvements that could have been backwards-compatible? I've never maintained a package or contributed to any language extension proposals. thanks!