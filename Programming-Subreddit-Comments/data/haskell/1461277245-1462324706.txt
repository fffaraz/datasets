It matters *that* you named the thing. `\case` makes the function "linear" (in the final argument) i.e. the input is used once (by the match). When I read a `\case`, I can guess that ~~no~~ each branch of the function is simple, and won't messily depend on the whole as well as a part (think `@`-patterns). 
What's the reasoning behind this? 
Doesn't it break whatever uses `mdo` as an identifier? (Not that I mind).
How is the haskell standard disregarded by ghc?
Author of [Threepenny-GUI][1] here. I wrote it to do precisely what you want: Display some stuff in a browser. Have a look at the [examples][2], they are intended to be very simple. [1]: https://wiki.haskell.org/Threepenny-gui [2]: https://github.com/HeinrichApfelmus/threepenny-gui/tree/master/samples#readme
that was a mistake you see i have trained my entire life for this exact moment. my skills in physical combat far exceed what you could comprehend. when i run places my arms flail backwards as i was trained by naruto himself. watch your back.
Yes, you can just write (,,,) 1 2 3 $ stuff instead of (1,2,3,) stuff But you can't write (,2,3,4) stuff more conveniently that (\x -&gt; (x,2,3,4)) stuff Which is mostly the point of `-XTupleSections`.
I'd like to know that too, leave him a comment and maybe he can clarify. He might be recommending a flag from [this](https://hackage.haskell.org/package/base-4.8.2.0/docs/GHC-RTS-Flags.html) but I don't know.
RecordWildCards
With `GeneralizedNewtypeDeriving` and `DeriveFunctor` both on, it is possible for GHC to take either one of two possible approaches to deriving a `Functor` instance for a `newtype`. I can't recall how GHC currently disambiguates this request. If the `Functor` instance at play (that is, the instance written for the type the `newtype` is wrapping) is Lawful, then the two instances are equivalent. If it's not Lawful, they're not equivalent, which is a bit painful. It's precisely this sort of ambiguity between extensions that a standard should try to solve. 
TypeFamilyDependencies
PartialTypeSignatures
As the author of `TypeInType`: Please, no. First off, `TypeInType` is *not* benign, for at least two reasons: 1. It enables `PolyKinds`, which changes the meaning of `data App f a = Mk (f a)`. In Haskell98/2010, this gets kind `(* -&gt; *) -&gt; * -&gt; *` but with `PolyKinds`, it gets the kind `forall k. (k -&gt; *) -&gt; k -&gt; *`. In obscure scenarios, enabling `PolyKinds` can get code that type-checks to fail to do so. 2. `TypeInType` means that you have to manually import `*`. It's also *way* too soon to think of enabling something like this by default, IMHO.
You just need some HOFs in a library somewhere: as3rd :: (a -&gt; b -&gt; c -&gt; t) -&gt; c -&gt; a -&gt; b -&gt; t as3rd f' c a b = f' a b c as4th :: (a -&gt; b -&gt; c -&gt; d -&gt; t) -&gt; d -&gt; a -&gt; b -&gt; c -&gt; t as4th f' d a b c = f' a b c d etc... Then you can do things like this: (as3rd (,,,) . reverse) "example" 'x' True 7 --&gt; ('x',True,"elpmaxe",3) (as4th (,,,,,,,) . (*7)) 23 1 2 3 4 5 6 7 --&gt; (1,2,3,161,4,5,6,7)
Sorry, but _ in expressions is already used for [typed holes](https://wiki.haskell.org/GHC/Typed_holes), which are way too useful to give up.
To each their own, I guess. I name useless variables that are used once `t`. I also don't have functions so complex I need to guess "linearity" by your definition. Then again I generally try to use fewer extensions rather than more, so I have that bias.
\**reads user name...*\* Checks out.
&gt;I, for the life of me, can't get them right for anything non-trivial. This is the deciding vote for me. If *you* can't get your head around this, there's little hope for most, if not all of the rest us.
See, here's the problem -- I like stack; I don't like Trump. Even if I did, I hate the slogan, as it both implies that Haskell is by-and-large crap to begin with, and that assuming such was the case, a single tool could fix that. I'll pass.
What about [threepenny-gui](https://github.com/HeinrichApfelmus/threepenny-gui)? Could even be usable for games with WebGL.
I think you're right. I was wrong. Thanks.
That's never going to be as fast and smooth as native though (at least not for any games that need a relatively high framerate). I guess my point was, all of the existing "solutions" are a crappy compromise. Apparently it's possible to use GHC as a cross compiler for both android and ios but it doesn't seem to be very straightforward and there doesn't seem to be a nice framework to turn that into apps to be released on the app store and play store.
I mean you could change the symbol used for typed holes, because it wouldn't break backwards compatibility, because code with typed holes in it can't compile by definition.
I'm not sure why you were downvoted. I think you're absolutely right. It seems to me like this tension is evident in the Haskell community.
Most of the php repo are related to [Phabricator](https://github.com/phacility/phabricator), which is in php It wouldn't be productive to rewrite it in Haskell
Which repo(s) are you referring to?
I wouldn't know.
&gt; Although cpp is in spirit purely functional, pragmatists insisted on the unnecessary and theoretically awkward construct `#undef`, which allows a single name to be re-defined. I'm disappointed he didn't say `#pragma`tists.
Including containing parentheses as part of the delimiting context means you have to care about `f(_)` vs `f _`, where nothing else in the language makes the presence or absence of parentheses significant. I'm not saying that a language that has _ that behaves like you ask can't exist. I do think that it doesn't feel very "haskelly", though.
Link?
ok I'm gonna totally fall out of the rest - so be it. From what I read you probably know your way around HTML/CSS/JS and just want to get shit done without learning or introducing much machinery for your project. So I would go with a simple framework like scotty or spock to serve your HTML/etc. - if you want you can add servant for the APIs but IMO you're fine with scotty/spock, especially as you probably don't need authentication and stuff
Oddly enough I found it to be the other way around ;) very informative on web dev. Guess it depends where you come from
Hmm... I admittedly don't have a good answer to that, besides some sort of symbol indicating the existence of `_`. (Like how lambda has `\` and sort of `-&gt;` as well), perhaps something like `\&gt;`, but at that point the gains in readability / conciseness diminish significantly: foo bar _ bazz (\&gt; foo bar _ bazz) (\x -&gt; foo bar x bazz) In that case the parenthesis would work like how they work with lambdas, as at this point it is really just a lambda with some sugar to save a few characters added in. But yeah that probably isn't worth it, darn :/
&gt;time tested I've tried to use Nix (on Mac OS X, Ubuntu) and NixOS five times and failed each time. This is at different times with different generations of Haskell support with Nix (including haskellng). Trying to get [bloodhound](https://www.stackage.org/package/bloodhound) to build on a laptop running NixOS otherwise successfully led to my entire OS install being broken and not being able to get it to build. I have a lot more examples of people being successful and happy without expert intervention with Stack than I do with Nix. It's not close. Nix is far from being a UX peach as well. I have a lot of affection for the Nix devs I've talked to (they've been very nice), but it's just not in a good place for me to be able to recommend it or anything like it right now when Stack works well and works _now_.
As other people said, were it only for the keystrokes I wouldn't care. What I don't like is introducing another symbol into the scope, bc it is another entity I have to mentally track when refactoring and such.
So is this one of those that don't really have negative effects? I've often felt like whenever scoped type variables is proposed as a solution, it's because my code is badly thought out.
Not to mention `stack build --nix` gives you the best of both worlds for (nearly) free. 
Just like with numbers, i.e. it's confusing for beginners but not a big deal in real applications. As long as you have a `Text` signature somewhere along the way the type inference tends to take care of most stuff.
Then we can finally joke about Haskell being much `ado` about `Nothing`.
That's a good point, yes.
&gt; Have you thought about working /around/ haskell.org Yes, absolutely, but in a broader context than you mean by the rest of your comment :) Yes, I think that other services should avoid pointing to Hackage docs at all. I just haven't followed up on that front due to not enough hours in the day.
FWIW, I've implemented an ATS version: https://gist.github.com/ashalkhakov/c0c9cadce1543dda91568d7d5ae18c61 It's probably quite a bit slower than the GHC version. The only difference is that there is no GC involved in this implementation (and the memory utilization stays at a steady ~240MB). It's possible to change the overall algorithm somewhat to recycle messages properly (so that old messages could be transformed directly into new messages, removing the source of memory re-allocation), if that would help.
What are you suggesting should be done then? Mention Stack as the only means to install/use Haskell on haskell.org for the time being? And switch back to Cabal once they've got their "UI polished" (if that ever happens) to become the default recommendation again?
&gt;&gt; Have you thought about working /around/ haskell.org &gt; Yes, absolutely, but in a broader context than you mean by the rest of your comment :) At the risk of implying more than you actually said: Is fpcomplete working on replacing Hackage in a similar vein as Stack was to cabal? Or are we talking about an alternative `haskell-language.org` domain?
I just wanted to say that I really appreciate your comments here. I debated responding to /u/fpnoob's ugly trolling and just held back.
To clarify: you could 100% do this without writing a single line of C.
Whatever... you either attack your opponents or you dismiss them as trolls *sigh*
Thx bae &lt;3 Honestly, if it weren't for Stack I would have ditched Haskell entirely. I can't justify the time and effort of figuring out what a decent `cabal` workflow looks like when I could be using Rust/`cargo` or F#/`paket` instead. Before I found Stack, I was despairing that someone - *anyone* - in the Haskell ecosystem took UX seriously. And when I've opened issues against Stack, you've closed them in hours or minutes. To me, that's worth standing up for. (By the way, I don't need to tell you this, but the conversation linked above did get quite a bit ugly. If you think gbaz1 and friends are not playing fair, maybe it's worth making some kind of official statement about, because right now literally all the information we bystanders get is whatever gets linked by your opponents.)
What advantage do you get by rewriting a perfectly fine peice of software and keep it up to date with upstream ph? 
I didn't know this! Can you show an example of their unsoundness?
All the time!
Or make tools that make Haskell great..
yeah, I'd prefer that over the alternatives
[removed]
I wonder why the reddit -&gt; haskell.org -&gt; reddit loop isn't simplified away.
I completely share your position. I would not have considered Haskell the language without stack the tool. It's amazing what /u/snoyberg did. I think many people, even of course among the cabal core dev, appreciate what he did.
...ow.
You know what they say - the best vengeance is ~~living well~~ writing a build system.
It seems that most people have no idea how horrible build tools in popular languages are. Take C or C++, they don't have anything like cabal was 5 years ago, and yet everyone use them.
I just went to haskell.org to see what this is actually about and while it isn't perfect the Downloads tab gives a reasonable objective overview of what is available. But a new user who just wants to try haskell isn't interested in that. And let's face it, that is the audience. I guess what's missing is a "Get started" tab to the left of Downloads which provides instructions like: Get stack $ stack setup $ stack new first project hello-world open Main.hs Get kicking Get stack should be substituted with a list of the most common OS distributions along with the idiomatic way to install stack. I don't think a template hello-world actually exists yet, but there is no shortage of candidates. Beef it up with two or three links to further learning material. And voila no need to discuss about the particular order in which the alternatives are to be mentioned anymore. As for the Haskell Platform: What is its actual purpose these days? As long as I know it it was pretty useless. There was a period of time when network wasn't that easy to compile on Windows and the Haskell Platform shipped it precompiled, so there's that, but otherwise? If people want to invest time into it that is great, but it is rather ridiculous to couple the point at which stack will be the recommended choice to haskell.org visitors to the release of a new Haskell Platform which itself is coupled to the GHC release cycle. Established processes and consistency are good, but no ends in themselves. Your first response to the pull request in question is completely lost on me, even when accepting the need for the Haskell Platform as axiomatic, who is gonna be confused if you say "Download stack" now and change that to "Download the HP" in two weeks. I doubt anyone who has the potential to get confused by this would even notice. I think it is a severe case of misjudging the audience. And I understand the frustation if something that is essentially a very clear (and minor) matter turns into to a swamp of discussion. That being said I don't think this brand of he said she said reflects well on anyone.
I can add that I also went through an extended phase of wishing I could simplify (\x y z -&gt; ...) even more, but after a couple of years of playing in Haskell, that feeling went away. I think, as I've seen in other languages, you just get used to what you have, and don't mind after a while. I don't mean to stifle innovation, or to suggest settling, but I also think I just learned to think in ways that very rarely require me to target anything beyond the 2nd argument. IOW, it used to seem important, but it's been a long while since I've even thought of it. I've really taken to the idea of single in/single out, pure functions, like math functions, and don't like having more than 2 or 3 args if I can help it, and I seem to most often be able to help it.
Honestly you might have more luck using elm and webvr.
&gt; I've often felt like whenever scoped type variables is proposed as a solution, it's because my code is badly thought out. Really? That's surprising to me, because I almost always end up with the extension enabled because I want to add a type signature for a local definition in a where clause... which is such an obvious thing to do if you're tracking down an inference problem. EDIT: I should say: Though I'm by no means an expert on it, but I can't think of a single "dangerous" effect of ScopedTypeVariables *except* perhaps that it might lead to local definitions (where, let) that are less general than intended.
At least for `GeneralizedNewtypeDeriving` and `DeriveAnyClass` there is a compiler error when both could be used as far as I recall.
&gt; I mean _ shouldn't have any ambiguity with my suggestion. It's not about compiler level ambiguity but for the end user. Things can be obvious and be a non issue which doesn't seem to be [the case.](https://www.reddit.com/r/haskell/comments/4fqoke/thoughts_on_using_as_syntactic_sugar_for/d2bdhqq) then it's probably save it's not obvious how it could work. Which means people will build a incomplete mental model of how it works and run into issues from time to time with the edge cases. Maybe the majority thinks that risk is worth it. Personally I find the explicit naming of parameters often improves code readability.
Yes, thanks, that's spot on. Basically, I did this kind of thing before by just opening a socket, responding to HTTP queries and spitting out some HTML. But that seems almost barbaric in 2016 with Haskell. Thanks for your suggestions, I found this tutorial using scotty + blaze-html: http://adit.io/posts/2013-04-15-making-a-website-with-haskell.html That seems like the straighforward solution to generating and serving a website I was looking for!
No, you just need to see the future. /s
Counterpoint: type inference for rank-n types is undecidable in general, and I assume the algorithm for inferring rank-n types may change over time. Rank-2 types, sure. Lots of people use `ST`.
This is the only one I'm 100% sure about.
There were issues concerning irrefutable patterns matching when these are used. Are they all resolved?
Only when you use it. Does it also matter when having Haskell2010 code?
nooooooooooooooo
Relevant: [Why I Hate Advocacy](http://www.perl.com/pub/2000/12/advocacy.html) - IIRC part is about people in the Perl community complaining that the use of PHP for the web site made Perl look bad. 
I for one love this extension. Is it unsafe?
What will be covered in Volume 2 of Happy Learn Haskell Tutorial. please share
Stack is the best thing that's happened to haskell in a long time. Previously I would try and use cabal and just give up, it would fail to install a reasonably large package. 
That unfortunately does seem to be the case.
Nice, one less extension in my .cabal file :) 
NoImplicitPrelude
Hopefully. :-) &gt; Is there a reason why you haven't added it to stackage? Yes. Inclusion with stackages comes with certain responsibilities for the maintainer, in particular concerning timeliness of dependency updates. I currently can't guarantee that timeliness, and I don't like to make promises that I can't keep, so I figured I better not include it into stackage.
Forgot the links &gt;.&lt; shine: [hackage](https://hackage.haskell.org/package/shine) - [github](https://github.com/fgaz/shine) shine-varying: [hackage](https://hackage.haskell.org/package/shine-varying) - [github](https://github.com/fgaz/shine-varying)
Ugh. I really wish Haskell could get hierarchical modules and the ability to wildcard import sub-modules. If I show a colleague a Haskell file and say "look how short the code is, only 20 lines of Haskell!" the first thing they'll reply is a snarky remark about the 40 lines of imports and language extensions.
Some people didn't like the workflow `arc` encourages, so Herbert was experimenting with some alternative ideas. FWIW, it looks like "push patches with `git push`" is on their prioritized roadmap (i.e. someone is paying the developers to do it), so in the future this may not be a problem for people who don't like the default `arc` workflows.
Thanks for sharing this. It neatly sums up a lot of thoughts I've had lately. (Also, bonus points for the quote that uses "quiche-eater" as an insult).
To me, it seems like the known-good package set approach only really works so well for.relatively popular and maintained packages (which is generally what industrial users want, so I can see that it works well from your perspective) but doesn't work so well when you require more obscure packages and or more custom choices while still avoiding redundant recompilation.
Fair enough ;-) btw, I just got the basic setup with threepenny working. My app is serving a webpage and dumping all relevant state as HTML. Very nice!
have you considered integrating Shake into Stack yet?
That's a strangely specific novelty account.
We had 3 applications, including Gershom, but due to the way the rotation is set up, we only had one slot up for nomination this time around. I voted for him on the grounds that: * He put in place the system we're using to maintain all of the infrastructure, delegating as much as we can out to the #haskell-infrastructure team. He's the reason that haskell.org really runs at all, let alone as smoothly as it does. * He has been the committee chair for the last 3 years (he was nominated against his wishes to take over for Jason Dagit), none of the rest of us want to step up to take over that role immediately, but this gives us a grace period to roll it over if we choose later on. "Better him than me!" * He's damn good at a tough job. I can't speak to the reasoning of the others on the committee, but the vote *was* unanimous.
https://twitter.com/gabrielg439/status/701871069607505921 :P
What I'm implying is that if `FloatNumber` and `DoubleNumber` don't themselves implement `Num` then the practical usefulness of this is questionable. And if they do implement it, that re-raises many of the original questions about how to avoid, or what to do about NaN.
KindSignatures
Ahah, yes, same reaction here 
It's being written as we speak. So far there are only one chapter mostly finished. We haven't fully decided what the volume will cover and how we'll plan it yet, it's still being shaped. The aim of the first book was to take the student's *reading* to the very start of beginner level, and *writing* to pre-basic level. That is, you'll be able to recognise and understand quite a few of the basic elements of Haskell programs when you read them, but you probably wouldn't be able to understand a lot of every day programs and you won't be able to construct programs yet apart from extremely basic ones. In the second volume, we're aiming to bring your *reading* level to appreciation of small, simple everyday Haskell programs used to do a variety of programming tasks. We'd like to take your *writing* up to being able to write your own tiny, not too ambitious programs without too much pain or annoyance; programs that do simple things. The danger here is that you try to do too much, and that burns you. We really don't want that to happens, so we have to write this very carefully. Sorry if this is too vague. It's still very much in progress, so it's difficult to pin down. 
Thanks, megaparsec is exactly what I was looking for!
No problem. Confirmed fixed in shine. shine-varying still has the wrong links, is that also waiting on hackage to update?
Yeah what next ? Classical painting mixed with programming ? Let's make sure nothing bad is inferred
As far as I'm aware an early version of Stack did rely on Shake, and they then removed it - I've no idea of the reasons behind that.
If I'm not mistaken, the difference is that this helps you create type-safe interfaces to APIs while Servant helps you build type-safe APIs. Unless Servant can do the former as well?
You can be offended by whatever you like, same as me.
thanks.
If you're looking for hash tables to help you with GC time, make sure you make at least as many as $NCPU: GHC can't yet scan a large mutable array in parallel.
One of the authors here. Fundamentally both Servant and WebApi lets you do the same thing. The difference is WebApi has chosen to use a typeclass based approach for representation of contract whereas Servant has this information in type level list constructs ((:&gt;) etc). This is a [previous discussion](https://www.reddit.com/r/haskell/comments/3zeqm0/ann_a_wai_based_library_to_build_web_apis/) about comparison between WebApi and Servant.
You should better ask this kind of question on stackoverflow instead of here.
I'm having a bit of a "mental clearout" - just tying up loose ends. I haven't really looked at this for a while, but I mentioned it enough times that I thought I should at least release where I got to! Curious to hear what people think.
Qualified imports are one of my most commonly-used features in Haskell. However, if you wish to use (many) Text operations unqualified, you might consider an alternate prelude that prefers Text such as BasicPrelude
I used to think they could co-exist, but now that GHC+base can't even handle some standard Haskell programs, I think I agree with you.
Did I understand right that you can't express things like `catch` with this?
I still hope to see literal extension compatible with heterohenous lists and such. Remembering the related trac tickets, i'll be incompatible with OverloadedLists, so i'm against OverloadedLists by default.
Looks pretty cool! Iâ€™m slightly confused about the purpose of the `Interprets` class. It kinda looks like Iâ€™m supposed to add `(Interprets (Reader Int) m, Interprets (State String) m)` constraints to my typesignatures if I want reader and state effects (leaving aside the more specific type classes such as `EffEnvironment`). However that doesnâ€™t work because of the functional dependency. The instance instance (Monad m, Interprets f (Eff h m)) =&gt; Interprets f (Eff g (Eff h m)) where interpret = lift . interpret also seems weird, since the fundep will force `f~g` and `f~h` because of the other instance. What am I missing? 
FWIW I managed to get it to work by changing the instances to instance (Monad m, f ~ g) =&gt; Interprets f (Eff f m) where interpret p = Eff (\i -&gt; i (InL p)) {-# INLINE interpret #-} instance {-# OVERLAPPABLE #-} (Monad m, Interprets f (Eff h m), m' ~ (Eff h m)) =&gt; Interprets f (Eff g m') where interpret = lift . interpret {-# INLINE interpret #-} Not sure if thatâ€™s what you intended but now it behaves like I expected it to behave. It requires a few more type annotations than the fundep version but I got everything to compile.
Oh right, `Either` can't be defined as a type synonym over `EitherT`.
Can you say a bit more about why `Free` is used in the "source language" for the logging effect? Also, in the type signature of `runLog`, Eff (Free (LoggingF message) e) m a should be Eff (Free (LoggingF message)) m a 
There's value in having strong, consistent opinions in your API, especially if it gets large. Haskell's aggressive cross-module inlining also make convenience wrapping / unwrapping relatively cheap. That said, having a poorly formed, or inconsistent opinion in the API a just another stumbling block to new users. So, default to leaving those decisions to the users until you've dogfooded enough to develop your opinons.
There's also the concern of burdening the consumer with dependencies, both technically and mentally. For example, if I choose to expose my library through a Lens-based API, then the Lens package becomes a build dependency, and I force the consumer-side programmer to understand lenses and design their code around them. Which may or may not be an acceptable tradeoff.
We definitely appreciate this. We're doing our best to complete it as soon as possible. However, we only currently have 61 paid readers at the moment, so it's not paying us enough to work on it full time. If you would like to see it finished faster, you could help increase awareness about the first volume, and if you have any ideas around doing this, we're very interested. Maybe we need to create a kickstarter for it?
By all means - if you're willing to rewrite all of that code "just because", do long-term maintenance of it, match feature parity with Phabricator, constantly support us and answer our questions, it might be worth considering. Oh, you'll probably also have to help us support migrating it and like I said, we wouldn't move unless we could be sure it'd be supportable for a long time by you, including things like major security and feature upgrades. And because Phabricator is actively developed, you're going to have to *exceed* them, consistently. You're already very far behind the curve. I can tell you for a fact I'm not going to do that, because I actually have things to do, rather than useless yak shaving for the next N years of my life. I get the feeling you probably won't want to do all that work, either. So why even bring up the point about GHC? That case actually had people who wanted to do the work, with clear goals and reasons. "Rewrite thousands of lines of code, because Haskell" is not a clear goal or reason.
Welcome to monad transformers! I remember when I first learned about using MaybeT, etc for this kind of thing. It was a definitely a significant lightbulb moment. This is the right idea, but `EitherT` from the either package has been superseded by `ExceptT` from transformers. The [errors package](http://hackage.haskell.org/package/errors) is the commonly accepted set of convenience functions for dealing with these things. It provides a bunch of really nice and fantastically named functions such as: hush :: Either a b -&gt; Maybe b hushT :: Monad m =&gt; ExceptT a m b -&gt; MaybeT m b note :: a -&gt; Maybe b -&gt; Either a b noteT :: Monad m =&gt; a -&gt; MaybeT m b -&gt; ExceptT a m b hoistMaybe :: Monad m =&gt; Maybe b -&gt; MaybeT m b hoistEither :: Monad m =&gt; Either e a -&gt; ExceptT e m a ...along with a host of others. As of version 2.0.0, the errors package [switched from EitherT to ExceptT](https://github.com/Gabriel439/Haskell-Errors-Library/issues/32), so that is what you should use. I do agree with /u/tdammers that if you're writing a library for consumption by others, it may be desirable to not depend on the errors package and only export `m (Either e a)` or `m (Maybe a)` functions. But if you're writing an end user application and dependencies aren't as much of a concern, then the errors package is definitely the way to go. 
Yea, `Interprets` is really just there to help you implement `MonadReader` and stuff. I think this might just be something I was working on but never fully fleshed out.
If itâ€™s just for helping you write `MonadReader` (and similar classes) there is no need for a typeclass, all you need is interpret :: p a -&gt; Eff p m a interpret p = Eff (\i -&gt; i (InL p)) However I quite like the typeclass in the way I presented it. Obviously it requires that you buy into `transformers-eff` to actually use it (which is what your docs say at well) but if you are willing to do that having a single typeclass which is parametrized by the effect is pretty neat.
Haskell doesn't suck. It's quite good. However, I've got a major gripe that leaves me skeptical. It's not with the language. It's not really with the community. It's just with *reality*. So, here goes. The difficulty of the language is not that bad, except for the fact that there will probably never be a time when anyone but an elite programmer can bet her career on Haskell. For that reason, I don't see it taking off. I don't see managers being willing to adopt it and I don't see many programmers being able to put in the time necessary to learn it well. I hope that I'm wrong on this. There are enough Python jobs that you can bet your career on it. You could bet your career on Java, although you'll have to become a manager in 10 years to keep sane. At this point, it wouldn't be ridiculous to bet your career on Clojure. A Java programmer who's remotely as good as the average Haskell programmer can get $200/hour consulting gigs, reliably. In the Haskell world, that doesn't exist. Having tried to build a Haskell team and failed (cheap company, sparse talent pool, short-sighted management averse to investing in people) I would say without reservation that if I were in the same job today, I'd use Python or Clojure. From a management perspective, using "the best language" is less important than using a good enough language with low risk. The situation is possibly good for employers. There are a lot of absolutely brilliant Haskellers who'll work for relatively average salaries. If I were a CTO at a stat-arb fund with reasonable confidence of being there for 5 years, I might bet on Haskell, in the same way that Jane Street bet on OCaml. That said, the decisions that drive language adoption are rarely made from positions of comfort. I don't want to blame "Haskell's community" because that sounds like I'm blaming the people, which I'm not. It's a great language and a strong community, but the sad fact is that it's difficult *enough* and the career rewards are sparse enough that few people are going to invest the time to understand it well. This would have been less of a problem 30 years ago when people could expect to be at a company for 5+ years. Back then, learning a hard language was an upfront cost but people wouldn't be averse to doing it because they had a sense that it would pay off. In today's economy of mandatory job hopping, people don't want to bet their careers on "weird" languages that require extra effort to learn. 
Have you tried out Stack and Stackage? We have close to 2000 packages in there already.
One way to handle variable left hand values is to use the variable values as an index into a map. Here's a similar question on stackoverflow.com : http://stackoverflow.com/questions/35756175/parse-json-with-aeson-when-first-key-is-variable . [The accepted answer to that question ] (http://stackoverflow.com/a/35762627/509928) uses that technique ( mapping from text ). The code below is a modified version of the code in that answer that works with your data. The resulting datatype is different from what you specified but once the data is parsed, you can repackage it however you like. {-# LANGUAGE DeriveGeneric #-} import Data.ByteString.Lazy as BSL (readFile) import Data.Text (Text) import Data.Aeson (FromJSON, decode) import Data.Map (Map, toList, map) import GHC.Generics (Generic) -- Record representing a single referrer data Ref = Ref (Map Text Int) deriving (Show, Generic) instance FromJSON Ref main :: IO() main = do inh &lt;- BSL.readFile "json.txt" case decode inh :: Maybe (Map Text Ref) of Just parsed -&gt; print parsed Nothing -&gt; print "Unparsable" The code above, a .cabal file to help build it and the test data are collected in this github repository : https://github.com/dc25/parsing_json_with_dataaeson 
Hi, Sorry for the delay with merging that PR. In the future, please don't hesitate to nag us via the issue tracker or e-mail if you think we're being slow to react. With regard to your complaints about #2607, the bug tracker is the appropriate forum for things like that, and I suggest moving the discussion there.
I believe he's just showing that transformers-eff *can* interoperate with a Free-based effect, just as it can interoperate with any other monad. It's not required.
I'd like to suggest that a tested version of something similar to this example get added to the Aeson tutorial at https://artyom.me/aeson I've suggeted it on the comments there.
I wonder what else could be cached besides `deriving`
Yes, but this is also true of everything defined in a module. Currenly a module is cached until it is changed. Theoretically you could put every definition[1] in its own module and therefore get a finer level of caching. [1] Bar mutually recursive definitions and maybe some other unusual cases 
Can you define the JSON format? Working with an array would be more natural than using the key value pairs.
I would use something like this: parseJSON = fmap f . parseJSON where f :: HashMap String (HashMap String Int) -&gt; [Referer] f = _todo 
Agree, also would prefer closing tags.
I'm not sure what you mean by 'random', but this seems to work: import qualified Data.Aeson as AE import Control.Monad (forM) import Data.Aeson (parseJSON, FromJSON) import qualified Data.HashMap.Lazy as HMap import qualified Data.Text as T data Referer = Referer { domain :: String , pathAccesses :: [(String, Int)] } deriving (Eq, Show) newtype Referers = Referers [Referer] deriving Show instance FromJSON Referers where parseJSON (AE.Object obj) = do lst &lt;- forM (HMap.toList obj) $ \(dom, refobj) -&gt; do paccess &lt;- HMap.toList &lt;$&gt; parseJSON refobj return $ Referer (T.unpack dom) paccess return (Referers lst) parseJSON _ = fail "Error" And you decode the **Referers** newtype.
The stack trace only goes as far as `HasCallStack` constraints go. So I believe if your library defined a partial function and didn't add that constraint, then you will get a stacktrace... up to the definition of your library function. It will *not* show you who called that function, which is probably the information you really want.
I would be willing to look into haskell if not for the bad press about the community being aggressive and having little to no diversity.
`map head` is a function that's composed with the rest. You could put parentheses around `map head` and it would mean the same thing it means now.
I think `data LoggingF message a = Log message a` must be what is intended, by the way. This is more like what appeared in the previous post (which also included a 'log level'.) Here, though, `LoggingF` ~ `Const`. But `Free (Const message) r` ~ `Either message r`, which, if I understand, can't be what is desired.
Your suggestions describe exactly how we approach teaching. We *do* take real world things and put them as examples that we drive teaching the language topic with. However we have to progress slowly, otherwise people will get confused, so we're pretty limited by the fact that we haven't shown a huge amount of Haskell yet, let alone got the students to the point where they can use it themselves. This limits the size of the real world examples we can use. An address book is quite real world, and quite useful. So is a shopping list, or a movie inventory. These are all from the first volume and yes they're *very* limited functionality, but that's only because at this point we haven't explained very much of Haskell yet. Your suggestion about doing Web Development is definitely something we've already planned for, but I don't think you realise how much groundwork a student needs before web development can be approached. Also, you're suggesting Yesod, which isn't necessarily a good fit for everybody. (Our preference is Scotty for beginners, and possibly Servant or Snap rather than Yesod because they're more flexible, and we have a bit of an aversion to using Template Haskell, though we could eventually tackle Yesod, once we get to that point as well). Ruby and Rails is probably not the best example to compare this to, because Rails was the thing that popularised Ruby, and the main thing that made it popular is the fact that DHH had a demo video of him building a blog in 15 minutes, and it looked easy because there simply wasn't much code. It turns out the only reason he could do that, though, is because in Rails, there are a lot of things hidden from the user. Ruby is a *very* easy language to learn if you come from Java or PHP, which is where a lot of the people interested in web development come from. Rails has a *lot* of automatic "magic", including code generators. Haskell is almost the complete opposite of this. Things are far more explicit in Haskell rather than being implicit (or hidden) like in Rails. This means things are both much more flexible in Haskell, but also you're much more in control of how things are built. Obviously if you're new, this presents a problem because you have to learn more things. However, if you want to become an excellent Rails developer, the same things apply as if you want to learn Haskell web dev, and one could very easily argue that the end point that Haskell will take you to is a much better place than the end point of where Rails can take you to (as a programmer, and in terms of being able to maintain and understand your own codebase, and the codebase of the framework you're using, not to mention the fact that Haskell's resultant code tends to be much more efficient and faster for a lot of cases). Our aim is to definitely get people to the point where they can build websites, games, and other many other types of useful programs as they'd like, but it will take learning a lot more Haskell to get to that point. We're not interested in just demonstrating how to do web development with Haskell. There are lots of places where people can see this stuff if that's what they're interested in (The Snap website, for example, has a video intro on how to build a twitter client of sorts using Snap). We're more interested in teaching people how to build up their skills so they're truly their own, and so they can build things for themselves and know how to problem solve whatever they need to do. However, we don't want this to be a pain, or difficult. I hope you don't think our Volume 1 is a "normal" book that has too much programmer-jargon and is difficult to understand. We have what we consider to be a completely different methodology: real-world-topic driven learning using examples rather than programming-language-feature driven learning. We've tried very very hard to craft it in such a way that you don't need to be a programmer at all to learn, and so that it's easier. We're also planning on adding some stuff other than just video so that people who prefer to watch can do that, too. Thanks very much for your long reply. Full of good suggestions, which we're very much already aware of and have thought of, so stay tuned and we should have some stuff you're really excited about in the not too distant future. (Stuff that *you* would call "real world"). 
Looks like wonderfully clean code to me. I found the absense of WAI interesting, probably treepenny has it's own way of doing thing (which probably also explains the `Haskell.initFFI()` in `static/dashboard.html`).
I don't know much about web app technology, so I can't really compare threepenny to other approaches. I guess I can only say it's the one framework that kinda made sense and was approachable ðŸ˜€ I did struggle a bit with the complexity of HTML/CSS and the speed of the DOM and RPC style access is lower than I'd like. If there's a better way to write this kind of app, I'd love to know!
Hamlet really does add some great features for type-safe HTML programming - including interpolation syntax for attributes in general and class and ID in particular, not to mention the various loop and conditional blocks. What is causing you to use the word "disaster"? As a data point, we outsource large amounts of HTML programming to third-party HTML shops. Yet our web programming team, who are not Haskell programmers, still prefer converting the thousands of LOC of received HTML into Hamlet. It's easy to convert, and the gains are substantial. But if you really do want to dumb-down Hamlet and use plain HTML syntax with only interpolation, there are several easy ways to do that too. One is to do a simplistic conversion of plain HTML to Hamlet: prefix with '\' all lines whose first non-blank characters are a tag. You can do that with a simple regex search-and-replace. With this method you can still optionally add some indentation so that you can still use the various loop and conditional blocks in Hamlet, and you can intersperse regular Hamlet with this "dumbed-down" Hamlet within the same template. Another method is to use plain text templates instead of Hamlet templates. (See the section near the end of the [explanation of Shakespearean Templates](http://www.yesodweb.com/book/shakespearean-templates) in the Yesod book.) With plain text templates, all text other than interpolations is copied literally to the output.
Great article. WebAPI seems intuitive in how it is built up, but more verbose. What seems a bit unintuitive in `servant`, but maybe not available at all in WebAPI is some distributive property that should apply to a subset of endpoints. For example if one endpoint does not require authorization (login), while the others do, then with type-level combinators the authorization combinator can be put in front of a set of endpoints. It is not clear to me how that could work in the WebAPI. Another thing I'd like to understand how to do in WebAPI would be to create a type that represents a common set of arguments, request or response headers, or authorization requirements. This can be a type alias in `servant`. How would this look in WebAPI? 
Hmm, I'm not sure this is wrong. With `[]`, we know that the matrix has 0 rows, but we have no information about how many columns it has. This implementation makes the general assumption that the input matrix was at least 0xN for any N. Similarly `transpose (repeat [])` is `[]`, as is `transpose (take n (repeat []))` for any `n`. In this case we are changing a Nx0 matrix into a 0xN matrix. The real problem is that `[]` loses information about how many columns the matrix had.
It might not be too helpful but I recently wrote a series about optimizing parsers for parsing large textual files [here](https://hbtvl.wordpress.com/2015/12/14/efficient-parsing-of-large-text-files-part-4-and-conclusion/). How large is the parsed log file ? What happens when you make fields of `LogMsgInfo` strict ? The larger `String`s will benefit from being stored as `Text`.
&gt; This bugs me that Haskell makes it so easy to write word count but the performance is pessimal and no trick will help. Writing it like you would do in C should help ...
Actually this isn't necessary: $ time wc -w gcc.log 11434863 gcc.log wc -w gcc.log 2,91s user 0,03s system 100% cpu 2,937 total $ cat wc.hs module Main where import qualified Data.Text as T import qualified Data.Text.IO as T main :: IO () main = T.interact (T.pack . show . length . T.words) $ stack ghc -- -O2 wc.hs &amp;&amp; time ./wc &lt; gcc.log 11434863./wc &lt; gcc.log 1,76s user 0,28s system 100% cpu 2,041 total I didn't cheat (and warmed the cached in both cases), but this seems fishy ...
Final installment: with lazy text : ./wc +RTS -s &lt; gcc.log 1,77s user 0,03s system 100% cpu 1,797 total 52MB max residency.
/u/tekmo: Does this approach support dependent eliminators?
No. This is still just the calculus of constructions under the hood (no inductive constructions, universe polymorphism, or dependent elimination)
LYAH has a section on sequenceA: http://learnyouahaskell.com/functors-applicative-functors-and-monoids
Frege features an interesting twist on this: https://dierk.gitbooks.io/fregegoodness/content/src/docs/asciidoc/underscore_dot_notation.html
why not just use records?
Already known indeed! `transpose` was one of the examples used in the [original paper](http://www.soi.city.ac.uk/~ross/papers/Applicative.pdf).
I don't have the answers. It might be easier working with aws since it's all been implemented. Ovh uses openstack but that's all I've got just now.
We should change `Data.List` to follow this more natural behavior. P.S. Your statement is only true for most `n`, not all `n`.
If backwards compatibility issues were ignored I would 100% support that change. With backwards compatibility issues it could be tough, but you are right that you cannot make a verb that makes sense in both contexts. Mostly due to: (foo x) and (`foo` x) Reading identically but not meaning the same thing.
If only, like /u/dllthomas said, the creators of backtick notation had decided to make: x `foo` y Desugar to foo y x
I mean that still wouldn't solve the specific problem I am talking about. Such as modeling `(\x y -&gt; foo x bar y bazz)` in a more concise way. Would it?
Calculus of constructions allows encoding of inductive and coinductive types. Look at the Natural Number and List example. (Is that what you meant by inductive constructions.)
This may help: https://en.wikipedia.org/wiki/Calculus_of_constructions#Defining_logical_operators
My understanding is that the natural number and list examples in the post are not true inductive constructions. An example of something you can do with inductive constructions that you can't do in `annah`/`morte` is to define a length-indexed list with a type-safe `head` operation.
You can find a more tetchnical explanation about transpose [] in my post here:http://myhaskelljournal.com/heuristic-guidance-from-equational-reasoning/
&gt; Another method is to use plain text templates instead of Hamlet templates. I can't find it there. how?
I can't help but feel that this is a perfect use case for something like `pipes` or `conduit`. Then you don't need to keep all the files in memory and the memory growth isn't a huge issue.
I'm pretty sure I've closed tags without any trouble in the past... Have you actually tried it?
As a counterpoint to that: in programming languages we have a big problem with inertia due to network effects; this sometimes manifests as an economic problem of vendor lock-in and monopoly pricing. Sometimes the government even gets involved. Sometimes, when a programming language "succeeds," it's not because there are "two right ways," but because of inertia and how hard it is to switch from the wrong way to the right way. Sometimes, in fact very often, even the people who are choosing the successful language explicitly say they do not want to, but they have to go along because of some kind of inertia. (For example, although outside programming languages, probably nobody in the whole world thinks that IPv4 is better than IPv6, but also nobody has volunteered to give up their IPv4 address. Everyone waits for everyone else to do that.) But continuing to go the wrong way just because of network effects serves to perpetuate the problem even more! It increases the size of the network even more! So it's a problem. Now, is PHP an instance of this problem? I've deliberately avoided saying so. But one thing is for sure: a lot of people believe it is. Whether they are right or not, this article does not address their issue.
[removed]
`transpose = ala ZipList traverse`
yes, they got closed 2 times.
Actually you can just add any Hackage packages, with versions, as extra-deps to your stack.yaml file. That way you get the best of both worlds: a stable "backbone" of packages that are known to build together, plus any other packages you need.
&gt; We'll be opening the student application period on April 25th So... Today? 
a slightly lengthier answer if you're interested :) group [1,1,1,1,1,2,3,3,4,4,4,4] -- [[1,1,1,1,1],[2],[3,3],[4,4,4,4]] sortBy (\x y -&gt; compare (length y) (length x)) [[1,1,1,1,1],[2],[3,3],[4,4,4,4]] -- [[1,1,1,1,1],[4,4,4,4],[3,3],[2]] map head [[1,1,1,1,1],[4,4,4,4],[3,3],[2]] -- [head [1,1,1,1,1], head [4,4,4,4], head [3,3], head [2]] -- [1, 4, 3, 2] The `.` operator composes the functions. For `f . g`, it applies the arguments to g, then applies that result to f. In your case, map head . sortBy (\x y -&gt; compare (length y) (length x)) . group -- is equivalent to f . g . h -- written as f . g . h $ x -- x is the argument -- also written as f (g (h x))
&gt; I may not know the exact internal workings, but you should be able to massively improve performance by left factoring the grammar and removing as many trys as possible. And this is what I hate about parser combinators (as we have them now) :(
Isn't Option already a newtype that wraps any Semigroup in a Maybe giving a Monoid? ;)
logging-effect is missing from that list, did you consider that library?
Just use [stack](http://docs.haskellstack.org/en/stable/README/), at the moment the Platform is not the most recommended way to start.
[removed]
Thanks, stack is command line only, isn't it? I wanted to use the Plattform because it supposedly has an interface. On the other hand, do you know how i can uninstall the platform?
I had a labor-mill offering Clojure services ping me privately after I posted a Haskell job opening to a Slack network. They were banned shortly thereafter.
No snark intended, but isn't the key value of Haskell the fact that skilled developers in the language should allow you to avoid "a slightly extended time to market"? That doesn't address your developer sweat shop question. But if the language doesn't allow for high productivity with high quality, why use it?
No snark taken. That is just the hypothetical situation. It is in no way meant to imply that the delay in release was the fault of Haskell. 
I don't remember where The Platform installs; I think it's somewhere in the `/Library` hierarchy. But definitely get your hands on `homebrew` for OS X package management, and install `stack`. It's command line only, but honestly that shouldn't be a downside. The power it offers is *well* worth having to use the command line for it. Not using `stack` will result in some very difficult problems. As for IDE, I personally use [atom.io](https://atom.io), which has plenty of great plugins for Haskell. Here are the ones I use: * `language-haskell` - Every language requires its own plugin for atom to properly highlight syntax and provide very basic autocomplete. * `haskell-ghc-mod` - You first have to install the `ghc-mod` tool (`stack install ghc-mod`). This is the bulk of the IDE experience. It gives atom in-editor error reporting and linting, as well as features like the ability to hover over something to see its type. Absolutely essential to my workflow. But in order to get `haskell-ghc-mod` to work when you're using `stack`, you have to jump through a hoop. See, `stack` works by isolating different versions of packages and different versions of the compiler. None of it is made available globally by default, because the goal of `stack` is to keep them separate, so that different projects can use different versions of everything without anything colliding. You can enter a project's environment with `stack exec cmd` in your project directory, which runs `cmd` in a shell environment that has the project's version of `ghc`, and all the other tools. So, for the `haskell-ghc-mod` package, you have to launch atom using your project's stack environment. Easily enough, you just go to your project directory, and use `stack exec atom .` to open this directory in atom under your project's stack environment. With all this working together, you'll get the IDE experience that `ghc-mod` provides. * `autocomplete-haskell` - This uses the `haskell-ghc-mod` plugin to provide smart autocomplete, suggesting available functions and giving you a quick view of their types. * `ide-haskell` - This is essentially a front-end for various other plugins. It's the package that makes `haskell-ghc-mod`'s type info and error handling and whatnot all appear in the editor. If you have `stylish-haskell` installed via `stack`, it will also prettify your code for you. * `ide-haskell-stack` - This is the least important package. It just provides a couple of buttons that run `stack build` and a few other `stack` build commands, and displays their outputs in the editor. I honestly don't use it, because I prefer to use the command line for this stuff. With all this setup, I really feel like I have an IDE for Haskell. It's a little screwy, and not nearly as nice as something like IntelliJ for Java, but it's miles better than just writing Haskell with no tools. Definitely recommend it. But most importantly **USE STACK**. It solves so many problems that you'll never otherwise see coming.
Hey, joining the train, did you use emacs to be able to compare the overall experience? I never used atom, maybe I should take a look...
I'm a plebeian. I've never taken the time to learn `vim` or `emacs` =P So I can't compare to those, but if you've ever used Sublime Text, atom is basically just Sublime with all the issues fixed, way better plugins, and nicer... everything.
Haskell loses to ruby in terms of "time to thinking you got it right" and wins in terms of "time to actually getting it right" 
&gt; but I can't imagine it's a fast development process. I would guess that it's more maintainable. My recent experience, though anecdotal, says that Haskell is both faster *and* easier to maintain. The exhibit: simulating user interaction against arbitrary websites using PhantomJS. The first version was written in JavaScript (because that's what Phantom expects); the second version, in Haskell, took less time to develop and turned out a lot more reliable, despite having to invent a Haskell/JavaScript bridge in the process.
Hypothesis: all the benefits we love to tout about strongly typed functional programming languages, if real, make them well suited for outsourcing. Easier refactoring, reduced amount of code, somewhat lessened need for unit testing, types as (human language-agnostic) documentation... they should reduce the amount of "folk wisdom" that must be communicated for successfully transferring a project overseas, especially if the focus is maintenance instead of new features. And that even accepting that functional languages might be more difficult to learn. Transferring "folk wisdom" must be done for each project. Learning a language â€”assuming you can retain talent in your organizationâ€” is accumulative.
That's plausibly because you had already modeled the problem; if you'd written the first version in Haskell, it might have been as bad as the initial Javascript version.
Not so, mostly all of the client we signed were in a situation close to, if not exactly, the hypothetical.
Actually, I think they all correlate with the [g-factor](https://en.wikipedia.org/wiki/G_factor_(psychometrics) (and strongly I would guess), but this correlation isn't perfect of course. Capitalism can bring out the worst in us, not because it is bad, but because it is an amplifier.
I think lightening the mood won't work. :(
Is it easy for someone to mirror a snapshot for their own usage (private server)? (mirror = stackage snapshot + hackage downloads)
Becoming fluent at reading typeclass instances is also important, because a lot of functionality is hidden there: (Applicative f, Applicative g) =&gt; Applicative (Compose * * f g) "Mmm, so if you compose two applicative functors, the composition is automatically an applicative? Cool!" (Monoid a, Monoid b) =&gt; Monoid (a, b) "A tuple of monoids is also a monoid." 
The package index can definitely be cloned, though I don't remember the details of the syntax. Check out the yaml configuration details on haskellstack.com for details
[removed]
One question is whether these workers have anywhere better to work. If not, you might actually say this is a good thing, since they would have to do something worse if this wasn't in place.
Maybe we should add a way to put hashes of cabal files somewhere.
I wasn't eligible to vote, but he had/has my vote. I'm glad to see him continuing to be involved and leading things well.
haskell.org was not accepted this year, here are previous posts on the subject: 1. https://www.reddit.com/r/haskell/comments/48eurt/haskell_summer_of_code/ 1. https://www.reddit.com/r/haskell/comments/4dm1nc/summer_of_haskell_status_update_and_call_for/ *edit*: ah, never mind, I only now noticed that you called it "Haskell Summer of Code" instead of "Google Summer of Code", indicating that you're already aware of this subtlety :)
&gt; even just a few months in, the type system will work in your favor In my experience, using Haskell professionally, in a context where refactoring is frequent the type system is working in my favor in the first few *hours*.
/u/edwardk has been saying he'll announce Real Soon Now :-)
No big thing here, I just wanted to share it because there are so few example of `GHC.Generics.datatypeName` out there, and in Haddock there are no instances for `Datatype` show (see https://hackage.haskell.org/package/base-4.8.2.0/docs/GHC-Generics.html#t:Datatype).
I'm sure you are right. However, I am looking backward not forward. If I write important code today, I probably want it to run in ten years without changes.
&gt; Instead, I mean a long-term support Haskell that people can code to and teach to for an extended period of time. How is a ten-year+ lifetime help support the above when teaching and coding techniques are changing, arguably improving, all the time? 
Most other languages are more afraid of backwards incompatible changes, so you generally don't have to worry about this kind of thing. The downside being that those other languages don't progress as fast.
Oh hey, this is a perfect example of what I'm talking about: https://www.reddit.com/r/haskell/comments/4f47ou/why_does_haskell_in_your_opinion_suck/d2h2u96
We need to take away their best option to show that we care!
Who said it was their best option? In any case you can read about how over 2000 modern day slaves were just freed from a southern Asia fishing setup. A Pulitzer prize was given to the journalist exposing the injustice. Reports show that many of the slaves thought it was their best option.
Then why don't you just keep using the same compiler and libraries you did 10 years ago, today? There's no enormous reason you couldn't use GHC 7.10.3 and whatever set of packages you chose today for the next 10 years, short of some horribly catastrophic failure somewhere (and any technical reasons, most fixable I'd think). Red Hat is easily supported for 10+ years, pay for it and then deploy your systems on that and you'll have a base to build on. The real problem here I assume, labor-related. You want bug fixes and 'support' for this with a 10 year commitment and signature on the dotted line, but who will provide that? I suppose that's the question you're asking, but also, it's kind of the answer too, when you read it rhetorically. More specifically, you're asking to see if there's enough collective will to subsidize the cost of that (presumably since nobody wants to pay it themselves). It is not just GHC that must provide that. Libraries must commit to this as well. If package authors don't support more than the past 3-4 years of GHC versions, there's much less point in supporting GHC itself for 6-7 years past that point. You're still stuck with the same libraries, and if you want anything new from them, which will be the bulk of your functionality - presumably - you're back on your own. So you need a *lot* of commitment to this idea for it to be broadly useful, and that will, presumably, take years just to change (because it's a shift in the entire way most of the ecosystem treat things, now, as is their habit). IMO, at least. In lieu of that, if the upstream developers do not want to support things for 10 years, then you have to do what you've always done: backport fixes, etc into your own toolchain and build system yourself. Vendor dependencies yourself and maintain your own forks. Ensure that things still work for yourself. The work has to happen, somewhere. This is not uncommon, nor unique to Haskell and its community. At my first job, we maintained our own builds of (then recent) GCC/JDK versions on our build machines, which were ancient CentOS nodes, we vendored all our dependencies or had pre-built artifacts for consistency, etc. We had backports of random things to older versions of Boost, components we had completely rewritten, and bugfixes (or hired people to rewrite and enhance, then forward-port upstream.) This was done in the name of longevity, like your proposal. 10 years was a realistic timeframe for this project to maintain some things. This was for a large combo Java/C++ client/server setup (few 100 kLOC in total). That was all because we had to distribute to clients on that platform. Nobody supported this setup for us, and we did it ourselves instead of paying anybody else. Much of this work didn't have much to do with C/C++ or Java evolving rapidly at the time, honestly, as much as it did "that stuff is old and most people have moved on so you're on your own". This was in 2009, FWIW. As noted, there are also people who will do this work for you (*insert obvious self-advertisement here*). But only for money, as usual. I guess my point is, there's nothing necessarily stopping you now from accomplishing what you want - today - other than the size of your own wallet, or your own time. Like I said - what I think you're getting at is, you want is a subsidy on the cost of all this work - more people to join in and make this need clear, and collectively share the cost of having to deal with it. That could happen. But honestly? I feel like I've had this discussion several times before, and every time people come up empty handed or without any commitments. And based on similar experiences, I'm not actually sure the need is really there, nor that it's Haskell specific. It'd be a good business opportunity to look into, though.
I'm eagerly waiting as well!
Okay, suppose it is not their best option. Would it not then be more efficient to inform the employees that they have better options? Convincing people to avoid something convenient because it is unethical rarely seems to go well (see: global warming, meat-eating, etc.), primarily because people don't like sacrifice.
thanks. Fixed it.
touchÃ©
 deriving Eq
If stack doesn't make any breaking changes, I don't see any reason that it shouldn't still work when code is ten years old.
thanks for the detailed feedback. I did fix the two obvious mistakes so they won't confuse other readers. I see what you mean by applicative vs. monad as "parallel vs. sequential". Your point is I think what I meant to describe in footnote 4 of the blog post. http://www.holger-peters.de/haskell-by-types.html#f4 
As with all such arguments "its all ok, as long as **I** am not the one being abused."
So we don't have another Real World Haskell situation: Tons of effort put into making a great educational resource, only to stagnate due to rapid ecosystem changes.
This is a superb post; Lots of solid information plus links to great ways of learning more. I use friends.reddit.com/comments as my primary entry-point into reddit in an attempt to keep the signal-to-noise ratio high, and your posts are consistently excellent. I appreciate it!
Edward sums up my reasons for voting for Gershom quite well. Simply put, the job of the chair requires quite a bit of time and energy, and I am continually impressed by Gershom's skill and dedication at it.
monad-log can easily have namespace too, `Control.Monad.Log.NameSpace` module provide a simple `NameSpace` type, which you can directly use in a `MonadLog NameSpace m`, or embed it into your own logging environment data type.
I don't have problems with that...
Technically the proletariat *is* the ruling class in most of the world at this point.
Indeed. I'm updating the site right now. I'd planned on doing it during the flights I've been on for the last 24 hours, but then unexpectedly lacked internet access.
Very close. We've put up a [summer.haskell.org](http://summer.haskell.org) site, and I'll throw a link to the google form to submit applications on top shortly.
Everything evolves and I think there's value in teaching the students that things change and how to deal with it. Emphasize the importance of testing and using refinement types for lightweight formal specifications. About RWH. It was real-world back then. It's not anymore and it would be much better to update it instead trying to stop the world. 
Unfortunately not everything is better with Atom: performance is definitely lagging. Otherwise it's almost perfect! 
lol What are you talking about?
:\^)
Not really Haskell related, but this puts a rather "evil" idea in my head for how Ecuador can deal with this problem. A bit of Hayekian creative destruction doesn't have to wait for the bad times. The government could take the worst performing companies into conservatorship after a period of poor performance, compensating the stockholders based in part on reported profits, and sell them off to new owners. It's like stress testing the economy, you know? I imagine firms will no longer want to underreport profits.
So pretty much what Cuba did and then got blockaded?
I just left a job and don't like the company. Nothing suspicious here.
I didn't know about friends.reddit.com, looks convenient!
Spearman's g-factor is a statistical abstraction over test subscores having to do with cognitive ability. There is no reason to think it has anything to do with with morality.
According to various sources, it correlates with a ton of things, like Happiness and [Kindness](https://books.google.com/books?id=qvBipuypYUkC&amp;pg=PA7&amp;lpg=PA7&amp;dq=g-factor+kindness&amp;source=bl&amp;ots=dK9zYmW371&amp;sig=dAzU6SlSbNPor8mNYTu-DGFnTgE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjUxOSfpKvMAhVEsIMKHV9WD4AQ6AEIHTAA#v=onepage&amp;q=g-factor%20kindness&amp;f=false), so its not too much of a stretch to suppose that it correlates with morality. I'm just Haskeller, not a psychologist though.
Thank you, this is excellent info. Started left-factoring tonight and will continue tomorrow.
Yes, you can always use -XNo..... to turn them off.
The interpolate package
Though I use `OverloadedStrings` constantly, I do find many instances in which explicit type annotations are necessary (despite the OP saying it happens rarely). An example is in Aeson's `object` function: if `OverloadedStrings` is on, all string literals in an `object` invocation must be given an explicit type annotation. 
These laws are assuming a semantics for IO, but there isn't one. In particular, I'd be interested in what you mean by "=" here.
https://www.reddit.com/r/haskell/comments/4gh1tu/summer_of_haskell_now_accepting_applications/
[acme-everything](http://hackage.haskell.org/package/acme-everything), of course. 
Capitalism being only one of countless avenues where greed is the true culprit.
To think this should happen to a classless and stateless programming language! 
I am generally interested in graphics so I'd like to work on projects in that area. I don't have concrete ideas but a few areas that interest me are: - working on the POVRay back end for Diagrams. - making a declarative UI framework for mobile/web development on ghcjs/purescript. - working on the Haskell OpenGL bindings or writing a wrapper that makes graphics code look less imperative. - webgl library for ghcjs. Or lay the foundation for a larger 3D library similar to three.js for ghcjs 
Getting a good WebGL library for ghcjs could be interesting. When I was working on the `gl` package, I tried to keep in mind how everything would need to change to support a WebGL backend for it, and ultimately decided I'd have to build a low level `webgl` package, and then let something common sit atop both `gl` and `webgl` to exploit their similarities (can run many of the same shaders, etc.) while acknowledging their differences (you don't want to pay for pointer accesses to needlessly mocked up typed arrays to do _everything_ in OpenGL when targeting webgl. The API should reflect the right practice for the target.
Yes don't include `OverloadedStrings`.
One project for this summer will be specifically about CodeWorld, a web-based educational environment using Haskell. If you're interested in this angle, I've added some thoughts here: https://cdsmith.wordpress.com/2016/04/25/codeworld-summer-of-haskell-2016/
For graphics programmers: lambdacube3D. Shaders are pure functions on their own and it allows you to code shaders through haskell without any side-effects.
Was there any disagreement regarding `NoMonomorphismRestriction`?
With `RebindableSyntax` you can specifically overload the `fromString` function, so I think you could make string literals monomorphic with something like `fromString = Data.Text.pack`, if you don't want `String` but don't actually need them to be polymorphic. I think a less general way to name a different monomorphic type for string literals might be preferable to having `OverloadedStrings` by default.
https://hackage.haskell.org/package/vulkan
I'm interested, but I'm unavailable this summer. However, I have an end of study project to do next fall semester (that can be continued to the winter if needed). Is there some chance I could get into a program like this? I would love to contribute to the Haskell community while getting some University's credit
John Wiegley's Git packages are cool. He's got repository lenses. I made an isomorphism between my app state type and a folder hierarchy of JSON files. Made it so that each JSON file represented a piece of state in such a way that the monoid sum of each parsed file, commutatively, gave the full app state. Then stored that in Git. Also aeson-diff which can generate structural diffs in the JSON Patch format. If your state is in Git, then your client can ask "what's the difference between revision X and the latest state?" and you can quickly generate a small diff using Wiegley's Git stuff and aeson-diff, and apply it with any JSON Patch application library.
Is there a function that goes back to create an isomorphism?
You can 'compile' it by erasing all types. This would preserve the semantics and prove that there are no types needed at runtime.
&gt; The most legitimate argument for Python being faster to write is library support. This is what I had in mind with my comment.
Not quite as funny as you think actually :) {-# LANGUAGE MonomorphismRestriction #-} Similarly, there `ImplicitPrelude`. GHC is nicely consistent in the naming.
I'm under the impression that `MultiWayIf` can parse misleadingly. e.g. if | foo = ... | bar = if | this = ... | that = ... | t'other = ... | baz = ... The baz case looks like it is part of the top level if, but it is part of the inner if. Has this been fixed? 
The layout bug has been fixed with GHC 7.8 according to [this](https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions/basic-syntax-extensions#multiwayif) blog post: &gt; WARNING: In GHC 7.6, the use of MultiWayIf doesnâ€™t affect layout, instead allowing the previous layout (prior to the if keyword) to remain unchanged. This was changed shortly afterwards; in GHC 7.8 and later, MultiWayIf affects layout, just like ordinary function guards do. I can confirm that it is not reproducible with GHC 7.10.3. 
The other thing that happens is that languages get new versions (different to compiler versions), and backwards incompatible changes are allowed across language versions but not compiler versions for the same language. Look at Java, for example. Java 8 came out fairly recently and has loads of breaking changes, but Java 7 will still be maintained (with bug fixes etc) for the foreseeable future. I think Haskell would benefit greatly from a system like that, but it's just a question of who would be willing to do the less interesting job of supporting older versions, not being allowed to introduce any new innovations. The language standards were sort of meant to achieve this, but with the large changes to `base`, lots of code originally written for Haskell2010 doesn't compile any more.
There's also ExtendedDefaultRules which lets you set a default for stringlikes.
Are you referring to author names? I see absolutely no reason why we should require that.
My biggest gripe with Haskell has always been how hard it is to make a $&amp;*&amp;Â£! histogram. This package looks amazing.
Yes, of course that can be done, in fact `logging-effect` is solving more general logging problems, eg. in pure context without io effects. OTOH, monad-log focus on manage logging in `MonadIO m`, it leverage fast-logger to manage all the io so that user don't have to provide their own, and proper flushing on exception..etc are built-in, so they are not competing on the speed, but on the usability of different use case.
What I'm saying is that you're really proposing "Let's just sweep the abuse under the rug.", rather than "Let's solve the abuse once and for all.".
&gt; Other safe extensions (there was no feedback against having them on by default): &gt; &gt; * ParallelListComp (7) Then let me be the first to speak out against that extension. I was bitten by this once when it was part of `-fglasgow-exts`, by writing `[foo x y | x &lt;- xs | y &lt;- ys]` instead of `[foo x y | x &lt;- xs, y &lt;- ys]`. This one character difference does not lead to a type error, but rather a program that does something completely different. If this extension is on by default it will lead to hard to find bugs, especially because many people don't even know that parallel list comprehension exists.
For the record, this is already supported by all-cabal-hashes, which uses Git's native support for history to provide this mechanism. It also makes it really easy to get access to the historical revisions (via a simple `git show` call).
Yep, here's an example: {-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE OverloadedLists #-} module Example where import Data.Text import Data.Map.Strict numbers :: Map Text Int numbers = [("one", 1), ("two", 2)]
This hp/cabal vs stack disagreement is starting to look a lot like [this](http://i.imgur.com/z1nADRj.jpg)
This about it like this. The pure bit of your program isn't doing anything out in the real world. It's joining together instructions to make an action plan. So the `&gt;&gt;` in `addTeaBag :: IO () &gt;&gt; poorWater :: ()` concatenates the two sub plans into together into something I might choose to call `makeTea :: ()`. The interesting thing is `&gt;&gt;=` means you don't need to finish the entire action plan before the runtime can start acting on it. You can start doing things and then use the results of what happened to decide what to do next (sort of agile-y if you think too hard about it and like industry buzzwords :) ).
If data types would have a proper [Lift](https://hackage.haskell.org/package/template-haskell-2.10.0.0/docs/Language-Haskell-TH-Syntax.html#t:Lift) You can force compile-time evaluation of `fromList` for example by: `$(lift ([("one", 1), ("two", 2)] :: Map String Int))`
SHA-1 wasn't chosen in Git for its cryptographic qualities, and you wanna read [this](http://blog.sn4t14.com/post/5/2016-03-08/You're-a-moron,-Torvalds,-not-a-cryptographer) before putting too much faith into Git for cryptographic purposes. It's still not easy to break for the general population, but cryptographically speaking, Git is not state of the art.
I'm trying to make the point that maybe different technical solutions could coexist peacefully as neither is universally better than the other
I'm definitely down-voting FlexibleInstances.
My vote goes to `monad-loops`.
This is not a serious moral argument though. Analogously, if you smoke, then you like an idea of having cancer more than you like the idea of not smoking. If you use addictive drugs, then you like cocaine more than you like living a prosperous life. If you bang dirty hookers without a condom, then you like having STDs more than you like an idea of not banging dirty hookers without a condom. It seems this marvelous model can explain all aspects of human behavior. What it does not explain though though are changes in collective well-being, which is what morality is concerted with. If one voluntarily chooses to have cancer (e.g. by excessively smoking), it does not mean that it's automatically the most moral choice out there. The effect that choice is going to have on his own well-being o happiness of people around him is just an externality when it comes to that economical argument. On the other hand, collective changes in well-being and happiness are foundational to any rational moral framework (e.g. consequentialism). So, you can't really say whether something is good or bad by looking at which economic choices people make. edit: rephrased a bit
Sure, that's also what I'm proposing by informing them that they have better options. It's more the overthrowing-of-the-ruling-class that seems unnecessary and bad.
Well if you don't think the ruling class are a horrible, violent social structure that threatens the long-term viability of the human race and gorges itself on the underclasses, i could see that. But, you know, that would be a very strange observation in our world.
That's so clever. I've actually wanted this for a long time, that is, string literals just being `Text`. Thanks.
What about including `NoImplicitPrelude`? We just recently had the discussion on beginner friendliness and different preludes with varying degree of target audience. Wouldn't an explicit prelude also help beginners by making the imports more obvious? I'm tending towards `Glasgow2016` though.
Worse, IIRC, `MonadComprehensions` significantly slow down list comprehensions.
This might help! :) http://www.happylearnhaskelltutorial.com/1/main_road.html But basically by the form of their combination (how you combine the elements together implies order).
Would love to apply, but I will be at an internship already.
It's a good concept but I found it weird that list notation such as [1,2,3] and 1:2:3:[] are *not* considered equivalent (it looks like constructors evaluate to the list extension). In particular, you cannot simulate lazy evaluation e.g to evaluate `map addOne (map addOne [1,2,3]))` you have to fully evaluate the inner map before applying the outer map to the head. 
If you want to try this out with Stack, use this [stack.yaml](https://gist.github.com/tfausak/8e9f3733450f929059aa704d6d8cf12a): compiler-check: match-exact resolver: ghc-8.0.0.20160421 setup-info: ghc: linux64: 8.0.0.20160421: url: https://downloads.haskell.org/~ghc/8.0.1-rc4/ghc-8.0.0.20160421-x86_64-unknown-linux.tar.xz macosx: 8.0.0.20160421: url: https://downloads.haskell.org/~ghc/8.0.1-rc4/ghc-8.0.0.20160421-x86_64-apple-darwin.tar.xz
I also require a definition of "indistinguishable" ;)
Aren't there many one-character bugs that can wreak equal havoc on your programs? I've even enabled parallel list comprehensions in CodeWorld, and middle school students find it intuitive and easy to use! I think this whole effort is misguided... but that one less than most.
The link you provide is not saying that kindness is g-loaded. Flynn is explaining the concept of g-loading, which is just the observation that some subscores correlate more strongly with the g-factor, by using an example. &gt; people who where good tended to be further above average in terms of kindness than tolerance Here he is drawing an analogy between the g-factor and some "goodness factor." In this case he makes the claim, more to illustrate the point than because it has rigorous scientific backing, that kindness is more strongly correlated with goodness than tolerance is. The concept of g-loading is not entirely uncontroversial. The g-factor is derived from factor analysis of a set of sub-tests. A test is said to be highly g-loaded if it correlates strongly with the g-factor. Then high g-loading just means that a subtest behaves very much like the average subtest from the test cluster. On top of all this psycometricians use g-loading to try to determine what makes a good subtest. The fact that g-loading has an impact on which tests are chosen means that tests can become g-loaded because they are g-loaded. The positive feedback loop inherent in intelligence testing makes the whole process suspect. It seems like you have read a bit about IQ from the camp of classical psychometrics. If you want to learn more about some of the problems with IQ I would recommend reading "The Mismeasure of Man" by Gould. He sees himself as a bit of a crusader agains the dark arts of social science, so some of the stuff he writes is a pretty clear distortion of the truth, but he does raise some really good points. Of course if you want to learn more about the dark arts you can also read "The Bell Curve" by Herrnstein and Murray.
Thanks, this is the answer that I'm expecting. 
I wondered about GADTs myself. Nobody suggested them and they were not even mentioned. There are some issues with deriving for GADTs and pattern match exhaustiveness checking (improved a lot in GHC 8). Stepher Diehl listed GADTs are benign.
This is what I hope to achieve. So much of my day is spent messing with code I cannot ever hope to fix and dealing with a bureaucracy that has come to expect outages on any significant change.
When you parse code from an external input it makes more sense to use simple structures since they're comming from an untyped world. It makes more sense to use fancier representations for EDSLs because you want to utilize the host language's facilities as much as possible.
They certainly seem a lot more competent than all other possibilities. I mean, the obvious comparison would be the populist right - I'll take neoliberal elitist technocrats over them any day.
By the way, polymorphic variants and records seem to solve the expression problem, and they are rather simple. Am I the only one who thinks so?
I do not have a good practical example at hand, but I recently learned that one practical application of expression problem is to look at it "backwards": how can it be ensured that certain forms are removed from the expression without duplicating code. For instance, a good solution to that problem would allow you to implement a de-sugaring process in the compiler and express that de-sugaring on the type-level in a modular way. You may find extensible variants (Morris, 2015) interesting, which I think offer a fairly good solution using type classes and closed type families. Also, Morris gives a nice overview of other existing solutions. References: Morris, 2015 -- http://homepages.inf.ed.ac.uk/jmorri14/pubs/morris-haskell15-variants.pdf 
Correct - and anything that *isn't* a highest priority regression isn't stopping it any further. So, find those bugs, please!
The situation is actually surprisingly subtle. It's not really that "`FlexibleInstances` can be used to violate coherence", as I wrote. It's *more* accurate to say that "`FlexibleInstances` can be used to violate coherence even without defining any orphan instances" -- with separate compilation and orphan instances, coherence can already be violated trivially, of course. But even *that* is not *fully* accurate. The accurate diagnosis is that one or more of the `instance`s in the linked example *is actually an orphan*, even though it doesn't *seem* that way. In other words, the implicit rule that "an `instance` is an orphan if *and only if* its head mentions neither a type nor a `class` defined in the same `module`" is naive and insufficiently stringent. Rust's designers ran into this same issue not long before their 1.0 release, because unlike Haskell they *do* want to ban orphans, and they discovered that with MPTCs and flexible instances it's actually much more complicated than it sounds. [There are many different ways the rules can be formulated](http://smallcultfollowing.com/babysteps/blog/2015/01/14/little-orphan-impls/), but the gist of it is that either `instance Show (Foo MyType TheirType)` or `instance Show (Foo TheirType MyType)` *must* be an orphan, in other words: *the order matters!* (The same applies in a `MultiParamTypeClasses` scenario.) And this feels wrong at first to just about everyone (including me), but the reason it actually *does* make sense is that coherence with an orphan ban -- in other words, with a strict link-compatibility guarantee -- is zero sum: for every potential `instance` that a given `module` is allowed to define, all other `module`s must be prohibited from doing so. Put another way, for every possible `instance`, there exists *exactly one* `module` which has the right to define that `instance`. So given an `instance` which mentions types from two separate `module`s in its head, you must choose one of the two to give the right of definition to; based on, for example, which of the two types comes first. ------ EDIT: To tie this back into the `Glasgow2016` thing more explicitly, the issue could probably be phrased most usefully as "simple, obvious, and naive orphan rules are no longer sufficient to prevent link-time coherence violations in the presence of `FlexibleInstances` or `MultiParamTypeClasses`" (and the more complex but sufficient rules are neither obvious *nor unique*). It does seem like `MultiParamTypeClasses` might simply be fundamental enough that you'd want it in any `Glasgow2016` regardless; and once you've already paid the price for that, there's no longer much reason to hold back on `FlexibleInstances` either. And the real-world impact is probably not that big. It's just good to be aware that the impression of these extensions as clearly benign and unproblematic isn't *entirely* accurate.
Yes.
`lambdacube3d` has somewhat different goals than I do in this regard, but it is definitely an interesting case study in a design that works in this space.
With so little information, it's hard to tell whether the problem is in your code, GPipe, LLVM, or GHC. My fool-proof method in hard-to-debug cases like those is: minimize the program as much as possible, removing all non-essential dependencies, then inline your dependencies and keep minimizing until the problem is obvious or at least small enough that you can report it to the proper maintainers.
no one said git cryptography is state of the art, but it's still quite strong *despite* using SHA-1. While SHA-1 is not recommended those days, the way git uses it make it far more resistant than the underlaying hash. none of the links are actually relevant to git right now.
A given lts from stackage, which is available for use via stack or via cabal, is totally fixed. It's as immutable as the number 5. The only downside is that you probably want the new versions of everything eventually so that you get new features. You didn't have to go past GHC 6.8, you didn't have to go past GHC 7.4, but you probably did to get new features. If you don't care about added features, just never upgrade anything and it will work for as long as your hardware runs.
Please use the bug tracker instead of reddit to report bugs: https://ghc.haskell.org/trac/ghc/
[removed]
&gt; neither is universally better than the other Not even close.
[removed]
I think you are right.
Implying the populist right isn't a product of neoliberal policies.
I would love to apply! But to be honest, I do not have quite big skills. I have written many other languages, even in very big and long-term projects, but haskell is my first functional language. I have no idea what a newbie like me could contribute to the community, and mostly I would profit from this by really learning haskell. Should I apply?
&gt; Am I the only one who thinks so? No! I think most people either (a) don't know the concepts, or (b) underestimate their value.
 Talk by Matthias Blume: http://research.microsoft.com/apps/video/default.aspx?id=104004&amp;r=1 Discussion in LtU: http://lambda-the-ultimate.org/node/4394 
I sort of stayed out of the voting and only glanced in that thread a few times. But seriously -- pattern synonyms, partial type signatures, type families, *type application*?!? One of those features doesn't even *exist* yet in any usable form in any actual production compiler (no, -rc4 does not count), so you can bet it will change spectacularly in time. The other two have gone under a massive number of revisions in the past few releases as features and extensions (injectivity, closed families, pattern sig signatures) have been refined several times in different ways. Yet, on the other hand, somehow applicative do is *too early* to include? And everyone remained suspiciously silent on GADTs, despite being, IMO, one of the more well-trodden areas. Several of the other features would basically imply type-equality anyway. I imagine the largest, strangest inconsistencies basically arise from the fact Reddit is mostly a terrible place for making these decisions. Many of the remaining things are benign or otherwise fine IMO (e.g. the syntax sugar, EmptyCase, lambda case, binary literals, comprehensions). By themselves, those could be swept into a new standard pretty easily, IMO. Others have more scope creep than you think (if you get rid of Type Families, you have no `DeriveGeneric`). Some others are a little grosser than people think (`MultiWayIf` was broken when we first implemented it, and it's a bit ugly to fix IIRC). But some of these are just off the wall in terms of complexity and language impact, regardless of how much people like them (and are willing to forget or step around the problems). Given all that, IMO, the list by itself is not worthy of `-XHaskell2016`. It's more like `-XGHCBehaviorIn2016` or `-XGlasgow2016`, for short. But many of these extensions are still not fully understood, and they add substantial complexity to the language, even if they were. That's way too risky.
Whether Haskell is faster or not depends on what you're making and what you're comparing it against. For a web 2.0 social app, Rails is going to be faster for getting something to market, but then the Hell really begins. This is partly because there are just so many damn libraries for Rails apps. /u/ephrion has it.
I'm not sure where you're going with that argument. Yes, the populist right is a product of neoliberal policies. It is also a product of social-democratic culture, sensationalist media, human irrationality, leftist science-denial, muslim refugees and other things. While some of those things are bad, others (helping refugees, and, IMO, neoliberal policies) are good.
Given that we only have 3-4 slots, I confess that it'd be rather difficult, but not impossible, to get a purescript project accepted collectively by the pool of potential mentors. If the purescript community were to figure out how to collectively fund a slot, we'd happily earmark and administrate it, though.
There's a way called "Object Algebras" that "is a relatively lightweight pattern that doesnâ€™t require fancy language extensions. In particular, it can be easily implemented in mainstream languages like Java and C#." https://oleksandrmanzyuk.wordpress.com/2014/06/18/from-object-algebras-to-finally-tagless-interpreters-2/ (c.f. â€œExtensiblity for the Massesâ€ https://www.cs.utexas.edu/~wcook/Drafts/2012/ecoop2012.pdf) it does seem to require embedding an interpreter inside a language, though.
Well, i think i'm going in two directions: Even if neoliberalism is a local optimum for whatever your heuristic is, i think it's plainly obviously not a global optimum, and not even a particularly good local optimum. Second, neoliberalism creates the conditions that lead to its instability; it cannot be maintained indefinitely because of the structural defects of capitalism; if a socialist revolution does not succeed, a fascist one will. Personally, i don't need to justify my beliefs by pretending i can fully understand the system of global capitalism that grips all human societies. I know it's broken and I think a libertarian socialist society would be best for me. So if it's going to collapse, of course i'm going to endeavor for that collapse to be in my best interests.
[optparse-generic](https://hackage.haskell.org/package/optparse-generic) is very very cool. 
If you don't like having `Typeable` constraint, you can create your restricted class, and give it single global instance. The way Haskell works today, it cannot conclude `Typeable` from `TypeName`, so you don't give your functions the whole power of `Typeable`. class TypeName a where typename :: proxy a -&gt; String instance Typeable a =&gt; TypeName a where typename = show . typeRep 
It's the same way a list "controls" the order of its nodes when it is traversed. 
We decided this year to mimic the structure of a program that we know well, from having participated in it for several years. Copying the general structure of the program was a lot easier than ensuring there wasn't any "gotchas" or employment laws that could catch us unawares. If we decide this is a thing we want to repeat, or do other things that involve paying a stipend for work in the future, then we may well try to open things up for those projects, as I've received a couple of requests along those lines, but for this year we'll stay true to the spirit of GSoC.
The only worry I see with that is you might risk the language being successful. And everyone knows we have to avoid that at all costs.
Was worried someone might say that, I might take a gamble and try a minimal glfw-b example.
No one claims that they're defining a standard with -XHaskell2016. No standards are coming anyway (the process/effort couldn't be more dead in the water), and at this stage people just want a way to turn a bunch of probably good extensions on by default/alias. Someone in the community is trying to be useful by getting some stats/polls about extensions, and it would be damn useful that someone listen on the other side, instead of the usual stonewall of "lusers are voicing their useless opinions again. bah !".
Haskell standards are agreed on in some formalized process. This is the community informally trying to define a safe and sane extension of the Haskell 2010 standard as a set of compiler-specific extensions.
I was just reading through[0] and stumbled upon the NegativeLiterals extension. It seems to make literal negation a bit more predictable and correct. Would there be a reason for not including something like this in a -XHaskell/Glasgow2016 extension? Would it change the default behaviour too much (although looks like it is for the better). [0] https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions/basic-syntax-extensions#negativeliterals
a more constructive response is to clarify the question: monads don't control the order of evaluation in the lazy language of haskell, the `IO` type does (which happens to be a monad, as /u/beatricejensen mentions). I would start with the fact that haskell has `case`: case e of Left l -&gt; ... -- we know `e` has been evaluated to `Left _` Right r -&gt; ... -- we know `e` has been evaluated to `Right _` and then read these detailed explanations: https://www.fpcomplete.com/blog/2015/02/primitive-haskell http://blog.ezyang.com/2011/05/unraveling-the-mystery-of-the-io-monad/ 
This looks like a super great way to explain stuff like folds to people that have never used them before! I remember grokking them after going through the evaluation on paper, doing pretty much what is done here. Great work :)
You can solve it with ["open witnesses"](https://hackage.haskell.org/package/open-witness), I believe. Basically you have a type `IOWitness :: * -&gt; *` that's an instance of `TestEquality`, and given any type `T` you can create a value of type `IOWitness T` at top level using a snippet of Template Haskell, like this: myTWitness = $(iowitness[t|T|]) (Internally, `IOWitness` just contains an `Integer`, which is freshly generated each time.) This is enough to allow you to create an expression that can be extended with new forms: data Extendable = forall t. MkExtendable (IOWitness t) t You can then add interpreters for `Extendable` like this: interpret (MkExtendable wit x) = case testEquality wit myTWitness of Just Refl -&gt; doThingWithT x Nothing -&gt; doOtherThing Note that all the forms you add to the expression will be kept separate, even ones with the same type. You can also use open witnesses as a replacement for `Data.Typeable` and `Data.Dynamic`. That said, you can equally solve the expression problem with `Data.Dynamic` anyway...
&gt; instead of the usual stonewall of "lusers are voicing their useless opinions again. bah !". Since you apparently want to read into my words, allow me to read into yours: you want change to happen because you're frustrated, not because you actually have compelling understanding of the impacts these things could bring with them, because you're willing to conflate "having experience and reservations with the implications of these suggestions" with "stonewalling because he disagrees!!!" Furthermore, I'd also suggest I *flat out* actually have more experience with seeing the first hand implications of what changes like this can bring (especially when users just constantly seem to bitch at us - like you're doing now - no matter what we do, whether it's changing something because people wanted it and it broke, all the way to not changing things), and as a result I have no problem suggesting that my say should hold more weight than others on certain matters - although I'm sure you'll think that's elitist and I'm very mean or whatever. Now, reading into words like that isn't very nice. So let's stop doing that. In fact I believe I specifically said that a lot of these extensions are benign; my problem is more that the decision making process being done here is half assed and Not Really Compelling if you ask me, because there are wild inconsistencies in what could be suggested. It's right to raise a concern about that kind of decision making process, because it can end up causing us more pain down the road. "The Community" isn't coherent at all on these matters. Now, do you have anything to actually counter the points I made regarding these inconsistencies (like throwing a lot of actually benign things in - perfectly OK - with very much not-entirely understood extensions, or even ones *that have been released*), or are you just going to say "I want it", go home and play with your ball because I'm mean?
Have a look at the PI Calculus and functional programming, a google search produces a lot of stuff. There is even a language called PICT based on the pi calculus: http://www-sop.inria.fr/mimosa/Pascal.Zimmer/mobility/pict.pdf
committees and community seem to share only a common 4-letter prefix these days
The community shares this view... It's mostly the committee that carries on in its detached bubble deciding against the community
i want my compiler *now*, seipp
&gt; That said, you can equally solve the expression problem with Data.Dynamic anyway... It is well known that in the untyped (or dynamically typed if you prefer that language) case then the expression problem trivializes. In fact, if we sacrifice type-safety then the problem trivializes in a typed language too.
Yes, there has been another rework of the pattern match checker. It should now avoid blowing up during compilation although it may instead give up (with a warning telling you it did so) in the case of complex pattern matches.
For the record, these aren't quite yet official. We are currently waiting for a few binary distributions to come in before declaring this release final. When this happens I'll send an announcement to the usual places.
The [These](https://hackage.haskell.org/package/these-0.6.2.1/docs/Data-These.html) package and the align function(s). It allows to join (in the SQL way) Maps and other *containers*.
I've heard this as an explanation for `IO` specifically, but that's the wrong way to think about it. It's helpful to think of IO as separating execution from evaluation: just *evaluating* an `IO a` (with, ie, `seq`) won't cause it to do anything, you'll just have an evaluated `IO a` value instead of a thunk. You need something *additional* to make it work, whether it's executing a program starting from `main` or explicitly using `unsafePerformIO`.
The equality you're stating in the first one is that the ThreadIDs you get will be the same. That's... how is that useful? The order you fork things can in fact change the outcome of other stuff. In fact in Haskell compiled without -threaded, it can pretty reliably change things.
Or we can allow separate string literals: t"hello" :: Text s"hello" :: String It could be defined `t = id @Text` but that forces you to add parentheses.
no no no nonono no nono no monomorphism! can restrict the types like i do... (to the tune of https://www.youtube.com/watch?v=mN1z0EO7KK4 )
Yeab, and tbat might exactly be the problem: some people don't care as much about mainstream and value correct and ideal sokutions over works-for-most and pragmatic solutions (in other words: mainstream)
It seem to remember `MultiParamTypeClasses` / `FunctionalDependencies` / `FlexibleInstances` / `FlexibleContexts` being mentioned in a thread about why coming up with a Haskell 2016 Report was non-trivial. Not just because of issues like you've mentioned, but also because you need to take into account what other Haskell compilers have done in this space and work out what is / what should be the common behaviours, how far the report should specify things etc... Tightening that up in GHC (ie having more things produce warnings about orphans) in accordance with a Haskell 2016 would also be a nice demonstration that GHC and the Report are different things, and that changes can flow in both directions.
[removed]
Well, looking at consequences, not using sweatshop labor is worse than using it, since then the people doing it have to do even worse labor. Same thing applies at a broader scale: outlawing it is worse than keeping it legal. That said, the better third option is to provide them with better third options (say education, or better jobs). Then outlawing sweet-shops is moot, since no one would work there (and if they are, you haven't provided enough better options yet). Video: https://www.youtube.com/watch?v=S7Qf0ey-pOo&amp;feature=youtu.be&amp;t=4m14s
I hoped that it was clear based on context that I was talking about Cabal-the-executable, not Cabal-the-library. 
I need to access uriScheme part of a URI or URI.
I got interested in this question while reading the Servant paper, which lead me to the "Data Types a la Carte" paper, which talks about the typeclasses approach. Ultimately, I came to the conclusion that it was probably a more useful technique for library authors, such as the authors of Servant who have turned each API endpoint into a *type* and who must make it so that anyone using the library can add to the functionality available without extending Servant directly. I'm an amateur at all this, though, so I am mostly glad to see someone else asking the same question. 
&gt; I think you could make string literals monomorphic with something like fromString = Data.Text.pack, if you don't want String but don't actually need them to be polymorphic. Yes, you can do this. I have done it (in CodeWorld), and it works great!
Haskell Platform installs GHC so that I can use ghci from the command line. Stack does not.
I disagree, but I also do not want to claim that basically every programming language has gotten this wrong. I also do not have data to back this up in any way; which is why I started this with a question in the first place though. But I'd like to elaborate on my reasoning here a little: From personal experience, I have always wondered where all those built in functions came from, not just with Haskell. Haskell *does* provide `NoImplicitPrelude` though. The recent debate about different preludes, made me wonder if having `NoImplicitPrelude` in `Glasgow2016` would be valuable. My motivation behind questioning if this was easier for absolute beginners is that with an explicit prelude import, having ftp and not ftp preludes would be much more obvious. If preludes become more pervasive, haskell beginners would have to know about the implicit prelude and what `NoImplicitPrelude` does anyway. I certainly remember the first time I had a codebase with `NoImplicitPrelude` in the `.cabal` file's extension section and `import Import` statements almost all over the code and wondering why my accustomed to function were not available anymore or did behave different.
what's your final verdict? which is faster?
Since when did type classes become "temperamental machinery"? The canonical way to solve it in Haskell is outlined in [this comment](http://lambda-the-ultimate.org/node/4394#comment-68002) on LtU. Ultra-short version: Lift each constructor to its own data type, use type-classes to do pattern matching. FWIW, I find that the expression problem is *usually* not worth bothering with, but then I don't really do much (E)DSL work, so YMMV. I don't think it's *particularly* heavyweight, but it *is* a quite unusual style for Haskell, so it does have a cost (at least) for readers of your code.
Use `(&amp;)` in `Data.Function`: Î»&gt; :t (&amp;) (&amp;) :: a -&gt; (a -&gt; b) -&gt; b Î»&gt; "hello world" &amp; length 11 
You may be thinking of '&amp;' from Control.Lens. https://hackage.haskell.org/package/lens-4.13.2.1/docs/Control-Lens-Operators.html#v:-38-
[Yes, it does.](https://github.com/commercialhaskell/stack/blob/master/src/Stack/Solver.hs#L111) It also depends heavily on Cabal the library.
I'm not sure I'd argue that it's slow for what it accomplishes, but I do expect that the thing that would most improve my Haskell productivity is a faster GHC.
stack ghci doesn't work for you?
Just be careful. On Linux the way Stack handles the GHC installation is quite fragile as it seems (at least this was so the last time I checked) to use only a single binary build for all Linux distributions which, as a GHC developer, I advise against relying on blindly for mission-critical uses. There's a reason I currently build GHC specific to each of the 5 currently "alive" Ubuntu releases in my PPA rather than using a single build for all. Then there's the related issue that Stack doesn't properly check that the environment actually provides the necessary DSOs and header files, failure to do this easily results in a broken installation. In order to do this properly, Stack would have to inspect the environment it installs into, and select one of multiple configured and built binary-dists specifically to the environment detected. Also, it would have to either bundle DSOs and headers, or be able to call the system package manager via `sudo` to install missing dependencies. OTOH, in the latter case, it'd be more principled to install GHC via the system-package manager, which would automatically avoid the system-dependency issues in the first place. A different way to do this properly would be to install only into Docker image environments managed by Stack, as then you can control the environment in a more principled way, and this would be a much more robust design for Stack than the current fragile method. NB: I'm not defending the HP but rather pointing out a commonly overlooked issue when GHC is installed from a "generic" binary distribution which affects the HP in theory as well, except on Linux it's more customary to install GHC via the system package manager.
Obviously it's a bug! I think they've narrowed it down to the type inference being buggy actually.
I think that's a terrible way to do it! GHC obviously deals with this just fine, what about swift makes it impossible to infer the type?
that would seem a very sensible idea for HP
For the benefit of those who know how to use haskell effectively, please do not retire `cabal-install`. We're just typically quiet, waiting for the storm to pass.
Swift treats all things as namespaced by their containing class / struct / enum. It's not an inference thing, it's more of a namespace thing; like when you do a `import qualified ... as X` in Haskell, and you have to do `X.Constructor` instead of just `Constructor`. Again it's about namespacing, and Swift chooses to slightly increase clarity (and verbosity) by having constructors be differently namespaced than Haskell. This is largely to keep consistent with Swift's OOP nature. It wouldn't make much sense if constructors weren't namespaced the same way that everything else contained within a type is. This convention is pretty unique to OOP, and Swift is first and foremost an OOP language.
You know, if we could use generics to define brand new datatypes structurally, we could calculate the union of data types, or automatically calculate more efficient representations, and fully solve the expression problem in a very tidy way.
&gt; So if I understand correctly, the everything notation would be divided into two main sub-notations: do-notation for unary constructors and their Functor/Applicative/Monad hierarchy, and proc-notation for binary constructors and their Profunctor/ProductProfunctor/.../Arrow type classes. &gt; &gt; In both cases, there would be a default desugarization which targets the strongest type class (Monad/Arrow), and then several law-based simplifications which gradually rewrite the desugared code into equivalent operations from weaker type classes. That sounds about right. &gt; I wish I had a principled way to enumerate all the relevant type classes. Which ones do you think should be included? I suppose it's important to include the ones that actually occur in practice but beyond that I don't know. `PFunctor` and `QFunctor` look too obscure to me.
OK, the solver could be pulled out and put in stack.
~~With [`RebindableSyntax`](https://ocharles.org.uk/blog/guest-posts/2014-12-06-rebindable-syntax.html),~~ you can reassign the `($)` operator to something else. EDIT: Don't need the extension, apparently.
stack is built in the cabal spec, not the cabal implementation of the cabal spec. This is sideways
&gt; Just be careful. On Linux the way Stack handles the GHC installation is quite fragile as it seems (at least this was so the last time I checked) to use only a single binary build for all Linux distributions which, as a GHC developer, I advise against relying on blindly for mission-critical uses. Stack already provides different GHCs and Stack executables depending on the flavor of Linux, e.g.: This isn't true, e.g. https://github.com/fpco/stackage-content/blob/master/stack/stack-setup-2.yaml#L86.
Can you elaborate?
To be clear, the amount of effort that GHC is willing to expend in pattern checking is configurable with the `-fmax-pmcheck-iterations`. It's currently set such that the majority of user code will compile without triggering the limit while ensuring that GHC's memory usage remains sensible. For specific examples of code that triggers the limit I can point to a few modules in GHC itself: * [`compiler/cmm/CmmOpt.hs`](https://github.com/ghc/ghc/blob/378091c9ef6d25a076125f0b82dce1a155c0e8f0/compiler/cmm/CmmOpt.hs): These pattern matches aren't terribly large, but I believe the presence of the guards makes checking rather costly here. * [`compiler/nativeGen/X86/CodeGen.hs`](https://github.com/ghc/ghc/blob/5d98b8bf249fab9bb0be6c5d4e8ddd4578994abb/compiler/nativeGen/X86/CodeGen.hs): I can't quire recall why this one requires so much work * [`compiler/types/OptCoercion.hs`](https://github.com/ghc/ghc/blob/378091c9ef6d25a076125f0b82dce1a155c0e8f0/compiler/types/OptCoercion.hs): Here the trouble is that the matches are extremely guard-heavy, which requires significantly work more by the checker. If you run into the limit and aren't developing on resource-constrained hardware then I'd recommend you just bump up the default iteration count.
I stand corrected, I see there's two flavours of GHC for linux per wordsize, one for GMP v4 and one for more recent versions. However, this doesn't account yet for the subtle variations in ABI, syscalls and library calls we find in non-EOL'ed Linux distributions. We do use Autoconf to test for such properties, and there's always the risk that the properties detected at configure time don't match the ones on the platform you deploy to. That's a long-standing problem in the Linux world, and the reason that cross-distribution binary distributions are such a pain. Just look at how Valve is struggling with their steam runtime on Linux.
I haven't had issues with GHC taking too long unless I use compression flags. I also tend to use lightweight packages and avoid overkills.
It has not gone to waste : it improved a lot the haskell experience. but in terms of overall project organization and value delivered, stack is *also* improving, and quite mindblowingly amazing at that, all across the spectrum
They are very simple, especially when compared to various encodings of structural typing like "data types ala carte". There is much still to explore and implement in the SystemF space, I'd like extensible records/variants, impredicativity and first-class modules.
Sorry, I did not mean to imply that at the current state cabal-install is better than stack in any way. I was merely trying to express a difference in philosophy that I've observed. Vetted package sets is an idea that works most of the time, but it requires resources to maintain and coordinate, so it is in my opinion a pragmatic approach. Of course, as I've been told multiple times, stack doesn't require you to use the package sets, but many of the benefits come from that and it's the default mode of operation. Again, currently, it doesn't seem that cabal-install is better than stack. I think nobody is claiming that. At least in the comments that I've read, it's only stated that cabal-install is also improving and developing a different solution to the same problem. So *currently*, cabal is not better than stack, but that is the thing with flexible and works-in-100%-of-the-cases solutions: the take longer to develop, and yet still won't be perfect. So again, it's a difference in mindset: pragmatic (works now, is better atm, already implemented) vs idealistic (needs research, not yet implement, so worse situation right now) solutions. And again, I don't want to say that pragmatic solutions as necessarily worse, although you may recognize that I myself prefer idealistic solutions (which might also be because I don't use haskell professionally :) 
wont happen as the cabal devs are against splitting out the solver 
[I hope you get banned together with fpnoob](https://github.com/haskell/cabal/pull/3381)
Yeah, good point, I agree with that. It's just that classifying the lesser of too evils as "good" seemed wrong to me, because that implies that there is no problem to be solved. It's like letting somebody get robbed because that changes their path and prevents them from getting raped. And, as you said, there might be possible interventions, besides outlawing it. Another thing I like are services appearing which let people anonymously tell about a company they work for, reveal their true salaries and describe the working environment.
Or (c) don't get to use a language with polymorphic variants :(
Aside: When looking for a function it can be useful to search for its signature on `hoogle` and `stackage` e.g. [stackage](https://www.stackage.org/lts-5.14/hoogle?q=a+-%3E+%28a-%3Eb%29+-%3E+b)
Nice!
Only for `base &gt;= 4.8`. For older versions, you should define your own anyway, unless you already have `lens` dependency.
For what it's worth, I've been using Cabal within Docker for a long time now and never used Stack. I was happy seeing the thread about new features coming into Cabal and I'm grateful to those putting effort into it. I'm not against Stack or Stackage either as I found some ideas interesting and think that having some competing products is healthy. I'm pretty sure a lot of people are quite about the subject and are just happy there is progress everywhere. Edit: it seems there is more than just a Stack vs. Cabal debate: https://github.com/haskell-lang/haskell-lang/blob/master/static/markdown/announcements.md The debate is good. But why a new subreddit... Edit: about the subreddit: Michael gives a reason here: https://www.reddit.com/r/haskell/comments/4ghznv/improved_hpcaballess_wwwhaskellorg_in_the_works/d2jhwm6
Yeah, you don't need `RebindableSyntax` for that.
It could be done unilaterally.
Stack integration isn't there yet but I'm using Atom Editor with all the haskell plugins and cabal build and it's fantastic.
I consider Leksah a very good Haskell IDE.
Thanks, any references on which plugins you use?
It's close enough for me. https://github.com/atom-haskell/haskell-ghc-mod/wiki/Using-with-stack
From my perspective as a PhD student, the good arguments for cabal-install appears to me as a research project into build systems, tackling the most challenging problems (though in fairness it's much more usable than most research projects). Most research projects aren't adopted per se, and that's fine. *If* they are adopted, they first need to become practical and worthwhile (which might happen soon, according to cabal-install developers), not before. Also, you only push research tools on users if you also care for polishing them enough. The pitfalls described by the [Cabal of Cabal](https://www.vex.net/~trebla/haskell/cabal-cabal.xhtml) suggest that's not (yet?) the case. Outside of Haskell research is seldom adopted; in Haskell, more frequently so. So instead of trying to keep cabal-install market share in ways that have been questioned *until* cabal-install gets better (if ever), let stack have its way and come back if you ever get something usable. Otherwise, you'll accumulate enough bad will to ensure`cabal-install` falls into irrelevance even if it fixes its usability issues.
That does not mean `cabal-install` or the HP should be available as an download option on the haskell.org front page, or exposed to end users. Which is the actual point of the discussion.
But Haskell.org *has* been stonewalling. So why should snoyberg keep ignoring that?
I wonder how much grief this causes the Hackage maintainers.
"FPComplete replacing GHC" is such complete unmotivated nonsense that you don't deserve an answer.
That was exactly my thought, although maybe there's a good reason to discover in the official announcement.
Austin, nobody wants to make any decisions on Reddit. Just gather the community feedback about what are the most desirable features of GHC Haskell. Nobody said -XHaskell2016 must be done now, before GHC8 is even released or even this year or next.... &gt; Many of the remaining things are benign or otherwise fine IMO &gt; (e.g. the syntax sugar, EmptyCase, lambda case, binary literals, &gt; comprehensions). By themselves, those could be swept into a &gt; new standard pretty easily, IMO. now, if that's doable, perhaps it should be done. The only problem is that these extensions don't have such huge impact as Type Families and MultiParamTypeClasses, for example. Are you saying that despite their pervasive use TypeFamilies and MultiParamTypeClasses are not "ready" yet? Which brings another question. Since "many of these extensions are still not fully understood", are they ever going to be understood? Do you mean, that people are putting themselves at risk by using these extensions in some unforeseen ways? If that's the case, GHC manual should carefully outline the risks of using each extension. There's some of that but not enough, it seems. BTW. Let me voice my deepest appreciation and awe about the work you and and all GHC contributors are doing. Everybody is most excited about the goodies coming in GHC8. Haskell2010 was a great stepping stone but time flies... It was 12 years between 98 and 2010. Maybe we just need to be patient for 2022....
Corrected. Thanks.
everything from https://atom.io/users/atom-haskell/packages other notable extensions are: haskell-hoogle, minimap, and fonts. I am using Atom with stack only and it's great. It always gives preference to cabal, so if you build the project with only cabal or only stack, that's what will be used; however, if you build with both, then cabal will be used (AFAIR).
I use these Atom plugins for Haskell development: * autocomplete-haskell * haskell-ghc-mod * ide-haskell * language-haskell * ide-haskell-cabal The ghc-mod delay when inspecting types is confusing, but this is the first Haskell IDE, which worked for me. Btw, stack also works with ghc-mod and atom.
I've got the same plus: haskell-hoogle ide-haskell-hasktags ide-haskell-repl git-plus
I've never had to deal with the much-dreaded Committee (hi guys!), but whatever effort goes into lowering the entry barriers (site, documentation, tools..) and simplifying the participation process is praiseworthy, IMO. The implications of having two websites for the same language are still not clear to me, though..
Could you comment on this, for those who haven't been following every mailing list exchange? Are we witnessing a schism within the language? 
This is very helpful, thanks! I'll update the post over lunch with your comments.
[haskell-mode](http://haskell.github.io/haskell-mode/manual/latest/) is an emacs plugin that supports stack (both for compilation and repl). It is actively developed and one of the more widely used and feature-rich ways to provide IDE capabilities for Haskell. I use it with [spacemacs](https://github.com/syl20bnr/spacemacs), which provides modal keybindings as in vim and a pre-configured [Haskell layer](https://github.com/syl20bnr/spacemacs/tree/develop/layers/%2Blang/haskell) with haskell-mode and [other plugins](https://github.com/syl20bnr/spacemacs/blob/develop/layers/%2Blang/haskell/packages.el).
I'm afraid you'll have to add it to HTTP protocol first, schema is not part of HTTP message.
Do you realise all function in Haskell are only functions of one argument? This is a really important thing to notice. Currying. This is how we're able to leave off "an argument"... because really, this: addup = \x y -&gt; x + y is the same thing as this: addup = \x -&gt; (\y -&gt; x + y) which is the same thing as this: addup x = \y -&gt; (x + y) which is the same thing as this: addup x y = x + y which is the same thing as this: addup x = (+) x -- or addup x = (x+) which is the same thing as addup = (+) which means `addup` is a function that returns another function. All function in Haskell do this... which means almost all functions of "more than one argument" are kind of higher order functions, or combinators in the sense that they produce other functions. This is why in Haskell we really only define HOF as functions that *take* a function as an argument and we're not that fussed about functions that give functions as arguments. Our tutorial on currying might be helpful here http://www.happylearnhaskelltutorial.com/1/function_magic.html (Sorry if you already know all this). So we'd generally say `map` is a higher order function in Haskell, but `(+)` isn't, even though they both return a function when you give them a single argument; (`map (\s -&gt; s ++"!")` versus `(+) 3`, for example) because `map` *takes* a function, whereas `(+)` doesn't. Also, in reading your previous article on Clojure, you seem to be contrasting ordinary values in Haskell with Clojure's STM... Haskell has mutable state, too! The ST (state thread) monad lets you build mutable values within an action, and we also have STM, too similar to atom in Clojure... STRefs and IORefs. This stack overflow question may help you here: http://stackoverflow.com/questions/5545517/difference-between-state-st-ioref-and-mvar 
What about the haskell.org/downloads page at this point seems like it is not providing a fair choice?
I'm aware of the fact that it's subtle (I was just reading about Rust's toils with this, though I can't say I fully internalize them). I would think of it differently though; it's not that one of them is an orphan (after all, orphan is specific notion which doesn't apply) but that they're both "coherence hazards", so to speak. I am, personally, willing to give up a strict link-compatibility guarantee to resolve this kind of incoherence, especially with a warning. I also think that it would be better (in Haskell, at least) to specify the _primary_ type parameter (defaulting to the first, but overridable) when defining the class.
never had this problem - have you set the `Haskell Process Type` at `Auto`? Of course your problem seem to be unrelated to haskell-mode - I would probably wipe the GHC installation and just go with stack
The indentation of `twoLabels` in your `labels` function is wrong, leading to `parse error on input â€˜twoLabelsâ€™`.
"just"
I run into the expression problem in the following guise: I want to define a surface language, a core language, and a series of type-safe desugarings that eventually translates the former down to the latter. I can sacrifice type-safety by using an intermediate ADT big enough to accommodate the output of each desugaring, or I can define a new, almost identical, ADT for each step (this gets tedious for 3 steps with a non-trivial surface language, and laughable for 20 or more). This makes a nanopass-style translation unworkable. That said, IMO the correct solution to the expression problem is something like polymorphic variants and extensible records, which need to be built in to the language to be useful.
Just to be clear about your position - you'd be fine with `Glasgow2016`, but not `Haskell2016`?
It is "fair", and that's a problem. There's a difference of philosophy here as to what the purpose of haskell.org should be: * One group sees it as a display for all the things the Haskell community has to offer, basically the website should be a scrapbook/collage of everyone's work and everyone should get a place on the relevant page for all to see and potentially check out. * The other group sees haskell.org as an important entry-point for new users and as such feel it's really important to focus what gets recommended to said users, because its clearly possible to recommend the wrong thing - or not recommend anything at all. Without focus, you're just confusing new users by forcing them to make choices they're not ready or eager to make, and there's a good chance one of their choices will not be what they think it is - and they will get burned. Right now the Download page isn't focused at all, it's trying to please everyone and no one. It's designed for the people who don't actually need to use the page. haskell.org doesn't know who its target audience is. And that's a problem. 
Sure, thank you :)
Would you care to make a short (a minute would be more than enough) video showing how you work with it? (i.e. a "standard workflow") Edit: A rapid show-off of features is fine. In fact, it's what I'm looking for!
With underscore I presume.
&gt; Are you saying that despite their pervasive use TypeFamilies and MultiParamTypeClasses are not "ready" yet? MPTCs, yes, sure. They're not incredibly useful on their own but they're so de-facto standard, they could probably go in despite lacking a companion tool. Type families, no. We've overhauled them several times in just a few years, and in GHC 8.0 even *more* features are coming. If you committed to this a year ago, people wouldn't have injective type families, and I'm sure that would also upset people (because we can never win). In fact it's not even clear out of the people who want injective type families, how well GHC's implementation will do. &gt; Since "many of these extensions are still not fully understood", are they ever going to be understood? I don't know. Maybe they will, maybe they won't. I can't see the future. But it's also not about whether we completely understand the thing - it's also about how risky it is, how big a change it is. Things like type application for example, fundamentally turn Haskell into a bit of a different language IMO, and even if that *is* understood, it's not entirely clear to me whether it should *be* Haskell. Should Haskell 2016 just be strict because we implemented `-XStrict` in GHC 8.0? Just because we understand or appreciate something does not mean it is always a good idea to unilaterally accept it. It is okay to say "No" to certain things. &gt; Do you mean, that people are putting themselves at risk by using these extensions in some unforeseen ways? No more risk then they were always at. But that's not what anyone is talking about. In the past, features like this have often changed because understanding of the feature changed, which necessitated semantics change, and in effect, broke other programs. We've done this several times, it's not uncommon, and it's kind of the whole reason why we accepted this idea of using extensions. If people really like Type Familes as GHC 7.10 implements them, and GHC 8.0 implements -XGlasgow2016 with them in that batch, and then they change *again* in 8.2 and broke your program - what was the point of even having the flag in the first place? It didn't actually save you time, or solve the outstanding design questions in the feature. It just made you type less. (And I'm sure some of the people who complain about that 'break' would likely be the same ones who voted for flag changes like this in the first place.) Instead, it simply solidifies the behavior of the compiler to "however it worked when my program type checked in 2016 with this version of the compiler". That's not a sustainable way to design a compiler and help the evolution of this features, and develop a language we actually understand well.
Surely stack still uses some of those cutting-edge parts of `cabal` since it depends on `Cabal` the library?
Right. It's a small solution to a small problem. As long as there's no pretense whatsoever of specification, standardization, or fixed behavior... and convenience isn't nothing. I mean, after we already broke the world just to make our import lists shorter, why the hell not? *Maybe* it could also serve as a kind of useful coordination point around "the extensions we'd *eventually* like to standardize", if the included extensions are well-curated and not the whole kitchen sink.
&gt; let stack have its way and come back if you ever get something usable sounds good at first, but I bet when cabal demands to replace Stack again in 1 or 2 years, we'll have another big ugly debate. The only difference being that the positions will be reversed. Also, if Stack gets its ways, then the infrastructure will have been optimised towards Stack which equals a regression relative to the needs of cabal. There would be lots of work needed to repair the damage done by the diametrically different Stackage-mindset discouraging PVP-version-bounds on Hackage. This will make it even harder for cabal to ever make a comeback. I really don't get why we all ought to ditch cabal and jump ship for Stack which has been around only for less than a year, whereas `cabal` has a much longer track-record and consequently a larger install base that would break slowly.
I highly recommend `flycheck-haskell` w/ the `haskell-stack-ghc` checker to get errors &amp; warnings highlighting in the buffer. I also use [stack-tag](https://github.com/creichert/stack-tag) to generate etags for entire snapshots. This allows me to jump-to-definition for any dependency.
[removed]
&gt; From what I recall of the download discussion the primary reason that the MinGHC install wound up listed on top was that it addressed the legitimate concern raised by Marlow about wanting to be able to actually run ghci after an install, which you have to admit is a pretty solid argument. To be honest, it sounds like a solid argument from someone who hasn't used Stack. All you have to do is run `stack ghci`. I could list a number of advantages of `stack ghci` vs just having `ghci` be the go-to command, but the technical merits aren't really too important. Two things are: * The text of the downloads page is atrocious. I won't try to explain why, since [this comment does a great job of it](https://www.reddit.com/r/haskell/comments/4ghznv/improved_hpcaballess_wwwhaskellorg_in_the_works/d2jh675). * There was [a public vote](https://docs.google.com/forms/d/1w2wKSxn5YN4LtSXYHvFT2IFw_BDaT_2cjUkP9pDeqLQ/viewanalytics?usp=form_confirm) on what should go on the haskell.org downloads page. Stack clearly one. The committee's response was to create a wall of text, put minimal installers at the top, and call it quits. It was bad all around, and leaves us in the situation I described above: haskell.org is not a good destination to send people to, so we need a new website. Next week will be better for me too, less holidays occurring :). Look forward to speaking with you.
If you want to rebuild the full URI from a HTTP request, you have to assemble it for yourself from the bits you have. * `http/https` from `isSecure` * `://` is a glue string * `host:port` from `Host` header (via `requestHeaderHost`) * add request path from `rawPathInfo` * and, finally, append `rawQueryString` Browsers don't send the URI itself, that's why you have to hunt for its parts all over the places.
A reasonable objection. I've edited my post.
For those looking to browse documentation for packages, you can still use Stackage: https://www.stackage.org/nightly Note that this outage does not affect Stack users, as it uses a high availability S3 mirror by default.
As near as I can tell it is actually already being done, at least in terms of getting the [namespace separated](https://github.com/haskell/cabal/pull/3381) right now.
Thanks for the offer! It's okay if it's outside this program, most students in my class doesn't get paid for their project anyway. My school is pretty open on arrangements for projects, as long as they can see that I'm working. I'm looking for something more practical though, I don't have a strong enough math background to do proofs for a project and I'm not really into research. 
I have a bunch of projects of a less mathematical bent. e.g. there has been some interest in rebuilding the core of `trifecta` so that it can stream content, support "racing" of two parsers, annotate with auto-complete hooks so that you can easily build context-sensitive tab-completion for a REPL, support indentation-sensitive layout, have documentation, etc. None of that is particularly mathematical.
&gt; Even though I believe the Haskell type system makes abstractions more leaky, I wonder what is meant by this?
This is a bad idea, by the way, unless you want to try to singlehandedly undo hundreds of years of math notation. "y = f . g" is shorthand for "y(x) = f (g(x))", and I doubt that convention is going anywhere. 
I have some experience binding to external SAT solvers from Haskell, but I haven't really looked at what all would be involved in wiring such machinery up to cabal, let alone getting it packaged. Given the sort of obscure bugs we've had with the existing solver, having a rudimentary proof of completeness would be a grand, but then extracting information for error reporting from an UNSAT core could be quite difficult.
Man, my Haskell game is rusty. Corrected. Thanks.
Right. It WILL make users happier in some respects, no doubt. But you're also correct, that: I think it's a small problem in the grand scheme, ignoring the larger points. I hate acting like a smug dick and coming off like Blizzard employees (zing!) who "know what's best for you" - and please just *say that* if I'm too dickish, I can take a punch - but at the same time, if we don't actually examine and fix the root problems, this is really all just running around for no reason, isn't it? I suppose that's my core problem here. This doesn't really solve any root problems. And software is constantly made worse by not solving root problems, but instead symptoms. As I mentioned earlier, it - at best - just allows us to get rid of a few pragmas here and there, down the line. &gt; Maybe it could also serve as a kind of useful coordination point around "the extensions we'd eventually like to standardize", if the included extensions are well-curated and not the whole kitchen sink. Well, I also think a lot of people *would* be willing to compromise on some of the dicier extensions, but in practice all of them still feel like big areas of research... Type families just can't be accepted if they're changing almost every year - it's a totally different feature than it was just like 2-3 years ago. If this thread had happened 3 years ago and people got their way and `-XGlasgow2013` implied `-XTypeFamilies`, people would have complained that we 'broke the language' without any kind of guard or backwards compatibility when we considered things like closed families or injectivity, which can greatly impact the semantics. As another example, the only 'controversial' one that *has been left alone* over the past few years, really, is FunDeps. Now, FunDeps are a bit 'out of style', but that's not a reason to keep them out, and we have a fairly good appreciation and understanding of their capabilities, IMO. It's true, they're not perfect, nor the topic of multiple papers per year. Relational programming sucks, vs just using functions. But it's a lot easier to accept those imperfection when they're not changing every other day as an active research vehicle. Research vehicles are great, but it's a counter example where perfect isn't necessary - just stable and tested, with the risks being understandable.
&gt; I suppose that's my core problem here. This doesn't really solve any root problems. And software is constantly made worse by not solving root problems, but instead symptoms. If the root problem here is "there is no progress on an updated official Haskell standard" (is it?), then that one feels completely hopeless to me. At which point it makes more sense to consider short-term (or even "short-term") palliatives. &gt; Type families just can't be accepted if they're changing almost every year - it's a totally different feature than it was just like 2-3 years ago. If this thread had happened 3 years ago and people got their way and -XGlasgow2013 implied -XTypeFamilies, people would have complained that we 'broke the language' I think the whole idea could *only* make sense if it both (a) included all of the major extensions which are now fundamental to Glasgow Haskell as it is actually used in the real world, which very much includes `TypeFamilies`, and (b) was *completely and utterly explicit* that it is *only* a shallow alias for a bundle of other `LANGUAGE` pragmas, with absolutely no more implicit guarantee of stability or anything else than writing those pragmas directly. Narrowly solve the problem of having to write 10 `LANGUAGE` pragmas at the top of every file, nothing else.
&gt; I'm not surprised this act is being interpreted negatively in this thread Don't you think there's some legitimate concern that the haskell-lang team is trying to revoke permission to use the open source code that the Haskell.org team has been using for quite some time?
Back up now? ~20:00 UTC Seems intermittent on the website.
I implemented something similar lately and found that the resulting AST would greatly benefit from [CSE](https://de.wikipedia.org/wiki/Common_subexpression_elimination). It's apparent in this post too: `(x2 + 5.1) * cos(x1)` is present at least twice.
&gt; So the solution you'd prefer is some kind of link-time overlap check? Yes. I suspect it's possible and not even all that difficult in theory, but requires adding more information to the components being linked.
[removed]
[Beautiful differentiation](http://conal.net/papers/beautiful-differentiation/) by C.Elliott is a great read on _automatic_ differentiation (as opposed to symbolic). It develops the subject in stages and eventually unifies the scalar-to-scalar, vector-to-scalar and vector-to-vector cases in terms of linear mappings (as they should), however I still find the notation somewhat dense. I believe the symbolic treatment of differential geometry to be an extremely useful addition.
anyhope for a armv7 binary? I am trying to compile using one of the previous 8-rcX but the compilation keeps failing on unknown OPTIONS_GHC and unrecognised flags (-this-unit-id vs -this-package-key, -fmax-pmcheck-iterations, -Wnoncanonical-monadfail-instances).
23:56 CET, still down :/
I can't get spacemacs to work with GUI to work properly. I have to launch emacs from a terminal via `stack exec emacs -- -nw`. Looks like a path problem but I don't know how to fix. Also I'm not sure if it's an haskell-mode, ghc-mode or fly-chexk problem.
*cough* https://www.fpcomplete.com/blog/2015/03/composable-community-infrastructure *cough*
&gt; And given the bad track record for HP delivering on time Historically, the HP very intentionally staggered releases after GHC releases to let things "settle down". The feedback says, "Please push it out the door sooner" and these days the HP comes out much closer to the GHC release. For instance, the HP for GHC 8 will come out at the same time or within days. Not weeks or months.
[removed]
Sadly, many, many good instances are ruled out by the fact that this requires you to be able to go both to and from a list using one instance. So you can't safely use this as a way to get nice syntactic sugar in EDSLs, etc. =(
[removed]
[removed]
yes, but what if it's ftp or anything else? also, isSecure isn't 100% accurate as it explains in the documentation.
[removed]
[removed]
[removed]
[removed]
Where can I read up on these ? I can't find anything related to it except for OCaml specific stuff: https://realworldocaml.org/v1/en/html/variants.html
It fits my workflow poorly. ghci starts up almost immediately and gives me all the packages I have installed locally. stack ghci gives me a bunch of passive-aggressive suggestions that I really should consider creating a project first, and then loads ghci with only base available. I don't want to create a directory structure and two configuration files every time I'm trying out an idea.
&gt; BTW. Let me voice my deepest appreciation and awe about the work you and and all GHC contributors are doing. Also, thank you. I was a bit bitter earlier because I feel pretty much like I have to constantly explain things like this, and it takes substantial amounts of effort to explain what I think is the less cut-and-dry reality of the situation. As a result I sort of intentionally overlooked this part of your comment, because it's easy for things like that to ring hollow when it feels like I'm just going to repeat this sometime soon anyway. But now I'm a bit more cooled off. But I do appreciate the thanks, really. And I've clearly been more of a dick in this thread than I should have been. Perhaps I deserve my sickness today, as I said elsewhere.
&gt; To be honest, it sounds like a solid argument from someone who hasn't used Stack. All you have to do is run stack ghci. I've used stack. stack ghci is not a replacement for ghci.
What? I don't call this passive agressive: &gt; Run from outside a project, using implicit global project config stack ghci also gives you whatever packages you have listed in your `.stack/global-project/stack.yaml` file, including local libraries. The good thing about this is that making a package available here has no impact on your actual projects, so you can't end up screwing up your package database. If you've never encountered cabal hell then I suggest you keep going with your current approach -- you will encounter it eventually.
This is what I do for the most part, sans the tags. My dev env is open and has been for some time: https://github.com/bitemyapp/dotfiles
Why would wai, an HTTP interface, ever receive an FTP request? The question you are asking doesnâ€™t make any sense... An HTTP server can only service requests that use the appropriate protocol.
That looks like a nice project, I would be happy to do something like that. As my next semester is next fall, I propose that I contact you around august to see what are my options
downloaded the binary. crashes every time I open a file. 
Wrapping emacs in stack is serious overreach. What about vanilla spacemacs? Instructions for stack are in a readme. 
[removed]
Well my mind is blown.
 stack ghci :l foo.hs
The Haskell version is significantly faster. I will definitely have to continue working with this language!
9 hours of downtime :| yikes
[removed]
Now that it's back, it might be a good time to mention that cabal-1.24 (out RSN, along with ghc-8) has support for automatic use of mirrors (with security so you don't have to trust the mirror operators). Unfortunate to have downtime just before that rolls out, but it's a good reminder of the importance not just of having mirrors but also them being used automagically.
Why did this thread's sibling get removed? 
So you agree it's not exactly the same.
You can modify the package index location in the stack.yaml file. I haven't done it in a while and don't remember the syntax, but it's documented with the rest of the config options
Huge thanks to duncan and austin for getting it back working again too! :-)
Just so you know, there are lots of options here: stack exec -- ghci foo bar stack exec bash # now you can run ghci If you really want to know how people do this in practice, I'd recommend asking on the Stack mailing list, as I'm notoriously bad at using ghci personally.
That shouldn't be a problem. AFAIK we've also had Russian students apply to prior GSoCs, although Ed is the real authority on this. But based on the requirements at the bottom of the page - you should be fine, I think.
&gt; major activities this year have consisted of getting GPG keys signed and organizing a replacement summer of code This is one of my concerns with the committee: I believe they need to reevaluate their focus. A better new user experience and improved tooling is in my eyes more important to the Haskell community than the summer of code.
yes, you don't want to have newcomers going to broken tools whose users don't even recognize the issues (stateful build, multi package, the list goes on) the lack of perspective of other users is really not a sound ground for taking a moral stance
I assume it has never been built?
Yeah it's better. I can't believe such a minor thing is used to argue over the technical merit of stack.. 
Exactly, that's the cultural divide. The problem lies when one tries to do the other when you should be cooperating.
My understanding is that if stack is installed into the recommended location (as described here: http://docs.haskellstack.org/en/stable/install_and_upgrade/#path) then it should be able to upgrade itself and hence that should suffice? Is there anything else that should be done to alleviate this concern?
I notice you use List types in the AST. Would using Vectors instead help at all with speed and GC?
Actually you did not wrote the python as I expected. I was thinking about something like : import functools @functools.lru_cache def fib(n): if n == 0 or n == 1: return n return fib(n - 1) + fib(n - 2) Here the `lru_cache` behaves exactly as the `memoize` function I described. However there is a minor difference between the python and the haskell implementation: in haskell the recursive call to `fib` inside the `fib` function cannot be replaced after by the recursive call to the memoized function, hence the trick with `memoFix`. On the other hand, in python, it is not replaced, python will always do a lookup in the global namespace to find the `fib` function, which is now the memoized one. That's one of the flexibility of python but it comes with a high runtime cost... As a side note, memoization like that is a nice trick, but at the first look we can have the feeling that it breaks the pure behavior of the function (because the memoize function must stores and edit a cache of the computed values). However, as long as there is no observable effect outside of the function, this does not break the purity of the function.
There are `PATH` related problems on OSX and NixOS, which can be worked around by excluding the `exec-path-from-shell` package in the `.spacemacs` file: dotspacemacs-excluded-packages '(exec-path-from-shell smartparens) I've also used this occasion to disable `smartparens` (auto insertion of closing delimiters), which I found *very* annoying. There seems to be a [caveat](https://github.com/syl20bnr/spacemacs/issues/2294) for some non-Haskell layers: &gt; Unfortunately, just excluding exec-path-from-shell isn't a good solution because other layers that could be used on Nix OS rely on it -- the rust layer, for example. That's why I reverted the work around I mentioned previously. Also, I have to start `emacs` from the haskell project's `nix-shell`.
Yes, I did comment, and I understand your concern. So the issue is that clearly when a user installs stack by any means, eventually it will go out of date. It is just that when a user installs the latest stack, you would like it to not be out of date immediately :-) By the way, does stack warn users when it is out of date and encourage them to upgrade? I'd ideally like to make the platform release process sufficiently smooth that new releases are "no big deal" although that will take further work. If we get to that state, then ideally say a "twice a year minimum" schedule might be possible, if that suffices? Adding the notice to encourage a `stack upgrade` as part of the installer process itself is also possible, and further "post-install notes" in general aren't a bad idea, actually... As a final thought -- and this is perhaps best, we could also change the post-install scripts to offer to run `stack upgrade` automatically, if that would make sense? I do think it is good if the platform ships stack, and I would like it to do so in a way that the stack team feels is appropriate. So I think I talked myself into that last option unless you have a better suggestion otherwise? (Generally, I've wanted to get together infrastructure for common automated builds of binaries across platforms of lots of shared stuff. This would proceed by first improving and cohering our builder story for ghc nightlies themselves. From there it would go on to include cabal, ghcjs, haddock, etc. Ideally spare cycles could also go to projects such as leksah and others that wanted to opt-in. Better cross-platform builder coverage and available binaries would serve a lot of projects. And the possibility of extending it to more exotic architectures would help the ARM story and the like. I hadn't actually thought of the platform as part of that list, but once the infra is in place, why not? I did some information gathering and tried to get an interest group together but other priorities took hold and it was backburnered.)
I'm not going to tell you not to ship Stack in the platform, and the coping mechanisms you mentioned will improve the situation. But as I commented elsewhere, this is just creating a situation that's slightly less sucky compared to the current platform, and not as good as straight Stack. So sure, if you've decided that the platform must ship, and that haskell.org must point to the platform (which you've made abundantly clear), this is making the situation incrementally better. But incrementally better is a far cry from good. Stack's getting a feature to notify users for new releases.
not an IDE.
How large is Hackage, these days?
The Aeson tutorial now has a solution to your example in it after I suggested this to them. :)
Does it load ok when you click on the Leksah.app now? Did the metadata collection work ok (does the Modules pane have stuff in it)? If you right click to find the type it should use ghci (which it will launch with either `cabal repl` or `stack repl` for the package the file is in). It should show the output in the Log pane.
Is there a guide on hosting hackage mirrors? And an estimate in bandwidth costs / disk usage? I'm sure people will be willing to chip in extra mirrors.
While there's some attention on hackage: Can I help somehow to make documentation appear? I'm not sure why sometimes my docs appear and sometimes they don't. I assume that the problem is known and the fix is somewhat involved because otherwise someone would have fixed it :)
how is you entering the equation here ? you do what you want. your tools work, use them. Just make sure to not bug others and leave them room to improve stuff when they have demonstrated their ability to do so
says the guy who gives snoyberg shit all the time. no one asked to "give up pursuing" whatever. go push the limits, that's what haskell is for, sure. and I hope when you are done, hopefully no one will give you shit because they are used to their cranky way of doing things
Oh sure. Even if its actually correct, the performance could still be unacceptable (like in this case).
Oh and as you can see: deciding `isSecure` is trivial. For an HTTP handler return false; for an HTTPS handler, return True.
I think OP's intention is this: assuming `output` prints a string atomically, the following two are indistinguishable: (,) &lt;$&gt; forkIO (output "hello") &lt;*&gt; forkIO (output "world") = (flip (,)) &lt;$&gt; forkIO (output "world") &lt;*&gt; forkIO (output "hello") because it both either 1) outputs "helloworld", or 2) outputs "worldhello". TL;DR equivalence with non-determinism hurts brains
And [Cyberpunk 2020](https://en.m.wikipedia.org/wiki/Cyberpunk_2020)! Coincidence? :D
Yes, I was inferring that you have more going on in `../global-project/..` maybe? Like davemenendez I get $ stack ghci Run from outside a project, using implicit global project config Using resolver: lts-3.14 from implicit global project's config file: /Users/michael/.stack/global-project/stack.yaml Error parsing targets: The specified targets matched no packages. Perhaps you need to run 'stack init'? Warning: build failed, but optimistically launching GHCi anyway Configuring GHCi with the following packages: GHCi, version 7.10.2: http://www.haskell.org/ghc/ :? for help Ok, modules loaded: none. Prelude &gt;&gt;&gt; 
You may want to check out this issue: &lt;https://github.com/haskell/hackage-server/issues/145&gt;.
Join `#hackage` on IRC and we can help investigate. Yes, the doc builder stuff needs overhauling. Volunteers? Probably a good solution these days is to attach it onto hvr's hackage matrix builder.
Perhaps I ran "stack init" in the global project at some point. It didn't seem to break anything and it made the messages go away.
As I mentioned in [yesterday's thread](https://www.reddit.com/r/haskell/comments/4gizad/ann_ghc801rc4_quiet_update/d2hxfh1), you can try this out with the following [stack.yaml](https://gist.github.com/tfausak/8e9f3733450f929059aa704d6d8cf12a): compiler-check: match-exact resolver: ghc-8.0.0.20160421 setup-info: ghc: linux64: 8.0.0.20160421: url: https://downloads.haskell.org/~ghc/8.0.1-rc4/ghc-8.0.0.20160421-x86_64-unknown-linux.tar.xz macosx: 8.0.0.20160421: url: https://downloads.haskell.org/~ghc/8.0.1-rc4/ghc-8.0.0.20160421-x86_64-apple-darwin.tar.xz
cabal: does not exist Best error message ever! (But, I really need to redeploy a build server and would rather not hack the build scripts to use a cabal mirror. Hope it gets fixed soon.)
Also, if you already set it up to run a bunch of JS benchmarks, it would be really nice if you contributed the script to the Idris repo, so we can track how it's doing!
I have to say I've found "the community" to be *extremely* helpful and friendly - or at least, they've tried to be :) 
&gt; Agda: isn't supposed to actually run code This is news to me. An in-joke, maybe, but not true. The MAlonzo and JavaScript backend can both produce runnable code.
I solemnly pinky promise to do my best. I'm super excited about the opportunity collaborate with such a lovely collection of folks. I, for one, hope to learn a lot from the other committee members as we collaborate. 
It sounds like you just want dependent types for the sake of it. What kind of use do you envision ? It maybe that you only need dependent types for a fraction of your work ; if that is so, you may use Coq to define and prove those parts and extract them to OCaml or Haskell.
&gt; the new subreddit is intended as a place to discuss topics specific to haskell-lang.org Respectfully, may I ask then why /u/bitemyapp, a moderator of the new subreddit, has posted something irrelevant to haskell-lang.org?
And as Ulf Norell often mentions: when the code produced by Agda is awfully slow, the responsibility quite often lies (in part) on the algorithm chosen by the programmer. A quadratic solution might be easier to certify than a more subtle linear one but the user shouldn't be surprised if it has a quadratic behaviour.
Any chance you could put the slides up?
If you mean for running a mirror as a simple static file set, then I think it's only ~6.5G. If you mean the primary server with all the docs etc, then it's a lot more.
Yes, I'll get to it and clean up some of the content. Going over the slides, I also forgot to ask the pop quiz to the crowd that I ended up adding in! Should've done that at the end. Womp womp.
have you heard about urweb? 
&gt; The MAlonzo and JavaScript backend can both produce runnable code. The JS backend really doesn't, though. Most of the standard library doesn't even compile with it (and I mean like... strings and bools, not just fancy stuff like coinduction).
This is also something that's relevant to type checker performance. Running programs in the type checker means needing to worry about their efficiency. A legit critique is that current DT languages don't give us good enough tools to reason about how our programs perform in the type checker. There's not really a good cost model, the evaluation strategy is sometimes under-specified, and we don't have profiling tools.
Well, one of the big things I think an IDE should be doing for me is managing the *project* or *solution*, not the files. So what that means for Haskell is, I want to just add files as I need them and have my IDE figure out how to update the cabal file (or whatever stack uses) so everything builds. Does your setup do that or have some way to do it? I ask because eclipsefp could do this years ago already.
[removed]
S3's outage lasted ~3 hours but only affected one region. If you look back over the last few years I suspect that Hackage has had significantly more unscheduled downtime than S3.
I understand that in Texas they have multiple ways.
Ur, which is reportedly _very_ fast, is a language targeted at full-stack web work. It doesn't provide "full" dependent types, but it provides a few important cases of "types with dependency" (my terminology, not theirs) especially geared for the needs of integrated well-typed code all the way through the stack from DB to web. http://www.impredicative.com/ur/ I don't think it has a _lot_ of widespread usage, but I do know a significant portion of https://bazqux.com/ is written in it, and bazqux is a very fine google-reader like rss reader which works very efficiently. (If I recall, the frontend of bazqux is all ur, and the backend process that does feed fetching is straight Haskell).
[Heh](https://github.com/well-typed/binary-serialise-cbor/commit/9528877a4d85642be787c05efee39d2b3e0e078e)
What did Michael do to deserve getting shit from everyone?
Yes, they did... for DependentHaskell, Phase1 :-) https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell/Phase1
you can simulate aspects of it in awkward hacky ways, but I wouldn't call it a super usable "dependently typed language". kind of like using C for functional programming. 
Is Haskell 2020 intended to be a new language distribution? Like a new prelude and stuff? 
You can use `ad` on ASTs, observe sharing and then generate, say, smoking fast GPU code for performance critical applications, just from the resulting AST. I believe this is an approach /u/edwardkmett has used in the past.
I think the problem here is that the author is using the term "abstraction leak" in a sense that's opposite to its common meaning. Usually it means that an abstraction's attempt to *hide* some implementation detail from its clients fails and the clients have to concern themselves with it. But if clients are passing in a database connection throughout the call graph then there isn't an abstraction in the first place!
What does the "ANN" in the title stand for? Whenever I read these, they have less to do with artificial neural networks than I expect.
/facepalm That makes much more sense. Thanks!
With respect to the state variable, yes, because again it creates a data dependency such that the only way to proceed is to evaluate the underlying functions in order. Harkening back to `IO`, `State` looks like this: newtype State s a = State { runState :: s -&gt; (a, s) } Where `s` is the state variable and `&gt;&gt;=` looks just like it does for `IO`. So if we have the following `State` action: incDouble :: State Int () incDouble = do modify (+1) modify (*2) Then `modify (+1)` must be evaluated before `modify (*2)` because the latter requires the state variable produced by the former. If we strip out the `newtype` wrappers and work directly with functions this becomes a lot more clear: -- State s a ~ s -&gt; (a, s) (&gt;&gt;=) :: (s -&gt; (a, s)) -&gt; (a -&gt; (s -&gt; (b, s))) -&gt; (s -&gt; (b, s)) f &gt;&gt;= g = \s -&gt; let (a, s') = f s in g a s' modify :: (s -&gt; s) -&gt; s -&gt; ((), s) modify f s = ((), f s) incDouble :: Int -&gt; ((), Int) incDouble = modify (+1) &gt;&gt;= \_ -&gt; modify (*2) Inlining `&gt;&gt;=` then gives us this simplified definition: incDouble :: Int -&gt; ((), Int) incDouble i = let ((), i') = modify (+1) i in modify (*2) i' Since `modify (*2)` uses `i'`, which in turn is produced by `modify (+1) i`, the only way `modify (*2)` can be evaluated is if `modify (+1)` is evaluated first. As a final note I want to emphasize again that the single order of evaluation is only with respect to the state variable. For regular Haskell expressions inside a state monad the usual laziness rules apply. As a quick example: updateState :: State Int () updateState = do i &lt;- get put (foo (i + 1) (i * 2)) Where `foo` is some function with type `Int -&gt; Int -&gt; Int`. In this case, the question of which of `(i + 1)` and `(i * 2)` gets evaluated first (if either of them get evaluated at all) can't be answered without knowing the definition of `foo` because of laziness.
Definitely cool! `reverseCons` is also too strict in its first argument. If I have `reverseCons (reverseCons [] 1) 2`, I'm not able to pop the outer bubble to get `2 : reverseCons [] 1`... why not?
&gt; TL;DR equivalence with non-determinism hurts brains In fact, there is a whole spectrum of equivalence relations for nondeterministic programs, called the van Glabbeek spectrum after my process algebra professor.
Friendly advice: pointing out behavioral inconsistencies which have the potential of chipping away at the reputation of idolized figures will only get you stoned to death with downvotes...
If only if was a bit cheaper, I would definitely pick it up. 
I personally don't think much of LLVM's JIT. If I do go through LLVM, it isn't to JIT, but rather to full on compile. I'm much more likely to emit OpenCL and have it run on the CPU or GPU, or emit CUDA and have things run on the GPU, though. 
Since this is for your job, I'd like to add to the chorus of people wondering if you really need dependent types for this. Depending on what job, you might even be better off with something more mainstream like [typescript](http://www.typescriptlang.org/). This might give you less language power, but it will give you more mature tools, a larger community, more libraries and tool support, etc. Depending on what you're planning to do, this might be a better tradeoff than a less mature technology with more powerful types.
:-) That (terrible) error message is also fixed in 1.24.
I think the designers of it felt "it wouldn't hurt" but in retrospect it has ensured that I can never really bring myself to use the extension except for list/vector-like constructions.
What level of Haskell experience do most people that apply/are in the program usually have?
[haskeline](https://hackage.haskell.org/package/haskeline)
Yea, unfortunately I don't think any such thing exists for Haskell.
Respectfully, why do you care if there's a new standard or not? I don't mean this to be a facetious question. I don't think there was a new standard in 2014, because Haskell was (and still is, IMO) in flux. The Haskell community has shown relatively little concern over standards (again IMO). While this may be limiting the language's adoption, even that's not clear. Even if a Haskell 2020 standard is adopted, I expect there will still be a lot of experimentation. That's due to the nature of the community and who has the time to contribute.
`IO` can always go off and do something like secretly change global variable, which can then affect later `IO` actions so it is pretty hard to have a well defined semantics for `IO`.
On that note, should it still be a good 2-3 months as usual before we see an LTS snapshot for GHC 8?
Ur/Web is explictly about full-stack web development, and they have a lot of safety features to justify the increased complexity of its type system. Documentation is sparse though. I would consider using Purescript, even without dependent types. It has row polymorphism, for example, which I consider important for web development ergonomics. 
From the Fine Announcement: &gt; I'd like to remind everyone that the Haskell Prime Process[4] relies on *everyone* in the community to help by contributing proposals &gt; which the committee will then evaluate and if suitable help formalise &gt; for inclusion. Everyone interested in participating is also invited to &gt; join the haskell-prime mailing list. So I'd imagine that the best way to see how things are going is to join the list and monitor it. As to why we should be more confident than before, I don't know. I would just hope that this time there's enough fresh blood on the one hand and pent-up demand for some further standardization on the other that momentum helps carry it through. :-)
You may be interested in Utrecht University's Helium compiler.
It's also possible to make GHC 8 work nicely with snapshots intended for GHC 7.10.3: 1) Upgrade to the git version of stack (soon to be released). It has a fix to https://github.com/commercialhaskell/stack/issues/1579 2) Add `allow-newer: true` to your stack.yaml. This turns version constraint violations into warnings. For a more complicated example, here's a stack.yaml which specifies how to build stack itself on GHC 8: https://github.com/commercialhaskell/stack/blob/master/stack-8.0.yaml
Great! Looking forward to my first ZuriHack.
Experience varies. I think the key would be picking a project that you think you can achieve within your experience level, and finding something that would benefit the community well.
I've only messed around with Haskell for fun so maybe some of my amazement is from ignorance, but at the same time, I can still tell that the OP's solution is elegant and powerful.
I'll see if I can make it!
How much experience do you have? The work is out there but it will take more effort on your end compared to finding freelance work in another language. Depending on how much you like Haskell, it may be worth it. You might have the most luck finding clients who don't care what language you use and just need a job done (automation,websites,etc). **warning**: if you take freelance work and write it in Haskell then when/if the client needs maintenance, you might be their only contact who knows Haskell. I have dealt with more than one situation where I needed to help someone w/ build or compile errors several months (even years) down the line. 
It would be great to have you with us again this year!
I vote 1A, 2C, 3D. `:type` should retain its current behaviour, and something like `:examples` should show examples of the type, specialised to non-scary types by applying the monomorphism restriction *and* somehow judiciously replacing type variables with concrete types: &gt; :type mapM mapM :: forall {t :: * -&gt; *} {m :: * -&gt; *} {a} {b}. (Traversable t, Monad m) =&gt; (a -&gt; m b) -&gt; t a -&gt; m (t b) For examples of specific instantiations, enter â€˜:examples mapMâ€™ &gt; :examples mapM mapM :: forall a b. (a -&gt; IO b) -&gt; [a] -&gt; IO [b] mapM :: (Int -&gt; IO String) -&gt; [Int] -&gt; IO [String] mapM :: (Char -&gt; [Int]) -&gt; String -&gt; IO [Int] E.g., specialise `Monad m =&gt; m` to non-scary monads like `IO`, `Maybe`, or `[]`; `Foldable f =&gt; f a` to `[a]`, ditto `Traversable`; and `*`-kinded type variables to non-scary concrete types like `Int`, `String`, `Char`, &amp;c. In general, the algorithm would be: for each type, specialise to the next available non-scary representative example for its kindâ€“constraint pair: * `Monad m =&gt; m` â†’ `IO`, `Maybe`, `[]`, `Either String`, â€¦ * `Foldable f =&gt; f a` â†’ `[a]`, `Maybe a`, â€¦ * `a` â†’ `Int`, `String`, `Char`, `Integer`, â€¦ You just have to define a suitable scariness ordering: anything â€œmore generalâ€ (higher kinded, higher ranked, higher order, more polymorphic) is scarier than something less general. 
I wish I had the power to create reality from my opinion. The struggle of newcomers not using stack is real 
I think I'm pretty good at working with pure functions in Haskell, working with data and doing equational reasoning, probably because I study math as well as computer science. That being said, I've never had much reason to work in the IO Monad or really any "closed" Monads that require you to "cordon off the impurity". I understand the category theory background of it to some extent, though I'm not sure if that would actually help. Do you have any suggestions for me? Things I could build or a piece of code I could look at to see if I'm ready for this sort of thing? I'm used to coding in C/C++, Java, JavaScript, I've done some decently complex IO and file manipulation in languages like that. I mostly like solving problems and writing pure functions to compute interesting or aesthetically attractive things or solve numerical or symbolic math-like problems. I really rambled a bit there.
That would be an *amazing* thing to have that be handled intelligently....
PhDs feel to have the right to do anything those days
Where would you recommend looking? Is the warning really a downside? I imagine you'd be able to charge premiums and don't have to do the work if the contract is worded correctly. 
If you do symbolic differentiation you do need some form of graph representation for sharing/reuse or you can get exponential performance loss, using Var and Let to turn this little algebra into a calculus, like you suggest, is one decent way to get there.
Why wouldn't you expect the same benefits that you'd get in other areas? Taking the head of an empty list is similar to a null-pointer exception in that it is unclear how to continue execution and it currently isn't modelled in the type system. Isn't dependent types just a way to avoid unexpected problems like this?
an implementation of a concepts draft already ships in gcc 6.1, out yesterday
I don't think you understand what a language standard is...
Where you see warning I see "competitive advantage"!!! Man it sucks when people you worked for have to keep hiring you doesn't it? :D
A PhD student is after all a student. You may have to be careful in juggling the demands of both, but nothing in the letter of the law gets in your way.
It would be great if the committee could publish a new draft of Haskell2020 with every GHC release + list the deviations from it. Personally, I'd like to have Haskell2020 extension which implements the current draft standard. Everybody would know that this extension is for testing only and will change arbitrarily together with the standard draft. So the process could be nicely incremental like this: - the committee decides that an existing extension should become standard and adds it to the Haskell2020 extension. - if a change is not yet implemented, the draft specifies it and it gets implemented directly as part of Haskell2020 extension - repeat until the standard and the implementation are consistent. - whatever gets done by 2020 becomes the new standard and the extension is released with the usual compatibility guarantees. This way we could all track the progress and participate. And there won't be any disappointments in 2020 :-)
It is very easy to implement it. I like this operator piping (|&gt;) that is widely used in ML, OCaml and F#. &gt;&gt;&gt; let (|&gt;) x f = f x (|&gt;) :: t1 -&gt; (t1 -&gt; t) -&gt; t &gt;&gt;&gt; &gt;&gt;&gt; 10 |&gt; sqrt |&gt; exp 23.624342922017807 it :: Floating t =&gt; t &gt;&gt;&gt; I don't know if it will conflict with any Haskell operator. As mentioned here there is also the operator (&amp;) from Data.Function. You can also perform reverse function composition using arrows (&gt;&gt;&gt;): &gt;&gt;&gt; import Control.Arrow ((&gt;&gt;&gt;), (&lt;&lt;&lt;)) &gt;&gt;&gt; &gt;&gt;&gt; :t (&gt;&gt;&gt;) (&gt;&gt;&gt;) :: Control.Category.Category cat =&gt; cat a b -&gt; cat b c -&gt; cat a c &gt;&gt;&gt; &gt;&gt;&gt; let f = (+4) &gt;&gt;&gt; (*3) &gt;&gt;&gt; (/5.0) f :: Fractional b =&gt; b -&gt; b &gt;&gt;&gt; &gt;&gt;&gt; f 5 5.4 it :: Fractional b =&gt; b &gt;&gt;&gt; f 10 8.4 It is the same as: &gt;&gt;&gt; let f2 = (/5) . (*3) . (+4) - f2 :: Fractional c =&gt; c -&gt; c &gt;&gt;&gt; &gt;&gt;&gt; f2 5 5.4 it :: Fractional c =&gt; c &gt;&gt;&gt; f2 10 8.4 it :: Fractional c =&gt; c &gt;&gt;&gt; And finally math-like composition like operator (.) dot: &gt;&gt;&gt; let f3 = (/5) &lt;&lt;&lt; (*3) &lt;&lt;&lt; (+4) - f3 :: Fractional a =&gt; a -&gt; a &gt;&gt;&gt; &gt;&gt;&gt; f3 5 5.4 it :: Fractional a =&gt; a &gt;&gt;&gt; f3 10 8.4 it :: Fractional a =&gt; a &gt;&gt;&gt; 
I am happy with happstack web framework. The modern way to implement a web site like this is to create a rest API in the server side that returns JSON data and the client-side (JavaScript) running in the initial web page performs AJAX request to the server and updates the user interface and builds the html widgets out of the JSON data. I would also let the UI job to JavaScript in client side because it would also allow web designers and people who doesn't know Haskell create the UI. &gt; maybe something that deals with AJAX style updates, etc. I guess you can also run Haskell in the browser now? Yes the continuation Monad is useful to spare the developer from the callback hell in asynchronous AJAX requests. &gt; I guess you can also run Haskell in the browser now? Unfortunately the only language allowed in the browser is the JavaScript that is full of bad designs. However there are languages that can compile to JS if don't like it. Some of them are Elm, Clojure Script, Type Script, Coffee Script and so on. 
Hmn looks interesting, how it solves problems like binary references within the file? imagine i have a header and it has a list of references for data structures (dword position_file, dword quantity). In binary and cereal, this types of formats are a pain to read, since they expect that you consume input linearly. 
I'm genuinely curious. Virtually all demonstrations of the applications of dependent types that I've seen are rather far from what would be practical for "developing a web application."
I don't know... The core idea is straightforward: extension and mixing of behavioral specifications should be path independent. [pushout](https://ncatlab.org/nlab/show/pushout); [prettier diagram](https://en.wikipedia.org/wiki/Pushout_%28category_theory%29). There was actually conversation after last week's Boston Haskell, about perhaps writing up something bloggish. In bar discussion after [Boston Haskell](http://www.meetup.com/Boston-Haskell/), /u/edwardkmett has suggested what I want is (modulo my fuzzy recollection) a lattice of conditional-theory pushouts. Where the theories are bags of types, operators, and laws. And thus theory extension, by deriving types, defining operators, and/or declaring laws, is path independent. And suggested looking at Lawvere theory, so perhaps [Lawvere theory](https://ncatlab.org/nlab/show/Lawvere+theory) and nearby pages. I come at it from software engineering. Lots of "that's *painfully* not the Right Thing" moments. "One is not a object; plus is not a message" (an old smalltalk criticism), so there's value in predicate types, and multimethods. Plus is commutative, so properties, laws, and proof obligations. Which depend on domain and context, so algebras and theories. And these pithy descriptions need to be compiled, so... extensible compilers, supercompilation, etc. For motivation, perhaps look at type hierarchies grounded in abstract algebra, like [numeric-prelude](https://hackage.haskell.org/package/numeric-prelude), and computer algebra systems, like [MAGMA](http://magma.maths.usyd.edu.au/magma/handbook/). And whole-program "start inference by gathering all type constraints in bags", like... something I've failed to find. 
&gt; publish a new draft of Haskell2020 with every GHC release + list the deviations from it *DeltaHaskell*, if we're suggesting codenames. I think it's a great idea.
Another thing is that a standard allows me to speak more precisely about what I know. Today, if I say "I know Haskell", which extensions I'm comfortable with change what that means entirely (DataKinds, GADTs, and FunctionalDependencies vs simply OverloadedStrings or MultiWayIf -- not that all of those would be in the new standard). It's someone like the difference between a function returning an error code (and having to enumerate valid error codes) and a function returning an ADT-- both might due the job, but one is more concretely defined and easier to understand.
I know, but until it is available in a standard form across most C++ compilers, not only the clang,bcc,vc++ trio, most developers won't be able to touch them for production code. This will take decades. Outside that compiler trio, there are vendors still catching up with C++11.
It might be longer, actually. First ghc 8 needs to actually be released. Then we'll switch the nightly builds over. Then we'll see. Obviously we encourage maintainers to be ready for ghc 8 asap, but the reality is it will take the Haskell ecosystem a while to adjust. We originally intended to release LTS 6 with ghc 8. Given how late ghc 8 is to arrive, we are considering ghc 7.10 again for LTS 6, and ghc 8 for LTS 7.
Cool! Are there any plans to do a -XGlasgow2016 or -XHaskell2016 thing? I do not see why a set of extensions should necessarily need a standard. Why am I so keen on rushing that? Because we can only really start using it a few years from now, once GHC 8.2 is the oldest version supported. A conservative -XHaskell2016 could form the core of a more embelished -XHaskell2020 that is fully specced. Alternatively I suppose there is the approach of external tooling such as https://github.com/sol/hpack/issues/94
Generally speaking &gt; I don't expect the language to be complex or fully featured and with tons of cutting edge extensions, type-classes, proof assistants, etc. pretty much contradicts &gt; [..] I need something [...] that has [...] dependent types. Usually "dependent types" implies a certain minimal level of inherent complexity. (Idris seems to be the current best bet for taming some of that complexity, but it certainly isn't a particularly "simple" langauge.) Regardless, if you're expecting to use $UNICORN_LANGUAGE across the whole stack from the browser to the backend, then the answer is currently simply: **No**. The best full-stack language I've used so far is probably scala.js + scalajs-react + Scala (JVM), but you *really* need to be careful about what you use for JSON interop[1]. It's not really dependently typed, but it does support non-trivial things like path-dependent types and has a TC type system. (Whether those things are a good or a bad thing depends on one's PoV :)) (GHCJS would probably be a strong contender if it supported full optimization -- Closure-compiler ADVANCED style. There's also some wonkiness around the installation, but it's not too bad.) [1] The Scala ecosystem is a goddamn mess at the moment wrt. JSON. My current preference is "circe". Avoid upickle at all cost. Best to avoid Argonaut -- use Circe instead (it's the spiritual successor to Argonaut and greatly simplifies things).
interesting to see how it performs against c++
https://github.com/FStarLang/FStar I didn't use it in practice, but at least it could be an option.
How does performance compare to https://www.fpcomplete.com/blog/2016/03/efficient-binary-serialization ? Any reason to prefer your approach?
a very subjective opinion
My bad, I didn't explain it well enough. Basically your program should not *rely* on the two printed in a certain sequence, just like it should not rely on the order in which functional arguments are evaluated in, for example C. (Think `foo(putchar('a'), putchar('b'))`). In this case, even for the first example, the scheduler may decide that the b thread should run first, resulting in "ab" printed. What's more, it turned out that `putChar` is not exactly thread safe. I compiled the code using `ghc -threaded -o testconc a.hs`, and only got "a" printed.
One specialty of the Haskell memory management is that is does [mark and compact](https://en.wikipedia.org/wiki/Mark-compact_algorithm) whereas most mutating systems can only do mark and sweep due to the need to preserve addresses. The fact that the reference graph can only expand by adding new nodes allows the garbage collector to split the heap into multiple regions depending on object age. While other garbage collectors do this as well, the Haskell implementation can get away with never collecting some regions again unless an outside reference vanishes due to the guarantee of immutability.
Matlab has had lambda functions for a few years now, but their notation is nowhere as flexible as that of Haskell. Regarding O(1) arrays with mutability, look no further than Data.Vector (https://hackage.haskell.org/package/vector-0.11.0.0/docs/Data-Vector.html) Re. workflow and batch reloading, iHaskell (Haskell interface to Jupyter); try its Docker-based setup, I found it working out of the box.
`haskell-ide-engine` is working on a better compiler API. https://github.com/haskell/haskell-ide-engine https://github.com/haskell/haskell-ide-engine/blob/master/docs/Protocol.md#json
Can't you already put a list of your most used extensions into a .cabal file?
A better question would be to ask yourself what interests you in general. Desktop applications? Mobile apps/games? Financial data crunching? Web development? Game development (graphics, game logic, AI, networking)? Device drivers? Embedded systems? Robotics? From there you can then possibly find a niche which interests you and apply the skills you are currently developing.
Awesome, thanks!
Torrent-like distribution for the mirrors? You probably could. But what problem are you trying to solve by that? In any case, it's important to perform mirror sync-ups atomically. First you need to make sure to mirror all source-tarballs for the package releases newly added to the incremental package-index, verifying their sha256 sums, and only then update the index-tarball (also sha256-verified) and the .json meta-data files. 
Stack and LTS Haskell are two different things. Stack (also based on Cabal) and cabal-install both will require people to update their libraries to work with GHC 8 after the release.
What is it for, what does it do?
Gosh. I know what I am doing for my Masters degree curriculum now...
Looks interestung... It it only had some example usage in the docs
I claim it is, due to parametricity.
[removed]
There is a whole `f` of `x`s that go in, and the `g` `x`s that come out can be any of them.
Oh, you're right! Thanks
Polymorphic variants are just the dual of polymorphic records. If you have a record type A with fields x,y, you can create another record type B with fields x,y,z and make it a subtype of A after the fact. Analogously, if you have a sum type A with cases x,y, you can create another sum type B with cases x,y,z and make it a supertype of A after the fact. This requires a bit of language support, but solves the expression problem in the most natural way IMO.
Came here to say this. Misses `README.md`, docs and examples. Maybe /u/markedtrees can elaborate a bit on what's interesting, hopefully providing some examples ('d be great if those become part of the project).
There was [some discussion of this previously](https://www.reddit.com/r/haskell/comments/4acbfv/efficient_binary_serialization/d0za70x). The short story is that `binary-serialise-cbor` is a bit slower, but supports more use cases.
It seems likely that some well-behaved and well-explored GHC extensions may make it into the standard. I'm curious on the other hand if there is existing behavior in GHC (no extensions) that could render it noncompliant with this future standard. I realize this is over the scale of years, so it's impossible to tell, but how likely is the committee to say "GHC does this, and making it not do that would require burning the whole thing down, make it standard" (or some suitable hyperbole) vs "The Standard says this now, so GHC is noncompliant". I only call out GHC specifically because it's sort of the de facto implementation, at this point.
Haven't checked the source either, and I couldn't find really clear information online. [This page](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC), which would be the canonical Wiki page for GC stuff, indicates that the remembered set (which is what the write barrier adds stuff to) is used for cells of older generation pointing to values in younger generations. This is not much information, and not sure how up-to-date, but it would rather support my idea that you need a barrier only when writing to an old location -- also this is how the OCaml GC, which I'm more familiar with, does it. Also [this ticket](https://ghc.haskell.org/trac/ghc/ticket/7662) is relevant to the OP question (but it discusses what could change rather than what already is there).
I happen to have access to an example, as the author gave me an [example implementation](https://github.com/gelisam/frp-zoo/blob/master/objective-example/Main.hs) of the [FRP Zoo](https://github.com/gelisam/frp-zoo#readme) toy program, even though objective is not technically an FRP library.
The copying collector still implements mark and compact. The difference is that it implements out-of-place mark and compact whereas the other one seems to do an in-place mark and compact (impressive!).
Another reference: http://community.haskell.org/~simonmar/papers/parallel-gc.pdf As I recall, using copying rather than mark/sweep or compact or etc for the nursery is a key choice, as so many shortlived objects are created. So if anything the generational hypothesis is somewhat "stronger" -- however I don't remember where to work for the actual citations on this. As pointed out elsewhere in the threads, due to thunks having internal mutation, immutability doesn't give a guarantee one might otherwise imagine (that older objects don't point to newer ones) and so iirc there are no "pure language" specific tricks used by GHC, though there are a large grab-bag of other particular tricks. That also perhaps answers how it relates to STG -- as, I think, do the wonderful slides posted elsewhere.
Is the provided naturality law incorrect? The rhs is fmapping f over `Object f g`, not `x`, right?
thanks, I read the transcript and it really helped me understand a few things.
I would rather it be called type-examples, type-specialization, etc. I would not use :examples because it's too generic, and might be useful for other things in the future (maybe haskell would one day include an examples section/pragma for each function that includes a small snippet of code. Examples from source file FTW) edit: nvm just saw the second comment in the thread, and Idris's :doc command looks nice
Looks like it's trying to model "objects" in Haskell. The f functor describes the public interface and since it returns in something like the state transformer monad you can have the object evolve over time.
Interesting. I've used both the relative path option and the symbolic link one. Both worked. No clue why `stack init` would have different constraints than what you need when building.
How does it deal with circular lists, graphs etc then?
Isn't it equally important to write a few notes for yourself? I have screenshots and notes on my website / github for everything. A few years down the road I might not remember what papers I referenced while developing, the neat little features I added, anything that worked out super well or totally sucked, surprising things that happened, stuff I was particularly proud of etc. It's also really fun to browse through! I also always take screenshots, because maybe in a few years I'll be on Linux/Windows/OS X and have no way of compiling / running a Direct3D/Cocoa/C#/Docker/whatever app. I just think it's a shame to spend hundreds of hours on something and then not spend ~1h at the end to document it, write some notes, take some screenshots, package it up somewhere etc.
Depends on the person, I guess.
Seems similar to Auto. https://hackage.haskell.org/package/auto-0.4.3.0/docs/Control-Auto.html
You cannot build them in Erlang -- you don't have non-function recursion, nor mutability (except at the Mnesia level). You have to introduce an indirection, for example have graph nodes be represented by integers, and in a list of neighbors give the integers instead of the node directly. Then a graph is a finite map from integers to neighbors. This is often a better choice of representation anyway: while the immediateness of direct value pointers is occasionally useful, programming with cyclic data structures is difficult and it's easy to shoot yourself in the foot. There has been work to make programming with cyclic data structures less fragile, such as [Language constructs for non-well-founded computation](http://www.cs.cornell.edu/~kozen/papers/nwf.pdf), Jean-Baptiste Jeannin, Dexter Kozen, and Alexandra Silva, 2013.
I whipped this up today, as part of some internal initiative at work. I just wanted a reliable way to type-check a project that already works with stack. Might come in handy for someone else.
I see, thanks for the clarification. Any workarounds that folks might do in the meantime before LTS 7 is out then? Perhaps a custom snapshot based off of an LTS one with the right version modifications?
Thanks for asking! I'd like an ambitious 1D 2C 3D, even though I doubt it's easily achievable. If we get 1A 2B 3D I won't complain though. It would be great if this not only specialised type classes but also specialised kind, such as defaulting Levity to Lifted in the type of $. I know that behaviour's behind a flag at the moment but they're both about simplifying types, so a generalised solution would be great. Here's the type signature that almost nobody wants to see: Prelude&gt; :t ($) ($) :: forall (w :: GHC.Types.Levity) a (b :: TYPE w). (a -&gt; b) -&gt; a -&gt; b Also, does option 2B simplify the type of (+)? I think most people were OK with ghci not specialising: Prelude&gt; :t (+) (+) :: Num a =&gt; a -&gt; a -&gt; a If it says that the type of (+) is `(+) :: Integer -&gt; Integer -&gt; Integer` then I think it would be best to also say what it's done, like this: Prelude&gt; :t (+) (+) :: Integer -&gt; Integer -&gt; Integer -- type is more general than this; use :type+ to see it So for me, the type of ($) should be: Prelude&gt; :t ($) ($) :: (a -&gt; b) -&gt; a -&gt; b -- type is more general than this; use :type+ to see it I do think a lot of people would like to the Foldable and Traversable functions specialised to lists though. But defaulting Show or Eq doesn't make sense to me. Maybe I'm misunderstanding? We'd also want this in Haddock of course :-). There, it's easy to be interactive, so instead of the comment and extra commands, just provide a clickable link, closer to what you'd have in Idris. Finally, ideally this mechanism to help with the complicated types in some advanced libraries. I won't mention any libraries by name but my guess is we can all think of some that would benefit from a type specialising mechanism.
cute.
Any chance students abroad (I for one live in India) would get some help with securing Visas to attend the hackathon? I would _love_ to attend, but I think I would need some help with procuring the Visa for this from ETH Zurich / Haskell.org / the people who are hosting the event
I thought there was absolutely no sound, but it turns out that it's just that there's no right channel audio. Headphone users be aware.
Thank you so much! I've been using the built-in flycheck support for haskell and it seems to always be wrong. Looking forward to giving this a try. Plans to add it to melpa?
They are already on github or bitbucket, but just don't want to have clone them for each project using them. About the bounds, stack (I mean stackage) kind of sort the problem : in theory you only need to put bounds on strategical packages to chose the correct snapshot and let the solver do the rest.
Were you using the `haskell-stack-ghc` checker?
Thanks for this! Tried it out using `quelpa` to install by putting this in my `init.el`. ``` (quelpa '(flycheck-stack :repo "chrisdone/flycheck-stack" :fetcher github)) (use-package flycheck-stack) ``` Being able to use flycheck in my executable and tests is enough reason to make me switch from `haskell-stack-ghc`. It was also very responsive on a large project. I hope you can make this as newbie friendly as possible. One of the main pain points I've been having with the different flycheckers is that no matter what they always seem brittle. I really want a reliable one that works 99.99% of the time, something that you get in a language with big tooling support like Java.
It's not like being useful is a prerequisite for participation. In fact, we will have a beginners' track this year again, trying to follow up on last year's feedback and to improve on how we can help interested people getting started so that the community grows.
&gt; They are already on github or bitbucket, but just don't want to have clone them for each project using them. Stack (as well as the stuff in the cabal-1.24 tech preview) takes care of the cloning for you. You just put something like this in your stack.yaml: - location: git: https://github.com/maxigit/your-package commit: 8ae45b6de4cc5427a19ece1389d868a2a4f73ea8 &gt; About the bounds, stack (I mean stackage) kind of sort the problem : in theory you only need to put bounds on strategical packages to chose the correct snapshot and let the solver do the rest. Yes, but when you are using packages in multiple applications this is only guaranteed to work if you are using the same snapshot in all of your applications. If not, you are opening yourself up to problems. Non-stackage dependencies give more potential for problems. Bottom line: stackage over-constrains the problem. This is fine if you're only ever working with one application. But if you're writing libraries for use in multiple applications it will become a problem sooner or later.
OT but I am smiling under me 'stache: **sold out**? I am starting to think boycotters were actually a viral marketing firm in disguise. *edit: downotes won't cancel the fact that there has been some controversy. Deal with it.*
Does it share the ghci process with haskell-interactive-mode?
&gt; I'm curious on the other hand if there is existing behavior in GHC (no extensions) that could render it noncompliant with this future standard. It'd be nice to minimize, but I'm sure there will be. E.g., there's been a lot of kerfuffle over the foldable/traversable generalization in the prelude. Will that make it into the new standard? who knows. (Fwiw I'd support removing all those functions from `Prelude` entirely, and requiring users to import the monomorphic versions from `Data.List` or the polymorphic ones from `Data.Foldable` and `Data.Traversable`. That way everyone can pick their poison. But who knows if this'll have any traction.) There's also a proposal I have in mind which should help to finally get MPTCs into the standard, but this means having something similar to `-XFlexibleInstances` but which doesn't match up exactly on a few details. If this were to be accepted, then there's some possibility for mismatch there too. Similarly, the current proposal for adding bang patterns doesn't necessarily line up with what GHC calls `-XBangPatterns`.
Also, it is very performant at least in cpu time and rendering terms. Now space wise not so much.
&gt; immutability doesn't give a guarantee one might otherwise imagine (that older objects don't point to newer ones) Interesting! I wonder whether there are circumstances where it makes sense to treat the results of forcing a thunk as being the same age as a container holding that thunk...
Another approach I tried out is [here](https://github.com/acowley/MetaPragma). It just does a substitution on user-defined metapragmas that each stand for a set of actual language pragmas. Rolling this functionality into `hpack` makes sense to me.
&gt; I'm just surprised to be the only one having this problem. I think it's because most people don't need the solver, and most people keep their packages in a subdir of the stack.yaml. We would like to support your workflow, though. Not sure if it's newworthy, please just bump the issue in the future. The plan is to just parse it the same way targets are parsed, or change how path-io works. We just need to use a different function. It's a simple change, we've just been busy with other things and this isn't causing any trouble for any of the main stack developers since we don't use that workflow. It's also good to encourage people to work on stack to fix these easier things so that they get more comfortable with working on it and submitting PRs.
However, since this seems to be causing you some degree of strife, I will just fix this now.
I wasn't thinking you were against the standard, I'm just pointing out that `-XGlasgow2016` (so far as I understand it) has different goals from Haskell 2020. (Fwiw, the part of `-XFlexibleInstances` I think is squirrelly is allowing non-linear use of type variables. The rest of it â€”allowing non-variables in argument positions, etcâ€” seems fine. And, it's not even that non-linear type variables are "bad" per se, just that they're often not what we mean, so we need some way of specifying/clarifying what we mean.)
As a counterpoint, lens already generalizes indexing somewhat with `ix` and `at`. If you did have a function of type `Map k v -&gt; r` which indexes into the map and wanted to generalize it you could change it to a function of type `(k -&gt; v) -&gt; r`, and then pass it `(!!) list`, `(!) map`, or any other function. I don't think there's a sane way to unify them that takes into account the differing semantics for updates/deletions though. Not to encourage bad habits, but I'm too lazy to look up the non-lens based total variants of `!`/`!!`.
It think Idris's ability to prove that a client and a server are both implementing the same protocol could be useful in at least one case: Use Idris-&gt;JS in the client browser to talk to Idris-&gt;ELF on the back-end via websockets. Which such proofs in place, you know that if you change one side and the code still compiles, you need not bump the protocol version.
Fixed in https://github.com/commercialhaskell/stack/commit/42a61fb278aeb99ad61bb088da9a87b2313ac325
Checking the history (https://wiki.haskell.org/index.php?title=Tutorials/Programming_Haskell/String_IO&amp;action=history) it hasn't been touched since 2011. The haskell wiki is a great resource, but lots of pages lack tending and updates. If you're finding it useful, it would be great if you could register an account and go through and fix links and the like as you go.
Let's figure it out! That's right, time for another [anti-tutorial](http://gelisam.blogspot.ca/2015/01/haxl-anti-tutorial.html) :) Given the name of the package, [Object](https://hackage.haskell.org/package/objective-1.1.1/docs/Control-Object-Object.html#t:Object) is probably the central abstraction in the package, so let's start with that. What does an Object do? Well, let's look at its destructor: runObject :: Object f g -&gt; f a -&gt; g (a, Object f g) Ignoring the `f`, `g` and `a` for now, this is clearly some kind of state machine, as `runObject` takes the current `Object f g` as input and returns the modified `Object f g` as output. Makes sense, since objects in the OO sense are about encapsulating state behind a fixed set of methods. Could the `f a` be describing which method to call? It would make sense, as such a description would need to be indexed by its return type in order to allow different methods to return values of different types. And indeed, in addition to the modified state, `runObject` returns an `a`. The only remaining piece of mystery is `g`. Given its position in `runObject`'s type, I'd guess that `g` is intended to be a monad. So, what is an `Object f g`? It's an OO-style object supporting methods from `f` and causing effects in `g`. It has a private state, which users of `runObject` are supposed to thread manually from call to call. Next, I see a bunch of composition operations. Seems like `Object` is a Category, but I don't see a Category instance. Maybe the library was implemented before Category became poly-kinded? *edit*: now that I've tried, I see it's the same story as `Set`'s lack of a monad instance: composition requires a Functor constraint on the indices. In any case: composing two objects would mean that calls on one object cause effects which are themselves calls on the second object. This is a bit surprising because I was thinking of the method calls as being described by a GADT with a phantom type which would only be inhabited at a few type indices, something like this: data CounterMethod a where Reset :: CounterMethod () IncrementBy :: Int -&gt; CounterMethod () GetCount :: CounterMethod Int But that doesn't work, since `g (a, Object f g)` has such a complicated index that `g` is probably inhabited at every inhabited type index, like monads are. The other reason this doesn't work is that "effects which are themselves calls on the second object" is clearly intended so that the first object implements its methods by calling methods on the second object, but in order for this approach to be practical it has to be possible for this implementation to call zero or more such methods on the second object, so `g` (and thus `f`) cannot represent a single method, it must represent a chain of method calls. By now it's quite clear that both `f` and `g` are intended to be monads, so something like this: data CounterMethods a where NoOp :: a -&gt; CounterMethods a ResetThen :: CounterMethods a -&gt; CounterMethods a IncrementByThen :: Int -&gt; CounterMethods a -&gt; CounterMethods a GetCountThen :: (Int -&gt; CounterMethods a) -&gt; CounterMethods a So, what is an `Object f g`? It's a way to implement the operations of monad `f` in terms of the operations of monad `g` plus some private state. [`liftO`](https://hackage.haskell.org/package/objective-1.1.1/docs/Control-Object-Object.html#v:liftO) seems to be the special case in which no private state is necessary. As far as I can tell anyway :)
I don't exactly know either. I imagine looking at the old prime.haskell.org trac would give some ideas as to how to e.g. craft a proper proposal. Here are for example the closed tickets for H2010: https://prime.haskell.org/query?status=closed&amp;milestone=Haskell+2010&amp;col=id&amp;col=summary&amp;col=status&amp;col=owner&amp;col=type&amp;col=priority&amp;col=milestone&amp;order=priority So then I can look at the removal of N+K page and see how it reads: https://prime.haskell.org/wiki/NoNPlusKPatterns Then I could look at the pending proposals, either from other milestones on the prime trac, or elsewhere, and craft something similar for the new process, including a proper report delta. At that point, I imagine I'd raise discussion on the mailinglist to point people towards it.
For 2C, I was was kicking around in some conversations a `{#SPECIALIZE_DOCS#}` pragma that would be checked like `SPECIALIZE` is but would just affect display. (I suppose we would also need a `SPECIALISE_DOCS` spelling to be properly portable across the Atlantic). I very much like the idea of a `:doc` command that could start with just this sort of behavior, and then be extended to do many of the other nice things proposed.
This is really good feedback. I'll see if I can convince them to use a different domain name with less official overtones
[Improved link](http://classicprogrammerpaintings.com/post/143178891892/haskell-programmer-seeks-forgiveness-for-using)
Thunk updates can create references from older generations to newer ones, in the same way as regular mutation, so you have to keep [remembered sets](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC/RememberedSets) as usual.
I had a bit of a look and while I see some similarities I don't really see how I would replicate the above behavior, can you give me a code example by chance?
[There sure are!](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC/EagerPromotion)
Alright then, count me in!
Yeah I have no doubt about that, as far as my experience goes, the haskell community is one of the nicest to talk to (and also rather patient). Still, if I take up place to learn from people, people should be able to learn from me, at least to some extent.
&gt; A better new user experience and improved tooling is in my eyes more important I totally agree, they should promote and focus their efforts on Stack and let cabal-install wither for good.
In other words nothing will be done about https://www.haskell.org/downloads ...
Boycotters who are petitioning sponsors, pillorying ticket holders and organisers because of the essay *(tl;dr, a close examination of how slavery is counterintuitive, and humans are terrible slaves)* of one speaker at the event, because of hot-button politics.
Having not killed pigeons while driving makes you a good driver? 
A question about CBOR; does the serialized representation contain information about endian-ness and word width? In other words, will something serialized on a 64 bit little endian system, deserialize correctly on a 32 bit big endian system? Edit : On the slide titled 'Separation of concerns - and Encoding AST' you give a data structure: data Tokens = TkWord !Word Tokens | TkWord64 !Word64 Tokens .... but isn't the size of `Word` dependent on the architecture? For 64 bit arches, `Word` and `Word64` are the same size. 
It is basically considered a full-time commitment.
I like IO, but I sometimes feel limited by the fact that it assumes that there is only one real world. It ends up deterministically sequencing every action even though sometimes it's overkill. I could imagine re-implementing IO with dependent types to differentiate between worlds: "World 0 a", "World 1 a", etc., would all behave like "IO a" on their own timelines. Then I would probably throw some unsafePerformIO to do the actual work. Is it a stupid idea?
When should `unsafePerformIO` actually be used? Only calling pure functions through FFI comes to mind.
Well, that's kinda what forkIO gives you.
When importing `#define`d constants, or on the output of seriously-I-mean-it pure functions. The runtime simply doesn't re-compute pure values.
looks like they've used unsafePerformIO when building this webpage...
This is pretty neat. As someone who has never really grasped what continuations solve, I feel like this is a much more helpful example than most of the `ContT` monad transformer articles I've read.
When you want to define something equivalent to a new language primitive.
It should be portable; there's nothing fundamental about the underlying format that prevents this, although some of the API might need to be shuffled a little to be more clear. But note that what you say is fine: there are explicit encoders for individually sized types, so you can be explicit. Also, sure, `Word` technically *is* `Word64` on a 64bit platform, but you can't know that unless you unbox it personally. So if you encode a 64bit word on a 32 bit platform, you can read it back later - it's strictly encoded as a 64-bit integer in the read/write paths, at the user level. The code is also currently broken on 32 bit architectures, and fixing that is mostly tedious, but I haven't gotten around to it. And actually this is really tedious because GHC's distinction of Word/Word64 internally is really stupid and confusing, but that's just an API difficulty, not a fundamental portability one.
Looks greate! I have a question. I'm currently using the default haskell flychecker. What does flycheck-stack do better? And is there some limitation, like can't check inline-c code? 
When creating top-level `MVar`s, `IORef`s, etc. And if they simply don't care about reentrancy.
Wouldn't it be possible to use ST instead of IO then?
Here are slides: http://dbp.io/static/fn-continuations-haskell-meetup-2016.pdf And, the same but with animations (ie, proper version of what was messed up in the talk): http://dbp.io/static/fn-continuations-transitions-haskell-meetup-2016.pdf
Not sure why the downvotes. My answer is the same (if less certain-sounding) as the upvoted answer.
No, this is intended for people who use and don't use that alike. There is another mode that reuses the process from HIM but it has infelicitous interaction with the REPL, as you can imagine. But I'm considering adding to ghci-ng a :check command that compiles the file but doesn't load it in scope or affect the REPL. This might be a best of both worlds scenario.
See my other reply. A :check command may be the answer. At this point I'm pretty happy with just making GHCi smarter than going in the opposite direction like ghc-mod and such.
The default haskell checker is 'ghc. That one doesn't know about what packages you want to use or source directories or cabal macros or special flags etc. `cabal repl` and `stack ghci` both provide this setup for you. I don't know about limitations. It can check any file that you can loaf into GHCi.
I think you're encountering a rarer problem because nobody uses the solver. It's more of a "if you must..." but it's not part of the regular work flow.
I think it's great that the community is trying out both directions. A while back I was optimistic that `ghc-mod` would gain REPL-like functionality, but I think that effort has petered out a bit. The `:check` command makes a ton of sense; thanks for working on this!
Part of the problem is that the higher the skill of a specific job role, the more people in that role seem to acquire false class consciousness, believing that they're somehow "too skilled" to be getting exploited (long hours, low pay) or have their compensation driven down by bad labor practices like cheap outsourcing. Ironically, this means that your average blue-collar worker is more likely to be unionized and more likely to stand up for their rights at work than your average white-collar "professional".
If you want to replace `.` and `$`, you can either do `import Prelude hiding ((.), ($))`, or roll your own prelude that defines these functions differently.
Yes, a :check command sounds ideal! I'm curious though, what do you see as the benefit of using ghci instead of a batch ghc process, if the ghci process isn't going to be used for interactive evaluation? Seems like you end up paying the RAM tax for no reason.
Yes.
One way to think about this is that lazy evaluation is already a kind of automatic use of `unsafePerformIO`. If Haskell didn't already do memoization of lazy values, you could build it using `unsafePerformIO` and `MVar`. Sometimes, though, the kind of memoization you want is more complex than what's automatically implemented by the language. In that case, you can use `unsafePerformIO` to get the same effect. A few examples: * http://hackage.haskell.org/package/diffarray uses unsafePerformIO to implement persistent arrays. This particular implementation optimizes for backtracking access patterns, but there are other approaches known in the literature that improve worst-case performance for any access patterns. * http://hackage.haskell.org/package/persistent-equivalence adds, on top of that, a persistent implementation of union-find, where path compression is critical to getting better amortized performance. By using `unsafePerformIO`, this retains a pure interface.
No. `ST` is useful when your goal is to produce one single value which, when forced, can be computed using temporary mutable state. In general, though, it's quite possible to have a data structure that you want to maintain over time, underlying many different values which may be forced in any order. `ST` is sometimes not the best tool for this task.
If I had to guess at the reason: the term 'SJW' is usually a mechanism for assholes to dismiss people and arguments without considering them.
Kinda like there's are 'deals' on when it's safe to use `unsafePerformIO`.
Agreed.
I would definitely contribute to a KickStarter. I wouldn't even mind if the creators just used the money to pay themselves for their time.
Yea this is definitely an issue. When you learn everything as "boxes", it's hard to imagine monads as anything more than a wrapper around a single value. This is fine for figuring out the intuition for Maybe, or State. But when you get to Lists as a monad, or parsers, or anything less than trivial, the "box" explanation falls apart and it becomes really hard to understand that not all monads are wrappers around single values.
That is neat. And really it is basically what I am suggesting. But instead of adding `apply` to a class, you are defining ` ` Or maybe some other character to represent a ` `, within `instance (-&gt;) Foo where`.
What about `StandaloneDeriving`? I like not cluttering up my data definitions with `deriving` clauses, and I honestly don't understand why this isn't permitted by default.
That's actually pretty neat, might use it, only annoying thing is not being able to use ` `. Isn't there like a rebindable syntax thing though?
The problem is a bit more nuanced than that. A functor isn't a box. However, *only* saying â€œa functor is something that has `fmap` defined on it and satisfies some lawsâ€ is a didactic failure as well; at least for me (and, since I'm likely not special, for many other people) laws are useless when it comes to acquiring intuition for a typeclass. Giving several examples of functors doesn't help *that* much, since in most cases you're only able to give 3â€“4 examples before the reader/listener gets bored. Again, generalising from one example, but: when you're a beginner, knowing that `IO`, `[]` and `Maybe` are functors doesn't help with building the intuition. â€œBoxâ€ seems to be a good *partial* intuition to me; most functors that beginners see (trees, lists, streams, `Maybe`) are boxes. If you can say that `Maybe` is a box that can be empty or contain something, why do you take issue with saying that a list is a box with many (potentially an infinite amount of) elements, or that `Const` is a weird box that is guaranteed to be empty? The commonly-cited example of `Const` not being a box always seemed weird to me. By the way, `IO` is a box too â€“ it contains a value and produces a side-effect when you open it (or it might get stuck or blow up, such things happen). Also note that â€œknowing what functors truly areâ€ isn't a virtue in itself. There are several reasons why it's good to know about functors: 1. You can recognise functors. 2. You can recognise what *can't* be made into a functor. 3. You can reason about functors. The box analogy doesn't help with 2. but it helps with 1. in many cases. Just handing out a set of laws doesn't help with 1. (because, let's face it, few beginners would attempt to prove a set of laws about every type they see) and I would argue that it doesn't help with 3. either â€“ while knowing laws is nice when it comes to corner cases (or when you want to be rigorous about everything), having an intuition/heuristic that gives you a quick answer in 95% of cases is much, much more useful. While I'm at it, I'd like to suggest my own analogy that could be used instead of boxes â€“ *producers:* 1. Lists produce values. Infinite lists produce values indefinitely. 2. Works with `IO`, too. And functions. 3. You'd have to stretch the analogy a bit when it comes to ordinary boxes (`Maybe`, etc.), but saying â€œcontainers â€˜produceâ€™ values because, well, you can take values from themâ€ should still be pretty understandable. 4. Interacts nicely with another helpful (IMO) analogy â€“ monads as callbacks. 5. `fmap` â€œinstallsâ€ a converting function at the end of the producer, nothing more. Makes sense why you can't use `fmap` to change the number of elements in a container. Also very neatly ties `fmap`, `(.)` and â€œpipelinesâ€ together. Disclaimer: I've got little experience with teaching people Haskell and I might be disastrously wrong; pedagogy is a very complicated topic and there are lots of unknown unknowns lurking here, which is why any attempts to *reason* about what works and what doesn't â€“ without having actual experience on your hands â€“ should be met with distrust.
Some of the problem seems to come from always talking at the value level and avoiding talking at the type level. It's a type function, it takes one type and gives you another. And then the type mapping f :: * -&gt; * in the type level comes with a function mapping fmap :: (a -&gt; b) -&gt; (f a -&gt; f b) in the value level. So you can say a functor is a type function that lets you bring `a -&gt; b` functions over to `f a -&gt; f b` in a meaningful way... such as mapping over a list or tree, transforming `Just` values, post-composing function return values, doing something "after" an `IO` computation like `.then` on a JavaScript promise, etc etc.
Paging /u/ezyang
Great feedback! I actually do teach people Haskell and the producer analogy may be very useful in the future.
Definitely, somehow it got lost. 
Needs more applicative: )) pathFinder t s n = head $ filter ((==s) . head) paths where paths = [t] : (paths &gt;&gt;= map &lt;$&gt; flip (:) &lt;*&gt; n . head) A smidgen of pointfree: pathFinder t s n = head . filter ((==s) . head) . pathGen $ [[t]] where pathGen = join . iterate (&gt;&gt;= map &lt;$&gt; flip (:) &lt;*&gt; n . head) This was fun. Thanks.
I'm afraid there isn't any good summary here. Your best bet is to look at the commit history. `4183976003ab9da51583d78debec798cd53789cc`, `919e5c16a449a2cd6ed5c4017477294435a1b1a2`, `19ab5256a1a0a4e7d4ed0d36973fb43eeeda447f` are the first things that jump out. None of these will be ground-breaking, but performance work is by its nature incremental.
Well, presumably a system for redefining whitespace would have a way to deal with this but if you implement it in a naive way, this would give infinite recursion: ( ) f x = f x So there would be an added complexity of additional rules for that as well. Also, you can still be very general while using operators instead of changing deep syntactic meanings.
If you haven't, please do.
Thanks for your reply!
`Data.Map` is basically a no-SQL database
Sir Simon of Jones first of his name, lord of ghc
Sir Simon of Peyton Jones as he'd be the first to correct you. ;)
Wait is Jones not where he hails from? 
I include the `forall`s simply because I always prefer to use GHC with `-fprint-explicit-foralls` on. It's in my `ghci.conf`. For this particular proposal, because both defaulting variables and issues around specified variables (for use with `TypeApplications`) are involved, I think including the explicit foralls makes good sense. I am not, at all, proposing that printing `forall`s become the default. When I teach Haskell, I do include the `forall`s (though I don't require students to write `forall` in their type signatures), as it makes the scoping explicit. Without the `forall`, we're relying on knowledge about the difference between capitalized identifiers and lowercase ones.
Clearly he hails from Peyton Jones. It is a small hamlet, where it seems the main pastime is to rail at being confused with the inhabitants of Jones.
Every use of `runST` is basically a use of `unsafePerformIO` where the type is what guarantees its use is actually safe. So you might well use it on operations that produce and consume pointers, which is what the `bytestring` and `text` packages use it for. (Incidentally, `bytestring` also has [`accursedUnutterablePerformIO`](http://hackage.haskell.org/package/bytestring-0.10.6.0/docs/src/Data.ByteString.Internal.html#accursedUnutterablePerformIO), which can pretty much only be used for things which don't allocate memory.)
You're right: let x = undefined in x `fastEquals` x might give `True`. 
Congratulations, Simon! I can only speak for myself, but I am certain that everyone in the Haskell community is thrilled that you have received this well-deserved honour. EDIT: Oops, sorry. s/honor/honour/
"The educational materials we create encode our values" so happy to be educated about that. I wonder what are the values of Descartes's community. I canâ€™t tell if the Cartesian community wants me. RWH took much effort and I guess they did not make that much money. Everyone wants easy tutorials, good scientific material, but it's not achieved through complaining. And contrary to what the author thinks I am willing to bet the author of RWH wrote the book for that *precise* reason, so it would be better to see him as being thankful for it.
I've been struck by how lucid his papers are, and he seems such a nice guy too. Good news.
I looked at the paper you linked. It seems interesting. But their pictures don't seem simpler to me than the ones at http://dkeenan.com/Lambda/ (Essentially lambda calculus looks simpler to draw, if you replace all variables with their De-Brujin indices.)
I disagree. "Box" or "Container" is an excellent way to explain what a functor is, that captures the majority of cases that people actually use. For sure, it should be accompanied with the fact that there are some examples that don't fit the metaphor precisely. With real experience, a more accurate intuition will follow.
Iâ€™ve never tried it, but FFI/JNI is probably the most direct route. If you write this in Java: public class Foo { public native void bar(); } And write this in Haskell: foreign export ccall "Java_Foo_bar" bar :: Ptr () -&gt; Ptr () -&gt; IO () bar :: Ptr () -&gt; Ptr () -&gt; IO () bar jniEnv thisObj = putStrLn "Hello from bar!" Then you should be able to compile the Haskell code as a dynamic library (`foobar.{so,dll,dylib}`), and load it from Java using `System.loadLibrary("foobar")`. Then everything should work as expected as long as youâ€™ve got the types correct.
uh, so how does this work in the UK exactly? I mean, why does becoming a fellow of royal society suggest a knighthood? Wiki suggests its the british equivalent of a national academy, right? So, they normally get knighted eventually?
I also recall our friends at Tweag threatening to drop `inline-java` unto the world, as a byproduct of their work on Haskell-Hadoop integration. Edit: apologies, I replied too quickly. OP wants to call Hs from Java, not the other way around.
From what I gather knighthoods are merit candy they give out for state service. His merit, as far as the state is concerned, would be spearheading the new CS curriculum and being a warm, fuzzy, thing... which is more achievement than many a permanent secretary can claim.
Interaction combinators are born out of the research on proof nets, a general field designs to find a "better syntax" for proofs in linear logic. I'm a bit surprised by your claim that there "is not much research" on typing interaction combinators; they are born out of type systems and the semantics of the lambda-calculus. Yves Lafont's 1990 article on Interaction Nets have a proposal for a type system, for example. I'm not familiar with interaction combinators in particular, but I would recommend reading on linear logic and proof nets in general to get a feel for the domain.
[removed]
I think approximately speaking lambda calculus is the simplest way to describe the logic of computation while a Turing machine us the simplest way to describe the mechanics of computation
Albeit that the Turing machine is simple by virtue of leaving most of the work in describing the mechanics to the programmer of the machine :)
Hey there [scottrobertladd](https://www.reddit.com/user/ScottRobertLadd), there's a library called GHCJS that allows you to write completely in Haskell and then output JS. More specifically check out [reflexfrp](https://www.reddit.com/r/reflexfrp) which has a web app specific package [reflex-dom](https://github.com/reflex-frp/reflex-dom/) that uses GHCJS to compile Haskell into web GUI's. edited: spelling and correct links 
I like that way to think, but I find interaction combinators a much simpler way to describe the logic of computation. Once you manually translate a few Î»-calculus programs to those combinators, you feel how cleanly the logic is captured by the resulting graph, in a way that is hard to understand without doing it yourself. Manually reducing complex Î»-calculus terms through inets with a pencil and an eraser is an insightful experience.
I have had success with GTK, using OpenGL for 3D controls and `diagrams` for 2D controls. Proof: https://twitter.com/acid2/status/603579370322718721
I enjoyed this a lot. Thanks. 
I also dislike "context" explanations for these. A "context" is never properly defined, and if you nail down what is meant, it's just circularly defined to be a functor. So the explanation is pointless.
Best haskell speaker I've ever had the pleasure of listening to. 
&gt; producer analogy Huh. That seems to be a better analogy than any I've seen before. Thanks, /u/peargreen! It also ties in nicely to co-/contra-variance: a covariant functor is something you can get `a`s out of, as opposed to a contravariant functor, which is one you can put stuff into. EDIT: ... which is exactly what /u/hjljo said one comment below. That's what I get for posting in my excitement before reading other replies.
It is a popular book, quite a few people find it approachable. Honestly, I haven't noticed the fat jokes when going through it, when I was first learning Haskell.
Given the price difference I would just pick up the latest and greatest RPi. Did you manage to get any piece of Haskell code with major dependencies (lens, web server stack etc.) compiled this way? All the blog posts I managed to find (i.e. https://pr06lefs.wordpress.com/2015/09/06/ghc-web-dev-considered-harmful-on-arm/) make it sound like it's a lengthy odyssey full of bugs, crashes, failed 10h builds, obscure Linux wizardry just to get something super fragile barely running in the end (maybe).
There don't seem to be any bindings for Aerospike. Here's all the DB binding available on hackage: https://hackage.haskell.org/packages/#cat:Database . Given that, you could build your own but I imagine that is too much of an ask for your project. If you want to use haskell, I'd recommend a different database (probably postgres or Redis). If you really want to use both, then I'd look at http://www.aerospike.com/docs/client/c/ for ideas on how to write the FFI. Another (hacky) option is to use one of the other clients (I'd use go) to make a small local HTTP server + API for what you need and have haskell hit that API. Overall though, I'd probably just use postgres.
[removed]
Just skip the math problem.
Maybe AC-Vector? When I google â€œhaskell Data.Vector linear algebraâ€, this is the 3rd link for me: https://hackage.haskell.org/package/AC-Vector-1.2.2/docs/Data-Vector.html (below linear and hmatrix).
Types go on the lines. The rules of combination just are whatever the logic says, tho.
&gt; if people get offended by this its their problem and nobody else's This dichotomy between â€œjustified offenceâ€ and â€œunjustified offenceâ€ becomes much more vague when you replace â€œoffenceâ€ with â€œsufferingâ€. Should we try to reduce people's suffering unconditionally? Or should we only do it if it's â€œnot their faultâ€? I can see how the latter approach is attractive, but my problem with it is that I can imagine a world where the â€œif it's your fault, you're fuckedâ€ principle is applied universally, and I don't want to live in this world. So, at least to me it doesn't matter whether it was intended as a joke or not. If it offends people, *even when it seems that they are totally unjustified*, I'd want to solve this problem somehow. Of course, â€œsolve this problem somehowâ€ doesn't necessarily mean â€œcensor out the joke / boycott the book / put a trigger warning on itâ€. Moreover, in this case I'd say that the problem is too small anyway, so we don't have to do anything about it. *However*, â€œwe won't do anything with it because it's not worth itâ€ is still different from â€œwe won't do anything with it because it's not a problem at all and if anyone gets offended by it they should go to gym / stop being crybabies / etcâ€. **EDIT:** replace â€œsufferingâ€ with â€œunpleasureâ€. My definition of â€œsufferingâ€ is unconventional and I don't know where it came from.
Likewise, it was probably the best interviewing experience I've had.
Yeah. Could have been a bit harsher on those damn emos...
Look at https://github.com/Frege/frege
In these diagrams, term formers are the nodes, types are the lines connective nodes. This is just a notational play on syntax trees, ultimately, and the typing rules are the same ones you'd use for the normal proof terms, just written with the proof net notation.
Some textbooks have exercises that are presented on different gradiations, where people are supposed to pick and choose and some are expected to be rather tricky. Others have exercises where you're supposed to do them all before proceeding. It seems pretty reasonable to me that someone could have gotten the wrong sort of signaling from RWH, and gotten stuck on something that was supposed to be optional, or gotten stuck trying to figure out something unrelated when they should have just looked it up. It's small things like that, in terms of signaling, that are tricky for authors to anticipate but can make a big difference in terms of how well a book succeeds with a reader. I've seen plenty of people bounce of similar things in other books....
I think every proficient Haskeller knows exactly the baggage Haskell comes with â€” often with a side of academia. We also want the big ideas to spread and believe Haskell has those (in spades!). I'm sure we want to be a community that not only welcomes everyone, but makes everyone feel welcome. Flattening the big ideas is not a lesser honor than scaling them or building them in the first place. Let's treat great exercises/examples/explanations (the threeâ€¦ E's?) of big ideas as an intellectual challenge and before we know it you'll see C code with Kan extensions :)
Not a full answer, since I can't conceptualize exactly the problem you're trying to solve, but I thought this talk from /u/kwef at Compose was a very nice overview of the travails of using type level nats currently, as well as some future work that may simplify things: https://www.youtube.com/watch?v=u_OsUlwkmBQ
&gt; we can revisit the notion of type error slicing to say that what we're interested in doing is tracking the provenance of the typing judgments, so we can try to shrink the problem and see if we get the same error By this do you mean that we want to take the blamed constraints from naive tracking and then try to shrink them? https://www.andrew.cmu.edu/user/avigad/Papers/constr.pdf has choice constraints where you can assign metas a set of possible solutions, and unification finds one that makes the program typecheck. I really like this as a way of implementing tactics. The obvious ways of implementing this are either tracking what we need to undo if we fail due to each choice constraint, or just trying every combination. Do you have ideas for nicer ways of implementing this? It would also be nice to remove the definition ordering in the presence of macros requirement, and give macros backtracking like tactics. I think this is fairly straightforward if you already have backtracking. 
Sorry for being a bit vague. What I'm trying to do is something like (simpler example that hopefully illustrates the point; I'm not actually using `Data.Reflection` as it seems unnecessary and unhelpful for this): data Nat = Z | S Nat -- using DataKinds data Zp :: Nat -&gt; * where Zp :: Int -&gt; Zp n negateZp :: forall (n::Nat). Zp n -&gt; Zp n negateZp = (reflect (Proxy :: Proxy n) -) or, equivalently, data Sing :: Nat -&gt; Nat where {SZ :: Sing 'Z; SS :: Sing n -&gt; Sing ('S n)} convert :: Proxy n -&gt; Sing n convert (Proxy :: Proxy 'Z) == SZ convert (Proxy :: Proxy ('S n)) == SS $ convert (Proxy :: Proxy n) but without doing things inductively with classes, i.e. class Reflect n where reflect :: Proxy n -&gt; Int instance Reflect 'Z instance (Reflect n)=&gt; Reflect ('S n)) since GHC seems to be unable to deduce from this that `forall (n::Nat). Reflect n` or at least failed when I tried it.
Haskell has a language standard. If you implement it, you've written a Haskell.
Here. As you can see, I used a recursive, class-based approach for the problem of casting fixed-length vectors to permutations. The big issue here is the need for `casts :: Int -&gt; Less n`, which is to be defined in terms of this reflection: instance forall (n::Nat). (Casts (Vector n Int) (Permut n))=&gt; Casts (Vector (S n) Int) (Permut (S n)) where casts c = let cc = casts c :: [Int] in let p = reflect (Proxy :: Proxy n) in let cnc = filter (/=p) cc in Rperm (casts $ length $ takeWhile (/=p) cc) $ casts (casts cnc :: (Vector n Int)) Anyways, thanks for the help. I think I'll try playing with the class-based approach a bit more and see if I'm missing something. I know I can rework the `Less` so that `NilLess` expresses that `0&lt;= a` instead of `a&lt;=a`, which will solve some issues, but I don't know if it solves all of them.
Thanks a lot for sharing. When I was younger, I was looking for systematic resources on programming topics, but 99% of findings were tutorials not very well written on "how to" recipes. When someone shares resources that answer a lot of "why" questions, it's a great pleasure.
It helps if you use an explicit `State` newtype, because then the definition becomes: fmap f x = State (\s -&gt; second f (runState x s))
and what exactly does 'second' do, apply the function to the 'a' in State s a?
It all comes back to curried functions. If you understand currying, what follows may not be useful to you, and please just skim; otherwise, read on. At its core, Haskell only has functions of one parameter. To model functions of two parameters, Haskell defines * `f x y` as an expression to mean `(f x) y` * `f x y = blah` as a definition to mean `f x = \y -&gt; blah` In other words, what *looks* like passing two arguments to a function is actually just: 1. Passing the first argument, and getting back a *function* as the result; then 2. Passing the second argument to that *new* function to get the final result. Keeping in mind the desugaring above, note that: parList strat [] = return [] can be seen as defining a function that takes *one* parameter called `strat`, and returning a new function! That new function has the type `[a] -&gt; Eval [a]`, which is the same as `Strategy [a]`. (The full desugaring isn't straight-forward, because you need to convert the pattern matching to a case first... I'm trying to gloss over that detail.) It's good to get familiar with this transformation... any time you see something written as a function with multiple parameters, there's a decent chance that it's also interesting when seen as a function of fewer parameters, which returns a function. So, for example: * If `f :: a -&gt; Bool`, then `filter f` is a function of type `[a] -&gt; [a]`. * If `f :: a -&gt; b`, then `map f` is a function of type `[a] -&gt; [b]`. * if `f :: a -&gt; [b]`, then `concatMap f` is a function of type `[a] -&gt; [b]`
If it was a physicist problem, it wouldn't be in the book. 
Yes, exactly. The type of `second` is `Arrow a =&gt; a b c -&gt; a (d, b) (d, c)`, which you can make a bit more friendly by choosing `(-&gt;)` for `a`. The type is then `second :: (b -&gt; c) -&gt; (d, b) -&gt; (d, c)`.
Really nice news!
I did not. Thanks! 
Unfortunately it is not that obvious to me, so I'd need more than that to understand what you are trying to convey... if you feel like you know a solution please consider publishing it under a blog post or paper eventually! Not forgetting to add a few examples (such as writing dependent vectors on those things), for us mortals to follow through.
What do you call name capture on that system? There are some graphs that don't even encode Î»-terms (e.g., you can't get to `Con(Dup(x,x),Con(y,y))` from Î»-calculus, but it is a very useful graph), so the whole concept of variables get blurry to me. Although it is clear to me how those graphs capture beta reduction, I admit I'm very confused with what types would even mean here.
:thumbsup:
&gt; By this do you mean that we want to take the blamed constraints from naive tracking and then try to shrink them? Yep. Basically just use a quickcheck style shrink, which can be done rather sloppily by trying to remove each one in turn to bound the total time spent as O(n) worse than the original problem. With something like the revision control monad, the goal is to make it so the re-typechecking can be done efficiently. We don't need to the minimum context, just a minimal one. The key for me here is to ensure that the way we generate new names is deterministic enough that we can identify if we get the same error even in a slightly stripped down context. This means the initial typechecking pass needs to set up some data we'll use in later passes wherein we're trying to run things in stripped down form. 
Why do you think so?
:) Thanks!
When the goal of the book is to teach you how to solve a programming problem rather than a geometry problem, I think that her complaint about missing information is perfectly reasonable. It would have taken the author of the book very little effort to phrase the problem like this: &gt; Write a function that finds the foo of X and Y (hint: you can determine foo by ...) Furthermore, the offline world still exists for many people (not everyone has it 100% of the time even when they *do* have access to it). Being forced to have a connection to the internet in order to follow along in a book is rather inconvenient. A couple years ago, I spent 2 weeks with one of my inlaws who lived out in the country with no internet. The closest place wifi hotspot would have been about 10 minutes away. Would I have been considered lazy for not wanting to travel so that I could get information that *should have been in the book*?
You mentioned in the video that using Homebrew "doesn't work always." I've never tried using Homebrew myself, but this is the first negative experience report I've heard for it. Is there a bug report filed on this, or more information available elsewhere?
Are you saying that it isn't obvious that the correct solution brings no additional Haskell knowledge? I agree that they could have put the formula straight into the book, but I simply can't understand why someone would waste a month on something OBVIOUSLY unrelated to Haskell and then get frustrated with anyone but themselves.
how experienced are you with screenflow ? I did a few videos on nixos and felt I underused it. which is cool, one does not need to make it a hassle
Haha we're ok with it. Why do you ask?
because it seems well done. although i noticed some quirks. it's great by the way, to be able to convey meaning through videos like that. demystifies the whole thing..
Aw that's lovely. Thanks. You're probably noticing the final cut pro editing more than our skill with screenflow. The major problem with most videos we see is that they often skimp on the editing. Editing takes a fair amount of effort, but the quality is so much better, even though we're using a very lightweight production style, Final Cut Pro X makes this process quite enjoyable.
quirks : resolution. I use 720, and the command line is still blurry, that's odd. you use only 20% of the screen area for code. very minor, the sound is a bit resonating, (filters for fixing ?). just saying this to improve, that's great..
I am interested in the same thing and my best guess right now is [linear sequent calculus](http://assert-false.net/arnaud/papers/A%20dissection%20of%20L.pdf).
Let me rephrase. I know it's been used as a practical tool, but viewing it as a research project is a better way to recognize its merits and excuse all its faults. Take the "cabal break" command that used to be named "cabal upgrade" as one example. More generally, the Stack workflow is one that has been well-tested by tons of ecosystems. And it has some problems that people live with. But it's a good engineering approach, that works well in most cases. The cabal-install is best regarded as an ongoing research project into solving those problems, which lacks yet a fully successful evaluation. Those problems are actually hard, so it's fine that it takes a while to solve them, and cabal-install offered for years an incomplete solution. The problem has been with insisting (up to now) that people *use* an incomplete solution. Finally, please point out one issue that article points out that is now obsolete. I've been using it for a while, after I've wasted hours of my life after cabal-install and learning some of those workarounds on my own.
Ah, now this is a question I understand :) So your two instances have proved `Reflect Z` and `forall n. Reflect n =&gt; Reflect (S n)`, and you know that by induction this means there is a proof of `Reflect n` for every `Nat`. However, in Haskell, this does *not* mean that `forall n. Reflect n`. That's because an instance is not an "irrelevant" proof, that is, it's not sufficient to know *that* there exists an instance for `n`, you must also know precisely which instance this is, because you need a concrete implementation in order to call the corresponding methods. Here's an example in which having `forall n. Reflect n` yields unrealistic power. data SomeNat where SomeNat :: Proxy (n :: Nat) -&gt; SomeNat whichNat :: SomeNat -&gt; Int whichNat x = case x of SomeNat proxy -&gt; reflect proxy -- doesn't actually compile As you may know, `Proxy` takes zero bytes at runtime, so `whichNat` has no way to know which `n` this `proxy` is for, and yet it manages to recover the underlying `Int`! This clearly can't work, or we'd be able to compress any document down to zero bytes. That's why it's a good thing that the above call to `reflect` doesn't compile even though we know that whatever that `n` was, there clearly exists a `Reflect n` for it. I've wrapped the `Proxy` in a `SomeType` existential to make it clear that in addition to the lack of runtime information, we don't have any compile-time information about `n` either, but that's not an important distinction. Consider this simpler example: fromProxy :: Proxy (n :: Nat) -&gt; Int fromProxy proxy = reflect proxy -- doesn't actually compile You might think that this time the call to `reflect` is justified because this time we'll know at compile time which `n` to call `reflect` on, because you might think that it's always going to be called at a concrete `n`, like this: three : Int three = fromProxy (Proxy :: Proxy (S (S (S Z)))) in which case it's clear at compile time that `three` is supposed to be 3. But that's not how type-checking works. Each top-level declaration is type-checked separately, so if `fromProxy` was allowed to type-check, then I'd have a function of type `Proxy (n :: Nat) -&gt; Int` I could call in `whichNat` instead of calling `reflect` directly. So that doesn't work either. In order to compile, `fromProxy` and `reflect` need the constraint `Reflect n`, to make sure it's called with a concrete `n` like in `three` and not with an unknown `n` like in `whichNat`. You cannot have `forall n. Reflect n` or you'd circumvent that constraint. You can, however, have `Sing n -&gt; (Reflect n =&gt; a) -&gt; a`: withReflect :: Sing n -&gt; (Reflect n =&gt; a) -&gt; a withReflect SZ x = x withReflect (SS n) x = withReflect n x With functions like these, you can carry `Sing n` values around until you need a `Reflect n` constraint, at which point you can use `withReflect` to discharge it. This way you'll take care of the constraints as they occur, instead of accumulating them and ending up with dozens of constraints which would be automatically satisfied if `forall a. Reflect a` was true. For more information about the way in which writing proofs in Haskell is different and sometimes more complicated than in a true dependently-typed language like Idris or Agda, I recommend the paper [Hasochism](https://personal.cis.strath.ac.uk/conor.mcbride/pub/hasochism.pdf).
Beyond things like "this book/movie/whatever contains graphic scenes...." (which we already do) trigger warnings are absolutely a bad idea in that they're not practical. The set of things that could "trigger" someone you don't know is potentially infinite. In this example, would the author have *known* to put a trigger warning that you might not like his jokes? If he thought that, why include them in the first place?
&gt; In other words: yes, we want to keep our dominant position, even though currently Stack works better for users. that sums it up perfectly and why control of Hackage and haskell.org needs to be taken away from those that abuse it and who don't want Stack to replace cabal-install &gt; Assuming Backpack'16 is implementable (unlike Backpack'14) â€” I've not yet studied that paper. Who needs *that* over-engineered research project? Backpack makes no sense for Stack at all. This is yet another of those academic experiments done by Cabal devs to sabotage Stack. ;-(
No complaints about the content of the video, I agree completely with not going into a deep explanation in a short tutorial video. I just wanted to know if there was some problem on the Stack or Homebrew side that needed to be resolved. Thanks for putting the video together. My only other thought is: would it be worthwhile to include a short one-line installer script on the Stack downloads page? We already have such a thing for Travis, i.e.: curl --insecure -L https://www.stackage.org/stack/osx-x86_64 | tar xz --strip-components=1 --include '*/stack' -C ~/.local/bin If we target that at `/usr/local/bin` instead and throw in a `sudo`, it may Just Work.
 GHC (since 7.8) supports "native" type-level naturals. This is how I would use them for prime fields (probably the simplest "real" example): {-# LANGUAGE DataKinds, PolyKinds, KindSignatures, ExistentialQuantification, Rank2Types #-} import Data.Proxy import GHC.TypeLits newtype Zp (p :: Nat) = Zp Integer deriving (Eq,Show) pretty :: KnownNat p =&gt; Zp p -&gt; String pretty zp@(Zp a) = show a ++ " (mod " ++ show (order zp) ++ ")" proxyOf1 :: f (a :: k) -&gt; Proxy a proxyOf1 _ = Proxy order :: KnownNat p =&gt; Zp p -&gt; Integer order = natVal . proxyOf1 modp :: KnownNat p =&gt; Integer -&gt; Zp p modp k = zp where zp = Zp $ mod k (order zp) instance KnownNat p =&gt; Num (Zp p) where (Zp a) + (Zp b) = modp (a+b) (Zp a) - (Zp b) = modp (a-b) (Zp a) * (Zp b) = modp (a*b) fromInteger = modp signum = const (Zp 1) abs = id Now you can say things like x = modp 100 :: Zp 17 pretty (x*x*x) However, usually in a program you don't want to fix the moduli, but be it depend on some external input. This makes the whole thing a bit convoluted for my taste, but it is possible: data SomeZp = forall p. KnownNat p =&gt; SomeZp (Zp p) asProxyTypeOf1 :: f a -&gt; Proxy a -&gt; f a asProxyTypeOf1 y _ = y mkSomeZp :: Integer -&gt; Integer -&gt; SomeZp mkSomeZp a p = case snat of { SomeNat pxy -&gt; SomeZp (asProxyTypeOf1 (modp a) pxy) } where snat = case someNatVal (fromIntegral p :: Integer) of Just sn -&gt; sn Nothing -&gt; error "mkSomeZp: input is not a natural number" withSomeZp :: SomeZp -&gt; (forall p. KnownNat p =&gt; Zp p -&gt; a) -&gt; a withSomeZp some f = case some of { SomeZp zp -&gt; f zp } With this, you can write: sx = mkSomeZp 100 17 withSomeZp sx $ \x -&gt; pretty (x*x*x) Here 17 is a normal value, so for example it can come from a command-line argument. 
I think it would look more [like this](https://www.phactual.com/wp-content/uploads/2015/01/blackhole5.jpg) then.
Yes, it's a whole new set of commands because basically all of the functionality is rewritten from scratch, so getting perfect BC is going to take some more elbow grease. &gt; Should that be "overstate"? You're right!
How would she know the difference? My background, before learning to program, was in the arts. The last math courses I took were in high school 20 years ago (Algebra and Geometry). I suspect that I would have struggled to solve the problem as well, especially since I would have lacked the knowledge to know what search terms I would need to use to find the answer. Most of us live in a culture that celebrates tenacity. We teach our children songs like the "Itsy Bitsy Spider" and read them stories like the "Little Engine That Could". Honestly, I don't know how anyone can sit there and imply that she was foolish to keep at it when the flaw was in the problem as it was presented. When she gets to a hard problem to solve once she masters Haskell, should she give up as well? When I write software for others, I fully expect them to give me enough domain specific knowledge that I can do the job. It is an incredible waste of time for me to research this information for myself. Why are we expecting anything less from a book?
This looks amazing! I love that the UI is also getting streamlined (no more `cabal install --only-dependencies`, although it seems `cabal configure` is still required for some reason? Also, project configuration files! So many goodies in cabal 1.24. Congrats to everyone who worked on it!
Because this functionality leaps significantly ahead of stack. With stack I still regularly have "stack hell" where I have to delete my ~/.stack directory and build everything from scratch. With `cabal new-build`that should not be necessary.
&gt; Who needs that over-engineered research project? Backpack makes no sense for Stack at all. This is yet another of those academic experiments done by Cabal devs to sabotage Stack. ;-( Nonsense. Don't mix up the cabal team with GHC hackers (including Simon Peyton-Jones). Or has GHC pushed an experimental extension without hiding it behind a flag? (Again, my knowledge stops at Backpack'14, so if you have substantial points to make, I'm listening).
Stack and cabal-install solve different problems. Nix-style local builds address the use-case where you want to use a dependency solver to help you pick the latest and greatest versions of packages on Hackage, without positing the existence of Stackage. Perhaps one could argue that this is never useful, but at the very least it's useful for doing dev on an as-yet unreleased version of GHC which has no Stackage distribution.
Well, arguably that's just a bug in Stack, which could be fixed, and while I've never had to delete my Nix store I imagine there might be a bug which would necessitate this. (There is a problem in that your Nix store will just grow bigger and bigger and we don't have a way of GC'ing it at the moment.)
If you care about reproducibility, I think Stack is still your best bet. But if you want to use a *constraint solver* to get your package set, because, for whatever reason, the Stackage distribution is not good enough, I think new-build will work quite well. Or maybe you want a tool which doesn't go around downloading GHC binaries; that'd also be a reason to use Cabal. cabal-install is committed to supporting dependency resolution with a solver, while at the same time improving reproducibility (which Stack has done really well--we're still behind in this regard.)
Because stack only works with fixed snapshot and is light years behind on this functionality.
&gt; builds are not reproducible This is currently true but a new-freeze mechanism is in the works; we just decided that the benefits of new-build as it stood were big enough to warrant releasing without new-freeze. &gt; Kinda like getting some of the benefits of stack without the benefits of stackage (reproducibility, always a build plan)? That's the idea, because Stackage is a two way street: yes you always have a build plan, but of course all the packages (and the versions you want) need to actually be on Stackage.
&gt; Stack and cabal-install solve different problems. Maybe I'm missing something, but to me it seems like they're solving the same problem, but only slightly differently. As far as I can tell, this new cabal is basically just stack minus Stackage. Whether or not that's a good thing depends on your use-case, but regardless, it seems to be solving the same issue: A hell-less build system where it's very hard to break version compatibilities.
/u/StackLover is a troll. Do not feed it.
&gt; A hell-less build system I'd agree... &gt; where it's very hard to break version compatibilities. Well, if someone puts bad version bounds, then there's not much cabal-install can do (but a Hackage trustee could fix it.)
Exactly. I want both depending on context. I want whatever incompatibilities to be spread among many users, which gives me reasonable insurance it's actually fixed, meaning I will spend 0sec on secondary issues when I try something. The sign off has great value. And liberty to move on to new versions should I need to. 
When we were doing dev on new-build I would intermittently see a problem like this. The bug turned out to be insufficiently clever recompilation avoidance. But what I find surprising is that you had to blow away `~/.stack`, as opposed to just the local build directory.
Having stackage is really good as it provides a reference point for library authors to aim at
Yes, this is a little irritating (we've had some discussion about it.) The problem is that if you are in a directory with a Cabal file, new-build's behavior is to build just that Cabal project. So maybe if there is no Cabal file we should just build everything but I've gotten used to just specifying the targets. If you are quite bothered by this please file a ticket!
&gt; you no longer need to compile packages with profiling ahead of time; just request profiling and new-build will rebuild all its dependencies with profiling automatically. Does this mean profiled and non-profiled versions of a package can coexist in a sandbox?
I've gotten used to it from using stack, which always builds everything, even in a subdirectory. I can see how cabal can't deviate from that, and so it might be confusing to only build everything if you're somewhere else. Perhaps there could be a special target/flag to build everything? `cabal new-build --all`? It saves a lot of typing and remembering all package names in larger projects (I'm now working on a project with 11 packages), and in very large projects it's completely unfeasible (at work we have over a hundred).
True, which is why hackage should require upper and lower bounds for all packages. With this release cabal-install now has a gen-bounds command, so specifying bounds should be much less onerous for users.
In the case of Frege, it appears that it is [different from Haskell](https://github.com/Frege/frege/wiki/Differences-between-Frege-and-Haskell). (The reason I note this is because I once used Fay which advertised itself as a Haskell for javascript, but it didn't even have type classes!) 
I had lots of problem since I changed computer. I had GHC 7.8.3 installed on my old computer, so all of my stack files where made to work with GHC 7.8.3. I used a few packages which were not on stackage so have to use the `resolver: ghc-7.8.3` . When changing computer I had to install 7.8.4 instead because `stack setup` doesn't provide GHC 7.8.3 anymore (which makes sense) but my stack files are not valid. It took me ages to fix them. Having said that, I also tried fixing the problem with cabal instead but endup using stack ;-).
Just enable profiling and Cabal will automatically rebuild all deps with profiling. Let us know if this doesn't work; it should!
such pre chosen packages can be computed somewhere, genius. They are the same goal, with different kind of guarantee but to say that manual checking and signoff is *oppposite* to automatic checking is as much troll as your little stack lover friend
Do you see any possibility to address manually specifying version ranges? The problem I always have with cabal's dependency solving approach is both reproduceability and that my version ranges are mostly a lie. I don't know which versions work with my code. Even if I knew at some point, every time I modify it I'd need to check that I don't accidentally broke compatibility with something. And I can't predict the future either. My code would always bitrot and not build anymore 3 months later. I could cabal.freeze every time I add a dependency, which is what Rust's Cargo build tool does. The equivalent of updating to a new Stackage snapshot would be to delete the freeze file and run the solver again. I guess to really solve this it would require a change to Haskell/GHC and the way module interfaces are specified / versioned etc.?
Congratulation ! I hope at some point the cabal/stack war will stop and those tools will merge. If the are features there is no need to divide the community and the efforts. The main reason I'm using stack at the moment is to be able to use different version of GHC.
why not having added a flag `--new` instead ? The command names makes all existing tools (editors, makefile) incompatible with the new build style. 
For example, stack recompiles all extra packages when you enable/disable profiling. cabal nix-style should keep all versions of the same of package with different compilation options in cache.
Yes, there is so much great stuff in this release. A few other minor highlights in addition to the stuff already mentioned in the comments: * New 'cabal install' option: '--offline' (#2578). * Remote repos may now be configured to use https URLs. * Support for dependencies of custom Setup.hs scripts 
I've been using this a lot for working on `binary-serialise-cbor` to give it a test run. One of my favorite 'hidden' features of this new interface is that if you run `cabal new-build foo`, then switch your `$PATH` to test a new GHC, and run it again, it always does the right thing and just rebuilds. Before with a sandbox you had to get the path correct or cabal would get angry. So now I just have multiple terminals, each with a different ghc sourced into `$PATH` and I can test them all. This has proven very useful in making sure CBOR is going to work for 3 compilers upon release. The compiler version can also be fixed in `cabal.project` for larger applications to ensure you don't "accidentally" screw things up with the wrong compiler or whatever (that field is probably less useful for libraries unless you're specifically tied to the GHC version). This currently implies a full rebuild if you `new-build` with a new compiler however (i.e. if you `new-build` with 8.0, then 7.10, then 8.0 again, the final 8.0 build is a full rebuild and does not skip anything yet), so maybe I'll look into fixing that. Also this works no matter what even if you `rm -rf dist*` and then do a rebuild, it just deletes the working copy, making it closer to stack. Muuuch nicer for doing clean rebuilds. The no-op build is much, much faster, essentially instantaneous. Overall it's quite nice to use and does polish off a lot of rough edges in Cabal's UI once and for all. I'm also much happier with the "directional" command interface where you always just say What You Want and it just does that, rather than specifying with a combination of `--enable-{tests,benchmarks}` and `-f` flags to select things or do selective building.
&gt; builds are not reproducible Builds are not always reproducible with stack either. I had the problem using ghc-7.8.3 as a resolver and not being able to build it on a newer machine because stack setup only installs ghc-7.8.4.
&gt; That's the idea, because Stackage is a two way street: yes you always have a build plan, but of course all the packages (and the versions you want) need to actually be on Stackage. Not really, since you can always add external deps as 'extra-deps', or even build a custom snapshot. Similarly I think you can use this to upgrade single packages beyond the snapshot you're using, but I'm not sure about that.
If I understand correctly, you have to specify the specific version of each external dep in `extra-deps`. But maybe, empirically, the number of extra deps people add is never large enough so that a dependency solver would be useful.
you are part of the whatever community
I thought the book would remain unfinished, great to see that somebody has taken up the torch after Paul Hudak's death.
Yes, everything needs to have a fixed version. In many ways this is good, since it gives you reproducible builds. But many people already do this with plain cabal (freeze files). I rarely need or want a dependency solver. Only when getting a package ready for a new version of GHC, or perhaps when doing an upgrade of a package very low in the dependency tree (although there I often need --allow-newer).
I didn't use an lts resolver because I created the stack file from an existing cabal file and followed the stack instruction which told me at some point to use `stack init --solver`. The resulting stack file had every packages as extra-deps with their exact version. Changing the resolver to ghc-7.8.4 involved using the solver which then was (sort of buggy).
Hm. Yea starting with an lts and using solver to get the extra deps automatically would have been better. Dunno if that would have worked at the time that you did what you did.
I can't answer the question about the solver, but I can confirm that reuse is a big issue. Almost a year ago now, I switched to using a build script that behaved like the new cabal commands, except it used the actual nix store as a cache. I keep meaning to write up a post mortem, but the short version is that while it works brilliantly in a technical sense, hackage version churn has an enormous negative impact on reuse. I've since switched to using Stackage, as the release schedule tempers the churn, and the community aspect means you're not accidentally pinning your local hackage snapshot to one with a serious bug in some core library. Having tried both approaches while managing development on a dozen or so packages, Stackage is a pretty stark improvement even though I went into my experiment somewhat biased against it.
I don't think the existence of separate tools that explore different points of the design space is necessarily a bad thing. In Java you have Ant, Mave, Gradle...
One place where dependency resolution is quite useful is for toggling feature flags and components in packages, and this can help you avoid building more dependencies than you actually care about for a package. But yes, I am sure there are many people who don't want or need a dependency solver.
Yes, the point of using a dep solver is that you can get as new packages as possible, which basically immediately implies you will churn as much as package authors are uploading to Hackage. I mean, reducing churn is the whole point of Stackage, and it's just not the problem Nix local builds is trying to solve. You could very well use Stackage in place of cabal-install's dependency solver, and get the same churn guarantees (maybe someone can write up some detailed instructions how to use Stackage with Cabal).
Wouldn't it make more sense to compile to Java source code since the Jack toolchain for Android now skips class files?
Maybe you can ask this as a separate thread in /r/haskell ? I'd love to see some broad discussion on this topic ..
Using a resolver is the entire point of using `stack`
Thank goodness I'm not the only one whose bounds are a lie, as a newbie this was a big source of anxiety for me. I felt there was something (about how to choose bounds) that everybody else knew and I was missing, but couldn't work out. I don't know about other newbs, but if I can't find the answer to something I feel like everyone else knows and thinks it's obvious, conclusion: maybe I'm to stupid to learn Haskell. I'm vaguely hoping every library is using Semantic Versioning, that would at least make the bounds somewhat predictable.
Would it be possible to use different command names instead of just appending `new-` to everything? For instance, how about the new interface to Cabal be accessed via a completely different root command. So instead of `cabal new-build`, we could say something like `cbl build`. You'd still get backwards compatibility, you'd still have all the original sub-command names, and it would all be more terse to type out.
Right, that's why I reported on trying it both ways. ETA: To clarify, since probably not everybody follows my adventures in Haskell tooling, I still use Nix but find it more usable in conjunction with Stackage rather than solving against arbitrary hackage states.
Okay. Are there any plans to attempt to do solving that _is_ dependent on what's already built?
Well, I'm pretty sure the solver does have this capability (because that's what `cabal install` does today). So while we think of solver determinism as a feature, maybe this could be added as a flag.
In my case it invalidates some (like the `base` which is different for each ghc version). Enough to not be able to build the project.
I don't think they present much technical difference, but compiling to Java bytecode might be slightly simpler (from STG) because it will be unnecessary to deal with the ceremony involving handling java's surface syntax. The fact that one toolchain skips class files doesn't change the large number of tools that are oriented around working with JVM bytecode.
And presumably the profiled libraries will get a different hash than the non-profiled libraries, and thus coexist peacefully? (This is one place where stack fails to avoid unnecessary recompilation in my experience.)
I have routinely had `extra-deps` sections 20 packages large; I usually end up using `stack solver` to get the initial set, and then stick it in `stack.yaml`.
Yes, every library uses a variant of semantic versioning. See the wiki page on the [PVP](https://wiki.haskell.org/Package_versioning_policy). There are occasional version numbering bugs -- as there are bugs with just about everything humans do -- but they are generally rare.
I don't think it is as much of a straw man as you suggest. If you ask for a Stackage LTS based on a GHC version which is not the system-wide GHC, of course Stack must download GHC. In my experience, it's very easy to ask for the "wrong" LTS.
Here's an idea. This would at least allow you to embed a probability monad. Use the type `type Prob a = (a -&gt; Double) -&gt; Double` that calculates an expected value! It is a continuation monad. For example, `coin :: Prob Bool` would be `coin f = 0.5 * f True + 0.5 * f False` (expected value of coin toss with sides labelled by `f`). `uniform :: Prob Double` would find the expected value of a uniform distribution after mapping a function. You can find the variance of a `Prob Double` by passing in `(^2)`. Note that this will probably need to rely on approximations (since it is noncomputable otherwise), but it *ideally* represents a perfect thing.
A way to restrict it a bit is to todo `forall r. `[`AffineSpace`](http://hackage.haskell.org/package/vector-space-0.8.7/docs/Data-AffineSpace.html)` r. Diff r ~ Double =&gt; (a -&gt; r) -&gt; r`. This will mean that the result has to be a linear combination that adds up to 1.
I would typically structure this as having a Monad/Applicative that models sampling from a distribution. Then you can do things like MCMC sampling, Hamiltonian MC, rather than naively executing the sampler. The Applicative models independence, allowing you to "move" independently in each side in the MCMC sampler, this gives a powerful form of Gibbs sampling that never costs you anything, cutting a turning an exponential slowdown to a multiplicative one. This is mentioned at the very end of the ApplicativeDo paper as one of the examples, but for space reasons we weren't able to really do it justice.
This works for easy stuff, you can even use my `integration` package to do a lot of non-trivial work. The `hakaru` folks do this. The downside is that what you really want is good way to build continuous functionals, so that as you 'narrow' a the resulting real narrows in, so that you can build this up out of successive approximations. `(a -&gt; Double) -&gt; Double` is an overly simplistic calculator for continuous domains in general. `integration` can handle ~ one dimension of continuous sampling, but error starts to pick up quickly when you abuse 1d quadrature schemes on higher dimensions.
If all the packages I use were uploaded by you, I wouldn't need Stackage. Stackage is a proxy for that: it's a bare minimum cross-package test that things build together and test suites pass. My point about not being the only Haskeller stuck with a buggy library isn't about package popularity, but specific versions being found to be faulty. This is another weak signal, but the fact is I'm more likely to hear about a bug or security issue if it is present in a Stackage LTS than if it is not.
You can add extra dependencies to stack, it doesn't only work on a fixed snapshot.
So you want to compile density functions may be ? http://research.microsoft.com/apps/pubs/?id=189021
As already discussed in this thread, Stack has full support for the dependency solver mode of operation. Can we stop having discussions based on false information?
Thanks. I'm looking at it.
Good idea, it seems it didn't succeed much here :)
My guess is that requiring upper bounds will be even more harmful than that: people will start putting made-up upper bounds on their packages (like most people do with base today), and we'll have even _less_ information about the bounds a package really has.
I think a big part of the problem is in saying "monads are like X" rather than "X can be thought of as a monad". An example contains the seeds of generality/abstraction, but rarely gets you the whole way there. I don't think fiction and boxes are wishy-washy or hard to logically reason about. It's very helpful to think about transformations of concrete objects when learning group theory. My biggest issue with the whole "box" approach is that what's really box-like is a trivial, unwrappable data constructor, which has obvious Identity Functor and Identity Monad instances.
I tend to prefer just building a generative model then deriving the rest from that stochastically. EV can be calculated by average sample value, you can approximate the quantiles by taking order statistics on a bunch of samples, etc. While hacking around in the applicative you can try to keep more of the information about the samplers around opportunistically, exhaustively solve some discrete cases, etc. The HMC machinery just lets me make use of that sampling engine more efficiently when things get messy. 
I recently started a project for a simple monadic game engine, and I wanted it to be structured "correctly", so these are the resources I found. (1) At a high level, [the wiki article](https://wiki.haskell.org/Structure_of_a_Haskell_project) suggested by /u/markedtrees is a good start. It wasn't quite detailed enough for me. (2) [The Cabal User Guide](https://www.haskell.org/cabal/users-guide/developing-packages.html) goes more in-depth in certain places. You'll have to pick through for what you need. (3) The [Pandoc](https://github.com/jgm/pandoc) GitHub repo seems to follow the guidelines well and was easy to pick through to get a feel for the structure. I figured out a lot from this. &gt; Like, do you put all your types in one place, or in the places they are used? I ended up creating a subdirectory `Objects` which contains my various data structures, as well as some simple functions for interacting with them. This gets imported at a very low level and is exported from where I import it (so you rarely need to import it explicitly). &gt; A lot of people have one module importing all of their internal modules and then that file is what you import as a user of the library, etc. This is what I ended up doing. It's so much nicer to do. Just `import MyModule` and you've got everything you're going to want. Love it.
My first approach to structuring an application is usually to split it into a library that contains the domain logic, and one or more executables that implement a UI (whether that be a web server, a CLI, an API, or a desktop GUI). Some of the default Stack templates do exactly this. Within these parts, it depends what exactly the part does, and where the logical boundaries are, the most important concern being that encapsulation happens at the module level, so all the code that is somehow "internal" to a subset of the functionality goes into a separate module. Another technical concern comes with Template Haskell, where you can only import functions called in TH, not define locally, which means that whatever I want to call from TH needs to go into a separate module. And yet another concern is avoiding cyclic imports. Often, I'll start out putting everything into one module, and then factor out parts as the code grows. More often than not, I end up with a top-level structure that's more or less MVC, or input - logic - output, or parse - transform - generate; I find that Haskell lends itself very well to this approach. For a library, or a larger subset of the application's modules, I often write an "entry point" module that contains nothing but imports and exports, similar to what many libraries do (e.g. `Data.Time` re-exports most of the `Data.Time.*` modules).
I actually expect proper bounds to be even more important with `new-build` than they were before, as now the local package db's content does not affect solving anymore (and we may also allow to hide/unbias part of the global package db in future). So the cabal solving is less constrained and tends to pick bleeding edge versions more easily (rather than being biased towards already registered install-plans). In the past the major argument against overly-restrictive version bounds was cabal's tendency to run into reinstall-situations (unless using sandboxes) which would break the user package db. But now we finally have the proper infrastructure in cabal to have several install-plans which are not a proper subset of global-beat mono-builds *without* cabal throwing up and break your package-db. And if desired, the user can also overlay a Stackage snapshot constraint-set (and yes, there's more convenient UI planned for that).
It wasn't ready for publication when you asked, but now I can finally point you to https://www.reddit.com/r/haskell/comments/4hiwi0/announcing_cabal_newbuild_nixstyle_local_builds/ 
To be fair, a start up isn't there for teaching, but for making money. If you're looking for a paid job that's capable of teaching you, you should look for a larger company. The downside is, most of them won't teach you Haskell, but Java EE or .NET from years ago. :/
I think my preferred approach is something like Stackage, or at least what Rust's Cargo does, just auto-freeze on every new dependency. That way at least builds are reproducible by default and updating is a conscious choice.
FP Complete prefer to hire experienced Haskellers for the obvious benefit of lower risk and quicker turn around, but still hire devs who are experienced in general, especially in a needed domain, and want to up their Haskell expertise.
This sounds interesting and reminds me of the motivation for index-freezing (which among other things allows having an explicit and conscious per-project `cabal update`). Is cargo's auto-freeze operation described somewhere in more detail?
It sounds like it will be much easier to reproduce these failures though (so it should be easier to get them fixed). Currently people with an old versions installed in their package DB might just reply "works ok for me, no idea why yours is failing" (since the solver quietly uses different versions based on what is already installed). edit: s/there/their
That might be technically true, but try to use stack with hackage. I tried and it's a pain. You can't say: "Take all packages from hackage, newest that the solver allows". No, you can generate a `stack.yaml` (with `--solver`) and have fixed dependencies. If there is indeed a way to use stack with hackage, please enlighten me (and fix the documentation to mention it). And please don't say that freezing/soft-freezing is the way to do it. It is the right way for deployments, but not necessarily when developing.
There is a setting for that now. See `compiler-check` and `compiler` in the [stack yaml documentation](https://github.com/commercialhaskell/stack/blob/master/doc/yaml_configuration.md)
The workflow is different. Cabal implicitly runs the solver every time you run the install command. Stack does it with an explicit step. You may prefer cabal's implicit behavior here. I could write (and have written) at length on why it's bad behavior. I'm not sure what you're trying to accomplish where having your dependencies change out from under your feet is a good thing. Do you want to describe a scenario where implicit changing dependency versions is a good thing?
I see that you're taking a regular spreadsheet with the default language (let's call it: Excel), keeping Excel but adding Python, R, SQL, etc. I can see the business case for this: spreadsheets always evolve into needing some Visual Basic extension, or for more sophisticated users, dropping to C++ or the like; so why not provide that power out of the box? I think a broad audience is going to love that. Especially the SQL querying seems nice. This product is especially interesting for me, because I have a mental prototype of doing [spreadsheets in Haskell](https://docs.google.com/presentation/d/1lh9_QlLKtW4L5WsFkgRggyIFORXAkTUu6Y470VkM4uI/edit#slide=id.p). I think that the "grid" spreadsheet style with only atomic cells, and the coordinate referencing system is fragile and [much research backs this up](https://en.wikipedia.org/wiki/Spreadsheet#Programming_issues), the world of lay-person-accessible-programming needs shaking up a bit. But my alternative is a bit of a departure, so I expect you won't be taking this direction. I have [a teeny tiny demo](http://chrisdone.com/haskell-spreadsheet-prototype/), though, that I whipped up in an hour. (It uses TryHaskell which has a high calling overhead.) One day I'll get time to implement something real, with vector/matrix/map editing and such.
&gt; index-freezing (which among other things allows having an explicit and conscious per-project cabal update) Wouldn't index freezing have to involve checking the whole cabal index into version control so all team members working on a project get the same index? That seems like a pretty bad idea in terms of size requirements.
Maybe, my point is I'm sure build are reproducible in theory. In practice it's a different story. If you have to modify the stack.yaml of a package in order to be able to build it, then that's kind of defeating the object of the stack.yaml, isn't it ? Anyway, I'm not complaining about stack. Some people which have a straight forward workflow, or don't use stack that much, think it's all puppies and rainbows. That's just not true. Stack is great when it works as cabal is. When you are encountering problems, things are less straightforward, even though the stack team is really reactive and helpfull.
Index-freezing as I'm working on requires the new incremental `01-index.tar` which contains all of Hackage's index history. So we can uniquely specify any state of the index simply by a natural number which can be either passed by commandline (for one-off uses) or set permanently via `cabal.project` (checked into Git) or just made sticky by `cabal.project.local` (via e.g. `cabal new-configure`). There's also different addressing/filtering modes planned for different use-cases, but the two basic/primitive strict ones are either a dense serial number counting the events since the start of time, and the other one being the posix-timestamp of the last event to include in the snapshot. It's left up to users if and how they'll make use of this feature. Or if there's demand, we may provide a sticky-by-default mode of operation for cabal projects.
Try deleting the sandbox or using a new one just for this package
I know you have. When developing a library and following pvp, it's nice to easily be able to switch dependencies. See whether you can adjust your bounds. &gt; I'm not sure what you're trying to accomplish where having your dependencies change out from under your feet is a good thing. That's not what's happening and you know it. It only happened when `cabal install --..`ing. I don't know how that's going to be with `new-build`, but I will soon. --- Please don't tell me how I want to develop. Freezing is necessary for deploying. No doubt about it. But I want to develop on the (b)leading edge not on some old snapshot.
I've just created a new sandbox using cabal sandbox init I still get the same error. 
You could try exact real arithemetic: https://wiki.haskell.org/Exact_real_arithmetic Essentially, you are still sampling, but your number is a function that allows you to specify how precise you want you it to be (and therefore how many samples to take).
It's turning into Calling-Things-Hell Hell.
As a current cabal-install user I must agree with /u/snoyberg. The value of bounds is their meaningful semantics. There is information that only the author knows about which dependency versions are mostly likely to work with this package. That information is extremely valuable. Littering cabal files with meaningless bounds only obscures the real information and renders it useless. Dependency constraints in the cabal file should be viewed as semantic information provided by the author about the package. They should *not* be viewed as explicit constraints on the behavior of build tools. Each build tool should (and does) have its own tool-specific mechanism for controlling its behavior.
You're probably referring to the Git-based proposal to have an equally growing forever index? I can imagine that if Hackage was being designed from scratch today it would probably be designed around Git, and would maybe look totally different (c.f. http://melpa.org/). However, based on the existing infrastructure, `00-index.tar` was already growing forever, the only thing that changes is that we now also include `.cabal` revisions and `preferred-version` revisions to the growth. And for providing integrity, we also include `package.json` entries containing sha256 checksums. Finally, `01-index.tar.gz` is appended only, so we only need to download the delta since the last `cabal update` (typically only a couple of KiBs), since the prefix of `01-index.tar.gz` will remain unmodified. So this is just extending an existing concept rather than throwing the existing one away (which we'd still need to support for legacy purposes anyway) and start from scratch and start depending on an external tool/protocol (i.e. Git) with its own accidental complexities.
&gt; I don't know how that's going to be with `new-build`, but I will soon. For `new-build` the solver is only re-run when there's actually a chance that the result will be different, i.e. when the input to the solver has changed. I.e. when you add a build-dependency or when `cabal update`, or when you change parameters affecting the result such as `--constraints` or `-w /opt/ghc/$VER/bin/ghc`. The design principle for `new-build` is to appear less state-full than `cabal install` was, but also be clever about avoiding redundant work by caching results.
Can't you just copy https://www.stackage.org/lts-5.15/cabal.config into your source-tree?
Calling-Things-Hell considered harmful!
Yes, but it's explained in a sentence or two. Every time cargo downloads a new lib it puts the version it picks in a freeze file. It's only changed if you do an explicit update (either everything or just one dependency). Executables are supposed to have their freeze files committed, libraries should be flexible (like Haskell PVP) because otherwise you would be unlikely to find a build plan for using multiple libraries with single version dependencies in a project.
I really hope it doesn't stop. The competition in Haskell tooling has really driven improvements. They are exploring different ways of doing stuff as well. 
Required bounds are not the solution. Bounds are required where they are needed, but it is hard to automate that. Lack of an upper bound on a dependency has a legitimate meaning. It means that given what the author knows about the release pattern of this dependency and how the dependency is used in this package, the author believes that the current package version is likely to build with future versions of the dependency for the foreseeable future, even after major version bumps of the dependency. That is often the case. Forcing authors to specify a wrong upper bound in that case causes unnecessary work. Automation to make that unnecessary work easier is not the right solution. The problem is when authors know that their package might break at the next major version bump of the dependency and don't bother to tell us about it. Automation could help make that happen less often.
Exciting! Can any work that `ghcjs` has done be shared? 
Better still, what incentive can we give to authors to spend more time on documentation? 
This sounds like a fantastic approach, but way beyond a SoC project. I suggest picking one small but important chunk of this grand plan that could be done easily in a single summer. It would be amazing to have a usable JVM output for GHC.
That's what I do. But, I think /u/Tekmo 's point (correct me if I'm wrong) is that `cabal.config` is not part of the package metadata (i.e. not downloaded from hackage), like library versions are. One less thing for the user to forget. You could inline the constraints into your `build-depends`, and then comment most lines out, but that's messy and less clear than `resolver: lts-5.5` (just like naming expressions with variables). 
alright, my life is one project, now having sandbox or no makes no difference to me, still maintaining the packages is painful and often breaks
Something like that. I've never really made this representation work, despite lots of trying. Giving up and just using samplers has worked better for me. I do use a similar (a -&gt; r) -&gt;_L r representation for covectors, requiring the -&gt;_L arrow to be linear, though.
I agree that a bit of competition doesn't however, one things I really liked when I start using Haskell was cabal. We add one tool to do everything (managing dependencies, build, doc etc ..) and that was great because everybody was using the same tool and it was well supported. Now, things like editors, ghcid, needs to support both tools and it's not great. I spend the week-end trying to make syntax checking on Spacemacs works with stack and I've been considering going back to cabal instead. And what if some of those tool decides now to only support cabal or stack ?
I understood you started stack to solve practical problems, but I guess your energy and time would better used somewhere else than working on stack if cabal could offer everything stack offers ? I mean would you consider merging stack into cabal if it was possible ?
The Perl community has something called Kwalitee - http://cpants.cpanauthors.org/kwalitee.html. I'd like to build something similar for Haskell, and there would be "Kwalitee" metrics for documentation. Do all functions have documentation? Do all functions include a code block (assumed to be an example)? Etc. Kwalitee would be directly displayed on Hackage when users are browsing, and it is designed to help them select high-kwalitee packages to build on top of. The incentive is thus to accumulate some easy points, so people are likely to use your package. Likewise, people in the community might want to help raise kwalitee for their favourite packages, so explicitly see that documentation has a low kwalitee score points them in the direction of some low hanging fruit.
You're running into this issue: https://github.com/HaskVan/HaskellKoans/issues/9. It seems `HaskellKoans` and `testloop` are not being maintained, and Stack won't help you here. I suggest using this learning resource instead (Spring 13 version of cis194 course): https://github.com/bitemyapp/learnhaskell.
The biggest pattern I've found useful is to have `.Types` modules and have everything else "flow outward" from there. When you start an app, you may want to start simple with one module called `MyApp.Types`. But as the app grows you will sooner or later want to break that out into `MyApp.Types.Foo`, `MyApp.Types.Bar`, etc. Make sure you limit these `.Types` modules to the bare minimum of nothing more than data types, type class instances, and maybe smart constructor/accessor functionality. When I started out with Haskell I frequently discovered myself running into cyclic dependency problems, and using this pattern has been the best way I've found to avoid that.
Yes, adding support for pre-defined package collections has been the plan for some time.
Thank you very much, looks like I have to abandon Haskell Koans, then. I'm surprised that apparently I installed Haskell incorrectly, while all I did was following the [learn you a haskell](http://learnyouahaskell.com/) tutorial. 
I tried going through the book some time ago, but the lack of cross-platform support (it only worked on Windows) kept me from getting very far. Great to see this update!
Any and all incentives to improve documentation seem like good ideas to me. Slightly related, I would encourage beginners and experts alike to consider making pull requests to libraries they use specifically to improve documentation; a contribution doesn't have to be code, and it's a great way to solidify knowledge!
You want Hakaru: https://github.com/hakaru-dev/hakaru In this particular case, you do not get a closed-form without some intervention: the issue is that change of variables is needed to see the closed form, and Hakaru does not do this by default. The reason is that such changes of variables only sometimes make things better (they can make things much much worse too). But this is exactly what Hakaru was designed to handle.
I don't want to argue with you, but please be reasonable. What's "full support"? Of course it can mean that you are able to emulate a workflow, but if it does not have first-class support in the ui and is awkward to use in that way, I can reasonably disagree with "full support".
Can't upvote enough; I think that sending PRs improving docs is one of the best ways of gaining a deeper understanding of a library. &gt; Any and all incentives to improve documentation seem like good ideas to me. Agreed, as long as said incentives don't involve shaming authors for not having done a better job.
Make Hackage a wiki: make it easy to comment, add examples and fixes.
Not being able to build project A without a huge amount of work when project A is buildable also may qualify as Hell. It certainly qualifies if the only reason is build tool bugs or quirks. It probably also qualifies if the build tool is behaving correctly, but some other author has damaged the state of Hackage by uploading a package with wrong dependency bounds. Neither of those cases occur with modern cabal-install for a skilled user. The bugs and quirks that used to cause that are long gone. And cabal-install provides ample power for a skilled user to quickly locate and maneuver around broken dependency bounds on Hackage. But for inexperienced users, Cabal Hell still exists with this definition.
Exactly ;-)
Haddock already has something that can calculate which parts of the code are documented, in case you would like to look into building this.
Fair enough, but this case is not specific to cabal and even not Haskell. I've been switching to a new computer and so far everything which hasn't been dockerized properly has been a pain ... sorry Hell. I had a problems with - stack (not pulling the required version of ghc) - cabal (not working at all, thus using stack). - ruby version incompatible with my scripts - ruby not being able to find local dependencies. - R , not being able to install reshape2 because it doesn't exists anymore or not compatible with R 3.0. - SQL queries silently failing because I've upgraded (using docker) to MySQL 5.7 instead of MySQL 5.6 - not being able to reinstall a Sails application (sort of Rails for node JS) because the available version to download have changed the API. - Problem migrating from PHP 5.5 to 5.6, etc Ironically, I only managed so far to solve the problem related to Haskell (thanks stack) and have to go back to my old machine for all of the others. None of corresponding the community (R, Ruby, node etc ..) advertise those types of problem as "Hell". I just think it's damaging Haskell image to call everything Hell. The reputation of Haskell is, it's hard to learn, there is no decent tooling (read IDE) and we have Hell . Only attractive for Death Metal teenagers fan ;-). 
I use stack. I didn't know about `--package` :-)
That's not just `servant-client`?
*TL;DR Hypernumbers was a Scottish startup doing similarly interesting stuff a few years back, with an Erlang backend rather than Haskell.* Sounds like a great project. There was a startup in Scotland kick started by Gordon Guthrie ([twitter](https://twitter.com/gordonguthrie)) that developed a vaguely similar project, called Hypernumbers. Rather than being Haskell based, the backend was implemented in Erlang. The idea: be compatible with Excel, whilst overcoming Excel's limitations. Here's a white paper about Hypernumbers, which describes how it addresses the problems of: coordination, proliferation, permission management, version control, verification, and audit and data management. **Beyond The Desktop Spreadsheet**. Proceedings of EuSpRIG 2011 Conference. G Guthrie and S McCrory. http://arxiv.org/pdf/1111.6870.pdf Here is a screencast of Hypernumbers in action: https://vimeo.com/45132937 AFAIU, Hypernumbers is no longer a commercially active project. Instead, it's all been open sourced: - Hypernumbers implementation : https://github.com/hypernumbers/hypernumbers - Hypernumbers manuel: https://github.com/hypernumbers/hypernumbers-manual Disclaimer: I've nothing to do with Hypernumbers. I know Gordon and I've seen him demonstrate Hypernumbers in the past and I was always impressed by two features: 1. A user could have a call centre making real phone calls using this spreadsheet software within a minute or two of creating a new spreadsheet. 2. Every cell in every spreadsheet in the Internet universe had a unique URI, so with the right permissions using their permissions management, computing a cell's value in one spreadsheet could use the HTTP URI of a cell in another.
Got you covered. This is what I wrote the `haskell-process-load-or-reload` function for. I've got it bound to `F5`. It's my general purpose "load" command. * Open the module with your test suite in it. * Hit `F5` to load it in. * Hit `C-u F5` and it says "now running reload". * Now all future invocations of that command in other buffers will run `:reload`. * *hack hack hack* * Once you're done, hit `C-u F5` again to toggle back to `:load` mode. I do this all the time. Writing tests and such. 
Thanks, I was kinda hoping you'd show up! When I load `Tests.hs`, switch to `Lib.hs` and run `(haskell-process-reload)`, does this add the `Lib`-module to my repl? Or only... well, reload it if it's already in scope because `Tests.hs` imported it? Because I want the `:m +*Lib` import and only got the normal import so far. 
Everything in Haskell has a higher standard, even the hell. ;)
The word "hell" isn't unique to the Haskell ecosystem, and isn't specifically tied to cases where the whole system was "borked". In particular, I thought cabal hell was a special case of dependency hell, in which case clearly cabal hell doesn't define the limits of what can be called hell. Even if cabal hell was the first ever metaphorical use of the word "hell", that doesn't mean other metaphorical uses are invalid. Even if cabal hell was the first ever, defining and literal use of the word hell, metaphorical uses based on that would still be valid and, in any case, even literal word meanings change - with old meaning becoming obsolete being an obvious reason. And of course you are not the world authority on which things are hellish enough to be called hell (and if you were, I'd have to remind you that a borked system is a pretty trivial thing compared with, e.g., an eternity of torture). In short, linguistic prescriptivism - down with this sort of thing! 
Ok, but cabal hell with sandbox is nowhere near as painful as cabal without, so why use the same word for two things which doesn't even share the same magnitude when English provides us with so many words ? As the meaning of word is indeed volatile and the only authority is the speakers themselves, maybe we should use "bell" for minor problems and could differentiate between the old genuine cabal hell and the new annoying cabal bell ? 
I saw this talk. Terrible. 1/10. I want my money back.
I knew we shouldn't have invited that Seipp joker. Everybody knows that crypto should only be done in C. ;)
'cabal heck'?
Exactly ;-)
Wait, don't you mean you _do_ think it's a straw man?
To my recollection, this talk had the coolest demo of any of them. EDIT: Except for the GHCJSi demo by Luite. Sorry, this one gets second place.
&gt; When you want to start building tests or benchmarks, you use install. And that has the potential to significantly change your build plan. Just as a point of information, the new build code here will solve for testsuites/benchmarks enabled by default wherever possible, but only build them and their extra deps when you ask to build them. So you won't get a different solution (and don't need to re-run the solver) when building tests/benchmarks. A (hopefully unusual) exception is when we cannot solve for the extra deps of the testsuite/benchmarks in the first place, and you then can try `--enable-testsuites` to force the solver to enable them, even if that causes different versions of other things to be picked. This is of course explicit and can be either passed on the command line or stashed persistently in the `cabal.project` (or `cabal.project.local` via the `configure` command). Anyway, the point is there's no confusing flip-flopping of deps just because you want to build tests or benchmarks.
Please do.
Hey Rob, thanks for sharing! Will take a look.
Yep - CEO of AlphaSheets here, just wanted to confirm this. Send your resume, it can't hurt!
I don't believe anyone has ever learned about monads in functional programming from a lesson. It's an understanding that comes from working with them for a while. In my case, I was learning parsec when it clicked. "Monad is an interface that lets my intuition from how IO works carry over to other contexts!" My advice is to not worry about it. Understanding will come. If you want to speed it up, there are two things that help: 1. Write code. Just write code. Look for similarities between the way different pieces of code are constructed. But mostly just write code. 2. Study Haskell's type system, and GHC's extensions to it. Learn about kinds. Look at how types are pattern matched during unification. Learn about simpler higher-kinded abstractions, like Functor. Even learn about details that aren't directly related to monads, like higher-rank types, GADTs, MultiParameter Type Classes, functional dependencies, and type families. The more you understand the way the type system works, the more information you get from the type `m a -&gt; (a -&gt; m b) -&gt; m b` and simpler (and even more obvious) it seems.
We've always said the two primary solutions to dependency hell are nix-style builds and (optional) curated collections. There's no argument against curated collections here. We'll get to both.
It's worth noting that the plan with cabal is indeed to add support for *optional* use of curated package sets (published on hackage, and with some extra flexibility to enable some new use cases). Certainly we'll never force anyone to use curated sets. Being able to work with the bleeding edge (and all the other flexibility) is a feature.
Hmm... I'm seeing code like this data AIResponse = AIResponse { responseId :: String , timestamp :: String , result :: AIResult , status :: Status } deriving (Generic, ToJSON, Show) I'm sure you know this already, but you're going to very quickly run out of unique record field names at this rate. Do you want your library to be compatible with versions of GHC earlier than 8? Then you should probably disambiguate record field names like so data AIResponse = AIResponse { aIResponseId :: String , aIResponseTimestamp :: String , aIResponseResult :: AIResult , aIResponseStatus :: Status } deriving (Generic, ToJSON, Show) The `Aeson` library fully supports deriving JSON instances in this case, although I can't remember at the moment if it works "properly" when you use generics (as opposed to `TemplateHaskell`). If you don't mind your library only working with GHC 8+, then you should look into the nascent extensions dealing with this issue, such as `OverloadedRecordFields`. 
Can you post the slides? I can't see the code blocks.
You can take a look at the output of: stack templates I usually work with the chrisdone template, so I start my projects with stack new myproject chrisdone Add to this the previous remarks in this thread about using a `Type` module and making the logic flow from there.
I think something that would help the quality of documentation is the ability to include non-API docs. You see people doing things like adding "Tutorial" modules to their packages, or making little websites, because there's no other way to achieve this with Hackage/Haddock. Whether this should be solved in Hackage/Haddock is another issue. Haddock does API documentation and, whilst it has its warts, it does a reasonable job. The solution might be a "haskelldocs" site which combines API documentation with other forms of documentation, written in some standard way.
I'm not trying to establish anything, I just think it's unfair to pretend cabal sandboxes didn't solves anything arguing that "Cabal hell is still there". Cabal sandboxes definitively solved some problems and the "Cabal hell" people are still experiencing is different and lighter that the one cabal sandboxes tried (and succeed) to solve. 
I know something that makes no sense : you
README markdown or txt files actually get appended to the end of the package page on hackage these days. See for example: http://hackage.haskell.org/package/lens
Depends a lot what your likings are, what you might be prepared to learn or not and what you call mastering them. You certainly don't need category theory for using them or deducing their properties by reading their type off. How to grow is a very fair and important question. reading and writing can not hurt, but it's good to have it directed somehow
We're in the Bay Area - Atherton, to be precise
What do you think of the idea of approaching monads from category theory? I actually managed to explain what monads were to a friend who doesn't program by explaining the category theory, then explaining how it conceptually applies to programming. It required giving a crash course on category theory, but that's honestly probably worth it if you're learning Haskell. Learning what a monad is from the perspective of category theory might be more fundamental and simple than directly applying it to programming with no theory. But I've never tried to teach monads to a programmer before so I have no idea if this approach would be worthwhile.
Interestingly another paper from that site http://www.leafpetersen.com/leaf/publications/hs2013/hrc-paper.pdf gives more details of the compiler but makes much more modest claims, only saying that it overall achieves parity with plain GHC.
This discussion reminds me of the talk "The Future of Computing: Logic or Biology" by Christian Albrechts. As I see this, following Albrechts, the "real life" analogies come from another analogy: Comparing the computation to a mechanical system. The other option is to think of computations as logic. This could enable explanations like: 1. Here is an *ad hoc* construction(or definition) 2. These are the construction's logical properties 3. And here are some ways to use this constructor to perform these interesting/relevant computations. I think this is similar to the approach some algebra courses take. For example, I don't remember any mechanical analogy for a matrix.
I always wonder how one person can accumulate so much knowledge. Normally, I just think 'ahh, that's just the domain he works in/has a hobby in', but you seem to have expertise in every domain I can think of. Is there any particular black magic involved I don't know of?
Thank you! I am willing to learn as much as there is to learn, I enjoy learning, and Haskell is a very cool language, especially with my love for math. I know Category Theory isn't a must for Haskell, but I would love to learn it, as it seems like a very interesting subject. I am reading every once in a while, but I find that my learning, especially with programming, goes really well when I have some clear path, such as a written tutorial or a playlist. If you have any suggestions I'd love to hear them.
I'd be interested in your explanation, if you won't mind. I have tried to get into category theory by myself, but I didn't find a good start, so maybe you will give me the jump start I need to get into deeper (and more interesting) Category Theory.
Thanks for the advice, I'll be sure to try it! I think I understand the basic idea of monads, and I see how they are being applied in small example code, but if you'd ask me to understand them in some more complex code or try to write something with them I'd struggle. Anyways, I'll follow up on your advice and related topics, by the way, if you won't mind giving me some links to read about the topics you gave in #2, that would be great, thanks.
Check out [Category Theory for Programmers](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/). It goes deeper than you need to in order to understand monads, but it's awesome. To just get to monads (which he hasn't written about yet), all you really need to understand is categories and functors. Then it's trivial to explain that a monad is an endofunctor on C where two certain morphisms exist in C: `x -&gt; m x`, and `m (m x) -&gt; m x`, alongside a couple of laws. The intuition applying this to programming is that a monad represents an embellished computation, where you can nest the computation arbitrarily and always be able to normalize it to a single layer of embellishment. As a functor, this means you can essentially do imperative programming; taking values out, doing an embellished computation, normalizing with monad, and repeat to perform arbitrary, sequenced, embellished computations.
The stack workflow and the platform workflow are different. Stack installs a single executable from which other commands are invoked, while the platform puts `ghc`, `ghci` etc directly in your path. In particular, you can get `ghci` using stack by calling `stack ghci`. (assuming you've called `stack setup` first to get a ghc somewhere in your system that is).
The Blog by bartosz milewski is great for learning category theory. He also publishes chapters of his book "category theory for the working programmer" there.
I'm pulling my library if people can frame it on Hackage with documentation that could break later and that I didn't assent to. Most of my users use Stack anyway, so referencing a git repo is not out of the question. Mission creep for an already drowning project that can't keep current features (doc building) for the past three years is not a good idea.
That sounds quite interesting! I suppose that I should probably contact him directly? There is not much time left to write a proposal... but I think it would be possible to come up with something. On your previous message, you mentioned a proof of completeness and error reporting from an UNSAT core. Aren't these two different projects? The first one would be more a proof-oriented task, isn't it? The second project seems closer to using a library and try to analyse and pretty-print what the SAT solver returned. Or I might just be confused.
They can be hidden while under review.
Yes, you're right. I think my brain for turned around and I claimed that my claim of a straw man wasn't a straw man itself... or something.
Nice, much better than having the set of installed packages affect the plans! I am wondering though, how much does it matter that it's deterministic if it is determined by a hackage index, which gets updated? Even if you did lock down a particular hackage index version, you are then relying on the behavior of the solver not changing as cabal evolves. Since we are using explicit configurations with stack, it is more directly feasible to have deterministic, repeatable builds.
You mean a binding to Vulkan? Yes there seems to be one on Hackage: https://hackage.haskell.org/package/Vulkan There are also several bindings to OpenGL.
There is also http://hackage.haskell.org/package/vulkan which seems to be a more active project. I only came across it yesterday so I can't say anything about it other that it exists - but it does require ghc-8...
You can follow the official guide for stack [here](http://docs.haskellstack.org/en/stable/README/) and get everything set up :)
I'm not familiar with GHCJSUnrender, sorry =/ But i don't recall seeing that type class anywhere in GHCJS base. It might just be something defined by that library? Not sure. Anyway, I'd recommend going to the git repo for ghcjs-servant-client and submitting a pull request (I'm assuming it's on github?) upgrading it to newer servant versions. I'd also probably recommend taking a look at servant-reflex and seeing if you can port it to your in house FRP library.
Seems like there's no `new-install` yet.
The problem is you can't install these multiple definitions at the same time. We call this problem hell hell.
is `hell` its own fixpoint?
/u/soenkehahn is working on that.[0] I think the main issue there is getting CI to not take forever with GHCJS (and not blow up the cache). /u/arianvp made a version some time back for an earlier version, and there's also a servant-client fork that Plow Technologies seems to be maintaining[1] (in addition to the things mentioned by others). If anyone wants to help with CI, that'd be very welcome! [0] https://github.com/haskell-servant/servant/compare/client-ghcjs-wip-2 [1] https://github.com/plow-technologies/ghcjs-servant-client
I think this slightly overstates things. [This documentation](https://hackage.haskell.org/package/yesod-form-1.4.7.1/docs/Yesod-Form-Bootstrap3.html) for Yesod.Form.Bootstrap3 isn't covering a specific functionâ€”it's documenting that entire module. For plenty of smaller packages, that's all that's necessary. The README feature that sclv talks about also works for small to medium size packages I don't think Tutorial modules are bad, but to go beyond that maybe something like Read The Docs would be good? [Stack uses that](http://docs.haskellstack.org/en/v1.0.4/README/).
Whoa. Didn't think we'd see projects requiring GHC 8 so soon. It's not even fully released yet!
I think much of their research has been on automatic vectorization of operations. They're a hardware company and I think Haskell has the best semantics for discovering computational dependencies to get the most out of hardware.
why didn't you call it LambdaSheets? :)
`Box` isn't a monad, unless you have an infinite supply of boxes (depending on category).
Still too much work. You've already found the symbol you want to improve documentation for, now you have to go hunting for it again in the source file. But what if it was re-exported from another module? Time to play more hide-and-seek!
So it's just something we're exploring at the moment. The intuition is it's a quick way to have sticky behaviour by default, without completely locking out all the flexibility by completely fixing all versions of all deps. Of course for serious repeatability you want the more intentional approach, and that will of course be supported. The general idea is we want to support both a frozen (with or without curated collections) workflow, but also a minimal/implicit config "don't ask me silly questions" workflow using the solver. It's worth keeping in mind that flexibility is also a virtue here, and one that's somewhat at odds with very explicit approaches to repeatability. Different use cases call for different degrees of flexibility vs locking things down. Eg. production apps want to be carefully versioned, whereas in other cases being easily able to try the same code in different contexts, e.g. with different compilers and versions, different libs etc etc is useful. So yes, summary is we want to do both, conveniently.
Right not yet. It's one of the big remaining bits. The UI for that has mostly been hashed out already, but not yet implemented.
I had to roll my own inserter here: https://github.com/tonyday567/hrefactor/blob/master/elisp/hrefactor.el#L210 I've been slowly covering the hlints, but the regex work aint a great motivator.
Thanks! That helped me :) I mostly get what a monad _is_, but I'm having a hard time working and thinking with monads and using their properties. 
I still think it has to link back to source control in the end. I want the package author to sign off on the docs; I don't want to pile on to the mythical hackage dev team; and I, as an author, don't want to have to copy/paste back into my repo.
The [NICTA course](https://github.com/NICTA/course) on GitHub has you implement from scratch a bunch of different monads, their instances, utility functions that use them, and some transformers. A fantastic intro, though it can be quite tricky.
There was a great article I read that showed how you could think of a compiler as just being an interpreter that worked on different data types. I think it used lisp. I really liked that way of thinking about it but I can't find the article. 
I thought it is called hell because you are going from one problem to another with no end to it - like eternal torture in hell.
I sure did, and Chris as well. We've got some great guests lined up, we just need to find the time. I'll talk to Chris about getting the ball rolling again and let you all know.
haskellbook.com
There's no code at all: https://hackage.haskell.org/package/Vulkan-0.1.0.0/src/src/Graphics/Vulkan.hs
Thanks, glad you enjoyed it! I'll take a silver medal gracefully.
Yes, will do.