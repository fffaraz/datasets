Thanks for the useful information. You may want to record this in an additional place; indexing reddit for a post one remembers is difficult.
Great tutorial. Looks like a small syntax issue in the "Instances" section (s -&gt; z): `{-# LANGUAGE GeneralisedNewtypeDeriving #-}` Also needed to import Data.Monoid for (&lt;&gt;). The final (no pun intended) header looked liked this after fixing errors: {-# LANGUAGE GeneralizedNewtypeDeriving #-} {-# LANGUAGE TypeApplications #-} import Data.Monoid 
Good catch. I will fix it. Thank you for pointed out 
Sorry but i was checking because that code compile and run without anything extra and I have found that things you mentioned are not needed: - Extension can be both with `s` and `z`. [See here ](https://downloads.haskell.org/~ghc/8.4.2/docs/html/users_guide/glasgow_exts.html) - Also there is no need for importing `Data.Monoid` because `(&lt;&gt;)` is in Semigroup on GHC.Base defined. Beyond this, thank you anyway for keeping an eye and help me with errors Best 
 putResults :: a -&gt; b -&gt; a Because `b` isn't part of the class signature, this effectively reads putResults :: ∀ b . a -&gt; b -&gt; a - meaning this is a function that, given this particular `a` and _any_ `b` which the _caller_ of the function gets to pick, it'll give you an `a`. Your implementation however forces `b = [String]` which violates this signature (because a `b` that has to be equivalent to `[String]` is _not_ the same as allowing `b` to be anything, like `Int` or `Double`). Aside from that you also called it `getDomain` in the class definition and `getUrl` in the instance definition, which I assume is just a typo. Now there are various ways you could go about fixing this, but the first question you should really ask yourself is this: _Why have a typeclass to begin with_? Why not just type MyData = (String, \[String\]) getUrl :: MyData -&gt; String getUrl (url, _) = url putResults :: MyData -&gt; [String] -&gt; MyData putResults (dom, emails) newEmails = (dom, nub $ emails ++ newEmails) ?
Yeah you're right, I was on 8.0.1 on this machine, sorry. I updated and works with just GeneralisedNewtypeDeriving. I'm also surprised they added an alternate spelling? That's interesting. 
I want the crawler to know that once the data is passed in, it can get the domain and pass the results as it's parsing a webpage. How do I ensure that without a class?
I have no idea why you think you _do_ need a class or even how a class would help in any way shape or form with that. Haskell type classes are for enabling ad-hoc polymorphism. I have a bit of a hard time giving you advice here because while it's very obvious that you have some rather critical misconceptions about how this works, I can't tell what they are. It might help if you gave a more detailed breakdown of what you were actually trying to achieve.
I edited the post you responded to as you were typing your post, the clarification might help! I'm definitely new to haskell and trying to piece it together. Just a hobbyist at all this stuff, I have very little OO, knowledge, just what I picked up from Python
Yes, thing is that the `class` keyword in Haskell has absolutely _nothing_ to do with the `class` keyword in Python other than that they're spelled the same way.
I understand that, in Haskell it's more comparable to an interface in most languages right? My thought was that if there's an interface (class) that says you can get a url out, and put parser results in, then the crawler can handle the parsing and manipulating of data then send back the results for anything, so long as it fits that interface.
Do you think these electronic voting guarantees are still true if *all* those machines are maliciously running hardware&amp;software fixed by foreign intelligence agencies?
So, the crawler gets an `a`. What it knows about that `a` is that it can get a URL from it, and that it can get a new `a` by using `putResults` which requires a `b`. The only way it knows how to get a `b` is with a parser function of type `ByteString -&gt; b`. So, a couple of questions - why the `getUrl` function? In your example the URL is constant, why not just pass in the `URL` has an initial parameter. Are you expecting it to change? Why is the channel passing around `a`s when what you seem to be interested in are the `b`s? So the problem is that you have parts of a solution in mind but I still don't quite know exactly what problem you're trying to solve.
I built a crawling library, that contains all the functionality needed to crawl all the pages on a particular donation. To give you a specific example, I want to be able to take a SQL row that contains an id, a url and field for email addresses, pass that row into my crawler along with a parser that finds email addresses. The crawler should be able execute the parser on every response body it finds, and pass the parser results into putResults which modifies my data (SQL row in this case). Then when it has fully crawled the domain, it will pass the data back out in the same form, ready to be put right back into the sql database. Does that make sense? 
Yes. Having all the machines behave incorrectly would be obvious. The real danger is operating above the noise but below the signal floor, strategically.
Why would it be obvious? Obviously they'd be fixed to do it subtly.
The proximal problem is that your type class as specified does not (and, indeed, *cannot*) know anything about `b`, so the only sensible definition of `putResults` is `const`, which doesn't help you very much. You could, with `-XMultiParamTypeClasses` and `-XAllowAmbiguousTypes`, instead write class Dataish a b where -- there's already a class called Data getUrl :: a -&gt; String putResults :: a -&gt; b -&gt; a then, with `-XTypeSynonymInstances` and `-XFlexibleInstances` write an instance like instance Dataish MyData [String] where getUrl' = fst putResults' (url, emails) newEmails = (url, nub $ emails ++ newEmails) Now we're 4 language extensions in before we can even write putResults ("this", ["hello"]) ["there"] -- ("this",["hello","there"]) But, annoyingly, we can't write `getUrl ("this", ["hello"])` because `b` doesn't appear in the signature for `getUrl`. We can get around this with yet another extension, `-XTypeApplications`. Then we can write getUrl @MyData @[String] ("this", ["hello"]) -- "this" That was a *lot* of machinery (and some very concerning language extensions) for something that oughta be straightforward. You don't really need a class for this. You could instead write something like data MyData a b = MyData { getUrl :: a, results :: b } deriving Show And now you can write type MyTestData = MyData String [String] Now you can create a value like mkTestValue :: String -&gt; [String] -&gt; MyTestData mkTestValue url emails = MyData url emails someValue = mkTestValue "https://google.com" ["this is an email"] -- MyData {getUrl = "https://google.com", results = ["this is an email"]} addEmails :: MyTestData -&gt; [String] -&gt; MyTestData addEmails d newEmails = MyData (getUrl d) (results d ++ newEmails) -- addEmails (MyData u r) newEmails = MyData u (r ++ newEmails) is also a reasonable definition addEmails someValue ["another email"] -- MyData {getUrl = "https://google.com", results = ["this is an email","another email"]}
You can run a Hoogle server locally, open the UI in a browser, and click through to the generated Haddock docs for both your code and dependencies.
Interesting. do you think the reason is that it's necessary or useful be because of the GIL (to prevent a more problematic alternative, like corruption)? I ask because I want to write a Haskell Emacs-Module, and (afaik), Emacs's ELisp interpreter also has a GIL.
*Eyy, another year!* It's your **6th Cakeday** beerdude26! ^(hug)
The empty escape (`"\&amp;"` = `""`) is also handy to hack around syntax highlighters that don’t understand gaps (`"a\ \b"` = `"ab"`), for instance: "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent\n\ \tristique nunc quis lacus interdum porta. Nulla sodales dolor\n\ \massa, vitae porttitor quam semper a. Morbi bibendum justo eget\n\ \justo dapibus, id fermentum tellus commodo.\n\ \" If the highlighter interprets the final `\"` as an escaped quote, then the highlighting “runs away”, but if you write this: "…\ \justo dapibus, id fermentum tellus commodo.\n\ \\&amp;" It usually works fine. ¯\\\_(ツ)_/¯ 
Haskell beginner here so forgive me if I'm wrong, but your first code example won't run for me. I put this in `Fractal.hs` file: -- module Fractal.hs type Point = (Float,Float) add :: Point -&gt; Point -&gt; Point add (x1,y1) (x2,y2) = (x1+x2,y1+y2) mult :: Float -&gt; Point -&gt; Point mult r (x,y) = (r*x,r*y) scaleAround :: Float -&gt; Point -&gt; Point -&gt; Point scaleAround r p q = ((1-r) `mult` p) `add` (r `mult` q) triVertices = [(sin(2*k*pi/3), cos(2*k*pi/3)) | k &lt;- [0..2]] f :: Point -&gt; [Point] f q = [scaleAround 1 / 2 p q | p &lt;- triVertices] kpow :: Monad m =&gt; Int -&gt; (a -&gt; m a) -&gt; a -&gt; m a kpow 0 _ = return kpow n f = f =&lt;&lt; kpow (n-1) f startPt = (0,0) :: Point nGens = 10 outputData = kpow nGens f $ startPt Then I launch `gchi` from my terminal and get this error: [Prelude] λ&gt; :l Fractal.hs [1 of 1] Compiling Main ( Fractal.hs, interpreted ) Fractal.hs:16:8: error: • Couldn't match type ‘Point -&gt; Point -&gt; Point’ with ‘(Float, Float)’ Expected type: Point Actual type: Point -&gt; Point -&gt; Point • Probable cause: ‘(/)’ is applied to too few arguments In the expression: scaleAround 1 / 2 p q In the expression: [scaleAround 1 / 2 p q | p &lt;- triVertices] In an equation for ‘f’: f q = [scaleAround 1 / 2 p q | p &lt;- triVertices] | 16 | f q = [scaleAround 1 / 2 p q | p &lt;- triVertices] | ^^^^^^^^^^^^^^^^^^^^^ Fractal.hs:20:14: error: • Variable not in scope: (&lt;=&lt;) :: (a -&gt; m a) -&gt; (a -&gt; m a) -&gt; a -&gt; m a • Perhaps you meant one of these: ‘=&lt;&lt;’ (imported from Prelude), ‘&lt;=’ (imported from Prelude) 
If you regularly find yourself changing pure functions to effectful ones, using `Identity` up front has the small benefit that you don’t need to change all the notation of your code like you ordinarily would, since it’s already in monadic style. But you don’t need to go as far as in your example!
They are probably using a different Prelude like Protolude that exports (&lt;=&lt;) you can fix the second error by adding `import Control.Monad ((&lt;=&lt;))` at the top of the file. Not sure what's going on with the first error.
The GHC user manual would be an excellent place to make a note of this.
I'm excited I've gotten the discussion started, but a little less excited my name got spelled wrong ;) The reason I am hesitant to use `capability` and `fused-effects` and their ilk is --- while I'm sure that they're great libraries --- they seem to be missing something fundamental. _Defining new effects and new interpretations must be the easiest thing in the world._ When I'm writing application code, I really want to be able to spin up the exact abstraction at a minute's notice. Every single line of boilerplate in the way of my new effect/interpretation is one more chance for it to not get written. To me, it's not enough that an effects library solves the problem, it also has to be pleasant to use. * `mtl` is pleasant to use because everyone already understands how it works, it performs well, and the type inference is good. * `freer-simple` is relatively pleasant to use. After you've grokked your head around it conceptually, it's super quick to spin up new effects and interpretations. And the error messages are usually pretty helpful. Unfortunately (as many have pointed out), it can't satisfactorily model higher-order effects, and its performance is balls. * `fused-effects` solves the perf and the higher-orderness at the cost of a lot of boilerplate. An effect and interpreter combination that in `freer-simple` would be maybe 20 lines often can balloon to over 100. * I don't know enough about `capability` to make a comparison here, but the announcement sure looks like there is a lot of janky boilerplate I need to write (even though it might just be in a deriving via statement.) Maybe it's just a case of me needing to wrap my head around it, but I'm not holding my thumbs. None of this is to say that the above libraries are bad in any way, merely that I'm dissatisfied with the current points we've explored in the design space. What I want is the ability to quickly prototype the effects and interpretations I want, and then get to work. Performance usually doesn't matter, unless it does, and I'd like to have to worry about it then _and only then._ I want to be able to use all of my existing knowledge of Haskell and its fundamental building blocks to inform how I interact with these effects. _I already understand_ how functions work, but I have no idea what magic is happening behind your carriers/deriving-via/whatever. What I want is a library that hides all of that shit from me, and lets me write the things I want to write in language I already understand. It's fine if the library is complicated and does really smart things, so long that I don't need to be a developer of the same calibre as the library-designers in order to use it. It's a little soon to say too much, but I think I've worked out all of these problems in my own library. It's boilerplate free for simple effects, interpretations are just functions, can model higher-order effects no problem, and, with care, can has the performance of `O(mtl)`. Hopefully a few days of really polishing the user-experience will have something I'm proud to show off. Stay tuned!
Awesome, both those worked. Thank you very much for the help. May I ask another question? When I load the module and run `outputData` I get a seemingly endless list of tuples. Is there a way to plot this data? Does haskell have a plotting library?
This makes sense, thank you so much for taking the time to type it all up!
I shouldn't pile on, but since I had the same objection as Tarmen I will comment. Although the MTL typeclasses `MonadX` are _intended_ to be used on a monad transformer stack, and are only instantiated for monad transformers, they are not bound by definition to refer only to monad transformers. `MonadState s m` just says that `m` has a state of type `s`, not that `m` is a monad transformer stack that has a `StateT s` somewhere in it. I think the term MTL may be misleading here: the T means "transformer", but the unique content of the library is actually the typeclasses, which are tagless final abstractions for the transformers that are actually defined in the `transformers` package, not `mtl`. But everyone always calls these classes "MTL style", the point of which is _not_ to have to drag around an explicit monad transformer stack in all your type signatures.
Can't; they'd fail one of the validation steps. If they pass both cast-as-intended validation for all voters (which any voter can validate on their own) and counted-as-cast validation/audit for the final tally (which can be done independently by a voter or group of voters, though it is more involved than for cast-as-intended), then all votes have been counted-as-intended. Best chance is to change only the votes cast by the voters that will definitely not perform cast-as-intended validations. Maybe a little more than that, because a few cast-as-intended violation might not get the attention they deserve. This could affect the final tally without triggering a counted-as-cast violation.
Thanks for sharing. I also have something similar in my emacs config: ; search work under the cursor in hoogle (setq browse-url-generic-program (executable-find "firefox")) (defun haskell-search-hoogle () "Search hoogle for the word under the cursor" (interactive) (browse-url-generic (concat "http://hoogle.haskell.org/?hoogle=" (thing-at-point 'word)))) (define-key haskell-mode-map (kbd "C-:") 'haskell-search-hoogle) 
I infer that you may be new to programming, I apologize if I am incorrect in this assumption. It is considered polite behavior to share the solution to any problem that you have asked for help on. This is because future programmers who have the same problem will often use google to search for a solution - If your post doesn't mention the solution, it's not useful to anyone who comes after you. 
Open source does not help if the machines come with a precompiled binary.
I like the idea of the VL free monad a lot. I was missing this crucial bit: "if we simply drop the comment restricting the form of ops, we get a less constrained free monad type which supports more effects". &amp;#x200B; The ergonomics of the record-of-functions are key and many variations on that theme are possible. For example, instead of relying on newtypes + deriving Via, one could use a more "classy lens" approach like \`(MonadReader env m, HasLogger env) =&gt; ...\`. &amp;#x200B;
In the counting committees all runnibg parties are represented -- down to the voting location level! This gives yoy reasonable assurance that there was no systematic/fradulent miscounting, as **all** the representatives from the non-favoured party would be very vocal about it.
Exactly.
Maybe I'm in the minority, but I absolutely loathe this paradigm. It feels like using a 10 ton hammer to solve a 5 lb problem - I want to read basic text, maybe with 2-3 colors, in a single font. We've literally been able to do this with computers since like 1970. Why in the hell should I launch an entire local webserver just to support that case? Not only am I sucking up an order of magnitude more resources to accomplish a basic task, but I've also introduced an incredibly long chain of possible failure states, and even potential security issues. Hell, if you wanted to browse some HTML in a browser, and that HTML was stored locally, you don't need a \*\*\*\*ing webserver to do that, browsers have been able to render local HTML over the filesystem since their inception almost 3 decades ago. /rant 
The haskell report changes *very* slowly. But multi-parameter typeclasses is a fairly non-controversial extension, and is widely used. I believe AllowAmbigousTypes was the concerning one. 
To plot something like this, I'd use rasterific (http://hackage.haskell.org/package/Rasterific) personally.
&gt; What I want is a library that hides all of that shit from me [...] Have you considered that perhaps this problem cannot be easily solved at the library level and actually might need a changes in the language itself? Nobody seems think that we can shoehorn, for example, Rust's borrow semantics into Haskell (the language would need to be extended with something like linear types). Fewer and fewer people seem to be happy about shoehorning dependent types into Haskell (again the language needs to be extended to be able to do that without boilerplate). Why do you expect to be able to shoehorn an effect system into a Haskell?
&gt; but a little less excited my name got spelled wrong ;) Oh dear! That's unforgivable… I'm very sorry. I've fixed the blog post, at least. &gt; Defining new effects and new interpretations must be the easiest thing in the world. With capability, just as with capabilities-as-records, definining new effects is really easy. You just go ```haskell class HasLogger m where log :: LogLevel -&gt; Text -&gt; m () ``` I contend that it's, actually, even easier than with Freer, because you define functions which have precisely the type which you can use. On the other hand, you get to the boilerplate when you define handlers. That's because type classes are harder to combine than records (or other Haskell value). &gt; the announcement sure looks like there is a lot of janky boilerplate I need to write (even though it might just be in a deriving via statement.) Maybe it's just a case of me needing to wrap my head around it, but I'm not holding my thumbs. The point of capability, actually, is to reduce the boilerplate of creating type class instances, by giving you combinators to define common instances. Everything is trade offs, in this story, and you may not want to make quite the same as I do, though. And that's all fine ;-) . &gt; Hopefully a few days of really polishing the user-experience will have something I'm proud to show off. Stay tuned! I'm looking forward to it!
How do you think `simple-effects` compare to these other solutions?
Just a small nitpick: `const` isn't the only sensible implementation here since the class can know everything about `a`.
Edited! Thanks.
Not sure how well known it is, but I recently discovered https://hackage.haskell.org/package/turtle for this use-case.
Reminded me on [this animation](https://gfycat.com/PracticalFatKentrosaurus), while taking a different approach.
I'm happy to announce a new serialisation library, winery. It features high performance, low space usage and extensibility. The major difference from other approaches is the presence of schemata. A schema is derived from a datatype and it conveys various information such as field or constructor names, types etc. This grants high availability of data unlike binary or cereal, where you can no longer read data if the datatype changes. Also, it's much more compact than CBOR or JSON because all the metadata are factored out to a schema. Here's a benchmark result comparing the performance of commonly used libraries (processing 1000 of 8-fields records): | | encode 1 | encode 1000 | decode | length | |-----------|----------|-------------|---------| ------- | | winery | 0.28 μs | 0.26 ms | 0.81 ms | 58662 | | cereal | 0.82 μs | 0.78 ms | 0.90 ms | 91709 | | binary | 1.7 μs | 1.7 ms | 2.0 ms | 125709 | | serialise | 0.61 μs | 0.50 ms | 1.4 ms | 65437 | | aeson | 9.9 μs | 9.7 ms | 17 ms | 160558 | 
As others have said, without the code it is difficult to say the reason for the error. &amp;#x200B; But usually "ambiguous occurrence" means, that the type checker is inferring that there are more than one possible types for an expression, hence it is ambiguous.
Cool! Are you using GHC's compact regions support from the [ghc-compact](https://hackage.haskell.org/package/ghc-compact) library?
I am trying to get the subsets of a set which have n-2 elements. For example : \[1,2,3,4\] should give back \[\[3,4\],\[2,4\],\[2,3\],\[1,4\],\[1,3\],\[1,2\]\] I have a bit of code which almost works: `make_lists xs = sequenceA $ replicate ((length xs) - 2) xs` and it gives me back : \[\[2,2\],\[2,6\],\[2,9\],\[2,12\],\[6,2\],\[6,6\],\[6,9\],\[6,12\],\[9,2\],\[9,6\],\[9,9\],\[9,12\],\[12,2\],\[12,6\],\[12,9\],\[12,12\]\] all the possible subsets which are there how could modify that to filter out the ones I don't need? 
How does performance compare to flat, store and persist? [https://github.com/haskell-perf/serialization](https://github.com/haskell-perf/serialization)
What does "sustainable" mean?
Turtle is what I use if I need scripting from within an existing Haskell project. It's worth a look!
No, it has nothing to do with a large working set and I don't think compact regions would do anything
I wasn't aware of the repo. Going to add winery to it.
By sustainable, serialised data can be read without the original code thanks to the schemata. Many serialisation libraries (binary, cereal, store, etc) don't store anything other than raw values.
Does winery support migration if the schema changes? Or would that be built into a higher level library?
Because there are multiple examples of it being done?
Yeah, MPTCs is fine, though TypeFamilies gets you everything I've ever thought about using MPTCs for without having to drag in fundeps and TypeSynonymInstances (and sometimes AllowAmbiguousTypes, Flexible/UndecidableInstances, etc.), which feels cleaner.
I'd be interested in how it works on e.g. several hundred megabytes of data with nested, complex data structures? Those were the cases we optimised cbor for, not 1000-record microbenchmarks, e.g. cereal is supposedly "only" 30% slower in this instance but in reality I've seen it more like 2-4x slower on programs people actually write. Similarly, aeson is "only" 3x slower, but you're going to see factors of 10 or more when you get into just a few megabytes of serialized data. Every Haskell serialization library can look amazing good when the whole dataset fits in your CPU cache. Any non-trivial workload is going to blow that limit immediately, and most likely you're going to be processing large amounts of data rapidly. That's where things like heap usage optimizations matter, and things microbenchmarks cannot tell you.
The proof for radix tree will be a lot bigger, since you have to provide each node you've touched (and 17 hashes in patricia-tree case). 
Indeed, you make a good point, as a whole it is fairly reliable (if you have trust in your government and your people). If you believe that a vast majority of people implied in checking the process are neutral then you are fine. But again, you just rely on that belief, and you can't prove nor disprove anything. I am not stating that the electronic voting is a better system don't get me wrong, I am just saying that have no choice other than "trusting" without proving, in the current system. &amp;#x200B; That's why blockchain might be a good candidate if proven sound.
Not only that, the main reason is proof. Proof is a part of tree structure (its size ~ logarithm of whole tree size) that you have touched during operation (and you can repeat and validate the operation on its proof alone). To minimize the proof size, the tree is made binary.
Yes. Removing a field or adding a constructor are trivially compatible, and there is API for handling the other way around. &amp;#x200B; For example you can handle a case "bar" is missing (using ApplicativeDo): &amp;#x200B; instance Serialise Foo where bundleSerialise = bundleRecord $ const $ buildExtractor $ do x &lt;- extractField "foo" y &lt;- extractField "bar" &lt;|&gt; pure 0 pure (Foo x y)
&gt;[https://goalkicker.com/HaskellBook/](https://goalkicker.com/HaskellBook/) This looks like an auto-generated book, probably from StackOverflow. Can anyone vouch for it? &amp;#x200B;
Yes, that’s not actually a book but rather a compilation of topics. Although, it also can be useful as a reference 
Thanks for the feedback. I tried to make winery support a variety of structures, even mutually-recursive ones so I'd happily take up a challenge. Sorry to hear that your benchmark is no longer compatible...
Asterius dev here. We have a tail-calls mode now; see [here](https://tweag.github.io/asterius/ahc-link/#common-options-for-controlling-outputs) for details :) As for gc, the wasm gc proposal isn't landing soon, and even if it does, it doesn't play nice with ghc runtime's pointer tagging tricks. So we have our own copying gc right now.
I think he's referring to the fact that ghc-compact can be used to dump a Haskell data structure directly to disk with without an encode/decode step. You just dump the memory as-is. But that would be incompatible with a concept like winery, where there is a schema. ghc-compact follows the memory layout of GHC.
It would probably be better, IMO, to have a separate repository containing all that code that can be more easily kept up to date as a big benchmark for various libraries, since it's a bit tucked away. No clue how much work it would be to resurrect for the new index format, though...
Neat. The lack of a schema is one of my biggest annoyances with [acid-state](http://hackage.haskell.org/package/acid-state). I'm hopefully this could provide a solution.
Can you sort `xs`? Are there duplicates in `xs`? If yes and no, filter on an `isStrictlyIncreasing` predicate. --- My approach: subsets 0 l = [[]] subsets n [] = [] subsets n (x:xs) = map (x:) (subsets (n-1) xs) ++ subsets n xs make_lists l = subsets (length l - 2) l --- Hylomorphism: data Sets pivot set = Unit | Void | Split pivot set set deriving Functor make_lists l = refold alg coalg (l, length l - 2) where coalg (_, 0) = Unit coalg ([], _) = Void coalg (x:xs, n) = Split x (xs, n - 1) (xs, n) alg Void = [] alg Unit = [[]] alg (Split x l r) = map (x:) l ++ r
I tend to agree that at the current level of access we allow voters to voting machines, it makes little difference if "the software" is "open source", since we don't have any method of confirming the software that is *supposed* to be on the system is actually there. We can audit source code on gitlab all day long and it won't tell us jack about the touchscreen put in front of us on that day, unless we can verify a connection between the two.
What kind of tagging tricks? I think they also tagged 31 bit pointers in WASM to distinguish references from pointers. I would still recommend to reconsider to use the WASM GC if it becomes available. Probably you are using a Cheney-style STW GC right now in Asterius? If a WASM GC becomes available, you would get minor parallel copying plus concurrent and parallel m&amp;s on V8 at least. I wouldn't want to replicate that in a GHC WASM runtime.
Thanks for the post. Though instead of passive reading, I strongly encourage everybody to go through Let's Lens [1] course material and get all the aha moments the author of the post experienced. It's very easy to jump in, all the boring stuff is already done for you. I enjoy going through the exercises, huge thanks to the authors. I also highly recommend the SPJ Talk on lenses [2]. He is an exceptional explainer. [1] https://github.com/data61/lets-lens [2] https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation
&gt; programs people actually write There are plenty of programs that only need to serialise and deserialise tiny amounts of data (hence maximum performance not being a real concern for those) &gt; people should really, really stop using aeson as an serialization format You mean JSON. And there are reasons to prefer it (semi human-readability, serialisation and deserialisation already supported pretty much everywhere). It’s good to have alternatives for other use cases available, but your view on this seems to be overly narrow.
I'm working on a project which involves performing static analysis over an AST, and I thought it would be a good time to try and apply the catamorphisms I had been reading about, a la recursion-schemes. I've run into a couple questions though. First off, I hear catamorphism used synonymously with fold, but it seems to me that cata is more general than a fold, in that you can choose how to combine accumulated values depending on the context. Is that right? Also, I try to define an actual foldr instance - or something close - as a cata, but I'm having trouble deciphering the errors, which all revolve around the `Fix` type.
https://www.reddit.com/r/haskell/comments/b2xhe6/writing_a_lambda_calculus_evaluator_in_haskell/
&gt; There are plenty of programs that only need to serialise and deserialise tiny amounts of data (hence maximum performance not being a real concern for those) I've heard that one before. Except then before you know it the data was "only" 5mb and the runtime difference was still nearly 20x, because programmers are essentially terrible at estimating. Computer programmers really need to get over pissing away every CPU cycle they can because they think they can estimate things correctly (which is almost always untrue). &gt; And there are reasons to prefer it (semi human-readability, serialisation and deserialisation already supported pretty much everywhere). It’s good to have alternatives for other use cases available, but your view on this seems to be overly narrow. Wrong. For a serialization format, there are no absolutely reasons to prefer it over any number of better alternatives in almost any language. If you're using it for interop or strictly debugging, that's acceptable. If you're using it so you can save 12 seconds of your own time reading something and in return your users get bloated programs, you're kidding yourself.
I'm working on a product that can easily run between a couple minutes to a couple days and then spits out of a couple kilobytes of JSON that are supposed to be processed by customer controlled tooling, including one "internal customer" web frontend. I can assure you that absolutely nobody cares about saving a couple of milliseconds on the final output step.
You're going in a lot of circles to basically say "my company does this thing and therefore, it's good and justified when people piss away their CPU cycles based on code that doesn't hold up to actual use cases beyond the most trivial microbenchmark." You can just say you don't actually care at all about efficiency, my friend.
&gt; By sustainable, serialised data can be read without the original code thanks to the schemata. Unless you’re serializing executable code, this isn’t possible. You still need your old deserialization code to deserialize an old format. I assume you’re storing the structure of the data, which is basically a form of versioning. 
My personal favovire for now IntelliJ-Haskell. It has good autocomplete with quick docs function and installation is really simple.
What? Why would the deserialisation code be in the output? There's a 'constant' schema deserialiser that reads the outputted schema just like in say protobufs. Storing the code would be unsafe and insecure (as corrupted data would break the application) and would probably be slow.
Does this support bracket?
Worked initially on Emacs. Can’t say it was a struggle, but today’s Visual Studio Code makes me happier. 
Not a concern most of the time, but sometimes heterogeneous data (optional fields?) or situations where the whole schema isn't known upfront (think extensible APIs) are hard to represent in schemas. Should I look elsewhere for that or do you have something in store?
[similar question asked a couple weeks ago](https://www.reddit.com/r/haskell/comments/ax6s6i/what_is_the_best_haskell_ide_experience/)
`ipprint` is not included in Stackage, so you'll need to define an extra dependency in your `stack.yaml` (given you set up a project with `stack new`). Then you'll probably need to run `stack build ipprint` and just in case you'll hear something about a "hidden" package, `:set -package ipprint` in GHCi could help.
[`pretty-simple`](https://github.com/cdepillabout/pretty-simple#readme) is a nice alternative which is on Stackage. Setup information may be found on the GitHub README (linked above).
I use this [https://github.com/haskell/haskell-ide-engine](https://github.com/haskell/haskell-ide-engine) with vim. Perfect.
`Parser` is a datatype. It could be closest in Java to final class ParseResult&lt;A&gt; { A value; String rest; ParseResult(A value, String rest) { this.value = value; this.rest = rest; } } interface Parser&lt;A&gt; { public ParseResult&lt;A&gt; parse(String); } This is a mechanical translation; Java doesn't have "function types", so you have to replace them with `interface`s. Generics work relatively similarly so you can just reproduce those. `(a, String)` is shorthand for a pair of values, an `a`, and a `String`. We can get that with the `ParseResult` class above.
You forgot the square bracekts in `[(a, String)]`! The `parse` method should return `Stream&lt;ParseResult&lt;A&gt;&gt;`. 
I believe the reason for MPTCs not going into the last (2010!) report was that there semantics are still a bit unclear, particularly around type inference when there's no fundep and global coherence when there is a fundep. But, I wasn't involved in preparing that report. The GHC implementation is rock solid. In practice, you are only gaining a feature and essential complexity when you enable the extension.
I think we can all agree that React has been around long enough to be considered a viable technology for web development, and also, Angular was a terrible idea from the start.
Front End Web Development, unless you are crazy and really enjoy doing high stakes divination when it comes to web technology.
Yes, of course, thanks. I've fixed it.
 newtype Parser a = String -&gt; [(a, String)] This doesn't actually compile, it's missing the data constructor. Do you mean `type` instead of `newtype`? Because that is indeed a synonym like you mentioned. Or if you want `newtype` you can add the constructor name: newtype Parser a = Parser (String -&gt; [(a, String)])
Your counter to "your view is overly dogmatic, there absolutely are situations where JSON is the right tool" is "I have worked on tools where that is not the case". Are you seriously telling me you can't see the problem with that? You genuinely can't understand that different criteria necessitate different solutions in different circumstances, or the fact that you know of an instance where a particular solution was appropriate doesn't mean it's appropriate in all circumstances?
sorry, it should be: type Parser a = String -&gt; [(a, String)] or newtype Parser a = Parser (String -&gt; [(a, String)] I would love to understand what is the difference between two definitions. 
The `type` version is just a synonym, a shorthand. If you have a function that takes a `String -&gt; [(a, String)]`, then you can also pass it a `Parser a`. `newtype` creates a new data type called `Parser` that contains a value of type `String -&gt; [(a, String)]` internally. In Java terms it's kind of like creating a new class which "wraps" around the given type, using a private property. None of the functions/interfaces that worked on the inner type work on the new type anymore.
Emacs with Intero and stack. It has excellent completion via company and flycheck integration. 
My guess is that the API for Data.Set was developed before GADTs were widely used and maybe even before GADTs were added to the language.
I think you're missing the original argument, which was that library A outperforms library B in sensitive cases and the rest don't matter, so why prefer library B at all?
I'm pretty sure it's just performance; I don't recall how big the impact is, but everyone I know who has tried using it has said that the performance is abysmal
there is a `Functor` instance for a `Set` data structure implemented as a GADT here http://hackage.haskell.org/package/set-monad the performance is abysmal as soon as you need to access that data, which defeats the purpose of a sorted linked list
Oh yeah, I always use JavaScript or Elm or PureScript for this 
`Set` can't be made a lawful `Functor`, even with GADTs because it's possible to write `Ord` instances where two things which `compare` to `EQ` are distinguishable. Consider: module Bad where newtype Bad = Bad {getBad :: Int} instance Eq Bad where _ == _ = True instance Ord Bad where compare _ _ = EQ -- fmap getBad . fmap Bad $ Set.fromList [1,2,3] -- vs. -- fmap (getBad . Bad) $ Set.fromList [1,2,3] The two `fmap` lines must be equal according to the `Functor` laws, but any `Set` implementation that removes duplicates will mess that up.
What are the differences between nix, stack, and cabal? When should I use each one?
&gt; None of the functions/interfaces that worked on the inner type work on the new type anymore. Although you can "poke them through" using `deriving via` or `GeneralizedNewtypeDeriving`.
Because many people can agree that the default prelude has some faults but no one can agree on what it should look like.
The default Prelude has a lot of warts like partial functions, functions that use `String` instead of `Text`, etc. People don't like this so they replace them. There are several popular alternative Preludes like Protolude, Relude, etc. but people have different needs and tastes, so they create their own. 
I'm maintaining the [relude](https://github.com/kowainik/relude) prelude project for many years and I can share some thoughts on the topic. 1. Standard default library `base` is not perfect. Things always can be improved. But literally almost every Haskell project uses `base`. This makes it extremely hard to change something in `base` without breaking half of the ecosystem. The process of introducing breaking changes to `base` takes long time. However, in prelude projects it is slightly easier to change things, so you can adopt best-practices faster (for example, replacing default `String` type with `Text`). 2. Prelude projects can be seen as a sandbox where you can play with ideas to decide what suits best for a standard library. 3. Different people have different thoughts on what should be in the standard library. No one can agree on the design that satisfies everyone. 4. Well, frankly speaking, one of the reasons for having so much prelude projects is due to the fact that Haskell allows to replace default `Prelude` with something else, in this can be done in a convenient way. 5. Having convenient custom prelude is a way that can increase your performance in a huge way actually. It is hard to give benefits of custom prelude. However, whenever I'm writing general-purpose libraries I'm trying to avoid using custom preludes in there to not introduce extra dependencies for users of my library.
See the "Faster laziness using dynamic pointer tagging" paper. 2 lower tagging bits for 32-bit and 3 for 64-bit. And even if we work around that, the memory model is different; gc repurposes the table as an opaque store to gc-ed objects, but when translating from cmm, everything lives in the linear memory. &gt; I wouldn't want to replicate that in a GHC WASM runtime. Ideally we'll have both wasm native gc &amp; our own gc, so with native gc you may get smaller output code and better performance, while losing some features provided by ghc's runtime (e.g. weaks). I haven't started working on wasm native gc yet, since we already have a prototype gc now and time is better spent on making other stuff work :)
the standard library `Prelude` is minimal (plus it exports several partial functions without warnings or names like `unsafeHead`). but, any large project (or any experienced developer who has their own preferences and patterns) should have its own prelude, which re-exports some packages and utilities.)
This is cool, I wrote a library that makes pretty extensive use of aeson and if something is going to be way more performant that could be nice. &amp;#x200B; Does the library support altering record names for serialization? Because of the way records work in haskell I often end up prefixing field names with a keyword, then using aeson derive an instance but strip off the first n characters. Does winery support that?
For me it was mostly a learning experience. Looking at all functions and see if they are partial etc, ...
&gt; Ideally we'll have both wasm native gc &amp; our own gc, so with native gc you may get smaller output code and better performance, while losing some features provided by ghc's runtime (e.g. weaks). I haven't started working on wasm native gc yet, since we already have a prototype gc now and time is better spent on making other stuff work :) Agree :)
I think there's not much reason to do that in winery. Altering field names is useful for JSON when working with existing data/protocols, where winery does not really have any (it won't be difficult to support though). 
winery defines primitive serialisers and deserialisers to/from the winery format. Users are supposed to derive serialisers and deserialisers using Generics. Deserialisers may also be hand-rolled using a high-level API.
Let's Lens is also being presented at YOW Lambda Jam in Melbourne, AU, this year: https://lambdajam.yowconference.com.au/proposal/?id=9503
Also, Haskell has enough flexibility that dropping in a new prelude is really, really easy.
Yeah I hv no programming experience n it’s a bit nerve-racking. I’m sorry I can’t share my code cause it’s part of my assignment but my problem was I defining the type while the file tht I was given define it already (import from another file)
There's a pretty "direct" obstacle, at least regarding base's `Prelude.Functor` and your (totally natural!) idea to wrap `Set` as-is. Let's see. data DSet a = Ord a =&gt; MkDSet (Set a) I imagine there are classes `DSet` could instantiate that `Set` cannot (e.g. an `Insertable` class). But we would also technically lose some functionality (e.g. `empty` and `singleton` don't currently require `Ord a`). (And no `singleton` means no `Applicative` :( ) But, to your question, we still can't even write `Functor`. There is no non-trivial term inhabiting this signature: mapDSet :: (a -&gt; b) -&gt; DSet a -&gt; DSet b A hint as to why: notice we can't even cheat with the incorrect `mapDSet _ _ = MkDSet empty`! HTH.
cabal uses hackage and version bounds stack uses stackage, each release of stackage is (modtly?) a selected subset of hackage that is known to work together plus a particular version of GHC. nix is not haskell-specific, but tries to bring some aspects of purity to the software distribution world, builds and environments are like pure expressions, with repeatable results (values) and immutable inputs. I'm not personally a big fan of stack, and do most of my development with cabal. However, if you are new (and this question indicates you might be) I think it provides a better initial experience and is really quite flexible. I would recommend you use it to start your first Haskell project and take advantage of it as long as you can.
I settled with vscode and vscode-ghc-simple (The most important feature for me: It shows the types!). Works out of the box and does not eat all my RAM. I tried other Haskell-plugins a while ago, but they where non-trivial to install and consumed considerable amount of RAM and CPU cycles and where very unresponsive. I also use the plugins haskell-linter and hfmt. Have to run "stack install --copy-compiler-tool hlint hindent hfmt" once per stack resolver to make them available to all projects with that resolver. I start vscode with "stack exec code" inside my project directory to make sure the last two mentioned plugins find their tools. I have to copy a tasks.json with a build-command into every project to tell vscode how to build and parse errors. But in general i am mostly happy with this setup.
Interesting thanks. Isn't that kinda an evil Eq instance though? It seems totally reasonable to me to stipulate `a == b` implies `f a == f b` and for "unsafe" functions distinguishing them all bets are off. I know this is a hot topic about what the right Eq laws are but it does seem more of an Eq implication rather than a set-specific problem
Could you elaborate on why you consider partial functions to be a wart?
partial functions in that they can raise runtime errors. e.g. head. head [] will raise an error, versus something like headMaybe :: [a] -&gt; Maybe a
Ah yeah this makes the impossibility really clear. You need to synthesize an `Ord b` out of thin air so baking the dictionary into the `DSet` doesn't save us
`cabal` and `stack` are both build tools for Haskell packages. I wrote brief blog post about them where I compared their basic workflows: * https://kowainik.github.io/posts/2018-06-21-haskell-build-tools
`cata` for lists is isomorphic to `foldr`. foldr step init = cata alg where alg (Cons x r) = step x r alg Nil = init cata :: (ListF b a -&gt; a) -&gt; [b] -&gt; a -- for lists cata alg = foldr step init where step x r = alg (Cons x r) init = alg Nil `cata` is generalized to work with any (regularly) recursive type, by taking an algebra for the base functor of that type. `gcata` further generalizes `cata` to have a gathering / "distribute co-monad out" step and some "effects" can be represented as a co-monad. (`ana` and `gana` are unfolds, using a co-algebra, and optionally a scatter / "distribution monad in" stop.) `Fix` is one of the fixed-point encodings for haskell. If `X` is isomorphic to `Fix F` then `F` is the base functor for `X` (and `X` is also isomorphic to `Mu F` and `Nu F`). As an example, `Fix (ListF a)` is isomorphic to `[a]`: toList :: Fix (ListF a) -&gt; [a] toList (Fix Nil) = [] toList (Fix (Cons x n)) = x : toList n toFixListF :: [a] -&gt; Fix (ListF a) toFixListF [] = Fix Nil toFixListF (x:xs) = Fix (Cons x (toFixListF xs)) --- toMu :: [a] -&gt; Mu (ListF a) toMu [] = Mu (\f -&gt; f Nil) toMu (x:xs) = Mu (\f -&gt; f (Cons x (toMu xs)) fromMu :: Mu (ListF a) -&gt; [a] fromMu (Mu cata) = cata alg where alg Nil = [] alg (Cons x xs) = x : xs toNu :: [a] -&gt; Nu (ListF a) toNu = Nu coalg where coalg [] = Nil coalg (x:xs) = Cons x xs fromNu :: Nu (ListF a) -&gt; [a] fromNu (Nu coalg seed) = case coalg seed of Nil -&gt; [] Cons x next -&gt; x : fromNu (Nu coalg next) (My `toMu` could probably be lazier / more productive.)
Awk is under-appreciated. Good work.
Shameless plug for my preferred setup (which, crucially for me, supports navigate to library source code inside the IDE) https://gist.github.com/androidfred/a2bef54310c847f263343c529d32acd8 
But this implementation of Eq is whatever you may expect it to be: reflexive, transitive, symmetric. Plus, if `a == b :: Int`, then `Bad a == Bad b`.
The docs for Eq also mention extensionality/substitutivity and negation. It is not just those three properties.
I like [Leksah](http://www.leksah.org/), an IDE written in Haskell. I've used it for my hobby Haskell programming for a while. It has a nice set of useful features, helped introduce me to the Haskell build ecosystem (Cabal, QuickCheck, HLint), and it continues to improve. [Sceenshots](http://www.leksah.org/screenshots.html). Download and build instructions [here](https://github.com/leksah/leksah/wiki/Leksah-0.15.1.4). Mac OS X and Windows have pre-built packages. On Linux, there are a variety of build instructions (I use NixPkgs).
You seem to have missed where I explained to you that performance isn’t the only factor.
Why is this answer getting downvoted?? It'd be good to know if this answer doesn't make sense for some reason.
This is the answer (if you can get it to work), I find that you need to install Intero locally to the project rather than globally, but it's a joy to use once it's up and running.
Not really, most lists aren’t empty and there are tons of situations where you use a partial function because you know you’re never hitting the error case - case in point, `div`
If they're not empty then that should be represented by using NonEmpty. Lists may not be empty until they actually are and then BOOM runtime error :( it's better represent the data you're working with using the right types
I think it should be at least annoying to write functions that can error at runtime unexpectedly, not easy.
The point is that it is not always practical to do so. There is no total function `Empty-&gt; NonEmpty` obviously, and sometimes you have intermediate computations that work on potentially empty lists but in this case provably can’t actually result in one. If you can hoist invariants on the type level that’s fantastic, but it really just isn’t always practical to do so.
&gt; what should be in the standard library. No one can agree on the design that satisfies everyone. But having to satisfy *everyone* is an unattainable goal, no? This is why `base` is in such a terrible state. I used to like `foundation` but after discovering RIO to me it seems that [RIO](Ihttps://old.reddit.com/r/haskell/comments/ayoluu/rio_the_standard_library_for_haskell/) is now the most promising candidate for becoming Haskell's de-facto standard library. It's near-optimal as it's condensing years of experience and best practices into a single Haskell library and I think it raises the bar significantly for any competitors. &gt; However, whenever I'm writing general-purpose libraries I'm trying to avoid using custom preludes in there to not introduce extra dependencies for users of my library. If everyone uses the same standard library you don't introduce an extra dependency. Take RIO for example which is going to be used as dependency throughout the Yesod ecosystem and it would be foolish not to reap its benefits if you're already depending on it anyway.
Ya that's understandable for sure! But I'm of the opinion that partiality isn't practical in code that needs to run in production, pretty much relegating it to test code. If there's an error state it's important to represent that for yourself and your users :) 
Please avoid criticising without basis. So far your comment contains no arguments and is completely opinion-based and this kind of communication rarely is constructive. &amp;#x200B;
This gives me hope. Maybe things aren't as dire as I perceived. Yet.
Regarding satisfying everyone: I think people also publish preludes they themselves use, and depending how that fits with others' needs and how "polished" it is they might get popular. But what I assume is: there are preludes that are written for themselves, and then published. I have mine for example (not public since how it is developed is a mess), it depends on lens and a few other things that I always use. I would guess I'm not the only one.
The tone of this comes across to me as insulting and slightly ad hominem. Please consider whether you are communicating in a respectful way.
What is the difference between this and, oh, `Writer w` not being a valid `Monad` if `w` isn't a valid `Monoid`, which no one ever objects to? (I'm not arguing that there isn't or shouldn't be a difference; but I've never seen this spelled out.) I can think of two possibilities: * `Monoid`'s laws are *laws*, while `Eq` has no laws, only 'customary' properties. One instance following its *laws* shouldn't depend on another instance following mere *customs*. * The `Monad (Writer w)` instance has `Monoid w` in its context, but the `Functor Set` instance (if it were possible, which per nifr's comment it isn't) wouldn't have anything in its context. An instance's correctness should only depend on the correctness of instances that *that instance* mentions, not on instances only used by some other functions on the same type. (This has clear principle-of-least-surprise appeal.)
That was ages ago. You're clearly holding grudges and this is not healthy for you nor the Haskell community. Maybe FP Complete did some minor mistakes in the past but they've apologized and are now \[leading by example\]([https://github.com/snoyberg/stack-coc](https://github.com/snoyberg/stack-coc)). What we should do is should focus on the here and now rather than dwell on some misunderstandings everybody had already forgotten about.
The thing is, "provably can't actually result in one" might be true for the current version of the codebase, but since that information isn't encoded statically in any way, nothing will force you to recheck your proof when you refactor. &amp;#x200B; Of course, the only alternative is to statically encode every invariant you have, which isn't really something you can do, or at least it isn't feasible most of the time, but for this specific case we have the \`NonEmpty\` type that exactly encodes your proof. 
Yes. If you drop the restriction of `VLMonad` as described in the post. You just go ```haskell class HasBracket m where bracket :: m a -&gt; (a -&gt; m b) -&gt; (a -&gt; m c) -&gt; m c ```
`simple-effects` seems like it's in-between capabilities-as-type-classes and capabilities-as-records. A bit like the “classy lens” approach proposed by Faucelme in [another comment](https://www.reddit.com/r/haskell/comments/b3kkto/tweag_io_capability_is_about_free_monads_its_a/ej0sack?utm_source=share&amp;utm_medium=web2x) I think that we will need more experience, as a community, before we can design which flavour we really like best.
No it can’t. In this case all it’d do is move the partiality from the head function to the non empty conversion.
If you have a constructive way to prove your function produces only non-empty lists, you don't need partiality at all. Even if you don't, and decide to go for a runtime check it's much better to error out as soon as your invariants are broken. Not only when you accidentally discover they are. But most importantly, if my function returns non-empty lists but I decide not to encode that in the type, I'm effectively forcing each user of my function to implement a check. `NonEmpty` allows that information to be propagated so the work only has to be done once.
I think you could automate this process of setting up the environment to your personal requirements mentioned above.
The guesses details are probably true but it still doesn’t explain why Set can’t be changed as suggested. Others above explained how ‘fmap’ doesn’t have the power to introduce the new instance. You’d have to create a new constrained function type class for this.
[Here's what I said the last time I saw this question](https://stackoverflow.com/a/51185465/5684257). Because `Writer v` isn't a single thing. Each `v` generates its own `Functor (Writer v)`. If `v` is also a `Monoid`, it also generates `Monad (Writer v)`. The answer to "is `Writer v` a `Monad`?" changes depending on `v`. But `Set` *is* just one thing, so it either is or isn't a `Monad`. The answer is it isn't. Restating the above: -- the constraint changes depending on v -- sometimes its satisfied and sometimes not \v -&gt; Monad (Writer v) :: Type -&gt; Constraint -- this constraint is either satisfied or isn't -- specifically, it isn't Monad Set :: Constraint
My favourite Haskell IDE currently is no IDE at all. I've tried a couple of things out there, but always been dissatisfied: with installing/setting the thing up in the first place, or with it breaking on changing around projects or versions of things, or with it not working at all for scripts, or being locked into a particular editor that I don't necessarily want to have to switch to, etc etc. That said, I do use ghcid sometimes &amp; I really like it. It's relatively lightweight, which I find dodges many of the potential issues I run into with other tools.
For the exact same reason `vim` has a lot of differently configured keybinds. It's pretty easy to do, and people have strong opinions about what works for them, so they take advantage of it. Unfortunately, unlike `vim` keybinds, it's extremely common that people will try to convince you that their option is objectively better than everyone else's. So, while anyone anywhere would agree that it would be complete madness to adjust the default vim bindings because it would invalidate decades worth of available documentation and training material, people in this community are frequently insane enough to suggest that we adjust the default prelude, despite the obvious reality that Haskell's documentation situation is significantly more dire than vim's. Please note, despite the growing numbers of POTENTIAL alternatives, the consensus is still very much that a custom prelude is an optional set of tools to enable small groups of like-minded contributors to work together on closed-loop code-bases, and not a prototype for a better prelude. There are a small number of vocal opponents to this consensus, please ignore them.
&gt; The docs for Eq also mention extensionality/substitutivity and negation Could you point to the doc? AFAIK, there is no explicit `Eq` or `Ord` laws, but I feel like ignoring some argument's field should be lawful for them. And for that, it is possible to make an example similar to rpglover64's
There are a LOT of options here. Most generally, you'd defer processing sending to some batch sender, probably in another thread. A popular, basic solution might be to extend the store of messages to send with a 'processing status' flag, and have the batch sender update the status between 'awaiting transmission', 'pending transmission', and either 'sent' or just delete those items from the queue after they've been sent. This gets hard to manage, as you're managing concurrency in a DB table, but it can be a workable solution, and ACID garuntees on the DB help ensure nothing gets missed. A more advanced solution might be to use a pub/sub messaging system as your send queue, and have multiple subscribers to that fifo queue. You then use the fifo queue as a task pool, and throttle publishing based on queue size. This is a strong solution but can suffer if the messaging queue goes down and doesn't have adequate redundancy to recover messages that are 'in flight' when that happens. Kafka (for example) helps with that, but is complex to manage, and isn't perfect. It's a very good idea to start with timestamp based task assignment, like you're doing, first, and introduce more complex architecture as it becomes necessary.
If you install intero with `stack build --copy-compiler-tool` you only need one install per GHC version, instead of one install per project.
That sounds sort of like the famous [Lisp Curse](http://www.winestockwebdesign.com/Essays/Lisp_Curse.html).
It's not the `Eq` instance that is the problem here. Rather, `Set` operates on the category of objects that can be compared for equality. The morphisms in this category are not all functions, but only functions that preserve equality. `getBad :: Bad -&gt; Int` is not such a function. Specifically, you should only be able to say `fmap f :: Set a -&gt; Set b` iff you know that `forall x y. (x == y) =&gt; (f x == f y)`
https://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#t:Eq
Oh sure. Having written an LSP server I can confirm that the LSP spec is worse is better made flesh. But it’s still a whole lot better than not having anything at all.
Cool, I've actually done very little web programming in the last few years so I don't have a great sense.
Because Haskellers get intoxicated with the philosophical-mathematical idea of first principles. They want to start from first principles: the ones of themselves, without regard for any history and tradition. So they reinvent the wheel again and again.
Better I will wait a few weeks for the next occurrence of M Snoyman. The Yesod ecosystem is a sucession of frankensteinian monsters incompatible among them.
&gt; If my function returns non-empty lists but I decide not to encode that in the type, I'm effectively forcing each user of my function to implement a check. I don't think that approach scales. Case in point, in your example, you're only forcing each user of your function to check if the list is empty if they care whether or not its empty. Unless you encode your entire function's specification into its type, you're always leaving some information on the table, and encoding some functions specification into its type, even in languages that make it possible and appropriate to do so, is often orders of magnitude harder than implementing the function itself. Even then it's often better to use "dynamically" derived information based on the expressions you are evaluating, rather than quotient types like NonEmpty. Otherwise you have to write wrapper functions to transfer your invariants around; for example, how do you write `sort xs` when xs is a nonempty list? You have to write `sortNonempty = makeNonempty . sort . nonemptyToList`. And now how does the next function in the pipeline know that the head of that list is the minimum element of the original list, if that's important?
I think these issue might also get a bit better if/when vscode no longer dominates the client space. As there become more Vim/Emacs/Atom clients, is becomes more important for servers to write toward common ground, for the common protocol to move toward a Pareto optimum, and for new features to be implemented (on both sides) in a standard-compatible way. It really does sound like most of these problems are due to the "MS standard development" process, i.e. do whatever you want for as long as you can, then publish a "standard" that is simply documents (poorly) what your implementation is *supposed* to do, and announce that it has "wide implementation support" (i.e. your established product, which doesn't *actually* follow the standard) on day one. ECMA was willing to publish several such specs in the '90s, and even ISO got played into standardizing MSOOXL (or whatever their abbreviation was). Still, I think it best to continue working against the common protocol, and trying to improve it, rather than to create a new protocol, particularly if you / your group is only a minority on either side of the protocol.
&gt; the consensus is still very much that a custom prelude is an optional set of tools to enable small groups of like-minded contributors to work together on closed-loop code-bases, and not a prototype for a better prelude ... yet.
&gt; encoding a function's specification into its type, even in languages that make it possible and appropriate to do so, is often orders of magnitude harder than implementing the function itself. Yes, and no. Trying to stick absolutely everything you know about the return type into the function signature, with stacks of dependent sums, is ugly for a lot of reasons. Providing equalities in the same module that tell exactly what a function does is really not that hard, and if you want to expose all the implementation details can be done rather mechanically. If you go back and change the implementation, you'd have to update (or remove) those equality proofs, but that's actually just choosing whether maintaining the existing API is worth the effort. --- I'm not a big fan of partial functions in the Prelude, not so much because they aren't useful, but because the error message I get on the undefined path doesn't have enough context. That said, I did most of my Haskell programming before we had stack traces at all, so it was worth it to write: case myProc x y z of [] -&gt; error "myProc gave an empty list, which shouldn't ever happen, on " ++ show y (x:xs) -&gt; whateverSuccess x means rather than `whateverSuccess (head $ myProc x y z) means`. All things being equal, I prefer to push errors to compile time, by having the type system enforce invariants, but I know all (other) things are not equal so partial functions or faster, but "bigger", types are sometimes the best solution. (I keep thinking about how `NonEmpty` is (or was) several times slower than `[]` for the same tasks because some optimizations didn't fire or some REWRITE rules were missing.)
That is the previous post in the series. The titles are almost the same: 1. Writing a lambda calculus *evaluator* in Haskell 2. Writing a lambda calculus *type-checker* in Haskell
&gt; Trying to stick absolutely everything you know about the return type into the function signature, with stacks of dependent sums, is ugly for a lot of reasons. Yes, I agreed with you already, although you stated the solution better than I did: &gt; Even then it's often better to use "dynamically" derived information based on the expressions you are evaluating, rather than quotient types like NonEmpty. &gt; I'm not a big fan of partial functions in the Prelude me either: &gt; That said this is not a defense of partial functions in the prelude; I'd much rather `head` had the type `[a] -&gt; a -&gt; a` with the 2nd parameter being the default result for empty lists. The user can always write `head xs (error "xs can't be empty here")` 
What would the implementation of that function be? :)
At some point, maybe if we get a new version of the spec, considering those kinds of sweeping changes could be appropriate. Massive breaking changes need more justification than minor QoL increases. In the interim it's wonderful that people are experimenting with options, really, that's awesome, and I begrudge nobody the use of tools that work for them. I also see no fault with trying to prove out that a given choice is a solid improvement on what we have. However, evangelizing the use of a custom prelude as a best practice for all projects, or as an instructional aide, is in my opinion, bad for the ecosystem. And as a point of advice, I'd recommend newer users treat them as an advanced tool for specific occasions.
Awesome! Thanks for that.
I think there's a ring of truth here, though I think your presentation was un-flattering. The 0th wheels were logs; the first "invention" was separating the wheel from the axle. First wheels were still solid. The first reinvention of the wheel was rims-and-spokes, it significantly reduced weight and inertia, but slightly reduced durability. The second reinvention of the wheel went a different direction, adding cogs, which could be interlocked. For a while materials improvements were the main change. The third reinvention of the wheel was the application of a (solid) tire of a different material. The next innovation, mirrored the first re-invention and used hollow, but *pressurized* tire. I don't know where we are on the design of the Prelude, but I'm not sure it's finished and starting from first principles is one way to approach innovation. Things like AMP and FTP are other ways (which, honestly have shown more successful) to make changes to Prelude, but they can only add, not remove, and sometime important innovations are about removing rather than adding (as the wheel illustrates).
That's a recent addition (*base* 4.12) I hadn't noticed yet; thanks for drawing my attention to it.
These don't seem like unfixable issues with the spec. 
Taking a function and writing the equalities for it's implementation *is* mechanical: fn pat11 pat12 pat1n = r1 fn pat21 pat22 pat2n = r2 fn patm1 patm2 patmn = rm fn_eq_1 :: (x1 = pat11) -&gt; (x2 = pat12) -&gt; (xn = pat1n) -&gt; (f x1 x2 xn = r1) fn_eq_1 Refl Refl Refl = Refl .. fn_eq_2 through fn_eq_m are similar. Binders in the patterns are lifted to implicit arguments fn_eq_x. These equalities 100% specify the behavior of the function. Getting from them to what you *want* to prove is the work of lifetimes, but fully specifying a particular implementation is as easy as writing it.
i think this setup needs updates..
I think we probably need the equiv to ACID for CSS. Once some has a publicly visible conformance suite that people are making references too then things will get better.
Very nice. Cool technique, good exposition. Also, I'm seeing this right after encountering [Ghosts of Departed Proofs](http://kataskeue.com/gdp.pdf), which someone posted in this sub a few days ago, and which also features a nice use of phantom types. I think they both give a relatively lightweight (API wise) option for added type safety. 
\`Set\` itself has such an \`Eq\` instance. And it's the \`Eq\` instance we virtually always want for \`Set\`! Two sets are equal if they have the same elements. But \`Data.Set\` exposes several functions that expose structural differences - \`showTree\` can maybe be handwaved as "for debugging so it doesn't matter" but \`splitRoot\` will also give different results for equivalent \`Set\`s. &amp;#x200B; What would be really interesting is some way of talking about different equalities and tracking some guarantees about what equalities are respected, although I've no idea whether it'd pull its weight from an engineering standpoint.
I don’t think anyone is claiming to remove partial functions completely, just that having them imported by default makes them way too easy to use without much thought. Partial functions require care and should require some combination of an explicit import or some sort of suffix/prefix indicating their unsafety, ideally I would prefer a `!` suffix if we had a different lexer.
Personally I’m more of a fan of `Maybe` than default values. I can always trivially convert a `Maybe` to a default with `fromMaybe`, but the converse is impossible, leading people to use things like `Just &lt;$&gt;` beforehand. Ideally these functions would be in type classes like Foldable so they work on Seq and Vector as well. 
Unsafe to me implies UB (unsafePerformIO/unsafeCoerce/unsafeHead) when used incorrectly. So I’m glad it’s not called `unsafeHead`, I wish we could add a `!` suffix but unfortunately Haskell lexer doesn’t work that way. 
I always appreciate standard test suites. Not sure the best way to put that together, though.
&gt; It's not the Eq instance that is the problem here. I disagree, it absolutely is the `Eq` instance that is the problem &gt; Specifically, you should only be able to say `fmap f :: Set a -&gt; Set b` iff you know that `forall x y. (x == y) =&gt; (f x == f y)` But that's [a law of Eq](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Eq.html) and thus its perfectly reasonable to expect that.
I think this: scheme :: Int -&gt; Int -&gt; (Int -&gt; Int -&gt; Int) -&gt; Int -&gt; Int scheme baseCase0 baseCase1 recCase = aux where aux 0 = baseCase0 aux 1 = baseCase1 aux n = r1 + r2 where r1 = aux (n - 1) r2 = aux (n - 2) is supposed to be: scheme :: Int -&gt; Int -&gt; (Int -&gt; Int -&gt; Int) -&gt; Int -&gt; Int scheme baseCase0 baseCase1 recCase = aux where aux 0 = baseCase0 aux 1 = baseCase1 aux n = recCase r1 r2 where r1 = aux (n - 1) r2 = aux (n - 2) &amp;#x200B;
Thank you. I still think that the substitutivity (negation is valid for `_ (==) _ = True`) is too struct requirement and excludes many practical cases, including the Set applications. The [Set documentation](https://hackage.haskell.org/package/containers-0.6.0.1/docs/Data-Set.html) still says "equality is an equivalence relation instead of structural equality". It is even more funny that `Ord` is still allowed to be partial order which is known to blow up sorting algorithms and probably would not work with `Set` as well.
That is a good response I admit. But for my taste there are too many people redesigning weels, and less ones working in harder unsolved problems, like regenerative braking in order to run 500 miles without recharging.
Since you're using Stack, it's pretty easy. Just add the path to `hawler` under `extra-deps` in your `stack.yaml`. For instance, if the project you're using it from is in `C:/Dev/Projects/foo`, you'd add this to `foo/stack.yaml`: ```yaml extra-deps: - ../hawler ``` I *think* this should work even on Windows, although I don't have a Windows machine on hand to try it. If that doesn't work, Stack also supports pulling a dependency directly from a Git repository, so you could upload `hawler` to GitHub/GitLab and do something like this: ```yaml extra-deps: - git: git@github.com/&lt;your-username&gt;/hawler.git commit: &lt;insert SHA here&gt; ```
Are we talking about the same thing? I looked through the other comments and it looks like you posted some code that shows you do support it: ex. If I have a: \`\`\` data Record = Record { prefix\_foo :: Int } \`\`\` &amp;#x200B; and the bit of JSON I'm decoding looks like { "foo": 10 }. &amp;#x200B; can I not write a Serialise instance like: \`\`\` instance Serialise Record where bundleSerialise = bindleRecord $ const $ buildExtractor $ do prefix\_foo &lt;- extractField "foo" pure $ Record {prefix\_foo} \`\`\` &amp;#x200B; ? &amp;#x200B;
This is not Stack Overflow, I would try posting here: https://stackoverflow.com/questions/tagged/haskell
That to me is not a satisfying explanation at all. In the `Writer` case if you have are using a lawful `Monoid` then you have a lawful `Monad`, if you are using an unlawful `Monoid` then you have an unlawful `Monad`. In the `Set` case if you are using a lawful `Eq` then you have a lawful `Monad`, if you are using an unlawful `Eq` then you have an unlawful `Monad`. The fact that one of the instances is explicitly mentioned in the instance vs in the fundamental properties of the data type itself is not significant. In both cases the instance resolution is done behind the scenes without anything explicit being done by the user.
That could very well be true. I don't feel replacing Prelude a compelling task.
&gt; while Eq has no laws, only 'customary' properties. As of now [Eq](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Eq.html) kind of does have laws, even if the documentation hedges a bit by saying that the report itself doesn't define laws for Eq. I do also know that a wide variety of uses of `Eq` DO rely on substitutivity.
/r/haskellquestions might be good, too.
Use pattern matching and recursion: findA [] = ... findA (x:xs) = if x == 'a' then ... else ... 
Thank you! It's tough to figure out what needs to be done in your cabal file and what needs to be done in your stack file!
I think the point is that `Set` is only a valid monad if *all* `Eq` instances (even ones not written yet) are valid whereas the validity of `Writer v` depends only on `v` being a monoid – it is irrelevant whether other `v`s are valid monoids.
`Set` is a valid `Monad` if the types you use it with have lawful `Eq` instances, `Writer v` is a valid `Monad` if the types you use it with have lawful `Monoid` instances. 
I just keep ghcid running and use typed holes a lot. 
[Somewhat related](http://www.jucs.org/jucs_10_7/total_functional_programming/jucs_10_07_0751_0768_turner.pdf).
What have you tried? Are you aware that `String ~ [Char]`? Have you used `foldr` or some other process to deal with lists?
Well `String`is still implemented as a linked list of 32 bit characters and that has not yet been changed.
Seeing Scala beside Haskell makes me appreciate Haskell even more
 magic = hylo f g . (,,) 0 0 where f (F k t) = print k &gt;&gt; t f N = return () g (_, _, []) = N g (i, _, ([]:s)) = g (succ i, 0, s) g (i, j, ('a':t):s) = F (i, j) (i, succ j, t:s) g (i, j, (_:t):s) = g (i, succ j, t:s) data F x n = F x n | N deriving Functor Note: This is a working solution, but intentionally obfuscated (including introducing poor style). Please don't ask homework questions, please ask questions about your homework.
This can be made a bit more ergonomic with type application for specifying the phantom types.
Dude, you make FPComplete look like a sh*tshow.
I don't see how ``` fromFlag' @ShowBuiltins x where fromFlag' :: Flag t -&gt; Bool ``` is more ergonomic than ``` fromFlag ShowBuiltins x ``` --- In fact the latter is **better**, as it requires you to write the tag. It's redundant for a reason. In many cases the `t` variable in `fromFlag'` (or similar `toFlag'`) could be solved by type-inference. But I **require** an explicit tag, so one have to think what one is doing. ``` doSomething :: Flag Foo -&gt; Flag Bar -&gt; IO () doSomething bar foo = do if fromFlag' bar then doSomethingWhenBar foo else doSomethingUnlessBar foo ``` That will fail (because of `doSomethingWhenBar` signature), but if we write ```haskell doSomething bar foo = do if fromFlag Bar bar then ... else ... ``` GHC will complain more, and "sooner".
Thanks!
You're right. I usually see this with nullary phantom types and I got mixed up.
That's odd, because we have had violations of substitutivity in the platform (if not base) since forever. Set and Map both expose a least one function that can expose differences in insertion order. I'd say most implementations of abstract data types (in Haskell and other languages) will often compare equal "as Xs" (where X is queue or set or whatever abstract type), i.e. even when they are not structurally equal even if/when they expose functions specific to the implementation (not part of the ADT) that can reveal their structure. I'm all for `Eq` being reflexive, symmetric, and transitive "by law", but I think substitutive (conguent?) is a bridge too far. (Note that `Double` and `Float` have non-reflexive instances `let x = 1/0*0 in x == x`, so even that ask isn't entirely without tradeoff. [I vote for removing the Eq instances from `Double` and `Float`.])
I hope FPco pays you well. 
I like this approach and use something similar (not specialised to Bool, but I think that's its most common use case). You can do it with type applications instead of funny variables. It's not obvious how, but someone here showed me a free months back: https://www.reddit.com/r/haskell/comments/9t0p5n/monthly_hask_anything_november_2018/e8ucb0g That also lets you use type-level strings for your phantom types, which is less boilerplate and less to import (but slightly uglier to use). And as a really petty thing, it's possible to implement using GADTs such that you only have one element in your import list (i.e. I do `import Util.Tag (Tag(..))` and get the type `Tag`, a constructor `Tag` that I can't misuse, and a deconstructor `untag` that I can't misuse.)
When I clicked on a link starting with "Oleg" I thought to myself, "I'm not going to understand any of this." Not that Oleg though.
``` tagged :: forall tag tag' a. ((tag == tag') ~ 'True) =&gt; a -&gt; Tagged tag' a untag :: forall tag tag' a. ((tag == tag') ~ 'True) =&gt; Tagged tag' a -&gt; a ``` is indeed clever. And it requires `AllowAmbiguousTypes` (which is the point). --- `newtype` is `Bool` in memory, GADTs would create a box. Probably not an issue in practice (at least for command line flags), but who knows. --- About `Symbol`. I'd say it's two-edged sword. Yes, there are less boilerplate. But on the other hand, `Flag "verbose"` from library `foo` is the same as `Flag "verbose"` from library `bar`. Yet, `Flag Foo.Verbose` and `Flag Bar.Verbose` are different. Anyway, your approach would work with `PolyKinds` so users can choose between `Symbol` or (empty) `data` types.
I don't think you can chalk up wanting to replace `String` with `Text` or the like to be any sort of intoxication with philosophical-mathematical ideas.
All good points. I use it in an application, so conflicting tags isn't really a problem I expect to have, but the tradeoffs would be different for a library.
Indeed, that's fixed. Thank you very much!
&gt; even if/when they expose functions specific to the implementation (not part of the ADT) that can reveal their structure. In the `Eq` documentation it specifies that substitutivity only applies to "public" functions, and things that reveal the internal structure of abstract types are never really considered public. &gt; I'm all for Eq being reflexive, symmetric, and transitive "by law", but I think substitutive (conguent?) is a bridge too far. I personally completely disagree, the main time i'm using `==` generically without knowing the exact type (and thus care most about the laws) is when I am specifically targetting substitutivity. For example checking whether or not to rerender in libraries like Miso, or checking whether or not to send a diff over the network. &gt; (Note that Double and Float have non-reflexive instances let x = 1/0*0 in x == x, so even that ask isn't entirely without tradeoff. [I vote for removing the Eq instances from Double and Float.]) I vote for just having a non-IEEE `Eq` instance with `NaN == NaN`, although `Ord` should probably be removed / replace with some sort of `POrd` superclass.
I think we're just talking past each other. I thought you asked &gt; What makes `Set` different from `Writer` that `forall v. Monoid v =&gt; Monad (Writer v)` but not `Monad Set`? I answered &gt; Not `Monad Set` because standard Haskell's `Monad` is insufficiently nuanced, and it manifests in {this} manner. and I left off the &gt; But `Set` *is* a monad, on the subcategory of `Hask` where the objects are all `Ord`. bit because I thought that would be confusing. I think you thought I meant &gt; `Set` is not a monad because [unsatisfying explanation] so it's my fault for being cryptic. However, I do think your language is a bit imprecise. You keep drawing parallels between `Writer`/`Monoid` and `Set`/`Ord`, even though they are fundamentally different. I think your argument (what I think your argument is based on the clues you've given) that `Set` is a valid functor is actually the same as mine. Would you agree? &gt; The functor implementation for `Set` takes the form &gt; &gt; fmap_Set :: (Ord a, Ord b) =&gt; (a -&gt; b) -&gt; Set a -&gt; Set b &gt; &gt; `Set` is not a `Functor`, which is a Haskell typeclass that is locked down to representing only endofunctors on the category `Hask` of Haskell types and functions. It is a *functor*, in the broad mathematical sense, and an endofunctor at that. The category it operates on is the subcategory of `Hask` where all the objects are in `Ord` (no restrictions on the arrows). &gt; &gt; The mathematical `fmap`, for any functor between any two categories, has a signature something like &gt; &gt; fmap :: forall (a :: Obj C) (b :: Obj C). (a -C&gt; b) -&gt; (F A -D&gt; B) &gt; &gt; In the case of `F = Set`, the "`Eq`-ness" of `a` and `b` are part of their `Obj`-ness; they are (lawfully) `Eq` because they are objects of the aforementioned subcategory of `Hask`. Therefore, a proof of the functor laws can assume a lawful `Eq` for the type arguments to `fmap`. &gt; &gt; In order for a type to be lawfully `Eq`, if `(==)` is not structural equality, there cannot be any ("public") functions that discriminate values `(==)` considers equal. It's a bit of a weird law—the restriction is more on everything else *but* `(==)`—but some things in life are just a little funky. Due to the existence of `getBad`, `Bad` is not lawfully `Eq`, therefore the counterexample that `fmap getBad . fmap Bad /= fmap (getBad . Bad)` is invalid.
Ah yes we are talking past each other. My point was mainly that if there was a way to make `Set` a standard Haskell `Monad` (there isn't), but where invalid `Eq` instance would violate laws, that it would be perfectly OK with me as the same happens with `Writer`. Admittedly not a super important discussion as we can't make it an instance anyway, but basically I was voicing my disagreement with [this](https://www.reddit.com/r/haskell/comments/b3ye68/why_arent_sets_etc_implemented_as_gadt/ej35rjr?utm_source=share&amp;utm_medium=web2x) originally comment that claimed we *shouldn't* make it an instance.
`Double` and `Float` don't have substitutive `Eq` either: &gt; let x = negate 0.0 &gt; let y = 0.0 &gt; let f = recip &gt; x == y True &gt; f x == f y False --- &gt; things that reveal the internal structure of abstract types are never really considered public Then why export them from the main module? Most of them aren't even isolated into an .Internal module, which is **still** part of the *public* API. Or non-public function do you mean any off a potentially infinite list of functions that cause problems for your argument but that you won't enumerate?
What is the right way to marshal Data.Text strings to and from a C FFI? I want to support multiple encodings e.g. utf8, utf16, latin1. I took a peek at what Data.Text.Foreign does (only supports utf8): peekCStringLen :: CStringLen -&gt; IO Text peekCStringLen cs = do bs &lt;- unsafePackCStringLen cs return $! decodeUtf8 bs So I copied this, but I seem to get a lot of issues (double free, bad values) which makes me wonder if there is a better way. 
&gt; `Double` and `Float` don't have substitutive `Eq` either: I don't consider them particularly relevant to our discussion, as those types are all kinds of problematic, be it their `Eq` instance, `Ord` instance, `Monoid` instance, `Enum` instance or their `Num` instances. &gt; Then why export them from the main module? Most of them aren't even isolated into an .Internal module, which is still part of the public API. All the ones in `containers` are absolutely not in the main module, they are in `Internal`. And regardless of `Internal` sort of being a part of the public API (not sure I fully agree), everyone is fully aware of the fact that improper use of `Internal` (or `seq`+`undefined` or `unsafeCoerce` etc.) can break various typeclass laws. &gt; Or by non-public function do you mean any off a potentially infinite list of functions that cause problems for your argument but that you won't enumerate? I basically just mean every function that requires `Internal` or `unsafe*` to define. Anything from the public module + everything derived from such is perfectly fine.
Copying out something I said [deeper in the tree](https://www.reddit.com/r/haskell/comments/b3ye68/why_arent_sets_etc_implemented_as_gadt/ej5cmlx/?st=jtkkc8cq&amp;sh=f2a59973): &gt; The functor implementation for `Set` takes the form &gt; &gt; fmap_Set :: (Ord a, Ord b) =&gt; (a -&gt; b) -&gt; Set a -&gt; Set b &gt; &gt; `Set` is not a `Functor`, which is a Haskell typeclass that is locked down to representing only endofunctors on the category `Hask` of Haskell types and functions. It is a *functor*, in the broad mathematical sense, and an endofunctor at that. The category it operates on is the subcategory of `Hask` where all the objects are in `Ord` (no restrictions on the arrows). &gt; &gt; The mathematical `fmap`, for any functor between any two categories, has a signature something like &gt; &gt; fmap :: forall (a :: Obj C) (b :: Obj C). (a -C&gt; b) -&gt; (F A -D&gt; B) &gt; &gt; In the case of `F = Set`, the "`Eq`-ness" of `a` and `b` are part of their `Obj`-ness; they are (lawfully) `Eq` because they are objects of the aforementioned subcategory of `Hask`. Therefore, a proof of the functor laws can assume a lawful `Eq` for the type arguments to `fmap`. &gt; &gt; In order for a type to be lawfully `Eq`, if `(==)` is not structural equality, there cannot be any ("public") functions that discriminate values `(==)` considers equal. It's a bit of a weird law—the restriction is more on everything else *but* `(==)`—but some things in life are just a little funky. Due to the existence of `getBad`, `Bad` is not lawfully `Eq`, therefore the counterexample that `fmap getBad . fmap Bad /= fmap (getBad . Bad)` is invalid. In a sense, because `Bad` is lawlessly `Eq`, it is removed from the domain of `Set` and cannot be used to "talk bad" about it.
&gt; All the ones in containers are absolutely not in the main module [Wrong](http://hackage.haskell.org/package/containers-0.6.0.1/docs/Data-Map-Lazy.html#v:splitRoot).
That to me indicates it should be documented as `Eq` violating and specify how you should use it to preserve `Eq`. I still stand by statement as to what things SHOULD be. Its similar to how fromAscList is documented to be broken if the list isn’t ascending. Really no different except it’s about how you use the output rather than what you give as input. 
So, you agree that `Eq` shouldn't be substitutive "by law"? Or you agree that abstract data types should freely ignore the `Eq` law and provide non-substitutive instances? Being substitive is a property of an `Eq` instance, not of a particular function.
&gt; Seriously non-substitive Eq breaks the assumptions of many libraries including vdom libraries and network diffing libraries. I really don't think it does. And, I would like an example, preferrably using a *reasonable*, reflexive, symmetric, transitive but *non-substitutive* instance (like `Set`'s). I believe I've backed up my assertions in this thread with examples.
If you remove the `yaml` tags from your codeblocks they will display correctly.
 ALTER TABLE animal ADD CHECK ( ( type = 'cat' AND cat_name IS NOT NULL AND cat_age IS NOT NULL AND cat_food IS NOT NULL AND dog_name IS NULL AND dog_owner_id IS NULL AND bird_name IS NULL AND bird_song IS NULL ) OR ( type = 'dog' AND dog_name IS NOT NULL AND dog_owner_id IS NOT NULL AND cat_name IS NULL AND cat_age IS NULL AND cat_food IS NULL AND bird_name IS NULL AND bird_song IS NULL ) OR ( type = 'bird' AND bird_name IS NOT NULL AND bird_song IS NOT NULL AND dog_name IS NULL AND dog_owner_id IS NULL AND cat_name IS NULL AND cat_age IS NULL AND cat_food IS NULL ) ); I guess this is where the question "you can, but should you?" comes up. The article is very good, but trying to map ADTs to a relational DM is a nightmare. This is the sort of thing that convinced me to switch to a document-database in my solution, and accept that I don't have all the nice joins (well, this an trying to encode lists... it's even worse with generics, since they're infinite sums).
I think that wouldn't quite work as-is since you're not keeping track of the index. As far as I can see this approach would require something like this: findA = go 0 where go [] = ... go (x:xs) = if x == 'a' then ... else ...
Very clever approach! I'll have to try this sometime if I run into a situation like this.
Yeah I would not use that approach unless they have relatively few constructors with relatively few fields. I just used it on a data model where the sum type was essentially `data T = A Int | B | C Char | D` and creating all the join tables wasn't worthwhile.
Doesn't intero do that automatically anyway?
This is what I use. It also works well with Windows, which is a plus. Still, I wouldn't recommend it to a beginner unless they already know Emacs and/or are interested in extreme efficiency and customization on Windows.
I haven't had to do it in a while :P so kind of rely on comments to point out outdated things. Do you see anything in particular?
Is Leksah still maintained? I tried it a while ago and don't remember it working very well. (BTW, the linked prebuilt packages are outdated now.)
I guess it's more of a philosophical question, but I do wonder why we try to squeeze our data into the relational corset when types of persistent storage which support a 1:1-mapping are available. I've really cooled to relational storage over the years. I get the theoretical advantages of it, but the constraint that each cell can only contain atomic data (0NF) causes no end of trouble. Normalizing is a bunch of busywork that ends up with an extremely brittle an awkward DB-schema that's hard to change and hard to use, whereas it should be the DBMS's task to do the indexing/joining for you in an automated fashion (just my opinion, though). There was a case where I had abstract generic base classes (this was C#), inheritance, generic lists, and sum types, and I ended up with over 20 tables which still wouldn't work with Entity Framework. I imagine it's even worse in Haskell (though you article made it look reasonably painless). The thought of trying to encode an arithmetic expression tree in the DB... it's murder. Do you have any thoughts on/experience with using JSON as a persistent storage?
This approach is [naive](https://www.reddit.com/r/haskell/comments/alrm5v/monthly_hask_anything_february_2019/efip3zf/), it does _not_ give you a free MyTypeclass unless MyTypeclass has no laws.
Almost every persistence problem I've run into has been quite amenable to a relational database. Relational databases also offer the most powerful and flexible querying, and the best ability to guarantee data quality and consistency. While I have run into things that a relational database isn't great for, most SQL databases also support JSON document storage or binary `BLOB` storage, and it is *usually* better to use these features of a relational database rather than spin up a whole new database (or, god forbid, use *the filesystem* directly). Postgresql's JSONB column supports fairly efficient queries and indexes, too, which is pretty awesome. The "Object-Relational Impedance Mismatch" is a huge problem in OO languages, but it's not so bad in functional languages. Sum types are the most awkward thing to represent, certainly, but they're not all *that* bad - these three approaches each work fairly well depending on the data you're representing. Ultimately, you've got a lot of domain concepts. These concepts have relations to each other, and certain operations or domain problems are easiest if you express the relationships in a certain way. So you write those data types, the operations fall out naturally, and away you go. Sometimes you need a different representation. So you write the conversion functions, then the new operations fall out naturally, etc. Storing to a relational database is just one of those things, but it permits an absurd level of flexibility for querying the data.
Java kinda has function types as-of Java 8, allowing you to write `Function&lt;String, Stream&lt;ParseResult&lt;A&gt;&gt;&gt;`. It's nasty though.
Alternatively, one could bundle up an IO action in the newtype, which would give the benefit of keeping the code to construct the record located I'm one place, at the cost of forcing the caller to invoke an arbitrary effect. I wonder if that might be better in more complex cases, where splitting up the construction code to different places might make it harder to understand
You describe the symptoms of a bikeshed: easy to have an opinion, relatively minor benefits (which naturally are difficult to compare). Though I don't mean to be dismissive of the thoughtfulness and good design that went into many of these libraries.
I thought me saying vdom/network diffing was basically sufficient for it to be clear what the issue would be. Anytime you have a type T with a non-substitute Eq instance, and therefore some `f` that can discriminate between “equal” values, you can trivially cause problems in any diffing-type situation. If you display `f x` anywhere in the vdom then if `x` changes in a way that affects `f x` but that is still “equal”, then the vdom will NOT update, but will instead maybe or maybe not update at some random time in the future depending on various optimizations the library uses. Likewise for diffing over a network, which could be particularly problematic if you are relying heavily on exact synchronization such as doing a bunch of pure stuff on both the server and the client with code sharing under the assumption that everything will always be equal. 
I'm not fully convinced that pushing the random number generation into the saving part is a good idea. If I jumped into a codebase and I saw a type class called SaveData, I'd expect it to do just serialization. Say call some JSON library. But it's actually running non-trivial computations? Feels weird. 😐
The following will give you a lawful free functor: ``` data Free c a = Free (forall r. c r =&gt; (a -&gt; r) -&gt; r)) ``` I provide it in https://hackage.haskell.org/package/free-algebras Together with instances which prove that's indeed a free functor.
You can totally do that, but the derived schema still has prefixes.
The default options are usually some sort of record. Here's an example using `aeson`: {-# language DeriveGeneric #-} data Person = Person { personName :: String , personAge :: Int } deriving (Generic) -- length of "person" personLength = 6 instance ToJSON Person where toJSON = genericToJSON defaultOptions { fieldLabelModifier = drop personLength } 
i agree
I agree.
It seems like it would be fairly easy to get an infinite list of UUIDs using a pseudorandom generator.
&gt; -- length of "person" &gt; personLength = 6 personLength = length "person" now you don't need a comment (which could get out of sync).
The diagrams library documentation describes one version of this approach: https://archives.haskell.org/projects.haskell.org/diagrams/doc/manual.html#faking-optional-named-arguments Roughly, each function taking options takes them as a record, and every such record has a default value defined in terms of a typeclass Default. This provides a `def` value for that record containing the default values. Then, two versions of every such function taking that record exist, one with all the default options and one taking the record of options.
This question would be just as unwelcome on Stack Overflow.
I definitely agree that it’s overkill, although “a whole webserver” isn’t actually very big or complicated—it’s just an intermediary between a program and a browser—and some inefficiency is to be expected when you have something that’s easier for the developer (compared to making a native UI) and it does have the advantage that the web UI paradigm is well understood by users (developers in this case). That’s why I think projects like `threepenny-gui` are really valuable, because they normalise this technique and encourage browser/OS developers to optimise it and make it “more native”.
To get indices, I usually just zip with [0..]. Much easier than passing some int yourself. 
This entire line of reasoning is a damning indictment of the current state of software development. We're just completely fine with doing the most complicated, bloated, and frankly stupid bullshit because we're too intimidated by the scope of what's necessary to apply any of the lessons we've learned in the last 30 years towards actually improving anything. If we spent a tenth of the collective energy that we spend layering abstractions on top of each other on trying to improve their foundations, these problems wouldn't exist, but nobody can put a dollar value on it, so it won't get done.
I thought the surfeit of Simons was a fluke but now the Olegs are congregating too. 
As an alternative, would it make sense to encode the default as a phantom type like in newtype Flag t (def :: Bool) = MkFlag Bool ?
Considering how it's good practice to do qualified imports I suggest changing WineryException and WineryRecord to simply Exception and Record. It's rather ugly to have Winery.WineryException littered in the code.
I think there are two other alternatives to this: - Make convert monadic - Add a rng dependency to the convert function Occasionally I wish for some type magic that automatically generates lifting functions like withDeps :: (HasDeps m l, SplitDeps f l r) =&gt; f -&gt; m r withDeps :: (..) =&gt; (Rng -&gt; Config -&gt; a) -&gt; m a But in this case this would mean constantly splitting the rng which'd presumably be bad for preformance.
I agree, how about using visible dependent quantification: fromFlag :: forall t -&gt; Flag t -&gt; Bool fromFlag _ (MkFlag b) = b This can be used as before, except that now the phantom type is passed at the type level instead of the value level.
I never thought of that! It seems much easier than the `go` pattern - I'll have to use it next time I need this.
I would like to see this in the Prelude, not in a Interlude or Textlude that almost nobody would use. If for frivolous reasons like including cool but almos useless clases like semigroups, haskellers are willing to break the entire ecosystem. It is much more justified to do it in order to redefine String as text.
What people usually miss here is the learning aspect. I think many libraries in the haskell ecosystem exist because people wanted to experiment with things. This is reinventing the wheels, but in a good way. I prefer this approach to the approach of taking everything for granted and just reusing existing building blocks as black boxes. Obviously those black boxes are also important and it is good to have high quality libraries which can be used for the real production stuff.
Out of curiosity as a noob, care to explain why you prefer cabal over stack?
Then `Flag ShowBuiltins 'True` and `Flag ShowBuiltins 'False` would be different types.
so from *Haskell98* to not-yet-released version of GHC ;)
My default is to encode flags as sum types data TopoOrder = TopoForward | TopoReverse deriving (Eq, Ord, Enum, Show) instance Default TopoOrder where def = TopoForward mostly because pattern matching feels nicer without the unwrapping step. Guess view patterns could solve this?
I’ll have to look at Alga… Without that context, I’m not sure exactly where you are going, but one place that immediately comes to mind where a relational model is transformed into a graph (of sorts) is in [entity relationship diagrams](https://en.m.wikipedia.org/wiki/Entity–relationship_model)(really, you usually go from the ERD diagram to the relational model). I’m not sure, however, if there is enough formalism in an ERD diagram to map it into a mathematical graph. Thanks for sharing.
I am using `servant-auth-server` to create a JWT based authentication scheme for an API and it works great. I now want to create another _realm_ of authentication to separate the notion of **admin** and **authentication**. From reading the `servant` docs I think this is possible, however I am struggling to figure out how to fit it into `servant-auth-server`. Any advice or a point in the right direction would be much appreciated!
Desktop link: https://en.wikipedia.org/wiki/Entity–relationship_model *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^246182
There's a better way you can find in this gist: https://gist.githubusercontent.com/chessai/a5f28486e2f001ccf657d964e706acf5/raw/04eef6a20bbf36322bc8a04095c4940da3be770a/aesont.hs
I remember doing something similar while [parsing(https://github.com/danidiaz/vdpt/blob/master/library/VDPT/Parser.hs#L28) database query plans, to delay giving ids to nodes. The numbering of the nodes was done later, using `mapAccumL`: numberNodeTree :: Tree (NodeId -&gt; Attributes) -&gt; Tree Attributes numberNodeTree = snd . flip mapAccumL 0 (\i f -&gt; (succ i, f i)) 
Cool idea. I'm not 100% sure what I'd use it for, but I'll keep it in the back of my mind!
Design was shamelessly "stolen" from rustup: https://github.com/rust-lang/rustup.rs/tree/master/www If you have any suggestions, find bugs or have better ideas for the design, please open a PR or ticket at https://github.com/haskell/ghcup/issues Also if you want to help on supporting more distros or even help with some CI issues like https://gitlab.haskell.org/ghc/ghc/issues/16134 help is certainly welcome. 
What is the advantage of ghcup?
It's a lightweight tool that installs and manages multiple distinct versions of ghc and cabal in a simple way, and does nothing else. Which is exactly what some people want :-)
Advantage over what exactly? Your package manager? That really depends on your distribution and how well GHC is maintained there. ghcup will always allow you to: 1. install the very latest GHC (and a lot of older versions) 2. have multiple GHC versions installed in parallel and easily switch between them 3. easily delete all traces of it by removing the ~/.ghcup directory If your distro can offer you 1. and 2., use your distro packages! Also check https://github.com/haskell/ghcup#design-goals
We’re planning to use it for certain exception throwing functions in our codebase. For example, we have a lot of places that look up a database relation that should always exist. For those cases we assume the row exists so we don’t have to do Maybe handling, and give a just-in-case should the row not be present. An automatically generated link to the line that threw the error is less tedious and more informative than the manual note though.
How likely is ghcup to work with msys2?
It would be interesting to compare with stack which does this on per-project basis. For example, in one project I have \`resolver: lts-13.12\` which uses ghc-8.6.4, in the other one it's \`resolver: lts-11.1\` which uses ghc-8.2.2.
stack and ghcup have vastly different design goals and scope. However, using different ghc versions in different projects is fairly easy with cabal, see: https://www.haskell.org/cabal/users-guide/nix-local-build.html#cfg-field-with-compiler
Thanks.
Thanks.
Does [algebraic-graphs](http://hackage.haskell.org/package/algebraic-graphs) not solve the save problem in Haskell without any unusual types?
But why? Why would you reinvent one of Stack central features which has been perfected there over the years and supports all platforms including Windows? Is this politically motivated?
That's pretty cool. I figured something like this existed or would exist sooner or later but this is the first I've seen it.
FWIW some people find the `Default` typeclass objectionable (due to it's lawlessness I guess? I'm not one of them) but the same technique can of course be accomplished just by exposing your own classless equivalent of `def`.
Alternative: stack can use ghcup to manage installing GHC versions and reduce their maintenance burden.
I agree that it's a trade off. You can't/don't want to encode everything in the type, but having a requirement that the given list is non empty is extremely common. I can only speak from personal experience obviously, but we've gotten a lot of mileage from such a simple type. 
How do you get to this from the Haskell homepage? Is this an orphaned link?
To be sure, CloudHaskell is just about communication between multiple Haskell processes *on the same (Linux) machine*? Why is it called "cloud"?
If the packages get broken, they will get rebuilt next time you try to compile anything that depends on them. Executables won't get broken. So usually it's no big deal. &amp;#x200B; BTW, you may try \`cabal new-install\` next time. It has a different set of trade-offs and avoids the warning that bothers you. 
People want choice. There will always be multiple options out there to install haskell and manage projects. Haskell has never been an opinionated language (hence the 1 million language extensions) and it never will be. This is the case for 99% of languages. the notable exception being golang where it’s main purpose is to be extremely opinionated. So to answer your question (why?): because people can and will.
I think 99% is too much. Is JS opinionated? Probably no. Are Python, C# and Java opinionated? Probably yes.
Thanks a lot; this is very interesting. However, I struggle to write a Show instance for the datatype you wrote above. Is my version not isomorphic to this? I believe the "forall r. c r =&gt; (a -&gt; r)" is just my "compileNumF" function. Sorry I'm not used to working with such abstract types.
Thanks for your response. In this case I didn't provide an Eq instance --- one would need to define it through compileNumF. This automatically fulfills the laws I can think of such as fromIntegral x + fromIntegral y == fromIntegral (x+y) As I said to the-coot, I think my NumF is isomorphic to the datatype he provided, but I am not 100% sure.
I don't get it; `Free Ord` is a free Ord, `Free Monoid` is a free Monoid, etc., but `Free Functor` isn't a free Functor, in fact it is ill-kinded. Are you using "free functor" in a non-conventional way, to mean that (1) it's a functor and (2) it can be used to create a free MyTypeclass for any MyTypeClass of kind `* -&gt; Constraint`?
Yes, I missed the part where anyone pointed out the other factors that made it better choice.
Why do some people hate milk but some love it?
why do some people hate brexit?
I was being hyperbolic. But also, I was referring to the way the language is installed/managed. I’m not sure about C#, but I know that you can install/manage python and Java is many different ways.
This is my take https://mobile.twitter.com/mattoflambda/status/1010941557413797889
Is there some package that collects Haskell string type under one class? What I need: `funcThatWorksWithDifferentStringTypes :: (StringType s) =&gt; s -&gt; a` I've noticed there are several packages that should do, but I can't get stack to build them. What am I doing wrong/ what should I do to get my wanted function?
Yeah. Not sure I’d go that…scathing with it, but I think we’re on the same page. It saddens me. I like the idea of things like Rust, and one of the languages I’m working on, which try to improve the ergonomics of “low-level” programming—in other words, using the hardware efficiently without needless layers of abstraction (even GC among them).
justified-containers seems relevant here, and there's discussion vis a vis alga here: https://www.reddit.com/r/haskell/comments/6qu04u/justify_your_existence_with_justifiedcontainers/
I think most Haskell users are either neutral or positive about Nix but you don't usually notice them. But as "bad news travels faster" the ["haters"](https://twitter.com/snoyberg/status/888002088600576000) are often more noticeable and might appear over-represented. 
In my opinion, Servant is by far the best REST framework. But it uses some more advanced type level features, so you have to get into it a bit (which is not an issue due to the good docs). For database I'd say either persistent or beam, I've worked more with beam and like it, but both have good docs and work well
have you had a look at spock or servant? do they fit the bill?
In my opinion, Servant is by far the best REST framework. But it uses some more advanced type level features, so you have to get into it a bit (which is not an issue due to the good docs). For database I'd say either persistent or beam, I've worked more with beam and like it, but both have good docs and work well
ghc and cabal are inherently more fundamental than stack. As cabal evolves (such as with Nix-style build) and/or as people compose other tools with it to subsume stack (such as Nix), stack has less to differentiate itself. The Haskell community is never going to converge on stack, so it makes sense to improve the things we all *have* converged on.
Usually you should try to design a library so that the types can guide usage and enforce any internal invariants you have (i.e. users shouldn't be able to break your library, and you shouldn't need to spend a lot of documentation text telling the user what they can't do with it, etc). Often this means hiding the data constructors (e.g. `Text` in the text package). But other users and library writers may have reasons to want access to these internal bits (e.g. to write an efficient hash function for `Text`), so you might export some helpful but dangerous utility functions or data constructors in an `Internal` module, either preemptively or after someone asks you. Often `Internal` modules will have an unstable API and so are more for library writers to use at their own risk. 
I've seen people hate Nix because it broke their tools (some which assumed stack, for instance). Although I'm pretty sure you can write a stack.yaml to get the stack CLI and nothing else if need be. Also, it's sometimes hard to see the value of Nix in medium-scale, Haskell-only projects. Once you have a project with various disparate languages, Nix becomes really useful for composing those different build outputs.
[Haskell's Yesod](https://www.yesodweb.com) seems to fit this bill quite nicely. While it's typically billed as a server-side-rendering framework, it's more than competent at easily serving up JSON APIs (cf. [this example](https://www.yesodweb.com/book/json-web-service)). Anecdotal example, but I have a friend who's currently using Yesod to build out JSON APIs for a project at work and considers it to be trivial plumbing (as it should be). --- Is there anything that the community can do to contribute more examples aside from the stuff presented in the Yesod Book that would be helpful for this?
Yesod and Persistent. They have the most complete documentation and examples. Yesod, despite having a lot of features for web pages, is a very capable JSON server library as well. These libraries are backed by FP Complete, which use them extensively in many real-world commercial projects. There's a good book written for these two libraries, and an active community of users that have solved many common and boring web programming problems. Servant is deeply complicated and can be extremely confusing and difficult to work with. I have used it extensively in production at several companies, and the complexity cost is pretty huge. With that said, if you're creating a stateless JSON API with very simple auth requirements, you don't need anything that doesn't have a library already, you *need* to write/maintain generated/derived clients, you *need* to have generated documentation, and you have a highly skilled Haskeller on board to solve problems related to advanced type level programming, then Servant is the best in class by a large margin. However, as you stray from those qualifications, it quickly lags behind other libraries in terms of productivity. I wrote the [`servant-persistent`](https://github.com/parsonsmatt/servant-persistent) example repository with guides, which you may find helpful if you choose to go with `servant`. I've also updated /u/pbrisbin's [`yesod-minimal`](https://github.com/parsonsmatt/yesod-minimal) example to show that `yesod` isn't some giant Rails-like framework. I'm also a maintainer on the [`persistent`](https://hackage.haskell.org/package/persistent) and [`esqueleto`](https://hackage.haskell.org/package/esqueleto) libraries, and I'm happy to help with getting them up and running :)
lactose intolerance?
Lactolerance. *** ^(Bleep-bloop, I'm a bot. This )^[portmanteau](https://en.wikipedia.org/wiki/Portmanteau) ^( was created from the phrase 'lactose intolerance?' | )^[FAQs](https://www.reddit.com/axl72o) ^(|) ^[Feedback](https://www.reddit.com/message/compose?to=jamcowl&amp;subject=PORTMANTEAU-BOT+feedback) ^(|) ^[Opt-out](https://www.reddit.com/message/compose?to=PORTMANTEAU-BOT&amp;subject=OPTOUTREQUEST)
No one hates nix. Nix is just too invasive to install that's all. Some people manage to install it. Other have a hard time installing it (me for example). That's all. If nix would have been as simple to install and use as stack I'd probably use it. &amp;#x200B;
Some of it is just familiarity. I learned cabal first. Some of it is a slight preference to work against the latest version on hackage, instead of waiting for a new stackage release. Most of it is that cabal plays well with the packages and GHC that I can get from my OS distributor, so I can use that distributor's support structure which I both trust more and am more familiar with than Stackage support. In my limited experiences, the Stack provides much easier, nearly semanless, experience as long you let Stack handle everything. If you don't... well then you'll *have* problems, but I find it easier to deal with those problems when I'm using cabal.
Well this answer clearly isn't helpful... OP is looking for reasons _why_ Nix is both beloved and detested.
Can you give me a specific -- rather than abstract -- example? I.e. instantiate your variables in the above with concrete values? My examples were quite specific. I'd like to witness *actual breakage* caused by non-substitutive `Eq`, since we do have several in base and platform package.
lol nix means nix
I don't understand why you're talking about `Eq`; you're trying to define a free `Num`, not a free `Eq`, so it is the laws of `Num` you should worry about. _looks it up..._ &gt; The Haskell Report defines no laws for `Num`. All righty then, if `Num` has no instances, then your definition is fine! ...wait, no, I have completely misread your definition! I thought you wrote this: data FreeNum a where Embed :: a -&gt; FreeNum a FromInteger :: Integer -&gt; FreeNum a Add :: FreeNum a -&gt; FreeNum a -&gt; FreeNum a Mul :: FreeNum a -&gt; FreeNum a -&gt; FreeNum a Negate :: FreeNum a -&gt; FreeNum a Abs a :: FreeNum a -&gt; FreeNum a Signum a :: FreeNum a -&gt; FreeNum a That would be a free `Num` in the sense that `FreeNum a` has a `Num` instance even if `a` doesn't, and also we can embed values of type `a` inside the type `FreeNum a`. What you have written instead doesn't have a `Num` instance because e.g. `Add` has the wrong type, `Add :: a -&gt; a -&gt; NumF a` instead of `(+) :: a -&gt; a -&gt; a`. So I think you are using the word "free" in a non-conventional way, and I don't understand what it is you are trying to achieve.
For a `Show` instance you'd need to assume both `Show a` and `c a` and then you can plug `id` to get the value the continuation represents: ``` instance (Show a, c a) =&gt; Show (Free c a) where show (Free f) = show (f id) ``` 
I'm a huge nix enthusiast but I can't really blame people for not liking it. The main reason being that nixpkgs (a standard library / package repo written in Nix) is a convoluted mess with a decade of technical debt. If you could rewrite nixpkgs from scratch I think you could improve the experience tenfold.
&gt; I would struggle to write a Show instance for the datatype you wrote above I don't think you can write it for an arbitrary `c`, but you can easily write it for e.g. `Free Monoid`: ab :: Free Monoid Char ab = Free $ \embed -&gt; embed 'a' `mappend` mempty `mappend` embed 'b' -- | -- &gt;&gt;&gt; show ab -- "('a')('b')" instance Show a =&gt; Show (Free Monoid a) where show (Free mkR) = mkR (\a -&gt; "(" ++ show a ++ ")") &gt; Is my version not isomorphic to this? I believe the "forall r. c r =&gt; (a -&gt; r)" is just my "compileNumF" function. Note that `forall r. c r =&gt; (a -&gt; r) -&gt; r` parses as `forall r. c r =&gt; ((a -&gt; r) -&gt; r)`, not `(forall r. c r =&gt; (a -&gt; r)) -&gt; r`, so there is no `forall r. c r =&gt; (a -&gt; r)` in `Free`. Furthermore, the type `Free` says that it is compatible with any `Num` interpreter out there, so the fact that you can write one particular interpreter for `NumF a` is not enough to conclude that `NumF` and `Free` are isomorphic.
Indeed that type will only model constraints of types of kind `Type`. Free functor in Haskell is hidden in `kan-extensions` package as `Coyoneda`.
That's not exactly the right approach, if I'm understanding you right. Administrator use the same authentication as other users; however the *authorization* system grants administrators different permissions / operations / access. Authn vs. Authz.
You can also make a higher-order version of `Free` for typeclasses of kind `(* -&gt; *) -&gt; Constraint`: data Free1 c a = Free (forall f r. c f =&gt; (forall x. f a -&gt; f r) -&gt; f r) But I bet you already knew that.
What type are you using on the C side that supports multiple encodings? It's much simpler to simply fix the encoding as UTF-8 and use Data.Text.Foreign.
There's very little overlap of appropriate use cases between the various types. Use the one that matches your use case; conversions are easy to put at the edges/seams.
You may not be able to. If you current have a package A compiled against package B version 1.0 which is itself compiled against C version 2.0, then you try and install package D that depends on B and C version 3.0, you'd have to rebuild B so that with will link properly with the new version of C, but that would break the compiled version of A. Even if you could find A (note that it's not in the graph of dependencies of D), recompiling it might not work, or you might need to use a different version of A that in compatible with everything in the dependency graph of D. Nix would solve this by having two differently compiled package that were both B version 1.0, but I don't think cabal will do that.
Just because the web framework has libraries to build html output does not mean it has no capacity to serve json. I use yesod for JSON rest apis. And it is very trivial to use. In fact many of my handlers are just 2 lines: fetch from database, and marshal whatever I fetched as json.
I'm confused by both of your paragraphs. You can install nix through `curl https://nixos.org/nix/install | sh`, and I think that debian also has a package for it. I believe you can install it without root permissions, too. Nix isn't easy to use, that's definitely true. What do you find deceptive and divisive about the post?
[https://icfp16.sigplan.org/event/haskellsymp-2016-papers-experience-report-types-for-a-relational-algebra-library](https://icfp16.sigplan.org/event/haskellsymp-2016-papers-experience-report-types-for-a-relational-algebra-library)
You can't do Nix halfway and it's easy to get discouraged if you don't think the benefit is worth the trouble. It totally is tho
You void the warranty if you depend on an `Internal` module. Always include Internal modules. Everything should be exposed because you can't predict everything that will be needed. They'll know what they're getting into.
&gt; Being substitive is a property of an Eq instance, not of a particular function. I completely disagree, you absolutely have to take into account what functions should be called on a data type / what invariants you are supposed to uphold / what things are considered "observable" rather than "debug-only" or "an implementation detail". For example `seq` + `undefined`, particularly in combination with a library like `spoon` makes a whole bunch of instances "unlawful". `unsafeCoerce` and `unsafePerformIO` also can cause similar issues. Do you think we should remove `instance Ord k =&gt; Monoid (Map k v)`? Because `Map` only obeys the `Monoid` laws if you don't use `splitRoot`: ``` mapIsTrueMonoid :: Map Int Int -&gt; Map Int Int -&gt; Map Int Int -&gt; Bool mapIsTrueMonoid a b c = splitRoot (a &lt;&gt; (b &lt;&gt; c)) == splitRoot ((a &lt;&gt; b) &lt;&gt; c) quickCheck mapIsTrueMonoid *** Failed! Falsifiable (after 31 tests and 20 shrinks): fromList [(0,0)] fromList [(1,0)] fromList [(-1,0),(2,0)] ```
I thought you were going to say "ACID for databases" and my eye started twitching.
Check out Relude's classes that provide toStrick, encodeUtf8, etc.
&gt; Do you think we should remove instance Ord k =&gt; Monoid (Map k v)? Because Map only obeys the Monoid laws No. I don't think the associativity law is very important. I don't think we should drop it, because I want Monoid and mathematical monoids to be a strong analogy. But, I generally only need a unitial magma or just a plain magma -- and there's no `Magma` or `UnitialMagma` so I use unlawful (specifically, non-associative) Semigroupoid or Monoid instance whenever I care to.
I don't see why one would use "haters" to talk about commentary like that in those tweets, regardless of the scare quotes.
I guess as long as you aren't submitting packages to hackage with non-associative `Monoid` instances then you do you. If you aren't in favor of dropping associativity of `Monoid`, then why do you seem to be in favor of dropping substitutivity of `Eq`? It's basically the exact same situation, where morally pretty much everything obeys it, but certain functions can be used in sneaky ways to expose "violations" of it.
I disagree. E.g. `unordered-containers` doesn’t expose internals, and everyone seems to be happy. Rather, hide implementation details. And if something is not possible via public interface, people will report. Also, as a user, if I need a feature, as a quick solution I vendor the library (it’s relatively easy with all: cabal, stack, nix). And then contact the maintainer to find a way to extend public API. If you expose all internals, and because people are lazy, they will depend on the internal bits, and in worst case: don’t tell you about missing pieces in public API. As an anti-example I can mention `zlib`. Virtually every non trivial user needs to depend on `Internal` module. It’s not internal, it’s “low-level”.
One alternative I've thought of was to have an api like \`foo :: State kwds () -&gt; args -&gt; returnVal\`, with \`foo\` internally providing its own default value of \`kwds\` and using the provided \`State kwds ()\` to modify it, giving \`default :: (Applicative f) =&gt; f () = pure ()\`. It has the advantage that you can have multiple functions which use different default values of the same type, \`default\` is no longer lawless, plus using \`State kwds ()\` rather than \`kwds -&gt; kwds\` gets you lens update syntax, though that's a pretty minor point. &amp;#x200B; Probably the biggest downside is you would still likely need to export the default value as a separate identifier so that library users could actually see what they are, since it should be documented, but the documentation could get out of sync (ideally you would want the default value to be automagically inserted as part of \`foo\`s documentation).
Look at his account name, how old it is, and what did he post and in what context here on /r/haskell. Believe me, it is not as innocent as you think. Also the wording he chose clearly shows bias. Most of us here are not ideological warriors. We are just engineers who use the tools that are at our disposal. None of us hates nix, because none of us uses it. &amp;#x200B; As fo the nix installation, back when I tried it it was not easy at all. In fact it did not even work on my distro (arch). I simply do not have the time to fight with the tool, nor I have any need to come back to it when stack does everything I need and gets out of my way.
Ha!
What do you see as the main improvements that would be made in a rewrite? The impression seems to be that dynamic typing and turing completeness encourage poor lib design, rather than it being merely a historical accident. I have less issue with turing completeness and `fix` specifically but definitely feel guided away from good architecture by the language
One of the reasons I now keep a nixpkgs submodule in my config is so I can `rg` for all those idioms and lib functions. Glad I'm not the only one. Nix the language (and, more significantly, the libs in nixpkgs) has improved a ton in the last year, but it still takes a while to get comfortable with nontrivial things. I still have a very poor sense of scoping (do I want self, super, pkgs?), but my config is much nicer now that overlays are so good.
&gt; How do you ghcup is recommended at the haskell platform page https://www.haskell.org/platform/#linux-generic
I don't think those are good arguments against exposing internal modules. &gt; E.g. unordered-containers doesn’t expose internals, and everyone seems to be happy. unordered-containers is a widely used package that has had time to stabilize its interface. Internals are much more useful for newer and less maintained libraries. &gt; If you expose all internals, and because people are lazy, they will depend on the internal bits, and in worst case: don’t tell you about missing pieces in public API. That doesn't sound realistic to me. I can understand laziness leading to misuse of a badly documented feature, but internals are very explicitly not meant for regular use. Could I not use the same argument to say: "If you expose `unsafeCoerce`, because people are lazy, they will use `unsafeCoerce`"? No, people won't do so, because it says "unsafe" on the tin, and there are commonly accepted benefits to not using unsafe stuff. &gt; As an anti-example I can mention zlib. Virtually every non trivial user needs to depend on Internal module. It’s not internal, it’s “low-level”. I don't know what to say to this. As you note, there is a difference between "internal" and "low-level". Now that it does mean "low-level" for zlib, it's a non-example. Was that module originally meant to be "internal"? The distinction between "low-level" and "internal-don't-use-this" may be a bit unclear, this can be addressed with explicit notices about the purpose of internal modules for your package. Similarly, maybe some newcomers to open source don't realize they're supposed to report stuff missing from the non-internal interface: then you can add a sentence about it in the docs. It's no use worrying about people who still won't report after being told to. Exposing internals allows people to do strictly more than without, and there is a very clear boundary to prevent misuse. If people are still willing to cross that boundary, that's their responsibility. The only case against this practice is if it is actively harmful in some ways. Maybe it's a bit of clutter, but so far I find it bearable.
Shouldn't lines like: -z "${BOOTSTRAP_HASKELL_NO_UPGRADE}" Actually be: -z "${BOOTSTRAP_HASKELL_NO_UPGRADE-x}" So that uses such as `BOOTSTRAP_HASKELL_NO_UPGRADE= ./script.sh` work (a test for definedness instead of test for non-empty).
"learn you a haskell for greater good" if you have excess free time and enjoy learning new syntax piecemeal with no high-level explanation of why or how it is applied in the real world.
Why do you say "with very simple Auth requirements"? I've used Servant in multiple production code bases with different mildly complex authentication schemes and I never thought Servant was an obstacle. Though to be fair, you do need to be even more comfortable with the type-level stuff than regular Servant use.
I think that's a bit unfair. I learnt Haskell from LYAH and found it a great resource. It works particularly well when followed by [What I Wish I Knew When Learning Haskell](http://dev.stephendiehl.com/hask/), an overview of the Haskell ecosystem (and which is honestly one of the best resources I've found in any language). If you read those two resources in that order, that should give a good grounding in Haskell.
Thanks for taking the time to respond in detail. I can see `cata` is clearly isomorphic to `foldr` on lists. thanks to your explanation! I'm still not convinced this applies in general though. Take for instance a binary tree (a fine analog to the needlessly specific AST I've been working with recently): data Bintree a = Leaf | Node a (Bintree a) (Bintree a) With the corresponding base functor: data Bintree r a = LeafF | NodeF a r r 
&gt; unordered-containers is a widely used package that has had time to stabilize its interface. Internals are much more useful for newer and less maintained libraries. Yet none of its versions have ever had any `Internal` modules. &gt; Could I not use the same argument to say: "If you expose unsafeCoerce, because people are lazy, they will use unsafeCoerce"? `unsafeCoerce` is not internal, it's part of **stable** (but *unsafe*) API. There is a crucial difference. It's exposed so people can use it when they need to, but it's part of **public and versioned** API. Internals can change without notice; `unsafeCoerce` won't in minor `base` bump. &gt; I don't know what to say to this. As you note, there is a difference between "internal" and "low-level". Now that it does mean "low-level" for zlib, it's a non-example. Was that module originally meant to be "internal"? Exactly as with `unsafeCoerce`. As a library author, you **have to** think what's the interface you want to expose. You can expose unsafe features, they are not internal. --- I don't remember anyone actually changing internals drastically anyway; if they did, it resulted in major version bump anyway. Having major version **is not an issue**, if people comply with a versioning contract we have. So one may expose every bits in `Internal` or whatever module, but please *make it part of public and versioned* API. --- So TL;DR make it clear what modules are part of versioned API. I argue that all public modules should be.
&gt; BOOTSTRAP To me (in terms of user interface), setting a variable to an empty string is equivalent to "unsetting" it. But anyway, this variable is not documented and was introduced for the travis build. While ghcup is strictly non-interactive, the bootstrap-haskell script very well can be. So this could instead be made interactive: do you want to install GHC? do you want to upgrade? do you want to install cabal-install? ...
I’m reading Haskell from First Principles. Super easy to read and the exercises complement the chapters well. 
You answered it :) I've done the same, even implementing custom authentication before `servant` had anything built-in. It's just a ton of complexity and difficult type-level programming that usually doesn't make me more productive.
Seconding [Haskell Programming from First Principles](http://haskellbook.com/). I've helped several people become comfortable and productive with Haskell using this book as a primary resource.
[string-conversions](https://www.stackage.org/package/string-conversions) should work.
Learn you a haskell is pretty nice.
I use nix and Haskell, but have at times found the nix advocacy seen here quite off putting. Standard tech advocacy problems.
I didn’t know that, I’ll read more about it, Thanks.
Im using scalpel-core and wreq to create some scraper. I thought it’ll be good to have my functions generic. Maybe that’s not actually needed after all. 
Strong typing would be good but the NixOS module system does a good enough job emulating one that I don't find it a *killer* mistake. Don't get me wrong it's not nearly as good as a real type system, but it's good enough for nix's purposes. Recursion encouraging poor lib design doesn't really make sense to me; to me it's like saying if statements encourage poor lib design. The way nixpkgs uses open recursion is incredibly useful and to my knowledge would be impossible or disastrously complicated to do provably totally. The main improvement would be replacing all the ad-hoc idioms with the NixOS module system. Partly as a shim for the lack of type system, but also because it would be a drastically simpler, more declarative, and self documenting system.
I hate nix!
I have been wondering about this too. Other communities such as Rust and OCaml seem to not have the notion of Internal modules, they just hide the details and expect you to either file an issue or deal with it.
Why? Are you forced to use it? If it is by your own choice, why hurt yourself? 
Graham Hutton’s book _Programming in Haskell_ is the most balanced introduction to the subject. I like it more than the _First Principles_ book, for most readers. I appreciate the enthusiasm of LYAH but it’s not good. 
Thank you, will go with this 
The question isn't helpful either. *Everything* us both beloved and detested. 
Sanity. 
&gt; `unsafeCoerce` is not internal That's not the point I wanted to make. I was objecting to the argument that internals are bad to expose because lazy people will misuse them, and brought up `unsafeCoerce` as an **analogy** of something that is exposed, yet people know not to use it. &gt; As a library author, you have to think what's the interface you want to expose. It's pretty clear cut to me. If I make a package `my-lib`, it typically has two modules: MyLib # Public and versioned API MyLib.Internal # Wild west, use at your own risk Of course I think carefully about what is exported from `MyLib` and how to organize it. But why is it a bad idea to *also* make the rest of the package available in `MyLib.Internal` to whoever might find it useful? I do not trust myself to foresee all the possible use cases of the code I write, and exposed internals are a frictionless way of allowing experimentation.
Haskell wikibook.
Don't forget that some are in between :)
Haskell from First Principle is probably the most up to date and comprehensive resource. I'm not a fan of the style of the book myself, since it's often very theoretical and makes you reimplement typeclasses all the time, but I also can't really think of an alternative. Learn you a Haskell is more of a sneak peak into the absolute basics and real world Haskell is a bit dated.
http://haskellbook.com/
`cabal new-build` and `new-install` will do that. Nix as well.
Long history of `==` operator not being substitutive in programming language contexts. Also, I see `Eq` as an equivalence relation (which is not necessarily substitutive), not as decidable equality.
On the flip side, JS and Python make it intentionally difficult or impossible to hide the internals (without putting them across and FFI boundary). Python says everyone is a "consenting audit"; if you use an "internal" symbol (leading single underscore), you take the responsibility to keep your came up-to-date with any changes to (including deletion of) that internal symbol. I think I'd like to see an ecosystem where API / ABI extraction and versioning is fully automated, and while you could certainly make the system take conventions like leading sing underscore or .Internal sub-module(s) into account, I think the guarantees you get are stronger when you ignore such conventions an work from the true, "raw" syntax and semantics of the language. But, I'm also practical most of the time, so I'll take whatever works well.
Excellent choice.
 I've heard it suggested that if your library ends up needing to expose an "Internal" module to provide a user with some extra feature, it probably means your internals should be spun off as a separate package which the first package depends upon. 
From my experience: - Nixos is nice in theory but it's hard to find up-to-date help and you don't have something like launchpad so you have to manually keep niche packages up to date - nix with stack is great but breaks on windows because of path issues - nix standalone is nice if you mix a bunch of languages but it is hard to learn and lacks tooling/autocompletion/documentation so setting stuff up is timeconsuming and error prone
Just to get my data point in as well, I have been bitten more than once by having to program around the library author trying to protect me. And filing an issue, waiting until the problem has been addressed or submitting a PR yourself, will *still* force you to jump through hoops, since now you have to depend on a specific version of the library, or a specific git revision, or your own fork if you don't have time to wait for days until the author has responded. Depending on an Internal module could have been supposed to be an intermediate solution anyway - you import it, implement some prototype, and then you can still go the "official" route but at least you have something working in the meanwhile. Another thing: Being able to import a data constructor to do some printf debugging has saved me a lot of time. I mean what, should I have done "equational reasoning" instead? You can of course make your API surface just flawless, and split off separate packages and whatnot, but you still have to start off somewhere. Show some humility, accept that your library won't be perfect, and trust people to not shoot themselves in the foot with the internals.
That's a hoot. I'm also a noob, reading LYAH right now while teaching it to a dozen or so teenagers. It's a relatively fun read, and the kids are learning some FP concepts while managing to write useful programs. From my point of view, though, I feel like my understand of the heart and soul of Haskell is very superficial. I'm grabbing some of the other references people have listed here and am looking forward to continuing my study long after I stop teaching it to the kids. &amp;#x200B;
It will distroy tjeir livelihoods? Will force huge disruption in their professional and private life? May reignite sectarian tensions and lead to terror attacks? Will force them to move to another country since their company will move abroad? On the individual level any weakening of UK/EU, or even UK/third party relationships will touch some deeply. But how does that matter to current discussion?
Surely, Rust and OCaml are much closer to Haskell than Python or Javascript :). Is this some kind of "circular political spectrum" observation?
That's true, one can do that, though it's not free construction any more, but one can fix that :). The following would work for `c` for higher kinded types (i.e. `* -&gt; *`): ``` newtype Free2 c f a = Free2 { runFree2 :: forall g. c g =&gt; (forall x. f x -&gt; g x) -&gt; g a } ``` This is isomorphic to `Coyoneda` via: ``` toCoyoneda :: Free2 Functor f a -&gt; Coyoneda f a toCoyoneda (Free2 f) = f liftCoyoneda fromCoyoneda :: Coyoneda f a -&gt; Free2 Functor f a fromCoyoneda (Coyoneda ba fx) = ba &lt;$&gt; (Free2 $ \g -&gt; g fx) ``` The continuation has to be in the right category, for functors these are natural transformations, e.g. maps of type `forall x. f x -&gt; g x`, that's why this type appears in `Free2`. You can even define a free category in this continuation passing style: https://hackage.haskell.org/package/free-category-0.0.2.0/docs/Control-Category-Free.html#t:C
That's the definition I was _trying_ to write; I'll stop commenting on this post before I further embarrass myself with mistakes :)
Design issues aside, you have to add the package under `dependencies:` inside your `package.yaml`. Personally I would regard this a Yagni case. If you really do need the increased generality, refactoring later should be pretty mechanical anyway. In the meanwhile, enjoy your type inference.
This is a highly debatable subject. I'm from the camp claiming that the Internals convention is a mistake and have even made [a post explaining why in detail](http://nikita-volkov.github.io/internal-convention-is-a-mistake/). You can also visit [the associated Reddit thread for discussion](https://www.reddit.com/r/haskell/comments/a0ioo3/internal_convention_is_a_mistake/). In short, whenever you feel the itch to expose some module as an internal one, you can interpret that as a signal to release it in another library, which the subject library will then depend on.
Is the `IsString` typeclass what you're thinking of?
Wow, you really don’t want people every hearing about this tool do you?
Wow, you really don’t want people ever hearing about this tool do you?
Some people aren't interested in re-learning how to package software from the ground up before they can use something from GitHub. Other people think solutions like Docker are good enough, especially because this doesn't require drastic changes to their workflow (they're wrong). Some people have been burned by misuses of Nix, insufficient documentation, and confusing failures. Even if you commit to using Nix, it takes quite a while before you see the payoff. I'm fortunate to have been part of team that hugely benefited from our investment, and I use it as much as possible both professionally and personally now.
For different reasons.
&gt; Believe me, it is not as innocent as you think. Also the wording he chose clearly shows bias. Most of us here are not ideological warriors. I'll just say that this is a curious statement given [your past combative comments](https://old.reddit.com/r/haskell/comments/auwlx6/another_idea_haskell_and_intellij/ehktkg2/?context=1).
I think you're right that Internal modules sometimes lead to a bad situation where necessary functionality is exposed but maintainers wash their hands of responsibility for it. There are also a lot of ghc's internal functions and various `unsafeFoo` functions scattered about that have the same issue: undocumented save for a "don't use this unless you're Really Smart". It's lazy and really not fair to users.
This *would* be great, but ghcup is Linux only. Stack can install ghc on Windows, Mac, Linux.
Agreed. We use nix at work, and I don't think we could be half as productive without it -- it glues together a complex polyglot system with many services and tests in a single sane way. On the other hand, I actually really hate having to do anything interesting with it myself, and find it very confusing. So I love and hate it both at once :-)
Servant isn't as bad as people say if you learn to "look into the sun" and parse the giant type errors that it craps out. Typically you just scroll to the top of the giant error (mostly a bunch of servant types) and it does give a clear message.
I was just comparing adding more languages to the discussion. I think they may be more relevant due to their larger size -- perhaps their approach scales better. I don't think Rust, Python, or Javascript are really close to Haskell. OCaml is historically closer, but I'm not sure that matters for the question.
Get Programming in Haskell is a bit more hands-on than First Principles, in my opinion, so that may be a good choice.
&gt; undocumented save for a "don't use this unless you're Really Smart". It's lazy Sometimes it reflects a general, universal lack of knowledge. They could be slightly more explicit, but basically it's a statement that "if you use this, and anything breaks, you get to keep both pieces". There are dozens if not hundred of places in both compiler internals and libraries that operating under the tacit assumption that `IO a -&gt; a` doesn't exist, for several different reasons. When you use `accursedUnutterablePerformIO`, you violate each of those, and no individual can tell you all the ways you program can now go subtly wrong. That's also true of things like `unsafeCoerce`. Even for "little" things like exposing the internals of a Fibonacci heap, while you are no longer tangled in the compiler internals, that **vast** majority of analysis on such an implementation of an abstract data structure is done under the assumptions of it's invariants. Internal modules may allow you to violate those invariants, violating those assumptions all over the place, invalidating most of all the existing analysis, leaving what anyone knows about the behavior very much impoverished.
I had to look it up in package I wrote :), but the thing to remember is that the continuation must be a morphism in the right category (e.g. functors, categories, etc).
Very interesting. What's the pattern for structuring modules in place of types? I could think of a couple different ways to organize stuff.
I'm referring to the module system described here: https://nixos.org/nixos/manual/index.html#sec-writing-modules Basically you can define options and associate type checkers with them.
I don't know how well-known this command is, a coworker told me about it today. Thought some people might find it useful.
https://www.reddit.com/r/haskell/comments/b489he/return_a_function_to_avoid_effects/
That's a good first book. Unfortunately we do not _yet_ have any intermediate-level Haskell books explaining how to use real world Haskell patterns \(applying monad transformers for instance\). Manning's upcoming [**Haskell in Depth**](https://www.manning.com/books/haskell-in-depth) seems to be promising in that regard. And there is [**Intermediate Haskell**](https://intermediatehaskell.com) however the release date for that one is uncertain. Until then one is probably better off learning these intermediate concepts [by doing](http://www.haskellforall.com/2017/10/advice-for-haskell-beginners.html). 
I think it is too limited. I'd want it to go back and forward in the history of each dependency and prove the range of versions that my package builds against (including tests). A local build matrix that results in more permissive, tested dependencies.
I was excited after reading this, since my attempts with Haskell plugins for IDEA like an year ago were bad (two plugins - one pretty good, but breaks randomly even when env. is not changed and second one [I think that was intellij-haskell] was nothing more than syntax highlighting). So I treid IntelliJ Haskell again, created with it a new project, waited almost **half an hour** (having ssd and i5, wth?) for it to install dependencies, only to discover that (at least for me) it still provides only syntax highlighting... Even basic navigation inside same module was broken - I got either modal dialog "resolving" ending with nothing found or small popup "finding usages" which never finished (waited several minutes in a project of size like 10 lines). I tried closing project, restarting IDE (several times), always same result - just a text editor. I was very disappointed. 😫 I really like the language so far (I have written a [language interpreter](https://gitlab.com/monnef/moORBen) among other things, I guess I am slightly advanced beginner), I only wish Haskell ecosystem (library documentation, IDEs) wasn't so bad/unfriendly...
I agree that this would be a useful behaviour.
I'm sorry to hear that. Although I can confirm first run can take a while I haven't suffered from other problems, maybe because of enviroment differences? (I use Ubuntu 18.04) I would try checking if latest stack is in path you use in plugin settings.
I think that this is a great suggestion. I have used this book and it was extremely thorough and helped me a lot.
Before \`cabal\` and \`hackage\` building Haskell code was tricky. You'd try to compile some code and you'd get an error like \`could not import module Foo\`. And then you'd have to go on altavista and search for \`Foo haskell\` and hope that you found some personal website on university server that contain a \`.tar.gz\` file for that code. Hopefully the student hadn't graduate and the webpage was still available. &amp;#x200B; So when GHC 5.04 introduced hierarchical modules, they created a new library: &amp;#x200B; [base](https://downloads.haskell.org/~ghc/5.04.1/docs/html/base/index.html): the Prelude, and a *large collection of useful libraries.* &amp;#x200B; This is a good thing at the time. But once hackage came around, base became an unnecessary and even harmful thing. \`base\` has too many unrelated concepts and is too hard to change. It is part of why \`String\` is still so prevalent in Haskell despite seldom being the best data type for whatever problem you are trying to solve. &amp;#x200B; But, whittling \`base\` and \`Prelude\` down to almost nothing is not really worth the effort at this point in time. &amp;#x200B; Because \`base\` is so obviously flawed people keep trying to replace it with something better. But, really, less is more. For example, why is \`stm\` a separate library by \`MVars\` are in base? Probably because \`MVars\` were invented before hackage and \`stm\` was invented after. Not because \`MVars\` are somehow more worthy of being in \`base\` than \`stm\`. &amp;#x200B; &amp;#x200B;
thx for all the suggestions..:) I will take some times to look at above options..
&gt;Different people have different thoughts on what should be in the standard library. No one can agree on the design that satisfies everyone &amp;#x200B; If you look at the release notes for GHC 5.04 when \`base\` was introduced it says: &amp;#x200B; [base](https://downloads.haskell.org/~ghc/5.04.1/docs/html/base/index.html): the Prelude, and a *large collection of useful libraries.* &amp;#x200B; \`base\` predates \`hackage\` and was a sensible way to try to make commonly used libraries more widely available so you don't have to use altavista to hunt down the source code. &amp;#x200B; Now that \`hackage\` exists, a large part of what is in \`base\` doesn\`t really need to be there. &amp;#x200B; \`Prelude\` is perhaps an interesting idea for beginners, but in large applications I am importing so many modules already, \`Prelude\` isn't really saving me much time or effort. Especially since I tend to use explicit import lists anyway. All \`Prelude\` is doing is obscuring where the data and functions really come from -- which isn't really that helpful. &amp;#x200B; So, the usefulness of \`base\` and \`Prelude\` are highly questionable. Getting a formal consensus on a standard library is nearly impossible and of questionable value. With \`hackage\` and \`cabal\` there is little need for a standard library. You might argue that surely \`Data.Bool\` belongs in \`base\`/\`Prelude\`. But if it was split off into a separate \`boolean\` package, I think you'd hardly notice the difference. Of course, hardly noticing a difference is also why people haven't bothered to do it. &amp;#x200B; On the other hand, if you had to explicitly choose to depend in \`string\` and \`import Data.String\` vs \`text\` and \`import Data.Text\`. I bet \`String\` wouldn't be as prevalent as it still is. &amp;#x200B;
If there was no `Prelude` or `base` and you had to explicitly choose `String` vs `Text`, I'm guessing we'd see a lot less `String`. 
&gt; But if it was split off into a separate `boolean` package, I think you'd hardly notice the difference. I think there will be noticeable difference. The problem is in making decision where to put instances. If we go the proposed way with splitting `Data.Bool` into separate package then why not put `Eq`, `Ord`, `Functor` into their own packages? But in that case, what is the better solution: make `boolean` package depend on `eq` package and have an instance inside `boolean` package or make `eq` package depend on `boolean` and have an instance inside `eq` package? I see `base` as a collection of fundamental, most basic and useful data types and abstractions that play together nicely. GHC got the `Contravariant` typeclass (which is rather fundamental) in the `base` library since version 8.6.3. There are other basic useful abstractions that proved themselves popular and extremely useful: `Validation`, `These`, `Profunctor`, `Witherable`. Putting all such basic pieces into single library solves the problem of orphan instances and actually provides consistent standard library.
[removed]
If there was no `Prelude` or `base` I'm guessing we'd see a lot less haskell. ¯\_(ツ)_/¯ 
You dropped this \ *** ^^&amp;#32;To&amp;#32;prevent&amp;#32;anymore&amp;#32;lost&amp;#32;limbs&amp;#32;throughout&amp;#32;Reddit,&amp;#32;correctly&amp;#32;escape&amp;#32;the&amp;#32;arms&amp;#32;and&amp;#32;shoulders&amp;#32;by&amp;#32;typing&amp;#32;the&amp;#32;shrug&amp;#32;as&amp;#32;`¯\\\_(ツ)_/¯`&amp;#32;or&amp;#32;`¯\\\_(ツ)\_/¯` [^^Click&amp;#32;here&amp;#32;to&amp;#32;see&amp;#32;why&amp;#32;this&amp;#32;is&amp;#32;necessary](https://np.reddit.com/r/OutOfTheLoop/comments/3fbrg3/is_there_a_reason_why_the_arm_is_always_missing/ctn5gbf/)
Is there a Haskell equivalent of this Rust package https://github.com/alexcrichton/cc-rs? What it does is basically provides a portable way of compiling C/C++ dependencies (esp. if there's not a lot of complicated stuff going on).
I think my main problem is finding the correct name to write once I’ve seen the package at hackage for example. Where do i look for the correct name to write in the dependencies? Furthermore some libraries are as follows: Data.something. Do i write in the dependencies “something” for this package? I think i dont need the generality and ill read about the correct string type i need. If wreq return lazy bytestring, should i just continue with it if i don’t particularly need strictness?
I’ll look it up. Thanks
I am using Kubuntu 16.04, but I don't think it makes much of a difference (stack on its own is working with various compiler versions across few projects). I did verify before installing plugin I have most recent stack. Problem is, everything plugin does (the long installation when creating new project) ended with success, so I am not really sure what might be the wrong... I might try updating IDEA itself (I don't have newest one, mainly because last two times I updated the releases broke TypeScript/webpack support in different ways and I was forced to downgrade).
Cabal already supports this, so there's not much need for a separate package just for invoking a C compiler. I've never had to use this, so I can't tell you how it works; I'd advise checking the Cabal [documentation] (https://cabal.readthedocs.io).
This was also asked [on SO](https://stackoverflow.com/questions/55330709/questions-about-implementing-the-ruler-function-using-streaminterleave). To start off, I'll ask the same question I asked there: &gt; Exactly which part are you stuck on? Do you understand most of it except for the actual `ruler` definition, or do you need help with understanding the various other `streamXXX` functions as well? My motivation for asking this is that it could help get an answer which is more targeted towards your specific difficulties: there's several functions in your code, and it would help to know exactly which one(s) you need help with.
&gt; If wreq return lazy bytestring, should i just continue with it if i don’t particularly need strictness? Yes.
You can think of 'ruler' as the limit of the following process: 1. Start with an infinite steam of 0s. 2. Add one to every element of the stream. 3. Interleave the infinite stream of 0s with this new stream. 4. Repeat. So it evolves like this: 0 0 0 0 0 0 0 0... 0 1 0 1 0 1 0 1... 0 1 0 2 0 1 0 2... 0 1 0 2 0 1 0 3...
&gt; I think my main problem is finding the correct name to write once I’ve seen the package at hackage for example. As an example the package at https://hackage.haskell.org/package/recursion-schemes is called recursion-schemes in cabal/stack configuration. Not that some (most?) packages on hackage may not be in a particular stackage release. You can use stackage search instead of hackage search to limit the result to just the curated known-working in specific releases, packages -- or to find a hackage package to see if it is in the release you are targeting. You can also list hackage packages in the stack extra dependencies; I don't know the specific syntax but "recursion-schemes" would be part of the syntax that references the package linked above.
Here's another limiting process to think about. You start with this: 0 x1 0 x2 0 x3 0 ... Then you fill `x1` with (element 0) + 1: 0 1 0 x2 0 x3 0 ... Then you fill `x2` with (element 1) + 1: 0 1 0 2 0 x3 0 ... And repeat: 0 1 0 2 0 1 0 x4 0 x5 0 ... 0 1 0 2 0 1 0 3 0 x5 0 ... 0 1 0 2 0 1 0 3 0 1 0 ...
It depends on the monad you choose, eventually, to represent \`m\`. Most likely you will choose it to be some sort of \`ReaderT \_ IO\` if you have such a capability. In which case you can use the \`bracket\` function from the \`unlift-io\` library.
Hi, I have been working on a project for 6 months (GSD (Get your Stuffs Done) | Basic Todo list for demonstrating CQRS/Command Sourcing/FRP in Haskell ), I would really appreciate your opinion about it :-) 
How was the experience of using streamly?
Interesting project, but how does it relate to Functional Reactive Programming?
It went well : 1) Reactivity From the github contributors is really good 2) Expressivity of the API is good and easy to use 3) The library is quite complete, the only thing that is not yet there and that caused me pain was the absence of **MonadError** for the streams. I have been obliged to wrap the lib a little to have Safe Streams. The guys know that they have to do it, it's just a matter of time...
From the Sourcing of Commands to their Consumption, it's an asynchronous data-flow. Actually, it's one data-flow per Aggregate to be more accurate :-) And so the application is a combination of in-memory and persisted streams... I have used the [EventStore](https://eventstore.org/) for the persistence (they have a [Client](https://gitlab.com/YoEight/eventstore-hs) implemented in Haskell... ) I have tried to explain it [here](https://github.com/Eventuria/gsd/blob/master/doc/technical.md) :-)
I think it is very fashionable to bash LYAH. I too read LYAH from cover to cover and it was fun in parts, frustrating in parts and its provided me with a basis to move to comfortably approach other resources. Yes its flawed but a heroic attempt by the author (then a student) back when few printed texts existed. The community is better of for a multitude of resources.
academically, rasterific is far better than the rest because it is based on [http://www.vpri.org/writings.php](http://www.vpri.org/writings.php) papers
what do you think personally about Alan Kay?
This is actually great, stack has always seemed to find a way to confuse me in the past.
Looking at this open issue (https://github.com/haskell/cabal/issues/1325#issuecomment-17753525), I don't think Cabal supports this.
The expression "Functional Reactive Programming" is generally used (at least in the Haskell community) to describe a specific programming paradigm, based on first-class "behaviors" (values that can vary over time) and "events" (conditions that occur at a given time). In other contexts I've seen people use the term "reactive programming" to describe asynchronous data-flows. The name is similar, but the concepts are as different as Java and JavaScript :)
What did `stack configure` do? I don't remember ever using this command, even last year. It doesn't exist anymore.
You're right, I can't remember why I wrote that either. A quick search doesn't reveal any use-case either. Thanks for pointing it out.
Thank you for asking. I posted the same on SO. I can understand the `streamXXX` functions, but I don't understand the `ruler` function. Why is it using `(streamMap (+1) ruler)`
I believe they are the same things (just 2 different ways of calling things) : * **Events** are **stream of incoming values**, each value is associated with it's time of occurrence * **Behaviors** (*Signals*) are **dynamic values** that vary continuously over time "Events" and "Behaviours" are just 2 types of streams ("dataflow") : * Discrete Streams * Continuous Streams I don't have Signals in that project but in one of my previous job it was the case (We were capturing biometrics from sensors...btw the name of the company is OMSignal - ) "CQRS/Event-Sourcing/Command-Sourcing.." which I would package into something call "Distributed Domain Driven Design" is an architecture where you capture all changes to an application state as a sequence of events (as opposed to classical CRUD application). The discrete stream is this basically : `Commands -&gt; Command Consumption -&gt; Events -&gt; Read Model Projections` a bunch of flows combined together where the sources are "events" (Command at the beginning and then Events downstream ...) "Events"/"Discrete Streams" are the heart of the beast :-) 
Finally. We've needed a generic solution to this for, well, forever.
Ah...I think I finally got it. Thank you so much for the answers!!!
The purpose of a Prelude, in my estimation, was to give us a common basis from which to extrapolate. By including what is included we can assume that people know how these things work if they say they "know Haskell". There's also a measure of convenience involved. Most replacement preludes seek to improve on the convenience factor, eliminating the 5-10 imports that people find themselves including in every file. What is the advantage provided by reducing the Prelude?
Perhaps a foolish question: how does `gitrev` work when building a package that was *not* just directly cloned? As I understand it, tarballs on Hackage do not include `.git` folders. I guess you could try to get the .git folder included in tarball, but I'm not sure that's a good idea either... Having some sort of Cabal hook for including the git hash before making the tarball (and making that observable from the final executable/library) seems very useful though. It is very frustrating to find out that a bug report you got was actually from some unreleased version of a package.
What are the restrictions on kindedness? Also, `cereal` or `binary` next, plz. 
I'm pretty sure we'd end up with \`boolean\` depending on \`eq\`. &amp;#x200B; \`base\` only solves the 'orphan instance' problem for a small set of types and classes, it does not solve the bigger orphan instance problem. &amp;#x200B; If everything was split out, people would still end up just depending on \`boolean\`, \`eq\`, \`functor\`, etc, libraries and they would form a de facto standard library. &amp;#x200B; As noted, you would hardly notice the difference. It wouldn't really be an improvement, but also, things would still pretty much work the same except for a few extra dependencies and imports. &amp;#x200B; So, if nothing \*has\* to go in \`base\` then perhaps \`base\` should only contain the things that nearly everyone agrees should go into \`base\`. Things which are very fundamental and unchanging. So perhaps things like \`Eq\`, \`Functor\` belong, but probably not \`String\` and \`Text\`. &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
&gt; What is the advantage provided by reducing the Prelude? I think there's good arguments for both, a batteries included Prelude as well as for a minimalist Prelude. For the latter: The more names a prelude provides the higher the risk it'll steal names you'd want to use for something else. A simple example is `Word` which could refer to actual words in a natural language or `id` which would be an obvious choice for denoting an identifier or database key. If you go for a batteries-included Prelude at which point do you stop adding more to it? If you go for a minimal prelude there's an obvious lower bound.
Why you need a book if you have a computer?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/haskell_jp] [What to make Internal?](https://www.reddit.com/r/haskell_jp/comments/b5bkzl/what_to_make_internal/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
you can do one yourself in less that 50 lines
Appreciate your view, but I tend to stay focused with a book when I am trying to learn than a computer 
In my understanding event-sourcing projections are more similar to the Elm model, in which the current state is computed through a sort of `foldl` on the events. This is quite different from the FRP approach, in which there is no projection at all. Behaviors are kind of composable projections themselves. I use CQRS everyday at work, and I've played a little bit with FRP libraries in my free time. To me the two paradigms look quite different. Unless you mean it in a very broad sense (for example both allow for a declarative style) I struggle to see how they are the same thing.
There's also `RebindableSyntax` if you want to go all out and implement your own machinery for desugaring and if...then...else.
Wait you can rebind `if...then...else`?
&gt; What is the advantage provided by reducing the Prelude? I don't know that this is an argument for reducing the Prelude, but I would generally prefer to keep as much as possible out of the packages that as part of GHC, so that they can be updated independent of the GHC release schedule. (And, ideally would work with a different Haskell implementation.)
https://gist.github.com/ahammar/2657492
Curious -- why is TH needed to derive these instances? Is there something about the use of GADTs?
Does this work for guards too as in absolute x | x&lt;0 = -x | otherwise = x ?
Thanks for providing a nice example of how to tie all these concepts together! One question that I've been wondering about for a while: given that you've already got a datatype of commands, it seems like this kind of approach should lend itself well to state machine model based testing (see e.g. [quickcheck-state-machine](https://github.com/advancedtelematic/quickcheck-state-machine) or [Hedgehog](https://teh.id.au/posts/2017/07/15/state-machine-testing/index.html)). Do you have any thoughts on that?
yeah, I don't know the Elm model but the read projections are the "sink" (end flow of the stream) and yeah it could be `left-fold` but you could decide to only `map` and not reducing (which is pretty limited I admit... I'm doing it for the "Monitoring" projections) you could do CQRS without FRP, like you could do CQRS in OOP and not FP, I agree with you. They are not the same thing. By the way, I would rather say "Distributed Domain Driven Design" Architecture rather than CQRS Architecture, it's a set of different practices/tools/patterns that are bundled together (CQRS/ Command/Event Sourcing, SEDA, FRP / FP....) , IMO, FRP is a natural companion for CQRS since you can consider the pattern as streams. It simplifies the implementation drastically and yeah as you said you can have declarative style everywhere which is great :-)
From the release notes: - An integer literal 368 means `fromInteger (368::Integer)`, rather than `Prelude.fromInteger (368::Integer)`. - Fractional literals are handed in just the same way, except that the translation is `fromRational (3.68::Rational)`. - The equality test in an overloaded numeric pattern uses whatever `(==)` is in scope. - The subtraction operation, and the greater-than-or-equal test, in n+k patterns use whatever `(-)` and `(&gt;=)` are in scope. - Negation (e.g. `- (f x)`) means `negate (f x)`, both in numeric patterns, and expressions. - Conditionals (e.g. `if e1 then e2 else e3`) means `ifThenElse e1 e2 e3`. However case expressions are unaffected. - `do` notation is translated using whatever functions `(&gt;&gt;=)`, `(&gt;&gt;)`, and `fail`, are in scope (not the Prelude versions). List comprehensions, `mdo` (The recursive do-notation), and parallel array comprehensions, are unaffected. - Arrow notation (see Arrow notation) uses whatever `arr`, `(&gt;&gt;&gt;)`, `first`, `app`, `(|||)` and `loop` functions are in scope. But unlike the other constructs, the types of these functions must match the Prelude types very closely. Details are in flux; if you want to use this, ask! - List notation, such as `[x,y]` or `[m..n]` can also be treated via rebindable syntax if you use `-XOverloadedLists`; see Overloaded lists. - An overloaded label `#foo` means `fromLabel @"foo"`, rather than `GHC.OverloadedLabels.fromLabel @"foo"` (see Overloaded labels). So, looks like no dice for guards.
I'd argue you _can_ do Nix halfway (e.g.: binary packages with `patchelf`ing), it's just that the marginal cost of doing Nix fullway is not that large compared to the marginal benefit.
Gitrev returns the string “unknown” if there isn’t a git repo (not ideal, I know)
&gt; An integer literal 368 means `fromInteger (368::Integer)` That seems circular.... wouldn't `(368::Integer)` be expanded into `fromInteger (fromInteger (368::Integer)::Integer)` and so on for infinity?
Maybe check out this 6-part series: https://ptival.github.io/card-game-01
Nope, stops at one step, just like without RebindableSyntax. There's no surface syntax for a monomorphic `Integer` literal; if there was could use that in the expansion, but there's not. So, instead they are using `(368::Integer)` to try and get that across. Suppose `123#` was always the `Integer` 123. In that case, normal Haskell desugaring would turn `234` into `Prelude.fromInteger 234#`, and RebindableSyntax desugaring would turn `234` into `fromInteger 234#`. `345#` wouldn't need to be desugared at all, it would just be an monomorphic `Integer` literal.
The idea is that `368 :: Integer` denotes a primitive construct in the *desugared language*, which is distinct from the *surface language* we write code in.
If you want impurity for debugging purposes you use [`Debug.Trace`](https://hackage.haskell.org/package/base-4.12.0.0/docs/Debug-Trace.html).
No, I've run into the exact same issue
Thanks! That'll help a lot. The beginning of haskell has been a confusing mess so as i said in the beginning of my post, i probably got a lot of things wrong. 
I recommend sticking with it and chatting with people on IRC, the functional programming slack, or Haskell discord. Lots of people are around to help, this particular subreddit tends not to be one of them (usually). (There's also /r/haskellquestions) I'll also say that it's a community-driven language and ecosystem, so you are welcome to _help_ improve things, and in a way you have by delivering us this valuable beginner's experience report. You've just done it in kind of an annoying way. Sarcastic spongebob meme, really?
About the installation issue: Did you do `stack install`? Because that is most likely not what you want. It's the same as make install, it builds thr binary and copies it to a global location. Just use `stack build`
&gt; TemplateHaskell scares me with it's grotesque complexity &gt; The game of choice is Arkham Horror thinking_face.png
Sorry if the post came out as annoying, it's written from a place of frustration which intentionally or unintentionally, shows.
 a = foldr (\x y -&gt; x : y) [] "dog cat" b = foldr (\x y -&gt; x : y) "" "dog cat" main :: IO () main = print (a == b) This prints `True` for me. Can you show more code that might reveal why GHC is suggesting `AllowAmbiguousTypes`?
I completely understand the frustration, your points are all valid. I'm not sure what to say beyond that, the short-term way forward is to slowly push through all of the warts and traps. Long term hopefully tooling, error message, library, and documentation quality all improve significantly.
How are you learning Haskell? You are running into problems that are common for beginners to have, and most good beginner materials cover thoroughly. [Haskell Programming from First Principles](http://www.haskellbook.com/) covers this stuff very well. &gt; But I found Haskell's errors to be very rarely actually helpful. Haskell's errors are worse than Rust and Elm's, for sure. I've found that they're better than JavaScript, Java, C++, Ruby, and Python errors, though. I'd note that this error message tells you the important bits: the line number and the term that is confusing GHC (`numbers`). It would be nice to have a "teaching Prelude" that has type errors for common problems related to polymorphism - an instance of `Num` for `[a]` that produces a helpful type error (much like Elm's) would be fantastic, as that is a common stumbling block. &gt; package management The problems you are experiencing with `cabal` are known as "Cabal Hell," and are the motivating factor for the development of `stack` and the `cabal new-*` set of commands. Haskell cares more about version compatibility than most languages. If you try to install two libraries which require conflicting versions in `npm`, for example, you get both versions of the library installed. This means that you might run into errors if those two libraries communicate and produce different results. Haskell requires that any library is installed *once* and with a compatible version. So if you do `cabal install wreq`, it'll pick a bunch of library versions. If you then do `cabal install warp`, it'll try to pick library versions that are compatible with `wreq`. However, it may not be possible to do that, given the versions that were picked when building `warp`. If you specify both `warp` and `wreq` at the same time, then it can probably pick a working build plan. `stack` solves this problem by using an index of packages known to work together - Stackage. `cabal new-*` solve the problem by building things in a totally sandboxed way, similar to how `nix` works. I tried installing all of the executables you mentioned using `cabal new-install`, and it worked. `stack install hfmt` failed with an error because it doesn't know which version of `hindent` to use. For installing binaries, I recommend using `cabal new-install`. For managing libraries, you should be using the `*.cabal` or `package.yaml` files to explicitly list the dependencies you need in a package/project. &gt; Decoding JSON/YAML/etc. The way this works in Rust was inspired by Haskell, so I'm not sure the problem you're encountering. It's relatively straightforward to do this [if you look at the documentation for the JSON decoding library, `aeson`](https://hackage.haskell.org/package/aeson-1.4.2.0/docs/Data-Aeson.html). &gt; Actions, a good idea used as a gimmick This is one of those things that takes a while to gain fluency with. &gt; ... If i just want to print the value of a variable as a debugging measure, ... then you [`import Debug.Trace`](https://stackage.org/haddock/lts-13.14/base-4.12.0.0/Debug-Trace.html) and do `printf` debugging to your heart's content. Haskell, unlike Elm, gives you escape hatches to write impure code when you need to (you rarely need to).
$ stack build `No setup information found for ghc-8.6.4 on your platform.` `This probably means a GHC bindist has not yet been added for OS key 'linux64-ncurses6'.` `Supported versions: ghc-7.10.3, ghc-8.0.1, ghc-8.0.2, ghc-8.2.1, ghc-8.2.2` Same error on both. Which one is generally recommended you'd say? Cabal or stack?
Say what you will about sarcastic Spongebob, but it let me use spongebobMeme :: String -&gt; String spongebobMeme = zipWith spongebob [0..] where spongebob n x = bool x (toUpper x) (even n) in a talk I gave at work.
I've also been experimenting with this idea! You can also represent it using (Endo Settings) as your monoid for traced and just use contramap to append mutations onto your config 👍
That is a very thorny question. In this exact case, it looks like you need to run `stack setup` first, but I haven't used `stack` in a few years, so I'm not sure.
`stack setup` grants me the exact same error. Could just be Void Linux's fault considering it's quite a weird and unpopular operating system.
Interesting! Thanks
From the GHC release notes: &gt; You cannot use a deriving clause for a GADT; only for an ordinary data type. gelisam comments with more detail [here](https://www.reddit.com/r/haskell/comments/7awyue/having_problems_with_gadts_and_deriving/dpdmfya/) about the why of it.
This is just my opinion, but I prefer stack by far. Not having to worry about having the whole team on the correct compiler version, have dependencies that are garantueed to work together (man I miss this in other languages). About that stack error, you could try installing `ncurses5-compat-libs`
It has just dawned on me that I can write a script to generate all of the aforementioned types. This, at the very least, removes tedium and many potential errors, but I'll have to rerun the script if I decide to change a field.
No, this is only line I have on my GHCi foldr (\\x y -&gt; x : y) \[\] "dog cat" &amp;#x200B; Here is the error I got: `• Could not deduce (Foldable t0)` `from the context: (Foldable t, Data.String.IsString (t a))` `bound by the inferred type for ‘it’:` `forall (t :: * -&gt; *) a.` `(Foldable t, Data.String.IsString (t a)) =&gt;` `[a]` `at &lt;interactive&gt;:147:1-28` `The type variable ‘t0’ is ambiguous` `• In the ambiguity check for the inferred type for ‘it’` `To defer the ambiguity check to use sites, enable AllowAmbiguousTypes` `When checking the inferred type` `it :: forall (t :: * -&gt; *) a.` `(Foldable t, Data.String.IsString (t a)) =&gt;` `[a]`
This reveals the problem. You have `OverloadedStrings` on, so GHCi doesn't know that `[]` has type `[Char]`. You can fix this a couple of ways: Add an explicit type signature: foldr (\x y -&gt; x : y) [] "dog cat" :: String Explicitly type `[]`: foldr (\x y -&gt; x : y) ([] :: String) "dog cat" Type applications: foldr @_ @Char (\x y -&gt; x : y) ([] :: String) "dog cat" The explicit type signature is the easiest and least weird way, imo.
&gt;If you try to install two libraries which require conflicting versions in `npm`, for example, you get both versions of the library installed. This means that you might run into errors if those two libraries communicate and produce different results. I wonder if it would be safe to install both versions if libraries depending on them don't expose potentially conflicting datatypes in its public API.
You can drop in this helper function and then `debugLog` will work like `Debug.log` in elm: import Debug.Trace ( trace ) debugLog :: Show a =&gt; String -&gt; a -&gt; a debugLog msg x = trace msg' x where msg' = msg ++ ": " ++ show x 
WFM % ghci GHCi, version 8.0.1: http://www.haskell.org/ghc/ :? for help Prelude&gt; foldr (\x y -&gt; x : y) [] "dog cat" "dog cat" 
I wrote a guide to getting started without needing to learn the module layout as a way to further reduce barrier to entry for Haskell: https://www.ahri.net/practical-haskell-programs-from-scratch/
This is really cool, do you think it'll be able to handle generating \`FromJSON1\`/\`ToJSON1\` instances for GADTs at some future point?
Why have even a minimalist Prelude? Even more minimal would be to have to import every symbol you use. The desugaring still works even if the involved symbols are not in scope. 
On the issue with debugging, it's indeed quite bothersome (`Debug.Trace` is far from a perfect solution), but I would like to say that the problem is not with purity, but rather purity-using-monads, which leads to pure functions having a very different structure from impure ones. Purity is not the end goal, the point is to render the assumptions we make about our programs explicit, in this case, the effects they may use. Being able to distinguish pure functions from impure ones at all is only the first step. Monads were just the natural way to approach the problem with Haskell. They have their pros and cons, but the only way to understand them was to put monads into practice in the first place, and Haskell pioneered the way. There doesn't have to be a divide between pure and impure as wide as in Haskell. Algebraic effects is a pretty popular alternative to monads for that reason (in academia circles at least). But in Haskell, we're stuck with monads. It's worth stressing that, contrary to many other languages, Haskell is strongly driven by academic research. Of course, this doesn't mean that it is designed by people living in ivory towers for people living in ivory towers. The point is, although you have some valid criticisms, the will to advance the state-of-the-art in pure functional programming does not always align with the objectives of a seamless user experience.
often times when there are type errors it's also quite useful to replace the term that's causing the type error with a hole, and then ghc will tell you its desired type as well as the things that are bound that you can use to get the thing of the desired type or you can use `Debug.Trace`, which has already been mentioned
Might I offer: spongebobMeme :: String -&gt; String spongebobMeme = zipWith ($) (cycle [toUpper, id]) 
I *always* forget about wacky `cycle` constructions. That's definitely going in the next version of the talk.
Not sure if this belongs in `r/emacs/` but here we go.. I typically use `Vim` with an extremely bare-bones configuration. I'll either run `ghcid` or `stack ghci` for type checking and type holes. However, this weekend I started experimenting with `emacs` and various Haskell plugins. I setup `haskell-mode`, `flycheck-mode`, and `intero`. Between these emacs plugins I get a lot of really cool features: automatic linting and typechecking on file saves, checking the type of any expression in the minibuffer, looking up the definitions of types at a keystroke, inserting type signatures automatically, and hoogle lookups. I'm not convinced I really need all these features but its fun to mess around with them. How do `Intero` and `Flycheck-mode` interact? When I check `M-x flycheck-verify-setup`, I see `Intero`, `haskell-stack-ghc`, and `haskell-hlint` as my enabled Syntax Checkers. - Does that mean `Flycheck-mode` is managing `Intero`? - Do `Intero` and `Flycheck-mode` each run their own GHC instance? - Is there a big performance overhead to having an IDE setup like this? - How does `GHC-Mod` relate to these packages? Does it replace any of them? Act as a backend for them? What does it add to the equation?
The `kind-generics` library (of which I am an author) provides generics for GADTs. It would be interesting to see how aeson looks with it.
Would him changing his compiler version to one of the 'supported versions' work as well?
With your method, how do you define dependent behavior while retaining order-independent results?
While everyone is giving suggestions about the issues you posted, there is something else to remember. Haskell was never intended to be a language for industrial use. Simon Peyton Jones has said on numerous occasions "Avoid success at all cost". Haskell was created to be first and foremost a research language. &amp;#x200B; Elm and to a lesser extent purescript are written more for industrial use. And there are other projects like Eta ([https://eta-lang.org/](https://eta-lang.org/)) that is trying to develop an "industrial" haskell. If practicality is your main goal then I would stick with elm on the front end and use one of the other ML family languages that were written for practical use on the server like F#, ReasonML, Ocaml etc (obviously, these will have trade off as well) &amp;#x200B; Eventually, as you see more companies adopting Haskell you will see these things iron out just like python did, but for now Haskell still has very strong roots in the research domain
&gt; Haskell was never intended to be a language for industrial use. I don't think that's correct! I recently looked at the Haskell Report and imagine my surprise when I found [this list of Haskell design goals](https://www.haskell.org/onlinereport/haskell2010/haskellli2.html#x3-3000): &gt;The committee’s primary goal was to design a language that satisfied these constraints: &gt; &gt; 1. It should be suitable for teaching, research, and **applications, including building large systems.** &gt; ... 
Life is too short to type out the same dozen imports in every file.
The problem with a lot of the alternatives is that they run in the JVM, in BEAM, or stuff like that. I greatly prefer native code
&gt;"Avoid success at all cost" Not "(Avoid success) at all cost" but "Avoid (success at all cost)"
That's true! It's one of the things I'm willing to trade the poor package management for when using Haskell!
Interesting. Maybe that new. I heard Simon Payton Jones say what I was talking about in this podcast : [https://www.microsoft.com/en-us/download/details.aspx?id=9958](https://www.microsoft.com/en-us/download/details.aspx?id=9958)
I thing this “Avoid success at all cost” wasn’t meant to mean that Haskell is not for use in industry but that Haskell designers should care more about doing things right instead of creating quick ad-hoc solutions that will spoil the language and cause you trouble later.
I hope this comment does come off as mean, but honestly, statements like this: &gt; Haskell has some huge flaws which make me unable to see it as being an serious alternative for general-purpose use, either for me personally, or for enterprise use. I've split my points into categories to make it easier to follow. really have *no* place if you're expecting a _nice_ answer. The fact is that many people and companies do use it for 'general-purpose' use. In fact, many people find languages like go to be jokes, given their lack of support for basic programming constructs, like algebraic data types and parametric polymorphism. Do you understand why going on a Go forum, expressing this viewpoint, and then asking questions would elicit a poor response? Then please don't do this here. Okay, now to your actual questions: &gt; Compile-time errors Haskell has strictly more type-system features than the languages you mentioned. Error reporting in Haskell-like type systems is not a solved problem in the way it is for Rust, Go, and Elm, which remove type system features to keep the error messages manageable. This is of course a point of aesthetics, but, in practice, this is rarely a problem for general purpose use. &gt; Package Management I'm surprised this is here. I've used a lot of package ecosystems, and stackage is by far the best. Everything compiles with everything else. All compilers and packages are installed automatically. What more can you ask for. GHC 8.6.4 is supported by the latest stackage LTS nightly, so I'm guessing you installed an old version, but I mean, complaining about windows 10 because windows 95 didn't support something is silly. I was able to install both your packages by doing ``` stack install --resolver lts-13.14 wreq warp ``` &gt; Decoding JSON/YAML/etc. I have never used Wreq. However, I looked it up, and the documentation has a [whole section](https://stackage.org/haddock/lts-13.14/wreq-0.5.3.1/Network-Wreq.html#g:23) regarding decoding responses. There's even an `asJSON` function which takes a response with a `ByteString` and yields a response with any old Haskell type for which there is a `FromJSON` instance. There's even an example, which I've copy-pasted: ``` {-# LANGUAGE DeriveGeneric #-} import GHC.Generics (Generic) {- This Haskell type corresponds to the structure of a response body from httpbin.org. -} data GetBody = GetBody { headers :: Map Text Text , args :: Map Text Text , origin :: Text , url :: Text } deriving (Show, Generic) -- Get GHC to derive a FromJSON instance for us. instance FromJSON GetBody {- The fact that we want a GetBody below will be inferred by our use of the "headers" accessor function. -} foo = do r &lt;- asJSON =&lt;&lt; get "http://httpbin.org/get" print (headers (r ^. responseBody)) ``` I'm not sure what was difficult about this really. I tried compiling this example, and other than import some modules, it mostly worked ot of the box. Have you not seen this? &gt; If i just want to print the value of a variable as a debugging measure, that'd require me to restructure my entire code, making pure functions into actions, adding do blocks, changing type annotations to include IO, using &lt;- instead of chaining functions like I had been doing. Some builtin compiler-feature for printing would be nice, instead of using function/actions. Use `trace`. 
Most likely would but that seems like me avoiding the issue and not solving it. Besides, i'd like to continue using my Linux distributions package manager for all my programs.
Thank you! That helps a lot. Should I repost my answer above on SO, or is there no need since you now seem to understand?
 &gt;really have *no* place if you're expecting a reasonable answer, because they immediately set a confrontational tone. The fact is that many people and companies do use it for 'general-purpose' use. In fact, many people find languages like go to be jokes, given their lack of support for basic programming constructs, like algebraic data types and parametric polymorphism. Do you understand why going on a Go forum, expressing this viewpoint, and then asking questions would elicit a poor response? Then please don't do this here. I've gotten plenty of wonderful responses actually, you people seem like an overwhelmingly helpful community. This was written out of frustration, confrontational tone was the most natural way i could convey my point. &gt;I'm surprised this is here. I've used a lot of package ecosystems, and stackage is by far the best. Everything compiles with everything else. All compilers and packages are installed automatically. What more can you ask for. GHC 8.6.4 is supported by the latest stackage LTS nightly, so I'm guessing you installed an old version, but I mean, complaining about windows 10 because windows 95 didn't support something is silly. From what I've seen Stack does appear to solve a lot of the issues i have. But I can't get it to work (check other comment thread on this post), I'll make some more attempts. &gt;Use `trace`. Thanks to you people i now know UnsafeIO is a thing
Nice representation of an idea. Shouldn't there be also `Comonad` instance for `ProjectBuilder`?
I think there are very few places where constraint on an existential variable is helpful. I would figure out how to model it using algebraic datatypes. So, going from your description and with some help from the wiki, here's the direction I'd head: data Card = Card CardName CardBody newtype CardName = CardName String data CommonType = CommonPhysical | ... data Hands = One | Two data UniqueType = UniquePhysical | UniqueMission Sacrifice [Location] | ... -- There are 9 'types' of card, each of which has various attributes, so there -- can be 9 constructors in the sum type. I've added only some of the attributes for -- each type data CardBody = CommonItem Price (Maybe CommonType) (Maybe Hands) (Maybe Bonus) [Effect] | UniqueItem Price (Maybe UniqueType) (Maybe Hands) (Maybe Bonus) [Effect] | Ally Bonus | Injury Restriction | ... Do you have any comments? Maybe you've written something similar and found it lacking?
Actually, don't bother about my sibling post - I didn't see your PM saying to put the answer on SO. I actually just thought of a 'nicer' answer than the one above, so that's the one I'll write up there.
You could potentially use TemplateHaskell instead of a script and then they would stay in sync automatically... at the expense of longer compile times.
[`serialise`](http://hackage.haskell.org/package/serialise)!
I'm having a hard time understanding how it becomes order-independent. Can someone give a hint?
`(==) :: Eq a =&gt; a -&gt; a -&gt; Bool` Looks like `eq` would have to depend on `boolean` to me!
&gt; Elm and to a lesser extent purescript are written more for industrial use. I've heard this opinion before, and I find it somewhat humorous given Elm was born because the author was working on an academic project, and PureScript was born because the author specifically wanted to use pure FP at work.
+1, well written intro.
&gt; i now know UnsafeIO is a thing Please don't use it. Someone mentioning it in the context of your post was a terrible idea.
&gt; Algebraic effects is a pretty popular alternative to monads for that reason (in academia circles at least). But in Haskell, we're stuck with monads. I'm asking this out of ignorance, but what are examples of Algebraic Effects systems other ones using Free Monads?
[Bowling Scores](https://unknownparallel.wordpress.com/2012/11/05/bowling-on-a-tardis/), maybe?
Probably, but the question becomes "can you tell if there will be potentially conflicting datatypes" I general, and I'd be very surprised if you could. That level of introspection *usually* becomes equivalent to the halting problem, and I doubt this case is any different. Of course, that's not even accounting for the probable reason for the restriction: haskell libraries are installed "flat", but npm installs each dependency *inside* the library that needs it. Without that there's no way as a library to tell which version was installed "for you" and which version is someone else's. The entire package-resolution and installation mechanics would need to be modified, which would probably be lots of work^^TM
I never used the tardis-monad, but I used a similar technique when I implemented an assembly EDSL. Not my post, but very similiar: [http://wall.org/\~lewis/2013/10/15/asm-monad.html](http://wall.org/~lewis/2013/10/15/asm-monad.html)
Anecdotal, but I definitely downloaded a package which had Tardis as a dependency once. Can't remember what it was, though.
I understand it's an awful idea for many cases, I'll only be using it for debugging 
Haha, surely you knew this existed...
Of course \*I\* knew...but now everyone knows ;)
It supports many arguments of arbitrary kinds, but there are implicit restrictions on how the type arguments are constrained by the constructors. At present the code assumes that everything but the last argument is a parameter, and the final one is the index (so that it can write an instance of `FromJSON (Some (T a_1 ... a_(n-1))`)
You can't (at least not easily) but I've never needed those so it does the trick for me 😄
Not a lot there unfortunately... https://packdeps.haskellers.com/reverse/tardis
One of these, looks like: https://packdeps.haskellers.com/reverse/tardis
Personally I find Haskell code hard to read compared to languages like python. Moreover I think Haskell lacks ide and serious ecosystem compared to other languages. Fortunately, because I really like the language...
you can trace with: import Debug.Trace increment :: Int -&gt; Int increment x = traceShow x (1 + x) purity is why haskell is unique. if you can't see value in purity, a lot of the motivation for learning haskell (as you say, struggling with illegible type errors) isn't there. at least, that's how it was for me.
You can, if you write: vlength :: forall a n. KnownNat n =&gt; Vec a n -&gt; Integer vlength _ = natVal (Proxy :: Proxy n)
Very fair. Much better to fix the root issue :)
its pretty useful for debugging though
@_ @
Hah! I didn't really think about it that way when I listened to the interview. I misinterpreted what he said to mean that industry success would be a counter force to the underlying philosophy on why they created Haskell. That makes sense!
I finally ran into a nice use for this sort of idea just a few weeks ago, while working on a not-yet-released library for doing terminal-graphics applications using Reflex. We wanted to add some monad transformers that would help widgets keep track of the unused screen real-estate and tile themselves, and automatically give a reasonable tab order, so that the user can use the keyboard or mouse to change focus. So basically, to deal with focus, we needed the focused widget to be able to send each of its neighbours an (FRP) Event -- both the one before it in the do-block (which appears to the left or above it in the layout), and the one after (which appears to the right, or below). Such a "time machine" is perfect for that, and has worked out rather nicely so far -- the code for each of our "tiles" can start out with a thing that receives an Event from the previous widget which will tell us when to focus, and send an Event back in the other direction, firing when we have the focus and the user presses Shift-Tab. Then at the bottom of the tile's code, we receive an Event from the next widget again telling us when to focus, and send out another Event in that direction for when the user presses Tab. There are obviously other approaches we could take, but I was happy to finally have a use for reverse state after about a decade of having the idea in my head. :)
Idris has a pretty rich effects system that you can read about here: http://docs.idris-lang.org/en/latest/effects/introduction.html.
These posts that describe a single idea expressed in a production setting are really helpful
I use Vim so finding an IDE isn't much of an issue. Do you have much experience with functional programming? I know that when i started i found it hard to read as well, it takes time to get used to.
I actually sue the tardis monad a lot. I'm currently working on beam-mssql, a backend for the Beam database library for MsSQL. Part of this is talking the protocol that Microsoft SQL server uses, which is called TDS. I've implemented support for this in my [tds](https://github.com/tathougies/tds/blob/38e0169162f216ec3f63dcc63515878973f03bec/Database/TDS/Proto.hs) library. Part of TDS requires us to build a packet where the fixed-length lengths and offsets of some strings are sent out before the actual strings themselves. In the imperative world, this means encoding everything into a buffer and then going back to 'fixup' the offsets and lengths. Of course, since Haskell can time travel, this isn't necessary. Instead, in haskell, we build the entire thing in one go just asking for the future values as we go. Works as advertised -- you can literally read the future.
I'd over-generalize and program a card game where an uncommon card may have a price. Then I'd code a check for content validity of games of your particular card game family, in particular verifying that no uncommon card has a price. This check is run only once at the start of the game. If you really insist it may even be run during compilation via TH. As long as your game content (the set of any possible card kinds) is immutable, this check is as good as type-checking. So make sure to hide the constructors of the card kinds and never use them in the game engine (the bit wtih game rules, UI, AI).
Not particularly elegant or practical, but it admits a very straightforward `Traversable` bubble sort. Sorting generic `Traversable`s appears to be a rather hard problem. Alternatives are: * Dump the `t a` into a `[a]`, sort it, then use `State` to tear the `[a]` back apart. Lore says this requires incomplete pattern matches. * Use a funky `Applicative` to perform an [insertion sort](https://elvishjerricco.github.io/2017/03/23/applicative-sorting.html). * Use a related funky `Applicative` to perform a [heap sort](https://github.com/treeowl/sort-traversable) ([Original Gist](https://gist.github.com/treeowl/9621f58d55fe0c4f9162be0e074b1b29)) [Original here (also mine)](https://stackoverflow.com/a/47446642/5684257): -- | @(True, u)@ if no change, else @(False, u)@ with some -- progress towards being sorted. bubbleTraversable :: (Traversable t, Ord a) =&gt; t a -&gt; (Bool, t a) bubbleTraversable t = end $ flip runTardis init $ forM t $ \this -&gt; do sendPast (Just this) (bubble, finished) &lt;- getPast let this' = fromMaybe this bubble precog &lt;- getFuture let (this'', bubble', finished') = case precog of Just that | that &lt; this -&gt; (that, Just this', False) _ -&gt; (this', precog, finished) sendFuture (bubble', finished') return this'' where init = (Nothing, (Nothing, True)) end (xs, (_, (_, sorted))) = (sorted, xs) Each "cell" of the collection communicates with its neighbors. As "time" passes, we progress down the collection. The greatest element so far seen, the bubble of bubble sort, moves forward in time. Each cell checks whether the bubble is coming. It sends itself backwards to fill the gap it leaves behind. Then, it pulls the next cell in, and decides whether to swap it with the bubble or not. It keeps whatever the smaller element is, and sends the bigger one up as the new bubble.
That's what an IDE is for.
Rotations of a list is one: -- | &gt;&gt;&gt; rotations "abcd" -- ["abcd","bcda","cdab","dabc"] rotations :: [a] -&gt; [[a]] rotations = flip evalTardis (id,id) . traverse f where f x = do xs &lt;- pure [] &lt;**&gt; getPast &lt;**&gt; getFuture modifyBackwards ((:) x .) modifyForwards (. (:) x) pure xs In general, I have found myself using it a couple times when I want a "right fold with effects" kind of thing. I have also used it when parsing a simple assembler language to do jumps to the right point. However I've found it works much better as the "tardis applicative": I have not been able to branch on both past and future state without causing an infinite loop (I wouldn't be surprised if it wasn't a proper monad in that sense).
&gt;Does it have a large enough community for asking questions? \#reflex-frp on freenode IRC. &amp;#x200B;
&gt;Statements like this really have *no* place if you're expecting a reasonable answer, because they immediately set a confrontational tone. I'm with you in principle but I think it's really important to bear in mind: a lot of the time you will receive accurate criticism that is not polite. Critics should not use that tone, but IMHO it's not appropriate to respond to good-faith criticism by criticizing the tone of the criticism. (Most commenters seem to agree in that they ignored the tone.) We're lucky to get criticism at all -- people who don't like Haskell, most of the time, will just stop using it. That especially means the giant audience of people who learn Haskell for school and never use it again because they either didn't "get it" or it wasn't suitable for their problem.
I think either can be very difficult to read. This is true for languages that 1) doesn't have common idioms (haskell), 2) makes it easy to introduce a lot of dependence on state (python), or 3) is simply written unidiomatically (any language I suppose).
The main reason Haskell struggles to find a wider base is just that it's more *different* than other structured languages. All the effort put into learning C or Python or Go or what-have-you can, without too much trouble, be translated to other structured languages. This is also true for a lot of LISP langs, which aren't always taught in introductory curriculums. I think it's easy for beginner-intermediates to not appreciate how hard it was to just get to that point, how hard it was to just intuitively and quickly recognize and understand certain syntax blocks, control structures, and idioms. As a complete beginner any code can make you frustrated because you don't *get* what's happening at all. Haskell is like all over again for most programmers so it's a turn off.
Suddenly everything makes a lot more sense. Thanks a ton!
&gt; such as no way to know when an element has actually been added to the DOM https://hackage.haskell.org/package/reflex-0.6/docs/Reflex-Network.html#v:untilReady
\* looks at C++ standard process \* "yea let's not do that"
there are a lot of issues with reflex, here are my main gripes: * the compiled javascript size is absolutely massive * gc pauses are always noticible * frp will dictate how you design your user interface, or you will have to deal with the legibility and performance penalty incurred by `RecursiveDo` * the reflex-dom source code and api is a mess * reflex-platform expects you to use shell scripts to manage your build environment (it wouldn't be difficult for them to just [supply a nix overlay](https://github.com/layer-3-communications/reflex-platform-simple)) * reflex-frp maintainers seem slow to respond to non-obsidian systems employees * it is very easy to write inefficient or unidiomatic reflex code that said, there were is also a lot to like: * the `reflex` api is nice and well thought out * code sharing is extremely nice * mostly error free (I would sometimes get errors related to the shims, or loops caused by RecursiveDo but it was very rare and never unpredictable) despite all of this I would say you decision should go a little something like: do I need code sharing? yes -&gt; can it be prerendered? 
Off the top of my head I know about [Eff](https://www.eff-lang.org/), [Koka](https://github.com/koka-lang/koka), [Frank](https://arxiv.org/pdf/1611.09259.pdf), and that there is [WIP for OCaml](https://github.com/ocamllabs/ocaml-effects-tutorial).
It’s not new. Haskell has always been intended to be used for real applications as well as research. 
Fair enough. I probably misinterpreted what Simon Peyton Jones meant in the interview.
I write Purescript at work and to be honest.. I wish the Prelude had more of the fundamentals. It doesn't make a lot of sense to have separate packages for `Maybe` and `Either`, imo. I mean even tuples are not in the Prelude. This ultimately means that every package in Purescript has more dependencies, which can be annoying. I think there's something to be said to have a common baseline set of primitives to work from in the Prelude.
`unsafePerformIO` is a compiler hook for library writers to handle some very specific cases, with great care. `trace` is fine to use for debugging.
I saw it used in an AI programming contest once. The forward state monad portion allowed the competing AIs to read previous states of the game. The reverse state monad portion, if I recall, allowed the game engine to get the resulting lazy list of all game states.
`ProjectBuilder` is just a type alias. It has the following definition: type ProjectBuilder = Settings -&gt; Project There is already a `Comonad` instance for `Monoid m =&gt; ((-&gt;) m`. Since `Settings` have a `Monoid` instance then every function from `Settings` to something else is a comonad. Alternatively you can look at the type of the `(=&gt;&gt;)` operator: (=&gt;&gt;) :: Comonad w =&gt; w a -&gt; (w a -&gt; b) -&gt; w b If you specialise `w` to `(-&gt;) Settings` you get (=&gt;&gt;) :: (Settings -&gt; a) -&gt; ((Settings -&gt; a) -&gt; b) -&gt; (Settings -&gt; b) In my blog post both type variables `a` and `b` are equal to `Project`, so this still allows to use comonadic behavior for the `ProjectBuilder` type. 
A few days ago I wrote this comment which describes a good bit of my evolution with Reflex. &amp;#x200B; [https://www.reddit.com/r/haskell/comments/b23y5g/wheres\_the\_buzz\_around\_haste/eiqfg72/](https://www.reddit.com/r/haskell/comments/b23y5g/wheres_the_buzz_around_haste/eiqfg72/) &amp;#x200B; Also, Kadena recently released a Reflex app at [https://pact.kadena.io/](https://pact.kadena.io/) which lets you write smart contracts with formal verification in the browser. Reflex definitely has a learning curve, but it also has a pretty high payoff--especially when you consider that you can also use it to write Android and iOS apps.
Try again, at least startup time has dropped substantially since (the issue about slow startup times)[https://github.com/rikvdkleij/intellij-haskell/issues/393] has been tackled.
Interesting! Does that solve the problem in https://github.com/reflex-frp/reflex-dom/issues/110 ?
oh, I didn't know about this. that's pretty disappointing. it looks like there are plenty of hacks to get around this, I guess
Nice one! It's useful, but targets a different audience. My target is the audience that has picked up some Haskell, and now needs to write something useful in it. Maybe a university project. Maybe a POC to show the usefulness of Haskell at work. When I wrote this, I was midway through writing my first Haskell libraries, and couldn't find good material that covered these things. Thus, this article.
`True`. 
Thanks, I’ll search in stackage from now on. 
Your code for Haskell and Elm is same, but errors are different. It looks like in Elm "+" is not polymorphic, and this is the error it reports. In Haskell, there cannot be such error because you could declare instance of `Num` for a list, and "+" would make sense then. But it would not work with your code because it should have both arguments of same type, and since you have there on one side element of list and on the other tail of same list you get the error "cannot construct the infinite type: a0 ~ [a0]".
I wonder are now newcomers evaluate all that auxiliary information which GHC prints with the error. Does it really help or just scares people off?
&gt; my Linux distributions package manager for all my programs I used to like it too. You could still try it. Then you don't need to use either cabal or stack to install dependencies. Debian for example does have many haskell libraries packed. Probably cabal then would be good choice to manage your project, though I have not been doing it for quite a long time. However, with distribution packages you have only one version of each libraries (if any) and one version of ghc. So you don't have new features, and other possible good stuff. So nowadays I prefer to use only stack and uninstall all haskell-related packets.
&gt; This probably means a GHC bindist has not yet been added for OS key 'linux64-ncurses6'. This is probably https://github.com/commercialhaskell/stack/issues/4144 as jvanbruegge advised. I often see some annoying low-level bugs at Windows after it upgrades. I wonder how other languages avoid them. Maybe just wider use of nodejs so that there is better testing. 
Fair enough, but I wouldn't say `servant` blocks you from working with complex authentication schemes. It's just that a rather experienced member of the team spends a day setting it up, and then you forget about if for 2 years.
Is the dataset available somewhere?
Well, just my opinion, I don't think he's anywhere near the same caliber as the main figures you'll find in the Haskell community. Most of his arguments are barely relevant metaphors that break down under the slightest scrutiny. Sometimes he just uses demagogy, like when he says "Do you see any functions in this room? I see a lot of objects." (Maybe he does suffer a condition that makes him see vtables). Anyway, it's hard for me to keep this respectful and honest at the same time.
Example below. It's not 1:1 to your case, but should showcase an example of reading wordcounts in (almost) constant memory. I read a dataset of repeating words that was 500M. The resulting map structure contains only 3 elements so its memory use should be minimal. Lines are read in a streaming fashion so won't collect memory. Disclaimer, I wouldn't write it like this, I would use either conduit / pipes to abstract away the streaming. But this style makes for a simple down to earth example. ``` {-# LANGUAGE OverloadedStrings #-} import Data.ByteString (ByteString) import qualified Data.ByteString.Char8 as B import Data.Map.Strict (Map) import qualified Data.Map.Strict as M import Data.Monoid (Sum(..)) import Prelude hiding (getLine) import Control.Exception (try, IOException) type WC = Map ByteString (Sum Int) insert :: ByteString -&gt; WC -&gt; WC insert k = M.insertWith mappend k 1 foldStdin :: (ByteString -&gt; a -&gt; a) -&gt; a -&gt; IO a foldStdin f acc = do x &lt;- try B.getLine case x of Left e -&gt; let _ = e :: IOException in return acc Right b -&gt; let acc' = f b acc in acc' `seq` foldStdin f acc' main :: IO () main = do wc &lt;- foldStdin insert mempty _ &lt;- M.traverseWithKey (\word c -&gt; B.putStrLn $ word &lt;&gt; " = " &lt;&gt; B.pack (show (getSum c))) wc return () ``` ``` 65,104,394,832 bytes allocated in the heap 48,148,248 bytes copied during GC 74,200 bytes maximum residency (2 sample(s)) 36,392 bytes maximum slop 0 MB total memory in use (0 MB lost due to fragmentation) Tot time (elapsed) Avg pause Max pause Gen 0 62607 colls, 0 par 0.254s 0.241s 0.0000s 0.0001s Gen 1 2 colls, 0 par 0.000s 0.000s 0.0002s 0.0003s TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1) SPARKS: 0(0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled) INIT time 0.001s ( 0.001s elapsed) MUT time 14.772s ( 14.784s elapsed) GC time 0.255s ( 0.242s elapsed) EXIT time 0.000s ( 0.004s elapsed) Total time 15.028s ( 15.031s elapsed) Alloc rate 4,407,424,337 bytes per MUT second Productivity 98.3% of total user, 98.4% of total elapsed ```
When `Settings mempty Any True mempty &lt;&gt; Settings Any False mempty memtpy `, then `Settings Any False Any True mempty` 
Hm, I'd say that you don't need to learn Stack to solve the problem in your first paragraph. As for the latter, that's a fair point. I have a script automating stuff for these single file projects and it might be good to build in a way to transition to a proper Stack or Cabal project because as you say, it's certainly an important part of the ecosystem. Might be a chance for me to learn nix too :)
I think it suits very well indeed ! I didn't know about it, I'm going to try it eventually ;-)
Usually the latter :| 
The file [https://ptrace.fefe.de/wp/timings2019.txt](https://ptrace.fefe.de/wp/timings2019.txt) contains the explanation of how to generate the data set. You need to compile the lines program from [https://ptrace.fefe.de/wp/lines.c](https://ptrace.fefe.de/wp/lines.c) though.
Very cool. Needs around 7 seconds on my machine and a peak resident set size of arount 650 MiBi. Can you probably explain, why my attempts failed so horribly?
I didn't read your code too thoroughly, but I believe you are reading the full file into memory and only then you start building the word counter. This is bound to be inefficient.
I've just noticed it after writing my comment, and deleted it afterwards. I have no clue why it wasn't really deleted Thanks for the explanation though!
Cool! I'd be curious to hear how it turns out!
I'll come back to you when I have something to show :-)
The gifs in your README and documentation are way too fast and end too suddenly. I suggest linking to videos that can be paused :)
Example of using the builder and endo. Not sure if you had this in mind, but at least it's a working example if someone wants to try it out. data Settings = Settings { name :: String , terminal :: FilePath , modKey :: String , keyBindings :: [(String, FilePath)] } deriving Show type Builder = (Endo Settings -&gt; Settings) -&gt; Settings overrideComonadic :: (Settings -&gt; Settings) -&gt; Builder overrideComonadic f builder = builder (Endo $ \self -&gt; f self) overrideComonadicMod :: Builder overrideComonadicMod = overrideComonadic $ \self -&gt; self{modKey = "alt"} overrideComonadicBindings :: Builder overrideComonadicBindings = overrideComonadic $ \self -&gt; self{keyBindings = [(modKey self, "foo")]} main :: IO () main = do print $ extract ((`appEndo` Settings "comonadic" "" "" []) =&gt;&gt; overrideComonadicMod) print $ extract ((`appEndo` Settings "comonadic" "" "" []) =&gt;&gt; overrideComonadicMod =&gt;&gt; overrideComonadicBindings) print $ extract ((`appEndo` Settings "comonadic" "" "" []) =&gt;&gt; overrideComonadicBindings =&gt;&gt; overrideComonadicMod ) 
Nix works really well with Haskell BTW. There's another article on my blog where I kind of exploited their interaction to speed up CI builds. [https://sakshamsharma.com/2018/03/docker-hakyll-builds/](https://sakshamsharma.com/2018/03/docker-hakyll-builds/) In any case, if you're working on a project for more than 1-2 days, then I firmly believe it needs to have proper modules, at least at some logical level of separation. Not only does it promote healthy coding style, it is good for abstraction and theorizing, makes it easy to read the code at a glance, and also makes incremental compilations faster if you're not changing a module that is imported by all others. Given how slow GHC is, I find the last thing to be immensely valuable once you've hit 1k or so lines of code.
The idea of shrinking the Prelude isn't new. In fact, this was considered for the Haskell Report long time ago. Quoting from https://prime.haskell.org/wiki/Prelude : ---- &gt; # Shrink the `Prelude` &gt; &gt; People sometimes complain that the Prelude steals lots of good names from the user. Think of `map` or `(+)`. Yes, most of the time we want these names to have a standard interpretation, but occasionally it is useful to be able to redefine them to mean something else. For instance, there are several proposals to change the prelude's numeric class hierarchy into something more flexible involving rings, groups, and so on. &gt; &gt; But it is tedious if the user of such a proposed replacement for the `Prelude` must explicitly hide the standard prelude in every importing module. &gt; &gt; Thus it might be useful to trim the `Prelude` down to the bare minimum possible. Most users of e.g. list functions would then need to `import Data.List`, users of numeric functions would need to `import Data.Numeric`, and so on. Of course, some users (e.g. university teachers) might want to collect a bunch of utility libraries into a single import resembling the current `Prelude`. But the point is that they could choose the features they want to expose to students, and hide those they want to avoid as well. For instance, there are certainly some teachers who would like to be able to ignore the class overloading system altogether at the beginning, then perhaps introduce the concept later on, once the basics have been covered. --- Also people have been complaining about the `Prelude` for ages. Some of us still remember that [RRFC (Ranty RFC) from 2007](https://mail.haskell.org/pipermail/libraries/2007-March/007236.html) which I'm reproducing below in its full length because it's interesting to see how well those words have aged and also because I feel like we've all forgotten how unsatisfying the situation was when `containers` or `bytestring` were still part of a huge monolithic `base`! ---- &gt; # Why the `Prelude` must die &gt; &gt; This is a ranty request for comments, and the more replies the better. &gt; &gt; ## 1. Namespace pollution &gt; &gt; The `Prelude` uses many simple and obvious names. Most programs don't use the whole `Prelude`, so names that aren't needed take up namespace with no benefit. &gt; &gt; ## 2. Monomorphism &gt; &gt; The `Prelude` defines many data types (e.g Lists), and operations on these types. Because the `Prelude` is imported automatically, programmers are encouraged to write their programs in terms of non-overloaded operators. These programs then fail to generalize. &gt; &gt; This is a highly non-academic concern. Many widely used libraries, such as `Parsec`, operate only on lists and not the newer and more efficient sequence types, such as bytestrings. &gt; &gt; ## 3. Supports obsolete programming styles &gt; &gt; The `Prelude` uses, and by means of type classes encourages, obsolete and naive programming styles. By providing short functions such as `nub` automatically while forcing imports to use sets, the `Prelude` insidiously motivates programmers to treat lists as if they were sets, maps, etc. This makes Haskell programs even slower than the inherent highlevelness of laziness requires; real programs use nub and pay dearly. &gt; &gt; More damagingly, the `Prelude` encourages programmers to use backtracking parsers. Moore's law can save you from nub, but it will never clarify `"Prelude.read: no parse"`. &gt; &gt; ## 4. Stagnation &gt; &gt; Because every program uses the `Prelude`, every program depends on the `Prelude`. Nobody will willingly propose to alter it. (This is of course Bad; I hope Haskell' will take the fleeting opportunity to break to loop) &gt; &gt; ## 5. Inflexibility &gt; &gt; Because of Haskell's early binding, the `Prelude` always uses the implementation of modules that exists where the `Prelude` was compiled. You cannot replace modules with better ones. &gt; &gt; ## 6. Dependency &gt; &gt; Because every module imports the `Prelude` every module that the `Prelude` depends on, mutually depends with the `Prelude`. This creates huge dependency groups and general nightmares for library maintainers. &gt; &gt; ## 7. Monolithicity &gt; &gt; Every module the `Prelude` uses MUST be in base. Even if packages could be mutually recursive, it would be very difficult to upgrade any of the `Prelude`'s codependents. &gt; &gt; ## 8. Monolithic itself &gt; &gt; Because the `Prelude` handles virtually everything, it is very large and cannot be upgraded or replaced piecemeal. Old and new prelude parts cannot coexist. &gt; &gt; ## 9. One-size-fits-all-ism &gt; &gt; Because the `Prelude` must satisfy everyone, it cannot be powerful, because doing so would harm error messages. Many desirable features of Haskell, such as overloaded map, have been abandoned because the `Prelude` needed to provide crutches for newbies. &gt; &gt; ## 10. Portability &gt; &gt; Because the `Prelude` must be available everywhere, it is forced to use only least-common-denominator features in its interface. Monad and Functor use constructor classes, even though MPTC/FD is usefully far more flexible. The `Class_system_extension_proposal`, while IMO extremely well designed and capable of alleviating most of our class hierarchy woes, cannot be adopted. &gt; &gt; ## 11. Committeeism &gt; &gt; Because the `Prelude` has such a wide audience, a strong committee effect exists on any change to it. This is the worst kind of committeeism, and impedes real progress while polluting the `Prelude` with little-used features such as `fail` in `Monad` (as opposed to `MonadZero`) and until. &gt; &gt; ## 12. There is no escape &gt; &gt; Any technical defect in `Map` could be fixed by someone writing a better `Map`; this has happened, and the result has been accepted. Defects in the `PackedString` library have been fixed, with the creation and adoption of `ByteString`. Defects in `System.Time` have been fixed, by the creation and adoption of `Data.Time`. Ditto for `Binary` and `Array`s and `Network` and `Regex`. But this will never happen for the `Prelude`. A replacement `Prelude` cannot be adopted because it is so easy to take the implicit import of the default one. Nobody will go out of their way to `import Prelude() ; import FixedPrelude`. Psychology trumps engineering. &gt; &gt; ## 13. There can be no escape &gt; &gt; The `Prelude` was designed by extremely smart people and was considered close to perfect at the time. It is almost universally hated now. Even if all the issues I describe could be fixed (which I consider unlikely), the `Prelude` will almost certainly be hated just as universally five years hence. &gt; &gt; ## 14. My future &gt; &gt; Given all these issues, I consider the only reasonable option is to discard the `Prelude` entirely. There will be no magic modules. Everything will be an ordinary library. HOFs like `(.)` are available from `Control.Function`. List ops come from `Data.List`. Any general abstractions can be added in abstract `Sequence`, `Monad`, etc. modules. Haskell will regain the kind of organic evolution whose lack currently causes Haskell to lose its lead over Python et al by the day. 
Hehe yes I completely agree and I'm certainly not advocating writing involved programs in single scripts. In my view the approach I wrote about is geared around getting up and running really quickly or on contained scripts like Shake files. Such is my agreement with you that I think having an easy way to migrate _out_ of a single script to a proper project is very important!
Definitely makes sense. Being able to use a single script to do nifty stuff is very cool for sure. There's a lot of stuff in Haskell that requires external packages. Some very basic stuff. Your script makes it easy to write single file scripts that use external packages.
I think there is a catch here: each word is separated by a new line, while in the other excercise, it can be either a space or a new line
Two thoughts I had immediately: Data.HashTable doesn't support unpacked hashtables (where values are in a flat array instead of an array of pointers to heap objects). This is really bad for performance here and afaik there is no library that offers this in haskell. I am not sure if the best solution would be an extra hashtable implementation, some backpack hackery or a typeclass abstraction over arrays that picks a nice representation. Strict bytestring are a tuple of ByteArray, Offset, and Length. If you call 'words' we get a bunch of ByteString's which reuse slices of the ByteArray chunks. The hashtable will keep references of these ByteString slices for comparisons so the bytearray chunks will stay in memory. So we save on some copying but use more memory though gc effects might play some weird role. Manual calls to `copy` would help if you wanted to reduce peak memory usage.
Maybe I'm missing something here but can't you just use [Cabal scripts](https://www.haskell.org/cabal/users-guide/nix-local-build.html#cabal-new-run)?
The first rule of teaching Haskell to beginners is to *not* talk about `unsafePerformIO`. Extracting pure values out of `IO` is almost never what you actually want. I remember going down that path myself. `ghc` and the boot libraries collectively use `unsafePerformIO` a fair bit, since they are at times dealing with very low-level concerns. Yet even there, it gets used in just under 2.5% of files! `unsafePerformIO` is used at most 117 times in the entire GHC + boot libraries codebases. I'm omitting the testsuite and various test files. I additionally tried to rule out line comments, but this won't catch mentions in block comments or pragmas: ghc$ rg "^[^-].*unsafePerformIO" --glob "*hs" --glob '!testsuite' --no-heading --glob '!nofib' --glob '!*tests*' | wc -l 117 These uses are concentrated in only 50 (out of 2019) files: ghc$ rg "^[^-].*unsafePerformIO" --glob "*hs" --glob '!testsuite' --no-heading --glob '!nofib' --glob '!*tests*' --files-with-matches | wc -l 50 ghc$ rg --glob "*hs" --glob '!testsuite' --no-heading --glob '!nofib' --glob '!*tests*' --files | wc -l 2019
I don' understand, why reading the file as a whole should be inefficient. Its just 270 MiBi of data on a machine with 8 GiBi ram. That's like 2.5 % of ram. And the original haskell solution and my solutions fill the whole 8GiBi of ram in seconds if using strict bytestrings.
I compiled everything with -O2 and I didn't mind having 270 MiBi ram occupied by reading the file as a whole. But it fills all my 8 GiBi ram with strict bytetstrings. (not Masses solutions though, which is fine).
I'm working on making a presentation video actually :-) Thank you for the feedback :-) 
I haven't found one that worked consistently and I tried VS Code, Vim, Atom, Sublime Text.
This shouldn't affect that much, assuming that the line length &lt;&lt; number of lines.
The size of a file on disk doesn't really tell what the file would be in memory. In my experience it's always much greater on memory than on disk. Not an expert, but IIUC there's multiple reasons for this; data structure overhead, thunks etc.
Your solution and performance expectations seem totally reasonable to me at first glance. Usually functions like your `addOne` are a red flag for space leaks, but if that's the case here that would more be an issue in the hashtables package (which I'm not sure I've used). You might try forcing the v+1, in case the library is not value strict. Some things to try: - eliminate the sort and time again - look at output with `+RTS -s ` to get a sense of whether you have a leak - heap profile 
The original discussion is correct. There is a monoid freely arising from any semigroup by freely adjoining a distinguished identity element. It isn't related, but this statement of yours is incorrect: "In monoid, we can prove that it exist and only exist one idempotent element, which is the identity element." Consider the (banded) monoid generated by the elements [a,b,i] with a^2 = a, b^2 = b, and i the identity. Or, more familiarly, consider the naturals under multiplication. 1*1 = 1 and 0*0 = 0 -- two idempotents! For an example of an algebraic structure where all idempotents are equivalent to the identity, consider not monoids, but groups!
You might find this library helpful [`bytestring-trie`](http://hackage.haskell.org/package/bytestring-trie).
I love Nix because it lets me automate basically anything. I hate the lack of types, especially "infinite recursion at undefined position"
In theory it could be possible to have a datasets with only words separated by spaces, no? That's said, it should be easily solved by using Attoparsec or something else that stream the contents, as you initially said :)
I'm admit that I am very green in haskell and that I have some difficulties with the [arithmoi](https://hackage.haskell.org/package/arithmoi-0.7.0.0) package and especially with the `Mod` data type ([here](https://hackage.haskell.org/package/arithmoi-0.7.0.0/docs/Math-NumberTheory-Moduli-Class.html)). What I'm trying to do is, given a list of integers `xs` and a integer `n` I want to create a list of element `x :: Mod n` so I wrote: listToMod :: KnownNat m =&gt; Integer -&gt; [Integer] -&gt; [Mod m] listToMod n xs = map (\x -&gt; (x :: Mod n)) xs It may be obvious why it does not work, but I would love some help to get it right.
You can’t at once criticise critics for using that tone and say that you can’t criticise critics for using that tone. Criticism of tone is sometimes warranted; the problem is when you start to conflate tone with content. It’s an easy mistake to make, because the lines aren’t as black and white as some people like to think.
There seem to be several different "this"s in peoples minds.
ephrion has already answered your question, but you might be interested in Lindley and McBride's [Hasochism paper](http://homepages.inf.ed.ac.uk/slindley/papers/hasochism.pdf), which provides patterns and a vocabulary for faking dependent types in GHC Haskell. Some of the patterns it discusses are present in the `singletons` library.
Good idea. Please propose it to the [Core Libraries Committee](https://wiki.haskell.org/Core_Libraries_Committee).
Same issue as https://www.reddit.com/r/haskell/comments/avtfzv/comment/ei8gx6e - using :: is an assertion, not a cast.
Oops, I messed up the OP post a bit. The goal is to get a free `Semigroup` instance, _via_ `GSemigroup`, via DerivingVia. Today I usually write: data MySemigroup = ... deriving stock (Generic) deriving anyclass (GSemigroup) instance Semigroup MySemigroup where (&lt;&gt;) = gmappend and it's this last little bit I want to shave away with `DerivingVia`. But for this particular type class, which itself is in `base` but the generic-deriving machinery is in `semigroups`, I think the newtype I want would actually belong in `semigroups` to avoid an orphan instance. newtype GenericSemigroup a = GenericSemigroup a instance (GSemigroup a, Generic (Rep a) =&gt; Semigroup (GenericSemigroup a) or something like that.
I didn't say it's wrong to feel that critics have bad tone -- I share travis' feelings and they are fine, which is why I'm agreeing with him. I said it's wrong to respond to critics -- that is, the original poster, whose post my comment was not directed at -- by saying they have bad tone. That disinvites criticism. I wouldn't ordinarily reply, but I'm annoyed that you're trying to technicality me on this -- this is basic optics. If you run a restaurant and someone says "this tastes like it came out of a microwave" you can privately think "that person is rude for accusing me of microwaving my food," but you don't *say* that, because it disinvites criticism. Smart people are really good at explaining away criticism and this is one of the tactics people use to do it -- frequent critics who don't like having their comments explained away will just leave.
In the category `Hask`, objects are types like `Int`, `String`, `Maybe Int`, `Maybe (Maybe Int)`, and so on. Functions represent morphisms, and polymorphic functions like `id` or `(: [])` each represent many morphisms - so `(: []) :: Int -&gt; [Int]`, `(: []) :: Maybe Int -&gt; [Maybe Int]`, etc. are separate morphisms. And also (since `(-&gt;)` is a type constructor) there are the objects `Int -&gt; [Int]` and `Maybe Int -&gt; [Maybe Int]` and so on, and those types are inhabited by monomorphic versions of `(: [])`. I haven't seen any discussion of how this relates to higher-ranked types. ("Category theory for programmers" doesn't seem to mention those, and "thinking with types" doesn't seem to mention category theory.) Are there any complications with that? E.g. if we consider the function \f -&gt; (f 3, f True) :: (forall a. a -&gt; [a]) -&gt; ([Int], [Bool]) Is there an object in `Hask` for the type `forall a. a -&gt; [a]`, inhabited by the polymorphic function `(: []) :: forall a. a -&gt; [a]`? Or if you try to do that, do things break?
I've always loved Intero's philosophy of: "If stack ghci works, Intero just works". It's simple, yet full fledged enough, to stay my main IDE in a process for years.
Resplendent!
I have no experience with embedded systems, but isn't laziness going to be a non-starter? Perhaps a strict language like Idris might be a better starting point and sufficiently Haskell-like for your purposes?
Has your course covered folds yet? Folds (eg via `foldl` and `foldr`) explicitly encode the idea of walking a list (or other `Foldable`) and building up an accumulator by repeatedly combining the current accumulator with the next element in some way.
Missed this reply somehow, thanks for the response. Yes you are very much right. I have since learnt I am dealing with _authorization_ as opposed to _authentication_. I found this (issue)[https://github.com/haskell-servant/servant-auth/issues/73] which proposes a couple of solutions. While these are more than adequate, I do like the idea of possibly wrapping routes in some function that could be run before the route is hit (which would allow authorization or whatever to happen) but I think this is not possible at the minute.
The `GHC.Generic` types now have `Semigroup` instances, which removes the need for a separate class `GSemigroup` (I wrote about this peculiarity in https://blog.poisson.chat/posts/2018-03-28-generic-data.html). So now generic deriving of `Semigroup` instances for records is reduced to this, no extra dependencies: instance Semigroup MySemigroup where a &lt;&gt; b = to (from a &lt;&gt; from b) For `DerivingVia`, there is a newtype to be found in my [generic-data](https://hackage.haskell.org/package/generic-data-0.5.0.0/docs/Generic-Data.html#t:Generically), but it would be nicer to have in base of course.
Every time you use unsafePerformIO, a postdoctoral student of category theory is fired. Please be conscious. Do not hurt the planet 
A good starting point for something like this might be Purescript. It is strict and more importantly does not depend on a large runtime system, so you can roll your own. There is also an ecosystem of alternative backends that could give you some ideas.
I'm starting to understand that. But I still don't get how to instantiate a value for a dynamic phantom type. 
My conclusion as well. I've tried multiple things: vscode, hie, neovim, emacs, maybe I'm forgetting something. I find Intero's design choices to be most reliable. Maybe this will change once [ghc-lib](http://neilmitchell.blogspot.com/2019/02/announcing-ghc-lib.html) becomes more widely used.
Wouldn't higher-order polymorphism refer to polymorphism over higher-kinded type variables like functors? Just like higher-order functions refer to functions that abstract over functions.
I think you probably realize all this, but traditionally, embedded systems have been targeted primarily by low-level C(++) because efficiency (both memory and speed), which is basically the opposite end of the spectrum from typical Haskellers' preferences. As engineering improves and these requirements are relaxed, it's become increasingly common to target w/ higher-level languages, and I suppose there's nothing preventing someone from doing it with a functional language. &amp;#x200B; In terms of your project, I'd think carefully about what exactly you want to achieve. GHC's compilation pipeline goes something like Haskell -&gt; Core (small functional language) -&gt; C-- -&gt; C // LLVM // Native. So a C or LLVM backend would let you do haskell on embedded systems, but I suspect it wouldn't leave you feeling like your project was very haskellic. A Core compiler would perhaps be more interesting, but it seems probable that the more stages you take on yourself, the worse your results will be (as measured by performance of compiled programs). The end-product be less "useful", but I'd personally be tempted to implement your own functional language with haskell-like syntax and compile it all the way down to native yourself. This would let you compare/contrast between your language+compilation differed from eg. standard Haskell/its compilation, according to how the constraints of your target (primarily low memory usage) affected design decisions.
Have you ever heard of ATS? http://www.ats-lang.org/Home.html#What_is_ATS I'm not certain how lightweight the compiler is, but it's a dependently typed, ML-like language which compiles to efficient code which may be appropriate for lower-level concerns.
Since Functor is carrier of a polymorphic function, I think Functor polymorphism is just a Natural Transformation.
Instead of `unsafePerformIO` use `delayedPureValue`: {-#NOINLINE delayedPureValue#-} -- # delay the assignment of a pure value until the value is needed. The value is determined by an IO computation (pending full EKmett CT certification) delayedPureValue :: IO a -&gt; a delayedPureValue= unsafePerformIO 
Some links on topic: [https://hackage.haskell.org/package/atom](https://hackage.haskell.org/package/atom) [http://leepike.github.io/Copilot/](http://leepike.github.io/Copilot/) [https://ivorylang.org/ivory-introduction.html](https://ivorylang.org/ivory-introduction.html) [https://haskellembedded.github.io/](https://haskellembedded.github.io/) [https://www.seas.upenn.edu/\~acowley/papers/hocl.pdf](https://www.seas.upenn.edu/~acowley/papers/hocl.pdf) [http://www.cs.ox.ac.uk/ralf.hinze/WG2.8/27/slides/mark.pdf](http://www.cs.ox.ac.uk/ralf.hinze/WG2.8/27/slides/mark.pdf)
Oh I think I habe to clarify. ~~like functors~~ like abstractions over functors
I promise I'm not trying to nitpick. I understand the argument that criticising the tone of criticism disinvites further criticism (which is useful independent of tone), but I don't understand what difference it makes whether when you're criticising them, you direct it at them or only in their vicinity. Yes, your criticism of OP was not directed at OP, but OP can see this comment thread, so what difference does it really make? &gt;Smart people are really good at explaining away criticism and this is one of the tactics people use to do it -- frequent critics who don't like having their comments explained away will just leave. I agree, but /u/travis_athougies didn't use the tone as an excuse to dismiss OP. It was an aside. Yes, maybe even that is enough to disinvite some critics, but I don't think everything has to be optimised for the critics. /u/travis_athougies is right that the tone of criticism can disinvite answers, so it's a two-way street. That matters because the answers can be valuable for bringing in newcomers as well — if you care about that — if they adequately show the criticism to be unfounded in parts where it is.
There is effort being put into implementing this functionality but the main issue is doing so in such a way that users do not pay a performance penalty when they don't use it: https://github.com/reflex-frp/reflex-dom/milestone/1
I'm (re)writing bindings to the ODBC API (on linux right now). The API functions use SQLCHAR*, and SQLCHAR is a typedef for unsigned char, so unsigned char *. Regarding encodings, the API story around Unicode is confusing, to say the least. I'll have to get to the bottom of it eventually. It is certainly possible for the underlying vendor-specific driver to marshal strings to a variety of encodings, independently of how it is stored in the dbms (so it will re-encode over the wire if asked). But it turns out that the problem was caused by the SQLGetDIagRec function lying about the length of the error message string it returned. If I assume a null-termed string (which it is returning for now) and ignore the returned length (i.e. don't use the *Len functions) then all is good.
Reminds me of how higher-rank polymorphism is encoded in Scala trait NT[F[_], G[_]] { def apply[A](fa : F[A]): G[A] } Which makes sense. Scala is full of ways to encode cool things by turning your head and squinting. I've always found this sort of thing fun!
Wait, I misread your original post. What's `GSemigroup`? I thought you wanted `GenericInstance` to have a `Generic`-based `SemiGroup` instance, so you could write this: data MySemigroup = ... deriving stock (Generic) deriving Semigroup via (GenericInstance MySemigroup)
Ah! So the `travisB =&gt;&gt; gitHubB` part doesn't mean one is ran "before" the other, they are just mappended by the `=&gt;&gt;` operator.
Yeah! That. `GSemigroup` is from `semigroups`, it has a default instance for `gappend` given `Generic a, GSemigroup (Rep a)`, used like in my example above. As you point out `DerivingVia` + a newtype obviates this type class.
For C `unsigned char *` you want to use Haskell `ByteString`.
My first contact with functional programming was with Haskell. Years later I started to use Elm for real world apps, but I never reached a point where I could use Haskell for backend, so I went to Elixir using Phoenix Framework, which is great, productive, but doesn't have runtime guarantees. When trying to make a REST API using Haskell, everything is difficult, missing libs, or too many libs for a single problem, and people doesn't seem to worry about it because they doesn't use Haskell for this kind of problem. Is this changing? I really miss safety and the clean and expressive syntax of haskell for my backend.
Maybe I'm missing something but why can't we just make use of `DefaultSignatures` and define `Semigroup` in base like class Semigroup a where (&lt;&gt;) :: a -&gt; a -&gt; a default (&lt;&gt;) :: (Generic a, GSemigroup (Rep a)) =&gt; a -&gt; a -&gt; a (&lt;&gt;) = gmappend ? Then if you have `DeriveAnyClass` enabled you can treat `Semigroup` as if it was a built-in class: data MyTy = ... deriving (Generic, Show, Eq, Ord, Binary, Semigroup) profit!
The general approach for programming embedded systems in Haskell seems to be along the route of embedding a DSL in Haskell and then generating C code from it. Eg: [Ivory Lang](https://ivorylang.org/ivory-introduction.html), [Haskino](https://github.com/ku-fpg/haskino). As pointed out by others, it is very hard to have a predictable cost model of your computations when working in a lazy language (something you would dearly want to do for a resource constrained device).
I would call that higher-kinded polymorphism, but there may very well be a way to stretch "higher-order polymorphism" to mean that indeed. I was thinking that "higher-order polymorphic functions" are those which abstract over polymorphic functions.
I completely ignorant when it comes to embedded systems but wanted to share that Rust is very good from functional standpoint and I heard performant enough for low level stuff, so maybe extending Rust making it more purely functional could be an idea?
I am not an embedded guy, but "large runtime + garbage collection + design patterns that promote constantly creating new objects instead of modifying the old ones" doesn't seem like a great match for embedded systems.
Yeah, my first thought was "rust sounds like a better fit".
Make a helper function that takes the input list, plus an extra parameter, which is the sum of all the numbers we've seen so far. Something like this: foo :: Num a =&gt; [a] -&gt; [a] foo xs = go ??? ??? where go [] acc = ??? go (y:ys) acc = ???
Not exactly embedded but the [Futhark](https://futhark-lang.org/) language might be interesting to you. Also [Sixten](https://github.com/ollef/sixten) is a fun experiment
&gt; Which language had good support for writing DSLs? Haskell.
If you want syntax for free and are fine with s-expressions, I recommend [`s-cargot`](http://hackage.haskell.org/package/s-cargot) :) Just one piece of the puzzle though.
The more modern way to do this now with `DerivingStrategies`: data MyTy = ... deriving stock (Generic, Show, Eq, Ord) deriving anyclass (Semigroup)
I think a scan would be more appropriate than a fold.
Ok but why is this considered "more modern"? The supposedly more modern way seems to be almost twice as much to type than the vintage way of writing just one line deriving (Generic, Show,Eq, Ord, Semigroup) and not having to remember whether a class needs to be declared `stock` or `anyclass` and I fail to see the benefit for `data` declarations. 
Ah, indeed you are exactly right. To OP: A scan is like a fold but instead of just emitting the accumulator at the end it emits a list of all of the intermediate values.
Hask is not formally defined, you can even find a few comments about the fact that [Hask is not a category](http://math.andrej.com/2016/08/06/hask-is-not-a-category/) on the web. In Haskell+RankNTypes, quantified types still have a pretty special status. For example, `id :: forall a. a -&gt; a` cannot be specialized to `(forall a. a) -&gt; (forall a. a)`, because type variables range over simple types only. There is an [`ImpredicativeTypes`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#impredicative-polymorphism) extension that may sound like an answer, but I mention it only to say that it's 100% broken (and the docs I linked say so). So if you want `id` to correspond to identity morphisms, `Functor` to correspond to functors, that doesn't seem compatible with quantified types as objects in the case of Haskell. But outside Haskell, that idea seems plausible.
To convert an `Integer` to some numerical type you can use `fromInteger` (more precisely, it has to be an instance of `Num`, which `Mod m` is). map fromInteger xs
how?
I'm afraid "a more efficient haskell" is probably too ambitious a goal (since it's the same goal as ghc), and the best way towards achieving it is likely to contribute to ghc. But it seems to me "efficiency" isn't as important as _predictable_ (or even provable?) resource usage. Maybe you could make a contribution there by using some kind of refinement mechanism, like Liquid Haskell? Or maybe what you really want to do is try to extend GHC to other architectures? That's probably attainable via LLVM. Mostly though I agree with the idea of exploring DSLs though.
There is more than one way to get a `Semigroup` instance for a datatype. Off of the top of my head, we have: * `Data.Semigroup.First` * `Data.Semigroup.Last` * `Data.Semigroup.Min` (requires `Ord`) * `Data.Semigroup.Max` (requires `Ord`) * `Data.Semigroup.Generic.GSemigroup` (requires that type is a product of `Semigroup`s, cannot be used for data types with multiple data constructors) Using `DefaultSignatures` would privileges one of these over the others. In fact, `DefaultSignatures` frequently has this problem. This is one of the motivations for `DerivingVia`, which I think is a better solution here.
Oh very cool, thank you! Ive been toying with an idea of a unboxed lisp based on growable vectors or somesuch, so looking at what they did with these languages will be helpful. (I know other folks have done lisps like this, Im just interested in thinking about it)
&gt; I used to like it too. I still do; particularly for xmonad and add-ons. That said, it definitely has disadvantages, and I think stack is actually a better choice for developers. (The *users* of (e.g.) Pandoc should be able to simply get it from their OS distributor.)
Well, I wouldn't mind report updates happening on a 3yr. schedule. :)
I think [this thread](https://www.reddit.com/r/haskell/comments/alrm5v/monthly_hask_anything_february_2019/egv8z0y/) from last month might be helpful.
`DeriveAnyClass` and `GenaralizedNewtypeDeriving` conflict with each other. You can do both without `DerivingStrategies`. So we get a slightly more verbose declaration that communicates more clearly without ambiguity.
I believe the most popular library for this purpose is [servant](http://hackage.haskell.org/package/servant). I've played briefly with it and liked it, and afaik it's commonly used in production environments.
Nah, “order” just refers to the “nesting depth” as it were—it can apply equally well to quantifiers (a “higher-rank” function is a higher-order polymorphic function), kinds (a “higher-kinded” type is a higher-order type constructor), and of course functions, but it’s good to be specific about *of what* the order is that you’re talking about.
&gt;extract always knows how to get value from the context w. In particular, this means that instances of the Comonad typeclass could be only for non-empty structures. Can't you just do something like? data List (isempty :: Boolean) a where Nil :: List True () Cons :: a -&gt; List x a -&gt; List False a And then only give a comonad instance for `List False`?
I use [http://hackage.haskell.org/package/generic-monoid](generic-monoid) for this. 
Reminds me of Java's interface functions: interface Function&lt;T, U&gt; { public U apply(T x); } In Haskell I guess it'd be something like: class Function f where type Parameter f :: * type Result f :: * apply :: f -&gt; Parameter -&gt; Result map :: Function f =&gt; f -&gt; [Parameter f] -&gt; [Result f] map f l = let iter [] = [] iter (h : t) = f h : iter t in iter l Might be a neat way to encode this sort of thing in a new functional language.
Yes, you can. But `List False` is a non-empty structure :) According to the type of your list, it can be in two modes. You just define instance for one of the modes.
A couple years ago I experimented with porting Idris to the Arduino, [https://github.com/stepcut/idris-blink](https://github.com/stepcut/idris-blink) I think a language like Idris is perhaps an even better choice than Haskell for embedded systems because you can use the type system to describe the hardware in detail. As a trivial example -- you can represent a string of 50 RGB leds as, `Vect 50 RGB`. (Admittedly, Haskell can do that now too). A more involved system would be using the type system to describe protocols. Maybe something like https://github.com/edwinb/Protocols One advantage of Idris (at the time anyway) is that it outputs relatively portable C code, and the RTS (runtime system) was very small. Implementing a new compiler is a lot of boring work that has already been done. There is nothing new or interesting about writing a syntax parser, etc. One reason Haskell was created was because researchers got tired of creating their own languages from scratch and wanted some sort of lazy language that they could add experimental features to with out having to start from the ground up. In summary, Idris has already done the 'boring' part of implementing a lightweight compiler for a Haskell-ish language, and you just need to do the interesting part of adapting it for embedded development and demonstrating how it is a compelling choice. 
&gt; Void Linux Jesus! You should've lead with that :) You can't expect tools and libraries maintainers support every linux distro in existence.
Equally disorganized response - I haven't had time to really think it through but I've had the nagging suspicion that working in the relational model is fundamentally wrong. N-ary relations are a generalization of binary relations and the thought seems to be that fewer N-ary relations would be easier to conceptualize for users and better for storage or performance. The natural tendency is to maximize N, but the resulting relation rarely means what its user thinks it does. This is especially problematic when changing data. Normalization rules were created to formalize these problems. Normalization is actually pretty funny. * you don't need all those binary relations, just pull them up into N-ary ones * see, you get all this information from one relation * What? you need to update that information * ok, go for it - but be careful to always update z to f(x,y) * That's too hard? reduce N and go to second normal form * oh, but be careful to always .... * You know what, you should just reduce N and go to third normal form * . * . * . * You know what just make N=2 and go to BKxyzPotato normal form Seriously, all advice on structuring relational databases for correctness ultimately boils down to going back to binary relations. The big problem is that we don't allow ourselves to have different formats for different needs. Validation and manipulation need N=2. Presentation and storage space benefit from higher N. Manipulation performance has an optimal N for different manipulations that can be hard to determine and balance. tl;dr model relational data as a bunch of functions whose domain and co-domain vary over time and put a view on top of that to recover n-ary relations for presentation or serialization 
Exactly how embedded are we talking?
Thanks for doing this write-up! Don't get discouraged by getting ~50% of people downvoting you (Reddit is an Outrage Machine after all, and for some odd reason people chose it to discuss programming problems). You made effort writing this all down and it is valuable. While most of the issues you bring up are considered solved by Haskell practitioners (and from experience with training people professionall in Haskell I'm relatively certain that after you put in another 100-200 hours, you will feel so too), it's very important to be continuously reminded of what problems newcomers to the language face. Because that is the way things improve: See what people struggle with, and improve tutorials or write new books or improve compiler or libraries to address it. If you want to maximise your impact on improving this, continue to write down all the things you find weird and unintuitive, and once you're a Haskell intermediate or expert in the near future, revisit that list and help fix these things (because then you will be able to).
It's not that fast and easy to add something to `base`. But it's should be pretty easy to add this `newtype` to one of the alternative preludes. As a maintainer of `relude` I would be glad to see this feature implemented there :) And when it actually appears in `base`, we can reexport this data type from `base`: * https://github.com/kowainik/relude
There are several alternatives for setting up the parser / interpreter for your DSL, I strongly recommend the `megaparsec` library for this case. Alternatives are generally less user friendly and/or focus on parser performance, which you don't need for parsing a simple query/filter DSL. You may want \`attoparsec\` for parsing out the logstore entries, depending. \`megaparsec\` can be surprisingly performant for all it's added features, and might be good enough for your case. They are similar enough that you can probably start with one and switch out for the other if you need to without much refactoring, as long as the format isn't terribly complex, which most log formats probably aren't. For handling streaming network IO you'll want a good streaming library, I recommend `conduit`, but you have several good options in this space.
Perhaps another interesting point is: Many Haskell experts have at some point been in your situation, being frustrated with an apparent wall of complexity, questioning whether it's really necessary. I've been in that situation multiple times. The answer to whether it's worth it often changes quite a bit depending on what your task is and what past experiences are. If one is mostly facing "easy going" web programming, or other tasks where most harder-ish problems can be solved within a day, it appears natural that Haskell feels like overkill. But after typing one's 10000th `if err != nil` at 3 seconds each, debugging one's 1000th `NullPointerException` at 20 minutes each, or investigating one's 5th barely reproducible nondeterministic production `segmentation fault` at 2 weeks each, one starts to long _really hard_ for tooling that just guarantees that one never has to deal with that crap again (or at least drives the probabilities in that direction a lot). In that situation, one becomes very tolerant to investing initial time to get past steep learning curves. And after that, the learning curve is gone, and one sees no issue in continuing to use the harder-to-learn-but-now-easy tool for the "easy going" tasks as well.
I am sceptical. The definition of CString (from Foreign.C.String) is Ptr CChar, but Bytestring is: data ByteString = PS {-# UNPACK #-} !(ForeignPtr Word8) -- payload {-# UNPACK #-} !Int -- offset {-# UNPACK #-} !Int -- length deriving (Typeable) ... which is not equivalent. I expect that using ByteString naively in my foreign imports will lead to tears. I am using this code to marshall from CString to Text: bs &lt;- unsafePackCString cs return $! decodeUtf8 bs 
There are marshalling functions for `ByteString`s: https://hackage.haskell.org/package/bytestring-0.10.8.2/docs/Data-ByteString.html#g:25
The simple way, of course, is to do this: ``` data CardType = CommonItem | UniqueItem | Ally | Injury data Attribute = Price Int | NumberOfUses Int | CantBeStolen -- | in reality, would also have names and descriptions data Card = Card { cardType :: CardType , cardAttributes :: Set Attribute } deck :: [Card] deck = [ Card CommonTime (Set.fromList [Price 3, NumberOfUses 2], Card UniqueItem [CantBeStolen] ] ``` One reason for trying to use a fancier type is to prevent you from accidentally creating invalid cards. But no cards are created from thin air in the game, they are all known before the game starts. So a simple testsuite will ensure you have only generated valid cards. Another goal might be to prevent runtime errors where you attempt to use a card incorrectly. To do that, all the critical information about the card needs to be available at the type-level. It sounds like you are trying to achieve that by creating a unique type for each possible combination of attributes -- like `PriceNumberOfUses`, `PriceCantBeStolen`, `NumberOfUsesCantBeStolen`, etc. That would be unwieldy. A different option would be to lift all the attributes to type level lists. So you would have something like: ``` deckOfCards :: Cards '[ Card 'CommonItem '['Price 3, 'NumberOfUses 2], Card 'UniqueItem '['CantBeStolen] ] ``` That deck only has two cards, but you know exactly what they are. Here is a simplified example for a standard deck of cards, https://github.com/stepcut/deckofcards That only gets you as far as ensuring you have a valid deck of cards. The next step would be to create a 'shuffle' function. The type will ensure that when you shuffle the deck you don't end up with missing cards or duplicate cards. Though it won't ensure that the deck is actually 'shuffled'. Similarly, the `deal` function can enforce at the type level that each card is either dealt to exactly one user or is still in the pile of undealt cards. More challenging is ensuring at the type level that every function only moves the game from one valid game state to another. While *possible* it is not clear it is worthwhile. Unlike a library like dependent vector types where functions are going to be mixed and matched by many different users -- implementing a game like arkham horror is a closed system. It's probably easier and faster to just use simple types and write a testsuite. I (partially) implemented machi koro, and I just ended up with simple record types, some trusted primitives like `bankTransfer`, and a test suite. In short, your method seems insufficient to ensure that cards are not accidentally cloned or lost during game play. But enforcing that gets your pretty far down the type rabbit hole, and still doesn't solve all your problems. 
&amp;#x200B; Get Programming with Haskell -&gt; Think with types -&gt; Haskell in Depth
An EDSL may also be a possible approach, for example [frp-arduino](https://github.com/frp-arduino/frp-arduino).
I've had success using the [AccelerateHS library](https://www.acceleratehs.org). Its API was simple and Haskell-y, it integrates with the Haskell ecosystem well, and all in all it's very comfortable to use. Stepping outside of the realm of Haskell, you might be interested in [Futhark](https://futhark-lang.org), an ML made to run on the GPU.
Thanjs for asking this. ^(Though judging from oast similar questions there will be barely any positive andwers.)
You may want to check out the workshop series for [Functional HPC](https://icfp18.sigplan.org/track/FHPC-2018-papers), which has been running since [2012](https://sites.google.com/site/fhpcworkshops/fhpc-2012) ([2013](https://sites.google.com/site/fhpcworkshops/fhpc-2013, [2014](https://sites.google.com/site/fhpcworkshops/fhpc-2014), [2015](https://www.conference-publishing.com/list.php?Event=ICFPWS15FHPC), [2016](https://icfp16.sigplan.org/track/FHPC-2016-papers), [2017](https://icfp17.sigplan.org/track/FHPC-2017-papers), and has now become [FHPNC](https://icfp19.sigplan.org/home/FHPNC-2019)). Not all of the work that has been presented there over the years has been core HPC material, or in Haskell for that matter, but it's the closest thing to a collection of relevant resources. Generally however, there's not been a lot of work real HPC work done in functional languages, let alone Haskell. [Accelerate](http://www.acceleratehs.org/), as pointed out by others, is very good, but it's only a small part of what you need for HPC, and it doesn't deal with the data structure issue at all.
[I mentioned you can do this in Scala](https://www.reddit.com/r/haskell/comments/b5pojg/higherrank_types_in_standard_haskell/ejfunwm/), but you can actually write this exact code in Java! import java.util.List; import java.util.List; import java.util.Arrays; class Main { // The interface is like the type class static interface Permutation { // Higher-rank type and all! public &lt;A&gt; List&lt;A&gt; perm(List&lt;A&gt; as); } // Implementations of the interface are our Haskell types static class Id implements Permutation { public &lt;A&gt; List&lt;A&gt; perm(List&lt;A&gt; as) { return as; } } public static void main(String[] args) { System.out.println(new Id().perm(Arrays.asList(1,2,3))); } } &amp;#x200B;
OverloadedStrings was the culprit ;)
Here you have the start: https://wiki.haskell.org/Simple_Servers You should add some parsing and voila! you have your own "framework" 
There's a new paper (not yet freely available) about Haskell and energy efficiency from the same research team: * [http://greenlab.di.uminho.pt/2019/03/11/new-paper-in-jss/](http://greenlab.di.uminho.pt/2019/03/11/new-paper-in-jss/) * [https://doi.org/10.1016/j.jss.2018.12.014](https://doi.org/10.1016/j.jss.2018.12.014) &amp;#x200B; Here a couple of links about the team: * [http://greenlab.di.uminho.pt/](http://greenlab.di.uminho.pt/) * [http://green-haskell.github.io/](http://green-haskell.github.io/)
I hope for a future where Rust &amp; Haskell have a baby. I know that Rust already has a lot of **traits** from Haskell, but I want more cross contamination.
I have a simpler solution. No extensions, no type classes :P ``` import Unsafe.Coerce coolThing :: ([a] -&gt; [a]) -&gt; ([Integer], [String]) coolThing perm = (unsafeCoerce perm [0, 1], unsafeCoerce perm ["Higher", "Rank", "Polymorphism"]) examples :: IO () examples = do print (coolThing id) print (coolThing reverse) print (coolThing (reverse . rot 2)) print (coolThing (rot 1 . reverse)) rot :: Int -&gt; [a] -&gt; [a] rot n xs = zs ++ ys where (ys, zs) = splitAt n xs ``` \&lt;/troll&gt;
Some random thoughts: &amp;#x200B; 1. Haskell is, denotationally, a non-strict functional language; what does this mean? well, in a strict language \`f \_|\_ = \_|\_\`; always, while in a non-strict language \`f \_|\_ = some\_value\_that\_might\_be\_|\_\`. Now, GHC implements a version of said non-strict Haskell using an operational call-by-need semantics (lazy evaluation). It's the implementation of this operational semantics on a regular CPU that implies some of the things mentioned in this thread: run-time, garbage-collection, etc. artifacts that might not work well for the resource-constrained world that is embedded systems. However, a compiler such as Clash ([https://clash-lang.org/](https://clash-lang.org/)) compiles a (semantic) subset of Haskell to circuits; where the generated circuit definitely does not comply to any operational call-by-need semantics, but it does comply to the denotational non-strict semantics. So what's my point? well, given that Clash can transform a semantic subset of Haskell to a circuit, using a fixed amount of chip area, perhaps you can translate such a subset of Haskell that always uses the same amount of fixed memory. 2. Programming for embedded systems usually means being confronted with resource constraint explicitly, the usual resource constraint being memory. So perhaps you can build an EDSL that generates C-code, that tracks (via the type system) how much memory is being used at any point in time. Another option might be to pick some embedded target with fixed compute latencies (no caches (but scratchpad memory instead), no OoO-execution, etc.), and have an EDSL that tracks how much time a computation takes. Then have a code-generator that guarantees the execution will at most as much time as annotated in the DSL. Then if you have time you can extend this to worst-case vs average-case execution time; pick a platform with variable latency (i.e. cache), etc.
That's a useful list! Let me add RAW-Feldspar: &lt;https://github.com/Feldspar/raw-feldspar&gt;
Analyzing languages against C for energy efficiency on architectures specifically built to execute code. What would this look like on LISP machines? Furthermore, what’s the effect of the programming language vs the effect of architectural changes and enhancements? Also what’s this data look like with real workloads over you benchmarks? This data is an interesting start, but there’s a lot more investigation that needs to be done before drawing any meaningful conclusions from this. 
With Haskell syntax
Use of electricity is a side effect: https://xkcd.com/1312/
&gt;Analyzing languages against C for energy efficiency on architectures specifically built to execute compiled C code. What would this look like on LISP machines? Where are you going to find a LISP machine? Why would that be a meaningful metric? Almost all hardware is 'built to execute compiled c', or, more accurately, has grown symbiotically with C compiler development over the course of the last 40 years. So, measuring the energy consumption of various languages on such hardware architecture at least has some practical output connected to expected real-world conditions, we can derive some sort of value from those measurements. If we want to be truly agnostic with regards to execution platform, we lose ALL value from this kind of statement - We could write a hardware JavaScript engine that I'm sure would be drastically more energy efficient than the average interpreter running on commodity hardware, but that means absolutely nothing. &amp;#x200B;
And higher ranked traits - currently you can't define things like a `Monad` trait.
Why does that mean nothing?! That’s thinking small. The great Hennessy and Patterson have also recently pointed out that the advent of high level languages taking over the computing world, the related domain specific languages, and the ending of Dennard scaling, make this a perfect time to look into architectures for different languages again, similar to the lisp machines of the past. https://m-cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext
&gt; What would this look like on LISP machines? C runs just as well on LISP machines. There isn't really anything LISP machines are better at then normal machines; they are just built such that they are more convenient for building LISPs.
&gt; What you'd like to do is to create as many threads as there are cores OS threads or green threads?
Especially if comfortable temperature depends on it: https://xkcd.com/1172/
When testing concurrent programs, it's useful to be able to control the scheduler in a deterministic way. Does your library make that possible?
A quick test with my [stdio](http://hackage.haskell.org/package/stdio) library gives ~6.7s on my machine, would you please help me test on yours? add stdio and unordered-containers to your cabal file and compile with GHC 8.6. ```haskell {-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE TypeApplications #-} module Main where import Std.IO.Buffered import Std.IO.StdStream import Control.Monad import Std.Data.Vector as V import Std.Data.Vector.Sort as V import Std.Data.PrimIORef import Data.IORef import Data.HashMap.Strict as HM import Std.Data.Builder as B import Data.Function main :: IO () main = do wcMapRef &lt;- newIORef HM.empty go wcMapRef wcMap &lt;- readIORef wcMapRef wcVec &lt;- V.pack @V.Vector &lt;$&gt; forM (HM.toList wcMap) (\ (w, wcRef) -&gt; do wc &lt;- readPrimIORef wcRef return (w, wc)) let wcVec' = V.mergeSortBy (flip compare `on` snd) wcVec putStd $ forM wcVec' (\ (w, wc) -&gt; B.int wc &gt;&gt; B.char8 ' ' &gt;&gt; B.bytes w &gt;&gt; B.char8 '\n') where go wcMapRef = do line &lt;- readLineStd if V.length line &lt;= 0 then return () else do let ws = V.words line forM ws $ \ w -&gt; do wcMap &lt;- readIORef wcMapRef case HM.lookup w wcMap of Nothing -&gt; do wcRef &lt;- newCounter 1 modifyIORef' wcMapRef (insert w wcRef) Just wcRef -&gt; do modifyPrimIORef wcRef (+1) go wcMapRef ``` 
Thanks for pointing this out - for solutions I can use right this minute, this is the nicest I’ve seen. (Having something in base would be even better ofc.)
I agree that for native foreign imports, you'll probably want to use `Ptr CUChar` ~= `unsigned char *` (not sure if you need `Ptr` or `ForeignPtr`). But, I wouldn't deal with those values directly much at all, and use `ByteString` (or some other non-`Ptr` type) for all but the final/initial foreign wrappers. Note that your `decodeUtf8` there fixes the encoding. But, if you can count on a particular encoding, you are right that `Text` is probably better than `ByteString`.
Green threads. Either \`forkIO\` or \`forkOn\` is used, depending on the computation strategy: \`ParN\` or \`ParOn\` respectfully.
Thanks that is good advice! 
If you specify a single worker, with either `ParOn [1]` or `ParN 1` or simply `Seq`, than the order of computation will be deterministic, otherwise all bets are off. When many workers are being used only one part is guaranteed to be deterministic, namely the output of computation. For example there is a function in the library implemented using the scheduler: ```haskell traverseConcurrently :: (MonadUnliftIO m, Traversable t) =&gt; Comp -&gt; (a -&gt; m b) -&gt; t a -&gt; m (t b) ``` There is a guarantee that order of input elements of `t a` structure will match the order of elements in output `t b`. But the actual order in which the elements will be computed with `(a -&gt; m b)` is non-deterministic with many workers. There is also another example in the [README](https://github.com/lehins/haskell-scheduler#keeping-the-results-of-computation) that talks about it. Hope I answered your question. If not, could you elaborate with an example of which part you'd expect to be deterministic.
&gt; But after typing one's 10000th if err != nil at 3 seconds each, debugging one's 1000th NullPointerException at 20 minutes each, or investigating one's 5th barely reproducible nondeterministic production segmentation fault at 2 weeks each, one starts to long really hard for tooling that just guarantees that one never has to deal with that crap again (or at least drives the probabilities in that direction a lot). You can get rid of segfaults in a load of memory-safe languages. You can get rid of NPE (or equivalent) in *most* languages with generics / parametric polymorphism. (Java makes it hard; but other JVM language do have the ability to create non-nullable types.) The `err != nil` can be solved through judicious use of exceptions, though unchcked exceptions may be it's own basket of snakes. I like Haskell for all these reasons, too, but they aren't what makes it truly unique.
&gt; class Function f a b where &gt; apply :: f -&gt; a -&gt; b Here, all the type variables are in the instance head, so it's equivalent to plain, low-rank functions. `type Function f a b = f -&gt; a -&gt; b` It's when you have one or more (implicit) `forall`s in the body where you get "raise the rank" by passing a typeclass dictionary instead of a qualified function. If we can lower everything down to rank 0/1, type inference is much easier. Makes me wonder if there's not some form of quantification/indexing that can occur at higher ranks that doesn't admit a lowering like this. I think all the cases in the article and thread use unrestricted, universal quantification which (at least in Haskell) guarantees parametricity. Perhaps if there was some interplay between the types, so that you accepted functions that were universally quantified, but over a smaller domain (e.g. the range of a closed type family), so that they could be non-parametric, those functions couldn't be passed as lower rank.
Have you looked at [Timber](http://www.timber-lang.org/)? It's a language for real-time embedded systems, that's based on O'Haskell, which was based on Haskell. It allows you to specify timing constraints on all the operations. It doesn't seem to have much work on it in the past ten years, but it would be an interesting starting point.
&gt; Personally I always wanted to experiment with a little language that directly captured closures (where normal functions -&gt; can't close over anymore) You should, it could be very interesting, since you might be able to more easily forgo garbage collection in more circumstances. (Something Haskell-like but without GC pauses would be nice, even if I had to `bracket` most of my memory uses.)
:) It would be a bit safer by replacing `[a] -&gt; [a]` with `[Any] -&gt; [Any]` ([`GHC.Exts.Any`](https://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Exts.html#g:17)) since that would prevent things like `coolThing (map (+1))`.
&gt; You don't want to fork of as many threads as there are tasks, that will just slow things down. What you'd like to do is to create as many threads as there are cores and let those threads compute all these task one by one. The library seems really ergonomic for e.g. ParOn, but I'm not sure I understand when your scheduler would be superior to the rts's. I.e. the paragraph above describes green threads.
[removed]
It's not meant to replace `RTS` not to be superior to it :) I mean forked threads are still managed by RTS. It is meant to solve a set of problems that are not trivial to do by a simple usage of `Control.Concurrent.forkIO` or even with the `async` package. Consider the example function in the comment above [`traverseConcurrently`](https://hackage.haskell.org/package/scheduler-1.0.0/docs/Control-Scheduler.html#v:traverseConcurrently) and compare it to [`mapConcurrently`](https://hackage.haskell.org/package/async-2.2.1/docs/Control-Concurrent-Async.html) from `async`, do you see the difference? The latter will create at least TWO threads per element, where the former will only fork as many as you specify. Green threads are cheap, but they are not for free.
Thanks! I note with interest that [wikibooks](https://en.wikibooks.org/wiki/Haskell/Category_theory) takes a different approach, &gt; Actually, there is a subtlety here: because (.) is a lazy function, if f is undefined, we have that id . f = \_ -&gt; ⊥. Now, while this may seem equivalent to ⊥ for all intents and purposes, you can actually tell them apart using the strictifying function seq, meaning that the last category law is broken. We can define a new strict composition function, f .! g = ((.) $! f) $! g, that makes Hask a category. We proceed by using the normal (.), though, and attribute any discrepancies to the fact that seq breaks an awful lot of the nice language properties anyway. I'm not sure how much that resolves. At a semi-educated glance it looks like it does give you a category, but not one with all the nice properties we pretend to have.
Maintainer of tardis author here, fun to here that people use it! If you ever run into performance issues please let me know. I have not put any thought into optimization of the library, but would be happy to give it a try if people can give me some use cases &amp; examples to work with.
Don't you typically want way more green threads than cores?
vs code with hie and nix, https://github.com/alanz/vscode-hie-server and https://github.com/domenkozar/hie-nix 
Author of IntelliJ-Haskell plugin here. I would like to help you. Do you want to create an issue and give some more info? See [https://github.com/rikvdkleij/intellij-haskell#getting-started](https://github.com/rikvdkleij/intellij-haskell#getting-started) how to enable Event Log to see what is going on. Also check that you installed the non-Haskell dependencies.
Why would you want to do that? For example. Say you have a quad core CPU with hyperthreading, which means you'll have 8 capabilities. If you create 8 threads each of which is performing some very heavy computation that is totally independent from each other, you will saturate your whole CPU. Forking off more threads, be it OS threads or green threads you will only slow down the computation. In fact, setting `+RTS -N7` instead of `RTS -N` for above case will usually increase overall performance for the program, since there are other things running on the computer, like the OS for instance :) There are of course cases when you do want a lot of green threads, namely when your threads are doing very little work, which is usually the case for highly concurrent programs. But for computationally intensive programs, it is usually the opposite. 
So basically Idris.
An appropriate name for such a baby would be Raskell! But, more seriously, I really wish rust had generalized newtype deriving. There are some crates with macros providing GND-like deriving for some common traits, but haven't found anything generic. Understandable that the more generic thing wouldn't exist, GND / safe coerce / roles can be a bit of a can o worms.
The point of green threads is to be able to write concurrent code with blocking I/O in a synchronous style. If you use them in that way, they you want more green threads than cores, so that one of them will be available to run when the others block. If there's no I/O, then sure, there's no benefit to having more green threads than cores, but the cost is still low.
I'm building the backend for a turn based game in Haskell and would like to do the front end in another language using Haskell's FFI. The game has state that is not needed by the front end but the state still needs to be accumulated through each update call from FFI. In particular, I have something like: &amp;#x200B; update :: CInput -&gt; HState -&gt; (COutput, HState) &amp;#x200B; Here CInput and COutput are types known to both Haskell and the other language. HState really only needs to be understood by Haskell. Is there an easy way to do this without marshaling HState back and forth everytime I call update()? &amp;#x200B; Are there other approaches for using FFI with Haskell libraries that have state? &amp;#x200B; Thanks in advance! &amp;#x200B;
There is a package for it: https://docs.rs/newtype_derive/*/newtype_derive/index.html
You absolutely right. But that is not a scenario for a work stealing scheduler. Key word you said is the "concurrent" code, where I am talking about parallelization of computation. What this package helps you to do is to take care of the opposite scenario, where you have a lot more work than available computing power. Although forking off millions of green threads and throwing them at such problem will get the job done, it will be drastically slower. Green threads are cheap, but they are not free.
The reflex infrastructure for Android and iOS apps seems like it should be completely generalisable to any GHCJS based web app (I think that means, today, "Reflex or Miso"). And Reflex development is still brain intensive\* enough that anyone who can get the infrastructure to do anything non-hello-worldy probably has enough skills to be able to do it. Is this not so? I have thoroughly enjoyed Reflex. Compared to my day job involving redux/react/typescript for client-side rendered frontends, it is incredible. Compared to elm, it actually makes me want to keep going, and makes me keep finding new projects I want to implement. But it seems the Elm/Miso architecture is more up someone's ally, and if they want it, I think they should spend next Saturday installing reflex-platform, and then changing their \`main\` so that instead of calling out to reflex-dom's render with your reflex app, it calls out to miso's render with your miso app. And I'm sure that's easy, because Reflex puts you in charge. You aren't going to spend a week searching for the seams and then a month finding a tool small enough and developing the motor skills precise enough to unpick the thread. You'll just delete one line of code and replace it with another, plus update your cabal and imports. (After you've done step one, you might start crying because you don't know nix yet, but that's your problem if you use miso or if you use reflex, so you've neither gained nor lost.) &amp;#x200B; \* Mostly infra/documentation. I can now make reflex-platform do \~anything I want - oh it was so easy once I understood nix -, and my needs are simple but decidedly non-standard, but last time I tried installing obelisk (maybe two weeks ago?) it broke (just on installing it in a fresh directory), and since I have a deadline of this weekend for my two-or-three evenings a week project, I haven't investigated why yet. If it breaks and you need to work out why, it's much more brain intensive than the intention. Maybe next week I'll work out why &amp;c &amp;c.
Do you have any benchmarks to illustrate how much faster it is using your library?
please no, rust being regular is one of my favorite things about its syntax
Wish I was this bored. Glad I'm not this stupid.
Right but those capabilities given by the RTS settings are _not_ green threads, so that last-core slowdown isn't relevant to this discussion at all.
Loving the fact that Pascal did very well overall.
I mean, sure. It is being used for array processing library (massiv), which I have bunch of internal benchmarks for. But if you just like to compare a simple case here you go: ```haskell module Main where import Control.Concurrent.Async as A import Control.DeepSeq import Control.Scheduler as S import Criterion.Main main :: IO () main = defaultMain $ map (mkBench "square" (return . (^ (2 :: Int)))) [10000, 100000] mkBench :: NFData a =&gt; String -&gt; (Int -&gt; IO a) -&gt; Int -&gt; Benchmark mkBench name f n = bgroup ("(" ++ show n ++ ") - action: " ++ name) [ bgroup "scheduler" [bench "traverseConcurrently Par" $ nfIO (S.traverseConcurrently S.Par f [0 .. n])] , bgroup "async" [bench "mapConcurrently" $ nfIO (A.mapConcurrently f [0 .. n])] ] ``` Which results in: ``` Benchmark scheduler: RUNNING... benchmarking (10000) - action: square/scheduler/traverseConcurrently Par time 37.44 ms (35.84 ms .. 39.05 ms) 0.993 R² (0.987 R² .. 0.998 R²) mean 37.75 ms (36.69 ms .. 39.25 ms) std dev 2.564 ms (1.485 ms .. 3.887 ms) variance introduced by outliers: 25% (moderately inflated) benchmarking (10000) - action: square/async/mapConcurrently time 131.3 ms (126.1 ms .. 137.5 ms) 0.998 R² (0.995 R² .. 1.000 R²) mean 123.9 ms (120.6 ms .. 126.8 ms) std dev 4.784 ms (3.300 ms .. 6.606 ms) variance introduced by outliers: 11% (moderately inflated) benchmarking (100000) - action: square/scheduler/traverseConcurrently Par time 526.8 ms (440.2 ms .. 642.7 ms) 0.995 R² (0.983 R² .. 1.000 R²) mean 514.7 ms (490.8 ms .. 528.0 ms) std dev 23.04 ms (5.743 ms .. 30.90 ms) variance introduced by outliers: 19% (moderately inflated) benchmarking (100000) - action: square/async/mapConcurrently time 1.607 s (1.214 s .. 1.942 s) 0.993 R² (0.974 R² .. 1.000 R²) mean 1.667 s (1.591 s .. 1.719 s) std dev 74.34 ms (39.35 ms .. 102.2 ms) variance introduced by outliers: 19% (moderately inflated) ```
Does Idris run without garbage collection? ATS seems closer to Rust than Idris.
Note that the example below will be faster with `mapM`, since all it does it just squares each element of a list, which means what you mostly see is the scheduling overhead, but using less threads is still a clear winner.
Super unrelated, but it's somewhat unfortunate that "like C" has become the same thing as "normal" when it comes to programming languages.
I think you should just export update :: CInput -&gt; COutput and handle the fact that it's stateful within the Haskell? Though this may require architectural changes. Alternatively, you can make the other languages accept it and pass it back as eg. a binary blob, the internal structure of which it knows nothing about (eg. via the cereal package).
In our case, we *could* have just used MonadFix (in fact, we also relied heavily on MonadFix), in precisely the same way that one can always avoid using the State monad, and instead pass all the values around by hand. The ability to have values being transmitted in both directions between each of the actions in a do-block was important to obtain the abstraction we wanted to obtain, which is the ability to simply place tiles in a row or column, and have Tab and Shift-Tab automatically cycle through them. Of course we could pass the Events where each widget triggers its two neighbours by hand, but this would be error-prone and syntactically a bit of a mess -- and wouldn't work at all with other mechanisms like those which maintain dynamically-changing lists of widgets.
Whenever you do `withScheduler Par` in the `scheduler` library it will create as many green threads as there are capabilities, which is exactly what you want for computing bunch of embarrassingly parallel tasks. So yeah, it is very much relevant for the discussion. 
Rust's syntax is appalling.
Have you compared your performance with the [Par monad](http://hackage.haskell.org/package/monad-par), which is also a work-stealing scheduler?
What's appalling about it? It presents ML family features in a syntax that's approachable to C/Java/C++/etc. programmers. There's nothing inherently wrong with that.
I have not until now. Thanks for suggestion. Here a simple comparison: https://github.com/lehins/haskell-scheduler#benchmarks 
I added some benchmarks to the readme: https://github.com/lehins/haskell-scheduler#benchmarks
to me, it's not about aesthetics. rust 1. is easy to parse 2. has one canonical format just think of how bad all of the haskell source code formatting tools are, despite how easy it is to write parserrs in haskell
I think Monads themselves are pretty difficult with the pervasive linearity Rust has
Pascal did horribly on power usage!
It was #6 out of 26, that’s not so horrible.
If you write anything non trivial it becomes a shotgun blast of apostrophes and ampersands and colons.
How do I view them? Is there a little video or something?
Right. Defining the trait is one thing, implementing it is another matter altogether!
I wonder, were the Rust community be given the same power as the Haskell community, would they use it to create a complex more advanced idiomatic Rust that you could only read and understand after months of training? There is this weird phenomenon in Haskell, I attribute it to Haskell having a syntax that feels limiting, but a type system that seems almost limitless. There are two branches of idiomatic Haskell. One is the style anyone would recognize as generic "functional programming" and be able to work with and be productive after a week or two of practice. The other is where a dozen language features are enabled, and the latest and greatest utility libraries are imported and Haskell shows itself to be as flexible and alienating as Lisp. An expert can wield extreme expression under purity and safety. A beginner stands no chance. It would be interesting to see more expressive power come to Rust. But it would be a bad thing if it would come at the cost of being beginner friendly. Rust might be challenging to write as a beginner due to the borrow checker, but at least it's easy enough to read.
If you want a *serious* adoption you should have to use a Go syntax IMO.
Yeah. "::".
&gt;to me, it's not about aesthetics. rust But it is. For ordinary users that is.
Discussion on r/rust: [https://www.reddit.com/r/rust/comments/b6bu2c/idiomatic\_monads\_in\_rust\_a\_pragmatic\_new\_design/](https://www.reddit.com/r/rust/comments/b6bu2c/idiomatic_monads_in_rust_a_pragmatic_new_design/) &amp;#x200B;
&gt; I attribute it to Haskell having a syntax that feels limiting, but a type system that seems almost limitless. I find this phrase to be completely at odds with my own experience in regards to Haskell - could you elaborate?
See sidebar. "Learn You a Haskell" is probably the closest thing.
i wrote [an educational propaganda](https://libeako.github.io/c/ID_1371518733.html)
I remember there was a humorous haskellforfunandprofit.com which was literally fsharpforfunandprofit.com but with the word F* replaced with Haskell. Doesn't seem to be up anymore. Or maybe it was for another language than Haskell?
If I'm reading this correctly, that's for execution time and power usage considered jointly. For power usage it looks tied for last place with Haskell.
Maybe Stephen's list of bits and bobs? [http://dev.stephendiehl.com/hask/](http://dev.stephendiehl.com/hask/)
Just skimming the paper, I can't find a single instance where either Pascal or Haskell performed in the bottom third. Usually both are somewhere in the middle with pascal being somewhat more energy efficient. (which is unsurprising, since both are compiled and should hence perform much better than the interpreted languages) 
Probably because they aren't even remotely similar?
I think next time I see someone call monads a "hack around purity", I'm going to observe that I've seen half-a-dozen implementations of a "monad" in other languages, and what's clear is that the reason Haskell can use it is that it's the only language that has all the necessary features to make it work, make it safe, and make it worth a try. Dynamic languages can do it without much syntax fuss, but generally give no benefits as a result. Static languages either can't do it at all (there's no meaningful way to define a "monad" in Go), or it's very klunky. This is one of the better implementations of the monad interface/typeclass/trait in another language I've seen, and it's still got some sharp edges and is fairly complicated to read and a bit hacky to use. I think it's a reasonable supposition that if the author had not set out to deliberately copy monad, that there would have been very little guiding them in that direction from the language itself. Or, to put it another way, the several "You Could Have Invented Monad" tutorial/walkthroughs are true enough, but you only would have invented them in Haskell. Any other language, probably including Rust, you would have been bounced out of the process by several issues you encountered, if you didn't already know from the beginning that's where you wanted to go.
Bullshit. Both are tutorial sites aimed at introducing existing programmers to functional programming, as well as the language specifics.
&gt;I wonder, were the Rust community be given the same power as the Haskell community, would they use it to create a complex more advanced idiomatic Rust that you could only read and understand after months of training? I've seen a number of people report that their Rust experiences suggest Rust is already there for a lot of people. At the risk of being a bit anti-egalitarian, both Haskell and Rust are ultimately not really for everybody.
How do I handle the state-fullness within Haskell? I thought each FFI call would create in own isolated runtime environment and get cleaned up by the end of the call. Maybe I just need to study the FFI a little bit more. Thanks for your response!
Questions "is there X for Y" are often would be better with little clarification what is X. As for you question, what's wrong with "haskell.org"?
&gt; Haskell having a syntax that feels limiting I don't believe that this is a property of Haskell but rather a property of our education and cultural background. Imperative coding is the norm that pervades our education and culture. I feel that it would be a much different story, if we learned an FP language say in highschool. 
This sounds very interesting! Although I suspect the lazy evaluation semantics will make this quite unusable in embedded systems. It would be great if your compiler could build from C, and build the ghc1 stage compiler. That would allow us to compile ghc without a pre-existing ghc on the system.
*Learn You a Haskell* is a 'book' with a fairly limited amount of content. There is an order of magnitude more content on *F# for Fun and Profit*, and the content is covered at a much deeper level too.
I just discovered [https://hackage.haskell.org/package/base-4.12.0.0/docs/Foreign-StablePtr.html](https://hackage.haskell.org/package/base-4.12.0.0/docs/Foreign-StablePtr.html) which seems to do what I need. I'm still confused what happens with the runtime between FFI calls from another language into Haskell. 
If you want a lot of good language tutorials one resource (among many) is http://typeclasses.com, but they charge for most of their lessons.
I only looked at the summary but it seemed to me that Pascal (which too many people have dismissed) performed pretty well overall. It was designed as a teaching language, is a damn decent systems programming language and I wish I could use it (or Modula 2 or Oberon) to teach data structures instead of Java, Python, C++ 
&gt; I thought each FFI call would create in own isolated runtime environment and get cleaned up by the end of the call. This is almost certainly not the case, depending on how you do it. The RTS would be created in `hs_init` and wouldn't go away until `hs_exit`. Also, last time I checked the RTS wasn't fully reentrant, so you couldn't actually *safely* call hs_init twice in the same program. Even if you write the front end in a another language, you could have Haskell call it, in that case your `HState` can just be part of the environment around a call to `update :: CInput -&gt; COutput`. If your "main" is another language and you are calling into Haskell, I do believe the simplest way to carry around some opaque Haskell value is to use Foreign.StablePtr.
It might be context-free, but surely it's not regular! --- I'm actually fine with explicit `{;}`, doing away with `[]` reserved to lists / ranges, and C++ style comments. But I want Haskell style function application.
&gt; Also, last time I checked the RTS wasn't fully reentrant, so you couldn't actually safely call hs_init twice in the same program. I'm wrong about this. The GHC RTS is re-entrant, it's just not re-startable. So: hs_init() hs_init() hs_exit() hs_exit() works, and the RTS hangs around until the second hs_exit() call. Whereas: hs_init() hs_exit() hs_init() hs_exit() does not work. The RTS is torn down on the first hs_exit call, and even if the second hs_init doesn't signal an error or crash the process, it won't actually bring the RTS back to a ready and working state.
So I think the first thing is that "database" would be more like "database = map item \[1,2,3,4,5,6\]". &amp;#x200B; The function signature of "totalSales" looks correct, but note you'll need some way of checking that a Year already exists in a \[(Year, Sales)\], so you need to add a lookup function. &amp;#x200B; You should look into "pattern matching". As you've noted you can pattern match on the 4-tuple like "(\_, \_, Year, Sales)", but you can also pattern match on lists using its constructors "\[\]" and ":". &amp;#x200B; Hopefully that should get you up and running :)
Thanks!! This fully answers my question!
The thing that's probably tripping you up is that this isn't really a database, so asking google and stackoverflow about Haskell databases is going to lead you to all the wrong places. You probably want to give the `map` and `filter` functions a close look and see if you can work out a way to combine them into something that solves your problem. Good luck! :)
Thanks, but when I use map I get this error: &amp;#x200B; salesData.hs:37:16: error: • Couldn't match expected type ‘a -&gt; b’ with actual type ‘[Item]’ • In the first argument of ‘map’, namely ‘[item 1, item 2, item 3, item 4, ....]’ In the expression: map [item 1, item 2, item 3, item 4, ....] In an equation for ‘database’: database = map [item 1, item 2, item 3, ....] • Relevant bindings include database :: [a] -&gt; [b] (bound at salesData.hs:37:1) | 37 | database = map [ item 1, item 2, item 3, item 4, item 5, item 6, item 7, item 8, item 9, item 10, item 11, item 12, item 13, item 14, item 15, item 16, item 17, item 18] | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [1 of 1] Compiling SalesData ( salesData.hs, interpreted ) Failed, no modules loaded. &amp;#x200B;
Yeah but what can you really do? That information needs to be represented somehow.
Here is a video https://mstdn.io/@mane/101829283758598441
I feel like most of what people have suggested so far are missing the point. fsharpforfunandprofit fills a need for F# content around: * The hows and whys of structuring F# programs * In the small * In the large * Philosophy around why to use functional programming/F# * Worked examples * ...in digestible chunks It's not really just an F# tutorial. Now, does the Haskell community as a whole have content for all of these topics? Absolutely, but it's all scattered as collective knowledge across various blog posts, research papers, Haddock documentation etc. That's fine for people who are already initiated, but seems like it would be frustrating for newcomers. The closest thing I can think of closest to funandprofit that is a single source is Real World Haskell, but that leans much more on the "worked examples" side of things, without giving an intuition of how to glue things together in the large. I haven't read haskellbook.com, so I can't really comment on that. Probably the closest thing would be to just read funandprofit, as well as Scott's other work, Domain Modeling Made Functional, and translate the concepts into Haskell. Absorb Haskell-specific ideas from blog posts as necessary. Sorry that there isn't something more aggregated.
&gt; their Rust experiences suggest Rust is already there for a lot of people Also seems true for Haskell. Obviously, the large number of academic and industrial papers, experience reports, blogs on Haskell makes me believe that it's "already there" for a lot of people. &gt; both Haskell and Rust are ultimately not really for everybody Isn't that obviously true? Replace "Haskell" or "Rust" with any other programming language, food, drink, indoor/outdoor activitiy, book, movie, music, etc. and it'll still hold true. 
Every discussion around HKTs immediately goes to Monad and why do we need Monads, when something like Traversable is far more compelling. It probably has the best "real-world" elevator pitch since it accomplishes something we need to do _all the time_, and it doesn't have the M-word baggage.
Oh, I thought the graph at the top summarized results but it's merely an attempt at eye candy. Disappointing.
&gt;Isn't that obviously true? In the limiting sense, sure. But there are languages that are a lot more for "everybody", like Go or Python. Haskell is reasonably far at the "discriminating tastes" side of the scale. I'd put it on the end except then I'd have nowhere to put Agda or Coq.
&gt;Maybe Stephen's list of bits and bobs? &gt; &gt;http://dev.stephendiehl.com/hask/ not bad, thanks
you're totally right, this is what I meant. the fact that https://github.com/rust-lang/rfcs/blob/master/text/1331-grammar-is-canonical.md exists is super nice, to me. I agree that haskell style function application would be nice, as would hkt etc (as well as do sugaring rather than that awful (?) Option syntax)
I'm not OP, but I can share my experience and interpretation. There is so much power in the Haskell type system -- it can encode so many things and give you so many options in creating types that other languages usually cannot even dream of. Even something as simple as a sum type \`Either String Bool\` requires so much hacking in other languages. But one practical thing that limits the power of the types you can design is Haskell's syntax. Many things that would be otherwise great tools sometimes become bogged down in trying to get around Haskell's syntax when writing more complex types.
I completely agree. Traversable is where higher kinded abstraction gets both obviously useful and way harder to fake than faking monads. It is mentioned at the end of the post, with a comment about thinking the trick described won't work. 
I started reading this thinking you were French because of your punctuation, but you're are Hungarian. Hungarian also puts spaces before ending punctuation?
One can "tie the knot" and create infinite data structures which are stored in finite memory. This is done for the data type `data T a = T (T a) a (T a)` with: wrap :: [a] -&gt; T a wrap xs = let (l, r) = go l xs r in l where go l [] n = (n, l) go l (x:xs) n = let l = T l x re (re, r) = go l xs n in (l, r) &amp;#x200B; One can find a discussion of this at: [https://wiki.haskell.org/Tying\_the\_Knot](https://wiki.haskell.org/Tying_the_Knot). &amp;#x200B; I'm curious if you can do this for a 2 dimensional tape with a constructor like `data T a = T (T a) (T a) a (T a) (T a)` and you want to write a function called `tile :: Matrix a -&gt; T a`.
Haskell beginner here. In the code `migrate` type signature gives me the following error in compilation. I have `{-# LANGUAGE FunctionalDependencies #-}` included. ``` Could not deduce (LoadData i0) from the context: (LoadData i, SaveData o, ConvertData i o) bound by the type signature for: migrate :: forall i o. (LoadData i, SaveData o, ConvertData i o) =&gt; IO () ``` Any help / suggestions ? Thanks.
I sent a PR to your repo adding streamly ([https://github.com/composewell/streamly](https://github.com/composewell/streamly)) to the benchmarks. Streamly is a streaming library with inbuilt concurrency support, we have not announced it officially yet, we would like to add a few more features before we do that, but it is quite usable. It supports most of the features that you listed above and many more except pinning threads to a cpu. Here are the results of the benchmark run on my machine (quad core macbook pro) after adding streamly. streamly seems to be 3x faster (17.92 ms) than the best (59.60 ms) in the list. Let me know if there is anything unfair or wrong with the benchmark. ``` benchmarking Sums: 1000/unliftio/pooledMapConcurrently time 59.60 ms (58.41 ms .. 60.80 ms) benchmarking Sums: 1000/monad-par/parMapM time 63.44 ms (59.38 ms .. 68.36 ms) benchmarking Sums: 1000/scheduler/traverseConcurrently time 76.03 ms (68.57 ms .. 82.39 ms) benchmarking Sums: 1000/async/mapConcurrently time 129.3 ms (124.5 ms .. 133.5 ms) benchmarking Sums: 1000/base/traverse . par time 115.3 ms (100.4 ms .. 128.2 ms) benchmarking Sums: 1000/base/traverse . seq time 404.8 ms (347.4 ms .. 478.9 ms) benchmarking streamly time 17.92 ms (16.96 ms .. 18.96 ms) ```
It's like you can read my mind! I want just the same for haskell where you have like one source of high level information about language and approaches/patterns used in it explained in simple words
&amp;#x200B; I wish there was some place where complex things are explained in simplest possible words. Just watch one of the video of the author to understand what I am seeking for. [https://www.youtube.com/watch?v=E8I19uA-wGY](https://www.youtube.com/watch?v=E8I19uA-wGY) &lt;- this was my first video on functional programming and man it was soooo good
Fully agree there. Making something like Agda both practical, fast, and useable for more developers will be a jackpot for programming. Imagine writing software minus the security issues.
&gt; The thing that's probably tripping you up is that this isn't really a database I mean you're right, but _pedantically_ you're wrong; This totally is a database, it just isn't a RDBMS or anything like that.
Agreed. These days, a lot of technological discourse requires a sort of double-telepathy: You need to speak as though you think like other people think you think. :)
&gt; I'm curious if you can do this for a 2 dimensional tape You can. &gt; you want to write a function called `tile :: Matrix a -&gt; T a` I would say that's a bit of a bear, but certainly possible. You'll probably want to use `fix` explicitly rather than writing it as a set of mutually recursive definitions. Here it is for 2x2 matrices, without using `fix` explicitly: data Matrix a = MkMatrix ((a,a),(a,a)) data T a = T (T a) (T a) a (T a) (T a) tile :: Matrix a -&gt; T a tile (MkMatrix ((ul, ur),(bl,br))) = tul where tul = T tbl tur ul tur tbl tbl = T tul tbr bl tbr tul tur = T tbr tul ur tul tbr tbr = T tur tbl br tbl tur Results: &gt; showT 2 . tile $ MkMatrix ((1, 2), (3, 4)) "(T (T 1 4 3 4 1) (T 4 1 2 1 4) 1 (T 4 1 2 1 4) (T 1 4 3 4 1))" 
Reads like something I'd write. Rather efficient to read as well.
We're working on adding a big new chunk of free content, some of it similar in purpose to fsharpforfunandprofit. We haven't been at this nearly as many years as Scott Wlaschin has, so we've got some catching up to do. 
not sure how I should read it... it's like some sort of art-house for me. no intention to offend you in any way
I mean it's very cool
The thing is, dynamically typed languages usually get hung up on return: return :: (Monad m) =&gt; a -&gt; m a What return does has to depend on the type of result you're asking for, which in a dynamically typed setting is pretty much unknowable. In lieu of this, we can instead translate the (Monad m) constraint as an additional function parameter: return :: Monad m -&gt; a -&gt; m a (&gt;&gt;=) :: Monad m -&gt; m a -&gt; (a -&gt; m b) -&gt; m b after all, something like this *could* be the type of the Monad methods -- then they're just operations for extracting two fields from a record. But this is syntactically super-noisy, to the point that nobody's going to want to use it. Having to explicitly pass the monad you're using to each occurrence of `(&gt;&gt;=)` and `return` is just absurd, and trying to do some sort of thing where we pick a single monad to use over a larger scope still means it's going to be awkward to really make use of monad transformers or anything like that. In statically typed languages, you need to be able to have higher-kinded polymorphism to even be able to attempt the awkward record-passing method, and most of the type systems out there in non functional languages are too impoverished even for that.
\`lookupRegexCommon\` seems to be applying a regular expression to a piece of text and returning info about the matches. The regular expression is applied from an starting offset, but match positions are translated so as to start counting from the beginning of the doc. &amp;#x200B; Incidentally, it seems like the signature is slightly more complex that it needs to be. The function takes as parameters both a regular expression and a function which applies the regular expression. It would be simpler to pass the partially applied function. &amp;#x200B;
There are (at least) two ways for represent non-empty and possibly-empty (cons) lists: data NonEmpty a = Single a | Cons a (NonEmpty a) type List a = Maybe (NonEmpty a) --- data List a = Nil | Cons a (List a) data NonEmpty a = a :| List a The [base functor that I added to recursion-schemes for NonEmpty](http://hackage.haskell.org/package/recursion-schemes-5.1.2/docs/Data-Functor-Base.html#t:NonEmptyF) is some weird blend of the two. I think that's because [Data.List.NonEmpty uses the second form](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-List-NonEmpty.html#t:NonEmpty), but that form isn't obviously a uniformly recursive data type. Both representations are clearly isomorphic, but is there an operational advantage to one or the other? If so, which? If it is the first one, should I rewrite the base functor to match what would be generated by the template haskell? A larger question: do we have good pre-benchmark guidelines for which of several isomorphic represenations of a data type to use? If we keep what's already there: Is it an API breakage to turn a single-constructor `data` type into a `newtype`? Should we change the base functor to be a `newtype`, either way? It seems like that would cut down on indirections, and I think recursion-schemes is about to break API and release a 6.x version anyway, and GND or deriving-via should let us have all the same instances.
For some things it's more efficient to not be switching between tasks a bunch because of e.g. cache locality or memory usage.
Thank you for bringing `streamly` to my attention, I've heard of it, but haven't used it at all until now. Didn't realize it can do stream processing concurrently, which is cool. Despite that the result you posted look so good, unfortunately they do not benchmark the same thing. I created a separate PR that goes into some detail on why it is: https://github.com/lehins/haskell-scheduler/pull/2 In the nutshell what's going on is that your library is doing exactly what it was designed to do, namely avoid the list allocation and perform computation in constant memory. Once you change it to the exact same functionality (traversing over a list with lists) it performs on par with `async` On the other hand, if you compare similar functionality implemented with `scheduler` you will see that: ``` benchmarking NoList/scheduler time 15.52 ms (15.31 ms .. 15.71 ms) 0.999 R² (0.998 R² .. 1.000 R²) mean 15.40 ms (15.31 ms .. 15.51 ms) std dev 262.6 μs (189.5 μs .. 327.4 μs) benchmarking NoList/streamly time 25.24 ms (24.71 ms .. 25.90 ms) 0.998 R² (0.995 R² .. 0.999 R²) mean 25.07 ms (24.77 ms .. 25.43 ms) std dev 745.4 μs (430.5 μs .. 1.043 ms) ``` I guess my laptop is a bit older, since the same code is faster on yours. For those that are interested, but not enough to go look at the benchmarks here are the two relevant calls: ```haskell λ&gt; let n = 1000 :: Int λ&gt; let elts = 100000 :: Int λ&gt; withScheduler_ Par $ \s -&gt; replicateM_ n $ scheduleWork s $ pure $ F.foldl' (+) 0 [0 .. elts] λ&gt; S.runStream $ asyncly $ S.replicateM n (S.foldl' (+) 0 $ S.enumerateFromTo 0 elts) ``` 
&gt; the reason Haskell can use it is that it's the only language that has all the necessary features to make it work, make it safe, and make it worth a try The entire definition of Monad in Scala: trait Monad[F[_]] extends Functor[F] { def pure[A]: F[A] def flatten[A]: F[F[A]] =&gt; F[A] } That's it. Yet, Scala has very little in common with Haskell! It's object-oriented at core, and strict by default. The only two things you need to make monads convenient really are: - higher-kinded types - some kind of type class resolution mechanism (just so you don't have to provide the instances manually) So I really can't agree with: &gt; you only would have invented them in Haskell
It would be better to ask more pointed questions about the code. I doubt that anyone's going to translate it all to C# or something.
&gt;(setq haskell-mode-stylish-haskell-path "brittany") I have installed haskell-ide-engine, but i don;t know how to use brittany. any hint?
(I'm the @Centril, referenced in the article) &gt; [...] I'm going to observe that I've seen half-a-dozen implementations of a "monad" in other languages, and what's clear is that the reason Haskell can use it is that it's the only language that has all the necessary features to make it work, make it safe, and make it worth a try. I don't know what leads you to this belief. Monads work perfectly fine in Idris with the same do-notation. Without purity you won't get the same equational reasoning properties but we could take a sort of "fast and loose reasoning is morally correct"-esque perspective here and pretend we have purity. I think you overestimate how uniquely suited Haskell is for monads and how ill-suited other languages are. &gt; This is one of the better implementations of the monad interface/typeclass/trait in another language I've seen, and it's still got some sharp edges and is fairly complicated to read and a bit hacky to use. &gt; I think it's a reasonable supposition that if the author had not set out to deliberately copy monad, that there would have been very little guiding them in that direction from the language itself. Oh sure; Rust owes a lot to Haskell and this is deliberately considering monads from a "what can we steal from Haskell" perspective. As one of Rust's language designers, and Haskeller myself, I frequently think about what I can steal from Haskell. ;) &gt; Or, to put it another way, the several "You Could Have Invented Monad" tutorial/walkthroughs are true enough, but you only would have invented them in Haskell. Any other language, probably including Rust, you would have been bounced out of the process by several issues you encountered, if you didn't already know from the beginning that's where you wanted to go. I think monads in Haskell were the happy accidental combination of Moggi, Wadler, et. al's work as well as the need imposed by laziness. [Simon discusses this in a good talk](https://www.youtube.com/watch?v=06x8Wf2r2Mc).
I think he was just trying to do the needful
Added some more benchmarks for `replicateM`, which look a lot more realistic: ``` Benchmark scheduler: RUNNING... benchmarking Replicate Sums: 100000/scheduler/replicateConcurrently time 15.96 ms (15.57 ms .. 16.41 ms) 0.997 R² (0.994 R² .. 0.999 R²) mean 16.45 ms (16.08 ms .. 16.92 ms) std dev 1.159 ms (597.6 μs .. 1.895 ms) variance introduced by outliers: 32% (moderately inflated) benchmarking Replicate Sums: 100000/async/replicateConcurrently time 23.61 ms (23.06 ms .. 23.99 ms) 0.998 R² (0.996 R² .. 1.000 R²) mean 24.05 ms (23.76 ms .. 24.82 ms) std dev 994.0 μs (440.0 μs .. 1.836 ms) variance introduced by outliers: 15% (moderately inflated) benchmarking Replicate Sums: 100000/streamly/replicateM time 25.23 ms (24.32 ms .. 25.90 ms) 0.997 R² (0.993 R² .. 0.999 R²) mean 25.67 ms (25.28 ms .. 26.93 ms) std dev 1.371 ms (388.9 μs .. 2.535 ms) variance introduced by outliers: 20% (moderately inflated) benchmarking Replicate Sums: 100000/base/replicateM time 55.53 ms (55.24 ms .. 56.00 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 55.84 ms (55.61 ms .. 56.23 ms) std dev 531.8 μs (214.1 μs .. 749.4 μs) ``` some libraries are tricky ghc optimizes it away, so they are missing from this list and I'll work a bit more on it some time later
[Idris 2 / blodwen](https://www.youtube.com/watch?v=mOtKD7ml0NU) is looking like a good step towards that?
Haskel is a language that helps you a lot with its type system. I like to specify function types before I think about implementing them. This is why I believe, you might want to think about your `database` function again. Its type should probably be `database :: [Item]` (which is `ListItems`). Also it might help to think about what the following functions do. Then, how to implement them. Do that, one by one. When you are done with that, you are almost finished. year :: Item -&gt; Year sales :: Item -&gt; Sales allSales :: [Item] -&gt; [Sales] allSalesPerYear :: Year -&gt; [Item] -&gt; [Sales] sumSales :: [Sales] -&gt; Sales
Rust's grammar requires context sensitive parsing over raw bytes but LL(k) over the alphabet of Rust tokens.
I'm part of Rust's Grammar working group that seeks to implement that RFC. The intention is nice, but we have a lot of work ahead to make a canonical grammar a reality. The Haskell report seems ahead of us in all honesty.
Also the fact that ````return```` derives from the idiom that a monad gives rise to (lax monoidal functor with tensorial strength in category theory or applicative functor in haskell) makes a ad hoc implementation in a language that lacks a proper structure that more awkward.
&gt; That's fine for people who are already initiated, but seems like it would be frustrating for newcomers. Am newcomer. Can confirm.
Generally you do want more green threads than capabilities so that work can continue on a capability when one green thread is blocked on that capability; I would be surprised if this didn't improve performance as it's exactly how the RTS has been designed to be used. There are definitely workloads where it would be very useful to have a \`ParOnWithThreadsPerCap \[1,2,3\] 4\` combinator for workloads where the green threads will block for whatever reason. If you believe this is wrong, I'd love to see some benchmarks to demonstrate :)
Generally, when it is possible that one thread can be waiting for another, then yes, I'd agree with you, you do want more green threads than capabilities. But this library implements a work stealing scheduler, which means it is not designed for concurrency. One worker will not wait for another unless there is no more work to do. For the scenario you describe ghc's RTS already performs very well and libraries like `async` do fine job at helping you **safely** spin off as many threads as you seem fit. I say safely cause handling exceptions is no easy task in Haskell. Let me ask you though, what do you think will be faster, [creating a million green threads](https://stackoverflow.com/questions/1900165/how-long-does-it-take-to-create-1-million-threads-in-haskell) to handle a million of independent tasks where each task can fully load one core, or create 8 threads (on 8 cap machine) that will take care of all of those tasks one by one? &gt; I would be surprised if this didn't improve performance as it's exactly how the RTS has been designed to be used I know this is wrong ;) Just kidding, it all depends on the problem and for some case maybe the approach you describe would be optimal. See the [readme](https://github.com/lehins/haskell-scheduler#benchmarks) for some benchmarks. Hopefully more will come in the future. &gt; If you believe this is wrong, I'd love to see some benchmarks to demonstrate :) If you'd like to experiment, please, be my guest. You can spin up as many workers as you'd like with `scheduler` as well. For example 3 times the number of capabilities, where three green threads are pinned to each capability (just as you described): ``` n &lt;- getNumCapabilities withScheduler (ParOn [1..n*3]) $ \scheduler -&gt; scheduleWork scheduler ... ``` or simply 3 time as many and RTS will choose which capability is less loaded: ``` n &lt;- getNumCapabilities withScheduler (ParN (n*3)) $ \scheduler -&gt; scheduleWork scheduler ... ``` In any case, this `scheduler` library helps you deal with tasks, not threads.
That's fair, but at the same time isn't Stackage a closed system anyway? Rust/Cargo, JS/Npm and Go I've had no problems with. 
 &gt;But after typing one's 10000th `if err != nil` at 3 seconds each, debugging one's 1000th `NullPointerException` at 20 minutes each, or investigating one's 5th barely reproducible nondeterministic production `segmentation fault` at 2 weeks each I understand your argument, infact I've used the same argument. But this isn't unique to haskell. Rust also solves these problems and while it does have a steeper learning curve than other languages, haskell is still greatly more difficult to learn. 
&gt;I think next time I see someone call monads a "hack around purity", I'm going to observe that I've seen half-a-dozen implementations of a "monad" in other languages, and what's clear is that the reason Haskell can use it is that it's the only language that has all the necessary features to make it work, make it safe, and make it worth a try. Dynamic languages can do it without much syntax fuss, but generally give no benefits as a result. Static languages either can't do it at all (there's no meaningful way to define a "monad" in Go), or it's very klunky. .NET languages have reified generics so a zero parameter class is able to know its own interface, is able to instantiate itself, and then is able to call the methods defined on its own instantiation. I was able to craft a C# monad transformer library by using static extensions and pairs of classes to mock a Haskell monad. 
Haskell 102: https://youtu.be/Ug9yJnOYR4U
Stack runs for me flawlessly. Not a single error and bootstrap from just one downloaded file. But then again, I'm on ubuntu.
[https://www.reddit.com/r/haskell/comments/b61h3n/energy\_efficiency\_of\_programming\_languages/ejhlfg0?utm\_source=share&amp;utm\_medium=web2x](https://www.reddit.com/r/haskell/comments/b61h3n/energy_efficiency_of_programming_languages/ejhlfg0?utm_source=share&amp;utm_medium=web2x)
You should ignore the term 'database' and instead think of this as a 'data structure' - What data structure in Haskell exists, or can be made, that allows you to 'insert' or 'retrieve' items of a given type? Given an instance of this data structure, make functions that insert, retrieve, or analyze the contents.
Haskell doesn't even have things like lifetimes though..that's not fair. Plus, what about pass by value vs reference? I'd rather see `&amp;` which is succinct.
I second [accelerate](http://hackage.haskell.org/package/accelerate). I'm surprised again and again at the quality of the PTX code it generates, and its memory management is pretty sane. It's far and away the most reasonable way to do GPU computing in my opinion. &amp;#x200B; As far as CPU-side single-core programming goes, if the [vector](http://hackage.haskell.org/package/vector) package has the operations you need, you're in luck. I was able to get pretty good performance on numerical integration at a previous job, with no fancy tricks other than Data.Vector.Unboxed. &amp;#x200B; As far as statefully-defined things go, I've been thinking about the "right" way to do this a lot lately. If you're willing to write kind of ugly code, I've found that unboxed types and the ST monad cause GHC to generate pretty reasonable LLVM. It's a very different way of doing things than using sharing recovery (which is accelerate's big trick) or stream fusion (vector's big trick), but it's better than FORTRAN IMO. If you're OK with double-checking the generated LLVM IR or machine code yourself (and if you're doing heavy numerical work, that's not unthinkable or crazy by any means) this might not be a bad way to go. &amp;#x200B; A lot of the code in BLAS, LAPACK, and friends is essentially functions with FORTRAN type signatures that are implemented in assembly, and usually that assembly is generated by some kind of macro. Imagine if you had something like the [inline-c](http://hackage.haskell.org/package/inline-c) package but for LLVM IR. Then, one could use template-haskell as a macro system for generating high-performance numerical code. You could have macros for loop-tiling, doing AD, etc. The result would be Haskell functions (again, probably with unboxed types/ST) whose bodies are library-generated LLVM IR. LLVM has support for GHC's calling convention, so it's conceivable that one wouldn't even need to pay an extra cost when calling such functions from other Haskell code. &amp;#x200B; Anyone else exploring this area? It'd be great to unseat the current state-of-the-art embodied by FORTRAN with inline assembly.
Could you elaborate on the limitations in syntax? Or perhaps you mean you'd like a better mechanism for abstracting over syntax?
Tried with more recent IDEA version (2018.3.5) and cleared cache directory and it seems to work now :). Time for another tiny Haskell project. Thank you for your work!
These are really awesome talks! Will they be recorded and released online?
I understand that, I'm just interested in the general function which takes a variable size matrix. Thanks for the illustration though.
Great to hear!
Yep, *Learn You a Haskell* is like an introduction to the language while *F# for fun and profit* is like how to really use the language
indeed i was
if you have any suggestion how should i improve it then please let me know
Don’t forget: data List a = Nil | NonEmpty a data NonEmpty a = a : List a
I don't me no disrespect but you sir are an idiot.
Hm, I don't think you understand Haskell very well. Rust coolaid is strong.
&gt;The only two things you need to make monads convenient really are… I'd add one thing to that list: Flexible enough syntax to chain binds without billions of braces from nested .flatMap() calls and lambdas. If you don't have something like do-notation, you need overloading of infix operators (for &gt;&gt;=) and a way to write lambdas without enclosing them in braces. Otherwise monads will be of very limited use. &amp;#x200B;
I said it in a previous post, but I am enjoying this series. Thanks for posting!!
dante runs `ghci` from `PATH` in your project directory and enables flycheck ([but doesn't do it well](https://github.com/jyp/dante/issues/52#issuecomment-348910386)). That's been called `inf-haskell` since 2004. People have been running `M-x inferior-haskell` just fine for 15 years. It's almost like there's more to the problem than meets the eye. 🤔
Does it work on ghc 8.4?
!remindme 2h
I will be messaging you on [**2019-03-29 15:34:05 UTC**](http://www.wolframalpha.com/input/?i=2019-03-29 15:34:05 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/b6shgj/haskell_101_tech_talk/ejnlsiu/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/b6shgj/haskell_101_tech_talk/ejnlsiu/]%0A%0ARemindMe! 2h) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
You wouldn't invent them in Scala, because Scala doesn't have the problem(s) they solve. (Without getting too far into how much the "problem" is or is not a problem.) If you want mutability, you just use it. If you want a "flatmap" operation on something you would probably just write it. Would Scala have implemented it if Haskell didn't? I'd say there's a good chance it wouldn't have... and remember, most of the reasons why are things you'd probably consider *good* things. It's not an attack. (You can arguably see the... err... non-nativeness, for lack of a better word... in the very name of the definition there. The use of "flatten" tends to suggest to me that someone was thinking of this definition strictly in terms of what it does to lists. Yes, the monadic list implementation "flattens" the list. But that's not an even remotely sensible description of what the IO monad implementation does, or the STM monad implementation, or the State monad implementation.... or indeed much of anything except concrete containers, and not necessarily even all of those. It's clearly something implemented after the fact, not a native invention, or if it was a native invention, something never really understood at an abstract level.)
 &gt; We expect you to: be passionate about what you do Sorry, I only care the normal amount about programming, I just like it a lot, it doesn't ignite my loins.
See my reply to the Scala post. "Make it *worth a try*" is a critical aspect that I suspect you may have overlooked. I suppose I should have emphasized that better. Be honest with us: Even if Rust grows *all* the features necessary to make the monad interface/trait/whatever slick, do you seriously expect anyone to use it? Are *you* going to actually use it? Again, most of the problems solved by monad in Haskell are problems that don't exist in Rust. Without the smooth gradient introduced by having IO forced on you monadically, then the state monad, and other monads, you're unlikely to convince large numbers of people to re-write their Rust code in the monadic flavor just to get STM or some other exotic monad implementation. It's not worth a try in Rust, and as I said to Scala as well, most of the reasons why are things you'd consider *features*, not weaknesses. A Rust programmer would really have to have their back against the wall to want to use STM at all, for instance, because why wouldn't they just put a bit more effort into making the borrow system do what they want? I can't think of a use case off the top of my head where that's not a better solution than STM; it would have to be something very weird and specific, and the engineer in question would still be well advised to see if more thought and design can remove the "weird and specific" so they can use Just Plain Ol' Rust instead of having to rewrite the critical swathes of code in this style.
&gt; Be honest with us: Even if Rust grows all the features necessary to make the monad interface/trait/whatever slick, do you seriously expect anyone to use it? Are you going to actually use it Yes. I don't talk about it because it's fun. Users run into the need for GATs (generic associated types) in Rust and I do as well. When you do it often feels like running into a brick wall. HKTs would encode many patterns better. &gt; Again, most of the problems solved by monad in Haskell are problems that don't exist in Rust. &gt; [...] &gt; [...], you're unlikely to convince large numbers of people to re-write their Rust code in the monadic flavor just to get STM or some other exotic monad implementation. Rust code already does use monadic flavor and functors are commonly used. c.f. [`.flat_map(...)`](https://doc.rust-lang.org/nightly/std/iter/trait.Iterator.html#method.flat_map), [`.and_then(...)`](https://doc.rust-lang.org/nightly/std/result/enum.Result.html#method.and_then), [`.prop_map(...)`](https://docs.rs/proptest/0.9.2/proptest/strategy/trait.Strategy.html#method.prop_map), and [`.transpose()`](https://doc.rust-lang.org/nightly/std/result/enum.Result.html#method.transpose) (==&gt; `sequence`). Folding over lists, trees, and whatnot in a general way also seems like a natural thing to want. We just lack the ability to reason about these abstractly and I don't see why folks wouldn't do that if given the ability. It doesn't feel substantially different than the case for parametric polymorphism in the first place. It is more a matter of degree. Also, you don't need exotic STM monads or whatnot to make this worthwhile. There are plenty of vanilla ones to make it worthwhile. &gt; It's not worth a try in Rust, and as I said to Scala as well, most of the reasons why are things you'd consider features, not weaknesses. Yet people do invent monads in Scala including reasoning about them abstractly. People even invent monads in Javascript. &gt; A Rust programmer would really have to have their back against the wall to want to use STM at all, for instance, because why wouldn't they just put a bit more effort into making the borrow system do what they want? I think you are focusing too much on STM. Not every effect can reasonably be baked into the language itself. The complexity cost of that would in my view be unreasonable and rather ad-hoc.
I seem to match almost all points, including the bonus points (one failing: no open source, my previous projects were not deemed open source material by the suits, not even partially); does remote mean globally remote or US remote? 
&gt; or on-site in our office in the center of Moscow It looks like globally remote &amp;#x200B;
It seems so. Thanks for pointing that out! 
Haskell has lifetimes and a borrow checker?
&gt; This post was written for just /r/haskell , to a Haskell community. If I had targeted it for a larger community like Hacker News or /r/programming, I would have spent some more words making it clear that while point is a defense of the monad use in Haskell, it isn't an assault on the unworthiness of other languages because they can't do monad. That's fair enough. I think perhaps a better fit for Rust is an effect system or something of that ilk. Effect polymorphism is being considered right now for `const fn`s because it is necessary for code reuse. Generalizing to other effects (`async`, `try`, `#[target_feature = "avx"]`) would be interesting. &gt; Don't tell anyone else, this is just between you and me... but my primary programming language right now is Go, and I'm actually not that perturbed by it. I think perhaps I'm one of those people who would never touch Go. ;) Let's not judge each other too much for our preferences. &gt; (I use a looooot of the skills and mindset I've gotten from the harsher languages like Haskell and Erlang, and that helps a lot. Just because Go doesn't "require" my IO to be separated from my pure code doesn't mean I can't do it, etc.) Anecdata (since we're sharing experiences): Once upon a time I had an assignment in a course re. parallel functional programming which involved encoding map-reduce in Erlang. If I recall right, at some some we had a bug due to some untypedness which also had to do with IO. This caused a bug that took an hour to find (and we were on a deadline!). Haskell would have caught it instantly. That did leave a sour taste. I, and I presume most people in /r/haskell, think that type system facilities for reasoning about effects is great. Rust has recently stabilized `const fn` and right now these are referentially transparent. In the future, I think they won't be due to `&amp;mut T` references, but at least they will be deterministic. This excites me. &gt; (Many in /r/haskell would, and I understand, appreciate, and respect their perspective. But it's not my perspective.) I'm probably one of those people you reference. ;)
Like I said, certainly possible, just a bit of a task to do. And, when the size isn't statically known, you'll need an explicit `fix`. I think it will look something like this: tile mat = fst $ fix f where f (_, ragged_edges) = let r = g ragged_edges in (r, edges r) g edges = {- use edges and mat to produce result -} edges :: T a -&gt; _ That structure *should* guarantee you write something that is productive enough, but still has in-memory loops.
&gt; If I recall right, at some some we had a bug due to some untypedness which also had to do with IO. I respect Erlang for a lot of other things, but I think it's a \_terrible\_ "functional language". You pay a lot of the price to have immutability, but it fails to reap any of the benefits. It doesn't need "immutability"; it needs to be unable to pass "references" between processes. Within a process there's no compelling reason for it to be immutable; it just klunks up the language something fierce. And then trying to do any "functional" things is just incredibly verbose. In 2019, I think it's a reasonable thing to say that rather than being a subset of Haskell, Erlang should have been a subset of Rust. Being unable to pass references between processes is a very small subset of Rust, but for the 1980s would have been amazing. Given the time it was first written, I understand where it was coming from, and for the time I think it was a great experiment, though.
That's true, but it's fair to say that it's more comfortable to work from Europe than the US, because it's a much bigger time overlap with the core team.
 data IList (empty :: Bool) a where Nil :: IList True a Cons :: a -&gt; IList e a -&gt; IList False a data List a where AnyI :: IList e a -&gt; List a type NonEmpty a = IList False a type Empty a = IList True a --- `recursion-schemes` doesn't deal with indexed recursion (ala W-Types) or really any sort of non-uniform recursion (general nested types), though so I think I'll leave off those approaches.
&gt; Given the time it was first written, [...] It's also good idea to consider the environment: Ericsson and telecoms. &gt; It doesn't need "immutability"; it needs to be unable to pass "references" between processes. Within a process there's no compelling reason for it to be immutable; it just klunks up the language something fierce. In Rust, annotating bindings with `mut` would not be neccessary for soundness purposes. However, it is quite a nice check that makes reading code easier and encourages more maintainable software. &gt; In 2019, I think it's a reasonable thing to say that rather than being a subset of Haskell, Erlang should have been a subset of Rust. Well... I think the bigger difference here is the runtime system. Rust barely has one and doesn't have GC and so that would make the experience wildly different imo.
Would you mind demonstrating, in code for precision, what you mean?
I would think yeah. If you use MonadRandom everywhere where a randomness is required, you can then print the seed and use that to debug issues. Additionally, the declarativeness of Haskell should also make it easier to reason about all of the random element. There was recently a Tardis post in this sub that talked about the water level problem. The fact that you can solve the problem in such a declarative way is incredible.
I don't doubt your claims about performance. My point is that I don't find the way you motivate the library re performance as compelling as it could be because it's not obvious why the overhead of scheduling green threads should be larger than whatever it is your library is doing. It doesn't follow from "we fork fewer threads" that we gain anything wrt performance, i.e. green threads _are_ "tasks": these are all just metaphors. I'd probably personally find it more convincing to just see benchmarks and a bit of interpretation as to the operational overhead your library avoids (if you can identify it). I hope that makes sense.
It was Scala, another FP+OO language that runs on a managed runtime https://web.archive.org/web/20161117161052/http://scalaforfunandprofit.com/
&gt; Scala doesn't have the problem(s) they solve How do you explain the thriving community around pure FP idioms in Scala, then? For instance: https://github.com/typelevel/cats &gt; If you want mutability, you just use it What if you want to track effects in the type system? &gt; If you want a "flatmap" operation on something you would probably just write it The entire point is to abstract, in a principled way, over a class of well-specified behaviors. "Just writing it" doesn't make any sense in that context. &gt; The use of "flatten" tends to suggest to me that someone was thinking of this definition strictly in terms of what it does to lists Note that this is just my take on Monad. [It's called bind](https://scalaz.github.io/scalaz/scalaz-2.9.0-1-6.0/doc.sxr/scalaz/Monad.scala.html) in one of the two most popular FP libraries. But I find that "bind" isn't a good name as t already means many things in programming. I used "flatten" because in my view it represents well how the double type constructor application is _flattened_ into a single application. I was not thinking about lists. Flattening is about removing nesting. I think _flattening_ a _nested_ `IO` value makes perfect sense. 
&gt;We use a pure functional language tools such as Haskell and Nix to make the rapid development of correct, safe and modern software systems possible. It is also in part the philosophy at our consultancy. Nice to see Nix being adopted in professional environments. &amp;#x200B;
I generally don't use comprehensions, and I wouldn't mind if we didn't have the syntax at all. But, we should definitely only desugar to something in base (preferrably Prelude) by default. I wonder if maybe `RebindableSyntax` could be / is already used for comprehensions; then you could use whatever `(&gt;&gt;=)` and `fail` that are in scope.
What exactly do you want so see demonstrated in code? Every monad is an idiom. In Haskell, for example, ````return```` is just an unfortunate artifact from before [Applicative =&gt; Monad](wiki.haskell.org/Functor-Applicative-Monad_Proposal) instance Monad m where return = pure is redundant. I consider languages that implement monads like this to be awkward not for a very different reason that it is considered bad practice to use &gt;&gt;= where you only need ap or fmap.
The only hilarious thing about it is that it is not a newtype. It is very probable that they will add some context to this type in the future (configuration? state?), so it will become a "real" monad. This way they are saving themselves a lot of pain of switching everything from "normal" functions to do notation when it bacames necessary.
&gt; I generally don't use comprehensions, and I wouldn't mind if we didn't have the syntax at all. Curious. What is your reasong behind this?
&gt; ax monoidal functor with tensorial strength in category theory or applicative functor in haskell What makes return a "lax monoidal functor with tensorial strength in category theory or applicative functor in haskell"?
Sure it's one of the contenders for this. I think it suffers badly from poor documentation. Coq has been around forever. Agda has some good material for it but is less practical. Disciple is way to young. This whole area hasn't consolidated enough but I can't wait to see that happen and have something practical and well-documented emerge. It also seems that there is too much separate effort. Given the number of people who can do this kind of work is so small and there seems to be not yet enough industrial funding it would be so much better if everybody pooled their resources.
Not return. Idioms are called Applicative Functors in Haskell. For a comprehensive CT introduction the whole abstraction shenanigans, [this](https://bartoszmilewski.com/2017/02/06/applicative-functors/) is not a bad read. 
In order to better understand some trippy effects of the \`ContT\` monad transformer, I once wrote a tiny \["minimal viable dungeon"\]( [https://github.com/danidiaz/dungeont](https://github.com/danidiaz/dungeont) ) using brick. I got the impression that brick would work well for a full-blown roguelike. 
I like them for simple list processing. But is it possible that disuse of them for other purposes is because that's what we intuit comprehension to be for, rather than any unreadability of the syntax itself? Desugaring to something in base I agree with. I suppose it's irritating to not have a concept of "filterable" in base. Re. RebindableSyntax, I don't quite follow, that's the point of MonadComprehensions already? Though fail comes in for pattern matches rather than guards and introduces a MonadFail constraint. This relates to point 2; if we're going to allow monadcomprehensions it seems distasteful to allow runtime errors to occur so easily. 
I assume you're talking about the default implementation: some v = (:) &lt;$&gt; v &lt;*&gt; many v many v = some v &lt;|&gt; pure [] Both recursively call each other; but if `v` fails, then `some v` fails. If `some v` fails, then `&lt;|&gt;` allows `many v` to recover with a default value (`[]`). The key observation is that for `some v` and `many v` the computation will only complete if `v` fails *eventually*. Otherwise they will recurse forever.
Over the years many people have made games of one sort or another with Brick, so I bet this would be totally feasible. I've added some examples of those projects to the README: https://github.com/jtdaugherty/brick#featured-projects
Yeah I think I get it. And in many's case when the first one fails you'll get an empty list, whereas in some you'll just immediately get whatever the fail state of the type is?
exactly.
Outside of mimicking a set comprehension from an academic paper, I don't think in those terms. I've either already written the map / mapMaybe or half of the [] Monad `do`-block before I even think about them, and I don't find them particularly readable over either of those. I really can't think of anything I'd rather do with that syntax, and hysterical raisins is enough to keep it until there's another use.
I've been told that `MonadFail` is meant to be for `Monads` that can handle `fail` without runtime error / loss of totality. (For example, fail :: _ -&gt; Maybe_ = const Nothing and fail :: _ -&gt; [_] = [].) RebindableSyntax would allow you do use something other than `Prelude.fail` and something other than `Prelude.(&gt;&gt;=)`. I've only ever used it for indexed monads, but local bindings like: fail _ = pure Nothing (&gt;&gt;=) = flip (witherM . fmap (fmap Just)) Might let you use `do`-notation for `Applicative, Witherable` instead of `Monad`. (Or maybe not, you'd have to play with it a bit to see if you can get the types to align the way you want.)
Thank! I'll play with it a bit.
Conversely, my loins are aflame with passion for function programming... Monads really get me going.
We've been around longer, so just keep at it, and you'll do fine. Plus, GHC does have a number of extensions that are most commonly used and make changes to the grammar. So, in practice you can't take the report and write a tool to analyze Haskell code "in the wild". The haskell-src-exts package is a better guideline for Haskell grammar "in the wild". The most important thing (besides incremental progress) is to be agile enough that `rustc` development achieves the pace they want while still following the spec. If `rustc` development feels hamstrung by your efforts, they might stop considering the canonical grammar in their efforts, and if they abandon it, it won't matter how nice it is.
I asked that in a different comment, but let me ask you too. What do you think will be faster, creating a million green threads to handle a million of independent tasks where each task can fully load one core, or create 8 green threads (on a machine with 8 capabilities) that will take care of all of those tasks one by one? It is ok if you don't have a use case for `scheduler` library, for example I wouldn't recommend rewriting `unagi-chan` with it ;) This library implements a work stealing scheduler that is designed for handling CPU bound tasks. If you are dealing with concurrency it is much better to rely on RTS and a library like `async`. If you'd like to see benchmarks check out README, there are some. Hopefully more will come in the future.
&gt; We've been around longer, so just keep at it, and you'll do fine. I hope so. :) &gt; Plus, GHC does have a number of extensions that are most commonly used and make changes to the grammar. So, in practice you can't take the report and write a tool to analyze Haskell code "in the wild". The haskell-src-exts package is a better guideline for Haskell grammar "in the wild". Ah, good point. I do love `-XLambdaCase` and such extensions; they make writing Haskell extra pleasant. &gt; The most important thing (besides incremental progress) is to be agile enough that rustc development achieves the pace they want while still following the spec. We're currently at the stage where we have a [basic grammar](https://github.com/rust-lang-nursery/wg-grammar/) that can be parsed with a GLL parser but which does not deal with disambiguation whatsoever. The idea is to then add mechanisms for disambiguation and refining the grammar incrementally, testing it against the ecosystem, and adding unit tests. We can parse most of `rust-lang/rust` itself however but the output is a highly ambiguous parse forest. So most of our issues right now are about tooling, not the actual grammar spec. &gt; If rustc development feels hamstrung by your efforts, they might stop considering the canonical grammar in their efforts, and if they abandon it, it won't matter how nice it is. So Rust's processes work a bit differently here. The grammar WG works under the language team (which I'm a member of) which decides what features go into the language and not. The compiler team does most of the development of `rustc`, including optimizations, improved diagnostics, but also language features. The language team then decides when to stabilize features. The eventual goal, which we are not close to yet, is that we require any changes to the syntax to be formalized into our canonical grammar before something can be stabilized. We also have thoughts about how to deal with unstable stuff and how to help out with the evolution of the language itself by providing a common framework for specifying and testing syntax changes.
They did record last year's talks and made it available on Youtube.
You don’t get a weird feeling deep into your thighs? Must just meant to be for those “rockstars”.
Thank you for the kind words!
About month later - how's it going? Find some favorite resources? (Tab is still open on my phone :).)
Looks cool. Check out [massiv](https://www.stackage.org/lts-13.14/package/massiv-0.2.8.0) for dealing with 2d arrays. I think it'd simplify things for you a bit.
Guix is a philosophy and lifestyle choice, and Nix is a package manager
Does NixOS not draw out the same ideological largesse?
&gt; I’m going to throw a () in there and see what GHC complains about. I usually use typed holes for this sort of queries, they provide more context. But () works surprisingly well here.
\&gt; GHC 10 or greater back to the future? :)
Not only that, but for some strange reason introductory haskell material has an inexplicable love affair with them, which does a great job of confusing newcomers.
I tried nixos, not a huge fan of it tbh. Guix is great, I just wish the language was Haskell instead, but it's close enough I guess !
&gt; I'm curious if any haskellers here have gone down this rabbit hole About a year ago my my previous distro (xubuntu) bricked when I attempted to upgrade to the latest version. I had already been aware of and sold on the ideas of NixOS and Guix so I took the plunge. I encountered the same dilemma, and ended up going down the NixOS rabbit hole. I don't have much to say in the way of a comparison because I haven't tried Guix. I largely dismissed it due to not being able to opt into proprietary software. I prefer free and open source software, but I can stomach a binary blog video driver, printer driver, and a little drm in Firefox to allow me to watch Netflix. I guess these are mostly personal things and depend on how convinced or motivated by the GNU philosophy you find yourself. There are advantages I think Guix has in using Guile scheme that I'm aware of. Editor support is likely much better for Guile than Nix. I use Emacs and most schemes enjoy first class support in this editor. I also feel like it would have easier to learn Guile than Nix despite being so simple. I think this mainly because I like to do exercises. I personally find it hard to bridge the learning gap between playing in the repl with Nix and actually using it to package things. This is exacerbated by not actually needing to package anything. I use NixOS in my free-time and I have managed to get by an entire year without needing to package anything. However, Nix has some advantages. I like the fact that it's purely functional. I find it much easier to read and comprehend other peoples code in purely functional languages because it forces a style of programming with less moving parts (or things to keep in your head at once). The fact that it's a DSL also has advantages in this department by limiting the set of things you can do. I've read through most of the introductory material on the Nix expression language and feel reasonably comfortable reading real code out there. As for the NixOS operating system it's amazing. Once or twice a week I'll update / upgrade all software on my entire system and it's magical how solid everything feels. I've only ever had to roll back once, I don't even remember why, but I remember it working. Also, the OCD side of my really enjoys being able to uninstall something and have it be fully removed. I think it's worth the learning curve. Much like Haskell you price the learning curve price upfront and reap the benefits afterwards. One of those benefits being not having to reinstall and reconfigure things (when reformatting) because you can declaratively specify that in the language and not having to feeling the need to format in the first place because the system doesn't slowly become a mess. 
Excellent, thoughtful answer. Thank you so much!
Thank you for the detailed answer. Though I am not the one who asked the question, I am in the same boat and using Ubuntu now for about 15 years I feel the need to find a replacement, because it just goes out of hand and the last 18.04 LTS on a top-of-the-edge hardware feels so sluggish that even typing on the terminal makes me make mistakes - I never experienced this. I am really ready to make the change now, but I need some software to be supported, because I rely on it for my work. Do you know if Skype and VirtualBox work on NixOS?
&gt;Do you know if Skype and VirtualBox work on NixOS? Yes and yes.
Thank you very much! I will give it a try, in the hope to have a more solid base, that will allow to have easier configuration and migration when I need to replace my computer, get better Haskell support and have more choice of software. Maybe I will get XMonad working.
Can you tell us a bit more, please? What did you not like in NixOS? How are you using Guix? What is your configuration? What do you like, what you do not like?
Try installing it in a virtual machine first. The install can be rough because you have to manually do everything yourself. I also use Xmonad! There's some documentation [here](https://wiki.haskell.org/Xmonad/Installing_xmonad#NixOS) about how to set it up. Here's [my config](https://github.com/willbush/system/tree/master/nixos) if you're interested. I'm kinda still a newbie compared to how good a lot of other people are with Nix so keep that in mind. I use a very minimal Xmonad config, but that should give you an idea of how to get it working.
Here are some of my personal org-mode notes (written for myself). I exported it as ASCII, which is not the most readable, but maybe it will help. 1 NixOS ======= 1.1 Wiki [https://nixos.wiki] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 1.2 Searching for help ~~~~~~~~~~~~~~~~~~~~~~ 1.2.1 Github Gists ------------------ Search through Github Gists using queries like the following: [https://gist.github.com/search?l=Nix&amp;q=haskell] 1.2.2 Github Repos ------------------ Search through Github Repos using queries like the following: [https://github.com/search?l=Nix&amp;&amp;type=Code&amp;q=brittany] 1.2.3 Freenode #nixos --------------------- The nixos channel has [logs]. However, I'm not sure how to search them. [logs] https://botbot.me/freenode/nixos 1.3 NixOS Configurations ~~~~~~~~~~~~~~~~~~~~~~~~ [Here] is a nice gist on Nixos with a bunch of links worth reading. [PaNaVTEC] dotfiles has a ton of awesome things and a folder for nixos. [https://github.com/Infinisil/system] [https://github.com/kmicklas/config] Another good user config that seems to use spacemacs, haskell, and xmonad [Here] https://gist.github.com/rehno-lindeque/4eb5a325202c840cb8c9 [PaNaVTEC] https://github.com/PaNaVTEC/dotfiles 1.4 Rust ~~~~~~~~ [https://www.mail-archive.com/nix-dev@lists.science.uu.nl/msg33296.html] 2 Nix ===== 2.1 cheat-sheet ~~~~~~~~~~~~~~~ 2.1.1 Searching for packages ---------------------------- ,---- | | # Searches nixpkgs for a package. | nix search simple-scan | | # Search for a haskell package | nix-env -f "&lt;nixpkgs&gt;" -qaP -A haskellPackages.brittany | `---- 2.1.2 Installing and uninstalling packages ------------------------------------------ ,---- | # Install a haskell package. | nix-env -iA nixos-haskellPackages.brittany | | # install a package with nix-env | nix-env -i nix-repl | | # uninstall something installed using nix-env command. | nix-env --uninstall cachix `---- 2.1.3 Cleanup and Optimize -------------------------- ,---- | # Collects garbage. The -d flag deletes all old generations of all profiles in | # /nix/var/nix/profiles. Make sure current generation works before doing | # so. Call with sudo to collect system garbage. | nix-collect-garbage -d | | # Optimize nix-store by hard linking symlinks. | nix-store --optimize &amp; `---- 2.1.4 Misc ---------- ,---- | # Performs nix build on default.nix. | nix build | | # Opens repl with default.nix. | nix repl | | # List your nix channels. Call with sudo to see system channels | nix-channel --list | | # Update your nix channels. Call with sudo to update system channels. | nix-channel --update | | # list nix-env generations | nix-env --list-generations | | # opens a shell with the given packages available | nix-shell -p ghc haskellPackages.ghcid cabal-install | | # Opens shell with ghc package in context and runs the given command | nix-shell -p ghc --run 'cabal new-build' `---- 2.2 Learning Nix ~~~~~~~~~~~~~~~~ [Nix by example] [Nix and Haskell in production] [https://github.com/shajra/example-nix] [Nix by example] https://medium.com/@MrJamesFisher/nix-by-example-a0063a1a4c55 [Nix and Haskell in production] https://github.com/Gabriel439/haskell-nix/ 2.3 Nix shell shebangs ~~~~~~~~~~~~~~~~~~~~~~ - [http://chriswarbo.net/projects/nixos/nix_shell_shebangs.html] - [https://nixos.org/nix/manual/#use-as-a-interpreter] - [https://github.com/nrolland/helloNixShebang] 
Came here with similar sentiments 😄
I haven't even tried Guix because I just *hate* lisp but really like lambda calculus. Plus it's far more likely that any use case is supported by nixpkgs than by guix.
There's one thing that I didn't find mentioned in this thread that may be useful to clarify. And all blocking IO should be in `foreign import safe` FFI. If you use `-threaded`, GHC will use a separate OS thread for each green thread that uses `foreign import safe`, no matter what the `+RTS -N`settings is. For example, you may run at 100 OS threads even with `-N4`. But the number of OS threads will never exceed the number of green threads. This means that you can limit the amount of blocking IO going on in parallelel by limiting the number of green threads (`forkIO`). You may want to do this e.g. on spinning HDs to limit seek thrashing, or open file exhaustion when downloading stuff in general. For computation-bound tasks, spawning more green threads than you have cores is almost never a good idea because the only thing one gets is scheduling overhead (even if it's relatively small, it can make integer-factor differences when there are really many green threads) and increased memory usage. For CPU-bound tasks that overhead only makes things somewhat slower, which may not be critical, but for memory-usage-bound tasks, the program may simply crash being out of memory if the parallelism is not restricted.
To pay those costs up front without an apparent reason to do so is a bit silly. If you were to switch to a monad that involved state of some sort -- or pretty much any noncommutative monad, you'll still end up paying nearly all of the cost going over everything and making sure that all the operations/effects are occurring in an appropriate order with respect to whatever it was you added. Or more likely, you won't, and then there will be a few bugs related to that. Writing things with Identity lets you express an ordering on operations which doesn't actually matter, and may not be the one you actually wanted (but it's fine because for the time being it doesn't matter). I would always wait until I actually knew what I was doing before making this kind of transformation. It's not that hard, and it really can wait until you have some idea of why you need a monad.
That's right, my argument was in general about learning more complex tools to solve harder problems. Rust users would typically already be convinced of that, but for Go, Java or C users these might be good examples of real benefits worth accepting more up-front learning. In the same vein, there may also be extra benefits one gets accepting Haskell's learning curve over Rust's (beyond the problems they both solve). I don't think the answers are super clear yet here, but "increased composability", "(ubiquity of) pure functions and the ease of reasoning about them" and "higher productivity due to not having to think about memory all the time" are contenders that may make a Haskell-and-Rust-expert who has crossed both learning curves pick Haskell for many tasks.
There are essentially two problems here: As for why you need the `Ord c` instance: You always needed the `Ord c` instance, even in the original definition. After all, to compare two maps for ordering, you must be able to compare the values for ordering as well. Otherwise, you could have different maps that compare as if they are the same. When using `deriving` at the end of a data type, it generally always knows what constraints the derived instance needs. The original datatype specifially derives as such: instance (Ord a, Ord b, Ord c) =&gt; Ord (NestedMap a b c) This implicit inference of constraints is what may have caused a bit of confusion about the original instance. However, when using `StandaloneDeriving`, *absolutely no such inference happens*. Instead, you're meant to explicitly enumerate all required constraints. Specifically: deriving instance Eq c =&gt; Eq (NestedMap a b c) deriving instance Ord c =&gt; Ord (NestedMap a b c) I believe that if you change the deriving instances to that, you can remove the `Ord c` constraint in the GADT. As for the second problem: the entire point of `GADTs` is that you need the given instances to *construct* the value. After that, you can pattern match on the value to get the constraints at a later time. I'm not sure about the internals of how `Data` works, but it presumably needs to construct a `NestedMap`. In which case, it needs to find an instance of `Data a`, `Data b`, `Data c`, `Ord a`, `Ord b`, and `Ord c`. After all, these are the instances that are required to construct *any* `NestedMap`. All these instances must be enumerated in the instance definition. The `Data` instance will presumably also require some more constraints which are typically inferred, but needs to be explicitly stated when using `StandaloneDeriving`.
Thanks for this thorough reply! This makes a lot of sense. I just tried it out and this works: data NestedMap a b c where NestedMap :: (Ord a, Ord b) =&gt; (Map a (Map b c)) -&gt; NestedMap a b c deriving instance Eq c =&gt; Eq (NestedMap a b c) deriving instance Ord c =&gt; Ord (NestedMap a b c) deriving instance (Data a, Data b, Data c, Ord a, Ord b, Ord c) =&gt; Data (NestedMap a b c) I'm sorry but I \_thought\_ I had laid out the minimum "failing" example but there is one more piece to my issue. The bigger example includes using safe copy: import Data.SafeCopy (SafeCopy, base, deriveSafeCopy) [redacted] $(deriveSafeCopy 0 'base ''NestedMap) But with this, I get the following: Exception when trying to run compile-time code: Found constructor with existentially quantified binder. Cannot derive SafeCopy for it. From the few research I had made on this issue, there seems to be no way around it and I'll need to write the instance myself. But is that true?
It denotes the same thing as in type signatures: A function from t to Integer. `Env t` is the type of functions from t to Integer.
Having looked at the definition of `deriveSafeCopy`, it seems like there is a hard-coded constraint that *all existential and universal quantifiers*, which is what gives `GADTs` its power, prevents the function from successfully deriving. Consequently, there are really only three choices: implement your own deriving functionality (unlikely), implement the instance yourself (tedious), or avoid using `GADTs` entirely. Come to think of it, if you're porting away from `DatatypeContexts`, I'm not sure if you even need `GADTs` in the first place. While I'm not completely sure of it, I believe removing use of `DatatypeContexts` is a backwards-compatible change, since it only imposes further requirements on using the data type without given you anything in return. That is, after all, exactly why the extension was removed in the first place. `GADTs` is only really necessary if you want to be able to later get access to the constraints through pattern matching.
The `type` keyword denotes a *type alias*. The type on the left is exactly equivalent to the type on the right. Consequently, you can substitute the type on the left, `Env t`, with `(t -&gt; Integer` in every type signature in which `Env t` appears. So eval :: Env t -&gt; Exp t -&gt; Integer can equivalently be read as eval :: (t -&gt; Integer) -&gt; Exp t -&gt; Integer This is typically done to simplify type signatures (if you make an alias for a very long type signature, for example) or give more descriptive names to existing types (which seems to be the intent here). Perhaps the key intuition to gleam from this is that the type of functions is no different from ordinary types, so you can use them in `data`/`newtype` declarations or type aliases. Haskell is very consistent in that way.
I see. I still can't quite figure out how to actually make an environment that gives values to variables though
Thanks this helped a lot. I still can't quite figure it out though. I now defined the following x = Variable "x" y = Variable "y" z = Variable "z" e :: t -&gt; Integer e v | (v == "x") = 1 | (v == "y") = 2 | (v == "z") = 3 | otherwise = 0 I thought this would create Variables "x", "y" and "z" which, in the environment e, have the values 1, 2 and 3 respectively, and any other variable having the value 0. However when compiling and then trying e "x" I get: error: Variable not in scope: e :: [Char] -&gt; t Am I missing something here? t is a generic type right??
Leksah does not require nix. It merely requires a few no haskell packages as prerequisites. You can install them with one line command on any linux distro. For example ubuntu: sudo apt-get install libgirepository1.0-dev libwebkit2gtk-4.0-dev libgtksourceview-3.0-dev Then proceed and install leksah either with stack or with direct download and cabal. Up to you.
Could I please see the full code? In particular, I'd like to see where you are referencing `e`. One issue that I can see, however, is that that definition for `e` won't work. The type variable `t` in `Env t` can stand for any type, but in the definition for `e` that you have given there, you are assuming that `t` is a `String`. The `==` function can only compare values of the same type, so when you compare `v` to `"x"`, for example, you are implying that `v` is necessarily of type `String`. In other words, the type signature of `e` is too general. The correct type signature of `e` would be `e :: String -&gt; Integer`. Something to keep in mind is that, as a type variable, `t` is just a "placeholder" for a type. So a type signature of `e :: String -&gt; Integer` is a valid type for `Env t`, where `t = String`. It's a lot like variables in math, where variables can be assigned to values (like `x = 2`). So when you call eval :: Env t -&gt; Exp t -&gt; Integer with the `e` function, then `t` will be "assigned" to type `String`. If you were to instead call `eval` with a function of type `Integer -&gt; Integer`, then `t` would be "assigned" to type `Integer`. Hopefully all that makes sense. 
I got it to work. Basically an instance of an environment that defines variables needs a specific type, not t. Or it could be t as well with something like: e :: t -&gt; Integer e v = 1 e being an instance of an environment here. It's basically what you just told me. Changing the signature to :: Char -&gt; Integer (I replaced "x" with 'x' etc.) Thanks for all your help I really understand this a lot better now. I do have one more question. We are also supposed to now make an instance of Show, where we define how an expresison is printed. I already did it (and it works) for everything except Variables, but I don't know how to print a generic type t. Here is the function: instance Show (Exp t) where show (Variable v) = show v --THIS LINE DOESN'T WORK :( show (Constant c) = show c show (Sum exp1 exp2) = "(" ++ show exp1 ++ " + " ++ (show exp2) ++ ")" show (Less exp1 exp2) = "(" ++ (show exp1) ++ " &lt; " ++ (show exp2) ++ ")" show (And exp1 exp2) = "(" ++ (show exp1) ++ " &amp;&amp; " ++ (show exp2) ++ ")" show (Not exp) = "(not " ++ (show exp) ++ ")" show (ITE exp1 exp2 exp3) = "if " ++ (show exp1) ++ "then " ++ (show exp2) ++ " else " ++ (show exp3) So yeah basically the marked line doesn't work that way and I don't know how to make it show the variable name tbh. So I'm guessing if it's a variable: x = Variable 'x' then it should show an actual x in the terminal. Any ideas on that?
Awesome! I'm glad it makes sense. I'm happy to help. The reason your `Show` instance doesn't work is that without any constraints, the compiler has no way of knowing anything about `t`, including whether or not it is a type that can be converted to a `String`. The way to solve this is by using what are called *typeclass constraints*. In essence, these constraints limit the possible types that are valid "assignments" for `t` to those which have instances of the given typeclass. So your instance declaration should then be instance Show t =&gt; Show (Exp t) where So this says that this instance of `Show` only applies to the type `Exp t` when `t` also has an instance of `Show`. These constraints can also be used in type signatures so that you could, for example, write a function with a type signature `Show t =&gt; t -&gt; Integer`, and use the `show` function in its implementation.
I figured that was the reason as well but I didn't know the solution. It worked like a charm, thanks so much for all your detailed help :) Got the whole program to work now. Idk if you care but I thought since you helped me make this I'd just share it here so you could see the final result - if you don't care that's fine as well though :P data Exp t = Variable t | Constant Integer | Sum (Exp t) (Exp t) | Less (Exp t) (Exp t) | And (Exp t) (Exp t) | Not (Exp t) | ITE (Exp t) (Exp t) (Exp t) type Env t = t -&gt; Integer eval :: Env t -&gt; Exp t -&gt; Integer eval e (Variable v) = e v eval e (Constant c) = c eval e (Sum exp1 exp2) = (eval e exp1) + (eval e exp2) eval e (Less exp1 exp2) = if ((eval e exp1) &lt; (eval e exp2)) then 1 else 0 eval e (And exp1 exp2) = if ((eval e exp1) == 0 || (eval e exp2) == 0) then 0 else 1 eval e (Not exp) = if ((eval e exp) == 0) then 1 else 0 eval e (ITE exp1 exp2 exp3) = if ((eval e exp1) == 0) then (eval e exp3) else (eval e exp2) instance Show t =&gt; Show (Exp t) where show (Variable v) = show v show (Constant c) = show c show (Sum exp1 exp2) = "(" ++ show exp1 ++ " + " ++ (show exp2) ++ ")" show (Less exp1 exp2) = "(" ++ (show exp1) ++ " &lt; " ++ (show exp2) ++ ")" show (And exp1 exp2) = "(" ++ (show exp1) ++ " &amp;&amp; " ++ (show exp2) ++ ")" show (Not exp) = "(not " ++ (show exp) ++ ")" show (ITE exp1 exp2 exp3) = "if " ++ (show exp1) ++ " then " ++ (show exp2) ++ " else " ++ (show exp3) I also hard coded function `e` and `expression` to make it easier to test x = Variable 'x' y = Variable 'y' z = Variable 'z' e :: Char -&gt; Integer e v | (v == 'x') = 1 | (v == 'y') = 2 | (v == 'z') = 3 | otherwise = 0 expression = ITE (And (Less x (Constant 3)) (Not y)) y (Sum (Constant 5) (Sum (Constant 2) z)) Again - thanks so much and have a really awesome day/weekend :)
Typed holes can give *really* bad results if they're inferred to by polymorphic. In this case, GHC complained that it found a type hole `_f` with inferred type `_f :: t1`.
I care very much, it's great to see newcomers experience success with Haskell. Thank you for sharing. :)
Same way you makes function.
You should just write newtype NestedMap a b c = NestedMap (Map a (Map b c)) deriving (Eq, Ord, Data) and define the necessary constraints at use sites.
 &gt;If you need to pass information from one piece of the program to another, you just copy the keys you still need from the old map to the new one. (def newmap (select-keys oldmap \[:key1 :key2 ...\])). No legacy cruft carried along. That pattern is problematic because you coupling the old map with the output for the new map. You would break things if the old map changed its structure.
Hey r/haskell! I'm starting a blog series for Unix core utilities where I break down the implementation piece by piece. I'm planning to go from `cat`, `which` and `sleep` to `uniq`, `tr`, and maybe `grep` + `sed`. Let me know what you think!
&gt; You pay a lot of the price to have immutability What price? 
super useful, id love to read all those blog posts asap ;)
No, you just do `foldr (\x y -&gt; x : y) "" "dog cat"`
Well, sure, but they already knew that. They just wanted to know why they couldn't use `[]`.
What happens if an exception occurs while reading the files?
Hmm, what about binary data? Would be interesting to see how that would be handled, compared to a text-handling version.
Nice article! I was expecting some discussion/mention of lazy IO and the streaming behavior of `cat`. Maybe that's more advanced/nitty-gritty than you had in mind. But it does seem like an important "real-world" aspect of `cat` and Haskell's `readFile`. And I've now started wondering about open file descriptors… it's not easy to be a polite OS process! Looking forward to your articles.
great, thanks. i will def. have a read
These are excellent points, and a key part of why the core utilities are so useful. I'll definitely bring them up in future articles!
It throws an error, just like real `cat`.
i know :) i was simply following the recommended build instructions and it led me to wanting to explore NixOS or GuixSD
Could you elaborate? cat /dev/urandom &gt;/dev/null doesn't throw any errors here.
Data.ByteString.Lazy is perfect for this! We can handle binary by using that module's IO functions ```diff +import qualified Data.ByteString.Lazy as L type Argument = String -type FileContent = String +type FileContent = L.ByteString collect :: [Argument] -&gt; IO [Either IOException FileContent] -collect = mapM (try . readFile) +collect = mapM (try . L.readFile) display :: [Either IOException FileContent] -&gt; IO () -display [] = getContents &gt;&gt;= putStr +display [] = L.getContents &gt;&gt;= L.putStr display files = do mapM_ toConsole files when (any isLeft files) exitFailure where toConsole (Left exception) = hPutStrLn stderr $ show exception - toConsole (Right content) = putStr content + toConsole (Right content) = L.putStr content ``` Here's a copy of the source with these changes - https://public.anardil.net/share/code/source/lazy_binary_cat.hs
Presumably because you are piping the output to `dev/null`
nice! I always wanted to do something like this to learn haskell!
cat /dev/urandom &gt;myfile.bin also doesn't fail FWIW...
I could tell you many things, but it would not be ethical haha, that was a nice way to tell me about that error. I meant Haskell 10, so ill just remove it
If you noticed, I also made a PR to correct that text to my best understanding. I’ll be happy to discuss the details in private. My contract information is e.g. in my GitHub profile.
I tried this, but unfortunately it still chokes on the `deriveSafeCopy` with: ``` • Could not deduce (Ord b) arising from a use of ‘safecopy-0.9.4.1:Data.SafeCopy.SafeCopy.getSafePut’ from the context: (SafeCopy a, SafeCopy b, SafeCopy c) bound by the instance declaration at /Users/pierre/Projects/mesh/node/.stack-work/intero/interoLoeXRc-TEMP.hs:61:3-36 Possible fix: add (Ord b) to the context of the instance declaration ```
You are right, I tried that at first but forgot why it didn't work until I tried removing the constraints on the data type again. It gives me the following error: Could not deduce (Ord b) arising from a use of ‘safecopy-0.9.4.1:Data.SafeCopy.SafeCopy.getSafePut’ from the context: (SafeCopy a, SafeCopy b, SafeCopy c) bound by the instance declaration Possible fix: add (Ord b) to the context of the instance declaration So like you said, I'll need to do it manually.
This is great, I like that you are not just throwing functions together but actually describing the thought process that leads to a finished program. Very useful for learning Haskell. On a side note, you can use "hPrint stderr exception" instead of "hPutStrLn stderr $ show exception".
Since they're not redirecting `stderr` and `cat` won't be aware of where its output is going, I don't think this is the case.
The difference and relationship between "forall types" and "type abstractions" is clearer with dependent types, where types are terms. The pi type, also known as the dependent product type, generalizes the function, or exponential, type. With the pi type, the codomain is a function of the input. The Curry-Howard interpretation of the pi type is universal quantification (forall). Of course, with dependent types, regular, value-level functions can take types as inputs or produce types as outputs. Type constructors are just special functions. So, both polymorphic terms and type abstractions can be interpreted as functions of types. However, polymorphic terms have the dependent function type while type abstractions can have the regular function type. This is [maybe](https://github.com/agda/agda-stdlib/blob/e9c6f576d96ba8c66d54573356ad6f674f260f66/src/Data/Maybe/Base.agda#L24) in Agda: data Maybe {a} (A : Set a) : Set a where just : (x : A) → Maybe A nothing : Maybe A It takes a `Set a` and returns a `Set a`. (The `a` is for universe polymorphism, which deals with the hierarchy of universes.) This is the [type of map](https://github.com/agda/agda-stdlib/blob/e9c6f576d96ba8c66d54573356ad6f674f260f66/src/Data/Maybe/Base.agda#L78) in Agda: map : ∀ {a b} {A : Set a} {B : Set b} → (A → B) → Maybe A → Maybe B Notice that the inputs `A` and `B` appear in the codomain. 
Great read! :D Very well written and informative, and it's exactly the kind of posts we need these days to help people see the simplicity of writing IO in Haskell :) &amp;#x200B; One small nitpick -- \`main\` is not a function :) It's a value of type \`IO ()\`.
Nice writeup. Two notes: You current implementation leads to a space leak in the following piece of code: ``` mapM_ toConsole files when (any isLeft files) exitFailure ``` `toConsole` will generate the `String` contained inside `files`, which will be retained because of the next call which keep a reference to `files`. By switching both statements, such as: ``` let !hasFailure = any isLeft files mapM_ toConsole files when hasFailure exitFailure ``` the strict computation of `hasFailure` does not trigger the read of the files. As a matter of comparison, your implementation memory usage grows linearly with the number of bytes read, mine runs in constant memory. Second thing, your implementation will run exceptions in the middle of the `mapM_ toConsole files` call if any issue appears during the read. 
You're correct, of course. Though to be more specific I meant using functions that would search for a given unique key in the top level map or any nested maps. So if a necessary key and value was not present it would fail, but if it got moved it would still succeed. I didn't articulate my point well. I know Haskell has many excellent features and a much more flexible static type system than C++ or Java, but my experience with static types in C++ and Java in big projects is horrific. In small projects the static type checks are an enormous aid. Once you get past a certain size, you keep finding new cases where data needs to flow from one part of the system to another in a way you didn't originally plan. And then you're faced with ugly choices: * Some kind of global variables. Yuck. It should be obvious this is a horrific option. * Make 'god objects', single enormous types that hold far more information than they should, all kinds of unconnected information, just so they can still be used in other parts of the code. Then you can use your 'god object' everywhere and pass it around freely, but you've got objects so big it's very nearly as hard to reason about as global state variables. * Add the information just where it's needed, and then you need to adjust types all throughout your program to pass the new information around and translate it between different portions of the code. This is trivial when you're adding "| Foo ..." to one type and another pattern match to one or two functions that use that type, but a *combinatorial explosion* of types, functions, and pattern matches (or language equivalent) in a larger project. My day job is a 1 million line Java program and 95% of the code I write is threading trivial changes through dozens of classes - a new data type or a new option for an existing one is created or calculated or input in class A and applies in class X and there are 23 classes between, and I've got to find the least ugly way to adopt all 23 to pass it along. More concretely we have several hundred custom reports based on several hundred data sets in all kind of combinations, and then the business team comes along and says they want to add a new end user account setting that adjusts the query parameters. Conceptually this is ten lines of trivial code, "if user accountType includes _____ then when accessing columns x, y, and z multiply by factors x1, y1, and z1 respectively". But even if you translated our project into Groovy or Kotlin (which are two different flavors of "java without all the redundant syntax") I would still have to modify 15 different files to get this to work. And the type system protects me from a runtime crash but doesn't protect me from delivering the wrong data - I need tests for that. And as far as management is concerned, showing wrong data is worse than crashing.
&gt; One small nitpick -- `main` is not a function :) It's a non-function value of type `IO ()`. Is this a meaningful distinction? A constant is just a nullary function after all. If we wanted to be precise we could say that `main` is an expression representing an unevaluated computation which produces only `()` and may have effects outside of the program itself.
I appreciate your use of exclamation marks. Am feeling hyped right now.
Great idea since the end goal is understandable. I look forward to the next articles.
One problem with this representation is that it will often leave the size `undefined` in situations where it should really be `Infinity`. Consider things like `unfoldr (\ () -&gt; Just ((), ())) ()`, for example. Also, saying that it "doesn't actually add any overhead on the existing functions" seems clearly wrong because it clearly does just that by adding an extra record field to maintain and an extra indirection to access the actual list. Also, `init` works on infinite lists out of the box, so you should update the table. Also, the semantics of `safeReverse` on infinite lists is way unexpected. I'm sure this function has its uses, but I thing the name is misleading.
I guess it's April 1st *somewhere* in the world.
Oh hell yeah!