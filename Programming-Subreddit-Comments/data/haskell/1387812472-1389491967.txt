I upvoted your comment because it brought a quick smile to my face. However, I also feel obligated to point out [HalVM](https://github.com/GaloisInc/HaLVM).
What does function stability mean?
You can read all about it in Section 10.3 of [the history of Haskell](http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/history.pdf). I personally think making seq unrestricted was a mistake and people refactoring should suck it up.
Lazy evaluation ftw
Top 10 of his continent. Terran.
hahaha thank you, very clever. I was hoping to extend it to include the definitions of the words and filter out conjugation forms of verbs, leaving the infinitive. If you can do that in shell I will be thoroughly impressed! lol
A progamer? We're talking someone in KR GM?
Yeah, I'm not sure what to make of that, but I do think it's interesting that the first thing you lose is "protected" methods. A point for the pure OOP crowd.
Complete speculation, but I'm sure there's a hell of a lot of internal demoing and politics between a few internal Haskell projects and "let's rewrite the whole damn thing".
Why would you want a monadic DSL here?
I appreciate that you need a way to weed out those who are clearly unqualified. It's not so much the programming test that worries me, it's the open ended nature - "and anything else you see fit to implement". It encourages polishing the program, writing Restful servers etc, with no real understanding of the use case (since here, there is no use case). I am sure I could implement the first version in 10 minutes with "tight but minimal" code, e.g. simple data type, O(n) lookups for everything, Read/Show serialisation. Would such an implementation be assessed favourably? &gt; something like this shows who is determined enough to bother using their spare time to show motivation That's the bit that worries me. It finds out who has spare time that they are happy to throw away after a job application with no guarantees and no transparency in the ranking process (you might give feedback, but it's not clear in advance if adding tests or turning it into a server is "the best" use of additional effort). That said, the transition from PHP to Haskell is a good choice - I wish you all the best with it.
Top 10 Asia Terran ? There is no that many possibilities ..
Asian!
ez. I will just offrace as a protoss. gg no re. 
What's nice about Bindable is that it is one less fewer term, as "bind" is already part of the definition. And it matches Foldable and Traversable. 
The only times you would ever look at a definiton is when "you are still learning what [it] means". That's what a "definition" is.
You are a heretic and I love you for it.
This still leaves the issue of cryptic appreviations. Sure, if an identifier is used a lot, then it helps to abbreviate, but in type declarations I want to see "producer" not just "prod". Having junk like "STRef" in an API is inexcusable. It is a StateTransformerReference (or really a Reference in the StateTransformer module: StateTransformer.Reference). This one is especially annoying because we also have a State monad, which is similar but critically different, make State an attractive nuisance. I guess it's OK to provide standard short abbreviations, but ccanonical names should be full. Haskell shouldn't look like VimScript (and even vimscript's "au" is short for an identifier "autocmd" -- you can use either. 
http://byorgey.wordpress.com/2013/04/29/monad-transformers-a-cautionary-tale/ The Haskell name there is still pretty bad, especially because monad transformers have the "inside out" quality (hey, like C!) Where the structure of the declaration is inverted from the structure of the type it creates: `StateT s (Maybe a)` declares the forms `Just (a, s)` and `Nothing` ; while `MaybeT (StateT s a)` declares the forms `(Just a, s) and `(Nothing, s)` This trips ups even experts like Brent Yorgey, a leading Haskell programmer and educator. 
More lazy evaluation, please? Which country is it in?
The operator naming styles, and Cabal hell, together tell me that haskell programmers don't use much third party code. They use a few core libraries and then wrote the rest themselves. Corollary: you don't see a lot of multifnctional software like emacs or Eclipse or Firefox. You get single purpose programs that do one sort of thing very reliably.
The first time I read his post I misread his type class methods as both behind in the `IO` monad. This led me to mistakenly believe he wanted some sort of monadic syntax. However, he probably still wants some sort of free object.
Which one is the Haskell name? "Reader" ?
Well, it *would* be their code if they wrote it. ;-) I just find the idea difficult to visualize in my head.
In Haskell, if you have a function IfAThenBElseC a b c, it can return a result (if a is true) without ever receiving c. It is partial application, which is more semantically sophisticated than currying. It is similar to C/Java's short circuit evaluation of conditionals and Boolean operators, but in Haskell any function can do it. On second thought,that isn't actually a semantic blocker. We already have tricks with `flip`, which is like keyword parameters except it does argument permutation instead of naming. 
No more than 10 of them, I would say.
But what if I have a use case Y for abstraction X but I don't know a priori that is a use case for X? It would help if my google/hoogle/haddoc search turned up some matches when I search X, if X is a common use case for Y.
It's curious that some of the most demanded features match up very closely with the features of Repa and Accelerate, yet a statistically significant amount of the respondents indicated they haven't tried either.
I would guess people know that Repa and Accelerate probably do what they need, but they haven't tackled it yet. For me, Haskell libraries often fall into 2 categories: - pick it up and start using immediately with little trouble (like containers, vector) - can't make heads of tails of it, takes significant effort to learn, ends up being very powerful (like parsec, lens) The second category can be quite an investment to add to your toolbox
I could have written it as third f (a,b,c) = fmap (\c' -&gt; (a,b,c')) (f c) if I had wanted to swim in a sea of parentheses, with no operators anywhere in sight.
Interesting subject. I am slightly disturbed by the amount of typo/spelling mistakes though (I am not a native English speaker).
38 upvotes and no comments. Everyone's brain is buzzing about “how can I use this?” A collective, “uh, oh! Hmmm! :-)”
Nice! I've been using Paul Hudak's SOE libraries for a while for teaching (http://cseweb.ucsd.edu/classes/wi13/cse230-a/lectures.html) but they are a bit of a hassle to build across platforms. Do you know if there's any work "porting" that -- at least the graphics and FRP bits, to `gloss` or `diagrams` or similar? If not, perhaps I'll just button down and do it...
Ah, isn't Idris based on edwinb's Epic framework which is specifically designed to be a generic backend for dependently typed languages? :)
That certainly looks cleaner to me, and makes the meaning more immediate with only the slightest increase in verbosity.
When you've written a few thousand lines of code that fmaps just like that one, you tend to favor the other presentation, as it scales with applicative notation. This one only works for this limited case. Consider that every f (a,b,c) = (,,) &lt;$&gt; f a &lt;*&gt; f b &lt;*&gt; f c for example has no presentation that looks like the expansion I gave.
If you don't mind sharing, what about the Go rewrite idea was so bad? I've not used it much but it seems like a language that'd be suited for this sort of problem (maybe not as much as Haskell but still...)
Stability is a property of functions from denotational semantics. All ordinary lambda terms are stable. Basically, it means that a function's result happens for *a reason* and that this reason is assignable to something. Formally: given a function `f : A -&gt; B` and an argument `x : A`, for any `y : B` such that `y &lt;= f x` there exists a minimal `x' : A` such that `y &lt;= f x'` and `x' &lt;= x`, that is, for all `x'' : A` such that `y &lt;= f x''` and `x'' &lt;= x` we have `x' &lt;= x''`. So if you have a function, an argument, and an information bound (a consistent but less defined value) on the result of that function with that argument you can produce a new minimal argument to that function which is consistent with the original argument and provides the minimal amount of information to get this information bound on the result. parallel or is the function which satisfies these equations (this is order irrelevant) por (True,x) = True por (x,True) = True this function is not stable, as `(True,_|_)` and `(_|_,True)` are both consistent with `(True,True)` and both result in `True` but there is no other argument that is less defined than either of these and returns `True`. You can not assign *why* `por` returned true. `por` can be implemented in Haskell por (a,b) = (a || b) `unamb` (b || a) `unamb` is the "I don't want to be stable" operator. That is fine. It is interesting. Stability is limiting. But you have to know what you are giving up. Haskell with `unamb` isn't lambda calculus anymore--it is something more flexible and much more terrifying. 
That's a great approach from the data-processing perspective. Though it's not necessarily a good idea from the linguistic perspective, which the OP seems to be after. Linguistically, I'd probably do something like store all the strings (including punctuation) in a trie; and then use some heuristic to collapse the entries for "foo", "foo.", "foo!",... This way you can still handle things with conventional periods either at the end (because "foo" —sans period— will not occur often) or in the middle (because you don't erroneously conflate "e.g." with "eg"; that sort of thing mattering in some languages)
Definitely use Text here, it's the tool you want. Performance is similar to ByteString, and you can avoid worrying about encoding issues (too much) which is a big win for any language that steps outside of 8-bit ASCII. Even among languages using "the roman alphabet", not everyone uses Latin-1; and that's easy to get confused with Latin-15; and really, all this nonsense is what Unicode was invented to deal with.
It does sound like a cool program to have on hand. Definitely useful for language learners who want to start tackling news articles, novels, etc. Keep us informed on how things go :)
&gt; Rather than a debate, I think it's a research opportunity. Indeed. For a while I've been thinking about how to integrate multi-mode relational programming from LP into the term-based syntax of FP. It'd be lovely to simply assert that a given function implementation is invertible and have the compiler not only automatically generate the inverse, but also generate the proof that they are inverses— definitionally, not just propositionally. (And similarly for sections and retractions; and all of these for n-ary functions/relations.) This is also, IMO, related to Roshan James' thesis work, which is being carried on by Amr Sabry and his IU students. Roshan and Amr were focusing more on the perspective of "information effects" and their relation to reversible/invertible computing. Invertible conditionals and fixpoints are quite nifty. Of course, all the current work is restricted to the first-order setting; no exponentials, just like classic Prolog. But since we now have Curry and lambda-Prolog, seems like that shouldn't be insurmountable. Alas, I already have a dissertation I should be working on in lieu of redditing ;)
&gt; but I prefer not to think of it as a debate, because that tends to push people back towards their core positions. Right-o! I should've been more circumspect in my choice of words. Though I've always ignored such boundaries in arguments, I should do better about not reiterating them 
&gt; The next thing I'd love to see is improved support for cross-compilation. It is currently possible using the LLVM backend, but you have to do a little hacking around by hand to do the final linking and stuff. What sort of thing would you like to be able to do? &gt; My personal opinion is that anytime you are writing an application -- you should think really hard about if you really should be writing a library that has a thin executable wrapper around it. Trying to add it after the fact is hard. I agree! Idris was always intended to be able to do this via its elaborator, so I'm glad Brian got around to lifting it out so that I didn't have to :). I have some half baked ideas for other front end languages that I'll probably never get around to trying, for example... Probably what we really need next for this to be properly useful is some proper documentation!
It never really was "based on" Epic as such, just used it as a convenient back end, since that library already existed. I wanted to try some other ideas though, so made a new defunctionalising back end instead. You can probably get at that via this library...
&gt;&gt; something like this shows who is determined enough to bother using their spare time to show motivation &gt; &gt; That's the bit that worries me. That's the bit that worries me too. This sort of filtering often has the effect of weeding out women and minorities, since they have less free time available to meet these "invisible demands". You want someone with actual Haskell experience; and that's perfectly fine. But it's important to make sure your filters accurately target that demographic itself, rather than some proxy target which just happens to be correlated with it due to systemic bias in the field.
&gt; anytime you are writing an application -- you should think really hard about if you really should be writing a library that has a thin executable wrapper around it. IME, *every* application wants to be a thin executable wrapper around the library screaming to get out.
I've had a look at the info on NSProgress, and I can't see how to make it work nicely with a command line app that's called repeatedly; if it were being called with some flags and then sent the progress on stdin it might be possible. I also can't find a way to set the progress of Terminal.app itself, but it may be possible to have a separate cocoa app showing the progress. I think this is beyond my Objective-C skills; if someone else is interested, info can be found at https://developer.apple.com/library/Mac/releasenotes/Foundation/RN-Foundation/index.html#//apple_ref/doc/uid/TP30000742-TRANSLATED_CHAPTER_965-10_9Progress
&gt; Here’s one way: Consider what happens when you pick one element of a monoid and apply it to all elements — say, by left multiplication. You get a function acting on this set (in Haskell we call this function an operator section). That’s your morphism. Notice that you can compose such morphisms just like you compose functions: left multiplication by “a” composed with left multiplication by “b” is the same function as left multiplication by “a * b”. The set of these morphisms/functions — one for each element — together with the usual function composition — tells you everything about the monoid. It gives you the categorical description of it: A monoid is a category with a single object (“mono” means one, single) and a bunch of morphisms acting on it. It should be noted that this is a corollary of the Yoneda lemma.
Yes, I wonder about that too. That statement sounds discriminatory to me.
Thats a pretty big and beefy programming test task (the RBAC sec system). How do submitters know you won't simply bogart their code and not hire or pay them? Its not a small project. At all.
Why do some people have this dislike for Singapore?
Great post! Fay has been on my list of things to check out properly for a while. I do wonder whether there'll ever be a good solution for sharing server/client code, though - it seems like Fay isn't totally interoperable with Haskell just yet? Oh, and I should point out: setClassName :: Element -&gt; String -&gt; Fay () setClassName = ffi "%1['innerHTMl'] = %2"
&gt; Once we include this, our page behaves exactly as before. Except we also get that warm fuzzy feeling of knowing that we got to write Haskell to generate all of this. I feel that you've missed the real selling points of Fay. I mean, sure, warm fuzzies 'cuz it's Haskell, but as I see it, there are two main advantages over just using JavaScript: * type checking * server-side Haskell and client-side Fay can share the same code for data definitions* *I think. Is that right, Fay users/developers?
Hmm, sounds like I should have expanded on what the warm fuzzies were - the benefit of writing something Haskell-like is not that it looks like Haskell, but *is* the type safety, composability and lazy evaluation. We get all of that with Fay. Not only that, but being a subset of Haskell, I can often re-use existing code to sharetypes - so I get even more interop.
How does this work without typeclasses - do you have to skip out on any `deriving`? I suppose you could define all your instances separately?
Go "ignores basically all advancements in programming language design from the last few decades." - Tikhon Jelvis http://www.quora.com/Go-programming-language/Why-wasnt-Go-written-as-a-functional-language/answer/Tikhon-Jelvis
Haskell seems to filter out women, and what Americans would call visible minorities. 100% of the applicants so far over all rounds since June were male, task or no task, split between Asian and Caucasian. I was surprised at the lack of Indians considering they are very well represented, particularly in Singapore, in technology. Minority is a US-centric term. The 5 Haskellers so far represent 5 nationalities, 4 languages and 3 continents. In Singapore, Caucasians are a minority, and there's a world of difference between Taiwan and mainland China, which you might not pick up from the photos. These are merely statistical concerns as we hire purely based on code quality, since we are only concerned with output, and not quotas or making politically correct people happy. We don't even bother with looking at degrees anymore. There's one college dropout and one high school graduate on the team and both are very good programmers. By and large, Haskellers are on the same page no matter their backgrounds.
The argument reminds me of https://blog.jcoglan.com/2013/11/15/why-github-is-not-your-cv/
logrithmic size (binary vs unary)
I don't know of any such porting effort.
The whole survey was about "array-oriented" computing and Repa/Accelerate were always meant to target that. Moreover, both the respondents as well as the libraries were influenced by common practices in Haskell (e.g., map and fold combinators).
What inference algorithm is being used to deal with RankNTypes?
Hilariously, I've only ever used the websockets library, proving that you don't need JavaScript for that ;) /pedant
Long, but hard to digest article. Too many floating details remain, and the reader left unconvinced about practical usage. The choice of logWF name is dangerously one T away from jokes.
Fay is smaller and has a better FFI interface. If ghcjs begins to produce code of the same size, and gets a better FFI interface, both extremely unlikely, but possible, there won't be much of a reason to use Fay. Currently, ghcjs is good for "applications" (where the code size doesn't matter much), and Fay is better as a general JavaScript (emphasis on "script") replacement.
For making a user facing web application I think Fay will remain a better fit. The output is smaller and working with JS libraries is easier. GHCJS will certainly improve on these fronts but I think it won't be able to reach the simplicity of Fay on these points. I've been experimenting with using Fay to make web based admin interfaces to some daemons. I think GHCJS could be a better fit here. Code size is not really an issue and it might be helpful to be able to use more hackage libraries to share domain logic. 
You don't get any help here unfortunately. It would be nice to add some optionally generated runtime type checking for all FFI declarations during development. It shouldn't be that hard to do! Part of it is also that you don't have to write all these FFI declarations yourself. You can create a package for a 3rd party libraries, such as fay-jquery, so you don't have to reinvent the wheel. Then you can also write more high level abstractions on top of these libraries. In my experience most bugs do come from incorrect FFI declarations. But it might be a good sign, everything else you're doing usually works as intended. 
Singapore has a very staunch legal system that is not at all OK with many liberties people in North America or Europe might be used to. The most often sighted examples are * heavy handed policy towards pot use * not in any way friendly to LGBTQ peoples
Nice post Ollie! The code looks great so I'll just mention some related things I thought about while reading it. Install and compile instructions are missing, but this is easy: cabal install fay fay-base fay File.hs There are a lot of simple FFI declarations, these do not have to be copy pasted every time you start a Fay project. [fay-dom](http://hackage.haskell.org/package/fay-dom) contains basic DOM operations such as createElement. fay-dom is not much more than a stub since most people seem to be happy using jQuery and [fay-jquery](http://hackage.haskell.org/package/fay-jquery). As in normal Haskell, you probably don't want to use String, [fay-text](http://hackage.haskell.org/package/fay-text) contains a Text type that uses JavaScript strings. I never use Fay Strings nowadays. Brackets in the FFI is used interchangeably with dot notation setClassName = ffi "%1['innerHTML'] = %2" appendChild = ffi "%1.appendChild(%2)" This is totally fine, but if you are writing a library or planning to use code in production you should stick to brackets so you can compress the output with google closure without it renaming these external property names. Most of this rightfully didn't make it into the article, but I thought I should point people in the right direction. 
&gt; It would be nice to add some optionally generated runtime type checking for all FFI declarations during development. Like in [Pyret](http://www.pyret.org/)? That could be really handy in a language like Fay.
Won't comment on the first (although it is a bit rich to call Singapore's policy towards drug use heavy handed and then put the US in the same comparison). As for your second point, there's plenty of openly gay people and the only disadvantage they have over straight couples is that they are not eligible for the multitude of discounts on real estate and other help that the government provides families (as an expat, you're not eligible anyway). I consider Singapore to be a considerably freer country than both the US and the UK in almost every dimension that matters, from taxation to the brakes on starting, running and owning a business. Yes, you probably will find cars and real estate quite expensive to purchase, and guns are unfortunately illegal, but cabs are cheap, you can rent a decent place easily, and the police does its job (unlike in most Western countries) so the impact of these restrictions on your life is limited. A free health insurance market, for example, means coverage at a considerably lower cost than in the US or Europe (if you look at the NHS as being financed by your taxes; in countries like France, you can directly compute the hit on your income). I see both European and American politicians continuously pillaging their electorate and simply do not understand how one can accept to work more than half one's life for the government. This is a personal opinion although shared by many people here.
That's the thing, I'm surprised that /u/ocharles had to "reinvent the wheel" that is basic javascript with the functions like `document.createElement` and `document.getElementById`. I wouldn't mind so much if the common functions were pre-implemented using FFIs, because at least then there might be tests and the community checking it. Having to do such basic things yourself each time sounds error prone. Fay seems like a good start for a hs-to-js compiler, but that makes me feel that there's a long way to go still. Maybe it's just that /u/ocharles missed the import that defines many of those for you, or maybe I'm just being too harsh. I do understand needing to do this for the `WebSocket` object, though. I can imagine that it'd be impossible to make a compiler that worked generically with even half of the massive JS ecosystem.
Well, being as I am not a lawyer, and have never visited Singapore, I only have Wikipedia to go by. [LGBT rights in Singapore](http://en.wikipedia.org/wiki/LGBT_rights_in_Singapore) [Misuse of Drugs Act](http://en.wikipedia.org/wiki/Misuse_of_Drugs_Act_(Singapore)) For comparison, in Washington State (where I live), same-sex marriage is legal and the first business licenses for (non-medical) pot stores are being handed out.
That is a benefit, but what if I make a mistake in my FFI declarations? Where do those get checked? If it's only at run-time, then I feel that Fay is only halfway there
We built our own systems already. The standards have been set high because the work is hard. The task is relatively quick for someone experienced and covers a broad enough number of areas that we can get a good idea what people with no open source code are good at, and how experienced they are (in the sense of how many mistakes have they learnt not to make, do they comment the code appropriately, etc.). I don't think a small task like "find a data structure that does X well" is a good filter for anything other than basic computer science knowledge and quick thinking. Project Delta is named after Delta Force, which makes applicants walk non stop for days on end to test their motivation since motivation correlated most strongly with performance in combat; in some ways the idea is the same here. Our first hire did considerably more work than the above - about a week's worth of evenings - and he turned out to build most of the systems, working tirelessly until he had breakthroughs, always refusing to admit defeat in the face of impossibility and ever changing specs. He didn't do that well at whiteboard coding - most of us didn't, actually - so we also dropped this. Work is done by yourself, in your own time, and producing results without the pressure of a deadline; accordingly an entry test should imitate those conditions as closely as possible. Most people won't be hired, but there are 6 positions open. That's a considerably higher chance of getting hired than most other Haskell positions we have seen, and worth the effort. It's worth doing the task if you are less experienced or don't have code up on GitHub since we will probably end up with a mix of experienced old guys and promising young ones to grow into the position.
Ah, but isn't everything?
Quite possibly, I'm not familiar with that name. I just mean that the record update operation is polymorphic in the type of the field in the original object, since the type of the field can change during the update.
You can use `sortBy (comparing (Down . snd))` instead of `flip compare`. The `Down` newtype from `Data.Ord` gives you reverse ordering for free. You could also say `forM wordList $ \word -&gt; do ...` for more control-structurish code. And check out the transformations that HLint will give you, such as ``(`T.snoc` '\t')`` instead of `(flip T.snoc '\t')`.
&gt; That's just being more detailed, it isn't saying what the claim is based on. I only tried provided that details/background that seemed most likely to clear up the question: &gt; What are you basing that on? There are of course many different gaps in knowledge, understanding, perspective possible so I probably did not hit exactly the right one. If you think there is something more likely to clear up the misunderstanding I would like to see how it compares to my effort.
What I'd really like to do is compile Idris code to the Raspberry Pi 'bare metal'. So, no Linux on the RPi -- just the idris RTS running on as close to nothing as sensible. Similar to the old hOp/House stuff. And a bit similar to the newer HalVM stuff as well. I was going to start by trying to just do simple cross-compilation from Ubuntu/x86 to the Raspbian/ARM. Then investigate whether is better to try to target some existing minimal bootloader/libc/libgmp setup, or try to create an RTS that is self contained. My eventual goal is to create a domain specific language for creating light sequences that targets TT. I've been reading [Idris, a General Purpose Dependently Typed Programming Language: Design and Implementation](http://eb.host.cs.st-andrews.ac.uk/drafts/impldtp.pdf) and looking at the source code. Loving it so far. 
The FFI is usually the point of most errors in Fay applications, i.e., when you touch JavaScript. :-) (Similar to using C from Haskell. When you get a segfault, it's normally a C library that's the culprit.)
There's a [dom](http://hackage.haskell.org/package/fay-dom-0.2.0.0/docs/DOM.html) library. I think /u/ocharles was demonstrating how to use the FFI. Realistic Fay programming involves using the FFI a lot, because you use JavaScript libraries a lot.
That makes sense, and it's good to see that someone has already put in the effort that I wouldn't want to do.
I'd imagine that the JS that Fay generates is more "readable" and more closely resembles the Haskell source it was generated from.
I've not seen much but the lazy evaluation and thunks mess with things. It's readable in the small scale but the bigger picture can get obfuscated if you lack focus. Edit: I should say the currying messes with things. Three argument functions get wrapped in three individual functions, which is easy to understand but syntactically distracting.
how are Dedekind cuts yoneda? That sounds cool.
Is this a joke? It reads like a parody of both recruiter-driven job adverts and Ruby-on-Rails rockstar machismo.
This is true for anyone with choldren or family.
No, because in German, ß upcases to SS. ẞ isn't new, but it's extremely rare, and most Germans don't know it has ever existed. To the vast majority of German speakers, ß upcases to SS, end of story. This is about the German language, and human languages don't have bugs. Unicode is correctly representing how German does things, which means Unicode doesn't have a bug in this instance.
&gt; The ẞ got created ... long before Unicode.
Unicode isn't broken. Languages are complex, and Unicode forces programmers to face that fact. It's the first encoding to do so.
So this is Cayley's theorem for monoids. Why is this relevant in the original context, i.e. regarding a monoid as a category with one object? Is the OP saying that he wants to regard a monoid as the subcategory of Set consisting of the underlying set of the monoid as the only object, and maps equivariant under right multiplication as morphisms? I guess that works, but that's somewhat non-obvious and unnecessary. Not all categories need to be subcategories of Set. You can build a category with any single object `x`, and the elements of the monoid as the morphisms `x -&gt; x`. The object `x` doesn't need to be a set, and consequently the morphisms `x -&gt; x` don't need to be functions. Also, Cayley does not follow from Yoneda unless you already know that a monoid is a special case of a category.
Actually, /u/ocharles doesn't really know what idiomatic Fay is - so this is how things look to a new developer who spends an hour researching something before writing a blog post about it :) It's really good to hear that people are already starting to build libraries to abstract this commonality out. I wasn't too bothered about writing all the `ffi` blocks, because I figured in practice you'd only really be writing them once.
&gt; For one thing, *all* the compatibility stuff is non-optional. And yet, to use Unicode, we already carry a bunch of tables. We could carry a bunch of others instead and still be just as compatible. &gt; This includes Emoji: Whatever you think of it, it's politically important to have it, and so it goes in. Emoji are much newer than anything else in Unicode. Besides, if you want a totally broken system, look at the encoding of emoji. * [Country flags](http://en.wikipedia.org/wiki/Regional_Indicator_Symbol) are encoded as `&lt;RIS letter #1&gt; &lt;RIS letter #2&gt;`. Now codepoints are not only context-sensitive, which itself is fine, but also [very time-sensitive](http://en.wikipedia.org/wiki/Flag_of_Ethiopia#Historical_flags), which is directly contrary to one of the design principles of Unicode (paragraph 2.2, "Stability"). * There are two different sets of emoji with roughly the same spectrum of facial expressions, but one has a generic face shape, the other has cat ears. No `&lt;facial expression&gt; &lt;mark: cat face&gt;` or anything like that, just two distinct sets. &gt; Welsh and Klingon aren't nearly as politically important, so they can wait. If the UC decides which languages to include, natural or otherwise, by their political importance, then that would be a truly despicable reason. &gt; It's complicated in a dangerous way. That's true for the realization you suggest, but it's not the only possible way to implement it. I hope you understand that your suggestion consists only of question marks. Stop thinking in C-style strings and XML-style trees. Use an ordered tree structure with valued leaves, where every node is prefaced with the exact size of a chunk and the maximum tree height (zero for leaves). It's not like it's hard to write a text editor that can properly deal with this. I'd even bet it's about as hard as writing a "normal" Unicode-oriented one. If you still insist on using a stream-like encoding, forget about the end markers. Use only start markers (and possibly some repeat markers, if your transport/storage requires that level of paranoia), which encode the path in the tree up until then, or even just the 'codepage' in use.
Have you considered the Bay Area? We have an extremely vibrant and growing Haskell community. I went to both BayHac and HacPhi, and the former was twice as big with some really exciting projects going on. Obviously, we have plenty of general-purpose tech talent as well. It's expensive, but so are Sydney and New York--comparable, even.
That answered it perfectly, thank you :)
The details will matter as to whether it's a strong update. The latter is primarily used in typed assembly languages, which need an operation to change the type of a byte vector into another type to support type safe heap allocation. If your references are simply polymorphic, it sounds like you either perform a strong update on every set (what about aliasing?), or you'd need some sort of dynamic type check on get to ensure soundness.
The accepted answer for http://math.stackexchange.com/questions/37165/can-someone-explain-the-yoneda-lemma-to-an-applied-mathematician explains the Dedekind cut-Yoneda connection.
/me stands and ovates.
Children and family defiantly are a limit on that stock pile of resources in many ways. The difference commonly pointed out is that children/family are often a choice whereas being a woman/minority is not. At the extremes it is possible to run away from children and family, shirking whatever duties you associate with them. It is not possible to run away from being a woman/minority/etc to the same extent.
Typo: setClassName = ffi "%1['innerHTMl'] = %2" should be setClassName = ffi "%1['className'] = %2"
I'm not sure if this answers your question, but the records are persistent. Every field update copies existing fields into a new record before updating, so I don't think soundness is an issue here, unless maybe when using the FFI. See here https://github.com/paf31/purescript#record-updates I may have missed something though. You can also see the generated Javascript [here](http://tryps.functorial.com)
&gt; Minority is a US-centric term. I don't think so. What counts as a "minority" may vary depending on the setting, but the basic problem of discrimination and exclusion remains. When I talk about minorities in CS/IT I'm talking about people who are underrepresented in CS/IT, whether on the local scale or on the global scale. I would not consider asians nor indians to be minorities in CS/IT, neither locally(=US for me) nor globally. Whereas black peoples —whether African, African-American, British, etc— *are* underrepresented both locally(=US) and globally. &gt; These are merely statistical concerns If that's your opinion then I know to advise my friends and coworkers against applying to your company.
I think Ollie needs some kind of "Haskell St. Nick" Reddit flair.
Bravo!
There isn't any reason women cannot become Haskell programmers if desired. I'm one. So are friends of mine. The required investment is the same as it would be for men. No, my complaint is about requirements like: requiring a list of repos for F/OSS projects you've worked on, or requiring applicants to provide a mockup of some poorly specified project. Requirements like these purport to measure one thing but actually measure something else entirely. Consequently they are a form of "invisible" discrimination since they serve to reinforce systemic biases despite appearing on the surface to be bias-free. The problem with requiring F/OSS repos is twofold. First, it requires that people have enough free time (and social inclination) to contribute unpaid labor. Due to systemic biases, women already perform more unpaid labor than men because it is expected that we will be the ones to maintain the household and take care of the family. Therefore, women will on average tend to have less free time, or inclination, to contribute additional unpaid labor to F/OSS. This is one reason why there are [vastly fewer women in F/OSS than in CS/IT as a whole](http://flosspols.org/deliverables/D16HTML/FLOSSPOLS-D16-Gender_Integrated_Report_of_Findings.htm). Second, women who do get into F/OSS (like me) are often relegated to non-coding positions (documentation, UI design, art/music, advertising/outreach, etc). This is again due to systemic biases against women. Some of these biases happened in the past, leading women to become writers, artists, musicians, and socialites rather than coders. But many of these biases are ongoing: assuming women are better at those tasks than at coding; men being unwilling to perform those tasks (I'm looking at you Haskell documentation!); developing an exclusionary male-dominated coding atmosphere; etc. Regardless of the reasons behind the bias, the result is that women in F/OSS will on average have far fewer code contributions than men. These issues are discussed at greater length by [Ashe Dryden](http://ashedryden.com/blog/the-ethics-of-unpaid-labor-and-the-oss-community). As evidenced by the studies she cites, she's hardly the first one to bring them up. The poorly specified mockup suffers from similar problems, but especially the one about having the free time and inclination. Requiring a mockup isn't necessarily a bad thing. When both the goal of the design and the metrics for judging success are clear, it can be a great tool for determining the actual skill of designers. However, when the goal and success metrics are unclear or poorly specified, then the mockup no longer tests for actual skill. Instead it ends up testing things like how much free time the applicant has, how much they can grow/bloat a project idea, how good they are at choosing project ideas the judge will think are novel, etc.
&gt; Given that women have higher educational attainment We do? Sure, if you look at [*all* undergraduate degrees](http://www.ncwit.org/sites/default/files/legacy/pdf/BytheNumbers09.pdf) then 57% of them go to women. But if you look at [undergraduate degrees *in CS*](http://blogs.computerworld.com/sites/computerworld.com/files/u28/women2.jpg) then we've never gotten more than 40%, and for the past few years things have held steady around 20%. And if you look at [masters degrees in CS](http://blogs.computerworld.com/sites/computerworld.com/files/u28/women3.jpg) or [doctorate degrees in CS](http://blogs.computerworld.com/sites/computerworld.com/files/u28/women4.jpg) the story is much the same.
Thank you!
I wonder whether you could publish some of proposed solutions? It could be extremely helpful to compare different approaches to the same problem.
&gt; Country flags are encoded as &lt;RIS letter #1&gt; &lt;RIS letter #2&gt;. Now codepoints are not only context-sensitive, which itself is fine, but also very time-sensitive, which is directly contrary to one of the design principles of Unicode (paragraph 2.2, "Stability"). &gt; There are two different sets of emoji with roughly the same spectrum of facial expressions, but one has a generic face shape, the other has cat ears. No &lt;facial expression&gt; &lt;mark: cat face&gt; or anything like that, just two distinct sets. Seems like they're damned if they do and damned if they don't: If they make it context-sensitive, their solution is too complicated; if they make it context-free, they're using too many codepoints. &gt; If the UC decides which languages to include, natural or otherwise, by their political importance, then that would be a truly despicable reason. It's how the real world works. When I said you were ignoring nuance, this is precisely what I meant. &gt; Use an ordered tree structure with valued leaves, where every node is prefaced with the exact size of a chunk and the maximum tree height (zero for leaves). Clever, but how does this work when someone inevitably uses *signed* types for those numbers and someone else inevitably uses *negative* values in them? &gt; Use only start markers (and possibly some repeat markers, if your transport/storage requires that level of paranoia), which encode the path in the tree up until then, or even just the 'codepage' in use. And how does this handle copy-paste?
From first glance looks vaguely similar to Daan Leijen's HMF inferencer for higher ranked types. Was it adapted from this work?
No reason you can't still use lattice and/or ggplot in a haskell program, if you use [this quasiquote (Rlang-QQ)](http://hackage.haskell.org/package/Rlang-QQ)
Sorry, [Domain Driven Design](http://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215)
&gt; Being a woman doesn't lower your free time, that's what we're getting at. It was just a completely baseless assertion. I was not trying to make any assertions. Only to clarify/illuminate a commonly held thought/argument. &gt; free time, The common argument is not about free time specifically but rather a stock pile of general resources.
I love it. I don't care about the spelling mistakes. Three are so few articles about this.
Today that tinge of sadness hit. Not only did my chocolate advent calendar run out, but 24 Days of Hackage finished. I will join your ovation. Bravo to Ollie et al.
Wouldn't a better lattice to use be `{True &lt; False}` (the opposite of the normal order on booleans)?
Not gonna lie, that'd be pretty rad!
&gt; So, joinStates breaks associativity, and so the set of states that a Result can take on is not a join-semilattice. But, is this a problem? Does breaking associativity actually matter in practice? It doesn't seem to matter for the code presented in the article, since asyncAnd only spawns two threads. In fact, the definition of States is tuned for the computation of the join of precisely two lattice elements. Associativity is therefore irrelevant. The counterexample *TrueBot* V *BotTrue* V *F* just cannot arise here.
I think the argument is that if you only do it for two puts into the lvar, you can never witness different behavior under different associations of operations...
Lenses are like the dot-access you get in OO languages, but composable with themselves AND everything else (normal functions) as they are just functions.
&gt; * HTML templating libraries - pretty much all of them. Disagree. I've found the haskell templating libraries pretty sparse. I can't find a good runtime templating library.
There's folks working on That. Stay tuned. 
Carter! I want to know more!
There's another problem with your line of reasoning. It's a case of applying an inappropriate statistic. The only relevant statistic is for the subgroup of people who are interested in, and at least conceivably qualified for the job. Thus we can probably restrict ourselves to statistics on CS majors with Haskell experience. The woman stereotype that influences the general gender statistic: single mom in low-paid job is unlikely to be relevant to a statistic regarding this group. 
If one of the threads comes back `True`, how would you distinguish between knowing the final answer (when both threads have come back `True`) and not knowing it yet (when the other thread is still running and might be `False`)? At the very least, there at least needs to be a distinction between `TrueBot` and `TrueTrue`. If I've misunderstood your comment and you're actually arguing that `Top` and `False` should be synonymous, then I agree that in this example it's silly to have a `Top` state that can never be reached, but in the more general problem it's useful to have a `Top`-like state that is greater than all others, because the penultimate states might not have a well-defined ordering.
Well, I'm saying that if we merely want to compute `and`, `{True &lt; False}` is the ideal lattice.
The names are slightly misleading. They're really `TrueTrue` and `FalseAny`, and neither one should be higher in the lattice than the other. They're both final states of the computation (_edit: and thus they should never be joined; if they do get joined, the result should be an error like `Top`_). I agree that `{TrueBot &lt; False}`, but that's already listed in the article. _re-edit: Ah, perhaps I see what you mean now. Since `TrueTrue` and `F` are both final states and should thus never be joined, it's not really important what the result of their join should be (because this computation should never occur). If we made their join be `F` instead of `Top`, the poset would indeed be a join-semilattice, thus solving the problem described in the article. As long as we operate on at most two threads in `asyncAnd`, this should work. I'm a little uncomfortable with this because we're relaxing the guarantee on `getResult`, but I think it would work nonetheless._
I've tackled Repa &amp; Accelerate. Lens is still beyond me... There's so much hype about it and so few good tutorials.
It's strange that fay receives much more attention when haste is better in every way — full Haskell, all hackage etc
Thank you again for a wonderful series, and merry christmas!
They're almost invariably compile-time libraries; the reasons mainly boil down to the fact that the Haskell community is deeply in love with compile-time guarantees and checks. A compile-time templating system can leverage Haskell's powerful type system; a template is then little more than another piece of code, compiled into your application, except that it's written in a syntax more suitable for templates than plain Haskell. For a language like PHP or Python, where there is no compiler in the traditional sense, nor a meaningful type system, such an approach doesn't make much sense - you can't get compile-time type checks anyway, so it's better to go with something that allows for a "real-time" development workflow.
The whole idea of `LVar` is that it's on a lattice; the theory behind them depends on that fundamentally. If it's not on a lattice - it's not an `LVar`.
Yeah, but perhaps you could say that you can finish early when you hit something different than Top.
FP Haskell Center allows you to install extra packages from Hackage, arbitrary Git repositories, or snapshots taken of other FP Haskell Center projects. This configuration can be handled on the project settings tab (click on the three horizontal lines icon in the top-right of the IDE). We [have a SoH tutorial](https://www.fpcomplete.com/school/using-fphc/fp-haskell-center-settings-editor/fp-haskell-center-settings-page#extra-packages) detailing all of the settings available, including extra packages.
&gt; The UC is a standardizing body, not a political organization. Wow. How little you know. At that level *everyone* is a political organization. &gt; If you make the specification clear enough, that would be non-conformance, and therefore simply wrong. And if wishes were horses, then beggars would have meat for Christmas. &gt; Having a good reference implementation with a super-permissive license is also a way to prevent this bug from being introduced. Maybe. Until it's patented, at least. As for the rest, you have a much higher opinion of what the average programmer can do without screwing it up than I do.
you can learn a lot about lens just picking your favorite data type (preferably with some nesting) and following the front page instructions to make a lens for it. helped me anyway 
Yes, but I'm working on a haskell program whose entire purpose is to fill in templates, so I need runtime templating.
The signatures are in the links to Hoogle. I tried to reduce the details since the article was too long.
A real problem of integration in which this solution were the easier solution would involve different workflows, web services with failures etc. would produce an article five or ten times longer. This is one of the most simple examples and yet, as you said, I had to leave many details aside. You have to extrapolate his utility for real (i.e. really complex) problems.
Wait, purescript isn't pure?
Sadly not any more. The plan is to add some sort of effect system later, but in order to be practical for now, values with type Unit can have side effects if you use the FFI. Suggestions for better names are welcome: https://github.com/paf31/purescript/issues/58 :)
Ok. I didn't mean to put words in your mouth.
I found it enlightening anyway. :)
This is awesome -- one of the reasons I like writing Haskell so much is because it seems to closely resemble writing mathematical proofs -- I typically don't go through this much effort (though, I surely will start to!) but often find myself rewriting things in simpler forms once I realize they're the same, which is essentially the same as realizing that two statements are equivalent in a mathematical setting. It's interesting to see that we can go further and actually prove things about Haskell programs. I've seen this done before in small bits but a well-written article about it is very nice to have around. Thanks for the writeup!
Great article, thanks. What exactly are the features of Haskell that enable this equational reasoning? Purity? Does `unsafePerformIO` break equational reasoning? Non-strictness? What kind of equational reasoning can one do in a strict language? Unrelated and vaguer thought: the way you've expressed the properties of `replicateM_` and so on look an awful lot like Haskell itself, but with "improper" expressions on the left-hand-sides. From an equational perspective, what restrictions does Haskell make to the LHS of equations, and why? I recall something called n+k patterns? What happens if you generalize this even more to allow anything on the LHS and RHS?
That does sound quite similar to my first experience with that library. I was also learning OpenGL from C++ tuts, so I was doing some serious doc surfing to figure out how it all worked. I plan on doing some more OpenGL/C++ stuff in the next few months, so I guess how I'll see how I feel about the high level Haskell library after I'm a bit more cozy with OpenGL in general.
you want a windows setup, on debian? what
I wrote a [blog post](http://liamoc.net/posts/2013-11-13-imperativereasoning.html) that more formally expounds the same subject.
I've just created an account to try it out. I'm failing to understand many things. * [One thing](https://www.fpcomplete.com/school/using-fphc/fp-haskell-center-settings-editor/fp-haskell-center-settings-page#extra-packages) tells me that settings are stored in a `project-settings.yml` file. [Another thing](https://www.fpcomplete.com/school/to-infinity-and-beyond/project-settings-file) tells me that they are stored in a `.project-settings.yaml` file. * How do I create such a file? I see nothing in the UI for "New File". In fact, I can't even see how to create a new `.hs` file. * I think I'm not meant to edit the `project-settings.yml` file, but use the "Settings page" for it. It took me a while to find it: it's those three bars in the top right. Strangely, that button opens a tab on the left, which is where I was looking for it in the first place. * In the "Settings" page, I go to "Extra Hackage packages" and I'm given a form with three fields: "Extra packages", "Name of this package", "Version of this package". Name of *what* package? The package that happens to be "missing" in my case is `scotty`, so I put that in the first and second fields. * I don't understand why the IDE can't just get all this info from my `.cabal` file, since that's exactly what it's for. * I then couldn't work out how to save my settings. Do I have to commit them? Surely not, as it asks me to pay for that, and surely I don't have to pay to just save files. After reloading the page, I concluded that it saves things when I type them. * I then couldn't work out how to recompile to test whether it's fixed it. Turns out it recompiles when you change files. Does this include the hidden `.yml` file? It's not clear. Is the error message in the "messages" console from before I edited the settings, or after that, implying that my settings changes didn't help? * After changing another `.hs` file, it briefly whirrs in the top-left, then goes back to the previous failed state, saying, as before, "Could not find module `Web.Scotty' Use -v to see a list of the files searched for." * I tried adding `-v` to the compiler flags in the settings, but that didn't make any difference. This isn't meant to sound negative, it's just documenting why I've quickly gone back to using my own machine!
The type system is such that you can pretty nearly universally reorder and lift expressions. For instance, all of these work they way you'd think they would. apply f x = f x rev x f = f x forall f x . f x == apply f x == rev x f forall e1 e2 . e1 e2 == let x = e1 in x e2 These let us pretty much rearrange code however we like and also give really powerful inclining capability, two of the major features involved in equational reasoning.
Excellent post. Just wanted to say thanks in a more substantial way than an upvote :).
You're welcome! :)
the canonical representation of programs should not be text, but rich &gt; data structures: Abstract syntax trees Why not both? … [Maya](http://en.wikipedia.org/wiki/Autodesk_Maya) style: Everything you do in the visual interface is a command executed in the command-line interface, and vice versa. And in fact, most of the UI is actually made of CLI code too. So doing a few things with the mouse, then opening the CLI, selecting the stuff, dragging it to the shelf (icon bar) to become a button, and then going to the menu and editing that button like a script, to add control flow, widget panels, etc, is really easy and feels very elegant and sane. How a shell for professionals should be. Forcefully merging it into yet another marginally different language nobody will learn, instead of it just being two views of the same code, seems like a idea that’s neither really good nor really bad, as it removes the advantages of a view that is 100% text or $newParadigm, and isn’t really having both at the same time either. So one loses something. In other words: At least for me, the benefits are rather marginal, and I seem to definitely lose something. (Also that something might also be marginal.) So why would I use it? Not that it isn’t a great direction to go in. Just: [Dare to *go further*. ***MUCH further***. :\)](http://www.titaniumteddybear.net/wp-content/uploads/2010/07/moar-because.jpg)
I wouldn't worry too much about the project settings file. To quote the second link you provided: &gt; Most of the time, you'll never see this file, since the IDE provides access to its contents via a GUI. You can view the file by pushing your project to a remote Git repository and looking at the contents there. In other words, when using the IDE, you *only* change settings via the settings GUI, not the file. The file format is documented only for the case of a merge conflict, or when viewing the file after pushing to an external Git repo. The name and version fields on the extra packages page should be clarified a bit. It just means the name/version which would be used for taking local snapshots that could be installed in other projects. And there are very good reasons to not use a cabal file for these kinds of things. The simplest reason is that a cabal file doesn't provide the functionality of installing from a Git repo or a local snapshot. Reddit probably isn't the easiest way for you to get support. Our support team is very good at providing responses to questions posed in our feedback widget.
I implemented both versions, so it's from experience. Consider the case of computing `x+y` by induction on `x`. If we're using binary, then this will have a recursion depth of `log_2 x`; whereas if we're using decimal, it'll have a recursion depth of `log_10 x`. GHC doesn't tail-call optimize instance resolution, therefore these are the actual recursion depths. So whenever `x` is "large" you're going to overflow the stack-depth limit for instance resolution. With binary this happens very quickly, whereas with decimal you can handle moderately large numbers with ease. Yes, you can always pass a flag to adjust the stack-depth limit. But that's annoying and you'd have to keep adjusting the limit as you scale your program. And yes, they only differ by a constant factor, so in principle it shouldn't matter. But in practice the binary version is just too annoying to work with. This is why I implemented the decimal version in the first place. The fact that it makes the code and error message prettier is just a side-effect.
This is fucking great. I love it. My personal intuitions when programming are always structured visually around the abstract syntax tree. My intelligence is far stronger visually, spatially, and diagrammatically than it is lexically. For people like me this is very intuitive. I will help work on this for sure.
Figuring out the very first moment when you can return an answer is exactly the point of `LVar`s, and it's exactly why a lattice is required. Have a look at [Lindsey's overview](http://youtu.be/8dFO5Ir0xqY) of `LVar`s.
Thanks - good call. Good job I almost never use "wont" as my usage was wrong. [Edit - stray apostrophe]
My only regrets about this is that I didn't follow through with my own ideas in structured editing. I was thinking around much the same lines and I can't wait to see how far this project goes. It seems to me however that a more useful strategy would be to have separate projects for the editor and the programming language, as surely every text format that exhibits a formal structure would benefit from advanced structural editing. As a language, Lamdu looks intriguing with open type unions and conjunctions that I've always wanted to have.
You should add the standard disclaimers, like this is unreleased software, etc., etc. That said, this is a great idea, to get some more coverage before the release is branched. Are you in contact with the release managers?
&gt; Reddit probably isn't the easiest way for you to get support. Our support team is very good at providing responses to questions posed in our feedback widget. What /u/Jameshfisher documented here is probably more of a usability report than a support request. He had difficulty discovering the "Settings" UI and the format for the "Extra Hackage packages" pane. The GHC error message is misleading, as the "-v" options does not apply. &gt;&gt; This isn't meant to sound negative, it's just documenting why I've quickly gone back to using my own machine! Well, it seems that a potential customer has been scared away by poor useability. To be frank, UI design is hard, and I tend to agree that the the FP Center IDE does not do a good job at this point in time.
No harm done :)
We're really hoping for contributors, if you want to contribute, contact us. 
Working with data structures rather than text is precisely what separates an IDE like visual studio from all the crap out there. I'm all for any thing that makes it easier for extensions to manipulate and understand code, things like [Roslyn] ( http://msdn.microsoft.com/en-us/vstudio/roslyn.aspx)
Very cool. I would kill to have RankN, even Rank2, in Elm.
This looks really interesting! Is there anything I can help out with?
Hmm, fair enough. I made one myself, with a bit of inspiration from jinja2: http://hackage.haskell.org/package/hpaco-lib (there's a CLI front-end in http://hackage.haskell.org/package/hpaco, which can be used as a standalone template interpreter/compiler). The pressing need for it disappeared as I switched jobs, but if you find it useful, let me know and who knows I might start working on it again. I'm sure there are similar packages out there though, just with different stylistic choices.
This sounds like an attempt to make publishers convert from LaTeX to Word, if you get the parallel. I'm not saying it's necessarily a bad idea, anything new is good, but I'm just a bit skeptical. Most of the assumed benefits of the new approach, sound like something ReSharper would do, if they choose to support Haskell some day. 
I've implemented several rudimentary structure editors over the years, but so far I haven't been able to implement something great. There are a lot of little things that make it hard to get a fluid experience. I hope this project will be succesful, but I've become convinced that to get the most out of this you have to design the language for structural editing from the ground up (the opposite of what picpic is saying with a seperate project for the programming language). I think one thing that may finally tip the balance is touch screens. They work very poorly with flat text, but they work great with a structural editor.
&gt; Remember, a monad is an object where you have defined a meaningful way to chain functions on that object. A monad is... well, I don't want to say what a monad *is*, but this definitely isn't what it is. Where do return and bind fit into it? What about polymorphism?
There was not enough context for me to tell if the author meant object as in an oop object or if the author meant something different or more abstract.
&gt;mzero &gt;&gt; return x = return x. That should be: mzero &gt;&gt; return x = mzero
I think it still doesn’t mean what you want it to. “The website seems wont to show me screenshots” means “The website *does* usually show screenshots.” If I’m right in understanding you as meaning that it *doesn’t* show them, then you want “The website won’t show me any screenshots”, or maybe “The website seems loth to show me any screenshots”.
Och - "won't" in the reply was a typo - "wont" in the first was a mistake. For some reason I'm *wont* to add extraneous apostrophes when typing which I seem to miss when reviewing. 
Doesn't that boil down to not having the interesting key bindings set up properly?
I was Haskelling for quite a while before working at ISC for about half a year, so I think I've got a good understanding of the differences: While many of their goals are the same, the overall philosophy is very different. In particular, the Intentional Workbench is focused on the projectional editing and construction of domain specific languages. It has very few assumptions about the languages that you construct. This flexibility is powerful, yet leads to some inconveniences. For example, this means that if you want a typesystem (referred to as a "verification", which is really any abstract interpretation), you need to be prepared to roll your own for each language. If you want these verifications to work across languages, then the verification is going to need to be aware of both languages. My overall takeaway is that the goal of allowing any language semantics is in fundamental opposition with the goal of having sublanguages interoperate. I think Lamdu is on the right track to avoid these pitfalls - due to using a powerful core semantics that we all love and trust - typed lambda calculus. While the Lamdu page doesn't seem to talk about domain specific languages very much, this is nice for them for numerous reasons: * It forces all of your embedded languages to have some degree of reason-ability due to referential transparency. * It allows some degree of interoperability without knowing about eachother, due to sharing the same typesystem. * You don't have to go about re-expressing the rules that are common to many languages, such as variable bindings and function application.
&gt; or a Haskell-like Given that you mention your approach would be well-suited for Haskell-like languages, I feel obliged to ask the question: can you estimate how much work it would take to port the editor to, say, [agda](http://wiki.portal.chalmers.se/agda/pmwiki.php) or [idris](http://www.idris-lang.org/)? With support for [interactive editing](http://wiki.portal.chalmers.se/agda/agda.php?n=Main.QuickGuideToEditingTypeCheckingAndCompilingAgdaCode) (which btw should [land in ghc](https://ghc.haskell.org/trac/ghc/wiki/Holes) at some point)?
I wondered about that...
Well, currently we have a partial/preliminary implementation of type inference for our own little language which is very incomplete. We'd love to use Agda or Idris instead, but we want to work purely with ASTs, no text. Also an API, and not a protocol to another process. As well as explicit type variable parameters/instantiation (collapased at the UI level), row types and polymorphic variants. 
Thanks all :) I decided to completely remove any claim about what a monad is; I assumed that people understood monads already so there is no need to inject anything.
I can't seem to install it. I deleted my .cabal and .ghc and ran cabal install but I get: &gt; &gt; cabal: Error: some packages failed to install: &gt; derive-2.5.13 depends on haskell-src-exts-1.14.0 which failed to install. &gt; haskell-src-exts-1.14.0 failed during the configure step. The exception was: &gt; ExitFailure 1 &gt; lamdu-0.1 depends on haskell-src-exts-1.14.0 which failed to install.
Yes! We could use a lot of help, especially with type theory experts, but not only. Please message if you're interested in contributing, we'd love to have more contributors.
Another benefit of equational reasoning in Haskell is that often you can use it as a proxy for reasoning about the time or space complexity of running a program. My favorite example is showing that the following definition of `find` runs in constant space (contrary to the expectation of many newcomers who assume that `foldr` = linear space): find :: (a -&gt; Bool) -&gt; [a] -&gt; Maybe a find p = foldr go Nothing where go x r = if p x then Just x else r An illustrative example: find (==99) [1..] == foldr (\x r -&gt; if x == 99 then Just x else r) Nothing ([1..]) == foldr (\x r -&gt; if x == 99 then Just x else r) Nothing (1:[2..]) == if 1 == 99 then Just 1 else (foldr (\x r -&gt; if x == 99 then Just x else r) Nothing [2..]) == if False then Just 1 else (foldr (\x r -&gt; if x == 99 then Just x else r) Nothing [2..]) == foldr (\x r -&gt; if x == 99 then Just x else r) Nothing [2..] -- … == foldr (\x r -&gt; if x == 99 then Just x else r) Nothing 99:[100..] == if 99 == 99 then Just 99 else (foldr (\x r -&gt; if x == 99 then Just x else r) Nothing [100..]) An asymptotic upper bound on the maximum number of syntactic tokens that will occur in an evaluation step gives you also an upper bound on the memory usage of the algorithm. In this case, there are constant bounds on the token count of the expressions, therefore it's a constant-space algorithm...
Bind is the "chaining", and return is the identity of said chaining. Insert much handwaving here, but it's kinda sorta right if you squint hard enough.
Isn't category theory defined as "objects and arrows"? And a monad is just the category of types (or so I read somewhere). 
&gt; What happens if you generalize this even more to allow anything on the LHS and RHS? You get a general logic language, which is extremely inefficient. If you restrict it to horn clauses, you get prolog.
thanks! that's embarrassing...it sort of contradicted the sentence literally directly before it.
&gt; Isn't category theory defined as "objects and arrows"? Yes. Well, it is often defined *in terms of* objects and arrows. The definition posed above is not accurate for category theory objects or OOP objects or any other objects. &gt; And a monad is just the category of types (or so I read somewhere). No, or at least I can't think of a way to make that an accurate definition of monad.
Could you repost this? I'd love to have some more tailored discussion.
Our main difficulty at the moment is getting an incremental type checking &amp; inference system working correctly and quickly. Lamdu's master branch has a slightly buggy system. new_infer_integration branch has a new system written from scratch that has no known bugs, but is much slower in important scenarios, and has a bit less functionality. Both don't yet support sum types &amp; pattern-matching. This has been more difficult to implement than we anticipated. Why use our own type system, and not GHC's, Idris's, Agda's or Ermine's? Basically we wanted the following set of features: * As simple a type system as possible. i.e: we'd prefer a DT system to GHC's many DT-approximating extensions. * Explicit type variables (lambdas and applications) that the UI hides. It means the type system is simpler and less magical. * Row types and structural records. We use records extensively, in lieu of currying, to allow keyword arguments. Partial application is made concise and easy to read/write by hiding ETA expanded forms in the UI. We thus want a good record system. * Polymorphic variants. We want to be able to have precise types everywhere. We think having structural records alongside anonymous sum types (with named constructors) for subexpressions will make it easier to have intermediate precise subexpression types. In Haskell, for example, one is unlikely to define a one-off record/sum type just for some intermediate expressions. Thus, Haskell programs are more likely to use ugly Either/(,) compositions and/or partiality for those cases. * The type system needs to be able to resume inference after filling of some holes. i.e: Type-check expression with holes, then try various resumption values in the hole where the cursor is to figure out which is valid and what eventual types they will have. Preferably without duplicating inference work. * Purity, of course. We don't think any existing type system combines these features, and we wanted the pedagogical experience, so we started implementing our own type system. We've rewritten it twice since, but we now understand there's much expertise gathered in the field that we could use. If you're willing to lend a hand, advice or even implementation work, we'd love for you to contribute :-)
What I meant is that if you have 3 threads and they come back True, True, and False, then after the first two you'll have `TrueTrue` and `getResult` will decide we're done and give you the wrong answer before the third thread finishes. I think we're saying the same thing but in different words.
I'm not sure where I should post it. I'm not talking specifically about Haskell here, in fact the calculus I use is more restricted than haskell in many ways. If you want to post it somewhere though, I'm happy to participate in discussion.
It does allow for a lot of neat refactoring tools, as well as syntax aware navigation (ie navigate to all references, navigate to definition). It even builds in a lot of tools to analyze the code, and tell you about cyclic complexity and dependency, and rates the code based on that. It also has a code analysis where you can apply rules to the "style" of the code, such as forbidding Hungarian notation, forbidding short forms for variables names etc. This allows you to get most of the benefits described by Lamdu (refactoring is safe, formatting is automatic so no merging of different format, incremental errors, no syntax errors, all that stuff). If those screenshots are correct, and my assumptions are correct, it's turning haskell into a visual editor? Similar to the many drag and drop editors (which also let you type in values to fill in the blanks). Or is that just formatting that appears after you finish typing? Most of the benefits of it come from the first few, and it's from having the editor be aware of the syntax. There are more and more features that it could implement with this, but I think it's more worth to focus on giving this AST to extensions, so extensions can implement the cool features they think of. 
My work here is done.
"Chain" is a bit heavy, I think, but the idea of associating functions to some value isn't bad. It'll get you Cont.
If anybody is looking into playing around with different installs of GHC on the same system, my [`ghc-version`](https://github.com/duairc/ghc-version) script may be of interest.
Any idea why did haskell-src-exts fail to install? And which version of GHC are you using?
Author here. I'm by no means an expert on anything I've written about here, so I would appreciate any corrections.
You convinced me! :-)
Not really a comment on the article, but the blog in general. Why did you go with IntenseDebate instead of say Disqus for comments?
Quesrion: So the the main point of the comparisons between the imperative vs pure sum total implementations is that in the pure case we can use proof by induction? Is the IO monad considered an embedded language as argued later in the article? It would be interesting to see the proof-based approach compared to the empirical unit testing based approach for determining correctness.
No argument that the term grammar is very nice. But the 14 space--separated ungrouped symbols adds up to an eyeful. I mean, when LaTex or Unicode is available, authors often choose to fancy symbols to replace the trigraphs. One could say that at least the "=" is obvious, but "=" is only special when it stands alone as a unigraph. Haskell treats most punctuation almost like alphanumerics, except when it doesn't (parents for tuples vs grouping) -- but to give Haskell credit, most "special cases" can be thought of just "keywords" (`| = \ -&gt; ::` are all keywords like `do let in`) that are made up of these characters that just happen to be "punctuation/symbol marks" in English. A rich display layer could add a lot of benefit, to give subtle hints as to which punctuation marks are combined unto single names, and to guide the eye to operator precedence. (Like subtle text color fading in order of precedence).
The team has been very response to bug/feature requests reported via the Feedback widget in the tool. It is almost embarrassing that they provide so much attention to tire-kickers like me. But I don't 'demand' support, and I tell myself that I speak for the future potential customers who will open their wallets when they see the slick UX debugged with help by my feedback.
On any fpcomplete page, click the hamburger icon and click "Feedback". It is easy and will help the product get fixed for future people like you. The trouble you described is legitimate, but was far less expensive than the cost (to me anyway) of debugging one cabal install version conflict and the plain boredom and CPU of running ` cabal install `. 
Maybe because cryptic comments like this don't really educate people about haste.
GHC doc is good but more concise and technical. http://www.haskell.org/ghc/docs/7.4.1/html/users_guide/ghc-language-features.html FP Complete docs are more accessible to dullards like mem
&gt; which punctuation marks are combined unto single names Actually that is fairly easy: any run of symbol characters. Precedence is always an acquired skill, specially with extensible operators though. Here it associates as every f (a,b,c) = (((,,) &lt;$&gt; f a) &lt;*&gt; f b) &lt;*&gt; f c and all of those operators have the same fixity and bias left.
&gt; Correct me, if I am wrong, but if you are capable of proving termination of any program in your programming language, have you not already left the real world again Certainly. But showing that evaluation terminates doesn't mean you can't have a turing-complete monad that is executed separately -- much like Haskell marshalls side effects into the IO language rather than the evaluation (modulo `unsafePerformIO` etc.), you could have a "Turing" monad that adds a `fixM` construct allowing unlimited recursion, but _evaluation_ is still strongly normalising. It's a conceit, obviously -- you'd still have to prove that your `Turing` programs terminate painfully and individually, but for all programs that do not need to be expressed in this language, you still get your free termination proofs.
Disqus was inserting ads into my blog, and it loaded slower.
Not so much that we can use proof by induction (that arises due to inductive definitions of data types) but that we can reason about correctness simply by reduction and algebraic reasoning. We don't need to build a machine and prove temporal properties about it, we can simply manipulate some expressions in a way that is familiar to any high school math student. Yes, the IO monad is such an embedded language, although Haskell's "pure" evaluation is much more permissive (wrt. partiality) than what I was using.
Isn't `e { x = t , y = u } = e { x = t }{ y = u }`? Could you explain why you can't generalize multiple record updates?
Total languages **are** typically turing complete, despite what most people claim. All you really need is primitive recursion on natural numbers (bounded iteration), which is terminating. If you want to code up a function which may not terminate, you can do so by giving it a natural number argument which says how many steps to execute it for before giving up. Now you can execute your program by giving it the number of steps you're willing to wait. If it gives up (times out), try again with a larger number. Repeat as necessary. This isn't a "cheat". In the end this isn't any different from "execute one step of the turing machine. Did it terminate? No? Okay, execute another step...". Although yes, total functional languages are not turing-complete in exactly the same way as the untyped lambda calculus. The rules of the game change a bit, but so what? There are also more sophisticated (i.e. modular, efficient) ways of embedding general purpose programs in total languages (e.g. kamatsu mentions a partiality monad, see also codata and coinduction), but this is the general idea. The point is, you can't dismiss total programming languages on the grounds that they're "not expressive enough" -- that is simply false. In fact they are **more** expressive in a sense -- they can express both total and non-total programs (in different ways), and furthermore they allow you express the distinction!
&gt; click the hamburger icon and click "Feedback" (Isn't this supposed to be a speech bubble, not a hamburger?)
I frequently feel conflicted but the extensions I use. Should I try to use as little of them as possible, or take full advantage of all of them? Hm, decisions... Many of them like FlexibleContexts feel like they should've been there in the first place, things that just seem like they should work. Others like RankNTypes are kinda indispensable when you want what they offer, no obvious workaround. I frequently use syntactic sugar like LambdaCase, RecordWildcards, MultiWayIf, BangPatterns, OverloadedStrings, TupleSections, ViewPatterns etc. They make the code a lot more compact and elegant (IMHO), and could always be replaced by more verbose code if the extension becomes unavailable or conflicts with something else. I rarely use things like GADTs, FunDeps, Type Families, etc. Haven't fully wrapped my head around some of the more advanced type system features, don't seem to stumble into use cases often enough (or just don't recognize them). In the end it seems that my code is way more likely to be broken by library changes than deprecated / changed extensions. The syntactic sugar ones are easily replaced by standard code, and others like RankNTypes are unlikely to go away. Maybe the biggest argument against using many extensions is that each one increases the burden on both humans and tools trying to parse the code.
What do you mean by "data table/formula based charting library"? The links referenced don't seem to answer this.
This is sort of like how codata works. In a language with a distinction between data and codata, there are codata types which represent processes which may not terminate, but which only step on command, sort of like it's thunked, so that your execution system for problems that really do need to be non-terminating can sit there and twiddle on the codata until it finishes, if ever.
I know this, but thanks for elaborating :)
You can also feed partiality monad into a non-terminating RTS and it'll execute as a Turing complete program, even if the modeling discipline was total.
I am sure there are many great things you gain by reducing the expression-strength of a language. I am also aware that many models are based on this principle - and I don't want to dismiss this - this is also done for imperative programming (PLTL + omega-words, csp, .. ). I only found the given comparison one-sided. And on the whole "total languages are typically turing complete"-part: huh? What kind of argument is that? You can prove termination -&gt; they are not Turing-complete. Like I said, I am sure you can still do great stuff with it and you gain cool properties that you can use to build stronger type-systems or provide better methods to prove stuff, but that changes nothing about this basic property. And if your are able to embed stuff inside this "runtime-system", that you programmed using total programming-languages, then you can only prove these gained properties about your runtime-system. I don't think there is much ground to argue about this. This is also clearly stated in this article/paper.
Since no one else has mentioned it, I'll go ahead and say it: You don't want to allow arbitrary nontermination anyway. The only extra thing it allows over a total language is getting stuck in an infinite loop. All correct calculations will terminate anyway. The same way type systems prevent classes of incorrect programs, totality checking prevents other classes of incorrect programs. The typical objection people raise at this point is "What about servers, or games, or other interactive things that depend on user input?" And the answer is that IO isn't a calculation anyway. It's something else, and isn't bounded by the same constraints as calculations are. Sure, you can't prove IO is terminating - but you can prove that the work done by your program in response to any IO prompt *is* terminating. This view even carries to things like realtime games - timer ticks are IO prompts, of their sort. So throw out nontermination. It's just a source of bugs.
It seems that an equivalent model in haskell would be a list of values of some type, and a suite a accessor functions to get at the attributes of those values. Given these, then plotting becomes straightforward using list comprehensions: plot_points_values .~ [ (attr1 v, attr2 v) | v &lt;- data ] But after Ollie's post, I guess you know this...
ugh, I don't know what you want me to say. I already told you that I agree with you, that you can do great stuff with this programming-model. There is really no point in discussion about the rest. A program is Turing-complete if it can simulate the universal Turing-machine or compute the exact same class of functions that can be computed by said universal Turing-machine (this is what wikipedia states and also what I learned in university and read in books back then). What different definition is there? You can show that it is not possible to prove termination for any of these programs - that in fact it is a contradiction for this class of programs (i.e. the general halting problem). You can show this property for a program that you wrote or any program that you can encode in a given programming language, then it is not Turing-machine and thus not as expressive. It is also rather simple to show this: you can't write programs that never terminate in your model. This might sound stupid to you, but you started this argument and it is simply false - like you said so nicely. I don't see this going anywhere.
I've got the beginnings of a haskell web app for community generated micro-reviews of science papers at [reffit.com](http://reffit.com) . It takes ideas from StackOverflow and Reddit (reputation, stream of posts, tags, comment sorting by voting, followers, anyone can register to write, read even without registration), and tries to focus on building a single-page overview of a paper's good and bad points, from an insider's perspective but in language that anyone could understand enough to evaluate. The idea is very near to my heart - a means to bridge the gap between experts' opinions on new research papers in science and what the public hears about those papers. More about the rationale later, if anyone asks. I'm posting here hoping that someone is interested in co-ownership of the idea and moving forward with the implementation. Doing it right, alone, is taking too long, and I'd really like to see the site be sturdy enough that we could pitch it to scientists and build up a little community. With some code cleanup it might also be a nice example for people learning web programming in Haskell. I would love to blog about the process, except for the sinking feeling that I'm doing things in a way that really shouldn't be imitated :) I'm kind of new to this (and I have to point out how amazing the web ecosystem in Haskell is - that without knowing what an HTTP request is a month ago, I've been able to cobble together what I have so far. Unbelievable libraries, documentation, and blogs). Site is built on Snap and acid-state, with bits of copy-pasted jquery and css here and there. The site is built up enough for you to sign up an play around posting comments. Any guidance re: direction or code are appreciated. If you think post-publication peer review and Snap programming are interesting please drop me a line! 
This introduction belongs in your `README.md`
I can only see a contradiction there if you say that running IO is something that can be done within the total portion. But since this was about *pure* total programming, you can't run IO within the constraints of a pure total program. You have to return a description of the IO you want to run, instead. This is almost exactly how a total variant of Haskell would work. Model IO as codata, just enforce totality between IO constructors. It does let you work really hard to encode infinite loops, but it still gets rid of many of them. And it gives you a giant class of expressions for which you can be 100% certain you don't have infinite loops. This is the same benefit you get from Haskell's type system - you get giant classes of expressions for which you are certain you don't have certain bugs. How is that not something good for the real world?
Thanks for open-sourcing this. The more Snap projects that get open-sourced the better. I haven't hand a chance to look at the code yet but I will! Thanks again!
&gt; A programming language is Turing-complete if it can simulate the universal Turing-machine or compute the exact same class of functions that can be computed by said universal Turing-machine Intuitively, this is the definition. There is some imprecision around precisely what we mean by "computes". You can say, as you do for the untyped lambda calculus "Here's my reduction relation, and we say that a term M computes a function f if, for every value x, M x reduces to f(x)". We can also choose to say that a program M of type Nat -&gt; A -&gt; B in a total programming language computes a function f : A -&gt; B if for any value x, there exists a natural number n such that M n x reduces to f(x). Here we have modelled possibly non-terminating computation by specifying an upper bound on how long we're willing to wait. We recover the unbounded execution behaviour by asking only that the exists some time n by which the program computes the result. If this is our notion of "computes", then our total programming language is Turing-complete.
And what do you do if your program terminates 1 round before it would find the correct and exact solution, but a tm or tm-equivalent would have found the solution? And what if you find a problem that does exactly this for every N you specify - maybe even by problem-definition, because it is possible? Like I said before, if you are able to compute any problem that you want to solve withing your model, do it. That's what it's for. You don't gain anything by changing definitions. /me is off, have fun
&gt;It's a conceit, obviously -- you'd still have to prove that your Turing programs terminate painfully and individually, Not even that, it'd be the equivalent of proving that your IO programs are pure to run them. Instead, if you'd just let main to be of type Turing a, and hope for the best when executing it.
Good point - done.
Yep, demo is at reffit.com . I've been going back and forth on threaded discussions. I thought maybe reddit itself is a better forum for actual discussions, reffit could link to a reddit thread.. Do you have a preference?
That's the first small bit, but I implore you to play with R. Take a look at how lattice charts works. It's certainly possible to emulate, but without the nice interface it's not half as fun. Nor ever going to replace R. My current workflow is to dump that list comprehension to CSV and import to R (though I may try that quasiquoter next time).
Huh? When is it ever a good decision to make language choices out of loyalty? A native speaker is never wrong. Do what comes naturally, and you're helping to form a cultural consensus.
I prefer threaded. I think it's the only way to have a productive discussion no matter what the subject/site. Anything else is too hard to get caught up when coming in late. Otherwise you get pages and pages of quotes, "this", "/thread" and "no u".
Taking a program from IO is only nonterminating if your program accepts nonterminating programs, which a total language does not.
PS - if anyone wants to play around on the site without making up a username and password, login w/ username: test password: test. 
Sweet, I'm all ears! What should go into it? What does a full-blown alternative to journals look like? Either here or shoot me an email (my address is on the github page).
Some extensions are internally logically inconsistent. For those, loyalty (to type-safe, referential transparency) makes sense.
If you click Feedback in the page banner, you get a little form that references the current URL. Consider pasting this comment into feedback, maybe it will get routed somewhere that can get the page updated. 
The key to a site like this is not the technical implementation details, but the user base. Do you have a plan on this front? If not, I suggest an initial target of Haskell papers. That audience is already interested in your project from a technical perspective, so it might help build traction. One thing I would particularly like is a list of concepts, classic papers that introduced them, and their historical importance. (haskell.org has a good list of papers without annotations you could use to seed your database.) Then, you could move into more general PL and CS theory, then more broader science.
I assume what c_wraith meant is that any IO program can be decomposed into two coroutines: one which performs the actual IO, whereas the other performs the computations on whatever pure value the IO results in. Then, we can prove that each entry point in the pure coroutine will call back to the IO coroutine after a finite length of time. That is, every path through the pure coroutine is terminating. Sure, a program could run forever just waiting for keyboard input or something, but that's uninteresting.
Just because a Turing machine can go into an infinite loop does not mean that this particular TM "computes a function" in any particular class of functions. Indeed, it is perfectly sensible to say this TM does *not* compute a function. Given some class of functions `F`, and some space of machines `M`, we say that `M` is `F`-complete if and only if for every `f` in `F` there exists some `m` in `M` such that `m` computes `f`. Notably this implication does not go the other way! We have no guarantee that for every `m` there exists some `f` which it computes! If we say that non-terminating machines do not compute functions, then the space of all TMs is far larger than the class of Turing-computable functions. Therefore, so long as we can cover all the Turing-computable functions with some other formalism, who cares whether we cover all the TMs?
how about the irc channel? i'm augur.
Not sure what you mean by 'loyalty'? I was thinking along the lines of, how likely is it that an extension will be deprecated entirely, replaced by a better way of doing things, will be in conflict with another extension in the future etc. and how hard it will be to update the code if and when that happens. I also think readability by humans &amp; tools is a real concern. Even popular tools like hlint seem to be confused by some language extensions, and if you have a codebase that uses a lot of them beginners will have a harder time contributing. Not saying that you shouldn't use extensions, just thinking out loud about the pros and cons, wondering how others see the issue.
I’m sorry, but the former is a lot easier to understand. Maybe because it doesn’t ejaculate pointless foreign words like ”catamorphisms” and pointless symbolism over me that can be replaced by normal native human words. How is that wall of formulas simple? Not that the former is much better at symbolism. “Δ”? Seriously?? I’m surprised there’s not a pseudo-letter in there that isn’t even a letter in any language at all. Why does one always need a translator to translate anything a mathematician writes into human? Have you people never heard of “**meaningful identifiers**”?? I heard it’s kind of a foundational programming concept.
I definetely think the project is a very good idea: however it seems pretty hard to bootstrap it to a decent critical mass. The discussion on papers on research *social networks* such as [ResearchGate](https://www.researchgate.net) are very poor: reddit threads are often much better. I believe the focus on papers is a good choice: do you know [BibSonomy](http://www.bibsonomy.org/)? Maybe some kind of integration might benefit both projects?
When we choose epsilon, we know by culture it is generally a small value. In fact mathematical symbol are shorter and easier to read when you are used to them. But for most people it just a big wall of meaningless symbols. This is why, using long word can be a better option when you want to talk to people not used to your specific mathematical field. While symbol are a better choice between experts. 
Indeed, and I wrote the article with said experts as my intended audience, so I didn't feel any reservation about use of symbols or jargon.
It also handles derailing well. Internet discussions tend to digress into completely different topics, it's good to be able to efficiently ignore a whole line of discussion that you're uninterested in.
^__[Verified]__: ^/u/t3traa ^^[[stats]](http://www.reddit.com/r/so_doge_tip/wiki/stats_t3traa) ^-&gt; ^/u/imalsogreg ^^[[stats]](http://www.reddit.com/r/so_doge_tip/wiki/stats_imalsogreg) __^Ð5 ^Doges__&amp;nbsp;^__($0.002)__ ^[[help]](http://www.reddit.com/r/so_doge_tip/wiki/index) ^[[stats]](http://www.reddit.com/r/so_doge_tip/wiki/stats) 
I'd just like to mention a different (and, at a glance, more complete) semantics: [Python: The Full Monty](http://cs.brown.edu/~sk/Publications/Papers/Published/pmmwplck-python-full-monty/) (OOPSLA 2013).
I don't have any money to give out. My plan is for normal use of the site to be free and without ads. A pie-in-the-sky idea is that, if the site becomes very popular, I could set up a system for search committees to commission reffit reviews of specific papers from users, and that might be a reasonable place to take a cut. Or I might look for funding routes like an NSF grant, or selling swag. But any such thinking might be premature until I can convince myself that people will register and use the site in the first place :) *If* the site made money, then yes it would be split fairly among the folks working on it.
I think temporal logic is the phrase you want
It's all too easy when replying with an elaboration to make it seem like you're trying to teach people to suck eggs. 
I tried using fromMaybe, but I ended up with a stack overflow. Not really sure what the difference is from just using regular pattern matching.
That's a great idea. I hadn't put it together that I could grease the tracks with users who are also interested in the project. My plan for generating content is to go to labs around my department and offer to do journal clubs on new papers with them, they have their laptops out and we're posting our comments to reffit as we go, then encourage them to continue that way in their next journal club meetings. The list of classic CS papers in their contexts sounds nice - a little more wiki-ish than what I had in mind - but maybe easy to bring something like this in as a "reading list" (like a playlist on youtube). Clicking a field tag could bring you to a field-summary page that lists the most popular reading lists, most popular papers, and most active users for that field tag. But I think you're right that on the point that implementation details (and little features, by extension) are much less important than the user base itself. Do you think that the crux of the issue of getting a user to sign up and write is for them to see the papers they're interested in when they first visit the site? If you saw a listing of 10 famous old haskell papers with one or two comments each, would that be enough enticement to make an account, add comments, and post your own papers?
Really? That's lame. I never noticed that, though I use adblock, so I don't really see much to any.
Strictness
What's the plan for type classes / canonical structures?
Happened to me also. They wanted me to write a book on LLVM.
Ah yes, adding a call to `seq` fixed the problem.
It is a speech bubble. But I prefer cultic's interpretation of a hamburger. =p
I'm fairly new to all this, and don't have a strong background in post high school maths, just what I've picked up from reading reddit. I read most of that "wall of formulas" by expanding out each symbol into what I've learned it represents. I don't think in symbols and I can't read the formula's as quickly as normal prose, but I can still work over it and see what it's saying. To me the difference between the symbolic representation and the same thing written in full prose is all the extra, very precise, information conveyed by the symbols and how concisely it conveys meaning. It allows me to jump around a line of symbols and work out what is going on much more easily that the same information expressed in 10-20 lines of prose.
Very nice, and very clear. Since you are clearly talented at describing different kinds of semantics, could you please make an improvement to the [wikipedia page](https://en.wikipedia.org/wiki/Semantics_(computer_science\))? In particular, the "Approaches" section needs help. The definition of "Operational semantics" is particularly bad: "whereby the execution of the language is described directly (rather than by translation)."
&gt; But that only works, if you have a very special mind that can think in symbolism instead of e.g. concepts or words. Which is exactly, why normal people have such a hard time with what they call “math”. Hm. [I guess Haskell isn’t for you?](http://www.reddit.com/r/haskell/comments/1tb9qa/haskell_for_all_lift_error_handling_with_lenslike/ce6ap5x) &gt; Haskell is not for all! &gt; Because many refuse to learn, to think and to understand. They want things to be mashed and spoon-fed to them, are the loudest screamers when they aren’t, and see asking them to turn on their brains as an “insult”.
Anyone know what language the code samples in the book are implemented in?
Good idea. I'm not sure if cabbage goes good in a burrito.
It seems like this post stopped short of having a point.
The "modeling IO as codata" bit is worth focusing on—it's a formal way of saying that so long as you don't walk away from the program and never provide it with the next bit of desired input then you can be certain the program will continue to run. Which is a pretty nice promise. I think there's a huge question there still given that you almost certainly cannot expect the above promise to be fulfilled in practice—imagine a TCP socket being dropped, you must model a timeout somehow.
Is this open source? Can we see your progress? This is something I have considered doing myself (just for fun).
This *will* be open sourced (the library and all testing and benchmarking code) as soon as I get permission to do so from my employer (yes, doing this in my own time, but thats what my employment contract says I have to do). Basically there is zero chance this will be accepted for GHC 7.8 as its nowhere near finished yet, but a very good chance it will be ready long before the freeze for GHC 7.10. 
Wonderful! It will be pretty ambitious to beat C performance though, I expect GMP to be optimized to no end. I recently asked people on IRC about the current integer-simple status, and it seemed like the project was abandoned for the most part. And now you come along, tackling this exact issue - pleasant coincidence.
Well, I don't want to encourage you to lower the bar, but if you get within 2x or 1.5x of libgmp, I'd suggest you consider that total success. If you beat it entirely, great! But... if you really could work something out in Haskell that manages to beat libgmp, I'd start asking some serious questions about libgmp. libgmp is the sort of library that is willing to drop straight to assembler, on a _per-architecture basis_... with no criticism particularly intended of Haskell, you're going to have a hard time beating that from a pure-Haskell perspective.
I wish you success. I for one find the gmp dependency annoying, and would love to see it go away even if it means some kind of performance hit.
Not only that, gmp has had a few decades of work put into it, and its author knows the domain exceedingly well. The OP's desire is laudable, but the wording sounds like he's setting himself up for a fall by vastly underestimating the domain.
Which brings up the point that even if you beat libgmp once, you'd have to replicate that on every common architecture, too!
Ah, I see you're going to use the WillCoredump monad in this code. ;)
Integer-simple was never intended to replace the GMP. The only reason for its existence is as a proof of concept to quell worries about GMP being under the GPL. Once the concept was proven, it's purpose was done so it was abandoned.
Please start with [Karatsuba multiplication](http://en.wikipedia.org/wiki/Karatsuba_algorithm). That is easy to implement, and it will make the library somewhat usable, though still far out of the league of GMP. For comparison, Python used a simple Karatsuba implementation for years before anyone complained. EDIT: IIRC the simplistic Python implementation used naive multiplication up to a rather arbitrary cutoff of 1024 bits, then switched to Karatsuba. That seemed to work reasonably well. On today's hardware you might try starting out a bit higher.
My concerns as well. Though GHC uses only a handful of the available operations, which is a small point in favour of this effort. I'd have looked at BSDish variants (OpenSSL?) for binding first.
It wouldn't surprise me if GMP has made some architectural decisions to improve &gt;Word performance, while sacrificing a bit of &lt;Word performance, as that's what it's really designed for. How is your code comparing for &gt;Word performance at the moment?
"I have discovered a truly remarkable implementation of multiple precision arithmetic which this margin is too small to contain."
That is certainly true.The whole point of GMP is to optimize the cutoff points for switching to more and more complex algorithms with better and better asymptotics for larger and larger integers, but with more and more overhead for moderate sized integers. That said, Erik has discovered that GMP seems to be suffering from some bitrot lately, so there is low-hanging fruit.
It was more than just a proof of concept and it's not abandoned. It was intended to provide a practical way to use GHC to compile statically linked binaries that are unencumbered with LGPL for the large majority of programs that do not need to do any large integer arithmetic. It still serves that purpose well. It sure would be nice to have a new library that *does* support some simple large integer arithmetic, though. There was a previous library that was only a proof of concept. It didn't actually do large integer arithmetic at all. But it helped Duncan (or was it Ian?) figure out the gory details of how to wire such a library into GHC.
I think the problem is the opposite. GHC has too many arithmetic-related primops to support because of a hardwired assumption that it is using GMP. I also think there are some built-in compensations for GMP's imperative memory model. I hope that the fact that integer-simple has already been integrated will help you with those things, though.
&gt; GMP isn't even used for Integers unless they don't fit in a machine integer I haven't really looked in detail at either of the existing implementations. My first step was to benchmark them. Then seeing that Simple was slow on multiplication, tried to figure out why. The explanation was the use of lists. 
If I'm only writing Haskell code and if its slow in comparison to GMP on one particular architecture then thats a problem that should be fixed in the GHC backend for that architecture, not in the Integer library. 
Bryan, please accept my sincerest thanks for providing us with Criterion. It makes benchmarking Haskell code so painless and makes displaying the results a by-product of collecting the data. 
For the small Integer case GMP stores them as J# Int# whereas I store them as: Small !Sign {-# UNPACK #-} !Word The differences between mine and the GMP version are: * I store an unsigned Word and a separate sign. * I make the Sign and the Word strictly evaluated. * I unpack the Word into constructor (which I think should be the same as using Int#). 
"No problem, my margin is a bit wider." -Andrew Wiles
Yeah, thats a problem with Criterion output being HTML/CSS/JS. Its designed to allow you to mouse over it and I just took a screenshot. My latest results, comparing GMP, Simple and my New library are here : http://www.mega-nerd.com/tmp/new-bench-integer.html If you mouse over the bars you will get numbers. 
Android, anyone?
Fyi, [This `ghc-devs@` posting](http://permalink.gmane.org/gmane.comp.lang.haskell.ghc.devel/2804) may be relevant/related to your efforts
Thanks that is useful. 
&gt; I assume Sign is something like Yes. &gt; Thus, your Small constructor is modelling a signed 33-bit (or 65-bit) integer? Yes. I'm on working only on amd64 for now, so I'm working with 64 bits plus a sign which is effectively 65 bits. &gt; How do you detect whether the result of an arithmetic operation still fits into your Small constructor? Precondition testing and promoting them to the Large constructor when necessary. 
&gt; However, if you want to create statically linked binaries from Haskell source code you end up with your executable statically linking libgmp which means your binary needs to be under an LGPL compatible license if you want to release it. Note that LGPL-2.1 section 6.a allows you to use static linking, while still using a proprietary license for the application that uses the lib. You just need to also provide the app in a relinkable form (ie a big .o file). Producing that relinkable form is doable with the GHC/Cabal toolchain (we've done it for one client), and it could be automated if anyone really cared.
&gt; Note that LGPL-2.1 section 6.a allows Thanks Duncan. While strictly speaking you are indeed correct, as the author of a widely used, multi-platform C library under the LGPL, I do not know of *any* person or company releasing software using my library in that way. Whenever I mentioned this as a possibility to people they've always considered it too hard. I've had a number of people threaten to write a BSD licensed replacement, but nothing has come of that afaiaa. 
&gt;&gt; How do you detect whether the result of an arithmetic operation still fits into your Small constructor? &gt; Precondition testing and promoting them to the Large constructor when necessary. Then I'm wondering why you can beat `integer-gmp`'s performance, as `integer-gmp` makes use of some overflow-testing/aware `Int#` primops which aren't available for `Word#` (yet) to avoid promoting small ints to large ints. Do you inline aggressively?
What're the big differences between GHC and hugs?
Hugs is an interpreter written in C mostly for educational purpose (though serious, big applications have been written or run with it). It supports the H 98 language standard and (possibly) a few extensions. It has inspired the GHCi mode of GHC.
I suspect Erik's benchmarks comprise of almost-trivial (from the libs' perspective) problem sizes, where GMP cannot shine due to (e.g.) invocation overhead.
a bit? a lot.
I was looking for something like this just a few weeks ago and came across an android port of hugs: https://github.com/conscell/hugs-android Unlike the iOS port that the OP mentioned, this is a just a console app and requires a terminal emulator to run (not to mention a text editor - I settled on Vim touch). It would be nice to polish this and put on the google play store....
...so you're hoping to write high-level generic code which gets compiled by a [sufficiently smart GHC](http://prog21.dadgum.com/40.html) into the best optimized machine code for all architectures? ;-) IMO, `Integer` is one of those low-level areas, where you have to get your hands dirty, and write platform specific code to get out decent performance to match what other languages already provide.
Oooh, it looks like you're doing well! :) 
Nice. It would be great if this had real-deal DT. I have been learning Coq recently and I like both its DT and its explicit type variables (the latter becomes necessary to work completely with the former). Its approach, which might be worth taking here, is to make the semantics include explicit type variables (type lambdas) but allow them to be implicit and inferred for more compact notation; as opposed to the semantics not formally including them and rather dealing with them exclusively in the type constraints and unification as Haskell does. Actually, as I understand it, the first stage of ghc does actually compile into explicit System-F(*whatever) after the type inference. I am also glad you are adding anonymous sums (coproducts).
For libraries, I personally try to use extensions as little as possible, to the point of absurdity(*). For applications, I use whatever extensions I feel are useful. (*) newtype Absurdity = Absurdity Absurdity pointOfAbsurdity = Absurdity pointOfAbsurdity
Cool - I bought a copy and it works ok. A better default keyboard would be nice, with more keys on the first page, but other than that quite nice. 
I did not know that I may be unintentionally using a LGPL component in my program. 
Thanks a lot! Should beat my current method of writing a bunch of code in Vim touch, synching to my computer at the end of the day via dropbox and fixing all the type errors I introduced during the day.
for reference, my favorite paper on multiplication tricks and hacks: http://cr.yp.to/papers/m3.pdf 
I hacked on the source when I compiled it to JS with Emscripten, it's rather hackable! Very imperative C code, but pretty easy to follow nevertheless. Undaunting, small codebase, easy and fast to compile. I like those qualities a lot. GHC by comparison is daunting and huge.
Perhaps the slowness of integer-gmp is due to some FFI overhead?
I really wish I could release the code now. My test with GMP Integer vs Int would look something like this: C.whnf (foldl1 (+)) intList C.whnf (foldl1 plusInteger) integerList where intList = fmap (take 1000 . R.randoms) R.newStdGen integerList = map (\x -&gt; G.smallInteger (unboxInt x)) intList unboxInt :: Int -&gt; Int# unboxInt (I# i) = i For this test, I found GMP's Integer 13x slower than Int. 
&gt; hoping to write high-level generic code Hell no, this is not high level code. This is Haskell code that looks like C. I'm using Data.Primitive for random access arrays and while I was getting it working I was getting all the low level C issues you could thing of; off-by-one errors, reading out of bounds, memory corruption, segfaults and extended debugging sessions. For this really low level code, yes, I am hoping that GHC can compile it into highly efficient machine code for all the architectures. 
I'm beating integer-gmp where integer-gmp isn't very good :-). I've got bang patterns all over the place and lots of tight loops in where clauses. Apart from that, I'm not doing anything special apart from writing some low level HalfWord addition, subtraction and multiplication functions that include overflow handling. I'm hoping to replace these with PrimOps that do the full Word versions of these operations. I have written 
The hamburger icon is the one that looks a bit like a Greek Xi, 3 horizontal lines. I may have named the wrong icon in my post. http://ux.stackexchange.com/questions/32877/what-is-this-side-menu-called-that-can-be-found-in-many-multi-touch-apps-and-wh
That's a fair assumption. I'm hoping that my library compares well across all benchmarks. Some of the ones I've chosen so far are significant because they are needed for a fast naive factorial function :-). I am however on the lookout for test cases where GMP is strong. For instance, my most recent results http://www.mega-nerd.com/tmp/new-bench-integer.html show GMP beating mine for the test titled "Product of 10 huge (~2500 decimal digit) Integers". Notice how mine is at least in the same ballpack as GMP while Simple is *way* behind. I hope to close that gap when I implement Karatsuba multiplication. 
Wow, that is good. Thanks! 
&gt; I'm beating integer-gmp where integer-gmp isn't very good This isn't a very good answer. It doesn't explain how you're beating it, or what is bad in integer-gmp. If the sum stays small, then the entirety of the loop is going to be calling out to `addIntC# :: Int# -&gt; Int# -&gt; (# Int#, Int# #)` which adds and tells you if a carry happened. The question is: how are you beating this with your Haskell operations on `Word`? I actually suspect I know the answer. `plusInteger` is marked `NOINLINE`, and is recursive anyway (if the result isn't eligible for small + small = small, then it promotes to big and calls itself recursively). But, this isn't really necessary; the recursive structure could be removed, and it could probably be inlined, and that is probably the entirety of the performance difference between your code and integer-gmp for this test; forcing a function call (which is much slower than an add). And this isn't actually testing any GMP operations, just the wrapper functions in Haskell. It's also entirely possible that forcing a funtion call is a significant amount of overhead relative to what GMP does in your tests, so it may be giving you artificially slow results there, too. integer-gmp should be fixed in this respect before we go declaring that we've matched or beaten GMP.
I agree with the overlaping "active" node names. You never know if you will have some node name that is, or will become a html standard tag in the future.
Well, part of the problem may be that in `integer-gmp` the result of arithmetic operations involving at least one large integer is a large integer (even if it would fit into a small-integer). It'd be interesting to see how many of the `scanl (+) 0 integerList` values would fit into a small integer, to see how significant this contribution might be to the overhead.
It's a nice idea and approach. I got it right away, because you can't really not get it ;)
So much more makes sense now :)
One of the things my code currently does is demote a value from Large to Small if it fits in a machine Word. 
On the other hand it has TRex!
I wonder how much of my money went to Mark Jones.
If you need a console app starting and then poking with stdin messages, I'm sure I can arrange that from Shake.
fyi, I've implemented a proof of concept for `integer-gmp` at [GHC #8638](https://ghc.haskell.org/trac/ghc/ticket/8638) 
Yeah, that's the long term plan. :-)
What about the less controversial, easy to implement but very useful ones like OverloadedStrings, ScopedTypeVariables,...?
When was this written? If it was a while ago, it is still very relevant today. &gt; and elaborate systems for version control: by their suggestion of power, they rather invite than discourage complexity. This is certainly true of git. It definitely invites complexity, though I think it's power outweighs the complexity it adds to a project.
Until I saw that this was a '96 paper, I assumed that was a direct callout on git. Pretty funny how well it applies. I try not to take that kind of grumbling too seriously though. While I fully agree with Djikstra's general points on the importance of managing complexity, some tasks are just complex by nature and require more tools and more elaborate methods. Version control only seems simple until you start laying out all the use cases that come up. Really, I think git is extremely simple as far as VCS goes. Like in SVN you have to worry about the server version vs the local copies, branches vs tags vs trunk, and many other concerns. All of those things are different types too. In git (generally, simply speaking) a repo is a repo is a repo, whether local, server, or on a peer's machine. You've got revisions in a tree, and pointers (branches) to specific revisions in that tree. You can build very complex structures using git, but the tools and types provided are relatively simple.
C# supports any amount of whitespace between them, it can be useful with some of the obnoxious names and chaining you end up doing. AnObject.SomeLongFunctionName() .AnotherLongFunction(param1, param2);
It also has some interesting extensions that GHC doesn't!
Apologies for the atrocious sound here. We had a poor improptu setup, and did the best we could in postprocessing. Any further steps we could have taken to cut down on the noise would have muffled the voice even further. As the video commentary notes, we debated not posting the video at all. However, I feel strongly that there is _very_ interesting material here for properly inclined compiler hackers. So, while its not easy listening by any means, hopefully at least a few people will be able to get some useful ideas out of this. We also have just posted Jürgen Cito's lightning talk on a lightweight webapp, which has significantly better sound quality: https://vimeo.com/80863583 The NY Haskell meetup (http://www.meetup.com/NY-Haskell/) meets the 4th wednesday of every month. We haven't announced Jan's yet, but will soon, and the speakers are set to be quite good.
A suggestion for future recordings is to put a recorder between the mic and the speaker, and just sync the audio with the vid cam later. It would be nice if there were a link to the slides.
We usually have a real videocam with a feed straight from the board. This was a last-minute venue, and it was just an iphone :-(. You may be right that a two-iphone setup coulda been better, but syncing sound properly without real equipment is devilish. On the slides, I'll ping luite again and hassle him for them :-)
Gershom asked me for the slides quite some time ago so that's entirely my fault (I've been busy doing non-Haskell things, see my lack of commits to the GHCJS repository in the past weeks, pretty bad (should improve soon, I have some fixes ready already)). Anyway I'll dig up the slides and put up a link
 sortWith flippedComparison [1..5] == [5,4,3,2,1] flippedComparison a b = case compare a b of LT -&gt; GT EQ -&gt; EQ GT -&gt; LT Why not just sortWith (flip compare) [1..5] ?
How about the same de-echo filters they use in mobile phones? The weird part is that the one who introduced him could be easily understood. Why didn’t he use that same microphone that person did?? Why did nobody check the sound while recording at the beginning?
Where can I get a sticker like that for my Mac? ;)
Hugs hasn't been properly maintained in quite a long time. The last time Hugs was popular was contemporary with GHC 6.6[1]. But, `OverloadedStrings` was introduced in GHC 6.8.1. Don't recall about `ScopedTypeVariables`, though (lacking GADTs etc) you can almost always refactor code to get rid of the need for it; so that's not really a technical limitation. A quick google reveals [this documentation](http://cvs.haskell.org/Hugs/pages/users_guide/hugs-ghc.html); can't say how up-to-date it is though. [1] They were more or less extension-compatible at that point. Though, there were some fun Hugs-only extensions like TRex. And some extensions worked differently between Hugs and GHC, namely the handling of incoherent instances.
I like your attitude! Also, I really like Elm!
http://hackage.haskell.org/package/base-4.6.0.1/docs/Data-List.html http://www.haskell.org/hoogle/?hoogle=sortBy I forget the exact relationship between Elm and Haskell, but Elm's sortWith with should be called sortBy, for consistency. Arguably Elm's names are better, but consistency is best. And why does Elm use Order instead of Ordering? 
yes i wonder if theres a way to DSP the audio so that its understandable? Maybe some compression or whatever will make it sound better. I really wann hear this since Im playing with Yesod some lately. I have some software to do it if I can get the audio file isolated. How to do this? 
Git is plumbing, not porcelain. Raw git is too complex for end users because it allows many different workflows. Bitbucket I think encourages users to pick one of these workflows per project.
With bigotry like that, I can see why you don't want your investors to know the way you are promoting your Haskell conversion.
&gt; if there truly is discrimination against women and black programmers, why not arbitrage the hell out of the market's mispricing by starting a company that hires only female and black programmers? Posters in /r/Haskell are among the smartest on Reddit. I bet you can think of a reason why discrimination may not be arbitragible. I would appreciate if you would make the attempt and post your result. Hint: (spoiler alert. Stop reading if you don't want it.) For many years Harvard University discriminated against racial minorities. Why didn't another University welcome those talented overlooked young adults, and develop a reputation as a better university overall? Another hint: what was wrong with the USA's "separate but equal" educational system of the 1950s -- in terms of opportunity provided to youngsters, and how might it have been inappropriate even if it was efficient with respect to the economic power of the graduates of the luckier half of the system? 
Women who aren't married don't have obligations to maintain a household. And maintaining a household isn't a timesuck unless there are chikdren. The vast majority of volunteer open source projects are created by childless people. And while women are often expected to maintain a household, those same women aren't expected to have an income. Among the privileged classes who have leisure time to dedicate to higher education and volunteer work, women have time to code as men do. Women face challenges that follow on from challenges and discouragement that girls face, and of course in unfree societies women are held out of education and employment entirely, but your aim is off the mark here.
Really comforting to see these Haskell leaders saying things that novice users say. They understand the pain points. What was Oleg saying about how Type Families compose vs Fundeps? My gist of experience is that TF is easier to reason about but less powerful than fundeps. But what about composability?
Sadly, phone browsers are lacking in script ability :-(
Is a `Profunctor f` (over `a b`) also a `Functor (f a)` over `b` and `Contravariant (f b)` over `a` ? What does 'pro' prefix mean here? Is a bifunctor like a profunctor? Why are the naming patterns inconsisent? Just because the types were invented separately, I wager. type Covariant a = Functor a 
Wonderful post and post series. Wish list: Hackage should like to quality blog posts from each package's doc page Hackage should cross link (TvTropes style) between related packages that are generalizations or transfirmations/duals of each other.
* then link to your blog post anywhere you would post your ideas.
Yes, please pressure the to complete folks to add comment support. I have been aching just to fix typos in tutorials.
Love that your search query was all URI escape codes. &lt;3 haskell
Is there a theoretical way the unsafeperformio could be made 'safe', by the compiler analyzing and taking care to not inline expressions that depend on unsafeperformio? From the type system POV, it would be like desugaring the unsafeperformio expression from a to IO a, and then forcibly changing the types of all conflicts from a to IO a, until a consistent whole-program type inference solution is reached. You wouldn't get controlled order of effects, but you could at least get non-duplication, which would be useful in some cases. 
Save'd as the inaugural member of my Awesome Haskell Resources collection of saved Reedit posts.
Pics of those lattice chart examples?
Related: [BSDNT](https://github.com/wbhart/bsdnt) just went 1.0 today…
Yes, f.e. [my effects implementation](http://hackage.haskell.org/package/effects) is just a stack of continuation monad transformers. But instead of using type classes to automatically lift, it uses effect identifiers which you have to pass around. So perhaps effects are not very useful as a library in Haskell. They are probably more useful in new languages where support for effects is built-in.
Also, the 'on' combinator would be useful.
Usually people work around this by making calls to `unsafePerformIO` their own top-level expressions and annotating them with `NOINLINE`. However, there is still the second problem that you can't reason about the side effects any longer because the effect itself is no longer a value within the system.
Still waiting for GHC 7.8 to use this. Would you perhaps have a docker image lying around with everything installed?
[OP](http://www.reddit.com/user/knife_sharpener), as of 22 days ago, has only posted links to dataflowbook.com. This sort of advertising is against Reddit's [rules on self-promotion](http://www.reddit.com/wiki/selfpromotion) and I will be reporting them to the moderators from now on.
There's a vagrant setup that afaik still works https://github.com/ghcjs/ghcjs-build
I mostly agree with the essay, except with the part about version control. You want a powerful version control because it makes it easier to collaborate on achieving simple, elegant solutions.
Make one out of cardboard from a cereal box and stick it on with tape, [like this!](http://i.imgur.com/wbOoidX.jpg) No one will ever suspect! 
I was looking for something similar and came across your post. What have you decided to do in the end? How did things turn out?
didn't work for me. $ docker run -i -t afriel/ghc-head:2013-12-25 cabal --version cabal-install version 1.18.0.2 using version 1.18.1 of the Cabal library $ docker run -i -t afriel/ghc-head:2013-12-25 ghc-pkg list $ running ghc or ghc-pkg gives blank output
But see also: http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.pdf You can mostly recover from the damage that seq does to reasoning in that way, but type case is much harder to cope with.
If infinite loops are impossible, it definitely is not Turing complete. But why do you claim infinite loops are impossible? I think you're right, but it's certainly not claimed in the document linked, and not guaranteed by having GHC's type-checker as the back end.
I'm going with Elm, it's pretty mature and much like Haskell, but better for small Web projects in many ways. 
The term "Turing complete" is used pretty loosely nowadays. For example, it is commonly claimed that [HTML+CSS is Turing complete](https://github.com/elitheeli/stupid-machines), because you can implement [Rule 110](https://en.wikipedia.org/wiki/Rule_110) in HTML+CSS. But it requires a user to click a button in a web page repeatedly to power that "Turing machine".
looks like scheme
It is not a surprise. Haskell is not a programmable programming language and because of this all of its new sub-languages or committee-driven extensions create a Tower of Babel of layers of Turing complete DSLs. Just like C++. How many languages do you have to know to write non trivial programs in Haskell? (Template Haskell, Concurrent Haskell, Data Parallel Haskell... ) I always wandered. Why does Haskell need any real code when a type system is sufficient enough to program in? Type system in Haskell is like generics in Java. Nothing will scream when you switch '-' function with '+' or similar. Type system will not save you. Haskell's double layered generic syntax is weird at least. Edit. Someone said: If there is a pattern in your program (for ex. more than one Turing complete language jumps out) it means that something went wrong and you are doing too much. You should rethink your program and maybe start from the beginning. 
It's looking pretty good. But `Integer` would be better than `Int` in some places.
I love to see explanations of good testing! QuickCheck is really a treasure. My pet project involves sending and receiving data through a JSON format. So there's a translation layer basically amounting a few instances of `FromJSON` and `ToJSON`. It's not a very complex format, but it has some variety and opportunity for error. I wrote the instances blindly, but at the first sight of a bug, I was able to quickly create the necessary (trivial) `Arbitrary` instances, and just run the most basic test: propIsomorphic :: ComplexThing -&gt; Bool propIsomorphic x = decode (encode x) == Just x This found the bug instantly. It's such a schoolbook example of QuickCheck, but that property is such a "fundamental theorem" of any kind of serialization, and running it with random data is a pretty good exercise. With that and a few "golden" test cases, I can feel somewhat confident about changing this module.
"Winning Ways" is a really fun book; I highly recommend it. And yes, it's simply irresistible to play around with that stuff in Haskell as you read. Nice post!
That's actually too sloppy. In fact, it should be: floor . (1.5*) Otherwise you could get 1.5.
No difference. The surreal numbers are the games that are numbers in the sense of this blog post (and the original source, On Numbers and Games).
Happy new year to you as well! :)
Haskell -&gt; Lisp partial function. Nice one. Well, in the end Haskell programs will be written using s-expressions. ;)
Are you familiar with the language Prolog? Type class constraints are just Horn clauses. We have a bunch of predicates (type classes) and relations (MPTCs), and our clauses (instances) are of the form `(c1, c2,...) =&gt; c0` which says that if we can satisfy all the goals on the left hand side then we can satisfy the goal on the right hand side. A program is just a set of clauses. And whenever we need to know whether an instance exists for a specific set of types, the Haskell compiler runs type class resolution— which is essentially the same sort of thing a Prolog interpreter does when you ask if it can prove some goal. Thus, when programming at the type level using type classes, you're essentially programming in the same paradigm as Prolog, aka: traditional logic programming. Programming with MPTCs works a whole lot better with fundeps, but they're not strictly required for making the point. Structurally, logic programming and functional programming are very similar. In both there's a strong reliance on recursion, a focus on being declarative, an avoidance of side effects, etc. However, operationally the two paradigms are very different. In functional programming everything comes down to function application and case analysis. Whereas in logic programming everything comes down to unification and search[1]. Because of these operational differences, the mental model of how to write programs in these two paradigms are also very different. Which is why I suggest that it'd be nice to unify them; rather than using different paradigms at the term- and type-levels. [1] Usually backtracking search, as in Prolog and Haskell type classes. But some languages use forward-chaining (e.g., Datalog) and some languages mix both forward- and backward-chaining (e.g., Dyna). Either way, there's some sort of underlying search algorithm baked into the language.
Happy new year to you as well!
Merry new year 2 u 2!
Folks on #haskell advised a new name for my `deleteBy'` functions: `deleteWhen`.
Men who aren't married don't have obligations to maintain a household. And maintaining a household isn't a timesuck unless there are chikdren. The vast majority of volunteer open source projects are created by childless people. And while men are often expected to maintain a household, those same men aren't expected to have an income. Among the privileged classes who have leisure time to dedicate to higher education and volunteer work, men have time to code as women do. Men face challenges that follow on from challenges and discouragement that boys face, and of course in unfree societies men are held out of education and employment entirely, but your aim is off the mark here.
Sorry, this time it's just totally wrong. It should be: (+) &lt;*&gt; (`div` 2) But still wonderful! Thanks, and please keep them coming!
Applicative is not appropriate IMHO since we need flow control, to do different things depending on external inputs. 
Thanks. Certainly the GUIs gives an illusion of control with a real cost in flexibility and more hidden complexity. It is possible to start and interact with non Haskell code. In the example, a haskell process that read the mail queue can call the mailer deamon using sockets. it can even start it if necessary. This same process can send a message back to the workflow when the mail is sent. if this message does not arrive after a time, the workflow can be programmed to do the appropriate actions. The example is too simple but the article too long and I did not have space for more details without annoying people with my dyslexic writing and my poor english (I´m not native english speaker). I will detail things more in other articles. 
Moreover, usually one of the integration problems is that there is not a single backend, for example, a database, but many of them. Usually two or more of them have to be updated for each single transaction (as seen from the point of view of the final user). So unfortunately a simpler solution like yours is not an option.
Thinking more the main piece of advice I can think of is to try and find a better example to work with. As you said your explanation was as simple as you could make it due to the complexity of the example. Alternatively spliting the article could work, but then you run into the problem of losing the point again...
Thanks. For sure!
I also hate the git UI, but I haven't tried other version control systems yet.
Maybe that's the reason why the bartender gets angry. I don't know.
Nice! As added bonus, it's yet another practical example/reference of consuming the GHC API, which is a bit daunting if you try to go at it strictly from the haddocks.
Come on, dont't be shy. Admit it.
Complexity goes somewhere. Git puts the complexity in your tools. SVN puts the complexity in the communication between developers. Though you could argue that Git introduces a lot of accidental complexity.
Fascinating read! In the back of my mind I always wondered why none of the documentation / tutorials talks much about Haskell's memory model. I assume this is also part of the reason why I rarely, if ever, hear about using atomic operations in Haskell. The standard libraries only seem to offer atomicModifyIORef / atomicModifyMutVar. I've done a fair bit of lock-free programming in the C/C++ world, can I apply the same techniques in Haskell, or is the missing memory model/primops/barriers/fences etc. making this hard/impossible at the moment?
Posted 14 hours ago: http://www.reddit.com/r/haskell/comments/1u6p0a/so_you_want_to_add_a_new_concurrency_primitive_to/
A link was posted to one of the previous meetups, or to this subreddit. I'd browse the meetup page for it in the comments.
Would it be possible to just steal C++'s memory model? I would love to see something like that encoded in Haskell: data Model = Relaxed | Release | Acquire | AcquireRelease | Consume | SequentiallyConsistent readWithModel :: Model -&gt; IORef a -&gt; IO a writeWithModel :: Model -&gt; IORef a -&gt; a -&gt; IO () readIORef = readWithModel Relaxed writeIORef = writeWithModel Relaxed This shouldn't break any existing programs, as `IORef`s already have a relaxed memory model, and it means that the Haskell community doesn't have to do the work of devising a good model. As you said in the email thread: &gt; AFAICT all the researchers working on relaxed memory models have their hands full with things like C++ Why not take their work?
Apologies for the noob quesion, but what is the difference between Int and Integer?
Literate programming is absolutely amazing for tutorials. This format is awesome!
Ints are bounded to fit in word size (although at least 32bits), Integers grow boundlessly until memory exhaustion. 
I was just looking at reactive-banana this morning and saw that it implements [an `Apply` typeclass](http://hackage.haskell.org/package/reactive-banana-0.7.1.3/docs/Reactive-Banana-Combinators.html#t:Apply) which has that behavior. It's interesting because unlike the examples you've suggested which can be handled naturally by `(&lt;*&gt;)` and some natural transformation between functors, `(&lt;@&gt;) :: Behavior t (a -&gt; b) -&gt; Event t a -&gt; Event t b` transforms the `Behavior` in a way that depends on the structure of the input `Event` functor.
If you just want to reuse Hoogle for Elm, you'd be more than welcome: http://neilmitchell.blogspot.co.uk/2011/03/hoogle-for-your-language-ie-f-scala-ml.html
Your examples all involve lists, so maybe we don't need to abstract any further than that? import Data.Foldable (&lt;&lt;*&gt;&gt;) :: (Foldable f, Foldable g) =&gt; f (a -&gt; b) -&gt; g a -&gt; [b] xs &lt;&lt;*&gt;&gt; ys = toList xs &lt;*&gt; toList ys For bonus marks, I dimly remember there being a `Cofoldable` (or some such) class on Hackage somewhere that gives rise to a generic `fromList` function.
As for laws, you would probably want `Interpretable` to be an `Applicative` morphism: as . pure = pure (,) &lt;$&gt; as x &lt;*&gt; as y = as ((,) &lt;$&gt; x &lt;*&gt; y) (of course, it should be natural too: as . fmap f = fmap f . as but that's a free theorem, IIRC)
Added some Readme info, need work on some build &amp; install instructions, once there is more time. If you have time, give me some ideas or help! It be useful to have a functional console client for Wikipedia. Reduce the latency, resource usage and potentially do some ``pipe-work'' ;)
Another possibility is that you might be able to work with a composition of the two functors in question. The composition of two `Applicative`s is always also an `Applicative`, and the Platform comes with the [`Data.Functor.Compose`](http://www.haskell.org/platform/doc/2013.2.0.0/packages/transformers-0.3.0.0/doc/html/Data-Functor-Compose.html) that gives you the `Applicative` instance for free. A little utility library I once wrote for myself might be of help here: {-# LANGUAGE RankNTypes #-} import Control.Applicative import Data.Functor.Compose -- | Lift an action from the outer functor into the composite. liftO :: (Functor f, Applicative g) =&gt; f a -&gt; Compose f g a liftO = Compose . fmap pure -- | Lift an action from the inner functor into the composite. liftI :: Applicative f =&gt; g a -&gt; Compose f g a liftI = Compose . pure -- | Lift a natural transformation from @g@ to @h@ into a morphism -- from @Compose f g@ to @Compose h g@. hoistO :: (forall x. f x -&gt; h x) -&gt; Compose f g a -&gt; Compose h g a hoistO eta = Compose . eta . getCompose -- | Lift a natural transformation from @g@ to @h@ into a morphism -- from @Compose f g@ to @Compose f h@. hoistI :: Functor f =&gt; (forall x. g x -&gt; h x) -&gt; Compose f g a -&gt; Compose f h a hoistI eta = Compose . fmap eta . getCompose elimO :: Compose Identity g a -&gt; g a elimO = runIdentity . getCompose elimI :: Functor f =&gt; Compose f Identity a -&gt; f a elimI = fmap runIdentity . getCompose joinCompose :: Monad f =&gt; Compose f f a -&gt; f a joinCompose = join . getCompose The idea is that you might be able to lift your `Applicative f =&gt; f a` and `Applicative g =&gt; g a` values into `(Applicative f, Applicative g) =&gt; Compose f g a`, work in the `Applicative` instance of that type, and then finish it off by sending the `Compose f g` to the final type you want.
This looks like a variant of the vhost middleware except you are stripping out the path and have a nested DSL. https://github.com/yesodweb/wai/blob/master/wai-extra/Network/Wai/Middleware/Vhost.hs I think you could forgo the writer DSL and use Alternative urlmap :: UrlMap urlmap = mount "bugs" bugsApp &lt;|&gt; mount "helpdesk" helpdeskApp &lt;|&gt; mount "api" $ mount "v1" apiV1 &lt;|&gt; mount "v2" apiV2 &lt;|&gt; mount "" mainApp * yes * yes
Will do. I am learning about these as I am making this, but I'll prioritise whatever is of interest to most people. I intent to have it both cabal-installable and homebrew (or any package manager installable). For the record, the output of the command just now is a wiki-formatted wikipedia page. Once I implement the filter (probably through Pandoc) I will document output examples.
***Edit: PDF+LaTeX*** http://purelytheoretical.com/programming/ttfunc.pdf The aim here is to provide a type-theoretic account of what constitutes a type that is functorial in some variable (e.g.the type `a -&gt; Nat * b` is functorial in `b` but not `a`). The intention is to use functoriality and its attendant `fmap` principle to build mu types that have a recursion principle more like what we're used to seeing in the combinator encodings, as found in typical presentations for `Fix`, e.g. data Fix f = In (f (Fix f)) type (f :*: g) a = (f a, g a) instance (Functor f, Functor g) =&gt; Functor (f :*: g) where fmap f (x,y) = (fmap f x, fmap f y) etc. thus letting you define `type List a = Fix (K () :+: (K a :*: Id))` I think my primary worries are the possibility of the `fmap` principle getting stuck (which may mean it needs to be a term not a principle), and also the reasonableness of this in general. ***Edit*** I think probably since we'd want unique functoriality proofs, the constant functor rule should be restricted to variables like D, a type, b type !- tyctx --------------------------------- D, a type, b, type !- a functor b since functoriality is simply carried up through type operators.
That's pretty much what the atomic-primops package does; it uses GCC to compile C functions that perform atomic ops, and then calls the generated object code as opaque function calls. Unfortunately there are two problems with this: first, the extra function calls (and lack of optimizations around them) mean this approach is much slower than it could be, and secondly GHC doesn't promise that it won't reorder stuff around function calls (this is dependent on the type of foreign call, Edward's post covers several of them), it just currently doesn't actually do so. These two effects are somewhat balanced: if you attempt to speed up your algorithm by inlining code (that is, inlining it into a C-- block) that comprises the foreign call, ghc will have a greater opportunity to rearrange things in a way that will ruin your day. If you leave your foreign calls completely opaque, the C-- optimizer won't be able to mess with them, but you're paying the cost of extra indirections. My problem is that I have two (relatively primitive) reads that need to occur in a certain order, and monadic sequencing is no longer necessarily preserved by the C-- phase. That's just high-level assembler, and the optimizer is free to move reads so long as there are no intervening writes. It certainly could swap the two reads, which would break my algorithm. There are various approaches I can take to get around this, but all of them involve some type of sub-optimal performance compromise.
Infinite loops only show up at the limit. So, whether you have them or not is similar to whether your belief in the natural numbers necessarily implies the existence of an infinite ordinal. You need arbitrary loops to do things like define addition by induction. Because natural numbers can become arbitrarily large, this addition function needs to be allowed to make arbitrarily many recursive calls in order to ensure that it eventually hits the basis case. However, just because natural numbers can be arbitrarily large does not mean they can be *infinitely* large. The fact that you cannot give a finite bound on the number of recursive calls needed, does not imply that you will not hit the basis case after finitely many recursive calls. If our inputs are all strictly less than 5, say, then we can guarantee that the function will make no more than 5 recursive calls. But what about when the inputs are only strictly less than 7? or 42? or a million? Given any such finite bound we can prove the totality of our induction, but we're going to miss out on being able to add together certain natural numbers because they're too large to satisfy our precondition on inputs. But we do know that as we keep raising the bound we get closer and closer to covering all inputs. Because our proof of termination is uniform regardless of the actual bound used, we can say that *in the limit* our proof of termination will cover all inputs. However, taking this limit is equivalent to taking the supremum of all the natural numbers. Saying our argument holds in the limit is akin to declaring that such a supremum exists; if the supremum exists, then for every natural number smaller than the supremum (aka *all* the natural numbers), we know our function terminates. But the problem is, the supremum of the natural numbers not finite. The supremum of the natural numbers is not itself a natural number; and that is why we can't give a finite bound on the number of recursive calls needed. Make sense?
It's not just an inter-block jump: it's an inter-procedure jump. (I think I may have used the wrong word in my email. Technically, we do do some block optimizations, for example, eliminating blocks which are just jumps to another block).
Tho crucially there's a distinction here between infinite induced behavior vs. infinite representation. `a*` is a finite regex ofcourse, but it's behavior, as represented by its unfolding, is infinite. The same being true of cyclic data. Almost all grammars that are of any usefulness whatsoever are like this too, representing infinite languages in finite means. Finite representations are vastly more interesting, I feel, than truly infinite representations, just on a spiritual level.
I lost all interest when I saw that.
Aha, so *that's* an arbitrary loop. Going back to the example, however $ typo &lt;&lt;EOF (define (fac n) (if (== n 0) 1 (* n (fac (- n 1))))) (fac -1) EOF Notice the -1 in stead of 5. That's no arbitrary loop anymore, right? It's provably infinite without talking about a supremum. I misunderstood because I thought you were talking about arbitrary stop conditions. For the sake of example I used a negative number, while they're not allowed by the language. Of course we can encode negative numbers (and even an arbitrary large random access tape) using the naturals. 
I know this is unhelpful, but have you considered doing this with Ott or Latex? I can't read ascii inference rules easily at all.
This is really great! I'm delighted that you've found IHaskell useful. I'm happy to answer any questions anyone has about it! (I'm the author.)
Sounds like Randall Munroe has been doing some functional programming, since this is his [second joke](http://xkcd.com/1270/) related to the subject.
[Image](http://imgs.xkcd.com/comics/functional.png) **Title:** Functional **Title-text:** Functional programming combines the flexibility and power of abstract mathematics with the intuitive clarity of abstract mathematics. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=1270#Explanation) **Stats:** This comic has been referenced 4 time(s), representing 0.05% of referenced xkcds. --- ^[Questions/Problems](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Website](http://xkcdref.info/statistics/)
To my understanding, whoever wrote the entire backend for that really long comic did it in Haskell.
I'm still not entirely sure what the goal is here. For example, * What setting are you trying to deduce functoriality with regard to? In Haskell or some other specific language? In a CCC, topos, or other categorical model? * What is the (desired) fundamental theorem of your system? Soundness? Completeness with respect to some model or theory? If so, then re which model or theory? * Is the goal to prove something new about functors themselves? or is it to capture what we already know? If the latter, then what is the goal of capturing this knowledge: to implement `fmap` generically? To write a library for automatically proving the functor law? etc.
............oh. wow I'm dumb.
Well, `-1` isn't a natural number. This program will diverge, but that's because `fac` is (implicitly) defined by induction on the natural numbers— and so, when given something that's not a natural number it goes nuts. Yes the program goes into an infinite loop (assuming you're using `Integer` so things won't overflow to positive numbers). But that's just a fact about this particular program. Or, if you prefer, it's a fact about the language that it allows nontermination. Doesn't say anything about whether allowing nontermination is required for Turing completeness, nor anything about whether Typo is Turing complete or not...
ok, i've updated my comment above
On point 1: I'm not trying to deduce functoriality on anything. It's just an attempt to provide the ability to have judgmental functoriality in TT so that mu types can be defined in a way parallel to their usual definition via fmap, etc. That is to say, something like this: data Fix f = In (f (Fix f)) fixRec :: Functor f =&gt; (f a -&gt; a) -&gt; Fix f -&gt; a fixRec alg (In x) = alg (fmap (fixRec alg) x) In TAPL, for instance, mu types aren't defined like this, but instead they're given a relatively simply definition, and the recursion principle is achieved via `fix : (a -&gt; a) -&gt; a` which is generic and not tied to mu types, and which also permits the construction of codata as mu types, as exemplified by the definitions given for `Hungry = mu A. Nat -&gt; A` and `Stream X = mu A. Unit -&gt; {X,A}`. with the obvious computational rule. I want to instead capture the intuition of mu types and their recursion principle directly, as above with `Fix`, but that requires having `fmap` to translate the `f (Fix f)` term into an `f a` term, hence, functoriality needs to be present in the type system. Ideally, this would also provide a nice translation directly into CT for any model category with least fixed points, with the obvious catamorphism semantics. On point 2: who knows! I'm not sure what the question even means, per se. Soundness and completeness would be nice, of course, since thats what computation rules (beta and eta) are supposed to provide, but I'm not sure if the functoriality judgment messes these up. The principled-ness of it, or at least the attempt at it, is aimed towards showing that functoriality is/must be an admissible judgment given existing rules. One point 3: No, the goal is as above.
Go check out Martin Steffen's thesis, "Polarized Higher-Order Subtyping" (no, not polarized in the linear logic sense, polarization in the co/contra-variance sense). The key points are: identify functoriality with positive polarity, and that polarity induces a subtyping relation in your type system. *Edit.* I was reviewing my notes from when I was looking at this problem last spring, and I remembered that I had implemented a simplified type-checker for Steffen's calculus; but in the end, I decided that the proof obligation 'f(-) is a functor' was sufficiently simple that it could just be done with something like type-classes. This made the implementation a lot simpler too, since there was no longer any subtyping nonsense.
Hi NiftyIon, I've just installed ipython and IHaskell on OSX and I can run `IHaskell notebook` but can't actually run any of the haskell cells successfully. When I run `IHaskell console` I get a neverending stream of these errors in my console: ERROR:root:Exception in callback &lt;zmq.eventloop.stack_context._StackContextWrapper object at 0x1023d4788&gt; Traceback (most recent call last): File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py", line 440, in _run_callback callback() File "/Users/aaron/.ihaskell/ipython/lib/python2.7/site-packages/IPython/kernel/channels.py", line 161, in thread_send self.session.send(self.stream, msg) File "/Users/aaron/.ihaskell/ipython/lib/python2.7/site-packages/IPython/kernel/zmq/session.py", line 646, in send stream.send_multipart(to_send, copy=copy) File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 261, in send_multipart self.on_send(callback) File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 227, in on_send self._check_closed() File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 498, in _check_closed raise IOError("Stream is closed") IOError: Stream is closed ERROR:root:Exception in callback &lt;zmq.eventloop.stack_context._StackContextWrapper object at 0x1023bb578&gt; Traceback (most recent call last): File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py", line 440, in _run_callback callback() File "/Users/aaron/.ihaskell/ipython/lib/python2.7/site-packages/IPython/kernel/channels.py", line 161, in thread_send self.session.send(self.stream, msg) File "/Users/aaron/.ihaskell/ipython/lib/python2.7/site-packages/IPython/kernel/zmq/session.py", line 646, in send stream.send_multipart(to_send, copy=copy) File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 261, in send_multipart self.on_send(callback) File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 227, in on_send self._check_closed() File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 498, in _check_closed raise IOError("Stream is closed") IOError: Stream is closed ERROR:root:Exception in callback &lt;zmq.eventloop.stack_context._StackContextWrapper object at 0x1023d4788&gt; Traceback (most recent call last): File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py", line 440, in _run_callback callback() File "/Users/aaron/.ihaskell/ipython/lib/python2.7/site-packages/IPython/kernel/channels.py", line 161, in thread_send self.session.send(self.stream, msg) File "/Users/aaron/.ihaskell/ipython/lib/python2.7/site-packages/IPython/kernel/zmq/session.py", line 646, in send stream.send_multipart(to_send, copy=copy) File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 261, in send_multipart self.on_send(callback) File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 227, in on_send self._check_closed() File "//anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py", line 498, in _check_closed raise IOError("Stream is closed") IOError: Stream is closed 
I've been noticing a lot of jokes about Haskell lately. I wonder if it's at stage 2 of this: 1. First they ignore you 2. Then they laugh at you 3. Then they fight you 4. Then you win Stage one took a very long time.
Even if I go [full Kurtzweil](http://en.wikipedia.org/wiki/Ray_kurzweil#Health_and_aging), I doubt I'll live long enough to see stage 4.
I love Haskell and the jokes as well.
FWIW, in my case I can avoid the problem by performing my second read within an `atomicModifyIORef`, and the performance from doing so is quite good (in some cases better than without any ordering guarantees). Haskell maintaining silence on this matter is perfectly reasonably to me, except for one little nit. The CPU memory model already leaks out (witness IORefs). If we want to have a language where it doesn't, then the compiler needs to be aware of the architecture's memory model when generating code for operations on IORefs, MVars, and similar where some synchronization is necessary. As you point out, these all currently have de facto behavior that's specified to varying degrees (typically under-specified). Perhaps adding a memory model to C-- would be the best way to specify the behavior of all these operations and ensure that the compiler respects it?
He seems to be being quite strict in his assessment of Haskell, ironically enough.
I gave it a shot because this was posted in r/haskell, in the hopes it would somehow have something to do with the practice of FP. It didn't really. I'll give the next episode a shot. I use Clojure (prefer Haskell) too but I really don't want to hear from Uncle Bob of all people about FP. I've read what he has to say and he isn't that informed. If he were, he wouldn't be using Clojure. I would kill for a podcast that talked about FP and Haskell :(
Ok, then I guess I have the right idea! - GHC can't re-order reads across unsafe foreign calls, otherwise all kinds of C API wrappers (OpenGL etc.) which exchange data with Haskell through Storable objects would not work reliably. - Now there's work being done to give GHC native primitives like memory barriers and fences with special, well-defined optimization semantics, while the currently available approaches are either 'fast with poorly defined semantics' or 'completely opaque and slow'. I'd certainly love to have fast and safe atomic operations on normal, non-storable, unpinned Haskell data types and higher-level libraries like a vector supporting atomic ops or even entire data structures like an atomic List/Queue.
You've seen the Haskell cast, right?
`Interpretable` = [natural transformation](http://blog.sigfpe.com/2008/05/you-could-have-defined-natural.html).
Anybody noted that it is a *false* claim about Haskell that is jokingly refuted? In adition, a deeper truth that is behind this is that there is no way to avoid side effects in programs, except you don't run them.
An explanation I like is that Haskell has effects rather than side-effects.
That was disappointing (ho, ho, lazy and nobody called it). Kind of expected more. But xkcd has a wide audience, so I'll take the exposure in good spirit.
&gt;or want to live in a pure heaven. Haskell is pure ideology What? The point of Haskell is to know where purity ends and where it begins in code. You have clearly not written any Haskell. &gt; (in contrast to Agda or Idris or Coq) Agda and Idris are basically Haskell with dependent types. Coq can be used with Haskell. There's also Haskabelle. &gt;I tried and still don't understand this pro Haskell noise. I've written a lot of Clojure, Common Lisp, and Haskell. I've written a small amount of OCaml as well. You need to do some learning, I'd suggest starting here: http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way/
If you want to construct fixpoints, it's not entirely clear if functoriality is enough. Plenty of people are interested in that question. It's clear that *strictly positive* functors do have fixpoints, so the work by my colleagues on *containers* might be a good place to look. The presentation of inductive families in *The gentle art of levitation* is about coding up a collection of concrete presentations of containers, then delivering Mu. We give the dependent induction principle rather than the nondependent iterator. Amusingly, the dependent induction principle more or less amounts to the nondependent iterator for the singleton type of the type that you started from. So, interesting questions include: which functors have initial algebras? which functors have singleton functors?
Yeah, want more episodes like the first one.
No, it was just a lucky hit in terms of an intersection of what I wanted to learn at the time when I listened to the episode. The moments I liked most was the rapid-fire explanation of the concepts framing Lens. There wasn't as *much* of it as I'd have liked, but I enjoyed what was there. Just want more content really :(
Though it's not very much of an explanation, however true it is.
Indeed. I want to leave open what counts as functoriality here, only requiring that whatever definition you choose is compatible with mu types. I wanted to avoid going the levitation route, as well -- my primary aim is actually to provide a clear relationship between mu on the one hand, and native inductive types on the other hand, such as G !- M : Nat --------------- ----------------- G !- zero : Nat G !- suc(M) : Nat G !- M : Nat G !- Z : X G, r : X !- S : X ----------------------------------------------- G !- natrec(M, Z, r.S) : X vs just defining `Nat = mu a. 1 + a` and getting constructors/recursors automatically. It ought to be that we can take the mu definition and unfold it a little bit, until we have compositions of various things, and then we can "overwrite" the compositions, turning, say, `&lt; inl &lt;&gt; &gt;` into `zero`, etc. thereby recovering the native definitions mechanically. From this my intention (which I may have gotten to, at this point!) is to dualize and get a native interpretation of nu for codata.
Thanks! I'm going to give Elm a try for a game.
I find it funny that people waste the time to bash languages they dislike on reddit. I like clojure too, why don't you go and write some instead?
I always wash my hands before writing Haskell.
If it's any consolation, Ghandi never actually said that.
Well, if a language has no infinite loops, then all programs are guaranteed to terminate, meaning the halting problem can be solved generally (trivially) for the language. We know there's no general solution to the halting problem for a Turing complete language, hence, the language must not be Turing complete.
For those who don't know -- last year's xkcd april fool's jokes was implemented using Haskell: http://www.reddit.com/r/haskell/comments/uved7/waldo_the_haskell_powered_codebase_behind_xkcds/ And other parts of the site have had bits of Happstack code in them for ages.
You can see the result immediately: add1 x = 1+x main = print (add1 3)
Thanks :) Could you explain why this is better? I guess it means you could then use UrlMaps with any code that works on arbitrary Alternative instances, but I can't imagine that would happen very often.
Forgot to mark new_joke() as impure. 
If this is true, you shouldn't have any problem substantiating it then right? Can you please?
Not quite what you're asking, but Prelude&gt; :t fmap . fmap fmap . fmap :: (Functor f1, Functor f) =&gt; (a -&gt; b) -&gt; f (f1 a) -&gt; f (f1 b) Prelude&gt; :t fmap . fmap . fmap fmap . fmap . fmap :: (Functor f2, Functor f1, Functor f) =&gt; (a -&gt; b) -&gt; f (f1 (f2 a)) -&gt; f (f1 (f2 b)) So trivially &lt;*&gt; can be used on (arbitrarily) nested functorial types.
You want me to write it to a file, compile and then run? Are you from the past? ghci: parse error on input `=' Lack of consistency? Two languages in one environment? I assume your code is correct.
And `Apply` is the "module" structure corresponding to the "monoid" structure `Applicative`.
And what, pray tell, has influenced your expert opinion on the matter that would make you come to such a conclusion?
Buildable comes from Gershom Bazerman's FP Complete writeup "Building up to a Point via Adjunctions".
Many more? I wouldn't consider 3 more to be *many more*. There are podcasts out there with thousands of episodes, running daily for years. 4 episodes isn't even a podcast, it's a short miniseries.
Yeah, in GHCi, you have to use `let`. I find it annoying too. Regardless, GHCi is the interactive environment. If you want to run Haskell scripts on the fly, you should use `runghc`. With no args, `runghc` will use stdin. Watch: $ runghc add1 x = 1 + x main = putStrLn "" &gt;&gt; print (add1 3) ^D On the following line, we get the output: 4 (If you use a heredoc, presumably, you won't have to stick in the extra `putStrLn`, but whatever.) As for being from the past, I must confess that I *am* from one second ago.
I came in to say exactly this. With algebraic datatypes, functionarilty coincides with covariance, so looking at the well-established literature of subtyping will give you plenty of type-theoretic treatment of functionarility. In the OP document, there is a clear interested in the dynamic semantics of fmap-elimination; this corresponds to elimination of the subtyping conversion rule for positive type operators in Steffen's calculus of higher-kinded types -- while on star-kinded types it just applies the function described by the subtyping witness. I don't remember whether this dynamic aspect is described in Steffen's or later work. There is a slightly shady thing at play here. You can see subtyping conversions as (retyping) functions, and their dynamic semantics corresponds to an (erasable) function application. But in the other direction, you can use this subtyping calculus to compute with fmap *on arbitrary functions*, even those that are not subtyping witnesses at all; just *pretend* that they are coercions, and everything will work as expected.
[Hoogle](http://www.haskell.org/hoogle/?hoogle=%21%21) is very useful 
[Prelude documentation](http://hackage.haskell.org/package/base-4.6.0.1/docs/Prelude.html#g:11), see "List operations" second to last before the subsection "Reducing lists". &gt; [1,3,12,13] !! 2 -- 2nd element of the list (head is 0) 12
If you have functoriality and parametricity, then you have fixed points, though parametricity is, as of course you know, inconsistent with a set theoretic semantics of types. In fact, Derek Dreyer and I have shown that you can get the dependent induction principle from the iterator, assuming parametricity. (I don't know how any of this is related to containers, which is a gap in my understanding.) 
Thanks! 
Hey there. First of all, make sure you're installing IHaskell from it's repository on Github. The Hackage package is severely outdated and will be updated soon, but at the moment is going to be broken. Second, IHaskell does not need you to install IPython - it will install its own local copy if needed. Anaconda ships with an incredibly old version of pyzmq, which is likely causing this to break. If you don't use IPython itself, I'd suggest getting rid of Anaconda completely. Finally, make sure that you have zeromq4 installed when installing IHaskell. If you have homebrew, just make brew install zeromq should work (if you brew update first and uninstall old zeromqs). I hang around on #ihaskell (not #haskell) on chat.freenode IRC if you'd like help, or feel free to file a Github issue!
Why is that?
I did not know about this and was looking for a new podcast*. Thank you. *having exhausted History of Rome and used as many analogies to Caesar on Reddit as is possible
Purity is the best thing about Haskell. I've been working on a project in my spare time. I wrote quite a lot before writing tests. I went back and wrote tests this christmas. Not a single bug was found. I've never done that before in my life. I was actually getting desperate as more and more tests I could think of were just passing.
ghci evaluates expressions inside the IO monad. It's not a hack. I don't know why I am feeding a troll.
I don't think it's fair to call it dated if GHC 7.8 doesn't even have a release candidate yet, and a Haskell Platform shipping it is likely &gt;6 months away. That being said I'd also be curious if this use case benefits from the upcoming RTS changes!
This is basically the kernel of my answer as well, because that's how [`Data.Functor.Compose`](http://www.haskell.org/platform/doc/2013.2.0.0/packages/transformers-0.3.0.0/doc/html/Data-Functor-Compose.html) works: newtype Compose f g a = Compose { getCompose :: f (g a) } instance (Functor f, Functor g) =&gt; Functor (Compose f g) where fmap f = Compose . fmap (fmap f) . getCompose instance (Applicative f, Applicative g) =&gt; Applicative (Compose f g) where pure = Compose . pure . pure Compose f &lt;*&gt; Compose x = Compose $ (&lt;*&gt;) &lt;$&gt; f &lt;*&gt; x
Who cares if it behaves differently from what you expect? What is it for? How many more hacks lurks beyond this? And remember that this is a simple "hello world". W live in XXI century and times of code-compile-test cycles should be long gone. What would you expect from language when basic tools are broken? How is this all different from good old C++? ;) Someone is trolling you badly and it is not me. 
Parametricity is another really powerful thing. With parametric polymorphism, just like with mu types, you get a whole slew of new types that aren't minor compositions of old types. In fact, you get more, since you can do nu as well, I think. Parametric polymorphism is super powerful. What I'm looking to do is understand the small little modifications that TT makes when adding individual (co)recursive types, by grounding the behavior of mu and nu type theoretically, and then finding a mechanical conversion from any given `mu a. F` or `nu a. F` into a primitive (co)recursive type that doesn't make use of mu or nu, thereby having only a single addition to the type theory.
I don't think having different Alternative instances is very valuable. In the common case the DSL choice probably doesn't matter a lot for new users. The reason to use Alternative is that this seems to be exactly what Alternative was created for, so there is no need to shoe-horn Monad into the solution. This should make your implementation a little easier. but I think the main benefit is for a Haskell familiar with Alternative. They will immediately recognize what is going on when they see Alternative being used. It is easier for users create abstractions, although that is probably not very useful in this case. Those not familiar should be able to use &lt;|&gt; just fine. I believe this is how Snap &amp; HappStack do their routing, so you might be able to look at their code. Yesod instead has an embedded DSL mainly for the purpose of boiler-plate free type-safe routes. All of the mountings here could be done with the existing yesod routing by routing to the applications as subsites of a Yesod app. Putting Yesod in the front has the advantage of being able to use yesod-auth and other Yesod apis to authenticate and authorize users. Alternatively, the yesod-routes package does not at all depend on Yesod and could be used to create a routing DSL similar to what Yesod has but that only deals with WAI apps.
I don't think lpsmith is talking about the new parallel IO manager that will be in GHC 7.8, but the "new" IO manager that came with GHC 7.0. Note that the post is from April 2010. Edit: 2010, not 2007
The post is from April 2010 (2007 is his Haskell starting date). [GHC 7.0.1 was released later that year,](http://www.haskell.org/ghc/) but you're right, had he used the functionality from then-HEAD he would probably have mentioned it.
Yes, I am referring to the old "new" IO manager. I've been using Haskell since the last millenium, so it's still pretty new to me. ;-)
Not saying this is the best Haskell code I've ever seen, but I feel like I've seen worse. Any particular part that makes you feel like it's "clearly" a first project?
It is prettier :)
He's not worth my time. So much meaningless crap has come out of his mouth, he's not an authority on any subject and he's frankly never done anything that makes him worth listening to.
I also recommend Dan Carlin's stuff, particularly the Hardcore History series, including the roman republic miniseries and the genghis khan miniseries.
Soooo goood.
Thanks!
Off topic but what is wrong with Clojure, for its niche? I mean, if you have strong pressure to work on the JVM already...
My bad, haven't been around long enough to know about more than one 'new IO manager'! ;-) 
Thank you.
It was already posted: http://www.reddit.com/r/haskell/comments/1uafw8/xkcd_haskell/
This is what we get for calling things new X. :) Andreas wisely named the new new io manager the parallel io manager to clarify what's new.
&gt; he's not an authority on any subject and he's frankly never done anything that makes him worth listening to. Just in the context of fp or oop as well?
I would say both, but most certainly not FP.
In the context of this subreddit, Clojure is a Lisp and in typical fashion not statically typed, so it's lacking in a fundamental area. At least it does have immutable data structures which is rare for Lisps (even Scheme's purists are doing pure operations on mutable data structures).
Presumably their business model is one of selling [their commercial IDE](https://www.fpcomplete.com/business/haskell-center/fp-haskell-center-highlights/) and their [commercial Haskell deployment services](https://www.fpcomplete.com/business/haskell-center/overview/). It looks like, to access the IDE, you need to make a project - which is free. However, these IDE tools for emacs are currently just a replacement for a [couple](https://github.com/bitc/hdevtools) of [existing](https://github.com/kazu-yamamoto/ghc-mod) Haskell plugins ([more here](http://www.haskell.org/haskellwiki/Emacs)), so currently you won't really gain anything from switching to FP Complete for these services. *Edit:* Replaces evil redirection-only link to `ghc-mod` with github link.
Hey, this is really cool, thanks. I'm even more impressed that you managed to build something like this after only a couple weeks with Haskell. I've messed with the language intermittently for a few months now, and the most I've managed to do is hack at my XMonad config and build a few websites with Hakyll. I'll be reading through your source code to gain a little more practical Haskell background.
The point at which the code actually runs has no basis on whether that language is Turing complete. In this view, Haskell is not Turing complete as it requires a user to start a program. Machine code is not Turing complete as it at least requires someone to turn on the computer.
hm, I got an "xdg-open: unexpected argument '&amp;'", because &amp; is of course interpreted by the shell
A full length talk on this: "Combinatorrent - Writing Haskell Code for Fun and Profit" - talk given at the 2012 Erlang User Conference: http://www.infoq.com/presentations/Combinatorrent-Haskell-casestudy 
Simple thing. If we assume that new languages should reduce complexity then Haskell by trying to solve problems of state introduces at least one: space leaks. I've yet seen no single programmer using Haskell who is not fighting this bug. And this bug is serious one, because it is real one - not some kind of personal preferences (syntax/tools). You cannot just write code in Haskell, compile it and say it is done like a C programmer would do. Haskell programs can crash without warning. Haskell made a constant thing which is taken for granted in strict languages and made it variable (read: unpredictable). You cannot replace mutability bugs with space leak bugs and claim this is an improvement.
I don't understand how I'm supposed to be listening to the podcast. All I see is the following text and links, but no videos nor sound players: Episode 1 – Robert C. Martin 3 Replies In this episode I talk with Robert C. Martin, better known as Uncle Bob. We run the gamut from Structure and Interpretation of Computer Programs, introducing children to programming, TDD and the REPL, compatibility of Functional Programming and Object Oriented Programming Our Guest, Robert C. Martin CleanCoders.com @unclebobmartin [...] Topics Clojure Structure and Interpretation of Computer Programs [...] A giant Thank You to David Belcher for the logo design. Since I don't see any logo either, I assume that there is a component I must be missing, but I don't see it under Chrome, Firefox, nor Safari. Maybe they are again having trouble with their hosting provider?
Oh, sh... I forgot I had left that. Wanted to try to emulate an "open in the background" option (xdg-open doesn't seem to have such option), but never tested it out. I'll fix that in the next release (in a few hours). Thanks for the bug report!
Well, I don't think my code is anywhere close to being good enough to use as a reference. I'd rather advise you to have a look at Real World Haskell (see sidebar) which was my read while I was building _rascal_ and helped a lot on practical aspects.
Pushed the fix to GitHub and Hackage. Don't hesitate to tell me if you still encounter issues (don't have a Linux box to test right now…)
Racemetric is a simple athletic race registration and credit card processing app. It's all written in Haskell (Snap, Heist) and JS on top of PostgreSQL. The idea is for race organizers to accept credit card registration payments within minutes instead of having to talk to sales people, get a merchant account, build a custom web page, etc. As a runner and rower, I'd like to have all my race results in one place and one format. Of course I'm looking for customers as well as any feedback you have to give! Feel free to contact me directly at luke@racemetric.com. Here's the HN post for reference: https://news.ycombinator.com/item?id=7013227
Thanks =) Written all w/ haskell-mode and lately structured-haskell-mode of course ;-)
Thanks! There are many many competitors actually and my hope is to be able to beat them with a better product. I'm just not happy with any (active.com is the biggest right now). Technology is totally an afterthought for most of them. Glad you liked my open source contributions! I just like working on open source projects really =)
Sorry about that, I made an update, which lost the media element for my plugin apparently. It should be working now.
Oh, thanks for the tip. Funny, I finished *Learn You a Haskell* today, after procrastinating for quite a while, instead relying on API documentation and skimming source code in attempts to intuitively learn the language. I would say Haskell's early learning curve definitely lends itself to more formal training (books, not blog posts and Stack Overflow). This activity reminds me of learning C, but it has been somewhat of a curveball considering the recent trend toward "easiness" in language design. This afternoon I opened my copy of *Real World Haskell* and worked through some of the example problems. It is fantastic how the knowledge from *Learn You* is made concrete with the exercises in this book. Your code is still actually helpful - just reading the module imports gives quite a sense of what is out there to play with. On the other hand, it won't compile thanks to the module imports ...
&gt; You cannot just write code in Haskell, compile it and say it is done like a C programmer would do. Haskell programs can crash without warning. In my experience, the "write, compile, done" approach to programming is far more successful in Haskell than it is in C, due to Haskell's far superior type system. I agree that space leaks are a big flaw in Haskell, but I don't see it as such a dealbreaker. Then again, the C programmer probably doesn't see mutability bugs as such a dealbreaker, either.
The Haskell community is pretty solid in general. Tekmo, for example, has an approach that isn't shared by the larger Haskell community, I'd love to hear him talk about why he codes the way he does. An interview with Don Stewart that covered different material than the Haskellcast could be useful, dons has been all over the place. If you want people from Clojure: technomancy (Leiningen, grenchman), David Nolen (ClojureScript, Om) , mikera (core.matrix). People from Elixir probably wouldn't be terribly fruitful. I guess the point is that I'd like to learn more about the practice of FP beyond simply using functions. Persistent data structures, Functors, Monoids, Monads, etc etc etc
Thanks for the suggestions. I would like to cover a broader range of languages, but started with some Clojure people, since that is the community I came into functional programming through and know some of the names of. My goal is to try to work to reach out into the larger functional programming community and get to learn about other ways of functional programming and get that recorded so that others interested in the topic of functional programming can benefit as well. I will put Tekmo and Don Stewart on my list of Haskell people to reach out to and find some topics to discuss. Thanks for the suggestions! --Proctor 
Outstanding work!
Yawn.
 src/HsImport/ImportSpec.hs:18:8: Could not find module `HsImport.Parse' Use -v to see a list of the files searched for. Failed to install hsimport-0.2.6.2 It seems that HsImport.Parse is not being found on hackage. I see that module on your github repo, but I don't see it in hackage. I'm going to try to install this from the github repo and report back.
hsimport works from installing from your github repo.
It might be the case that the generated GLSL needs some extra pragma on certain systems. Could you put the whole output of the example somewhere? Also, we need to know your OS and video card model.
I thought that the primops do generate the right mem ordering code for various CPU architectures? (idk how well its been tested on ARM mind you! :) )
yup! some work relating this should be happening to ghc in time for 7.10 i think...
does this information help https://gist.github.com/osa1/f611d94510a980bd28f8 ?
 * `cabal sdist` to generate `$TARBALL` * `cabal upload -c $TARBALL` to ensure package passes local quality checks (not sure entirely what these are) * assuming `$TARBALL` unpacks into `$TEMPDIR`, `tar xzf $TARBALL &amp;&amp; (cd $TEMPDIR &amp;&amp; cabal configure &amp;&amp; cabal build &amp;&amp; cabal test) &amp;&amp; rm -r $TEMPDIR` will give errors if you have forgotton to include a module (These are old notes from before I started using sandboxes.)
It looks like the whole thing dies at the beginning when creating the OpenGL context. Do other OpenGL applications work for you? We could also use the output of glxinfo on your system. By the way, we can take this discussion to the project’s [issue tracker](https://github.com/csabahruska/lc-dsl/issues).
Yes, in the past I've been testing this by creating a tarball with 'cabal sdist' and then installing the tarball with 'cabal install'. But, well, I just forgot about the whole thing again. I think it would be quite nice - and IMHO also the right place - if 'cabal sdist' would check about these kind of problems. 
I imagine this would be hard for cabal to do in general. e.g. a module that is only imported on a certain platform, or only when a specific flag is enabled at configure-time.
Hi Derek, just some random thoughts case length args of 0 -&gt; 1 _ -&gt; read (head args) :: Int is better expressed as case args of [] -&gt; 1 (x:_) -&gt; (read x) :: Int If you want to be cunning you can write printDeck = do deck &lt;- shuffleDeck let hands = dealHands deck printHands hands as import Control.Monad ((&lt;=&lt;)) printDect = printHands &lt;=&lt; (return . dealHands) &lt;=&lt; shuffleDeck You can also remove duplication here printHands hands = do printHand "North" $ north hands printHand "East" $ east hands printHand "South" $ south hands printHand "West" $ west hands using printHands hands = mapM_ (\(s, f) -&gt; printHand s (f hands)) players where players = [("North", north), ("East", east), ("South", south), ("West", west)] You could pull out the lambda to make that a bit neater, but I hope you get the general point about using `mapM_` to reduce duplication. Is there a particular reason to use `type TableHands = [(Player, Hand)]` rather than `type TableHands = Player -&gt; Hand`? Good luck with your program. Looks nice so far!
Hi derek, nice! I think it's well written, so not much remarks: * what the use of function makeBid? * for function dealHands, I would use import Data.List.Split dealHands deck = zip [North..South] (map sort $ chunksOf 13 deck) Do you plan to retain a state in your game (for example, the state of the hands as the players are playing)? If yes, the Writer monad might not be enough, I would use a State monad and possibly some FRP.
Just want to say thanks to everyone who comments (here and in other places) - for a beginner in Haskell, it's extremely useful to see different idiomatic ways of writing code, etc. Wish Reddit had proper syntax highlighting, something like StackOverflow's: ```haskell code ``` After all, SO also uses Markdown, so it should be possible?
That looks nice. Are there plans to work with a stock hdevtools? Sorry to look in the mouth of a gift horse, but I tend to work with different machines &amp; systems — custom installation steps get in the way of happy hacking. I’ll give a try regardless, but I’m more likely to add it to my workflow if I know I can install it with a one-liner. On an unrelated note, thank you as well for the explanation on how to get hdevtools to pick up the sandbox config. It sure beats my very manual way!
Looks very neat - I saw you yesterday on HN as well. Great to share the fact that you can actually build neat general purpose (ie. not Hackage etc) web apps with Haskell... Do you think you will be able to share any of the source? I realize you probably don't want to open source the whole application, but perhaps there are snippets you could share, for example in a blog post format where you talk about how you solved certain problems etc. Or perhaps there's even some functionality you could factor out as a small library etc? Given that web development with Haskell is a lot less mainstream than Rails or Django, having models of how people integrate different libraries etc is hugely helpful! Best of luck
Reddit and SO use different flavors of markdown though.
Thanks for the comments. makeBid is a bit cludgy. I'm using 'Clubs' as a suit, but 'Club' as a bid constructor. A better solution might be to just use data Bid = Trump Suit Int | NT Int | Pass Yes, state will be needed at some point as the hands change during play. Will look into State + FRP.
I don't think that refactoring of `printDeck` is quite right. Perhaps this, instead: printDeck = shuffleDeck &gt;&gt;= printHands . dealHands Your suggestion to use `type TableHands = Player -&gt; Hand` is a delightful one.
Thanks for the feedback. As you can see, I'm still an imperative programmer at heart. I'm not sure how type TableHands = Player -&gt; Hand would be used. Is it a function? 
As unlikely as it sounds in this case, you might also want to think about replacing `[North..South]` with something more friendly to refactoring; e.g. this will break if he reorders the constructor declarations in the definition of `Player`. The simplest would be to add `Bounded` to the deriving clause: data Player = North | East | West | South deriving (Show, Eq, Ord, Enum, Bounded) Then you can use `[minBound..maxBound]` instead of `[North..South]`. I also can't help but give a plug for [universe](http://hackage.haskell.org/package/universe); to use it, you would additionally add instance Universe Player and then you could use `universe` instead of `[North..South]`. If you add `Universe` (and `Finite`) instances for your other base types, you could also replace `fullDeck` with `universe` (or `universeF`) everywhere. It's a bit more boilerplate, but it's less actual code (yay!).
Yes, that is correct. It is a function that takes in a Player as input and gives back as output a Hand. 
Great! I didn't know about universe, it will come handy very often.
About a handful of times every year, I get a stack overflow, go "oh, right", add a strictness annotation, and carry on. Is this supposed to negate the huge advantages I get with Haskell?
This looks like good work! My one comment is that you seem to be relying a bit much on pre-built data structures. For example, a `Hand` could be represented as data Hand = Hand {clubs :: SuitHolding, diamonds :: SuitHolding, hearts :: SuitHolding, spades :: SuitHolding} This gives you a static guarentee that the hand will be divided into suits, and allows easy access to each part, while also automatically creating the `clubs`, `diamonds`, `hearts`, and `spades` functions. Similarly, `TableHands` could be defined as data TableHands = TableHands {north :: Hand, east :: Hand, south :: Hand, west :: Hand} This approach has additional advantages in terms of type classes. Instead of writing `showHolding`, you can write `instance Show SuitHolding`. Instead of writing `showHand`, you can write `instance Show Hand`. The last, and arguably most important advantage of custom data structures is that they prevent bugs early. A `Hand` should always be sorted, but there is nothing in the list structure that enforces that. In fact you could accidentally pass a `Deck`, which is almost certainly not sorted, into a function that expects a `Hand`. Similarly, all the cards in a `SuitHolding` should have the same suit, but there is nothing in the type that enforces that, and a full `Hand` could be passed in where a `SuitHolding` was expected. In summary, I'd use the following data structures: data Hand = Hand {clubs :: SuitHolding, diamonds :: SuitHolding, hearts :: SuitHolding, spades :: SuitHolding} data Card = Card Suit Rank newtype Deck = Deck [Card] newtype SuitHolding = SuitHolding (Set Rank) -- You never actually use the suits of the cards data TableHands = TableHands {north :: Hand, east :: Hand, south :: Hand, west :: Hand} Besides this point, I think that your code looks very nice and modular. I have a few minor quibbles that don't matter too much: * I like your idea of using a `Trump` constructor to simplify the `Bid` type. * The ordering of the `Rank` type seems backwards. I though that face cards and the ace were the highest ranks, not the lowest, though I don't actually know bridge well. * The name `sortByLength` seems misleading. To me, that implies a function of type `[(Suit, Int)] -&gt; [(Suit, Int)]` which sorts by the length part. Instead, you might call it `compareByLength`. * I don't quite understand what the `FiniteWeightedList` is for, but you could make it parametric in the key type - there is nothing in that code that requires it to be a string, and you might want to use a more exact type as a key later.
Sorry if that is confusing. I encounter the following problem when trying to install `curl-aeson`: ~$ cd ~/devel/haskell/rascal rascal$ cabal configure Resolving dependencies... Configuring rascal-1.1.4... cabal: At least the following dependencies are missing: curl-aeson -any rascal$ cabal install curl-aeson Resolving dependencies... In order, the following would be installed: attoparsec-0.10.4.0 (reinstall) changes: text-0.11.3.1 -&gt; 1.0.0.1 blaze-builder-0.3.3.2 (reinstall) changes: text-0.11.3.1 -&gt; 1.0.0.1 hashable-1.2.1.0 (reinstall) changes: text-0.11.3.1 -&gt; 1.0.0.1 unordered-containers-0.2.3.3 (reinstall) aeson-0.6.2.1 (reinstall) changes: text-0.11.3.1 -&gt; 1.0.0.1 curl-aeson-0.0.3 (new package) cabal: The following packages are likely to be broken by the reinstalls: yaml-0.8.5.2 pandoc-citeproc-0.2 hakyll-4.4.2.0 pandoc-1.12.2.1 ... zlib-conduit-1.0.0 lens-3.10.0.1 conduit-1.0.9.3 linear-1.3.1.1 case-insensitive-1.1.0.2 Use --force-reinstalls if you want to install anyway. rascal$ So presumably it is my own dependency management that is the issue. I am going to attempt to fix this (messing around with `cab` for true package management), because I would like to see your code at least run :)
I unfortunately can't help, I'm also lost by cabal's dependency management (or lack thereof). I have to --force-reinstall half of my packages on a regular basis :'( If you find a satisfactory solution, please tell me!!! ^ ^
&gt; I don't think that refactoring of printDeck is quite right. You're right. It should be printDeck = (printHands &lt;=&lt; (return . dealHands)) =&lt;&lt; shuffleDeck at which point it starts to lose some of its intended niceness. Your version is indeed equivalent, though I always try to make the data "flow" in the same direction in any given defintion, in this case right to left. &gt; Your suggestion to use type `TableHands = Player -&gt; Hand` is a delightful one. Ah thanks!
Indeed, I found a solution. Install [`cab`](http://www.mew.org/~kazu/proj/cab/en/), a wrapper for dependency managment using `cabal` and `ghc-pkg` that blends the two into something of a classic package-manager. Then do something like $ cab check And it will spit out a message like the following: The following packages are broken, either because they have a problem listed above, or because they depend on a broken package. pointedlist-0.4.0.4 hint-0.3.3.7 snap-core-0.9.3.1 http-types-0.8.0 case-insensitive-1.0.0.2 socks-0.5.1 pem-0.1.2 system-filepath-0.4.7 hashable-1.2.0.10 enumerator-0.4.19 concrete-typerep-0.1.0.2 unordered-containers-0.2.3.1 One thing to do here is just remove them. You might not need them at all! First check the reverse dependencies on a package: ~$ cab revdeps -r pointedlist 0.4.0.4 ~$ The empty response means there are no reverse dependencies for `pointedlist`. Now we check to see if there is another version of `pointedlist` around, just to be sure we are not breaking anything we use ourselves: ~$ cab installed | grep pointedlist pointedlist 0.4.0.4 Ouch! Only one copy of `pointedlist` around, but at least it's not being used. If we want to fix the package, we can check out the info available: ~$ cab info pointedlist * pointedlist (library) [...] Versions available: 0.2, 0.3.3, 0.3.4, 0.3.5, 0.4.0.1, 0.4.0.2, 0.4.0.3, 0.4.0.4, 0.6 (and 7 others) Versions installed: 0.4.0.4 [...] So there are updates around, cool. Let's grab the latest one and delete the old version (since no packages use it, anyway): ~$ cab install pointedlist 0.6 [...] Installed pointedlist-0.6 ~$ cab delete -r pointedlist 0.4.0.4 Deleting pointedlist 0.4.0.4 Anyway, you can continue like that, killing old packages, or you could just build in a sandbox. `cab` has nice support for this that doesn't require any extra configuration. Just `cd` to the proper directory and run `cab init`: ~$ cd devel/haskell/rascal rascal$ cab init Writing a default package environment file to ~/devel/haskell/rascal/cabal.sandbox.config Creating a new sandbox at ~/devel/haskell/rascal/.cabal-sandbox Then, just like with `cabal`, do `cab configure`: rascal$ cab configure Resolving dependencies... Configuring rascal-1.1.4... cabal: At least the following dependencies are missing: aeson -any, ansi-terminal -any, curl -any, curl-aeson -any, mtl -any, process &gt;=1.2, vector -any Now we build these packages in the sandbox: rascal$ cab install aeson ansi-terminal curl curl-aeson mtl process vector And configure/build again: rascal$ cab configure Resolving dependencies... Configuring rascal-1.1.4... rascal$ cab build Building rascal-1.1.4... Preprocessing executable 'rascal' for rascal-1.1.4... [1 of 6] Compiling Paths_rascal ( dist/build/autogen/Paths_rascal.hs, dist/build/rascal/rascal-tmp/Paths_rascal.o ) [2 of 6] Compiling Rascal.Conf ( src/Rascal/Conf.hs, dist/build/rascal/rascal-tmp/Rascal/Conf.o ) [3 of 6] Compiling Rascal.Constants ( src/Rascal/Constants.hs, dist/build/rascal/rascal-tmp/Rascal/Constants.o ) [4 of 6] Compiling Rascal.Utils ( src/Rascal/Utils.hs, dist/build/rascal/rascal-tmp/Rascal/Utils.o ) [5 of 6] Compiling Rascal.Types ( src/Rascal/Types.hs, dist/build/rascal/rascal-tmp/Rascal/Types.o ) [6 of 6] Compiling Main ( src/Main.hs, dist/build/rascal/rascal-tmp/Main.o ) Linking dist/build/rascal/rascal ... rascal$ Awesome - a working development sandbox, plus sane Haskell package management from the start!
Yes, instead of storing a list of `(Player, Hand)` pairs you can just store the function that maps a `Player` to his/her hand. Still, this may be a little too clever for its own good. A `Data.Map Player Hand` would also be a nice approach. 
I don't know what that is, but it sounds serious.
Yes, it's missing bids at the 2-level. Beginners (like myself) normally use strong 2 openings and then go on to weak 2s.
Looks nice, but most of this is already available from cabal (like `cabal sandbox init`). I guess I should get used to use sandboxes, but my main problem was twice the same package, same version, but built against different dependencies, one from my GHC install, the other in my user-based cabal installation…
Right, `cab` is just a wrapper for `cabal` and `ghc-pkg` that makes it a bit more sensible to use. The main technical benefit is just recursive deletion of packages. Regarding distribution-provided Haskell packages versus Cabal packages, my recommendation for developers (after a bit of experimenting, and doing some wacky stuff like setting up an auto-deploying Hakyll installation using Git) is to install GHC via your distribution's package manager, and keep everything else local to cabal in your home directory. For some more reading, check out [this post](https://bbs.archlinux.org/viewtopic.php?pid=853041#p853041) on the Arch BBS. Even if you're not using Arch, the general advice still applies.
I like the idea of using the type system to enforce some of the assumptions I'm making. Yes, the name `compareByLength` is much clearer. I originally had the ranking going from `Two .. Ace` but then ended up using `reverse` in a quite a few of the functions. Hlint doesn't like code using reverse much, so it made sense for me to reverse the constructors. FiniteWeightedList is there because if you look at the distribution of random bridge hands, most (85%+) of the bids are 1C, 1D, 1H, 1S or 1NT. Some of the rarer bids only occur 1% of the time. For a tutoring program I wanted the rarer bids to occur with the same frequency as the common bids. FiniteWeightedList ensures that the distribution matches a given set of weights. I did look to see if there was something similar already for Haskell but couldn't find anything.
That looks suspiciously much like Arrows. Edit: Please, stop upvoting this, I forgot ProductProfunctor don't have composition.
ʘ‿ʘ
Well you already have code that opens strong 2 clubs already, that's the beginning of a weak 2 system right? :)
[Arrows are the strong monads in the category of profunctors](http://takeichi.ipl-lab.org/~asada/papers/arrStrMnd.pdf) 
Is the paper different from the one last year? I only skimmed both, but it looks very similar.
You can go from functors to typeclasses and back, but the translation is not necessarily nice to use.
Absolutely! As I wrote in the blog post, this is really just a temporary release to make sure people are aware of the renewed project and stop attempting to use the old versions. The module names will be sanitised when we split the package.
Then throw in existential types and you've got the essentials of a module encoding.
Wehr's (and others') work compares and contrasts Haskell's type classes with ML's modules in terms of organizing code, i.e., terms and types. The main tensions are 1) the different treatments of abstract types (which you can see this paper addresses by introducing a new kind of scoped type abstraction to Haskell) and 2) the total lack of implicit module resolution in ML. With Backpack, on the other hand, we do not at all investigate the question of "type classes or modules?" Instead, we start with the understanding that Haskell type classes accomplish a lot of what ML modules accomplish, and vice versa, so we look at how to otherwise incorporate ideas from the ML modules literature into a design for a new kind of modularity in Haskell (which organizes whole Haskell modules rather than Haskell terms and types).
I think this sounds about right: data Bid = Trump Suit Int | NT Int | Pass | Dbl | ReDbl | YellAtDirector Might as well go for completeness! (Oh, it would be so beautiful if we abstracted convention cards into a type and then implemented a bidder that bid according to a convention card)
It's a great interface, but if you make it the implementation and then you modify it you run the risk of holding onto more memory than you need to.
No doubt, but it's fascinating how close you can get.
You can get arr from "profunctor" and "category": arr f = rmap f id = lmap f id Where `id` is of course the arrow, not the function.
Lisp doesn't even have currying.
Does the fact that it's accepted mean we can expect to see this in GHC by 2016?
read is partial. From the context it looks like it's being used for command line parsing, and if the user gives a bad input the only message will be something like &gt; Prelude.read: no parse which might mystify you even if you wrote the program yourself. Check out readMaybe, which is a recent addition to the standard library (import Text.Read, I think) or if your command line parsing needs are anything beyond trivial you will want to look into one of the libraries that does this; optparse-applicative is pretty good for relatively straightforward needs.
FWIW, https://github.com/clojure/core.typed/wiki/User-Guide There aren't many languages whose documentation apologizes for not being Haskell, but Clojure is one. They are trying to bridge the gap.
It's not *all* untyped... http://www.reddit.com/r/haskell/comments/1u9icu/a_new_fp_podast_has_started_functional_geekery/ceibiga 
Similar libraries exist for other Lisps, too.
&gt; What you’re actually doing is giving Haskell a puzzle (x = x + 1) and saying “Go find me a solution”. Mathematically speaking, there are only 2 possible solutions to that puzzle: ∞ and −∞. So when you saw it hang, Haskell wasn’t merely taking its time – for no good reason – before giving you back the value 2. Instead, Haskell’s runtime was trying its hardest to give you a correct result by taking every integer value it could think of, one by one, and checking whether it was equal to its successor. This is wrong on every count that would matter. If you wanted to get all domain-theoretical about it, you could say it's "trying to find as solution" to `x = x +1` but even then, definitely not in the arithmetic sense...
Great idea for a post! Quality documentation on hackage is fantastic when it exists. Now, off to fix my package documentation :P. EDIT: [done](http://hackage.haskell.org/package/atom-msp430-0.5.3/docs/Language-Atom-MSP430-Compile.html) :).
I really doubt it. When I had attended a talk given by the author, a little less than a year ago, he hadn't yet really considered typeclasses, and they're still listed as future work in the paper. Type classes end up adding a lot of complication to the module system, particularly because of orphaned instances. edit: a lot of the talk in the [thread about backpack from 5 months ago](https://pay.reddit.com/r/haskell/comments/1id0p7/backpack_retrofitting_haskell_with_interfaces/) is about the problems that typeclasses pose to backpack.
All great points. Lowering the barrier for entry for littler races is one of my motivations. I like the idea of using streetview too. Chip timing for small races is a hard one to crack but it'd be awesome. Thanks for the feedback!
Thanks so much!
Did you omit an `Applicative (p (a, a'))` constraint in the type signature of `(***!)`? (The first one.)
I second this idea, it might also be worth looking at HPX, https://github.com/boostcon/cppnow_presentations_2013/blob/master/tue/managing_asynchrony_in_cpp.pdf?raw=true
I just keep picturing a player getting sucked into a card dealing machine and hands coming out. This repeating over around the table until there are no players left... only hands.
Safari just gives me: &gt; Apparently, Chromium and Google Chrome has a bad entry in the DNS cache which cause them to resolve to a bad page. Please refresh the page several times until the good DNS is back. Sorry for that. But I'm not using those browsers :(
Can someone explain to me how why we can't use typeclasses as interfaces?
I found netwire 5 disturbingly without docs, if thats related.
Ah yes, very nice. 
Yes, thank you!
There exists a form of profunctor composition: http://hackage.haskell.org/package/profunctors-4.0.2/docs/Data-Profunctor-Composition.html but I don't know how it differs from regular arrow composition.
Yeah, but that's composition of profunctors, not composition of the values whose type are profunctors applied to stuff.
Does it work on iPad mini?
They seem to be somehow related: procomposed :: Category p =&gt; Procompose p p a b -&gt; p a b It seems that, it your profunctor is a Category, you can move from Profunctor composition (when the two profunctors are of the same type) to Category composition. It's as if Profunctor composition was more general, in some sense. You can compose profunctors of different types, unlike with Arrows.
Hi! Do you think you can submit a patch to the GHC devs? This recursive plusInteger business surely looks like some low hanging fruit 
Note that the ML module community claims that system-F style (or Odersky-Läufer) existentials are *less modular* than ML modules. They give you abstract types, but they force you to explicitly flow the definition of the types to the toplevel of the abstraction, which can become a significant burden on a large scale. See [this short](http://lambda-the-ultimate.org/node/4865#comment-78106) message by Andreas Rossberg, or the work on [Open Existential Types](http://gallium.inria.fr/~remy/modules/Montagu-Remy@popl09:fzip.pdf) by Montagu and Rémy.
Well, profunctor composition is to categorical composition as products are to monoid composition.
Ianal, but imho you're violating the license of phash by distributing your bindings under the MIT license. phash is GPLv3, and your binding, being a derived work, must be distributed under the same terms.
You can't use typeclasses to do that, what you can use for poor man's modules are existential datatypes.
Do any of you know of an introductory book on programming languages and type theory? I know "Types and Programming Languages" is standard, but it's a bit of a tome and kind of overwhelming to deal with while I'm still in school taking classes. I want to invest some time into learning some of the core concepts, but looking for a slimmed down book. I would like to be able to read papers like this more easily. Any suggestions?
A good general text would be Bob Harper's recent "Practical Foundations of Programming Languages", available in print and online: http://www.cs.cmu.edu/~rwh/plbook/book.pdf If you want to understand module systems, you should check out the syllabus for a "type systems for modules" seminar that my advisor, Derek Dreyer, held a couple years ago: http://www.mpi-sws.org/~skilpat/modsem/
Thanks a bunch :)
How about ghc-mod? How about not needing vim-hdevtools? Maybe I should just work on making the latter less intrusive...
I've implemented a version of 'findsymbol' for 'ghc-mod' (https://github.com/dan-t/ghc-mod), but it's too slow, because the reading of the package database takes a few seconds and therefore it's not very usable for interactive usage. The overhead you're getting for the first call of 'hdevtools findsymbol', is the overhead your're getting for every call of 'ghc-mod findsymbol'. I've added a new command to 'hdevtools' so it's quite sensible to extend 'vim-hdevtools' to add a binding for this command. At the end I want to get my changes of 'hdevtools' and 'vim-hdevtools' merged back. 
May want to use [standalone-haddock](http://hackage.haskell.org/package/standalone-haddock) to build the docs you upload manually. This way cross-linking will kinda-sorta work. Though not sure if you'll need to post-process the result to get Hackage to accept the upload.
My problem with hdevtools is that it doesn't support sandboxes and remaps standard keys in Vim with versions that rarely work. It also can't replace ghc-mod completely for me since the latter has a bunch of more features. I'd love to be able to though; hdevtools is in deed much more responsive.
I'll do more tests, but after talking to people, things are more complicated unfortunately. For one, if you remove the `NOINLINE`, GHC will actually optimize the recursive code into the non-recursive code. So there's no problem in that case. I'll have to check if the non-recursive code is faster with the `NOINLINE` in case. For two, the `NOINLINEs` are there on purpose, and it seems to be a wontfix. integer-simple actually has them, too, and according to my tests, there's essentially no difference between integer-gmp and integer-simple on machine-sized integers. If there is a difference between integer-gmp and this new implementation, it wouldn't surprise me if it's due to inlining. But, the new implementation is going to have to add `NOINLINE` to all its functions. Apparently inilning causes too much code blowup. I'm not really sure what integer-simple is faster at. Maybe small-but-larger-than-machine-sized integers. And maybe it's FFI overhead, or maybe it's the extra function call on off-diagonal cases. I'll have to test more to make sure. 
Technically SO just uses a javascript library to pull off its syntax highlighting. However since subreddits can't add javascript it can't be duplicated here.
Maybe I'll give it another shot over the summer or something -- I picked it up with a lot on my plate past year and maybe that wasn't so conducive to learning.
Previous discussion http://www.reddit.com/r/haskell/comments/1id0p7/backpack_retrofitting_haskell_with_interfaces/
Sorry, I fail to see how copy-pasting his code to create my own bindings for another library (possibly MIT-licensed) is "linking to libphash". So he can provide me with his code and tell me it is licensed under MIT. For my example use case that would be perfect; I don't need to know libphash exists, and I don't need to link against it. Now you're right in the case I write a program that use his bindings, *and* that I link my program against libphash. But this is not the only useful value of his source code.
Whether or not you compile the code is not a special case, that's not how the GPL works. He must license under the GPL because his code uses libphash: https://github.com/MichaelXavier/phash/blob/master/src/Data/PHash.hs#L25 Now, I would argue that it's a mistake to make pHash GPL instead of LGPL for exactly the reasons you specify, but that's the way it is.
Oh cool! MSP430 stuff! I did that in one of my introductory CS courses. 
Damn. I was asking in #haskell yesterday about this very issue and the folks I talked to thought it was alright to use MIT for this since I was not redistributing source, but just providing bindings. I myself am not very well versed in licensing which is why I pretty much just license everything as MIT. I was under the impression that people using my library would need to use GPL if they actually linked to libphash using my code, but that my code itself could be MIT. Judging by that FAQ it seems like I need to make my code GPLv3. I'll get on that today.
You certainly do not need to make your code exclusively GPL. If you want people to be able to modify your work in order to use alternative libraries (no idea if they exist, but hypothetically), then you can dual license your code under GPL and MIT, allowing the end-user to make the choice. Anybody using your code against libphash will need to distribute their code under the GPL, but anybody who forks your code and modifies it to work with a different library could take the MIT option. It's an option, anyhow. Personally, I'm not actually sure that you need to GPL your work at all. The link given by LambdaBoy explicitly says "GPL or a GPL-compatible license", and http://www.gnu.org/licenses/license-list.html#X11License says that the MIT license (X11 license) is compatible. I believe that you're fine how you are, but if you do feel the need to explicitly name GPL, you can still do GPL/MIT. 
Contact the software freedom law center if you have more questions: https://www.softwarefreedom.org/ It can take them a while to get back to you, but they will respond eventually.
Here are the others: * [Divide and Conquer 1/2](http://youtu.be/Ijni9-Y8jik) * [Divide and Conquer 2/2](http://youtu.be/s4Tvia-dYrM)
The Pearls books is one of the most compelling examples of Haskell programming I've found for introducing newcomers to Haskell. I ran through the Sudoku solution with some people the other day and it went very smoothly. What are you using for your Vim setup by the way?
Thanks for this. The next chapter has Theo, Anne, Mary, Jack, and a Teacher hashing out a functional pearl. As long as you're making videos, it could be fun to see this scene acted out.
Cheers!
Closest thing is probably ajhc, it has a comparatively much smaller runtime.
You can get around using the Array (which is just an elaborate encoding of a bitwise defined Int anyway) using Bits, yielding this solution: import Data.List import Data.Bits accumBits :: [Int] -&gt; Int accumBits = foldr (flip setBit) 0 first0 :: Int -&gt; Int first0 n = go 0 where go i | testBit n i = go (i+1) | otherwise = i minFree :: [Int] -&gt; Int minFree = first0 . accumBits which is equivalent to the video solution, at least for QuickCheck-sized lists. ([Full source paste](http://codepad.org/MZ8hCncU)) (The takeWhile business can be handled with an added `filter/takeWhile` before `accumBits`, which is probably a good idea when you want to use numbers larger than your int size.)
I'm VERY far away, but I'd like to know: what's your Haskell web stack like?
Just the language, no batteries (the C app its embedded in provides those). I accept that unlike Lua, calling Haskell again within a registered function would not be supported and would produce non-Haskell, broken undefined behaviour. I'll accept whatever limitations required to make this work. Edit: I just want the functionality of the ghci interpreter in my C app, with the ability to add callbacks. 
It is amazing, all of it. Just wowed by lens every time I use it. 
Note the amendments made to the instructions in the followup: http://fuuzetsu.co.uk/blog/posts/2014-01-06-Hackage-documentation-v2.html
Do you guys use third party JavaScript libraries? (Angular, jQuery, Backbone, etc) Do you have anybody that knows JavaScript well? (and more pointedly why did you choose the haskell-&gt;JS route?) How much JS are you compiling? Is it minimal or are you building out full dynamic apps?
Free monads are abstract syntax trees. Their algebras are interpreters.
Maybe link to Hugs?
We use jQuery. I'm not sure we have any actual JS experts, but we have a few people who are quite competent with it; I've been generating JS from Haskell since 2010 (but I'm certainly not a JS expert). We chose to generate JS mostly because maintaining Haskell (or something similar to Haskell) is much easier than maintaining JS. Also things like Haste and GHCJS offer the hope of one code base for various backends (though I don't think this is really feasible for non-trivial projects yet). We are generating a non-trivial amount of JS with Haste, and a much more modest amount of JS with jmacro. The jmacro code sits between a more traditional HTML/JS UI and our Haste generated business logic.
Cool, thanks for the response.
Regarding the order of `Rank`, you're automatically deriving `Ord`. Do you realise that in your scheme `Two &gt; Ace = True`?
&gt; profile (so we are optimising the right thing) and write benchmarks (to check our changes make things go faster). &gt; For this article, we're going to skip all that, and only look at the generated Core, C-- and assembly - making guesses about what should go faster. That can be a big mistake. As comparing asm “inner loops” ignoring the ~~rest~~ the parts where it jumps. 
Are they? Or can they be (amongst other things)?
I forgot about Hugs! It looks like the language and interpreter are implemented in C, so I'll see if I can embed it. 
That trick only works if you start Vim from the project root and only use that Vim instance for that one project. Getting it to work in the other cases will be trickier because you'll basically have to reimplement hdevtools's code for per-project sockets. It would be easier if there was a buffer-local `hdevtools_options` variable, but really it should just be added to hdevtools proper ...
Not being snarky - can anything be done to correct the misunderstandings here?
Yes, it's a fatal mistake. Unfortunately, profiling the bits where it jumps to can be very difficult: http://stackoverflow.com/q/20881177/160673 I've revised the text a bit to make it try and dissuade people who come after from skipping those vital steps on the grounds that "some blog post skipped them": &gt; Before making things go faster we should write test cases (so we don't break anything), profile (so we are optimising the right thing) and write benchmarks (to check our changes make things go faster). To write this post, I did all those steps, but the post is only going to look at the generated Core, C-- and assembly - and be guided by guesses about what should go faster. 
So /u/thegoodmachine is a useless troll but I can vouch for the aleph cloud guys being some of the brightest engineers in the valley. If I wasn't already "gainfully" employed is jump at this. 
Contact the author?
No I didn't realise that. Thanks for pointing that out.
It's a great little MCU, and so cheap! Work on the library has been slow of late, though...
For the example in the blog post, yes, if you want to predict the assembly code, use C. However, that example is really just the tight loop in a lexer inside a much bigger program, and I don't want to write the rest of the program in C. If it was easier to include a C snippet inside a Haskell file (e.g. no separate C FFI) I would have probably gone that route straight away. The other advantage of sticking to Haskell is that while the last version of break is quite low level, actually calling the break function is pretty safe and simple. We've wrapped the scary bits into something that can be reused easily. In C, to get the efficiency, we wouldn't be able to add the nice layers of abstraction.
I appreciate it, just as I am about to start implementing cabal-install functionality.
Somehow I have the feeling that there is more of a sub-type relation between free monads and ASTs than an equivalence relation, but I'm no expert in these definitions. I can imagine performing other computations than building ASTs with free monads, and even more I can imagine building ASTs without the use of free monads. Therefor statements like 'Free monads *are* abstract syntax trees' confuse me. Can you explain what kind of reasoning goes into saying they are the same thing? Edit: you're previous comment nicely explains how to get to an AST using the constructs from a free monad, but don't really convince me the terms are equivalent.
(From memory...) In Ralf Hinze's Backtracking monad paper - he used a "term representation" of a monad i.e. an AST, but it wasn't a free monad - free monads use a fix-pointy construction similar to the Resumption monad. I'd contend the terms are not equivalant.
there seems to be a comment-section right below the post ...
&gt; If it was easier to include a C snippet inside a Haskell file (e.g. no separate C FFI) I would have probably gone that route straight away. Henning Thielemann has used the LLVM for this purpose. In the [synthesizer-llvm][1] package, he generates LLVM code on the fly and calls it from Haskell. [1]: http://hackage.haskell.org/package/synthesizer-llvm
This was posted yesterday and deleted after picking up a couple of similar negative comments. Odd it came back - still a mix of the odd and the wrong 
Real FFI noob question here : in a realworld big project like yours, how involved is it to write a few looping functions in a separate C file and use them from the haskell sources ?
I didn't realise that. I checked first if it had been posted but it didn't show up.
I'd also take into account https://www.gnu.org/licenses/gpl-faq.html#GPLModuleLicense 
How would this break function work on files which contain a zero byte?
Ha! Yes, you're completely right.
Despite thinking the link is interesting and guessing that many other Haskellers will agree, I am compelled to ask: is this appropriate for r/haskell?
It's not too hard. I tend to avoid it partly out of laziness and partly because I prefer to stick to Haskell. If this was a major bottleneck in something I really cared about I wouldn't hesitate - as it is, my code is already 3x faster than the competitor C code (in the real Ninja project), so I'm not too fussed.
just out of curiosity, what motivated the happstack decision?
A related monad to the free monad is the free completely-iterative monad. The difference is betwen _μ_`x. a + f x` vs _ν_`x. f (a + x)`. When you want to admit cut, what you really want is something like the latter. If we ignore the mu vs. nu distinction, then you just get is that `Iter m a = m (Free m a)`. This monad gets reinvented a lot. 
It should now be cabal-installable and I am in the process of uploading it to Hackage. Edit: http://hackage.haskell.org/package/inquire-0.1/candidate
reddit already has a nice way of dealing with this. Personally I thought it was pretty interesting. No haskell code in there, but I was already thinking of some while reading it.
Mostly variety... in the words of the great Thelonious Monk, "You can't wear the same hat everyday." We're also interested in trying out the pipes backend for happstack whenever it is released. 
If you had used `Integer` instead of `Int` as a storage, you would get flexible sized bitstrings.
How expensive is that `ByteString.snoc` operation? Could the implied `memcpy` be more expensive than maintaining the length in the inner loop? EDITED TO ADD: If it turns out that the `snoc` operation is expensive, you could eliminate it while still eliminating the need for a boundary check in the inner loop. First, find the final separator's address by working backward using a loop that does boundary checks. Now you can split the rest of the input as before, by working forward *without* boundary checks, until you reach the final separator's address, which you can test for in the outer loop.
Wait, did his post disapear? Can't see anything. (But yeah, that guy is baiting on every post ) edit: great, his post doesn`t appear on this topic, only in /u/ page.
Pretty trivial, in my opinion. IF you're already knowledgeable about C. The FFI docs kinda assume you know how linking, memory, etc. work in C. I started messing around with the FFI with a healthy amount of C experience and I immediately felt at home. If you already use cabal, linking C code is literally a matter of specifying files/linker flags in your .cabal and it works. Personally I've used the FFI several times just because it was trivial to write in C and doing it in Haskell was more work than the FFI. The only real annoyance happens when you want to access C instructs from Haskell, specifying a Storable instance can be a pain. If you don't need structs it's pretty pain/annoyance free.
I've just pushed changes to 'vim-hdevtools' and 'vim-hsimport', so it should work now. Please also see the updated README of 'vim-hsimport'. Well, I don't know if it's such a good idea that 'hdevtools' should support this out of the box. I don't like too much auto-magic. Currently you can give the socket file and the package database as arguments to 'hdevtools'. 
Type classes is the next thing on the agenda. I completely understand why some think that the merely tentative support for type classes weakens our argument that we "retrofit" Haskell but, well, you gotta start somewhere. Note also that, despite the title, this whole system was designed with any "weak" module system in mind, not just Haskell's. As for a GHC implementation, there's hope but no concrete plan. Like Ed Kmett said in the other reddit post, this is going beyond what one PhD student -- without GHC expertise -- can reasonably accomplish. And personally I'm interested in coming up with a practical design more than hacking it into place; other people are much, much better qualified for the latter than I am.
Nice, that is cleaner. I'm sure there ways to one-up all of the solutions in this book with little constant time improvements - there are no strictness annotations, UArrays, STM (well, that's cheating), etc. But the important part is the O(n) solution vs. the naive O(n^(2))!
Oh, I didn't hear about Tim. Great hire!
Yes, but `Bits` is closely tied to `Int` type, and I wanted to avoid the `fromIntegral` noise.
It's O(n), not O(n log(n)).
whoops, it's been a while since I made this
Maybe OP's hoping for a library to come out of this?
Yeah, to be honest, this is off-topic, but thanks a lot for posting!
what is `return` for that?
and /r/musicTheory (i've seen threads about Schenkerian analaysis, but never atonal analysis over there
The problem with C/FFI is: it doesn't get inlined.
Yes, especially for people like me who only visit r/haskell.
&gt; Free monads are abstract syntax trees. People keep making this analogy, and I found it really, really confusing when I tried to figure out free monads. I think a better analogy is this: free monads are a particular normal form representation of programs, which eliminates some meaningless differences between ASTs. Recently I was ~~explaining~~ motivating free monads to somebody familiar with Lisp in the following terms. Lisp programmers often write macros and interpreters, and in these languages there are two types of construct that appear over and over: 1. **Binding forms:** expressions analogous to lambda or let. `with-open-file` macros, pattern matching macros, etc. 2. **Sequencing:** expressions that evaluate their subexpressions for their side effects in sequential order. These are often implicit inside the body of binding forms. An s-expression based language, as a Lisp programmer would implement it, allows us to write pairs of expressions that every informally "correct" interpreter would treat the same way. Two examples (in Scheme): ;;; Sequencing: these three should have the same value and side effects. (begin a (begin b c)) (begin (begin a b) c) (begin a b c) ;;; Binding: correct interpreters treat these two alike as well. ;;; (This should look familiar!) (let ((y (let ((x m)) (f x)))) (g y)) (let ((x m)) (let ((y (f x))) (g y))) What free monads do is give you a normalized representation that cannot express the difference between the ASTs in each of those two groups—it maps all of them to the same value. Therefore, with free monads it's impossible to write an interpreter that fails the informal correctness criterion I gave for those expressions—there is no difference it can observe! So here's what's confusing: which representation are we calling "the AST"? Most people, and in particular the people unfamiliar with free monads, will refer to the non-normalized representation as an "AST," and expect there to be a straightforward mapping between the surface syntax of the language and the AST, where nodes of the AST correspond roughly to expressions of the language. Free monads just don't generally work like that, which is why I find the analogy unhelpful. For example, there are examples where the functor is a function type: data PromptF next = Say String next | Prompt String (String -&gt; next) The intuitive idea that with free monads one can "inspect the program's AST" (which Tekmo has verbalized in this discussion already) falls apart as soon as you inspect a `Prompt`—at that moment, the rest of the program is completely opaque until a `String` is supplied! (Somebody once told me to think of `String -&gt; next` as an "infinite branch," but that just doesn't help.) And again, when somebody who doesn't already understand free monads hears the term "AST", they expect that it provides the ability to statically inspect the whole of the program.
He didn't mention it, but the second is only a monad when `f` is a monad. `return` is `return . Left` and `join` uses the underlying `join` to merge the two adjacent layers at the point of collapse (if applicable).
This potentially scratches and itch I've had for a sometime. I've been looking for an example of applying Haskell code supplied at runtime to input. Looking forward to digging in. 
To evaluate Haskell code at runtime, we use the [hint](http://hackage.haskell.org/package/hint) library, which uses the GHC API to evaluate a string containing an arbitrary Haskell expression. In particular, it can evaluate a value of type `String -&gt; String`, which you can then apply to your input.
Places I've gotten stuck before. - Parsing input into strings and feeding each line to hint meant reloading hint for every line - Starting one Hint instance and then feeding it input always seemed to require pointing hint to a .hs file outside of the binary which was inelegant. Curious to see what happens in hawk 
Well, I'm not sure ASTs have an exact formal definition. So at a minimum we're going to run into the same sorts of issues that, e.g., the Church–Turing hypothesis runs into. There's also the issue of what ASTs *could be* vs what they actually are in practice. In practice we typically use context-free grammars to construct ASTs, and then perform post-hoc analyses (e.g., type checking) to prune out the bad trees. Using the construction above we can embed CFGs into free monads[1][2], and can translate free monads into CFGs. However, by moving to more powerful grammar formalisms, it's always possible to incorporate the post-hoc analyses directly into the grammar itself. Once we start doing this, it's not clear when the connection to free monads breaks down— or whether we must merely generalized to free indexed monads, etc. [1] There's a particular case where I worry that the free monad may over-generate with respect to the CFG, though I haven't confirmed whether it actually turns out to be a problem or not. [2] Another issue is how exact we want the translation to be. For example, TSGs and CFGs have the same weak generative capacity. But for ASTs we are more interested in the strong generative capacity, for which TSGs are strictly more powerful.
The approach I just told you about has none of those drawbacks. Try this in ghci: &gt; import Language.Haskell.Interpreter &gt; Right f &lt;- runInterpreter (setImports ["Prelude"] &gt;&gt; interpret "(+1)" (as :: Int -&gt; Int)) &gt; f 3 4 &gt; f 5 6 As you can see, I have only evaluated "(+1)" once, but I have called it twice. Also, the string "(+1)" and its inputs are both values inside my program, no .hs file required.
Very neat, will definitively try to work this into my workflow... The Unix tools are great, but whenever it gets a little bit more complicated, I can never remember all the different options... Seems like a very nice design with the word, and line separators (both in and out), and map or apply.
See Control.Monad.Trans.Iter in the free package. It borrows the return of the base `m`.
Is there also an intuitive way of understanding cofree comonads?
I've always imagined building a csv processor based on tibbe's cassava like this. Always consider it a tragedy that csvtool was in OCAML ;-)
Anytime you get this kind of problem (where commenting out your type signature allows the code to compile), comment out the type signature, load the module in GHCI (or Cabal repl), and do :t &lt;name of your function&gt;. That will tell you what GHC is expecting. I'm not entirely sure if this will help you. Sorry, I'm fairly new myself. EDIT: Apparently ["Haskell 98 prohibits class method types to mention constraints on the class type variable"](http://www.haskell.org/ghc/docs/7.0.4/html/users_guide/type-class-extensions.html). Not sure if this is still true in Haskell 2010.
Ordinarily, I would agree. However, the function in question is only defined inside a let statement and is thus not accessible in GHCI. In my actual code, GHC complains that it cannot infer the type of the function, which was why I was trying to specify the type in the first place. It's only my toy example that compiles fine when the type is commented out.
This seems like a weaker claim than the one I'm trying to make. This says that `t` is any type in the `NormalizedList` class; I really want it to be _the same_ type as whatever `normalize` was called on. Is there a way of telling Haskell that? Perhaps this fix is good enough and I don't actually need the stronger claim; I need to think further about that.
Note that the type variables in the signature for `divideBy` are fresh (unrelated to those `n` and `t` around it), you could have written: `divideBy :: (Fractional b, NormalizedList a) =&gt; a -&gt; b -&gt; a`. &gt; I really want it to be the same type as whatever normalize was called on. It has to be if you look at the types - `fromList` will give you back any instance of `NormalizedList`, but since the result of that is returned from `divideBy` and then that returned from `normalize`, the type is constrained by the return type of `normalize` to be the same. Essentially your type for `normalize` is guaranteeing this.
Ah, that's an excellent point! Thanks very much; this is quite helpful.
Why are you showing fibonacci as a recursive function when there is a valid substitute that runs in constant time: https://en.wikipedia.org/wiki/Fibonacci_number#Computation_by_rounding
Probably because it's a simple example.
These slides might inadvertently reinforce some common misunderstandings about functional programming, monads (in particular, IO) and purity: * Slide #3 implies that IO (or monads in general) are impure: "Impurity in a pure language: the IO monad". By the definition of "pure" given in #10, a function with IO in its type is pure (as long as unsafePerformIO isn't used). This suggests another definition of purity is implicitly being used here, but it's not clear what that is. From the definition in #10, it does *not* follow that programs used to perform i/o are necessarily impure, though this is a common misapprehension. * #16 implies that "Functional vs Imperative" is a dichotomy, or at least that they are opposing ends of some spectrum. But we can use imperative style in Haskell, for example with do-notation. This isn't at odds with the definition of "functional" from #10: it desugars to expressions and pure functions. * #40 says, "lots of useful things are not pure". Such as what? It would be helpful to distinguish between implementations (which could be pure code or not) vs the computations those implementations express, and point out that purity (as already defined) restricts *how* we implement things, not *what*. Without this distinction, such a statement could reinforce the misunderstanding that purity is at odds with useful applications such as performing i/o. 
Is the outcome of Dijkstra's petition recorded? Did utexas switch to Java?
Because that doesn't introduce anybody to lists or laziness. The point was to show them that FP is _different_, not to solve the problem of generating the Fibonacci sequence.
Zero-based page numbers were a nice touch.
As Porges said, the `t` in `divideBy`'s sig is understood as a fresh variable by default. You can change this behaviour to the more sensible (IMO) one you expected with the extension {-# LANGUAGE ScopedTypeVariables #-} (in the first line of the file).
is he for Haskell or for Java?
&gt; [...], in the specific comparison of Haskell versus Java, Haskell, though not perfect is of a quality that is several orders of magnitude higher than Java, which is a mess. I think this is the only weak argument in his petition. Although maybe true at this point in time (2001: Java 1.3), he just states his opinion about the two languages. I think, personal opinions are not helpful to convince someone to use a technology and should be saved for non-technical discussions.
did you read the 1½ page letter
Haskell was also very different back then. Whatever his opinion was, it must have changed a lot for both languages.
Shameless plug: you can use `pipes-concurrency` to easily avoid deadlocks. The way it works is really simple: every read or write can potentially fail and the library instruments the garbage collector to abort any reads or writes that would trigger a deadlock. Despite the name, the core machinery of the library is `pipes`-independent, so you can mix it with other streaming abstractions or even use it without a streaming abstraction at all. If that's not clear enough, there is a [really long tutorial](http://hackage.haskell.org/package/pipes-concurrency-2.0.1/docs/Pipes-Concurrent-Tutorial.html) explaining how it works in detail.
It had them when I learned the language in 1999, so it had them. In fact, Haskell 1.4, from what I've heard, even had monad comprehensions instead of specific list comprehensions. What a shame they got rid of that.
Of course they did. The corporations who have the ear of the universities value "practicality," by which they mean "people trained in exactly the buzzwords they want on candidate resumes." Why would you listen to your Turing Award winning professor over the people who donate huge funds to the university?
Not to say that the university made the better choice here -- either of what to do or who to listen to -- but I wouldn't grant any special merit to Dijkstra's opinions on how to educate students. In fact, his ideas in that department are downright wacky, and bad.
Infinite trees decorated with values at each node. Comonadic extension is relabeling each node by a function of its subtree.
Not as easily optimized in the list case though, IIRC.
I'm curious to hear why you say that.
The University of Oxford teaches Haskell as the first programming language in their CS course.
&gt; personal opinions are not helpful to convince someone to use a technology Depends on whose opinion it is. He's writing to give his expert opinion, not give an objective survey of the two languages.
I guess you mean you're curious what his opinions on education were? Dijkstra apparently believed that CS should be run like a math department -- and not a contemporary math department, but a pre-PC math department. He didn't like PCs, he thought first year students should not even program computers, but just write code on paper to be graded. Now, I could refute that idea in various ways -- most especially by describing how it would totally sap the motivation of the students most interested in programming -- but I take it that it's so self-evidently bad that I don't even have to explain why.
If it were a software engineering department then it would be self-evidently bad, but it's a computer science department where the goal is to teach students how to formally reason about programs.
The University of Chicago uses Haskell in the honors intro course and Racket in the standard one.
I'm actually pretty familiar with Djikstra's beliefs and wanted more to understand your misgivings. I could see an argument that Djikstra was too severe there, and also one to state that some of the most modern interactive programming environments provide useful hands-on lab experience, but I think CS is too young to state unilaterally that his methods are wrong. I'd believe in a heartbeat that his *goals* differ from most modern CS curricula and I'd want to see any argument begin there.
Reddit doesn't count downvotes on /u/ pages to prevent mass downvoting.
&gt; If it was easier to include a C snippet inside a Haskell file (e.g. no separate C FFI) I would have probably gone that route straight away. Yes, an "inline Cmm" feature (akin to inline asm in C/C++) would be nice.
&gt; he thought first year students should not even program computers, but just write code on paper to be graded Is that really such a controversial idea? I've come across many CS professors who share similar notions and there's a strong argument that we're graduating too many undergraduates who don't have a strong understanding of how the notion of logic and proof are central to CS.
if you add this extension to your code it will work {-# LANGUAGE ScopedTypeVariables #-} 
Imperial College London also teaches Haskell as its first language. My little brother enjoyed it very much, and was less keen on the Java classes that inevitably followed.
I'm not a programmer by trade, but by hobby. After being exposed to some introductory text (LYAH) and comparing the languge to the things I've learned in school (COBOL, C, etc..) I can say as a language Haskell is leaps and bounds ahead in all facets. All the normal languages are swamped in an enormous amount of boilerplate which makes them completely unfriendly to a newbie. In Haskell, I'm merely worried about my problem and just coding it out. Python has similar properties but doesn't prevent you from "doing bad things" with variables. I can see his point -- Haskell puts the newbie into the "problem solving" rather than "code typing" mode. I immediately was drawn into jamming through it and learning because I was already accomplishing something. To learn C or something like it by contrast (Java, C++) included you learn language mechanics before you can do anything at all, then you have to memorize their equilivalent standard libraries... after you are done you throw that out because no one does anything with the std libraries. :) Haskell has some other great properties in terms of algorithm optimization that are apparent even at my VERY basic level. Also, C/C++/JAVA require you to have a completely set up toolchain and know how to work that. 
Very nice. Here's a little example I came up with to count different file types (I added import Control.Arrow to my prelude.hs) find -type f | hawk -ao"\t" -d. "map (B.unpack . head &amp;&amp;&amp; show . L.length) . L.group . L.sort . map last" In this case you can do this with unix tools: find -type f | awk -F. '{ print $NF;}' | sort | uniq -c I can really see myself reaching for hawk when things get a bit more complex though! Nice work!
In what sense is that constant time?
&gt; there's a strong argument that we're graduating too many undergraduates who don't have a strong understanding of how the notion of logic and proof are central to CS There might be a strong argument that that is the case, but there is not a strong argument that that implies anything like Dijkstra's advocated approach to education.
Wow awesome! any article/study explaining their choice and/or the results obtained from it?
Doesn't Edinburgh do a lot of Haskell? Ironically given that it is the Glasgow compiler.
&gt; I'm actually pretty familiar with Djikstra's beliefs and wanted more to understand your misgivings. Oh, OK. Well, I think this kind of approach alienates the kind of people who enter CS because they want to accomplish things with computers in the world. It creates a situation where the authority of the teacher substitutes for the experience of connection between intrinsic motivation and the course material. "Do this, because I said so, and trust me that one day, it will be relevant." It's the same kind of education that results in kids in primary school asking why they have to learn this. At college age, the students will have a very different attitude, where they actually do trust that the material will have some relevance, but the experience of it is the same. In some fields, you really cannot get around this. If you are doing mechanical engineering then you can't necessarily provide the opportunity for students to actually build things that are actually useful to them or to the world, or at least that they consider neat. Certainly not things that correspond to much of what they are learning. That is mainly because it costs too much money. Similarly, if you are teaching a course on writing symphonies, then it would be too much to expect the students to hear their work performed, at least before many iterations. That would be absurdly expensive. You would just have to treat the students' symphonies as works of music theory. At least, that used to be true -- before the invention of computers. Since the invention of computers, what was prohibitively expensive has become practically free. It is now possible to make the computer perform the symphony directly from the score. Now the students can have a completely different relationship to the course material -- one that is, in my opinion, incomparably superior. But regardless of whether the relationship really is superior or not, my point is that students will respond to such teaching methods with disdain. They will think, "this old fogey is making us jump through hoops; when do I get to use a computer already?" At least, many students will think that, and many of the best students.
Thanks! I haven't gotten into language extensions yet; I need to look into that further. My first instinct is not to screw around with those because I don't want to deviate from the standardized language, but they seem more common than I at first realized. 
Proof is sadly not a requirement on many CS courses. Even then the proof tends to be done in old fashioned Hoare logic with some Pascal like language rather than structural induction. Hoare logic should be taught of course. If I wanted to prove the correctness of a program I wouldn't write it in Pascal though.
How is this related to a choice - functional vs imperative? 
I'm not saying that it is.
They started teaching Haskell to freshers a few years back (not coincidentally as a certain P Wadler arrived on staff).
It's on the blog, like he says, written up so you don't have to look at a PDF scan of a letter.
Given his area of expertise was as a teacher and someone involved in language design, that's quite a bold statement.
&gt; it's a computer science department where the goal is to teach students how to formally reason about programs If you'd asked ten of my peers what "computer science" was about while we were studying it, you'd have got ten different answers, but I'm quite sure none of them would have agreed with your statement above. Of course formal reasoning is a big part of CS, but the idea that the scope of CS is so limited is crazy. There are huge areas of undergraduate-level material that are far better understood with the aid of practical experience building stuff than they ever would be working through the corresponding theory alone.
fascinating that you picked up haskell as a hobby programmer. HOw well did you know C and COBOL before you tried it out? 
Many of the techniques used by todays' programmers are based on Dijkstra's work. This is hardly outside his area of expertise.
Is there any plans to have support for embedding in [Hell](http://www.reddit.com/r/programming/comments/1r7oci/hell_a_haskell_shell/)? It would be nice to be able to have a shell toolkit which protects against at least some typos.
I haven't tried Hell yet, but I would like to. What do you have in mind? From what I understand, Hell already allows you to evaluate arbitrary Haskell expressions. Which other features of Hawk are missing from Hell?
There's some extra irony that Edinburgh is also the home of ML.
Great slides for the target audience... full stop! Coming from imparative programming, and then discovering ML, the most entertaining thing for me, by far, was figuring out how to write map, foldl, (++), etc etc using only recursion and pattern matching, which my little brain had never really encountered before. When I teach newbies about Haskell, they get a huge bang out of the exercise of inventing fold, and building binary tree type and search/insert functions. I might steal your slides if I ever do a Haskell intro for my non-Haskell friends.
This relates to my personal story. My first programming course ever was in Java, and I hated it. I found the whole course and tools miserable, so by extension, I thought I hated programming in general. Because of this I decided to persue a degree in Electrical Engineering. Later, I took a networking class that involved Linux and C ,and I loved it. I loved it so much that I changed my carreer path. Now, after several years of C and Python programming, I am obsessed with Haskell. I wonder how different my life would be if I had started out with Haskell.
&gt; Many of the techniques used by todays' programmers are based on Dijkstra's work. If you had said "Many of the techniques used by today's educators are based on Dijkstra's work" then you would be right.
Chalmers University of Technology in Sweden have a Haskell course as introduction to the CS program. https://www.student.chalmers.se/sp/course?course_id=20693
&gt; I'll point out immediately that the program is called "computer science", not "software engineering". Yes, "computer science," not "pure math" (the latter being what I actually studied formally, incidentally). There is no "pure" computer science. The entire field is "applied." It is not analogous to physics. People do not study computers because they want to know the mind of god. There is no knowledge for its own sake in computer science. 
Imperial College Computing student here, I'm actually tutoring first year students taking the Haskell course. It is indeed the first language taught, and for pretty much the reasons Dijkstra mentioned. It's a great course if not that in-depth, but an optional course runs in parallel that teaches knot-tying, monads, type-level programming, GADTs, type families, etc. EDIT: For what it's worth Java is followed by Assembly, C, C++ in the first year and Prolog in the second year, so there's no particular emphasis on any one language and they try to get you to be comfortable with different paradigms.
Indiana university teaches scheme in their intro class for majors
See, this. This is the problem. Most computer science programs are just trying to be software engineering programs, and failing at that. They teach either too much theory for a practical software engineering degree, or too little engineering practice. Then they teach too little theory to become valuable computer science degrees, and far too much practice. There's a lot of confusion about what each degree should focus on.
This is what I wish more schools did. Give us homework to verify a proof, and then take us through implementation of that proof in lab. Some of my classes have been like that at MSOE (I'm in their Software Engineering course), whereas others have been pure abstract theory with minimal testing, and pure this-is-how-you-use/do-this. Operating Systems, oddly enough, served as a good example. We were taught how to resolve busy-wait issues and some approaches to deal with thread management problems, then we applied that to a lab, where we implemented our fix on a *nix system.
Totally—I believe that compromise here is best.
I did look into this - it's pretty easy to write a version of readFile0 that does a very small constant overhead over readFile (e.g. ~15 asm instructions) and returns the buffer plus the null. Profiling shows the readFile0 doesn't even show up in my real test case, so I didn't bother. 
IANAL also, but my understanding is: the work as a whole (that is, including phash) must be GPLv3, but as an API is not copyrightable (in the US), programming to an API does not constitute the creation of a "derivative work" and does not imply any obligations. See the recent case of Oracle vs Google for the question of copyright of APIs. 
Some number universities teach Lisp as a first language based off of http://mitpress.mit.edu/sicp/ (eg University of York, England). http://mitpress.mit.edu/sicp/adopt-list.html has the list actually.
That'd be really cool! thanks for the reply, I'll take a look at the lectures...
&gt; The key word is "science" Not really. Computer science is simply not science. It is a branch of mathematics. Why it's called CS is some kind of historical accident. This is definitely a term whose meaning cannot be understood better through its component words. &gt; And I'm pretty sure there is "pure" computer science, though perhaps you're using a different definition than I would. Automata theory comes to mind. [...] there are plenty of CS-ey topics that I'm interested in purely for the sake of knowledge There is pure math in (or "behind") everything that has math. But in order to come under the banner of computer science (or any other applied discipline) it has to have practical application. That is how topics get put into that discipline in the first place. It's not to say that they are not interesting from a "pure" perspective; it's just to say that they are not CS for that reason.
Yes, I could have added that and then I would be right twice!
Citation needed on that one. I never heard of anything like that. 
Hmm, I think the last time I actively coded anything was over ten years ago. I've played around with Java and C# just never liked them much. Oddly, I never did anything with the COBOL despite being the top student in the class. :) I probably have successfully forgot about 99% of that. I know enough C to plug around having coding in it since about 1989, but again just my silly little projects.. I tend to do these things like other people videogame :) The parallelization is what initially drove me to Haskell. I'm fooling around with some network monitoring experiments and the ability to spawn up a bunch of queries concurrently and process them in that fashion is what drew me.
From a personal perspective, I found the theoretical side largely lacking. I found a lot of joy in my algorithms class, though. The class itself was heavily tilted toward "practicality", but I found a number of opportunities to use my existing knowledge (I majored in mathematics) to inform my code, even reducing whole methods to one-liners with a few paragraphs of proof. I think this should be given greater prominence. That I found it myself was a pleasant coincidence, but I think there is a lot of beauty that is missed because these things are not discussed. This is anecdotal, of course, but it seems to mirror the experience of other CS students (though they don't seem to complain as loudly or as often). EDIT: I was also just reminded of my computability class, which felt a lot like "intro to proof techniques for CS students". I wish that the foundations class that I took in the math department had been a prereq, because it would have saved a lot of time in the first several weeks discussing the basics of proof.
I didn't say it isn't math. I said it isn't pure math. It is applied math. Theoretical physics doesn't actually have the same relation to nuclear engineering as CS to software engineering, because the mission of physics is to model the physical world even when it has literally no application. "Theoretical" in physics is not even opposed to "practical" or "applied," it's opposed to "experimental." Even theoretical physics is still applied mathematics. And so is CS.
Language extensions range from "trivial" to "What, I crashed the type checker?" [School of Haskell](https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions) has an introduction to some of them, mostly on the easier side (which is just what you're looking for).
What do you mean by "proper interpreter server"? Do you mean something along the lines of a GHCi library? If that's something you (or anyone else) is interested in, drop me an email - I'm working on [IHaskell](https://github.com/gibiansky/IHaskell) which I hope can serve as a partial replacement and enhancement to GHCi, and I have been thinking about splitting out the "evaluation" portion of it into a separate library for others to use. I wasn't sure there was any interest in it, but it's relatively stable and pretty much supports mostly anything that a Haskell file might contain (except ignoring pragmas and "foreign" things and other weirdness).
Isn't an Mvar () the same as this lock data type?
I don't know. I've always used `STM` exclusively in my own code because I've never had to do high-performance concurrency. Edward Yang (the author of the post) would be the best person to ask.
and have been doing so for 20 years, ever since they started offering a CS degree.
Indeed. Even the SICP holdouts at, Berkeley I think? , switched to Python with robots. The world is different now, computing has moved from the realm of math to technology. Math is great, but it shouldn't push the technologists out of computing. The Haskell class deserves a lovely home in a CS curriculum, but not required for everyone who gets involved with computers. But compared to Java, that's a different story. Java is not appropriate for beginners at all. public class HelloWorld { public static void main() { System.printLn("Hello World"); } } javac HelloWorld.java java HelloWord.jar Just shoot me. 
There is [a performance difference for list comprehensions vs do notation](http://www.reddit.com/r/haskell/comments/1etnc9/beginner_question_how_are_list_comprehensions/ca3vnxm) (the list comprehension version is about twice as fast). I heard a rumor that turning on monad comprehensions actually disables this optimization for list comprehensions (due to that it just translates it to bind/guard calls), but I haven't tested it.
The use of some extensions is quite common. The community vets new extensions, and the widely-used and well-regarded ones hopefully enter a future version of the standard.
So copying the array didn't show up but scanning it for separators did? I would have thought that copying would have been, if not more costly, at least comparable in cost to scanning since a copy includes a scan. Was the copy getting optimized away somehow?
You mean *binary* page numbers! ;)
1. Yes, I do realise... and I guess that's why I ended up talking about laziness so little (didn't get to the slides, and I only really talked about it when explaining `fibs`). It was just a good name for the talk! I'd have changed it if I'd thought of something better. 2. I was using 'assert' in the more colloquial sense, I guess - where asserting and defining mean the same. But I changed it in the slides! 3. As a first introduction to FP I wasn't too worried about that. Fun was the goal! I agree about those benefits of FP - I definitely should have been a bit clearer on those. In my talk, I focused more on the fundamental language differences (i.e. the lack of statements and abundance of expressions). Those benefits are a consequence of the basic differences. I wasn't trying to convert anyone - just broaden their minds. And several of them did leave the room going 'Haskell is awesome!' ;)
These guys were super happy even with composition - we didn't get to folds or anything further than `map`.
I love Haskell. I just wish it was *less* like Math, the language. And more visual. As my mind just isn’t a mathematician’s but also very visual. Anything that mostly consists of one-letter variables, (obscure) operators and structure (like brackets) is undecipherable to the brain of a non-mathematician or maybe -engineer. Maybe, one day, I will make a more visual editor for Haskell. (No, I don’t mean bullshit like colorful clickables. I still mean a code editor for keyboards. But one where those things I mentioned above have been transformed to a UI for other minds.) Then maybe I can undo the damage Math, the language, Java, C++ and even PHP did to my mind…
Ha good eye. That's the something Heist does so that "parse . render == id" https://github.com/snapframework/heist/issues/8#issuecomment-12328613 Just out of curiosity, are you nearer the east or west coast? Multiple people have told me the latency is good which surprises me since I haven't done much to optimize it. Just Snap server behind Nginx reverse proxy.
The Australian National University uses it as the only language taught to first semester students. Next semester is Java, then C/asm, and a few others are used (Python in HPC as well as C, Ada is thankfully taught in real time and embedded systems, and was used in concurrent and distributed systems)
Given that it is largely `pipes`-independent, is there any chance of `pipes-concurrency` dropping its dependency on `pipes` and becoming a standalone library? I love the what you've done with the library, but requiring a dependency on `pipes` seems unnecessarily onerous, expecially if you are using another streaming library or no streaming library at all.
Odd. Most mathematicians are *extremely* visual thinkers.
Yes I know. But that isn't relevant to the question.
It should be easy to solve all those problems with some well-crafted rewrite rules. Hmm..
upvote to add c++ to [BrainFuck](http://en.wikipedia.org/wiki/Brainfuck)! 
*Excerpt from linked Wikipedia article about* [***Brainfuck***](http://en.wikipedia.org/wiki/Brainfuck) : --- &gt;**Brainfuck** is an esoteric programming language noted for its extreme minimalism. The language consists of only eight simple commands and an instruction pointer. It is a Turing tarpit, designed to challenge and amuse programmers, and was not made to be suitable for practical use. It was created in 1993 by Urban Müller. --- [^(about)](http://www.reddit.com/r/autowikibot/wiki/index) ^| *^(/u/sflare can reply with 'delete' if required. Also deletes if comment's score is -1 or less.)*
Here is a pessimal comparison done by people I would trust to get the benchmarks right: http://www.researchgate.net/publication/220997900_Comparing_the_performance_of_concurrent_linked-list_implementations_in_Haskell/file/79e4151129793f18ec.pdf However, it is easier to beat MVars with TMVars than you might think. Part of it is the fact that in STM, you don't have to pay the cost of exception handlers that you would need to do with safe MVar access.
some neat reading this post points to regarding about how to optimize transactions http://dl.acm.org/citation.cfm?id=1345237 ("transacational boosting") http://dl.acm.org/citation.cfm?id=1481845 (paper on haskell concurrent data structures) both these papers are freely available elsewhere, its just easier to look up the citations via the ACM pages 
wait, theres exception handlers in Mvar reads? is this still true for the atomic reads in 7.8?
Yes, I can definitely split off the core. I just need to think of a good name for it. Edit: Also, note that `pipes` was *supposed* to be a light-weight dependency until Edward decided to make `void` depend on such a large number of packages
It's also used alongside Java for the basic courses in data structures and algorithms. I used to hang out in the CS student lounge, "The Monad." Gothenburg is keepin' it real with the John Hughes legacy!
A good library would certainly help to implement such a server, but what I'm after is something that can be interacted with programmatically from other processes and not necessarily with Haskell, as most editors are not Haskell-scriptable. So basically I want some kind of RPC. Maybe that's what this "IPython kernel" stuff is?
Cool. There is also a 0.1 version on Hackage of a Haskell implementation of the Bitcoin protocol, Haskoin, that looks very promising. Nice to see Bitcoin getting some Haskell love.
&gt; Whether or not you compile the code is not a special case, that's not how the GPL works. The GPL 3 distinguishes between distribution of source code (section 5) and distribution of object code (section 6). As such, it *does* matter, whether you distribute a work in source code, or in object code (e.g. as compiled executable).
Which question? My responses were the claim (paraphrased) that dijkstra's opinion doesn't count because it was outside his area of expertise.
Copying is a C++ memcpy that is almost certainly copying a word32 at a time. The original scanning for separators was quite slow - I could probably have stopped at the ByteString one if I'd wanted.
I've had bridgewalker on my phone since the beginning, but I never knew the back end was Haskell. Cool!
I'm assuming it's `semigroups` that's pulling in all of the dependencies? If so, what is your reasoning behind staying with `void` rather than including the datatype in `pipes`? Personally, I'm much in favour of small, light-weight packages and dependencies. It seems very silly to me that the vast majority of `pipes` dependencies come from what is essentially a one-line data declaration (I know it's not quite that simple, but still). On that note, didn't `pipes` used to have its own void type? (`X` I think? I didn't use pipes until `4.0` so I'm not sure).
Yes, that's the way it used to be. Originally pipes had the `X` type instead of `Void`, but then I got requests from several people to change it to `Void` and I obliged. However, that turned out to be a mistake because Edward kept adding more dependencies to `void` instead of getting it merged into `base`, which exaggerated the problem. I'm considering making a breaking change to go back to the `X` if Edward doesn't make the effort to merge `Void` into `base`.
Congratulations!
I know I personally would prefer going back, but of course that's your decision, and it doesn't really effect me on a practical level. It can't personally see any benefit in `Void` being related to the `semigroup` package, except from a "it technically works so we should include it" standpoint. I also don't see the point of it being `hashable`, because who is going to be hashing an uninhabitable type? I would be interested in hearing use cases for that, besides as a stand-in for an inhabitable type later on.
This is from about 18 months ago - does anyone know if the author (Jon Purdy, I think) reads r/haskell? Any update on what it's like working at SpacePort?
I've always found the public image of a mathematician as being uncreative and stifled as amazing and terrible. Mathematicians have much more in common with musicians than they do with, say, engineers.
can anyone explain how does this relate to Haskell or help Haskell programmers?
Oh, I was vaguely responding in jest to that part because I felt it to be an distraction. On the other hand, having recently got involved in teaching several comp sci classes at a state university, it's certainly the case that some of my material I'm teaching is derived from Dijkstra. I might add that in my college days, one of my professors was a Dijkstra graduate who was certainly teaching Dijkstra lessons!
Thanks! I've started reading the book several times but kept stopping for various reasons. Maybe the different format will help me get through it.
And someone needs to start cracking down on the terrible notion of sticking return statements into the middle of functions! 
To me, the mere fact that this question gets asked is itself interesting. Given Haskell's background as an "academic" or "research" programming language, I suppose that the interest of the topic at hand to the Haskell community was at one time self-evident.
&gt; with aeson, encoding and decoding of JSON bytestrings are both up to twice as fast Ooh, exciting. I've been benchmarking various binary serialisation packages (along with a new fast impl intended for the `binary` package) and I included `aeson` in as a reference. I figured any self-respecting binary serialisation package should be faster than printing + parsing JSON. As you'd expect, the good ones are but it's surprising how close aeson gets, at least on the serialisation side (parsing is quite a bit slower). So it'll be interesting to compare with the new aeson. Obviously the new serialisation I'm working on will be much faster still ;-) (just got the tests passing today, ready for the perf tweaking) And credit where credit is due: this faster serialisation is based in large part on ideas and code from Simon Meier.
If OP had not used pretentious words like “homotopy”, and said “A continuous deformation of one continuous function to another.” instead, the coolness of such a concept, especially for types, would have been clear to us. But nooo… gotta stroke the old ego very much!
There's a huge overlap with the type theory community and the haskell community. Many features in haskell are based off of modern type theory research, and there is a lot of type theory research being done in haskell. 
Mathematicians (and other kinds of researchers) make definitions/words for ideas that are useful. If we had to repeat the definition of homotopy every time we wanted to discuss the idea we would have an even harder time communicated about it. Using these 'pretentious words' is actually about improving communication, not ego stroking. Yes, it does require the reader to become familiar with the definitions.
I would say that anyone interested in getting a better head for advanced Haskell types could do worse than to learn a little Coq or Agda.
Heh, I am developing a library that uses them. Good news I guess.
I basically rewrote Universe the other day for a project of mine, then realized it already existed. It would actually make a nice addition to the standard library, I think. Very useful.
I'm actually really interested in hearing more from him! That job sounds like a perfect fit for me, too.
/r/haskell doesn't really represent the Haskell community.
Of course I do. :) All of the Spaceport engineers were acquihired by Facebook; we were a half dozen at the end. The product is still available but unsupported. Parts of it have been open-sourced at https://github.com/sibblingz but the compiler is notably absent. I’ll bug some people about that. If you’re curious about its architecture and the challenges we faced writing it, I have a half-finished blog post around somewhere. Or you can just ask. ActionScript is a laughable excuse for a language; writing a compiler for it was an adventure, to say the least. In praise of Haskell, it let us work quickly and refactor aggressively. We only got into dependency hell a few times, and using [alloy](http://hackage.haskell.org/package/alloy) as our traversal library was both a blessing (productivity) and a curse (long rebuilds). Working at Spaceport was an excellent experience—I learned an enormous amount about working under time constraints, got to apply my compiler knowledge, and gained good friends and colleagues in the process. I miss the startup environment, but I can’t complain about Facebook, as I still get to play with Haskell—the Haxl project, if you recall Simon’s slides about it. And you can still find me working on [compilers](https://github.com/evincarofautumn/kitten) in my free time. 
Hi. See [my reply](http://www.reddit.com/r/haskell/comments/1usjw5/the_big_mud_puddle_so_i_write_compilers_for_a/celh9ik) to /u/crntaylor.
Oddly enough, anyone that has spent more than 20 minutes around Bob Harper would realize he really doesn't like Haskell.
compared to what
Standard ML: http://www.amazon.com/The-Definition-Standard-ML-Revised/dp/0262631814 edit: One of his blog posts about Haskell: http://existentialtype.wordpress.com/2012/08/14/haskell-is-exceptionally-unsafe/
Excellent.
lol what? 
Yep. Type Inference Being Undecidable Considered Unimportant :) One of the most important realizations that occurs during the dependent types indoctrination is the value of using the highest-level abstraction and then working down from there. Types are a higher level abstraction than terms, so we should prefer to write out our types, and then see how much of the terms we can infer. With bidirectional type checking, we can achieve a pretty nice balance here.
20 minutes? You're so generous :)
I have been looking for this type of thing for a while. I've written interpreters but never compilers -- thank you for writing a formal introduction on the topic!
ok, I'm looking at http://hackage.haskell.org/package/base-4.6.0.1/docs/Control-Concurrent-MVar.html, and I don't see any mention of operations that set an exception handler... or maybe I'm looking in the wrong place?
On the subject of Bitcoin/Haskell love I wrote a blockchain parser which can be found at https://github.com/richardfergie/bitcoin-blockchain-parser
google "Bob Harper kidney"
honestly if you want tools that you can use RIGHT NOW, and you want to focus on doing good science *right now*, the numpy/scipy ecosystem tooling is really what you should be using. I am invested in building better data analysis for haskell, with some tooling public on github but not yet ready for serious use as yet. Focus on doing good science. Unless you want to invest in helping build the next generation of data analysis tooling :) 
He doesn't like laziness and he doesn't like the fact that bottom inhabits every type. What's wrong with that?
Ahh. I see nothing wrong with it. Homotopy type theory is highly interesting and may prove to be the future of programming languages, if an implementation can be discovered. Personally, I value the community of Haskell and the hard work of everyone involved more than the language itself. If a better language comes along, why shouldn't we all move to it?
That's not very convincing. You still seem to be missing the point. Consider someone who was actually very influential in how CS is taught: Gerald Sussman. Can Dijkstra even compare to Sussman, *in that respect*? I don't believe so. Your anecdote does not suggest otherwise.
The thing I love about category theory-based type classes is that they very directly evoke a visual style of programming. For example, combining monoids or categories is just placing things side by side in a line. No complicated threading of values.
As far as it goes, nothing I guess, but his simultaneous insistence that the types of ML aren't inhabited by bottom and worse is rather disingenuous.
[`modifyMVar`][modifyMVar] and friends. [modifyMVar]: http://hackage.haskell.org/package/base-4.6.0.1/docs/src/Control-Concurrent-MVar.html#modifyMVar
Awesome! Glad to see one of my favorite tools from the python world available in haskell
Wow great article and a happy ending!
Has anyone successfully got this working on windows? I get stuck when trying to install unix-2.7.0.0 (even in cygwin). cabal.exe: Package unix-2.7.0.0 can't be built on this system. Failed to install unix-2.7.0.0 cabal.exe: Error: some packages failed to install: unix-2.7.0.0 failed during the building phase. The exception was: ExitFailure 1`
I'm pretty glad a completely nasty comment made me learn something so pleasant. I am happy and grateful that Bob is well.
Hey there - author here. I rely pretty heavily on `unix` for things such as creating pipes to capture stdout and hijack stdin, so I honestly have no idea if it's possible to get IHaskell working on Windows. I myself have very little Windows dev experience, but would welcome any info on whether it's possible (and if so, what I should add to the README). Happy to answer any other questions people might have, too.
Looking through the code, it seems that it uses closed type families. Does this mean we have to wait for ghc 7.8? I guess that means we will only have to wait '2 months'.
Very nice. I might use this for a paper I'm writing currently.
Cool! But I think `:info` / `:hoogle` / etc should go into the notebook output instead of a separate window. If they did, it would make IHaskell an amazing teaching tool.
Why would that help? That was originally how it worked, and later it shifted. I can pretty easily set an option for that, to decide whether or not to use the pager.
Doesn't explain what homeopathy has to do with anything. Bob's your uncle?
Yes, it should definitely be possible. It's based completely on IPython, and you basically just run a local server in order to use the notebook. It might even be possible without any changes to the Haskell code, just editing the IPython profile a bit... More info here: http://nbviewer.ipython.org/github/Unidata/tds-python-workshop/blob/master/ipython-notebook-server.ipynb The same should work for IHaskell.
Makes sense. I've added an issue on Github https://github.com/gibiansky/IHaskell/issues/126 - if you'd like to keep abreast of these changes just add a comment or something. Also, feel free to add any other features you think might be useful as features (I don't promise they'll be implemented, but it's useful to think about them anyways!)
I made a Criterion benchmark: http://htmlpreview.github.io/?https://github.com/nh2/inner-loop-benchmarks/blob/master/report.html Clone it here: https://github.com/nh2/inner-loop-benchmarks Surprisingly, version 5, 6, 7 are over 4 times slower than version 4! Neil, can you check on your machine?
gotcha, thanks
Cool! I'll keep on eye on this project, it could be pretty handy in the future.
This is fantastic. I've been having a lot of trouble learning Haskell, mainly due to Python being my bread and butter (it's easier to stay by your rock), but I really want to learn Haskell. This will definitely give me a kick in the right direction. Edit: Wow this thing needs a binary. This is a ridiculous install process. I really wish Haskell would try the thing it failed on to see if I installed the right thing, as getting through build.sh takes about 5 minutes, and when it fails, it starts over (although it seems like it passes over some things). Maybe in build.sh try to see if all the dependencies are there before running through the build process? The notebook doesn't seem to be working. I have jinja2 installed, but `IHaskell notebook` tries to reinstall my python packages, and fails on Jinja (it doesn't download from the right spot or something). 
I'm not saying that Dijkstra's opinion doesn't count. I'm saying that his opinion is not an expert opinion on *this* question (i.e., the question of how to teach first-year CS). Making fundamental contributions to CS does *not* make you an expert on how to teach first-year CS! I didn't name Sussman because he "did great stuff." I named him because his approach to *education* has been replicated. He's someone who has influenced CS *education*. Vastly more than Dijkstra, from what I understand. As far as I know, Dijkstra has not influenced CS education very much if at all.
Please report an issue on Github or email me. In order to avoid dependency issues (esp since it uses effectively IPython HEAD) IHaskell installs everything it needs on it's own from source. I've had huge amounts of problems with different OSes packaging python differently and different versions of libraries and IPython, so I've resorted to this method. Since I only have one computer, it still needs a lot of troubleshooting. File an issue on Github or shoot me an email at andrew dot gibiansky at gmail, and I'll try my best to help out. FYI, build.sh is meant mostly for developing it when you need a *complete* reinstall - you can also just install from Hackage via cabal install or install only ghc-parser, ipython-kernel, and then ihaskell via normal cabal install mechanisms.
The video is incredibly choppy for me. Is there any way to download this set of lectures?
The video is kind of weird in that the upper left video is super low res, while the larger video on the right is super low framerate. Put side-by-side, it's just about tolerable to watch.
Hah, man, those first few chapters were like climbing up wall. The middle isn't so bad, though.
It's actually a simple lisp interpreter. But made purely as an exercise to teach myself haskell, so please be gentle :)
homebrew will install `zeromq4`, however ihaskell will have `zeromq3` as depedency? There is `zeromq4` in hackage. Maybe this is just problem for me?
That's an odd statement. There are academics here, there are commercial programmers here and there are hobbyists. 
Ignore the troll. They never post anything but plain nonsense or inflammatory nonsense here. 
Got it all worked out. Pip should install Jinja, not jinja, and Jinja will download MarkupSafe, so keeping MarkupSafe in the dependency list will cause it to download twice, erroring when pip tries to download it again and sees that the file exists.
Great! The simplest ByteString version is also almost the fastes. The graphs have the unity 'ns/ms/us', but under the graph the same numbers are displayed with the unity 's'. Is this a criterion bug?
Oh man that unix package crap crushed my spirit years ago. I may have even posted to reddit about it. Sad it is still hurting people :-(
What the Sage folks did was wrap a preconfigured Linux installation in a VirtualBox virtual machine image, and let Windows users download that.
&gt; The only real annoyance happens when you want to access C instructs from Haskell, specifying a Storable instance can be a pain. If you don't need structs it's pretty pain/annoyance free. I don't know if it's still the case or if there's a better way of handling it with cabal, but the last time I tried using C code, one of the quite annoying things was to get it working with ghci. If I'm rembering correcly, I had to build a shared library of my code by hand and giving this shared library explicitly to ghci with the '-l' flag. 
Thanks for posting this. I love "doing X in transit" because it reminds me of http://www.secretgeek.net/lisp_truth.asp :)
Good point, that should be kept in mind. This means we must also loop from C.
You can with overlapping instances.
cabal 1.18.2
I mean, you need to run "cabal update" to update the packages. Are you installing IHaskell-0.3.0.0 ?
Appears to have a bug on linux which prevents IHaskell from launching: https://github.com/gibiansky/IHaskell/pull/133
Cool! That's just what I want. The version being installed needs to consider the packages already installed. So I removed all packages and did a reinstall. 
This looks like an interpreter, not a compiler.
Great! I was waiting for this for a long time. I would also love to see this as a repl in a browser, similar to the fp complete ide. I was thinking of using d3js for plotting.
I'm not saying a community can't be varied, I'm saying you cannot use /r/haskell as a representation of what the Haskell community is like as a whole, considering the masses of Haskellers I know that don't come on reddit but frequent Google+, Twitter, Haskell-Cafe or the IRC, or no social media at all. Without numbers there's not even any accurate statements to make about the demographics. It's not a “wouldn't it be nice if everyone got a long” philosophy discussion, it's a basic factual dispute. Posting something to the various social media places and you'll see different reactions, you can't take just one and say “the Haskell community is that”.
Yes, he pointed that out [six hours before you](http://www.reddit.com/r/haskell/comments/1uv8fm/writing_a_compiler_in_transit/celzjfm).
What does `IHaskell console` do? I don't have ipython installed. If I fail to install **/usr/local/bin/git clone --recursive https://github.com/ipython/ipython.git /Users/eccstartup/.ihaskell/ipython-src** (actually I did), next time I start `IHaskell console`, all procedures will restart.
This is a REPL in the browser :) What you're looking at here is just the exported HTML, effectively a snapshot of the notebook. (Or is that not what you meant by a REPL in the browser? Do you mean an embeddable one?) The idea of using d3.js is a really awesome one - some people over on the IPython side have implemented that recently. If there's a library you want to provide a d3-based wrapper for, talk to me, and I'll be happy to get you started on that!
IHaskell console is IHaskell with a console interface, whereas IHaskell notebook is the notebook interface. IHaskell attempts to avoid dependency issues by just downloading and installing all its Python dependencies locally. (It creates an ~/.ihaskell directory, uses Pip to download some Python libraries and installs them that directory, then clones IPython from github and switches to the right commit, installs that to the directory, then runs IPython that it installed.) If you are having issues, please email me or file a Github issue :) The installation is tricky to get right, and I'd really like it to be maximally painless. If things are failing, you may want to try remove ~/.ihaskell and rerunning `notebook` or `console` - it will redo everything from the beginning. 
&gt; This is actually not true - I was bit by an inlining issue. GHC didn't inline the break function when it was used in more than one place. I fixed it by copying the function (an INLINE pragma would also have done), and now the functions get faster in the order Neil wrote them in the blog post. Well, this would have been just too nice. Could you please update the html report file? Thanks! 
The `ipython` from pip is very outdated compared to HEAD. IPython installs everything *locally*, so it doesn't matter what you have installed (or shouldn't matter, anyways). So you need to let it finish installing from Github. It seems to be working fine up until the part where it tries to git clone that repository - the first time due to user interrupt (ctrl-c) the second time due to network error (if you can git clone it yourself, IHaskell should be able to as well.) You will want to rm -rf ~/.ihaskell, rerun, and just let it finish (it might take a while to download everything, but as long as it doesn't error, it's fine). If you want to check for network errors, just git clone ipython yourself first :)
I can `git clone` ipython successfully without this `IHaskell console`. However, if I `git clone` it first, and rerun, I will get an error because it is `git clone` instead of `git pull`. And this process will stop. (The first installation will also rerun even though the files are already in `~/.ihaskell`.) So I wonder if I can change some configuration file so that I will decide these installation process myself. (The issue is that I have the files there however I **must** do a reinstall.) Thanks.
I don’t see any *music* in there. Only sound effects. Rather silly sound effects at that. You need to read up on the concept of a *groove*!
Thanks for sharing your clear and authoritative definition of music.
Hi, that's nice! How do you manage transitions? New sounds/patterns are started when you validate the line (so no transition)? A silly question: is your DSL turing complete? I'm wondering if we could create a DSL to create musical pattern that would always compile and produce sounds (whatever key you press). When you play piano, whatever key you press, you get a sound. With your system, most of the keystroke combination will result in a compilation failure. So my (rather idly) question is, is there a language able to produce complex musical *patterns* and in which every program compiles?
Here's a new feed of tidal cycles that has just started: http://tidalexperiments.tumblr.com/
I have the Debian IPython package(s) installed. Is there any way to use them rather than cloning and rebuilding?
Good question! Yes there is no transition, in practice I time evaluations so that the jump makes musical sense. As patterns are functions from time to events, there isn't a sense of 'starting again' each time an evaluation is done though, if that makes sense. Different live coding systems handle this problem in different ways. For example, JITLib in SuperCollider actually does an audio crossfade between one state of the code and the next by default, which can sound really nice. As it's embedded in Haskell I guess Tidal is Turing complete. Yes there are a few DSLs which don't have the possibility of syntactic incorrectness, either by having an open grammar (if that's the right term) or by having a structured editor. Actually I made a visual front-end to Tidal called Texture, which does not have the possibility of syntax errors (in theory). I think this means it should be easier to learn without starting with theory. http://yaxu.org/colourful-texture/ Betablocker is another language without syntax errors, inspired by corewar: http://www.pawfal.org/dave/index.cgi?Projects/Betablocker
been interested in tidal for a while and have really loved chordpunch &amp; associated acts. the pattern-a-day thing is great!
Synthetic homotopy --- you define for yourself what "continuous deformations" exist, subject to certain coherence rules, and calculate things based on that. An admittedly not great wiki article on the general idea: http://en.wikipedia.org/wiki/Synthetic_geometry
To twist what cartazio said - invest in helping build the next generation of data analysis tooling :) Monads only seem hard until you read typeclassopedia twice, and then they're easy forever. I'm a boneheaded scientist who's been mostly living in haskell for the past year and I loving it, despite my lack of CS education. If a scientist can do it, surely an engineer can! Yep, plotting is a headache. But that becomes less important when you get into haskell. In matlab, 90% of my plots are wonky because my code has so many bugs... (so in a sense plotting is mostly a debugging tool). Switch to haskell and you don't need your debugging tool anymore, so you don't need your plots so much. CS people say that "if it compiles, it runs" is an overstatement. But from the coding-scientist perspective it's an understatement. Worst case, do your computing in Haskell and write the results to a CSV file and load that in MATLAB to plot it. Of course JSON would be nicer, for (for god's sake) Matlab doesn't support JSON. If matplotlib does 3D plots, use that instead. For just the plotting. But, Haskell does have some stories developing on the plotting front, too: http://nbviewer.ipython.org/github/houshuang/math-with-ipython/blob/master/information/chapter-1-hs.ipynb http://hackage.haskell.org/package/Chart-diagrams And there is good work going on for getting haskell to talk to javascript - if javascript gets a great interactive graphing library, that might end up easily accessible, too.
"Homotopy Type Theory" (HoTT) is in some sense a misnomer. The reason why homotopy enters the picture is that models of such a type theory tend to have many of the characteristics of categories that allow an abstract formulation of homotopy theory (i.e. Quillen model categories). In reality, though, a model of HoTT need not be a Quillen model category. Also, this idea of "deforming" one function into another must be taken with a grain of salt. What you can do is define a semantics for type theory that interprets every context as a topological space, and every type as a (Serre) fibration. However, to work with HoTT you don't necessarily need to do that. Although it can be useful sometimes (for example, it shows that HoTT is consistent), it is more often just confusing. Furthermore, there are models of HoTT that are not strictly speaking made of topological spaces. Of course, you can still do some form of homotopy theory in them (thanks to results such as [this](http://arxiv.org/abs/0803.4349)). It's not the familiar homotopy theory, though, but a more abstract version based on axiomatic notions of fibrations and acyclic cofibrations. A better name for HoTT is probably "higher-dimensional type theory", since it makes clear what aspect of type theory it is focusing on (i.e. types where equality has some sort of "globular" structure). If you are familiar with abstract homotopy theory, then it might be natural for you to think of this as a language for that domain, but otherwise, it's probably just unnecessarily scary. I'm not sure you would even need to mention the word "homotopy" when introducing HoTT, were it not part of the name itself :) 
On the plus side that post was one of the catalysts that led to the new Typeable mechanism in 7.8. Like Harrop he has helped drive us to improve. ;)
My normal ghci usage involves alternating between typing expressions and using :i/:t/:k/hoogle to get more info about functions/types. It really needs to be in the same window as I do it so often. Your project looks great, btw. 
I think the point is that the smaller video provides an idea of what the professor is gesturing while the right side is intentionally "snapshotty" so that it's easier to read the board. If you resize your window to be larger then there's a row of snapshots on the bottom (every 15 seconds maybe) which appear to be taken from the right video and tend to be fairly legible shots of the board. I actually really lovei t.
I wrote a series of blog posts on the same topic: http://paolocapriotti.com/blog/2013/11/20/free-monads-part-1/. This particular result is in part 2: algebraically free monads are free.
Wow. iPython was the one thing I missed when I left python behind for haskell. Ironic that a python project might help me with haskell. 
It also shows that initial algebras of functors are a very special case of the more general notion of "initial algebras of monads". Of course, since left adjoints preserve colimits, the initial algebra of a monad is just the monad applied to 0, so it doesn't seem a very useful concept. However, this idea can be used to give a semantics to "higher inductive types" (HITs), which are inductive types where you not only have constructors for elements, but for equalities between elements as well. You can think of the syntax of a HIT as a presentation of a monad M, and the resulting HIT itself is then M(0). For a normal (non-higher) inductive type, the monad M is the free monad on the functor for which one would normally take the fixpoint.
You should try using the pager as they currently do. It's the same window, just a special location for documentation. Imho it's rather convenient, but it doesn't display in exported data.
Having watched the first lectures, most of the themes went way over my head, but I found the explanation Bob Harper gives of the relationship between "entailment" and "implication" very illuminating. I had wondered about that.
As another artist (although primarily visual in my case) interested in Haskell, I find Alex McLean and Tidal incredibly inspiring. I haven't really used Haskell for any of my art projects yet, but seeing work like this really helps to keep me motivated. The performative aspect of Tidal in particular is something I mean to spend some time exploring as it is one of my pipe-dreams to build some similar graphics tools for myself.
I only spent a couple of minutes looking at this but it did remind me of "How Equal Temperament Ruined Harmony: And Why You Should Care" (http://www.amazon.co.uk/How-Equal-Temperament-Ruined-Harmony/dp/0393334201)
I know, but he defends Haskell also. He just likes ML more. 
A little know option is that you can say &gt; cabal repl my-test-suite If you have more than one component in your cabal file.
Refreshing to hear that monads aren't too bad. I consider myself relatively "up" on the concepts needed to succeed in Haskell---so think I will proceed slowly and see where it takes me. Went through first few chapters of LYAH recently, and it's been pretty damn cool so far---so that's also a good sign i suppose. I'm making SciPy &amp; Matplotlib my default programming language/exploratory tool; that probably wasn't clear in my OP. The structural inconsistencies in Matlab and good word-of-mouth from others led me to go that route last year. But I'm still very much a Python noob: most of my work at the moment is in the laboratory, so programming is something I do only on occasion these days. When my PhD is finished, depending on where I end up, I may easily find myself coding frequently again. Thus my interest now. I had occasion to investigate "real" databases once recently, but didn't end up needing to use one. I'll keep JSON in mind for Haskell compatibility if/when I get that far, or if I need to use one from Python. Thanks for the tips. I'll check these H packages out too.
I'll definitely have to check Julia out or keep it in mind for the future! Looks really promising. Thanks for your thoughts. I don't often have to deal with freakishly large computations, so pure speed isn't my primary concern---but having the ability to interface with an appropriate tool may be handy in that event. As I mentioned elsewhere, since my original post I've gone through a few chapters of LEarn You a Haskell. Very intrigued by what I've seen so far.
I guess the recent jump to the magic `1.0` version of `text` was doomed to be followed up by a major version bump shortly :-)
That is life-changing! I did not know you could do that.
My background is in physical science as well, and as an exercise, I'm currently working on porting the code examples in [Structure and Interpretation of Classical Mechanics](http://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Classical_Mechanics) from Scheme to Julia, and with a potential interest in making a study group or lesson plan, and intend to make these resources public. As for Haskell and physics, this is one of my favorite examples of how Haskell can be directly applied (check the links via the Internet Archive): http://www.haskell.org/haskellwiki/Numeric_Quest
*A bit from linked Wikipedia article about* [***Structure and Interpretation of Classical Mechanics***](http://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Classical_Mechanics) : --- &gt;**Structure and Interpretation of Classical Mechanics** (**SICM**) is a classical mechanics textbook written by Gerald Jay Sussman and Jack Wisdom with Meinhard E. Mayer. It was published by MIT Press in 2001. The book (ISBN 0-262-19455-4) is used at Massachusetts Institute of Technology (MIT) to teach a class in advanced classical mechanics, starting with Lagrange's equations and proceeding through canonical perturbation theory. --- [^(about)](http://www.reddit.com/r/autowikibot/wiki/index) ^| *^(/u/notthemessiah can reply with 'delete' if required. Also deletes if comment's score is -1 or less.)* ^| [^(commands)](http://www.reddit.com/r/autowikibot/wiki/commandlist) ^| [^(flag for glitch)](http://www.reddit.com/message/compose?to=acini&amp;subject=bot%20glitch&amp;message=%0Acontext:http://www.reddit.com/r/haskell/comments/1urwxd/engineering_and_research_tasks_plotting_file/cemcm2p)
Yes managing changes in this way might well make sense, and folks like Julian Rohrhuber have explored scheduling source code changes for the future, a strange kind of revision control. I'm OK with making a change and then deciding what the next change should be based on hearing the results, though.
Sheesh. It’s cheap to criticize. Maybe some “groove links” would be helpful. Thanks for sharing this cool project, OP.
Not strictly about Haskell, but these notations come back quite often while reading papers about (Haskell) type theory and such, so I figured this might be of interest.
&gt; [...] but his simultaneous insistence that the types of ML aren't inhabited by bottom and worse is rather disingenuous. How so? LCF, for example, crucially relies on that property so I feel the above needs an explanation.
Yep, I exported each function one-by-one and compiled it, so there was only ever one use. In my real code I based this on break does have an INLINE pragma.
Agreed, I'd love to see the information - very curious how it works out.
You can use `cabal run &lt;component&gt;` to both build and run an executable in one step.
/u/yaxu, I'm curious if you've looked at or played with other systems for live coding, and had something to say about how Tidal differs from them? The only other system I've seen in my admittedly limited experience was Overtone (and it looks like you posted about it [here](http://toplap.org/overtone/), but I'd love to hear about others.
Nobody is claiming that there's no partial functions in ML. The question is whether you can write a program of, say, type nat that normalises to something other than a numeral. The answer for ML is, I believe, no.
Um, you just copied my comment verbatim from the comment section on the blog. Isn't that a little strange? Or I guess it frees me to upvote it now. 
i've some preliminary (work in progress pieces) starting to land on my numerical haskell org here: http://www.github.com/wellposed , especially http://www.github.com/wellposed/hOpenBLAS. That said, wait a few weeks before having a look, a lot of what i'm making public right now is prealpha. That said, i'm committed to building tools that put numpy/scipy/r to shame, and are jointly expressive and performant both in the hands of its users :) 
Actually, if you read the `glibc` source, `memcpy` is actually a little cleverer - for large enough inputs, it just manipulates the virtual memory mapping to effectively copy whole pages at a time.
[Hawk](https://github.com/gelisam/hawk#readme) uses hint, so we have encountered this problem. Our solution was to detect whether the `hawk` executable was sitting inside a folder called `.cabal-sandbox/bin` or `cabal-dev/bin`, and set `-package-db` accordingly. Although, after reading this article, I'm no longer sure that we did that last part right.
We only have two models of HoTT (including univalence) at the moment -- the simplicial set (which doesn't compute) and the cubical set (which we think does). Both of these are very topological in character. Or am I missing something here?
The reason Haskell isn't a fully dependently typed language isn't typechecking, or even type inference. Its that we currently want to work in a setting without termination proofs required. Now as we get better at working in languages requiring such proofs (or allowing coinductive constructions as well, even better!) then we may well get "the next haskell" as something that looks much closer to Idris (or idris might even get there itself). But in the meantime, Haskell captures pure functional programming that we "know" how to actually write lots of stuff in, pleasantly. (and also some researchy bits, of course!)
There are models made of Reedy-fibrant diagrams over inverse categories. See: http://arxiv.org/abs/1203.3253
Hey, well you've already found http://toplap.org/ and I guess http://toplap.org/about/ The most active livecoding related subreddit is http://www.reddit.com/r/Algorave/ This would need an essay to answer well, but.. Tidal is a mini-language for pattern, whereas most popular live coding environments like extempore, supercollider, overtone and fluxus are more general purpose. Also many include fully-fledged DSP engines whereas Tidal relies on external software to actually make sounds (in response to OSC messages). That said supercollider and some others include their own pattern DSLs. There are other mini-languages, ixilang and some of Dave Griffiths' experiments for example.
Thanks for the link to /r/Algorave !
this has renewed my irrational fear of operational semantics
I haven't got time to try either (yet) but I guess to have an awk-like processing in Haskell. My guess is that it just require an API of hawk - not only a commandline.
Hi yaxu, big fan of your work! I've played around with Tidal quite a bit, and I want to hack around with some low-level stuff. The only problem is that sometimes I find it hard to work out what's going on where and what all the types actually mean/represent. Do you think you could possibly comment your code?
You can use cabal build and also have it in your .cabal-sandbox/bin by doing `cabal build; cabal copy`.
Wouldn't a non-terminating computation count as normalizing to something other than a numeral?
Sorry, I don't understand. What is an expression of type nat then? Are you using the Scott-style semantics?
I think the paper by Julian Hook contains a section starting that work. If not, his dissertation from Indiana University does.
don't be afraid!
Would it be possible (but perhaps not guaranteed to be complete) for the compiler to check for circularity in these definitions? (If I understand your comment, the problem is "begging the question" where for example the definition of 'fmap' is just 'fmap'). I imagine (without details or proof ) something akin to compiler extensions like UndecidableInstances or RankNTypes
Yes there is a ticket for that https://github.com/yaxu/Tidal/issues/4 I just committed some documentation for Time.hs, will try to get to Pattern.hs soonish.
That's a good point. I meant that values of type Nat in ML are always normalized whereas in Haskell they may be considered programs of type Nat. But you're correct, a term of type Nat is indeed an ML program of type Nat with various side effects, including non termination..
I meant something like [try haskell](http://tryhaskell.org/). But only that you can also plot directly in the browser. That's where I would use d3js for plotting. I'm fairly new to Haskell and don't know all the tools yet. You used Chart for this purpose, which looks awesome, I'll definitely look into that. When I have a project with plotting I'll come back to you! 
I really wish this had been around years ago when I first got into reading programming language literature! This is a great post.
Your welcome. I just thought it was cool. It is very good that the entity who wrote it ended up on the thread as well
So excited to try this! Has anyone successfully built it on mac/mavericks? or a recent ubuntu? edit 0: In particular on mavericks, I can't get past: &gt; fatal error: too many errors emitted, stopping now [-ferror-limit=] when `cabal install`-ing `zeromq4-haskell` edit 1: my bad. `brew` issues... 
I'm in Utah, so the west coast. bah, I think that having that four extra bytes `&amp;#10;` is dumb and just makes the html less readable.
**Update**: Cabal is included now, in version `2014-01-11.2`. Problems with it have been resolved. ~~Cabal is actually *not* installed in this release, something that will be remedied soon.~~ Ben Gamari's patches in his [llvm-dynamic](https://github.com/bgamari/ghc/tree/llvm-dynamic) branch are used to build LLVM with dynamic options. `DYNAMIC_GHC_PROGRAMS` is "NO", but for the `afriel/ghc-head-dynamic` repository, `GhcLibWays` is set to `v dyn`. $ ghc --version The Glorious Glasgow Haskell Compilation System, version 7.7.20140110 $ ghc --info [("Project name","The Glorious Glasgow Haskell Compilation System") ,("GCC extra via C opts"," -fwrapv") ,("C compiler command","/usr/bin/gcc") ,("C compiler flags"," -fno-stack-protector") ,("C compiler link flags","") ,("ld command","/usr/bin/ld") ,("ld flags","") ,("ld supports compact unwind","YES") ,("ld supports build-id","YES") ,("ld supports filelist","YES") ,("ld is GNU ld","YES") ,("ar command","/usr/bin/ar") ,("ar flags","q") ,("ar supports at file","YES") ,("touch command","touch") ,("dllwrap command","/bin/false") ,("windres command","/bin/false") ,("libtool command","libtool") ,("perl command","/usr/bin/perl") ,("target os","OSLinux") ,("target arch","ArchX86_64") ,("target word size","8") ,("target has GNU nonexec stack","True") ,("target has .ident directive","True") ,("target has subsections via symbols","False") ,("Unregisterised","NO") ,("LLVM llc command","/usr/bin/llc-3.4") ,("LLVM opt command","/usr/bin/opt-3.4") ,("Project version","7.7.20140110") ,("Booter version","7.6.3") ,("Stage","2") ,("Build platform","x86_64-unknown-linux") ,("Host platform","x86_64-unknown-linux") ,("Target platform","x86_64-unknown-linux") ,("Have interpreter","YES") ,("Object splitting supported","YES") ,("Have native code generator","YES") ,("Support SMP","YES") ,("Tables next to code","YES") ,("RTS ways","l debug thr thr_debug thr_l dyn debug_dyn thr_dyn thr_debug_dyn l_dyn thr_l_dyn") ,("Support dynamic-too","YES") ,("Support parallel --make","YES") ,("Dynamic by default","NO") ,("GHC Dynamic","NO") ,("Leading underscore","NO") ,("Debug on","False") ,("LibDir","/usr/lib/ghc-7.7.20140110") ,("Global Package DB","/usr/lib/ghc-7.7.20140110/package.conf.d") ] As soon as I've added Cabal (there will be a point release for this) I will write a blog post on how to use this for local development as an alternative or in addition to Cabal sandbox.
Are there any requests for alternative builds? Is there something (other than Cabal) that I am missing that should be installed in these images? Let me know here or send me an email at mayreply@aaronfriel.com. Profiling may still be broken for `dph`, I'm not certain (I have to do a distclean and rebuild to check). If profiling only works with a disabled `dph`, is that something people would want?
Thanks for the very interesting pointer. However, I disagree with your interpretation of the paper, at least to the extent I have looked at the frontmatter (obviously I don't claim to understand all the constructions involved fully). The models he presents seem to be derived from other models. I.e. given a category C, which is a model, then C^I is a model where I is an inverse category. So our "ground" models remain topological in nature at the moment, and our derived models are in some sense categorifications of them. I note that this is in accord with what you wrote above, that such models are "not strictly speaking made of topological spaces". But I do think it disagrees with your conclusion that the "homotopy" in HoTT is then somehow a misnomer. The geometric/topological interpretation of type theory is not something we should minimize here, nor attempt to bury under categorized versions thereof. It opens the possibility of drawing on a whole family of proofs and techniques, as well as providing a new intuition for many things we've already been doing. For the practice of computer science, that aspect of HoTT seems _key_ to me.
merging Void into base isn't edward kmett's decision to make, though he is on the committe that would decide such.
Uuum, no, it’s still worse in functional programming languages. Because the original problem of having to walk through entire data structures is still there as you can’t have a graph data structure with loops. (As in: Pointers to directly access things deep inside structures. (See the problem of implementing a [hash] map.) And your whole weird scenarios are complete nonsense that never happens in any game engine unless the coder somehow lost his mind halfway through. You do not modify the source world tree EVER while in physics/AI routines! You “*clone*” the whole world tree, and *only* read from the one while you *only* modify to the other! And you save most of that work with copy-on-write deduplication. This was never a problem with any game engine. It isn’t so desperately needed, that it validates the horrible boilerplate mess of tree zippers. And I love Haskell.
&gt; Because the original problem of having to walk through entire data structures is still there as you can’t have a graph data structure with loops. Although the World data might form a graph of interconnected objects, there's still going to be an array or some other iterable object containing references to every game entity. You can map over that in Haskell as easily as you can loop from 1 to 10 in an OOP or imperative language. &gt; And your whole weird scenarios are complete nonsense that never happens in any game engine unless the coder somehow lost his mind halfway through. &gt; You do not modify the source world tree EVER while in physics/AI routines! You “clone” the whole world tree, and only read from the one while you only modify to the other! He goes on to say exactly that is the solution. That you need to write to a clone or a list of updates to be performed instead of the World data itself. The point of the article was that this *aversion to mutation* is quite easy and arguable easier in FP than in OOP. Here's the closing paragraph, in case you didn't get that far: &gt; *If I were writing such a simulation in a functional style, then the fixes listed above would be there from the start. It's a more natural way to work when there aren't mutable data structures. Would it be simpler than the OOP version? Probably not. Even though entity updates are put off until later, there's the question of how to manage all of the change information getting passed around. But at one time I would have thought the functional version a complete impossibility, and now it feels like the obvious way to approach these kinds of problems.*
But lenses, just like any other such construct, is internally still massive tree-walking. It has to be, as long as it’s a functional language. I mean the fact alone that you have to have this elaborate concept that actually has a name, to do something that essential, shows that something is very very wrong. Ditto for monads. And yes, I still love Haskell. And I want it to stay as great and elegant and all as it otherwise is. But this is something that needs to be fixed. Without any compromises whatsoever. (That the point.)
And in OO-land you have "setters." The fact that they have a name there, too, doesn't seem to bother you.
ghc-vis integration would be great, to introspect data structures
Agreed. There's also some stuff going on in upstream IPython that would let the interactive aspects of ghc-vis be implemented. I talked to the ghc-vis developer, though, and he says it's pretty tightly integrated into GTK - so while I'd love for it to happen, and it *should* be possible, I don't know if that'll happen any time soon...
Ok, haters are here. It's time for /r/truehaskell everybody! Now go away. Here are some [memories](https://gist.github.com/quchen/5280339). Edit: Sorry, I was pissed.
I think procedural, not OO, is the gold standard here. Especially if you want the simulation in real time, or have memory constraints, it's quite hard to argue for functional.
Typo: pooling its input buffer.
&gt; I mean the fact alone that you have to have this elaborate concept that actually has a name, to do something that essential, shows that something is very very wrong. what, better if we didn't have names for them? because making things awkward to talk about makes them less wrong?
Hey, /u/eegreg, I'm replying to let you know that the both ghc-head and ghc-head-dynamic include Cabal with version `2014-01-11.2`, which is the new `latest` tag.
My understanding from having talked to him is that the libraries mailing list already approved of this, but he's not willing to merge it in yet because it would take him a lot of time to update all of his downstream libraries.