Although I can't vouch for it, [99haskell.org](http://www.99haskell.org/) looks promising. 
You can also write a separate integer square root function. The lazy (as in common sense, not in Haskell sense) version would be something like isqrt :: Int -&gt; Int isqrt n = floor (sqrt (fromIntegral n :: Double)) (or similarly a ceiling version, if that what you need). Such a function could be also implemented without using `Double`-s at all, which would be the proper way especially for large integers, but that would be more complicated.
I think even better, and afaict the idiomatic way in Persistent, is to add a type signature to the filter used in the `selectList` function: users &lt;- runDB $ selectList ([] :: [Filter User]) [] [Here's](https://github.com/yesodweb/yesod-scaffold/blob/0186facf75cd20f4eeb14af472f29be898d05b67/test/Handler/HomeSpec.hs#L31) where this function is used in the Yesod scaffolding 
I think I've hopelessly mis-explained what I meant to be getting these responses, so I'm just going to stop now. ;)
&gt; If it doesn't have to work, you can meet any other requirement. ‚Äî Gerald Weinberg
Even worse, the very next source listing defines a function that doesn't check the `Null` case.
Looks like it is time to rebuild [comonad.com](http://comonad.com).
Ah, sorry for my misunderstanding. Yeah other things need a bit of manpower. But I find most of the things decent. I was able to have a working environment of Latex from nix which I didn't expect. :)
if you wanted to check for factors of 50, instead of checking numbers until the square root of 50, you can check numbers until tje square of tbe number is 50.
I noticed you use docker for local development and build AMIs with ansible. Have you considered [packer](https://packer.io/)? I've only played around with it on toy stuff but the idea is that it lets you use your provisioning tool(s) of choice (ansible, chef solo, shell script) to multiple outputs (docker image, AMI, Vagrant). Sounds like it may replace some homegrown stuff you have.
The way to do it: propose $50 (or whatever) to a developper to spend an hour looking at the code and let him give you an estimation of how long he think it will take. Then pay him or not to do the job if you can afford it or not.
Being able to have runnable code on standalone blogs is huge!
For the vast majority of application of matrices and linear algebra outside of graphics the size really should be dynamic.
Thanks for your reply.
I use the alpha version of [Hoogle 5](http://hoogle.haskell.org/).
shout out for explaining the "header", in particular importing specific names. I think every "example file" should do this. like when I was getting started with servant (which has extraordinary documentation btw), running the example file, it took me a bit to find what came from where. although, with the recent school of haskell development, this can one day be moot :)
Aren't macros too dangerous? 
Cool! Regarding your decision not to use type classes ([here](https://github.com/LuxLang/lux#but-why-not-just-use-type-classes)), have you watched the [Type Classes vs. the World](https://www.youtube.com/watch?v=hIZxTQP1ifo) talk? I ask because the way you justify your decision makes it seem like type classes are just a weak feature, whereas a more charitable perspective is that the type class coherence condition makes type class composition sensible and makes dumb data types (like Data.Functor.Compose) powerful at the cost of requiring newtype hacks.
How?
&gt; wow. This sounds amazing. Are there any other PLs with documentation at that level? Go.
Yes.
This looks pretty awesome. Some of these concepts (strongly-typed curried functions in a Lisp, monadic macros, *pattern-matching on macros*) are just *beautiful*. I love these kinds of languages, and I hope this goes somewhere! A few comments/nitpicks: * The `(: (Type) (value))` syntax for type constraints seems clunky for such a common operation... My first thought was to make types callable, so you'd write `((Type) (value))` instead--which would work in a normal Lisp, but not with currying. * There seems to be a lot of symbol noise, particularly with `#` for constructors. I would argue that, if you're going to visually distinguish specific syntax elements with symbols, it should be types, rather than constructors. * Is there any reason you decided to go with `##` for comments? Using `;` is pretty standard for Lisp, so that could be confusing.
In Chrome, you right-click on the search textbox (on Hayoo/Hoogle/Hackage page) and choose ‚ÄúAdd as search engine...‚Äù. In Firefox, the same but ‚ÄúAdd a Keyword for this Search...‚Äù. No idea how it's done in other browsers, but google ‚Äú&lt;browser name&gt; custom search engine‚Äù.
Why isn't it just enough to use `Validation` with a monoid that: 1. Supports O(1) right appends; 2. Supports `take n` in O(n). Maybe I'm missing something and this sort of solution would be harder than I assume. I'm also a bit confused at your comments about "avoid[ing] unnecessary work." Reason being that we're talking about applicative functors here, so fundamentally, there's no notion of aborting the computations in the middle based on the results of its effects. I don't know if this is a problem for examples that use lazy I/O, but if we used a type like `Compose IO (SmartValidation e) a`, that's going to do a lot of unnecessary work no matter what. Another set of candidate solutions would be to use `State` (or an update monad [as discussed recently](http://www.reddit.com/r/haskell/comments/34kouv/edward_kmett_how_to_replace_failure_by_a_heap_of/)) so that actions can observe the errors produced earlier in the same computation and choose whether to add to them or just no-op. This might be implementable as a wrapper that composes around arbitrary `Applicative` functors.
This is just fantastic! Has anyone considered integration with GitIt wiki? It would provide flexibility of Pandoc for the markup and git based storage. 
Oh... thank you.
Macros are a lisper's best friends. Besides, Lux macros will have access to the compiler state, so they'll be able to get typing information that they can use to make more accurate code. In fact, there's already a macro called "using" in the prelude that does the job of ML's open by examining the type of the structure and generating for you some pattern-matching code that extracts all of the fields.
http://stackoverflow.com/questions/1587635/haddock-for-cabal-installed-modules
If you didn't already, I advise you to take a good look at [1ML](http://www.mpi-sws.org/~rossberg/1ml/). It explains how to implement your modules-as-records idea while keeping type inference decidable and all the usual functor awesomeness.
Will this work in a sandbox? Like I have a new sandbox but nothing installed. When I do the reinstall, will it reinstall only the packages in the sandbox or also the packages installed globally? 
Glad you like what you hear :D
Welcome! Note that the existing SoH has haddocks which are queried when you change the cursor position. Haddocks in Haskell wiki code would also be cool. Hopefully at some point we can add support for SoH style code blocks in the wiki! Your comment made me think of something, though. With the current design, you'll need to first click the run button to compile the code, to get links to haddocks. It'd be rather nice if info about code was available without explicitly running it. I've added this to the wishlist.
I look forward to it. Thanks for all your excellent posts! (and code!)
A friend of mine, a well known computer scientist, once said that any properly designed programming language does not need macros. I tend to agree strongly with that sentiments. Macros are hacks.
This sounds great! I'm looking forward to publishing more content on SOH! The git-based editing sounds like a very sweet solution for publishers. I assume you will tag comments with the hash of the commit that they are referring to and display accordingly?
I've never wrapped a C++ library, but I'd be surprised if there were not a tool to generate bindings automatically.
I've written bindings to fairly complex C++ libraries, and while it's definitely annoying (much moreso than C), I've never run into a situation where it simply wasn't possible. Perhaps if you provide some more information, I or someone else can help you break through! Feel free to send a PM if you don't want to discuss publicly.
I've never tried anything like this, but I suspect for those who have, you'd get better answers if it were clear: - which approach(es) you already tried - what (kinds of things) you're trying to wrap - where your frustrations currently lie Bonus points if you can give a point of reference to where such a task is implemented more easily in another (functional) language. update: In particular, have you perused the [haskell wiki](https://wiki.haskell.org/Cxx_foreign_function_interface)?
I question whether it is any clearer though. Great writeup BTW! Very thorough and simple at the same time.
Any reason to not use real sets here?
Along with the "bind as you go" approach I mentioned elsewhere on this thread, you may want to look at: https://github.com/wavewave/fficxx 
I don't know anything about NotePad++ but for autoformatting you may be looking for `hindent`.
hmm, I found something in cabal called stylish-haskell - is there a way to use it out of npp? - I have nppExec installed so it would be fine if i can call it from my console so that it is formatting my code and updates my npp view. Is that possible?
Yes. If you want to find redundant patterns too, then you should keep the branches in a list.
&gt; (,) ## The empty variant, aka "void" Typo? Shouldn't this be `(|)`?
What's up with the colons and semicolons? `text:++`, `#lux;Cons`, etc. If it were up to me, I'd choose `/` for these things instead. `text/++`, `#lux/Cons`.
how does F# help here?
In addition to this you'll need a heuristic to bail out when the set representation is getting too large. The worst case with this algorithm is exponential (since the problem is NP-complete it probably is exponential with all algorithms), and I've found that the worst case actually happens in practice, albeit very rarely.
Yes. There are some mistakes in the README. I'm actually fixing them right now to commit the new README.
Probably. But also quite possibly XML.
Macros are not a hack. The syntax of most languages ARE hacks instead. Just consider that you can implement both "if" and "let" expressions on top of pattern-matching. How many languages actually do that? Only Lux. Macros just allow users to design the language as they see fit, instead of imposing on them 1 shape of the language and pretending that somehow your design choices can account for all use-cases.
I've checked TR and I played around with Clojure's core.typed. But I think discriminated unions are actually better to use than full unions, as full union types can get complicated/confusing and don't even account for some use-cases.
It is possible to do fixed price projects. But the client who admits to knowing nothing about the technical details is not the one who fixes the price - it is the contractor, after seeing all the details. The price will be *higher* in the end than what you would pay at the hourly rate. Nevertheless, there are cases in which paying that extra premium does make good business sense.
Regarding JavaScript: A ClojureScript port of the compiler is already in the roadmap, precisely to generate JavaScript. Oh... and Lux macros will work perfectly in the JS version, unlike Clojure macros. So you'll be able to share code without any hassle.
Well it does but you still have to write a wrapper layer because F#/C# will require managed types or a C stdcall API.
[This](https://github.com/coursestitch/coursestitch-api/blob/master/src/Main.hs#L24-L32) was my approach to a main function that can actually run either Postgres or Sqlite - I, too, had a lot of trouble figuring out which parts were generic between them! I check for an environment variable to use in production, and if it's not present then I use a local Sqlite database. The `RunDB` stuff is from [here](https://github.com/coursestitch/coursestitch-api/blob/master/lib/CourseStitch/Models/RunDB.hs), which was my eventual solution to a generic `runDB` function that I could pass to all our handlers. [This](http://stackoverflow.com/questions/28341784/when-is-a-generic-function-not-generic) SO question was where the community helped me wrangle the types for that one!
Is there any link to the slides? I could probably read them in 5 minutes instead of watching a 2 hour talk.
i'm searching for more explanation on it, but i believe i remember ekmett saying that the haskell opengl library was a programmatically generated wrapper - i.e. it parses opengl headers and automatically generates a haskell wrapper. edit: [here it is](https://github.com/ekmett/gl)
[This](https://www.reddit.com/r/haskell/comments/2uoton/edward_kmett_encapsulation_vs_code_reuse/cob65px) comment contains a link, but as it states, much of the content of the talk was in response to audience questions, so you won't get nearly the whole content reading the slides.
I just gave up and continued to use C++. It's far from the worst imperative language to program in. In fact, if you know what you're doing r.e. C++11/14, it can be downright pleasant.
I'm pretty sure you can make calls to unmanaged APIs using the Marshalling stuff, but in order to make it friendly you need to write a wrapper on the managed side of things.
Um... Lux macros ARE lisp macros... Would you mind mentioning the lisp-specific issues you mentioned?
I've bumped into this several times as a musician and young haskeller (or maybe an older version of this). Fascinating stuff. I appreciate the post because I just got off for the summer and was thinking about doing some Haskell audio. Having this as a resource will be great It'd be cool if anyone could link me any other cool audio signal projects in haskell
This is pretty nifty. What kind of issues could you use it for?
Actually, it has been posted too often, for my taste. It just run through [Hacker news](https://news.ycombinator.com/item?id=9487881) again, yesterday.
A similar approach is to use the [!bang patterns](https://duckduckgo.com/bang.html) of DuckDuckGo. From their website: Languages (haskell): !h !hackage !haskellwiki !hayoo !hgl !hgle !ho !hoogle !hoogle If you know the package name (let us say `foobar`), the way to go is `!hackage foobar`. *Edit:* Of course it has the disadvantage of calling the DuckDuckGo website and then redirecting you to Hackage. For the case of slow internet connection (the complaint of the OP), the solution of /u/peargreen is more appropriate.
The author recently passed away, so this is on top of many people's head. Take it as a homage to an important member of the community. 
sounds like a regression to me.... :-/
Ah. Thanks for the explanation. 
This is strange. I tripped over -Wall not enabling all warnings before already, and now there are even less enabled by default? Maybe it would be better to have a sort of verbosity system for warnings, like increasing via -W, -W2, -W3, -Wall.
fficxx is what convinced me to give up. The size and scope of what I face is very similar to Ian-Woo's. It's clear that although he was willing to dump a ton of effort over several years into his binding tool and the binding itself that he never got there. Later, some poor sucker [picked it up as GSoC project and was never heard from again](https://mail.haskell.org/pipermail/haskell-cafe/2013-August/109548.html). At some point, you look at piles of charred, rusty armor outside of the dragon's cave and say to yourself, maybe I'll leave this particular monster un-slain.
But what about an extensible *language*? Somd things like RecordWildCards and LambdaCase could just be macros, rather than having to hack extensions directly into the compiler.
I'll definitely miss the shadowing warnings. :-(
I'm not a lisp expert....I'm told by friends who are that there are issues with side effects that impact incremental builds (which personally wouldn't bother me as I've long given up on using incremental builds for anything) and, much like with C-style macros, they tend to obscure understanding. It may be that your specific experience has been different and that's fine but based on what I've seen over a 35 year career, I remain in strong agreement with the view that a well designed language doesn't need macros. So if you like macros, just consider your language to be not that well designed -:)
Might just be a [documentation bug](https://ghc.haskell.org/trac/ghc/ticket/10386)?
Going to enable most of these in all my projects now. They've saved me from bugs before..
Hello, if someone can answer my response https://mail.haskell.org/pipermail/haskell-cafe/2015-May/119517.html that would be helpful. UPDATE: This is just a documentation bug.
Needs type search. :/
My view is that the benefit of a common language far outweigh the benefits of modifying the language for some extra convenience in some cases. The fact such language changes have a big barrier to entry can thus be seen as an advantage in forming a common language and ecosystem that everybody agrees on.
That's a cool project. It looks like opengl is some low-hanging fruit because they publish their clean, organized API in XML. So, if you happen to have ekmett on staff, it's just a couple pages of parsing arrows and you're home in time for dinner. I think swig used to convert C APIs to XML and libclang used to as well but, for reasons lost to history, both projects gave up the practice.
Fair enough. I was just hoping for someone to come along and say, I gave up too. Now when I need to interoperate with C++ I use this other cool language which gives me some things I like from Haskell but also plays really nice with C++. I guess there's a few votes for F# and something called MLton. Just this morning I stumbled onto [Felix](https://github.com/felix-lang/felix) which I'm building right now and may be just exactly what I'm searching for. Check out the feature list: * generates highly optimised ISO C++ * advanced resource manager organises compilation and linkage * often runs faster than C * glueless binding to C and C++ libraries * lightweight threads with channels * asynchronous network I/O * thread safe garbage collection * strictly statically typed * overloading * first order parametric polymorphism * polymorphism with constraints * multitype Haskell style type classes * type classes with real semantic specification * semantics can be checked by theorem provers * strong functional subsystem * pattern matching * first class function, sum, and product types * Tre based regexp processing built in * bindings to Gnu GMP and Gnu GSL included * user definabled and inline extensible grammar * builds on all platforms * runs on all platforms * open source FFAU (free for any use) licence 
Does it all have to be done at once? Can you post an example of a difficult call? I was surprised by the amount of progress I made just with Emacs macros and Parsec. If you really need something more industrial strength there's the [libclang bindings](https://github.com/chetant/LibClang) which worked pretty well the last time I used it.
4GB of RAM? You can buy laptops with 32GB of RAM. Having said that, GHC has never been a good piece of software. It's just that it has an academic aura around it. GHC has never been free of error (unlike some other systems). 
Maybe take a look at http://hackage.haskell.org/package/cplusplus-th ? I've manually written C++ marshalling code before‚Äì it's unfun, it's ugly, but it's doable: https://github.com/iand675/hs-mesos 
I'm not aware of any standard functions for this. However, it is very easy to implement using `either :: (a -&gt; c) -&gt; (b -&gt; c) -&gt; Either a b -&gt; c`: Œª: :t either Just (const Nothing) either Just (const Nothing) :: Either a b -&gt; Maybe a Œª: :t either (const Nothing) Just either (const Nothing) Just :: Either b a -&gt; Maybe a Another possibility, in case that you already use lens, would be `preview _Left` and `preview _Right`. 
[Hayoo](http://hayoo.fh-wedel.de/?query=Either%20a%20b%20-%3E%20Maybe%20a) says there's [`leftToMaybe`](http://hackage.haskell.org/package/either-4.3.3.2/docs/Data-Either-Combinators.html#v:leftToMaybe) in [`either`](http://hackage.haskell.org/package/either). There's also `rightToMaybe` and several other combinators.
I'm not sure whether OP has edited the code into his post after you've read it, but almost the same code is already there ;).
&gt; I will say this: deploy binary to server that just freaking runs and never crashes (OK haven't been in prod very long but over a month in prod-like UAT), uses an order of magnitude less RAM than comparable Java services and starts up way quicker. VERY happy with the stability and performance. This is great news. It concurs with what I'm finding. I'm currently (partially[1]) Haskellizing a company in Chicago and one of the things that I point out is that I know of about 25 companies that've moved to Haskell, but zero that were unhappy to do so and only one[2] that turned back. [1]: Not a total Haskellization. If we have 5 full-time Haskell engineers by this time next year, I'll be satisfied. 10 and I'll be ecstatic. [2]: I'm not going to discuss that case on a public forum other than to say that it wasn't Haskell's fault at all. Had zilch to do with the language. 
And `leftToMaybe` is defined as leftToMaybe :: Either a b -&gt; Maybe a leftToMaybe = either Just (const Nothing) So this seems to solve the problems. Also, the `either` package is something I consider almost essential. I think I'm using it in every application I'm currently working on.
why are you porting the clojure to clojurescript, rather than say writing the feature in Lux and compiling itself to JavaScript? (honest question. you obviously understand language design more than me, maybe bootstrapping is too hard/inefficient)
Awesome! Put it up high on my reading list. Thank you to the late Paul Hudak!
I once mistyped that and then left it, liking the way it looked. thanks for reminding me
yeah, in a library of mine, for GHC 7.10, I think I'll have to rename my nice &amp; operator to -&amp;- well, at least it doesn't conflict with lens anymore heehee.
I prefer the bizarre and confusing way: primes = 2 : [3..] `knockout` composites primes merge x'@(x:xs) y'@(y:ys) | x &lt; y = x : merge xs y' | x == y = x : merge xs ys | otherwise = y : merge x' ys composites (p:ps) = p*2 : [p*3, p*4..] `merge` composites ps knockout (x:xs) y'@(y:ys) | x &lt; y = x : knockout xs y' | otherwise = knockout xs ys
I'm not sure I like tying all your FRP primitives to `IO`. The code is also not particularly exception safe, despite being in the only context (IO Monad) where you can reasonably deal with asynchronous exceptions. I'm not sure if other libraries do this, but it seems like you may want to have events / behaviors that don't fire / update unless the value actually *changes*. I'm a little concerned that this simple approach might be *simplistic* to the point of have problems with event cascade / avalanche. I still quite enjoyed the article. I've yet to try my hand at a non-trivial FRP program, so my comments might just be showing my naivety.
If the only options you allow people to give are "all" or "nothing", you'll get nothing most of the time. I'd encourage you to take the parts that work and possibly contribute back improvements.
Your best option, going from this and the comment below, is to parse three code and put out something that you could read in another tool (or template Haskell). http://eli.thegreenplace.net/2011/07/03/parsing-c-in-python-with-clang is about C++. This may not be very much work, or a whole lot, depending on all kinds of factors.
You are basically creating a singleton type for each Nat type and mapping between them, right? Doesn't Proxy (_ :: Nat) do the same thing except without creating a new data type?
If you liked the idea of a short-but-na√Øve implementation of FRP but didn't like the IO, you might be interested in my [homemade FRP](http://gelisam.blogspot.com/2014/07/homemade-frp-study-in-following-types.html) implementation from last year, as it's not using IO.
I was looking at your [fltkhs](https://github.com/deech/fltkhs) for inspiration (it looks like a ton of work and you started by making that massive C binding). I have a similar deal. Nothing really hard per-se, just a huge amount of objects full of getters and setters. I wasn't thinking to use Parsec to parse C++. There's just so much that's crazy about C++ parsing. Is that what you did? I'd be interested to see how.
\#LensDidIt
That's a great idea! I don't have much experience with the arrowized FRP libraries, but I'm planning on delving deeper into them, so maybe in a few weeks...
[Internet Archive](http://web.archive.org/web/20150129012424/http://code.haskell.org/~dons/talks/dons-google-2015-01-27.pdf)
Keep watching the home page at http://icfpconference.org/icfp2015/index.html -- I expect the call for participation will be linked soon! You can also follow https://twitter.com/icfp_conference to get the latest news.
indeed, fficxx is fine, its mostly that the student bit off way more than they could chew and it was a project that really had more design complexity than is appropriate for a GSOC.
you can also do `_ &lt;- ...` or `() &lt;$ ...`
If you don't care about indexing but just want some guarantees about the minimum length that is available (safe head, tail, etc) there is a very similar implementation available in the mono-traversable package now. * http://hackage.haskell.org/package/mono-traversable/docs/Data-MinLen.html * https://github.com/snoyberg/mono-traversable#minlen
Thanks everybody for the feedback.
Its great to see this, thanks. Perhaps you should consider putting the GHC version in the URL, when this starts becoming more of a permanent feature than an experiment.
What is the http cmd tool you use?
More importantly... is there any chance the talk was recorded so that we may see a video of it at some point? :)
Thanks, I'll make that note :) I'll also consider adding a section/note on using singletons with these instead of these ad-hoc typeclasses. thanks!
`cabal install hoogle`
Ok, that makes sense. So it is only really an issue when the underlying compiler changes, and only for people making use of the nightlies. And people using the nightlies should be aware of this.
I asked Don before. Unfortunately the answer is no. The recorded talk won't be publicly released due to legal reasons.
Author of reactive-banana here. Alas, I would like to dispel the non-myth that FRP libraries are easy to implement -- they are not. Stephen Blackheath (the author of sodium) sees it the same way. Believe me, if they were, the source code for reactive-banana would be a lot simpler. :-) It is certainly possible to put something together that covers most aspects in 200 lines. For instance, the [`Reactive.Banana.Model`][1] module, which records the definitive semantics for reactive-banana, is just 150 lines long and should be fairly simple to understand. Of course, while it's *the* reference for FRP semantics, it has the significant drawback that it cannot be used in an IO context. :-) Unfortunately, covering *all* aspects is a lot harder. Here are the tricky issues: * *Deterministic ordering*. If two events occur simultaneously, the `merge` combinator should put the occurrence from the left argument before the occurrence from the right argument. (By the way, you can use the non-deterministc `merge` and `calm` to implement a deterministic `merge`.) * *Recursion*. It should be possible to use the `Behavior` obtained via `accumB` (`hold` in your code) recursively with [`apply`][apply] in the argument of the accumulator: behavior &lt;- accumB 0 $ apply ((+) &lt;$&gt; behavior) event1 * *Sharing*. This means that if you use an event twice, say `event1` in let event1 = fmap expensive event0 in merge event1 event1 then its value should be cached and not be recomputed. And the really tricky issues are: * *Garbage collection*. Unregister event handlers that do not produce an observable result, for instance because they were `switchE`d away. Last, but not least, I didn't read your code thoroughly, but I think there is a subtle bug in the `switch` combinator: * `switch` for Behaviors needs to add an occurrence to the `_behaviorUpdates` event whenever the switch itself happens -- after all, the Behavior just updated. In contrast, `swichtE` for events does not add an occurrence at the moment of the switch itself. [1]: http://hackage.haskell.org/package/reactive-banana-0.8.1.2/docs/src/Reactive-Banana-Model.html [apply]: http://hackage.haskell.org/package/reactive-banana-0.8.1.2/docs/Reactive-Banana-Combinators.html#v:apply **EDIT**: It's called "sharing". The more specific "observable sharing" is an implementation technique to achieve the former. 
:) "... imprecise descriptions like ‚Äúselecting objects,‚Äù taken straight from the hunter-gatherer lexicon of our ancestors."
In case this helps, we do provide a programmatic way to determine the GHC version used by each nightly: https://www.stackage.org/nightly/2015-05-05/ghc-major-version
I myself would like Data.Maybe to be more extensive in it's utility/transformation functions. I'm using my own module Data.Maybe.Extra where I have these two utility functions: eitherToMaybe :: Either a b -&gt; Maybe b eitherToMaybe = either (const Nothing) (Just) maybesToMaybe :: [Maybe a] -&gt; Maybe [a] maybesToMaybe [] = Nothing maybesToMaybe xs = toMaybe $ catMaybes xs where toMaybe [] = Nothing toMaybe ms = Just ms
please have an RSS feed :D
by the way, the next iteration of netwire, `wires` will also have the same behavior. There will be no implicit inhibition...only explicit `Wire m a (Maybe b)`s :) so you're in good company!
this is great :) thanks for this, I'll definitely add a mention about this style.
You're welcome. Forgot to mention that this approach also allows to implement deforestation using rewrite rule: toCvec . fromCVec ‚áí id It could be generalized to arbitrary product types AKA heterogeneous vectors (fixed-vector-hetero)
&gt; I'm pretty sure you can make calls to unmanaged APIs using the Marshalling stuff, but in order to make it friendly you need to write a wrapper on the managed side of things. yep. f# has exactly the same problem for c++ stuff. for C you can pinvoke/dllimport etc and for C++ you need a full fledged wrapper just like haskell. For mixing C++ and .NET you can at least use C++/CLI but this does not work on mono. in total i think the situation is pretty much the same.
...since you weren't explicit... what would you consider a "decent" CPP library? Does `cpphs` fit the bill? :-)
Minor thing, you could also use `Data.Maybe.listToMaybe` and remove the empty list case as `catMaybes []` works just fine: maybesToMaybe = listToMaybe . catMaybes
I'm not using listToMaybe since I want to return the entire list, not the head. For `[Just 1, Nothing, Just 2]` I want `Just [1,2]` and for `[Nothing, Nothing]` a `Nothing`
Ah, that's what you mean by observable sharing? Not sure why you call it "observable", but in that case, the trick of using a trie which I promised to blog about (still in the pipeline!) when you last told me about observable sharing should indeed cover that case. To review, my na√Øve implementation of FRP was to represent an event as a function from some global event `t` to the list of events triggered by this `t` and a new function ready for the next `t`. {-# LANGUAGE DeriveFunctor #-} data Event t a = Event { runEvent :: t -&gt; ([a], Event t a) } deriving Functor merge :: Event t a -&gt; Event t a -&gt; Event t a merge e1 e2 = Event go where go t = (xs1 ++ xs2, merge e1' e2') where (xs1, e1') = runEvent e1 t (xs2, e2') = runEvent e2 t With this version, there is no sharing, so if `event1` occurs two times in the event graph, its function will be evaluated twice. expensive :: Int -&gt; Integer expensive 0 = 1 expensive 1 = 1 expensive n = expensive (n-1) + expensive (n-2) &gt; let event0 = Event (\t -&gt; ([t], event0)) &gt; let event1 = fmap expensive event0 &gt; let event2 = merge event1 event1 &gt; :set +s &gt; fst (runEvent event1 33) [5702887] (7.93 secs, 1,877,064,232 bytes) &gt; fst (runEvent event2 33) [5702887,5702887] (13.51 secs, 3,742,726,248 bytes) Combined with recursion, this led to really bad performance. The solution I found is to use a trie to cache the result of the function: {-# LANGUAGE DeriveFunctor, TypeOperators #-} import Data.MemoTrie data Event t a = Event { runEvent :: t :-&gt;: ([a], Event t a) } deriving Functor merge :: HasTrie t =&gt; Event t a -&gt; Event t a -&gt; Event t a merge e1 e2 = Event (trie go) where go t = (xs1 ++ xs2, merge e1' e2') where (xs1, e1') = untrie (runEvent e1) t (xs2, e2') = untrie (runEvent e2) t Then the top-level definition of an event becomes a large trie of all the possible sequences of `t` events which could occur. Since the trie is constructed lazily, only a single path through the structure is evaluated, and the nodes along this path remember all the results which have already been evaluated. This means that if our graph contains two copies of a given event, we no longer need to evaluate it twice: &gt; let event0 = Event (trie (\t -&gt; ([t], event0))) &gt; let event1 = fmap expensive event0 &gt; let event2 = merge event1 event1 &gt; :set +s &gt; fst (untrie (runEvent event1) 33) [5702887] (7.94 secs, 1,872,392,688 bytes) &gt; fst (untrie (runEvent event2) 33) [5702887,5702887] (0.00 secs, 1,549,512 bytes) &gt; let event2 = let event1 = fmap expensive event0 in merge event1 event1 &gt; fst (untrie (runEvent event2) 33) [5702887,5702887] (6.65 secs, 1,871,888,616 bytes) And the best part is: as we dive down deeper and deeper into the trie, ghc is smart enough to garbage-collect the earlier parts of the trie, so we use a constant amount of memory. main :: IO () main = do print (loop e0 0 8000000) print (fst (untrie (runEvent e0) 1)) accum1 :: Int -&gt; Event Int Int accum1 s = Event (trie (\t -&gt; ([s+t], accum1 (s+t)))) e0 :: Event Int Int e0 = accum1 0 loop :: Event Int Int -&gt; Int -&gt; Int -&gt; Int loop e n maxN = if n &lt; maxN then loop e' (n+1) maxN else head xs where (xs, e') = untrie (runEvent e) 1 Since the second print reuses the trie, the entire trie is forced to remain in memory: &gt; ./Main +RTS -s [...] 3,415,941,800 bytes maximum residency (15 sample(s)) But with the second print line commented-out: &gt; ./Main +RTS -s [...] 940,925,648 bytes maximum residency (17 sample(s)) 
The GHC compiler size table is interesting. GHC binary ranges from 272M to 1.1Gb depending on options!!! &gt; No doubt there will be performance trade offs in size vs performance. Measuring that would make it super interesting and extremely valuable for everybody, I think. EDIT: the whole post is interesting :-) not just the GHC compiler sizes...
In particular, left = preview _Left right = preview _Right
I use the [`errors`](http://hackage.haskell.org/package/errors) package. There are a bunch of functions like this in the [`Util`](http://hackage.haskell.org/package/errors-1.4.7/docs/Control-Error-Util.html) module. You just need to import `Control.Error`. You might want to read the related article on [Ollie Charles's blog](https://ocharles.org.uk/blog/posts/2012-12-04-errors.html) and also [Gabriel Gonzalez's original article](http://www.haskellforall.com/2012/07/errors-10-simplified-error-handling.html).
So, I [modeled the compilation options](http://t3-necsis.cs.uwaterloo.ca:8091/GHC/Options) and it resulted in 36 configurations (click on "Configure with ClaferConfigurator" button, type 40 instead of 10, and then on "Run" in the "Instance Generator" window). You have 34 configurations (and 15 for GHC itself) and I must not understand something. The version of LLVM you use only matters when the GHC Backend option `llvm` is chosen, right? When you choose `asm` LLVM does not matter? Or is it rather the property of the existing GHC you are using for the build? Or should LLVM matter only when you choose `perf-llvm` option? Can you have a different LLVM for `perf-llvm` and `llvm` options? 
From the FAQ: Will the tutorial be videotaped? No. The ally skills tutorial is heavily participation-based and it is important that the tutorial be a safe space in which participants may admit ignorance and make mistakes without fear of reprisal. However, the full curriculum for the tutorial is available online under a [free documentation license](http://geekfeminism.wikia.com/wiki/Allies_training). Also, if you work at a company, you can contact [the Ada Initiative](https://adainitiative.org/what-we-do/workshops-and-training/) about teaching a workshop like this one for your colleagues at a very reasonable price.
Thanks for providing another example of why this workshop is so badly needed! üíú
You didn't answer my questions.
Thanks for this. All the explanatory material I found previously about doing this kind of thing left a lot to be desired.
He mentions on [his project page]( http://projects.haskell.org/cpphs/) that we are free to contact him. OP, you interested in doing that?
I build small web applications for work (I usually use yesod). A lot of failures are modeled with `Either`, and I occasionally need to convert to `Maybe` (to lose information about the error). I also find `mapLeft` and `mapRight` to be pretty useful. `mapLeft` is good for transforming the "error" values or just for appending things to the beginning. And I like `mapRight` because, even though it's just a specialized `fmap`, it's more monomorphic and in some contexts can be easier for me to read. I seldom use `EitherT` itself though. Also, I haven't seen this used anywhere yet (except in a `vinyl` demo), but now that we have `Data.Either.Validation`, I hope to see more libraries use this rather than implement their own.
Yeah, IIRC I had a few infinite recursion errors because of shadowing.
Why would I put comments from random MRAs trolling functional programmers for lulz on slides for a workshop for functional programmers? Protip: when you're pretending to be a Haskeller, use a different sock account than the one you use for trolling Haskellers on /r/programming.
you have installed `stackage-cli - 0.0.0.4`. The latest version is `stackage-cli 0.1.0.1`. I'm having trouble with the latest version.
Aw, you're cute when you're defensive üòä
I want to learn Haskell and I live in Nottingham. Can I have the job?
Isn't digest auth over HTTP still considered pretty insecure?
[Erik's recommendation](https://github.com/haskell/cabal/issues/2561#issuecomment-96920764) was to disable ~~cabal update~~ `cabal upload` entirely. Hopefully /u/sclv's work on getting SSL support into cabal-install will land quickly, but it seems that in the interim the cabal team has decided to not go with Erik's approach.
Sorry for the confusion. :( The plugins are available as follows: * (stackage and stk are in stackage-cli) * init, purge, and upgrade are in stackage-cabal * sandbox is in stackage-sandbox Feel free to add an issue to any of the relevant issue trackers. Documentation will soon be updated. Note that as an alternative to cabal, you will soon be able to get the stackage suite of tools via http://www.stackage.org/download, although I'm still working on a few things to polish that up so maybe stick with cabal for now.
I wasn't asking why you weren't answering my questions; I'm pretty sure I know that already. Nor will repeating lies make them true. I'm genuinely sorry that you're so resistant to inquiry (given the lack of anything even vaguely resembling either humor or art in your subsequent comments, I've set aside the performance art/satire hypotheses). I remember being in that state of mind, and will no doubt be back there again, and it was a pretty piss-poor way in the long term to cover up my pain and hurt.
typo alert: update -&gt; upload 
I use the same technique of bundling the necessary dynamic libraries and gconv files for the git-annex standalone tarball distribution. I've observed users untarring that onto arbitrary wacky NAS boxes and it just working, which is pretty nice. I later adapted that for use in propellor too, when it's deploying docker/etc containers. This allows propellor to bootstrap itself to run inside an arbitrary docker container. http://joeyh.name/blog/entry/propelling_containers/ Later, I used propellor's ability to bootstrap itself this way to let it copy itself onto a VM and run there to replace its content with a clean reinstall of Debian. http://joeyh.name/blog/entry/clean_OS_reinstalls_with_propellor/
* Yes, following the jQuery numbers makes it impossible to follow the PVP. Having a higher 0.1 when jQuery does is fine, since that's probably a point when PVP says I should 0.1 bump anyway. Changing the interface without changing jQuery is much harder - so far I was just very careful with designing an interface in the beginning. It's certainly a drawback, but I considered it the best trade-off. * Yes, you could embed the file, but then I'm picking a String-like library (would you like a String, a UTF8 bytestring, Text), adding Template Haskell dependency and probably other complications. It's a reasonable way to go, but I wanted to keep the dependencies light. You can easily combine the two by getting the file at compile-time in the Q monad - I should probably give an example for that. * I hope we don't have a Haskell package for each JS package, and jQuery is the most used by quite a way, so it's worth doing something special. If someone replaces this with a better and more direct JS-specific Haskell-integration thing, I'd be very happy.
I updated the gist with a version that uses TypeLits. Not quite sure what your problem there is. It seems to work just as expected for me.
Or how about just using `sequence`? Edit: never mind, the function in question didn't do what I thought it did.
Especially because of the whole mutable state thingy.
Very cool! I've often wanted a type level debugger / have GHC generate an explanation of its inference process. Something else you might be interested in is type error slicing: http://homes.cs.washington.edu/~djg/papers/seminal_pldi07.pdf The principal is essentially that we can try out different replacements of sub-expressions with "undefined". This causes a deluge of info, but it makes it more likely that a very accurate type error is somewhere in the results. Might get some interesting results by mixing the two tricks. Personally I'd rather have GHC tell us info than cleverly driving it to yield the info, though.
Also, use fpcomplete's Hoogle: https://www.fpcomplete.com/hoogle It indexes more stuff. 
That is pretty cool! I'd gotten to that point but didn't realize that the examples needed to be expanded. That might be a good thing to consider when adding support for such things in haddocks, since it means the examples take up less screen real estate by default. This might make people more likely to add examples, which IMHO would be huge for improving Haskell documentation.
Hmm, I suppose we will, good idea! I've noted it down. I'm not sure if comments will be supported in the initial version.
I used (D)COM for this, but that's also not cross platform, and in any event the code you're working with has to support COM.
If you are generating the HTML file, just append the output in at a relevant point. Criterion is using the hastache library which is a templating library supporting splices, so that defines a new variable js-jquery to split in, and at the relevant point, defines what it means. Shake also has some variant of templating, using [this module](https://github.com/ndmitchell/shake/blob/master/src/General/Template.hs).
...ask him in a polite, non-threatening way I assume? =)
&gt; You could make the first number in PVP match up with jQuery's: 20401.x.y.z for jQ-2.4.1. Chances of jQ's number exceeding 99 are low. Yep, that would work. The current approach has the advantage that people can say "I want jQuery == 1.11.2", and the Haskell constraint matches up. I think in reality I'll just only change the API on a jQuery/Flot minor version bump (or ideally not even then). 
Interesting that the University of Nottingham is using the American terminology "Assistant Professor" rather than the British "Lecturer".
yes. *edit*: to be more specific, read my answer to a similar question an stackoverflow. https://stackoverflow.com/questions/28284039/can-i-write-a-function-using-datakinds-that-returns-a-value-of-type-encoded-by-t/28286213#answer-28286213
This is a great article, very comprehensive! I wish there was an `OverloadedLists` extension that didn't give runtime errors. Perhaps by not defining lists in terms of `fromList`, but in terms of `nil` and `cons`?
It's not very surprising when you consider what the options actually do! But I would say that. `quick` builds for example, imply 'optimized libraries with -O, unoptimized compiler with -O0'. An unoptimized compiler means during the build (of the compiler itself) we do very little inlining, so there is very little 'code growth' due to cross-module inlining across all these hundreds of modules. On the other side of the table, we have `perf`, which implies "libraries optimized with -O2, GHC optimized with -O2". This kicks up inlining a lot, which will increase binary sizes for the final build tree just for the same object files - they will get bigger. Finally, building in `perf` mode also does things like "build a profiling copy of every library", and nowadays we also build a *dynamic* version of these libraries. So it's not just the number of modules: the size also grows due to the multiplier of having libraries built different "ways". So `perf` also probably has a multiple copies of every library too. All-in-all, ~200mb to 1GB doesn't surprise me. But there are surely some tricks to help alleviate this (I may have one I speculate could drop the binary distribution by maybe 10%, but I haven't tested that).
aw no bachelors :(
`stackage purge` deletes the `cabal.config` file in the sandbox folder, if the purge is cancelled. (choosing `n`). Looks like a bug. Or is it the way it is supposed to be?
/u/ndmitchell I tried to write a PR to switch `cmdargs` over to `js-jquery`, but I could not get it to play nice for some `autogen`-related reason that I couldn't debug. I'd be interested in seeing the right way to bring the jQuery path into scope so it can be integrated properly.
&gt; How does `Event Int a` differ from a simple (infinite) list `[[a]]`? The latter lacks interactivity. I can obtain a complete list (of list) of mouse click events and then transform the whole list into a complete list (of list) of frames to draw, but (without unsafePerformIO) I cannot lazily append the mouse clicks to the first list as they happen and lazily read off the next frame to display from the second list. Unlike reactive-banana, I eventually instantiate `t` to the type of the input events, so my function representation allows me to step my entire event graph forward each time I receive an input event. The trie version is equivalent, it simply encodes all the possible frames which can be given in response to all the possible button clicks. A list (of list) of frames would only encode one particular history, not all possible histories, which is too restrictive since I don't yet know which history I'll be interested about at the moment at which I construct the graph.
I'll take a look later. It's only the cmdargs-browser that uses jQuery, and I didn't think anyone was using that. Or is this just a general attempt at cleanup?
There's an issue for that. :) https://github.com/fpco/stackage-cli/issues/44 Reducing dependencies is lower priority right now than building out new functionality, but it's definitely something we are thinking about. We want to make the Haskell tool chain as easy to get as possible.
When you perform a `cabal install`, use the `--enable-documentation` flag. I don't know of any way to retroactively add docs to already-installed packages, so you'll have to blow away the sandbox and start again. This is a use case where stackage purge makes sense, but personally I would just manually delete the sandbox.
Thanks for talking to him!
So we should keep the option in the build-system to be able to configure&amp;build GHC w/o `cpphs`-code (like we do for GMP via `integer-simple`)?
&gt; [update 2]: cabal install stackage-0.7.3.2 should install the plugins as expected. Yes, just installed `stackage-0.7.3.2` today and got all executables (on Windows). Thanks!
&gt; following the install instructions I get... Can you clarify which install instructions you are referring to, and at what point you're getting that error? It seems that cabal doesn't like your config file. It should recognize the "constraints" field is without barfing like that. Do you have anything weird going on with ~/.cabal/config?
I just set `documentation: True` in the global `cabal.config`
I was just looking for bugs to close on projects I had been using, and that one looked pretty tractable.
I followed the quick start instructions at http://www.stackage.org/. I think I have stock ~/.cabal/config. Atleast I can't recall ever editing that. I did have a cabal sandbox though. Can that be the issue?
This is very important!
Less than I charge, so he has that going for him. ;)
Here's how you might use [Ersatz](http://hackage.haskell.org/package/ersatz) to solve the SEND MORE MONEY puzzle: http://lpaste.net/132169
Actually, they just fixed that. It now outputs "haskell lang"
typo: &gt; without it's host without its host 
So you are trying to make money by being "skilled" in an obscure "fad", which is not new or surprising however you picked the wrong fad; your demo is people without money and without any industrial ambitions -- you'd have been better off choosing Clojure, which is a much stupider language, however it appeals therefore to stupid people, many who happen to live in Califnornia and also use MongoDB (not coincidental); i.e. the intersection of people who use clojure and people who have money is non-empty, although i'd prefer it were. I am sure your software is very "powerful" because of referential transparency, separation of pure and clean code, monads, lazy ("no it's actually non-strict") evaluation, and category theory. It's bold that you want someone to pay you money to ruin their company (by getting them to use Haskell, which is an academic toy language). You're aware that the "Haskell in Industry" page is comprised of almost all lies, right? I can hear you now, "but the advantages of haskell are referential transparancy... also trivially parallelizable, great multi-core support" except that that's a lie, haskell (and all functional languages) are horrible for paralleization because they don't play well with the cache; you can't abstract away from time and space without consequences, another reason why again a pragmatic company should never use a language designed by professional wankers. However, you used the word "hack", so I know you are down-to-earth and chill. 
Bof, "constants", "for boucle" with forM, I/O as first example... 
The instructions should be compatible with a sandbox. I don't understand why it is complaining about "constraints" listed in your home .cabal/config. The error message said it was on a certain line in that file. Have you looked at that file to see what it's talking about? I don't think the default cabal.config has any "constraints" field in it.
A few of these examples are misleading. The variables section completely misses the point of let bindings and starts off using it in "do" syntax. Also, "for"-loops or loops of any kind don't exist in functional languages (recursion is used instead), this example is using some built-in monad functions that are more complex than someone just starting can grasp. Don't get me wrong this is a cool project, but the original source was not geared for a functional, let alone purely functional language. In my university course on functional programming in Haskell we didn't even touch IO until 10 weeks into the semester, after covering the concept of a monad. I would suggest retooling your examples for a more functional oriented guide, so as to avoid giving newcomers the wrong idea about the programming paradigm and concepts.
The important part is that you managed to make yourself feel superior to an entire group of people while running away from any responses that might change your mind.
very useful tips from a practitioner's experience to an audience of practitioners. However it's not exactly for beginners; personally I'm not a big fan of "doing things because you trust your teacher". 
Agreed. I can see a new wave of people coming into #haskell and #haskell-beginners asking about non-idiomatic things from this set of examples. After clicking through the first few, I see a lot of imperative-style stuff I've never done in Haskell.
&gt; Also, "for"-loops or loops of any kind don't exist in functional languages (recursion is used instead) Saying this is like saying "while" loops or loops of any kind don't exist in imperative languages (cmps and jmps are used instead). Surely it matters little how the loops we use (e.g. `forM_`) are implemented, and what's interesting is how they behave?
Sorry. I tried to simplify the path given in the above post to remove some of my project specific stuff. Ended up mauling it beyond recognition. The cabal.config it complains about is not ~/.cabal/config but http://www.stackage.org/lts/cabal.config.
They may not be that useful to a haskell programmer but to a Golang hacker, it provides familiar ground. Equating Golang structs to Haskell types is missing the entire point of types in a functional language. Wadler's papers goes through why: [Propositions as Types](http://homepages.inf.ed.ac.uk/wadler/papers/propositions-as-types/propositions-as-types.pdf)
Thrift, protobuf, Cap'n Proto are as close as I'm aware of.
I don't like the term struct in this context, especially without an explanation that it is not used at all for records in Haskell.
Why do you believe that the way you were introduced to Haskell should be standard for all? If a "newcomer" sees highly-abstract functional code, what "wrong idea" will form then? It may be easier to attract people into a foreign domain by providing something that's familiar operationally. If said novice plods on, a functional style as in an understanding and application of type theory will emerge.
I agree with you, but not everyone does. Some people think this kind of "operational learning" is detrimental to understanding and will lead to more confusion in the end. I'm not quite sure, but I've had a lot of success by teaching operationally like you say.
A quick thank-you to Snoyman and co for finding the gconv dependency - I'd been stuck for days debugging containerized app that was exhausting all the memory on the box at startup, which turned out to be this exact problem.
&gt; Also, "for"-loops or loops of any kind don't exist in functional languages (recursion is used instead) Lazy lists are the Haskell equivalent of for-loops. In my university we teach lists and list compreensions to CS freshmen before recursion. It is surprising who far you can go with that.
That's not unreasonable for short term expert consulting...
AWS EC2 *is* powered by Xen.
You might check out [modern-data](http://ireneknapp.com/). Think protocol buffers, but self-describing and dependently-typed. If your types are fairly static, you might try encoding them as a [json-schema](http://json-schema.org/).
Hmm, the only one of these I've heard of is Cap'n Proto, which I couldn't find a Haskell implementation of. I'll check out the others.
&gt; You might check out modern-data. Think protocol buffers, but self-describing and dependently-typed. That's... wild and interesting. Presumably it would run in some type of compile-time AST-gen phase, like TH or Rust compiler plugin macro?
I stand corrected.
Thrift has a very slow Haskell implementation. See https://issues.apache.org/jira/browse/THRIFT-2236 Hopefully someone from the community who's got the Haskell chops to have it solved, reads this thread and does a PR for it :)
Honestly, I've not used it. The idea is intriguing, but it may not even be ready for use, yet.
Every community has its trolls.
Which is a shame because Cap'n Proto [used to be written in Haskell](https://capnproto.org/news/2013-08-12-capnproto-0.2-no-more-haskell.html).
Care to give a TLDR of the paper? 
More like Haskell by bad example. I'm normally a mellow guy and will excuse a lot of stuff in the name of dumbing things down for novices, but damn, translating a loop `break` statement as `error "break"`? I mean, holy crap, look at the last one in the "For" example: import Control.Monad main = do forM_ [1..3] $ \i -&gt; do print i forM_ [7..9] $ \j -&gt; do print j forM_ [1..] $ \_ -&gt; do putStrLn "loop" error "break" Better renderings of that one: forM_ (take 1 [1..]) $ \_ -&gt; do putStrLn "loop" -- Or how about the following, which is equationally equivalent... putStrLn "loop" 
The title makes it sound a bit more basic than it is -- this is full of really nice advice about all the things one wants to do besides just define raw data types in order to make these types more useful and rich.
The problem with all of these is that they don't support sum types. I'm considering writing an in-house data description language for that reason.
&gt; we didn't even touch IO until 10 weeks into the semester Did you do everything in GHCi up until that point?
What kind of obstacles (if any) did you run into while compiling pandoc with GHCJS?
I had to [fork](https://github.com/osener/pandoc) and butcher pandoc to remove native-dependent functionality and bits that either don't work well in a browser environment or not needed for markdown.rocks. I'm sure someone smarter will be able to compile vanilla pandoc with a more disciplined approach and careful flag toggling, though my fork also allowed me to bring down the output size. Also, currently closure compiler's ADVANCED_OPTIMIZATIONS flag breaks the app. 
Some of the comments in this thread are unbelievably vitriolic for no good reason. It almost gives one deja vu about the old Lisp forums. No doubt a lot of beginners will feel put off by this kind of attitude.
Before I answer, let me give some background for other people who might be less familiar with programming language theory. First, there's a technique to translate mutually recursive data types and mutually recursive functions over those data types to System-F called "Boehm-Berarducci encoding" ([this is the paper where it was introduced](http://www.sciencedirect.com/science/article/pii/0304397585901355)). I've implemented that in Annah, which also augments that to work with System-Fw (so that data types can have type parameters). I'm currently working on implementing the reverse direction (System-Fw back into data types), and while I have had some initial success, it's still an incomplete translation and a work in progress. If somebody knows of prior research that explains how to do this please let me know, otherwise I'll just keep working on figuring it out on my own. Second, `morte` lets you save expressions to files and reference those expressions using hash tags. For example, if I save this expression to a relative file named `id`: \(a : *) -&gt; \(x : a) -&gt; x ... then I can import that expression within a local program just by using this syntax: #id This hashtag import of files feature is on Github, but not released on Hackage, yet, so if you want to follow along you must build Morte from Github. **Answer to #1:** The goal is to permit the user to compute things at compile time (i.e. by normalizing expressions in the calculus of constructions as I blogged about [in this post](http://www.haskellforall.com/2014/09/morte-intermediate-language-for-super.html)) or at runtime, by embedding an abstract syntax tree representing computations that would map onto an external target language. That abstract syntax tree is what translate to language-idiomatic datatypes and functions. To spell that out, let's use the simple example of a boolean data type (i.e. `True` and `False`) and boolean operators (i.e. `and` and `or`, for example). In Haskell, we would encode that as: data Bool = True | False and b1 b2 = if b1 then b2 else False or b1 b2 = if b1 then True else b2 Now, there are two ways we could embed that in the core language. * Approach #1: We boehm berarducci encode the above `Bool` type and the boolean operators (eventually using `annah`, but right now I'll do it by hand): You would save this to a file named `Bool`: forall (Bool: *) -&gt; forall (True : Bool) -&gt; forall (False : Bool) -&gt; Bool You would save this to a file named `True`: \(Bool : *) -&gt; \(True : Bool) -&gt; \(False : Bool) -&gt; True You would save this to a file named `False`: \(Bool : *) -&gt; \(True : Bool) -&gt; \(False : Bool) -&gt; False You would save this to a file named `if`: -- Remember that a hashtag imports the file of the same name and embeds it as an expression -- We can use the file named `Bool` in the type of the binding \(b : #Bool ) -&gt; b You would save this to a file named `and`: -- Read this as: -- -- \b1 b2 -&gt; if b1 then b2 else False -- \(b1 : #Bool ) -&gt; \(b2 : #Bool ) -&gt; #if b1 #Bool b2 #False You would save this to a file named `or`: -- \b1 b2 -&gt; if b1 then True else b2 \(b1 : #Boo ) -&gt; \(b2 : #Bool ) -&gt; #if b1 #Bool #True b2 Then we could do compile-time computations using morte: $ morte #and #True #True &lt;Ctrl-D&gt; ‚àÄ(Bool : *) ‚Üí ‚àÄ(True : Bool) ‚Üí ‚àÄ(False : Bool) ‚Üí Bool Œª(Bool : *) ‚Üí Œª(True : Bool) ‚Üí Œª(False : Bool) ‚Üí True ... and Morte would deduce at compile time that it reduces to `True` and has type `Bool`. Ideally, then, we would use Annah to resugar that Boehm-Berarducci encoded `True` and `False` back into the equivalent data type, but that's still a work in progress. * Approach #2: We defer the computation to an external language (i.e. Python, Haskell, Scala, Javascript) This means that instead of embedding the `Bool` type, we would instead embed an AST of the `Bool` type and all of its operations, equivalent to this Haskell code: data Bool = True | False | Or Bool Bool | And Bool Bool Note that we would **still** Boehm-berarducci encode that to embed it in the calculus of constructions, but the difference is that normalizing the expression wouldn't do any real work. It would just produce a syntax tree that might look like this example: \(Bool : *) -&gt; \(True : Bool) -&gt; \(False : Bool) -&gt; \(Or : Bool -&gt; Bool -&gt; Bool) -&gt; \(And : Bool -&gt; Bool -&gt; Bool) -&gt; And True (Or False True) In other words, we defer the actual computation to whatever our backend language is. That language would translate that syntax tree into its own language and then perform the computation at runtime instead of compile time. So what I'm going to do (as part of `annah`) is actually provide **both**. There will be a large set of compile-time operations that you can use for anything that you want to be evaluated at compile time, but there will also be a (very large) AST of shared operations that all external languages must support that would be executed at runtime. Both sets of operations (compile time and runtime) would be embedded within the language (under separate namespaces, which in `morte` just means separate directories/paths). In this hybrid compile-time/runtime setup, the compile-time language is solely responsible for assembling the "runtime AST" and the runtime AST is actually what gets mapped onto language-specific data types and functions. If you are familiar with thrift/protobufs, you can think of this "runtime AST" as being "thrift for code". It's a shared set of data types, operations, and IO actions that all target languages must support so that they can share useful code that can perform high-efficiency data manipulations and side effects. **Answer to #2:** Morte supports newtypes and the distinction in the types is preserved in the core calculus. See [this section of the morte tutorial](http://hackage.haskell.org/package/morte-1.1.2/docs/Morte-Tutorial.html#g:5) to see how newtypes are encoded. Morte encodes them in such a way that they are not alpha-beta-eta-equivalent to the original type. **Answer to #3:** The most likely scneario is that I will downgrade Morte to System Fw. The only reason I originally kept the dependent types functionality is that it's the only way I know of to enforce type class instance coherency. For example, if I wanted to enforce the coherency of the `Ord` instance for a `Set` operation like `union`, I would model that in the types as: union : forall (o : Ord) -&gt; Set o a -&gt; Set o a -&gt; Set o a In other words, the `Set` type would be parametrized on the `Ord` instance it uses and the above type ensures that the two `Set`s use the same `Ord` instance when we `union` them. However, I won't know for sure whether or not to keep dependent types until I flesh out the shared runtime AST that all target language support.
The whole cmdargs-browser stuff needs a fair bit of polish - it worked, but I never really documented it etc. It needed a Wai upgrade, and still needs to be made to work with the latest jQuery. However, the actual convert to js-jquery was pretty easy: https://github.com/ndmitchell/cmdargs/commit/2dd097b39b92e91e722ac3f31ba07804bb7ab3f3
Yes, we used ghci for a good portion of the semester. I am not advocating for the specific course structure, just giving my experience. 
I will freely admit that I actually do not understand the trade-offs that well. My decision to use a pure type system was entirely because Simon Peyton Jones' paper named [Henk: a typed intermediate language](http://research.microsoft.com/apps/pubs/default.aspx?id=67066) spelled out how to implement it in excruciating detail. The choice of which pure type system to use was completely arbitrary: I randomly chose the calculus of constructions initially but I may eventually downgrade to System Fw. The second (minor) reason was that I was much more familiar and comfortable with System Fw primarily through Haskell since that's (close to) what it compiles to (or at least used to before they permitted dependent types to the core language, from what I hear) and I was already very familiar with many important results of lambda systems. I've never been really into type theory in general, so I was never really exposed to Martin-Lof type theory. Almost everything I know about type theory is what I've learned via exposure to Haskell.
Cool! Well, anyway, Morte looks pretty cool. Hope you're having fun with it!
You don't even have to "just use it," there are simplified explanations that can be offered that will help. For example, you can refer to `getLine` and `putStrLn "Hello world!"` as *actions*, drawing a distinction between them and functions; a function can never execute an action, and you use specific constructs (`&gt;&gt;=`, `do`-notation) to combine actions. You can also use these constructs to define helper functions like `forM_` to cover common patterns you will find in code: forM_ (a:as) f = f a &gt;&gt; forM_ f as
The more natural `Foldable` instance for `Sum` and `Product` is the one that extends to a legal `Traversable` instance, and you get it out of the box with `deriving Foldable`. 
&gt; I don't want a runtime error when I test my app that says "Error when parsing Cookie with 'Snickerdoodle'". I know you don't, but as long as you consider the other end "untrusted", it is a simple fact of life; compile-time checks cannot possibly cover things that do not happen at compile time. Compile-time checks cannot guard against network issues, version mismatches, deployment failures, etc. The single-source-of-truth approach reduces redundant manual labor and the errors associated with it, which is very desirable, but it doesn't give you cross-language type safety, and you can still end up with code on both sides that typechecks and compiles fine, and then deploy one but not the other and have things catch fire.
&gt; I've never heard of any of these (they seem to be from before my time), but I don't see how your language could have a semantic, compile-time-type-checking awareness of the AST described in them unless that was somehow hackwired on to the compilation process for each language. Well, with SOAP, the typical workflow is something like this. First, you write a class that represents your API, and you use decorators to indicate which methods to expose as SOAP calls. The magical SOAP library then takes this information and exposes the class as a SOAP web service, and part of that is a WSDL, an XML document that describes the service and its methods, and links to a schema that describes the types that are involved in these methods. On the client, you then import the WSDL and use it to generate a "proxy" class, which mirrors the methods on the server, but forwards them all through the magic SOAP library and has it call into the server. So pretty close to the "single source of truth" approach, but more convoluted, and with the server unilaterally acting as the authority over the types - if the server changes, the client becomes invalid without further ado and stops working. This change management part is one of the reasons why people started dropping SOAP the instant JSON made its appearance.
yeah that guy/gal might even be worth more, given the costs of improper security.
Been meaning to try this for quite a while, but never got around to it... very cool to see that it's possible; I might even have a use case for this.
This is awesome! BTW, I'd use something simillar to lodash's `debounce` to avoid it refreshing while I edit, I only need it to refresh when I stop typing.
Frege. However, not sure if that counts, as many see it as a Haskell dialect. 
It is great. I had this dream once that there was an app is just like https://regex101.com/ but for parsec and attoparsec. In the left I was writing code and in the right it was highlighting text real time. All tokens were in different colors and it was bright and sunny outside. This reminded me of that dream.
It's hard to come up with a graph representation that works for everyone, as it really depends on the connectivity of your graph.
[PureScript](http://purescript.org). Although "other than Haskell" is debatable since the type system was so influenced by it.
Awesome! So since it is based on Pandoc, can you add LaTeX math support in Markdown?
Ooh, thank you!
The ML module system (or at least its OCaml realization) is essentially equi-expressive with Fomega (but very different in its programming aspects) (and that is more expressive than just higher-kinded type *constructors* alone (without type-level computation tweaks such as type families)). However, programming with HKT is challenging, if only because the module system has a high cost syntactically. You should have a look at the article on [Lightweight Higher-Kinded Polymorphism](https://github.com/ocamllabs/higher) by Jeremy Yallop and Leo White, which discuss the issues and provides a library implementation of a nicer-surface approach.
This is something I had to remove manually because it would bring the browser to a crawl indefinitely. I might add a JS based Markdown -&gt; HTML backend to do tha in the futuret.
Thank you! Yeah, I need to inspect ghcjs output to find what breaks closure compiler's advanced optimizations as it cuts 1 MB from the gzipped size.
Since you care about output size, did you try compiling it with Haste? 
Wow, this looks great!
somebody really should do something like https://www.youtube.com/watch?v=5-OjTPj7K54 for haskell just saying...
Thank you for all your help and being so patient with my questions :). And thanks a lot for reflex of course!
Can you try again? Right now it won't trigger a render if the process takes more than 300 milliseconds where it starts to feel laggy. 
Chung-chieh Shan and Dylan Thurston wrote an excellent series of articles on this exact problem. http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers1/ http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers2/ http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers3/ http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers4/ Any prefix of that article series would serve as a very strong "hint" as to how to do this efficiently, but overall they wrote up a technical tour-de-force. I learned a lot by reading through that series.
This is something GHCJS [supports](https://github.com/ghcjs/ghcjs/wiki/Deployment#multiple-pages--incremental-linking) although when I tried it I had a problem with it not recognizing `--generate-base` flag, I definitely need to revisit this again.
I used Clojure and Common Lisp before shifting everything over to Haskell. Honestly, ghci is nicer than the lein REPL, Emacs integration or not. I provided some of my thoughts and [those of others that used Scheme/Lisp before in this post.](http://bitemyapp.com/posts/2014-04-29-meditations-on-learning-haskell.html) Workflow differs in that I can start with a higher level, simpler/cheaper to iterate abstraction before I start writing real code (types). I, kid you not, will manipulate "bottom" or undefined values with types associated with them until I get the type I want, *then* I'll write the code. I've been shopping this pattern around to see if other people do this - some do but many don't bother. I think it's something that was forced upon me because I was punching above my weight on something. Analogous to working with a proof assistant (please don't nitpick me on this). &gt;How about the debugging experience? Eh. Kinda sucks if you want breakpoints, but I almost never do this and the limits of my debugging will boil down to using trace/logging. But this really is quite rare, at least for me. I've had programs in Python and Clojure I couldn't figure out until I stopped debugging and started logging. &gt;Are there significant differences in library availability? No, but there's more diversity in library *style* so you won't always find a Haskell library that suits your "Haskeller personality" - this isn't a big deal. First world problems. You may need to write some trivial wrappers around a third party API, but if you're a Lisper, this should be nothing new. &gt;How readable/maintainable is the average code to the average skilled lisper/haskeller? Haskell code is *way* more maintainable for my dumb brain :) &gt;Have you experienced significant differences when collaborating in each? Types help with collaboration/coordination a lot, IME. One thing that Lispers will be confused by is that Template Haskell, while having all the power of Lisp macros, isn't really used in the same ways. We have non-strict evaluation semantics so having first-class language constructs from functions is a given without the use of macros. Instead, TH is primarily used for boilerplate killing - often to auto-generate typeclass instances a la Generics (another option). Also, using macros in a language that typechecks the results is really quite nice. &gt;What types of projects do you find more suited to one language over the other? I'd use Haskell for just about anything I do in my day-to-day (backend services). I don't use a Lisp anymore. One irritation: make GHCi save the state between reloads dammit. Wipe 'em if they don't typecheck, but preserve them if they do please! (Twan has complained about this too) I end up sticking experimental definitions in my source files and loading into GHCi, removing them when I'm done writing the code or farming them out to a util module.
There's a deep correspondence between types and logic also known as the Curry-Howard Isomorphism. So, types as models of computation mirror a intuitionistic proof system, i.e.: * simple types correspond to (intuitionistic) propositional logic * dependent types correspond to (intuitionistic) predicate logic The mapping for simple types is the following: * conjunction &lt;-&gt; product types * disjunction &lt;-&gt;sum types * implication &lt;-&gt; function types * true &lt;-&gt; unit type * false &lt;-&gt; bottom type You can work out the predicate notions of logic. You may want to give Wadler's paper a go. It's a good introduction the Curry-Howard Isomorphism and of all the academic papers it's approachable to non-academics. 
Exporting to a Gist is not a bad idea actually, I might look into that. You're right, I've added that link in a rush after I realized I don't have an about page or anything that links to the project other than the initial example document. [My initial design](http://i.imgur.com/TCdcgzk.png) had plenty of space for this kind of stuff but I think a minimalistic one that focuses on the content is better for this and I didn't want to resort to an ugly "Fork me on Github" ribbon that takes away from that. About publicization, may I ask what do you have in mind? I've also posted this to [Hacker News](https://news.ycombinator.com/item?id=9512293) but they don't seem to be interested in this sort of thing. Please feel free to help in this regard, I have no intention of monetizing this.
Have you tried UglifyJS? It has given me similar reductions as closure compiler advanced but doesn't break ghcjs output. 
Is this bug still present: https://ghc.haskell.org/trac/ghc/ticket/9022 As a Debian unstable user I'm keen to know. And, you know. For other reasons.
I switched from Common Lisp to Haskell. SLIME is a superior development environment, for information, navigation, inspection purposes, and Lisp images are better for faster iteration of code and developing of live running systems (e.g. updating a rendering function while working on an OpenGL app or a web server). Lisp's syntax is superior. Its macros and metaprogramming are much better. Implementations like SBCL are much better at showing you what assembly your code will compile down to. Common Lisp's decimals throw an exception when you divide by zero, I consider this a superior solution to Infinity. Its native support for documenting functions via docstrings is simply better than Haskell's "tacked on" let's-use-a-bar-and-preprocess-the-module-with-haddock hack; languages should recognize that we document things. It also support 'symbols. And it has a conditions system with restarts which is superior to pretty much any other error handling system I've seen elsewhere, for development purposes, and the debugger just works. I got interested in Haskell because of its purity and functional programming, I held my nose when dealing with its horrible syntax (which I still do to this day, but I was really distraught back then at the Perl code I had seemingly been creating and nearly gave up), and then got roped into liking the type system, and a deeper appreciation of laziness beyond tricks came much later. Haskell also has much better set of standard classes; it has equality as a class, instead of the 5 or so functions and one for every data type that you have in Lisp, same for ordering, numerics, etc. Partial evaluation makes for pretty predictable argument order, argument order in Common Lisp is hard to predict. Most CL codebases I've worked on were not maintainable due to arbitrary side effects and no type system to support refactoring. CL is just a bad language for code that matters. Back when CL was my goto-language I found libraries and implementations lacking, but these days with Quicklisp and SBCL I think the situation is better. GHCi is pretty poor in comparison to SLIME, there's no comparison. Lisp suffers from ugly macros, Haskell suffers from operator line noise. Where Haskell makes code hard to read by trying to abstract everything with types, CL programmers make up for it with macros and abstractions without any static enforcement. Lisp also suffers from selfishness on the part of the programmer because of how the language is setup, Haskell code comparatively is made to be shared. Lisp functions like to be kitchen sinks with all the bells and whistles (the pathological case is the LOOP macro), Haskell functions like to do one thing like UNIX commands and lazily be glued together. Community-wise CL's has generally always been unfriendly and bitter in my experience. In Haskell you basically write assuming GHC. Lispers try to write "portable" code, where portable means that this code will behave the same on different compilers with preprocessors for each compiler, or by using dumber libraries with fewer dependencies. I consider this a waste of mental effort. In the end I care above all about maintainability and re-usability. For that Haskell is an obvious choice and has been since 2007.
&gt; One thing that Lispers will be confused by is that Template Haskell, while having all the power of Lisp macros, isn't really used in the same ways. That's because it doesn't have all the power of Lisp macros. :( To achieve the equivalent of `(foo (bar mu) zot)` where `foo` is a macro, you have to write `$(foo [ [| [bar, mu] |], [| zot |] ])` (or something equally horrendous, and hope that bar, mu and zot exist because if they don't TH will complain!), which doesn't quite have the same syntactic abstraction feel. If we had the full power of Lisp macros we'd've had lambda-case and multi-way-if years ago. All we have is splicing. We don't even have a way to re-use the GHC parser when resorting to [quasi|quotes|], which is why Nikita's record package is so syntactically limited.
Re: deeper appreciation of laziness - I've pointed to resources like Let over Lambda for getting an appreciation of macros in the past. Do you remember where your appreciation of laziness came from?
Agda has this in spades.
Not really, just over time from programming in Haskell. But Lennart's post on laziness sums up my feelings pretty well: http://augustss.blogspot.hu/2011/05/more-points-for-lazy-evaluation-in.html
I always assumed Haskell didn't have python/lisp style docstrings because of referential transparency. But now that I think of it, I guess we could have a `Documented` typeclass - it'd be no worse for referential transparency than, say, `Typeable`.
Ah yes I'd seen this before. I was hoping you'd encountered something I haven't :)
Seems to me that macros alone wouldn't enable creation of a record system that does everything everybody wants. That's why things like [this](https://github.com/ztellman/sleight) exist for Clojure. Same *could* be written for Haskell, but nobody's bothered. Probably partly because it would horrifically terrify much of the community. I'm ambivalent myself. I will have stronger feelings and a sharper sense of what's wrong when I've had more exposure to the pain points you're talking about.
yeah lisp macros are great. and I'd never want to wrap my whole module in a quasi quotation. what would it look like for a macro system to be able to write lambda case or multiway if? that sounds very hard. like to say "change the parser/tokenizer once you see the '\\case' keyword, into another parser." Haskell uses a leader and parser generator right? what if, vaguely speaking, we had a slow-but-easily-extensible parser maintained by GHC HQ (not saying that they don't have better things to work on, just to discuss this fantasy). that way, the standard parser stays fast and correct, editors can agree on features, etc. but, users could import their own parser extensions, which hopefully aren't too long, and ideally would like some parsec code or something, wrapping the "base" declaration parser with their own alternative. and not have to wait years for a new syntactic extension that touches ghc. and you wouldn't need to build a whole preprocessor. have people done work on whether this is feasible or even desirable?
Biggest difference for me was the code maintainability over the long haul with larger SLOC projects. Clearly easier in Haskell. The delta of one over the other for the other points was not nearly as large. 
Recall that typeclasses attach to types, not to values. So that doesn't quite work out as one might hope...
Thanks... although I have no idea what any of that means.
Does the customer specify the type of the contract, or will you guys infer it? 
I'm quite a bit rusty on my C++ templates, if I ever knew them this well. Can you do something like this? data Triune f g a = Triune { _f :: f a, _g :: g a, _c :: f (g a) } If you can't do something like that, you don't really have HKT.
wonderful and surprisingly snappy (for me at least). How optimistic are you that your "butchering" of pandoc can be be transformed into a mergable changes that will allow pandoc to run in GHCJS without compromising its normal use?
sounds like very valuable work. Besides being confident GHCJS output will actually work with closure compiler, ClojureScript makes a lot of the fact it is "optimised for" the closure compiler, I'd be interested to look into how to achieve this - whether scattering @const and @nosideeffects annotations everywhere would actually make for faster code - or compilation. *Edit:* nothing I have said is of any value, as usual, see: https://github.com/ghcjs/ghcjs/issues/258
Can you explain?
This looks awesome! This is going to be tremendous for many Haskell users, but I bet especially anybody doing gamedev. I can't wait to try this out :)
I think there isn't enough typechecking to complain about that sort of thing. Edit: It actually does check arity -- and this compiles without errors: template &lt;template&lt;typename&gt;class f, template&lt;typename&gt;class g, class a&gt; struct Triune { typedef typename f&lt;typename g&lt;a&gt;::result&gt;::result result; };
It's interesting that you'd mention syntax. When I was learning CMUCL, this was a huge pain point for me, enough that I eventually gave up. The very things you list as features (macros and metaprogramming) were what hung me up. Haskell's election of a more complex fundamental syntax seems to pay off in that you don't need to rely as much on things like variadic functions or macros to make code terse and understandable.
I sort of shifted focus from a beginner learning Haskell to someone vaguely familiar with Haskell learning concurrency, or someone not familiar with Haskell seeing how simple concurrency can be. Are there any particular sections you feel were unclear as someone not familiar with Haskell? I thought most of the programs would at least be vaguely sensible, but I may be relying on having internalised Haskell's syntax. EDIT: assumed by 'some of these' you meant my tutorial. Was I right?
It's still relevant though (since I had to give preference to merging Cabal support, GHC 7.10 support and the improved base library, work on the new intermediate representation has taken longer than I initially planned). Eventually more of this should be done by GHCJS itself, but closure compiler shouldn't break the output. The RTS does have annotations, and I used it closure compiler succesfully at some point, but something has broken since.
I've actually not written very much Go. But yeah, as /u/bss03 said, you really don't ever do this in Haskell. Runtime reflection isn't really preferred as a way of solving problems. In what sort of situation would you want to do this? Typically, if what you want is for a function to behave differently for different types, you'd use a typeclass: class Thingable a where doThing :: a -&gt; IO () instance Thingable Int where doThing i = print i instance Thingable String where doThing str = putStrLn str main = doThing 5 &gt;&gt; doThing "hi" Kind of like interfaces in Go.
Really, I disagree. We should be onboarding new users with Haskell solutions to problems, rather than Haskell translations of Go solutions.
I used `for_` from `Data.Foldable` in my example to avoid the scary m-word :p. Though its implementation is a little scarier.
Can you elaborate? How would you use this to document one function but not another when both have the same type? Edit: s/but but/but not/
Great comment! Pretty insightful.
Did you use the async exception mechanism for aborting background computations once the results aren't relevant anymore? The try-purescript example uses this: When the user stops typing, compilation of the purescript source is triggered. If the user resumes typing when the compilation is still going, the running compilation pass is aborted with an async exception. https://github.com/ghcjs/ghcjs-examples/tree/master/try-purescript
Interesting. Smacks of Smalltalkishness quite a bit
&gt; Lazy evaluation does help on the point of "everything is a list" (although clearly not enough that people had no cause to move to things like `Foldable`). You've completely missed my point. Oh, how I envy your innocence. :-P Some arguments that you often hear from Lisp programmers: 1. "Oh, we don't need record types because you can build records out of cons cells." 2. "Oh, we don't need arrays because lists can do the same thing as them." 3. "Oh, we don't need associative dictionaries because we can represent them as lists of key/value pairs." 4. "Dude, don't worry about the performance of having so many lists around, that's premature optimization, and all those data structures would get in the way of our awesome Lispy exploratory programming!" 5. ["It is better to have 100 functions operate on one data structure than 10 functions on 10 data structures."](http://stackoverflow.com/questions/6016271/why-is-it-better-to-have-100-functions-operate-on-one-data-structure-than-10-fun) 6. **EDIT:** ["Lists are useful in exploratory programming because they're so flexible. You don't have to commit in advance to exactly what a list represents. For example, you can use a list of two numbers to represent a point on a plane. Some would think it more proper to define a point object with two fields, x and y. But if you use lists to represent points, then when you expand your program to deal with n dimensions, all you have to do is make the new code default to zero for missing coordinates, and any remaining planar code will continue to work."](http://old.ycombinator.com/arc/tut.txt) &gt; I think the whole original argument for lazy evaluation was to improve the semantics for lists in lisp, actually. Wasn't the paper called "Cons should not evaluate its arguments"? Well, I wouldn't say "whole," but what you said is a thing. One completely made up but still interesting way to look at Haskell is this: as an experiment on whether you can get rid of Lisp's macros simply by using (a) pervasively lazy evaluation and (b) a lightweight syntax for lambdas (that doesn't actually require you to type `lambda` all the damn time). A huge percentage of Lisp macros come down to these things: 1. Exercising control over when and how many times expressions are evaluated. 2. Introducing new variable binding forms into the language. 3. Sequencing effects. 4. Syntactic sugar to avoid having to type `lambda` so much. 5. Boilerplate elimination 6. Efficient compilation of embedded DSLs. The answer to this faux "experiment" is that vanilla Haskell basically gets rid of motivations (1)-(4). (5) doesn't go away, which is why we have Template Haskell. (6) is why we have rewrite rules.
 findCharAtIndex idx = head . drop' (idx - 1) . concat . sort . map integerToWord $ [1..999999999] You have 100 million haskell strings, something like 40 characters long each, so thats about 4 billion characters. Then think about the fact that these are unboxed `Chars` held in a linked list, and that makes you require something like 20 bytes per string character, making it require something like 80 billion bytes of memory. You need to take a more sophisticated approach here. When I did it a few years ago, I basically assembled a regular expression that matched a string if and only if it was the written version of a number in the range. It's not too tough to look at a regular expression (with no '*'-repeat symbols) and determine how many strings it matches and what their total length is. You also need to be able to break a regular expression down by the first letter, i.e. "If we know the first letter is 'S', what is the regular expression that matches all the remaining strings?". data Regexp = Sequence [Regexp] | Alternatives [Regexp] | Literal Char You might also need an EmptyRegexp, I forget. Also, though you can normally do redunant strings in a regexp, like '(one|one|two)', you don't want to do that here, because the easy way to calculate the regexp matching set size would tell you it matches 3 strings of length 9, but it really matches 2 distinct strings of length 6. 
Sounds a bit like [Lamdu](http://peaker.github.io/lamdu/)
It's actually not much worse than I've seen on the /r/golang subreddit when someone submits either a new web framework or posts non-idiomatic code.
4 papers from IU!
1ML http://www.mpi-sws.org/~rossberg/1ml/
BTW, here's an awesome technical article from the Mirage OS blog about [implementing ASN.1](http://openmirage.org/blog/introducing-asn1) in OCaml via an embedded DSL of bi-directional combinators corresponding to the grammatical forms of ASN.1.
I never did understand such fanaticism! If a language supports certain constructs that might make it easier from people well versed in other languages to work their way into the language, why not? Not everybody is in the same level in any language, and it takes time for people to get used to writing idiomatic code in any language! Well, I suppose there are crackpots in every community!
Ah yeah, I definitely agree there. That's why I feel a less literal translation is needed, and went that direction myself.
Not parent; and I'm personally mostly fine with operators, and think that haskell's syntax per se is quite nice. But I think that haskell's syntax leads to code that is quite ugly to look at and difficult to follow, especially in application code. I think the main culprits are there are just too many ways to bind things, combined with having different sorts of composition that flow from right to left and left to right arbitrarily. For a while I've wanted to experiment, taking chunks of code and rewriting them without e.g. any `case`, or lambdas.
Like the few syntactic sugars provided in Haskell like `[a,b,c]` literals and `(a,b,c)` tuples and `(do x; y; z)`, Lisp macros are fundamentally polyvariadic, they just sometimes decide how many arguments they will accept, so you would indeed just have `(alt p q r)`. I find (alt a b (idiom f x (alt y z)) c) (do x y z) far more readable and easier to type and edit than a &lt;|&gt; b &lt;|&gt; (f &lt;$&gt; x &lt;*&gt; (y &lt;|&gt; z)) &lt;|&gt; c x &gt;&gt; y &gt;&gt; z 
There are too many tech things called Unison already [\[1\]](http://www.cis.upenn.edu/~bcpierce/unison/) [\[2\]](https://unison.com/) [\[3\]](https://www.panic.com/blog/the-future-of-unison/).
&gt; take application by juxtaposition, when I first saw some Haskell I thought "this. is what code should look like". Application by juxtaposition is alright, in my Lispy opinion. Heck, you could say that Lisp does that but with obligatory parens around the term. :-P &gt; user defined operators increase readability to my eyes. they shrink line noise by telling you to "read the nouns", once you've learned a library's verbs. for example, would you prefer: &gt; &gt; (alter (alter p q) r) &gt; &gt; to: &gt; &gt; p &lt;|&gt; q &lt;|&gt; r Your first example is a bit of a strawman. The idiomatic Lisp way of expressing an associative operator would be like this, as a variadic function or macro: (alter p q r) 
Ok, however multilines formatting make them ok (kindof ;-)) = a &lt;|&gt; b &lt;|&gt; ( f &lt;$&gt; x &lt;*&gt; ( y &lt;|&gt; z) ) &lt;|&gt; c and do x y z (or `do x; y; z`)
There's [core.typed for Clojure](https://github.com/clojure/core.typed) and there's [Shen](http://www.shenlanguage.org/learn-shen/types/types_basic.html)
How have you found the compilation time, and the development feedback loop working with GHCJS? **edit:** I ask this as I found this to be the biggest overhead in a large project I've been writing in [_purescript_](http://www.purescript.org) (it's still post 1.0 so these things are expected), and I want to see how it compares
20 years ago, my first "professional" sotfware experience (it was placement) was 6 months of Lisp. At the time I wasn't aware of functional programming (neither my mentor) , I was doing imperative programming in Lisp,I managed to do what I had to do, but it wasn't really pleasant. Now of course, I realize that I missed something and Lisp is much much more powerful than I though, and I could probably apply every concept from Haskell to Lisp but there is nothing to enforce it : therefore you can struggle doing imperative style in Lisp without seeing the light. In Haskell, you don't have the choice. What I really like about Haskell (and it comes mainly from purity I think ) is the fact that you have "hard contraints" and can design things so that some mistake can't be made. Example, you can design a DB system so that every call to the database HAS to be done within a transaction, (I've done similar things in Ruby using block : it makes it harder from developper to call things outside of a transaction but it's still doable). Another example is design an OpenGL library so that, calls can't be done outside of a GL context : in Haskell it's how it works naturally, but I would say it's nearly impossible to guarantee in any other language. 
I am aware of core.typed, didn't remember about shen. However I would love to know the Haskell comunity opinion on those languages/libraries...
Forgive my ignorance, but would would be the immediate benefit of this? Easier deployment? Tooling?
The source is on github https://github.com/unisonweb/platform
Any links to PDFs yet?
&gt; make ghci rememver state Specifically "cabal repl" would be even greater, then the history can by scoped to the project... Woot.
I love this and have been wanting something like this for a long time. I think you should add a webdev demo to the readme, running a JSON API with Spock or something and showing how you can make changes in the haskell code and then just refresh browser (avoiding the need to reload in GHCI etc).
How did you get around the native dependencies of citeproc-hs, I've been trying to compile it to JS for a while now with no joy.
https://github.com/mpickering/icfp2015-papers That's all of them that I could find at the moment.
You can already have it, as long as you're willing to tolerate some additional square brackets and commas: alt [a, b, idiom f x (alt [y, z]), c] do' [x, y, z]
Yes, I do understand. I'm just highlighting the fact that this requires some implicit assumptions, and those assumptions aren't guaranteed as strongly as the assumption that GHC's type checker is reliable.
Docstrings to my mind mostly function to make repl documentation easier. I don't much care for documenting the entire chain of a computation - just top-level functions and value. But what I do care about is being able to tell that a computation isn't inspecting a docstring, unless it's obvious from the signature that it might do so.
You'd probably want IO, or some other datatype that's only escapable into IO, in the signature of `docs`, though.
&gt; especially anybody doing gamedev. I agree! It reminds me of Casey Muratori's "[looped live code editing](http://m.youtube.com/watch?v=xrUSrVvB21c)". Halive is providing live code editing. The video explains that *looped* live code editing is a variant of live code editing in which the program doesn't start over from the beginning each time you change your code, nor does it stay in the same state you left it in, but rather a few seconds of gameplay is recorded and is being replayed in a loop, so you can see the impact of your changes on a specific player interaction in a particular section of the game world. He goes on to explain that if the code is "properly designed", implementing looped live code editing in C is easy. By "properly designed", he means that the entire game's state is in one place, and that the game's behaviour is deterministically determined by the game's state and the player input on every frame. Sounds like a pure function to me! For this reason I thought that most Haskell games are probably already "well-designed" according to his definition and it should therefore be possible to provide a drop-in library for looped live code editing. Unfortunately, as /u/moosefish made me realize, FRP programs (which is how I write my Haskell games) might be deterministic in terms of their input, but they are not a function of input + some state. Instead, their state is intertwined with their code, and there is no easy way to extract the state from an event graph nor to restore this state after reloading the new code. I'm still looking for a solution.
True - *if* there's no further compiler support, then you can't document two functions differently. But my thought would be that there would be compiler support. 
I think you have to get users used to having their code controlled as a tree and not think of their code as a free form whiteboard. Part of the motivation behind `hindent` is to encourage people to embrace automation, as a stepping stone toward more structured environments.
&gt; Lisp's syntax is superior ... What syntax? ;) Seriously though... I would kill to work with a programmer as opinionated AND informed as you. Really excellent overall assessment of the pluses and minuses. One thing I noticed while taking "language trips" (you know, away from your day-job language, which in my case is Ruby) is that inevitably I find something I feel like I NEED in my day-job language. For Lisp it was... well, [it's fucking Lisp](http://c2.com/cgi/wiki?LambdaTheUltimate). With Haskell it was the strict separation at the language level between side-effect-free code and side-effect-possible code (which I suppose necessitated the type system... I think? In any event, I think that being able to enforce purity at the language level is a good idea for ANY language.. Can you say "nondeterministic test suite pain"?). With Clojure it was Lisp + ClojureScript (I'm a web dev) + having a geek-crush on Rich Hickey. With Elixir/Erlang it was the concurrency and the pattern matching and, in Elixir's case, macros in a non-homoiconic language. And with ALL of these, it was speed relative to Ruby. ;) Is a language even possible that has all the following features: purity enforcement, immutable data, pattern matching AND currying by default (Elixir/Erlang can't really do currying, sigh), macros, actual syntax ;) , monads/monoids, functional, compiles to JS, extremely cheap and completely isolated process spawning, arbitrary user-defined prefix/infix/postfix operators/functions, actor model style message passing (but possible to do in a deterministic way, since the messages themselves are side effects... unless all concurrent process actions are deterministic relative to each other... I hear haskell actually has a concurrency library that somehow features determinism), a not-braindead dependency scheme, a nice build tool (a la [Mix](http://elixir-lang.org/getting-started/mix-otp/introduction-to-mix.html))... A nerd can dream.
&gt; Things get trickier with upper case : `f.g H.k` would be read either as `f . (g H) . k` or `(f.g) (H.k)` depending if H is a module or a constructor. ? without a space between the capitalized name and the period, the capitalized name is *always* interpreted as a module identifier ghci &gt; import Data.Maybe (fromJust) ghci &gt; import qualified Data.Maybe as Just ghci &gt; :t Just.fromJust Just.fromJust :: Maybe a -&gt; a ghci &gt; :t Just . fromJust Just . fromJust :: Maybe a -&gt; Maybe a
Most of the fancy words don't need to mean anything to you yet. You can use Haskell perfectly well without knowing what a zygohistomorphic prepromorphism is. At the end of the day Haskell code is still just a bunch of functions with types. Giving names to particular kind of functions just makes it easier to talk about them.
Ok, your are right, but that make Haskell syntax even more hawkward ;-) Apart from that, I love Haskell syntax.
Many IDE's ship with a vim mode, but an experienced vim user won't be fooled. No vim script, plugins, and missing motions or incorrect motion/action interactions. IDE plugins can be relatively complex compared to emacs and vim. not to mention less powerful. Yet, many people try to make an IDE out of vim. It's not that vim/emacs users all dislike IDEs, the people who make IDEs don't really understand why people like vim/emacs. It's not about how you organize your code, but how you read and edit it.
Objection. 5 only does not go away because of top level language features such as ADTs, typeclasses and so on, that you obviously can't abstract using functions. If you restrict yourself to the core lambda calculus and implement your ADTs and typeclasses as functions, you can get rid of your boileblate writing "macros" just as normal functions. Point "6" and "3" are taken care of if you use a strongly normalizing subset of the lambda calculus as a pre-compiler (since that gets rids of the layers of abstraction), and opt for an optimal reduction strategy on runtime (not lazy evaluation, interaction nets). OFC nobody has done that yet, but saying you can't do it because Haskell can't is wrong. 2 is the only real reason you ever need macros: fancy ways to assign variables.
I'm a lisper and have no haskell experience, but does haskell have customizable readtables and something similar to [character syntax types](http://www.lispworks.com/documentation/lw51/CLHS/Body/02_ad.htm)?
In your module example it would bind to f . (g H.k) Also if you throw in spaces between yours .s it's a lot more readable
Also, restarts the application from scratch, whereas this apparently lets you hold on to your state.
Simple example would the `and` and `or` functions, which both have the same type signature: and :: [Bool] -&gt; Bool or :: [Bool] -&gt; Bool How would you document both of those functions?
After reading author's blog posts I conclude that he has a really nice vision of what programming should be (as opposed to what it's like currently). In fact, I've been thinking about the issues he addresses for quite some time. Searching for a solution, I found [Lamdu](https://peaker.github.io/lamdu/) ‚Äî and it is closest to what the author describes. Structural editing and stuff. What saddens me is that the author decides to invent his own programming language, and this endeavor is doomed to failure, the task is too big. Unless this language gets Haskell FFI.
Cap'n Proto has tagged unions. You just can't define them as separate types. They have to be written inline.
I used to use Visual Studio for C++ dev and it was great. However, I use vim now, because I can use it for *everything* : C++, Ruby, Haskell, config files on a deployment machine , etc, without having to (re)learn new short cuts, each time I'm learning a new language. In other word, no languages is special enough (apart maybe from Unisson or **visual** programming languages) so it deserves its own IDE.
Some of the items posted are quick placeholders, but some are extremely detailed lessons with extensive runnable code. Thanks to all who've contributed so far. If you're a Haskell learner, you may want to start checking out School of Haskell [here.](https://www.fpcomplete.com/school/starting-with-haskell) And [this recent post](http://www.reddit.com/r/haskell/comments/34uc8y/school_of_haskell_20/) describes the future of the SoH.
Programming language is to idiomatic programs as human language is to prose in a standard dialect. It is provably easier to communicate and share literature (programs) if everyone is using the same dialect. Sure you can understand other dialects, but when there's already an accepted one for the community; the community encourages it's use.
UglifyJS runs out of memory when I throw the complete output at it (even with the `--max_old_space_size` flag the upper limit is [pretty low](https://github.com/joyent/node/wiki/FAQ#what-is-the-memory-limit-on-a-node-process)) and recompressing closure compiler's output doesn't yield a significant difference.
I'm running the computation in a thread which gets killed when new input is received. I've also added a threadDelay call before pandoc conversion to achieve debouncing and to make sure that the expensive computation doesn't run when it's not needed. I'm amazed how well this runs even though the code runs in a single threaded context. Thanks for the great work!
If you get it merged upstream I'll add this in five minutes :)
I've had to manually remove the features that depended on native libraries
The relevant xkcd that everyone has probably seen 20 times: https://xkcd.com/927/
Ah! My apologies, I had misunderstood your question the first time. I agree with both statements: `Int :-&gt;: a` is equivalent to `[a]`, and data Event0 a = Event0 (Int -&gt; ([a], Event0 a)) and data Event1 a = Event1 (Int :-&gt;: ([a], Event1 a)) are equivalent to data Event2 a = Event2 [([a], Event2 a)] Furthermore, data Event3 a = Event3 ([a], Event3 a) is equivalent to [[a]] However, `Event2 a` is not equivalent to `Event3 a`! So that's the technical reason why `Event Int a` is distinct from `[[a]]`. But it's much more interesting to look at the semantic differences. We're now using lists to represent three different concepts. The nested list represents the fact that more than one event can occur at once. newtype Simultaneous a = Simultaneous [a] The outer list in `Event2` represents a choice between many futures depending on which Int value we receive next from the outside world's external event source. Typically, instead of Int, `t` would be instantiated with a type representing the events which are external to the FRP event network, such as mouse click positions or keyboard keys. newtype Choice a = Choice [a] And I think that the outer list in `[[a]]` is supposed to represent the sequence of frames as they occur, where each frame is represented by the list of simultaneous events which occur during this frame. newtype Sequence a = Sequence [a] With these new precisions, your question can be reformulated as: what's the difference between `Event Int a` and `Sequence (Simultaneous a)`? The answer is that the latter isn't accounting for `Choice`, whereas the former does: data Event4 a = Event4 (Choice (Simultaneous a, Event4 a)) So where is `Sequence` accounted for in `Event4 a`? In the recursion step. `Event4 a` is not equivalent to `[[a]]`, it's equivalent to [`Forest`](https://hackage.haskell.org/package/containers-0.3.0.0/docs/Data-Tree.html#t:Forest)` [a]`. Each node is holding a list of simultaneous events, each node has an infinite list of immediate children representing the different choices of Int we could receive from the outside world immediately after reaching that node, and a path down the tree represents the sequence of frames we want to present when receiving the corresponding sequence of Int events.
The article says it's strongly typed. Does that mean statically typed?
School of Haskell is awesome. It would be nice if it would be a bit more comfortable to browse through all those great articles. I find it very hard to find something specific. Maybe a good tagging system could help here?
I don't know if it fits into what you're thinking about but I've been using [graphmod](http://hackage.haskell.org/package/graphmod) occassionally to get a grasp of how much interdepency there is in my code base. It's just a simply visualization tool for import statements across your code base, but I've found it quite valuable at times. I've just found [SourceGraph](http://hackage.haskell.org/package/SourceGraph) which seems to be much more in depth. I'll have a look at that. &gt; For example, lines of code, average line width, number of functions, is error or undefined called, etc. I do this sort of thing with ad-hoc shell scripting. Lines of code is trivial with `find . -name "*hs" | xargs wc`. Non-comment LOCs is a bit trickier but I'm sure it's also doable with bash. I'd do average line width in pretty much the same way as `wc` also outputs character count and average line width should be that divided by LOCs. Finding out whether `_|_` is used anywhere should be a matter of `grep`ing. I don't keep track of metrics like these, but it should be straight-forward to write a small script that does the calculations and then writes (appends) them to a file.
Exactly. I don't spend 70% of my time dealing with persistence/serialization. This problem is now easy within the same language/system (ex Ruby on Rails, or even with plain file Read/Show instance in Haskell). Problems start when interfacing with other system : an existing database, a csv from my bank etc ... and I don't see how Unisson adresses those problems.
And probably half them where intended to replace all the existing ones ;-)
At least the name *Unison* suggest the idea of unification.
&gt; t seems Unison has a real shot at solving some of the most serious problems arising from code-as-plain-text. Could you elaborate ? I mean , which problem does it solve and how ? As I pointed, how does **semantic** editing deals with refactoring ?
[cloc](http://cloc.sourceforge.net/) looks to be pretty good at finding lines of code. Thanks for pointing those tools out.
About "lists are useful because they're so flexible": that's actually one of my major pain points with Lisps. After a while, I have no idea which lists represent what thing. Everything just gets mixed together and not in a good way. "Is this a coordinate? Is it a name? Is it an account balance? I don't know and I have to stop dead in my tracks to figure that out before I can move on. Please, let me rewrite this with a proper type system."
&gt; It would be much easier (IMHO) if things stuck together were bound together My personal preference would be to *require* whitespace around operators. This would allow identifiers to include operator characters, like `list-&gt;` instead of `fromList`.
I'm semi-interested. How do you imagine the logistics working? E-mail problem sets back and forth?
I'm not even lying. 
Thanks for putting this together! Of the papers thus far, "Which simple types have a unique inhabitant?", 1ML and RRB Vector all standout to me as having immediately applicable/usable ideas in the relative short term.
I'm interested.
Thanks to all those who've contributed - I can now get a ghci up and running on my raspberry pi 2 with very little effort :)
Just a reminder that [IRC channels](https://www.haskell.org/irc) also exists.
Ah, yes. I should learn to read tickets.
Me too, I'm not avaliable for live meetings doe. 
I would like to learn also, I liked the idea of some kind of project. 
&gt; what would it look like for a macro system to be able to write lambda case or multiway if? Here's how I'd do it in my imaginary Lispified Haskell: (defmacro (lambdacase branch ...) (x &lt;- fresh-variable) '(lambda (,x) (case ,x of ,branch ...))) -- example usage of the lambdacase macro (def foo (lambdacase (Nothing -&gt; 0) (Just x -&gt; x))) (defmacro (multi-if '(| ,guard -&gt; ,expr) ...) '(case () of (_ | ,guard -&gt; ,expr) ...)) -- example usage of the multi-if macro (def (bar x) (multi-if (| (&gt; x 50) -&gt; 'big) (| (&lt; x 20 -&gt; 'small) (| _ -&gt; 'just-right))) 
Ohhh right, duh. C for example would be statically typed because the types are determined at compile time. Thx for clarifying.
Interested!
Because *every bigger application is ugly* trope. Some people have a problem with writing things, which could be better. And writing things that are better is very difficult. 
Hi, CS student here, learning Haskell atm (ninja edit: not at university!) by reading a lot (almost done with LYAH) and starting a project for getting into it. Would also like to get in contacts with other beginners, so everyone, feel free to PM me!
Whether or not you really make it available at runtime isn't as important as just giving it a nicely integrated syntax in the language -- especially "in" the function, instead of before it, so you see the subject before you read the description of it. Haskell's "-- " comment syntax, coupled with the haddock requirement of putting the docs before the function, aren't as nice as Python or Clojure docstrings, IMO. As for runtime, I think it'd be possible for GHCI to be a bit smarter and learn how to extract documentation for functions regardless of the syntax -- even the current one should be possible. I really wish GHCI would grow that feature, as well as, perhaps, the ability to show the source code and closure-environments of functions.
Didn't expect a solid Schemer to drop in here! Thank you for detailing the clear sticking points you've had when learning Haskell. Haskell does feel a lot more like "doing math" to me; whereas my lisp exposure--mainly Clojure--has felt more like "programming". Also, your summarization of the philosophical differences between the two languages is quite provocative. I must admit that my functional programming experience is limited enough that I haven't begun to start explicitly thinking in terms of designing EDSLs. In retrospect though, I see how this has naturally happened in a way even in my C projects.
I was hoping somone from Clojure would join in the discussion! Thank you for the very explicit rundown of your experiences. The motivation for this post actually stems from preliminary research for a project I am working on. One piece will be a web frontend. I would love to hear more about your experience with ClojureScript. Also, do you have experience with Haste or similar? The Haskell wiki has a good amount of information regarding JavaScript "alternatives": https://wiki.haskell.org/The_JavaScript_Problem
&gt; Haskell does feel a lot more like "doing math" to me; whereas my lisp exposure--mainly Clojure--has felt more like "programming". I suspect this has a bit to do with the controlled side effects in Haskell. When side effects can enter into your code, it suddenly feels a lot more like traditional programming. Edit: And the typeclasses in the standard library.
Python is strongly typed.
The JVM is pretty bad as a target for programming languages not using Java OO since it has a lot of assumptions baked into the low level that only really apply to Java and other languages doing its particular brand of OO.
Perhaps it is possible to define a truly complete set of transformations? Of course it's always possible to "cheat," and to allow a transformation that breaks (say) typechecking. It seems to me that the limitation of never breaking merely syntactic correctness, at least, would not be onerous at all.
In fact I think the kind of thing you are talking about would be much *easier* to implement and that would be one of the big wins to this approach. Something like regex replace, for example, is never possible to do cleanly in (say) Haskell because you never know: you could be inside a quoted string that contains some Haskell code. But, with (say) a DOM interface you can very easily descend the tree and do a regex substitution on all paragraph texts, or on all href= attributes, and so on. Just the simple ability to *safely* rename a variable (in every place where it occurs, but *not* in places where another lexically-scoped variable has the same name) is so much outside the limitations of the text-editing approach, that it is a regular hassle. You simply can't do it in an automated fashion; in vim, you need to use s//gc and verify each change. That's one of the things that makes me want a structured editor -- one of the regular annoyances of using vim where I think to myself, "here I am doing this manually, and yet it's a job for a machine."
How is that even _remotely_ relevant? There are _zero_ standards under discussion here, much less a new competing standard. Just a new tool, with no indication whatsoever that it's trying to unify existing tools.
#haskell-beginners on Freenode is a great place to get help. I am learning Haskell too, and I am working on the NICTA course recommended by bitemyapp. Hop on IRC, I am there quite a bit.
Well, I'm a former math students and I am now studying quantitative finance with a keen interest in programming languages and other CS-related things. I had never heard of paradigms before last year or something, and I got interested into learning functional programming to better understand it. I already know a bit of OOP (Java), a bit of imperative (C), and a bit of relational (SQL), and I am curious as to the kind of things that are made easier with functional programming - or faster.
That's a very nice talk. Too bad the questions are not repeated because they are very hard to hear. 
What would be the type of this list?
We disagree as I don't hold normative positions when it comes to programming.
We could go full Agda and allow things like "‚à£‚äñ‚à£-‚â±" or "cancel-*-+-right-‚â§" :)
Eek good point. It isn't a well typed list. idiom :: ([a] -&gt; f b) -&gt; [f a] -&gt; f b Doesn't quite capture everything that should be expressable via idiom brackets. You need an hlist for that. The same goes for the do' macro. Unless each of the actions in the list is `m ()`, it is ill typed. And in that case, the macro neglects the ability for monadic bind to pass the value from one computation to the next.
I hold normative positions. like "People *should* make more functions pure, when they can, in any language". why even Haskell if you don't use purity, functional abstractions, rich types, etc? is your point just to hook people in? they might then find Haskell horribly awkward and abandon it.
I highly recommend this video to answer that question https://www.youtube.com/watch?v=vYh27zz9530
Every one is free to code as they please. 
(I know, that's why I brought it up, to "steelman" the argument). statically typed polyvariadic functions don't always work. yes for "alter", no for "idiom" (ie "&lt;$&gt;" and/or "&lt;*&gt;", as in another comment). for any functions that can't be made polyvariadic, what's better than an operator? also, a backticked identifier I count as an alphanumeric operator, because it's infix.
Alas, `Data.Dynamic` exists in [`base`](http://hackage.haskell.org/package/base-4.8.0.0/docs/Data-Dynamic.html), so there'd be a much greater tendency for confusion with that naming.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Technical standard**](https://en.wikipedia.org/wiki/Technical%20standard): [](#sfw) --- &gt; &gt;A __technical standard__ is an established [norm](https://en.wikipedia.org/wiki/Norm_(sociology\)) or [requirement](https://en.wikipedia.org/wiki/Requirement) in regard to technical [systems](https://en.wikipedia.org/wiki/System). It is usually a formal document that establishes uniform engineering or technical criteria, methods, processes and practices. In contrast, a custom, [convention](https://en.wikipedia.org/wiki/Convention_(norm\)), company product, corporate standard, etc. that becomes generally accepted and dominant is often called a [*de facto* standard](https://en.wikipedia.org/wiki/De_facto_standard). &gt;A technical standard can also be a controlled artifact or similar formal means used for [calibration](https://en.wikipedia.org/wiki/Calibration). Reference standards and [certified reference materials](https://en.wikipedia.org/wiki/Certified_reference_materials) have an assigned value by direct comparison with a reference base. A [primary standard](https://en.wikipedia.org/wiki/Primary_standard) is a technical standard which is not subordinate to any other standard but serves to define the property in question. Primary standards are usually kept in the custody of a national standards body. A hierarchy of secondary, tertiary, and check standards are calibrated by comparison to the primary standard; only those on the lowest level are used for actual measurement work in a [metrology](https://en.wikipedia.org/wiki/Metrology) system. A key requirement in this case is (metrological) [traceability](https://en.wikipedia.org/wiki/Traceability), an unbroken paper trail of calibrations back to the primary standard. &gt;A technical standard may be developed privately or unilaterally, for example by a corporation, regulatory body, military, etc. Standards can also be developed by groups such as trade unions, and trade associations. [Standards organizations](https://en.wikipedia.org/wiki/Standards_organizations) often have more diverse input and usually develop voluntary standards: these might become mandatory if adopted by a government, business contract, etc. &gt; --- ^Interesting: [^Technical ^Standard ^Order](https://en.wikipedia.org/wiki/Technical_Standard_Order) ^| [^Unicode ^Technical ^Standard](https://en.wikipedia.org/wiki/Unicode_Technical_Standard) ^| [^Specification ^\(technical ^standard)](https://en.wikipedia.org/wiki/Specification_\(technical_standard\)) ^| [^Evolved ^HSPA](https://en.wikipedia.org/wiki/Evolved_HSPA) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cr4bhmr) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cr4bhmr)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I hacked around with core.typed a little. This is a big pain, but you *can* individually declare symbols to be ignored by the type checker. But if we're going to that much effort, seems like we might as well just type-annotate the library.
LiquidHaskell is refinement types and relies on an SMT solver to work IIRC, dependent types can do much more but take more work to define and satisfy constraints. There are some notions the two might be unified (refinement and dependent types) but they're still orthogonal approaches to enforcing constraints. My understanding is that the idea would be to bring in refinement types to get simpler (SMT-understandable) constraint enforcement without proof obligations. I want both \^_^^
Wow. Hitting lisp for the first time in an occupational context (?) seems pretty challenging unless there's a strong support culture on your dev team. Sounds like it wasn't that way in your case. How'd you start hacking with Haskell?
Don't forget to fill in bottom values. I use a Prelude that adds warnings to "undefined" so I don't do this. Not a lot of caveats beyond that. Help to just have a video of me doing this. I'll get 'round to it.
Agda does this!
I seem to remember some modules that looked like they were from an implementation of that old wolf, goat, cabbage boat problem judging by their names.
How often do you rename the same syntactic construct though? I find myself much more often generating one class of syntactic objects from another, e.g. create one function per constructor or one SQL CREATE TABLE per plugin or something along those lines using those tools.
Refinement types *are* dependent types, or one flavor of them to be more precise. And any dependent typechecker needs some sort of a constraint solver to work.
He called it "making dependent types practical", yet more than half-way in he talks about having to prove associativity of natural number addition... Real practical there, yeah.
Fair enough.
I think this speaks to the importance of adding more community features (like commenting and rating and replying) as SoH gets open-sourced. 
Is there a reason to have both `setValue` and `value` if it uses `Lens` anyway?
Sure, but that does people who are actually using other languages and want to foray into Haskell no good. Most people won't have the time or energy to start plodding through a language, master its idiomatic usage, and then start contributing to the community. People work with what they're comfortable with, and getting into idiomatic use of a language takes years of programming and many aha moments to get it. This is presumably a tutorial for beginners, and as such, should be tailored for people to get a smooth transition into Haskell. Confounding a beginner with hardcore pure idiomatic usage of Haskell does neither the learner nor the community any good.
So the problem that this solves is ... building a Haskell image on OSX? Is that the only reason you have ghc in your Dockerfile ? It would seem like I could just take haskell-scratch and add the Heroku stuff to it otherwise.
In my defense, I tried to acknowledge this first with a joke and then more earnestly slightly later. There are lots of reasons dependent types aren't used in "real world" programming languages, and this talk was a fairly narrow look at just a couple of them.
Who is working on the `auth` stuff? I'd like to help if possible.
&gt;&gt; Lisp's syntax is superior &gt; ... What syntax? ;) One of the brilliant and beautiful designs about Lisp is that its syntax is absolutely minimal and consistent (something that every other language seems to be inconsistent with by special casing everything.) It doesn't get simpler then [S expressions](http://en.wikipedia.org/wiki/S-expression): (op arg1 arg2 ... ) For sequence, function calls, variables, etc. Absolutely Fucking Beautiful ! Sadly my assembly and C heritage makes me want to vomit when I see Lisp and Haskell code. :-/ But I've always admired Lisp's for its simplicity. It's a pity [Readable Lisp S-Expressions](http://readable.sourceforge.net/) never took off ..
From [heroku blog](https://blog.heroku.com/archives/2015/5/5/introducing_heroku_docker_release_build_deploy_heroku_apps_with_docker): &gt; `heroku docker:release` starts the container and extracts the `/app` directory into a tarball that is deployed to Heroku using the [Release API](https://devcenter.heroku.com/articles/platform-api-deploying-slugs). The tarball includes both the language runtime and the source code for your app In our case slug will include only the final binary. So answering your question: Yes, it builds Haskell **binary** on OSX. We can't deploy *images* to Heroku. Contrary to building minimal docker images for AWS ELB, we could build minimal patch to [`heroku/cedar`](https://registry.hub.docker.com/u/heroku/cedar/dockerfile/) image. Or actually an `/app` directory, which when added to `heroku/cedar` will contain the working app. E.g. after I run `heroku docker:start` and start the result image with `docker run -ti heroku-docker-...-start bash`: app@9231e919d997:~/src$ du -h /app 4.0K /app/src 9.7M /app/target 9.7M /app To conclude one could either: - use some GHC docker images to build an executable, commit it to repository, and push to Heroku - use some GHC docker images to build an executable, and deploy it to Heroku using [Release API](https://devcenter.heroku.com/articles/platform-api-deploying-slugs) - use the approach described from the post, as the most parts are automated there. - use the approach described from the post, but use https://launchpad.net/~hvr/+archive/ubuntu/ghc packages to install ghc and cabal The last one contains less DIY-parts, but unfortunately forgot about this morning totally. It definitely makes sense, as one runs tests on Travis using those packages anyway.
Actually, we evolved from AngularJS (first version) to reagent. Angular 1 was already far better than using raw javascript or just jquery. Clearly clojurescript is a winner in every aspect with regards to javascript + some framework. The ability to use lisp as your html is great (see hiccup). I wasn't in charge directly of the front, I just made minimal modifications. But we integrated external dependencies without any problem (highcharts for example). So far the experience was quite positive for the team. Concerning the Haskell alternatives. For now I simply tried elm which has a Haskell-like feeling but don't have most of its abilities nor complexities. Actually, the code organisation of elm seems to be the alpha and omega of a good front-end architecture. But it is not immediately transferable to reagent for efficiency reasons. https://github.com/evancz/elm-architecture-tutorial/#the-elm-architecture If I had the choice of weapon I would use elm today. I made a proof of concept with elm + highcharts (which is highly non functional in spirit) without any problem. But if the application you want to achieve is _very_ complex, or if you want to share a big part of your code between the back and the front, then I would certainly try haste or ghcjs.
The name is *Unison* with a single s.
Somehow I missed this. Wonderful, thanks!
*Standard* in the comic can be seen as a placeholder, you can replace it by language/IDE/etc ... and get new comics which then are relevant.
Do you find the associativity of natural number addition impractical?&lt;/joke&gt;
Ah, I see. Indeed, `Event4` is a forest of all possible execution paths, depending on the global inputs. Of course, the problem with this approach is that it is restricted to a fixed global input type. In other words, you're most likely restricted to, say, `(Keyboard, Mouse)` as input, and cannot add new inputs on the flow, for instance for new UI elements like buttons. That said, there might be a loophole: It could be possible to use universal quantification over the variable `t` to assemble the right types on demand. However, I have no idea how to do that.
It's not *just* trying out a new different thing though. It is calling out the other things as bad and claiming that it will be a superior alternative. If you cannot see how an xkcd that talks about formal standards is relevant to a discussion of informal standards, then let's just stop arguing the point.
Duplicated, or just similar-looking? In my quick glance through this code, I didn't see any real duplication. Using complex type-system tricks as a kind of file compression on the source code doesn't necessarily make the code faster or easier to read.
Yes! When using FRP, the flow of data is very strict: data flowing into something is an argument to it, and data flowing out is part of the return value. If you only have access to the return value of a widget, there's no way to set its value - this is how we maintain functional purity. `setValue` is a lens used to add a command to the *input* of a widget, which will set the widget's value when it fires. In the case of a textInput, for example, `setValue` will allow you to provide an `Event t String`. `value`, on the other hand, refers to the *current value* of the widget, which can be set by the `setValue` input but also by the user's actions. In the case of a textInput, `value` is a lens to a `Dynamic t String`.
If you start your own Haskell company, you will either (A) succeed, or (B) have the use case you desire, so it's a win-win! :)
Interesting although it seems impossible to justify migrating from clojure to haskell for purposes of "extending the runway". I'm not a huge fan of clojure, but its not like its a dead language or substantially deficient. One must wonder why they chose clojure in the first place...haskell has been robust and well known since the time of clojure's first appearance
Duplicated code is not necessarily bad. The only case where I consider duplication bad is when you want to enforce that two things are consistent and duplication might allow them to drift.
You are precisely right. That's why I cannot for the life of me understand why the dependent types-popularizing people can't just put all such silly, obvious crap somewhere into the core lib and focus on stuff that is at least remotely interesting to programmers? Spending 12 minutes to prove that appending lists is associative and presenting it at the end of the talk like it's the crown jewel of programming with proofs is just... not sexy.
I know, I have my own idiosyncrasies with aligning/indenting, what is inline or is a function, etc. but it's just my opinion that Haskell has good strong opinions. And "opinions" aren't always a bad thing, especially when they are right.
Maybe they haven't looked around that much back then. Like everyone who's not using Haskell or better by today. To figure out what language is better takes a lot of time and when you lack time or motivation your decisions will be less reasonable. 
&gt; harmless. /u/yitz says: &gt; `try` is rather costly in Parsec
He also does conflate closures with curried functions. You need the first to have the second, but that everything is curried is not implied by a language supporting closures.
I must agree. I rather read/write some "non bad" duplication by /u/Tekmo's definition, then hurting my brain by aggressively/compulsively de-duped code.
Well, there's HalVM.
Yeah, it seemed really strange that they didn't actually describe the resolution to their CPU problem.
Agreed, but in this case, because it is redundant I feel the optimizer will remove the dead code since it's essentially start = stream if *stream == c stream++ return success else // but stream is already equal to start so a smart optimizer can elide this: stream = start return failure
Because you could use a higher-order function defaultCase def caseof [] = def defaultCase def caseof (c, r):cs = if caseof == c then r else defaultCase def caseof cs And call it like: defaultCase Nothing foo [(Bar, Just ...), (Baz, Just ...)] which may not save any keystrokes but is a big win for sloc golf. :P
so why is something like "lparen" mentioned in at least four different places: the module export, the data type, the character that should be printed, and the associated parser combinator? why are these things, all connected to the same idea, spread throughout the file?
I guess we'll have to start from scratch.
Mathematica is plain text under the hood though, it just displays it in a fancy way. Max/msp is visual programming. The real advantage comes when moving away from *plain* text entirely, but retaining textual programming.
i don't - i haven't used haskell in years. i'm just curious why this is considered so good (rather than just "neatly formatted"), and how it might be improved.
I love clash. Hardware is what got me into Haskell in the first place. However, I have not been able to accomplish anything much with it. Hardware design environments are very hairy and if you think it's hard to interface Haskell with C++, try interfacing with Verilog. Maybe the problem is there's only a handful of experts in the world on functional hardware design and each of them is working on his own project sharing nothing with the others: Clash, Chalmers, Kansas, Xilinx, Wired, Hydra, Bluespec, Chisel (Chisel might be a winner -- to bad it's Scala), even Intel had a Haskell guy 10 years ago; and Conal Elliot's compiler for a sadly failed FPGA startup. I wish they could join forces to make the EDA equivalent of FP Complete.
I admit that this example is kind of stretching it. I regret putting this pattern into my comment.
If you give `r` the type r :: forall a. ConfigM a -&gt; IO a does it work? In my uninformed opinion this looks like it's related to the Monomorphism Restriction, but I don't know what the GHC extensions trigger it. I could be completely wrong. Anyone else have an idea?
I also want to give a more serious answer to your original question. The Cap'n Proto library was originally written in Haskell, but [later rewritten to C++](https://capnproto.org/news/2013-08-12-capnproto-0.2-no-more-haskell.html) for efficiency reasons.
Perhaps initial prototyping is better in dynamic languages.
You have to read and write a few of these things to appreciate how quickly things can spiral out of control. To read something large and complex that doesn't look like it was puked up by a heard of sick cats is sublime. Here's a similar component in the Go language: https://golang.org/src/text/template/parse/lex.go See how it makes you want to claw your eyes out?
There are a few things which I like in the compiler, most of which are not written by me. Code gen, type checking, the doc-generator, and error message handling all have some neat code. The `pattern-arrows` library gives quite a nice way of dealing with pretty printing and operator precedences without having to think about `showsPrec`. One of the goals from the start was to try to write Haskell which a beginner could understand and contribute to, so hopefully that is still the case. The traversals code used to use the `syb` library, which was elegant, but ultimately too slow, so we wrote all the traversals by hand. Many of them could be expressed in terms of `Traversable` or `Lens`, which is on my TODO-list.
You just have to explicitly turn them on so you don't get warnings (which you won't see until you turn on warnings).
&gt; One of the goals from the start was to try to write Haskell which a beginner could understand and contribute to, so hopefully that is still the case. I'm suddenly inspired to start seeing what I can contribute to!
I'm already building my next work project with Servant on the backend!
it's funny to think, that Haskell is actually a 90s language and ML is a 70s language...
I'm a bit more neutral on what is the "right" approach. Intrinsic checking tends to invade the structure of everything you write in the language. If I refactor code written in Agda, I usually have to throw most of what I'd written away, because the proof is the object I have left over at the end, not the process of generating the proof. That said, of course, Coq gives you tactics. You can work robustly with tactics to solve classes of problems. Build a tactic that solves some entire problem space, and then you can refactor within that space until your heart's content. This is pretty much the main lesson I took away from Adam Chlipala. Coq gives us a limited form of extrinsic reasoning through the Program construct, which was given to us by Matthieu Sozeau. Many of my best experiences with theorem proving have centered around building a custom tactic for solving the obligations incurred by the Program construct and then just effectively 'programming in haskell'. This lets me blend both Adam and Matthieu's techniques, letting me get away with simpler tactics than `crush`, and just specify the interesting bits. I don't think Chris' talk sold how effective that can be. I'll offer up an example: Every proof and most of the implementations in the fist ~300 lines of https://github.com/ekmett/homotopy/blob/master/Core.v#L296 are handled by such a custom obligation tactic specified in 11 lines of coq. https://github.com/ekmett/homotopy/blob/master/Core.v#L113 With that we can pretty much steer Coq by spelling out the gist of what it is I want to prove so long as we can get there by using simple applications of paths that are in local scope. Those 11 lines provide me with something like 400 satisfied obligations that we aren't laboriously working through proof by proof. e.g. Program Definition Types : category := {| hom := Œª (x y : Type), x -&gt; y |}. gives the entire definition of my category of types. `id`, `compose`, the class of objects, all the proofs are derived by that simple tactic. So not only did the `Program` construct derive my proof for me, but it also built most of my implementation. Program Definition Paths A : groupoid := {| groupoid_category := {| hom := @paths A |} |}. gives me a groupoid for paths. Program Definition ap `(f : A ‚Üí B) := Build_functor (Paths A) (Paths B) f _ _ _. gives the (infinity,1)-functor that is the action on paths, etc. I don't think this works for everything we want to say in a dependently typed language, but I think it is an interesting approach and that the above example showcases quite how terse it can be compared to traditional approaches, and that the work at U. Penn on Zombie is probably the most active work in that space.
&gt; blockComment = P.try $ P.string "{-" *&gt; P.manyTill P.anyChar (P.try (P.string "-}")) &gt; lineComment = P.try $ P.string "--" *&gt; P.manyTill P.anyChar (P.try (void (P.char '\n') &lt;|&gt; P.eof)) comment leader trailer = P.try $ P.string trailer *&gt; P.manyTill P.anyChar (P.try trailer) Probably doesn't actually reduce typing or optimize better. Maybe it reads slightly better? &gt; [ P.try $ P.string "&lt;-" *&gt; P.notFollowedBy symbolChar *&gt; pure LArrow &gt; , P.try $ P.string "&lt;=" *&gt; P.notFollowedBy symbolChar *&gt; pure LFatArrow &gt; , P.try $ P.string "-&gt;" *&gt; P.notFollowedBy symbolChar *&gt; pure RArrow &gt; , P.try $ P.string "=&gt;" *&gt; P.notFollowedBy symbolChar *&gt; pure RFatArrow &gt; , P.try $ P.string "::" *&gt; P.notFollowedBy symbolChar *&gt; pure DoubleColon symStr `strSymIs` symTok = P.try $ P.string symStr *&gt; P.notFollowedBy symbolChar *&gt; pure symTok I could see this being mildly useful. &gt; , P.try $ P.char '(' *&gt; pure LParen &gt; , P.try $ P.char ')' *&gt; pure RParen &gt; , P.try $ P.char '{' *&gt; pure LBrace &gt; , P.try $ P.char '}' *&gt; pure RBrace &gt; , P.try $ P.char '[' *&gt; pure LSquare &gt; , P.try $ P.char ']' *&gt; pure RSquare &gt; , P.try $ P.char '`' *&gt; pure Tick &gt; , P.try $ P.char ',' *&gt; pure Comma c `charIs` t = P.try $ P.char c *&gt; pure t Same here. I'd probably have noticed it after the first one and wrote it, even. &gt; , P.try $ P.char '=' *&gt; P.notFollowedBy symbolChar *&gt; pure Equals &gt; , P.try $ P.char ':' *&gt; P.notFollowedBy symbolChar *&gt; pure Colon &gt; , P.try $ P.char '|' *&gt; P.notFollowedBy symbolChar *&gt; pure Pipe &gt; , P.try $ P.char '.' *&gt; P.notFollowedBy symbolChar *&gt; pure Dot &gt; , P.try $ P.char ';' *&gt; P.notFollowedBy symbolChar *&gt; pure Semi &gt; , P.try $ P.char '@' *&gt; P.notFollowedBy symbolChar *&gt; pure At &gt; , P.try $ P.char '_' *&gt; P.notFollowedBy identLetter *&gt; pure Underscore charSym c nfb t = P.try $ P.char c *&gt; P.notFollowedBy nfb *&gt; pure t Actually, I'd probably have written the nfb = symbolChar case and then let the `'_'` case as a one-off. Again, mildly useful. The effort the programmer when though *noting* the similarities via alignment is really enough for me to find things readable, and depending on the names of the local functions and the distance to them when I'm reading the code, the duplication might actually be more readable. I doubt any of those locally-defined functions would impact performance either way. I think it's a wash, and that the code is pretty good.
As I mentioned on the HN post, I simply haven't had time to drive this issue home, since I found a workaround a couple of weeks ago. It's something I'll take another stab at in the nearby future to see if I can get closure. Re: your comment, were there any parts where the article felt too unreasonably rosey? I tried to paint a fairly realistic picture of the ecosystem based on personal experience. In fact, I had to double-check with Snoyman that I wasn't pooping on Haskell too hard with all the criticisms I was listing.
It was just a matter of timing and personal experience. I hadn't heard of Haskell back when I was trying to pick an ecosystem to implement our backends in, and Clojure was what all of the people I really looked up to were using at the time. Eventually I felt like I couldn't get the kind of properties I was looking for from Clojure, and Haskell seemed like a natural step forward. To some extent the company's technical trajectory was in large part a reflection of my personal technical journey. This was a lot easier to pull off when I was the only person doing engineering on the team, and it gets progressively harder with every additional person on board. And yes, transitioning stacks (and upkeeping both at once for a long time) is a giant commitment for a small team, not something you take lightly. Nicely migrating between two APIs coexisting side-by-side is an interesting experience. Fortunately, at this point the team is used to my gambles and they were sold on the benefits. So far no regrets.
It's a hello world, like doing fib in a functional language. You can learn a lot about a language by the way that it handles a toy problem.
I don't agree. I have experience in Ruby and Common Lisp, but since I learned Haskell and OCaml I don't want to prototype in the two first ones ever again. Not having types makes things a lot harder to reason about, and much more error-prone. 
Thank you very much. Enjoy the conference! I will look forward to a video in the upcoming months.
Some things are just impossible (or damn near) in typed languages. The example that comes to mind is dynamically affecting bindings (e.g. Python's `locals()`).
Servant looks like the most type-safe HTTP server library. Does anything else come close? I haven't done any Haskell web APIs yet (though it's something I do a lot of in my line of work, with different languages), this looks really appealing. I'll probably do a learning project by implementing an HTTP API in Haskell and I'm pretty set on Servant for now.
Actually we use SCSS because the template we use was done externally and until now we didn't really need to use garden. Concerning elm the great part is you don't have to worry about html or CSS anymore. If you still prefer to use external templates. Then you should look at elm-html. In Haskell you could also use yesod shakspearian templates to represent html and CSS. Or directly use blaze. I remember I made my own minimal library for handling CSS with yesod. 
*Sigh*. The point I'm trying to push forward is this - does the community as a whole want greater adoption of the language, or does it want to remain puristic? I think therein lies the crux of the matter being debated. I posit that a language will grow and become more vibrant by getting as many people as possible into its fold. That means embracing people with different points of view and different approaches to the same problem. This means that the barrier to entry for the beginner must be kept as low as possible. After all, becoming competent enough to code idiomatically in any language takes years of working with that language, doesn't it? On the other hand, if the whole idea of a community is to have people write perfectly "idiomatic" (whatever that means) code and work within the confines of what the community as a whole deems beautiful code, then sure, have at it! This just means that the community will not be as big or vibrant as it could possibly have been. By the way, that Abelson quote actually has very less to do with the point you're trying to put forth. They're merely implying that code should be human-readable, and not obfuscated beyond recognition. I don't think anyone would call the programs that the OP has written obfuscated or unreadable.
Absolutely not! For prototyping, there's nothing better than "railroad-oriented programming", where you just describe the shape of your data using types and then fill in the blanks.
Note that GHC already computes something related: the topological ordering of modules. When you look at each module in order, you can be guaranteed that it only depends on things before, not after. This accounts for transitive dependencies. Are you sure that's not what you wanted here? To solve the mutual recursion problem, one fix is to encode each module as (ModuleName, IsBoot), count hs-boot file as being IsBoot=True, and a SOURCE import a pointer to IsBoot.
Right, I know GHC is doing something similar, I'm just not sure if it's easy to tap into via its API. I didn't try =) Edit: This already accounts for transitive dependencies, no?
&gt; does the community as a whole want greater adoption of the language, or does it want to remain puristic? Haskell: Avoid (Success at All Costs) :)
Note that as per this message: https://mail.haskell.org/pipermail/cabal-devel/2015-May/010164.html There is a branch of cabal-install that is 95% to a complete https solution, using external binaries of curl, wget, or powershell for upload and download both. It _really_ needs some people to round off the last few issues and give it a thorough test on a range of systems, so that the patch can land in the main branch. If you're interested in securing hackage and cabal, and have a little time to spare, please jump in and help get this work through the finish line!
I have not personally seen any other HTTP server library that encodes anywhere near as much API information in the type system.
That work is vital, and I'm very appreciative that you're doing it. However, it will still fall short of the improvements I'm discussing here: * Cryptographic signature of the package index itself * Hash checking on package tarballs * Download size checking on package tarballs * Incremental downloads of the package index
We are used to the former, but we can be flexible and adapt to the latter way of doing things.
You're right it doesn't solve those problems. I just noticed your `stackage install` documentation linked to the start of that thread. I wanted to update with the current status of that thread, and take this opportunity to let people know where things stand, and encourage them to jump in and contribute. No attempt at competition intended :-)
I see a couple of technical/practical problems with using a Git repo to store Hackage's meta-data (rather than e.g. using a flat append-only `00-index.tar`): If we had a proper all-cabal-hashes (i.e. not the one we have right now with a truncated history). We'd have one commit for each transaction (that includes cabal-revision updates). And then we'd currently have *roughly 60000 Git commits*. Each Git commit is an object in the Git store, and requires each a tree of tree-objects (of which at least the top-level tree object and 2 sub-tree objects change w/ every non-empty commit), and then obviously the Git "blob" for each `.cabal` file. That's the logical data-structures involved. It gets a bit more complicated due to the way Git stores in delta-chains, but that gives us a rather huge data-structure. But more importantly, the current approach seems to require the Git repo to be extracted to the file-system. This results in *at least* 55k file entries being consumed (as there such many `.cabal` files), plus the `.git/` store (which even when git-packed takes up 146MiB -- and that's actually just for a truncated 326-commits Hackage history...^[1]), and the `00-index.tar` file you are gonna construct out of it... that's quite a bit of additional I/O operations for each `cabal update`... You are also gonna waste some space due to the filesystem block-size when storing 55k smallish (relative to a typical blocksize of 4KiB). For reference, the uncompressed `00-index.tar` currently is currently roughly 177MiB big... And finally the actual problem with extracting to a file-system. For example, the original `00-index.tar` is supposed to contain the following two entries: Thrift/0.6.0/Thrift.cabal thrift/0.6.0/thrift.cabal However, if you unpack to a file-system which doesn't keep apart those two entries (if for instance it's not fully case-sensitive) you're gonna run into subtle issues... In summary I think that using a Git-based solution for storing the `.cabal` meta-data doesn't scale well. The "stupid" append-only `00-index.tar` is simpler and has far less issues in terms of scalability and incidental problems inherited by the filesystem. ---- ^[1] I'm actually a bit surprised `git repack` has such a low compression ratio, as the current gzipped `00-index.tar.gz` only needs 9MiB (and for reference, a `00-index.tar.xz` would have about 5MiB -- that's a 35-fold compression ratio!) 
Yesod has type-safe links too (it's what inspired us to try `mkLink`). And it's hashing of resource filenames for better caching is really wonderful - another idea I hope we can steal (though not directly related to your question). It's always a little tricky to say what it is that is type-safe in any particular case, but as /u/christian-marie said, servant does encode a lot at the type-level.
I'd recommend actually trying the tool and/or reading the code to understand how it works. I realize I didn't go into those details in the post, but your concerns are based on things which I never implied and which aren't the current functionality. Specifically: * We clone with `--depth=1` so that the entire history doesn't have to be cloned by each user. It is, of course, available to anyone who wants that information. * The files never get extracted to the filesystem. We make the working tree reflect the `display` branch so that curious users will see a helpful README, and then use `git archive` to tar the files from the current-hackage tag. None of those files ever get written out. You are completely correct that extracting that data to the filesystem would be bad, which is exactly why we don't do it :)
Boot files are such a misfeature IMO. Avoiding cyclic dependencies via module splitting and parameter passing works. The amount of complexity in GHC to handle boot files just to facilitate this bad programming habit is terrible.
I'm actually surprised you were able to use `git archive`, as I was under the assumption that `00-index.tar` needs to be in "ustar" format (rather than the `pax` format `git archive` creates afaik). &gt; The files never get extracted to the filesystem. ...that's not 100% accurate though, is it? (-&gt; [Issue #5](https://github.com/fpco/stackage-update/issues/5) ) ;-) However, even if you use `--depth=1` for the initial clone, the issues unrelated to extracting the `.cabal` files to a filesystem still hold, as Git isn't able to compress the Git packs to well. I just cloned w/ `--depth=1`, and the packs still take up 63MiB (and that's what an initial clone costs you in terms of network transfer too! that's almost an OOM more than downloading `00-index.tar.gz` costs you) ; compared to the 146MiB with the 326-commit history. So if you keep updating your Git clone rather than re-cloning w/ `--depth=1` you end up growing your Git clone quite significantly over time. ---- PS: Doing a `time git clone --no-single-branch --depth=1 https://github.com/commercialhaskell/all-cabal-hashes.git` took 22 seconds for me, while `time wget https://hackage.haskell.org/packages/index.tar.gz` took under 3 seconds (both operations reported roughly 4MiB/s download-rate)
&gt; that's not 100% accurate though, is it? No, it's completely accurate. I'm not sure why you're so convinced that somewhere, someone is going to get all of the files onto their filesystem. It's simply not happening. I'm not even certain how you're reading that issue as implying something about the filesystem. And yes, initial download does get more information and takes longer than `cabal update`. All subsequent updates are significantly faster. We also get signing, SSL, and traceable history for free. However, you're not looking at a fair comparison there: `all-cabal-hashes` includes larger cabal files *and* JSON metadata files for hashes, which are not present in the 00-index.tar from Hackage. You should be comparing `all-cabal-files` instead. I can already hear the argument coming about the "ever expanding tarball" proposal coming, so I'll just answer that here: * The code doesn't exist today * When it exists, and when it's not buggy, let's talk * That approaches means that every single cabal file revision is going to add to the initial download size, making the argument for tarball downloading being more faster for initial use weaker over time (until, eventually, the initial git clone will be faster) * The tarball provides none of the security guarantees So we have something that is faster for all but the initial download and more secure, available now. The alternatives I know about are the status quo and a proposal that has yet to be implemented and has some major drawbacks (untested approach, and ever-increasing download size due to revisions). __EDIT__ I see the confusion, with the old way of doing things this would result in lots of files on the filesystem. That's no longer necessary due to using signed tags instead of signed commits. __EDIT2__ No, that wasn't just confusion, you were right, and the code was incorrect. I've gotten rid of that conditional and am now just taking advantage of the older subset of Git functionality. Thank you! For others reading: this was a problem with a workaround added to support older versions of Git. That workaround is no longer necessary due to usage of tags instead of branches.
&gt; So if you keep updating your Git clone rather than re-cloning w/ --depth=1 you end up growing your Git clone quite significantly over time. As opposed to the append only tarball which doesn't grow over time at all...
How trivial is it to port an existing scotty app to spock?
I'll try and make an analysis of the advantages presented from a Haskellist's point of view. 1) Less rope Everything said in this section only applies to the Oppressive Object Paradigm. Skipped. 2) Distinctions without a difference This seems to lament the awful necessity of giving names to types, and presents Unnamed Tuples as the superior way. Yep, a real advantage there, buddy - until you call a function on a tuple type it doesn't support, or confuse the order of the fields in one tuple with the order in another tuple. Hooray for runtime errors and silent erroneous behavior! Weak. 3) Abstraction as pattern recognition This is just saying "generics are a must-have". Haskell already has both parametric and *ad hoc* polymorphism. You can fold and traverse all the things easily. Skipped. 4) Speculative programming This is saying you need exceptions and `printf` style of debugging. Haskell has `Debug.Trace` and exceptions. Then there's a "type systems are hard" rant. Skipped. 5) Where's the fire? Scientists seem to prefer Python over Haskell. Well, it's their right. Skipped. Aaaand... that's it. Not a single compelling point. Which is pretty disappointing, actually. I had hoped for a more "everything you know is wrong" revelation from such a large amount of text.
Not having types makes things harder to refactor too which is crucial especially in the prototyping phase.
You can run older versions of CLaSH, but not the latest one(s). The reason is that the clash-prelude(http://hackage.haskell.org/package/clash-prelude) depends on the type-checker plugin ghc-typelits-natnormalise(http://hackage.haskell.org/package/ghc-typelits-natnormalise), where type-checker plugins are a GHC 7.10+ only feature. edit: correct ghc-typelits-natnormalise link
It does happen time to time but in general it's just not a problem. Integration tests handle this well.
While true, the state of Haskell libraries in the 90s wasn't really phenomenal. Libraries are an important consideration when choosing languages.
Which might not happen at all, if the interface disconnect is in some weird, remote part of the library you don't use anyway.
By far the best summary. There is no advantage in dynamic typing, types create an important scaffold in programming and Python is one example how easily you can fall down without. I don't say you can't program with Python, but by the time I find errors in my program, it is most of the time already used productively.
It makes the language more powerful. The set of abstractions you can create in Python is strictly larger with `locals()` than without it. As long as you are able to contain the usage of `locals()` so it doesn't leak in any way, it's not any more confusing than if you didn't use it. You get additional abstractions for "free".
I think there are advantages in dynamic typing (which is a special case of static typing) that this post didn't mention, but these advantages require quite a lot of skill to use properly (which puts the "types are hard" argument out of business) and are not really that widely-applicable. What I'm talking about is: - Lisp-style macros - unsafe C-style bit-twiddling If those two weren't advantageous over Haskell, we wouldn't need TH, `unsafeCoerce` and all their dirty friends.
I personally don't consider Java to be practically any more type safe than Perl. It's just too easy to break its type system, coercion from a type to any other type is easy, reflection breaks everything, and NullPointerException everywhere. Java is just not a good example, really. 
As everyone else is commenting, there actually were multiple really great statically typed languages available at that time were someone to actually have attempted to choose a great statically typed language. But they're definitely not the ones people turned to. There's still a historical theory of why dynamic scripting languages were even interesting and it can't be explained merely by saying that everyone was just simultaneously colluding to ignore ML, Miranda, Haskell. I wasn't around then, so I can't say for sure, but two possible explanations come to mind. * At this point in time the forces directing programming fashion were very much less diverse and "OO, whatever that happens to be"-at-all-costs appeared to push people toward languages like C++ and Java as the bringers of "new" static typing. * Engineering concerns were more around getting hardware and basic OS out and working *as products* than solving for concerns like developer productivity‚Äîthe number of community man-hours spent exploring languages was minuscule (compared to today where everyone and their brother makes a lisp in JavaScript) so whatever language appeared to have support was selected without wide concern for other options. Again, I wasn't there and I'm just sort of plucking from the scraps of stories I've heard. I'd love to get a deeper opinion from people programming and selecting and promoting languages at that time: why didn't you think about ML?
It gives you expressitivity, which is a good thing. But I agree, it's better to get expressivity some other way, but that's not possible with Python.
Why Python? Because otherwise I'd probably need to write it in a shell script. The advantages of not having a compilation phase *and* being able to tweak shit in place if necessary when working with code that fundamentally just glues together two systems that nominally communicate with untyped strings cannot be overstated. Could I do it in Haskell or something else? Yes, but then I'd have to build and deploy some shit when I need to get it fixed very quickly. Would I rather avoid such a situation? Yes, but it's close to impossible to verify that two systems using untyped strings as their input and output respectively are observing their contracts beforehand. Destroy the "Unix way" of using streams of characters as the way of communicating between programs and replace it with DBus (the communication over DBus is typed, right?) or something and then we'll talk.
So how do I know I can trust `E595AD4214AFA6BB15520B23E40D74D6D6CF60FD` to have verified every package? I can't believe one person can verify all hackage packages Shouldn't I trust packages on a per-author basis? 
You're correct. This is what I was trying to explain with the following caveat: &gt; There is still no verification of package author signatures, so that if someone's Hackage credentials are compromised (which is [unfortunately very probable](http://www.reddit.com/r/haskell/comments/352wb1/ann_critical_security_releases_of_cabalinstall/)), a corrupted package could be present. This is something Chris Done and Tim Dysinger [are working on](https://github.com/commercialhaskell/commercialhaskell/wiki/Package-signing-detailed-propsal). We're looking for others in the community to work with us on pushing forward on this. If you're interested, please contact us. This is for solving one part of the problem: securely getting data from some central store onto your machine. It does not guarantee that the other part of the equation (authors getting their data into that central store) worked correctly. For more information, see the [improved Hackage security](https://github.com/commercialhaskell/commercialhaskell/blob/master/proposal/improved-hackage-security.md) page.
&gt; There is no advantage in dynamic typing That just isn't true. It may, possibly, be true for you but it certainly isn't true for many other developers. A lot of good software has been written in Python. Amazing, I know, when it doesn't have a type system. You make it sound like people who like dynamically typed languages don't know what they're doing, which is an attitude that really irks me.
&gt; Scientists seem to prefer Python over Haskell. Well, it's their right. Skipped. I wouldn't be so sure about "prefer". Python is fairly popular, and it seems to me that Haskell is largely unknown among people who don't have a deep interest in programming languages. edit: also SciPy. I don't really think it's about languages themselves.
It should be pretty easy if you use the Web.Spock.Simple API [1]. It is very close to the scotty API, with the benefit of having faster routing :-) [1] http://hackage.haskell.org/package/Spock-0.7.9.0/docs/Web-Spock-Simple.html
Sure, but what about [Turtle](https://hackage.haskell.org/package/turtle-1.1.0/docs/Turtle-Tutorial.html)? Also, text parsing in Haskell is pretty neat, in my opinion.
For me it's the dynamism possible in the REPL. In Clojure, you load your "system" (essentially your IOC container) in the REPL, make changes to functions *anywhere in your code base*, test some input, see results in milliseconds. With F#, even with its REPL, you can't *change* compiled functions. So if you're working on a compiled function, you've got to make the change, recompile, and load the whole thing into your REPL session again, for a much longer work cycle. Maybe the Clojure work cycle is possible in a statically-typed language, but I haven't seen it.
You don't have to parse your JSON into a rich data structure - you can just keep it as a map from string to whatever :)
Well, it's sometimes easier to sneak in Haskell if it's disguised as a Java program... =)
(A corollary here is that someone who comes to Python from e.g. Java may tend to produce APIs that are very hard to use.) This. FML.
Yes, and that's why I made it a point *not* to analyze their reasons. There may be plenty: libraries, like you said; ease of learning Python because its syntax is reminiscent of C and C++ (which are what most scientists know from college or university years); perhaps the fact that Python is already installed on all machines at work and very easy to install at home. Their reasons may be whatever, but it's not an argument in favor of "dynamic typing".
ML has been great since the 1990s.
Well, Haskell can make things *just work* without any cognitive burden too. For example, I whipped up this baby in a minute, it handles lists of lists of floating-point numbers of arbitrary finite length of arbitrary finite length (sic!) without any rank-N types or any type annotations at all: deepAvg xs = let (sum, len) = foldl' (\(a, b) x -&gt; (a + x, b + 1)) (0, 0) (map (foldl' (+) 0) xs) in sum / len Can you make an example in Python in which you see that something *should* work and it magically does, but in Haskell it doesn't because of static typing?
Best news IMO: "Niklas Hamb√ºchen announced that he's backported the recent lightweight stack-trace support in GHC HEAD to GHC 7.10 and GHC 7.8 - meaning that users of these stable release can have informative call stack traces, even without profiling! FP Complete was interested in this feature, so they'd probably love to hear user input. ‚Äãhttps://mail.haskell.org/pipermail/ghc-devs/2015-April/008862.html"
Yes, but the incremental update costs are different. It'd also be interesting to estimate the number of transactions at which `git clone --depth=0` cold-boot would start becoming more economical than the initial `00-index.tar.gz` case.
Also, with the sources.list that I did use, I get a version of 7.8.4 where ghci works, but I can't seem to install any packages at all. No matter which one I pick I get several "assembler messages" about "selected processor does not support arm mod `movw r7;lower16:stg_bh_upd_frame_info', etc.
I'm confused. Does the fact that you're using the stackage tools from now on if you go this route mean that using secure package distribution will not use Hackage and will instead use Stackage? If so, I'm curious why this critical security work is happening in a way that only benefits Stackage.
http://www.degoesconsulting.com/lambdaconf-2015/#schedule-h1
So, will it go for 7.10.2 or a new binary builds will be made for 7.10.1?
You may well be right that there are type issues! I didn't really think it through. 
You'd have to wait for 7.10.2. And for that matter, a 7.8.5 release would be needed, too.
I'm interested.
Fortunately, with `-fdefer-type-errors`, you can test incomplete refactoring while retaining static types. Don't tell those people about things like that, it will only confuse them.
ghci-ng does this for Haskell.
Thanks so much for listening to us (users) on compiler performance, I'm excited to see builds go faster (hopefully)!
It won't go into 7.10 at all - they did the backport themselves so they can use it for their own compiler builds (see my other comment here).
There's already one bug [fixed for 7.10.2](https://ghc.haskell.org/trac/ghc/ticket/10293) on the way, and maybe a few more will get in before the release. In general we've kept somewhat poor track of this lately, but there are some simple improvements we could make to help keep an eye on things, and there seems to be a bit of low hanging fruit too. Reports of Real Slowdowns - preferably reproducible in generated code or in the compiler itself! - are very much appreciated as usual!
TIL.
Of course, especially from a practical point of view, Haskell in the 90s was a *very* different beast from Haskell now. I remember reading a pretty compelling case that Haskell had an inflection point fairly recently which made it far more practical and enjoyable, but it happened a bit before I started using it myself so I can't comment too much. And even now, ML (or, at least, OCaml), still have *some* of the frequently toted downsides to static typing, especially compared to Haskell. At the very least, it's more verbose in practice and makes some things difficult. (And I say this as someone who is currently employed as an OCaml programmer...) And finally, of course, the languages simply weren't popular‚Äîand popularity isn't all that closely related to quality. Even if Haskell and friends did static typing well back then, it wouldn't matter much if everyone was still using C++ and didn't even *know* about the alternatives!
And, of course, times have changed: not too long ago, it would have been Java standing in Scala's place. Perhaps even Java 1.4! (After all, who needs generics?) So I guess there has been progress of a sort :).
I don't remember which comedian I heard this from, but I found it very apt: if the best case you can make for your position is that *it's not illegal*, you might want to reconsider starting arguments about it in public. Put differently, having *the right* to do something doesn't automatically make it a *good decision* that others are compelled to endorse and respect.
[This](http://stackoverflow.com/questions/28341784/when-is-a-generic-function-not-generic) is the example I had in mind when I wrote that - I saw the obvious pattern of `liftIO` and `runSqlPool` on the same `pool` but couldn't actually pull it out into a helper function without type system magic. It didn't end up being that complicated, but it took a frustrating day to do something which seemed obvious. Of course, I appreciate the benefits of the types I'm able to express, and I do find that in most cases it either _just works_ or I've made a mistake - I use Haskell for a reason! - but it's not always a free lunch.
Thanks, That was my meaning!
I guess 'el' is fine because it takes string argument with a tag name, so whenever you look at the code the tag name is getting your attention, and you may also get on without having partially applied versions for various tags.
Probably not much worse but where is the comparable benefit to make it worth the effort? 
Thanks for posting this, I somehow missed it last time it was posted here. Very interesting stuff, I'll definitely play around with it.
Which makes your data pretty dynamically typed.
You're right that the integer-gmp license is the one license in that bundle which is very problematic. For people who care about licenses, the solution is to swap out integer-gmp and not use it altogether. That is an annoying procedure - and very problematic if your application needs an `Integer` type that is actually usable - but it is far less expensive than dealing with the license. Why start down that very bumpy road for yet another essential component of GHC?
i had this project i wanted to do in haskell. then a few weeks later i decided i needed to use it right now and wanted to finish it quickly. so i wrote it in php instead. that is the appeal of a simple scripting language. 
It seems like the [article](http://blog.jle.im/entry/unique-sample-drawing-searches-with-list-and-statet) mentioned in the article is easier to understand, and it's in Haskell rather than C++.
Can you please add Mexico to your list of countries. I want to contribute $150.
Implicits (instead of type classes), subtyping, and JVM interop come to mind.
It's probably less the recording and more the editing.
You're free to have whatever opinion you want. 
There are 5 tracks over 2.5 days, which means you need 5 complete sets of AV equipment and video operators, who have to arrive prior to the conference and stay for the whole event. Then you need to pore through over 70 hours worth of recorded content, making edits, tweaking audio / video, adding intros / outros. It's extremely labor intensive and will take weeks to edit, equipment and on-site personnel costs aside. FWIW we shopped around quotes and only found one quote slightly lower (but with subgrade equipment, no references, etc.).
Are they going to mic audience questions?
&gt; 1. Side effects are important and therefore should be easy to express. &gt; 2. Pure functions are important and therefore should be easy to express. &gt; 3. Indicating when a function is pure is important and therefore should be easy to express. &gt; &gt; Haskell is the first widespread language to follow all three laws. Remark: While Haskell is probably the first, [C++14 `constexpr`](https://en.wikipedia.org/wiki/C%2B%2B14#Relaxed_constexpr_restrictions) is a great way to add purity to C++: constexpr unsigned long long factorial(unsigned int k){ // can only use literal types here unsigned long long result = 1; for(; k &gt; 0; k--){ result *= k; } return result; } Then again, GCC only warned, but didn't error out when I accidentally forgot to `return result`, so there's that.
In my opinion, the key thing that matters is filming the main talks. Filming workshops makes less sense to me, as really interactive ones will film badly anyway, and also may make people more afraid to ask simple/basic questions -- i.e. filming changes the dynamic in sometimes unfortunate ways. For talks, on the other hand, filming would be great!
Easier to learn quickly because you learn a language of values without having to also learn a language of types and how the two relate.
My eyes became really big once I realized you can do this in C++14. Isn't this all evaluated at compile time? What's going on here?
That's amazing. But I hope I'll never see a `constexpr` counter in production code.
In mathematics, that's called an idempotent monoid. Take a look at https://hackage.haskell.org/package/idempotent-0.1.2/docs/Data-Monoid-Idempotent.html
See also: [band](https://en.wikipedia.org/wiki/Band_%28mathematics%29) (idempotent semigroup). E.g. the `First` and `Last` monoids arise from non-monoidal bands that are lifted to monoids using `Maybe`. Another example of a band would be `Set` under intersection. (OTOH, `Set` under union is a monoidal band, or idempotent monoid).
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Semilattice**](https://en.wikipedia.org/wiki/Semilattice): [](#sfw) --- &gt; &gt;In mathematics, a __join-semilattice__ (or __upper semilattice__) is a [partially ordered set](https://en.wikipedia.org/wiki/Partially_ordered_set) that has a [join](https://en.wikipedia.org/wiki/Join_(mathematics\)) (a [least upper bound](https://en.wikipedia.org/wiki/Least_upper_bound)) for any [nonempty](https://en.wikipedia.org/wiki/Nonempty_set) [finite](https://en.wikipedia.org/wiki/Finite_set) [subset](https://en.wikipedia.org/wiki/Subset). [Dually](https://en.wikipedia.org/wiki/Duality_(order_theory\)), a __meet-semilattice__ (or __lower semilattice__) is a partially ordered set which has a [meet](https://en.wikipedia.org/wiki/Meet_(mathematics\)) (or [greatest lower bound](https://en.wikipedia.org/wiki/Greatest_lower_bound)) for any nonempty finite subset. Every join-semilattice is a meet-semilattice in the [inverse order](https://en.wikipedia.org/wiki/Inverse_order) and vice versa. &gt;Semilattices can also be defined [algebraically](https://en.wikipedia.org/wiki/Algebra): join and meet are [associative](https://en.wikipedia.org/wiki/Associativity), [commutative](https://en.wikipedia.org/wiki/Commutativity), [idempotent](https://en.wikipedia.org/wiki/Idempotency) [binary operations](https://en.wikipedia.org/wiki/Binary_operation), and any such operation induces a partial order (and the respective inverse order) such that the result of the operation for any two elements is the least upper bound (or greatest lower bound) of the elements with respect to this partial order. &gt;A [lattice](https://en.wikipedia.org/wiki/Lattice_(order\)) is a partially ordered set that is both a meet- and join-semilattice with respect to the same partial order. Algebraically, a lattice is a set with two associative, commutative idempotent binary operations linked by corresponding [absorption laws](https://en.wikipedia.org/wiki/Absorption_law). &gt; --- ^Interesting: [^Arrangement ^of ^hyperplanes](https://en.wikipedia.org/wiki/Arrangement_of_hyperplanes) ^| [^Maximal ^semilattice ^quotient](https://en.wikipedia.org/wiki/Maximal_semilattice_quotient) ^| [^Free ^lattice](https://en.wikipedia.org/wiki/Free_lattice) ^| [^Join ^and ^meet](https://en.wikipedia.org/wiki/Join_and_meet) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cr73a6q) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cr73a6q)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
[Parallel Scientific](https://github.com/ps-labs) was using it for a while, but I don't know what happened with them. Their homepage now redirects to a personal website. http://cufp.org/2013/jeff-epstein-parallel-scientific-building-scalable.html
I think he means more in the &gt;= sense rather than the &gt; sense, to put it diplomatically. I think it's a typo for "at least as many". 
As r/Peaker noted, dynamic typing does correlate with ease of learning. If professional programmers often stumble on monads, what do you expect of scientists (i.e. non-professional programmers)? I'm not saying it's an unsolvable problem, but the current reality is that Haskell has a fairly steep learning curve compared to, say, Python. I don't think it's necessarily because of familiarity with C or C++ either, as not all scientists are know them. Aside from ease of learning, I would argue that* &lt;speculation&gt; scientists seem to favor heavy overloading over being explicit, perhaps because they are trained to work on a high level while disregarding the minute details that programmers are used to. If you look at the design of popular scientific suites (numpy, scipy, matplotlib, pandas, various R libraries etc), there is a tendency for one function to do several different things depending on what type you feed in (e.g. if it's a filehandle it reads the file, if it's a string it just uses the string). This type of design tends to not work poorly in Haskell because its type system makes it hard, ruining type inference, and even if you get it to work, any error messages might end up being confusing/unhelpful. As a result, scientists would have to use write code more explicitly, which they might perceive to be a bad quality for a language.&lt;/speculation&gt; [*] aside from the reason the OP article noted about regarding the difficulty of catching mathematical errors
&gt; The set of abstractions you can create in Python is strictly larger It's not strictly larger. `locals()` breaks lexical scoping and thus the procedure abstraction. For example, using `locals()` you can leak closure variables, which lets you distinguish procedures which would otherwise be identical: (lambda f: f())(locals) /= (lambda g: g())(locals) And, yes, I'm sure there are other Python features (cough) which already break procedural abstraction, but given how happy Python is to subvert its other abstractions, I'd wager that any abstraction that `locals()` enables can likewise be subverted.
This seems a bit clunky - that stack of closing brackets and parens in that `solve` function even made me think "aarrgghh!!! LISP!!!" (though it's not that bad really) and I'm wondering if the list monad is a plausible sell for C++... 1. Monadic code just isn't going to be as elegant in C++ as in Haskell, or even in some other functional language. 2. To get a backtracking evaluation order, you need lazy evaluation. To get that in C++ you have to explicitly implement deferred evaluation (something like "thunks") for yourself - and the overhead for that won't be optimized away when it's unnecessary as in Haskell. Backtracking isn't necessarily needed for "non-deterministic programming", but it usually has significant advantages over a breadth-first evaluation. 3. You can implement a backtracking solution to that problem in C++ much more simply and elegantly in a non-monadic style. TBH, if I were looking to sell monads to imperative programmers, my first example would be an "iterator monad" in Python, using iterators/generators in place of lists. Reason - Python generators (ignoring effects and effect-sensitivity - pure generators, if such a thing existed) are essentially the imperative equivalent of lazy lists. That said, it's a while since I used Python for non-trivial stuff - maybe the `itertools` module or something similar already supports a monadic coding style with iterators. Anyway, this is part 1 - it'll be interesting to see how it pans out. 
From the paper: &gt; Let A^good denote the ranks of the good spades at the end of the game, and B^good ‚âç A^good the racks where they have landed. A^bad = A‚àíA^good is the the set of ranks of the lost spades. To each element of A^bad there is an infinite chain of spots in 3√óB^good, and these chains are all disjoint. Using these disjoint chains we can use the usual ‚ÄòHilbert Hotel‚Äô scheme to This... seems... not to be a computable processs, but I get dizzy around infinities and the one night I spent at Hilbert's Hotel gave me the worst hangover ever so I'll just trust this for now.
I understand literally none of this beyond the first bits (before it descends into mathematical notation everywhere). Is there some code/a compelling runnable example? 
Entirely besides the point perhaps.. But these sorts of problems are best delegated to actual constraint-solvers, an SMT solver in particular can handle this in no time with little coding on the Haskell side. Here's one encoding: https://gist.github.com/LeventErkok/49347cc0b96d72efe17c (Note that even if you're coding in C/C++ you can use an underlying SMT-solver. Most solvers today come with bindings for most major languages; and almost always including a C-based one.)
The largest body of practical stuff I've seen is http://gallium.inria.fr/blog/ornaments-in-practice/ but I haven't seen any progress in a while. Unfortunately, it's still in the process of being condensed out from pure math. There are currently no languages that really support ornaments yet. The collection of hyperlinks is a good start though. However, I have never seen anyone refer to this OO analogy, so I wouldn't know where to point you. We're working on a different implementation that uses lenses and generic haskell, so hopefully once there is a concrete syntax and semantics it will be easier to produce practical examples. I tried to be extensive with the hyperlinking, because there really is a lot of background, but it's still a bit of a brain dump. So if there are particular things that don't make sense, I can try to add more pointers at least.
I think it needs at most omega steps of shape-up/ship-out, and any individual spot stabilizes after finitely many. The trouble is that you might not know when a spot has stabilized.
That is absolutely wild. C++ compilation is going to be *even slower* if ```constexpr``` gets popular. I'm going to need to run a ```static_assert``` just to verify. EDIT: Christ I tried it, and it works...there's no way this isn't going to be abused. constexpr unsigned long long factorial(unsigned int k) { unsigned long long result = 1; for(; k &gt; 0; k--) { result *= k; } return result; } int main(void) { static_assert(factorial(5) != 120, "factorial(5) evaluated at compile time"); return 0; }
I really disagree with this paper I love the fact they are trying to explain parsing but why did they invent whole new operators for bind fmap ect.
It certainly seems like it could use some `Instance` declarations, yes. I'm working through it right now using the author's notation, but I plan to go back and rewrite things in more idiomatic Haskell when I'm done.
As a beginning Haskeller, I appreciate this code. It's not quite out of my league, and there's a lot I can learn from this. Right now I'm studying the Pid Controller code because that's something I've done in the past.
Unrelatedly, can you give an example of what a termination proof would look like? Is it just another algorithm that fits in the terminating fragment along with a proof they produce equal results?
I was really hoping for a 96-point DON'T.
Indeed! Fixed now.
When A is finite, the existence of an injection A -&gt; B is also implied by Hall's theorem and can be computed with any max-flow algorithm. I haven't checked carefully but it looks like you can interpret the shape-up ship-out game as a flow algorithm for bipartite matching, and the fact that it's "canonical" is because you can use the order on N to break ties rather than an order on A. The interesting case seems to be when A is infinite. It looks like the proof requires creating an infinite sequence of maps A x N -&gt; B x N that converges pointwise (since each slot can change at most 2*N times), and then repeats starting from the function defined by these pointwise limits A x (N - 1) -&gt; B x (N - 1). Does anyone know what axioms you need to say that that function exists? It doesn't seem like something you can do constructively to me. The paper seems to indicate that it shouldn't require choice. I'm not sure what the relevant axiom in between is, and on reading the paper more closely it looks like the authors don't either.
I'm sorry, but I find every single one of these diagrams incredibly hard to understand. But as an outsider to the visual programming language community, my opinion is probably worthless.
The shapes seem a bit arbitrary and aren't as evocative of the original text as would be necessary for this not to require some memorization. I'm in the middle of writing a book (for learning Haskell as a beginner no less) so things like this are of interest to me. If you liked, I could take a screenshot of how we do things in the book, but it's decidedly *not* a graphical language for expressing Haskell code - in fact, the way we visualize things in the book is more like text (ideally matching original source as much as possible) with more aggressive use of two-dimensional space. I would recommend you not give up on the idea, but iterate on smaller components with experienced Haskellers and learners to see what visualizations seem to help the most. It's hard to know what works when your nose is against the paper the whole time. For example, given the code: y1 = f1 x y2 = f2 x It's not at all clear to me why those are visually connected in your drawing. They might be using the same free variable but they're totally independent declarations.
A screenshot of your book would be great. I am targeting this towards people who already know Haskell, so some memorization might be necessary. However, it might be a good idea to create a beginner version of the visuals that has more text labels on the icons. For y1 = f1 x y2 = f2 x The triangles are only connected because a line that represents the value of **x** runs between the two triangles. Perhaps separating the triangles more would make that more obvious.
Lots of good advice but I'm unsure about ExceptT IO being an antipattern. &gt; It's non-composable. If someone else has a separate exception type HisException, these two functions do not easily compose. One can always massage the errors and put them in different branches of a sum type. Also, it's a chore to add extra extra context to a regular exception, but it's easy to do when the error is a regular datatype. &gt; It gives an implication which is almost certainly false, namely: the only exception that can be thrown from this function is MyException. If often use it (for example in [process-streaming](http://hackage.haskell.org/package/process-streaming-0.7.2.1)) with the following implication: "I don't catch or try to recover any exception originating directly from IO, but for other types of errors I use the sum type." &gt; You haven't limited the possibility of exceptions, you've only added one extra avenue by which an exception can be thrown. That's a concern, yes. (The [catching all exceptions page](https://www.fpcomplete.com/user/snoyberg/general-haskell/exceptions/catching-all-exceptions) is also good. [enclosed-exceptions](http://hackage.haskell.org/package/enclosed-exceptions) is a very useful package.)
It is a pure before representing a side effect. You can construct it, pass it around as an argument to functions etc. It only becomes a side effect when it evaluated in the IO monad.
I don't fully understand this comment; however, a perhaps subtle difference between Glance and other visual languages is that lines in Glance represent values (ie. Haskell variables). In other languages lines might represent a time based flow of data between two visual blocks (eg. LabView). I purposely avoid using the term "data flow" when talking about Haskell for this reason.
Yeah, sorry about that -- I gave the upper bound for a reason. I'll look into porting it to a more compatible library, maybe just plain OpenGL. For now, it's only tested with GHC 7.6.3.
https://github.com/SKA-ScienceDataProcessor/RC Haskell code using distributed-process is a coordination layer for radiotelescopy data processing.
[tweag.io](http://tweag.io) seems to continue developing cloud haskell: https://github.com/tweag But I do not know how they use it in real world application. Can anyone (maybe from tweag.io) tell some story?
You are correct. A Haskell exception is more-or-less equivalent to a `Left` of type `Dynamic`.
We can dream.
Did you say you want to port SDL to OpenGL?!
Are there more texts on this? I always feel awkward dealing with errors in Haskell and I haven't quite found out a good middle ground. I feel like it would help if I got the perspective of several other people.
I've torn out my hair in dealing with exceptions in IO-heavy code plenty and I can pretty much underwrite that `MonadThrow`/`MonadCatch` with `throwM :: Exception e =&gt; e -&gt; m a` are the state of the art. Note: everything described below I've actually done before. The points in this system's favour: * It works for any exception. Restricting the type of exception that can be thrown might seem like a good thing at first glance, but it very quickly becomes a FUBAR. Consider something like data IndexException = NegativeIndex Int | TooLargeIndex Int Int lookup :: Int -&gt; [a]-&gt; Either IndexException a data RegexException = MalformedRegex Regex type Regex = String runRegex :: Regex -&gt; String -&gt; Either [String] It's, as the document said, non-composable. You can't do `runRegex "[a-z]*" &gt;&gt;= lookup 0` because `IndexException` and `RegexException` are different types. You'd either have to put both under a common sum type `Either IndexException RegexException`, or you'd have to create a bigger exception type `IndexOrRegexException`. Both approaches, after a few such exception-throwing functions, will end up with data MyException = NegativeIndexException | TooLargeIndexException | RegexException | FileNotFoundException | HTTPHeaderException | MalformedDateTimeException | FileNotFoundException | InvalidDigitException | UnexpectedBooleanValueException | UnexpectedExceptionException If you have this kind of huge sum, you don't have any meaningful guarantees anyway and just take the infinite sum over all instances of `Exception`: `throwM :: Exception e =&gt; e -&gt; m a`. * There's no issue with handling exceptions from other libraries. Related to the previous point: even if you were set on creating `MyException`, you might run into a library by someone else who had the same idea. In such a hypothetical scenario, you end up with a `ParsecException`, a `HTTPException`, a `UnixException`, and your `MyException`, of course. The latter you'd then have to extend with the constructors data MyException = WrappedParsecException ParsecException | WrappedHTTPException HTTPException | WrappedUnixException UnixException | NegativeIndexException ... And then imagine exporting `MyException` as part of your library! * The kind of failure that occurs depends on the Monad, with Maybe returning Nothing, List returning [], IO throwing exceptions as usual. This is the most general and comfortable solution, I've found. In a way, agnosticism w.r.t. the Monad parallels agnosticism over the exception type. If you have different functions with different Monads, you can't easily compose them and end up juggling error, Maybe, Either, ExceptT, ErrorT, and NonFatalLoggedFailureWithPartialResourceReleaseT. I agree with those who say that `MonadThrow` is not optimal and that it would be nice to see what kind of exception a function can throw, but I also think that going the route of Java's checked exceptions would be a mistake. This sort of information would, ideally, have to be generated by inference, or some dependent type voodoo, perhaps by shuffling a `PossibleExceptions : Set[Exception]` around. It should not be an obligation of the programmer, since in 99% of cases, writing out the list of throwable exceptions would be pointless busywork and nobody would do it. Addendum: I think it'd be really neat if we scrapped partial functions in favour of MonadThrow. In that way, you could replace head, tail, (!), (!!),... with safe ones (shameless plug: I did this in [listsafe](https://hackage.haskell.org/package/listsafe)). If you then REALLY wanted to have unsafe access, you could have a type class -- |Theoretical abomination. class MonadCatch m =&gt; MonadUnsafe m where unsafe :: m a -&gt; a instance MonadUnsafe Maybe where unsafe = fromJust instance MonadUnsafe (Either e) where unsafe = fromRight instance MonadUnsafe [] where unsafe = head
The [github repo](https://github.com/haskell-game/sdl2) has a higher level interface.
I tend to prefer exceptions only for things like "My SQL count(*) query unexpectedly returned two result rows" or "the row I just inserted in the previous line unexpectedly isn't in the database" too (i.e. in cases I just check to keep the case checker happy but do not expect to ever happen in practice at all, almost no matter how badly my assumptions fail to hold).
That sounds reasonable. I think if I were writing the coding standards the requirement would be "if you ever encounter the exception in a production system, rewrite the code to reflect it in the type".
But if I used that I wouldn't be able to release RoboMonad on Hackage until sdl2 catches up, which might not be any time soon...
I've thought about this a lot and experimented with functional programming in PHP. Since PHP 5.3, you can define [anonymous functions](http://php.net/manual/en/functions.anonymous.php) and higher-order functions. Partial function application is also trivial to define. So far, so good. Referential transparency would be possible if you used arrays for everything (hey, sounds like PHP already!), because those are copy-on-pass data structures. You can get references to them, though, which many built-in array functions do. It was a gigantic pain in the ass, so I just rewrote all the built-in functions using map and fold. The closure syntax in PHP is annoying (you need to explicitly mention the variables to close over), so perhaps translating it to `call_user_func` and `func_get_args()` calls might be useful. I don't know how that would work with higher-order functions, though, because you actually need to provide it either a closure or a string that contains the name of a function (perhaps generate dummy function definitions? Ewww). Concerning laziness: Since PHP 5.5, [generators](http://php.net/manual/en/language.generators.overview.php) are available. They work pretty much like every other imperative language implemented them: they're a (potentially infinite) for-loop whose execution is frozen after generating a value. Calling the generator again produces another value. You can plug a generator into another generator, so it seems we can recover most of the lazy semantics here. From what AST would you want to generate the PHP from? Core?
Yes, for serious applications the cheap-web-host-argument is not that important. I was more thinking like: Many people start with small projects and only later get to work on the serious stuff (or the toy-projects start to grow later). For early toy projects the cheap-web-host-argument matters, and people tend to continue to use what they already know (or what their current codebase already is written in). I use myself as an example here. I made editgym.com (small toy project) and am thinking about adding a backend for keeping highscores and stuff, but my cheap domain only supports PHP. Now I have the choice of * use PHP * buy more expensive webspace to use Haskell * discard the idea of a backend
Thanks for this very informative reply. &gt; From what AST would you want to generate the PHP from? Core? I have no experience in writing compiler stuff, so I don't know. Right now I'm more interested in using the end product and dreaming of inspiring somebody to create it. ;-)
That's not really answering your question but [haxe](http://haxe.org/) compiles to PHP. It looks like PHP but with type inference and other nice stuff. It also compiles to JavaScript, C, C++, Python, Java, etc ... I've used it a bit and find it quite pleasant even though I eventually gave up.
&gt; for any serious application For non-serious applications, which don't generate any revenue whatsoever, using the cheapest available web host makes sense, and so the fact that the cheapest web hosts support PHP but not Haskell is a significant deterrent to using Haskell for those non-serious projects. For example, when I implemented this [variant of tic-tac-toe in which perfect games do not end in a draw](http://gelisam.com/board/egg/) (instructions are in French only, sorry), I implemented the AI in Haskell, but couldn't run it on my cheap JustHost server, so I dumped all the moves into a database and wrote a dumb PHP wrapper which played from this lookup table. (for the purpose of this project, compiling to javascript would not have been an adequate solution)
I just realised I hadn't properly added all the demo files to GitHub, so people trying to run it will have been disappointed. Oops! They're all there now.
Thanks, I also found haxe. It looks OK, but I like Haskell more. ;)
OK, fair enough. I still feel that toy projects are important, because they (or their developers) can scale. One.com as an example provides cheap webspace with PHP for about US$1.5/month. But your offer sounds great. Thanks. :)
[Idris compiles to PHP](https://github.com/edwinb/idris-php) as well, and is much closer to Haskell than Haxe is.
Cool! But I'm not sure if it is already feasible to write a web server in Idris. I guess I now need a Haskell-to-Idris compiler. :D
I like the idea of a visual representation of Haskell code (maybe because I sometime do it in my head) but it seems that it becomes quickly really messy when things get more complicated, and of course that when you need it ;-) Also, some symbols are too similar or arbitrary. However, I really like the idea.
Therefore I really like what Rust did. They have only one kind of uncatchable exception, which is called panic. 
This is an exercise in a course in Haskell taught at Lund University (EDAN40). This is old, but it's also entirely for educational purposes.
Yes I'm not familiar with lenses. I had to go on the Haskell IRC and they told me that was using lenses, so I looked it up and lenses are definitely above me. And I agree, this project is much more fun to learn from!
&gt; Please, don't ever use this. HAHA TOO LATE
The days when PHP was the only easily available hosting option are long gone. These days you can get a full linux server on which you can run any software you want for a couple of dollars a month from lots of different providers.
Basically, yes. This is a little bit unsatisfying, but it's also hard to imagine how to avoid - if you think about the meaning of the type I showed in this talk for termination proofs, the computational content of that proof will necessarily include an implementation of the function. Still, I think there is a lot of value in separating this from the function itself. I won't reiterate everything I said in the talk, but the ability to avoid reasoning about termination up front without sacrificing the consistency of your logic is a big win for actually getting things done. One possibility for mitigating the pain of reimplementing the function when doing an extrinsic termination proof could be to integrate external termination checking technology. There is a lot of very fancy work on proving functions terminate that you don't really want to include in the core theory of your language because it becomes hard to do the metatheory. But it's probably possible to modify these termination checkers to produce primitive recursive versions of the functions that they prove terminate (though, to be clear, this would be new research). These machine-generated versions would be ugly, but we probably don't care because we're actually going to use and reason about the original programmatic version most of the time - we only need the termination proof once.
You're right. The article does explicity say that it doesn't address the question of the circumstances under which exceptions should be used. However, as a related question I am interested in knowing the justification for ever using them. I've never heard anyone come up with a rigorous reason why they are necessary except in very specific circumstances like internal invariants being broken.
If you have `runRegex "[a-z]*" &gt;&gt;= lookup 0 :: MonadThrow m =&gt; m Foo` then you can instantiate `m` to `Either SomeException`. MonadThrow *does* reflect the presence of exceptions in the type, just as well as `Either Dynamic`, but even better because polymorphic code with exceptional cases can be instantiated to the exception-handling technique of your choosing.
Wouldn't it be better for the extension to add an `ado` keyword instead of overloading `do`? But maybe I'm lacking general knowledge. If you're dealing with data that has both Applicative and Monad instances, which desugaring will be used? Are Applicative and Monad required by laws to perform the same way?
I'm not quite sure about this. If you use a newtype wrapper around ExceptT MyError IO, have a MyError constructor like IOExceptionThrown IOException, and then define a MonadIO instance that puts IO exceptions into the IOExceptionThrown constructor, doesn't that solve the problem of having more than one way to throw exceptions? I recently wrote a program which can fail for a number of reasons, all known at compile time, and I want to make sure that there is good error reporting in all of these cases. So I want to leverage GHC's exhaustivity checker to make sure every case is handled; I want to be able to pattern match on the exceptions. I also want it to be clear -- both in the error messages for users, and for other people reading the code later -- which errors indicate a probable bug in the program, and which indicate that the user has supplied invalid data to the program (that they need to fix). I can't see how to do this effectively using the above strategy; ADTs seem to work very well. See: * https://github.com/hdgarrood/purescript/blob/master/psc-publish/Main.hs#L61-L73 * https://github.com/hdgarrood/purescript/blob/master/psc-publish/ErrorsWarnings.hs#L23-L67
I actually run [my blog](http://www.gilmi.xyz) (scotty) on a 128mb ram budgetvm vps I bought for 20$/year. And I was also able to run it on heroku (for free). So yeah, I think toy projects can manage. edit: oops, 15$/year
Looks like you write everything pointfree, now, too ;)
FWIW, I compiled the whole of GHC with ApplicativeDo and it worked fine except for one place that had something like instance Applicative M where f &lt;*&gt; a = do f' &lt;- f; a' &lt;- a; return (f a) and ApplicativeDo turns this into an infinite loop, as it should! I think if we'd gone down a different route and made a more aggressive ApplicativeDo that re-ordered statements then it would arguably need a different keyword, but in this form where it doesn't change the semantics (for well-behaved Applicative/Monads) I think it's fine.
Only today, I learned about an emerging alternative editor for Haskell development. Can anybody else share their experiences? Is anybody working on providing more support for Haskell in Atom?
I think structural editing √† la [Lamdu](http://peaker.github.io/lamdu/) is a stronger choice for a future Haskell representation. This seems like it could be a good option for more complex Arrow notation, but for the case of plain function application it seems overwrought.
only while i'm prototyping out a function or streaming words into the reddit text field i find point free code cheap to produce and refactor but more difficult to read once you've forgotten all the points so i try to label them once i've convinced myself in ghci that the function does what i think it does as for point free prose the tradition is strong and words are often hard i generally prefer white space to extrenuous characters to lex
I like exception systems in that they allow me to separate reporting an error from handling an error. I find the separation helpful for clarity, with a nicely readable optimistic path on one side and the boring plumbing elsewhere. Sometimes you can also refactor recurring handling/catching patterns in one place. I place a high value on DRY so that matters to me. This separation between the place that reports and the place that handles is the essence of what I care about. Whether the errors appear in the types, and how it does so, are separate considerations. So for instance `Either`/`ExceptT` (which you mention) work for those purposes. At the same time how `Either` and `ExceptT` indicate the errors in the types is precisely why I only use them in select situation. Much has been said about composition already so I won‚Äôt repeat it again even though it is a big deal. Even if composition were solved I‚Äôd still rather not have the closed world of errors approach by default. While it is a gold mine for safe system-style programming (think Posix functions and `errno` and man pages which list all possible errors), most programs I write don‚Äôt fall into that category. Open world exception systems let me write programs that I can know for sure will not crash for those classes of errors I handle, and if something else happens then so be it. For an error like out-of-memory, crashing would in fact be the desirable default behavior for my programs. Having a program written yesterday with the at-the-time-unforeseen errors of today, even though it might crash, can also be more helpful than not having it run at all. Of course I have yet to find a type system where \x -&gt; (x `catch` \(ErrorFoo a b) -&gt; qux) is typed as 'from exceptional `a` to exceptional `a` with `ErrorFoo` handled'. And maybe if I ran into one I would change my mind. In the meantime I‚Äôm happy to stick with open-world exception systems even if they don‚Äôt play nice with the types. 
My point is that they are independent uses of a free variable, not as directly connected as they would be if they were `f1 (f2 (x))` for example. You've no reason to justify what you're doing to me, but it doesn't matter if it makes sense to other visual language designers - it should make sense to *Haskell users*, right? Who's your customer here? Don't go round in circle with me - get data. See what confuses other people.
One common use of exceptions is dealing with hardware failures. Most software is written with the assumption that the code will be executed as written. So when an "impossible" situation arises, it can be because of a failure in the hardware (real or virtual) delivered by the physical, hypervisor/VM, or supervisor/OS layers. The classic example is failure to read back something you just wrote, due to storage hardware failure -- such a cloud NAS outage or, heaven forfend, a RAM fault.
that is about as good as a bunch of server-side programmers can get :-)
Preivous discussion: http://www.reddit.com/r/haskell/comments/2yxgaj/atom_text_editor/
It feels that way to me, but it doesn't yet [support](https://code.visualstudio.com/Docs/languages) Haskell.
Honestly, I found it absolutely fine. You can look at the linked code if you like.
We can vote for Haskell support in VSCode https://visualstudio.uservoice.com/forums/293070-visual-studio-code/suggestions/7756542-haskell
I did. I liked the look of it!
We plan a proper post on this hopefully soon.
Non serious application written in Haskell? Are you serious?
Would `sequence` also be removed from `Traversable` (in favor of `sequenceA`)?
Maybe but not as much as some people using it in production üòä.
Yep with the same argument. Then `sequence` would ultimately be usable with just an `Applicative` constraint.
Thanks for the great explanation (and the really promising stated goals).
Without reviewing the code carefully, my immediate question is if you've tried the various different AD modes to make sure you're using the most efficient for your task.
Thanks for noticing! I had the `Num n` constraint because I used (-) and (0&gt;=), but since `Integral` offers `toInteger` anyway, it was unnecessary. I fixed it just now.
Last time I checked, `ad` represented derivatives and jacobians as lazy lists. For real performance, these need to be stored as vectors and matrices. And the truly fast libraries do even fancier things. I have an [alternative implementation](https://github.com/mikeizbicki/subhask/blob/master/src/SubHask/Category/Trans/Derivative.hs) of automatic differentiation in my subhask library. It lets you augment all the arrows of any subcategory of Hask with derivative information. Since it works on all arrows, it works nicely with vectors and matrices. I wouldn't recommend you use it yet though. The implementation isn't production ready. It still needs documentation and a more user friendly interface. But at least know that fast automatic differentiation is certainly possible in haskell.
&gt; Of course I have yet to find a type system where &gt; \x -&gt; (x `catch` \(ErrorFoo a b) -&gt; qux) &gt; is typed as 'from exceptional a to exceptional a with ErrorFoo handled'. There [one on hackage](https://hackage.haskell.org/package/control-monad-exception) that types things this way. I'm not saying it's the best solution or production-ready, but it is possible to build those systems in Haskell, under certain conditions.
Yes, if `ado` is the same as `do` with `ApplicativeDo` desugaring. You could have a simpler `ado` which only accepts independent statements (like Example 1 on the wiki) so that it could always be transformed to `Applicative`, and that could be done with no heuristics, but it wouldn't be useful for what we want `ApplicativeDo` for.
Sorry, I wasn't really talking about the design so much as the content. I should have said "the web site looks like it has good content".
But `|&gt;` is more a flipped `$` like `&amp;` and not really `.`. I also like `&amp;`: "Hello world" &amp; words &amp; reverse &amp; unwords
I love that a 18 line program generated a 195 line error message! And people say that Haskell has bad error messages. :P
Does not *swapping* already include the idea of vice-versa, I mean , is not swapping "::" and ":", and then swapping ":" for "::" not equivalent not swap anything ? ;-)
Remember we have a severe demographic bias problem in computing and Haskell-land. This image doesn't send a good message.
maybe if you use casadi-bindings instead of ad it'll be faster like here: http://code.haskell.org/~aavogt/ipopt-hs/examples/bench.html#williams-otto-process
I was considering that 'x desugars to use of something like: `class QuotedName a where quotedName :: Tagged Symbol (Maybe Language.Haskell.TH.Name) -&gt; a`
I find sugar that is difficult to desugar scary. This is one reason I dislike the arrow notation. I wish there were some simple desugaring of applicative do, because I can certainly see the utility of it.
The trick to explaining monads is not to explain monads and instead explain IO. From there you then introduce the abstraction.
You can think of "layers of f" in purely data structure terms. Or you can think of each layer as representing a strategy for making a command and handling a response. The general idea is that the *shape* of each node represents a command, while the choice of a subnode *position* within a node amounts to a response to a command. A value in `Free f x` thus represents a strategy for computing an `x` by interacting according to the "interface" given by interpreting `f` as a command-response system. The `Pure` strategy means that you give a value without making a command. The `Free` strategy means that you choose a command, then continue according to a different strategy depending on which response you get. The "substitute for `Pure`" behaviour of `&gt;&gt;=` corresponds exactly to sequential composition of strategies.
&gt; it becomes quickly really messy when things get more complicated Yes, I think this is one of the biggest challenges with a visual language. Some of the visual syntax of Glance (eg. shared parameters, bubbles, grids) attempts to address this by making the diagrams less messy. If a diagram gets too messy, it is always possible to replace lines with text, but that of course defeats the purpose of displaying it visually. &gt; Also, I'm not sure that having two symbols (ex: triangle and semi-circle) for the same thing (apply) is a good idea. First, it's confusing and then, you are wasting semicircle for something else. Wouldn't be better to have keep the triangle in both form of apply but with a different orientation (so basically it's always the same symbol, but just twisted)? I expect that some or most of the icons will change or be revised. The semicircle does not seem to be used very much, so it would make sense to use a different icon. I don't know how using the triangle for both forms of apply would work since the symbols are topologically different. The triangle apply has two connections for the argument, while the semicircle apply has two connections for the function.
&gt; I think the author is just trying to write down is vision without necessarily trying to sell it Exactly.
Your "however" is correct. Your "Your co-worker is correct" is incorrect if the co-worker has been correctly reported. To say `&gt;&gt;=` captures "sequential actions" is to say nothing about evaluation.
I suppose our disagreement is rooted in different understandings of the term 'sequential'. I take it as some approximation of 'the processor first executes this chunk of code, then the next'. Not sure if this is the accepted meaning outside of concurrency-related conversations.
`Applicative` is essentially a version of `Monad` where actions can never depend on the results of previous actions (whereas with `Monad`s they may or may not). But it seems we're using the term 'depends' differently: I would say that an expression `a` depends on an expression `b` iff we need to know the value of `a` in order to compute the value of `b`. In this terminology, `const` does not depend on its second argument. I don't know if GHC actually performs any such 'downgrading' optimisations, but conceptually it could. Incidentally, the ApplicativeDo proposal discussed here recently[1] goes into the same direction. [1] http://www.reddit.com/r/haskell/comments/35u04t/rfc_applicativedo_extension_for_ghc_712/
"Combinable stacked higher-order functions". 
I'm not familiar with that broader usage of the term in this specific context. But yes, I can see why one would call a sequence of nodes and edges sequential. ;)
Its how the coder has written the sequence in code, not how it is actually evaluated. The point is, that semantically speaking of *sequential* means, that at least somewhere is something sequential in it. If its not in the evaluation part, you can say its in the code part, which is obviously a sequence ordered by operator precedences and line order.
Have you seen [Control.Monad.Tardis](http://hackage.haskell.org/package/tardis-0.3.0.0/docs/Control-Monad-Tardis.html)? The Tardis is a monad that uses both state and reverse-state. Here's an [example](http://www.reddit.com/r/haskell/comments/1qcqwy/a_functional_solution_to_twitters_waterflow/cdbpuk9) solving the twitter water flow problem.
Thanks! This is a pretty clear explanation. So the statement that Monads aren't "sequential" (looking at the definition of `&gt;&gt;=` is enough to see this I say) is wrong. But he is not incorrect in the "wraps it and builds upon it". But these are two different ways of seeing `Free f`: one is seeing it purely based as a `Monad` and the other seeing it purely as a data structure that happens to implement `Monad` (and has the word "monad" in its name).
Yes and no. That one presents a closed world of exceptions that can be thrown (cf. the type of `throw`). I was suggesting the reverse: an effectful computation that can throw anything, save for what has already been handled. `catch` in this system will have a similar, but subtly different type.
But a monoid isn't necessarily a sequence either - for example integers with addition is a monoid. Sure, there's a sequence of additions as written, but it's hardly important - especially as addition also happens to be commutative. The meaning is the result - the sum - not something about sequences. The usual example of a free monoid is a sequence/list/string of symbols (the associative operator doesn't do anything more than necessary to glue its arguments together - ie concatenate them) but that's only one monoid (or rather one class of monoids). Actually, I'm curious whether a set is also a free monoid - whether "unioning" counts as just gluing arguments together or whether it counts as more processing than that. [**EDIT** - actually, I suspect no because a set has structure that isn't required for monoids, so the "extra work" is enforcing constraints that monoids don't necessarily have. I suspect the free commutative monoid is a bag (order of elements is irrelevant but duplicates are significant) and the free idempotent commutative monoid is a set (order and duplicates are irrelevant) - and I suspect there's a reason why people say "*the* free xxx".] Anyway, there's a reason for the term "action" WRT monads and that term works well for `State`, `IO` and `ST`. It even works well for something like `StateT s [] r` (a non-deterministic state monad). Since `IO` in particular is why Haskell originally gained support for monads, it's a sensible name. It works less well (actually, it's pretty confusing) for something like `Maybe` or `[]`, though, where the monadic values *aren't* "doing something", they're just values of that type. In that sense, even the term "computation" which a lot of people prefer is IMO a little suspect, though given that the RHS of a bind is actually a function there's a reason for it. So the bind places things in sequence, but that's not really the meaning of what's happening - the sequence is just a convenient recurring pattern in how to express *different* kinds of meaningful things. Personally, I think the trick to understanding monads is to understand lots of examples. The interface/abstraction itself is so simple there's not much "meaning" it can have. On a scale from recurring-pattern to meaningful-abstraction, IMO monads are much closer to the recurring-pattern end. If there was an intuitive meaning to "monad", category theorists wouldn't have needed to make up the word "monad" (though they might have anyway). Although monads are commonly called an abstraction, they're not really an abstraction in the sense of all instances having some shared meaning, only some shared pattern/structure. Actually, when you view some type exclusively as a monad (or monoid or whatever) you've *abstracted away* what makes that type meaningful and unique. 
I am not fit to answer how that works or if it fits as sequential `&gt;&gt;=`. My only guess is some sort of laziness happening that allows `getFuture` to rely on the result of its result (???) or funky GHC stuff.
now write an article explaining it
I'm not sure I like the overreliance on `join`; I suspect it's a *lot* slower than `&gt;&gt;=` for common monads like IO.
&gt; Your co-worker is correct; the monad laws do not prescribe that in the expression `a &gt;&gt;= f`, `a` must be evaluated before `f`. Let's not confuse order of evaluation with order of *effects*...
Both Unicode and binary data expect a lot of backslashes. JSON is seven bit ASCII.
This is most interesting to me because of a tutorial written by the same author: [Writing Atom plugins in Haskell using ghcjs](http://edsko.net/2015/02/14/atom-haskell/)
I'm guessing you meant: ff &lt;*&gt; fa &lt;*&gt; fb =?= flip &lt;$&gt; ff &lt;*&gt; fb &lt;*&gt; fa Yes, I never argued that "a monad is something that follows the monadic laws", my argument was that "they always lead to sequential operations". But the reverse monad (which I still think is sequential, just backwards, though in a way I never thought possible) and the partial order put that to the test (and unless someone can come up with a way to explain those things are "sequential actions" then I'm guessing it is a fat "no"). &gt; Free monads are an excellent example here, because what a free monad does is this: the expressions expr1, expr2 :: Functor f =&gt; Free f a have the same value if and only if the monad laws demand that they be indistinguishable. I understand that a Free monad satisfies the laws and results as a data constructor. My main confusion is when a free monad is used as an actual monad (what does the expression `Free f a &gt;&gt;= g` where `g :: a -&gt; Free f b` mean? Is that sequential actions?). 
*was down. It actually came back up pretty quickly this time. Right after I clicked "submit", naturally. Well at least if you notice a few anomalies in Travis CI jobs that ran recently, this might be why.
Hm, it's a slight shame that this relies on Data.ByteString.Internal and unsafePerformIO. It'd be sweet to have a Data.ByteString.ST, and I suppose, sour to maintain, hence its current nonexistence.
http://status.haskell.org explained that there was planned maintenance for security patches on our host resulting in systems needing hard boots, and blips were to be expected. Just for the info of all: it is worth checking that when you see systems downtime to know if its a planned quick disruption or a known issue or in fact is something unexpected and potentially worse. If there's going to be long downtime or if we're facing real problems we'll make sure to notify other channels, such as reddit. But for quick server bounces, I think it is fine to keep it to one central channel for communication. Also, for the info of all: Some other, less used services (not hackage, the wiki, or the homepage) may also blip sometime over the next twelve hours for the same reason. This mainly affects things like e.g. the ghc trac, phabricator, etc. If for some reason things don't come up properly after the reboots (they are configured so that they should, but one never knows), feel free to drop by #haskell-infrastructure on freenode and notify the admin team.
Note that the function on the homepage has now been renamed from `sieve` to `filterPrimes`. So it doesn't even claim to be any sort of sieve. Hopefully it'll help stave off any arguments about how it isn't the sieve of Eratosthenes since now not only does the word Eratosthenes not appear, but the word "sieve" doesn't either, and so nobody could mistakenly think that this claims to be such a thing.
&gt; (instructions are in French only, sorry) I only know very basic French, but if someone else is lazy: most of the game is pretty obvious, except a couple of things: making three in a row horizontally means you lose, and you can only play on a lower row than where your opponent played, if possible. The premise sounds interesting at first, until you realise it gives the first player a serious advantage (they win every round...) There's also a version 2 which you can play when you've beat the first version, but I have to run!
Also run [my blog](http://two-wrongs.com/) (Yesod) on a cheap VPS similar to yours. My biggest expense with that thing is actually, by a large margin, the license for the font! But I'm crazy like that.
I'm not sure what kind of "migration" Michael is talking about, and why he doesn't want to stay with `system.filepath`. So far, `system.filepath` is the only library I know of that has anything close to reasonable semantics for a cross-platform `FilePath` type. The previously "standard" library, `filepath`, is a constant source of nasty bugs in any real-world cross-platform application due the huge impedance mismatch between the major OSes about the concept of a path. I sincerely hope that the Yesod ecosystem doesn't downgrade back to `filepath`.
One difference here is the empty list case. You can't, by construction, create an empty list in your first example (as far as I can tell from the little context here). So long as that won't be a problem, I think it does indeed look nicer.
It is more that I acknowledge Simon Marlow as a force of nature and am reacting to his taking interest in this area accordingly. ;) I broached a number of simpler half-measures when the topic first came up, but none of them quite fit his use-case. At last check they were unwilling to even ask the folks working with haxl to use `traverse` rather than `mapM`, and asking if they'd be interested in having it work with just `pure` rather than `return` was met with a blank stare. Given that he's the 'man on the ground' implementing it, turning it into something he won't use likely just ensures it never happens at all. =P So in that respect, yes, I'd (rather uncomfortably) advocate letting `return` behave like a keyword temporarily when you turn on this particular language extension, if that is what it took to keep Simon gainfully engaged in working on this extension. Re: `pure` vs. `return`, etc, we could write 'Applicative comprehensions' using the existing monad comprehension syntax, with the same desugaring effectively, but without any return vs. pure concerns, but it'd do nothing to address Simon's usecase.
Is it easier to see the "not necessarily sequential" part of the picture if you think of monads as defined by the join function rather than the &gt;&gt;= operator?
No, because sets don't have a order.
I wrote up [a blog post](http://www.yesodweb.com/blog/2015/05/deprecating-system-filepath) with a little bit more information. In particular, cross-platform inconsistencies and bugs are the primary reason for deprecating system-filepath. Since GHC 7.4, I'm unaware of the kinds of cross platform issues you mention with the standard `FilePath`and filepath package. Can you provide some examples?
&gt; until you realise it gives the first player a serious advantage (they win every round...) This is by design. For all two-player perfect-information deterministic games, either: 1. there exists a perfect strategy for the first player which allows them to win every time, 2. there exists a strategy for the second player which allows them to win every time, or 3. if both players play perfectly, the game ends in a draw (if a game doesn't allow draws, it must belong to group 1 or group 2). Classic Tic-tac-toe belongs to group 3, and once either player discovers the strategy, Tic-tac-toe becomes boring forever. Worse: if you play against a perfect computer opponent, you can either lose or manage to force a draw, but you can never win. To make it an interesting challenge for humans, I wanted to create a group 1 variant. Of course, the challenge is to find the winning strategy, and once you find it the game becomes boring forever, so please don't spoil it :) To make such a deterministic game interesting for two human players, the game either needs to be so new that neither player knows the winning strategy yet, or the game must have so much depth that a winning strategy will probably never be found. That's the case of chess, for example. With only a 3x3 board, even with extra rules to increase the number of game states, it's too easy to brute force the entire state space and to find a strategy for perfect play. On the other hand, that means it's also easy to write a computer opponent which brute forces the entire state space, forcing the human player to find this perfect strategy in order to win. This is what I did for [the game's AI](https://github.com/gelisam/tic-tac-top). Feel free to look at the source code even if you haven't found the winning strategy yet: the source code only reveals the rules of the game, which you already know, so there are no spoilers. I think the [AI](https://github.com/gelisam/tic-tac-top/blob/master/src/AI.hs) module is pretty cool: its implementation of [MiniMax](http://en.wikipedia.org/wiki/Minimax) uses laziness and sharing to avoid recomputing the same game state twice, and also for figuring out in which order to fill in the array of game outcomes. Of course, that simple approach only works because I can afford to brute force the entire state space. Also, if there is a strategy which would allow the players to delay the conclusion of the game forever, there will be a dependency loop and GHC's runtime system will tell you about it. That allowed me to fix a flaw in the ruleset for Tic-tac-top 2, in which you're allowed to play on top of the center cell... unless your opponent just did, otherwise you could fill up the bottom row and then keep playing over each other's center move. &gt; There's also a version 2 which you can play when you've beat the first version, but I have to run! There is also a version 3 after that, whose rules are as follows: 1. As usual, three in a row horizontally means you lose. 2. You cannot play on a row above the row in which your opponent just played, nor on a column to the left of the column in which your opponent just played, unless there are no other available cells. Each time you lose, you revert to version 1. This is a bit annoying, but ensures that you only win if you have a winning strategy, as opposed to playing each version randomly until you manage to get to the next level.
Yes, "wraps it and builds upon it" is fine. And yes, you can see `Free f` as just some data structure whose internal nodes are `f`-shaped and which happens to have a sensible `Monad` instance. Or you can see `f` as describing a command-response interface and ask "what are the sequential programs which interact according to `f`?". The fact that the programs have sequential structure doesn't mean that they have to be run sequentially or even exactly once. What makes the monad "free" is that you can give its programs a monadic semantics which respects `return` and `&gt;&gt;=` exactly by any method of interpreting each command consistently. That is, you pick some target monad `m` and define a polymorphic function. doOne :: f x -&gt; m x Any `doOne` will do: there are no laws relating the behaviour of different commands which must be obeyed. That's the freedom which puts the "free" in "free monad". We may then construct doFree :: Monad m =&gt; Free f x -&gt; m x doFree (Pure x) = return x doFree (Free ffx) = do -- ffx :: f (Free f x) fx &lt;- doOne ffx -- doOne ffx :: m (Free f x); fx :: Free f x doFree fx It's easy to check that `doFree` maps `Free f` action sequences to `m` action sequences, in the sense that doFree (a &gt;&gt;= k) = doFree a &gt;&gt;= (doFree . k) Moreover, every such interpretation (aka "monad homomorphism"), taking `Pure` to `return` and preserving `&gt;&gt;=`, is generated by such a `doOne`. If you give me myDoFree :: Free f x -&gt; m x I'll construct yourDoOne :: f x -&gt; m x yourDoOne fx = myDoFree (Free (fmap Pure fx)) See? If `myDoFree` interprets any action sequence, I can run it on action sequences which consist of a single action. The requirement to respect `return` and `&gt;&gt;=` will allow us to scale to zero actions or more than one, and we'll find that doFree yourDoOne = myDoFree That's to say, free monads capture "sequentiality of action and nothing but". Of course, there are sensible ways to interpret strategies in a free monad, such as the backtracking interpretations of /u/sacundim's `Example`, which do not preserve `&gt;&gt;=`. Consider a = Free (Branch (Pure False) (Pure True)) k True = Pure () k False = Free Leaf Then leftBiased (a &gt;&gt;= k) = Just () leftBiased a &gt;&gt;= (leftBiased . k) = Nothing That is, the `leftBiased` interpretation is unsequential in exactly the right way (or, here, the left way) to handle failure. The `IO` monad is another example of a monad whose actions are sequential, but whose execution is usefully more curious.
I agree with all this; but it just makes me think about the fact that *this* blog entry is a vindication of the conservative IT manager's reservations about Haskell. Space leak due to unevaluated thunk is the null pointer of Haskell code defects - the classic problem people bring up when talking about the robustness of the language itself. The fact that it is much more rare than null pointer/reference in C++ or Java is irrelevant because we are talking anecodotes here. The other reservation is that the low adoption numbers mean that early adopters like Facebook have to find these bugs themselves in production. If something like this bug existed in Jackson it would have been found years ago and the cost of finding these bugs is amortized over so many different IT shops that you don't really have to worry about them. Facebook can probably expect to find more bugs in widely used Haskell libraries (I expect they already have).
I'd be keen to get a list of what you see as the issues too. Paths on different OS's are quite different, but I find the filepath veneer usually does mostly the right thing.
I agree. It's the monad *interface* that's sequential in character, offering the opportunity of value-dependency at every point where we `&gt;&gt;=` actions together. Indeed, the whole "applicative do" business is happening because the monad interface obscures possibilities to avoid sequentiality. Free monads give you nothing but the sequential interface. Other monads can compute in all sorts of hairy ways, but they offer the sequential interface. Anything with the two operations that satisfy the three laws is a monad. But it just so happens that those two operations and three laws define the very essence of what it means to *be* a sequential program. And we know that sequential programs can be run in nonsequential ways, constrained only by actual value-dependency. To implement a monad instance is to show that sequential programs can be run, not that programs will be run sequentially.
&gt; The implementation in GHC is simplistic at best and in many cases just wrong (e.g., on Unix when the user's current locale isn't the same as the one where the file path was created, and Unicode normalization then trashes the file path). I'd really appreciate specifics here. I'm not quite sure what issue you're referring to, besides the (truly annoying) fact that POSIX does not define a character encoding at all. But that's not a library issue, that's a system issue that we're stuck with. My blog post [links to John's announcement](https://plus.google.com/+JohnMillikin/posts/j7NCSdRHGvN); I'll refrain from paraphrasing to avoid misattributing statements to him. &gt; In any case, this sudden breaking change in many libraries is causing us a lot of pain. If you are really committed to this change, is it too late to move to a smoother deprecation path? Let me guess: you're not using LTS Haskell, correct? I don't know what the smoother deprecation path would be, did you have something specific in mind?
I just drew on level 3, what gives
I chose the language of pure computation because most Monad instances are pure, so effects should be executed by evaluating pure values. Is that incorrect or misleading? IO is of course a different story.
We're currently building up our Haskell competence. 3 years ago we moved all of our application development from PHP to Ruby. Haskell is our strategic next step language, from a technological perspective (as the markets we serve are not (yet) demanding for Haskell). Besides LambdaCms (which we do not yet use commercially), there is currently one internal project that uses Haskell: an ops tool for harvesting versions of dependencies. We plan to use Haskell over the coming year for web applications, but first we need to get several devs confident in Haskell to guarantee continuity. Learning Haskell when coming from imperativistan takes some time. 
Well, personally I don't see a complete visual representations as a very useful thing but that might be because I tend to think more textually in general. Some objective problems with the approach in general are * two representations are harder to memorize than one * it is hard to search for the meaning of visual representations (no visual hoogle) * it is hard to vocalize (talk about) specific bits of a visual representation you don't understand * it is hard to edit visual representations (at all and in particular efficiently) without specialized tools * we have no real mass change tools a la regex for visual represenations (meaning any refactorings but the simple extraction into a function you mean in your comment are harder) * a picture might be worth a thousand words but something in the order of 999 of them are not the words you intended to visualize These are just some of the reasons why I think visual programming probably failed over and over again when people tried it and even GUI designers tend to annotate their icons with words by now, unlike the predictions of the initial enthusiasts in that approach. Of course there are certain visual representations that can give you an overview of specific parts of a program (e.g. its module dependency graph or an entity relationship diagram,...) but abstraction, the main feature of a complete program (with all the details i mean), seems a better match for textual representation.
If you really, really want to generate list of primes in Haskell, then the best choice would be to use [arithmoi](https://hackage.haskell.org/package/arithmoi). It's immensely faster than any other Haskell prime sieve I've come across. 
The PHP community is large but almost every single library they produce is utter crap because the authors don't understand the concepts they are building on (or just don't bother). Database layers without transactions, ORMs that produces thousands of queries for the work done in a single web request,... It is not unusual to see PHP frameworks require 256MB, 512MB or even 1024MB of RAM per worker and a dozen seconds or more per request which makes using it without a Varnish cache or something similar impossible. Meanwhile there are almost no tools to properly profile it or inspect running processes considering the size of the community.
I keep getting puzzled by: &gt; Cannot add module Module to context: not interpreted
I would love to move away from the big blob of shared global state called the "filesystem".
We fix versions for releases, of course, so you haven't broken released versions that we need to support. On our dev branch we try to avoid upper bounds as much as possible to make sure we keep up. With cabal's new features, that usually doesn't cause too much trouble. Yes, thank you for soldiering on with PVP, we notice it and we *really* appreciate that. :) And yes, we are now placing upper bounds on all of the conduit/yesod packages affected this change, in all of our applications. Later we will need to adapt our code and remove the bounds. It's going to be a huge effort - the code changes are not always trivial, and there are a boatload of them. If there were a deprecation process like the one described above, we would have more time to implement all of those changes without falling way behind and missing bug fixes for the many other packages that also depend on the ones you changed.
&gt; When the false algorithm finds 7, it checks every number from 8 to 100, that‚Äôs 92 operations! No it doesn't. It starts from 11, and checks far fewer than 89 numbers, too (28, in fact). And you compare the number of "crossings-off" with the number of "checks" - that's apples and oranges. The *"true"* problem with that code, primes = sieve [2..] where sieve (p:xs) = p : sieve [x | x &lt;- xs, rem x p &gt; 0] is that it should have been optimized into primesQ = 2 : sieve [3..] 4 primesQ where sieve (x:xs) q (p:t) | x &lt; q = x : sieve xs q (p:t) | otherwise = sieve [x | x &lt;- xs, rem x p &gt; 0] (head t^2) t To compare with your timings for finding the 10000th prime, the first runs for 3.2 secs on my computer (compiled with -O2), and the second for 0.06 secs. So scaling to your 7.9 secs for the first code, it's 0.15 secs for the second, compared to your 0.25 secs.
&gt; This sounds like a very tricky problem Well, I may miss something important, but `ghc` can have an option like "-no-additional-modules" that prevents it from compiling modules that are not specified in command line. IIRC `cabal` always list all modules in command line, so such options will be enough to identify OP's issue immediately.
In my perfect world, we'd be able to get to a state where `return` was a top level definition and could have its type improved before we started work on `ApplicativeDo`, but I don't think Facebook wants to stop work on their use of Haxl for 2-3 years while we play catch up. Given that it is a language extension you have to turn on and explicitly opt into I think it can have an awful lot of lattitude in what it does, though.
Last year I sadly had to choose between doing this internship and doing an honours programme. and I chose the latter. But have to say Cies is a really motivated and inspiring guy from the few times I met him to discuss the project! Nice to see you guys still committed to this :)
Actually it very much depends on the modality. Some are monads, some are comonads, sure, some are other things entirely. e.g. Necessarily is a comonad. http://en.wikipedia.org/wiki/L%C3%B6b%27s_theorem uses a comonadic modality for instance. (!) in linear logic is comonadic but `Possibly` is a [monad](http://software.imdea.org/~aleks/papers/effects/possibility.pdf) as is (?) in linear logic. When you start playing with other logics though their modalities may be missing part of what you need to be a monad or comonad, some can be Applicative, others have their own exotic structures. It all depends on the axiom set you allow that modality.
But sets form a monoid under union.
Taken! primes = [n | n&lt;-[2..], not $ elem n [j*k | j&lt;-[2..n-1], k&lt;-[2..n-1]]] though I can't claim the authorship for that one. :) Empirically, it's more or less *exponential*. There's of course Wilson's theorem, spending exactly *one* test for each candidate number. primes = [n | n&lt;-[2..], product [1..n-1] `rem` n == n-1]
Ultimately getting there requires a migration path. e.g. Creating the class itself in one version, warning about missing instances of it, then cutting combinators over in a subsequent version. Due to the need for co-existence in the meantime the names of the combinators probably wind up having to be subtly different than the current ones in `Storable` as well, so if we decide we're going forward with it let the bikeshedding commence.
Not really. It was a lost war from the beginning. Good for him to finally accept it and stop wasting his precious time and effort. Eclipse is too complex and deters most people from joining in development and helping with the haskell support. Hopefully a much more approachable editors like atom or maybe even Microsoft Code with a lower entry barrier would see a more active participation. I mean look at haskell-mode for emacs. It has a whopping 88 contributors, despite being a mode for such bizzare (for most folks) editor like emacs. All because starting hacking emacs is much easier than getting into Eclipse plugin development. 
Hrm, I thought I had set CloudFlare to have more informative error pages already? I'll double check. What would probably be better instead would be to make sure nginx can somehow bail and show a maintenance page if the Hackage upstream isn't responding (since it has an immediate timeout anyway). We can look into this.
Sure, if you put a order in a set, you get a category that is a set and a sequence. But a set alone is not ordered and therefore no sequence.
If we did BPP, we can do X ;)
&gt; I would say that a monad is by definition a sequence because it is a monoid. It sounds like you are claiming being a monoid implies something is a sequence. Is that not what you meant? Most monoids aren't totally ordered, so they don't seem to have anything to do with sequencing. If being a sequence just means having an order, that seems tautological.
Now I wish I'd taken a screenshot to confirm I'm not crazy, at least to myself ;) In this case, I'm surprised to hear that changing an nginx config would have helped. I got the impression from Gershom's comment above that this was maintenance of the entire machine, not just the hackage-server process.
actually, Haskell is too small of a community to support the current fragmentation of tools. Instead of developing separate tools for dealing with cabal and builds (build wrapper) etc, it would be better to join forces around something that has more momentum, like hsdev, ghc-mod, or the ide-backend. Perhaps these tools could be enriched with some parts from the Eclipse-FP work? Also, maybe the Java integration parts could be separated out to be more reusable outside of Eclipse context? Eclipse is getting a bad rep. recently. Maybe it would be better to invest more into IntelliJ IDEA's Haskell support? EDIT: just yesterday, I had a conversation with a colleague: he likes Haskell the language, but he's not using it because he hates the quality of the tooling...
I used to love Eclipse years ago and I have even developed plugins for it. However, it has become way to unstable - null pointer exceptions coming out of nowhere, incompatibilities across versions and even platforms, etc. It has become a very heavy weight platform. Also, OSGi, which showed much promise, has been somewhat abused by Eclipse. I tried EclipseFP but gave up. I tried Emacs and I just cannot stomach it after working with Sublime Text. Personally, I'd love to see more effort put into Sublime or IntelliJ IDEA, which are more modern platforms. However, Sublime also has it's share of problems...
Something like [filestore](http://hackage.haskell.org/package/filestore)?
I found it to be fairly straightforward to port stackage-sandbox and stackage-setup to cope with this change. (They are, admittedly, small programs.) https://github.com/fpco/stackage-sandbox/commit/70010b76a308bae22f9c9f51ac4c005066e615d8 https://github.com/fpco/stackage-setup/commit/1bcb6d5ade26c02db0ff1c301ec7b44f82467b06 
If "must never have strange performance characteristics" is the conservative IT manager's requirement, then yes, Haskell is not for them, and we didn't need this blog post to know it. Such a person will have precious few languages to choose from though. Other teams want to be a bit more adventurous to get leverage in other areas.
But unlike assembly language, we still use it and manipulate it manually in our day-to-day interactions with the computer.
No, what I'm imagining is not really like that. Firstly my idea has nothing to do with versioning directly (although you could implement versioning on top of it). Secondly it eschews the notion of flat files. Everything would be a disk-backed algebraic data type, essentially. You can't get "File not found" errors and you can't make a syntax error in a "file" because everything would be checked by the type checker. Plus it would have garbage collection.
Wrapping/unwrapping and continuing to use system-file* - yes that's an option if someone takes over ownership of the packages. Which may end up being us, but it doesn't sound like these are high-maintenance packages. However, it's probably not the best strategy if we are the only ones in the world using it.
We have at least four or five fairly large applications that use it. In some cases, it is built in as component of a much larger ADT, with hard-wired assumptions about its interface.
A few weeks ago, I did exactly that - for my internal compiler's IR types, I just added JSON serialization. It's all autogenerated, no need to maintain XML schema or anything, and the JSON has all the data. Just import Data.Aeson import Data.Aeson.TH and then use $(deriveToJSON defaultOptions{fieldLabelModifier = ....., omitNothingFields=True} ''.......) so simple. Then what's needed is to add plumbing to provide that JSON to the outside, which is quite simple too.
&gt; To make it an interesting challenge for humans, I wanted to create a group 1 variant There are two generic ways of transforming a group 3 game into group 1 or 2. The first is to transform a draw in either a win or lose (like the komi in the game of Go). For tic-tac-toe, as player as an advantage, a draw result in a win for player two. The other way (like in Renku), is based on the "pie algorithm", and the idea is, player one play his first move, and player two can chose either to swap side or carry one. This forces the first player to not play the best move neither the worst one . However, your variants are problably more interseting.
I certainly agree with everything you say about why the algorithms in ad are very nice. And they're certainly better than my implementation (which is why I wouldn't recommend anyone use my own right now). The only thing I have an issue with is: &gt; Now it is true that we could do somewhat faster automatic differentiation if I supported vector-valued AD-variables[1] . I think "somewhat" faster is a gross understatement. I would expect to see a 10-100x speedup when vector/matrix valued variables. For small problems maybe it doesn't make too much difference, but if your jacobian has a billion entries, I can't even imagine how long that would take to calculate with in the list representation.
What if there was some kind of ad hoc sum type? (like the dual of records) Would that make things less tedious?
Your paste makes 9 look prime
We've had the issue open for 5-6 years to switch to a vectored internal representation. It isn't without its own costs though. Right now you can do AD on anything. This lets you use AD to compute symbolic results, etc. When you lock things down to a single value type to get it packed into tight arrays you lose a lot of that flexibility. It should probably happen, and implementing Double-restricted forward mode shows we can now in theory do it, but nobody has put in the elbow grease.
As an ide plugin developer who wants to support a range of GHC versions and not have to go through the horrible process of building against the user's GHC on their machine (or expect them to compile another program that is probably out of date with a new GHC release), that isn't really an option. 
Based on the [earlier](http://www.reddit.com/r/haskell/comments/2yxgaj/atom_text_editor/) and [recent](http://www.reddit.com/r/haskell/comments/35u8gh/using_atom_editor_for_haskell_dev/) discussions, it seems that Atom still has some problems: lag and file size limits, etc. But since it's backed up by GitHub guys, it has potential to improve quickly.
Note that it was not an unevaluated thunk. It was folding an associative operation with a `foldl'` that really wants to be used with a `foldr`. The Builder monoid's `&lt;&gt;` operation needs to be folded to the right, not the left.
Ideally, the function would be written to use the Builder's &lt;&gt; operation in a right associative way, rather than in an accumulating-paramater / foldl' style. If that's not possible, then Builder cannot easily be used. For such cases we would need a different kind of builder than can be used in a left associative way (probably in ST). If we look at the [old code](https://github.com/bos/aeson/blob/deb59828ac24cc4feb859a711ca16b257078ffbb/Data/Aeson/Parser/Internal.hs#L212) we see that indeed it's using an accumulating param style. It's not easy to change that here because it's working in the parser monad, and to use a foldr style it'd need to be returning the Builder as the result, not the parser action.
For a simple state monad like IO, standard optimizations turn bind' a f = join (fmap f) a into exactly what you'd write directly for `&gt;&gt;=`.
I sympathize with this. However, if you think of ApplicativeDo as an optimisation rather than a desugaring, then it makes more sense. (except in the case where we can desugar to an Applicative-only expression, which I suppose argues for this being a separate construct altogether).
The problem is that currently you have no way to represent ffi data that is opaque. In other words, there is a practical problem of how to write bindings for apis that require you to allocate an opaque ( no peek/poke) chunk of memory yourself. So yeah, all you know about this data is the size it occupies in contiguous memory and perhaps alignment. But practically when wrapping foreign APIs this is a common case. There maybe is room for a more refined sub class of copyable data.
I'm interested in helping you change the rendering code to something more cross platform. You mentioned that just using plain OpenGL would be good. Perhaps there is some cross platform Haskell game/rendering framework? 
&gt; I can't take great credit for the idea; it's just copying the memory management scheme of a pure functional language. Yeah, but here's the rub. I *often* ask programs to deal with a file system that I've manually manipulated. I *rarely* ask functional languages to deal with a heap/stack that I've manually manipulated. Now, for an internal persistent store where you basically tell users: "I keep important files here, do not touch." Then, sure you idea works, but there's *tons* of use cases for the file system that isn't *that*.
The [primes package](https://hackage.haskell.org/package/primes-0.2.1.0/docs/Data-Numbers-Primes.html) even mentions this in the documentation for wheelSieve. I'd guess you probably want a wheel that is just a little smaller than your L1 cache for optimality.
Sure, my point was more that the class you've described gives you no operations you can perform except allocate an uninitialized slab of memory before you go and do something specific to the data type you defined anyways. My major concern is the marginality of the usecase in terms of the benefits of actually abstracting over it. What is the appreciable difference between that and you calling `mallocBytes` by hand, getting back a `Ptr YourOpaqueConfig` and _then_ doing your ad hoc one-off initialization process? makeFoo = do p &lt;- mallocBytes someNumberOfBytesSpecificToFoo someInitializationProcedureSpecificToFoo p return p vs. makeFoo = do p &lt;- malloc someInitializationProcedureSpecificToFoo p return p You gained only gained a bit of ad hoc overloading for one step of a longer ad hoc process where the second step was specific to your data type. Knowing the size here has given you no real abstraction. With `Storable` you can `peek`, `poke`, etc. and do something with that knowledge. With this you can't touch it anyways. Shoving the size/alignment into a class doesn't really let you actually do anything new in aggregate. It just let you reuse the ability call to one function rather than have the specific initialization procedure for whatever it is you are going to do call it. That said, the case could get a lot stronger if we had a penchant for doing things with `Storable` like building up storables for products using shared alignments, but we don't really try to do anything like that today.
That's cheating, you skipped the "the" to make it 5 words. &gt; Monoids in the endofunctors category There u go. 
There's also `newtype Free f a = Free (forall r. (a -&gt; r) -&gt; (f r -&gt; r) -&gt; r)`.
Wow, nice one. I still remember with horror a customer's project where they had massive TH-generated functions and data constructors with 1000s of args, and a huge heap that was tickling some RTS bug (to this day only partially tracked down and partly fixed). Not the kind of thing you run into normally but when you really push the scale up.
Ok, thanks. But why would you make a Map for the location and an attribute for the player? Isn't it basicly the same thing? Why not either make a Map for both or an attribut for both?
Note that ide-backend-client was designed by Edsko to be a backend for Atom, so that'd be a good direction to pursue.
The problem with a sum type would be that you could add to it easily, but you couldn't subtract from it again (when you added a handler ). If every exception-throwing function could add new constructors to an open sum, you'd end up with the `MyException`-type after a while, albeit automatically. The [control-monad-exception](https://hackage.haskell.org/package/control-monad-exception) package that /u/bss03 mentioned does the same thing in spirit, though, but with constraints that get added when you throw an exception and discharged when you handle it.
I dislike the OOP way of organizing things, I find it clearer to have a few variables that capture all entities instead of having a deep network of objects that group things in arbitrary ways (it becomes even worse in languages with manual memory management). (In the paragraph above I assumed that what you meant is having maps of CardEntities in Player, now that I re-read your comment, it doesn't seem to be what you're saying.) &gt; Why not either make a Map for both or an attribut for both? If you make both an attribute (which is kind of what I'm advocating), you still need to have a Map/Set/list/... of all CardEntites somewhere. If you make both a map, then how do you tell what player a particular CardEntity that you found through the Location -&gt; CardEntity map belongs to? But ultimately I don't think there's a single best way to do it, it all boils down to what makes the most sense for your project (and to your personal preferences).
For future questions: reddit.com/r/haskellquestions
So many arguments about type systems boil down to deeply differing conceptions of what types are for. I can stick up for my expectations of types without presuming you share them. We should welcome the idea that "types" are a notion in flux.
Sorry, can someone please explain how data Moore a b = Moore b (a -&gt; Moore a b) ~ data Moore a b = forall s. (s -&gt; b) -&gt; (a -&gt; s -&gt; s) -&gt; s -&gt; Moore a b ?
Thanks for explaining!
Agreed. Eclipse is a joke forced on people by corporate Fiat, not something anyone uses by choice. I also never got eclipse FP to work. But vim is doing me just fine.
 data Moore a b = Moore b (a -&gt; Moore a b) First, create some existential state `s` and functions to project each field from it. = forall s. Moore s (s -&gt; b) (s -&gt; a -&gt; Moore a b) `s` could just be `()`, or it could be a tuple containing every field, or it could be anything else from which you can generate the fields. Now, turn recursive occurrences of the type into `s`. Before, `s` was some value that you can stash stuff, but it didn't really matter what it was. = forall s. Moore s (s -&gt; b) (s -&gt; a -&gt; s) Now, `s` must be used for anything that distinguishes the recursive structure from that which wraps it. You use the same projection functions to get each field, like before, but now instead of getting entirely new `Moore`s in recursive position, you just get new `s`s.
This is an example of a more general isomorphism between the greatest fixed point of a functor `f`, which in Haskell you can take to be data Rec f = Rec (f (Rec f)) and exists s. (s, s -&gt; f s) To see the connection, take f a b s ~ (b, a -&gt; s) Then `Rec (f a b)` is just `Moore a b` and the existential is exists s. (s, s -&gt; (b, a -&gt; s)) which is equivalent to data Moore a b where Moore :: (s -&gt; b) -&gt; (a -&gt; s -&gt; s) -&gt; s -&gt; Moore a b as claimed.
http://hackage.haskell.org/package/universe-1.0/docs/Data-Universe-Instances-Reverse.html has such an instance.
Okay, so we have people like Russell and Church introducing this notion of a *type* in a formal system, and we have languages like Fortran that took the engineering approach and assigned each variable a *type* according to the set of values it could take on. And nowadays we see these two ideas as applications of a single overarching concept.[1] BUT I wonder: were the two uses of the word "type" related initially? It seems likely that the two ways to use "type" originated independently. So, is it possible that the usage by both logicians and engineers of the same word for what turned out to be expressions of a common idea, came about by sheer coincidence? ---- [1] Yes, Petricek's thesis is, in part, that *type* is not something we define precisely. I admit I'm getting away from the point of the post here.
Thanks.
So in a metaphorical "what do we use this for" sense or in a "how do I think of this" sense, there are certainly different notions of types. What I am not convinced by is the idea that in a _purely formal_ sense there is not a family of rules to which we can associate the idea of a "type," which spans these systems. We certainly have A) the notion that a type must have a referent -- a term or a family of terms to which it can be related, and B) an inductive/algebraic quality whereby operations on terms induce operations on types (and vice versa -- operations on types inducing operations on families of terms). I'm not certain, in fact, if the article isn't tilting at a strawman, at least from the standpoint of CS research. In the sense that papers tend to be good at fixing a particular definition of what they mean by "type" or even "term" etc in a specific context. Even in popular discussion I think that an argument that "X doesn't have 'real' types" tends to get dismissed pretty quickly by more reasonable people in conversation.
Behold, my comprehensive list of "what's going on with ghci" questions: http://stackoverflow.com/questions/29170362/no-instance-for-eq-a-arising-from-a-use-of http://stackoverflow.com/questions/27845940/deriving-show-instance-works-with-ghci-but-now-with-ghc http://stackoverflow.com/questions/26772007/haskell-function-composition-resulted-in-type-mismatch-error http://stackoverflow.com/questions/7055146/why-do-3-and-x-which-was-assigned-3-have-different-inferred-types-in-haskell http://stackoverflow.com/questions/11439163/inconsistent-behavior-with-fromintegral-in-ghci http://stackoverflow.com/questions/9714697/type-defaulting-in-ghci http://stackoverflow.com/questions/8434808/how-to-build-matrix-of-zeros-using-hmatrix http://stackoverflow.com/questions/7799345/works-in-ghci-but-not-in-the-file http://stackoverflow.com/questions/8262020/why-cant-i-add-integer-to-double-in-haskell http://stackoverflow.com/questions/8655900/when-can-i-bind-a-function-to-another-name http://stackoverflow.com/questions/11003535/what-is-going-on-with-the-types-in-this-ghci-session http://stackoverflow.com/questions/15451501/why-does-signature-change-after-an-assignment http://stackoverflow.com/questions/18661866/foldr-vs-foldr1-usage-in-haskell http://stackoverflow.com/questions/19926992/haskell-ghci-why-type-of-1-is-num http://stackoverflow.com/questions/22942946/haskell-partial-application-doesnt-seem-to-work-with-on-why http://stackoverflow.com/questions/23942957/type-of-a-double http://stackoverflow.com/questions/24476975/monomorphism-restriction-ghci-and-let-expression http://stackoverflow.com/questions/28763101/haskell-strange-type-inference-narrowing http://stackoverflow.com/questions/28336108/why-is-22-0-double-in-a-hs-file-but-fractional-a-a-in-ghci http://stackoverflow.com/questions/30105691/why-is-it-that-sometimes-defining-a-value-in-haskell-changes-its-type 
It appears it's possible to have multiple remote repos, not sure if it'll fallback to the other one in case of an error or if it's just for hosting private repos. https://github.com/haskell/cabal/issues/1605 edit: tested by listing fpcomplete as the first repo, then blocking fpcomplete in my hosts file. Cabal install still worked, but it cabal exits with an error on cabal update. 
This overlooks the cost of keeping the priority queue. If you know beforehand how many primes you want and use an array you can eliminate the multiples of each prime as you go.
Same here, but when given the choice I would actually use Netbeans instead. Currently using IntelliJ in its Android Studio variant, and it is a bit of resource hog on my system. Not sure if that are side effects of Google's work though. 
How does a finite domain help you to decide if your functions diverge? It looks to me like you would still need to solve the halting problem which is not possible in a non-total language.
Divergence is endemic anyway, even when deciding equality for Bool, let alone functions. It's usually dealt with by thinking about it enough to not do it, then not saying anything about it, then bringing it up when somebody else doesn't.
I used eclipseFP for some time and really liked it. Thank you Jp for this nice tool. I eventually moved to emacs because it is lighter and I already use it for python &amp; org, but that is not something I would recommend to a complete beginner.
Amusingly, of course, there are many `t0`s for which it is unreasonable to expect `t0 -&gt; [Char]` to have a decidable equality, but the type of `f` and `g` is forall t0. t0 -&gt; [Char] and that type surely does have a decidable equality. Sadly instance Eq b =&gt; Eq (forall a. a -&gt; b) where f == g = f () == g () yields a "Simon says" Illegal polymorphic or qualified type: forall a. a -&gt; b In the instance declaration for ‚ÄòEq (forall a. a -&gt; b)‚Äô We may, however, do this newtype Koo b = Koo (forall a. a -&gt; b) instance Eq b =&gt; Eq (Koo b) where Koo f == Koo g = f () == g () and now &gt; Koo f == Koo g True
That means, the order isn't significant. Still there is an order dictated through how its applied to the operation, even when it doesn't make any difference. And no. Nothing gets ordered. It has a order in the first place. 
To expand on the top answer, you can do this for finite domains in Haskell, as long as the domain is an instance of Enum and the results are an instance of Eq. instance (Enum a, Eq b) =&gt; Eq (a -&gt; b) where f == g = let x = toEnum 0 in fmap f (enumFrom x) == fmap g (enumFrom x) But it's rather pointless to be honest and nothing but a brute force check. You also still have to ensure that the domain is actually finite "for practical purposes" (i.e. `Integer` is out). If you want to check function equality in the general case, equational reasoning is your best bet. Unfortunately, we still don't have a better tool for that than pen and paper. Maybe I get around to hack something together one day. Maybe not.
The big picture is that the craftsman comes first, and then comes the scientists. If the Wright brothers would have waited for the development of aerodynamics to construct their plane, we would have neither of both. If Watt would wait for the development of thermodynamics we would never have neither engines neither thermodynamics. That happens also with programming: programming is not computer science, in the same way that ferrari motor sport designers are not physicists. They are more like craftsmen. https://www.cs.utexas.edu/users/EWD/transcriptions/EWD04xx/EWD480.html 
&gt; big picture is that the craftsman comes first, and then comes the scientists. But isn't the story of Computer Science itself a beautiful counterexample? Church's [Simply typed lambda calculus](https://en.wikipedia.org/wiki/Simply_typed_lambda_calculus) and Gentzen's [Natural deduction](https://en.wikipedia.org/wiki/Natural_deduction) came long before any craftsman's practical programming language (let alone the work of Frege and Russel). [History of type theory](https://en.wikipedia.org/wiki/History_of_type_theory) 
Right, "the rest of the world" expects to interact via flat, unstructured, globally-shared mutable files. Until and unless enough infrastructure has been built around my new idea it will be of limited usefulness. However, it's exactly the same story as Haskell went through. Haskell was of limited usefulness until people caught onto the idea of pure, typesafe programming and built APIs to take advantage of it. With regard to your particular example, I'm imagining that instead of using vim to edit unstructured files that encode a structured datatype, one augments vim (or one's favourite editor) to edit guaranteed-structurally-valid-by-construction datatypes. 
The split between logical tradition (Russell et al.) and engineering tradition (Fortran etc.) is only a side note in my essay - and I think most of the essay focuses on the logical tradition (because that's my background!) Stephen Kell has a nice essay that goes more into the details about the engineering vs. logical split - not sure if it answers whether the two ways originated independently or not (I'd be curious to know!), but it is worth reading: http://www.cl.cam.ac.uk/~srk31/research/papers/kell14in-author-version.pdf [PDF]
That's interesting. Could you say more about *why* you want to manipulate the filesystem directly? What benefits does that provide over a hypothetical typed API? There must be something I'm missing. 
You can also do this without the `show` cheat: numDigits = (+1) . floor . logBase 10 . abs
To add to this, there are 3 isomorphic representations in Haskell of (simple) recursive data types data Rec f = Rec (f (Rec f)) data Mu f = Mu (forall a . (f a -&gt; a) -&gt; a) data Nu f = forall s. Nu (s -&gt; f s) s If you ever detect that you've got a type in one of the representations you can often learn a little bit more about the structure by converting it to another one of them. (It's worth stating that these are not normally isomorphic and certainly aren't in a total language. Haskell's laziness is what unifies them.)
You do give up most of unix that way though. That's a big pill to swallow. Too big for me, I'd say.
&gt; "Simon says" Yup, and that's what I'll be mentally calling all the error messages from GHC now. 
On the contrary, I would say you would *augment* the unix way with types and make it what it really should have been all along!
I did some digging inspired by your essay and couldn't get to a definitive answer. It seems by the origins of fortran people were using the idea of a "type" (classifying of course one or another assignment of numerical meaning to a byte of data, basically) without problem, so the usage must trace back to assembler. But there's even less I could find resembling documentation on the history of concepts in early assembler. A key moment in this lineage that your paper omits (perhaps for space reasons) is "Types are not Sets" (Morris, 73). I also think its important to note that the logical lineage, sometime in the 40s I suspect, got accustomed to thinking of types in set-theoretic semantics anyway (or perhaps it was in the 50s, but certainly it was well established by Howard). So I don't think the right opposition is set-theoretic/not historically, but perhaps types as "use-categories" (i.e. opcodes operate on bytes as though they are of a type) versus types as "belonging-categories". Also, I would be curious to know why Algol chose to speak of "modes" rather than "types" -- perhaps to highlight that precise distinction?
Of course, there's familiarity. That's certainly part of it. To do a real comparison, I'd really need more details about the hypothetical typed API. I'll often `grep` or `strings` files that have more sophisticated structures than just characters or bytes. I'll routinely delay organization of files until it's "needed", either the output of ls is too long, the display in Dolphin is has too many pages, or an organization is more obvious from the current (larger) sample instead of the initial samples. There are things that could be improved about the file system, certainly. I think arbitrary byte strings for names of files and directories is rather silly for something primarily meant to be both produced and consumed by humans; I'd rather see some sort of Unicode normalization occur even if that doesn't solve all the problems. I'm almost convinced that some variety of tagging is actually better than a fixed hierarchy, and the number of symlinks between /etc, /var, and /usr installed by Debian packages like postgresql and drupal7 serve as witnesses that it may be true even for "system files" that few users manipulate and even administrators rarely reorganize. I would like more "awareness" of file contents, but I'd also be highly resistant to a system that didn't let me `sed` my XML or Haskell source code. For example, `awk` would be a little more useful if it understood CSV and TSV and adjusted how it turns lines into fields automatically by default (with, of course, with an option to turn that behavior off) and most of the time I'd rather `grep` was able to DWIM when I'm recursively looking through a mixed bag of .doc, .pdf, .odt, and named-.doc-but-is-really-plain-text files. But, this doesn't really have to be a property of the file system, I'd be comfortable with either making these tools "smarter" or just learning different, "smarter" tools. (I do like the simplicity of UNIX tools in some cases -- sometimes I really do want to treat a structured file as "just a bunch of bytes" and send it through a UNIX tool, but that's actually the exception rather than the rule.)
Fair enough and thanks for going into a lot of detail! I guess the onus is on me now to develop something you'll like better :) It's going to be a long project though ...
Is it really that hard to use pure instead of return? These aren't 12-year-olds learning Haskell for the first time: they're Facebook engineers who program in Haskell professionally.
Think in terms of constraints: For example, if a card can't be owned by two players at the same time, a dictionary from players to a list of cards won't correctly model this, as you may end up with the same card in the list of cards of multiple players. So, in this case a link from the card to it's owner seems better. (You may create a helper function to go from the player to its list of cards.) About the location of the card, the same "constraint logic" may be applied: is the location a resource to be allocated for the card or not? I mean, can the card occupy two spots at the same time? Or, can a spot be occupied by multiple cards at the same time? A "yes, no" answer will model it as a dictionary from location to card. A "no, yes" answer will model the it as a dictionary from card to location. A "no, no" answer may use a bimap [1]. A "yes, yes" answer may end up in a matrix between location and cards. This may not be the most efficient solution, but if you can put the same constraints of your model into your code, it will help you avoid future bugs. 1 - http://hackage.haskell.org/package/bimap-0.2.4/docs/Data-Bimap.html
Sadly this won't work with Float thanks to it's broken Enum instance (it adds 1 each step rather than give you the next highest representable float). 
Would it be possible to instead have a function with type (a-&gt;b) -&gt; (a-&gt;b) -&gt; Maybe Bool which returns Just True when the parse tree for the two functions is equal (and therefore the two functions are equal?), Just False if it can be demonstrated the two functions are not equal and Nothing if this is undecidable.
Haskell will most likely not have a tower of universes. See the [Dependent Haskell ghc page](https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell), section "Merging types and kinds" for Richards' reasoning. edit: And you are of course right in general.
Kind of. You could restrict the function to only work on your own reified functions. For example, you can make things so that `1 + 3 * 2 :: Expr` evaluates to `Add (Lit 1) (Mul (Lit 3) (Lit 2))`. For that you can write an interpreter, and structural equality can be inferred by GHC. You can take this further to also work with different types and with variables and whatnot. To find a more general equality, or to prove inequality you'll need to restrict your reifiable expressions in a way so that they can be normalized in some way. That's a whole lot more work. 
It's a really interesting question, because that's the kind of problem where OOP is really good at and FP a bit awkward. The OOP approach is simple, everything is mutable, therefore a player can have a list of card entities, which can have a back reference to the player etc ... It's kind of straight forward and represents somehow the real world, but ... and there is a big but, this relies on mutable states and you are programming in Haskell because you believe that mutable states are evil and we could do better, the problem is how. The approach using ids sucks. It solves indeed the problem of mutability because for example Person won't refer directly to a CardEntity but its id, which allows you to modify the CardEntity. The problem of this approach, is your ids becomes pointers : You need to keep somewhere a list of Players (which is equivalent to your heap) and you have no guaranties that your ids refers to a player which exists. I can have player #1, #2 in my list of players, but nothing stops me to have a CardEntity belonging to player #3. Basically you are introducing a class of errors which are equivalent to null pointer, and segmentation fault in C/C++ and I think its a (BIG) step backward. Now, you can see there are static data and dynamic ones. Your players for example, don't change name. It's the same with the cards, they are static. On the other hand, where cards are, their orientations etc (your CardEntity )changes during the game, that the states of your games. So, the easiest, would be probably to keep your static data Person, and Card as they and put everything which is dynamic in a game state, which is basically a list of CardEntity + the state of the players (hit point etc ...) It's natural to have CardEntity reference to a Person, you can use `IxSet` to reverse the relation and find all the cards belonging to a player. However, a Map Person -&gt; CardEntity should work. 
Not released until July? I want to read it now! Btw will this also be available as an ebook? 
I know! This would be such a big boon in my Algorithms class right now.
For some totally weird reason, `Setup.hs` is compiled and linked against unix-2.6.0.1 and unix-2.7.1.0 at the same time... sounds like a Cabal bug
If I'm understanding, then all binary functions have an ordering, just because they have two arguments? That seems like a *very* loose definition of order. It doesn't seem like it would be that useful because everything would be "sequencing", by that definition. The application of the function might have a *syntactic* order, but the end result could end up having no observable order. It could even end up having an order completely unrelated to the syntactic order of the arguments (merging sorted lists together, for instance). This is the reason I disagree: there are many cases where you can't recover or observe the syntactic order of the arguments in any way, making it irrelevant in my opinion. Is the set union operation sequencing due to the fact that it's a binary operation (sets in the math sense, not the Haskell sense)?
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Query string**](https://en.wikipedia.org/wiki/Query%20string): [](#sfw) --- &gt; &gt;In the [World Wide Web](https://en.wikipedia.org/wiki/World_Wide_Web), a __query string__ is the part of a [uniform resource locator](https://en.wikipedia.org/wiki/Uniform_resource_locator) (URL) containing data that does not fit conveniently into a hierarchical path structure. The query string commonly includes fields added to a base URI by a [Web browser](https://en.wikipedia.org/wiki/Web_browser) or other client application, for example as part of an HTML form. &gt;A web server can handle a [Hypertext Transfer Protocol](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) request either by reading a file from its [file system](https://en.wikipedia.org/wiki/File_system) based on the [URL](https://en.wikipedia.org/wiki/Uniform_Resource_Locator) path or by handling the request using logic that is specific to the type of resource. In the case that special logic is invoked the query string will be available to that logic for use in its processing, along with the path component of the URL. &gt; --- ^Interesting: [^Entrez](https://en.wikipedia.org/wiki/Entrez) ^| [^Uniform ^resource ^locator](https://en.wikipedia.org/wiki/Uniform_resource_locator) ^| [^Question ^mark](https://en.wikipedia.org/wiki/Question_mark) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+crabaw1) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+crabaw1)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Or a GHC bug. We don't do dependency resolution for setup scripts (yet - it's [work in progress](https://github.com/haskell/cabal/pull/2515)), but just call `ghc --make Setup.hs`.
Why not a `Map Location [CardEntity]` or similar to represent multiple card entities per location?
There are many problems with the author's blog post. The following is a list of some of said problems: * When we talk about programming languages, it's rarely scientific in nature. How does one talk about Haskell or any other computer language scientifically? * Science is not about objective truth, it's about refining or rejecting "truth" based on empirical evidence. * Philosophy of science and how science is practiced are two very different things. Many scientists are completely unaware of philosophy of science but it doesn't in any way stop them from doing science. * As for the "meaning of types," Wittgenstein's "meaning as use" may be more fruitful philosophical avenue to pursue: http://plato.stanford.edu/entries/wittgenstein/#Mea * "What is the meaning of types?" is too vague to answer. Could you concisely answer what is meaning of numbers? But, I betcha that you if you read a paper that involves a notion of types it'll generally be very precise, e.g.: http://www.cs.ru.nl/~herman/onderwijs/provingwithCA/paper-lncs.pdf, the meaning of types in a given context isn't really so vague. As well, as when a Fortran or Java programmers talk about a type. They can either point to code to get a "precise" meaning--precise in the sense they can convey concepts and work with it within code. Regarding your quote "Science is much more 'sloppy' and 'irrational' than its methodological image," isn't this obviously true? Obvious in what way, you may ask? Well, humans discover/create science but what keeps science honest is its methodology.
That reminds me of something that I saw Tekmo working on a while back around the beautiful folding technique where the consumer was expressed as a `Nu` and then some serialization functionality was slapped on to it. Suddenly beautiful folds are also persistable.
&gt; The book comes with a simple proof assistant Is this publicly available? Is it scheme/racket-based?
I took a look at the Kell paper. Yes, it is a nice read. I didn't catch any statement in the paper that directly addresses the question of a separate origin for the two uses of "type", but he certainly *implies* they originated independently. Most of the history is in section 10, starting at the bottom of page 12. He says that the first formal usages of "type" in an engineering context occurred in the specs of Algol 60 and Fortran IV (1962). And these seemed to coalesce out of less formal usages of words like "type" and "kind" in the late 1950s. ---- FWIW, here is my own definition of *type*, which I currently use in my Programming Languages class. It borrows heavily from Benjamin Pierce, and seems to work not too badly in each of the various contexts in which we might use the term. (It even allows for "dynamic type checking", which, of course, some consider anathema.) &gt; A **type system** is a way of classifying entities in a program (typically expressions, variables, etc.) by the kinds of values they represent, in order to prevent undesirable program states. The classification assigned to an entity its **type**. I understand the troubles with defining the word that you bring up in your post. But, alas, I have a problem with using a term when I can't tell people what it means. :-(
My understanding is that it has a mini implementation of a prover of the boyer-moore sort (http://en.wikipedia.org/wiki/Nqthm) -- so in the ACL2 linage, rather than the coq or agda MLTT-inspired lineage.
Indeed, Carl's dissertation was [Modular Proof Development in ACL2] (http://www.ccs.neu.edu/racket/pubs/dissertation-eastlund.pdf).
Howdy! Can you post a snippet of code that triggers this and I'll try to figure it out? Thanks!
My point was more that if you squint at the `Alternative` combinators they look a lot like giving a grammar for a context-free language, but that can be misleading. Even if you only have `char :: Char -&gt; Parser ()` you can construct a context sensitive parser using nothing but that and `instance Alternative Parser`. You don't even need backtracking! Nothing keeps you from building an infinitely large tree for your grammar. With an infinite number of states you can side-step the usual things that ensure that such a structure can only handle a context free grammar. An "infinite context free grammar" can be used to parse a context sensitive language, as long as you "try out all the different symbols" in a bounded amount of work, hence the need for a finite alphabet. We can relax that a bit further. It can even work with an infinite alphabet so long as you have only finitely many intersections of the FIRST sets of all of the different parsing branches that could be alive at any given point in time and a `satisfy :: (Token -&gt; Bool) -&gt; Parser Token`. This trick can be useful for working with tokenized data. e.g. you might have an infinite set of identifiers but in general we still treat that whole class of things in a similar manner and just need to track which ones we've seen where and how. That can all be done in the logic that is building up the lazy grammar tree you are exploring. In a strict languages you need more pieces to build up such a grammar. The alternative combinators wouldn't give you enough to construct more than a finite tree. You get no tools for introducing cycles even, so you don't even get enough tools to build a "real" context free grammar that way without adding something extra to give you loops, you need some tool to help you tie the knot, be it that you use mutation or introduce explicit looping combinators to your API. In a lazy language you can have an infinite number of states in your grammar, and you can use the ability to tie the knot lazily to model cycles, but you can also just have an infinite tree of states that aren't repetitions of what you've seen before. The bricks here aren't complicated -- you can just have infinitely many of them!
Yep! I watched Casey implement that and it pushed me over the edge to finally get it going for Haskell. My first language was [Pure Data](http://puredata.info) which has no compilation at all (you're always working with a live system) and I've never felt as productive as when I was using it ‚Äî so I've always been a bit baffled that every language doesn't work this way! Implementing the "live looped" part should be no problem at all; I'll see if I can find some time to extend the demo to show that off. Would love to hear what you figure out about FRP's state issue!
The main difference is that Halive is a command line utility ‚Äî it's meant to work with basically any Haskell program the same way ghci/ghc/runhaskell/cabal repl/run do, so you don't need to put too much effort into specifically coding for it. And, it leverages the various OS file notification APIs via [fsnotify](https://hackage.haskell.org/package/fsnotify) to automatically swap the new code in every time you save it. But you should totally check out the source if you're interested in a plugin system for your app, as it's quite tiny and a straightforward demonstration of using the GHC API to do hot swapping. 
Is there a reason against Yesod also using this codebase, and all living happy after :) I mean: why duplicate efforts, especially since Halive seems to have a feature-superset of Yesod's reload functionality.
Oh and "yes", I'm a father as well (also of a 4 year old).
I'm a Dad to two girls. I've just started teaching the oldest one (7 years old) how to program using http://code.org hour of code exercises. I think I'll also introducer her to https://www.touchdevelop.com/ as that platform looks like the progression from clicking blocks together to textual coding might be easier. At some point I would definitely like to get he doing more functional stlye programming. I think that Haskell or something like it would help her develop her reasoning skills more than the kind of poke it till it works style of development in scratch. I don't think she's quite ready for that level of abstraction yet though! There's a nice account of teaching Haskell to a ten year old here: https://superginbaby.wordpress.com/2015/04/08/teaching-haskell-to-a-10-year-old-day-1/
If I ever have children at some time, the day they get back from school and state ‚ÄúWe learn how to program in Java!‚Äù, I‚Äôll put them in the street. Nah, just kidding. I‚Äôm a bit afraid of that appeal people have ‚Äúlet‚Äôs try to make our ultra young beloved ones to write some code‚Äù. I don‚Äôt know. I feel like at such age, it‚Äôs more important to build some Lego. I might be wrong, but I don‚Äôt want to put my children in front of a computer while their little friends are having fun building a hut outside. Of course, if it turns out that my children, on their own, want to learn how a computer works and how to solve problems, I‚Äôll be the first one to teach them. I just feel everyone should have the choice to choose what is interesting to one‚Äôs eyes, not one‚Äôs father/mother‚Äôs eyes.
So many Haskell jobs these days. Good to see!
&gt; However, even the view of "types as sets of values" does not capture some of the more interesting type systems. For example, languages with effect systems (or Haskell monads) can track what a computation does ... A computation might then have a type int &amp; {write œÅ,read œÉ} I'm wondering: wouldn't it be better to separate effect management into a distinct type system? Obviously values and side effects are orthogonal, but does this really invalidate the "types as sets of values" view? Or does it only mean that managing values and effects in the same type system is not rational? ("types as **named** sets of values" would be more precise, by the way: `int64` and `double` have the exact same set of bit-strings as values, but they are definitely different types)
Although this sounds really awesome, I don't think I could apply before approx. end of this year. I'll definitely keep track of what you're doing though :) Now to why I'm really writing this comment: You should seriously check your webserver configuration. SSL3 still enabled, TLS &gt; 1.0 not available and RC4 cipher enabled is really not good! Firefox even rejected showing me the page, because I set the minimum TLS version to 1.1. [SSL Report for your site on ssllabs.com](https://www.ssllabs.com/ssltest/analyze.html?d=liqd.net)
I had a quick look through the code. It looks very neat. I didn't see anything obvious to call out. Your README is also very nice! Good luck with the project.
I've started such a [translation](http://github.com/rodrigogribeiro/agda-software-foundations). But, I stopped due to lack of time. If someone could help me with this, it would be nice.
I too believe that Haskell can be made accessible to children. The point that I am trying to make, is that if we want to teach our children how to program, it would be nice to provide them with a tool that would help them. Something like DrRacket, or Processing or even better like Hackety Hack 
code.org is really awesome, and a lot of fun. After playing around with the exercises, I kept thinking what would a lazy, purely functional and statically typed block world would look like. I plan to start working on that project during summer...
I couldn't agree more with your comment, I too hate the idea of children spending their time in front of a computer screen and I don't think that they should be taught programming unless they express interest. But if they do want to learn how to program, it should be as easy and as creative as playing with Lego. I strongly believe that any future coding environment for children, should be based on physical computing and constructing. And I don't have anything against Java, or C or C++, or any imperative language for that matter, they are all great, but with Haskell it's just the way that it makes you think... 
Eclipse is crap though, it doesn't have anything to do with "maturing", only with people having the time to support a plugin for a low-quality Java IDE.
I've been teaching Haskell to middle school students (ages 11 to 14) with no previous programming experience, for about three and a half years now. It's been a blast. Other discussion has already pointed to the web-based environment I built, codeworld.info, which is based on GHCJS, and has a library stringly copied (largely, stolen outright) from Gloss. I'll try to address your other questions here. First of all, the question of purely functional versus imperative thinking, which you touched on, is an interesting one. A lot of people in the functional world, including the Haskell community, would like to believe that if we just taught functional programming first, then that way of thinking would come more naturally, and people wouldn't think of purely functional code and recursion and such as all that weird any more. That may be a partial truth, but it's mainly just wishful thinking. It turns out that imperative programming is in some ways built into our experience of the world. This will still be a struggle, even with "blank slate" students who have never heard of Java. On the other hand, "functional" thinking at a basic level reduces to exactly the kind of thinking that will make kids successful at mathematics. One of the reasons our brain naturally adopts an imperative approach to things is that it makes it easier to just fudge things as we go, when stuff doesn't really quite fit. Learning formal thinking is a valuable goal in and of itself. In the end, *this* is why I persist in trying to teach Haskell at young ages. I do think that it would be *easier* to teach Scratch, or Processing, or Java... but it would also be less valuable. Regarding the environment: there are still no perfect environments for this. For younger children, typing is a very real hurdle! Typing a word can, quite literally, take them 30 seconds. I'd like to have a good solution to this. I don't know what it is. All of the attempts I've seen so far are more limting than I'm willing to accept. I definitely think you should avoid too much abstraction too early in the process. Kids already struggle with the minimum amount of abstraction they need to accomplish things. I put a lot of effort into reducing the level of abstraction involved in CodeWorld, because it was the only way to make it feasible to teach with. So, for example, there is only a Number type -- not Int and Integer and Double and Float and Rational and a hierarchy of numeric type classes. Lists are okay. Anything touching on functors is too much. I get into parametric polymorphism incidentally, in the context of helping them understand error messages that include type variables. But there is only so much you can do at a time.
I mentioned Processing (https://processing.org/) and I think it is a good example of what I consider to be a "more complete coding environment". What is nice about Processing is its simplicity as opposed to other code editors or IDEs. Its standarized way of packaging and incorporating libraries with minimal fuss. The ability to create executables with no complications. If you add a repl and hoogle then you are ready to rock... As for your proposals I think that they are trully great, and I intend in the following days to look more thoroughly at codeworld. I would really like to help in any way that I can. 
&gt; A lot of good software has been written in Python. I never said anything else, in fact I use **quodlibet**, which is one of my all-time favorites. &gt; You make it sound like people who like dynamically typed languages don't know what they're doing, which is an attitude that really irks me. Another analogy which is fitting in the security aspect: Of course, people without seatbelts on, can be good drivers, but would you call them responsible? What is the advantage of not having your seatbelts on? Maybe that you can move a little better, but is it worth it? Static typing often prevents you from being lazy, and to figure out the details. 
I too would rather have my child run around and play with other kids, learn a sport, play with legos, play checkers, build a dam, etc., whatever he finds fun and engrossing. If he has an interest in programming or has an yearning to learn it, then I'd be all for it. I don't think you're wrong. I think there is too much emphasis on coding as a didactic tool but I think that playing with physical objects, games, and sports is far more important for a young mind and body than sitting in front of a computer trying to express an abstraction of some physical object or phenomenon in some computational form. It seems to me like a very inefficient and very narrow way for kids to learn.
Can you explain more about left-infinite structures?
I get nervous when there are more than one page of text without any line of code
[This relatively recent post talks about it in detail.](http://comonad.com/reader/2015/free-monoids-in-haskell/) The one-sentence explanation is that lists are biased towards "starting and extending to the right", and that bias makes lists lose some desirable properties, such as being the free monoids.
Wow! Could you please tell me which school that was? (PM is ok) I'm curious. I only learned some school in Brazil use(d) it.
I'm going to be "that person" and link previous conversations. I am going to list off some suggestions and then link how I found them. - [Why Haskell? Elevator pitch](http://www.reddit.com/r/haskell/comments/282yky/why_haskell_elevator_pitch/) - [Why Julia and not Haskell?](http://www.reddit.com/r/haskell/comments/2ldvae/why_julia_and_why_not_haskell/) - [Why Haskell for a project?](http://www.reddit.com/r/haskell/comments/h6r6t/ask_raskell_succinctly_why_haskell_for_a_project/) - [If someone asked you, "Why Haskell?"](http://www.reddit.com/r/haskell/comments/xlb1g/if_someone_asked_you_why_haskell_what_would_you/) - [Explain Haskell for a layperson](http://www.reddit.com/r/haskell/comments/lho6g/please_explain_haskell_for_a_layperson/) - [Haskell &lt;-&gt; Python](http://www.reddit.com/r/haskell/comments/1nzrlq/request_for_experiences_using_haskell_with_python/) - [Why Haskell is great (video)](https://www.youtube.com/watch?v=RqvCNb7fKsg) (yes I linked this on purpose) - [Should I learn Haskell?](http://www.reddit.com/r/haskell/comments/1hsneh/should_i_learn_haskell/) - [What isn't Haskell good for?](http://www.reddit.com/r/haskell/comments/1hsneh/should_i_learn_haskell/) - [Haskell &lt;-&gt; Clojure](http://www.reddit.com/r/haskell/comments/2mr7ks/im_debating_between_haskell_and_clojure_xpost/) - [Why Haskell?](http://comonad.com/haskell/community.jpg) - [Why Haskell is kinda cool](http://amtal.ca/2011/08/25/why-haskell-is-kinda-cool.html) - [Why Haskell? The big question](http://ezyang.scripts.mit.edu/blog/2010/01/why-haskell/) - [Why I find learning Haskell hard](http://connolly.io/posts/lazy-evaluation/) - [Haskell &lt;-&gt; Ruby](http://www.reddit.com/r/haskell/comments/1o5w3a/request_for_experiences_using_haskell_with_ruby/) - [Why Haskell?](http://blog.mired.org/2011/04/why-haskell.html) - [Haskell makes you a better programmer](http://dubhrosa.blogspot.co.uk/2012/12/lessons-learning-haskell.html) - [Haskell makes you a worse programmer](http://lukeplant.me.uk/blog/posts/why-learning-haskell-python-makes-you-a-worse-programmer/) - [Why the world needs Haskell](http://www.devalot.com/articles/2013/07/why-haskell.html) - [What can I use Haskell for (other than Project Euler problems)?](http://www.reddit.com/r/haskell/comments/fgfh9/what_can_i_do_with_haskell/) - [Why Silk uses Haskell](http://engineering.silk.co/post/31920990633/why-we-use-haskell) - [Why Front Row uses Haskell](http://www.kurilin.net/post/117369543198/haskell-at-front-row) - [Why not Haskell?](http://offthelip.org/2009/02/16/why-not-haskell/) [That's just the first page](http://www.reddit.com/r/haskell/search?q=why+haskell&amp;restrict_sr=on) I am going to propose a challenge which you may choose to take up: Ask a question that hasn't been asked and answered to your satisfaction already in this subreddit. I'm genuinely not trying to be a dick, I really don't know if this is possible. If an answer was not to your satisfaction, then when you ask the question (again) you could explain why it wasn't to your satisfaction so that the answers don't address the obvious. You'd probably need to search your question in the subreddit query box to find out if it's been asked before, I don't think reading all that stuff is remotely time efficient if you have specific questions in mind. If you don't, then browsing the threads will take less time than waiting for replies to stream in. This sort of thing is what [wikis](http://c2.com/cgi/wiki?ReallyValuablePages) were invented for. Accumulating and refining answers/knowledge, but most communities have gone the way of Mediawiki so it's moot. &lt;Soapbox&gt;Mediawiki / Wikipedia / Wikia are publishing platforms (CMSes) - not communities!&lt;/Soapbox&gt; I need to get back to working on my talks and book. I'll check back in to see if you took my challenge. :)
I don't understand the issue with some things throwing exceptions. If that's not to your liking then there are alternatives or you may provide your own alternative. 
Haskell's a great language and a lot of fun. It's like having `scalaz` natively supported. :) But if you're an IDE person there's nothing like Scala's IntelliJ support at the moment. That said I've used IntelliJ and Scala full-time and I still find myself preferring Emacs and `haskell-mode`. Disclaimer: YMMV.
I understand the sentiment, but I think there's a bit of a strawman in this line of thought. I don't feel there's really any suggestion in this thread that kids should be pressured to learn while their friends play. I personally had tons of fun attending computer lessons as a child. I was pretty much instantly hooked and, in fact, code felt more like magic to me than any toy that I can remember. I don't remember having that detract from climbing around in trees, pretending to have super powers, sleepovers, Lego, etc, at all. I also don't feel it's too selfish for a parent to want to spend time with their kids in this way. My mom taught math with games and though I didn't always enjoy being the guinea pig for these 100% of the time, there were definitely some fun ones and some fond memories! PS I think perhaps not quite ready for kids - the error messages especially need work - but [elm](http://elm-lang.org/edit/examples/Intermediate/Walk.elm) seems promising and fun. I am utterly convinced that I'd have had fun hacking about with those little sample games as a child. 
Neat idea. I think a next natural step would be to make it a monad transformer, then you could maybe do your example as an `EventualT (Map String Int) IO ()`
You can largely avoid #3. If you don't want an exception from head, don't use it. Pattern match on `(x:xs)` and `[]`. Likewise, use `reads` rather than `read`, `drop 1` rather than `tail`, and so forth. Most people seem to think that the unsafe functions were a mistake and should instead use some form of checked exceptions provided perhaps by `MonadThrow` or a particular instance of that class like `Maybe`. However, that might add unnecessary complexity especially when you are, in fact, confident that `head` will always work and you don't care about the case where it doesn't because you're sure it won't happen. If you do find yourself feeling uneasy about what you wrote, come join us on #haskell-beginners or #haskell and ask about a better way. PS: Exceptions are great to have for when you do need them, although you could definitely argue that you want to work with failure to get the head of an empty list in pure code, not just IO. PPS: On the IDE situation, things should actually improve soon, and in the meantime you could try using fpcomplete and School of Haskell. However I totally feel you on this. I got into F# before I got into Haskell and even with inferior tooling compared to C# having an IDE that can do things like tell you the type of what you're looking at with a cursorover or refactor a name is great. F# is probably closer to Haskell than Scala is, though.
I second this. Check out [ClassyPrelude](http://hackage.haskell.org/package/classy-prelude-0.12.0/docs/ClassyPrelude.html) (or find other preludes) that don't even include unsafe operations like head. Instead you're encouraged to use [headMay](http://hackage.haskell.org/package/mono-traversable-0.9.1/docs/Data-MonoTraversable.html) and similar total functions. 
There's a book called "*Discrete Mathematics and Functional Programming* that was released a few years ago that tries to teach discrete math and some proofs using SML. You can Google the title to find the table of contents on the author's website. 
your example about case classes, you just give product types, no sum types. I thought case classes were scala's ADTs?
Scala doesn't have proper first-class support for sum types. There are several ways to fake them (using case classes and accepting a supertype, usually, with some other things like closed traits)
&gt; I'm not certain, in fact, if the article isn't tilting at a strawman, at least from the standpoint of CS research. Depends what exactly you mean by "CS research". Often when people talk about X-research they mean the first-order concerns of solving particular X-problems within particular X-frameworks. However, this ignores the fact that those X-frameworks have to come from somewhere, that our goals in studying X isn't to solve X-framework-specific X-problems but rather to increase our understanding about X. The exploration of these higher-order concerns is what history and philosophy of science (HPS) is all about. Of course, once we take the long view and consider the entire history of the study of X, we come to find that the distinction between "X science" and "HPS of X" is not so crisp as we might wish. Breakthroughs in X-science often occur by discarding X-frameworks and reconceptualizing what it is that "X" could mean. So, yes, in terms of the day-to-day research in CS, nailing down what "type" means may not be especially important. However, in the long term, discussions of what "type" could mean will have (and have had) a dramatic influence on the long-term progression of research in CS. To make this more concrete, consider the Church‚ÄìTuring hypothesis. The hypothesis states that effectively computable functions are coextensive with things definable in particular formalisms. However, this hypothesis can never be proven because the notion of "effectively computable functions" is an informal one; the entire intellectual content of the hypothesis is in its attempt to formalize this informal notion. And even though it can never be proven correct, the formulation of the Church‚ÄìTuring hypothesis has had major ramifications on the entire field of CS, opening up the entire field of computability theory. In exactly the same way, "type" is an informal notion, one which cannot ever be fully formalized; and yet, our attempts at formalizing what "type" could mean will have (and have had) significant influences on the entire field of CS.
Glad you found it interesting! Fixed points are still a little mindblowing to me, but I think I'm starting to get a bit of a feel for them, and they certainly seem like a potentially very effective and flexible way to approach this sort of problem. If you have any questions about the library, want any extra features, or find a bug, don't hesitate to let me know!
Yeah...
&gt; Is there anything that Haskell can do and Scala can't? Whenever someone asks this question, I always think, "They're both general purpose programming languages. You can accomplish whatever you want in either language." However, there are indeed nuances. Scala interoperates with Java. This is probably the biggest point in Scala's favor over Haskell; you can interact with legacy Java code easier with Scala. Haskell's type system is much better. Scala's is more powerful -- too powerful. Haskell's hits a sweet spot, while Scala's misses any sort of sweet spot because it throws in everything and the kitchen sink.
If you already know Scala it should be quick to learn Haskell, much quicker than it took you to learn your first functional language. 
location is not a hard requirement. we prefer finding someone we can work together with out of the same office, but if you are really good, we may hire you even if you are located somewhere else.
I'm a complete n00b and a terrible programmer so you shouldn't take any of what I write under any serious consideration. I don't think that there is really a question of LanguageA vs LanguageB. In our day to day work we have to use many different languages because each one is more applicable to the problem that we are trying to solve. And it isn't really a question of OOP vs FP, for the same reasons as above. In some situations OOP is a preferable solution to FP. With that said, I fell in love with FP and especially Haskell's take on FP because of the way that it makes me think about computation. I believe it's the raw mathematical beauty, and the "artistic" value of FP drove me back to Haskell, even after I failed to learn it, time after time after time... So my opinion is this: For your work you should use whatever language is easier to solve the problem that you are facing. If you want to deepen your knowledge on FP then not only should you learn Scala and Haskell but also Idris and Coq and .....
use cabal to build it (in the root folder of the sample, where the .cabal file is): cabal install Paths_hsqml_demo_samples is something that cabal sets up. It allows the app to find out where it was compiled. It provides getDataFileName in that source file. See this cabal documentation if you are interested: https://www.haskell.org/cabal/users-guide/developing-packages.html#accessing-data-files-from-package-code I've been very happy with HSQML myself, one app I've built is there: https://github.com/emmanueltouzery/projectpad For other samples, I've also used that app as an inspiration: http://reichertbrothers.com/blog/posts/2014-06-06-hsqmlstocqt-haskell-qml-data-visualization.html
What's the benefit of generative exceptions? I've never heard of them before, but as far as I can tell from [the MLTon wiki page](http://www.mlton.org/GenerativeException) they are equivalent to having a -&gt; IO (Dynamic, Dynamic -&gt; Maybe a) Would you agree, or is my understanding of that page faulty, or is that page not comprehensive on the matter? 
As for the IDE, you should really try Emacs w/ a recent `haskell-mode`. While Emacs is not what you'd traditionally call an IDE these days (in fact it's much much more...) it's so far the most actively developed (and used) Haskell-IDE-like thing afaik. It's UI may seem underwhelming if you're used to more eyecandy GUIs... but don't let that put you off. (What exactly do you perceive as the worrying "Haskell IDE situation" btw? What can't you do with Emacs that an IDE can?) I'm probably a bit egoistic here (as I happen to be an Emacs fan), but I hope Emacs/`haskell-mode` remains one of the most popular Haskell editor in the Haskell community, as that secures continuing contributions/development, rather than having developers able to contribute (which are a scarce resource) dillute/spread over different Haskell IDEs and thereby slowing down development and improvements of `haskell-mode`... 
&gt;I just got too used to be able to be able to check the exact inferred type of anything by just pressing ctrl+q. The atom.io package ["haskell-ide"](https://atom.io/packages/ide-haskell) will tell you the inferred type of something if you hover your mouse over it. Might be a keyboard shortcut for it too. All in all that package works great for my haskell editing and I find myself using it (and by extension atom.io) a lot, even though I usually use vim as my main editor.
&gt; install ghc 7.10.1 and cabal 1.22.3.0 from source. I followed the instructions (just ran all the commands in order) and they worked on Ubuntu 14.04 perfectly. IMO, you can't beat sudo apt-add-repository ppa:hvr/ghc sudo apt-get install ghc-7.10.1-htmldocs ghc-7.10.1-prof cabal-install-1.22 export PATH=/opt/ghc/7.10.1/bin:/opt/cabal/1.22/bin:$PATH in terms of getting up and running with GHC 7.10.1 or any other GHC version supported by that PPA on Ubuntu...
What were the packages taking up most time to compile?
I'd say from an IDE I expect: * Find usages. * Navigate to definition (Including stdlib or external lib sources). * Autocomplete. * Show type for a value. * Show docs/type for a function. * Rename within project. * Automatic imports. * Debugging (I do hope that haskell does better debugging of functional expressions than jvms line based debugger) * Navigation - files/symbols, including libraries. Seems that from top of my head. If emacs can do all that, I'm happy üòÉ 
Seems a bit like `MVar` to me. `eventualKey` seems to be reading something out of an `MVar` and `mapUpdate` is putting something into it.
Note: You'll need to install `stackage-cli`, `stackage-cabal`, and `stackage-sandbox`. We decided that splitting these up wasn't very beneficial, so in the future they will all be in the same package again. But for now they're still split up.
I could get a lot of mileage out of this. =)
It could even be intentional in case of some ACME package.
Forget TH, anything with a custom Setup.hs can execute arbitrary Haskell code during `cabal install`.
Can someone explain ACME to me?
There's a category of joke packages on hackage: http://hackage.haskell.org/packages/#cat:ACME
Yes, I have been trying different AD modes, and Reverse seems to perform better than the rest. 
Heartily endorse all of this. We use a lot of twitters stack (thank you Twitter!) it is sad that essentially to be productive / readable you have to retreat to a very small subset of the language otherwise it becomes a hideous mess. Type inference really is horrible. I very much miss the dialogue that is possible with hole driven styles of development with Haskell. I miss not getting stream fusion and having to eliminate intermediate structures by hand etc. Haskell really does allow you to truly break out from OOP inba way that Scala just doesn't. I find myself often breaking out javap to check that we are java compatible in terms of generated class names so that service loading code will still work right etc. It's such busy work. You are never allowed to forget about Java, and were it not for a few killer libraries I would be dragging our platform kicking and screaming onto Haskell. As it is I will content myself writing all of our tooling in Haskell instead :)
nice hairpin!
I prefer to call it the right shift assignment operator though.
Well sort of, but most importantly you can suspend operations until values in the map become available. See the example and its output, after executing `op1` the map is still empty, because of the data dependency on key 'foo'. Only after `op2` is run, in which 'foo' gets inserted, `op1` resumes and puts in 'bar'. It is a sweet combination of continuations and state I think. Edit: I just looked up what `MVar` can do and it indeed seems to have some overlap.
Hmm, have you compared this functionality to [concurrent-extra](https://hackage.haskell.org/package/concurrent-extra) and [stm](https://hackage.haskell.org/package/stm). Not sure if overlapping or could take this further. And not sure if your solution qualifies as parallelism or concurrency. Good inspiration anyhow!
Here's your statutory reminder that as ADT is used to stand for both "Algebraic Data Types" and "Abstract Data Types", polar opposites conceptually, it is a perfectly useless acronym which should be avoided.
How many packages does this successfully install?
While "God mode" seems like too much for me, we use overall idea in our build infrastructure by pre-installing several packages (snap, yesod, hedis and few more) inside base ghc docker-image that builds projects. This really saves time later.
I like how the gift is green just like GitHub's color for Haskell.
In Emacs + haskell-mode + ghc-mod you press `C-c C-i` to see "info" with type and definition place of a symbol. You can also press `C-c C-l` to load current module into ghci and play with repl there.
Doesn't `cabal install stackage` do the right job of installing them all already?
I'm aware of two structural editing approaches for Haskell. 1. [Lambdu](http://peaker.github.io/lamdu/) is a gui-based structural editor. For screenshots look at the end of the page. 2. [Structured Haskell Mode](https://github.com/chrisdone/structured-haskell-mode) is a text-based structural editing mode for emacs. However, I haven't found time to try them out yet, so I can't say anything about their maturity.
Right. Seems to me that pre building Haskell LTS is perfectly enough for most.
What kind of killer libraries?
If I understand your problem, your are trying to load a system, with self reference, like indeed the pass of a compiler or an excel spreadsheet. What about this instead (everything is in the datatype)? import Data.Map as Map import Data.Maybe import Control.Applicative data M = M (Map String (M -&gt; Maybe Int)) eval :: M -&gt; String -&gt; Maybe Int eval (M m) s = do f &lt;- Map.lookup s m f (M m) m = M . fromList $ [ ("foo", (\m -&gt; do bar &lt;- eval m "bar" return $ bar*bar )) , ("bar", (\_ -&gt; Just 2)) ] main = print (eval m "foo") 
I don't see how you would ever be able to list emacs (or vim) as newbie-friendly.
The market is touch screens such as tablets. I've yearned for many years (since I first bought a Newton, played with it three days, and put it away) for a grammar-aware visual editor on a touch-screen. Someone needs to have the audacity to imagine what several decades of evolution would look like, if this was the only way any of us programmed, imagining from scratch the interface. From that perspective, Blockly is a first draft that hasn't reached any of the ah ha moments where the paradigm changes. A strongly typed functional language should have advantages here. While lisp syntax is too simple, Haskell's syntax is too arcane. It was never optimized for this domain, any more than a rectangular box is intended to fly.
There are 922 directories inside ~/.stackage/sandboxes/ghc-7.8.4/lts-2.5/lib/x86_64-osx-ghc-7.8.4
Sorry, didn't collect any measurements. Just ran it in the background and didn't watch it much.
Yes, at any time our projects use single version of single LTS. And for that, our base-image with GHC has just few this (but quite huge) packages built.
What was the final size of the installed libraries?
Maybe because the doc of Control.Monad.Fix doesn't really help. However, it looks similar to my solution. Could you explain how you would use it ? 
I wonder if something like blockly could be used as a replacement for [arrow syntax](https://www.haskell.org/arrows/syntax.html).
I don't know for sure about the equivalence you are proposing, but I won't rule it out! I think a few years back, Ed Yang demonstrated how to simulate generative exceptions in Haskell using IO, but it was very, very gnarly. But anyway, probably it can be done! (But it is doubtful that it would be less painful than just working around the lack of generativity. Part of the reason ML is so nice is that the idea, which is at the very heart of Brouwer's intuitionism, that at any given stage/point in time, you may generate a novel, never-before-seen object, is built directly into the language. Anyway, the standard use-case for generative phenomena is to have two different instantiations of the same thing not interfere with each other. In each case, you can sort of work around a lack of generativity through kludges (like IO), but having it natively is a huge win. (A big enough win that, for instance, after years of resistance, generative functors were added to OCaml recently). OCaml also has support for open data types like ML's `exn`, in the general case, which is pretty cool. Exceptions and a nominal abstraction (for generativity) was added this year to Nuprl as well, and was used to prove a weak version of Brouwer's continuity principle for baire functionals in type theory. In fact, you need generativity in order to get a suitable shared secret for computing the modulus of continuity for some sequence; otherwise, the sequence itself might interfere with your probing of it.
I think it would be possible to write a backend for browsersync, which gets refresh events through a websockets connections.
[Is this what you mean?](https://hackage.haskell.org/package/base-4.8.0.0/docs/Control-Arrow.html#t:Kleisli) It defines `ArrowLoop`, `Apply`, `Choice`, `Plus`, `Zero`, and, obviously, the base `Arrow` class. `(&gt;=&gt;)` becomes [`(&gt;&gt;&gt;)`](https://hackage.haskell.org/package/base-4.8.0.0/docs/Control-Category.html#t:Category) from `Control.Category` or just `flip (.)`. (Edit: I realize now I misunderstood the question. Tailcalled's response is best, I think.)
I meant, a working example (similar to my post above) would be great ;-)
I'd bet you can't do it in general. You're essentially trying to lift an arbitrary monad in Hask to an arbitrary Arrow ¬¥p¬¥. I doubt that's possible in general, but you can probably define a typeclass, if you have multi-parameter typeclasses: class (Arrow p) =&gt; ArrowMonad p m where areturn :: p a (m a) ajoin :: p (m (m a)) (m a) amap :: p a b -&gt; p (m a) (m b)
Isn't that equation just defining `m` as a monad over the category which has `p` as arrows? If that's the case, would a monad that's defined for general categories do the trick? e.g. class (Category p) =&gt; FunctorCat p f | f -&gt; p where mapCat :: p a b -&gt; p (f a) (f b) --ApplicativeCat goes here class (Category p, FunctorCat p m) =&gt; MonadCat p m | m -&gt; p where joinCat :: p (m (m a)) (m a) kleisliCat :: MonadCat p m =&gt; p a (m b) -&gt; p b (m c) -&gt; p a (m c) kleisliCat f g = joinCat . mapCat g . f I'm sure this exists but five minutes on package hasn't found it. [categories](http://hackage.haskell.org/package/categories) has a Functor instance that's more correct than mine (I forgot functors go between categories) but I don't see the monad instance.
I did make [a concept editor in the browser](https://www.youtube.com/watch?v=v2ypDcUM06U) once with editor combinators (no, I don't have the code anymore) and someone (I think /u/copumpkin) commented that it would be nice for a touch screen interface. But it doesn't really re-imagine programming, it just makes the environment understand syntax natively. In the end I didn't proceed with it because I didn't want to lose Emacs.
 &gt; cabal install --help ... --extra-include-dirs=PATH A list of directories to search for header files --extra-lib-dirs=PATH A list of directories to search for external libraries You need to use those to point cabal to the folder in which your local SDL2 headers and libraries are installed; they're probably subfolders called "include" and "lib" somewhere inside `$HOME/local`.
I tried that, but still get setup-Simple-Cabal-1.16.0-x86_64-linux-ghc-7.6.3: The pkg-config package sdl2 version &gt;=2.0.3 is required but the version installed on the system is version 2.0.2
Now that's a very different problem: you have two versions of sdl2 on your system (2.0.2 and 2.0.3), and cabal found the wrong version. If there is an `sdl2.pc` file in `$HOME/local`, you might be able to force cabal to use that one instead of version 2.0.2's by prepending its folder to the PKG_CONFIG_PATH environment variable... but it's probably simpler to uninstall version 2.0.2, as one version is probably enough.
You may be interested in how I do the Stackage Nightly and LTS Haskell builds, which have the exact same security concerns. The script is at: https://github.com/fpco/stackage/blob/master/automated/build.sh What I do is run all the commands inside a Docker container, and change which volumes are mounted, and which are mounted read/write, depending on which action is being done. On Mac, you can probably do the same thing with chroot. (And of course, remember to run as an unprivileged user!) This is really cool stuff, thank you for doing it and sharing!
I wanted to avoid compiling 2.0.3 system-wide because I just don't like having orphaned files around outside my package manager. But if I temporarily uninstalled 2.0.2, installed helm, it would find my $HOME/local/blah/sdl stuff and I would be safe reinstalling 2.0.2?
Yes.
 that's a good point. I meant "AlgDT"
Is typing still really that big a hurdle? I would have thoughts kids would be learning to type at quite a young age these days. 11-14 is the age where I was teaching myself C, not still learning to type. And I'd contest that there's no perfect environment. May I (quite seriously) mention the WarCraft 3 World Editor? It comes with full fledged game attached with extensive assets and complex functionality taken care of, so hardly anything has to be done to have something fully playable. Yet the Trigger Editor allows full GUI editing of code, and fundamentally forbids the creation of type and syntax errors. Having a literal drop down list of every valid option you might want to put in a given spot is... kinda handy if typing ability is limited. The only drawbacks I can think of is that it isn't free, and it's JASS underneath (and so, impure) not Haskell. Building a similar system for Haskell would be as close as I can figure to perfect for introductory programming, though it would be a lot of work (probably want to skip the "having an entire AAA game attached" part, if nothing else).
Why do you prefer writing your own shell.nix/default.nix over using cabal2nix?
Isn't this breaking encapsulation because things aren't fully encapsulated in the first place? If one has something like: newtype Fetch a = F {unFetch :: (MonadHttp m, MonadFs m) =&gt; m a } `fetchWeather` would have the type `Fetch Weather` and one should still be able to change the internals of `Fetch` without breaking user code. Am I missing something important? Because when things seem to easy that's usually the case.
I believe this article is missing a simple alternative that's extremely common in practice: have a `WeatherM` type with a monad instance and a restricted set of operations, and a `runWeather :: WeatherM a -&gt; IO a` function to translate it into the `IO` type. We can actually improve on this just a bit and get something that lets us have our cake and eat it: give the function a type like `runWeatherM :: (MonadHttp m, MonadFS m) =&gt; WeatherM a -&gt; m a`, then provide `MonadHttp` and `MonadFS` instances for `IO`, and now: 1. Clients who don't care about the range of effects in `WeatherM` computations can just `runWeatherM whatever :: IO SomeType`. 2. If a client really does insist on running `WeatherM` actions only if it will only use `MonadHttp` actions, then that client is responsible for coupling itself to the `MonadHttp` class. Then if the downstream library author makes a change that require filesystem access, the upstream compiler will correctly catch it. Note that we see *tons* of `runX :: SomeMonad a -&gt; AnotherMonad a` functions in Haskell libraries; this is a pervasive pattern: *monad morphisms*. We also we have tools that build on it quite a bit. Monad transformers *are* monad morphisms, and they tend to come with functions like this: runWhateverT :: Monad m =&gt; WhateverT m a -&gt; m a Free monads have this operation (though not all libraries provide it or call it this): interpret :: (Functor f, Monad m) =&gt; (forall x. f x -&gt; m x) -&gt; Free f a -&gt; m a There's the [`mmorph` library](http://www.haskellforall.com/2013/03/mmorph-100-monad-morphisms.html) as well, and I'm sure I'm missing a lot of other such tools. You know how OOP programmers often say "compose, don't inherit"? Maybe we Haskellfolk ought to say "[inject](http://en.wikipedia.org/wiki/Injective_function), don't subtype." If you need a subset of the behaviors of a standard monad, write a restricted monad type and an injective transformation from your monad into the standard one.
&gt; On Mac, you can probably do the same thing with chroot Or just use docker there also (http://boot2docker.io)
This thread wouldn't be complete without a couple of hyperlinks to Labelled IO stuff: [https://hackage.haskell.org/package/lio](https://hackage.haskell.org/package/lio) [http://www.seas.upenn.edu/~cis552/lectures/lio.html](http://www.seas.upenn.edu/~cis552/lectures/lio.html)
That could be a good example where restricted side effects are desired. Thank you.
Nothing original about it, you see this pattern all over the place.
 This is the first thing I've tried that actually makes package management for Haskell livable again. If you don't recommend it, what do you recommend?
I really like NixOS but http://www.stackage.org/ seems like a better solution. Requiring everyone to use Nix to develop on a piece of software is not always practical.
&gt; You can't have both. Sure. But you can have either. If you provide only `fetchWeatherIO :: IO Weather`, clients only have the first option you list: "using unrestricted side effects". Given `fetchWeather :: MonadHttp m =&gt; m Weather` together with `instance MonadHttp IO`, they get to pick their poison. It's strictly more general.
Divergence would be a problem, but the very reason we added `(*&gt;)` and `(&lt;*)` was to improve the asymptotics of code written with `Applicative`.
I've tried setting up emacs and atom today. Neither managed to do autocompletion (it should have been working from the docs) :(
see /u/GeniusBad's comment to op
Unless I've misunderstood what you wrote, the Effects library in [Idris](http://www.idris-lang.org/) is almost exactly like this. It is described in [this paper](http://eb.host.cs.st-andrews.ac.uk/drafts/eff-tutorial.pdf).
&gt; [‚Ä¶] variable bindings [‚Ä¶] x a = y a y a = x a
I've been using Nix almost exclusively for managing Haskell projects lately. Deep sandboxing (down to the glibc level), binary caching, and knowing that I'll always be able to build my old projects are all invaluable advantages. For my FRP libraries, asking users to manually build and install either GHCJS or webkitgtk was far, far too much. Instead, I was able to provide a [one-step installer](https://github.com/ryantrinkle/try-reflex) that uses a nix script with a binary cache server to give the user a 3-minute download instead of a 6-hour build.
Ahh, okay. It's a little disheartening to think of seeking out a new combination of tools, but maybe there's a more recommended framework. I saw that thread before I'll take a closer look at it, thanks again for your help. 
Goal: Learn about programming, haskell, and gamedev (physics, pathfinding, AI, image processing, etc) by using some haskell-based game engine to make dumb little games. Subgoal: run helm, which is installed via cabal. (though actually helm itself has been suggested against, so I might try something else entirely.) Subsubgoal: helm requires SDL2-2.0.3, ubuntu repos have SDL2-2.0.2, so get SDL2-2.0.3 available for helm. I could just do a system-wide install of SDL2-2.0.3 but I'd prefer not to install non-packaged stuff outside of $HOME.
I feel like they solve slightly different problems. Stackage makes sure you have a stable set of packages, which means you should find yourself in Cabal Hell less often. But the amount of effort it takes to spin up a new project is still an issue. Yes, I'm talking about building all the dependencies in cabal. But also lots of dependencies in other kinds of projects too. Nix seems like it'll help there too. I like to try new research software. I dabble in computer vision, natural language processing, and audio signal processing and machine learning, (all at a fairly amateur level). I frequently find myself installing new software with big onerous dependencies, which often conflict with stuff I already have installed. NixOS is perfect for dealing with this sort of recurring headache.
I suspect this post will get a lot of hate here, but it is true.
This sounds exactly like a job for cabal sandboxes. By using cabal sandbox you could install sdl-2.0.3 for just the helm package and not touch the global or user cabal database. I don't have a great link on hand for cabal sandbox, but hopefully a google search will yield some useful results. 
&gt; maybe there's a more recommended framework Helm provides an FRP framework, keyboard and mouse events, setting up the window across platforms, and graphics primitives including text, sprites and vectors. I recommend using [gloss](http://hackage.haskell.org/package/gloss), as it provides all of this except the FRP framework. And if you would like to use an FRP framework, I recommend using [reactive-banana](http://hackage.haskell.org/package/reactive-banana), as there is a wrapper called [gloss-banana](http://hackage.haskell.org/package/gloss-banana) which connects it with gloss for you. That's the setup I've used for [my last Haskell game](https://github.com/gelisam/ludum-dare-31#readme).
This is awesome. Thank you for your config files. I decided to hold-off on NixOS, for now. I want to get a Nix environment working under my Ubuntu. Can you give me some pointers on setting up the haskell environment (especially with this new Haskell-NG approach) and a sandboxed project? The files that you posted are for your NixOS. Would I have to do anything differently for just-Nix?
I'm in the same exact boat. I really want to become a proficient Haskeller, but my frustration with cabal drives me away every time. I'll start coding in Haskell again once either (a) cabal is massively refactored for usability, or (b) an alternative to cabal is presented. It's a chicken and egg problem: I'm not good enough with Haskell to fix cabal, and cabal is preventing me from learning Haskell. Hopefully Nix will help. :)
That would work, but would create a Linux package database instead of a Mac OS X package database.
Yes, I agree that most of these are valid challenges for a visual editor. I think making a purely visual editor that is better than a textual or structural editor is a very difficult problem. This is partly why I am focusing initially on viewing and understanding code, and not on editing. On a side note, Glance is focused initially on reading code because more time is spent reading code than writing code. A [recent study](http://www.inf.usi.ch/phd/minelli/downloads/Mine2015b.pdf) found that on average, 69% of developer time is spent understanding the code (reading or staring at the screen), and only 5% of the time is spent editing (typing in code). Nevertheless, I think most of these problems are solvable with a visual language. Glance has the advantage when compared to other visual languages, in that Glance has the underlying textual or structural Haskell code that can be edited, diffed, searched, pointed to, etc.. &gt; * two representations are harder to memorize than one For someone who already knows Haskell, Glance would be something additional to learn. For people new to Haskell however, [dual coding theory](http://en.wikipedia.org/wiki/Dual-coding_theory)[\[3\]](https://www.sciencebasedmedicine.org/brain-based-learning-myth-versus-reality-testing-learning-styles-and-dual-coding/) suggests that a visual language might make Haskell easier to learn (dual coding theory might not apply to visual programming languages though). &gt; even GUI designers tend to annotate their icons with words by now, unlike the predictions of the initial enthusiasts in that approach. This issue arises in textual languages too. For example, the debate about short and cryptic operators vs. longer descriptive names. This is less of an issue in Glance though, since the minimal set of icons used just to represent code syntax is small. If you don't like icons representing functions, you will be able to generate a Glance diagram that uses their text names instead. 
I completey agree. Despite being so young (or maybe it's even because of that?) the tooling is way ahead of haskell. And because everything is already built into the compiler it doesn't break as easily with new versions.
I'm glad to hear that the tool work has been paying off! This new version will also put the same color highlighting on your source files (if you use Emacs, and I'd love to help someone implement the same thing for other editors - all the smarts are in the compiler). With regards to the colors - I think this becomes far more necessary in a dependently typed language, because it's much more difficult to see what kinds of things you're looking at based solely on their names. Constructors need to be treated very differently from functions when trying to figure out an error message, but if they look identical, it takes a lot of context. In Haskell, these things don't show up as often, so fancy coloring hasn't been as necessary yet. It's really great to hear that the error messages are useful for someone. We get lots of bug reports about bad error messages, and it can be easy to forget that improvements actually do help some people. So thanks!
One thing to note with tooling, though, is that there are things that can be done with Haskell tools that we can't yet do. A limitation of Idris's internals is that all type checking is happening on a lossy translation to a core language. We maintain enough source information to point errors at the right spot in the file, but not enough to reconstruct the original tree - there's just a heuristic translation that attempts to reconstruct **a** source tree that fits the core term involved. This means we don't have a reasonable way to implement nice features like ghc-mod's "type of expression at point", structured-haskell-mode's tree-aware editing, or even simple refactoring tools like renaming an identifier across a module or alpha-converting a local binding. I wish I had time to make those things happen.
haskell is way way too difficult for a child to teach himself. most programmers cant even learn it properly. maybe with some guidance it could be done or just for amazingly inteligent kids, but most kids who start programming teach themselves. 
Yes, this HS_CPP = @HaskellCPPCmd@ @HaskellCPPArgs@ suggests that something in the configure script isn't working correctly. When I build an x86_64/linux to aarch64/linux cross compiler (currently works with git HEAD, but the ghc-7.10 branch is currently broken due to https://ghc.haskell.org/trac/ghc/ticket/10409) I configure with: ./configure --target=aarch64-linux-gnu -with-gcc=aarch64-linux-gnu-gcc-4.9 and during the configure process the @HaskellCPPCmd@ is detected as: checking how to run the C preprocessor... aarch64-linux-gnu-gcc-4.9 -E I assume that you have set `BuildFlavour` to `quick-cross` in `mk/build.mk`. 
Yep, it relates. Also, it was interesting to read. I would be interested in the details if you have time. Do you clone the repo on each build? Or do you just re-use the same container across builds? How do you work with ghci? Do you use cabal sandbox inside the container?
We never re-use a container across builds, so the final container sees a stack that is a pristine OS, build tools installed once, dependencies built once and then main code built once. The dependencies are built with an earlier version of the repo than the main code, because of the caching, but the hope is that the dependency detection shell script figures out when that needs rebuilding... The repo gets initially cloned in the dependency layer (because we need it at least then to know which dependencies to install) and then the third (final build) layer does a git update, which pulls in all the commits since the dependency layer was built. The dependencies and main code all build into a sandbox, yes. If you want to do things like ghci, you can docker run /bin/bash in one of the images and type "ghci" (or do other interactive stuff) We also support branches: each branch can have its own separate base+dependency stack. This paid off when working on an upgrade from ghc 7.8 to 7.10. The rebuild detection is basically generating a hash for all the files that could affect whether that layer needs rebuilding; then either reusing an existing docker image with that tag, or building+tagging a new one if that tag does not already exist. The main cause of problems there has been us not including enough files in those hashes (but that is a fairly standard "forget to manually specify the right dependency" problem)
I had one just the other night. Moving I bumped finagle to 6.25 they moved around a class such that while it's _name_ in the source code didn't change, the generated name changed (class was moved outside of a package object, `param.Tracer`, if you are a curious). Because the encoded name was changed it meant that classes that depended on 6.64 compiled fine, but failed at runtime. Not too hard to fix in the end, but still wasted a good 20 minutes of reading eviction warnings and dependency chasing to figure out which rogue lib needed an exclusion / bumping.
A few disconnected thoughts on this tradeoff: * You can't always move from flexibility to guarantees; if you give the clients some of your flexibility (by allowing them to extend your OO classes, or with higher-order functions), you lock yourself into a flexibility trap. I suspect a lot of programs gain a lot of bugs by attempting to move from flexibility to guarantees. * Guarantees seem to correspond to both open and closed products, closed sums, least fixed points, universal quantification. Flexibility seems to correspond to open sums, greatest fixed points, existential quantification, function types and linear types. * The article gives 'constructive proof' of its guarantees; it allows the user to swap in their own monads. * This is related to the expression problem in the obvious way. * Guarantees seem to reduce the number of bugs. This is suspicious, as guarantees and flexibility *seem to be* categorically dual. I suspect the problem here is that OO languages provide casts and other encapsulation-breaking features.
I had a course where we used Coq to prove and program. How does Idris compare to Coq? 
The reason why you need the dependent version of `funext` here is that your `myId` has a dependent type: the _value_ passed in as the first argument is used as the _actual type_ of the second argument. The simply-typed version of `funext` doesn't account for that case. This is why it worked for the first one: there, you've fixed `r` externally, so you're dealing with something known to be the same. In the second definition, the substitution of `r` has not yet occurred, so the dependent version of the postulate becomes necessary.
Haskell has the `fromInteger` conversion that you're mentioning here. It's a method on the `Num` type class, rather than being resolved as type-disambiguated overloading, but it is there. Just so Haskellers don't get confused: Idris doesn't have `!` patterns in the sense of Haskell, because it's already strict by default. What I think /u/lw9k is referring to is the `!` notation for monadic bind, which is a prefix operator that causes its argument to be lifted as if by `do`-notation to immediately under the closest binder. For example, one could write: main : IO () main = case !getArgs of [] =&gt; printUsage [_] =&gt; doStuff _ =&gt; printUsage instead of main : IO () main = do args &lt;- getArgs case args of [] =&gt; printUsage [_] =&gt; doStuff _ =&gt; printUsage which is _really_ nice when doing nested monadic stuff.
We don't have any benchmarks for idris-mode. It's still nothing as nice as what you can get with `structured-haskell-mode` or `ghc-mod` with regards to editing. What I've worked hardest on has instead been introspective features, for doing things like getting documentation, searching the library, and understanding error messages.
[Attribute Grammars](http://foswiki.cs.uu.nl/foswiki/HUT/AttributeGrammarManual#Example_1:_Calculate_the_sum_of_a_tree_of_integers) are probably the theory behind your monad. With AGs, you can say stuff like: * let attr1 be the fold of all the values in the nodes of a tree, starting from the root (more concretely, let's say we want to count all the uses of a particular function call in an AST) * let attr2 be an attribute that is made available to all tree nodes, and attr2 = attr1 (in other words, the value of attr1 is disseminated throughout the tree) * let attr3 be a chained attribute that is updated with each visit of a node (depth-first by default). More concretely, we define an update function that takes the data structure at a particular node and the attribute, and update it. The chained attribute is passed down the tree automatically. When we've gone through a subtree, the attribute is automatically passed upwards as well. So, attributes can depend on each other, and passing them through the AST is done automatically by the [UUAGC compiler](http://foswiki.cs.uu.nl/foswiki/HUT/AttributeGrammarSystem). 
This is amazing news! Just knowing that community recognizes that it's a pain point gives me hope. My fear is that cabal would be used as a gatekeeper.
I meant for the idris language itself, application performance. Those all sound like cool features.
[Here](https://www.haskell.org/cabal/users-guide/installing-packages.html#developing-with-sandboxes)!
You can do it in a restricted way, but it's not very powerful due to the lack of universe polymorphism. For example, suppose you were to try encoding length-indexed vectors. One approach might be to encode it as this eliminator: forall (a : *) -&gt; forall (n : Nat) -&gt; forall (Vector : Nat -&gt; *) -&gt; forall (Cons : a -&gt; Vector n -&gt; Vector (Succ n)) -&gt; forall (Nil : n -&gt; Vector Zero) -&gt; Vector n a However, using that eliminator you can't even write a type-safe `head` function of type: head : Vector (Succ n) a -&gt; a The part where it breaks down is coming up with a proper eliminator for the `Zero` branch. This is because the eliminator for `Nat` doesn't let you return a type. For reference, here is the definition of `Nat` that I'm assuming: forall (Nat : *) -&gt; forall (Succ : Nat -&gt; Nat) -&gt; forall (Zero : Nat) -&gt; Nat You can fix this by either (A) using universe polymorphism or (B) using a different core calculus (Jon Sterling suggested Martin Lof type theory, for example). So why even keep dependent types at all, then? The main reason I still have them is because they allow you to model coherent instances. For example, you can define a safe union function of type: union : forall (o : Ord) -&gt; forall (a : *) -&gt; Set o a -&gt; Set o a -&gt; Set o a In other words, you parametrize the `Set` type on an additional type parameter representing its `Ord` instance so that you can enforce that sets you `union` use the same instances. However, even with that I'm still not convinced that this restricted form of dependent types are worth it and I may eventually use System Fw instead. I'm postponing that decision until `annah` is complete because part of the scope of `annah` is come up with a shared set of terms/types/function/data-structures/side-effects that all interoperating lanuages must support and then by then I'll know better if the dependent types are worth it or not.
I'm thinking about rules like: Gamma |- b: Boolean, T: Boolean -&gt; * Gamma |- ifTrue: T(True), ifFalse: T(False) ---------------------------------------- Gamma |- if b then ifTrue else ifFalse : T(b)
If you are serious about deepening your knowledge of FP, I recommend learning Haskell. And if you've been programming Scala functionally, much of your knowledge will translate so you'll pick it up quickly and it will be pretty fun. [This guide](https://github.com/fpinscala/fpinscala/wiki/A-brief-introduction-to-Haskell,-and-why-it-matters) is a crash course in Haskell that we wrote as an appendix to our book, [FP in Scala](http://www.amazon.com/Functional-Programming-Scala-Paul-Chiusano/dp/1617290653/ref=sr_1_1?ie=UTF8&amp;qid=1431957716&amp;sr=8-1&amp;keywords=functional+programming+in+scala).
Exactly. However, it *would* be possible if you had a 'true' boolean type, with dependent eliminators and all.
Someone smart please make a morte decoder for javascript.
I know that Morte's dependent types are pretty neutered. At some point I decided to just release what I had because I figure the idea (of a distributed Internet of typed expression fragments) was more important than the specific core calculus used as the carrier language.
I had it set to `perf-cross`, but I got the same error with `quick-cross` now. This is seen in the configure process output: checking how to run the C preprocessor... gcc -E I also tried with ./configure --target=aarch64-apple-darwin14 --with-gcc=aarch64-apple-darwin14-clang --with-hs-cpp='aarch64-apple-darwin14-clang -E' but got the same error.
I did look at the source tomejaguar linked and your explanation but I'm frankly confused by this. The problem generative exceptions seems to solve for ML doesn't appear to occur in Haskell. You can't write cast because you can only catch exceptions in `IO`. If you wanted to try and make a cast you'd have to invoke `unsafePerformIO` and then all bets are off. In any event, `catch` is constrained to return an IO action of the same type as the one you're attaching `catch` to so I don't even see how you could do `IO a -&gt; IO b` with this. Am I missing something here?
Generativity does indeed prevent type unsafety in presence of effects, but that's not the main point. For the main point, see the explanation I gave in a comment to this thread (i.e. non-interference and abstraction).
Awesome. I'm so excited for this! I think the section on **Linking** is especially interesting because they motivate us putting more proofs in our code. For example, the definition of `#False` may change (and point towards `#True`) but if we include proofs of boolean laws, then we lessen the anxiety that a changed property would break current program behaviour (aside from an inability to compile - in which case you use your cached copy while the issue is resolved).
I've bought the book before and loved it üòÉBut the chapter didn't convince me. But thanks for that great piece of work!
You can: {-# LANGUAGE MultiParamTypeClasses, FlexibleInstances #-} import Control.Category import Control.Arrow import Prelude hiding (id, (.)) newtype KleisliT m a b c = KleisliT { runKleisliT :: a b (m c) } instance (ArrowMonad' a m) =&gt; Category (KleisliT m a) where id = KleisliT (arr return) KleisliT f . KleisliT g = KleisliT (g &gt;&gt;&gt; abind f) class (Arrow a, Monad m) =&gt; ArrowMonad' a m where abind :: a b (m c) -&gt; a (m b) (m c) instance Monad m =&gt; ArrowMonad' (-&gt;) m where abind f x = x &gt;&gt;= f areturn :: ArrowMonad' a m =&gt; a b (m b) areturn = arr return Yet I'm not sure whether this is a good idea. There are monad transformers to build up monad stack, there are also arrow transformers to build up arrow stacks; monads can be turned into arrows, and (some) arrows into monads... 
I've been fiddling with docker a lot lately. I think the cleanest way to go is with two containers. One for building, and another for running. I'd pick one container as your base image, say busybox. from that base, install ghc and whatever dependencies you need. So the workflow would go sort of like, develop on mac. when you think it's right, run the busybox ghc -static to get a linux build. then from your base image create a new container that's just the base image + the built executable. I don't think there's a great way to avoid all of that mapping, but you can abstract it away with a makefile, a fabric script, or even just some good old shell. The key is keeping your mac ghc and the busybox ghc in sync. Don't let tons of state accumulate in your builder image, it shouldn't get corrupted. don't rerun the container, just run fresh instances of the image every time. if it turns out you need to add more dependencies in the build tool, make a new image and update your build script to point to that. share cabal files, but don't share sandboxes. put the sandbox in the container. 
The obvious question is whether you really want the `ArrowMonad`s to also be `Monad`s. That probably depends on what op is trying to do.
Thanks again to everyone who's contributed recently.
Are these like Promises (from ECMAScript 6)?
Ha, okay. Glad you liked the book. :)
Getting a clean image for running as you mention without all the build time dependencies is really nice. That happens at the end of our process too but not using Docker directly.
Am I slow? Shouldn't `doubleEven` handle `Zero`? EDIT: Fixed now. Good job on a quick response!
I forgot to include that case in the Haskell code example, but the implementation at http://sigil.place/post/0/doubleEven does handle the zero case (it has to, since Morte is total and forbids cheating). I'll fix the post as soon as I can. Edit: Fixed
`joinCat` should come for free if you have either an Arrow or Applicative or Profunctor
Nix has some great ideas that we're actively stealing to improve Cabal. There's a good reason it comes up so much.
Hear hear! It's even possible to build megalithic monad types which aren't transformer stacks [0] but instantiate all of your product of interfaces! You can even pretty directly translate most (all?) mtl-like constraints into a free monad and then instantiate it and interpret it however you like. Or even skip the free monad bit and just use a Finally Tagless interpretation style. Now the downside is that `mtl` constraints assume that `mtl` effects commute, but really they don't and since you cannot specify effect ordering producers of `mtl`-style operations just have to trust that consumers "get it". Or, alternatively, `SomeMtlConstraint m =&gt; m a` can be seen as an operation defined on a set of several similar effect stacks and it's merely a matter of choice what the user picks. With actual transformer technology you're forced to reify the ordering entirely and therefore cannot use or suffer from this ambiguity. You can even "peel" `mtl` layers off using transformers. import Control.Monad.Except import Control.Monad.State op :: (MonadExcept () m, MonadState Int m) =&gt; m () op = do x &lt;- get if x == 0 then throwError () else return () peeledOp :: MonadState Int m =&gt; m (Either () ()) peeledOp = runExceptT op Which, of course, *partially* orders your layers. [0] well, maybe they could be decomposed as such, but that's beside the point
Thanks for the pointers, everyone.
sure, the haskell program is trivial - just outputs "hello" string to stdio here is the javascript code (commented out is just another version that was suggested on the net, tried both of them without luck) process.env['PATH'] = process.env['PATH'] + ':' + process.env['LAMBDA_TASK_ROOT'] // var child_process = require('child_process'); // exports.handler = function(event, context) { // var proc = child_process.spawn('./lambda-test', [ JSON.stringify(event) ], { stdio: 'inherit' }); // proc.on('close', function(code) { // if(code !== 0) { // return context.done(new Error("Process exited with non-zero status code")); // } // context.done(null); // }); // } var exec = require('child_process').execFile; exports.handler = function(event, context) { exec('./lambda-test', function(err, data) { console.log(err) console.log(data.toString()); context.done(null, data.toString()); }); } 
unless i am missing something, this is exactly what i was trying to do - execute a binary executable file from node.js
I feel that MonadError is somewhat replaced by exceptions' MonadThrow and MonadCatch and instead of MonadIO I often find myself using transformers-base's MonadBase IO which seems like the generalization of that special case MonadIO. Are there any non-obvious downsides to the latter? (I know the former is different due to a specific error type vs. the SomeException mechanic in exceptions).
Is there any clue about what went wrong on `config.log`?
I would love to see more functions modeled over MonadIO; that is, `putStrLn :: MonadIO m =&gt; String -&gt; m ()` instead of `:: String -&gt; IO ()`. All the other classes work smoothly, but IO actions in stacks tend to involve cumbersome applications of `liftIO`. It breaks the pattern.
Yes. Although, my original plan was to first target LLVM (since you can compile that to asm.js) and only then later provide a more idiomatic Javascript binding. This wouldn't really replace GHCJS since it will never support the full Haskell language. The restricted subset of Haskell you would be programming in would be very limited.
Posting here since it seems I'm not the only one who finds this question interesting, yet nobody has answered. So, just in case someone enlightened enough is lurking around...
The way I've seen it, there's been so much research on networking and distributed systems regarding performance, we've lost sight on what a nice interface looks like. That there's historically been a bit of a cult of difficulty among systems programs and researchers hasn't helped either. Yes, there are real tradeoffs on fault-tolerance / throughput / latency / etc, but you kinda have to ignore all that to realistically implement the gold standard for ease-of-use. Once that gold standard is there, sure try to incorporate the flexibility back in so "real-world systems" can benefit. But trying to tackle the PL and systems problems all at once is even more ridiculous. My vision is something like this can work "from the top", while things like http://research.microsoft.com/en-us/um/people/nick/coqasm.pdf can work "from the bottom". Eventually they'll meet up and computers will live happily ever after. If for the first 40 years we put performance before everything else, let's try something a little different for the next 40.
https://registry.hub.docker.com/u/library/haskell provides ghc and cabal, at least.
Sounds awesome. Are any of the Haskell sieves out there [Atkin's](http://en.wikipedia.org/wiki/Sieve_of_Atkin)? (curious to how it would compare)
To anyone who is interested in *producing* video content about Haskell, I would recommend contacting someone at O'Reilly to see if they're interested in selling a Haskell-oriented video series. That is, as long as you're okay with it being made available for a fee; they pay pretty well. Functional programming has certainly been getting trendier in the greater industry, so I'm pretty sure they'd be interested in the content. If you need some contacts just send me a private message through reddit. Here's the programming video section, in case you're curious about the kind of stuff they sell: http://shop.oreilly.com/category/videos/programming.do
&gt; Things that will probably be difficult to encode or not be shared as a lowest-common-denominator feature are: [...] Laziness (not because it's hard, but rather because few languages support laziness) What exactly do you mean, then, when you say you're not going to support "laziness"? I do see in the [`morte` tutorial in Hackage](https://hackage.haskell.org/package/morte-1.2.0/docs/Morte-Tutorial.html) that you can encode corecursive types as streams with existentially-quantified seeds‚Äîwhich certainly covers many use cases of "laziness." So does for that matter the free-monad encoding of effectful computations illustrated in the tutorial. And if the language is total, well, evaluation order doesn't matter anyway, right?
&gt; In my not very scientific tests Try using http://hackage.haskell.org/package/criterion for more scientific performance measurement.
i meant that not all target languages that Morte would convert between would support laziness, but after reading your comment I realized that many languages support lazy generators, so I retract that statement. Encoding laziness via corecursive types is easy, as you already pointed out. 
I'm curious if there are any specific motivating cases for this.
I think we can go even lower-denominator than lazy generators. There's no shortage of languages today that have objects with callable methods. So a colist could be represented in a host language as an object with four methods: * `isEmpty()` and `isPair()` * `head()` and `tail()` (which return a `Maybe`/`Option` result) Since you're invoking code that can return objects, this can support a graph where no chain of `tail()` invocations ever reaches a object with `isEmpty() == True`. Another alternative in some languages could be the Visitor Pattern. For example, in Java: interface Colist&lt;A&gt; { &lt;R&gt; R accept(Visitor&lt;A, R&gt; visitor); interface Visitor&lt;A, R&gt; { R empty(); R pair(A head, CoList&lt;A&gt; tail); } } **EDIT:** Well, thinking about this again, I clearly only had the consumer case in mind. Encoding values is another story...
The rest of the advice on this thread is good. You want to have an image for *building* code an an image for *development*. These 2 can share a base image. The building image should not use a cabal sandbox because a cabal sandbox exposes more cabal bugs and a single application build does not need isolation. You should be able to install all the dependencies in parallel with: cabal install --only-dependencies foo/ bar/ baz/ You want this step to get cached. You can cache it in the docker image or by having the ~/.cabal in the docker image be from a mounted directory.
Yes! This is heavily inspired by my work at Twitter. I'll give a couple of specific examples of problems that I was interested in applying this to: **Example 1**: You're building a web API to filter and retrieve some data, so you end up defining your own bespoke query language using URL/form parameters or JSON. Wouldn't be nice if you could just directly transmit the function (in a standard format) that you want to use to filter the data? **Example 2**: You are writing a domain-specific tool that some sort of configuration file. Under the hood you have some logging logic that you wish to let users customize, so every time somebody requests a new way to log things you add a new field to your configuration file that only one person/team cares about, which leads to configuration file bloat. You could consolidate all these disposable fields into a single field where the user supplies their own logging logic. **Example 3**: You want an easy way to distribute a tool, but you don't want to ask your users to `curl` into `sh`. You need a solution that is typed (so you can restrict the set of permissible effects the installer can invoke, effectively sandboxing it) and normalizable (so any user can audit the normalized program with all indirection removed in order to view the exact sequence of steps it will perform). **Example 4**: You want to estimate the performance of analytics jobs so that you can reserve machine resources in advance and fail jobs that are too expensive at compile time. Wouldn't it be nice if you encode your analytics transformations in a language that was minimal, total, and normalizable so that it's highly amenable to static analysis to estimate computational cost? **Example 5**: In many distributed analytics pipelines that horizontally scale you need a way to transmit code across machines. For example, at Twitter we love using commutative monoids to parallelize and reorder analytics computations (see the `algebird`/`scalding` libraries), but for these monoids to be maximally effective the data structures and code must be transparently propagated across the entire pipeline (from the hadoop job, to the data storage format, to the query server, to the client). Anywhere that we can't propagate the code and data structures invariably becomes a performance or API bottleneck. It would be nice if the functions, types, and values derived from them could be seamlessly transmitted and distributed. Note that I have not proposed any of these internally just yet since `morte` does not yet interop with any languages. So far, the full extent of my internal communications has just been to keep saying "Wouldn't it be nice if we had X?" where X is one of Morte's features, mainly to mentally prime people for the idea. Some people at Twitter read my blog (and /r/haskell) so they know about my side projects, but I'm waiting until the full language interop is working before I start pressing for `morte`'s use internally.
Very cool! Also curious how you mentioned you're doing maths for the first time since high school. I'm in the same boat. What kind of path are you following and what's your goal? I sort of feel like I'm hitting on things I want to know (primarily linear algebra and statistics for AI work) but then end up jumping around because I could use a refresher/need to learn something else to understand what I'm doing. 
Keep an eye on the [NYC Haskell User's Group Youtube Channel](https://www.youtube.com/channel/UCzNYHE7Kj6pBqq5h8LG9Zcg/videos) (there are also some older talk videos [here](https://vimeo.com/channels/843620)). We usually have two talks at our monthly meetup, one intended for beginners and one intended for more advanced Haskellers and we try to post videos for all of them. I guess they probably wouldn't be classified as tutorials, but they might be useful for you. Also, I should mention that we're always looking for speakers for the NYC Haskell Meetup. If you're interested, shoot me a PM.
What if you have a dependency on an old version of a package? Do you have to compile every old version and keep the binaries on your computer for that case?
I've been meaning to look at Criterion for a while. It's in a very long list of Haskell related things I've been meaning to learn. Perhaps this could be a good motivating example.
i'm using halcyon with drone - the docker part is dictated by drone, but it does mean you get parallelism and minimal rebuilding.
Initially I had to do a lot of discrete mathematics to cover concepts ranging from sets to graphs to probability. From there I decided to take a path that focused on abstract algebra. As it turns out this was a difficult choice because most resources assumed that you had studied linear algebra first. Though I think that was largely unnecessary, as I'm finding linear algebra easier to understand now that I know the abstract stuff. The hardest part of self learning is finding exercises to do and then building up the motivation to do them. Without homework to hand in the only motivation is knowing that doing the work is the best way to understand it. I'm currently in the middle of Ring Theory. From there I'd like to go more abstract into Category Theory and then Type Theory. As the abstract stuff is the most fascinating to me. Eventually I'll visit linear algebra and calculus. I also want to get into AI as I think that is the direction all human-computer interaction is eventually going to move in. A little while ago I used Haskell to design a [neural network with short term memory](http://rickdzekman.com/thoughts/emulating-1-bit-register-neural-network-short-term-memory/). But my path to the mathematics side of AI is taking a detour through abstract concepts rather than discrete ones.
I read this whole article in a Swedish accident. Tell me am I racist? Is it sweeds? Or both?
It's quite easy to use. You could probably whip up a benchmark using it in ten minutes or so.
I think in that case you'd want to find a Stackage LTS version that is compatible with your project's package requirements. So you might need to keep more than one LTS sandbox inside ~/.stackage to accommodate your various projects.
You might want to compare your sieve to [NumberSieves](https://hackage.haskell.org/package/NumberSieves) package. It has the actual sieve that Melissa O'Neill wrote, which has several optimizations not described in the paper, and that are not implemented in the `primes` package. There's also a chunked array-based Sieve of Eratosthenes in the [NumberSieves repo](http://hub.darcs.net/lpsmith/NumberSieves/browse/Math/Sieve/Primes.hs) that was written a number of years ago by some of Brent Yorgey's students under my guidance. It sometimes outperforms Melissa O'Neill's code. All it really needs to be released is for someone to go through and optimize out the even numbers.
I'm in a similar position, coming from a biochem degree with pretty shoddy maths skills (basic calculus, and the most competent lecturers were prepared to teach us biological concepts in terms of some pretty algebra, which is what's sparked my interest), but with a fascination for the abstract side of things. I'd be very interested if you had the time to compile a post of how you've attacked this path, and what your favourite resources have been, which don't assume too much about your background.
Appreciate the very thorough response. What sort of materials did you use for the discrete mathematical study? 
What's the motivation/benefit of using a Docker environment for Haskell development? Doesn't it complicate everything compared to Docker-less Haskell dev-env?
In `putStrLn`'s case, is that actually useful? I can see writing your own functions that abstract away which particular monad is being used (so you can write pure unit tests for functions that usually do IO, for instance). But `putStrLn`? Not convinced.
Haven't you ever had to write "liftIO $ putStrLn ..." ?
On a related note, does GHC-compiled code still have a performance penalty to transformer stacks as opposed to one big monad type? Can such code not be "flattened" by GHC?
I really like the idea behind this. Mainly, it provides a way to serialize a function in a safe (and seemingly efficient!) way, which is something I've wanted several times. I also like the Planescape theme for the names.