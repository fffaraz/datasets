Is Haste actively maintained and worked on? When I was looking into multi-tier web development languages/frameworks, Eliom was active, with Haste having similar goals but appearing dead (no offense intended). Oh, congrats, by the way!
I'd recommend just using PureScript if your goal is to just get an app done but with nicer Haskell/Haskell-alike tools.
Fantastic work.
I like this, but am I right in thinking that this approach starts to break down if you want another type into the mix as well? Say if you wanted to combine `Component` `String` and `Int` - you would have to cover every possible combination in a separate instance?
Well, what do they do differently that we can learn from?
I chose Eliom for now since I need multi-tier support and really don't want to deal with the NPM'sphere. I treat JavaScript just like machine code. It's only when I have to debug that I look at it. I should note that I got into multi-tier via Ur/Web but that ones is inferior to Eliom. It's all good though, it's fine if some of my tools are in Haskell and other in OCaml. It's a nice family I can rely on with one child to do the dishes and the other one mowing the lawn.
Sounds like Selda might get promoted as a database EDSL independently before Haste will be reinvigorated (if that makes sense). See sibling comment regarding PureScript.
You know the reverse of Selda might be nice, too. Exposing your HTTP API as SQL endpoints maybe even via MySQL or Postgres protocol. Then you could use Selda to query Sqlizer which would access a Servant or Haste API in the back. Though getting this right and with the expected runtime behavior (SQL is like a whole little operating system with schedulers, optimizers, etc) might be hard. That said, I've seen such projects in Erlang [recently](http://erlang.org/pipermail/erlang-questions/2018-March/095064.html) so it's not a foreign or outdated concept, I suppose. I know GraphQL is all the rage right now but it doesn't solve the hard part and is just a formalized, somewhat typed, JSON-RPC+REST alternative if you squint a little. GraphQL is like reinventing the XML based web service model of a decade ago.
That's the intent, at least. Particularly since adopting a new database EDSL is a rather less momentous decision than adopting another Haskell compiler. Selda kind of came out of a "ugh, can't we do something _not_ related to browsers for once?!" moment. :)
found a fix, thanks 
How much of your impression is "this is fun to research" and how much is "this would be practical"?
&gt; neither the one that is downloaded by stack nor the one that is first on your PATH If you want one other than what's "first on your PATH" can't you simply prepend to your PATH when you run `stack`?
You should probably prepend, rather than appending, lest you trip over a ghc already in PATH...
Thanks, edited.
What was the fix?
This is correct, afaict. If you need to manage automatically installing C libs and headers, or system things other than the select few that stack does handle, then "use stack's docker or nix integration" is definitely the way to go. (And I feel that nix is a more principled approach to this than docker, though I need more experience with nix to be sure.)
+1, I'd prefer not deleting such posts, as they provoke insightful discussion and information sharing.
Why not make a tarball of the exact GHC you want, and put it on a web server somewhere?
Thanks for this post. Surely very comprehensive. I'm applying for GSOC this year. Would you be willing to give feedback for my proposal?
Do you know if anyone has looked into implementing Sparud's idea in GHC?
Stack's managing of GHC versions is *really* useful for test suites across multiple GHC versions, and for developing independent packages that happen to have older or new deps. It resolves a pretty major pain-point with Haskell development. I'm not sure it's "magic" either. It's a fairly mechanical task of detecting the user's OS, and installing the applicable GHC at the desired version. If I were to try out `cabal-install`'s `new-` commands (btw, will they always be `new-`?), I'd probably still use `stack` if only to manage the available ghc versions.
Assuming you have multiple ghc versions already installed (which is easy!) then you can just pass the `-w` flag to cabal to pick which one you want to use. There's nothing to it. Also, you can specify a compiler directly in a `cabal.project` or `cabal.project.local`: http://cabal.readthedocs.io/en/latest/nix-local-build.html#configuring-builds-with-cabal-project
Congrats on defending the thesis! I read your haste paper before, the internal workings are well described, it's valuable for any Haskell-to-X compiler writer.
&gt;Assuming you have multiple ghc versions already installed (which is easy!) This is what I disagree with; `stack` is the only tool I know that actually does make it *really* easy to install multiple ghc versions. It Just Works™ for whatever OS you're on, with the same command for each, with or without your OS having GHC installed at all.
I wonder why JIT'ing Haskell has *not* been that successful. It seems like partial applications could effectively be "recompiled" at runtime for Great Good, enabling wacky data structures like: ``` data Thing a = Thing { len :: !Int, index :: Int -&gt; a } ``` Where `snoc` and `cons` just wrap the existing `index` closure; a JIT could step in to "unwrap" the nested closures, whenever it decides it's necessary, into one nice and efficient lookup table.
Do you have any plans to video the talks? If so, I would be interested in them.
For most unix distributions you can use the native package manager. (or chocolatey for windows). There are also generic bindists from ghchq. None of these methods require building your own ghc. I'll grant the commands and tools vary by OS, but that doesn't seem weird to me -- the commands for installing anything vary by OS.
Yeah, we'll record them all and post on our [youtube channel](https://www.youtube.com/channel/UCCeiYYR2fCXarkfSqqFBwuA).
Great. Read few pages, extremely well written.
Thanks, the examples make it seem like this fits my use case! I'm having a great deal of trouble incorporating it into my Stack project, though (it's not on hackage, and adding the following code to my `stack.yaml` didn't seem to help: ```packages: - location: git: https://github.com/maxigit/sql-fragment.git commit: c2fda4bd4614349b78979349723b08c04b2a1fb9```) Sorry for the generic question, but do you have advice that would allow me to use your package?
With a little bit of tweaking, you can get multiple interpretations for PHOAS: {-# language GADTs #-} import Data.Functor.Const import Data.Functor.Identity data Phoas v a where Var :: v a -&gt; Phoas v a Integer :: Integer -&gt; Phoas v Integer Plus :: Phoas v Integer -&gt; Phoas v Integer -&gt; Phoas v Integer Let :: Phoas v a -&gt; (v a -&gt; Phoas v b) -&gt; Phoas v b eval :: Phoas Identity a -&gt; a eval expr = case expr of Var x -&gt; runIdentity x Let e f -&gt; eval (f $ Identity (eval e)) Integer n -&gt; n Plus a b -&gt; eval a + eval b eval' :: Phoas (Const String) a -&gt; String eval' = fst . go (("x" ++) . show &lt;$&gt; [1..]) where go :: [String] -&gt; Phoas (Const String) a -&gt; (String, [String]) go names (Integer n) = (show n, names) go names (Var a) = (getConst a, names) go (name:names) (Let e f) = let (eStr, names') = go names e (fStr, names'') = go names' (f . Const $ name) in ("let " ++ name ++ " = " ++ eStr ++ " in " ++ fStr, names'') go names (Plus a b) = let (aStr, names') = go names a (bStr, names'') = go names' b in (aStr ++ " + " ++ bStr, names'') treeE :: Integer -&gt; Phoas v Integer treeE 0 = Integer 1 treeE n = Let (treeE (n - 1)) $ \a -&gt; Var a `Plus` Var a If you have the constructor `Var :: a -&gt; Phoas a`, you're being to polymorphic to allow other interpretations. The only one you can write is `Phoas a -&gt; a`. If we disallow lifting any Haskell term, and instead be specific about the terms that we can represent, then you have more freedom in how to evaluate.
Distro packaged GHCs or Haskell stuff generally just hasn't been the best in my experience. At least on Ubuntu and Debian, the GHC version (not version*s* :-() lag pretty far behind the latest stable release. PPAs and equivalent might be available, but I don't *want* to depend on some untrusted PPA or equivalent, and cross my fingers wrt how it will interact with system packages. I realise "external" deps just aren't a goal for `cabal-install`, but GHC isn't really an external dep given the nature of modern-day Haskell code, and it doesn't seem like much of a leap to have `cabal-install` be able to download GHC bindists based on the `--with-compiler` flag or `cabal.project` option. 
It’s really a historical accident. The (numerical) class hierarchy was designed by Joe Fasel. He was inspired by the number types in Scheme. Joe later learned that Scheme had abandoned that version of the number types, because it was too complicated. 
Or `compFuncs = foldr (.) id`. 
yes, you're righr, I hesitated a little to give this more compact form in the final solution
&gt; why does Integral require Real? Because "Real", perversely, means "can create a Rational from". https://www.stackage.org/haddock/lts-9.14/base-4.9.1.0/Prelude.html#t:Real
Thank you! The observation you made on polymorphism made it click for me. I see you introduced higher-kind parameters, which is similar to what one sees in the HOCL paper (http://www.seas.upenn.edu/~acowley/papers/hocl.pdf), are the two techniques related?
Thanks, though I was hoping to find a technique rather than a library, to further my understanding more than anything else.
Why would Enum require numbers? Certainly you can Enum other things, like chars.
&gt; It Just Works™ for whatever OS you're on, with the same command for each, with or without your OS having GHC installed at all. You seem very convinced about this. Would you be willing to enter a bet, that if I can disprove this, you'll stop using Stack? ;-)
The easiest would probably to download it manually and the path in your stack file. I haven't upgraded the boundaries for a while so you might have some trouble using it with the latest LTS. Alternatively if you just want to evaluate it, u would recommend using the same LTS as the one used by SQL-fragment. If you need me too I'll try to push in on hackage and stackage.
as far as i'm concerned, a ppa or anything else is no more or less trusted than a stack installed binary. (and for ubuntu and debian the ghc bindist itself works just fine, and then all you need to trust is ghchq, which, if you don't, then there's not much you can do!)
This would be a huge hassle. First of all, the commands are `yesod devel` on localhost, and `yesod keter` on build servers, not `stack build`. Would it work for that, now and for all future versions of `yesod-bin`, guaranteed? Probably, but it would require testing and verification. Next, this would need to be set up on every single person's computer, plus every build server, plus every new computer of either type that comes online in the future. When a new GHC is released, rinse and repeat all of the above. Third, it's not the same GHC for different versions/revisions of the product; it's currently managed via the `resolver` field in `stack.yaml`. So add fragile logic to support that. By this time, we have lots of new moving parts to break, so add tons of support and debugging time when various very creative user and IT error happens. Why, why, why all this complexity? Why can't I just tell stack "use this pre-installed GHC that I already have"? What's so hard about that?
Are you planning on updating cabal syntax again ;-)
it's the opposite, integral requires enum.
My understanding is that the different classes represent different mathematical structures, and Integrals happen to behave like a number of these classes. This is the way I see it: * The `Num` class *essentially* represents [rings.](https://en.wikipedia.org/wiki/Ring_(mathematics)). This is a common way of defining how numbers should behave. Rings include integers, reals, complex numbers, polynomials, vectors, matrices etc. They are things that you can add, multiply and subtract. * The `Ord` class are things that can be ordered. Not every ring can be ordered, so this is a separate class. Complex numbers would be part of `Num` but not `Ord`. * `Enum` are a class that *essentially* represents things that follow [peano arithmetic](https://en.wikipedia.org/wiki/Peano_axioms). They define a successor function, and multiplication and addition are defined by that. Integers follow peano arithmetic and happen to be rings as well. Ratios do not follow peano arithmetic, but are still rings. Something like `Bool` have succesors and predecessors, but do not follow the ring axioms. Since any real numbers follow ring axioms and are ordered, they are elements of both `Num` and `Ord`, but not all real numbers have a well defined successor and predecessor function, so they are not all `Enum`. `Integral` numbers are real numbers which also have well defined successors and predecessors, so they belong to `Enum`. 
I’m addressing the part where the OP asks why Enum doesn’t require Num.
Just to clarify, you want to use a GHC that * isn't one of the standard variants or builds (otherwise you could use `ghc-variant` and `ghc-build`), * doesn't already have a tarball (otherwise you could use `setup-info`), * is installed on the system machine, but * isn't the system default (otherwise you could use `system-ghc`), is that correct?
Thank you for the link; I think I had only seen the original, OCaml-based paper by Oleg.
That's strange indeed, I've never seen this error before. Which LTS are you using ? I'll try to upgrade sql-fragment so it works with yours.
You might be interested in [crew](https://github.com/maxigit/crew). It basically allows you do map a command to another (and remap arguments if needed) depending on the directory you are in. In your case, you'll need to copy `crew` as `ghc` somewhere in your path so that calling `ghc` effectively call `crew` instead. Then when you call `ghc` (in fact `crew`). crews looks for a `.crew` file in your current directory or above (like a .git file) to deduce which command to finally call. In your case the crew file would look like ghc: cmd:path-to-correct/ghc And that's it. I initially wrote it to get the correct version of ghc-mod and then replace `cabal` with `stack` so that when I press `, c c` in Spacemacs, it uses stack instead of cabal to build the project.
Well, I'm happy to try the `new-foo` stuff in earnest. I would actually *rather* use `cabal-install` than `stack`; it has the more flexible, more egalitarian, approach. Still, Stack seems like the more practically valuable tool. I'm not just trying to flame -- if I thought `stack` was inherently superior then I wouldn't bother with this discussion. ^^^(however... ^^^on ^^^flame ^^^wars... ^^^I ^^^really, ^^^sincerely, ^^^think ^^^that ^^^Hackage ^^^metadata ^^^revisions ^^^are ^^^evil ^^^:-() Part of the reason I haven't already is just that the Cabal "Quickstart" is pretty huge. It is fantastic in how comprehensive it (seems to) be, but it does not really facilitate a *quick* start. Its "cookbook" section -- which will presumably *eventually* be filled with common examples -- is one small example. I am also put off by how frequently the guide recommends not using a stable cabal-install release, but instead using HEAD, because of various bugs. (they all seem like bugs that have been fixed quite a while ago, but the guide has not been updated from when using HEAD was the only recourse a user had.) I don't think you *can* disprove that stack hasn't worked for me, though. I mean, it has; for what I've used it for -- multi-package projects, libaries, sometimes with deps pulled from git, gloriously simple CI runners (this is where easily building/running something from multiple GHC versions comes is most handy IMO), on linux, mac, and windows, and on multiple ghc versions -- stack is a blessing. I have just had far fewer issues with Stack than with cabal-install, when I was using that (i.e., up to ~2015). So, kudos to FP Complete. The workflow for stack is also quite attractive. If you want to update your deps without necessarily having intimate knowledge of the version history of them all, update your resolver, fix the upper (and maybe lower) bound in your cabal file (if any), and adjust to whatever breaking changes were introduced. I have no idea if there's a straightforward way of doing this with cabal-install. I guess just bump the index state forward and try to `new-build` again? But then, if we are back to using Hackage snapshots, I don't know why I wouldn't just keep using Stack. Maybe someday somebody will have made a universal Dhall-based Nix-esque (but less invasive, with a simpler UX, and more cross-platform) package manager, capable of managing, building, running etc. Haskell packages *and* external deps (including GHC). This is probably about as easy as it sounds. I can at least dream.
Is this just a very a complicated way of saying that `bar =&lt;&lt; foo x` makes the direction of application point in the same direction for both functions?
As there is a question about doing this with generics-sop below the linked post, here's one possibility: https://gist.github.com/kosmikus/76ab73d92fce76091ff7b2401f271649 
I think it would be quite useful, if you could get decent performance out of it. That's a big "if" though, depending on what sort of tasks one would use it for.
**Editor war** Editor war is the common name for the rivalry between users of the Emacs and vi (usually Vim) text editors. The rivalry has become a lasting part of hacker culture and the free software community. The Emacs vs vi debate was one of the original "holy wars" conducted on Usenet groups, with many flame wars fought between those insisting that their editor of choice is the paragon of editing perfection, and insulting the other, since at least 1985. Related battles have been fought over operating systems, programming languages, version control systems, and even source code indent style. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
There is another aspect of partial functions which I feel the other answers are missing. Partial application can replace an entire class of OOP idioms, namely the one where you want to share precalculated values across multiple function calls: Let's say you want to test whether a [ray intersects a sphere], where the sphere and the origin of the ray are constants and only the ray direction changes. With partial application you could write (if rayDir is a unit vector) intersects :: Sphere -&gt; Vec3 -&gt; Vec3 -&gt; Bool intersects sphere rayOrigin = let oc = rayOrigin - centerOf sphere ra = radiusOf sphere det1 = ra * ra - oc `dot` oc f rayDir = (rayDir `dot` oc + det1) &gt;= 0 in f In most mainstream imperative languages you would use classes or structs to store the intemediate values, then use two separate functions to first create the precomputed values and then apply them later. On the other hand, partial functions make this idiom really cheap. Such partial functions can be effectively seen as objects that bundle local state with functions.
I suspect it's different for different companies. In case of Tsuru you don't need to know any specific library or package, but only how to write good Haskell code and be able to solve problems.
As you can see in my un-abridged comment, I am not saying that every community save Haskell avoids this. What I am saying is that this avoidable, as exemplified by other communities. &gt; **In many language communities**, having several tools fitting the same need is healthy and doesn't create problems. Also editor wars were not infighting; I have no problem having some banter with F# or Ocaml users. It doesn't sour either community.
I have no go-to solution, and if I had I would have no standing to offer it. However, this situation reminds me of a conflict I had witnessed in academia, where the parties were willing to destroy things for no gain, just to make a point. I don't remind seeing this anywhere else.
Thanks a lot for your comment! Now I understand things better.
&gt; at least make an argument. I didn't downvote him. I personally think both stack and cabal have their advantages. But there is no point in arguing once we get to "I bet I can show how X is broken in some way". I'm all for having an argument. But of course he can show that. There is nothing to argue about that. Because when it comes to "is X broken in some way" both are terrible. It's just a product of the complexity of the projects. It doesn't really say much about the value one gets out of them.
Yes, it has -- the [two column layout](https://www.reddit.com/r/haskell/comments/84zvxr/two_column_layout_for_hackage_package_pages/) is now live :)
short answer: yes More information will provided later today; stay tuned!
If I understand correctly, the "ideal" representation for a real number is something like `Natural -&gt; Rational`, where the argument specifies the number of bits of precision requested. Change My View. :]
Hi All, I wrapped up a first working version of a little project to learn Haskell. You can add your portfolio to a sqlite db, and then get actual values and percentages of your cryptos. I tried writing the best Haskell I have, and I imagine you guys may have some suggestions for improvements. I included some different libraries, because they seemed useful or because it seemed like fun (Yay colors!) . If you have some time, check it out and et me know what you think. Thx!
That's what bugs me about GraphQL. There's only adapters and the interesting/hard part is missing. Similarly, speaking SQL queries or MySQL wire protocol is one thing, but actually doing something sensible and efficient means re-implementing parts of an RDBMS. And worst of all, you cannot really implement too much of the scheduler or optimizer in generic fashion anyway. It's data dependent after all. It's been funny watching the NoSQL wave come and go where users had to reinvent DBMS features on a level that's way too high/abstract. I mean, one by one they started to add pieces of a regular RDBMS since a simple K/V store has limited applicability.
The time is in seconds, each small tick is 10 ms and each big tick is 100 ms. I need to fix that to make it better readable (in milliseconds) and also put a label for units.
Nice! What is the fundamental difference between `streamly` and the other options?
Another cute possibility is `getEndo . foldMap Endo`.
Is it running on the GPU?
Oh, don't worry! I didn't mean you suggested anything; I was just complementing what you said: the code was an attempt to encode effect handlers (changing order and stuff), aaand it did work. :) I haven't tried with effects that don't commute yet. I'll probably play with that this week.
Huh, so I assume that as long as I'm not explicitly stacking the same effect twice this won't lead to any ambiguity. Also, thanks for the `NoMonomorphismRestriction` tip!
Ugh I did a terrible job trying to explain my point - you are of course correct and I understand why you had to use a concrete type. What I was trying to get at was that if you want to handle types like `a` and `m a` at the same time then you can run into difficulties with type classes like this class Foo a b where foo :: String -&gt; a b because to write instances you need to be able to refer to some type of kind `* -&gt; *`, so it is impossible to write an implementation that resolves down to `foo :: String -&gt; b`. So my thinking was that distinguishing at the kind level could help, because even though `(* -&gt; *) * == *` we use type classes to separate the `(* -&gt; *) and `*` parts. Looked at from a very high level, both our solutions address this issue. Your solution simply removes the necessity for targeting a `* -&gt; *` by providing an existential concrete type, which means that you have already said everything you need to say about the `* -&gt; *` in `MonadicT`. My solution does the opposite - instead of wrapping the unspecific `m :: * -&gt; *` I wrap all pure values in a different `* -&gt; *` (`Identity`). But from thinking about this I just realised that there is a much simpler solution that I think does exactly what /u/chshersh wanted, I've posted it separately.
&gt; like crash the extruder into the bed A well... Guess relying on proven solutions is a good idea then. 
I think the issue for myself (maybe others but thats for them to say) is that ship has sailed. I have been using stack since it was released and cabal new-build would need to offer something to compensate for switching costs. I personally don't see any compelling reason to switch today. Prior to stack we maintained our own build system that was somewhat conceptually similar, so the introduction of stack was of great value to us. Now maybe there are good reasons to switch away, time will tell. 
I think it’s a more idiomatic approach to think about the fact that endomorphisms with composition form a Monoid than finding a recursive definition and translating it into foldr form.
Is there a paper, talk, blog post, etc. where I can read in more detail about the complications Scheme encountered? 
Module list layout seems to broken for some packages (in 3 out of 3 browsers I tried). The effect is a big gap between two modules. For example: http://hackage.haskell.org/package/proto-lens
Nice, looking forward to it!
This is an old post by Gwern, but I just stumbled on it and thought it might be relevant given GSoC applications happening now. Maybe try emailing him to see if he has feedback for you?
Why is conduit so slow? o_0
Ah, no, you just need two, one for T and one for MonadicT.
Hello. That said, 'Streamly' is looking 'Streamly' good. I wonder what price is being paid for performance. Also, is there a particular reason to results for concat to be missing for both 'Streamly' and 'Streaming'?
That's one of many cases. I am talking about *any* case where you have one or more specific versions of GHC (e.g. 8.4.1) already installed somewhere, and you want stack to use them. There are many reasons that happens. Here are a few sample use cases: * A build server, used by many developers for multiple versions of GHC. Neither individual downloads nor "system-ghc" will work here. The symlink hack worked fine (so far). * Custom GHC builds. Why am I forced by stack to create a tarball and have it installed again? * WSL, currently not yet supported by stack. Before, I used custom GHC builds. Now I can use the PPA - but the only way to get stack to use a GHC from a PPA is via the symlink hack.
hi
All thanks to our speakers :)
Thanks. I'll do that!
Conduit is for good *space use* with *I/O operations*. This benchmark is testing *time use* with *pure code*.
To be fair conduit and pipes had more or less similar performance until the conduit 1.3.0 release. I guess while making the changes for 1.3.0 performance part may have got overlooked, some work may be needed to get it at par again. This is one benefit of such automated benchmarks, you get to know immediately.
That's not correct, if you read the README carefully and go through the benchmarking code, it is specifically testing for IO and not for pure code.
There is no bargain in my knowledge, the interface is very intuitive, just an extension of lists, in fact it is a list transformer. On the contrary, it has good performance despite providing composable concurrency which ideally should have additional overhead because some additional state needs to be passed for concurrent composition. It may be that it is just optimized well and the type is amenable to better optimization. BTW, it is implemented in CPS style, earlier when we used direct style performance was good but not predictable. There are still a few optimizations, including a small change in the type that we have are considering and it might do a tad better once we implement those.
If you ignore everything else I said, sure.
Ugh, I see it. Thanks. The problem is when module names run too long, the right-hand bar can overlap them before the screen gets small enough to force things into the one-col layout. Not sure the right approach to fix this in css. Perhaps just letting module links break? Suggestions welcome!
Thanks! I was using the latest LTS (11.1). 
Streamly does not have a concat operation yet and streaming somehow hung indefinitely when using that operation, need to look into that.
I don't think the use cases I outlined in [this comment](https://www.reddit.com/r/haskell/comments/85abjj/why_does_stack_install_its_own_version_of_ghc_and/dvxzp3a/) are special cases at all - I imagine these are widespread needs. For example, on a team build server, the number of GHC's that stack installs by default is `O(n^2)`. Using the system-ghc method, you need to re-invent part of stack to parse `stack.yaml` and figure out which GHC to use. A team build server is not a "special case". The feature I am asking for is much simpler than what you are describing. I want something like this: stack setup ghc-8.4.1 --ghc-path /opt/ghc/8.4.1 or stack setup ghc-8.4.1 --ghc-path /opt/ghc/8.4.1 --global The `--ghc-path` option makes a symlink instead of downloading and installing a compiler. The `--global` option adds support for the `/etc/stack/programs` directory, which is searched for a matching compiler after `~/.stack/programs`. That's it. The automatic compiler download feature of stack is really great for beginners, and also works OK for non-beginners in simple cases. But if I want to use my own installed compiler, why does stack make it so hard?
`Real` means "on the real line", as opposed to complex.
I want to run `make` and have it just work on Windows, Mac, and Linux. Is there automation that does this reliably somewhere using the naming scheme you're referencing here?
If `streaming` and `streamly` perform nearly the same and have similar APIs, why not just improve `streaming` to expand that ecosystem instead of splitting it? 
This was a fun exercise, thanks for sharing it.
Let's start from the right. `replicate (n-1) tail` gives you a list of `[tail, tail, ... tail]`, a list of functions. The `foldr` starts with xs and applies each of those `tail`s to xs. Remember that `($) f x = f x`, so `($)` is just function application. So at this point, you've applied `tail` n-1 times to `xs` to get a shorter list. Now, you take the `head` of that, and you get the nth element.
This looks really goofy on any package with a lot of versions or dependencies. And if the package has a real README it changes formatting when it drops below the metadata column which looks silly. What was wrong with the previous design? This feels weird and cramped.
Perfection! Thanks, I get it now. Any particular reason why the $ is in parentheses though?
I think it would be excellent for data manipulation, but I'm not so sure the type system would offer great benefits over SQL, R, Matlab, or Julia for the actual analysis bit. At some point you have your data cleaned and well formed and at that point additional friction/safety around specifying the equations you want to evaluate might not pay off. 
Yeah, this was assuming the absence of dependent types. Even without them you could do something like `Integer, [Boolean]`. Basically a float with possibly unbounded precision. Performance would be in the shitter though.
What I found myself wishing for while reading [Initial encoding with classy `Prism`s - another approach](http://qfpl.io/posts/backpack-for-initial-and-final-encodings/#initial-encoding-with-classy-prisms---another-approach) was a way put `pattern`s in type classes: class HasBase term where pattern Lit :: Int -&gt; term pattern Add :: term -&gt; term -&gt; term class HasMul term where pattern Mul :: term -&gt; term -&gt; term 
One thing worth checking out is that the http://hackage.haskell.org/packages/browse table is much faster to load now, so it should be pretty usable. It gives you in-browser filtering and sorting of the whole index!
Is Haskell the language of the future?
I mean, did you try using LTS 2.22 ? (It's a bit old but that will give you an idea)
http://www.datahaskell.org/ has some resources for doing data science, including a long list of useful packages. Tikhon Jelvis (https://www.quora.com/profile/Tikhon-Jelvis/answers) does Haskell Data Science at Target, I think, and he does a bunch of writing on Quora, so he would be someone better to ask.
&gt; Curious about what you mean by "if the package has a real description it changes formatting when it drops below the metadata column which looks silly". Look at `lens`, when the description gets below the metadata column, it switches to taking up the whole space. &gt; I think the sentiment is that lots of people care much less about the properties Sure. &gt; and are happy to have it on the side -- as a whole we also get higher information density. I don't really see how that follows. Before you could just mindlessly scroll past the header whereas now you can't ignore it because it's directly affecting the formatting of the page. &gt; I also took a look at http://crates.io and https://www.npmjs.com/ which do a similar two-col thing, so it seems to be a pretty common approach these days. Just because other people are doing something doesn't automatically mean it's better or even a good idea, especially in the web space.
Are you trying to imply the causation is that only inexperienced developers choose those technologies?
&gt;To be fair conduit and pipes had more or less similar performance until the conduit 1.3.0 release. I guess while making the changes for 1.3.0 performance part may have got overlooked, some work may be needed to get it at par again. This is one benefit of such automated benchmarks, you get to know immediately. Maybe add a small footnote about this? Seeing this on your benchmarks would only inspire more confidence. Very in spirit of the principle of charity.
There's a flag for that. `-fno-omit-yields` https://ghc.haskell.org/trac/ghc/ticket/367
This might be useful if you know about Frames: https://github.com/codygman/frames-example-cdi/blob/master/README.md https://github.com/codygman/frames-credit-card-trans-demo
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [codygman/frames-example-cdi/.../**README.md** (master → 62ffbbd)](https://github.com/codygman/frames-example-cdi/blob/62ffbbd1980ae918b46a1f0bd03365966224bfbe/README.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvya3bx.)
Surely, the purpose of these benchmarks is to be fair and correct and to help the authors of these libraries to improve them, the way they helped me improve streamly. I can even put a link to the results with older version of conduit as well.
&gt; Look at lens, when the description gets below the metadata column, it switches to taking up the whole space. Ah, gotcha. That's on purpose, and seems fine. Essentially the properties data becomes an "infobox". Anyway, the input is appreciated, and as you can see by the discussion on the various redesign tickets, there's a lot of openness to changes and feedback, but on the whole, in this case, I think people tend to prefer the new two-col approach, so I don't see it changing back -- though again, patching up weirdnesses and improving usability is definitely on the table...
Basically, it turns an infix operator into a prefix function and lets you do anything you can do with a normal function.
Once `new-build` becomes real-build is really the time to give it a shot, but I've been using it pervasively for the past period, and have found a lot of conveniences. It basically always just works for me -- i.e. it gets out of the way and lets me ignore it for the most part, as a build tool should! I don't have to worry about picking a resolver, I can upgrade deps on an ad-hoc basis, and i get sharing between my different projects, even when their deps don't match exactly, whenever possible. Also the `project` file lets me vendor deps when necessary very easily or keep all my funny project-specific flags (like extra lib dirs) in one place. The only drawbacks I know of are things that are being worked on -- finishing out the suite of commands so we have `new-install` for exes, improving `new-haddock` and `new-test` to some degree, and having weird paths to generated binary artifacts for the time being.
I added a note about conduit performance and also a link to the older results. I wanted to have the latest version of every package and realizing that conduit has a new version out, upgraded it only yesterday. Here is the link to the summary of individual ops with previous version of conduit https://github.com/composewell/streaming-benchmarks/blob/269ac94fc59c76267b89b07690d9ea290096b95b/charts/AllOperationsataGlance.svg .
nix is another tool that can effortlessly install multiple GHC versions. The command is something like nix-env -iA haskell.compiler.ghc841 # ghcjs, ghcHEAD, etc 
No, it's the language of the unemployed (sadly)
I missed the space vs time argument earlier, I think that is not a viable argument. All streaming libraries allow you to do IO in constant space, that is not a feature of conduit implementation, it's a feature of streaming design per se. If the argument is about the heap space allocated during these operations even then it is not correct. The allocations are usually proportional to the time. But to be sure, I have measured the allocations as well though not shown in these graphs, and they perfectly agree with the time graphs. I could have published those as well, just did not think it is worth repeating.
Given that streaming libraries are *entirely* about doing stuff in constant space, it'd be good to have those graphs as well :)
An enormous number of things is/were wrong with Hackage. I'm super-happy that things are apparently changing. You should be happy too. A short list of what was wrong: - no keyboard support - needless page reloads for most things that is done using async json loading these days. - no quick way to find similar packages - no information about stackage availability - no way to see the contents without scrolling - all in all extremely inefficient display of information - the browse page would take forever to load, because it loaded everything. - the "Index" usually displays a blank page(!) with all the letters in the english language. I could go on and on...
&gt; I don't really see how that follows. Before you could just mindlessly scroll past the header whereas now you can't ignore it because it's directly affecting the formatting of the page. Why should I scroll? If 95% of readers mindlessly scroll, then keeping that layout is worse than killing a kitten.
Perhaps a corollary to your point is that, while I admit I use SO for Haskell a lot, usually it's in the form of reading very in-depth, old answers to more general, conceptual questions that tend to not go out of fashion and become obsolete--which also seem to be more prevalent for Haskell on SO--as opposed to, say, CSS/styling questions (which I am also unfortunately quite familiar with as a web dev), which you'll often see a updates to in the answers, sometimes multiple as browsers change, new standards are added or old ones deprecated, etc. So I've been on SO for Haskell a good number of times in the past month--I have a bunch of Haskell-specific SO tabs open right now in fact ([for example, the first one I found in my tabs](https://stackoverflow.com/a/11048004))--but I have never posted a new question on Haskell in SO.
I used to be quite active on SO for Haskell. I stopped because I noticed that there were less and less genuinely interesting questions and more questions from students trying to get me to do their homework... Then again, maybe I'm the one changing. :)
Nice job! A complaint: there is really no point at all in sorting by: - name - description - tags - maintainer But sorting by these is nice: - downloads - rating - last upload
I would like to know if this means : "use X language" or "use X language in a professional context"
Also would love to see the GC allocation benchmarks. 
Thanks for the clarification
The naming is confusing—I wouldn't fault anyone for thinking Enum is related to Num like email is related to mail :).
I have fixed the units and added the unit labels.
Why should you be able to create a rational from such a thing?
Each chart absolutely *needs* to have a "more is better" or "less is better" indicator on the actual chart graphic. (Even if "seconds" is usually a unit where less is better.)
Yes, I had that in mind but just kept ignoring it :-)
Have you considered implementing the word type as an Int? To me it seems like a word is just a base 3 integer.
Hey, one problem with the new version is it looks like deprecated package versions are no longer marked? Perhaps something like: .deprecated { text-decoration: line-through }
I like your suggested command invocation, and have made a feature request on the stack repo. https://github.com/commercialhaskell/stack/issues/3931 Please do chime in there with any details I may have missed.
&gt; I don't think the use cases I outlined in this comment are special cases at all What I meant when I said yours is a "special case" is that you are OK with tying `ghc-8.4.1` to exactly one ghc executable, whereas the "general case" would be that people might have multiple variants of `ghc-8.4.1` that they would like to use.
How big of a project are people realistic managing? Mine involves 150ish packages in our source tree plus a couple of hundred more that get pulled in as dependencies. I admit that I have become very, very dependent on stackage to put together a set of packages that work together. I guess I see this mostly as just stack but different, maybe some stuff better, maybe some stuff worse. Maybe a difference between you and me is that I really don't want to get into fine grained dependency management any more than I need to. I do also need the dependencies to work across the codebase, so I don't have any of this package wants version x and another wants version y.
A typical situation...
I imagine in a monorepo-type situation of internal packages, I'd tend to omit all upper-bounds unless necessary, and just let those flow through the dependency graph? If I wanted reproducibility for a particular release, I could get that just using `new-freeze`. I don't see that as requiring anything fine-grained in dep management - quite the opposite. The pvp is fundamentally for released libraries -- having that as a base means that consumer code can iterate really rapidly and safely in my experience.
Sorry, I only have this from Joe Fasel.
I'm not 100% sure this is really all that necessary. The first piece of code you gave is an anti-pattern in my opinion. The `reflection` package is kind of cool, but I've never used it in production and really never would. The amount of danger it uses is too much (in my opinion). As it is, you can easily reify any arbitrary type class by just capturing its dictionary in GADT: data Dict (c :: Constraint) where Dict :: c =&gt; Dict c As it stands, you can even use this with the `reflection` package (although, I personally think you shouldn't for anything serious). Shouldn't this be enough, or am I missing something obvious?
That seems to be the implication. The data show that the majority of programmers have five or less years experience, so it is safe to say otherwise. Haskellers certainly know that the language has a rather steep learning curve and the concept of the monad is well-known as a blocking point for learning the language. 
I'm an unemployed Haskeller, too (although I use other languages professionally, Haskell is definitely my favorite). The data show that most programmers have five or fewer years of experience. I wonder what happens to people after five years... do they go into different fields or leave the software profession? 
My pleasure! :)
This seems obvious to me, but perhaps I'm missing something: I feel a lot of Haskell developers come from academia
I'm about to update the blog post with this information, but I thought I'd share it with you while it is hot off the press: ``` benchmarking add and mul - big/initial (no cps) time 14.38 μs (14.32 μs .. 14.45 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 14.35 μs (14.32 μs .. 14.40 μs) std dev 141.4 ns (103.5 ns .. 200.9 ns) benchmarking add and mul - big/initial (compose) time 74.97 ns (74.70 ns .. 75.29 ns) 1.000 R² (1.000 R² .. 1.000 R²) mean 74.97 ns (74.81 ns .. 75.22 ns) std dev 686.0 ps (500.7 ps .. 1.073 ns) benchmarking add and mul - big/initial (fuse) time 66.62 ns (65.17 ns .. 68.11 ns) 0.997 R² (0.995 R² .. 0.998 R²) mean 66.21 ns (64.96 ns .. 67.28 ns) std dev 3.868 ns (3.343 ns .. 4.731 ns) variance introduced by outliers: 77% (severely inflated) benchmarking add and mul - big/initial-bp time 53.63 ns (53.31 ns .. 53.96 ns) 1.000 R² (1.000 R² .. 1.000 R²) mean 53.77 ns (53.55 ns .. 54.01 ns) std dev 801.1 ps (687.8 ps .. 985.0 ps) variance introduced by outliers: 18% (moderately inflated) ``` There's the huge difference I was talking about :)
It's not THAT bleak. I've spent the past month writing Haskell professionally, replacing a Node server that we've been aching to get rid of for years. Assuming all goes well, we should have a deployment migrating nearly all of our `GET` API early next week! (Proper company-signed blog to follow.)
It's a anti-pattern when you're dealing with a type class, what I'm trying to say is that Haskellers often use type classes to represent interfaces – definitions that should have multiple implementations. Reifying an arbitrary instance doesn't help here, because multiple instances for the same can't be defined currently. Let's say I have an mtl-style class for communicating with DB, instead of defining RedisDBT, SQLDBT, MonadStateDBT, STMTVarDBT newtype transformers for each possible implementation, I'd like to be able to define local instances that work for any MonadIO without having to add yet another transformer on the stack, but also without passing a [Handle](https://jaspervdj.be/posts/2018-03-08-handle-pattern.html) in the code explicitly. In pseudo-haskell: class Store m where store :: String -&gt; m () retrieve :: m String -- | Global coherent instance for State-like monads instance (Monad m, MonadState String m) =&gt; Store m where store = put retrieve = get -- | Local dynamic instance for IO-like monads mkRedisStore :: MonadIO m =&gt; RedisHandle -&gt; Store m mkRedisStore h = Store { store = \v -&gt; redisSet h "1key" v , retrieve = redisGet h "1key" } -- | we use -&gt; instead of =&gt; to signify that we don't care about coherency in this case addPlusOne :: MonadIO m =&gt; Store m -&gt; m () addPlusOne store = flip unsafeLocal store $ do str &lt;- retrieve let newStr = str ++ " + 1" liftIO $ putStrLn newStr store newStr program :: MonadIO m =&gt; m () program = do putStrLn "Run over redis DB?" answer &lt;- getLine if answer == "y" then do handle &lt;- connectToRedis let redisStore = mkRedisStore handle forever $ addPlusOne rediStore else do runStateT (addPlusOne $ instanceOf @(StateT String m)) "1" 
Apply to IOHK! We're hiring a bunch of Haskellers.
I think the point is that you shouldn't be using a type class in this case, at all. I realize people do this, and I'm not sure why.
This is great!
&gt; The (numerical) class hierarchy was designed by Joe Fasel. He was inspired by the number types in Scheme. Joe later learned that Scheme had abandoned that version of the number types, because it was too complicated. This honestly reads like a line straight out of Hitchhiker's Guide. It's sad we have so many warts in our ecosystem. We should work on freezing them off. &lt;/captain-obvious&gt;
You dropped this \ *** ^^&amp;#32;To&amp;#32;prevent&amp;#32;anymore&amp;#32;lost&amp;#32;limbs&amp;#32;throughout&amp;#32;Reddit,&amp;#32;correctly&amp;#32;escape&amp;#32;the&amp;#32;arms&amp;#32;and&amp;#32;shoulders&amp;#32;by&amp;#32;typing&amp;#32;the&amp;#32;shrug&amp;#32;as&amp;#32;`¯\\\_(ツ)_/¯`&amp;#32;or&amp;#32;`¯\\\_(ツ)\_/¯` [^^Click&amp;#32;here&amp;#32;to&amp;#32;see&amp;#32;why&amp;#32;this&amp;#32;is&amp;#32;necessary](https://np.reddit.com/r/OutOfTheLoop/comments/3fbrg3/is_there_a_reason_why_the_arm_is_always_missing/ctn5gbf/)
Try introducing `letrec` or recursion into your DSL, and proper sharing suddenly becomes non-trivial: it would require knot-tying.
Nice! I'm excited to go to my first Haskell conf!
Good question. There have been releases since then (https://hackage.haskell.org/package/distributed-process) but i think nobody got around to updating the website. Maybe ping the maintainers to remind them?
Also, no reverse dependency info
Looks great. Would love to see io-streams compared as well if someone has the time. 
We still have a PR for that pending (it's the part of https://github.com/haskell/hackage-server/pull/551 that hasn't been deployed yet -- it really should be separated out). It also does similarity, I think? Anyway, it wasn't merged because while it had a better space profile than the earlier version, it was still rough going on the full hackage data set. It really needs someone to roll up their sleeves and sort it out to push it the last way through. Volunteers welcome!
Have you tried `cabal new-build`? That's what I've been using with no problems :-) (You can hit people up for advice and help on the #hackage channel on freenode as well, we'll be happy to help). Also -- if your PR is just stylesheet changes, and template changes, you could try to hack it up directly, then code the changes to those two files "blind" -- if you get 95% there I could push it the last way through.
If you are familiar with io-streams then it should not take much time to add it. Just take a look at the Benchmarks.hs file and you will know how to add it. Charts are auto generated, you just need to name the package in Charts.hs.
Yeah they are. That paper references this paper: http://adam.chlipala.net/papers/PhoasICFP08/PhoasICFP08.pdf which is what I had in mind when writing this.
Do you accept remote workers? 
&gt; How come `Enum` doesn't require `Num`? `Char`, `Bool`, `Ordering`, etc. are all `Enum` instances and have nothing to do with numbers. &gt; Why does `Real` require `Ord`? `toRational` implies a natural ordering. &gt; Why does Integral require Real? `toInteger` implies how to define `toRational`. The numeric hierarchy in Haskell really doesn't have a great mathematical origin story, basically folks seem to have looked around at the primitive types they had and made the coarsest distinctions they could that let them distinguish between them. If a distinction was finer-grained than was required for what became `base`, or would require more advanced type systems than were available in Haskell 98 then it didn't go into the library.
Because the people who wrote that part of the standard were by-and-large not mathematicians, I'd guess.
This isn't actually an optimal encoding in a lazy language. (You can get access to what you requested today using `CReal`, though!) A better encoding actually uses nested (contractive) linear fractional transformations to first pick out one of several overlapping regions on the real number circle, and then zooms in using redundant digit matrices. Why is this better? If I need one more bit of information I can preserve all of the work done thus far and continue to pump the lazy stream of digits. Unfortunately, decidability means that you need to use redundant schemes rather than something more obvious, like, say, continued fractions. The nested linear fractional transformation encoding has the benefit that it can require far far less precision on most of the intermediate results, while the conservative representation may have to pump a ridiculous level of detail out of intermediate computations to meet the end-goal precision requirements. The linear fractional transformation view is probably best elaborated upon in [Peter Pott's thesis](http://peterpotts.com/pdf%20files/phd.pdf). I have some improvements upon it that can handle more efficiently encoding some cases, (e.g. encoding `pi` without leaking the same amount of memory like Peter's version) but I probably reached a bit too far and haven't figured out how to handle all of the cases I want as efficiently as I want.
&gt; Haskellers often use type classes to represent interfaces – definitions that should have multiple implementations But this is not actually a good idea. It's an especially bad approach when there are multiple valid implementations for a given type, and that's the situation where you know that a type class is not appropriate. I personally favor records over type classes because in most cases I find that they are much better. For instance, although there is no global instance collection, I view that as a plus: it makes it clear where concrete "instances" can come from because they are provided explicitly as arguments. Such records can of course be decomposed and manipulated before being used, so that approach is also more flexible. This isn't to say that type classes are never a good idea; certainly they are, particularly in situations where there is *literally one and only one way to define an operation for a specific type*, but when it comes to the idea of "interfaces" and the expectation that runtime implementation choices will vary, I think a record approach is much better.
So, where can I send my application? Thanks. 
Automation results in eliminating outdated tools and replacing them with new ones. Just like modern weaponry replaced bows and swords and modern transportation replaced horses, advances in AI will lead to machines talking to humans directly rather than us typing instructions to perform some task.
Can probably get some small wins just by with strict fields and `-funbox-small-strict-fields` (or UNPACK pragmas) in `Streamly.Core` types. It's surprising to me that the CPS style has *more* predictable performance than the direct style. Aren't you really at the mercy of the GHC inliner with the CPS style? If something fails to inline (or just cannot inline) you are basically building up a linked-list of closures.
Hindsight is 20-20. It’s really difficult to design a good library that satisfies all constraints. 
I think it's pretty unrealistic. To do something like that, not only do you need AI that can speak and understand complex thoughts in languages, you need an AI that does it the way that humans do. What's also important is that we're hitting the cap for processor power.
Not only that, but stack's approach works on many platforms while PPA tie you to specific linux distro (Ubuntu).
I really dislike the new colour scheme. :-(
I updated my original comment
For now, wrap added.
It turns out there is no need for that env I postulated, but here you go: #! /usr/bin/env stack -- stack --resolver lts-11.1 script --package bound --package deriving-compat {-# LANGUAGE DeriveFoldable #-} {-# LANGUAGE DeriveFunctor #-} {-# LANGUAGE DeriveTraversable #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE TemplateHaskell #-} import Bound import Control.Applicative import Control.Monad (ap) import Data.Char (chr, ord) import Data.Deriving (deriveEq1, deriveShow1) import Data.Foldable import Data.Functor.Classes import Data.Traversable import Numeric (showIntAtBase) data Expr a = Var a | Let (Expr a) (Scope () Expr a) deriving (Functor,Foldable,Traversable) deriveEq1 ''Expr deriveShow1 ''Expr instance Eq a =&gt; Eq (Expr a) where (==) = eq1 instance Show a =&gt; Show (Expr a) where showsPrec = showsPrec1 instance Applicative Expr where pure = Var mf &lt;*&gt; ma = mf &gt;&gt;= \f -&gt; ma &gt;&gt;= \a -&gt; pure (f a) instance Monad Expr where Var a &gt;&gt;= f = f a Let rhs body &gt;&gt;= f = Let (rhs &gt;&gt;= f) (body &gt;&gt;&gt;= f) -- or use makeBound for some TH that generates your applicative -- and monad instances let_ :: Expr a -&gt; Expr (Var () a) -&gt; Expr a let_ rhs body = Let rhs (toScope body) -- To convert to the nameless representation after parsing, use `abstract`. eval :: Expr a -&gt; a eval (Var x) = x eval (Let rhs body) = eval (instantiate1 rhs body) -- eval (Let rhs body) = eval (instantiate1 (return (eval rhs)) body) pprint :: Expr String -&gt; String pprint = go 0 where go bvs (Var x) = x go bvs (Let rhs body) = let bv = newBoundVar bvs in "let { " ++ bv ++ " = " ++ go (bvs + 1) rhs ++ " } in " ++ go (bvs + 1) (instantiate1 (Var bv) body) newBoundVar :: Int -&gt; String newBoundVar n = showIntAtBase 26 (chr . (ord 'a' +)) n "" expr :: Expr String expr = let_ (Var "x") (let_ (Var (F "y")) (Var (F (B ())))) main = do print $ eval expr print $ pprint expr 
Are you implying we're not human?
&gt; Not sure how Kotlin might be explained though. Kotlin is backed by big players and likely to have good documentation, which experienced programmers would rely on before Stack Overflow. I’m not sure if it _is_ the case that they have good docs, just throwing a hypothesis out there.
Nice example, thanks!
The DL numbers in that table seem a bit low. Are they measured across some time period, or per version or something like that?
[Effect systems](https://www.youtube.com/watch?v=gUPuWHAt6SA) give you exactly this functionality, with a more sane / less unsafe interface.
Video linked by /u/isovector: Title|Channel|Published|Duration|Likes|Total Views :----------:|:----------:|:----------:|:----------:|:----------:|:----------: [Sandy Maguire: Don't Eff It Up: Free Monads in Action](https://youtube.com/watch?v=gUPuWHAt6SA)|Bay Area Haskell|2017-04-26|0:45:44|34+ (100%)|1,768 $quote Presented at BayHac 2017 https://wiki.haskell.org/BayHac2017 --- [^Info](https://np.reddit.com/r/youtubot/wiki/index) ^| [^/u/isovector ^can ^delete](https://np.reddit.com/message/compose/?to=_youtubot_&amp;subject=delete\%20comment&amp;message=$comment_id\%0A\%0AReason\%3A\%20\%2A\%2Aplease+help+us+improve\%2A\%2A) ^| ^v2.0.0
Thanks!
I see the distinction made between `Applicative` and `Monad` tasks, which makes me wonder if you have thought about looking at tasks in the `Arrow` classes as well? I think the spreadsheet `sprsh2`, with for example `B1`: B1 = do c &lt;- fetch "C1" if c == 1 then fetch "B2" else fetch "A1" could be seen as a form of `ArrowChoice`. This should allow us to calculate the possible dependencies of `B1`, namely `[C1, B2, A1]`.
Thanks I’ll do so if I get a minute. 
No, Haskell is the future because new programmers (few years of professional coding experience) starts with Haskell.
Agree!
Yes, this is a good point. In fact, some build systems (e.g. Excel) use over-approximation of dependencies and might be thought of as operating on `Task ArrowChoice`. However, note that there are also cases, where you cannot find any better approximation than "this task might depend on any key", which is the case with Excel's `INDIRECT` function. I'm not sure how this would look as a `Task ArrowChoice`.
Nope. Never used Matlab a day in my life. 
Awesome work. Just a UX tip -- add "higher is better" or "lower is better" for idiots like me who jump straight to the graphs, make snap judgements, and move on :) Any commentary on **why** the performance is so different between the libraries? Also, is `streamly` something that you wrote?
Right up until you need to decide if `callCC` is an effect. ;)
&gt; What if instead we could have representations of instances as data automatically and the ability to (unsafely) use local instances? Then we lose uniqueness of instance resolution, a feature Haskell is the only language that currently offers, in order to become just like everybody else. Then we'd have to care about the provenance of each and every instance and carefully supply vocabulary for managing it, rather than being able to rely on the fact that no matter how you refactor your code if it still typechecks afterwards you still get the same dictionary. Having used both extensively, I find the status quo far more interesting and the Scala solution quite fragile. You can change the semantics of your code simply by sorting your imports there. You also lose little libraries like `Data.Set` and `Data.Map`, ruin their asymptotic performance, or have to rely on messy things like convention and trust to ensure dictionaries are passed consistently, need to know whether you should percolate out every `Ord Int` constraint to the top level in case the user wants to overload it or if its safe to discharge it right away, and the answer is potentially different each time.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [composewell/streamly/.../**AsyncT.hs** (lazy-streaming → 103c918)](https://github.com/composewell/streamly/blob/103c918fe17a6a294efc59cdca0d272402adee1d/src/Asyncly/AsyncT.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvzo84t.)
Alright, you are the second person saying that, that means I should add it strictly rather than lazily! I do not know the implementations of other libraries very well so cannot comment authoritatively. Some reasons could be fundamental limitations with the design but there could also be reasons like not being fully optimized. Yes, streamly is the one that I wrote.
Sorry but can anyone please explain what 'à la Carte' mean? I've seen it several times but can't understand
Google says: &gt; à la carte &gt; ɑː lɑː ˈkɑːt,a la ˈkɑːt,French a la kaʀt/Submit &gt; adjective &gt; (in a restaurant) referring to food that can be ordered as separate items, rather than part of a set meal. &gt; "an à la carte menu" Which is the context we meant it in. 
Note, I'm not proposing to remove coherence. Just to be able to reify instances into data structures and supply them explicitly, e.g. `sortWith :: Ord a -&gt; [a] -&gt; [a]; sortWith (instanceOf @(Ord Int)) [1,2]`. Coherence would only be endangered when `unsafeLocal` is used. `unsafeLocal` is also not required, incoherent typeclasses could be specified explicitly using `reflection`: instance (Given (Store m)) =&gt; Store m where store = store given retrieve = retrieve given
Maybe we'll have to switch from Haskell to Lojban. :-)
I think this: http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf is the original use of the term. Here it refers to defining component datatypes and functions over them, and the easily combining these. Traditionally, if you add a new alternative to an algebraic datatype you have to go and change all your functions to take this new case into account. With the a la carte approach you just define the new alternatives and give a new function for processing them. Then you can combine your new datatype with our old datatype and the new function with the old function and you have a bigger datatype with a complete processing function without changing any code (only adding). Then there is compilers a la carte which uses this approach on syntax trees together with monad transformers to make modular compilers/semantics. Where you can write a language which for example only supports exceptions, and then combine it with another language to get that language, but with exceptions. This build system stuff does not seem directly related? But I assume the idea is that they are presenting a sort of modular approach where you can mix and match components to get a build system with a desired set of features.
&gt; I'm not proposing to remove coherence. Just to be able to reify instances into data structures and supply them explicitly. Local instances aren't sound with respect to coherence. You lose principal typings. `Given` _simply isn't sound_. If you use the `given` combinator twice in a given program at overlapping types, all bets are off. I provided it for convenience for the kinds of scenarios where a user loads an application-global configuration and doesn't want the type parameter, but apparently I didn't provide nearly enough of a screaming-loud warning. `reify` on the other hand retains by using quantification to force a fresh type variable into the mix. This is covered in somewhat less modern vocabulary in the original paper by Wadler on type-classes in section A.7 of [How to make ad-hoc polymorphism less ad-hoc](https://people.csail.mit.edu/dnj/teaching/6898/papers/wadler88.pdf) by Wadler and Blott in more modern parlance in Shan and Kiselyov's [Functional Pearl: Implicit Configurations](http://okmij.org/ftp/Haskell/tr-15-04.pdf) which provided the API I use for `reflection`.
I can't even get past `cabal install` which seems like it's due to cabal's dumb decision to use the system GHC. ``` src/Text/CSS/Parse.hs:18:1: error: Could not find module ‘Data.Text’ There are files missing in the ‘text-1.2.3.0’ package, try running 'ghc-pkg check'. Use -v to see a list of the files searched for. | 18 | import Data.Text (Text, strip) | ```
Do they also consider lazy (i.e. "non-strict") evaluation strategies as a build system?
Regarding "make": &gt; sadly unloved part of the software ecosystem Very true but the network effects of "make" makes things very convenient. More or less comes for free with most Unix/Linux/FreeBSD distros. It's so friggin' old that it rarely changes. And, ask your fave search engine a question on "make" and you'll find a reasonable answer.
&gt; Using records instead of type classes can lead to much slower programs. This may very well be true but but empirical statements can only be backed up by empirical data, i.e., measurements are in order.
&gt; for example, the first one I found in my current set of open tabs That's a great Q&amp;A -- it's in my shortlist of reference SO questions.
If I understand correctly, the reason you're considering Excel as a build system is that it is reactive; does that mean that the principles being discussed in the paper apply more generally to any program that allows the user to do reactive programming? The paper doesn't mention the "reactive" keyword anywhere, so maybe it isn't applicable...
My suggestion was to use `cabal new-build` (assuming you have cabal 2.0 or above) _instead_ of `cabal install`. Two subtleties exist. A) you may need to comment out the `allow-newer: rss-3000.2.0.5:time` line in `cabal.project` (this syntax may be too recent for your cabal) and B) you may want to substitute in a `cabal.project.local` for your version of ghc if you are using something prior to 8.2
No - they don't do incremental recompilation after mutation. 
Make couldn't switch from tabs to spaces because there were 10's of users. Rarely changes is an overstatement!
c.f. https://github.com/haskell/hackage-server/issues/703
Yeah, I'm not commenting on your proposal specifically, just your claim that "Haskellers often use type classes to represent interfaces." While it sometimes happens, in my experience it is more often the case that records are used. &gt; But if you go record approach your intent is no longer self-documented and you also have to pass the record around. As for having to pass a record around, it's true; it's more verbose and explicit, but I would consider it a trade-off sometimes very much worth making. I disagree about "self-documentation"; I think a record is no better and no worse than a type class in terms of intent, since in both cases the intent comes down to the documented semantics of the type class methods or record fields.
No doubt, tabs are gross, but it's tabs all the way down for "make" and it's been that way forever (as long as I can remember)--that qualifies to me as "rarely changes."
Where is this Mat's lab you speak of.
What's SpecConstr ?
I think it's just a consequence of the rising popularity in FP, and is probably why students are increasingly re-prioritizing their paradigms.
Pretty sure MATLAB is very popular outside of academia but with people who wouldn't necessarily call programming their primary job responsibility (e.g. engineers, scientists, etc).
I see what you're saying. I think you meant "understatement."
I hadn't thought about how to implement `mfix` / `afix`. Good point. 
I think they mean different "I" are in scope depending on which import comes last, but I don't know Scala 
I see, there is only an (optional) initial build. 
Thanks for the link! I read through the issue last night (I'm unfamiliar with hackage server), but to be clear: is the discussion about caching related to incremental update? That is, how soon reverse dependencies are computed once a new package version is uploaded? Given how fragile the haddocks on haddage can be sometimes, I don't think a "pending message" for computing the reverse dependencies, or just a nightly (or twice daily) computation which provides results that are up to a day stale, would feel that weird bad for users (iirc), and that it's definitely better than nothing. 
Edited - thanks!
quick tip: newtypes/type synonyms (I prefer newtypes) make code more readable.
There are optimizations that are possible and performed by GHC on dictionaries that are only sound because it can assume that given that the dictionary will always be the same, it can and will transform `a -&gt; (c =&gt; b)` into `c =&gt; a -&gt; b` during type checking and this is a "work-safe" transformation to lift out the passing of a dictionary to earlier points. The common `reflection`-based `Monoid` demo can be seen as one such example. In fact, it is the example that I wrote the library to handle, as I couldn't endure the performance of the alternative at scale. Calling something like `stimes (2^32)` for plenty of Semigroups makes use of 32 operations, but without this sort of lifting property, building the dictionary as a record and trying to call existing code will mean 2^32 operations.
Yes that's true, so their "number of years of professional coding experience" is still very low.
The first half of the sentence is more obvious: &gt; Developers who work with languages such as Cobol and Perl have the most years of professional coding experience. Those are not as popular today so typically older coders use them.
For me personally, I'll happily interpret this to mean Haskell is not hard to learn for newcomers. ;)
&gt; someone automatically sorted the imports in their IDE in a file in our codebase at S&amp;P on save, and we started getting a different implicit resolving That's unfortunate, but it's either a bug in the IDE or in the compiler; [according to the spec](https://www.scala-lang.org/files/archive/spec/2.11/07-implicits.html#implicit-parameters) the resolution of implicit parameters, which uses the same rules as the resolution of overloaded method calls, does not depend on import order. (In fact, it does not even seem to depend on explicit import versus wildcard import, contrary to what I said above.)
If this is what they mean, then they are most definitely wrong, according to [the spec](https://www.scala-lang.org/files/archive/spec/2.11/07-implicits.html#implicit-parameters). 
x-posted with r/NixOS given Haskeller's general interests in Nix. Slides: https://github.com/Gabriel439/slides/blob/master/nix-internals/slides.md
Some of these are very nice. In particular, the [http://cabal.readthedocs.io/en/latest/developing-packages.html?highlight=elif#common-stanzas](common stanzas) address the reason why I moved to hpack. I may well move back once Cabal 2.2 has become sufficiently widespread (meaning, when I can assume all my users have a Stack linked against Cabal 2.2).
https://ghc.haskell.org/trac/ghc/wiki/SpecConstr
No real reason but no one has implemented it yet. Here's the ticket. https://ghc.haskell.org/trac/ghc/ticket/10346 SpecConstr is also quite a bit more fragile than specialisation in my experience. 
Great work! I love seeing different ideas being unified under a single relatively simple framework. I’m curious, do you have any plans of working on something like cloud shake?
Yes! Before this paper it wasn't clear what that would look like. Now it's pretty obvious conceptually, just a load of engineering to go!
What if our build system wanted to optimize for throughput? If `B2` and `A1` don't possibly depend on the actual result of `C1`, then we can build `B2` and `A1` in parallel. This increases the overall throughput of the system, since we don't have to wait for the result of `C1`, in fact maybe `C1` doesn't even need to be built in this case. But yes, my made up situation here is probably quite niche. I don't really have any experience in creating a build system, so it is difficult for me to imagine what `Arrow`s could give us in addition to `Applicative`s. But I still think it is worth exploring.
Sounds good, looking forward to seeing the results!
I have no idea. At the time it was mostly dealing with the screaming. ;) Trying to reconstruct the scenario now, without access to any of the original details, for all I know it may well have been a thing like import foo import bar where there was also a `foo.bar` namespace adding our `bar` extensions. That's about as close to writing Scala as I'm willing to get in this day and age. =)
Seems like an IDE bug, then. Maybe the IDE rewrote: package foo // this imports foo._ into current scope package bar into: package foo.bar // this does not import anything from foo Something like this could happen in any language with wildcard imports, where an erroneous refactoring of import would expose a _different_ symbol than the one originally imported, without necessarily creating a type error.
The discussion isn't about the revdep issue at all. The caching there is cache-control stuff with the CDN, which we've since sorted out. The main issue is in-memory footprint of the new stuff. We need to pull just the new stuff into its own PR so we can have a discussion just on it. 
&lt;sarcasm&gt; We are clearly making such amazing progress with clearly communicating software design requirements from human to human, it seems that it's only a matter of time before the process of communicating software requirements from humans to computers is streamlined and automated. &lt;/sarcasm&gt;
The "Uncurated Hackage layer" proposal moved pretty quickly! Curious to see how this will work out in practice.
So, confession, I'm a hardware and C kind of guy that likes poking in Haskell and ATS (FP and dependent types are SUPER useful in hardware land). I haven't touched MATLAB in ages, so correct me if I'm wrong please. I remember one of the biggest drivers of MATLAB being easy access and use of the likes of LAPACK, which was originally made for FORTRAN77. You can set up bindings to LAPACK in C, but it can be a bit of a pain for a ton of reasons. 1 - C is row major, FORTRAN is column major, so you need to transpose matrices before LAPACK invocations 2 - Since LAPACK is for FORTRAN, all function calls follow a FORTRAN calling pattern, which in C means forcing everything to call by reference. 3 - Linking with LAPACK in C can often end up being system dependent which is a huge pain MATLAB stores matrices in column major fashion, and does all the dirt LAPACK work for you. That access make MATLAB really attractive to tons of people.
We are not the ones who will solve this issue, AI is. Just like AlphaGo Zero did not learn from us how to play go, but learned itself and kicked our asses.
Worth noting is also that software development is a very fast-growing field, so there are a lot more new ones coming in.
They will be very important in practice for usage of private libraries and Backpack IMO, so it's really exciting to see this getting deployed! (I don't believe you can currently use backpack on Hackage still, but private libraries are OK *I think*)
Makes sense (sorry, it was late when I read it, and I was skimming). 
The sample code doesn't properly handle empty lists. The following does: ors :: Prop r =&gt; [r] -&gt; r ors [] = false ors (o:os) = o `or` ors os dors :: PropDict r -&gt; [r] -&gt; r dors pd [] = dfalse pd dors pd (o:os) = dor pd o (dors pd os)
Alexa, take these 20 emails i got from my customers and these 20 word documents i created off those emails that are the workorders and these 20 excel spreadheets that are invoices for those workroders. Match them up and make it so I do not have to do it by hand anymore. Alexa matches the emails to workorder templates already filled in by human, identifies the data that was extracted form emails and into the word document fields. Also matches them to invocies and creates all teh database tables and fields and relations and crud data entry screens and reporting etc. After that it keeps monitoring how you use those data entry screens and reports and updates them as you continue using them. Simply pattern matching what human does with its own documents (emails, words excels etc) is enough to quickly build quite sophisticated CRUD apps and reporting. 
What I'm saying is that we don't even understand the fundamental rules of that system well enough to describe them formally in a consistent fashion. This would be more like a machine learning the rules of a game infinitely more common than go given minimal and often contradictory human input, and then from that understanding, building a successful strategy. This isn't teaching a computer to be better than humans at something humans understand how to do fairly well, this is teaching a computer to understand something humans don't, and then do well at it.
I don't see how this framework directly handles tasks that produce multiple output files. This is common -- e.g. `flex`/`yacc` (`.h` and `.c`).
Very cool! This will make many things much easier!
The encoding as pairs is completely sufficient - see https://github.com/ndmitchell/shake/blob/master/src/Development/Shake/Internal/Rules/Files.hs which is a complete implementation of rules producing multiple files on top of pairs (in fact, that's all in terms of lists, but the concept is the same). https://ndmitchell.com/downloads/paper-shake_before_building-10_sep_2012.pdf Section 6.3 might also be interesting. I agree it's not the clearest though, partly because we/I am not sure how to present it clearly, and partly because there wasn't enough space. It's probably the thing to write a blog post on.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ndmitchell/shake/.../**Files.hs** (master → 7847da0)](https://github.com/ndmitchell/shake/blob/7847da0e6e14b8165256d1f1f180718081ea772c/src/Development/Shake/Internal/Rules/Files.hs) ---- 
Lol, there is a [`HaskellReport`](https://spdx.org/licenses/HaskellReport.html) SPDX license identifier! 
Good point. I fixed it in the benchmark code but missed it on the blog post. 
You didn’t specify which RTS flags you are passing when you run the binary, so I am assuming that you are using the default nursery size. Try using something larger like 64 MB or 128 MB instead. This will make garbage collection happen less often. However, it will increase the amount of memory your application uses, but something tells me that this won’t be a problem on your hardware.
Do you have numbers of the speedup you gain with parallelism? My personal experience with a raytracer https://github.com/guibou/smallPTHS was that even if core are used at 100%, the speedup in far from linear (with 6 threads on a 12 hyper threaded machine, I had only 300% the speed of one thread, which really sucks: by comparaison the same C++ code runs at roughly 600%). But my code contains a (small) space leak I was unable to fix, so I'm curious if you were able to achieve something better.
Nice!
I'd love to see a concurrent GC added to GHC.
Congrats! Please post again, when you have the open source stuff ready, really interested in seeing what you did :)
Ok, remove 'single game' and replace it with 'a board game.' What I'm saying about the nature of machine learning has not changed. In this domain (board games) we understand how to formalize input (as discrete moves in a board game), and how to weigh outcomes (someone won). Both of those are absolutely 100% necessary for training a neural network, no matter how advanced - You need a discrete space of meaningful inputs and a way to verify or score the output. We don't have either of those for software development. 
You do not need to cover the entire software develoment. See my [answer](https://www.reddit.com/r/haskell/comments/85lghz/developers_who_work_with_languages_like_matlab/dw0gad5/) to another person. Basically pattern matching incoming documents like emails and attachments with outgoing documents produced by the employees (workorders, invoices, excel spreadsheets etc) is not that hard for AI. And AI already shown to be excellent in complex pattern matching. There are already AI built that make cancer diagnosis and early recognition of Alzheimer much more accurate than any human medical expert. CRUD apps is really just pattern matching inputs for storage and search (reporting). Teach AI how to build screens for data input and reports built on that data, and suddenly you eliminated millions of programmers. Does it matter that a few of us would still have jobs? 
Glad you posted this. Thanks. Is a "(nearly) best of both worlds" available, at least for a library maintainer? Something akin to: data R a = MkR {…} class C a where m :: R a I expect that code written using `C` will optimize as if the record type `R` had been inlined into its definition. If so, I just wanted to point that option out --- records and classes are by no means mutually exclusive options. And a library maintainer could even carefully use `reflection` to keep `C` internal (a la `runST` hiding state, but without type safety).
Sounds awesome. A job programming in a functional language in San Francisco sounds like a dream! Shame I'm a new grad and British.
I wish I had a source for this, but the story goes: Someone implemented concurrent GC for GHC a while ago. It was *really* complex and didn't perform better than the current GC setup, so it wasn't merged. 
Very happy to see company launch their product written with FP language and tools. Btw, have you guys considered using Elm in the front end?
Hi, I’m interested. At work, we have several Haskell+Elm. It works well, even though, well, Elm is nice but I’d like something more powerful. However, installing `purs` via `npm` is a true meh to me… What’s your experience feedback about that OP?
Also shame that SF is nearly unaffordable at high salaries 
Sounds like it was not the build instructions per-se but rather that you had a bad systemwide text library? One wild guess would be that if you're using arch you could be running into this: https://wiki.archlinux.org/index.php/haskell (Arch's bad packaging of haskell has been a source of constant complaints: https://bbs.archlinux.org/viewtopic.php?id=230166) (If you're not using arch, then it could be something else entirely...)
In general the consensus is arch really screwed up with their haskell packaging. *shrug*. https://www.reddit.com/r/haskell/comments/7jyie0/the_arch_linux_community_does_not_look_very_about/ https://www.reddit.com/r/archlinux/comments/7hanz7/whats_the_current_state_of_haskell_on_arch/
&gt;Match them up and make it so I do not have to do it by hand anymore. Even I have no frickin' idea what you're talking about. And I'm a human being, after all. 
I did some preliminary work on it already, and I plan to dive deep enough into the codegen once I have the time (a couple weeks, I estimate). It has a slight overhead due to indirect pointer chasing, but I hope to get that really low by writing some low level stuff for the codegen. It will likely not be faster than the parallel GC, at least on a normal core count machine. The primary goal is latency reductiin, to enable usecases where the lag is a showstopper (cough typing feedback cough VR cough). Tell me if you wanna help.
This is really interesting, I love the writing style, it's clear and easy to follow!
Came across [this blog post](https://www.linkedin.com/pulse/haskell-machine-learning-mitchell-vitez/) on this topic today, seems interesting.
The use of typeclasses to capture key elements of semantic "expressiveness" with regards to things _not in haskell per-se_ is really nice. It's like a "cheat code" to borrow some of the techniques of categorical models, but without having to go through any of the imposing mathematical formalism :-P
I don't think lein and sbt are comparable? They leave dep stuff elsewhere (either maven, or clj, etc), and basically _just_ coordinate builds. This is the opposite of stack, which relies on `Cabal` for the build stuff afaik and _mainly_ coordinates deps. The difference is also that they live on the JVM, so what they don't manage for you is your _install of the jvm_, which seems much more comparable to me to an install of ghc, to be honest. The big thing they have going for them is the uniformity of the JVM, which means that "installing" a compiler is really just downloading a jar, and there are no system interaction effects. In some aspects the jvm world is weird and frustrating, but in other ways that portability is really nice!
Or... you could just keep using stack. I mean, nobody is mad at you for using stack. I don't see why you feel so defensive about it. If it works for you, great! I personally think `new-build` is great, and much more suited to my style of development. If you feel otherwise, or don't care to investigate, it's no skin off my nose, any more than I feel any particular way about people that want to use languages I don't particularly seek to use myself. You write: "Now your issue is that we do not come back." Not really. Use stack, use cabal, migrate to clojure and use lein. Nobody's stopping you from doing whatever you want, and nobody particularly is anxious about whatever choice you want to make. We all just want to make the best tools possible to serve the people that want to use those tools. *shrug*.
That is not the vibe I get from the /u/hvr posts. He is clearly bitter and in a blame mode. 
&gt; Packages which are uncurated have no expectations on them regarding versioning policy. This seems to imply that packages which *don't* explicitly mark themselves as uncurated *do* have expectations on them regarding versioning policy. Please correct this potential misconception immediately by reiterating that all packages are still *encouraged* but **not required** to adhere to the PVP, regardless of `x-curation` status. We were specifically [promised by @gbaz](https://github.com/haskell/ecosystem-proposals/pull/6#issuecomment-372128025), author of the proposal, that "This proposal specifically does not strengthen the requirements for packages uploaded to the curated layer." I would have appreciated explicit acknowledgement of such in this blog post.
Awesome! Just had a look at the qfl repo, and the website! Looks amazing! Can you tell me more about Reflex? Looks good!
Thanks mate! Reflex is an implementation of Functional Reactive Programming. It's a bunch of fun. [We have a blog series on our website that includes exercises](https://qfpl.io/posts/reflex/basics/introduction/), if you're interested in diving in. I would recommend you start there. Also the people on Freenode in the #reflex-frp channel are super helpful if you get stuck.
For largue text generation, any chance of replicating in Haskell something like the postmodernist generator that I love very much? http://www.elsewhere.org/pomo/
there's eta, a haskell dialect https://eta-lang.org/
As others say above GHC has no concurrent GC implemented in mainline version. Concurrent GCs are optimised to improve response times, not throughput. Looking at eventlog MUT phase looks very nice but parallel GC phase has low parallelism (7% of 100%) due to -qn8. Which means some of your program's wall time is running GC (at least 25%-30% eyeballing pauses). What does '+RTS -sstderr' show for you? Instead of decreasing GC parallelism I would suggest exploring ways to increase it by throwing more work at each GC run. You can explore a few areas to do it: - increase MUT run before GC happens. Easy hack would be to try even larger allocation area as others suggest: +RTS -A128M (worth tweaking numbers around). This alone increases productivity from 40% to 98% on my 8-threaded box: runtime shrunk from 630s to 430s. Likely because garbage does not survive promotion to gen1 anymore. - play with stack sizes: +RTS -ki&lt;&gt; -kc&lt;&gt; -kb&lt;&gt;. Your eventlog shows frequent stack overflows. I wonder how hard is it for GHC RTS to handle those. Other avenues: - play with NUMA affinity: if your machine exposes NUMA memory locality https://ghc.haskell.org/trac/ghc/ticket/9221#comment:64 - try to write a multiprocess equivalent of the benchmark to see where scaling limit and compare it with single-threaded run to get the idea of what absolute speedup you can get on this machine
Wow, great! Could you share with us a link about the progress tracking? 
I tried nurseries from 1Mb up to 300Mb and GC threads from a few up to 96. 8 threads is optimal for this much heap allocation footprint. But all the rest (88) cores are drinking coffee during GC.
RTS flags I use (from .cabal file) are "-N -A15m -qb0 -qn8". Larger nursery doesn't help much. Yes, hardware has 128Gb RAM. It's this one: 96 Physical Cores @ 2.0 GHz (2 × Cavium ThunderX) 128 GB of DDR4 ECC RAM 250 GB of SSD in [the cloud](https://packet.net).
If you're "strictifying", why is `Box` not a `newtype`?
When I read this blog post, I didn't interpret it as a new implied requirement to adhere to PVP. But given the current high sensitivity of that topic, I agree it would be a good idea to word this more carefully.
Because then it would be named `Identity`.
streamly and streaming don't offer consumer monads which has an action to consume items one by one. pipes, machines, and conduit have something like await :: Consumer s m s
What library did you use to build out the GraphQL API?
Yes please.
Did this. Can attest Tony is a fantastic teacher. 
Is there a fresh scaffold for `yesod-1.6`? Code generated by `yesod-bin` appears to target `yesod-1.4`.
 instance MonadFail Box where fail = error =)
The library does pretty print records. For example, records in a list. Is that the question? It does not support diff friendly format. Could you elaborate on the need for this for records?
You mean building from scratch? Or some kind of magic?
To write a compiler: Sketch out a syntax tree for some interesting representative subset of the language you are interested in. This is just an ADT covering all the bits of syntax you want to deal with right away. Then write a parser for it using some parser combinator library. Pascal works out nicely for parsing with combinators as it was really the flag-ship language of the sort of "European school" of parsing, which is to parse by recursive descent; it is _designed_ to be parsed with combinators. You can iterate on this first part by adding more syntax and parsing more of the language as you get more comfortable or want more programs to parse. Figure out what typechecking the language you want looks like. Have that generate some kind of very well annotated intermediate state. Pascal works out fairly nicely here as almost everything is type checking rather than type inference, easing the job. Turbo Pascal compiled everything in one pass! Then scribble out an interpreter with some state for your stack, etc. There are plenty of options here from holding your environments really naively in maps and the like down to modeling a particular kind of virtual machine or bytecode. You may do a few of these before you find a model you like, but its great as you can share all the stuff above this part in the stack, and this is a place to quickly iterate and learn. Finally consider writing the compiler part after all those initial parts fit together. Each of these steps will give you more confidence you are on the right path.
Meaning something like (&lt;$!&gt;) :: Monad m =&gt; (a -&gt; b) -&gt; m a -&gt; m b f &lt;$!&gt; ma = do a &lt;- ma return $! f a which I use here and there.
Good question. You might be able to do something with a newtype with more bangs than /u/tomejaguar used (I'm not sure), but `Box` would still be interesting for a reason I forgot to mention. Can you see what unBox &lt;$&gt; traverse Box xs does?
One thing I find enlightening about the difference here is that the `instance MonadFail Box` I wrote above doesn't work for `Identity`. as `fail s &gt;&gt;= f = fail s` under `Box` but `fail s &gt;&gt;= f = f (error s)` if you try to use the same form for `Identity`, failing the `MonadFail` law. The extra monotone-distinguishable bottom in Box matters just enough to allow this distinction.
I guess you mean it forces the spine of `xs`?
I believe so. That's actually what I was trying to do in the beginning; I just sort of lost track in the middle...
This (self plug) article might help: https://gilmi.me/post/2016/10/14/lisp-to-js
That needlessly crashes when `n` is `1` (or `0`).
I tried different sizes. Even up to 300Mb, but the problem is that **memory bandwidth** has a cost and even with 128Gb of RAM, using more memory just slows down total run times. I found a bit better combination -&gt; -A40m -qn24.
Thank you very much for testing and moreover explaining my incorrect expectation! It's remarkable that the newtype encoding of single-method type class dictionaries can be a pessimization, since it forgets the property that the type determines the value (and any other characteristic --- eg typical usage --- that GHC might rely on when being relatively more aggressive when optimize dictionaries).
Arch's haskell packaging works perfectly for installing xmonad, pandoc, and other projects built with Haskell. /usr/bin is not a development environment.
&gt; few of us would still have jobs? Those few jobs would clearly be at least in Haskell!
You are executing an IO action twice (`ioThing`). If the action always returns the same value and is idempotent then the first one should be fine. If not, not. For example, if `ioThing` is `getLine` then you'll operating on one line and return some later line of input. If `ioThing` launches a nuke then the first will launch two nukes while the second will launch one. /u/Tysonzero gave another example with random numbers. If `ioThing` modifies an MVar or IORef then you will similarly be left with observable differences between your two code snippets.
Thank you for you explanation- it cleared some things up for me. MutableDatatype is an IOArray
I have just tried again with more combinations and nursery of 40Mb + 24 cores for GC have best throughput on 96 cores machine. I tried allocation areas up to 300Mb and the results are worse. I have tried with -k options but with not much success. About NUMA, there is no --numa option on Aarch64 GHC for some reason. But it actually use all the cores. I think that in machines with more than 16 cores concurrent GC will be the faster solution in throughput. Because stop-the-world GC is no more efficient enough on many cores. 
How many hours per month would you estimate you spend on it? I'd be interested, but I'm pretty pressed for time these days.
I use lpaste.net all the time and I have many important lpaste links stored in various places. How much care does it need? Is spam the only problem?
Ok, I'm reiterating it :-) Note the diff -- see line 43 (yellow) on the left, and line 48 (green) on the right. The statement about pvp expectations on packages is the same as before, except now the word "curated" has been inserted so it moves from "all packages should" to "all curated packages should". So this does not strengthen anything. It preserves the language as is, but now restricting it only to curated packages.
I created an epub version. The first look seems well. https://drive.google.com/open?id=1PnYpVUMRPKQAkE1NKsBc_lui3tduHJp-
Is the source available? People would probably help if there were a repo where they could submit PRs.
I addition to *do-notation* they can also be used for *let-* and *where-bindings*.
https://github.com/lpaste/lpaste
This is from a [SQL code generator](https://github.com/ulricha/algebra-sql/blob/master/src/Database/Algebra/SQL/Query.hs) that I wrote for my bachelor thesis. As it isn't exposed to the outside the focus is more on accurate representation and less about usability (also one of my Haskell projects). There is a pretty printer that supports DB-specific rendering. Not usable as a standalone library but maybe you can draw some inspiration from it.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ulricha/algebra-sql/.../**Query.hs** (master → 2a90470)](https://github.com/ulricha/algebra-sql/blob/2a9047036a66d36da7e6c57d06dedf67ab1468dd/src/Database/Algebra/SQL/Query.hs) ---- 
We used NodeJS and the Facebook GraphQL library. After the initial init/setup, all the resolver functions (business logic) are implemented and tested in Purescript! It's actually really cool because Purescript compiles down to JS so the GraphQL lib can call those functions as if you wrote them in JS. I'm writing a series of blog posts about how I accomplished this so stay tuned!
We're definitely open to trying things out in functional languages. The React site was already started so we didn't want to start over.
I've noticed a strange little annoyance with the redesign. Hyphens seem to be getting turned into hyphen-like characters that are not your run of the mill hyphens. For example, go to http://hackage.haskell.org/package/uuid and CTRL+F for `uuid-types`. As you can see, it's right there in the dependency list, but CTRL+F won't find it. That's because the text written is actually `uuid‑types`. Yes that's a different hyphen. It is "U+2011 : NON-BREAKING HYPHEN", instead of "U+002D : HYPHEN-MINUS {hyphen or minus sign}". I can understand that we don't want package names to be broken up across multiple lines, but still. The hyperlink is thankfully correct: http://hackage.haskell.org/package/uuid-types Since I tend to CTRL+F for dependencies on package pages once in a while, and many packages have hyphenated names, I find this quirk rather annoying.
Spam is basically the only problem. Otherwise, it requires no maintenance, apart from paying for the hosting. I just don't use it anymore or interact with people that do, so it's been neglected.
This tells you nothing. It could be due to any of the following: * Older developers have enough experience to see through trendy languages, while younger developers are still jumping on the latest bandwagon. * Older developers are too mentally inflexible to learn new languages that embody new concepts, while younger developers can pick them up easily. * Older developers have a big investment in their existing skillset and are therefore reluctant to throw it over for a new language in which they cannot claim a decade of practical experience, while younger developers don't have a big investment in any language.
&gt; If it is shut down without any takers for maintenance, could you provide the dump of the db to the haskell-infra team, so the content could be rehosted on https://archives.haskell.org/ (with appropriate domain name redirects put in place)? Sure. I can put dumps online somewhere.
Can you give a rough idea of how large a full compressed dump would be?
So, there are a number of problems with this. First is that you don't actually have any function that would generate a list for you to run a list comprehension on. Second, you're attempting to run a naive algorithm on the infinite space of all possible numbers when you don't actually need to - No fraction of your input can be larger than the input itself, so, there is no reason to reach for the infinite list of natural numbers to solve your problem, and trying to do so is just opening the door to getting stuck in an infinite loop while you try to work out a good solution. Use range snytax to do something like `[1..n]` and make sure that your function will terminate, as it will make debugging your attempts at writing this function a lot easier. Try breaking this up into smaller functions - One to generate a list of primes below `n`, and another to make a list of items from that list of primes in differing combinations. 
They sure can! I'm not sure how that's relevant though?
&gt; parser So it's seems I should catch up on... Just monads and ADTs? This seems doable within the time constraints I have (about 2 month). I'll go with Haskell then it seems! 
I volunteer to take it over. I would also consider being a partial maintainer if someone else wants to help out. Btw: What are the server costs monthly?
[removed]
I'll report back on what concepts/features in Haskell and FP I end up using once I'm done for those who have this same pressing question
Interestingly this suggests *against* using the Handle pattern that /u/jaspervdj recently wrote about - https://www.reddit.com/r/haskell/comments/82xw8b/haskell_design_patterns_the_handle_pattern/. Right?
We hire from all over the globe so don't let anything stop you from trying if you're interested!
Enrolled university students generally automatically qualify for the J1 Visa for summer jobs (some cities are absolutely overrun by rowdy Irish people working service jobs, good lads). My cousin from India had an internship at Google in SF also and didn’t seem to have any issues
I was maybe a bit cryptic. I just saw your snipped and had no intuition what all those Texts are doing. I try to use at least type synonyms in this case. &gt; data ParsedUser = ParsedUser &gt; { name :: !(Either Text Text) &gt; , age :: !(Either Text Int) &gt; }
How many posts from the same IP's do these botnets do? Fail2ban might help.
This is the `main` you're running that generated that threadscope screenshot? https://github.com/varosi/cgraytrace/blob/master/App/Main.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [varosi/cgraytrace/.../**Main.hs** (master → 0d2a5c3)](https://github.com/varosi/cgraytrace/blob/0d2a5c32d3ea36ed9ad344cdccc5139864104142/App/Main.hs) ---- 
I think if you strictify the `fmap` they should have the same behaviour: newtype IdentityS a = IdentityS {unIdentityS :: a} instance Functor IdentityS where fmap f (IdentityS x) = x `seq` IdentityS (f x) instance Applicative IdentityS where pure = IdentityS IdentityS f &lt;*&gt; IdentityS x = x `seq` IdentityS (f x) map' :: Traversable t =&gt; (a -&gt; b) -&gt; t a -&gt; t b map' f = unIdentityS . traverse (IdentityS . f) map' id [(), undefined] !! 0 ** Exception: Prelude.undefined
Cool. That'll be a good fallback (although it looks like you're likely to get some maintainer volunteers if this thread is any indication!). One thing to note is that of course it would be good to only put the "sanitized" dump online if it is to be publicly accessible -- i.e. absent private hpastes (not that they were super-private to begin with).
Good point! We can change the html codegen to produce nowrap spans instead in the next deploy. issue: https://github.com/haskell/hackage-server/issues/720
Would a CSV dump of the public pastes be usable?
It's 178M compressed. Very small.
In the title I wrote that it's at the commit (a87bdc4)[https://github.com/varosi/cgraytrace/commit/a87bdc4977bbc793eafec865744c6368d4a756d3], which is not on *master* branch. Those are minor GCs, but the picture is zoomed in. You could check .eventlog file that I gave in the title. 
I understand that CRUD apps seem very simple and full of schlep, but I think you're misunderstanding some core pieces of what happens at the human level when the rubber meets the road on your average software project. I do think intelligent assistants have the potential to be an incredible and revolutionary piece of software that will change a lot about how people in various fields get their jobs done - But I suspect very strongly that CRUD apps built by this sort of system would only be useful in a narrow subset of cases, or as prototypes. However, as a force multiplier for development and analysis work - I think we are VERY close to the world in which these sorts of systems are commonly used as research aides by business analysts or developers, and that could certainly have a significant impact on our industry... But nowhere near close to rendering programming languages obsolete.
Actually this is a great example of a problem where Haskell can really shine! First of all you will need a prime number generator: isqrt = floor . sqrt . fromIntegral divisible a b = (a `mod` b) == 0 primes = 2 : [x | x &lt;- [3,5..], all (not . divisible x) (takeWhile (&lt;= isqrt x) primes)] :: [Int] There you go! A list containing all primes fitting in a machine-size integer. The definition of primes can be read as follows: "the list of all primes starts with a 2 and is followed by all the uneven numbers that are indivisible by any prime number less than (or equal the square root) of that number." Now let's move on to the prime partitioning: ascending xs = and $ zipWith (&lt;=) xs (tail xs) partitions = map partition [0..] where partition x = filter ascending . mconcat $ map (\pr -&gt; if pr == x then [[pr]] else map (pr:) (partitions !! (x - pr))) (takeWhile (&lt;= x) primes) partitions !! 7 == [[2,2,3],[2,5],[7]] First of all we create a list that contains the mapping of all natural numbers to their respective prime partitioning. The partitioning of x is defined as: any prime p less or equal to x followed by the partitioning of x - p. This definition includes duplicates so we filter those out by only keeping partitionings that are in ascending order. The cool thing about this implementation is that everything is lazy. Each prime number is only computed once and more importantly we reuse the partitionings as well. This way you can work on partitionings that are really long. For example let's see what the last 10 partitionings are of 120 take 10 . reverse $ partitions !! 150 == [[71,79],[67,83],[61,89],[53,97],[47,103],[43,107],[41,109],[37,113],[31,37,41,41],[31,31,41,47]] 
+1
Surely someone reading this works for a Haskell-using organisation that could sponsor a virtual server?
Are you really writing a compiler? In that case, definitely check out this excellent post: http://www.stephendiehl.com/llvm/ Personally, I would suggest before getting all the code generation and optimization parts, write an interpreter first. My Pascal is pretty rusty, but I don't think its operational semantics is more complicated than simply typed lambda calculus with extensions like references. Loops can probably be desugared into recursive calls. 
--numa flags needs [libnuma](https://github.com/numactl/numactl) presence at GHC [build-time](http://git.haskell.org/ghc.git/blob/HEAD:/configure.ac#l1233)
Maybe it's possible to set up a Paypal account and ask for donations.
Actually there is this option, but at run-time it tells that there is no NUMA. I file this ticket - https://ghc.haskell.org/trac/ghc/ticket/14956
[This post](http://localhost:8080/blog/purescript-en.html) might help you get started with Purescript.
I wonder if `-qb0` (or a bug in gc load balancing) is your issue? From the threadscope it looks like threads are stopping productive work whenever any HEC requires a minor collection, which isn't how things are supposed to work. It looks like you're sparking an appropriate amount of work, and it's obviously being distributed to caps
Oh also doing an experiment that just does `forkIO` over the same chunks of work (rather than using par/pseq) might be interesting.
That's interesting. This is all very puzzling to me.
Nice. I had hoped that such a solution might exist, but wasn't aware of the particulars. Glad you're on top of it!
Just interns at the moment.
I think backpack does offer a solution to these problems. Points 2-4 are gained just by using Haskell. Backpack gets you points 1 and 5. Declare a module signature for the "interface" that this "component" exposes. Write your code in terms of that interface. Then you write as many implementations of the component as you want, and pick which one to use when you build the project. You can even have two separate executables in your `.cabal` file, where the only difference is which implementation you picked for that particular component.
Hey! Thanks for all your hard work. I remember lpaste from when I was learning Haskell — nearly four years ago! If you’re struggling to find a maintainer, onrock-eng would be glad to sponsor. Are the running costs reasonable?
Kotlin's also a pretty new-ish and upcoming language. I'd expect it more to be used by newer (more adventurous?) programmers first.
Some of the libraries, like pipes, lens, mtl and servant, are just amazing.
Funny how this blog appears after the same problem has been occupying my head for months. Also strange to see such a post from the author of [eff](http://atnos-org.github.io/eff/)! Doesn't the extensible effects framework solve the most important part of DI equation (defining modules)? The only thing left is to inject dependencies required by the _interpreters_ which can be done in a traditional fashion, same as in Google Guice for example. &gt; But that library relies on some "tricks" which are not available to Haskell, like the capability to collect objects of a given type at runtime (effectively doing an isInstanceOf test) or reflection to access the properties of a given case class. Not only we cannot do that in Haskell but we don't even have objects to begin with! Wrong! Haskell has everything necessary to provide the same runtime reflection capability – Typeable and Generic instances are available for most data types and for the function type, they provide all the same information Java would.
&gt; Is there anything you are specifically trying to accomplish here that isn't addressed by backpack? Not recompiling for each component change. The sought system should be reconfigurable at runtime.
&gt;I Haskell we don't have that kind of bad ideas such as automagically loading modules anywhere from code In Haskell there are many bad ideas, such as running the entire program in one monad without any separation or at most providing 1 pure and 1 IO implementation of the whole program (where only pure implementation is tested... IFF there are tests)
&gt;Also strange to see such a post from the author of eff! Are you sure about this. Because the article says at the start &gt;I have now been working with Haskell for around 3 months...
I think the most influential factor, though a bit indirect, is a broad desire to get things really right. This attracts developers who want to get things right. And they're more directly the reason things have advanced. "Works OK" is code for "we need to figure out how to make it better" here. Progress on all this isn't necessarily fast, but it feels superlinear. Every time a new improvement is made, it opens up more avenues for improving other things. That's all my best guess, anyway! 
I mean scala's eff https://github.com/atnos-org/eff
Prompted by this discussion, I got around to pulling it into a clean PR over master: https://github.com/haskell/hackage-server/pull/723
Ah, I see. The dynamic types is kind of startling in a Haskell tool. This particular case can be avoided by always having the rule's output be a list of the underlying type, but the directory/file split of different types is clever. I've not looked too closely at shake in the past, because it makes a design decision that I think is inappropriate for build tools (cabal and stack seem to have made the same decision). I want building software to be readily auditable, and transparent about what it does and what programs it calls. This means for the general case, it *shouldn't* be Turing complete. "To build, run this program with an embedded library/DSL that builds and crawls the dependency graph" is the exact opposite of what I want. Yes, in practice, many builds will be straightforward. And yes, I do see that the flexibility granted being necessary as an escape hatch in certain situations, though I think the right thing to do is put this programmability into external tools the build tool can reference. Instead of putting that logic into the build tool proper, you can extend rules with a "find_deps" field specifying an executable to run (at traversal time, not just pre-traversal graph building), rather than putting that logic into the build tool proper. This also solves the problem of depending on the rules, because those are external to the tool, and the update tracking that you use for everything else suffices. Dammit, now I want to write a build tool. 
You could also manually allocate memory off the GC heap and manage it manually
An ever-growing group of hard working developers guided by mathematical truths. 
I just hopped in around 3 years ago, and even in that time I've seen a lot of awesome growth and change. It's a lovely place :)
From what I understand, Haskell 2010 added and changed a lot of stuff for the better and friendlier.
* `stack` / Stackage happened. Whatever your opinions on Hackage and Stackage and so on (I personally couldn't care less as long as I have some place to download my packages from), `stack` was a big deal. I say this from experience: I spent a lot of time hitting against the wall that was trying to work with Cabal (in the days where there was no NixOS to make it easier), but `stack` came along, everything started to Just Work, and I began calling myself a Haskeller in a couple months.
For over 30 years I have been highly critical of Microsoft, but Microsoft Research's financial support of the Simons and others in GHC development has been a big reason that the project attained a critical mass and has grown so strong. I am grateful for Microsoft's support of this important work!
I'm quite happy that I can define dependencies in package.json and have my project running on a clean machine with a simple 'npm install'. Then 'psc-package install' and ALL dependencies are in place.
First of all, thank you for your work. I want to ask, is it possible to make video recordings of teaching sessions? I took both introduction-to-fp and applied-fp courses without guidance. And I should say that original Nicta course is one of the most important materials which made me interested in FP. But video lectures will definitely ease learning and probably will clarify why different constructs from the course are important (after taking the introduction-to-fp course I felt that I am able to implement monad transformers but I wasn't sure why exactly I should do it).
In [Escape From the Ivory Tower](https://www.youtube.com/watch?v=re96UgMk6GQ), SPJ says something like one of Haskell's greatest strength is its resistance to success. It's done better than many research languages, yet it hasn't become so successful that it's stuck with bad decisions for eternity. He believes that its long-term, low-key existence has enabled it to become quite refined. I suspect the same would be true of its supporting infrastructure. 
This is pretty much exactly how I approach things. ReaderT pattern for pathological problems. Mtl for domain modeling. Actual functions for business logic. Reasoning about these systems together is *far* easier than reasoning without any one of them.
Cheers, we hope you enjoyed the courses. The intro course would have been a bit of a tough slog without an instructor. :) The problem with recording the courses is that they are usually held over the course of three full days. 9am - 5pm. That is an enormous amount of footage, far too much really. There isn't a way to say which bits are worth recording either, as different questions will be asked. Which will prompt different sorts of discussions. The instructors will always adjust the course to suit their audience as well, not greatly, but it's still a change. The other thing to consider is that one of the main benefits to being able to attend the course is the interaction with the teacher and the other students. Knowing why some things are important is certainly a valuable side effect (tehe) of attending the course. So completing them on your own may deny you some of the richer explanations. That being said however, if you find there are topics that you would like to know more about then I'm happy to provide some resources for you. There are multiple Haskell meetup groups that quite often will record their presentations. It's likely that what ever you're interested in has been presented as a talk! In Brisbane (Australia) we have the Brisbane Functional Programming Group that meets monthly to discuss all sorts of things FP related. Our talks are recorded and you can find most of them on youtube on the BFPG channel: https://www.youtube.com/channel/UC3eIPN3YtTqPDDBbDVPlpWQ . We also have a smattering of talks on Vimeo: https://vimeo.com/album/2805119 . BFPG worked through the Yorgey Spring 2013 Haskell Course, and the recorded lectures were uploaded. There is also the New York Haskell User's group, which is a bit of a powerhouse when it comes to Haskell. They have a youtube channel as well: https://www.youtube.com/channel/UCzNYHE7Kj6pBqq5h8LG9Zcg . There are some links in the side bar of this subreddit that may have more. SO! It's not really practical for us to record these lectures for various reasons, but if you're curious about something in particular. Then I'm pretty sure you can find something that will help. :) PS: If you can't find a good talk or blog post about something you'd like to know more about. That is good incentive to dig a little deeper then create the content you want to see! Nothing like having to present on a topic to give you the motivation to find out more. 
By no means a comprehensive answer, but here are some aspects that come to mind. Overall, there was a concerted effort by a group of people -- largely, though not entirely grad students or recent postdocs at the time -- to turn Haskell from an academic tool into a genuinely viable language for development. Among those centrally involved in this effort (and there were more but these are names that come to mind) were Isaac Jones, Malcolm Wallace, Don Stewart, Duncan Coutts, Bryan O'Sullivan, Neil Mitchell, Roman Leshchinskiy and Ross Patterson. Their efforts focused on two areas that I'll address first -- packaging and performance. **Packaging** The use of Cabal as a packaging format started around 2005: http://web.archive.org/web/20050901082147/http://haskell.org:80/cabal/ Hackage started in 2006: http://hackage.haskell.org:80/ModHackage/Hackage.hs?action=view -- `cabal-install` started to be recommend for use a little while later, but before then at least we still had `Setup.hs` files and the cabal spec -- which were already a step forward! So actually, you dropped off right as things really began to take off! I know things weren't always smooth, but having a central package repository at _all_ was a big deal. **Libraries and performance** Along with a place to put libraries and a way to distribute and install them, there was an effort to produce high-quality basic libraries that were potentially performant. Before `ByteString` and somewhat later `vector` we didn't have efficient ways to operate on large quantities of data. So the overhead of representation cost us the performance we otherwise gained from all the optimizations made by GHC. The effort into those libraries meant that the idea that Haskell could come, with idiomatic code, within a factor of 2 of well-tuned C code went from a pipe-dream to a reality. The `text` library came a few years later, shepherded by Bryan. Johan Tibell's work on `unordered-containers` also deserves a mention. Performance went from something that you had to apologize for to something that was viewed as a _positive feature_ of Haskell. This was in part tied to the rise of scripting languages -- so people didn't ask how you performed against Java or C, but rather as compared to python or ruby. Don, in particular, was notorious for his posts and talks on how to achieve get performant Haskell code. **Success Stories** There were also applications of Haskell that generated interest. Audrey Tang started the first serious implementation of the proposed Perl 6 in Haskell (`Pugs`) in 2005. That brought in a raft of people who wanted to hack on perl, and instead ended up getting excited about haskell. Similarly, Dave Roundy's `darcs` was among the first (the first?) free-software distributed version control systems -- predating `mercurial` and `git` both by a few years. And then there was `xmonad` in 2007 -- which is still popular, and is still great. (and y 'pandoc` which continues to grow in popularity and features). All these things showed that good, real programs could be written in Haskell, and all ended up pulling people towards learning Haskell when they wanted to contribute. ** Parallelism ** GHC 6.6 (released 2006) was the first version to support use of multiple OS threads: https://downloads.haskell.org/~ghc/6.6/docs/html/users_guide/release-6-6.html The work through the 6-series of GHC compilers on concurrency and parallelism meant that as the need to write correct multithreaded code became greater and as the difficulty in doing so became more apparent, then Haskell again became a language that was seen as having something special to offer. The primary force behind this was of course Simon Marlow. ** Books ** RWH shows its age now, but it was the first book on Haskell oriented at an audience of regular developers, and the impact it had was huge -- not only in teaching people the language and introducing concepts, but just in _legitimizing_ it as a language (after all it now had a book from O'Reilly -- that meant it was "real"). LYAH, a little while later, also had a huge impact. Not only because it was even more "popular" in outlook, but because the lightness of tone and illustration really emphasized that this was a language that wasn't just for _very serious_ people. ** Web apps ** Haskell had a web framework for a very long time -- the venerable HAppS. But it was nightmarish to install. Eventually it was rescued and became happstack which is a fine framework, if not that trendy these days. But in the meantime, `snap` and `yesod` both came along. Since Haskell's native GUI story has always been rough sledding, the rise of server-side apps (and the diaspora away from J2EE, Struts, and PHP) was a very good fit for it -- especially combined with the fantastic green threads, concurrency support, etc. So the rise of "web 2.0" which was server-driven rather than semi-static was a huge boon for Haskell. And quality frameworks for webapps provided a big influx of interest. ** Early industrial adopters ** Here is the haskell in industry page from 2007: https://wiki.haskell.org/index.php?title=Haskell_in_industry&amp;oldid=18027 Risk-taking early industrial adopters pushed the limits of the compiler, established the legitimacy of Haskell for real-world use, developed and contributed back libraries they needed for their work, etc. You'll note quite a few financial firms there. This is due to SPJ's "composing contracts" paper (https://www.microsoft.com/en-us/research/publication/composing-contracts-an-adventure-in-financial-engineering/) which had a big impact on some people, and convinced them that FP was worth a try for their problem domain. There is I'm sure plenty to be said about the important role of Galois early on, but others are probably better equipped to speak to that. ** The #haskell irc channel ** Long one of the best channels on freenode, this was due to shae establishing early on a single rule: "be nice, or else". Active moderation and helpful regulars (won't name them all, but will name Cale!) who put an emphasis on helping newcomers meant that people learning Haskell on their own had a place where they could go and get the advice they needed. It provided a key social nexus for haskellers around the world to interact and collaborate. ** Summary ** There's a lot more to be said, but that's what comes to mind right now. I've focused on what I consider the "inflection point" period and the key developments there. Managing, surviving, and growing with the subsequent (and continuing) influx of new users and contributors, the tensions arising therein, the successes of subsequent projects and importance of subsequent contributors is another story, which gets more complicated because the cast of characters enlarges greatly, and also because it is still being written. The short story is that an enterprising group of people decided Haskell deserved to be a serious and widely-used language, and asked "what stands between now and that goal" and set out to tackle those missing pieces one-by-one. Along the way they found help and enthusiasm from many, and there were also plenty of elements of fortuitous circumstance in terms of being in the right place at the right time with regards to developments in tech more broadly. But capitalizing on that luck was only possible because it was a language and a compiler with so much to recommend it to begin with!
It's already useful for things such as [generic lens](https://hackage.haskell.org/package/generic-lens), where I can access my fields with `view #someField`, without `DuplicateRecordFields` I couldn't even define my someField label twice.
It's useful when a package provides TH/Generics generated instances for a datatype based on its field names. e.g. aeson's FromJSON/ToJSON classes. With DuplicatedRecordFields it's possible to share common fields with the same name across different types.
Prefix your record fields with the type `fooSomeField` is better than `someField @Foo` or `(someField :: Foo -&gt; Int)` or `view #someField` or whatever other weird contortion you want to do. Want polymorphic lenses? `makeFields` has you covered! Want Aeson deriving that drops the field name? It's trivial with `Typeable` -- dropTypeName :: Typeable a =&gt; Proxy a -&gt; Options -&gt; Options dropTypeName prxy opts = opts { fieldLabelModifier = fieldLabelModifer opts . drop (length (show (typeRep prxy))) } data Foo = Foo { fooName :: String } deriveJSON (dropTypeName (Proxy @Foo)) ''Foo And now you're in business. You can use `RecordWildCards` fearlessly, knowing that it's trivially easy to figure out what field a thing came from. `personName`, well, that's a person, `dogName`, well, that's a dog. `x ^. name` must be some polymorphic thing. Haskell doesn't have a record system, and *every attempt to give it one* is worse than just accepting that we don't have one.
This is indeed a form of the [Handle pattern](https://jaspervdj.be/posts/2018-03-08-handle-pattern.html) (is that what you mean?). However the "detection of the test env" does not happen from inside a module returning a different implementation because I want to separate production code from test code.
HLRDB is a type-driven library for modeling Redis-backed database architecture. It comes in two parts: the core package and the standard package. The core includes type-lifted Redis commands that act on declared paths, but makes no opinion on where or how those paths are created in the first place. The standard package includes the core and also gives you an easy API for declaring your data models. Places of interest: [some available commands](https://hackage.haskell.org/package/hlrdb-core-0.1.0.0/docs/HLRDB-Core.html) and [a simple demo](https://github.com/identicalsnowflake/hlrdb-demo).
Even if you don't want to use Parsec, you can always re-create a parser combinator library based on the same foundations. I did this several years ago in a similar situation (writing a small compiler, didn't know Haskell well yet, couldn't use libraries, etc.) and it was not terribly difficult thanks to a variety of references; I think http://eprints.nottingham.ac.uk/221/1/parsing.pdf was the main one I used.
&gt; Function application This indeed works. My issue with just using function application is the following: let's say I have a function, called `productionApplication` building my application for production, recursively calling `productionXXX` for each dependency. More precisely `productionApplication` is going to call `productionXXX` for its *direct* dependencies only. Now if there is a dependency for uploading S3 files which is not a direct dependency of my `Application` and I want to replace it with a component writing the files to a local directory instead what should I do? I need to parametrise `productionApplication` productionApplication :: uploadFilesComponent -&gt; Application productionApplication = Application (productionComponent1 uploadFilesComponent) productionComponent2 But that also means that I had to modify `productionComponent1` to thread in `uploadFilesComponent`. If you have a moderate graph of dependencies it becomes rapidly tedious (we have between 10 and 20 components in our micro-services).
LYAH is also completely void of any decent exercises.... Programming “clicked” for me when I read “learn ruby the hard way”. I was shocked that exercises were the missing piece. Reading through LYAH they get to a part where it’s like “oh let’s try to do X” and I have to stop myself from looking at the code so I can try to do it myself. And I’m not very disciplined so that’s hard for me.
There was actually a discussion about this a few months ago. Check the libraries list and haskell cafe.
You can already do: getField @someField someFooValue See https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.10.1.0/GHC-Records.html 
Happy to notify you about the progress. Choosing between this and the parallel GC should be possible at runtime, even though I am unlikely to take care of the relevant race conditions. The code gen might not be able to efficiently outputcode that can dynamically decide whether to incur the runtime overhead of looking out for concurrent changes, but the branch predictor should reduce the overhead if you don't run the gc concurrently. Any progress will be at my github, same nick, so feel free.
Meanwhile erlang is dead and stagnant other than the elixir ruby hipsters.
I'm very interested to learn how useful people find `DuplicateRecordFields`. There is an existing [GHC proposal for amendments to the extension](https://github.com/ghc-proposals/ghc-proposals/pull/84) that would restrict some uses of DRF in order to reduce the implementation complexity, and I'd like to know if this would cause problems for people in practice. Feedback here or on the PR is very welcome! Overall the impact of DRF on GHC's internals was higher than we expected, and I almost wonder if the benefits justify the cost, although I guess removing it again now to simplify GHC would be a hard sell (see `ImplicitParams`).
This is a nice trick! Hmm, and it would be broken by my [proposed simplification to DRF](https://github.com/ghc-proposals/ghc-proposals/pull/84)...
Thank you, this appears to be just the sort of thing I was looking for.
This adds absolutely nothing to the discussion.
Thank you for taking the time to spin this out! I can absolutely confirm the goodness of #haskell. It was a great relief to find that it was healthy and lively, compared to many programming communities that have abdicated their IRC presence in favor of slack.
Great thing we have you here to police bad comments, I was just about to get away with this one
For parsing: If you're allowed to, check out the ReadP module - it's a small parser combinator library, it lives in the base package, and while it's not as fast and feature-rich as stuff like attoparsec (or whatever the popular library is currently), it's probably more intuitive. If ReadP is off-limits, you can implement a similar library yourself. Start with the type newtype Parser a = Parser { runParser :: String -&gt; [(a, String)] Conceptually, it takes a string, turns some of it into something of type `a`, and gives you a tuple of the result + the rest of the string. If a string could be parsed in multiple ways, it gives you all the different options (hence the list). You can make Functor, Applicative and Monad instances for it, and write stuff like: get :: Parser Char -- parses any char get = Parser f where f [] = [] f (c:cs) = [(c, cs)] reject :: Parser a -- always fails reject = Parser $ \_ -&gt; [] char :: Char -&gt; Parser Char -- parses a specific char char c = do c' &lt;- get if c == c' then return c else reject twice :: Parser a -&gt; Parser (a, a) -- uses the same parser twice twice p = do a &lt;- p b &lt;- p return (a, b)
I'm being very clear. Many known haskell applications (such as hedgewars server) are written entirely in one monomorphic monad - they may define mtl classes, but get lazy on the n^2 instances, so they only have 1 or 2 implementations. That precludes them from writing very comprehensive tests, as they can't test application with ProductionService1 and MockService2, they can only test ProductionService1 with ProductionService2 and MockService1 with MockService2. That's not even mentioning packages with no tests, so many people bite the 'types &gt; tests' bait and advocate it on this reddit that it's not even funny.
Hackage in 2006 .. http://web.archive.org/web/20060303165044/http://hackage.haskell.org:80/ModHackage/Hackage.hs?action=view we've come quite a long way!
Thanks a lot for such verbose reply! Indeed I enjoyed the courses. And yes, what you are saying makes a lot of sense. I thought that probably, your format is close to what Bartosz Milewski did with his "Category Theory for Programmers" course. But recording 3-day workshop is hard work, and resulting video will not be very helpful.
All reasonable - thanks!
Looks neat and well thought out. Haven’t really used Redis but I might take this as an excuse to toy around with it more. It’d be worth pointing out in the readme that ⟿ is an alias for `Query` and the character is U+27FF LONG RIGHTWARDS SQUIGGLE ARROW. I prefer to use the TeX input method in Emacs for this sort of thing, but it doesn’t look like there’s a binding for this character. A better option might be ⇝ U+21DD RIGHTWARDS SQUIGGLE ARROW, which can be entered as `\squigarrowright`, or an ASCII approximation such as `~~&gt;`.
Huh, not allowing type applications for infix operators seems like an oversight. That is, I’d expect it to work, not that I’d actually use it much.
This approach doesn't seem to use "zoomy" lenses for focusing on subsets of the global state, does it?
[removed]
`elm` has that as well. :) I definitely need to have a look at purs!
Hey! I've been dabbling with Haskell off and on but I feel it's catching on as well. In my day job I use Clojure and sometimes think while coding: how would this look like in Haskell. Not all libraries that I compare are equally good. E.g. reading and creating Excel sheets. The support for this in Haskell is not yet on par with Java/Clojure (Apache POI), but it could be if enough developers are interested. Are you planning it for using it in your commercial work? What part of the ecosystem is superb or still lacking that you would need in your work?
I updated the readme to point out that `Query` is an alias of `⟿`. As for the arrow choice, I think I just picked the most visually distinct from `-&gt;` arrow I could find. I can add an ASCII variant I suppose, but that would be 3 aliases for the same thing, and it's already probably the trickiest concept in the library, so I don't want to make it more confusing.
Reminds me of [this video](https://www.youtube.com/watch?v=BKorP55Aqvg). I agree to some extent in that the "developer" would need to be proficient at expressing themselves in a specific and consistent manner in order to work efficiently with the compiler. I've had (programmer) colleagues come to me for help in translating their completely nonsensical idea into a formula/algorithm and be dumbfounded when they realize how it doesn't work because it makes sense in their head. That said, I wouldn't expect the compiler to actually make sense of nonsensical descriptions, I merely expect it to get good at working with vague information and providing easily digestible error feedback; the compiler would simply explain when contradictions occur and refuse to act on additional information if it doesn't make sense. "Make me a boat that sails the opposite way of the wind" would have the compiler either refuse to make anything, or make a boat (an average representation derived from samples all over the internet) and only refuse the latter condition. Personally, my favorite PL is Haskell, because it's very good at letting me know when things don't make sense without me wasting time thinking it does until the logic implodes during runtime. Because of my ADD I've never been able to sit down and design systems before starting the implementation, so I usually just try things out in iterations, creating the basic types and thus progressively discover what's logically coherent and what isn't. I imagine future PLs/compilers will simply be more advanced in this regard, automagically presenting reasonable working interpretations of a vague spec. People are better at iterating on their ideas as they are progressively shaped and get a better intuition for what makes sense given what they have working so far. Given a boat with a sail and some simulated wind, they can then come up with a sailing mechanic that makes more sense than having the sail react in opposite to the wind pushing against it. Maybe not the best example but hopefully you get the idea.
Video linked by /u/GiraffixCard: Title|Channel|Published|Duration|Likes|Total Views :----------:|:----------:|:----------:|:----------:|:----------:|:----------: [The Expert (Short Comedy Sketch)](https://youtube.com/watch?v=BKorP55Aqvg)|Lauris Beinerts|2014-03-23|0:07:35|187,377+ (98%)|19,534,933 &gt; Subscribe for more short comedy sketches &amp; films:... --- [^Info](https://np.reddit.com/r/youtubot/wiki/index) ^| [^/u/GiraffixCard ^can ^delete](https://np.reddit.com/message/compose/?to=_youtubot_&amp;subject=delete\%20comment&amp;message=dw3lqam\%0A\%0AReason\%3A\%20\%2A\%2Aplease+help+us+improve\%2A\%2A) ^| ^v2.0.0
To be clear, I've largely slept on haskell, aside from paying attention to some of the research that's come from the world it occupies. I say that only to emphasize that I don't necessarily think haskell is catching on _now_. The degree of growth and maturation I talked about up top I think is proof that it's been catching on for a long while. I've been using haskell in anger for ~3 months now. I've enjoyed it thoroughly, but don't think I'm at the point where I can usefully critique the community/ecosystem. I suppose opening this topic is part of my trying to build that understanding.
Has a ticket been filed? This seems really wrong indeed. I haven't looked enough to be able to file a decent ticket though.
Microsoft Research is an amazing resource. I often find it a pity that they have so little impact on Microsoft's products. That is changing though as the innovations in C# show. 
There isn't any global mutable state, is there? If not then zoomy lenses wouldn't be required.
&gt; Make me a boat that sails the opposite way of the wind" would have the compiler either refuse to make anything, or make a boat (an average representation derived from samples all over the internet) and only refuse the latter condition But a compiler wouldn't tell you that you probably need an airplane. You'd build a boat, spent time and resources just to get something that doesn't work the way you've expected. I had many times people saying "we need something like that" (points at a random stuff or feature). "Like what? What exactly do you want?" I ask, just to understand a bit later that they have no idea what *that random thing* is about, what it does. So how they could know what exactly do they want? People just tend to assume that everyone around thinks the way they do, and everyone should understand them as if they were reading minds. It bites them all the time in regular life, but with machines it becomes much worse. Machines are absolutely, fundamentally different. No matter how advanced and interactive compilers will be, unless a person has a mentality of a programmer and understands machines, this would be a talk between two persons with one of them being blind, and another one - deaf. 
Just my two cents: I'd much prefer if the main interface to the library were ascii-based. 
This is very good to keep in mind, thank you for the insight. 
I wish I'd read this a few years ago. I'm guilty of making several mistakes this article points out. Trying to create a MonadDB. Impure pipes and conduits. One thing that is missing is how to retain state in your program. I know fpcomplete recommends mutable references?
Zero value comment, but "fcuk, this is is awesome". We need more such content in the Haskell community. And I struggled for a good year to arrive at this pattern and still keep questions myself whether something should be a separate type-class or not.
I use them mostly in a `lens`-centric workflow where I never use the raw field accessors, just the lensed versions that get produced with `makeClassy` and the like.
It's a well known limitation of mtl: &gt; If you have n monad transformers and m monad interfaces, you need to define O(n * m) instances to ensure that each monad interface can pass through each transformer. This means n identical instances for each monad interface (or at least they would be identical if there was a sufficiently powerful complement to lift). But the code duplication isn't the worst part: the worst part is how this destroys modularity. It means that every third-party monad interface can only be lifted through the standard monad transformers. If you want to lift a third party monad interface through a third party monad transformer, your only hope is to define an orphan instance, which means your application will break if any of the libraries you depend on ever define this instance themselves. More info: https://lexi-lambda.github.io/blog/2017/04/28/lifts-for-free-making-mtl-typeclasses-derivable/ http://hackage.haskell.org/package/layers-0.1/docs/Documentation-Layers-Overview.html - quote http://www.parsonsmatt.org/2016/07/14/rank_n_classy_limited_effects.html https://www.reddit.com/r/haskell/comments/27vhd1/the_problem_with_mtl/
There's lots of information at https://wiki.haskell.org/IDEs
I like the idea that some types, like Int, don't need much coverage because we can assume that a library's own tests are already covering its cases. I wish this was configurable though; what if I'm the one writing the library which implements the Int case? What if I'm using a library which implements the Person case? Unfortunately, I think using type classes makes this kind of configuration impossible, so a configurable version of this library would probably have to be a bit harder to use. Another use case in which this configurability might be useful is for generating internal documentation. At work, our frontend and our backend exchange complex json messages, and we try to maintain a markdown document explaining what each part of the message does and what that part of the json looks like. Unfortunately, this document is often out of date, dramatically reducing its utility due to its unreliability. I think it might be a good tradeoff if we could programmatically generate examples of all the json messages; the prose explaining what each message does would be missing, but at least the json would always be up-to-date. It looks like `fake` would already allow me to produce this list of all json messages, so thanks! But with a bit more configurability, I think I would be able to get the best of both worlds! I would be able to write some prose describing part of the json, generate examples covering all the constructors for the type describing that part, and then somehow mark that type as already covered, so that when I generate examples in the rest of the document, I wouldn't waste the reader's time repeating examples of that part of the json.
VSCode with haskero, or Atom with ghc-mod, both pretty good
Watchman doesn't contain a visual component (it looks like I missed that aspect of your project at first). There's a bundled tool ("watchman-make") that can execute commands in response to file notifications, but the watchman daemon itself is concerned with allowing you to query for updates programmatically. When you get a batch of updates, the response will contain an opaque timestamp-like token (think of a database cursor, if that's familiar) which, when provided in subsequent queries, allows watchman to give you back the just the new notifications since the last time your client queried, and then you can rinse and repeat. The interface is a Unix domain socket on supporting platforms (e.g. Linux and macOS), while Windows uses a named pipe. The default encoding for messages is JSON, but you can opt in to a simple binary encoding (BSER).
If you are on a mac: http://haskellformac.com/ 
Don't you install Elm with npm also? https://www.npmjs.com/package/elm
Elm isn't a general purpose programming language anymore, it's a DSL for creating user interfaces (since the removal of Signals and addition of Html.App). AFAIK, they are using purescript as a wrapper around graphql on the backend, it's not possible to use Elm for this purpose. On the other hand, purs can be used to write web servers that would run on nodejs, it's a true general purpose language.
The system I work on makes heavy use of `TVar`s to have mutable and shareable state. You need an IO-capable monad to modify or read the reference.
Thanks. Found in the Haskell [Libraries Archives](https://mail.haskell.org/pipermail/libraries/). Subject is FunctorFix, discussed in [August](https://mail.haskell.org/pipermail/libraries/2017-August/thread.html) and [September](https://mail.haskell.org/pipermail/libraries/2017-September/thread.html) 2017. 
the term is "avoid success at all costs". There is continued discussion of the term and whether they mean "avoid success, at all costs" or "avoid, success at all costs", where the general consensus is the latter. Under this interpretation, success isn't the thing to be avoided, only that success should be pursued in the most correct fashion possible
&gt; One thing that is missing is how to maintain state in your program. I think the article mentions all the tools necessary to talk about state, but just omitted the detail. One of the main reasons for the ReaderT pattern is to keep state in `IORef`s so that you can deal with it in a somewhat exception safe way. I think this is important for state that needs to persist after the job. But the reason for the mtl-style in layer 2 is for domain modelling; i.e. if you need to model state that's local to the current job, MonadState might be a good choice, and the exception safety doesn't matter a whole lot. The internal state to a specific business logic function can probably be maintained with a local use of StateT, but this also might be a small enough case that manually threading state around isn't so bad.
Although I haven't gotten to use it extensively in production, it seems to work well with `RecordWildCards`(which I really like), so you can do things like: foo Bar{..} Baz{..} = ... return UsesSomeFieldsFromBarAndBaz{..} sorry for the non-example, but I find it's quite common that we either assemble a smaller record from a larger one, or some combinations thereof. So RecordWildCards wraps up all that boilerplate, while letting us keep the same name for records that have the same semantics
I didn't know this. Neat!
Server costs are just 6$ per month on average. The cost is usage based on DigitalOcean. It's a complete non-issue, I think! 
Server costs are just 6$ per month on average. The cost is usage based on DigitalOcean. Running costs are a non-issue.
I got to work in an office with these guys for 8 months. The team is chock-full of super talented people, and I learned a ton.
&gt; We're located in New York City and prefer to hire locally _sigh_
Ah, but which type variable should be instantiated? Should `r1 &amp; @R1 field` mean `(&amp;) @R1 r1 field` or `(&amp;) r1 @R1 field`? In this case we want the former, but is that always the case?
Also stated that they're willing to make exceptions for highly qualified applicants 
Hmm, my lisp-fu is weak; what does "inherited" mean in this context? It sounds like `S1` and `S2` are related in a way that would not be true of two distinct Haskell datatypes with a common field name?
It's unfortunate, but we still haven't managed to get our remote workflow as smooth as the way we work together in person. It's something we continue to work on, and I hope someday we won't have to worry about location.
This is my attempt to take the verification efforts from the ivory tower, and try to make them available to Haskell developers out there, but I’m not sure if this is the best way, so I am interested in the discussion here: Is this useful to you? Do you care about this? Would you use it? Should this be done different (e.g. a code copy instead of a reexporting package)?
I believe that was clarified here: http://www.lispworks.com/documentation/HyperSpec/Body/04_bb.htm It might be referring to the `:include` directive given to the macro; defstruct is expanded at compile time which lets lispers do that sort of thing. Upon further reading it might not be so helpful. Common Lisp does generate the names of the accessor functions for you with the names prefixed by the constructor name. If you try to over-ride the constructor name to create a clash you'll get an error. Ergonomically though it feels a lot like how I'd _like_ to use records in Haskell: ;gnu clisp 2.49 (defstruct foo (bar 0 :type integer) ) (defstruct bar (bar 0 :type integer) ) (setq m (make-foo :bar 2)) (format t "It is ~d" (foo-bar m)) ;; "It is 2"
Great! By run-time I mean at start time, not during the execution.
This will be a lot harder in this project.
Obsidian is an amazing group of people. If I didn’t have a job I love already, or an aversion to big cities, these are the sort of people I’d want to work with. 
what do they use to build mobile apps with Haskell?
Yes, I think that was the phrase he used in the video. But in that video, he definitely talked about the benefits of being, in terms of adoption, somewhere between most research languages and most mainstream languages. That wasn't necessarily be design, but it has worked in Haskell's favor. 
GHC has had the ability to compile to ARM for quite some time, and we polished that up and got it all working on iOS and Android with nice support from Nixpkgs. You can use the final result by following the instructions here: https://github.com/reflex-frp/reflex-platform/blob/develop/docs/project-development.md
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex-platform/.../**project-development.md** (develop → f003577)](https://github.com/reflex-frp/reflex-platform/blob/f003577699ad5a47f8275dad4f05cdb15c4bcdf5/docs/project-development.md) ---- 
You're a week late for the ides in March. Try again in April.
Not exactly sure what that means, but ok.
Some example code for how to put this into application would be great! Anyone has a link?
I see we still have highly fracture environment with at least 4 IDE backends (hie, ghcmod, intero, haskero) used by multiple frontends (vim, emacs, vscode, atom). What happened to the efforts to provide a single backend that has all the features and can be used by all the frontends? 
VSCode with the haskell-ide-engine
**Ides of March** The Ides of March (Latin: Idus Martiae, Late Latin: Idus Martii) is a day on the Roman calendar that corresponds to 15 March. It was marked by several religious observances and was notable for the Romans as a deadline for settling debts. In 44 BC, it became notorious as the date of the assassination of Julius Caesar which made the Ides of March a turning point in Roman history. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Ah, thank you
I'm gonna suggest some specific setups, but pick whatever you want (somewhat self-plugs): - VSCode with [the HIE LSP](https://marketplace.visualstudio.com/items?itemName=alanz.vscode-hie-server) - [SpaceNeovim](https://github.com/Tehnix/spaceneovim) with the [Haskell layer](https://github.com/Tehnix/spaceneovim-layers/tree/master/layers/+lang/haskell) (I'd go either intero or HIE for backend) - Atom with [atom-haskell](https://github.com/Tehnix/atom-haskell), which sets up a nice set of haskell packages - Spacemacs on the [devel branch](https://github.com/syl20bnr/spacemacs/tree/develop/layers/%2Blang/haskell) with intero I personally use VSCode and SpaceNeovim mostly, but honestly I sometimes switch around all four
/u/ElvishJerricco put it perfectly. State is an effect in your program to be managed just like all the rest -- is it a performance/concurency concern? Stick it in ReaderT. Is it a resource/external service concern? Make a specific class for it. Is it business logic? Use `State` locally, or pass parameters explicitly.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [syl20bnr/spacemacs/.../**haskell** (develop → 0fa3658)](https://github.com/syl20bnr/spacemacs/tree/0fa3658cd8e283825dcd0a54ce1579dec55eb568/layers/%2Blang/haskell) * [Tehnix/spaceneovim-layers/.../**haskell** (master → 5c3d720)](https://github.com/Tehnix/spaceneovim-layers/tree/5c3d72091b61d4ea7d0c35ed7636f916da4df204/layers/+lang/haskell) ---- 
I've seen a lot of recommendations for VScode but I've also seen stuff online for an intellij plugin is there a difference?
Personally I would, especially if you have a pretty self-contained example. Even if the behavior is explainable and expected, I think test cases like this are valuable to GHC developers and they have asked for them in the past. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [antalsz/hs-to-coq/.../**containers** (master → 4a6d3fd)](https://github.com/antalsz/hs-to-coq/tree/4a6d3fd1124be435a73d4937182276fb2fd1267c/examples/containers) ---- 
This is a neat idea! I would personally not use it due to name collision in the module, but could see the utility if the modules were `Data.Map.Verified`. Enabling `PackageImports` and writing `import qualified "containers" Data.Map as Map` on each line that I need a non-verified function is somewhat odious.
I knew that half decade of latin was gonna pay off one day!
Ok, but that is only because verification is not complete yet. If every function that you need were verified, then this would not be a problem, right?
Additionally, one of Stephen Diehl's books-in-progress has a section on writing your own parser combinator library - http://dev.stephendiehl.com/fun/002_parsers.html
Is that then running natively, using `jsaddle-webkit2gtk` (or `jsaddle-warp`) to render the UI?
Running natively, using `jsaddle-wkwebview` on iOS and `jsaddle-clib` on Android for rendering DOM. We use `jsaddle-webkit2gtk` for desktop and `jsaddle-warp` for rapid development.
It needs pointer chasing during runtime, if the pointer is tagged as now pointing to the new location of the object, instead of the normal info data. This will be assembler generated at compile time, so no, completely removing the runtime overhead is not possible. If it is not being used, your branch predictor should be efficient in reducing the overhead. 
Well, you would need to recreate versions of every other library that uses containers, since otherwise there would be conflicts.
The folks at Obsidian are great, and they're working on some really cool technology. If I wasn't very happy with my job I'd be applying.
The verification currently lives with the `hs-to-coq` repository, and does not yet automatically track `containers`.
ok, I've added `~~&gt;` as an alias for `⟿`
Very cool! Did you find any bugs in containers?
Was reading the Prelude (have an exam coming up which requires knowledge of some basic function definitions) and noticed that ```repeat``` isn't defined how I'd expect it to be. The prelude defines it like this: repeat x = xs where xs = x:xs Whereas I'd probably define it like this: repeat x = x:repeat x Is there any important reason for this? I'm pretty sure the definitions are identical functionally, but I was wondering if there was some interesting stylistic reason for this definition.
Interesting. I think having non-conflicting module names would greatly ease adoption. If you need a function from `containers` that `containers-verified` doesn't support, then your entire package component suddenly needs to deal with package imports boilerplate. I'd consider `sed -i "/import qualified Data.Map as Map/import qualified Data.Map.Verified as Map/"` a reasonable drop in, along with `import qualified Data.Map as MapUnverified` when you need non-verified functions.
I think this is really hard to get right and I respect companies who admit that they don't have it figured out and don't want to put a new person in the position of being the guinea pig for figuring it out. Even something as simple as receiving comments on a pull request or asking someone to finish something you are waiting on feels much harder when I am working from home instead of the office, let alone full time remote.
I care deeply about this. Thank you! I will try to start depending on these from my current work project as I get time.
I saw previous traffic about this project and rather like the method of deployment. I see other people are in favor of a unique module name - I prefer keeping the overlapping modules to maintain the drop-in replace-ability. If I find I need something from containers that is not in verified then I'd be more likely to forgo verified entirely and use containers directly rather than edit at least two files (the dependencies, keeping them in sync, the top of my module to add an import and the middle of my module to use the desired function). Would I use this? Yes, probably. Is this useful to me? Well I honestly don't think I ever went debugging and discovered a bug in the containers package so I'm not sure it would have saved me personally but removing that possibility at no cost besides a dependency name in the cabal file is great.
A friendly reminder to everyone in this thread considering this application (as well as any other applications posted here) that it is *always* prudent to have a local attorney look over any potential employment contracts (specifically someone specializing, or knowledgeable in employment law). I have seen some onerous contracts put upon junior engineers who don't necessarily know to get them checked over by a professional before signing, _especially_ in the world of consulting and contracting. (n.b. I'm saying this as someone who was very recently a junior engineer who was offered a somewhat onerous contract, and was able to negotiate a little more out of it after an attorney broke down precisely what the restrictions of what I was agreeing to entailed).
Unless I'm mistaken, it appears that `import` has to be the first thing in a stanza, but this isn't mentioned anywhere.
A recursive local binding optimizes more cleanly than going 'back to the top' of the function. 
Isn’t Haskero a VS Code integration of Intero? I’ve tried the three backends you mentioned as well as hsdev (has Sublime Text integration) and none of them has properly worked for me nor came close to it except for the fifth - ghcid, which is humble in features but works great for what it does.
There you go. Even more backends (ghcid) and a lot of frustration. 
I think, an interesting counterpoint to the `hs-to-coq` [was given by Gabriel Scherer here](https://github.com/nuprl/website/pull/118#issuecomment-358595090). Otherwise, I'm happy to see the tool applied to a realistic piece of software.
oh yeah I'm definitely gonna use a library for that
You can probably trust a company that pays their employees to work on open source.
Yes, Stackage is amazing and makes working with Haskell such a pain-free exercise compared to ie NodeJS
Modulo opimizations like fusion, `xs where xs = x:xs`, when iterated over, will construct an infinite list by having a single cons-cell that points back to itself, while `x:repeat x` will keep constructing new ones that contain the same data as you go (that's what `:` does, after all). They behave the same logically, but the memory usage is constant for the first one, linear for the second one.
Professionally - not just "while learning" - I'm using GHCi + emacs (without any "engine", just haskell-mode) + unix shell power. I like it much better than any so-called "powerful IDEs" that I have used in the past for other languages.
The real problem for me with the conflicting module name is the typical users's `ghci` experience. If both of them are built in the current context then you'll get failures in `ghci` when you go to `import Data.Set`, drastically worsening the user experience of users who have never heard of your library and just want to test something out. This was the death knell for `monads-tf`, which collided with the modules supplied for `mtl`. We eventually managed to mitigate that by setting `exposed: False` in its [package description](https://hackage.haskell.org/package/monads-tf-0.1.0.3/monads-tf.cabal) to keep it from breaking casual `ghci` users. That _may_ be enough to mitigate this concern here in practice. A different trick that could be cute would be to build a module signature using backpack that covers the `containers-verified` subset, so users can build against that and then instantiate their whole package against one or the other later on.
From the paper: &gt; We would like to have been able to claim the contribution of findings bugs in `containers`, but there simply were none. Still, our efforts resulted in improvements to the `containers` library. First, an insight during the verification process led to an optimization that makes the `Data.Set.union` function 4% faster. Second, we discovered an incompleteness in the specification of the validity checker used in the test suite.
I'm not sure what you mean by more configurable. From what I can tell, fake can support the case you're talking about. I intentionally did not provide any instances for the primitive types in Prelude for this very reason. If you want your Int primitive to be `[fromRange (1,100)]` you can do that. If you want `[fromRange (-100, -1), return 0, fromRange (1,100)]` you can do that too. You have the same flexibility for using the `fake` function for a particular field instead of `gcover`. I'm not sure how to provide much more power than it currently provides, but I'm certainly open to ideas! I don't understand the full picture of what you're talking about with the prose example, but that does sound like it's probably outside the scope of what fake is trying to do. It's just about generating plausible looking values for your data types. Adding other things on top of that should probably be done by a higher level wrapper.
Why not?
Huh, didn’t know the latter was possible. I can come up with an example— ki :: forall a b. a -&gt; b -&gt; b -- ~ forall a. a -&gt; forall b. b -&gt; b ki x y = y ki @Int 1 :: forall b. b -&gt; b ki 1 @Int :: Int -&gt; Int —but I don’t know off the top of my head where that’s useful. So yeah, the former seems like a reasonable default as a way to annotate explicitly which type you’re using an operator at, whether for pedagogical purposes, type-directed programming, or just code clarity. I honestly thought type applications already had higher precedence than function application, so I interpreted your example as `(&amp;) (r1 @R1) field`.
I haven't tried loading up from ghci with a backpack package in the global environment. You _might_ have to put `exposed: False` on the backpack package as well, not sure, but I'd expect not. If you were to do this through backpack you'd probably want the `library` to just expose the signature, then to have at test that linked in both `containers` and your the `containers-verified` library signature just to make sure they were compatible. I've used that successfully for 'signature checks' before.
I think the downside of the of the backpack solution is that you have no guarantee (like you would with the proposed solution) that the code you run with is actually verified. It should be fine, though if you provide a `containers-verified` interface and a separate `containers-verified-impl`, which pins the `containers` version.
That could work.
Are you guys capable of taking on a summer intern? I am based in the NY area!
Suppose your code is running in a web service, something bad happens, you suspect you have buggy code several layers deep in a stack of pure functions and you want to write some data from the suspect function to a log. What's the best way to do that?
The paper is quite even-handed and as I used to work with some of the authors, I certainly have no objection to it. But as a formal methods person myself I always like to stress the verification gap. Formal methods people used to promise the world and deliver little in terms of software correctness, and it lead to a FM winter where all formal verification approaches were dismissed as impractical even in domains where it's cheaper than the alternatives. As FM people we need to be clear about what it is we mean by "formally verified", and avoid making any grandiose claims.
The errors would be useless for the actual system, but neat to log for debugging for sure. I can't make up useful default values for i.e. `name` so it would have to be a `Maybe Name` in the database unfortunately. I'm thinking of mapping the db models onto models which would get default "missing" values when sending them to the client though. While we are on the subject, do you have any pointers to how I can accumulate the errors in a nice way? I have a vague memory of reading something involving `EitherT` on top of the effect and then traversing it in some cleaver way.
Why don't haskell `IO` libraries come with ready-made mocks and interfaces like in all other languages? Why is the boilerplate always usually to the user of the library instead of the writer like in all other languages?
You have multiple options here, really: * If you only want to know if "any" error occured, and dig out the logs for "multiple errors" in this case, you can use a signature like `parse :: LoggingT (Maybe Result)` * If you want to cut down the computation in case of any errors happening (which most of the parsers does), you can use `parse :: Either Error Result`. * If you actually want all of the errors when they occur, you need to complete the whole computation to see all of the errors that can happen. You see, this cannot be done in a monadic computation because you have dependencies: if I failed to produce `a`, say got an error, I cannot continue doing a computation like `a -&gt; m b`. Because of this, you can use a custom applicative for error collection since applicative computations don't depend each other. A type with a signature like `AccumErrResult a b = LErr a | RRes b` will do. Then you can write an applicative instance which combines any two cases on left and right. As a clue, you can think of a collection of error messages as a `Monoid` and you can accumulate everything that are `Monoid`s ([] is only one of the cases). For database part, If you think when you look at a row a user without a name makes sense, go for it. It's not relevant to Haskell part but if I would prefer seeing null names instead of some arbitrary default name as well.
You should probably benchmark the code to see if it’s superlinear so you can tell if it’s a bug or if you just need to speed it up. Also make sure u’re compiling with O2 enabled
I just looked very fast at your code, and the only two points that may be of help I can think of are: - add bang patterns to your record fields, of a `{-# LANGUAGE StrictData #-} pragma at the top; - replace `Data.List.intersect` with `Data.Set.intersect` (from `containers`), which will have better performance.
I've ~~shamelessly ripped~~ taken some inspiration from Cargo and RubyGems for an alternative homepage look, would love to hear opinions on the matter.
I use them to handle model migrations in an event-sourced system. I have `FooV1`, `FooV2`, FooV3`, which represents the different versions of `Foo` over time. I use `getField` to access the fields, until (maybe) we get a shorter syntax. By the way, what happened to instance HasField x r a =&gt; IsLabel x (r -&gt; a) where fromLabel = getField ?
It was omitted from the implementation of the [previous GHC proposal](https://github.com/ghc-proposals/ghc-proposals/pull/6#issuecomment-277616974) because we couldn't get consensus on whether to use that instance or another variant (e.g. to support van Laarhoven lenses). Unfortunately this was decided late enough in the process that the proposal as merged doesn't reflect that fact.
After a quick glance, your coding style looks good. This is almost certainly not related to minor optimizations such as "use vector instead of list" or "use more bang patterns". This kind of behavior almost always means that you are using the wrong algorithm. Brute force exhaustive search won't work for this.
`trace`? But wouldn't you `QuickCheck` your pure functions?
Modern reCaptcha doesn't involve solving puzzles anymore. You just click a checkbox and wait a second or two. It's trivial to implement, and none of the issues you mentioned are relevant anymore. It's well supported by a Haskell library.
[removed]
Where can we see the specification? I think it should be included somewhere in the documentation... I'm asking this, because I once verified my own insertion-sort in Coq. The specification said that the returned list should be sorted. Only later, I realised that `sort ls = []` satisfies that specification as well... So the spec is maybe more important than the fact that it's verified.
Looks less like a cli and more like a tui to me. https://en.wikipedia.org/wiki/Text-based_user_interface
You're right, it is, I didn't even know that acronym existed 
In my opinion the ruby one is useless (though highlighting search above all else is not bad, and npm does this too). The rust page is quite good and we can take inspiration from it. Python's is not very fancy looking, but the information density is good: https://pypi.python.org/pypi erlang/elixer isn't bad either in terms of picking lots of good dynamic content to highlight: https://hex.pm/
Why? I think this question is important. As I see it, there are basically two reasonable answers: 1. For fun. Whoever is doing the modifications has a blast with that kind of thing. Then: yes please, knock yourself out! 2. For function. Then I say no. Several websites I use regularly (including Hackage) have had their look overhauled in the last few years, and it's never made a lick of difference to me. It always ends up feeling a bit like wasted effort that could have been spent on something meaningful instead.
Perhaps you could try and put yourself in the position of a newcomer who might feel overwhelmed by all the information and just wants to get tinkering quickly. Why? You ask. Usability, by decreasing the information density in the main page.
There already exist `type-spec` package for type level testing: * https://hackage.haskell.org/package/type-spec Could you please describe difference of your package from `type-spec`? And, perfectly, put this into `README.md` for your project repository.
Heyo. I'm a designer and I've been wanting to overhaul everything about Haskell's public interfaces for a long time. I'd be willing to lend a hand if there's support for it.
Yeah, I had a similar assessment. However for a run time of 48H, even if `intersect` is the cause of a n^2 behavior, would changing it be enough to make it fast? I'm not that confident without profiling.
I tend to use modules whenever they contain enough code to be feasible (data type plus at least three exported functions) and import them qualified. `Person.name` looks nicer to me than either `personName` or `name @Person`.
I guess useless depends what your purpose is for going to hackage. It's interesting to see the different usecases and workflows get listed in the GitHub issue, I'd encourage other people to join in there! Personally, I'd greatly prefer the focus on search, over the more cluttered crates one, but that's my personal preference.
YES! Could you tell us more? What parts would you update, and how ?
As a long time user of hackage, I don't use the homepage at all and I like it like that. I just type modules like "data.csv Haskell" in Google, jump to the hackage page and sometimes correct the version I land on. This generally works great for me. I guess this is an argument for making the homepage targetted at beginners who might use it, and assume experienced users will not need the homepage. I could consider using the homepage, if it provided one stop shopping summarizing latest activity on all Haskell feeds - hackage uploads, Reddit posts, mailing list emails, Haskell weekly, irc channel activity, new GitHub repos, new Snoyman controversies (:
It might be surprising how much simple optimizations change the running time. You really do need to profile. It could be true that you cannot brute force search but profiling and benchmarking would reveal this quickly.
I just tried it and it works. Personal preference I guess.
In that same vein, Oberon provides a somewhat larger Pascal-like language.
That's only true if there are certain cookies and characteristics of your browser Google deems as signs of knowing you from before. This is very spotty and I guess you're suggesting one is always logged into a Google account as well. It's good to hear that there are those who never get asked to solve the puzzle, but for those who are regularly reCAPTCHA is the worst (hardest to solve) of them all. Also given the global reach of reCAPTCHA, I stand by my assessment that it cannot be legal in all browser's ip locations due to free labor provided to training autonomous vehicle models. I believe you when you say you never see a puzzle, but I hardly ever get a puzzle-free reCAPTCHA. Once a month maybe. There's maybe an option to choose reCAPTCHA in lowest level to increase the chances of that, not sure.
https://haskell.org/news has some of those.
Cool stuff. Implement mc directory comparison and two pane view, and I might switch ;).
It's usually installed with npm.
FWIW, the rubygems color scheme is very unergonomic. I like the current layout, but instead of the wall of text on the start page, either a list of new/updated packages like https://hex.pm would be more useful. Competition statistics like crates.io's "most downloaded" are risky and encourage gaming. What would actually be useful is a list of what's broken with the latest GHC release. A prominent display of infrequently maintained or lagging packages could be a lot more useful than "package Cassava is the most downloaded". In terms of layout, do not waste whitespace and do not introduce glaring light colors or low contrast text. Those seem to be currently popular design anti-patterns.
&gt; What would actually be useful is a list of what's broken with the latest GHC release. A prominent display of infrequently maintained or lagging packages could be a lot more useful than "package Cassava is the most downloaded". This is a very interesting observation, but I wouldn't put it on the Hackage landing page. It's more useful to library authors, and as such I think would fit in the Upload page, if anywhere. Moreover, how to generate this information? From Stackage membership information ? 
Your problem is algorithmic, but you probably knew that since your program works on smaller data sets. The problem is an exact cover problem, so you might want to try to research how to transform it into a form where exact cover algorithms works and implement one.
I also turn to google when I'm searching for haskell packages, but I don't do that for any other language. That is a sign that there is something wrong with hackage's front page.
Sure. I have no objection to the content of you original message; only it's tone. Remember that most people won't know that you do FM or that you have even-handed thoughts on the paper. A less aggressive comment might have been &gt; The package oversells itself; using `hs-to-coq` and then verifying the resulting translation loses important aspects of the Haskell code (such as laziness), and there is no proof that `hs-to-coq` is itself correct. A link to [this](https://github.com/nuprl/website/pull/118#issuecomment-358595090) post elaborating slightly on `hs-to-coq`'s shortcomings would also have been welcome. As a question for you, what are the odds you'd give that an arbitrary project "verified" by way of `hs-to-coq` still contains a correctness bug? What if the project doesn't use clever tricks with laziness (i.e. no knot-tying; no infinite lists)?
https://matrix.hackage.haskell.org ?
Thank you, multiple panes is a very good feature, I'll try to implement it, but I know nothing about mc directory comparison 
Can't find a success/failure matrix for GHC versions. Do you know where?
Start mc, navigate to a different directory in the right pane, press C-x d. Need more info?
I'm glad you got this put together! As someone who knows nothing about packaging, I suspect package numbskulls like myself will have better luck with different module names. But as someone who knows nothing about packaging, I should leave that decision to wiser heads.
From a quick look I think you try putting each polyonimo at every position. The first thing I'd try instead would be to fill up the space in order, say top-to-bottom left-to-right. For example if you've filled in the X parts here, XXXXXXXXXX XXXXXa.... .......... continue by trying to place a piece that covers 'a'. Then you can give up immediately if there is no piece that can fit there, rather than not finding out until you have filled up most of the board.
Imo, the ReaderT pattern is not something you want your business logic interacting with whatsoever. It's there to handle dirty low level details like exception safety. So it should be abstracted away so that the other layers never have to think about it
just search for package pages within it. e.g. https://matrix.hackage.haskell.org/package/lens
Nope, I will have a look at it as soon as I can 
https://arxiv.org/abs/1803.06960 discusses the problem of finding good specifications at lengths. In particular, we don’t just come up with our own specifications, but also prove externally sourced specifications: QuickCheck properties, and Coq’s abstract interface for finite sets. Some clear and Haskell-user-understandable summary of the spec would would be useful. But as a first approximation, simply look at https://github.com/haskell/containers/blob/v0.5.11.0/tests/set-properties.hs
To offer an example of what specification we use for verification: consider insertion, our spec says (1) insertion preserves the ordering in the tree, (2) it preserves the balancing conditions of the weight-balanced tree, (3) the `size` field at each node in the tree is maintained correctly (the `size` equals to the number of all its descendants), and (4) the tree returned by this operation contains the inserted element and all elements in the original tree, but nothing more. In addition to that, we also have many other sorts of specification coming from multiple sources. More details on how to find good specifications can be found in our paper.
Very, very cool! I love the use of borders to simulate tabs.
Yes, there is. I did have a look at ListZipper, but I figured that, because both my use of the zipper had always at least an element, some functions had to be remade (e.g. "remove" needs an item in case your are deleting the last element in the list) and some new function would have less cases to check with this same constraint. Also, I had already wrote almost all of the zipper functions (for the entries in a tab) without actually realizing I made a list zipper, so I ended up refactoring instead of adding a dependency and rewriting functions. I hope I was able to explain myself. I did not see pointedlist, it seems from a quick look that it has what I need, I will take a closer look soon. What about you? What made you contribute to the problem? :)
(It also handles a wider variety of input events than you currently handle.)
Nice, hidden but it's there if you know where to look. It seems to miss 8.4. Now if we could get the failing reports extracted into an overview page, that would be what we're looking for, no? I know it's practically impossible, but the naive developer in me thinks a new ghc release should be blocked if there are more than 10 packages that fail to build with the release candidates.
&gt; wasted effort volunteer contribution is not zero sum. If someone cares about hackage, and have experience with design or UX, they might fix it. That same person might not have spent their time on something else arbitrary. 
I know about the widget list and I avoided it for a couple of reasons: first of all I had initially thought of renaming inline (as most file manager with a gui) and doing it while trying to figure out how to use lenses (which I ended up not using either) was to much to ask of myself as I knew nothing about brick and I was getting nowhere. I also had another practical reason that right now I cannot remember at all, but I will reply to you if it comes to mind. Apart from these I agree, the list widget is a perfect fit, I will look into it further and switch to it if I can. I will still need the zipper for the tab list though.
Thanks! I just realized you are the one who made brick so I will also thank you for it, it's a very nice library! 
They should definitely document that explicitly
Can you provide an example? How does `HasField "fooX" a Int` or similar dispatch to the right `HasFoo`? Thanks!
I definitely think holding off on the simple accessor instance for functions was correct. 
I've done that, but then you get the `Data.Map` where every single importing module must import something qualified, as well as a separate import for the type name for convenience, since you can't mass reexport them as namespaces :/
I do that, (with the `&lt;_Type_field&gt;` naming convention in `reflex`), and vendor `dropTypeName` into all my aeson projects, but it's still obnoxious. 
Very cool, looks like not very much source code for all that it does.
You're welcome!
I don't use a `HasField "fooX"` class -- largely because I find it has awkward scoping/orphan isues, but it all just works when you let `makeClassy` build instances (and classes) for something class HasFoo a b | a -&gt; b` where foo :: Lens' a b with the right naming rules. `makeClassy` doesn't actually invoke the field accessors themselves, it does positional pattern matching after it figures out where from the names.
But if the fix is mechanical like that, hackage ought to posses the capability to issue a compatibility patch release until the maintainer issues a real release. No?
I meant automating the bound relaxation (or simply neffettivamente otifying the maintainer) only if the package builds with the new base/other package that got updated. Automating fixes is definitely not so mechanical
About the same story! I need a `NonEmpty` list zipper and did not find `pointed-list` until after having implemented one.
We have a number of specifications: * The quickcheck properties, interpreted as theorems and proved * Coq’s abstract interface for finite sets * Our own specification, based on “obviously correct” implementations of sets as functions of type `a -&gt; Bool` We also prove the internal invariants of the `containers` set data structures (ordered search tree, balancedness). For more details, see https://arxiv.org/abs/1803.06960.
Why not use Bootstrap?
Upper bounds adjustment is what I meant. Isn't that mechanical and could be done by Hackage on the server? Those would be special releases like text-2.3.1.0+8.4.1.
I will take a look at it. If you have any specific functionality you would like to see let me know
Thanks, I think that libraries are mostly responsible for that. 
Hackage and the Haddocks are a package. The Haddock work just isn't quite finished/merge yet: https://github.com/haskell/haddock/pull/721 The HaskellWiki has been trying to upgrade theme for a while, but it involves upgrading the mediawiki version, so things have been terribly slow. (https://github.com/ppelleti/haskell-wiki-bugs/issues/7). (I am assured some progress is being made, but if someone wanted to volunteer to help, it would certainly be welcome!) Hoogle could certainly use some attention and I don't think has gotten any! Given the size of hackage, a hackage-wide hoogle instance isn't really a reasonable thing to try to build or want. But I could imagine doing something with a more restricted universe of packages -- this is what makes the search on stackage hit a sweet spot.
Oh, and just to be able to field the claim that this is less useful because it does not allow reasoning about infinite data structures: I just added opt-in support to translate to Coq’s support for coinductive data types and verified this infinite trie structure: https://github.com/antalsz/hs-to-coq/blob/master/examples/coinduction/Memo.hs The proof in https://github.com/antalsz/hs-to-coq/blob/master/examples/coinduction/MemoProofs.v states that `lookupTrie (mkTrie f) x = f x`.
There's a lot of sentiment for something like this, and some ideas on next steps to get us there. Essentially we'd want some variant of so-called "soft" or "bumpable" bounds, as distinguished from "known-bad" bounds, and we'd also want automated build and test infrastructure related to this (which the matrix builder could be the basis for). cf also some empirical research: http://ifl2014.github.io/submissions/ifl2014_submission_14.pdf
impressive, thanks
I was first introduced to Haskell in 2009. I had no idea all this was still so young then!
My opinion is that it is definitely worth the $20-30. For learning it is going to be a help. The 3 pane approach (code, test variables, information about expressions) is good. Syntax completion and color coding are ok. The included environment is good to start with. More or less for the I'm getting started and want an IDE I'd say a no brainer. 
&gt; Want polymorphic lenses? makeFields has you covered! We ran in to compilation bottlenecks with `makeFields`. Classy lenses are great, but they incur coherence issues with class definitions. Either you need a single module (compilation bottleneck) where all your lenses are created or you define the classes separately from the instances. Defining classes separately from instances is certainly an option, but it ends up with two sub-optimal paths: 1. You define all the classes you'll ever need in a single module. This ends up also being a compilation bottleneck. Anytime you introduce a new class you have to recompile the world. 2. Every class gets its own module. This avoids the compilation bottleneck, but it means you have an ocean of tiny modules with piles of imports in every `data` definition point.
It's generally not too hard to dig down and add `MonadLogger` constraints and rewear pure functions in `m`. As long as that's the only constraint, you know the functions are still pure. The type system makes this chore totally safe
Pressing / to search the filenames would be awesome (as in searching in vim), as well as the ability to sort based on filename or date. I could keep going :)
They seem both useful and should not be too hard to implement, consider them in the TODO list. 
`ReaderT r IO a` gives *awesome* type error messages. Usually you just need a `lift` or two and you're golden -- this is annoying (eg in `Conduit i App o`), but it's so worth it. Every type variable with constraints is a point where GHC can give you bad error messages, so avoiding them where possible is great for making code more understandable, *especially* in the hairy/tricky bits where you want to be in Layer 1.
Probably but they may not have set up a very *good* contract even if they're good people, and if they have investors and the company doesn't perform quite like the investors wanted, said investors can start strongarming the company and without the right employment contracts, also strongarm the employees.
Um... {-# LANGUAGE DataKinds, TypeOperators, GADTs #-} import GHC.TypeLits test1 = () :: 3 ~ (1+2) =&gt; () -- ok test2 = () :: 3 ~ (1+5) =&gt; () -- type error! 
I'd be interested in maintaining the hosting side of things including modernizing it some. I have a few large servers I could move it to after updating and implementing a spam filtering system that works better. I have a slot of time open as I'm closing down a previous project.
&gt;what are the odds you'd give that an arbitrary project "verified" by way of hs-to-coq still contains a correctness bug? Seeing as I've encountered correctness bugs in _end-to-end_ verified code (always due to the verification gap, even though this gap is substantially smaller than this example). I'm not going to wager that there are no correctness bugs in containers. 
AFAIK, bootstrap 4 uses CSS grid, so I guess the end result is the same? I think it's mostly up to the developers, but I doubt that they'd be using any of the default bootstrap styling, in which case just using grid on its own is pretty much as simple(if not simpler, even) than using bootstrap.
That’s cool! I haven’t played with the constraint approach yet.
Functor unfortunately tells you nothing about the role of its argument. Ideally it would be representational, but there are plenty of instances folks make up that might have some set of GADT base cases, and explicit maps being tracked, which would force you to have a nominal role, where the functor laws are merely preserved by quotienting out what information you can get about that ADT outside of the module. With QuantifiedConstraints (they aren't out yet) one could envision changing the constraints on Functor to be like: class (forall a b. Coercible a b =&gt; Coercible (f a) (f b)) =&gt; Functor f or to weaken HasSubPart to take something not quite a "lens" class HasSubPart a where subPart :: forall f. (forall x y. Coercible x y =&gt; Coercible (f x) (f y)) =&gt; Functor f =&gt; (SubPartType -&gt; f SubPartType) -&gt; a -&gt; f a which can probably be coerced with QuantifiedConstraints. Another solution would be to use a fixed functor type that is known representational that characterizes the lens. e.g. `ALens` uses `Store` or `Context` for this purpose. class HasSubPart a where subPart_ :: ALens' a SubPartType subPart :: HasSubPart a =&gt; Lens' a SubPartType subPart = cloneLens subPart_ This has the benefit that it works today.
I am definitely in favor of doing a major overhaul of hackage in general. The recent update with the two-column layout was very nice, but it's not enough. Like some others that have commented, I, too, almost never find a package directly via the hackage homepage. Here's my workflow: 1. If I frequently visit this package's documentation, then I am often able to type a bit of the name and chrome shows me a direct link, which I use. 2. Otherwise, I just type something like `the-package hackage` in the address bar and go via google Visiting the homepage, there's a couple of things that stand out to me. - If I were to use the homepage to find a package's docs, then nearly the *entire* content of the homepage is irrelevant noise to me for 99% of my use cases. - The search bar, which is almost the entire reason I would like to go there, is way up in the right corner kind of like an afterthought. I'd honestly prefer to go straight to a search bar, and if it had auto-completion which could take me directly to a package, that wouldn't hurt either. I also have a few thoughts about the package documentation pages. I frequently want to find a specific function in the documentation, and I don't always remember which module it's in, so I go to the index. Here's what I need to do to get to the index and find a specific function - Click *any module*. - Click index way up in the right corner. At this point, we're greeted with a mostly empty page which has this tiny line of letters way up at the top. Just.. why? - Click "All" to get the entire index. - Ctrl+F for the function At this point, I can't even just press enter to follow a direct link to the function, because for some reason the link to the function is actually on the name of the module that's next to the function. I use the keyboard for as much of my workflow as possible, and as such I use a "vim-ish" extension for navigating websites. I can just use that to follow the link, but there is so much space between the name of the function and the link to it that it sometimes gets hard to tell which of the links is to the function I was looking for, since it's likely that all the links look the same in the vicinity. I literally sometimes feel like I have to move the mouse over to the function name and then move it horizontally until I hit the link, so that I'm certain I follow the correct link. I probably have some more minor annoyances, but I can't quite think of any right now. I'd be more than willing to pitch in to improve things, but even though I'm pretty good at being annoyed by poor design, I've come to realize that I'm much worse at improving it. 
How much would we lose by adding `Representational` as a constraint to `Functor`? I'm having trouble imagining a useful gadt that would be a functor only in this quotient sense, though I don't doubt they could exist
Check out `Data.Biapplicative.traverseBia` in the master branch of `bifunctors`. That uses such a beast.
I wouldn't be surprised. I'm not as confident as I'd like to be about `alterF`-related CPP in `Data.Map`.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://github.com/commercialhaskell/rio) - Previous text "rio" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
afaik, in the same let block, like with multi-line input: :{ let .. :} But not individually 
That optimization sounds like it will be very helpful, thanks! I haven't had time to work on this until just now due to finals, so I'll reply with that timing data soon. I just noticed a pretty huge bug. At some point I accidentally removed the code to terminate rays after a certain number of bounces, so in recent builds rays were only terminating when they escaped the cornell box. I'm now giving up after somewhere between 2 and 4 bounces, which makes a single sample take about 5-6 seconds.
Actually no, you can do mutual recursion just fine with multiline blocks even without a `let`.
I'm pretty sure Bootstrap 4 is Flexbox based. While Flexbox arguably still has its place, even inside grid layouts, I find [this](https://getbootstrap.com/docs/4.0/layout/grid/) way more verbose and less flexible than the examples above. You also have significantly less control over your layout. [CSS Grid: Moving From CSS Frameworks To CSS Grid (2018 and beyond)](https://www.youtube.com/watch?v=paMmgo4MhQ8)
I use [Leksah](https://hackage.haskell.org/package/leksah) for hobby projects, and I like it. It has the essentials covered (code sense / function lookup / hints, linting), a nice set of useful features, and is a fun, productive environment (once it's set up). The meta-data provides a great way to navigate, learn, and develop a code base. It is actively developed, and written in Haskell, so the better it gets, the better the Haskell desktop application story (hopefully). The installation story for Leksah has improved somewhat (now with Nix support); however, depending on your platform (and which versions of the dependencies you install), you may need to do some extra work. E.g., I build leksah with Nix on Ubuntu using the latest version from the [git repository](https://github.com/leksah) to work with newer versions of the gi-* packages. Your mileage may vary, but I find the features and user-experience more than worth the installation effort. 
[CSS Grid: Moving From CSS Frameworks To CSS Grid (2018 and beyond)](https://www.youtube.com/watch?v=paMmgo4MhQ8)
https://hackage.haskell.org/package/kan-extensions-5.1/docs/Data-Functor-Coyoneda.html http://hackage.haskell.org/package/free-5.0.1/docs/Control-Applicative-Free.html
It's not a "complete" text as you're looking for either, but another one to add to your list: my summer school notes "Applying Type-Level and Generic Programming in Haskell" from the Summer School on Generic and Effectful Programming 2015 (and somewhat revised from time to time after that). At Well-Typed, we also offer https://skillsmatter.com/courses/504-well-typed-guide-to-the-haskell-type-system / http://www.well-typed.com/services_training_type_system/ but the course materials for this are currently not public.
Justin Le's [blog](https://blog.jle.im/entry/introduction-to-singletons-1.html) [posts](https://blog.jle.im/entry/introduction-to-singletons-2.html) are also a good resource: 
Thank you, Andres. I guess I missed it because primary sources are easier to track. But it is so painstaking to systematize them. Even the table of contents of your work is much more comprehensive than what I could put together by now.
It does break type safety, although my counterexample uses an unlawful functor. I would also be interested in a counterexample with additional restrictions: if it exists: assume the `subPart` instance we want to coerce applies `fmap` at least once to get from `SubPartType` to `a`. By parametricity of `fmap`, is it not always valid to coerce `fmap (f :: X -&gt; Y) (x :: F X) :: F Y` to any type representationally equivalent to `Y`, even if the `Functor` instance is unlawful or partial? ``` {-# LANGUAGE GADTs #-} module BadCoerce where import Unsafe.Coerce data SubPartType = SubPart class HasSubPart a where subPart :: Functor f =&gt; (SubPartType -&gt; f SubPartType) -&gt; (a -&gt; f a) instance HasSubPart SubPartType where subPart = id newtype T = T SubPartType instance HasSubPart T where subPart = coerce_ id -- This simulates newtype coercing "HasSubPart SubPartType" where coerce_ = unsafeCoerce data BadFunctor a where Bad :: SubPartType -&gt; BadFunctor SubPartType instance Functor BadFunctor where fmap = undefined -- it's not actually a functor oops :: BadFunctor T -- Shouldn't be possible oops = subPart Bad (T SubPart) ```
I learned most of my tricks from [Renzo's SOT library](https://ren.zone/articles/opaleye-sot). I also wrote a similar guide on the [derivation of type-safe grpc for proto-lens](http://reasonablypolymorphic.com/blog/type-directed-code-generation), which covers variadic functions, type-level strings, custom type errors, and other trickery.
&gt; Where possible, Haskell should appeal to newcomers. It already has a reputation for being unapproachable, and the resources seem to assume that anyone using them is already a Haskell user, whereas other languages assume far less about the user's background. A-men. Can we carve this in stone above the main entrance ? Jokes aside, you're totally right. We are still plagued by multiple UX/UI issues, and it's high time to fix them. From harmonizing the page stylesheets to more intuitive package/module search to enforcing a stricter documentation policy (Hackage could reject under-documented modules perhaps?). Pursuit is so polished and I'd love if Hackage looked and behaved that well. /u/sclv , what do you think of a Hackage-hosted instance of Hoogle but restricted to the packages in Platform and few common others? /u/--cmr , could you join the discussion on https://github.com/haskell/hackage-server/issues/725 ?
Yes, I had given this some thought. If it were possible to link directly to the module where it's defined in, I think that would be best. Currently, we show alternatives like `type/class` and `data constructor` below the name if there are multiple things with the same name. Maybe it would be possible to do something similar and denote it as `re-export` or some such.
The posts expand on "Dependently Typed Programming with Singletons". Perhaps one should read that paper first, and give a look to the [last version](http://hackage.haskell.org/package/singletons) of the package as well. I also have found the [fin](http://hackage.haskell.org/package/fin) and [vec](https://hackage.haskell.org/package/vec) packages quite usefult to get a sense of what are the "basic operations" with depently typed programming in Haskell. They don't use the singletons package and define their own singleton types instead. The documentation includes useful overviews of related libraries.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](http://hackage.haskell.org/package/fin) - Previous text "fin" [Here is link number 2](https://hackage.haskell.org/package/vec) - Previous text "vec" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
&gt; what do you think of a Hackage-hosted instance of Hoogle but restricted to the packages in Platform and few common others? The idea sounds a bit weird to me. I'm also not sure about how to have "two types of search" nicely. In particular, we want to be really good about package discovery, and have been making some strides there. I'm not sure how hoogle-search would fit in, or how we could distinguish the two enough that people wouldn't accidentally use the wrong search and get more confused. (And I think admixing their results will give a worse UI for both). And also if you could hoogle search hackage and get some but not all packages searched, that tends to feel as though it would violate some principle of least surprise. I don't think it is insurmountable, it just needs good design. But opening a ticket to hash things out sounds like it makes good sense. (Also, what if we just used the hayoo API, which already searches all of hackage in a non-type-based way?)
[removed]
Sounds reasonable. If you flesh it out, opening an issue on the haddock tracker would make sense.
Yes, I read this post some time ago and I find it to be the most accessible of the dependent typing related writings of those I have acquaintance with. What I did not get from it is the overarching strategy, the architectural feel of the devices it introduces; it is more like a report, a collection of isolated devices. Type families as functions on *(for example)* natural numbers on type level clicked for me when I went through the code examples, but I cannot say I understood how to apply them to actual code and what for.
Yes, also potentially being dismissed without severance, as well as losing equity or options.
And the associated documentation such as https://hackage.haskell.org/package/streamly-0.1.1/docs/Streamly-Tutorial.html, and these benchmarks are a good sign.
Thanks Ryan, sounds awesome! will take a look.
Tomas Petricek explained a similar difference to me between F# and Clojure in 2010: https://stackoverflow.com/questions/2570628/difference-in-f-and-clojure-when-calling-redefined-functions
Can someone explain the following `MonadTime` instance? instance MonadTime ((-&gt;) UTCTime) where getCurrentTime = id Does it mean that any function that returns a `UTCTime` is now a `MonadTime` instance? And, if so, what’s the use case for this?
Please provide more information
Do you want a link to the discord 
What sort of thing is it? Like, is it a job, is it a study group? 
Not really job but not group it’s in between something like that 
If you’re interested I can send you link to discord and more info will be provided 
Here’s an invite https://discord.gg/hh6rV
Thanks I'll check it out
Have you joined the discord 
I agree Rust is closer to Kotlin or Swift than it is Haskell, but the knowledge I gained from learning Rust definitely helped me in learning Haskell. In particular: * Traits are very similar to typeclasses, moreso than interfaces are. * Both languages have Hindley-Milner type inference, and gaining intuition for it in one language helps with the other. * This may seem small, but "let" being immutable by default is a big shift in thinking which can start in Rust. * There is (essentially) no global state, so all a function can do is return a value and mutate its arguments. (And once you've learned Haskell, that'll help you in learning Coq!)
You should really write a bit about what this is about here
Well this is the description we are doing Haskell projects as you choose or the Haskell group chooses 
Well this is the description we are doing Haskell projects as you choose or the Haskell group chooses 
That doesn't really tell us anything. You should go more into detail here, instead of saying more info in discord imo.
Rust may help you learn Haskell by exposing you to traits/typeclasses with associated types/type-families, sum-types with pattern-marching and the ideal of immutability being a default. For a Java developer, Rust is a good way to get into the ML way of programming, and in particular get used to structuring apps without OOP. However there is still substantial differences between the two. The best intro languages to Haskell are Elm and Purescript, which both have vastly superior documentation (the Purescript book online is excellent) and a very similar syntax. I would say Elm is the best gateway language on the slippery slope to monads. 
What do you want to know
Well if you want to join it’s we choose a project the Haskell channel on my server and we do it 
I feel that this is the best answer.
Maybe you could elaborate...
You want to help me and my software group build Haskell programs 
Wasn't this same message posted a few hours ago, with a similar lack of detail? 
I'm saying this looks like spam,particularly with the repeated posts. I'd suggest that if you're not willing to share details in here, that you probably shouldn't be posting about it here. 
It’s not bad as not spam but what the projects are is chatbots simple games and programs 
Looking at the post history, I am convinced this is a chatbot built to recruit someone to build a chatbot.
No not a chatbot 
I thought it was interesting how this essentially implements elements of [Go's versioning proposal](https://research.swtch.com/vgo-intro). So cool that it's a one line patch!
Rust syntax is closer to Scala than Haskell. I knew HS before rust and not many things in rust remind me of HS. At least not syntax wise. 
How do people currently work out what dependency versions their packages are compatible with? My rather dumb way, is to use a script bisects stack LTS releases and set the package lower bounds to what they were in the oldest LTS that worked
To format code in a reddit post, begin each line with 4 spaces. This is very, very difficult to read.
Sorry its hard to read. Is there any way I could ake it eaiser to read.
&gt; To format code in a reddit post, begin each line with 4 spaces. 
Yep that's the code 
how do I get to ask the GHCi the question of why is x = y 
I'm not sure how new you are to Haskell so please forgive me if I'm suggesting something you already know or have tried, but I would like to recommend using the tracing functions in `Debug.Trace` ([see here](https://hackage.haskell.org/package/base-4.11.0.0/docs/Debug-Trace.html)). They're very, very useful for working out how you got an infinite loop when you don't wanna bolt IO all the way through your code.
Formatting that way actually really sucks in JSON because trailing commas are invalid. It makes adding and removing fields a hassle.
I actually started with Rust, then gradually learned more and more about Haskell. Getting experience with `Option&lt;T&gt;` and algebraic data types etc, then hitting walls when I start to need more advanced things like GADTs, higher kinded types, explicit effects, etc. has made it much easier to understand what is going on in Haskell. It also makes me appreciate the stuff you can write really succinctly in Haskell vs Rust, but also the stuff that Haskell has a hard time doing. I think both of them support each other.
Thanks for the additional insight. So in the general case, this is unsafe. But to get this instance working "today", I think it's safe to assume that Lens probably doesn't know about the newtype I'm writing in my own program right now, and so probably won't use any GADT/type family/etc. that can look through the newtype constructor and use a different type internally that will cause problems when coerced. I would think that the instance could be implemented by doing: newtype T = T ImplementsHasSubPart instance HasSubPart T where subPart = unsafeCoerce (subPart :: Lens' ImplementsHasSubPart SubPart) I would hope that the lens library wouldn't be able to do any unsafe operations on this type, not having any idea that T even exists. I suppose *I* might be able to, although in practice I only ever use some standard operators from the lens library over the value returned from this instance function. Perhaps even an instance derivation of this type could be auto-generated via TH? In my particular use case I use a lot of different newtypes over the same base container type, just to help keep type signatures readable/intuitive, and while it really isn't much of a hassle to manually write the instances for them I'm kind of curious now about I could reasonably get a somewhat safe alternative automatically generated. Unfortunately GHC gives me this error when I try to do that: Could not deduce (Functor f0) arising from an expression type signature from the context: Functor f bound by the type signature for: subPart :: Lens' T SubPart at ... or from: Functor f1 bound by an expression type signature: Lens' T SubPart at ... The type variable `f0' is ambiguous I'm not really sure how to interpret this, but it seems like GHC doesn't want to instantiate the return type of unsafeCoerce with the Functor constraint on it? I would have thought that it would automatically type check just via the signature of unsafeCoerce.
If I have a good feeling I'm using a newer API I add a bound. Otherwise usually by user bug reports of failed compiles. 
I'd say learning Rust is definitely a good idea. However it you're looking for a language that's sort of an entry level rug on your way to Haskell I'd recommend one of the less strict functional languages. - *Clojure* will help you with getting familiar with functional langauges, but it won't help with getting familiar with the type system - *Elm* is a great language that has a similar, though much simpler type system than Haskell (no type classes, monads or higher order types). It'll teach you the syntax, type inference, some basic naming conventions and pure functional thinking with **really nice compile errors**. They try very hard to make it very beginner friendly - *Ocaml* is also very similar to Haskell. Very similar Syntax (ML-style) and a lot of similar concepts. The type system is also similar to Haskell. Unlike Haskell and Elm it is not pure, so you can use things like `print` in any function, as you may be used to from imperative languages. And also the type system is a bit simpler than Haskell. My personal favourite to get started with is *Elm* but I also think *OCaml* is a great choice. The latter is a bit more useful, but I think Elm is the better teacher, especially because the compiler gives you very friendly and easily understood error messages.
I'm pretty sure it does work for `Coyoneda`.
IIRC Rust *used* to have full-blown Hindley-Milner type inference and then abandoned it in favor of something a bit simpler.
I'd suggest staying with the (probably newtyped) `ReaderT AppData IO` for the reasons [explained here](https://www.fpcomplete.com/blog/2017/06/readert-design-pattern). It's basically the one monad transformer transformer on top of IO which allows you to safely use concurrency functions like `forkIO` or `concurrently`, which are conveniently [provided](https://www.stackage.org/lts-11.1/package/unliftio-0.2.5.0) by implementing [MonadUnliftIO](https://www.stackage.org/haddock/lts-11.1/unliftio-core-0.1.1.0/Control-Monad-IO-Unlift.html).
True, Rust's type system is close to Haskell, but in practice I was put off by the huge amount of boxes and other wrappers necessary to get anything complex done. Maybe it has improved in the meantime :)
Coming from Haskell to Rust did help me understanding Rust's immutable variables and it allowed me to quickly be productive. I still tripped over Rust's memory handling, though. Specifically the ownership ideas and necessity to copy variables surprised me. Before passing them along to multiple different functions/methods. The clear compiler errors and documentation helped get past this quickly, however. One thing I still haven't mastered, however, and where I think knowing Haskell doesn't help, is Rust's lifetimes. That stuff still confuses me and I don't know how to make effective use of it without getting burdened by too much notation.
The type inference is better than c++'s auto. For one thing, subsequent usage of variables can determine their type, something not possible in c++. let v = Vec::new(); v. push(42i32); // v determined to be a Vec&lt;i32&gt; here
A note on naming. The first blog post defines `AppT m a` as `ReaderT YourStuff m a`, not `ReaderT App m a`. A Monad transformer such as `StateT s m a` typically comes with a non-transformer version, `State s a`, defined as `StateT s Identity a`. So I would expect the name `App` to denote the non-transformer version of `AppT`, not the type of the data to which the `ReaderT` has access. How about naming this data `AppConfig` instead?
Stackage build reports
Depending on how serious you are, Haskell may not be a good choice for real-time audio at all. The garbage collector can prevent your program from filling the audio buffer in time, resulting in a buffer underrun and noisy glitches. You don't want that if you're recording or on stage.
Does any tooling help you figure this out or do you need to remember which functions you use from every library and look up when they were added (in their current form)?
As lower bounds only ever *increase* (at least, in the normal case), I don't really need to remember when things were introduced.
unliftio only supports `ReaderT`... and types which are isomorphic to `ReaderT`. The OP is creating a newtype wrapper around `ReaderT`, so that newtype is just as suitable for use with unliftio.
I've actually done some work on this using accelerate and the jack audio driver. Ping me if you want to join forces. 
Can't you use some tricks/tune he GC so it runs at each cycle, so its constant, or things like that? I'm sure I've seen topics around about this. Last time I think was about some company that had hard time constriants on web request, and they had to ditch haskell at the time, but from the comments we had options to go aleviate now. It had a SO question too.
&gt;&gt; Why does `Real` require `Ord`? &gt; `toRational` implies a natural ordering. This is true, but Ord implies the ordering is total. Which is seems overly restrictive here. I wish we had a TotalOrd and a PartialOrd.
Yes, that's [what gloss does](https://github.com/benl23x5/gloss/blob/master/gloss/Graphics/Gloss/Internals/Interface/Display.hs#L54) for example. This is a tradeoff though, not a panacea. If you let the GC run when it wants to, most of your frames will be fast but some will be noticeably slower, causing a noticeable pause. If you force the GC to run on every frame, then all your frames will be equally slow, so your framerate will be lower but there won't be any noticeable pauses. If your application already needs to perform a lot of work on every frame, this extra cost per frame might be enough to disqualify Haskell as an implementation language. Moreover, since the GC duration is proportional to the total amount of non-junk memory, not proportional to the amount of junk created during the frame, this cost per frame will be higher for applications which need to keep a lot of data in memory. I think there are special ways to allocate memory which won't slow down the GC ([`malloc`](https://hackage.haskell.org/package/base-4.11.0.0/docs/Foreign-Marshal-Alloc.html#v:malloc)?), but that has costs elsewhere, as accessing the allocated data is no longer as straightforward.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [benl23x5/gloss/.../**Display.hs#L54** (master → b41f867)](https://github.com/benl23x5/gloss/blob/b41f867ffb6c05a40f778a29f99933b603c11a31/gloss/Graphics/Gloss/Internals/Interface/Display.hs#L54) ---- 
The real problem is more that with reals comparisons can bottom out so you want some kind of pump you can run to a given tolerance or a partiality monad, not the usual Maybe Ordering. 
The problem with your unsafeCoerce is it requires an impredicative instantiation of the type variables involved. In theory you could use instance HasSubPart T where subPart = runLens $ unsafeCoerce (Lens subPart :: ReifiedLens' ImplementsHasSubPart SubPart) to fix up this need for an impredicative `unsafeCoerce`. The unsafeCoerce above is truly unsafe, `though` as you can use it creatively to set up segfaults if used maliciously. So that is a lot more unsafe than the `subLens_` trick I mentioned where you explicitly use an `ALens` in the class and then reconstitute it in a combinator outside of the class into a `Lens` with `cloneLens`.
It was directly inspired by ocaml - the first rust compiler was written in ocaml after all. 
I've used both jack-audio kit and sdl (in separate projects) to do audio analysis and generation. Haskell is plenty fast enough.
A lazy version of this is running with `+RTS -xc` and pressing ctrl-c. This gives a stack trace of the cost centers.
This is a reasonable way to manage large composite state. I don't quite understand the paragraph about Hindley-Milner. If it's about top-down vs bottom-up type inference, I agree that, in this case, the instance with "top-down inference" seems the better one, some pragmatic reasons (that are implicit in your argument) being that we commonly know the outer context already (thanks to toplevel signatures for example), and that the same inner context could be used by many outer contexts. The other way around just doesn't make much sense to me for this instance. But there are still many other cases where we build up expressions with complex types from atoms with simpler types, where "bottom-up inference" seems preferable. Haskell's type system is rich enough that we are free to choose whichever way is the more convenient on a case-by-case basis.
Having written several non-optimized programs which generate or analyze audio in haskell, I say with experience that the language is plenty fast enough for 44.1khz stereo samples. The gc isn't an issue.
Did you try rates higher than 44.1? Were you using sdl for generation? If so, I imagine most of the generation would be handled by the c libraries so it might not be a fair comparison if ones usecase is not covered by said libraries. 
 Replace the post with this one or no one will see it
Give some examples please? It's not clear what you imagine a valid answer might be
No mention of the Haskell book?!
&gt; I think there are special ways to allocate memory which won't slow down the GC (malloc?), but that has costs elsewhere, as accessing the allocated data is no longer as straightforward. The [compact support (link)](https://hackage.haskell.org/package/ghc-compact-0.1.0.0/docs/GHC-Compact.html) is for that. You can allocate large data structures which the GC will not traverse for changes: &gt; Data in a compact region is not traversed during GC; any incoming pointer to a compact region keeps the entire region live. Thus, if you put a long-lived data structure in a compact region, you may save a lot of cycles during major collections, since you will no longer be (uselessly) retraversing this data structure. &gt; &gt; Because the data is stored contiguously, you can easily dump the memory to disk and/or send it over the network. For applications that are not bandwidth bound (GHC's heap representation can be as much of a x4 expansion over a binary serialization), this can lead to substantial speedups. 
I believe this is addressed by the answers to [*Is the composition of an arbitrary monad with a traversable always a monad?*](https://stackoverflow.com/q/42284879/2751851) at Stack Overflow.
I use this: nnoremap &lt;silent&gt; &lt;leader&gt;st :! (cd `git rev-parse --show-toplevel`; hasktags **/*.hs)&lt;CR&gt;:set tags=&lt;C-R&gt;=system("git rev-parse --show-toplevel")&lt;CR&gt;&lt;BS&gt;/ctags&lt;CR&gt; which tracks down the root of my current project, runs hasktags there, and then loads them in vim. Works pretty fucking sweet!
Slightly off topic: The problem description made me wonder: Is there a way to get truly mutable records in Haskell, ideally with unboxed contents? And I don't mean records that have IOVars in them, because the IOVars are additional indirections, which in theory results in extra performance penalty. I suppose with the arrival of linear types, many libraries will want to provide mutable actions under the hood.
https://github.com/ghc-proposals/ghc-proposals/pull/8
No I’m a icing person who has a team of developers We need more active Haskell developers to help not spam or anything 
Haskell analyzed audio (fft, etc) and did generation of audio frames. SDL was used to send the audio data to hardware. No, I didn't try higher sample frequencies.
Sorry, but how can I run this? I installed hasktags on my local haskell package lists, what should I do next?
This is the monad composition problem in disguise. Incidentally, [a thread discussing it has just been posted here](https://www.reddit.com/r/haskell/comments/872694/composing_monads/?ref=share&amp;ref_source=link). In your specific case, we have [the `ExceptT` monad transformer](http://hackage.haskell.org/package/transformers-0.5.5.0/docs/Control-Monad-Trans-Except.html#t:ExceptT)... newtype ExceptT e m a = ExceptT (m (Either e a)) ... so that `(&gt;=&gt;)` does precisely what you want: GHCi&gt; :set -XTypeApplications -- For the sake of demonstration. GHCi&gt; import Control.Monad.Except GHCi&gt; :t (&gt;=&gt;) @(ExceptT _ _) (&gt;=&gt;) @(ExceptT _ _) :: Monad w1 =&gt; (a -&gt; ExceptT w2 w1 b) -&gt; (b -&gt; ExceptT w2 w1 c) -&gt; a -&gt; ExceptT w2 w1 c If you want to see the unwrapped version, substitute the definitions of the methods of the relevant `ExceptT` instances as appropriate.
`$url$` is the field introduced by `defaultContext` for the page URL. I suspect that what you actually want is to add an `urlField "photoUrl"` field to your `galleryCtx` and then use `$photoUrl$` instead of `$url$` in the template.
&gt; then press &lt;leader&gt;st in a Haskell file managed by git and be amazed! Sorry what do you mean by this, I just generated ctags and TAGS files under my directory, and added code line you mentioned above to my .vimrc, then vim my.hs file, still can't locate my flags... 
Just a note there is already a Haskell project named Haskforce. https://github.com/carymrobbins/intellij-haskforce
Pretty close, at least. Keep in mind you can still build things like newtype Partial a = Partial (Partial a) | Done a out of repeatedly using the sum, case above, so you can't fully collapse things to just `Either e (p, a)`.
Thank you everybody to help me better understand monad stack concept. Finally I found the problem and I find the solution. But I am not sure why first version was freezing and second is not. Basically in my first version I use following to make my monad to derive monad reader. instance MonadReader AppState AppM This was giving following warning: /Users/huseyin/projects/spyglass/library/State.hs:20:10: warning: [-Wmissing-methods] • No explicit implementation for (either ‘ask’ or ‘reader’) and ‘local’ • In the instance declaration for ‘MonadReader AppState AppM’ | 20 | instance MonadReader AppState AppM | ^^^^^^^^^^^^^^^^^^^^^^^^^ Solution was to add monad reader to type declaration like this: newtype AppM a = AppM { unAppM :: ReaderT AppState IO a } deriving (Functor, Applicative, Monad, MonadIO, MonadReader AppState) I used `trace` to figure out what is happening. It seems like calling ask function was causing application to go into infinite loop (No other line got evaluated during the call so that should be where it stuck). I am guessing that `ask` call trying to find correct Monad Reader instance but creates an infinite loop. Here is the commit that fixes infinite loop. Does anybody understand why this fixes my code? I thought both implementation would be same. https://github.com/huseyinyilmaz/spyglass/commit/960cddae68727a96e66d7132062833bde16c65d3 
In the few (non-public) packages I've written, I sometimes used functions and data types that are documented to be only available for certain versions. I immediately put this information into the `.cabal` file. Inspecting the changelog is also a way to gather this information. 
Thank you for the suggestion. I used Debug.Trace before but I was not going to use it to solve this one. It helped me a lot to pin point the problem.
&gt; where should I run this command, under my project director Yeah you should run it somewhere in your project. If your project is also a git repository that the script will use git root to find Haskell files, otherwise it'll use your current directory as root. &gt; What do you mean generate tags for GHC, I mean generate tags for the GHC source code &gt; Tags should work only for my haskell project file right fast-tags and hasktags can only generate tags for Haskell files but you can combine tags files, for example if you also have C files in your project after generating Haskell tags with fast-tags or hasktags you can do `ctags --apend **/*.c` to add tags from C files to your Haskell tags file. &gt; I'm really confused, my laptop shows "fast-tags" command not found, but cabal info fast-tags shows I have installed the fast-tags You should add ~/.cabal/bin to your `PATH`: export PATH=$HOME/.cabal/bin:$PATH Hope this helps.
Thanks for the reply. I just implemented your suggestion like : galleryCtx :: Context String galleryCtx = dateField "date" "%B %e, %Y" `mappend` urlField "photoUrl" and using &lt;div class="wrap"&gt;$for(photos)$&lt;div class="box"&gt;&lt;div class="boxInner"&gt;&lt;a href="$photoUrl$"&gt;&lt;img src="$photoUrl$"&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;$endfor$&lt;/div&gt; But it strangely still yields the exact same result.
It gives three sufficient conditions for composing a monad and a pointed functor, but they aren't exhaustive.
My thoughts: How hard is `base` to modify, and what exactly is the process like? (I'm sure it's written down somewhere - I just don't know where). We've had some massive improvements rolled out recently (FTP, AMP), but I wonder how hard it is for less ambitious changes to actually get in. For example, we all agree `head` was a mistake. Can we officially deprecate it, and eventually change its type to `[a] -&gt; Maybe a`? Why and how, or why not?
Hell yeah I want to join forces!
I like BasePrelude -- just import everything in base by default, and prefer polymorphic versions in the case of overlap. Drastically cuts down on imports, but there's no magic.
Second try, now with a bit more thought. [This is the definition of `makeItem`](https://hackage.haskell.org/package/hakyll-4.11.0.0/docs/src/Hakyll-Core-Compiler.html#makeItem): makeItem :: a -&gt; Compiler (Item a) makeItem x = do identifier &lt;- getUnderlying return $ Item identifier x It produces an `Item` out of an arbitrary value *while using the identifier of whatever is currently being handled* (`getUnderlying`). That is why you get `photos.html` as the URL. Note that the type of `getMatches "images/gallery/*"` is already `Compiler [Identifier]`, which is what `listField` asks of you. That being so, simply replacing the first two lines in your do-block with... let photos = getMatches "images/gallery/*" :: Compiler [Identifier] ... should fix things. 
Nice! That was my intuition, based on the fact that you could get the kind of bandwidth I'm hoping for (four oscillators and four samples concurrently) on an Amiga in 1985.
I'm not a fan of replacement preludes, they produce massive cognitive overhead by introducing many functions with the same names but different behaviors as standard functions. Not to mention how most of them include non-base libraries, making it difficult to know the actual origin of any given function, because people have developed some sort of weird aversion to import statements.
The default `ask` is defined in terms of `reader` and the default `reader` is defined in terms of `ask` http://hackage.haskell.org/package/mtl-2.2.2/docs/src/Control.Monad.Reader.Class.html#ask Therefore, if you don't implement either of them you will get an infinite loop! It's worth paying attention to warning messages :)
Thank you!
I’m a fan of the small vehicle prelude as coined by sdiehl here http://www.stephendiehl.com/posts/protolude.html. I’ve used that approach at my former workplace Ambiata and on a number of private projects. It provides great bang for buck by removing unsafe or partial functions and re-exporting the things you would expect to be available. I either roll a small project specific prelude or use something simple like protolude, which is mostly right thought a few things annoy me about it. 
I see. That explains why I am getting infinite loop on first case. I also found out why first version is not same with the second version. it is because second version is handled by GeneralizedNewtypeDeriving (https://prime.haskell.org/wiki/NewtypeDeriving). I was under the expression that first and second version would behave exactly same, but it seems like that is not the case. 
People like to create their little worlds, don't they?
I'd use this if `stack` understood these cabal-esque dhall files natively, like it does for `package.yaml` files from `hpack`.
thanks again. Unfortunately, that is more or less where I started. It then tells me it won't compile because it expects ``` Compiler [Item String] ``` but it actually gets a ``` Compiler [Identifier] ``` So apparently listField asks a ``` Compiler [String] ``` It seems a bit strange that it should be so difficult since [here](https://jaspervdj.be/hakyll/reference/Hakyll-Core-Identifier.html#t:Identifier) it says already that the identifier is conceptually the same as a path . 
I am still hoping to make this work. I've been busy and so far it's been ok. Anyway I will check it out and see what it is about, thx!
From a software engineering point of view, one should only use `ExceptT` in pure monad transformer stacks because it falsely suggests that the error value is the only way our code can fail.
Another source of confusion is that there are multiple ways to implement those tricks. The most basic way is of course to squint at functional dependencies and use the Haskell type system as a logic programming language. Type families make this simpler by introducing functions on types, and at some point one can also play with the stuff from GHC. I think that getting acquainted with a logic programming language could make type-level programming simpler since you want to prove properties about your program.
You could post it on [LPaste](https://lpaste:net) and insert the link here.
Sorry, my Hakyll-fu is a bit rusty. [The type of `listField` is](https://hackage.haskell.org/package/hakyll-4.11.0.0/docs/Hakyll-Web-Template-Context.html#v:listField): listField :: String -&gt; Context a -&gt; Compiler [Item a] -&gt; Context b ... so if `galleryCtx` is... galleryCtx :: Context String galleryCtx = dateField "date" "%B %e, %Y" `mappend` defaultContext ... you'll need a `Compiler [Item String]` indeed. If I'm not goofing up *again*, you were on the right path, except for the `makeItem` problem I mentioned before: compile $ do photosId &lt;- getMatches "images/gallery/*" let photos = fmap (\ident -&gt; Item ident (toFilePath ident)) photosId let photosCtx = listField "photos" galleryCtx (return photos) `mappend` constField "title" "Photos" `mappend` defaultContext Here, I build items directly with [the `Item` constructor](https://hackage.haskell.org/package/hakyll-4.11.0.0/docs/Hakyll-Core-Item.html#t:Item), which simply pairs an identifier with some content (in this case, the content happens to be the identifier itself, downgraded to a path). The content of an item can be loaded through the `$body$` field: &lt;div class="wrap"&gt;$for(photos)$&lt;div class="box"&gt;&lt;div class="boxInner"&gt;&lt;a href="$body$"&gt;&lt;img src="$body$"&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;$endfor$&lt;/div&gt; If you find it confusing to use `$body$` in the template for that purpose, this should be enough to rename the field: galleryCtx :: Context String galleryCtx = dateField "date" "%B %e, %Y" `mappend` bodyField "photoUrl" -- The defaultContext might actually be unnecessary. `mappend` defaultContext
Nikita Volkov's [base-prelude](https://hackage.haskell.org/package/base-prelude) is worth a mention (`base` is the only dependency, has sane exports).
I use `rebase` which isn't doing anything too radical, just keeps the number of explicit imports to a minimum and prevents future clashes.
This probably isn't what you want to hear, but for everything that goes above the "light-weight" `GADTs+DataKinds` stuff, I would recommend to at least spend a little while playing around with a "real" dependently-typed language, to get a feeling for what the bleeding-edge type-level tricks are supposed to approximate in the first place. Even "benign" issues like some mild syntactic overhead may inhibit intuition. The restrictions that apply to type classes/families makes the overall experience a pain, as of this date. Unless you are able to prototype in a language where this isn't the case, until you are more comfortable with the ideas. Compare it to the problems people have with understanding `Functor/Applicative/Monad`, especially so when they try to understand them in the context of their imperative language of choice. Also, there is the community/literature aspect. For Idris, there is the (official?) "Type-driven Development" book. For Agda and Coq you will find literature as well. With Haskell, you get some blog posts and a few papers, giving you the feeling that dependently-typed programming is all about the Peano numbers and implementing `Vec n a`, just like how other people think monads are about `Option&lt;a&gt;` or whatever. This isn't meant to diminish the pedagogical resources that are currently available, not at all. But you have more choices available.
&gt; This process does look suboptimal to me. For one, too few people know about it. Many issues raised elsewhere on social media never get on there. When they do, some only get few responses, perhaps not at all, so it's difficult to gauge approval. When there is disagreement some discussions just die. I don't know whether there is a good way to follow up with things. People on the mailing list may have a better answer. Maybe submitting a patch regardless will encourage bikeshedding. The [`ghc-proposals`](https://github.com/ghc-proposals/ghc-proposals) and [`ecosystem-proposals`](https://github.com/haskell/ecosystem-proposals) are going well. Is there a similar one for core libraries? If not, there should be. GitHub has relatively low barrier to entry, high visibility, and a decent comment system with nice markdown support.
The "magic" of your mtl stack is all based on instances of all the transformers. If you check out the code for `MonadError` you'll see instance MonadError e m =&gt; MonadError e (LazyState.StateT s m) where throwError = lift . throwError catchError = LazyState.liftCatch catchError When your `StateT` is stacked on top of `ExceptT`, you need to `lift` all `ExceptT` operations. The mtl does this automatically for you when you derive from `MonadError`. You can create monad stacks of 3 or 4 transformers and they all would operate the same way, lifting from their current level to the next until you finally get to the top. The only problem you run into is the mtl is not "smart" enough to figure out which level of the stack you are interacting with if you have multiple of the same type of monad transformer. For example newtype AppM = AppM { runAppM :: ReaderT Int (ReaderT String IO) a } deriving (MonadReader) would cause an error if you called `ask` as the compiler wouldn't know which layer of the stack of `MonadReader`s you'd be referencing. You would have to manually lift your calls, making special functions to access them getInt = lift . ask getString = ask 
I'm not a fan. It's broken my builds in the past, and it's a huge pain to cross-compile. Whether you like `base` or not, it does at least work, and the same cannot be said of its competitors. 
Ok what code do I put in to have the input of x is y because and then the input repeats and so on 
Not yet mentioned by /u/wardaft: losing copyright over work you do in your spare time with your own tools, among other things.
[removed]
[removed]
[removed]
That's not an example and even if it were, it's only one. So far all I've figured out from this post is that you are quite bad at following directions. 
Thanks, this seems promising! I took all the datatypes and derived Generic and FromJSON instances so that I could construct a full SQL query from a user-specified input file. I've hit a roadblock with that, where I cannot construct a concrete instance of `ColumnExpr`: ```newtype ColumnExpr = CEBase (ValueExprTemplate ColumnExpr)``` I don't understand how this recursion works (there's no base case?). Do you have an example instance of `ColumnExpr` for me to look at?
looks like `parseCSVFromFile :: String -&gt; IO (Either ParseError CSV)`, which means `csv' :: Either ParseError CSV`. This is what the type error's saying: `extractColumn` is expecting its first argument to have type `CSV`, but instead you gave it an `Either ParseError CSV`. To get at the `CSV` in `csv'`, you can pattern match on it, use `fmap`, or do anything else to destruct it. case csv' of Left parseError -&gt; -- something went wrong in parsing Right actualCSV -&gt; extractColumn actualCSV 2
Oh, that is true. For example, I know of three distinct ways to write a generalized `zipWith`, described in these works: * "Do We Need Dependent Types?" by Daniel Fridlender and Mia Indrika was published in 2000, and it belongs to pre- *type level prolog* era. Rather, they define a collection of certain value level "numerals" that encode arity. * "Faking it" by Conor McBride was published in 2002 and improves on the previous result by translating it to type level prolog as we know it. * *(I wonder if there is something in between.)* * Fast forward 10 years, Richard Eisenberg, in a [blog post](https://typesandkinds.wordpress.com/2012/11/26/variable-arity-zipwith/), proposes another `zipWith`, in the spirit of the time, based on type families. I am not sure which of these approaches is simpler. I find all three of them quite convoluted, and requiring some mind extension, in a distinct way each.
Yes! This works. Thanks! I wonder how I could've come up with the ``` (\ident -&gt; Item ident (toFilePath ident)) ``` I saw that the Constructor takes 2 arguments, an identity and something else... I just did not put it together. Thanks anyhow, great help! (Now I only need to find out why Safari does not want to use the exif orientation data... which has absolutely noting to do with hakyll ) 
I've once noted some thoughts on removing head here: https://www.reddit.com/r/haskell/comments/6b3lh8/proposal_remove_init_from_prelude/dho42o9/?context=0 I think it would not be beneficial and don't see it as a mistake at all. :) 
Here is the comment linked in the above comment: Why are partial functions bad? For me personally, whenever I ran into a partial function exception, it pointed to design errors in my program or plainly wrong code that didn't behave as I expected and needed to be changed. A function that always works would mean having to deal with the alternative that it yielded Nothing, but what if you clearly expect your program to always yield something at that spot? I think that's akin to a beginner mistake of always catching Exceptions in Java instead of throwing them on to where they are to be processed in a meaningful manner. Some things are not meant to be handled in the program, but rather to make it crash and clearly point out that something went wrong. Errors in a program that are not recognized as such can corrupt everything that follows and make the real problems harder to find. The set of the four functions head-tail-init-last is not only well established in the reports that founded the language (in the standard prelude), they are also dual to each other. Removing one of them would mean throwing that duality out of the window, and removing all four of them would mean an extra step for getting access to some of the most common list functions that are a good window into the language for many tutorials too. If init', init2, etc. are not good for you, there are many ways of importing modules -- You could add an import for Data.List "as L" (without "qualified") to have the access path at hand when you need it: L.init . tail $ "hello" - and then just use the word "init" for your own symbols as you wanted. :) *** Comment by: [u/Shrinker42](https://www.reddit.com/user/Shrinker42) | Subreddit: [r/haskell](https://www.reddit.com/r/haskell) | Date and Time: 2017-05-17 11:51:10 UTC | *** I'm a bot. Please click on the link in the original comment to vote.
Don’t you use Nix? Binary caching and all that. Why should you care about dependency trees?
This could be *extremely* useful for repos with lots of packages. It's 95% boiler-plate which this tool could reduce.
QuickCheck is pretty popular.
&gt; Why should you care about dependency trees? One issue is [binary code bloat](https://www.reddit.com/r/haskell/comments/5lxv75/psa_please_use_unique_module_names_when_uploading/dbzegx3/), generally each dependency carries a risk &amp; cost; so it's desirable to have a dependency only when you're actually gaining something from it. As an extreme, even if it was binary cached, you wouldn't want to add all packages of Stackage as a dependency either... :-)
&gt; and a decent comment system with nice markdown support. ...which unfortunately is completely useless once you reply via your MUA integrated into Emacs (if you prefer to use tooling which was actually designed for text manipulation and to handle complicated Email/Usenet discussions), as then GitHub decides that your email body shall not be rendered as Markdown anymore... :-( But what's wrong with good old mailing lists? Except maybe for the inability to edit comments and supporting out-of-band emoji voting, I don't see the benefit. What's also missing from GitHub's commenting system is support for sub-threading (which is something that GHC Trac actually supports btw); everything is forced into a flat linear chain of comments which make it difficult to follow discussion sub-threads and who's replying to whom.
I'm personally hacking on lower level stuff pretty often (unusual for most nix users). Compilers, low level dependencies, globally applied ghc options, partially uncached checkouts of nixpkgs, etc.. It's not uncommon for me to have to rebuild the entire haskell world for projects I work on.
Glad to know it worked :) Amidst the various wrappers and layers of abstraction in Hakyll, it can take a while (at least it did for me) to notice that an `Item` can hold an arbitrary payload, tagged with an arbitrary identifier.
I wonder if .dhall files could be provided to fix dependencies to the ones given in a particular Stackage LTS...
I (haskell noob) am looking forward to a usable foundation and in the meantime rio. I just want a clean and solid standard library and not have to decide on a set of good packages (and language extensions). Also the recurring comments about things that are not good in haskell (String, partial functions, ...) demotivate me to invest more time into learning haskell, these projects would solve that. But I guess the opinions of real user are more important.
It's hard to enforce a strict policy against partial functions when you rely on `prelude`. We've had issues at work where new Haskellers are just not aware of the pitfalls of using partial functions. I'd prefer if it major version bumps to GHC fixed some of these problems instead. 
Really? Can you back that up? I don't think that's true in general, although there are cases where it enables useful inlining. On the flip side, it's *worse* for strictness analysis, and can be worse for register utilization sometimes. This isn't an obvious choice!
Also `DataKinds`, at the type level.
I think it works well for individuals too. I'm considering moving all my maintainer/author/copyright stuff to a single file and including it
Via the IPFS hosting? That's not bad at all.
Or just GitHub (or your own web server).
Getting these functions removed from the Haskell report in Haskell 2020 and slapping deprecation warnings on them in the next release would be awesome
&gt; we commonly know the outer context already (thanks to toplevel signatures for example) &gt; The other way around just doesn't make much sense to me for this instance. Top-level signatures are not mandatory and I saw a lot of remarks about how you can write no types and get type checking for free. In this setting the second instance makes perfect sense. All I'm saying is that it's a bad practice and a completely outdated point of view.
Question: Is there any difference between ExceptT and EitherT? As far as I can tell, the type is the same, but there are more functions for working with ExceptT than there are for EitherT.
Cool. I'm in.
John Baez's blog is the reason I got to know category theory, which is the reason I chose Haskell to learn. This is great.
Do dhall evaluates http dependencies every time it is run or it has some kind of cache?
Right now, every time on demand. I think /u/Tekmo plans to add a cache.
Makes sense, hashes are really important in this case. Thanks!
Its interesting that `cassava` is so much faster than anything else. Comparing the streaming interfaces it exposes with `csv-conduit` would be interesting.
What would it take for `memory` in particular to drop the `foundation` dependency? Is `foundation` just used internally, or would replacing it be a breaking API change?
&gt; but I cannot say I understood how to apply them to actual code and what for. One use case would be: Make sure that you only multiply vectors and matrices in the right dimension. Or that a function returns a list that is at least longer that the input. Basically, the type-level enocding of number enables one to enforce some number/length related properties at compile time. To be honest, I have not had any use of type-level number so far, but they seem to have their uses. (Am a beginner aswell)
for me it was the other way 'round :-) ... did know some haskell, therefore I learned about category theory, which lead me onto john's blog :-)
I see, thanks for the clarification.
I remember seeing this project. But, I thought that it was called intellij-haskforce. I guess that I will have to reconsider the name of this library. Thanks for letting me know!
I use the `tasty` testing framework for my "non-proof" testing needs. It supports and bundles together several different testing libraries. I use different libraries for different types of testing: * QuickCheck for stocastic property based testing when the input domain is large * SmallCheck for exhaustive property based testing when the input domain is small * HSpec for specific input/output tests
Looking at his chapter 1 post makes it seem like his online "course" is just going to be an information dump. The provided PDF for the book also has a lot of text readability issues as well. I'm hopeful that a little more effort will be put into preparing the material. 
Hedgehog is also gaining popularity in the property-testing space. It has pretty nice output, and I prefer the monadic style of using generators to the QuickCheck style where you need to define a newtype and an Arbitrary instance.
Thank you, rpglover64, for voting on comment\_preview\_bot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
I wish more PDFs written by academics had one column, not two, per page. The 2-column arrangement is harder for me to navigate when reading on a mobile device. (The OP has one column.) 
Do you have an account on the ghc trac? Anyone can set one up here https://ghc.haskell.org/trac/ghc/register I would suggest posting the screenshot of threadscope, description of how you compiled, and link to the repo or this thread (I'm not a GHC developer, I just post bug reports etc from time to time). I'd also be interested in following the ticket if you want to post it here if/when you create it.
Care to elaborate?
... [Part 2](https://medium.com/@jonathangfischoff/monad-reader-part-2-d812dda1d03e)
No doubt GitHub's UI has problems, but we already have the two proposals repos I linked on GitHub, so there is a precedent. Any important proposals that happen on one forum or the other can be cross-posted for more visibility. As far as what's wrong with the mailing list, the person I replied to had a point that resonated with me. Low-traffic and low-visibility. GitHub has a much more modern and inviting UI, and is well-connected to the open source world. You can reference other issues, repos, users, etc. painlessly. Issues are taggable and sortable, they don't just fade away over time. So markdown support doesn't render properly when replying by email. That's a shame, but _any_ markdown support is better than none. Comment-editing and emojis I see as downsides - as well as the (massive) downside of having a linear thread. It's annoying and unusable at times, but again, why _not_ at least try to modernize an important discussion forum that isn't seeing as much activity as it should.
I've never really felt the need for a partial *or* total head. If removing the partial version was seriously on the table I would say don't replace it. They're easy enough to define yourself.
Just out of curiosity, does GHC not do dead code removal? I wouldn't think that adding a package would actually result in any more code than I am actually using, ideally.
I like HSpec the most.
Sure, GHC does dead code removal. However, for the problem at hand we rather need link-time dead code removal (which even if it worked perfectly would likely be costly to compute); I'll refer you to [this older discussion for the issues involved](https://www.reddit.com/r/haskell/comments/3ikqqx/what_holds_back_dead_code_elimination_across/).
They are identical. You should use `ExceptT`. The `either` package included the `EitherT` type because `transformers`, back in the day, didn't have `ExceptT`. In the newest version of `either`, you can see that `EitherT` has actually been removed.
You have to handle partiality somewhere, so why not have convenient functions to do it.
Got it! Thanks for the explanation. :) 
Relevant [Article](https://www.schoolofhaskell.com/user/edwardk/unlifted-structures) and [lightning talk](https://www.youtube.com/watch?v=I1FuZrDhOEk)
At present it isn't possible to fully unpack an IORef into a normal data constructor without yielding a level of indirection. Many of the code transformations we do basically treat the data constructor holding onto contents as so much discardable garbage. Once you can have mutable fields all of a sudden object identity becomes something you can care about and in fact must preserve. This affects all sorts of optimizations we do today. There is a working proposal to add mutable record field support directly to GHC, but it's tricky for this very reason. My `structs` library takes a slightly different approach, building mutable objects in kind # by abusing the representation of `ArrayArray#` on the heap, and making it so those objects can point to each other. Then I wrap those objects in a single wrapper that holds onto them in kind *. This has the benefit that the wrapping can be discarded like any other wrapping, and in fact the compiler will almost always optimize it away via worker-wrapper and the usual optimizations that are used to unbox normal arithmetic. By having it as a separate entity, it gives me a place in the semantics for all the bottoms and usual nonsense to live without having to deal with them on every pointer dereference. Unfortunately, its not perfect, more of a proof of concept. Without some additional compiler support, it only handles objects that are made out of pointers to other objects (either in kind # or *), and can't hold onto unboxed fields. You also have to liberally unsafeCoerce the hell out of everything in order to trick GHC into letting you write this code! &gt; I suppose with the arrival of linear types, many libraries will want to provide mutable actions under the hood. Linear types don't really change the story here too much. I'd really want uniqueness rather than linearity for talking about mutable changes being done before you finally "publish" the result. Worse, in a non-strict setting, uniqueness isn't even enough, as you need to be careful not to eagerly do unnecessary work in a non-strict setting. Clean never really bothered to distinguish this sort of thing, risking you freely wasting work, but once you care about this, it starts pushing you towards having a whole lattice of constraints, separating out affine types and relevant types. But then you start realizing that relevance isn't the right notion for computation, so you start needing the demand analyzer for strictness analysis to interoperate with the rest of the machinery to know whether you should be building little chains of affine thunks affecting a single unique but non-strict resource, or if you should do the work now on a unique strict resource, etc. Overall it is a pretty big mess. In the meantime sticking to the sort of `structs` story above, one could modify that a bit to allow for freezing and thawing of structs and get something like the publishing idea, either through an `ST`-style region parameter discipline today or through a slightly more liberal linear region parameter discipline.
I'm not in favor of head/tail in prelude but there are safe uses of `head`. It isn't always entirely unreasonable.
Throwing Haskell out over this seems like throwing the baby out with the bathwater. I get that it's annoying, but it's not generally something that's going to meaningfully get in your way.
I think there must be some people who just downvote you on sight now. I see no reason for this comment to have a score of -2.
If this all is too much for you right now, I recommend starting with a simpler setup - snap, happstack and yesod are all quite "bells and whistles", but chances are you won't actually need all that. Try scotty; it's much simpler, and covers less of the problem space, which is good in that it also means that it makes fewer choices for you. If you like Sinatra or Flask, Scotty should feel a lot more like home.
Sorry, this wasn't meant as general advice. I see that it could be taken that way. It was meant to be a short, understandable explanation to the specific question as posed. You are absolutely right, local bindings `/=` auto omptimize special sauce.
I don't believe that GHC team is working on removing the ordering restriction for Template Haskell. The phase restrictions with Template Haskell are certainly annoying, but you need to work around them if you want to use Template Haskell. You don't ever have to -- you can manually define the lenses, or even use Template Haskell to generate the initial code and paste it into the project (using `-ddump-splices`). --- To provide some unsolicited advice: I've worked with a number of Haskell web frameworks, having built large applications with Yesod and Servant and trivial apps with Scotty, Spock, and Snap. Yesod is the best one to get started with. I think that your objections are misplaced -- you are right that Yesod uses a lot of Template Haskell, and `stack` works best for developing with the scaffold. Web programming (and database programming) involves a lot of duplication of work -- you can either duplicate the work (eg use something like `postgresql-simple` and `scotty`) and introduce potential for errors, *or* you can use a fancy advanced technique to avoid doing that work. Yesod and Persistent both use Template Haskell. Other libraries that eliminate the boilerplate use type level programming, generics, or other fancy magical features. I prefer Yesod for most projects because it is genuinely easier. The Template Haskell does a fantastic job at solving the boilerplate while providing type-safe routing, links, and handler dispatching, and the DSL ends up being an easy-to-read route specification for your app, too. The Template Haskell is optional (you can write the boilerplate yourself, if you want to), and it's used very well -- instead of manually writing error prone code, you write a DSL that generates many useful things for you, and keeps them all in sync. `stack` is currently the best build tool for Haskell -- why would you want to avoid it? It is purposefully designed to avoid most of the footguns that `cabal` has in it's default workflow.* I can understand avoiding `stack` if it directly contradicts some need the project has, but I don't understand why you'd avoid a *very* productive and useful library because it uses a build tool to make you *even more productive*. --- * Cabal has been making great improvements lately, and I hope that it someday surpasses stack -- then I'll have an *even more* pleasant time building Haskell projects! The cabal team does fantastic work, and I don't mean to denigrate their efforts, and I understand and respect that `stack` doesn't work for every use case.
Why yes, I have been in a bad university course before. 
If you really don't like this property of TemplateHaskell (I find the ordering restriction bothersome as well), you can just skip it and define the lenses manually. TH is just a convenience here. Pass `-ddump-splices` to GHC to see what its generating for you, paste it in your module wherever you want, and savor the sweet freedom. In general TH should rarely be required, if you don't use something that forces it on you. I hope you get on better going forward. Good luck.
His statement that program correctness should take into account performance regressions was very interesting for me. So I know Haskell is very expressive in terms of being able to describe what we want to compute. Can Haskell also be similarly expressive in how things are computed so we can be assured that a performance regression is restricted by the type system somehow?
I attended the some of the lecture versions of the chapters in this book this past January, and it was very worthwhile. I'm looking forward to studying it a little more extensively from the book!
On the other hand, I greatly prefer the two-column layout when reading papers in print. Perhaps both one-column and two-column versions should be produced? Or perhaps even better, use a format that is flexible enough to allow many use cases?
&gt; Other libraries that eliminate the boilerplate use type level programming, generics, or other fancy magical features. I think the issue here though is that these libraries, while using type-level programming, are ultimately still specifying all their stuff in the Haskell language. Even if I don't understand servant, for example, if I have an understanding of modern Haskell, I would immediately be able to see it's using type-level operators to define some kind of type. On the other hand, without reading the Yesod docs, there is no hope of being able to understand what it's DSLs mean. I really don't think it's fair to compare libraries that use Haskell (however complex it may be) to libraries that require you learn new languages.
For Haskell webdev without TH Spock and Servant are both good frameworks with batteries included. I'm working on a Sevant project at the moment and the only things I use TH for are Aeson instances (this can also be done TH-free with DeriveGeneric) and most (but not all) of my Persistent models. I generally do lenses without TH. 
&gt; [*lens*' adds] yet another layer of [...] alien syntax My usual advice to mitigate this specific issue while writing *lens*-using code is avoiding the *lens* operators as much as possible. In most circumstances, I'd much rather write... view (bar . baz) foo over (bar . baz) g foo ... than... foo^.bar.baz foo &amp; bar.baz %~ g (Also note how using `over` instead of `(%~)` makes it easier to see how similar `over (bar . baz) g foo` is to `fmap g foo`.)
There is another important downside to github: No (trivial/certain) way to drop to private messaging. More than once I wished to ask for clarifications/give feedback about some detail that I did not wish to bother all readers of the discussion with. Github forces you to either a) make it publicly nonetheless b) don't participate c) try to stalk people's mail addresses via github metadata or other means, to reply via mail. I think this fact was at least a contributor that one of the recent proposal discussions became such a complete, unstructured mess. (But the linearity that you mentioned probably is as much to blame.) For me, the mentioned downsides far outweigh the advantages. 
&gt;Btw. reasoning behind its creation to remove boilerplate from use of named fields aka record type by adding yet another layer of complexity and alien syntax and naming sounds ridiculous. This little rant is not only about lens, vast amount of other GHC extension combinations which make practical use of haskell very unwieldy dont use lens. It is worthless. don't use TH, quasiquotes and so in. If something use TH it is because it is poorly designed. or because it simply does some little syntactic sugar that is not worth the pain, like lens. 
Right -- I think TH is fundamentally _more_ magic than type-level programming and generics, which are now well supported and well understood, and fundamentally less pointy.
Additionally to the already mentioned tools - tasty with hunit and quickcheck - we use our own tasty extension: [tasty-bdd](git@github.com:paolino/tasty-bdd.git). Although still very fresh and unpolished, we use it for writing acceptance tests in our project. The benefit is, that keeps the tests small and they test one thing at a time which is enforced by the DSL. This in turn helps very much with the tests' readability after the project grows.
&gt; I really don't think it's fair to compare libraries that use Haskell (however complex it may be) to libraries that require you learn new languages. I strongly disagree. Sure, an arbitrary DSL requires learning that DSL. Let's look at the routing DSL: / HomeR GET /blog BlogR GET POST /blog/#BlogId BlogPostR GET POST This DSL is so simple that the overhead of learning it is smaller than learning most Haskell libraries -- it's easier than monad transformers, lens, servant, heck, most stuff that people talk about in Haskell. Advanced Haskell techniques are typically *much* more opaque and difficult to learn than a simple DSL like this. Having worked extensively with Servant's types and Yesod's DSLs, I've had vastly more trouble with Servant's approach than I have with Yesod's.
"Bad university course" aka any course on PDEs.
Template Haskell is certainly a more powerful language feature, as it can create new declarations. For features where you don't need this, generics and advanced types are fine. For features that benefit from this ability, Template Haskell is the best choice, and working *around* it with types/generics becomes extremely burdensome.
PDE?
Thing is, the routing DSL adds very very little compared to what you could achieve with a well-designed EDSL - you would get a little.bit more line noise, but in return, you have all of Haskell at your disposal, routes and route combinators are no longer opaque magic but rather just plain old Haskell functions or values, and they are naturally first-class things that you can pass around, compose, build up and dissect computationally, and you can even make your own combinators. But TH used like this is opaque and mostly untouchable - if you want to extend the DSL, you have to go in and actually modify it.
For what it is worth I use yesod for web development in the last 5 years probably. It provides a lot of convenience and gets the job done. Alternatively if you are into modern SPAs (react etc), you can completely detach the front end from haskell and use haskell backend to exchange data with frontend via json ajax calls. 
The PDF link seems to be down at the moment. Is it the same version as can be found [here](http://math.mit.edu/~dspivak/teaching/sp18/7Sketches.pdf)? Also, looks like there's some related course videos and such [here](http://math.mit.edu/~dspivak/teaching/sp18/).
The simplicity you've highlighted is -- in my opinion -- a very superficial one. While the DSL is straightforward enough for those familiar with web routing, I have *no* idea what's going on in the backend. Unless I have an in-depth understanding of Yesod, I have *no* idea how to compose these values, or if they even can be composed. I have *no* idea how to extend this DSL, because the interface is hidden within a package. Ultimately, Yesod's 'simplicity' is its downfall. It is unobvious how to extend this paradigm for more complex sites or routing patterns. On the other hand, with servant, since it just uses types, I know exactly how to extend it. In fact several third party packages extend servant's routing freely. If I want to add a new route component, I just instantiate some type classes. A new content type is also just adding type classes. This greatly reduces the mental load. All I have to focus on is understanding haskell, and an understanding of servant immediately follows. While Yesod's syntax is enough for a simple website, it is simply not enough for more complicated services, which is why I pretty much always use servant for any web needs of mine, even when a fuller framework would be better.
You cannot work around template haskell with types and generics because they solve two fundamentally different problems. No amount of type-level tomfoolery would ever allow you to declare a new variable, period. If you need to add declarations, use template haskell (or better yet -- a preprocessor). If you need to prove invariants to the typechecker or want straightforward code generation based on the structure of ADTs (which most instances really are), then use types and generics.
Partial Differential Equation
You lose the ability to generate new datatypes from it, unless I'm mistaken, which is the entire point of the DSL.
I agree with everything you wrote here. By "work around," I meant something like: &gt; If you are developing a library with generics/types, and would *really* like to be able to generate new datatypes/functions, then you will be in a world of pain trying to get around that. Template Haskell is the simpler solution here.
I would much rather someone use something with repetition, than battle with something larger that could risk them giving up on Haskell. The first objective should be to have someone using Haskell in any way that they can using documentation they feel comfortable with. There is plenty of time for refactoring later into the ideal framework, and a strong compiler ready to support ambitious refactoring.
Put the template haskell declarations at the bottom of the file.
&gt; All of these critiques hold for literally any DSL, including Servant's embedded type-level DSL. The solution to these problems is to look at the documentation or source code, just as it is in Servant/etc. I just really disagree with this. DSLs written in Haskell's normal syntax are easily extendable using normal haskell rules. &gt; The function mkYesod takes a String and a [ResourceTree String]. But this is exactly the problem. The internal structure is not exposed. If I want to extend it I have to write a new function that re-implements all the DSL parsing. For example, suppose I add a new HTTP verb in my custom backend, called 'PLOP'. I cannot extend Yesod's DSL to incorporate this verb without reimplementing the entirety of the mkYesod function. On the other hand with servant, i can define a new type `data Plop a` and then instantiate the `HasServer` class for my new `Plop` type, along with some others, and I can seemlessly integrate my new 'PLOP' verb with the rest of my ecosystem.
But then you can't use your lenses in any of the code within the file.
Did you ever find the range?
It seems to vary by discipline: mathematicians seem to prefer one-column layouts, computer scientists, two.
You only need to put the types together in a band above the template haskell commands when there are cycles between the types involved. If you are strongly allergic to re-arranging the definitions the cycles can often be broken by parameterizing your types at one point in each cycle. instead of data Foo = Foo ... Bar ... data Bar = Bar ... Foo ... having one of them take a type parameter 'a' for whatever it is that they happen to knot off through, lets you later on talk about a `Foo` full of `Bar`s, and define `Foo` without referencing the later type `Bar`. This can often buy you a bunch of instances for common classes that turn this pattern into a feature, type synonyms introduced after the other definition can get you back your original naming conventions. Ultimately TH has to run at _some time_ and needs information on types that are in scope at that time, to create more types and definitions to bring into scope. You don't have a complete "type" yet if you are referencing definitions beyond that part of the source file and there happens to be no universally sound way to shuffle all the things around to make sure that this isn't exposed to the user. Later definitions including types for data types in the file could be referring to stuff produced by the splice.
Those are fair points :) It's true that Yesod's quasiquoter isn't currently extensible like Servant's. Servant buys extensibility, but it must trade something dear -- error messages and incidental complexity. &gt; The error will be returned in the language I expect from GHC 'can't match type Plop with ...'. If only the errors were that simple! I've had Servant type errors that were &gt;10K lines long (that's how much scrollback my terminal keeps). I've had errors where instance resolution didn't work and the error message was inscrutable and non-local. I've had *kind* errors arise from failing to either have `PolyKinds` on or provide a kind annotation in a type class instance. Servant lets you do a ton, but there are a lot of footguns in that flexibility.
[All of arxiv is down right now](https://twitter.com/arxiv/status/978393984304340992). Putting the TOC and indexes of the two versions side-by-side suggest they are similar (e.g. there seem to be no major rearrangements) but not exactly the same.
How about HalesForce :)
http://xxx.lanl.gov/abs/1803.05316 leads to the book in an arxiv mirror that is working right now.
I honestly think that lenses wouldn't get half as much pushback as they do if they were a "magic language feature", instead of a plain-ol' library. People seem to have a higher tolerance for language-level magic than library-magic.
The enforced ordering when ``TemplateHaskell`` is in use is definitely annoying. If it really becomes a pain I will either move all of the splices to the bottom of the file, although I hate that. So I will turn to using ``--ddump-splices`` and inline the code in my files. That being said, I'm going to plug the [Applied FP Course](https://github.com/qfpl/applied-fp-course) a bit, as it is a minimal 'web app' example. Normally we run it as a three day course, but it's quite amenable to self-paced study. This isn't a comment on your abilities, rather I thought it would be useful to see a working example (that is designed to be poked &amp; prodded) of a Haskell web app. You can then take this and build on more things as your skills improve. 
Don't use npm. Use yarn. Npm is trash.
Code with lenses just isn't pretty. I think that's part of the problem. There is also kind of a conflict between record syntax and lens syntax, and I think it should just be one.
I've used both interchangably, but it's funny you say something like this because someone literally said to be in the kitchen the other day "don't use yarn, its trash, use turbo".
I would say that if you're attempting to learn a particular framework, then you shouldn't fight its idioms. This was a huge stumbling block for me when I was first learning Haskell. So, for example, if you are trying to use Yesod, then unless you have a deal-breaking reason, you should dive head-first into its style of template-haskell usage, etc. If you want to avoid such idioms for learning or ideological reasons, then it would be best to find a framework in line with your expectations. Scotty, or even raw WAI would be a good choice here. A big problem is that often in Haskell you can bend a library or framework to your will, and you can sink a lot of time into doing so, but it's usually a better investment to either adopt a when-in-rome attitude, or find another framework/library to work with.
I just use `node2nix` :P Works surprisingly well.
Oh dear, the red tape for nix'ing node would suffocate me.
I came up with a (somewhat unsatisfying) solution, but it does not resemble this at all - Thank you!
Load the file in GHCi and type `main`, then press Enter. What happens? What do you expect to happen? What do you want to happen?
I thunk you want [Spock](https://spock.li)
Actually a 'clever' implementation might be something like instance BooleanFunction a =&gt; BooleanFunction (Bool -&gt; a) where boolFuncs xs f = map xs (\x -&gt; boolFuncs True f : x) ++ map xs (\x -&gt; boolFuncs False f : x) instance BooleanFunction a =&gt; BooleanFunction (Bool -&gt; Bool -&gt; a) where .. instance BooleanFunction a =&gt; BooleanFunction (Bool -&gt; Bool -&gt; Bool -&gt; a) where .. and the last step for just 'Bool' has a magic instance that actually applies the combinatoric list to the function, or something.
Let's document this sucker more as a community. We'll try to contribute some documentation from work over time.
There is some limited support for a scenario at least kind of like this the library linked above. https://github.com/ekmett/structs/blob/4bf05ef1f277f72b683c90e282ddbcd8518770ba/src/Data/Struct/Internal.hs#L219 you can use a `MutableByteArray#` as a field of the `SmallArrayArray#` to hold unboxed fields in `structs`. I didn't feel the need to package up management of `MutableByteArray`s themselves, because you can basically get by with just the machinery in `primitive` for that.
Reading/scanning a physical book/paper is very different from on a screen though. What's preferable about 2 columns? I can understand newspapers/magazines with their space/cost constraints. 
I generally add exclusions for my repository mega folder (regardless of language)
Do you mean like this? https://i.imgur.com/VGPyMps.png I remember trying this some time ago, and I don't remember it working. Did you only add one mega-folder? Does it contain `.stack-work`/`~/.stack` folders? Or what does it contain?
Ah, yeah I wasn't fond of most of my undergraduate maths courses. 
I was trying to use some Streaming's grouping/chunking combinators from Turtle sources. Yes, Foldl fits naturely with Turtle's streaming design. However, as soon as it folds/consumes a 'Shell a' type, it breaks 'constant space streaming' from that point on (ie, can't just fold it into an 'IO [a]', then feed it to Streaming.each). That's also my (though vague) questioning that those two streaming type may not be interoperable at all (other than through dirty async IORef mutation). However, I'd like experienced haskellers either confirm that (and explain it better than me), or show a more principaled way to integrate them together.
Fwiw Yesod's routing DSL supports custom HTTP verbs just fine: /foo MyResourceR PLOP
DSLs and monad transformers are actually a rather lightweight feature too which brings in huge benefits in systems programming, IMO. 
`{-# LANGUAGE Lens #-}`
I think CS first years start off on Haskell, so the user group would probably be dominated by them.
&gt; People seem to have a higher tolerance for language-level magic than library-magic. What's magical about a lens? afaik it's just a type alias for `Functor f =&gt; (a -&gt; f b) -&gt; (c -&gt; f d)`?
I'm also a Haskell beginner – I come from a Scala background. The reason why I'm attempting to move to Haskell is because Haskell is more opinionated than Scala and is much more expressive. I understand your frustrations and to some extent I can relate to them myself. However, to me the most frustrating part of Haskell so far has been the tooling. Its very hard to have a workflow as efficient and as distraction free as the one I have with Scala and IntelliJ. Haskell's tooling has many options, though – but in my experience setting each one of them has led me down a rabbit-hole of issues where things don't work on my platform (Mac OS) or something else breaks. That is to be expected and I'm still grateful to the people attempting to make changes in the Haskell ecosystem. Now, Haskell being a language as old as it is, has its flaws. But in my experience – having written Scala for around 4-5 years now (I write and maintain all services powering [CentralApp](https://www.centralapp.com)), I'm very pleased with Haskell in general. I have the occasional frustration every now and then, but the language more than makes up for those flaws. Haskell requires you to make some serious upfront investments. And those investments seem to never end – especially with the reliance on things like TH. In my day job using Scala, I find it much easier that the framework I'm using sneaks in some things like configuration management through an impure paradigm. But that, in most cases, gives me less freedom to choose things on my own. In a production scenario, this can also end up being opaque – imagine one of the underlying parts of a framework breaking when under load and then you have no clue what is going on. I find the Haskell approach much more palatable. Even the heavy-weight frameworks in Haskell (I've been playing around with Spock a lot lately) seem to take a more barebones approach than things like [Play](https://playframework.com) in Scala. That gives me more visibility into what goes where immediately. For example, to this day, I find it hard to keep in mind how the bootstrapping phase happens when a Play application _boots up_. To me, it sounds a lot like you've jumped in on the deep end. My first attempt was something very simple with Scotty – and I wrote things to manage configuration, routes, and starting up the application. I manage to squint out the same kind of visibility on Spock – but that has been after my first serious application using Scotty (an application that starts up an http server, connects to rabbitmq and manages state through some medium-complex business logic -- this is supposed to run on a raspberry pi). Therefore, I'd suggest trying things out from a simple side first. Move on to bigger frameworks afterwards. The amount of expressive power in Haskell is bonkers. For example, I dearly miss the `where` syntax in Scala now. There's so much more clarity that one can achieve by simple means like these – delegate defining boilerplate after you're done expressing your business logic. Moreover, that area in Haskell is very flexible. The `do` notation is also one of the things I can no longer live without (Scala has its dirtier equivalent called `for`). Not to forget that Haskell as a language, for an FP enthusiast, brings all the ingredients of pure functional programming to the front of the table. That may seem daunting at first, but what that also means is that all of these ingredients are more accessible in Haskell. Its one of the very few programming languages that _requires_ you to change the way you think about programming. As a programmer, I can argue that learning various programming paradigms and borrowing good ideas is beneficial. In the end, it really boils down to what you consider important in a programming language. I do think your opinion would've been different had you tried something simpler than Snap to start with. Even if Scotty seems too barebones, try to bootstrap things yourself with it and converge towards a more serious application. It'll probably be even nice to learn how this can be done.
I'm partial to [basic-prelude](http://hackage.haskell.org/package/basic-prelude), which is fairly minimal, and actually used as the basis for some other preludes (e.g. classy-prelude). I think in general alternative preludes should simply repackage/rename types - the moment you start adding new code, it becomes impossible for other libraries to reference them without pulling in your entire dependency tree. It's interesting to note that classy-prelude is designed with this in mind - all the novel parts are separated out into mono-traversable, and the README explicitly tells library authors to depend on that instead of it.
You totally got me!
I stand corrected: λ D.F.Coyoneda D.Coerce D.Monoid&gt; :t coerce :: Coyoneda Maybe Int -&gt; Coyoneda Maybe (Sum Int) coerce :: Coyoneda Maybe Int -&gt; Coyoneda Maybe (Sum Int) :: Coyoneda Maybe Int -&gt; Coyoneda Maybe (Sum Int) 
Spivak's "Category Theory for the Sciences" is the book that I've had the most luck with while studying cat. theory. Maybe that's because it wasn't my first attempt (or even my fifth), but it felt much more relatable given I don't have much formal math education. For that reason, I'm excited to get my teeth around this one - and look at that juicy TOC!
The type errors aren't even nasty just for newcomers. We occasionally use it at work and I find it impenetrable sometimes. 
I don't think there are enough uses for it to warrant its inclusion in `base`. Just use `case`, or a lambda that only takes the first element. `( \x:_ -&gt; x )` is at least being very explicit that you demand the list be non-empty.
I agree with this, modern mtl patterns in particular can be really, really nice. Also, I think saying "too much abstraction" is wrong, very abstract things can really improve things, maybe the issue is the wrong abstractions.
[removed]
The pick should "obviously" be Servant, which is higher level.
I was put off by the fact that it requires the same binary to be deployed on every node, so no cross platform and in my mind you might as well build the possible computations into the binary since you are going to have to setup something to easily distribute the binaries anyway.
I disable real-time protection by hand whenever I build GHC (not building something with GHC). Probably a better solution is writing a daemon that polls the process list and temporarily disables Windows Defender/Indexing Service/etc when a certain process is running. I wonder if someone else finds this idea reasonable.
This isn't a simple question actually. But off the top of my head here are some issues that make Windows builds slower (Though YMMV): * You've already mentioned AV whitelisting of the build folder, what you haven't mentioned is your **temp** folder. For each compilation there are more files created than the `.hi` and `.o` files that end up in your output directory. So you have to prevent the AV from trying to scan those. I personally have my temp folder on a ramdisk and exclude the entire thing. Since it won't persist anyway I don't care that much about it. * Indexing services. Windows usually has multiple indexers running. For things like the start menu search etc. These also slow things down as it may be trying to (depending on where your project folder is) to index these files, only to subsequently having to remove the entries. So again I exclude certain extensions, like `.o`, `.hi`, etc (any binary file) globally and exclude my build folder from indexing. The only side-effect is slightly slower search in those folders. * `GCC, binutils` and even `GHC` are quite under optimized for Windows. And their unix way of doing things make them naturally slower. The build process for a single file is quite process forking heavy. Which on Windows is slower. So the fact that GHC calls out to external programs for so many things will make it slower. To put it in perspective, in GHC 8.2 (or maybe 8.4, I don't remember which one anymore) I changed it so GHCi doesn't repeatedly query GCC for library information, and we could now use actual Windows APIs that query the MFT instead of individual file/directory records. This change made a huge difference in GHCi startup and re-linking speed. Aside from this, binutils is just dog slow on Windows. Linking is just slow, which is why we still haven't enabled function-sections on Windows. and I don't believe it's a priority for them. Other things like the I/O subsystem in GHC being really only optimized for Linux and only "works" on Windows. lots of factors here. but GHC is slowly getting better. * The NTFS file system is not that great at dealing with lots of very small files. It's gotten better though. (The following information may or may not be accurate depending on your NTFS version). The short end of the story is that each file gets a record in the MFT table. when files are created and deleted their entry in the MFT table is not immediately updated, otherwise the disk has to perform more seeks and writes. Which means if you create lots of small files at a time only to delete them (we're talking in the thousands range here though. Something like compiling GHC itself would do) you consume lots of MFT records. If you consume all of it, it will fragment. If it fragments you'll get a performance hit. I however don't believe this to be much of an issue with modern NTFS. Windows by default seems to reserve about 12.5% of the disk for MFT (so the space is contiguous), and myself after putting reasonably hard strain on the system have an MFT of 1263MB. (Though like I said, my temp folder is on another drive on a ramdisk). * DOS short names are an issue. For each file and folder you create, NTFS will make a second attribute for these DOS 8.3 shortnames. So not only do you get an extra write/read (meta data has to be kept in sync) but the actual act of finding a short name is expensive. If you have lots of files that share the same prefix it is essentially a linear scan. It starts at ~1 and continues till it finds a free name. Now imagine doing this over and over again. It gets slow. That's why a lot of people disable DOS shortnames generation on NTFS volumes for performance. And it does make a difference. The problem however is that certain tools use this hack to get around whitespaces in paths to paths given to unixy tools like configure (Which is another reason I think configure based packages on Windows are almost always the wrong thing to do). * Last access time, by default NTFS tracks three time stamps, creation, access and modified. Of these, last access is a problem. It's updated relatively often and has to be stored in two places. NTFS batches these updates and typically flushes them every hour or so. But at some point you will get multiple I/O requests for updating these. And it's not a very useful meta-data anyway for temp files etc. So I have this disabled on my ramdisk, it's non persistent data anyway, no need to keep that meta data up to date. * Slightly off topic for your use case but: Change journals, Windows has a fairly neat way to determine if a batch of files have changed. GHC does aggressive re-compilation avoidance checks, but these checks are individual file metadata record queries (this is also the case for Make). For one or two files it doesn't matter. But when compiling a huge project such as GHC itself, make's inefficient way of determening changes and it's process heavy model makes it way slower on Windows. Shake may be a glimmer of hope here since we can optimize it for the platform. So I'd say if you have a large project, you should use a build system that won't penalize you. Anyways you can read more about this stuff at https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2003/cc781134 
I have tried two processes and the time is almost the same, i.e. with two processes the work done is twice larger, i.e. almost 200% faster. So the problem is with the parallel GC on many cores.
I think it sounds better to just add exclusions for the relevant processes. You just have to catch them all, e.g. stack, ghc, the linker and whatnot.
Yup, thanks!, fixed the link.
&gt; You'll need FlexibleInstances for this because we've specified a concrete type in the instance head. You can drop the FlexibleInstances requirement by adding a new typeclass: class BooleanInput t where truthTable' :: (BooleanFunction f) =&gt; (t -&gt; f) -&gt; [[Bool]] instance (BooleanInput t, BooleanFunction f) =&gt; BooleanFunction (t -&gt; f) where truthTable = truthTable' instance BooleanInput Bool where truthTable fn = do x &lt;- [True, False] tbl &lt;- truthTable (fn x) pure (x : tbl)
This is great! The first two points -- AV and Indexing -- would be good to collect in some central store of tips and advice for windows users...
I wonder if you can get forward-propagation search with this approach too. I must learn more about Applicative; this is a really cool post.
He mentioned lens, but isn't this more akin to prisms? Both of which have fairly good descriptions [here](https://artyom.me/#lens-over-tea)
Very cool post! I recall reading the algebraic graphs paper earlier, that explained things well too. Having links for things that might trip people up is handy as well. One very minor complaint: please don't link to mtl's WriterT, it has a space leak :P. Maybe link to [this one](https://hackage.haskell.org/package/writer-cps-mtl-0.1.1.4/docs/Control-Monad-Writer-CPS.html) instead?
What is conspicuous about this problem is the lack of objective analysis. Everyone seem to limit themselves to opinions, not bothering to back them with evidence of any kind. I tried to solicit some more or less formal judgements ([here][reddit_testing] and [on Stack Overflow][ stacko_testing]), but to very little avail. Overall, there are **two kinds of testing**: * __Verifying examples__. This is also called "spec" for some reason, though a specification should not and must not be limited to this. When you take a single example of the function's behaviour out of your mind and write it down, this is an example. An example of an example: if we are testing `(+)`, we may write down `2 + 2 == 4`. In Haskell, you can do this kind of verification with several devices. * [GHC assertions][assertions]. I have not had an experience of using them yet. They have the least overhead possible: no libraries needed, no code generated. * [Hspec][hspec]. It is straightforward to use. In many established projects, you will find hundreds of Hspec cases. * Doctests. There is this [device called `doctest`][doct] tool. It offers a way to verify examples (and properties) supplied in Haddock descriptions right in the module code. What it does behind the curtain is run `repl`, paste your examples there and see what it says back. You may also employ [`doctest-discover`][doct_disco] and `stack test --file-watch` in a separate terminal to observe correctness as you go. It is very comfortable to use `doctest` when you fight your way drafting a new module, to ensure your quick iterations are not breaking anything. Kind of lightweight test driven development, as applicable to Haskell. Once I started using it, it boosted my confidence big way, so I do advise the practice. * Trivial properties. You can use a property checking engine (see below) with a very simple property in the same way as you would use Hspec. I have heard people do this with Hedgehog. * __Checking properties__. This is also called "prop". The idea is that a tool generates any number of examples for the same point and verifies them all automagically, according to the rules you have provided. If any one fails, it will alert you. * QuickCheck is the Odin of this pantheon. What it does is generate cases at random (in QuickCheck, this is called `arbitrary`), then reduces the first failure it stumbles upon to a simpler variant, until the simplest failing case possible is located. Your part is to define the `arbitrary` method, the `shrink` method that simplifies a case (though this one may be left to be trivial), and any number of actual properties to be verified. As I understand, it was very innovative for its time. It is still the go to choice, but, since recently, there is some criticism and there are several alternatives, listed next. There is also some choice of helper utilities for QuickCheck that provide automagic `Arbitrary` instances for elaborate types and ready made properties for some usual algebraic ideas. The documentation for QuickCheck is unfortunately a little outdated, but there is [this article][begr_quickc] that I enthusiastically advise. * `doctest`, again. You can use it to write inline QuickCheck properties, though it is not so comfortable in the sense that multiline properties are not supported at the moment. Hopefully there will be patches to solve this, but the maintainer was not very active of late. * [QuickSpec][quickspec] / [Speculate][speculate]. These devices allow you to discover some algebraic properties of your functions automagically. Unfortunately, they are at a rather experimental stage, and I am unsure of their day to day usefulness. (I do ask the kind reader to let me know their opinion if they have applied these devices to any success in their Haskell practice.) * [SmallCheck][smallcheck] / [LeanCheck][leancheck]. These devices generate cases not at random, but rather incrementally, from simplest to more elaborate. Nobody could explain if or when it is a better approach than random testing. (See links in the beginning of the comment.) * [Hedgehog][hedgehog]. As I see it, Hedgehog positions itself as a better replacement for QuickCheck, with another approach on the algorithmic or architectural level. I did not dig deep enough in to offer extended commentary. Then, there is the question of **discovery and integration**. A device that solves this problem is called a test framework. The ideal solution would find all your tests, wherever they are, run them all in a bunch, perhaps in parallel, and show you some beautiful output. There are several test frameworks around, but the most modern and best maintained appears to be [`tasty`][tasty] (together with [`tasty-discover`][tasty_disc]). There are also [`HTF`][htf] and [`test-framework`][tf] but I do not think there are benefits to favouring them, as they are less maintained and support a smaller choice of testing libraries. Now, my personal experience and advice would be to use inline doctests and `doctest-discover` whenever you start drafting some code. By letting you write down the code and the example in one streak, it saves you a context switch and greatly helps concentration. You will also gain stronger documentation habits along the way. Then, as your code gets tougher and greater, you may complement the examples and properties with a full blown framework, but at this stage you will also need some mathematical and philosophical background in order to understand well enough what "knowing the code is correct" means. I suggest you watch [this video][duncan] where a well known guru shows how one may reason about hard to architect test cases with a category diagram, to have a taste of this level of thinking. [reddit_testing]: https://www.reddit.com/r/haskell/comments/7tq1hl [stacko_testing]: https://stackoverflow.com/q/48498033 [assertions]: https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#assertions [hspec]: https://hspec.github.io/ [doct]: https://github.com/sol/doctest [doct_disco]: https://github.com/karun012/doctest-discover [begr_quickc]: https://begriffs.com/posts/2017-01-14-design-use-quickcheck.html [quickspec]: https://github.com/nick8325/quickspec [speculate]: https://github.com/rudymatela/speculate [smallcheck]: https://github.com/feuerbach/smallcheck [leancheck]: https://github.com/rudymatela/leancheck [hedgehog]:https://github.com/hedgehogqa/haskell-hedgehog [tasty]: https://github.com/feuerbach/tasty [tasty_disc]: https://github.com/lwm/tasty-discover [htf]: https://hackage.haskell.org/package/HTF [tf]: https://github.com/haskell/test-framework [duncan]: https://www.youtube.com/watch?v=mhKUHpQZIoc 
This all goes nice and smooth (I actually [went ahead and wrote me](https://codereview.stackexchange.com/q/188982) matrix multiplication a while ago) until you have to do something more realistic about these matrices. How about user input? It occurs that to input a vector at run time is not at all trivial, and it more or less requires two different kinds of natural numbers: `KnownNat` and `SomeNat`. [Here is one example](https://stackoverflow.com/a/13246864) of handling dependently typed input. (Coincidentally, the author of the answer is one of the authors of Agda.) I have yet to wrap my mind around it.
Well, yes, since Template Haskell and typeclasses are both *code generation tools*, my idea is that anything that can be done with typeclasses, can be done with Template Haskell as well. *(I do not have a proof that it is strictly a superset, but it is plausible to believe. The ordering restrictions imposed by Template Haskell do bother me in this regard.)* In particular, `n` will have to be somehow known at compile time when dealing in typeclass based code as well, until Haskell actually gets dependently typed. *(This is also a contestable point; I would even enjoy seeing it refuted.)* Per chance you know of a treatise on Agda that is written, rather than spoken? Being a non-native speaker, it is problematic for me to parse the lectures, especially since they lack subtitles, and the scarce notes found within the repository alone do not seem too inviting. The Idris book looks promising, but, as Idris does not have Haskell back end, it must be be less useful to me than Agda in that I could integrate code from the latter, but not the former, with my current Haskell projects. Of these folks, Coq has the strongest literary foundation. Both [Software Foundations](https://softwarefoundations.cis.upenn.edu/) and [Certified Programming ...](http://adam.chlipala.net/cpdt/) look rather monumental, so it might be the farthest going and the most treasure rich of those rabbit holes we have around. It also has a Haskell code generator.
You can typically get away with using a sort of "Coyoneda" trick on each one of your data constructors. Instead of FooInt :: Int -&gt; Foo Int use FooInt :: (Int -&gt; a) -&gt; Int -&gt; Foo a when you are building an ADT you intend to consume later that would otherwise have to have a separate "Map" constructor to quotient out. Now you pass it `FooInt id` to start, and have a place in the constructor to put the mapped functions. The problem is now you're doing a lot more applications of these extra identity functions. Unfortunately, the difference is non-negligible and can cost about 2x performance in some cases. An example where this hurts is something like building a free monad transformer where you have to layer in (normally-trivial) `m` effects at every level to hit the laws on the nose, but quotienting out a secret 'Lift' constructor by not exporting it lets you avoid a bunch of `(&gt;&gt;=)`'s. There are also some annoying cases where knowing that you have an unmapped value allows for asymptotically more efficient operations that compute the same result.
Also take a look at [monad-unlift-ref](https://www.stackage.org/haddock/lts-11.2/monad-unlift-ref-0.2.1/Control-Monad-Trans-Writer-Ref.html) which implements `MonadWriter` in terms of `STRef`.
Uh thank you for the pointer!
The point is to avoid redundancy, and reduce boilerplate code. Whether data types are redundant depends on the architecture and design of the project. The question is, what is your single source of truth? If you can use the data types themselves as such, then you don't need to generate them, and chances are you can build the rest with less magical metaprogramming too.
That I'm not sure. I'm not a Haskell programmer by any means but what little I know about prism is that the type defined as f in task *is* Maybe. Prism on the other hand, uses Maybe f v. This could be an interesting research paper proving some implications from these facts.
[removed]
The DSL sure, but does the mkYesod function?
In which way does Prism use "Maybe v with no f"? type Prism s t a b = forall p f. (Choice p, Applicative f) =&gt; p a (f b) -&gt; p s (f t) Note that a Prism needs to work for any `f` satisfying the constraints, not just `Maybe`. In particular, I can instantiate a Prism with `p ~ (-&gt;)` and `f ~ Identity` to use it as a Setter: -- | -- &gt;&gt;&gt; bumpIfLeft (Left 0) -- Left 1 -- &gt;&gt;&gt; bumpIfLeft (Right 0) -- Right 0 -- -- or simply: -- -- &gt;&gt;&gt; over _Left (+1) (Left 0) -- Left 1 -- &gt;&gt;&gt; over _Left (+1) (Right 0) -- Right 0 bumpIfLeft :: Either Int Int -&gt; Either Int Int bumpIfLeft e = e' where f :: Int -&gt; Identity Int f = Identity . succ f' :: Either Int Int -&gt; Identity (Either Int Int) f' = _Left f e' :: Either Int Int e' = runIdentity (f' e) A Task also needs to work for any `f` satisfying the constraint: type Task c k v = forall f. c f =&gt; (k -&gt; f v) -&gt; k -&gt; Maybe (f v) This is not a specialization nor a generalization of a Prism, but lens does define a more general type, [Optical](https://www.stackage.org/haddock/lts-11.2/lens-4.16.1/Control-Lens-Type.html#t:Optical) which generalizes both, kind of: type Optical p q f s t a b = p a (f b) -&gt; q s (f t) type Prism s t a b = forall p f. (Choice p, Applicative f) =&gt; Optical p p f s t a b type Task c k v = forall f. c f =&gt; Optical (-&gt;) (Kleisli Maybe) k v k v 
Ah I might have been way off the mark. I typically use the Lens, with m being Maybe below. type Lens s a = Monad m =&gt; (a -&gt; m a) -&gt; s -&gt; (a, m s) It seems like I've just been calling this a Prism without it actually being a prism. Still gets the job done though.
If you are traversing over some finite, not too big structure that is already in memory, that might be an okay case for mtl's WriterT.
https://en.wikipedia.org/wiki/Line_length Traditional line length research, limited to print based text, resulted in a variety of results but generally for printed text it is widely accepted that line length fall between 45-75 characters per line (cpl), though the ideal is 66 cpl (including letters and spaces).
Servant is nowhere as fully featured as Yesod for building web applications.
This is brilliant ! I've been thinking for a while about how to rewrite a reporting tool memoising sql queries in intermediate tables. It is basically a makefile where each target s and dependencies represent a table. If table B requires table A the makefile will look like -- General rules to run a SQL file %.table: %.sql mysql &lt; $&lt; touch $@ B.table:A.table With B.sql being something like `select something from A`. The A.table and B.table file don't contain anything, they are just there to tell the system if and when the table as been computed. It works pretty well, but I have now about a hundred queries/report and it is pretty difficult to keep track of the dependencies. Also the most complex part are not written in SQL but in R and I've been planning to rewrite it using Haskell. Writting the query/report in Haskell is trivial but the hard bit is to be able to persist intermediate result (in a table) but also being able to generate accurate dependencies and being able to force rebuild and invalidate intermediate results. I've been looking at [shake](https://shakebuild.com/), [Haxl](https://hackage.haskell.org/package/haxl) but they didn't quite work. I tried my own way, but always get stuck with the Monad/Applicative problem. In order to be able to do static dependencis, you need an applicative interface, but in practice (to be able to access, the database, force recomputation) etc the code needs to be Monadic. So I came with something quite similar to `Task` (which I remember being called Task too) but never managed to be able make the choice between Applicative and Monadic. This `Task` seems to be exactly what I've been trying to figure out !!! 
I agree. However, I think it is better to endorse the safer case by default, since the additional overhead of adding it is minor at best.
Why is this important information not documented on Hackage!?
Liquid haskell?
`Optical (-&gt;) (Kleisli Maybe) k v k v` is cool, thank you! It doesn't look like there is any function that accepts this type of optics though. Anyway, it's good to know that we can express Task as a special case of an existing abstraction.
This workshop is always great fun, but it could use more submissions! Lots of people talk about Haskell being fast, so why not write up a short paper about it? 
Do you mean does mkYesod look for a function named plopResourceName, and call it? If so, yes it does
I've added the following footnote: &gt; (*) Beware: as of writing, standard WriterT transformers have a space leak which may be an issue if a task has many dependencies. You might want to consider using [a more efficient CPS-based WriterT transformer](https://hackage.haskell.org/package/writer-cps-mtl/docs/Control-Monad-Writer-CPS.html). 
Awesome -- it's great that you found this useful! Do you think it may be useful to release `Task` and associated functions as a library?
 Is highly unlikely that I'll have time soon to work again on this project even though reading this paper make me which I had. However I think is always best if everybody works on the same foundation that reinventing the wheel. So, as I'm sure this abstraction as lots of potential it's probably best to have a official package for people to use or base other packages on. 
Nah, I was just saying that if you already have a list in memory, and WriterT accumulates as many thunks as the length of the list, that doesn't make things *much* worse. Especially if you are accumulating into another list, instead of some "small" value that would become much smaller when evaluated.
Got it -- will do!
Can't see why you'd want that. GHC is almost always better at the StateT style of "mutation" used by the cps WriterT than the IORef style used by that. I'd wager that a benchmark would prove the cps one almost always faster
Let `TaskA` correspond to applicative tasks in your `Traversal` encoding. Then we have: `type TaskA k v = Traversal k (Maybe v) k v` Looking at the `lens` library, I don't think any existing function actually accepts such `TaskA`. Usually they want something like `Traversal k k v v` instead. 
There is no requirement for `Eq` to be structural in Haskell. In fact there is a pretty common instance for `Eq` that isn't structural due to the existence of -0. Prelude&gt; 0 == (-0 :: Double) True Prelude&gt; recip 0 == recip (-0 :: Double) False (More controversially, it also fails reflexivity due to nan) Prelude&gt; let nan = 0/0 :: Double in nan == nan False There are fairly legitimate cases where you want to use `Eq` to model some kind of quotient over another data type. e.g. if you only had natural numbers and not integers, you might model integers as the difference of two natural numbers, and then normalise by subtracting the smaller from both in order to compare. Another common example is to do something like an 'Arg' data type to provide argmin and argmax when used for comparisons, it only compares on the 'key' part. To be compatible with the Ord instance, the Eq instance would have to be non-structural as well.
Also, is Task a Monad, Apicative etc ? Is it possible to mix type for example in your spreadsheet example so that A1 et A2 have différent types ?
&gt; I typically use the Lens [...] below. &gt; &gt; type Lens s a = Monad m =&gt; (a -&gt; m a) -&gt; s -&gt; (a, m s) &gt; &gt; It seems like I've just been calling this a Prism without it actually being a prism. I'm afraid you weren't just using the word "prism" incorrectly, but the word "lens" as well! The definition you gave is definitely not that of a lens. Where did you take it from? First of all, a typo: the `m` is not in scope, presumably you meant to quantify over it: type Lens s a = forall m. Monad m =&gt; (a -&gt; m a) -&gt; s -&gt; (a, m s) Unlike in a type signature where unbound type variables are implicitly quantified, in type synonyms you need to quantify them explicitly. Next, you are quantifying over all _Monads_. That's way stronger than a lens. The definition from the lens library is: type Lens' s a = forall f. Functor f =&gt; (a -&gt; f a) -&gt; s -&gt; f s And the definition for a Traversal, which is stronger than a Lens, is: type Traversal' s a = forall f. Applicative f =&gt; (a -&gt; f a) -&gt; s -&gt; f s The reason for those constraints is that `Functor` only allows you to transform a value of type `f a` into a value of type `f s`, it doesn't allow you to combine two `f a`s into a single `f s`. This guarantees that the lens is only modifying a single `a`, not several. The `Applicative` constraint is more generous because it does allow you to combine several `f a`s, or even zero `f a`s, into a single `f s`. As a result, a Traversal is a more general optic than a lens, one which can point at zero or more `a`s inside an `s`. A `Monad` constraint allows you to choose which `m a`s to combine into a single `m s` depending on the `a`s produced by some of the `m a`s. This means that depending on the value to which you set the `a`s, it could choose a different subset of the `a`s inside the `s` and set them to that value. That doesn't sound like a very useful abstraction. One last problem is that you are returning both an `a` and an `m s`. The reason optics are defined as functions which work for any `f` is so that this `f` can be instantiated to different types in order to implement different operations, e.g. `Identity` for `over` and `Const a` for `view`. In particular, for lenses the only two operations which need to be supported are `over` and `view`, so since your type already supports both, you don't need to use an `m` at all! type Lens s a = (a -&gt; a) -&gt; s -&gt; (a, s)
One easy way to mix `Applicative` and `Monad` tasks is to have two task descriptions: `ta :: Task Applicative k v` and `tm :: Task Monad k v` and then you could ask whether `ta` contains a description for a key `k` via `isInput ta k` and, if it not an input, you can do static analysis on it, etc. If `isInput ta k == True` then you can check whether `tm` has the key. You will not be able to analyse `tm` statically, but you could still execute it.
I may be. I work in Reading and may be able to stop by. What's the scope? I'm a still a bit of a beginner.
You could also do a Haxl-style analysis of `Task Monad` when you can interpret tasks that do not actually use monadic bind `&gt;&gt;=` as if they were applicative (e.g. do static dependency analysis on them). We have some proof-of-concept code that does this, but we decided not to cover this in the paper, as there is a subtlety about satisfying the `&lt;*&gt; = ap` law, which takes up quite a bit of space to describe accurately.
Hi, I'm releasing a new version of the `squeal-postgresql` library with lots of new features, including type-level constraints, migrations, transactions and connection pools.
Liquid Haskell?
Well it’s quantified over all Applicatives, right—so I think you can just plug in the Identity and match on the Maybe like before to find the static deps, no? As for `type TaskA k v =...` and looking for combinators, I usually leave all the type parameters in place at least internally while wrestling with the algebraic structure. You can drop some for the public API, of course.
&gt; Well it’s quantified over all Applicatives, right—so I think you can just plug in the Identity and match on the Maybe like before to find the static deps, no? I don't think we can do this. Consider this task: sqrtTask :: TaskA String Double sqrtTask fetch "B1" = f &lt;$&gt; fetch "A1" where f x | x &gt;= 0 = Just (sqrt x) | otherwise = Nothing sqrtTask _ _ = pure Nothing There is no way we can determine if key `B1` is an input for `sqrtTask` without feeding it actual values: it will be an input only when `A1` is negative! 
I like it!
I think `Endo (Klesli m)` for some `Monad m` is the most common. One usage I've found is `type Filter x = Endo (Kleisli Maybe) x` lets you build a pipeline very easily (`Endo . Kleisli` is `(b -&gt; Maybe b) -&gt; Filter b`). Given that type, you can build up a pipeline of filters `x, y, z :: Filter a` by doing `pipeline = x &lt;&gt; y &lt;&gt; z` (or `fold [x, y, z]`). Here's where I first found it: https://twitter.com/porges/status/887489842828922885
Would you be open to taking summer interns?
Outer box only, no BIH: 4.6 to 5.6 seconds per sample Outer box only, with BIH: 1.4 to 1.6 seconds per sample Full scene, no BIH: 89.6 seconds per sample Full scene, with BIH: 5 to 6 seconds per sample
You also can't bind out of the monad with &lt;- I believe
ahhh, that is unfortunate. :(
I decided to take a crack at writing a (very naive) benchmark of the two. At least in my tests they seem pretty comparable, with the slight nod going to the CPS version, as you expected: https://gist.github.com/jkachmar/ff2db4d607c68f08bd4b6bed71b3c631
good
This is going to be fun
Unfortunately not. Part of our work is to assist GHC contributors in general, though. And we often act as mentors for Haskell Summer of Code or similar events.
It's worth noting that while sv's parser is slower, you can [substitute cassava's parser](http://hackage.haskell.org/package/sv-cassava) if it is sufficient for your needs. This could useful if you want sv's style of decoding, but need greater performance than its parser currently offers.
That's correct. It's type application combined with [type level natural numbers](https://hackage.haskell.org/package/base-4.11.0.0/docs/GHC-TypeLits.html#t:Nat). Squeal uses these for parameterized queries and manipulations, `$n` in SQL), which is very important for inputting Haskell values in your statements. And Squeal parameters are type-checked to be impossible to get an out-of-bounds error thanks to the `HasParameter` constraint.
It should definitely be strongly avoided. A huge amount of code assumes that if `x == y`, either directly or via transitivity, that `x` and `y` are completely interchangeable and may as well be selected by `unsafePerformIO randomIO`.
that seems *extremely* surprising -- can you post a source for that?
The unsafePerformIO stuff was of course hyperbole. But just see things like `Set` documentation where which element to keep when given equal elements is not defined / documented.
&gt; you can replace the printer that GHCi uses Btw, my current favorite is `Text.Pretty.Simple.pPrint` of [`pretty-simple`](https://hackage.haskell.org/package/pretty-simple) as it does the indenting and coloring in a single pass in native Haskell! It even color-codes parentheses based on indentation level.
Good idea! Haxl-style task analysis is tricky, and writing a blog post about it is the best way to really understand how it works :)
ah, ya I see what you mean. Each piece is dependent on computation, yes, but what it can give you (and what I was using for my Redis stuff) is that it gives you a guarantee that there's no *mutual* computational dependency in the traversal - i.e., that these may be executed in parallel. When I lookup multiple keys, I don't know statically whether they'll return `Just somedata` or `Nothing`, but what I *do* know statically is that the just-ness or nothing-ness of the pieces are all independent of each other, and thus can be done in parallel. I don't know if this is exactly what you need in your build system, but it does give you a static guarantee about where parallelism will occur, unlike some of the other systems out there which try to tear down an AST and compute where parallelism can be used at runtime.
Yes, indeed! We can still do accurate dependency analysis of `sqrtTask`. It's interesting that for us parallelism was a secondary concern: we perform dependency analysis primarily to make sure we build keys in the right order, not to achieve parallelism. And, of course, once you know the dependency graph, building independent keys in parallel is a good optimisation.
How do I use stuff like `nub` on `MyType`? Do I have to lift `nub` somehow into this `StructuralEq` typeclass?
Just to point out that `Traversal s (Maybe t) a b` isn’t actually a valid law-abiding traversal (you can tell from the type). By and large `lens` does still work OK when confronted with unlawful optics, so you may well be able to get by with `lens`’s combinators, but it’s a signal that the connection is probably not especially deep.
By the way, I've been reading your blog post on query aggregation and spotted this definition: data T x y a b = T (a -&gt; ([x],[y] -&gt; b)) This looks very close to *free applicative tasks* which /u/ndmitchell used when prototyping various task analysis functions: data TaskA k v a = TaskA [k] ([v] -&gt; a) This is essentially a list of dependencies `[k]` and a function that combines the obtained values `[v]` into the result `a` (for simplicity I'm omitting the `Maybe` and the parameter `k` here). I think Neil wanted to write a blog post about free tasks, including the monadic version.
Agreed. And furthermore, since we are really interested in the case when `s = a = k` and `t = b = v`, the resulting type `Traversal k (Maybe v) k v` does not seem to fit any of `lens`'s combinators, and neither does `Maybe`-less version `Traversal k v k v`. 
I do indeed. Perhaps Friday with luck. 
Yup, that was my original definition which turned out to be isomorphic to Traversal (with the parameters reshuffled). It was super surprising to me that this was the case, because I felt like what I was doing was not even related to lenses at all.
&gt; it’s a signal that the connection is probably not especially deep. I actually interpret it as a signal that many of the lens definitions (in terms of what they actually entail) are more algebraically fundamental than what the original lens+laws vision entailed.
I'm aware of existence of Scotty, just don't prefer run around in circles to avoid elemental issues. I think the *rant* is not about wrong or too complex selection of a framework but general flaws of ghc's ecosystem itself.
Thanks, that's how I attempted write more readable code. It is a big difference live in a little world of well accustomed DSL and work on a real world application composed of many libraries which should share and adhere to some common idioms.
We have R bindings via FFi and the elegant 'declarative' mcmc libraries. We also have the dataHaskell group which is making some progress on the ecosystem. 
This approach may only work when you use a framework which covers all project needs and is written with consistent API. I may live in a less ideal world or am just unlucky, but always need to mix in another hackage packages (various filetype parsers, support libs for payment gateways etc). 
&gt; Per chance you know of a treatise on Agda that is written, rather than spoken? There is a book called [Verified Functional Programming in Agda](https://dl.acm.org/citation.cfm?id=2841316). I read most of it, but unfortunately it uses its own "standard library". Don't expect to be able to use the official one afterwards. There is also the [Agda Wiki](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=Main.Othertutorials) which contains some tutorials and papers.
Squeal offers escape hatches to write raw SQL for something like this. Internally, Squeal uses these escape hatches to define all of its expressions. E.g. stArea :: Expression relations grouping params (nullity 'UnsafePGType "geometry") -&gt; Expression relations grouping params (nullity 'PGfloat4) stArea = unsafeFunction "ST_Area" 
I've kind of always wanted the ability to use an arbitrary State monad transformer in GHCi, but I've since come up with a work-around that lets you have a temporary in-memory state that you can use in GHCI. It is just a Map with Text labels associated with Dynamic types. module Global (global, peek, safePeek, what, defined, atomicUpdate) where import System.IO.Unsafe import Control.Concurrent.MVar import Data.Dynamic import Data.Typeable import Data.Map import Data.Text -- Store the environment in a persistent, thread-safe IO reference. -- This reference is not exported, and forced to be memoized using the "NOINLINE" pragma env :: MVar (Map Text Dynamic) env = unsafePerformIO (newMVar empty) {-# NOINLINE env #-} -- | Associate an arbitrary typeable value with a name. The value is forced with 'seq' before obtaining the state lock. global :: Typeable a =&gt; Text -&gt; a -&gt; IO () global name value = seq value $! withMVar env (\ map -&gt; return (insert name (toDyn value) map)) -- | Calls 'safePeek' but unwraps the element from the 'Just' constructor, or throws an exception. -- This function is often more convenient to use because you do not have to check the Maybe type returned by 'safePeek', you just use the element or let it fail. peek :: Typeable a =&gt; Name -&gt; IO a peek name = fmap (\ elem -&gt; case elem of Nothing -&gt; error ("Undefined " ++ show name) Just a -&gt; a) (safePeek name) -- | Lookup an element that has been associated with a name using 'global', return Nothing if hasn't been defined. safePeek :: Typeable a =&gt; Text -&gt; IO (Maybe a) safePeek name = withMVar env (\ map -&gt; return (lookup name map &gt;&gt;= fromDynamic)) -- | Perform an atomic update on a stored element if it exists, otherwise do nothing. atomicUpdate :: Typeable elem =&gt; Text -&gt; (a -&gt; IO (elem, a)) -&gt; IO (Maybe a) atomicUpdate name update = modifyMVar env (\ map -&gt; case lookup name map &gt;&gt;= fromDynamic of Nothing -&gt; return (map, Nothing) Just elem -&gt; do { (elem, a) &lt;- update elem; return (insert name (toDyn elem) map, Just a); }) -- | Returns the type of the value stored at a name. what :: Text -&gt; IO (Maybe TypeRep) what name = withMVar env (\ map -&gt; return (fmap typeOf (lookup name map))) -- | Returns True if the name has been defined with the 'global' function. defined :: Text -&gt; IO Bool defined name = fmap (\ elem -&gt; case elem of { Nothing -&gt; False; _ -&gt; True}) (safePeek name) 
This disallows all interesting `Eq` instances, doesn't it? If I'm free to choose `f :: a -&gt; b` and also free to choose the target type `b`, then I can choose `b` to be `Boolean` and then choose `f` to partition any equivalence class of size `&gt; 1` in `a`. This forces all equivalence classes to singletons. (This assumes `Eq` defines an equivalence relation, which I know technically it doesn't in practice - the `Double` instance for example, as /u/edwardkmett points out, fails reflexivity - but seems like a good idea in theory)
I don't know the reasons behind this, but I suspect that it may have to do with the fact that "public domain" isn't really a licence but rather the absence of a rightful copyright holder, and that "releasing works into the public domain" is not legally recognized in all jurisdictions - and where it's not, trying to do so amounts to "All Rights Reserved". CC0 is not recommended for software, and comes with many of the problems of "Public Domain". For a detailed explanation, see https://opensource.org/faq#public-domain Best would probably be to pick the most permissive license available.
It we are talking about types with exposed constructors, you say interesting and I say surprising and risky. If we are talking about abstract types it is assumed that `f` is restricted to functions built out of the exposed interface.
I think I was able to improve on your Tabulated class so that you don't need to define individual instances for Bool or Int: allValues :: (Bounded a, Enum a) =&gt; [a] allValues = [minBound..maxBound] class BoundedEnumFunc fn val | fn -&gt; val where makeTable :: fn -&gt; [[val]] instance (Bounded a, Enum a) =&gt; BoundedEnumFunc (a -&gt; a) a where makeTable f = do x &lt;- allValues return (x:[f x]) instance (Bounded a, Enum a, BoundedEnumFunc (a -&gt; b) a) =&gt; BoundedEnumFunc (a -&gt; a -&gt; b) a where makeTable f = do x &lt;- allValues row &lt;- makeTable $ f x return (x:row) It has some code duplication, but is much more general. Plus, it only needs FlexibleContexts, not UndecidableInstances.
Not sure why you ditched yesod. If you're coming from rails/django/whatever, it's the simplest and most natural next step. The TH-based DSLs provided by yesod are all optional - you can skip them all if you want. But why would you want to? They're great: similar to the corresponding DSLs in rails/dgjango/etc., but simpler, and safer due to type safety.
Ah snap. The reason is that Hackage is moved to Cabal-2.2 internally, and its license check is https://github.com/haskell/hackage-server/commit/a0f76c0a856b1f42ea55aeb3d4329364bfd6a7ed So you could either - change to `cabal-version: 2.2` + `license: CC0` - wait till the check is fixed for older `cabal-version` + `license: PublicDomain`, I opened an issue so we don't forget about this issue: https://github.com/haskell/cabal/issues/5238
All standard instances at least have transitivity and symmetry laws and it is only the floating point types which fail reflexivity. Failing reflexivity is unfortunately pretty damning because it means you can't use `reallyUnsafePtrEquality#` as a safe fast path to short-circuit equality checks on True. I personally find it interesting to consider what over-engineered solution would have to be added to `Eq` to allow for safe referential equality checks as a fast path. e.g. adding a member to `Eq` for `reflexive :: Eq a =&gt; Tagged a Bool` that lifts its value by `(||)`ing together the results for all constituent parts would allow all existing instances. One could bolt such a beast on and have it default using GHC.Generics, and add it to deriving code for `Eq`, at the cost of a metric ton of breakage if it wasn't defaulted. The other default would be to risk over-conservatively assuming `False` for user-specified instances by default instead and not use generics. I try to keep track of these sorts of over-engineered solutions, because occasionally we get compiler features that make them practical. Another example of a way over-engineered solution would be to add a member to `Functor` that lifts `Coercion a b -&gt; Maybe (Coercion (f a) (f b))`, to allow an efficient form of `fmapCoerce :: (Functor f, Coercible a b) =&gt; f a -&gt; f b` that works in O(1) for representational functors, and O(n) for the "moral" functors that aren't representational and can still handle more complicated functors like `Compose`. Similarly to the above a solution could exist in user space and `DeriveFunctor` could automatically implement it as returning `Just ...` for any representational functor, and return Nothing for a nominal functor. This one is interesting because with a little bit of tweaking to GHC, e.g. by making it so that `DefaultSignatures` can have more than one fallback and by adding `QuantifiedConstraints` we could have this one today! The same sort of multiple tiers of `DefaultSignatures` might allow the `reflexive` check to work as well for `Eq`. e.g. if `Generic` compute the generic solution, and if not, assume `False`.
Sorry, `CC0-1.0`
I use `doctest` just to make sure that my examples always work.
Nice!
Some states tax or license the use of "public domain" software or IP, please do _not_ release your software into the public domain and instead use CC0 or Apache2 depending on your goals.
IMO `==` and `Eq` both mean equality. IMO those functions should (or do already) have a variant that takes in a projection or an equivalence relation. Alternatively perhaps an equivalence class could be created, it should probably be a superclass of Eq, and there should probably be a law that `x ~= y = x == y`, that way if you combine some equivalence functions with some equality (or Ord / other subclasses of Eq) ones everything works out.
The OSI doesn't recommend releasing software into the public domain, but SQLite is public domain software, for example (on the other hand, it's complicated for them to accept contributions because of it). The OSI doesn't recommend CC0 because they're concerned about the language around patents; I think the point is debatable but in any case should only be a concern if you have patents in the software you're releasing (note that the CC0 language doesn't preclude a patent owner including a specific patent grant alongside the CC0 dedication). More broadly, people have varying goals and motivations to release their software, which is why we need options like CC0 in addition to MIT and GPL etc.
What? Citation?
The current aim is to have a talk once per month and every other two weeks to meet up to do whatever people are interested in (coding sessions or a study group or something else). I hope to make it as beginner friendly as possible.
Most ten ton hammers are worthless for 5 lb problems. Generally, not nearly as worthless as 5 lb hammers are for ten ton problems.
Awesome, thanks! I looked around at the Haskell / PostGIS ecosystem a while ago, know of any existing libraries with good types I could use?
See this comment's explanation: http://www.reddit.com/r/haskell/comments/87t7nn/-/dwff0ap
This is the first release of `tree-traversals`, so I'm definitely looking for feedback, critiques and suggestions. Names, module organization, documentation, it's all up for grabs. [`Data.Traversable.TreeLike`](http://hackage.haskell.org/package/tree-traversals-0.1.0.0/docs/Data-Traversable-TreeLike.html) is probably the best jumping-in point for the documentation. I made the decision to not provide an instance of `TreeLike` for `[]`, instead providing newtype wrappers for the two possible implementations ([`List`](http://hackage.haskell.org/package/tree-traversals-0.1.0.0/docs/Data-Traversable-TreeLike.html#t:List) and [`Flat`](http://hackage.haskell.org/package/tree-traversals-0.1.0.0/docs/Data-Traversable-TreeLike.html#t:Flat)) After all my work [on what I wound up naming the `Batch` applicative transformer](https://www.reddit.com/r/haskell/comments/81ux6k/name_that_applicative_transformer_or_let_me_tell/), I wound up not using it in the final package. Interested parties can still [find `Control.Applicative.Batch` in the commit history](https://github.com/rampion/tree-traversals/blob/6826580bf4e15976811090f577ae814638c1eb8b/src/Control/Applicative/Batch.hs). In the future if I (or someone else) finds a practical use for it, I'll probably wrap it up in its own package. One point I don't really highlight in the documentation (apart from that `instance (TreeLike leftTree, TreeLike rightTree) =&gt; TreeLike (Sum leftTree rightTree)`) is that `TreeLike` instances are allowed to be heterogenous over their type of subtree. This was the reason I stopped using `Batch` and switched to a different applicative transformer, [`Control.Applicative.Phases`](http://hackage.haskell.org/package/tree-traversals-0.1.0.0/docs/Control-Applicative-Phases.html#t:Phases). 
&gt; Maybe I should just say that in the package description. Please do, every package description should link to or at least mention the entry point for documentation (which are found across many different locations): whether it's the top-level module imported by the user, a `.Tutorial` module, a README and the repository, a website, etc. Some packages have extensive documentation, that's not obvious from either Hackage or its repo. 
`TreeLike` looks suspiciously like `Term` from the Scrap Your Boilerplate paper :) Were you aware of this when you wrote the library? If so, have you tried applying these techniques to [Uniplate](https://hackage.haskell.org/package/uniplate/docs/Data-Generics-Uniplate-Operations.html) or [Plated](https://hackage.haskell.org/package/lens/docs/Control-Lens-Plated.html) datatypes?
This is not using `ScopedTypeVariables`. It's just a type annotation and exactly how `DuplicateRecordFields` is meant to be used. [GHC user guide on `DuplicateRecordFields`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#record-updates)
I see what you're talking about: &gt; The beautiful thing about building a recursive traversal strategy out of non-recursive `gmapT` is that we can build many different strategies using a single definition of gmapT. I'd read (but not thoroughly grokked) the [paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2003/01/hmap.pdf) a while back, but I wasn't consciously aware of the connection. Definitely going to have to look closer at this - thanks!
When creating a type, you cannot have recursive synonyms. The correct way of doing your recursive data type would be to use the `newtype`operation like you were. data Literal a = Int Int | Float Float | Record [(T.Text, a)] newtype FixedLit = Fix (Literal FixedLit) 
&gt;Just use syntax highlighting, ignore ghc-mod/HIE/intero. Use GHCi and `ghcid` in terminals along-side Atom. What are the benefits of ghci and ghcid versus ghc-mod/HIE/intero?
Seconding this. I wrote and help maintain the `intero-neovim` plugin. It works *really* well for small projects that don't do anything weird. The performance becomes unusable on larger projects (10,000 or so lines). You can work around this by aggressively splitting your project into packages, and then only selecting a small subset of the targets to actually work with. The time I lose with the occasional OOM cycle more than makes up for the tiny convenience of having type-at-point. `ghc-mod` is slower than `intero`, and has way more trouble building and working in projects. I could never get it working for more than a few days before it'd break mysteriously again. --- So `ghcid` is great. It's just GHCi, and it watches your files and reloads whenever you save anything (and maybe runs your test suite, or maybe it runs a webserver, your call). It is fast, reliable, and does what you'd want. 
Thanks for the link! Seems like a simple way to pimp ghci.
Any for sublime?
For future reference the identifiers are all from the SPDX license list (easier than trawling the Cabal source): https://spdx.org/licenses/
CC0 is better than just saying that you release into the public domain, but it is still problematic. I believe there are two main reasons for this: the first being that it is not actually a license, but a declaration of the intent to give up your rights, with a license-like fallback that only applies when the declaration is void. So it is not trivial to judge in an individual case what the actual situation is - is it in the public domain? Or do we need to check the license? What happens when multiple jurisdictions are involved? What if you combine CC0-"licensed" works with other open source works? The second problem is that CC0, like the other CC licenses, was written with things like music, prose, or visual art in mind, not software, and it does not take certain software-specific concerns into account, such as patents, or the difference between source code and build artifacts, or the mechanics of linking, importing, and other forms of code reuse.
IIRC, Creative Commons themselves also recommend against using CC licenses for software.
Ok, so that is actually consistent with what I found here: resourceLoader :: (String -&gt; IO String) -- ^ A reader which processes file contents -&gt; [GlobPattern] -- ^ File glob; relative to the @site@ directory -&gt; SiteM [Value] -- ^ Returns a list of Aeson objects resourceLoader = resourceLoaderGen I will look into liftIO, have not used that before. Thx.
 type Task2 c k v = forall f. c f =&gt; k -&gt; Maybe ((k -&gt; f v) -&gt; f v) I noticed `Task2 Monad` is related to the Free Monad. According to [van Laarhoven free monad](http://r6.ca/blog/20140210T181244Z.html): data PStore i j a = PStore { pos :: i; peek :: j -&gt; a } newtype VLFree f a = VLFree (forall m. Monad m =&gt; (forall x. f x -&gt; m x) -&gt; m a) Free (PStore k v) a ~ VLFree (PStore k v) a ~ forall m. Monad m =&gt; (forall x. PStore k v x -&gt; m x) -&gt; m a ~ forall m. Monad m =&gt; (forall x. k -&gt; (v -&gt; x) -&gt; m x) -&gt; m a (Yoneda) ~ forall m. Monad m =&gt; (k -&gt; m v) -&gt; m a Thus Task2 Monad k v ~ k -&gt; Maybe (Free (PStore k v) v) I think this explains why `dependencies` must take `Task Applicative` in another way -- you can't do same thing against the representation which uses Free. I don't know this applies to `Task2 Applicative` same way (related to free applicative) but expects so.
[Unlicense](https://unlicense.org/) is for software. Also [0BSD](https://spdx.org/licenses/0BSD.html) :)
https://haskell.futurice.com/ provides a Python-based installer [`haskell-on-macos.py`](https://haskell.futurice.com/haskell-on-macos.py) for GHC + Cabal; it's easy to use and allows you to conveniently manage multiple GHC installations in an obvious way. usage: haskell-on-macos.py [-h] [--make-dirs] [--paths.d] [--ghc-alias GHC] [--cabal-alias CABAL] [--dry-run] [--force] [--verbose] [--install-dir DIR] [--download-dir DIR] command ... Install Haskell tools on macOS positional arguments: command Command to execute: all, install optional arguments: -h, --help show this help message and exit --make-dirs Create install directories (uses sudo) --paths.d Create /etc/paths.d/ghc (uses sudo) --ghc-alias GHC Create versionless ghc aliases --cabal-alias CABAL Create versionless cabal alias --dry-run Dry run: don't alter file-system --force Execute rules even destination exists. Likely to fail. --verbose Print commands executed --install-dir DIR Installation directory, default /opt --download-dir DIR Downloads directory, default ~/Downloads