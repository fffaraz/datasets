I‚Äôd never seen this project before, it‚Äôs quite interesting. 
Ah, I see what you mean. To be honest I did it like this because the previous distributions seemed to be just source tarballs on a server somewhere (like `http://ghcjs.tolysz.org/ghc-8.0-2017-02-05-lts-7.19-9007019.tar.gz`) so I figured that was the expectation. I am using GitHub to host these purely because it was an easy way to do it. I also assumed that the archive produced by `cabal sdist` had some important differences to a tarball of the repo itself, but perhaps I am mistaken there? 
Caveat: I am not a formally trained category theorist, I just play one on the internet. So looking at this: data CFTFinal f a where CFTFinal :: Traversable t =&gt; (forall x. t x -&gt; f x) -&gt; t a -&gt; CFTFinal f a I'm immediately reminded of [Coyoneda](https://hackage.haskell.org/package/kan-extensions-5.1/docs/Data-Functor-Coyoneda.html#t:Coyoneda). I'm pretty sure it is `Coyoneda`, just for a different category. We can generalize `Coyoneda` for arbitrary categories thusly: data Coyoneda obj arr f a where Coyoneda :: (obj a, obj b) =&gt; (a `arr` b) -&gt; f a -&gt; Coyoneda arr f b So the traditional `Data.Functor.Coyoneda.Coyoneda` is isomorphic to `Coyoneda () (-&gt;)`. Then we can consider a category where the morphisms are natural transformations, newtype f ~&gt; g = NaturalTransformation { runNT :: forall a. f a -&gt; g a } class FFunctor x where ffmap :: (f ~&gt; g) -&gt; x f -&gt; x g In this category, there's an isomorphism between `Coyoneda _ (~&gt;) x f` and `x f` forall `FFunctor x`. With an appropriate chosen wrapper newtype On a f = On { runOn :: f a } There's an isomorphism between `CFTFinal f a` and `Coyoneda Traversable (~&gt;) (On a) f` when `Traversable f`. 
&gt; how does it contrast with the three layer haskell cake? With the Three Layer Haskell Cake, you don't actually divide your code into three pieces: you divide it into many independent Layer 3 pieces, each of which uses their own DSL, ideally a non-monadic DSL. So that part is quite similar. Layer 2 also isn't a single piece: it's a technique for writing monadic code in a way which makes it easy to instantiate the code either in a small monad stack, for testing purposes, or in a large monad stack, used when running your application. I think it's a very good technique. I'm not sure I completely understand Layer 1, but I _think_ that if I was translating it into my Tree of DSLs approach, it would say that immediately below the IO root, you should put an AppT node, and then you should put all your other DSLs below the AppT node, not the IO node? What confuses me is the name `AppT`; typically, you would only have a single `AppT` at the top-level of your application, but afterwards Matt talks about capabilities, and you would definitely have more than one capability in your application. &gt; what would be an example of a moderately complex application designed like this Sure, let's look at [my Ludum Dare 31 game, "I've seen this room before"](https://github.com/gelisam/ludum-dare-31). [The root node is IO](https://github.com/gelisam/ludum-dare-31/blob/master/src/Main.hs#L248), no choice there. Most of my code is in other DSLs, but I do use IO to load the sprites. I use a few features of the IO DSL: [Applicative syntax](https://github.com/gelisam/ludum-dare-31/blob/master/src/Sprites.hs#L35), which all Applicative DSLs have, in order to concisely express how to construct each of my sprites, and also [exception handling](https://github.com/gelisam/ludum-dare-31/blob/master/src/Sprites.hs#L46), which only IO has, in order to default to some debug graphics if for some reason the assets aren't found. I then use a run function, [`playBanana`](http://hackage.haskell.org/package/gloss-banana), to attach an FRP node (using the pre-1.x `reactive-banana` which didn't use a monadic API) to my root IO node. I use this DSL in [`mainBanana`](https://github.com/gelisam/ludum-dare-31/blob/master/src/Main.hs#L26) to describe the main logic of the game, including player movement, picking up items, changing the contents of the room when the player goes to the next level, triggering gameplay-pausing popups such as the title screen and the level transitions, and playing animations. That `mainBanana` function is a bit longer than wish; in a Ludum Dare event, we only have 48h (or 72h, depending on the category in which we are competing) to finish the game, so at the end I was frantically typing code inside that one function without taking the time to refactor it into smaller modules. So let's look at the parts I implemented towards the beginning of the event, while I was still taking the time to structure my code properly :) For many of those features ([keyboard input](https://github.com/gelisam/ludum-dare-31/blob/master/src/Input.hs) and [player movement details](https://github.com/gelisam/ludum-dare-31/blob/master/src/Controls.hs#L32)), the code is in a sub-module, but it's still using FRP, so nothing new there. For [the graphics](https://github.com/gelisam/ludum-dare-31/blob/master/src/Graphics.hs), I used [`gloss`](http://hackage.haskell.org/package/gloss)' `Picture` DSL. Elsewhere, I invented my own DSLs for game-specific concepts. My levels were small enough that I didn't have to load them from a file, instead I described all of the levels in Haskell, using a custom DSL. Since the theme of that particular Ludum Dare event was "entire game on one screen", I felt like it didn't make sense to have conventional levels with completely different contents in each one, as I would count those to be different screens. Instead, there is only one screen, whose initial contents is described using a very simple DSL: [a list of lists of tiles](https://github.com/gelisam/ludum-dare-31/blob/master/src/LevelData.hs#L70). I often use lists of strings for those, so I can draw my levels as ASCII art, but not this time. Then, each later levels intentionally differs from the previous one only by a few tiles, to make it look like new elements are added and removed to the same screen we started in. For this, I again used the simplest DSL which could possibly work: [a list of tile coordinates and the tile to put there](https://github.com/gelisam/ludum-dare-31/blob/master/src/LevelData.hs#L102) (plus the messages to display when we enter or leave that level). In retrospect, that DSL ended up being pretty painful to use, as I kept entering incorrect coordinates, playing the game up to that level in order to discover my mistake, tweaking the coordinate, and trying again. So if I was spending more time on this game, one of the first things I would do would be replace this DSL with a more conventional one where each level is a list of list of tiles, and then I would diff each consecutive levels in order to obtain the list of changes. I made a DSL for describing [animations](https://github.com/gelisam/ludum-dare-31/blob/master/src/Animation.hs). I made a DSL for expressing [gameplay-blocking animations](https://github.com/gelisam/ludum-dare-31/blob/master/src/InputBlocking.hs), during which the player is not allowed to move. All those DSLs allow me to express the corresponding parts of the game in a more succinct and composable way than I could without a dedicated DSL. And DSLs are so easy to create in Haskell! Just create a type, give it a few instances (e.g. `Monoid`), and voil√†, you benefit from combinators (e.g. `mconcat`, `foldMap`) which others have written for DSLs which satisfy this type class. And they each get embedded in a parent DSL, in this case my FRP layer.
Note the way NihilistDandy wrote that, either instinctively or otherwise. You can see the human mental metaphor for "binding" right there in the expression, where the multiplication "ate" the whitespace around it, and the 2/4 and 5/3 pairs are _literally_ bound more tightly together than the addition, which still has whitespace in the second expression. I mention this because if a student asks what that means, I would completely suggest consciously leaning into this metaphor. Explain that it is a metaphor. But it's a good, natural one that fits our brains.
Cool. If it's helpful, you can re-use [hindent's test suite](https://github.com/chrisdone/hindent/blob/master/TESTS.md), which should make writing a tool like this easier. Quite a lot of people have contributed scenarios. You'd just have to format them according to your formatter style.
Are there any function programming games out there? There are tons of programming games, but they are all imperative. What I wouldn't give for a functional Zachtronics-grade game...
&gt; You shouldn't trust anyone who claims that 10-year-olds won't struggle with these things. Child development experts go back and forth on the details, but in general, it should be understood that 10-year-olds _literally_ lack the brain structures to understand the more complicated highly abstract stuff in Haskell. The brain structures that are used by adults to understand that stuff are _physically_ underdeveloped at this point, and are years away from even really starting to develop into their final form and over a decade away from full maturity (which actually seems to happen somewhere around 25 or so). If they start moving in that direction, don't stop them of course, because they're maybe a real outlier, but for the vast, vast majority of ten-year olds, even a vast majority of _gifted_ ten year olds, trying to explain a function that returns functions is probably not going to go well at all.
The "cofree"-ness property of `CFT f` is that any natural transformation `H :: t ~&gt; f`, where `t` is traversable, "factors uniquely" through `fromCFT`: there exists a unique traversable homomorphism `I :: t ~&gt; CFT f` such that `fromCFT . I = H`. Here, this unique factorization is calculated by `unfoldCFT`: `I = unfoldCFT H`. This definition is a special case of a (co)[free object](https://en.wikipedia.org/wiki/Free_object). This may seem daunting if you're not familiar with category theory, but the notion of free object doesn't actually depend on a lot of concepts beyond [categories](https://en.wikipedia.org/wiki/Category_(mathematics\)) and [functors](https://en.wikipedia.org/wiki/Functor). If you know monoids and monoid homomorphisms, these concepts may already look familiar. Trying to see how the definition of "free object" connects to free constructions in the wild, such as the "cofree traversable" here, or "free monoid" (lists), or the ever popular "free monad", might be a good foray into category theory. There are two categories under consideration here, the category of traversable functors `Tra` (where morphisms are "traversable homomorphisms" as defined in the OP) and the category of (Haskell) functors `Fun` (where morphisms are natural transformations). (Exercise: Check that they are indeed categories.) There is a forgetful functor between the two `F : Tra -&gt; Fun`. In other words, traversable functors form a subcategory of functors, and `F` is the corresponding injection. (Exercise: Check that this is indeed a functor.) Then a cofree traversable functor is a cofree object with respect to `F`. (Exercise: Guess the definition of a cofree object (this might be nontrivial if you're going through this the first time, but this amounts to flipping the arrows in the diagram defining a [free object](https://en.wikipedia.org/wiki/Free_object#Definition)) and check that this matches the above definition of a cofree traversable.) IMO the main challenge in following this is juggling with the abstraction. Especially when things are nested like "functors between categories of functors". It's quite easy to be confused when you are still learning about the very concepts of categories and functors. If you somehow manage to sort things out, this exercise mostly boils down to expanding definitions and checking that laws hold.
Sorry if you missed this, but for teaching Haskell, there is http://code.world/Haskell instead. This is definitely Haskell, and I'm definitely interested in making it useful to the Haskell community. I use it to introduce Haskell to my coworkers, and several universities use it for functional programming classes. There's a more idiomatic version of the graphics API, too (just `import CodeWorld` and use the Guide button for docs). You can also just use the console (for output; no stdin yet), or even write Quick check tests.
Thank you. Fixed.
&gt; Fortunately, it's fairly easy to add missing packages to stack.yaml. This was also suggested to the author of the blogpost. Quoting his response to that: &gt; *"Using `extra-deps` is very unpleasant- it's like manually solving Cabal constraints and writing down the answer. I only do it for key libraries I can't do without - even I wouldn't do it for HLint."*
1. Nix for dependencies, usually through [`reflex-platform`](https://github.com/reflex-frp/reflex-platform). 2. Vim for editing. 3. [`obelisk`](https://github.com/obsidiansystems/obelisk) and for development and deployment, which includes `ghcid`, `NixOS`, and `GHCJS` depending on the project 4. Git for source control. 5. A mix of GitHub and GitLab for hosting.
&gt; There is Stackage Nightly which has the latest code. There is Stackage LTS which has older and therefore buggier code, up to 2-3 months older I tend to find old code and new (less tested) code to be similarly buggy. 2-3 months seems like a reasonable compromise to me; i.e. not that old or new. What bothers me more is that we don't actually have a concept of LTS. Once a new LTS comes out, the last one becomes essentially unsupported.
The breakages that occur in 0.0.1 level changes are generally invisible changes, not API level changes, since API level changes usually require an x.y increment. So sometimes an invisible change is quite breaking, but no one realized it soon enough, and it can be quite difficult to track down. To me this is misuse of the 0.0.1 version increment, and that version should be deprecated and reverted, or fixed and upgraded past on Stackage. There's also the issue of new identifiers being allowed in 0.0.1 increments. In theory this can be protected against with defensive imports that list all the identifiers you need, but in practice no one actually does this. Luckily this class of breakage is trivial enough to resolve that I'm not bother by it. Still, I wish defensive imports were easier. Almost makes me want to import most things `qualified`... almost :P
Nice! Is there a vim plugin yet? Or is the format compatible with things like Neoformat perhaps?
In practice we use defensive imports almost exclusively at my current job, and I think in lots of other industrial settings too! The habit isn't too hard to get into with a little discipline, I've found, even without tooling support. (And gosh, I'd really like tooling support that was minimally invasive, and managed my imports cleanly, and _nothing else_).
status is manually updated. http://auto-status.haskell.org isn't, but it doesn't track planet :-)
Just fixed it. Box got stuck, needed a reboot. In the future, please email admin@haskell.org about this stuff -- you can't rely on admins checking reddit :-) Not related to haskellnews.org at all -- that was chris done's personal site, and he relinquished maintenance on it some time ago. It would be nice to take it over by the admin team at some point, or otherwise provide the same service, but it is an unrelated issue.
While this is on here, does anyone know of alternatives to Gloss for drawing 2d graphics with haskell? Gloss uses OpenGL, and since macOS has deprecated it I'd like to switch the game to something that works on macOS as well as Linux. The good news is that the game doesn't need much from its graphics library: just the ability to draw lines, circles, filled in circles, filled in rectangles, and text. I really liked Gloss's [Picture](http://hackage.haskell.org/package/gloss-1.13.0.1/docs/Graphics-Gloss-Data-Picture.html#t:Picture) type and am hoping to find something similar. I also might want to make a 3d game at some point and so am looking for cross platform alternatives to [not-gloss](http://hackage.haskell.org/package/not-gloss) as well.
Thanks, at first glance that doesn't seem to be related to ghcjs specifically. What is your setup?
I mean for visual stuff just start them out with Gloss. Then they can make games super easily, snake is like 50 lines of code. 
If you've already read two books, I suggest doing a project instead of more reading. You can always look for resources on specific topics as you need them. You can also subscribe to this subreddit, and maybe come hang out on discord (see sidebar). In both of these communitites, you'll be exposed to concepts and techniques you haven't seen before.
Duet looks pretty cool, and definitely fills a lower-level reasoning void that I haven't focused on. I guess the closest thing CodeWorld has is the Inspect window (for example, start with https://code.world/#P6F8HJo-k4Uc__CkIs77jhQ and click the Inspect button on the lower right). But that only works for pictures, and is designed to focus on the *values* of expressions, and not on the evaluation order. That makes it significantly different thing from a substitution stepper.
Thanks! Should have thought of that :-)
* For source **repository management**, we (the whole development organization which includes Python, Ruby/Rails, and Haskell) uses GitHub, thus we (the team) uses `git`. * For **dependency management**, we (the team) uses Nix (shell and also use the generated package Nix lambda for building in CI and using the resulting artifact for deployment) * For **editing,** I use emacs but other teammates use Atom and vim. * For in-dev-mode fast **feedback**, I use `ghcid` and `haskell-ide-engine` (HIE) which using LSP to communicate with my emacs' `lsp-mode`. I currently use both because I am a creature of habit but I might drop `ghcid` shortly since I have been gradually learning how to utilize HIE in my development mode more. * For **searching** by type signature, we (the team) uses `hoogle` (via Nix using `ghcWithHoogle` in our Nix shell.nix [https://www.reddit.com/r/haskell/comments/51w1f6/run\_your\_own\_copy\_of\_hoogle\_locally/d7fv67y/](https://www.reddit.com/r/haskell/comments/51w1f6/run_your_own_copy_of_hoogle_locally/d7fv67y/)) * For general developer "***porcelain***" (e.g. opening a REPL, running a build, etc.) we use `cabal`'s `new-*` commands. * For **testing**, we (the team) uses a combination of `doctest` (doc-driven tests sitting with the code also acts as extra documentation of usage examples), `hedgehog` (property-based testing). We (the team) are actively considering `tasty` and `tasty-discover` for a test framework driver/runner. * For **CI,** we (the team) uses Hydra which is based on Nix. After getting used to it, it works surprisingly well for our purposes of building artifacts, running tests, and deploying in our CD setup. We (the development organization for this product, including the Ruby/Rails side of the house as well as the Haskell side, *us*) uses Hydra's GitHub PR integration so it picks up the currently open PRs and runs builds, tests, etc on any new commits and reports back to the GitHub interface. We are considering providing our own GitHub Checks API integration with Hydra that we would eventually commit upstream to the open source project when it finally all works. * For **CD**, we (the product org, including the Rails web apps team and the Haskell services team) use Hydra via scripts we wrote to deploy with a fail-safe strategy to AWS using ASGs, fully baked AMIs (so we can dynamically scale fast, which is a requirement for us), and the appropriate load balancer (e.g. ALB, NLB, ELB) for that component. * For building macOS binaries for internal dev tools (written in Haskell), we are considering using CircleCI (since we can't connect a macOS instead in AWS to our Hydra/Nix distributed build cluster at this moment in time). Most of the web applications team runs macOS. Linux statically linked binaries are already built in Hydra. &amp;#x200B;
It depends on the program. Often I don't uses stacks or typeclasses or anything. I just write as pure a core as possible, then have a small library of IO functions that interact with that core, and then some code to glue the IO functions together. I.e. I try to write it as the composition of libraries, including in the IO stuff.
- `neovim` integrated with `brittany` and `hlint` for editing - `stack` for builds and dependency managmement - `ghcid` and `stack test` for file watching/test running during development - `CloudFormation` and `docker` for deployment (`docker` is also useful locally) - `CircleCI` for CI/CD
Emacs (`haskell-mode` + `projectile`), `hasktags`, `cabal v2-*` (with `v2-freeze` for freezing dependencies), `ghcid`, Git/GitHub for source control, TeamCity for CI. We're also experimenting with Nix.
In `compiler/cmm/CmmParse.y` it says: There are two ways to write .cmm code: (1) High-level Cmm code delegates the stack handling to GHC, and never explicitly mentions Sp or registers. (2) Low-level Cmm manages the stack itself, and must know about calling conventions. Can I get GHC to to output high-level cmm when building my code? The output from `-ddump-cmm` and `-ddump-cmm-from-stg` both looks low-level. 
There's [The Incredible Proof Machine](http://incredible.pm/) which I find to be quite fun, though perhaps it needs more tutorial and more flavor text to be a full-fledged game.
I've been experimenting with the JS library for WebGL called Babylon. Yes...it's JS. But Haskell can target that and I've played around with writing some sort of binding. Granted, it would not be at all like Gloss probably.
Try hie-nix (there‚Äôs a spacemacs module) I really think lsp could be the future!
Really neat! Thanks for sharing.
Is there a way to easily remove old LTS versions and old GHC versions from my global stack project (~/.stack/)? I think each of these can be a few hundred MB right? At least the GHC versions, and that feels like it could add up after a while.
You're welcome :)
Part of the goal of Hermetic is to provide an easy starting point for people to fork and make their own modifications, and a Haskell-&gt;JS toolchain would probably be too heavyweight for that. That said, I'm curious about Babylon for its own sake. What makes you like it?
Alternatively you could have three sublanguages, each one capable of interpreting the other two, similar to how one can write HTML in JS in HTML in JS.
Well mainly that it's a bit higher level than WebGL and has features for physics, etc. However, this makes me wonder how hard it would be to port Gloss to WebGL since it's a subset of OpenGL ES 2.
I don't think the analogy works, because there isn't really computing in HTML. You wind up having a container that can hold both, but the computation is being driven by JS or the silent participant, the browser itself.
I‚Äôll definitely take a look at the tests that have been added since version 4.
I think the functional paradigm is the one that should be used to represent all explicit computations anyway. So interpreting the other two would be quite similar to interpreting HTML: the language cannot describe computations or side effects, but it can represent programs in another language that can.
I think this blog post may have gone too far in a few places.
Here's a version of `FunList` that doesn't require parametricity tricks, but it probably doesn't support `fromTraversal`... {-# LANGUAGE DeriveFunctor, DeriveFoldable, RankNTypes #-} newtype Fun f b = Fun (b -&gt; f b) data FunList f a = Done (forall b. f b) | More a (FunList (Fun f) a) deriving (Functor, Foldable) instance Traversable (FunList f) where traverse _ (Done z) = pure (Done z) traverse f (More a y) = More &lt;$&gt; f a &lt;*&gt; traverse f y 
I think porting Gloss to anything would be really easy! Check how little it does for you (circles, lines, not much else): http://hackage.haskell.org/package/gloss-1.13.0.1/docs/Graphics-Gloss-Data-Picture.html#t:Picture This is actually one of my favorite things about it, it's very low magic.
Teaching closures in a language lacking partial function application seems to add incidental complexity, don't you think?
&gt; It took em At this point ("them") I think it's useful to mention that Stackage is a community process. If I as a user want a certain package to go in faster, I usually simply solve all its problems that prevent it going in (e.g. by helping its dependents go in, or by pull-requesting whatever necessary changes into the package to make it compatible with newer dependencies or compilers.
Could you elaborate which cases have super-linear performance? Can one have guaranteed-linear performance if certain rules are avoided?
In a way, sure, but it's no worse than factory classes in Java. Partial function application is not really intuitive, which is really what I think you should be aiming for in a first language.
&gt; What bothers me more is that we don't actually have a concept of LTS. I assume you're referring to the issues that were pointed out in [this previous discussion about "Stackage LTS" being a case of false labeling](https://www.reddit.com/r/haskell/comments/8xq0ac/proposal_twice_as_frequent_lts_and_stable_timeline/e24wxsf/)?
I‚Äôm skeptical of these kinds of arguments because I remember being 10. I taught myself Visual Basic without the aid of a book. My friend got me a pirated copy of VB6 on CD. And my math and English and science grades were below average. This says more about the outstanding discoverability in VB6‚Äôs IDE than anything else. I didn‚Äôt have anything special, I had motivation and time. I think talking about brain structures pigeon holes kids and imposes limits on pedagogy. Plus, the whole ‚Äúomigod functions returning functions, this is madness!‚Äù is a reaction of someone with a specific background, i.e. someone used to functions not doing that. Just like how ‚Äúcurrying‚Äù even has a name. You can teach Haskell to fresh newbies without needing to ever even explain that because all functions taking one input is simple.
I love this!
&gt; We‚Äôve now written a couple of non-recursive functions to ‚Äúquery‚Äù Trie. But what was the point, again? What do we gain over writing explicit versions to query Trie? I think the article misses out on saying that by using recursion schemes we are explicitly choosing well-founded forms of recursion. A catamorphism over a finite structure is a finite program - it is guarantee to terminate (provided you give it a terminating algebra). This is powerful, as it lets us rule out a big class of bugs (accidental non-termination/non-productivity).
OS : Gentoo dev-haskell/ghc-8.4.4 from haskell overlay
VB6 isn't exactly a good example for *abstraction*. I was the most gifted 10-year old in my (very small) school, and 10-11 was the earliest I could get started on abstraction. (Though, I'd been programming TI-BASIC and MS QBasic since I was 5.) Heck, VB6 is not even a great example for *programming*. ;)
While I think that having more tools in your toolbox is certainly a good thing, I'm not convinced by this argument. How frequent is accidental non-termination/non-productivity a problem in practice? If the compiler were doing totality checking, then I guess using recursion schemes might be helpful in proving those properties to the compiler, but GHC doesn't do that.
&gt; but GHC doesn't do that. Well that's exactly *why* we'd want to use something like this! Just like how our data types enforce things by being correct by construction, it'd be nice if our algorithms had similar properties. &gt; How frequent is accidental non-termination/non-productivity a problem Does it matter? This is Haskell, after all - we're striving for practical correctness. I'd say using recursion schemes is a pragmatic way to write some algorithms safely without needing the full power of a totality checker.
Yea. I think this is one of the biggest reasons that LTS is sometimes seen as a bad choice compared to nightly. You *do* get stuck with buggy dependencies, because no one ever fixes old LTSes. NixOS has the same problem. NixOS is *a little* better because a stable channel continues to receive stability and security updates for about three months after the next release is stabilized, giving you at least some time to upgrade while receiving updates. But there's still no real long term support version of NixOS. It's just hard to fix this though. You probably have to at least triple the man hours that goes into a release process to get a Linux kernel or Ubuntu style LTS system. There's work for the standard releases, and probably about as much work for each of, let's say, 2 concurrent stable releases. It requires a lot of manual patch work, including compiler patching.
Whether or not VB6 holds up to your standards, it has abstraction. No, I wasn‚Äôt learning about profunctors as a 10 year old. 
is brick compatible with FRP frameworks?
Could you elaborate?
I don't think those are the abstractions that are concerning. HKTs and HOFs are the main concern, since Haskell makes quite a bit more use of them than many other languages.
Sorry, ‚ÄúI may have gone too far in a few places‚Äù is a George Lucas quote that is itself a prequel meme, and I was just poking fun. But perhaps it isn‚Äôt the right joke to make in this particular community. :)
Ah, ok. 
HLint can enforce consistent naming policies. It's really helpful on a moderate sized team. 
It's high risk because something changed. When something changes it might stop working. Knowing that it only changed 0.0.1 might (if the stars align) tell you it's the same API. For upgrades, static type errors are easy, but subtle runtime changes are a nightmare. Why have both LTS and Nightly? If you are going to upgrade to nightly, just upgrade. Having Nightly HEAD and an older Nightly for prod can be valuable, and then you have a way easier path to roll forward, since if big changes happen you just match the nightlies.
Do so. It's like a free CI run by great people who just want your package to be the best it can be. Can't recommend it enough. 
LTS sometimes has code released a few days ago. It sometimes has code that is 2-3 months old with known bugs in that are fixed in later versions it hasn't included. You seem to be assuming it's old code, well tested and selectively patched - which is a perfectly reasonable thing like the Linux stable kernels. It's not that, it just has a name that suggests it is. 
It seems like the logical conclusion to your post is to release an LTS every day üòõ
The real solution is to drop the libraries that aren't really maintained anymore, and only "they" can do that. If it's someone depending on my stuff I do usually just go around and patch everyone, but when I depend on a changing library the things that are broken and be numerous and well outside my understanding.
You're basing this on the fact that it has code released a few days ago? Can new code not also be well tested if it's in an environment like Stackage LTSes? Or is the testing in lts-haskell not nearly good enough for that?
Relatedly, reading (co)algebras and morphisms in someone else‚Äôs code en lieu of explicit recursion is a big boon to comprehension. Especially in the complex cases.
LTS is tested on users machines. There's no magic test that it works beyond compilation. Nightly is the same. I think new code totally can be well tested. But there has to be the feedback loop of test, find bug, fix. Nightly has that feedback loop. LTS has less feedback. 
I thought package test suites at least were run on LTSes. That's at least something
That also happens for Nightly.
I'd be pretty sceptical of the idea that understanding the relatively simple concept of HOFs would be require a more developed brain than that of someone who already grasps the numerous and rather complicated abstractions you need to understand to program at all.
&gt; Nothing in the language itself stops you from writing this kind of code: This is probably true for any language. Whether the logic is expressed on the type level, in a contract or simply in a value, there is nothing to prevent you from these kind of errors. But further abstraction and DSELs make them less likely.
the global `stack.yaml` is a good place to put packages you like to experiment with frequently that aren't on stackage.
&gt; I didn't know it. Let's assign all URL imports to the Main package too (or the "Internet" package) So if two packages anywhere in your full dependency tree have a package with the same name compilation fails? Currently it does not unless there is a genuine ambiguity between two different direct dependencies, which is pretty nice. &gt; Foo.Bar would be searched for in the same URL path, just like in the case of resources (images etc) in a HTML file. The URL paths and the module names should have no relation, as I will explain below. Relative paths? Absolute paths? Relative paths that take into account the module name to know how far to go up? &gt; caching should be transparent and an optimization, so it should work as if URL's are accessed without cache. Whit this in consideration, my intuition says that it fixes that kind of issues. That will be catastrophically slow for large projects that are based on this URL-based approach, incremental building relies very heavily on the fact that things can't just change out from under you. &gt; But in the case of URL for GHC, It is less a problem, since that kind of feature is more intended for development or research. It's rapid iteration in any case. As long as the central repositories explicitly forbid URL imports then this feature would instill a lot less fear in me, as it would not be targeted at large / production projects. &gt; Could you point me to your hash suggestion? I'm very keen to identify modules by hashes, but hashes can be expressed inside the URLs, while at the same time let every other people either use them or not. Going back to the catastrophic performance issue aspect, you need to actually use and enforce the hashes locally for things to work out well. But basically if you have a hash and a URL (or even just a hash and some sort of strategy for searching for it, like IPFS), then you can use the hash to enforce that the content doesn't get changed, and to safely avoid re-querying the URL, as its the hash that defines the content, the URL is just where to (hopefully) get it if you don't have it already. &gt; Finally, URL imports makes possible to import any module of any package in hackage or stackage or git etc without the need to install the package : I'd be surprised if you could really ignore the cabal file completely and just use a purely source based approach of importing all hackage and stackage packages. I mean how are you going to know which `Foo.Bar` module `baz-qux
[removed]
One option would be some hands-on experience with reflex-dom. Reflex-platform provides a convenient way to set everything up and there are examples and tutorials. I am not even sure how do you do functional reactive programming with brick for one, because that would require integration with an frp framework (such as reflex).
Install obelisk and go through https://reflex-frp.org/getstarted ? 
Have you tried Elm? Not for actual development, just to learn the FRP part. It's specifically designed to lower entry barrier for FRP and once you get the hang of it, learning things like Reflex should be easier.
I have been using Elm at work since late 2016, and have written over 10k LoC of Elm since that time, and reviewed many more. Our team has a total of around 50k LoC of Elm. I feel like I have no substantial understanding of whatever underlying FRP concepts Elm uses. Elm doesn't use the phrase "functional reactive programming" in the same way it doesn't use the terms "applicative" or "monad". You may be using these mechanisms all the time, but personally, it wasn't until I read/wrote Haskell that I connected the dots and started being able to recognize applicatives and monads by name, and understand how they relate to one another. I have yet to write any FRP in Haskell, which is why I am still murky. The point being, you may be using concepts that have been borne out of FRP when writing Elm, but the language/community/documentation is great at abstracting these away and making you not realize that you are doing so.
Elm actually deliberately departed from a standard FRP model to an events-only model a couple years ago. Removes the nice features of FRP IMO.
I can't even figure out `unfoldCFT` without first going through regular `FunList`, and be forced to use parametricity tricks. That said, if you do allow yourself to go through the regular old `FunList`, `fromTraversal` is easy. Here's a composite version: data CFTFunList' f a = Done' (forall b. f b) | More' a (CFTFunList' (EndoF f) a) deriving (Functor, Foldable) instance Traversable (CFTFunList' f) where traverse _ (Done' z) = pure (Done' z) traverse f (More' a y) = liftA2 More' (f a) (traverse f y) unfoldTraversalCFTFunList' :: forall f t a. (forall x. t x -&gt; f x) -&gt; (forall b. Traversal (t a) (t b) a b) -&gt; t a -&gt; CFTFunList' f a unfoldTraversalCFTFunList' nat tr ta = go nat $ tr fpure ta where _done :: FunList x y r -&gt; r _done ~(Done t) = t _more :: FunList x y r -&gt; FunList x y (y -&gt; r) _more ~(More _ r) = r go :: forall r g. (forall x. r x -&gt; g x) -&gt; (forall x. FunList a x (r x)) -&gt; CFTFunList' g a go n g@(Done _) = Done' (n (_done g)) go n g@(More a _) = More' a (go (\ndf -&gt; EndoF $ \x -&gt; n $ runEndoF ndf x) (endof (_more g))) Even if you somehow avoid using `FunList`, you'll probably still need to traverse the entire structure in order to apply the natural transformation, so there's no real performance benefit to avoid it.
What I found helpful was to structure a complex program using FRP, in particular by trying to leverage some of the basic datatypes used in an FRP library and understand how things are set up around them. For instance, one possible formulation of arrowised FRP involves [monadic Mealy machines](http://hackage.haskell.org/package/machines-0.6.4/docs/Data-Machine-MealyT.html). Loosely, you can think of a value of type `MealyT m a b` as a stream that consumes values of type `a` and produces values of type `b`, using side effects provided by `m`. You can look at the [dunai library](http://hackage.haskell.org/package/dunai-0.5.1/docs/Data-MonadicStreamFunction-Core.html) which uses such a datatype; it can be quite instructive to read the instance definitions for this datatype. For instance, designing a game, you could have the simulation which consumes player input and returns game state, rendering which consumes game state and runs in some rendering monad, input handling which consumes nothing and returns input data within the IO monad, etc. Such a program splits up into different parts like that, and what's nice about FRP is that it encourages structuring your programs like that, and provides tools for manipulating these components as first-class entities. With arrowised FRP, I found that the most unintuitive part was learning how to program using arrows, which I found to be a bit of an oddity of Haskell (and it does feel like that style fell by the wayside recently, despite its sound theoretical foundations). 
The global setup shouldn't be relied on for any project you plan to share with anyone else. It's not just a smell in that case, it's a technical complication. But for projects local to you and your machine, it shouldn't matter.
Broadening one's expertise surely helps further learning, but I fail to observe any similarities between Forth and Haskell. Also, Haskell is not at all that hard to learn.
could it be that the op‚Äôs improved understanding of haskell resulted from *translating* something they understood deeply to haskell?
The claim that Stackage ensures testsuites pass is a bit of an eyewash as many of the testsuites aren't even run! https://github.com/commercialhaskell/stackage/blob/c8f7dd48812289827ce7048e3a58d5f68b7e9868/build-constraints.yaml#L4082-L4340 
Oh ok. My understanding of FRP then is that it's a higher level pattern that can be done with multiple programming paradigms. I agree Elm simplifies it a bit too much, but I just suggested it as a quick start to get familiar with FRP concepts. I have also been reading on how to achieve FRP in proper FP patterns like monads etc., and I found papers such as [FRP from first principles](http://haskell.cs.yale.edu/wp-content/uploads/2011/02/frp-1st.pdf) among other advanced intros to FRP that break it down in a functional way. There is also Conal Elliot's [papers](http://conal.net/), where he has several great papers diving deep into FRP patterns with functional programming. I specifically think [this paper](http://conal.net/papers/type-class-morphisms/), regarding "denotational design", which not only breaks down FRP into its FP parts, but also gives you a way to approach FRP with design techniques. At the core of it is a function from Time to some type, and this type has FP interfaces it has to implement in a specific way, like a Functor and Applicative instances which create the features of FRP such as having an action reducer and events with handlers.
Just to share with you my project: Stack Machine Metaprogramming -- a "bigger picture", everything to do with stack machine, implemented in PHP, JavaScript, Python, C/C++ so far ... and more to come. https://github.com/udexon/SMMP I will write a longer response as you trigger some important issues .... ;-) Thank you very much.
Just to share with you my project: Stack Machine Metaprogramming -- a "bigger picture", everything to do with stack machine, implemented in PHP, JavaScript, Python, C/C++ so far ... and more to come. https://github.com/udexon/SMMP I will write a longer response as you trigger some important issues .... ;-) Thank you very much.
Holy shit! It's even worse than the half year delay I observed for servant... *eleven* LTS series still ship with a compromised version of `text`! &gt; [...] released text-1.2.3.0 already in 2017 it took till mid-March of 2018 before it ended up in LTS-11.0, which was the very first LTS to feature text-1.2.3.0. Long story short, everyone using LTS-0 through LTS-10 in mission critical settings would be well advised to set a version of at least text-1.2.3.0 to avoid this known silent miscalculation bug.
I was in very similar situation - teaching my 11y old relative. I have chosen to use elm as the first language though, mainly because of : * no advanced type features * better error messages * time traveling debugger * easy visualization Since motivation is the most important part, I would choose some simple game (snake, tetris...) as the main goal, but it can be anything practical. Finally, when teaching the language, you need to make sure your student can evaluate the program by hand, before asking them to write their own. Be prepared to go over complex concept (like argument substitution, recursion, type variables, ...) multiple times. They will probably forget much of it before your next lesson. End the lesson as soon as the kid seems tired and unfocused. 
It turns out that brick is not compatible with FRP, yet. I think reflex has good tutorials on FRP.
Recursive jokes are just On Point instead 
It differs from reference counting because there is only ever always one reference to a thing, and everything referenced has a reference back to the thing that references to it. Every pointer points to a pointer pointing back to it; everything knowns what "owns" it, and two things owning one another is where computation (graph reduction rules) are applied.
HOF is another term invented to put a perfectly ordinary kind of expression on a pedestal. It‚Äôs another example of background influencing what you think is special. HKT the same applies. 
I would strongly recommend using [Dhall](http://dhall-lang.org) as configuration language! This would, 1. Automatically give a *massively* better error messages 2. It would allow changing parts of the config without messing with that huge 'all-the-fields' json you have now. 3. You could then automatically pick up parts of the config from environment variables (which is always neat) 4. You could host the config online for your team and it would be automatically fetched (and everyone can still patch in their preferred changes, if necessary) To do this, you would basically need to replace your json instances with dhall equivalents.
Interesting... so how does "performing duplication lazily" fit into this? It seems like if you duplicate something it'd then need to have two references back to the two things which reference it, but clearly this can't be what happens since it doesn't fit your description (also it wouldn't scale particularly well).
I was wondering if it might be possible to use one of the Haskell shell scripting frameworks as a shell-replacement. How would you go about doing this? Would it be feasible? Has anyone tried it? My idea is a bit inspired by eshell, but I would imagine it using Haskell syntax rather than the patchwork that is \`sh\` and friends.
Any advice on getting -fllvm to cooperate with a nix-supplied set of llvm packages?
I think it should be safe to just unlink it. Stack should reinstall it if you later need it.
I still really like [`graphics-drawingcombinators`](http://hackage.haskell.org/package/graphics-drawingcombinators) but it uses OpenGL too, hasn't been updated in a while and does not handle input. You could do the rendering without OpenGL with [`diagrams-rasterific`](http://hackage.haskell.org/package/diagrams-rasterific-1.4.1.1/docs/Diagrams-Backend-Rasterific.html).
You have a duplication operation which has two outputs and one input. When its input "owns" a data constructor, it duplicates the data constructor, and traverses all the data constructor's children inserting new duplication operations connecting the two new data constructors with the old data constructor's children. [Like this](https://ncatlab.org/nlab/files/bimonoid-eq1.png) if the red one is the duplication operator and the blue one is, say, a cons pair.
Whoa. That's *a lot* more than I expected.
What a well-structured FFI. I'll be using this next time I need a nontrivial parser.
will unlinking delete it? The goal here is to clear up disk space. In which case maybe I am misunderstanding unlink.
Yeah delete it.
I don't know if you can make the claim that it's not *going* to be compatible; there have been a few efforts/discussions started to explore this question, and at least [one of them](https://github.com/jtdaugherty/brick/issues/215) is not concluded.
Your work is garbage. 
Are you Muslim? LOL
There's definitely a step up in abstraction needed to think of a function as a mathematical object (i.e., a "value") in the same sense as something like a number. That's a pretty big step, and it's one that math teachers struggle to help their students through for years. Research shows pretty clearly that it's still a critical factor limiting success in university-level calculus classes. There's a danger, when it comes to higher levels of abstraction, in thinking that things are self-evident. Our brain likes to rewrite the past, and pretend that what's obvious to us now must have been obvious then. Hence, we imagine we would have understood things much easier "if only"... if only we hadn't had some misconception, or something hadn't been explained in an imperfect way, or it had been taught from the beginning in a more general setting. It's important to understand that these are just tricks our brains are playing on us. This is the same fallacy as the ever-present "Monads are just..." tutorials. The answer is the same: understanding comes from persistently struggling with examples, until your brain has the right connections to condense your understanding into the abstract form. Once you have that general perspective, it's nice -- but it's also futile to try to just convey it in its purely abstract form to someone new. It just doesn't fit into the way their brain organizes ideas.
&gt; no one says "multiplication binds tighter than addition"; instead, teachers say "multiply first" That's a really good point. The entire concept is referred to as the "**order** of operations". Nowadays, I often visualize such expressions structurally, i.e. as a tree. I wouldn't expect that to be the approach of a 10 year old though.
I love the "aside" boxes! I think its amazing how practical languages can be without full Turing completeness :) Would it be possible to write hylomorphisms without recursion? My gut tells me it would need to translate Nu to Mu, which is not possible, is that correct?
MilkroTik, I would be willing to pair program with you on some FRP terminal stuff.
I think you find the division between function and value to be if not instinctual then absorbed from / instilled by the environment *very* early. Even before we have the words "noun" and "verb" we are already processing them with different parts of the brain. HOFs violate this separation and need and additional part of the brain to translate.
I'm sorry, you are just wrong, and there is a massive amount of evidence that these *are* special ideas worthy of a unique name that is scattered throughout the history of programming and that continues to this day (with most languages still not having support for HKTs, and some lacking HOFs, and both having *specific techniques* required as part of the compilation process). This is the closest I can come to a respectful reply. I believe it will be my last in the particular sub-thread.
unlink() is IIRC, the name of the POSIX "system call" that is used to delete files on UNIX/Linux. It will free up disk space if there are no more (hard) links to the files data or if the file system shrinks directories and removing that entry brings the directory under the low-water mark.
I thought https://blog.qfpl.io/posts/reflex/basics/introduction/ was a good intro to getting your feet wet with the concepts. But my suggestion would be to just start a project where you try it out, starting small (buttons and stuff affecting other things on the page) and building up from there. https://github.com/ElvishJerricco/reflex-project-skeleton and https://github.com/obsidiansystems/obelisk are both pretty good at providing an environment to test stuff out in. 
I have a problem where I want to map a common function over a tuple of different types, but where the function still applies to both. I saw that using lens "over each/both" provides a way to map over all elements of a tuple, but it seems they have to be the same, i.e. if I wanted to find the length of lists in a tuple I would try: ``` example = mapPair ([1 :: Int, 2, 3], ["a" :: T.Text, "b"]) mapPair :: ([a], [b]) -&gt; (Int, Int) mapPair = over each length ``` but it expects `a` and `b` to be the same. Alternatively, if my input is `([a], [a])`, the caller fails since it provides two types. I understand the general polymorphic issue I'm facing here, but not sure how to solve it (tried throwing in some different forall quantifiers but to no avail). Any suggestions?
Not too familiar with Makefiles, why does suffixes get listed twice? .SUFFIXES : .SUFFIXES : .o .hs .hi
&gt; First, due to G√∂del‚Äôs theorem, the language implementation itself can‚Äôt be formally verified, which means users must absolutely trust it. Now call me an idiot, but I'm pretty sure G√∂dels theorem doesn't say anything of the sort.
Call me an idiot then, but I'm pretty sure G√∂dels theorem implies a language can't verify its own consistency. Perhaps I'm confusing things?
Can this be used to export cpython extensions?
Another piece of amazing work. Thanks.
I'm very confused why you think G√∂dels theorem is even relevant here. We're not talking about trying to find mathematical formalisms here, we're talking about procedures. There is some relation, but the halting problem / rices theorem would be _more_ relevant here. That being said, all that means is that you can't actually write a theorem prover for any theorems where finding proofs involves the solving of undecidable problems and expect it to always give you a proof. There's absolutely nothing stopping you from applying such a theorem prover to prove theorems about the theorem prover itself. Or more concretely, you could write a model checker for C in C, and apply it to its own source code. I assure you the universe won't implode if you try.
To be honest, I was just copy-pasting an argument I read earlier today as in ("due to Godel's incompleteness theorem"), which I can't find it no matter what, but I really wish I do now. Anyway, yes, you're probably correct noticing that this is the actual problem I was thinking of.
I have spent quite some time on FRP during my PhD thesis. I did not find a lot of state-of-the-art work in Haskell / type theory on FRP. (Correct me if I'm wrong.) So there might not be much. A resource are the KSWorld and KSScript papers in the STEPs project: [http://www.vpri.org/writings.php](http://www.vpri.org/writings.php)
I really enjoyed spending some time learning Forth a couple of years ago. It's an interesting way of looking at things, and can lead to some succinct DSLs. However, I have to say that it's very different from Haskell. For starters, Forth is untyped. Or, more precisely, the types are part of the operations, not the data. This is like assembly language. For example, there is not "32-bit integer" type, just stacks of bytes. But there are different operations for adding 16-bit integers vs 32-bit integers. All the meaning is in the operation. It's the ultimate target for malicious attacks. Haskell, on the other hand, has a strong static type system, to verify that you can't do the sorts of things you can do in Forth.
Thanks! Hm, I'm not completely sure, but I think it shouldn't be possible. You could easily write non-terminating functions, like finding the sum of an infinite list.
No, I think etoys is better
It's not finished yet, but I reimplemented parts of rxjs on top of the machines library 
The analogy to assembly is pretty apt with Forth. It's possible to implement the entire Forth definition in a subset of Forth with about half a dozen basic primitives. [Processors which directly execute a binary representation of those primitives have been built](http://sametwice.com/forth_hardware), making that subset their assembly language.
The way this experiment works is to create an executable by linking C and Haskell together, but you can also have GHC build a shared object library: https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/shared_libs.html#shared-libraries-that-export-a-c-api Then you could link to the shared library from Python presumably. Not sure how to automate it so that you could update either the Python or the Haskell and quickly try the changes the way you can with the unified makefile approach.
The pursuit of minimal cores is a fine endeavor, but now I'm personally more interested in cores which are small as possible **while** supporting fast elaboration and decent inference and decent error messages and powerful inductive types. It would be nice if lambda-coded types delivered practically, but I'm skeptical, and I don't how they could support the features that I'd like to have (e.g. quotient induction). I'm also not a fan of desugaring into highly verbose core for reasons of performance and complexity debt in surface-level language. My impression is that one can make a much simpler and faster proof assistant by making the core modestly more complex in the right way. Unfortunately, I don't think that the future production-strength proof languages will be **simple** on an absolute scale, e.g. implemented in 1000 human lines, or that they could be made that simple even in principle. 
Œª-encodings are verbose, but a solid name preserving scheme helps a lot. If you have some free time, is there any chance you could try my implementation of CoC with self types? I could make a Haskell lib and executable for you if you want. It is honestly the language that feels the best of all my attempts so far. Usability-wise doesn't feel much different than Agda, it has scope inspection, clear error messages, you barely see the Œª-encodings on them because names are well preserved, it has inductive types etc. Do you suggest any material on quotient induction? I don't know what they are but would love to learn what is interesting about it. 
I would recommend teaching scheme :) It is super simple to grasp basics, but you can then introduce as much advanced features as kid can handle (monads and laziness if you want to) and illustrate a lot of programming styles (functional, imperative, OOP, etc)
\+ racket has super friendly IDE, which can be a big plus to a kid, unless he already uses vim or emacs =D some relevan reading: [http://emmanueltouzery.github.io/blog/posts/2016-10-13-teaching-racket.html](http://emmanueltouzery.github.io/blog/posts/2016-10-13-teaching-racket.html)
I think Racket will be clearly better than Scheme, once you've decided to teach a one-off novel syntax. Racket is designed for teaching, and focuses on better errors and pedagogically appropriate language design in a way that Scheme doesn't. It's very reasonable to start with something with a more ML-like syntax, though, because your kids need to learn algebraic expressions and equations no matter what else they do. I worked very hard to simplify Haskell until it can do the job, but the result is substantially changed (it involved replacing the Prelude, eliminating currying, type classes, and much of the polymorphism, using the RebindableSyntax extension to change parts of the language, patching the compiler, pre-parsing the code to find and complain about some code patterns, and post-processing the error messages to reword some of the worst ones.) That's CodeWorld. If you don't like all of those decisions, then just using Elm would get you 50% of the way there (and better performance in a web browser, too) and is a lot easier.
[removed]
I'm not convinced by the three-way classification. Are there exactly three paradigms? Why? If you have a good answer to that, it might help finding a "core calculus" mixing the three. There's some good examples in the thread to practical languages with a mix of features, but I'm not convinced every feature is either imperative, logical, or functional. If I had to argue for 3 paradigms, I might go with something like functions, linear functions (imperative), and relations (logic) as the core "objects" of the three paradigms. Any thoughts? 
It depends on what you mean. Closures are an implementation technique, and I definitely wouldn't teach them, because you shouldn't be teaching *implementation* of functional languages at that level. But static nested scopes are actually pretty intuitive. For example, in CodeWorld, you might write: f(t) = translated(horse, x, 0) where x = t * 5 That's a nested scope: the `t` is in scope inside the definition of `x`. That wouldn't change if `x` escaped the enclosing scope. But no 10-year-old is sitting there thinking: "Hmm, this only makes sense because `x` doesn't escape the enclosing scope." They would still expect it to work. (In fact, it sort of does, in this case, since under the scenes, the implementation of `translated` just constructs a data structure with `x` inside.) In that sense, I'd argue that the consequences of closures (distinct from the implementation technique) are not at all too difficult for a 10-year-old. The implementation itself is, and even explaining why such an implementation is needed is probably too complex to worry about.
Well, yes, I meant Racket as scheme implementation (I know, I know, racket is no longer a scheme!, but for the purpose of teaching a kid, it is pretty much a scheme :)) Look into any forum where they discuss how to choose a scheme implementation, and it boils down to: if you do not know what you want, choose racket. Anyway, I believe both lisp family and ML family are important, but I also believe, that ~~scheme~~ Racket would be better for a kid. Algebraic expressions and equations are in lisp all over, and if you really want to introduce infix notations... well, you can do that with some macroses. I just believe that ML-family has much steeper learning curve which can kill all motivation. At the same time, you can illustrate much more with lisp, without pain, without sacrifice. You can even imitate types with tags. And lisp experience would be more useful if kid would latter learn more conventional languages, where OOP and mutations are all over. &amp;#x200B; p.s. Do not get me wrong, I think what you did with CodeWorld is awesome. You are obviously very passionate about haskell and want to share it with your kid, but I think scheme path is simpler and it would be beneficial for kid to learn not only functional programming :)
Are there any theorem provers that aren't ultimately based on the CoC?
The majority of them? Isabelle, HOL4, HOL Light, ACL2, NuPRL, PVS. That's just the interactive ones. Tbere's also the automatic ones like E, SPASS, and Vampire.
You can do this with generics, here with one-liner: import Generics.OneLiner -- for some appropriately defined C... mapPair :: ([a], [b]) -&gt; (Int, Int) mapPair = gmap @C length Here `gmap @C` takes `length :: forall a. [a] -&gt; Int` as a polymorphic function. Looking closer, it actually expects a function of type: length :: forall s s'. C s s' =&gt; s -&gt; s' And the constraint `C s s'` expresses that `s` is of the form `[a]` for some `a`, and `s'` is equal to `Int`. This can be defined as follows: class (s ~ [UnList s], s' ~ Int) =&gt; C s s' instance (s ~ [UnList s], s' ~ Int) =&gt; C s s' type family UnList a :: * where UnList [a] = a 
Probably something to do with bottom values, which make a lot of the mathematics approximate rather than precise. In particular, `(,)` is approximately but not precisely a categorical product.
Great article! A small note - you sat that to prove induction you must first prove relfexivity, but I think you mean *reflection* right? E.g. that if you eliminate a nat with its constructors you get back the same nat.
My guess is that the difference is something unimportant, like the extra bottom value there would be in a tuple-based representation. 1. `undefined` 1. `MkFoo undefined undefined` 1. `MkFoo 3 undefined` 1. `MkFoo undefined even` 1. `MkFoo 3 even` vs 1. `undefined` 1. `MkFoo undefined` 1. `MkFoo (undefined, undefined)` 1. `MkFoo (3, undefined)` 1. `MkFoo (undefined, even)` 1. `MkFoo (3, even)`
Yes, thanks for the correction.
&gt; So there might not be much. That's the point. FRP is simple but useful.
What do you know about FRP on terminal? Which terminal GUI library would you use for FRP?
I meant it like there is not much interesting stuff going on and I dont know good implementations in Haskell or similar languages that offer a good solution to FRP
I think the performance benefits of laziness are secondary if you keep writing your code as if you were programming in a strict language. I personally find that the real benefits are revealed when you internalise laziness and start taking advantage of it in your design. Like, for instance, if there is something you'll look up in two Maps you know you can lazily mappend the Maps and then look up your item if that makes architectural sense. Or you build infinite lists of action sequences only to `take` a single one to perform. I can imagine how laziness can appear to be a burden if you don't take advantage of it in your design though.
You can implement 2D games in Rasterific with reasonable performance. Rasterific seems to be by far the best 2D graphics library in Haskell. You have to write a thin sdl2 wrapper to get moving games with Rasterific. There is a version of Rasterific that scales to 40 threads with linear performance improvements per thread. Rasterific spends 90% if it's runtime for sorting. If you need more performance, you simply need to bind rust code in Rasterific for extra performance. I have written a GUI toolkit in Rasterific that is still in development.
Suppose we're working in System F extended with existential types (so, a theory without bottoms). I'll use explicit type abstraction/application and pattern matching for pairs/existentials. Then we can calculate: T ‚âî ‚àÄ a. a ‚Üí (a ‚Üí Œ±) ‚Üí Œ≤ U ‚âî (‚àÉ a. (a, a ‚Üí Œ±)) ‚Üí Œ≤ f : T ‚Üí U f ‚âî Œª (g : T). Œª ((A ; (x , h)) : ‚àÉ a. (a, a ‚Üí Œ±)). g A x h f‚Åª¬π : U ‚Üí T f‚Åª¬π ‚âî Œª (g : U). Œõ A. Œª (x : A). Œª (h : A ‚Üí Œ±). g (A ; (x, h)) f (f‚Åª¬π g) =Œ≤ f (Œõ A. Œª (x : A). Œª (h : A ‚Üí Œ±). g (A ; (x, h))) =Œ≤ Œª ((A ; (x , h)) : ‚àÉ a. (a, a ‚Üí Œ±)). (Œõ A. Œª (x : A). Œª (h : A ‚Üí Œ±). g (A ; (x, h))) A x h =Œ≤ Œª ((A ; (x , h)) : ‚àÉ a. (a, a ‚Üí Œ±)). g (A ; (x, h)) =Œ∑ g f‚Åª¬π (f g) =Œ≤ f‚Åª¬π (Œª ((A ; (x , h)) : ‚àÉ a. (a, a ‚Üí Œ±)). g A x h) =Œ≤ Œõ A. Œª (x : A). Œª (h : A ‚Üí Œ±). (Œª ((A ; (x , h)) : ‚àÉ a. (a, a ‚Üí Œ±)). g A x h) (A ; (x, h)) =Œ≤ Œõ A. Œª (x : A). Œª (h : A ‚Üí Œ±). g A x h =Œ∑ g So, f is indeed an iso under Œ≤Œ∑-equality, but not under Œ≤-equality. Now, my understanding of systems with bottoms is hazy at best, but I guess they wouldn't include Œ∑ in their notion of equality. At least in Haskell, `f : A ‚Üí B` and `Œª (x : A). f x` should be considered different, since we should have `undefined ‚â† Œª x. undefined x` (right?). If I've used too much type theory jargon, please feel free to ask for clarification.
At this point I'm pretty sure you are familiar with a large part of Category theory but in particular the [encoding of set theory](https://arxiv.org/pdf/1212.6543.pdf) in it. Have you tried basing your proof assistant on some (perhaps \infty, 1) topoi?
&gt; the halting problem / rices theorem would be more relevant here. I want to comment that Godel's first incompleteness theorem is a consequence of Turing's theorem; [blog post that contains this result](https://www.scottaaronson.com/blog/?p=710)
You can't verify the consistency of the type theory of a language writing programs/proofs in this language itself (via Curry-Howard). Unless this language is inconsistent. This is due to Godel, yes.
&gt; At this point I'm pretty sure you are familiar with a large part of Category theory Why you think that? I don't know much about category theory.
Have you considered an extensional type theory rather than an intensional one? You could use the NuPRL approach of having types be Partial Equivalence Relations over terms, which, with a bit of massaging, could probably be well behaved for the abstract algorithm. Type-checking does become undecidable, but using tactics to perform correct-by-construction proof alleviates this problem to some extent .
I tend to agree. Logo, the prototypical language designed to teach programming to children (and the first language I learned!) is a scheme dialect. It uses recursion instead of iteration, and you have access to higher-order functions. The next language I became really comfortable with was in high school, where I learned to program my HP48 calculator using reverse polish lisp, which is a beautiful language that has higher-order functions as well as an explicit forth-like stack, and that was very intuitive as well. The key thing, imho, about these languages, was that they were developed around interaction with a repl, and included facilities for interactive visualization and exploration of the state and environment of the program. (Which reminds me, I learned some hypercard along the way too, but found it more difficult because despite many of its nice features, it was actually on the whole less interactive).
I would tend to think that the reason it limits success in calculus classes is that there is too _little_ abstraction, not too much. If calc were taught with a modern notion of function from the start, things would go much better. Instead, we're taught that functions correspond to graphs, and therefore the idea of a "function that acts on functions" seems impossibly remote, and classes try to work around this by avoiding modern abstractions that help unify and clarify the theory. I'd be interested if there was research that specifically showed that this sort of approach did not work.
Hol light hits most of the criteria here. Tom Hale's lecture on the flyspeck project that formalized his proof of the kepler conjecture really was very motivating to me on why he picked it: https://www.youtube.com/watch?v=wgfbt-X28XQ (https://github.com/flyspeck/flyspeck) Not only is the core small, but it is also externally verified.
Oh I thought because you wrote so much about the internals of Agda et al. If you are not, you suggest you look into it, it provides a very interesting foundation for mathematics.
I believe you are correct. Godels theorem does imply that any proof language can't prove its own consistency. Godels theorem works on a diagonalization argument similar to Russel's paradox, and for very similar reasons Type : Type is generally a source of inconsistency in proof languages. But I believe you can apply the same workarounds that people use for Type : Type here. It's possible that if you split the language into universes, then the subset of the Language that allows Type[1] and Type[0] can prove that the subset with only Type[0] is consistent. Similarly adding Type[2] may allow proving that the subset only up to Type[1] is consistent. It may even be possible to prove the induction principle, that transforms a proof in Language+Type[n] that Language+Type[n-1] is consistent, to a proof in Language+Type[n+1] that Language+Type[n] is consistent. This still wouldn't prove (in-language) that the entire language is consistent, but would be a strong enough metatheory argument to show that it is, and would certainly prove that any program that used only a fixed number of type levels couldn't exploit any inconsistency.
It implies that either a language is consistent or it is complete in the sense of being able to verify its own consistency. An inconsistent language just means that it is possible for a statement to be both true and false, which isn‚Äôt a problem if you‚Äôre a Dialetheist. 
I just tried: Straight up `chsh -s /usr/bin/ghci` doesn't work for some reason, but you can do `exec ghci` on the last line of your `.bashrc`. Then, opening a new terminal drops you directly into GHCi. Obviously, you'd have to add some options so that GHCi loads a sensible set of packages and a nice prelude etc. GHCi is actually an okayish shell. It has autocompletion for files and a basic history. Compared with real shells, however, the ergonomics would be lacking. For example, you'd want a more lightweight syntax for calling external commands, more intelligent autocompletion and syntax highlighting. I've also found the Haskell shell libraries themselves -- turtle and shelly -- a bit annoying to use, but your mileage may vary.
I just read the Wikipedia article on Dialetheism and found it both interesting and very strange. Are there any applications of dialetheist (?) logics?
One benefit I can think of is that `Type :: Type` makes sense so you don‚Äôt need a hierarchy of sets with each higher one containing the lower level ones and being a superset of them to rule out non-termination. The clear application of such a logic is if you philosophically agree with dialetheism than you of course have no choice but to use a dialetheic logic.
I want to analyze some realistic Haskell programs with `perf`, playing with some compiler options, etc. wrt how they affect performance of the compiled code. What would be some good open source candidate projects? In particular I'm looking for projects that don't do a lot of IO and for which I can set up a little benchmark that will take at least 100ms to complete, with consistent timing. But all ideas are appreciated!
`rasterific` looks good, but can you explain the advantages of `diagrams-rasterific`? I thought being fast enough for things like games wasn't a goal of `diagrams`.
That looks awesome! Will you let me know if you release your GUI toolkit. From what I can see my ideal library would be `rasterific`, with a layer on top providing a gloss-like Model / View / Update interface, perhaps using sdl2 as you suggest.
Hi, OP here. While this isn't the most complex topic, I found value in it and thought it was worth writing down for others to know about if they didn't already.
I can let you know \*when\* I release it, but it might take years. If have studied this topic intensively. Basically you need: a) vector graphics b) FRP (spreadship programming) \[simple / KISS style\] c) constraint solving d) (probably a few other things.) The most important writings are by [VPRI.org](https://VPRI.org), but it might take years to understand them, depending on your level. &amp;#x200B; Personally, I would worry a lot less about IO vs. pure (Id rather be in IO if you know what you are doing). And worry less about type safety, because you don't get far anyway. Prof. Setzer writes in his papers you need dependent types and coalgebras / coinduction for GUIs, for example. &amp;#x200B;
Everyone knows the only real programming paradigms that exist are: - Single instruction stream, single data stream - Single instruction stream, multiple data streams - Multiple instruction streams, single data stream - Single data stream, multiple instruction stream - Multiple data stream, multiple instruction streams Everything else can be compiled to machine code. Kind of joking but... From a high level point of view pure functional programming/graph reduction is really a single instruction stream, multiple data stream model just based around expression trees and not arrays/vectors of data as is usually the case. Pure constraint-logic programming is based around a single data stream, multiple instruction stream model with a tree of constraints. In the end all the data should unify to a single data stream or the program fails.
&gt; There's some good examples in the thread to practical languages with a mix of features, but I'm not convinced every feature is either imperative, logical, or functional. Assuming there really are only three paradigms, they're orthogonal in some sense. In that case virtually all existing language features use a mix of all three paradigms, and finding the unit feature for each paradigm is the task for the language designer. The assertion about these three paradigms being the only one is probably yet another useful lie, but it maps neatly onto different parts of decision making: functional programming is like knowing exactly how to do something, logic programming is like knowing what to look for and trying everything, and imperative programming is like knowing how to ask someone else to do it for you.
Sure. You can further factor out nontermination into explicit datatypes in the vein of: newtype Stream a = Stream a (a -&gt; a) Programmers for safety critical embedded systems do a similar thing. They do a simple main loop like: int main() { for (;;) { do_tick(); } } And all other loops are implemented in fixed size chunks. Potentially unbounded tasks must explicitly yield control sort of like this. static char buf[BUF_SIZE]; static size_t to_write; static size_t pos; // a linker script maps this extern volatile char write_out; _Static_assert(JOB_CHUNK &lt; BUF_SIZE); void my_task() { for (size_t ii = 0; ii &lt; JOB_CHUNK; ++ii) { if (pos &gt;= to_write) { break; } if (ii &gt;= BUF_SIZE) { break; } write_out = buf[pos]; ++pos; } } _Bool post_job(size_t size, char const data[static size]) { if (pos &lt; to_write) { // couldn't post a job, task in progress return false; } memcpy(buf, data, size); to_write = size; pos = 0; return true; } Not that such systems have C99 or C11 support. It's kind of odd but I think it's pretty neat. Note that because every task is nonterminating and works in fixed chunks a scheduler in do_tick can be extremely simply along the lines of something like: static unsigned tick_count; void do_tick() { for (int ii = 0; ii &lt; 2; ++ii) { important_task_1(); important_task_2(); important_task_3(); } less_important_task(); } I don't work in such an area though so I may have gotten a few things wrong but they do use the same general idea of pure functional stream processing to avoid dealing with nontermination.
But that's not what the claim said, at all! &gt; First, due to G√∂del‚Äôs theorem, the language implementation itself can‚Äôt be formally verified, which means users must absolutely trust it. You can of course formally verify an implementation of a theorem prover! People do it all the time, and it is very important. What you can't do is prove that the axiom set is itself consistent, if you're using an axiom set of equivalent power. See for example, "the formal verification of hol in hol": https://www.cl.cam.ac.uk/~jrh13/papers/holhol.html Note that they just extend it by a large cardinal in the metatheory of the verifying hol.
&gt; I would strongly recommend using &gt; Dhall &gt; as configuration language! Or just Haskell, like XMonad does.
ÊàëÊ≤íÊúâ‰ºëÊÅØÊó•
I'm not sure how to m gocccccc you f&gt;tgayrloffggcc eof a snowman
Oh I get it now. Very interesting!
https://github.com/qfpl/reflex-workshop/ is a great introduction to the primitives (using reflex) and forces you to work through a bunch of examples to learn how the primitives feel "in the hand" rather than as abstract constructs. Also make sure you grok Functors, Applicatives and Monads, because you will need to use those notions extensively to manipulate values "inside" `Event`s and `Behavior`s.
I am so stoked for this.
Good Bot
Sure. Haskell would be a lot better than JSON, but it is more complicated to set up. I think Haskell as configuration would pay of if the configuration contains a lot of logic. On the other hand, if the configuration is a big bunch of records, Dhall would be much more ergonomic in my opinion. &amp;#x200B; &amp;#x200B;
I don't care.
I'll check out the self types. With the quotient inductive types the main complication is that one needs an ambient system where transports/coercions can compute, e.g. if `p : List A = List B` then `transport List p []` computes definitionally to `[]`. We need this because otherwise transports get stuck on quotient equality constructors and it doesn't hold that closed `Nat` expressions normalize to numerals.
Using `forM_` is very clever. It's probably your best choice if your file paths are already in a list. I personally prefer [Hakyll's pattern composition operators](https://jaspervdj.be/hakyll/reference/Hakyll-Core-Identifier-Pattern.html#g:3). Here's how I copy all static files in my blog: ``` haskell match ("images/*" .||. "fonts/*" .||. "js/*") $ do route idRoute compile copyFileCompiler ```
Your `fillHand` should just take a `Hand` rather than an `IO Hand`. `Hand` means "I have a hand". `IO Hand` means "I have a procedure that will give me a hand if I execute it". Also, since you write `let hand = fillHand createHand` and `fillHand` returns an `IO Hand`, this means afterwards `hand` isn't actually a hand of cards, it's a procedure that gives you hand of cards if you execute it. You probably meant `hand &lt;- fillHand createHand`.
&gt; I'm pretty sure G√∂dels theorem implies a language can't verify its own consistency. Only if your language is powerful enough to perform addition and multiplication in its type language.
Yes, in `printHand`, you run the *action* `h`, which will call all the various IO code that generates the hand. Also, this is a very imperative way to approach it. How about something like this: randomToss :: IO Toss randomToss = &lt;fill this out yourself&gt; createFilledHand :: IO Hand createFilledHand = makeTosses 8 where makeTosses 0 = return [] makeTosses n = do t &lt;- randomToss ts &lt;- makeTosses (n - 1) return (t:ts) printHand :: Hand -&gt; IO () printHand h = putStrLn ("worp: " ++ show h) turn :: IO () turn = do hand &lt;- createFilledHand printHand hand printHand hand -- should be the same hand ...
If I am understanding correctly, then I think what you need to do to accomplish your goal is write a function that is *not* the main entrypoint of your application, add the calls that generate the elm code there, and then in ghci or using a make target, run that code. 
So, you mean the make target should compile the code, then it should execute the binary and make something like ‚Äúcurl locslhost:123‚Äù - right?
It's possible - you can run arbitrary IO with template haskell - but I'm not sure it's necessarily the best way to go about it. Seems to me the more sensible approach would be to have a separate executable to generate the code, as suggested by another user.
I'm not sure I understand though, what is `transport`? Is it just `subst`? If so, this is defined as: ``` def subs [-A : Type] [-a : A] [-b : A] [e : (Eq -A a b)] (~e -[b : A] [self : (Eq -A a b)] {-P : {x : A} Type} {x : (P a)} (P b) [-P : {x : A} Type] [x : (P a)] x) ``` Which normalizes to `[e] (e [x] x)`, so, when you apply `refl` to it, it evaluates to `x` (which would be `[]` on your case). But I guess that's not what you're talking about?
Sorry, let me try to be more clear. I view the elm code generation as something separate from the application. Because Servant's "APIs are types" are so handy, we can generate code from them. With that in mind, it can get a bit confusing regarding when you generate that code. I would solve this by adding another make target, called "make elm-code". That would compile my servant app, and invoke an entrypoint that generated the elm code and that's it, nothing more. After that target is run, you'll have your elm code that you can copy/edit/run/whatever. To start the servant app, I'd have a make target, named "make webapp" that would build the app, and start it like a normal servant app. It would *not* do any elm code generation. Is that more clear?
The thing with IO Hand is that it's created based on randomRIO so I could not find a way to make it Hand instead of IO Hand. It would make more sense if I posted the whole code. Let me try hand &lt;- instead of let hand = , thanks!
&gt; The thing with IO Hand is that it's created based on randomRIO so I could not find a way to make it Hand instead of IO Hand. It would make more sense if I posted the whole code. Yes, you need to run it in IO _once_ to get the random result. But after that, you don't need the IO bits anymore!
Thanks, that helped a lot! I forgot that for actual VALUES you need to do the `&lt;-` to remove the IO, otherwise you're working with procedures and not values.
Because by writing hand &lt;- ioHand I can then use the non-IO version?
It's just `subst` in Agda. In Agda `subst List p []` only ever reduces if `p` is `refl`, but that's not enough. In cubical type theory this expression also reduces if `p` is built out from an equality between list element types, because the empty list contains no elements, so there is nothing to coerce inside the list anyway.
Hmm, interesting, but what would be a non-refl equality? I mean, refl is the only constructor of the type, so how?
But how can you have a non-refl proof of an equality? It is the only constructor of the type.
Do you have a PDF without the watermark? For some reason today, I'm finding it distracting.
That trac ticket was opened 10 years ago. Perhaps try seeing if they are no longer as slow?
In a function body, you can have any number of non-refl bound variables with equality types. Or, with quotient inductive types, you can have equality constructors which are also not refl.
How much faster is Hadrian than the old/current build system?
I think I'm getting the point. So, does `subst` simply and immediately rewrites to `x` no matter what `e` (the equality proof) is, even if it is just a bound variable, or are there situations where `subst` doesn't rewrite? If the former, then am I right in thinking that it is a matter of making `subst` evaluate to `Œªx. x` instead of `Œªe. e (Œªx. x)`?
I wish I could, just didn't find a reference that I found accessible and engaging. I don't have a mathematics background.
We finally got around to submitting the Hasktorch project proposal! [https://github.com/haskell-org/summer-of-haskell/pull/95/files](https://github.com/haskell-org/summer-of-haskell/pull/95/files) &amp;#x200B; If any students are interested in advancing functional machine learning, do get in touch.
If we're talking about `subst List p []` in particular, then it always reduces in cubical TT and in observational TT as well. In both of these systems, `subst` is not a primitive operation, but it is defined in terms of primitives in a way such that this particular example reduces. In general, `subst P p x` may or may not reduce.
Will this generate your Elm functions? http://hackage.haskell.org/package/servant-elm
FRP is not a gui programming specific concept but rather about events and values at some point in time, which gui programming depends on a lot so they are often coupled together. So it might help if you use just `reactive-banana` FRP library and plug in events from a separate gui library to get a raw FRP experience.
I could probably do the work myself to figure this out, but I'm wondering exactly how flexible this is. Here's my use case. In [CodeWorld](http://github.com/google/codeworld), there are two dialects of Haskell implemented. One is just straightforward Haskell. The other, though, is more like standard math notation, but shoehorned into a Haskell compiler. Because of this (but only in the second dialect) I want the formatting to omit the space between a function and a parenthesized argument. So, for example, I want `f (5)` to be reformatted to `f(5)`, also `f (x, y)` to become `f(x, y)`. Today, I have integrated hindent into the editor for the standard Haskell mode, but for the math-like mode, because I don't have a formatter that works, and adding a bunch of spaces would be worse than useless. Obviously, this isn't a reasonable thing to ask for a formatter designed for standard Haskell, but since you are advertising flexibility... maybe?
Hmm it seems to me that FRP is the low-level concept and the Elm architecture is the higher level-concept rather than the other way round from the fact that there is a lot more lingo and fine-grained control with a raw FRP programming experience with plugging in events from other libraries, etc
If we use a very generic FRP library like `reactive-banana` it's always possible to plug it into a brick app. We could say let bricks handle the GUI mainly and let `reactive-banana` handle lower-level events, say if you are doing a socket programming and have events coming from other sources other than GUI, you let the FRP library handle those events, construct an event network, and react to update brick's model. I have never used brick before but it might even be possible to register brick's gui events and convert them to reactive-banana's events so that you get all the FRP power it provides - though it seems to be a very roundabout way to build an application and it's probably an overkill. For learning purposes though, I don't think it's a bad idea.
That's nice, as well! I'll play around with this and see about including it in the article. I'd like to cite you if I use this; how would you like to be cited?
Awesome! I‚Äôm filing this one away for sure :~)
 let printTransactionInfo TransferTransaction{} = print (refNbr t) Note this syntax is available even when the constructor `TransferTransaction` is not declared as a record. This is also possible: let printTransactionInfo TransferTransaction{refNbr = r} = print r or, with `{-# LANGUAGE NamedFieldPuns #-}` let printTransactionInfo TransferTransaction{refNbr} = print refNbr or with `{-# LANGUAGE RecordWildCards #-}` let printTransactionInfo TransferTransaction{..} = print refNbr 
As promised, [here is the PR](https://github.com/ekmett/recursion-schemes/pull/63). Or rather, as it turns out, the first of many small PRs :)
I pretty much did all those things too, or their temporal equivalents. Nevertheless, there's not a chance in heck I would have been able to become fluent with HOFs or the deeper machinations of monads or any of the many other concepts I routinely play with today, because the brain just isn't there yet. I would also point out that my statements here are not just based on my own observation, but on science. The abstraction simply isn't there for the vast, vast majority of 10-year-olds. You can test this in a lab environment. You can also see people making the same sorts of errors as they keep trying to "fix" the math curricula. Over and over again people try to push advanced math concepts back earlier and earlier into the curriculum, or at least try to somehow "prepare" the students for it, and in general, it isn't producing improvements in mathematical accomplishment. The 1970s "New Math" that is to this day still mocked (Common Core's criticisms are nothing by comparison) was basically an attempt to teach children how to do mathematics by starting with set theory at the beginning, because, well, that's the simplest foundation of mathematics, right? It went poorly, to say the least. There are some things that children simply can't understand. You may get them to mechanically perform some simple aspects of the task, but they simply will not be able to understand the deeper bits. (This raises the interesting question of what possible other advanced things there may be that are really easy to some aliens or perhaps our far-future descendents, but for which _we_ lack the brain structures to understand them, no matter what clever training someone could try to come up with.)
Nice! How's their mobile editor? I currently use [Dcoder](https://code.dcoder.tech/) for writing Haskell on my phone, but they're still using GHC 7.6 :(
Could you explain what self-types are? I've run across the term before, but never seen it explained.
Thank you! Now that I know it I see I just missed it in the documentation somehow. And to answer the next question I was going to ask, this syntax can be used in a case expression as well: withTransaction t = print (amount t) &gt;&gt; case t of TransferTransaction {} -&gt; print (refNbr t) CardTransaction { cardNbr = cardNbr } -&gt; print cardNbr MICRTransaction { seq' = s } -&gt; print s
The problem is with any recursive syntax element, most notably expressions. The formatting rules often allow for multiple layouts at each level and the formatter performs a weighted search over the tree of all possible layouts until the best one is determined. As many of these layouts are considered to be of the same or very similar ‚Äúquality‚Äù, the formatter May have to explore a great number of different choices before it can guarantee to have found the best. You can guarantee linear performance by using only the ‚Äúvertical‚Äù layout option and not using the ‚Äúalign-or-indent-by n‚Äù indent option.
Thanks for the recommendation. I‚Äôll have another look at Dhall. 
I see, yeah that sounds right, I guess I was being a bit loose with terms. I meant higher level from a mathematical perspective, since more abstract is higher level because it means you have hidden more of the "non-essentials" and you end up with something more simple but more powerful. 
At the moment, this is not possible. Floskell will always put a space or line break between the function and its argument. I have thought about adding syntax-specific layout options, eg a layout that is applicable only to function application, but there is not even a framework for this in place, yet. 
Thanks, that really clarified a lot of things for me!
The idea of "pushing state to children" and "collecting states from children" can be generalized as "commuting functors". For example, the pair of functions (the arguments of `thing`) f :: s -&gt; a -&gt; b, g :: s -&gt; a -&gt; (s, s, b) can be instead represented as a single function (although this is not an isomorphism, the resulting type is a bit bigger): fg :: forall x. (s, TreeF a x) -&gt; TreeF b (s, x) fg (s, LeafF a) = LeafF (f s a) fg (s, BranchF a x1 x2) = BranchF b (s1, x1) (s2, x2) where (s1, s2, b) = g s a -- and where data TreeF a x = LeafF a | BranchF a x x -- so that Tree a is isomorphic to Fix (TreeF a) Abstractly, that function type has the shape `forall x. m (n x) -&gt; n' (m x)`, where `n` is `TreeF a`, the base functor of `Tree a`, `n'` is `TreeF b`, and `m` is the tuple functor `(,) s`. Something similar happens for `cothing`, except that we need to partially apply `f` to the initial state `s0` that goes to the leaves (its the only use of `s0` anyway). f' :: a -&gt; (b, s) -- f' = f s0 g :: a -&gt; s -&gt; s -&gt; (b, s) can be represented as: fg :: forall x. TreeF a (s, x) -&gt; (s, TreeF b x) fg (LeafF a) = (s, LeafF b) where (b, s) = f' a fg (BranchF a (s1, x1) (s2, x2) = (s, BranchF b x1 x2) where (b, s) = g a s1 s2 Again, this function has the shape `forall x. n (m x) -&gt; m (n' x)`, where `n = TreeF a`, `n' = TreeF b` and `m = (,) s`. Putting `f` and `g` into such nice forms allows us to express `thing` and `cothing` as very simple and general recursion schemes, both one liners: -- The essence of cothing transverse :: (Recursive s, Corecursive t, Functor f) =&gt; (forall a. Base s (f a) -&gt; f (Base t a)) -&gt; s -&gt; f t transverse n = cata (fmap embed . n) -- The essence of thing cotransverse :: (Recursive t, Corecursive s, Functor f) =&gt; (forall a. f (Base t a) -&gt; Base s (f a)) -&gt; f t -&gt; s cotransverse n = ana (n . fmap project) `transverse` is in the `recursion-schemes` library since very recently, but `cotransverse` is not (yet). We have: thing t s f g = cotransverse fg (s, t) -- where fg is defined using f and g as above (the first definition) cothing t s f g = swap (transverse fg t) -- where fg is defined using s, f and g as above (the second definition) where swap (x, y) = (y, x) There is a bit of noise in wrapping `f` and `g` into `fg`, but the idea is to use `transverse` and `cotransverse` directly instead of `thing` and `cothing`, passing them the specialized versions of `fg` instead of `f` and `g`. Although `fg` has a bigger type than the pair `(f, g)`, one could argue that the explicit pattern-matching in `fg` helps readability, compared to having each branch `f` and `g` as a separate parameter (if you look only at the type of `thing`, it may take a bit of squinting to tell what the various parameters correspond to). The code in a Gist: https://gist.github.com/Lysxia/9fe786329519c1f770b298aa6e5f7577 You can also implement `thing` as a `transverse`, using `forall x. n (s -&gt; x) -&gt; (s -&gt; n' x)` as the argument of `transverse`.
&gt; Are declarative and imperative approaches categorical duals? See [previous discussion](https://www.reddit.com/r/haskell/comments/a14wdl/categorical_dual_of_functional_programming/)
&gt; nul argument markers (probably not the right term; the underscores) It's called a "wildcard pattern". &gt; Is there a way to define printTransactionInfo below without needing the [...] underscores? Yes, you can use curly braces: withTransaction t = let printTransactionInfo (TransferTransaction {}) = print (refNbr t) printTransactionInfo (CardTransaction {}) = print (cardNbr t) printTransactionInfo (MICRTransaction {}) = print (seq' t) in print (amount t) &gt;&gt; printTransactionInfo t
Wow, thanks. I'll read it carefully and take a look on recursion-schemes!
Some GUI libraries are not compatible with FRP without significant changes to API.
reflex-workshop looks useful.
Wow, that's some wild code you have there :) I didn't realize that TypeInType made it possible to use `k` both as the kind of `a` and as the type of `doAmbig`; I previously thought TypeInType was mostly so we didn't have to bother with the difference between kinds, sorts, etc. &gt; or whether `AllowAmbiguousTypes` is just letting me play as long as possible. I do think `AllowAmbiguousTypes` is the culprit, since once I turn it off and add the required `Proxy` arguments, I can give `wrap'` the type which GHC infers for it. Which, interestingly, is now polymorphic over `k` instead of being specialized to `*`! import Data.Proxy class Ambig' (a::k) where doAmbig' :: Proxy a -&gt; k newtype WrapAmbig' k r = WrapAmbig' (forall (a::k). Ambig' a =&gt; Proxy a -&gt; r) wrap' :: (forall (a :: k). Ambig' a =&gt; Proxy a -&gt; r) -&gt; WrapAmbig' k r wrap' = WrapAmbig' data Test = Test deriving Show instance Ambig' ('Test :: Test) where doAmbig' _ = Test doShow' :: forall (a::Test). Ambig' a =&gt; Proxy a -&gt; String doShow' = show . doAmbig' @Test @a wrappedDoShow :: WrapAmbig' Test String wrappedDoShow = WrapAmbig' doShow' Here is what I think is going on. `doAmbig`'s type is `forall k (a :: k). Ambig a =&gt; k`, and `AllowAmbiguousTypes` correctly warns we can't infer which `a` to use from the type of `k`. We can usually resolve these ambiguities at the call site using `TypeApplications`, but maybe not always! In particular, when instantiating `WrapAmbig`, specifying `k` or `r` won't help GHC unify `(forall (a :: k). Ambig a =&gt; r)` with `(forall (a0 :: k). Ambig a0 =&gt; r)`. GHC doesn't unify that kind of types: foo :: Show a =&gt; String foo = undefined -- Could not deduce Show a0 from the context Show a bar :: Show a =&gt; String bar = foo &gt; but why is this working...isn't `Wrap2 Test (Wrap1 String)` the same as `WrapAmbig Test String`? Unlike `WrapAmbig :: (forall (a::k). Ambig a =&gt; r) -&gt; WrapAmbig k r`, neither `Wrap1 :: (Ambig a =&gt; String) -&gt; Wrap1 b a` nor `Wrap2 :: (forall (a::k). (b a)) -&gt; Wrap2 k b` has an ambiguous type. In general, `newtype`s are a good way to get those `forall`s under control; I wouldn't have guessed that they'd do the job here, but I'm not surprised to see that they do.
Okay, thanks. I figured it was a long shot. Is this something you would support as a pull request? I'd have to poke around and see what's involved first, but if it's something you want, I may be able to find time to play with some designs and do the implementation.
The computer architecture is imperative by default, so I don't think its a good idea to convince them using low-level examples. You may find a problem, such as quick sort, and compare the implementation of Haskell and java. You may also read about the benefits of FP, I believe if you're convinced, you'll convince them too. 
You might be interested in Lean: https://leanprover.github.io/
People will adopt approaches that significantly improve outcomes, whether in the form of reducing 'pain points', or through providing solutions to problems that are otherwise relatively difficult to solve. What sort of 'pain points' or 'difficult problems' have they mentioned to you which you feel FP approaches might be able to address?
I don't care what your friends think but this might be mutually interesting to all of you: http://www.veritygos.org
&gt; quick sort Oooh boy, that's not a good example to showcase Haskell...
Why? Haskell has the most elegant implementation of quicksort I've ever seen?
Does that quicksort implementation have the right asymptotic complexity?
 quicksort :: (Ord a) =&gt; [a] -&gt; [a] quicksort [] = [] quicksort (x:xs) = left ++ x:right where left = filter (&lt;=x) xs right = filter (&gt;x) xs ^^^ This is pretty good looking to most programmers, although I do concede the native code it produces is probably pretty ugly 
Is this shake based? Or a different build system? Can it be used other than building GHC?
Yes, no and I think no.
While that looks great, it has poor performance because it makes a full copy of the list at each iteration. A true version would use mutable vectors, and would look really ugly.
The Hackage documentation for Data.Vector.modify states ```Apply a destructive operation to a vector. The operation will be performed in place if it is safe to do so and will modify a copy of the vector otherwise.``` How does this work? How is Data.Vector able to provide the above guarantee? 
I see. Thank you.
What's your main thesis and how are you supporting said thesis to convince your friends? 
&gt; I fail to observe any similarities between Forth and Haskell Both allow and often make use of tacit programming.
I‚Äôm joining alantrying‚Äôs skepticism, also due to personal experience. However, perhaps you‚Äôre also right if the claim is about a ‚Äútypical‚Äù 10 year old. Hell, some folks don‚Äôt even develop these brain structure when they‚Äôre old enough to be president..
Yes, it has the right asymptotic complexity. It has higher constant factors, yes. The partition steps of quicksort are always linear in the length of the input. So the version that copies the lists runs within about 2x the space, and some multiple of the time.
First of all, I think you should stop convincing your friends that functional programming is "the future". Even if that's your goal, putting it that way just puts people on the defensive; it's akin to claiming that your knowledge is more valuable than theirs, and you know better than them. It's much better to approach the conversation from a different point of view: you have learned something cool that you'd like to share with them! They don't need to be convinced of its superiority to their current beliefs, just that it could be enlightening to learn more about it. Still, you would need reasons that there are cool ideas here. Some of them are: 1. Functional programming gives you new ways of decomposing programs, for example via higher-order functions, and by using laziness to decouple producers and consumers. Some of these, like partial application, are just part of the fabric of Haskell programming, and you can't help but start to internalize them in ways you wouldn't elsewhere. 2. Haskell gives you the ability to name and abstract over instances of new design patterns, such as various kinds of functors (applicative, traversable, monads, etc.), lenses, and recursion schemes. 3. Haskell's type system is a playground in which smart people have build some amazing things: types that describe web APIs, types that reify control flow patterns, etc. 4. Haskell, more than any other widely used language, encourages a declarative and denotational way of thinking about the meaning of code, using equational and compositional reasoning rather than tracing execution to think about what code means. Related to this, it helps you explore how much is possible in a fundamentally simple model of computation. Altogether, these things come together to say: you can look at programming in a way that is not accessible to most people when using the tools and languages that conventional software engineering offers. It's a new point of view, from which different things are easy or difficult. Whether or not it's the better one, having a new vantage point is rarely a bad thing. If that doesn't interest your friends, then maybe try having different friends. ;) Or, rather, keep your friends, but maybe find some new ones, too, to share this passion!
"Quicksort" means different things to different people. Originally (and to most, I would venture), Quicksort is efficient because of its nice constant factors and in-place operation. The Haskell version completely drops that. To others, Quicksort is nice because it is obviously parallel in nature. The Haskell version does preserve that.
I don't know what your friends are arguing like, but here's an imaginary counterargument to an imaginary person who disdains all these silly high-level languages because they're inefficient and abstract... So: Mathematica is basically a functional programming language. It's a nice example because I think Mathematica is a clear example of how electronic computers let us create high-level labor-saving tools for calculating and reasoning. Is Mathematica "better" than C? Is a car better than steel? No, they exist on different levels, for different purposes. Haskell supervenes on lower levels of programming. GHC is a massive feat of engineering. Haskell compiles to machine code. The runtime is written in C. It's a beautiful collaboration! Is it bad that Mathematica uses garbage collection and thus is "slower" than C? Is it bad that a car has automatic transmission? We invent labor-saving technologies like this simply to make work more convenient and efficient and create new possibilities. It's a tragedy if a mathematician working on solving a problem using Mathematica encounters a segfault or memory leak. Garbage collection itself is a feat of low-level engineering, and it's an interesting field of algorithms and data layouts in itself. We should celebrate these things. Now, as a Haskell programmer since 2003 who picked a CS university specifically for its focus on Haskell and functional programming, I never even argue that Haskell is better than any other language, or that it's some kind of pinnacle of computer engineering in general. It is a pinnacle of achievement within its own narrow tradition, that's indisputable, but you can always argue about whether that tradition is the "best" tradition, or talk about its flaws. We're not that good at empirically testing programming languages and methodologies, and I don't think there's any convincing scientific evidence that pure functional programming is better for any specific kind of coding tasks. Anyway, Haskell to a large extent comes out of a mathematical and logical tradition that has been less aligned with industrial, commercial goals. And that's fine. Haskell was not meant to be the next C++. The fact that Haskell at the moment is not a good replacement for C in low-level kernel modules is not an argument against Haskell. Haskell is a thing of its own. Practitioners of Haskell are both using it for its original, traditional, obvious purposes‚Äîdefining compilers and logical tools, let's say‚Äîand experimenting with how its paradigm stretches into other problems and fields, like web app development. It's grown and changed a lot over the past decade. So if your friends find that Haskell seems in any way interesting, they should learn a bit about it. If not, that's fine, they can go on their merry way. There's a lot of arguing between different camps that's kind of totally pointless. Besides, all programming languages are basically awful.
that is not the biggest concern. The above implementation requires O(n^2) time. For quicksort to be efficient you need to pick a good pivot. One way of doing this is picking it randomly, giving you an *expected* O(n\log n) time algorithm. You can use linear-time deterministic median finding, but that has large constant factors (and would also increase the size of that code by quite a bit). 
&gt; but I want to make them see that the core concepts of the functional paradigm can be used in low level. Take a look at [clash-lang](www.clash-lang.org) and [Arduino-frp](https://github.com/frp-arduino/frp-arduino).
&gt; The computer architecture is imperative by default Is it? How about FPGA? There have been several(?) projects aiming to compile Haskell into Verilog, and thus "directly into hardware".
Thank you for your perspective! That's a good idea, I should probably change my approach!
Well said! What university is like that?
Thanks for your answers guys! I really think I should change my approach and use some of the points you mentioned. I think I was the one pushing too hard, I need to let them be curious
They may like the futhark language, never used myself but AFAIU it's a "low-level" pretty fast functional language.
O(1) additional space vs O(N) additional space is a stark difference.
&gt; The computer architecture is imperative by default It is? When did that happen?
nice! thank you :)
Just wanted to add my $0.02. The reason we use Haskell for our _website_ is we find it easier to maintain then other languages _used for web servers_, while still being decently performant. Would we use Haskell for embedded programming? Probably not, but I honestly have no idea. Right tool for the job kind of thing.
At the price of a significant slowdown. That may not be an option for a tool that's supposed to run alongside IDE and, potentially, do the thing on every keystroke.
Seems about right, but I am more focused on the concepts behind the paradigm and how you can approach them even without using haskell and how the paradigm make you a better thinker/programmer/engineer for free
Assembly is imperative, so as von Neumann architecture. 
Assembly is imperative, so as von Neumann architecture. 
Assembly is pretty high level as far as EEs are concerned. Assembly doesn't model the signal flow or timing that actual electronic components use to interact with each other.
The people doing the low level programming, the C programmers, they are the original professional cast, so is their reaction to a perceived threat, you never gonna convince them. They know what is been taken from them and like those in the fifties, when the first compilers were introduced, they are opposed to it. It's so nice to see yourself kept busy for the foreseeable future with those well known hand made optimizations that you've become so good at. At best you can recite the "wise" platitudes we see even in the comments here: "No one is better, C is good at something, Haskell is good at something else, we can have both". Isn't that a clear example of what Alain Badiou calls paraconsistent logic, the logic of our time. But the "danger" is a real one this time
*Hey just noticed..* it's your **2nd Cakeday** mcgizzzle! ^(hug)
I would like to see the (image)s in the Nasic github documentation.
Thanks! It's Gothenburg, Sweden.
That's interesting, I study in University of Minho in Portugal and we learn FP and Haskell in the 1st year And it's one of the only in Portugal that approach this language!
I'm curious: is it obvious that the amortized running time is quadratic? The best case is Theta(N log N) whereas the worst case is quadratic. The latter is true even for imperative quicksort.
&gt; I just noticed that Tree f a is the freaking free monad, and I'm more than a little baffled by that. This shouldn't actually be all too surprising. One way of viewing a free monad is as an abstract syntax tree, where `&gt;&gt;=` substitutes all leaves with another tree, where that tree is dependent on the leaf being substituted.
Can someone post a link of a pdf version? 
I support any tutorial that requires the user to have some flavor of TeX installed to compile and view the tutorial.
Here are the pdf files I generated: * [Haskell 101](https://www.dropbox.com/s/p0zcfec8xi5brgs/haskell_101.pdf?dl=0) * [Haskell 102](https://www.dropbox.com/s/76awds42dokx9bb/haskell_102.pdf?dl=0)
You don't need nixos specifically, but [reflex-platform](https://github.com/reflex-frp/reflex-platform) will make use of the nix package manager (which works fine on most any variety of linux). I highly recommend making use of reflex-platform if you want to play around with ghcjs and reflex -- the dependencies can be quite tricky to deal with otherwise. Where I work, we make use of nix in essentially all of our projects, it's a good way of making sure that you have a stable set of dependencies across developer machines and in production.
I have some 2300 line dhall expression that is pieced from 59 separate imports, which seems approximately comparatively sized to configuration here. Dhall takes 0.6s to normalise it. If I encode the file after normalisation, decoding it then takes something like 0.06s. So, \~0.5sec when you change the config and then added 0.06s for the loading each time the tool is used. I don't think thats really bad. Also, for each keypress situation, you probably won't be spawning the process anew every time, so the configuration loading time is probably not significant there.
Avoid success at all cost
What else would you expect of Haskell users?
So many ponies
Citing this thread should be good enough. Thanks!
For me it's actually slower on Linux... :-) I did the simple experiment of cloning GHC HEAD twice, and running `./boot` + `./configure` in both, and then comparing a default build via both systems ## Current `make`-powered build-system $ time make -j4 ... real 49m56.353s user 153m43.659s sys 8m13.446s Re-running `make -j4` in order to measure the no-op baseline: $ time make -j4 ... real 0m4.906s user 0m4.607s sys 0m0.514s ## New Hadrian build-system A difference with Hadrian is that first Hadrian needs to be compiled before it starts taking over orchestrating the build. I wanted to measure this pre-Hadrian phase individually, so I first ran $ time ./hadrian/build.sh --help ... real 4m56.587s user 5m49.252s sys 0m7.730s and then invoked the actual Hadrian phase: $ ./hadrian/build.sh -j4 shakeArgsWith 0.000s 0% Function shake 0.007s 0% Database read 0.001s 0% With database 0.000s 0% Running rules 3256.587s 99% ========================= Pool finished (2805 threads, 4 max) 0.003s 0% Total 3256.598s 100% Build completed in 54m17s and then I re-invoked it again to determine the baseline no-op cost: $ ./hadrian/build.sh -j4 shakeArgsWith 0.000s 0% Function shake 0.012s 0% Database read 0.976s 10% === With database 0.063s 0% Running rules 7.946s 88% ========================= Pool finished (1503 threads, 4 max) 0.005s 0% Total 9.003s 99% Build completed in 9.00s ## Summary Comparing the `Makefile` build system to the Hadrian build system, - Make needs a total of ~50 minutes to perform a default build of GHC, compared to - Hadrian which needs a total of ~59 minutes (5m + 54m) to achieve the same effect Moreover, the baseline no-op cost is - Make is able to perform a no-op build in ~5s, whereas - Hadrian takes ~9s to perform a no-op build I haven't yet had time to measure other scenarios where Hadrian might come ahead of `make`, but currently it doesn't seem like Hadrian is an obvious win on the performance side. But performance isn't the main motivation for implementing Hadrian anyway; we should rather focus on its othe potential long-term benefits (maintainability, easier to write correct build rules, more accurate change tracking, convenience for GHC development, etc.).
Many ponies died to bring us this information.
What's the problem with it?
Is this the default flavour for both Hadrian and Make? It might be worth making a ticket about this. I hope Hadrian might be building something more than Make in those 9 minutes. Re benefits: Don't forget that Hadrian makes it possible to have all build artifacts in one folder _and_ that this folder is now relocatable.
When I try to return a TilesRow, the format seems to be different from what I need. This is the code where I create one: createTiles :: TilesRow createTiles = [(x, 1 + div (x - 21) 4) | x &lt;- [21..36]] error: ‚Ä¢ Couldn't match expected type ‚ÄòTilesRow‚Äô with actual type ‚Äò[(Integer, Integer)]‚Äô ‚Ä¢ In the expression: [(x, 1 + div (x - 21) 4) | x &lt;- [21 .. 36]] In an equation for ‚ÄòcreateTiles‚Äô: createTiles = [(x, 1 + div (x - 21) 4) | x &lt;- [21 .. 36]] | 125 | createTiles = [(x, 1 + div (x - 21) 4) | x &lt;- [21..36]] | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ But if I do `createTiles :: Staple` then it works. And TilesRow is essentially a Staple that enforces the values within go from low to high. \* &amp;#x200B; \* actually I'm not sure if that works yet either
Sorry, I should have been more specific. I need the values relative to the other Tiles within TilesRow to be rising. So this is allowed: (1,1) (2,1) (3,2) etc. But this is not allowed: (2,1) (1,1) etc. because 1 is lower than 2. So there needs to be a rule for both the first and second value of the tuple that says it must be &gt;= to the previous one.
Oh, yeah, I see the problem. When you write this: `data TilesRow = Staple`, all you do is create a type called `TilesRow`, and a function `Staple :: TilesRow`. I think you had something like this in mind instead: newtype Tile = Tile Int Int type Staple = [Tile] type TilesRow = Staple But then I don't see how Staple and TilesRow would differ, or what purpose would they serve.
Oh, thanks! I thought I had to make TilesRow a data type so that I can write "deriving (Ord)", I figured that might be the way to enforce the numbers go up only.
That's not how it works. `deriving (Ord)` automatically generates an instance of `TilesRow` for the typeclass `Ord`. That means that you can use the functions provided under the [Ord typeclass](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Ord.html) on `TilesRow`. To enforce the property, I would create a function that takes two `Int`s and returns a `Tile`, and hide the original constructor, so there is no way to create incorrect tiles.
Thanks :)
Pandoc
Thanks! I'll give it a go. To enforce the property shouldn't I do it on the level that creates the TilesRow and not the level that creates the Tile? Because it's about the Tiles themselves only going up in value. So this is allowed: (1,1) (2,1) (3,1) (3,2) But this is not allowed: (2,1) (1,1) etc. For example.
Oh, I misunderstood. Yes, in that case, you should do it on the TilesRow level.
If you look at [here](https://www.stackage.org/haddock/lts-10.3/vector-0.12.0.1/src/Data.Vector.Generic.html), there's a rewrite rule - "clone/new [Vector]" forall p. clone (new p) = p And the implementation of `modify` uses `clone` first modify p = new . New.modify p . clone Since a lot of functions in the vector package are marked as INLINE/INLINE_FUSED/INLINABLE, it might so happen that you get a nested `new` inside `clone`. The corresponding rewrite rule will fire and voila, you've avoided an extra copy. I haven't looked too carefully, maybe there are other rewrite rules too that get rid of `clone`.
That brings me back to the issue where if I enter an \[(Int, Int)\] it's not recognized as a TilesRow. For convenience I've changed (Int, Int) into simply Int. data TilesRow = TilesRow [Tile] removeMatchingTile :: TilesRow -&gt; Tile -&gt; TilesRow removeMatchingTile row s = filter (/= s) row error: ‚Ä¢ Couldn't match expected type ‚ÄòTilesRow‚Äô with actual type ‚Äò[Tile]‚Äô ‚Ä¢ In the expression: filter (/= s) row In an equation for ‚ÄòremoveMatchingTile‚Äô: removeMatchingTile row s = filter (/= s) row | 268 | removeMatchingTile row s = filter (/= s) row | &amp;#x200B;
Thank you for benchmarking! Could you please put this in a ticket on Trac? We've just added a GSoC project to optimise Hadrian [1], and it would be useful to link from it to this ticket as a baseline. [1] https://summer.haskell.org/ideas.html#hadrian-optimisation
It's because the types don't line up. `TilesRow` is a type, and also a data constructor. To make the difference more apparent, I'll rename the data constructor: data TilesRow = MkTilesRow [Tile] Here, `MkTilesRow` is a function with the type `:: [Tile] -&gt; TilesRow` When you supply a list of `Tile`, that is not a `TilesRow`, that is a `[Tile]`. However, we just have the function to turn it into a `TilesRow`: `MkTilesRow`. Here's the fixed version: removeMatchingTile :: TilesRow -&gt; Tile -&gt; TilesRow removeMatchingTile (MkTilesRow row) s = MkTilesRow $ filter (/= s) row
This makes me worry about the employees at Google
Ah, thank you! My book does NOT explain this properly then... Is this also allowed? removeMatchingTile (MkTilesRow row) s = MkTilesRow (filter (/= s) row) &amp;#x200B;
Literate Haskell :).
Yes, of course. The `$` operator is really just a convenience stuff, so one does not have to bother with parentheses.
Thanks for all the help! 
It uses rewrite rules, which works well when vectors are used in a linear fashion, because that makes GHC keen to inline them. [`modify` is defined as `G.modify`](https://hackage.haskell.org/package/vector-0.12.0.2/docs/src/Data.Vector.html#modify), which [is defined as `\p -&gt; new . New.modify p . clone`](https://hackage.haskell.org/package/vector-0.12.0.2/docs/src/Data.Vector.Generic.html#modify) Now, compose two of those operations `modify p . modify q`. Naively, each invocation does one copy of the vector. This expands to \v -&gt; new (New.modify p (clone (new (New.modify q (clone v))))) where the two copies are done by `clone`. Now there is a [rewrite rule](https://hackage.haskell.org/package/vector-0.12.0.2/docs/src/Data.Vector.Generic.html#unstream) that applies in the middle: {-# RULES ... "clone/new [Vector]" forall p. clone (new p) = p ... #-} This optimizes: \v -&gt; new (New.modify p (New.modify q (clone v))) Then [you can expand `New.modify`](https://hackage.haskell.org/package/vector-0.12.0.2/docs/src/Data.Vector.Generic.New.html#modify), and after some simplifications this becomes \v -&gt; new (New (do v' &lt;- clone v q v' p v' return v')) So we started from `modify p . modify q` which would naively `clone` twice, and now it only does it once, putting the two mutations `p` and `q` right next to each other.
&gt;Purely functional ‚ñ∂ Everything is a function No, everything is a value. Some values are functions. "foo" is not a function.
It annoyed me to have such a common misconception re-enforced by this source, and so early in the slides too.
I thought the "Cascading Contexts" section (in the 30's) was going to be a motivating example for monadic error handling. It's not. It looks like a mis-conceived example of a Haskell weakness. (?)
The day of the rope can‚Äôt come soon enough
I was able to get a pdf out of it but it has no text, only pictures and code. A see there are links posted to pdfs but I thought maybe it's time I get to know what this cryptic tex/latex/whatever is all about.
With nix environment specific build dependencies
It it, but much later - see around p. 140!
you're right, but no need to be pedantic : )
Open a bug?
The initial build time isn't exactly fair to include since it's a one-time cost. Sure it impacts development but you can't \*completely\* say that Hadrian is exactly 9 minutes slower. It's only 9 minutes slower \*once\*...then it's 4 minutes slower. If Hadrian ever becomes faster then this difference will be important because the initial cost will begin paying for itself.
It's building itself in 5 of them! It's really only 4 minutes difference when compared apples-to-apples and that's a small enough percentage that I think it's probably \*just\* slower.
As a side note, NixOS is extremely good. If you ever start getting the hang of Nix, it's worth it to start giving NixOS a look. The entire concept of a system config is just a Nix derivation, which provides a *ton* of useful features like nearly atomic system upgrades, a list of previous system configs for nearly atomic system rollbacks, trivial remote deployment, and a whole bunch of other stuff. Plus you get to configure everything declaratively with a functional language.
`data TilesRow = Staple` Means that `TilesRow` is a data with a single 0-ary data constructor called `Staple` - which has absolutely nothing to do with the `type Staple` you have in the line above. You probably meant `data TilesRow = TilesRow Staple`, although I'm not sure why you're making a new type alias and also introduce a new type that's pretty much the same as the type alias... &gt; But I can't get it to work. Additionally, I would like to enforce is that every element within TilesRow is from lowest to highest Int of the fst Int of the tuple. Keep it as a sorted list then? I'm not sure what exactly your `Tile` tuple is representing, but it sounds a bit awkward.
For me: practice. I just had to do many small projects. Now FP comes very natural to me. 
Can any veteran suggest if it would be good for a beginner? Before moving onto other texts.
To be fair, Haskell is what it is because people followed certain principles rigorously. It may seem pedantic, but it pays off on the long run.
if you use unit introduction (even if you squint and imagine it) this is true, `a` is isomorphic to `() -&gt; a` and as a nice bonus in that world function composition and application are the same thing.
Write a lot, get things wrong, refactor. The mindset and habits will come with time.
What do you want to amortize over? All possible input sequences? If so, I'm not sure that is an interesting thing to consider. 
 removeMatchingTile :: TilesRow -&gt; Tile -&gt; TilesRow removeMatchingTile (TilesRow tiles) tile = TilesRow $ filter (/= tile) tiles The issue you're seeing is that `filter` has type `(a -&gt; Bool) -&gt; [a] -&gt; [a]`, so you need to do some manual wrapping/unwrapping to make the types match up. `removeMatchingTile (TilesRow tiles) tile` lets you deconstruct the `TilesRow` and get at the `[Tile]` inside it. Then you can use `filter` as expected. Then you have to rewrap the whole thing in `TilesRow` so the types match up and you get a `TilesRow` out.
&gt; The initial build time isn't exactly fair to include since it's a one-time cost I disagree. It is totally fair to include here as this was a cold-build scenario. I.e. cloning a new GHC (or starting from a `dist-clean` situation). I do this quite a lot when working on GHC, as it's often the best way to ensure there's no left-over artifacts when switching to different Git branches or pulling in new commits or patches.
Well I guess what matters is that we compare use cases in particular. Hadrian will always have a 5 minute penalty for that use case and you're right that is worth noting. But for other use cases that 5 minute penalty might pay for itself.
Each way of programming has it place. One this that Haskell makes explicit is that controlling mutation, inheritance, etc... is nice. Take a look at this post, it talks about an architecture programming that identifies different layers of the code with programming paradigms: https://www.parsonsmatt.org/2018/03/22/three_layer_haskell_cake.html
&gt; Well I guess what matters is that we compare use cases in particular. Which is what I did here. It's my most common use-case.
That‚Äôs so cool! I love how typed FP turns up these casually mind blowing facts. 
I would recommend looking into `Data.Set`. Given some `Tile` defined as a pair of `Int`s import Data.Set (Set) import qualified Data.Set as Set newtype Tile = Tile Int Int deriving (Eq, Ord, Show) newtype TileSet = TileSet (Set Tile) deriving (Eq, Ord, Show) So now we have a `Set` of tiles which is ordered according to the value of `fst` and then according to `snd`. a = [(3, 1), (2, 4), (1, 5), (1, 3)] TileSet $ Set.fromList (fmap (uncurry Tile) a) -- TileSet (fromList [Tile 1 3,Tile 1 5,Tile 2 4,Tile 3 1]) So we'll write a function to nail that idea down mkTileSet :: [(Int, Int)] -&gt; TileSet mkTileSet = TileSet . Set.fromList . fmap (uncurry Tile) Now we can do createTiles :: TileSet createTiles = mkTileSet [(x, 1 + div (x - 21) 4) | x &lt;- [21..36]] removeMatchingTile :: TileSet -&gt; Tile -&gt; TileSet removeMatchingTile (TileSet set) tile = TileSet $ Set.delete tile set You get the sorting you want, and you get duplicate removal by without implementing anything extra (supposing that you don't want tiles to occupy the same space).
Hey, no problem.
Did you `make` complete without any errors? I saw some errors related to fonts: `LaTeX Error: File \`floatflt.sty' not found.` I had to install the fonts using `tlmgr install floatflt.` Similar error occurred for 2-3 different fonts. But after all were installed, the pdfs were generated properly. I have posted them in a comment above.
Found this: [https://github.com/google/haskell-trainings/pull/1](https://github.com/google/haskell-trainings/pull/1)
All possible orderings. For example, you'd have to consider all permutations of `[1 .. n]` (the actual underlying set isn't important...) plus additional cases for when things might be equal.
Be warned, this is true in lambda calculus but not in Haskell's type system. Ref: http://conal.net/blog/posts/everything-is-a-function-in-haskell &gt; Although I keep hearing ‚Äúeverything is a function‚Äù (and 3 is a nullary or constant function), I don‚Äôt hear people say ‚Äúeverything is a list‚Äù, and 3 is really the singleton list [3]. Or ‚Äúeverything is a pair‚Äù, and 7 is really (7,‚ä•) or some such. Or ‚Äúeverything is a Maybe‚Äú, and True is really Just True. Personally I don‚Äôt like to equate non-functions (number, bools, trees, etc) with 0-ary functions any more than I to equate them with singleton lists (or trees, ‚Ä¶) or non-Nothing Maybe values, etc. 
This helped! Thanks.
In replying to another user down below I realized my own understanding might be incomplete: "foo" is not a function. But is "foo" the normal form of `() -&gt; "foo"`? Would it be accurate to say "foo is the same `() -&gt; string` as `() -&gt; "foo"`?
In that case consider my post a clarification for others who like me didn't realize. ;)
thank you!
This isn't an intrinsic property of Hadrian at all, for instance windows binaries have always been relocatable. Making the Linux binaries relocatable took changes to ghc which is the real reason this works. Hadrian was just the only build system this was ported to simply the implementer choose to. 
Sorry, one more thing, how can I get the length of TilesRow? If I do "length xs" where xs is a TilesRow, it doesn't match the expected type.
Hmm, Google drive pdf viewer thinks these are broken files
Headline is a bit inaccurate: &gt; This is not an officially supported Google product.
While off-putting in the short-term, being pendantic is rewarded handsomely in the long-term. Otherwise, you are given / allowing the wrong mental model, and you have a weak base upon which to build your understanding.
A.... disturbing number of them, yikes.
When we say an algorithm has an amortized running time of T(n) (on n inputs) we typically mean that running the algorithm/a sequence of invocations of the algorithm takes O(mT(n)) time. Note that in this definition it is perfectly fine to keep feeding it the same "bad" input/operation, yet, somehow the total running time may be small. So, in that sense it would be perfectly fine to keep feeding quicksort the same bad, increasingly ordered list, m times. The total running time would then just be O(mn^2); and thus the amortized time is still bad. If you somehow force that the inputs are different every time, and you consider all possible inputs, like you are suggesting then you are you are doing an average case analysis. Admittedly, I'm not entirely sure what that would be here.
Yes, I realised this after reading through the material and README. I posted this right away after coming across this and seeing the project under google's organization. I would like to change it, but looks like reddit does not allow me now :(
My bad, average case is indeed the better term, agreed. I'm not sure how useful the figure for "repeatedly feed the same bad input" is anyways, considering the average case makes much more sense.
do not overcomplicate it; one does not need to prepare some grand mind change for functional programming; instead : that will come itself without your conscious effort; functional programming is just avoiding side effects; do not think about it any further; just do it &amp;#x200B; if you still want to read more before letting it on you : i have written [an introduction to functional programming](https://libeako.github.io/c/ID_302924781.html) &amp;#x200B; by the way : i think such not Haskell specific question should go rather into r/functionalprogramming
I know a little bit, I've built a couple FRP things in the past. I'd use `reactive-banana` and some terminal UI library like ncurses or vty.
i do not understand how having electronic background and having to mess with low level are connected; unfortunately low level is the only circumstance when functional programming may need to be dropped, but a try may be worth; i have written [an introductory into functional programming](https://libeako.github.io/c/ID_302924781.html) with the main goal to attract people into it
TeX_Tutorial.tex
Not a veteran but to me this is more of a lightning talk type presentation than a real tutorial. If you want to see if Haskell is something that might interest you, then it's nice. If you actually want to learn Haskell I'd still recommend Learn You A Haskell or the Haskell wikibook.
length :: Tilesrow -&gt; Int length (MkTilesRow row) = length row
Can you link me to your writing?
I particularly have troubles applying stuffs like monads etc to actual problems, do you have a ressources regarding this?
Oooh, got it! Thank you!
Any chance for a MLP-less version?
Any parser or pretty-printer library should fit the bill. All you need to do is supply an input of appropriate size. 
Examples of small but non-trivial type-checkers written using the bound library? I've been reading the examples in bound-extras such as [BiSTLC](https://github.com/phadej/bound-extras/blob/master/examples/BiSTLC.hs) but I'd like to see an example where you have enough going on that you're keeping at least a separate typing context to keep track of things. As an aside, is separating out inference and checking terms a thing people usually do for bidirectional systems like in that example?
Sorry I don‚Äôt know any particular resource. I learned them as I went. First avoiding them (you rarely *need* them), then using only simple use cases (printing and IO). Now I‚Äôm fine with the concept, and use it at will. I have also implemented my own parser combinators at some point. Probably that‚Äôs an intuitive way of understand monads. Although I should add that I do have a maths background... Maybe that has helped?
It‚Äôs Google, prepare to have your bug outlive you.
http://learnyouahaskell.com/chapters ^^ Great place to start out
the link is in my comment, it works, but [here it is again](https://libeako.github.io/c/ID_302924781.html)
I eye twitched too, but it be fair this whole set of slides is misguided. It talks about everything in the wrong order and appears to be aimed an audience that already knows Haskell but is framed like it‚Äôs for newbies. 
There is no golden road! Your primary method of abstraction in FP is the humble function. And you compose together a program by applying your functions to arguments. You have to declare everything and can assume nothing. This is in contrast to OOP where you think in terms of objects and message passing. The nice thing about a language like Haskell is that if you can break down the hardest code with substitution. You can understand the complex whole by substituting the smaller parts it is made from. This works surprisingly well when you're starting out. Once you understand functions, some basic types, and how to compose functions together you can get quite far with Haskell. It's a journey from there. And it's well worth the effort!
Shameless link to my comment on this a few months ago https://www.reddit.com/r/haskell/comments/9rz032/ive_written_a_free_beginners_guide_to_haskell/e8l9q07/ I agree that *if you squint* then you *can* think of Haskell values as functions. However, squinting like this is not what I would recommend to beginners trying to learn Haskell, because this is not the intended mental model for what functions actually *are* in Haskell.
No Comic Sans, though. Disappointed.
Why? In my mind everything is a function and values are just constant functions. That makes more sense to me coming from a category theory point of view. 
Like... tons. Wow.
Amortized analysis is very useful: say you have a data structure that supports operations, say updates and queries, in amortized O(log n) time, and you build have an algorithm that performs n updates and queries, then the *worst case* running time of your algorithm is only O(n log n). No need to worry if your algorithm happens to perform only expensive operations or not. Average case analysis on the other hand is much weaker: even if you know that the average cost of your operations/queries is O(log n) then you don't actually know if the average running time of your algorithm is O(n log n) or worse. Moreover, it does not give you any *worst case* bounds on your total algorithm.
My main advice for this is to stop trying to solve problems "using monads", and just start solving problems using data types like Maybe, Either, List, IO, State, Reader, Writer, etc. Once you do, applying monads becomes something you don't even have to think about.
Do you think that there exists no problem for which C is better suited than Haskell? Even if Haskell is better than C at 95% of modern problems, C is still useful. This is not a contradiction. A toolbox doesn't just need a hammer, even though "when all you have is a hammer, everything looks like a nail."
No need for additional clarification. Actually the use of type theoretic jargon helped a lot, thanks.
What is the correct syntax for using Template Haskell functions with extra type parameters? For example, I have a three parameter type and I want to derive `Show1` for it using the deriving-compat package, but `deriveShow1 ''(MyType a b)` gives me a parse error.
Thank-you very much for this.
Is there a Bronie contingent of the Haskell community that I wasn't made aware of?
Not a veteran but no. Especially with all the MLP stuff in it. You may start here: https://www.schoolofhaskell.com Though I'd recommend you dedicate the time to go through a book. Get Programming with Haskell is pretty good.
The type `String` is isomorphic to `() -&gt; String`: there is a [bijection](https://en.wikipedia.org/wiki/Bijection) between the two underlying sets of values. You can map back and forth between the two, but you still can't just put one in place of the other. It can be a useful fact in some settings, but it is far from being essential to functional programming. When people say "normal form" in topics related to Haskell they usually mean something from the area of rewriting systems. The idea is to describe computation as "rewriting" or "reducing" an expression bit by bit: 1 + (2 + 3) -&gt; 1 + 5 -&gt; 6 Then a *normal form* is an expression that you cannot "rewrite" anymore, which intuitively means it is "fully simplified" (but that intuition depends on the rewriting system being considered). Complex languages like Haskell usually need some type system to ensure that rewriting has nice properties ("well-typed programs do not go wrong"), and the first step to prove them is to ensure that rewriting preserves types. So it doesn't make sense to say that `"foo"` is the normal form of `\() -&gt; "foo"` because they don't even have the same type. Although you could define a different rewriting relation where `\() -&gt; "foo"` rewrites to `"foo"`, it would not be useful.
They used Celestia and Luna to represent pure vs. impure functions. I thought to myself, "nah, Luna's pure too, they should have used her first season Nightmare Moon form." Then I started laughing at work because wow, I'm a brony.
There are dozens of us!
I'm glad you liked it :) I'm about to start a refresh the workshop and tutorial to handle the changes that have since happened in `reflex-dom`, after which we'll be putting together a course to go along with our introductory and applied FP courses. It should be a blast. Well, working on it should be a blast, hopefully working through it will _also_ be a blast, but you never know with these things :)
`deriveShow1 ''MyType` and pray that it handles the extra parameters correctly (it's usually fine).
Google just open sourced some Haskell tutorials for kids. (Check it out.)[https://github.com/google/haskell-trainings]
Ah, okay. That was giving me some error earlier so I thought it must be wrong too. Looking back at it now, seems like it just can't handle the type parameter shuffling in the data type definition :(.
Isn't it a bit difficult to construct widgets with vty? brick has widgets, but it is not compatible with FRP libraries. How do you construct a GUI on vty or ncurses?
Unfortunately, Christopher Allen moved away from GHCJS due to performance issues. Nowadays, he prefers plain web applications(i.e. yesod + jQuery) and native UIs to GHCJS web application.
Is there any context about why google has haskell training material, or has chosen to release this? Particularly, is there something google is developing that uses enough haskell to warrant this guide? Is there some other haskell related activity that we can learn more about?
Thanks for the great response! So the existence of a bijection does not mean that some value and its isomorphism are the "same?" Is it fair to say something like "the statement `"foo"` is the 'same' as `() -&gt; "foo"` doesn't make sense because they aren't the same type?" Or is there some notion of sameness that doesn't require the values being compared to have the same type? (I took your first paragraph to mean that isomorphic values aren't really the same value.) 
Thanks, I'll try that
It has :)
When you have an isomorphism it can make sense to identify objects on both sides, making implicit use of the isomorphism. In fact you can do this in only one direction if you have a function rather than an isomorphism. For example, the `OverloadedStrings` extension allows you to write `"foo"` in place of `fromString "foo"`. People like to write code that way often because they often consider that `"foo"` the `String` is "the same" as `fromString "foo"` the `Text` in some sense, which is that many operations on `Text` have a counterpart on `String`. But of course, they're not exactly identical objects: `String`s are really lists of characters, while `Text`s have a more primitive representation. Sometimes it matters, sometimes not. This leads to the idea that "sameness"/"equality" is relative. But that relativity does not mean we can go around and say things like "everything is a function" wildly. In some context, it makes sense to equate certain things, but that will break down outside that context. Sometimes we can say `1` is "the same" as a constant function `\_ -&gt; 1`, but sometimes we do not care and we also write `1` when we mean it to be an `Int` that we want compiled to a machine integer.
I've read some of what Chris wrote about that. I vaguely remember something about it seeming a bit off to me. I've seen some use cases that are a bit slow with `reflex-dom`, but not to the point that there were showstoppers for what I / others were doing. I've also seen problems that seemed more like they were showstoppers, but were resolved by using functions / patterns of coding that exist for those particular cases. There is a bit of a discoverability problem with some of those patterns and ways of thinking, but hopefully that will get addressed over time. On top of that, `reflex-dom` itself is a moving target, so it's possible that my experience is drawn more from the current versions of the code than the version Chris was using.
Although the demo situation for Vty is not as mature as for Brick, you can check out the [demonstration program in the repository](https://github.com/jtdaugherty/vty/blob/master/demos/Demo.hs) or the [Hackage introductory documentation](http://hackage.haskell.org/package/vty-5.25.1/docs/Graphics-Vty.html).
This is not easy to sum up briefly; as best I can tell, right now the answer is "we're not sure yet, but maybe." There have been a few forks and efforts to explore this, including discussions in tickets on the Brick repository. None of them have been conclusive, but there does seem to be interest.
Nice work. I think GHC is pretty good at unboxing things in non-recursive code, so that you could avoid relying on its internals, using plain `Word` with the `{-# UNPACK #-}` pragma and `Data.Bits` instead.
1. Slow down on writing code. It is okey to think for 2 hrs and write for 5 minutes. There is a risk of over-generalisation here so be careful. 2. Get familiar with standard datatypes like list, maybe, set etc. It is a good idea to browse their API documentation to get an idea of some of the functions there. 3. Write down what is the input (informally) and capture it using a datatype. Here you might find yourself staring at some common datatypes like list, sets, maybes etc. 4. After 2 write down (informally) what you want to do with the input. 5. 2,3 can be broken up into stages and then write the transformation functions between these stages. Your final program is the composition of these. In general composition is something that is great. In any of these stages do not shy away from writing lots of (hopefully 1 line) functions. If you have managed to use standard algebraic datatypes in your program, hoogle can help you locate the function that you are looking for. &amp;#x200B; After you have solved the problem you can think of cleaning up. It is a good idea to use hlint at this point. Fix the code using the suggestions from hlint and each time try to understand each of the suggestions. The act of understanding the suggestions will help you rework you brain to think functionally i.e. composition and eta reduction. Again during the act of writing the program or at the end of it see if there is a library function that is applicable. Prefer using the library function. Generally think in terms of fold, map and its various cousins.
This is it. ``` type Tile = (Int, Int) type Staple = [Tile] ``` would suffice. Use ```newtype``` of you want to prevent your type aliases to typecheck as the native types
The repo says it's not officially supported by Google but rather put together by Googlers who are Haskell fans.
Cabasa is a cellular automaton simulator; however, it lets you easily experiment with novel CA designs using Chris Pressey's [ALPACA language](https://github.com/catseye/ALPACA). Thanks to the amazing `hint` library, it also lets you **write CAs in Haskell**! (although I am having a few problems with building; see below) Special thanks go to /u/apfelmus and /u/edwardkmett for creating libraries which, although they never landed up in the final version, were crucial for development. I have lots of free time now so feel free to reply with feature requests or bug reports! During the process of making this program I also created a pair of libraries for cellular automaton simulation; these are the 'engine' behind the GUI. They aren't on Hackage yet (I'll do this when they're a bit more mature), but are available [on GitHub](https://www.github.com/bradrn/cellular-automata). A high-level description of the essential concepts of this library is available in section 4.4 of the Cabasa user manual (Dropbox link below). Please try them out (they have haddocks which may be generated with `stack haddock --open`) and feel free to give feedback! Some extra notes which didn't quite make it into the README: - A prebuilt user manual is available [on Dropbox](https://www.dropbox.com/s/47mtwmqgi782sn0/UserManual.pdf?dl=0). - Currently there are no binaries available. This is because Cabasa uses the `hint` library but needs to use some Haskell dependencies (refer to [this conversation](https://www.reddit.com/r/haskell/comments/ac3ajk/is_it_possible_for_a_haskell_application_to_have/ed5rvac/) with /u/gelisam for details - thank you for helping me!). Help on this topic would be greatly appreciated! 
It's been updated! Thanks again. https://robertwpearce.com/hakyll-pt-4-copying-static-files-for-your-build.html#simplify-file-copying-with-pattern-composition-operators
The problem with using Haskell as a configuration language is that you need to have the GHC toolchain installed anywhere that you want to use the formatter (i.e. in CI or wherever), which is sometimes not an option. This is a bigger deal in the context of a large company where sysadmins are more likely to accept adding a self-contained executable than a (large and sprawling) GHC installation.
What I mean is that Hadrian has been engineered so that end up with all build artifacts in _one_ relocatable directory. AFAIK, make is not designed to work this way (although I'm sure you could coax it to do this - anything is possible given time, energy, and coffee). The usual approach is to have make scripts in each directory and the object files, interface files, etc end up littered all over the place. I like how each it now is to track artifacts form various branches. I just specify a different directory with `--build-root` and i'm off to the races. I've heard that `git worktree` provides similar facilities, but this is just so simple.
Can you export a JSON/YAML and time loading those?
And we already had a system in place that did this. The files all over the place are a minor thing, and at least it was easy to find the one you needed to edit. Now things are still in multiple files all over the place. Except in a cryptic format that no one else knows about. 
OOP is about long living objects, having and maintaining states throughout it's lifetime. You somehow need to model the real world as object. &amp;#x200B; Functional programming is all about data transformation. &amp;#x200B; Once I think in that way, everything just clicked for me. 
I claim that C is our only one and true enemy. Much progress has been made, pure lazy idiomatic Haskell is in the same order of magnitude with C. We need better compiler theory, abstraction is part of reality itself 
Yep,my PDF viewer does as well.
That's a fair point. Dhall sounds like a great fit for that kind of thing.
&gt; apt-get install haskell-platform nooo
It will take a long time to rid all documentation from those obsolete Haskell platform instructions and update everything to the current best practice curl -sSL https://get.haskellstack.org/ | sh 
Is that really isomorphic in Haskell? `undefined` and `\() -&gt; undefined` do not behave the same under `seq`.
Nothing's impossible with the power of friendship and magic! :)
4. Keep slideshow open until heat death of universe
It's ponies all the way down
Agreed. Thats horrible. I used the generic way with ghcup and it worked. Sometimes also Stack works.
Can brittany also do the vertical layout thing, or should I switch to this fancy new thing? I want to allow long lines, but still have things like `$` under each other when there's lots of them, something like lastDef thedefault $ filter complicatedcomparison $ sortOn satiatiety blahblahs ‚Üí lastDef thedefault $ filter complicatedcomparison $ sortOn satiatiety blahblahs
"google has release their **internal** haskell training" would do the job
The other day, someone claimed that they work on Haskell in Google. I asked if that's official. Replay was that people do use Haskell at Google (and plethora of other tools) but some projects are not public. Then I seen those. So I think its reasonable to assume some project is using Haskell and that Google on-boarded some non-haskellers for that project. Personaly, I think that it was a given, after Facebook showed Haskell in their production code. After all if Facebook can do it, Google can do it better, no? ;) (that's a sale pitch not a fanboy statement!)
The `data` keyword is used like this: data TilesRow = Row Staple deriving (Eq, Ord, Show) Here, `TilesRow` is the name of the type, and `Row` is a *constructor* for the type, which takes a `Staple` as input. You construct values like this: myStaple :: Staple myStaple = [(1, 2), (3, 4)] myTilesRow :: TilesRow myTilesRow = Row myStaple Notice that for `myStaple`, we don't need to use any custom constructor, because `Staple` is only an *alias* for a list of `Tile`s. Similarly, `Tile` is only an alias for a tuple of `Int`s. This also means that we can get away with stuff like this: myIntTuple :: (Int, Int) myIntTuple = (2,3) myTile :: Tile myTile = myIntTuple myList :: [(Int, Int)] myList = [(1, 2), (3, 4)] myTilesRow :: TilesRow myTilesRow = Row myList Which may or may not be a good thing.
thanks : - )
I'm a big fan of Get Programming with Haskell, and would recommend it to any beginner. 
[http://haskellbook.com](http://haskellbook.com/) is one of the best resources I've come across for learning Haskell.
How do you know that Haskell is *the* Universal Programming Language? I'll be honest, I feel this way about Bayesianism and frequentism. But Bayesianism *follows* from mathematics. Any deviation from Bayesianism must, according to logic and a few basic axioms like "you shouldn't be able to calculate probabilities two ways and get different answers", result in you being less accurate than a Bayesian in your position would. In short, probability theory *requires* Bayes. Haskell doesn't seem to be in the same position. It's definitely a lot more mathematical than most other languages, but I very much doubt that it is provable that using Haskell instead of C is always optimal\*. Using the form of mathematics more than hodgepodge bureaucracies is useful, but not itself sufficient. \* I'm not counting cases where "nobody else knows Haskell so this would make your code unusable by anybody else" and so on. Just the nature of the problem itself.
This isn't how most people learn. Discarding innacurate mental models in favor of more accurate ones is totally normal, and is necessary to some degree for extremely complicated subjects that cannot be assimilated in one pass. The idea that unlearning is painful is just another spin on the fact that learning can be hard sometimes.
That's an experience I got from making `splitmix`, which also happen to be PRNG :) http://hackage.haskell.org/package/splitmix
Very nice! I managed to build it on ArchLinux (gonna send a PR sometime later with an extension to your \`BUILDING.md\` since the package names differ quite a bit, if you want). I just have a problem when using rulesets defined in Haskell: [https://i.imgur.com/UJjCuD2.png](https://i.imgur.com/UJjCuD2.png) I was trying to load the supplied \`langton.hs\` file. &amp;#x200B; &amp;#x200B; &amp;#x200B;
Well.. Not really, since my dhall file has functions in them which is not really encodable as json. Stripping those out yields ~2000 line config, for which the numbers look like this. ``` benchmarking aeson time 2.406 ms (2.366 ms .. 2.444 ms) 0.997 R¬≤ (0.996 R¬≤ .. 0.999 R¬≤) mean 2.408 ms (2.378 ms .. 2.451 ms) std dev 122.5 Œºs (90.41 Œºs .. 173.4 Œºs) variance introduced by outliers: 34% (moderately inflated) benchmarking dhall time 91.10 ms (89.15 ms .. 92.88 ms) 1.000 R¬≤ (0.999 R¬≤ .. 1.000 R¬≤) mean 93.05 ms (92.33 ms .. 93.97 ms) std dev 1.271 ms (841.7 Œºs .. 1.865 ms) benchmarking dhall-frozen time 53.83 ms (53.27 ms .. 54.77 ms) 1.000 R¬≤ (0.999 R¬≤ .. 1.000 R¬≤) mean 55.89 ms (55.00 ms .. 58.05 ms) std dev 2.501 ms (830.7 Œºs .. 4.295 ms) variance introduced by outliers: 15% (moderately inflated) benchmarking cborg time 3.229 ms (3.190 ms .. 3.267 ms) 0.999 R¬≤ (0.998 R¬≤ .. 0.999 R¬≤) mean 3.245 ms (3.210 ms .. 3.332 ms) std dev 173.7 Œºs (77.51 Œºs .. 356.3 Œºs) variance introduced by outliers: 35% (moderately inflated) ``` I read the config and then do a simple `config==config` test to force the expression as both `Dhall.Expr` and cborg term lack NFData instances. The second number (dhall) is for loading an already normalized expression and the third (dhall-freeze) is auto-loading it from the cache (which causes one additional file load). The last value (cborg) is time for directly reading the dhall encoded file as cborg term. All in all, dhall is order of magnitude slower, but the speed is still in range where it is eclipsed by process starting time etc. Further, if you use encoded term directly, it is on par with aeson. Finally, adding actual marshalling to haskell records will add some overhead, but that is bit difficult to measure in this particular case.
I can partly sympathize with Chris' perspective here. However, I've built large applications with Reflex-DOM on GHCJS and they are *definitely* usable and plenty fast enough for the client. Reflex-DOM has also supported prerendering for some time now (and it keeps getting better) such that your initial page load can still be instant. I can see cases where you'd want to go a more traditional route, but categorically ruling out GHCJS is definitely not a good idea.
Hard to pick just one, but if I had to: [Conor McBride - What are Types for, or are they only Against?](https://www.youtube.com/watch?v=3U3lV5VPmOU)
Isomorphism doesn't require identical behavior on both sides. There's an isomorphism between `Natural` and `data Nat = Z | S Nat deriving Show`, but `show` works very different on them. Isomorphism requires that `to . from = id` and `id = from . to`, for `from x = \_ -&gt; x` and `to x = x ()` then, `to . from = \x -&gt; to (from x) = \x -&gt; (from x) () = \x -&gt; (\_ -&gt; x) () = \x -&gt; x = id` and `from . to = \x -&gt; from (to x) = \x -&gt; \_ -&gt; to x = \x -&gt; \_ -&gt; x ()` and then by unit-eta-reduction `\x -&gt; \_ -&gt; x () = \x -&gt; x = id`.
Reformatting for old reddit: benchmarking aeson time 2.406 ms (2.366 ms .. 2.444 ms) 0.997 R¬≤ (0.996 R¬≤ .. 0.999 R¬≤) mean 2.408 ms (2.378 ms .. 2.451 ms) std dev 122.5 Œºs (90.41 Œºs .. 173.4 Œºs) variance introduced by outliers: 34% (moderately inflated) benchmarking dhall time 91.10 ms (89.15 ms .. 92.88 ms) 1.000 R¬≤ (0.999 R¬≤ .. 1.000 R¬≤) mean 93.05 ms (92.33 ms .. 93.97 ms) std dev 1.271 ms (841.7 Œºs .. 1.865 ms) benchmarking dhall-frozen time 53.83 ms (53.27 ms .. 54.77 ms) 1.000 R¬≤ (0.999 R¬≤ .. 1.000 R¬≤) mean 55.89 ms (55.00 ms .. 58.05 ms) std dev 2.501 ms (830.7 Œºs .. 4.295 ms) variance introduced by outliers: 15% (moderately inflated) benchmarking cborg time 3.229 ms (3.190 ms .. 3.267 ms) 0.999 R¬≤ (0.998 R¬≤ .. 0.999 R¬≤) mean 3.245 ms (3.210 ms .. 3.332 ms) std dev 173.7 Œºs (77.51 Œºs .. 356.3 Œºs) variance introduced by outliers: 35% (moderately inflated)
I agree with /r/cdsmith wrt. convincing others. That being said: a practical, relatable example often helps. For low-level programming Haskell's type system can work as a proof checker for modelling the lower-level code. You can model a control system state machine in Haskell with quickcheck and various state-machine testing libraries. It also binds well to C so you _could_ also verify that your low-level library faithfully implements the state machine. This leads to fewer drone crashes and less down time in the lab.
I agree with you that discarding innacurate mental models is not only normal,but necessary and even likely for complex subjects. I think this works best though, when newer concepts inform your understanding of concepts that you learned previously. At any given point, there is likely one or two correct mental models, *given* what you know. Of course, if you add to what you know, then those mental models might become more accurate or nuanced. 
&gt;I have also implemented my own parser combinators at some point. Probably that‚Äôs an intuitive way of understand monads. This is great advice (write your own parser combinator library - not full fledge; just enough to get the big ideas). I would point you to Scott Wlaschin's \[tutorial, video, and slides on this topic\]([https://fsharpforfunandprofit.com/posts/understanding-parser-combinators/](https://fsharpforfunandprofit.com/posts/understanding-parser-combinators/)). Scott codes in F#, and has some great insights applicable to all functional programming languages. I translated the code to Haskell, but I didn't use any Haskell idioms - I tried to keep to the original F# as possible. &amp;#x200B; As you progress in your understanding of Haskell to monads and monad transformers, you can re-write your parser combinator in idiomatic Hasksell, and you will see how much power Haskell brings to the table. A \`Parser\` is just a specific instance of the \`State\` monad. To add error handling, wrap an \`ExceptT\` around it. What you get "for free" in the idiomatic Haskell version is very eye opening. Don't try to jump to this step; 9 / 10 it is going to be fruitless. Naturally work yourself up to monad transformers, and then I think it will be easier to see what I mean. &amp;#x200B;
The proposal actually changes the method of `HasField` to `hasField`, which can be used to both access the value of a field and update it. `getField` and `setField` are implemented in terms of it. Note that this would be a breaking change, if you have manual instances, so some code would have to be updated once this is implemented.
We have also been working on some similar geographic persistent/esqueleto stuff. The only functionality implemented is the stuff we use, so feel free to PR other things you need. https://github.com/polimorphic/persistent-postgis https://github.com/polimorphic/esqueleto-postgis
I couldn't find any non-watermarked versions (legitimately or otherwise), so you may have to contact one of the authors. I did find the slides for a talk given by the authors on this topic, though. http://www.lambdadays.org/static/upload/media/1487605199766609adambarwellldays17.pdf
Isn't this only a bijection? An isomorphism also need an homorphism (something where the behaviour (!)) is the same.
[Commutative Monads, Diagrams and Knots](https://vimeo.com/6590617) by Dan Piponi from ICFP 2009. I wasn't there but have watched this talk a few times since it came out, getting something new out of it every time. It is a lot more approachable than the title would have you think. It's somewhere between recreational Haskell and recreational maths, touching upon topics that often starred in [Dan's blog](https://blog.sigfpe.com/), namely monads, boxes-and-arrows diagrams, vector spaces, probabilities and more.
No, bijections need not be their own inverse.
Sorry, I'd assumed you were one of the authors. It's totally me, I'll get over myself! Up-voted anyway.
Looks like the repo in the OP is actually here: https://github.com/SatsumaLabs/persistent-spatial
Would this allow updating the field of a type whose constructor isn‚Äôt exported? 
Sure, this is only an isomorphism between types as sets, since that's where `(-&gt;)` constructs morphisms. If you were using a different arrow that preserved some sort of structure, then you'd have to make sure both `to` and `from` were that kind of morphism not "just" lambdas.
Honestly, I think the default type system, no extensions enabled, is pretty great when used well. I rarely feel the need to reach for stuff like DataKinds and GADTs. When I do want them, though, it's nice that they're there. 
Fixed. Thanks for catching that.
That's already possible. Just export the field, and while haddock might document it as just a getter, GHC allows it to be used for record updates.
Aw. Doesn't look like you can use it to change the type of a polymorphic record like you can with lenses.
... and yet they have like 1 master-level FP course, plus the "intro to Agda" one. I'm not bitter, just disappointed.
Interesting. I broke my library put of a project using CockroackDB (which is compatible with postgres core bit not postGIS). This looks like a better solution of being postgis-specific is OK.
Yeah, seriously. I'm trying to convince fellow coworkers that Haskell is useful and viable. Forcing people to go through extra steps for no reason doesn't necessarily help the Haskell cause... Yeah, I *personally* will probably go through the trouble of "compiling" these instructions, but will the typical time crunched "programmer" do that? I doubt it... That's why some "developers" wind up picking languages like BASIC. They just want to get on with work and don't necessarily want to do things in the most elegant manner possible. Such a mindset then causes a ton of spaghetti code written in BASIC, COBOL (or any other "easy to learn" language) to appear that no one can read because it was way to easy to "write". BTW: This is off topic, but I'm hoping GHC will "soon" support WASM as a target.
These talks I watched more than once * [HaskellerZ - Feb 2018 - Getting things done in Haskell and Zurich Friends of Haskell](https://www.youtube.com/watch?v=-X1vrxQUETM) * [Lambda Jam 2015 - Conal Elliott - Denotational Design: From Meanings To Programs](https://www.youtube.com/watch?v=bmKYiUOEo2A)
AWESOME
Oops! That is indeed the case. I've pushed working versions to the GitHub repo just now. I did a couple of extensive refactors a week or two ago, and must have forgot to change the rules. (If you're interested in how the rules work, they use the [libraries](https://www.github.com/bradrn/cellular-automata) I mentioned above; there's a good overview in Section 4.4 of the Cabasa documentation. And yes, please send a PR with instructions on Arch Linux!
&gt; I'm hoping GHC will "soon" support WASM as a target. Check out https://www.reddit.com/r/haskell/comments/ahmnrt/state_of_webghc/
[https://hackage.haskell.org/package/reflex](https://hackage.haskell.org/package/reflex) [https://hackage.haskell.org/package/reflex-dom-core](https://hackage.haskell.org/package/reflex-dom-core) &amp;#x200B;
Any particular reason for using this encoding instead of the van Laarhoven one, which can be more easily composed?
Unlikely, no. :)
Yes, self types simply allow a type to refer to its own value. For example, `foo : self x . P(x)`, then you can instantiate it, and get `inst(foo) : P(foo)`. Note that this has nothing to do with recursive types or recursive definitions. All it does is allow a type to refer to its typed value. It is pretty much a bridge between the type level and the value level of the language.
When I created the lessons, in 2016, the stack install procedure was a \`curl | bash\`. And I really didn't want to instruct hundreds of Google engineers to run a \`curl | bash\` on their google-owned machines. :) &amp;#x200B; And yeah, agreed, the haskell platform isn't great. But it was (and probably still is) the fastest way to get people started.
Thanks! I've uploaded the PDFs to the repo: [https://github.com/google/haskell-trainings/releases](https://github.com/google/haskell-trainings/releases), for convenience.
How has no one mentioned Wadler's [Propositions as types](https://youtu.be/IOiZatlZtGU) yet? :P SPJ's [Escape from the ivory tower](https://youtu.be/re96UgMk6GQ) is also one my favourites.
If you define a newtype over the numeric type (e.g.: `Double`) you are concerned with, you can define `REWRITE` rules with these equalities. They aren't in the language by default because I'm not sure they're actually _true_ for IEEE floating point numbers.
That seems to be what [fast-math](https://github.com/liyang/fast-math) does. This library is pretty old though--don't know if it still does what it says on the tin.
A simple way to answer this question is to use [https://haskell.godbolt.org/](https://haskell.godbolt.org/) and compare 1/x \* y and y/x in a Double -&gt; Double -&gt; Double function. It yields very different assembly code (74 lines against 30ish), so apparently GHC is not making those optimizations for you.
In general those floating point manipulations aren't safe because they will change the results. 
fast-math also has some correctness issues/issues with non-terminating typechecking (ie rules do not terminate in some cases). Other than a few problem RULES, it's generally good
In general you shouldn't expect the native code gen to do even really trivial strength reduction transformations (like `(* 2)` to `unsafeShiftL 1`). The LLVM backend does these sorts of things. I don't know about the specific case here, but you can try it out.
I very much second these two
Simplicity - this one is being wired into the compiler in some places. You can define a van Laarhoven one on top of it and compose that instead. Think of this as the compiler building blocks rather than the end user API. 
I searched Hackage and found almost nothing defining custom instances so hopefully not a big breaking change. 
Not without the recordings, the material was not made to be self-sufficient. I'm working on getting some recordings released, however! The goal of those lessons was to be good for beginners, by putting a lot of emphasis on writing code (see the codelabs): each session was three hours long: one hour of slides, one hour and a half of exercises, and room for questions. :) But what made it work was the fact that it was interactive; I don't know how well it will work with just a recording.
Crossposting from pcj: yeah, I've always regretted not using Nightmare Moon vs Luna instead: corrupted vs pure. But I thought this was understandable by people who don't know the characters, and it would only bother people that do. \^\^
I understand the need for simplicity, but we do already have Traversable in base, and `traverse :: (a -&gt; f b) -&gt; (t a -&gt; f (t b)) ` is arguably even *more* complicated already than `hasField :: (a -&gt; f a) -&gt; (b -&gt; f b)`. But then again, `traverse` is meant for the end user, and `hasFied` is not. Still, such a simple change (or addition?) would make `hasField` as end-user robust as `traverse`. If it's issues with low-level implementation, then maybe base could also export `traverseField` (in the same vein as `traverse`) as a really convenient end user wraparound that shouldn't require an entire library import.
I wonder if this meme originate from lambda calculus, which actually does encode values as functions?
This one about liquid haskell https://skillsmatter.com/skillscasts/13115-liquid-haskell-theorem-proving-for-all
And `Traversable` is tricky. Ryan wrote about it, and GND in http://ryanglscott.github.io/2018/06/22/quantifiedconstraints-and-the-trouble-with-traversable/ I think it's relevant to `HasField`. Unfortunately GND works only with the _last_ parameter of MPTC, so this isn't as pretty as it could be: (based on https://stackoverflow.com/questions/33245788/using-xgeneralizednewtypederiving-with-xmultiparamtypeclasses) -- I tried this in GHC-8.4.4 {-# LANGUAGE DataKinds, TypeApplications, StandaloneDeriving #-} {-# LANGUAGE FlexibleInstances, FlexibleContexts, FunctionalDependencies #-} {-# LANGUAGE AllowAmbiguousTypes, PolyKinds #-} import GHC.Records import Data.Coerce -- records: easy. data BikePerson = BikePerson { name :: String , bikesOwned :: Int } oleg = BikePerson "Oleg" 3 -- | &gt;&gt;&gt; bo -- 3 bo = getField @"bikesOwned" oleg -- newtype wrappers newtype Anonymous = Anonymous BikePerson someone = Anonymous oleg -- GND unfortunately doesn't work as it looks at the last argument of the class :( -- but we can still coerce the implementation. -- instance HasField "bikesOwned" Anonymous Int where getField = coerce (getField @"bikesOwned" :: BikePerson -&gt; Int) -- and we can write functions like: foo :: HasField "bikesOwned" x Int =&gt; x -&gt; Int foo = succ . getField @"bikesOwned" -- | &gt;&gt;&gt; ex1 -- 4 ex1 = foo oleg -- | &gt;&gt;&gt; ex2 -- 4 ex2 = foo someone However consider this variant: {-# LANGUAGE PolyKinds, InstanceSigs #-} class HasFieldVL (x :: k) r a | r -&gt; a where hasFieldVL :: Functor f =&gt; (a -&gt; f a) -&gt; (r -&gt; f r) -- Let's imagine this is autogenerated. instance HasFieldVL "bikesOwned" BikePerson Int where hasFieldVL f s = (\x -&gt; s { bikesOwned = x }) &lt;$&gt; f (bikesOwned s) -- We get nasty representation mismatch. Fun ends here. -- -- ‚Ä¢ Couldn't match representation of type ‚Äòf0 BikePerson‚Äô -- with that of ‚Äòf Anonymous‚Äô instance HasFieldVL "bikesOwned" Anonymous Int where hasFieldVL :: Functor f =&gt; (Int -&gt; f Int) -&gt; Anonymous -&gt; f Anonymous hasFieldVL = coerce (hasFieldVL @"bikesOwned" @BikePerson) This limits how much we can play with a new feature.
Is it actually a valid "optimization" for Doubles? 
`(*2)` and `unsafeShiftL 1` are different, no? 
What if someone is a novice Haskeller but an expert at MLP? Asking for a friend.
I really like this talk by George Wilson about contravariant family of functors! This talk gives a very nice intuition about these particular typeclasses :) * [YOW! Lambda Jam 2018 - George Wilson - Contravariant Functors: The Other Side of the Coin](https://www.youtube.com/watch?v=IJ_bVVsQhvc)
I also thought about this. Shouldn‚Äòt 1/x just change the sign of the exponent? I don‚Äôt really see the issue of (1/x)*y vs y/x. Even for x=0 there‚Äôs the same behavior. But with floats I am always unsure.
I can‚Äôt help but feel as though in the long run improvements to records are going to go somewhat to waste, except perhaps as a form of research into what features are important. There are a huge number of extensions and classes and pieces of syntax that revolve around records, and even with this included you still will not be able to do half of what you can do with proper rows/products/sums. Things like taking a subset of a record to create a new record of a ‚Äúsmaller‚Äù type or enumerating the field names of a record or mapping a sufficiently polymorphic function over a record or combining a record and a sum (`Product (map (-&gt; a) r) -&gt; Sum r -&gt; a`), should all be fairly easy with proper first class rows, but impossible with current Haskell. Not only will there be a huge amount more functionality that can be implemented, but you can remove a whole bunch of extensions and pieces of syntax and typeclasses, and replace it with just one powerful concept.
It doesn‚Äôt change the sign of the exponent, it has to do a lot more than that. 
Can you show me a reasonably fast GHCJS website? The few GHCJS websites I visited were somewhat slow to respond. &gt; I can see cases where you'd want to go a more traditional route For web applications that are not nearly as complex as facebook, GHCJS could be arguably used without much problem. But, using GHCJS with electron is painful. Passing a GHCJS callback with 5~20 arguments to electron API was not possible 2 years ago. `fltkhs` seems better than GHCJS route for native UI.
rather (``unsafeShiftL`` 1)
This works fine, so the issue is elsewhere in your code: import Data.List (intersperse) import qualified Data.ByteString as BS main = print $ intersperse ' ' "hello, world"
Very odd, I'll add the code later. It's character for character the code from the Final Project of Haskellbook
The native code-gen doesn't even reduce strict repeated subexpressions in my experience.
Could there be a caching issue somewhere? We had a similar issue recently which was resolved by deleting a folder under ~/.ghc The problem seemed to occur due to clashes between ghc(i) installed in a couple of different ways. 
Could there be another, non-qualified import of Data.ByteString?
These are not conference talks, but some youtube videos I keep going back to. * [Wrangling Monad Transformer Stacks](https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwimo-y515TgAhWBIDQIHWHCARIQtwIwAXoECAkQAQ&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D8t8fjkISjus&amp;usg=AOvVaw1ezjv7hFHBOrJIrZBHTdZc) * [Monad Transformers](https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjshKjU15TgAhVxCjQIHWkhDHAQwqsBMAJ6BAgGEAc&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DpzouxmWiemg&amp;usg=AOvVaw3JRyjTOQ0373qC1Z3bFPOH) * [https://www.reddit.com/r/haskell/comments/66xqro/putting\_lenses\_to\_work\_talk\_and\_slides/](https://www.reddit.com/r/haskell/comments/66xqro/putting_lenses_to_work_talk_and_slides/) And my other favorite: &amp;#x200B; [https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation](https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation) &amp;#x200B;
\&gt; Shouldn‚Äòt 1/x just change the sign of the exponent &amp;#x200B; Only if you knew that x was exactly some power of 2. That's because normalized IEEE floating point numbers are represented as a binary significand \* 2\^exponent. I will explain. &amp;#x200B; Let's look more closely at the significand. We will use single precision IEEE floating point because there are fewer bits so it's easier to use in our example. &amp;#x200B; A normalized significand (including the "hidden bit") is a binary number of the form: 1.sssssssssssssssssssssss. That is, the integer value 1 followed by 23 fractional bits. The first 's' bit represents 1/2. The last 's' bit represents 1/(2\^23). &amp;#x200B; Examples: The binary significand 1.10000000000000000000000 represents 1.5. The binary significand 1.11000000000000000000000 represents 1.75 The binary significand 1.01000000000000000000000 represents 1.25 ...you get the idea &amp;#x200B; So a normalized significand can represent many values in the range \[1, 2 - 2\^(-23)\], or \[1, 1.99999988079071044921875\]. &amp;#x200B; Note that a normalized significand obeys: 1 &lt;= significand &lt; 2. This is important later. &amp;#x200B; Returning to 1/x Now we know that x is represented by (significand \* 2\^exponent) So 1/x, when x is an IEEE floating point number, is equal to 1/(significand \* 2\^exponent) When significand is exactly 1, then as you said, we can just change its sign to compute the reciprocal: 1/x = 1/(1\*2\^exponent) = 1/(2\^exponent) = 2\^-exponent &amp;#x200B; But if significand is not 1, then we cannot just negate the exponent. Let's use s to represent our significand that's not 1. Recall that s must be normalized, and not 1, so s obeys this inequality: A) 1 &lt; s &lt; 2 &amp;#x200B; Now let's compute the reciprocal with some significand, s, that's not 1, and some exponent: B) 1/x = 1/(s\*2\^exponent) = 1/s \* 2\^-exponent &amp;#x200B; Given the inequality of A, we know that: C) 1/2 &lt; 1/s &lt; 1 &amp;#x200B; Thus, 1/s is clearly not normalized, and it must be to form a normalized floating point number (and any nonzero, non-subnormal, finite floating point value \*must\* have a normalized significand). To normalize s, we will have to make it obey the inequality mentioned earlier (1 &lt;= significand &lt; 2), and double it. To keep the represented floating point value essentially unchanged, we will have to subtract 1 from the exponent. In short, the result's significand will be 1/s and the exponent will be negated and subtract 1 from that. &amp;#x200B; result = (2 \* 1/s) \* 2\^(-exponent - 1) &amp;#x200B; And if you want a real example, consider performing 1/x when x = 3: Let's use [https://www.h-schmidt.net/FloatConverter/IEEE754.html](https://www.h-schmidt.net/FloatConverter/IEEE754.html) to help us. 1. Enter 3 in. We see that it's represented as 1.5 \* 2\^1 2. Enter in 1/3 = 0.33...3 (3 is repeating, of course). We see that it's approximated by 1.3333333730697632 \* 2\^-2 3. That's what I predicted: significand is (2 \* 1/1.5 = 2\*0.66....6 = 1.33...3) and exponent is (-1 - 1 = -2) &amp;#x200B; Caveat: we ignored the sign bit above, because it wasn't very interesting to the discussion.
\&gt; I don‚Äôt really see the issue of (1/x)\*y vs y/x &amp;#x200B; No, I don't believe you can make this change, in general, without changing the result. &amp;#x200B; Consider that (1/x)\*y, computed with IEEE floating point (or any finite-space floating point format) is actually: roundToNearestFloat(roundToNearestFloat(1/x) \* y) &amp;#x200B; which will in general be different from: &amp;#x200B; roundToNearestFloat(y/x) &amp;#x200B; And y and x are 'exact' floating point values. That is, when considered as inputs to this question, they are exactly representable as floating point values. &amp;#x200B; Now let me provide a specific example. Using IEEE single precision: &amp;#x200B; y=3 x=3 &amp;#x200B; (1/3) \* 3 = 0.3333333432674407958984375 \* 3 = 1.00000002980232 &amp;#x200B; vs &amp;#x200B; 3/3 =1
Perhaps instead of adding explicit support for first-class rows the best way forward would be to improve the facilities for Generic and type-level computation, to make "indexed" datatypes easier to construct and manipulate (and faster). Enumerating field names, getting a subset of fields, mapping over a record, all of those are possible right now, only inconvenient and slow.
Traverse isn't as wired into the compiler. I have no objection to exporting lensField or similar, or even the two bijections to turn it to/from a lens. That said, does anyone use lens without any flavour of a lens library? In which case I'd prefer to let a million variations bloom than give the answer baked in. 
Are you sure you didn't accidentally write `import Data.ByteString as BS` (notice no `qualified`)? Unintuitively, this will import `Data.ByteString` into the global scope *and* namespace to `BS` as well
This is one of my newest Haskell projects, and it's far along enough to where I'm comfortable sharing here. I always liked having my imports sorted but never liked using tools like brittany or hindent because they touch all your code. `sort-imports` is essentially a Haskell source code formatter that only touches your import statements. It will alphabetize by module name, and also sort the functions you're exposing. I mainly used `megaparsec` (and a little bit of `optparse-applicative` for command-line argument parsing). I think it's a small, practical example of writing a monadic parser. Please let me know if you have any feedback on my code or any suggestions for future features. I'm already thinking of adding optional language pragma sorting.
&gt; I always liked having my imports sorted but never liked using tools like brittany or hindent because they touch all your code What about `stylish-haskell`? You can configure it to only touch your imports and nothing else.
Oh I guess I never realized... oh well. One of the reasons I did this project was to learn, so I've still got that going for me :-)
Even though it‚Äôs a bit out of scope for Floskell, I think it‚Äôd be ok if it‚Äôs sufficiently orthogonal/isolated. What is unclear to me is the scope of this. Especially, how this style would look for non-unary functions and whether there is a vertical version, I.e. what is supposed to happen when we need to add a line break. Does it only apply to function application or are other syntax elements affected as well? What about function argument patterns?
Agreed! I don't think I would ever have learned what a contravariant functor is if it wasn't for George Wilson.
Would it be of any use to make `getField` and `setField` methods with default implementations, rather than standalone functions? I'm guessing they're not because ghc can optimize away the tuple efficiently enough that you wouldn't have any reason to provide custom implementations?
Yes, we will continue to experiment with this, and once we have some more examples, hopefully extract it into an independent module that could possibly be added to \`quickcheck-state-machine\` itself. &amp;#x200B; Note that this does \_not\_ require the mock to implement the full functionality of the system under test. For example, a mock of a DB might just return random data; we just need to make sure that we relax the comparison, ideally by applying a suitable "abstraction function" to the results of functions that translated from the actual result to some more abstract thing, for example merely saying "returned some data").
As far as I understand, the underlying principles of the state machine testing are the same; the main differences stem from the differences between \`QuickCheck\` and \`hedgehog\`. In particular, shrinking works quite different in the two libraries. It would certainly be interesting to see how some of the shrinking techniques in the blog post apply in \`hedgehog\`, but I've not tried.
This proposal is deliberately small, yes. Fundamentally the problem is that the design space for records is a swamp: there are many options with different trade-offs, especially if you want to maintain some level of compatibility with existing Haskell records, and it's hard to gather consensus, specify and implement major changes. See https://github.com/ghc-proposals/ghc-proposals/pull/180 for a proposal taking a rather more ambitious approach.
You can actually do better than `stylish-haskell` in theory ;) Instead of using `megaparsec` (as you did) and `haskell-src-exts` (as `stylish-haskell` currently does) you can try to use GHC source plugins (available only since GHC-8.6.3) to have more robust parsing and `ghc-exactprint` to have robust pretty-printing. There're a lot of Haskell syntax things which can fail manually written parsers very easily if you're not aware of them, for example: 1. `-XCPP` can introduce lines started with `#` in your code. 2. `-XPatternSynonyms` can bring pattern imports like: `import Colog (pattern D)`. 3. Sometimes type-level operators should be exported with explicit namespacing like `import GHC.TypeNats (type (*))` 4. Oh, and don't forget about comments in the imports. We probably don't want to remove them after pretty-printing. Though, I agree that your project is a cool learning project! I see value in having relatively small example with the `megaparsec` parsing library :) This can help beginners to understand the library and parser combinators better!
No, we dropped that feature as a deliberate simplification. Adding it means `HasField` needs two more type parameters, and the rules for when the constraints can be solved get rather more hairy (especially for fields of GADTs or whose types contain type families).
Exactly. `HasField` supports representation hiding because the constraint gets solved only if the corresponding field (selector function) is in scope. Having the field in scope is enough to do updates with current Haskell records, even if the data constructor isn't in scope.
If you're interested in real life usages of contravariant functors, you can also read my blog post about the `co-log` logging library which based on these concepts :) * [co-log: Composable Contravariant Combinatorial Comonadic Configurable Convenient Logging](https://kowainik.github.io/posts/2018-09-25-co-log) I heard from people that this blog post helped them to understand `Contravariant`, `Divisible` and `Decidable`.
How does it compare to gentoo's USE flags and user patches?
The ability to patch packages with NixOS is astounding. You can do far more than just add patch files to packages; you can modify their build processes and source code by changing the build commands *without editing the nixpkgs repo*. USE flags are something that NixOS is perfectly capable of, but it's rarely actually done by NixOS developers, unfortunately. Luckily, NixOS only ever installs the minimum that your system config specifies (along with the absolute basics like systemd and networking), so if you don't configure KDE, it won't be installed. But if you do install a package with optional KDE support, for instance, there's not a universal flag for "Make sure all such packages don't use KDE". Again, this is something NixOS could trivially implement, but it hasn't. However, most such packages are packaged in NixOS so that you can flip a configuration option on specific packages to disable optional library support like KDE. So there's rarely categorical flags, but frequently per-package flags.
The documentation says that the package is for use with databases, but can I for example use it with [`FingerTree`](http://hackage.haskell.org/package/fingertree-0.1.4.2/docs/Data-IntervalMap-FingerTree.html) as well? How would that look?
Not gonna lie, I had to laugh out loud when I imagined you using your phone to get your haskell fix :) So of course I had to try it out right away. If you use the _stacked_ layout it's pretty neat: https://i.imgur.com/mYm8g89.png What is your use case for coding on your phone?
I tried, but folks wanted something simpler. =(
What's curious to me here is that your attempt over at https://github.com/ghc-proposals/ghc-proposals/pull/158#issuecomment-412271564 got 8 upvotes, and thus by *far* got more consensus/support judging from the up/downvotes than any other suggestion/comment in that thread... ;-) 
There are basically no "valid" optimizations for doubles. More-or-less any two mathematically equivalent expressions will result in different values when computed with floating point arithmetic. Basically, if are using IEEE floats, and you are not a world class expert on them, then imho it's best to assume that all bets are off... That said, it is often important to think about these questions when doing numerics. A mathematically identical rewiting can often make the precision of the result significantly better (or worse), or drastically change the range where the result is valid up to a given precision.
&gt;single As many people said I don't think so, but a more disingenuous fact is that these functions produce different assembly code (and in this case I'm pretty sure it should be equal: \`\`\` module Example where import Data.Ratio itShouldWork :: Integer -&gt; Integer -&gt; Rational itShouldWork x y = (1 % x) / (1 % y) butItDoesnt :: Integer -&gt; Integer -&gt; Rational butItDoesnt x y = (y % x) \`\`\` I must have missed something 
You linked to docs for `base` 4.2, which came out when i was getting my first pimple. You're likely not able to even use (compile) that API. That being said, it can compile because the HashTable it makes is empty, so type inference will decide what the element type is.
Most giant orgs seem to work this way. I still get periodic updates for hugs I opened against Firefox and Debian ten years ago.
Several compilers are written in Haskell ([Agda](http://github.com/agda/agda), [Idris](https://github.com/idris-lang/idris-dev), [Elm](https://github.com/elm/compiler)). They're open source and cover a variety of problem domains (parsing, type checking, codegen).
For the same reason this works: foo :: String -&gt; [a] foo s = []
Unbound type variables get `forall`'d: new :: (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable ket val) is new :: forall key val. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable ket val) It's just how the syntax of the language is.
Or use [Aelve guide](https://guide.aelve.com/haskell)..
&gt; That said, does anyone use lens without any flavour of a lens library? I certainly have in the past (for ICFPPC); the van Laarhoven encoding composes better than others and doesn't require anything not in Prelude.
Is this a good rate for London? It seems pretty low for a senior position in a city with such a high cost of living.
Job description doesn't mention it but this sounds like [https://www.habito.com](https://www.habito.com/)
wow, thanks! 
[https://reflex-frp.org/](https://reflex-frp.org/) is fully GHCJS but with pre-rendering.
Using jsaddle makes the JS/Haskell interop *much* less painful than using the FFI. It's slower than FFI but unless your API is part of a CPU bound computation it's fine.
That is a very low rate for a senior Haskell dev located anywhere but especially in London
Wow, I'll have to check the GHC source plugins and `ghc-exactprint` out! I'm definitely not handling your second and third examples, but I think I'm okay on examples 1 and 4. Thanks for the feedback! :-)
So... What do you think about this role: https://functional.works-hub.com/jobs/HaskellEngineer-Mar-2017-feaf3 ?
I learned haskell first and scala second, I think the transition is pretty painless. I used [Programming in Scala](https://www.artima.com/pins1ed/). The first edition is free online. The 3rd edition is up-to-date if you can spend a little money. Off the top of my head the main sticking points were - Functions aren't curried by default, see [partially-applied functions](https://www.artima.com/pins1ed/functions-and-closures.html#8.6) - To my knowledge haskell doesn't have an equivalent to [implicit parameters](https://www.artima.com/pins1ed/implicit-conversions-and-parameters.html) 
[Herbie](http://herbie.uwplse.org/) is a pretty cool tool to help with this problem. It used to have a [GHC plugin](https://github.com/mikeizbicki/HerbiePlugin) to do it for you automatically, but it looks like it hasn't been worked on since 2015.
&gt; - To my knowledge haskell doesn't have an equivalent to [implicit parameters](https://www.artima.com/pins1ed/implicit-conversions-and-parameters.html) GHC Haskell does have implicit parameters, actually - [ImplicitParams](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#extension-ImplicitParams). However, they have some scoping issues that make them a bad idea to use in practice. There is also a library, [reflection](http://hackage.haskell.org/package/reflection), which implements something quite similar to implicit parameters but without the scoping issues that plague the language extension.
even worse?
Thanks, good to know! I'm not an expert, but as I understand it many of the situations where scala uses implicits Haskell covers with classes instead. Like for example scala's `sortBy` has an implicit `Ordering` parameter where haskell would use `instance Ord`.
This is a *terrible* comp offer.
Low or high depends on what your individual needs and expectations are. A handy gauge is to take your current salary and location, and plug it into the new city, London, and see roughly what you'd need to maintain your standard of life. E.g. let's pretend I live in San Diego and make a net salary per month of $7000. [Numbeo reports](https://www.numbeo.com/cost-of-living/compare_cities.jsp?country1=United+States&amp;city1=San+Diego%2C+CA&amp;country2=United+Kingdom&amp;city2=London&amp;amount=7000&amp;displayCurrency=USD): &gt; You would need around 8,174.08$ (6,241.60¬£) in London to maintain the same standard of life that you can have with 7,000.00$ in San Diego, CA (assuming you rent in both cities). This calculation uses our Cost of Living Plus Rent Index to compare cost of living. This assumes net earnings (after income tax). That would mean you'd need at least ¬£74,899.2 *after tax*, which before tax would be about ¬£120,000. I've lived in a few cities and I've found Numbeo's prices, esp. rent and general prices to be accurate.
That‚Äôs basically the exact opposite of what I said and exactly what I think we shouldn‚Äôt do. Filling Haskell with records extensions and Generic/Data etc. improvements and various deriving extensions and so on won‚Äôt give you a cohesive expressive composeable result. Some of what I said can be done by reaching for a specific extension or automagic class if you know what to look for. But some I‚Äôm pretty sure you can‚Äôt do properly. For example getting a subset of the fields of a record and putting them into a new ‚Äúsmaller‚Äù record on the fly does not seem possible. Something like: type FooBarBaz = Record { #foo = Int, #bar = Bool, #baz = String } foobarbaz :: FooBarBaz foobarbaz = { #foo = 1, #bar = True, #baz = ‚Äúfoo‚Äù } foobar :: subset [#foo, #bar] FooBarBaz foobar = subset foobarbaz Something even remotely close to this is impossible in current Haskell. The best you can do would be either two separate data types with overlapping field names + some overlapping field name and record wild card extensions to make it happen, and it would be far less elegant and general and more verbose. Or perhaps you could use higher kindled data + a manual function. Regardless it‚Äôs all way worse than a decent row/record/variant implementation. 
Don't really find Numbeo to give that nice a comparison. I just compared Copenhagen to a bunch of cities, amongst others, London. While rent might get a bit higher in London, eating (your other main cost) is lower than in Denmark. Considering a starting engineer salary is $5.5k/month pre-tax, and that Denmark has one of the higher taxes compared to other countries, I can safely say that you will be able to live very nicely for $9k/month in London, pre-tax. That pay rate would be about what a senior position would go for here, at a company that knows the market rate. 
What currency does your $ refer to?
Yes, I'm continually amazed by these low-ball ranges that some companies try to offer. UK ranges on StackOverflow Careers from example also often seem very low. I wonder how that is working out for these companies.
&gt; GHC Haskell does have implicit parameters, actually Haskell's implicit parameters are name-based and really have little to do with Scala's implicit parameters. The latter are type-based and used for synthesizing terms based on types, a generalization of type classes if you will.
Self-promotion time :) In my library [red-black-record](http://hackage.haskell.org/package/red-black-record) (built on top of [sop-core](http://hackage.haskell.org/package/sop-core)) one [can write something like](http://hackage.haskell.org/package/red-black-record-1.1.0.0/docs/Data-RBR-Examples.html#g:2): &gt;&gt;&gt; :{ let r = addFieldI @"name" "Foo" . addFieldI @"age" 5 . addFieldI @"whatever" 'x' $ unit s = getFieldSubset @(FromList [ '("age",_), '("whatever",_) ]) r in putStrLn (prettyShowRecordI s) :} {age = 5, whatever = 'x'} What are row-types, ultimately? They are a data structure‚Äîsomething like a map‚Äîlifted to the kind level. In the row types proposal, there are debates about how should the structure be. Should the entry for each field be a single type, or something like a stack of them? Both alternatives have pros and cons. Instead of blessing a specific "kind", we could try to create better tools for creating our own kinds. Not that I have a clear idea of what those tools should be...
[Vinyl](http://hackage.haskell.org/package/vinyl) records definitely support [subset operations](http://hackage.haskell.org/package/vinyl-0.11.0/docs/Data-Vinyl-Lens.html#t:RecSubset) along with a whole bunch of other operations. The current type-level machinery for indexing them is a bit hard to read, and (when I last worked with this years ago) getting good runtime performance requires the optimizer to cooperate, but it's very much possible in current Haskell.
Yeah, I can see how Traverse might not be as baked in. w.r.t. using lenses, I think people do all the time without knowing it. Every time you do `traverse . traverse`, you're composing optics. Doing things like "modify this field using an IO action" would be useful for `hasField`/`traverseField`, or "modify this field using an action that might fail". It can also be thrown into a `traverse . hasField . traverse` chain to do nested traversals.
I was offered around that rate for a different (non-haskell) senior position in London and had to decline. In general London and Silicon Valley are very different in terms of compensation. However, when we consider that we could be in a software salary bubble, London (and Europe in general) seems better equipped to handle the fallout as their developers are already lower paid. Like Chris says below just to be able to afford London comfortably you need to make 120 thousand pounds a year. So there is a huge disconnect between what the city requires and what the city pays. The only alternative is to take the job and live "uncomfortably" in a tiny flat with beans-on-toast for dinner. Which people cope with by spending most evenings in a pub.
they are trying to low bell people who will immigrate. talking to some of these companies I got (much) better figures right from the recruiter after telling them the offer is super lowball. on the other hand they wanted me to fly in for an interview on my on expense and after expressing my shock they agreed to pay. uk job market is whack
Well, I‚Äôm not saying you *need* that much to live in London, that depends on your needs, I just picked a random monthly figure in dollars. Working it out, San Diego taxing of 120k yields about 83k after California state and federal income tax, which adds up to roughly 7000/mo net. I think that the average salary in San Diego for a senior dev is 108k, so 120k is apparently above average. 
`-XCPP` is very problematic, as far as I know `ghc-exactprint` can't handle it by design.
Sounds great !
I should have been more clear. I meant not possible to do with Haskell records, using library level rows/records/variants will of course let you do things like this because that's kind of the point. I just want a fully supported solution for performance and ergonomics and standardization and so on, particularly since as far as I am aware all the best library level solutions use huge amounts of unsafe* and/or partial functions under the covers.
I wouldn't say a generalization, there are things you can do with type classes / guarantees you can have that you can't have with implicit parameters.
While I'm not a senior engineer, my salary falls a bit under that range. Still can save a decent amount after tax and rent....but I don't have a car, limit train usage, make coffee at home, rarely eat out and don't drink alcohol, ha.
I agree in principle that we should be provided building blocks by GHC / the language and then go from there, but I honestly can't think of any building blocks "below" rows, records and variants. GHC needs to guarantee that the building blocks it gives you are type safe, and it must know how to store them in memory and how to optimize them. So I think GHC really does need to be well aware of what a `Record` and a `Variant` is, and therefore also what a `Row` and probably `Array` and some other primitives are. IMO Haskell/GHC should provide: Row :: Type -&gt; Type Record :: Row Type -&gt; Type Variant :: Row Type -&gt; Type Array :: Type -&gt; Type Product :: Array Type -&gt; Type Sum :: Array Type -&gt; Type Where `Row a` is basically just `Map Symbol a` (no stacks IMO, users can implement that on top if you need to, it should be about what converts well into physical structures on disk), and the rest are what you would expect. I could also see a need for `Row (Metadata, Type)` of sorts for things like packed-ness and strictness, but something along the lines of the above would IMO be a great set of building blocks (plus your usual builtin types like integers and floats and so on).
A generalization will often fail to give guarantees that the concrete instance offers. Addition is commutative, a semigroup isn't (necessarily).
In the categorical perspective yes, but when talking about language features when someone says something is "a generalization of X" I expect to be able to do everything X can do and enforce every invariant X can enforce.
It‚Äôs that, plus threading parameters through a list of functions. Which, if I understand correctly, is more often done in Haskell through the use of `Reader`
Yeah. I opened the slides at work.. closed them rather quickly. Maybe I'll check the slides out at home to see if they have good Haskell content, but the images were off-putting.
AFAIK median senior dev salary is 55-60k in the UK
&gt;Using distinct names for type constructors and data constructors is not that difficult: &gt; &gt;`data List a = [] | a : [a]` Don't you mean `data List a = [] | a : List a` ?
I do, thanks!
Hoo boy, don't even get me started on transformers, with StateT/ReaderT/WriterT/MaybeT/ExceptT. Even in base we have Sum/Product/Any/All/First/Last/Endo/Ap. Although on one hand, it makes it easier to use these libraries because you don't have to remember a separate type constructor *and* data constructor. Imagine having to remember if it was Last or MkLast or Lst.
The thing that helped me out the most in the early days of moving from Haskell to Scala-for-work was the realization that the type inference in Scala was very, very different from in Haskell. Once I'd internalized that, I learned to add lots of type signatures at the first sign of trouble so that I could work out what was going on more efficiently. It was pretty frustrating to get errors about "on line 400 - expected type: C, actual type: D" when my real mistake involved the unrelated types A and B on line 100. In my first week the development experience felt a bit like working with JS but with random yelling from the compiler. Some of that might have come from being brand new to Scala, some of it might have come from trying to bring too much of my Haskell mindset with me, and some of it came from trying to use Scalaz without understanding the various idioms that it uses on a medium-to-deep level. I had to do less of that as I internalized some of what Scala was doing on the inference front, but there was always some still to do. Reading the error messages and binary searching towards the correct thing to annotate / working out what to annotate with was pretty good for learning some how the Scala compiler things as well. So yeah - if in doubt / if the compiler seems unreasonably angry at you, try adding type annotations.
I guess it is that difficult then ;)
Ha! I'd have to agree if I typed this from scratch and confused `List` and `[]`, but I actually copied the `data List a = [] | a : [a]` line and modified the LHS, forgetting about the RHS because it's a pile of hand-written HTML. So if it reveals anything, it's that copy-pasting code is error prone, which is universally acknowledged I think =p
We could adopt a consistent naming scheme, e.g. `newtype StateT s m a = MkStateT { runStateT :: a -&gt; m (a, s) }`. I have to admit we'd face some inconsistencies, I keep forgetting if it's `runIdentity` or `getIdentity` or `unIdentity`.
Don't worry, I'm only messing.
I have an updated version that I use when presenting this training at Google in the MTV campus. I have removed most of the pictures, added semantic based highlight to the code examples, and there are other updates. I am planing to release it in a week or so.
This is indeed something people ask about during the training. I will probably correct it some time soon. One thought is to replace this statement with "Functions are everywhere". A suggestion to replace it with "Functions are first class citizens" seems not distinguishing enough from the closure support present today in most mainstream imperative languages.
I'm not sure what you're referring to. I don't know of anything unsafe or partial in vinyl and can't really imagine where they'd be hiding. I can't speak to other record libraries.
Yes.
But all type variables are forall'ed implicitly. What I can't wrap my head around is how this seems to break [parametricity](https://www.schoolofhaskell.com/school/starting-with-haskell/introduction-to-haskell/5-type-classes#parametricity). For example: Prelude&gt; :{ Prelude| f :: a -&gt; b Prelude| f = id Prelude| :} This seems equivalent to `new` in that it uses an unbound type variable `b`. Why does the compiler give an error for `b` here, but not for `val` above?
Doesn't that break [parametricity](https://www.schoolofhaskell.com/school/starting-with-haskell/introduction-to-haskell/5-type-classes#parametricity)?
I'm more interested in why it worked at all, rather than actually using it now. All type variables are universally quantified, but `val` isn't used as a function parameter, similar to the `b` in `foo :: a -&gt; b`. Doesn't that break [parametricity](https://www.schoolofhaskell.com/school/starting-with-haskell/introduction-to-haskell/5-type-classes#parametricity)?
That function doesn't typecheck because f says "give me a value of type a, and i will give you a value of type b". In `f x = x`, the x on the LHS unifies with 'a', so GHC knows x :: a. It expects a value of type b on the RHS, so when you return x, which GHC has determined has type `a`, you get a type mismatch. 
It doesnt break parametricity. Like the link says, the caller of a parametrically polymorphic function gets to determine the type. Imagine a function `dollar :: (a -&gt; b) -&gt; a -&gt; b`. This function says "give me a function from a to b, a value of type a, and i will give you a value of type b". It could be implemented like so: `dollar f x = f x`. The resolved types of `a` and `b` are decided by its caller. Consider: showInt :: Int -&gt; String showInt x = show x myInt :: Int myInt = 5 -- note signature of dollar here moneyBags :: String moneyBags = (dollar :: (Int -&gt; String) -&gt; Int -&gt; String) showInt myInt This behaves mostly the same in the presence of constraints, as can be seen in the first example of that link. 
the reason `f :: a -&gt; [b]; f x = []` typechecks is because f says "give me a value of type a, and i will give you a list of values of type b". Consider that you can't really produce any values of type 'b' here, the function signature doesn't contain such information. All you have to produce your output is a single value of type a. Based on this, the only thing you can do is return the empty list. Something like `f :: Monoid m =&gt; a -&gt; m` gives you the Monoid constraint of `m`, but it doesn't give you a way to produce an `a` given an `m`. The only way we can write such a function then is to have a value of type `m`, e.g. `x :: forall m. Monoid m =&gt; m`, which is just mempty. So the only total implementation of f is `f x = mempty`. Considering this second f, we can write the first f, which i will rename g, as `g :: a -&gt; [b]; g x = f x`. This is because lists ([]) have a Monoid instance that demands nothing from its underlying type (does not constrain it). The Monoid here is List, c for which mempty is known to be [], which means g = f, so this typechecks.
I agree with these points: &gt; Type-level programming that uses -XDataKinds requires ticks to disambiguate the namespace, [which is] unpleasant to read [and write] &gt; ... &gt; Development of future extensions is hindered by [DataKinds ticks, etc.] I disagree with these points: &gt; Teaching novice programmers requires a thorough explanation of the difference between [types and values] I don't consider this a pain point; it's the nature of the language. Even when we present a datatype like `data Identity a = MkIdentity a`, the student needs to understand why `Identity` can't be used as a value, and why `MkIdentity` can't be used as a type. Forcing a shared namespace wouldn't speed this process. It reduces confusion for the newest of new programmers, but that reduced confusion doesn't help them understand the language any better. &gt; Communicating with fellow Haskellers, sometimes I have to clarify if I mean [a type constructor] or [a data constructor] I haven't a problem clarifying this by saying "the X type constructor" or "the X data constructor". &gt; We would be better off if Haskell did not have this style of punning in the first place. Designers of new programming languages, my plea to you: learn from this mistake and resist the temptation of meaningless overloading! Separate namespaces for types and terms normal and okay for languages that have such a distinction. It was not a mistake when Haskell was a Hindley-Milner based system, it wasn't a mistake when it upgraded to System F. Now that we're moving towards a dependently-typed setting (where there is no distinction between types and terms), it is a hindrance, but still not a mistake. Separate type/term namespacing is only a 'mistake' if you plan to make your language dependently typed 'later on'. In which case, the real mistake is not building a dependently typed language to begin with.
I‚Äôm not sure how else you can get O(1) read and/or O(1) mutable write? You need an arbitrarily large heterogenous and contiguous data structure, which Haskell does not properly support. 
&gt; "the difference between [types and values]" I don't consider this a pain point; it's the nature of the language. The fact that things in these different sublanguages look similar is pretty bad. Instead of "you can't write that because terms/types are separate" I have to explain "you can write it but it means something entirely different, because ..." Take the singleton list, `[a]`. One student (no tech background) has been struggling with the concept of a singleton list: "if there's just one thing, that's hardly a list!" I was guiding them in the right direction by explaining the desugaring of `[a]` into `a : []`. It did not help one bit that when it came to `[a]` in types, I had to start my explanation with "this one is actually not a singleton list..." Talking to non-programmers is surprisingly good at revealing the pain points of a language. &gt; I haven't a problem clarifying this by saying "the X type constructor" or "the X data constructor". It gets old pretty quickly. In a technical discussion it is common to refer to a thing many times, and the overhead of saying "the ... type constructor" is 3 words, which is a lot for a simple reference by name. &gt; it is a hindrance, but still not a mistake. You appear to be using a different definition, but for me a mistake is a decision that backfired, even if at the time it seemed reasonable. &gt; In which case, the real mistake is not building a dependently typed language to begin with. Not necessarily. One might start with a non-dependently typed language because it is easier to design and implement (no need to think about totality checking, higher-order unification, etc). On the other hand, eschewing separate namespaces for terms/types is _easy_, just don't add this (mis)feature.
doesn't London have a very high cost of living? also, the ~$105,000 salary for a senior engineer, is that common? or is this company low-balling because the labor market is too small? 
yeah, London has an elevated and increasing cost of living. also, the ~$105,000 salary for a senior engineer, do you know is that is common? or are those two companies just low-balling? (because the labor market is too small?) markets are markets, but no way a senior engineer generate anything less than an order of magnitude more wealth than that. 
&gt; The fact that things in these different sublanguages look similar is pretty bad I'm not convinced. I understand that it can be confusing to beginners, but I don't think that's enough to call it "bad". There's no cognitive penalty after you learn that fundamental lesson (outside of DataKinds code, where we agree on there is). A big part of my position is that I disagree with this: &gt; Talking to non-programmers is surprisingly good at revealing the pain points of a language *Anything* about a language can be a pain-point to a non-programmer, because they're not yet equipped to reason about these kinds of things. I think there are very few elements of language design that can be informed by non-programmers' opinions. You do sometimes have to work harder to teach the language. This is why, for instance, [data61/fp-course](https://github.com/data61/fp-course) starts off with its own list datatype instead of using the built-in one. I don't think imposing this artificial restriction is worth the small amount of explanation it saves.
Is there some way to `show (Nothing :: Maybe (a -&gt; b))`?
I have been teaching Haskell to bachelors for half a decade. I could not agree more with this post. &amp;#x200B; It's a little thing, but when you have a big pile of little things they aren't that little any more.
You can build vinyl-style records that are backed with a vector. I recall actually doing that myself a few years ago, but I can't find the code. You can use `Any` to store any lifted type; since you have access to the type of each element, this can be done safely. (I don't think that's how I did it; I don't recall.) For my use cases, using a vector/array did not prove helpful; the constant factor was much worse than the nested strict pairs that vinyl now uses internally, and it seemed unlikely that records with hundreds of fields would be used. Now that I think about it, I seem to recall also implementing vectors of vinyl records using the usual trick of converting it into a record of vectors. Unfortunately the code would need to be largely rewritten anyway, because vinyl has changed extensively in the intervening half-decade. I don't know if the current representation would allow implementing unboxed vectors like I could before. In the end I satisfied myself that the O(n) access and modification was not a significant runtime cost, and that in a variety of applications GHC would actually optimize the record away entirely, much like we're used to intermediate lists and tuples being eliminated.
&gt; So we have ‚Äúexceptions‚Äù, which, like anything fun in programming languages, was invented in Lisp in the 60‚Äôs.
Yes. But you need to import orphan instance from `base` for that: ghci&gt; import Text.Show.Functions () ghci&gt; show (Nothing :: Maybe (Bool -&gt; Bool)) "Nothing" ghci&gt; show (Just not :: Maybe (Bool -&gt; Bool)) "Just &lt;function&gt;"
Part 1: https://dkwise.wordpress.com/2019/01/18/fractals-and-monads/
I think one good place to start is by learning about the [interact](https://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#v:interact) function and how to use it to turn pure functions of type `String -&gt; String` (or eg. with [Protolude](https://github.com/sdiehl/protolude), `Text -&gt; Text`) into programs that operate on stdin -&gt; stdout. Write a bunch of those programs to solve real world problems or coding challenge problems. You can learn quite a lot about how to use a powerful subset (the literally and figuratively, "pure" part) of Haskell in ideal conditions that will come in handy later as your consciousness leaves your earthly body behind and ascends to the monadic plane. Here's an example of a coding challenge problem I did recently where the goal was to sum the ascii values of the input, which admittedly plays rather well to the strengths of this approach: import Data.Char main = interact $ show . sum . map ord ...but it does scale surprisingly far beyond this point. You can play with the same functions that you pass to `interact` in ghci or via [ghcid](https://www.parsonsmatt.org/2018/05/19/ghcid_for_the_win.html) but especially if you like to learn by building projects I think `interact` is a good tool to have in your toolbelt.
I have a few pitfalls I keep running into: # 1) forgotten 'case' keyword in a Scala pattern match. When it doesn't compile and I can't see anything wrong with the code, at least 40% of the time it's because I forgot to unpack the result using 'case' *(Haskell)* `case doSomething of` `Result x -&gt; undefined` *(Scala)* `doSomething match {` `case Result x =&gt; ???` `}` # 2) You can 'manually curry' by use more than one pair of parens in a function definition: Instead of declaring: `def client(auth: Auth, req: Request) = ???` Try declaring: `def client(auth: Auth)(req: Request) = ???` &amp;#x200B; Now you can do: `val authedClient = client(auth)` instead of `def authedClient = req =&gt; client(auth, req)` &amp;#x200B; # 3) for-comprehensions are the closest thing to do-notation I use them ALL THE TIME. They are so much nicer than chaining flatMaps together (I am often required to use Java). For-comprehensions are joy. &amp;#x200B; # 4) I recommend the cats library [https://github.com/typelevel/cats](https://github.com/typelevel/cats) for your Functor/Applicative/Monad traits. &amp;#x200B; # 5) Another source of mysterious compile errors - I often want to write something like: `doSomething map foo map bar` But it doesn't work, so I manually add in the parentheses and then it's good. `doSomething map {foo(_)} map {bar(_)}` Don't know why.
I'm not so sure about that. For example, monad comprehensions clearly generalize list comprehensions ‚Äì yet, if you look at a monad comprehension there are strictly less invariants you can rely on. For example, the compiler can't _enforce_ that it won't misbehave due to getting a flawed monad instance that does not respect the monad laws, similarly to Scala not being able to _enforce_ that all type class instances are coherent.
If you sign in you can see even more details about the job, including information about the company.
I start out by teaching `data List a = Nil | Cons a (List a)` and just using lambdas and cases, and then have a whole lesson about syntactic sugar. The amount of it in Haskell is definitely overwhelming and needs gradual introduction. The overloading of names in `[]` is particularly troublesome, in my experience too. 
I have taught Haskell to students for some years. There is some truth to this. On the other hand, I have also programmed a lot in Standard ML (and taught it, for that matter), which does seperate term and type syntax to a greater degrees (lists are e.g. `int list` and pairs are `int * bool`). I have come to prefer the Haskell notation, even though I learned ML first. Not everything has to be optimised for ease-of-learning. However, I can't really argue with the fact that this is troublesome when some of the fancier language extensions are enabled.
I am starting with Haskell (or rather I have been starting for almost a year now). I find it incredibly tough btw. One thing I found out I think is that it becomes slightly easier if you architect your app with Conduits for example (same would apply with Pipes I guess). I think it goes along the line of your recommendation because doing so lets data flow through Conduits and easily apply transformations.
Interesting comments in this thread. I would be *very* interested to see hard evidence of higher base salaries for permanent London senior engineering roles that are not in finance or FAANG! 
&gt; uk job market is whack I think it's probably more likely that it's the US programmer job market that's whack (in a good way, for programmers, while it lasts). I think it's because the salaries in the US are in some kind of bubble due to the Bay Area. Programmers coming from the US, or those that have worked for US companies, relatively speaking, probably have an inflated sense of self worth that puts their pay grade on par with doctors. Salaries for programmers in the UK are more in line with what you see in the rest of Europe: https://www.daxx.com/uploads/average-software-developer-salary-world20181116.png But compared to the rest of the UK, 70-85K for what looks like basically backend work and nothing special apart from "Haskell" is almost entirely inflated from the usual 40-50 up to 70-85 just because it's in London. If you have the option to choose companies that pay more, e.g. US ones, or international ones like Google or Facebook, then you can negotiate an inflated salary and maybe get away with it. But otherwise, the job market is what it is.
You know, `runST` kind of looks like `ruST`. Interpret this comment however you please.
&gt; Interpret this comment however you please Running it as code in the rust repl [gives some interesting results](https://github.com/rust-lang/rfcs/issues/655)
I don't understand why you need `ST` for O(n) hashable `nub` at all. Why not exploit Haskell's lazy data structures and fold the list into an immutable `Array`?
I'd rather "exceptions" than `if err != nil { return err }` any day.
Sorry if this doesn't quite fit /r/haskell, but I find this to be a place of more wide interest than the language only, and always found it to be a welcoming place for everyone interested in Curry-Howard correspondence. If you disagree -- feel free to downvote :)
I'd be surprised if it's not better to just [use this nub](https://hackage.haskell.org/package/discrimination-0.3/docs/Data-Discrimination.html#v:nub). Plus, hashing guarantees that you have to touch all of the data even if it really isn't necessary, and it has the possibility of collision.
I made simple clockings and with Int's it was kinda hard to get it faster (actually wasn't able to make any improvement, maybe made something badly), but with a list of Text-words (almost 75k with 5,5k uniq words), the st-array based nub took about 5% of the time taken by Data.List.nub. &amp;#x200B; A little change &amp;#x200B; &gt;if j \`elem\` current &gt; &gt;then pure () &gt; &gt;else writeArray arr index (j : current) &amp;#x200B; made it even roughly 10% faster than in the blog-vrsn. &amp;#x200B; Thus thank you for shareing this!
Could you explain your suggested approach a bit more please?
As u/dnkndnts suggests, how does it compare to Data.Discrimination.nub ? 
That's what `Either` abstracts over.
You keep linking to an article about parametricity, but I don't think you understand what the article is saying. I can't explain parametricity better than the article, but I can try to fix your misunderstanding if I can figure out what that misunderstanding is. In order to do so, could you please explain, in your own words, what you think parametricity is and how it is applies to `f`, and `foo`?
&gt; What is your use case for coding on your phone? I'm an addict :(
The `Data.Discrimination.Grouping` didn't have a `Text`\-instance and had to unpack each, but even so, it was twice slower than st-array based solution, i.e., `Data.Discrimination.nub` took roughly 10% of the time taken by the `Data.List.nub`. (I didn't take a look of what it would mean to write `Text`\-instance.)
We wrote [a similar tool](https://github.com/simspace/simformat) at work, it also breaks long import lists over multiple lines. A slight difference between the two tools is that we choose to put qualified imports at the end rather that interleave them with non-qualified imports alphabetically.
About hashing &amp; the array: it uses 256 element array where each element is a list. Do the collisions matter? &amp;#x200B; Btw, do you happen to have the Text-instance of Data.Discrimination.Grouping somewhere laying around easily shareable? :)
[Paul-Andr√© Melli√®s](https://www.irif.fr/~mellies/) has a nice take on the drinker's paradox. From his point of view, a proof is a game between Prover and Denier, and a difference between classical and intuitionistic logic is that in intuitionistic logic strategies need to be **innocent**, meaning that the strategy has to be independent of what your opponent is doing. That's a rather vague description, but this paradox illustrates it nicely and I think will explain it. In trying to prove \[ \exists x. D(x) \Rightarrow \forall y. D(y), \], the Prover has to justify the existence of an *x* which satisfies the formula \[ D(x) \Rightarrow \forall y. D(y) \]. Classically, the proof proceeds by the Prover introducing a certain witness *x*&lt;sub&gt;0&lt;/sub&gt;, to which the Denier responds with another witness *y*&lt;sub&gt;0&lt;/sub&gt;. For this witness to imperil the proof strategy of the Prover, we would need that *y*&lt;sub&gt;0&lt;/sub&gt; is not drinking, but *x*&lt;sub&gt;0&lt;/sub&gt; is. At this point however, in classical logic, the Prover is allowed to backtrack on her choice of witness by picking *x*&lt;sub&gt;1&lt;/sub&gt; = *y*&lt;sub&gt;0&lt;/sub&gt;. Knowing that either \[ D(x_0) \Rightarrow D(y_0) \] or \[ D(y_0) \Rightarrow \forall y. D(y) \] must hold, the Prover succeeds. To frame this more precisely: in sequent calculus, *y*&lt;sub&gt;0&lt;/sub&gt; is introduced using the introduction rule for ‚àÄ: we establish a formula of the form \[ \forall y. P(y) \] by establishing \[ P(y_0) \] for a free variable *y*&lt;sub&gt;0&lt;/sub&gt; (to be thought of as a "general witness" introduced by the Denier). This witness is manipulated by the Prover: as noted above we tautologically know \[ D(y_0) \vee D(y_0) \Rightarrow forall y. D(y), \] a fact which the Prover then uses by introducing an existential: \[ D(y_0) \vee \exists x. D(x) \Rightarrow forall y. D(y). \] The universal quantification then happens, resulting in \[ \forall y. D(y) \vee \exists x. D(x) \Rightarrow \forall y. D(y), \] essentially concluding the proof. Note that in the proof, the existential step happens before the universal step, as the Denier is using the witness *y*&lt;sub&gt;0&lt;/sub&gt; ‚Äì which is introduced for the purpose of the universal quantification that happens later ‚Äì to perform an existential quantification. This makes sense: after all, we must make use of *y*&lt;sub&gt;0&lt;/sub&gt; while it is still around, before it is captured by ‚àÄ. This proof strategy is inherently non-constructive, because it relies on information that the Denier provides during the course of the proof; it doesn't end up conctructing a specific *x* that satisfies \[ D(x) \Rightarrow \forall y. D(y) \], rather it establishes that one must exist through a back and forth argument with the Denier. This strategy isn't available in intuitionistic logic, where innocence of the strategies means that the Prover is not allowed to rely on information that the Denier reveals during the course of a proof. Indeed, in intuitionistic logic, a proof of \[ \exists x. D(x) \Rightarrow \forall y. D(y) \] consists in a specific witness, not a general argument establishing its existence. 
&gt; if there's just one thing, that's hardly a list! Maybe try defining the following ridiculous types, writing a few functions with them, and then demonstrating how much simpler those definitions become when we use `[a]`? data RealList a = LastTwo a a | Cons a (RealList a) data ZeroOrMore a = Zero | One a | TwoOrMore (RealList a) I say they're ridiculous, but if we want to make illegal states underrepresentable, `RealList` is sometimes useful, e.g. yesterday I needed to produce a validation error which lists all the fields which conflict with each other. It doesn't make sense for a single field to conflict with itself.
I think you can just use `nubWith unpack` or make an instance of `Grouping` similarly by using contramap unpack. There may be a smarter way, but that's what I'd do.
&gt; I keep forgetting if it's `runIdentity` or `getIdentity` or `unIdentity` Me too! In my own code, I use the following convention: `unFoo` unwraps the `Foo` newtype, used when implementing functions which need to look at the implementation details, e.g. instances. `runFoo` is the intended, public way to eliminate a constructor. I never use `getFoo`. For example: newtype Parser a = Parser { unParser :: StateT String [] a } runParser :: Parser a -&gt; String -&gt; Maybe a runParser p s = case runStateT (unParser p) of [(a, "")] -&gt; Just a _ -&gt; Nothing This way, I don't have to guess, it's always `runFoo`, unless I'm implementing `Foo`, in which case I can see that it's `unFoo` right there in the file I'm editing. Or that it's `runFoo`; sometimes the intended public way to eliminate a newtype is to unwrap it, in which case in call the unwrapper `runFoo`.
[removed]
&gt; I understand that it can be confusing to beginners, but I don't think that's enough to call it "bad". I should clarify that I meant "pretty bad when it comes to teaching beginners", since that was the point that we were discussing. &gt; There's no cognitive penalty after you learn that fundamental lesson You operate under the assumption that once a thing explained/learned, it has no lasting cognitive burden. In reality it takes time for the required knowledge to settle down (neural pathways take time and repetition to develop, I guess?) A person may *know* the difference between `[a] :: _` and `_ :: [a]` in theory, and yet continue to confuse them in practice until it becomes their habit to mentally parse terms and types differently, which is not even a useful habit in the grand scheme of things. &gt; Anything about a language can be a pain-point to a non-programmer, because they're not yet equipped to reason about these kinds of things. True, and I'd rather have them focus on things that are actually important, not a name resolution quirk. &gt; I think there are very few elements of language design that can be informed by non-programmers' opinions. This is not a matter of opinion, I don't have to ask a beginner what they think of a language feature to see what effect it has on their learning process. All I see is a non-essential (mis)feature that creates a barrier for learning. &gt; You do sometimes have to work harder to teach the language. If it was a matter of me working harder to teach, I'd live with that! It is, however, also a matter of the student working harder to learn, and I'd like to eliminate as much friction in this process as possible. &gt; starts off with its own list datatype instead of using the built-in one. Great evidence that the built-in type is unfit for teaching. &gt; imposing this artificial restriction is worth the small amount of explanation it saves Reduced complexity is not an "artificial restriction". Having two namespaces instead of one is artificial complexity for the sake of a syntactic pun, give me a break.
[removed]
Thanks, didn't get the contramap unpack right away, but there was a function 'hashing' and results seem to be very good (in this simple setting, see below). Thanks again! (I'll probably replace some of my nub's now.)
[removed]
I'd rather a function returning \`IO (Handle, Maybe String)\` than a function returning \`IO Handle\`, which doesn't even give the compiler the chance to warn me to handle the extremely common situation where the file I'm attempting to open doesn't actually exist.
 l :: forall a. [] a l = [] type Destroy a = a -&gt; () d :: forall a. Destroy a d = \_ -&gt; () type Endo a = a -&gt; a e :: forall a. Endo a e = id newtype Const a b = Const { getConst :: a } c :: forall x. x -&gt; forall a. (Const x) a c x = Const x The problem in your `f` is not "`b` is universally quantified" (and please don't call it "unbound"‚Äîit *isn't* unbound!). The problem is "I need a `b` and I don't have one." But, in general, not all `f`s require a `b` to make an `f b`. data [b] = [] | (:) b [b] Declares [] :: forall b. [b] by fiat. You don't need any `b`s to make a list of `b`s. You also don't need any `b`s to make a `Const x b`, only an `x`.
Here, I'll do you one better, and avoid giving you a bogus handle on failure openFile' :: FilePath -&gt; ReadMode -&gt; IO (Either IOException Handle) openFile' path mode = try $ openFile path mode
[removed]
As an aside, do you know if there are some benchmarks for the discrimination package? I haven't found any yet.
I was thinking something like this. ``` bucket :: Int bucket = 1024 myHash :: Hashable a =&gt; a -&gt; Int myHash = (`mod` bucket) . hash addToListAndNub :: Eq a =&gt; a -&gt; [a] -&gt; [a] addToListAndNub x xs | x `notElem` xs = x : xs | otherwise = xs addToHashListAndNub :: (Eq a, Hashable a) =&gt; a -&gt; Data.Array.Array Int [a] -&gt; Data.Array.Array Int [a] addToHashListAndNub a as = accum (flip addToListAndNub) as [(myHash a, a)] empty :: Array Int [a] empty = listArray (0, bucket - 1) (replicate bucket []) hashNub :: (Foldable t, Eq a, Hashable a) =&gt; t a -&gt; [a] hashNub = concat . elems . foldr addToHashListAndNub empty Œª&gt; hashNub [9, 0, 2, 1, 0] [0,1,2,9] Œª&gt; hashNub ["hello", "world", "world", "hello", "hello", "world", "hello", "!"] ["world","hello","!"] ```
https://github.com/ekmett/discrimination/blob/master/benchmarks ? You can run them with `cabal new-bench` from the project root.
That's interesting, thanks for sharing. I'll have to check out how your tool does things compared to mine. Funny, I know someone that works at Simspace! Tell Peter Becich I said hi :-)
and do not overload the tuple syntax either; (x, y) should be a pair of values and not a type of pairs; for type of pairs x \* y is better; even better to choose different parenthesis for tuples, as () are already taken by grouping it is a general user interface design rule to not use the same presentation for multiple meaning
Now if that was in base, and the throwing `openFile` was in some package, maybe called `ExceptionalIO`, I'd be a very happy camper. Except I expect nobody would bother to write `ExceptionalIO`, as even in Rust, a languaged much less focused on type safety than Haskell, nobody's bothered to write a package that replaces the sum-type-returning file IO functions with type-unsafe functions that panic on failure. Even in Go people prefer `if err != nil { return err }` to using a stdlib wrapper than panics on failure.
Oops, my bad. Those don't seem comprehensive but it's a good start. Thanks!
Data.List.nub is O (n^2). It's terrible for anything more than a handful of items. nubOrd is a lot better at (n*log(n)) and it's usually not hard to come up with an Ord instance.
I've recently gotten into the habit of using `IO (Either MyErr a)` instead of an `IO a` that can throw exceptions, and I've realized that I like it a lot more. I've done this in [sockets](http://hackage.haskell.org/package/sockets-0.1.0.0/docs/Socket-Datagram-IPv4-Undestined.html#v:receive), in [ping](http://hackage.haskell.org/package/ping), and in [posix-api](http://hackage.haskell.org/package/posix-api). The nice thing is that it's trivial to write: unhandled :: Exception e =&gt; IO (Either e a) -&gt; IO a unhandled action = action &gt;&gt;= \case Left e -&gt; throwIO e Right a -&gt; pure a And now the throwing behavior can be recovered trivially. But, going the other way, getting the `Either` variant from the throwing variant, is harder, less type-safe, and performs worse.
Instead of backtracking, I prefer to imagine this as "playing infinitely many rounds" of a game. Classical proofs then correspond to winning a game by "winning every round from some point on". You play a game against the Adversary: ‚àÉx means you choose the value of the variable x, ‚àÄy means the Adversary chooses the value of the variable y. In P ‚Üí Q, the Adversary will supply you with a proof of P, and to win the round you will have to construct a proof of Q. So you play the game corresponding to the drinker's formula: ‚àÉx.‚àÄy.P(y) ‚Üí P(x). First, you set x to an arbitrary value, say 1, to which the Adversary answers with a value y, say y = 666, and gives you a proof of P(666). To win the current round, you‚Äôd now have to provide a proof of P(1) - but you don‚Äôt know how to do that, and the given proof of P(666) may not help at all, so you forfeit the round. The next round begins. This time you set x = 666. The Adversary answers with a value y, and gives you a proof of P(y). To win the current round, you have to prove P(666). So you just copy the proof that the Adversary gave in the first round! Thus, if the Adversary ever wins a round, then you will win all subsequent rounds, thus winning the game. Otherwise, the Adversary never wins a round, so you will win all the rounds. In either case, you will win every round from some point on. (That's right, it's still Ouroboros - classical reasoning justifying classical reasoning)
 accum :: Ix i =&gt; (e -&gt; a -&gt; e) -&gt; Array i e -&gt; [(i, a)] -&gt; Array i e accum f arr@(Array l u n _) ies = unsafeAccum f arr [(safeIndex (l,u) n i, e) | (i, e) &lt;- ies] {-# INLINE unsafeAccum #-} unsafeAccum :: Ix i =&gt; (e -&gt; a -&gt; e) -&gt; Array i e -&gt; [(Int, a)] -&gt; Array i e unsafeAccum f arr ies = runST (do STArray l u n marr# &lt;- thawSTArray arr ST (foldr (adjust f marr#) (done l u n marr#) ies)) Two notable things: thawSTArray copies the array pretty sure this is really very slow. I think it's `O(n^2)` - either you scale the bucket count with n or you get to traverse linked lists that scale with n. You could fix this by removing the nested loops so you only use accum without the foldr. But that's only efficient because accum uses ST.
Yes, nice explanation, thanks. I still like 'backtracking', because in the final proof none of this back-and-forth ends up appearing. In your example, it's as if Prover had lucked out and chosen 666 from the get-go. P.S. One of the important points that Melli√®s makes is that we should not favour the point of view of the Prover over that the Denier/Adversary. Putting them on equal footing has allowed him to provide a unifying framework that brings together linear logic and intuitionistic logic by allowing him to decompose implication [; A \Rightarrow B \quad \leftrightarrow B^* \vee A, ;] where * denotes changing perspective by interchanging Prover and Denier. This makes up for the inability to express implication in terms of negation in intuitionistic logic.
So, * When I thought that code I assumed there would be an efficient one-element `add` instead of a multi-element `accum`. Hence the unnecessary `foldr`. * I had no idea `Array` used `ST` internally. You just blew my mind. 
I found it helpful how the article distinguishes the puns using color. From a users perspective, I'd like to see syntax highlighting support for this. Would this be possibly using something like `ghc-syntax-highlighter`?
I found it odd that the haskell mailing list is apparently "Python powered", as per the bottom of [this page](https://mail.haskell.org/mailman/listinfo/haskell). Is there good reason for this, or am I misinterpreting it?
Do you mean if it's USD? If so, yes. If you mean what I converted from: USD 5.5k ‚âà DKK 36.5k, and USD 9k ‚âà DKK 60k ‚âà GBP 7k.
I suspect the people commenting it to be a terrible comp. are used to US salaries and living costs. As someone from EU (Denmark specifically), the salary seems reasonable to me. Am I wrong in this assumption? I can definitely see it being low, if it was Silicon Valley, where the cost of living is *much* higher. I can compare London with Copenhagen, and it honestly isn't that big of a difference, outside of the rent maybe going a bit higher, but Copenhagen is also quite expensive. Even with a 50% higher rent than I currently have, I could still live very comfortably with that salary. And that's ignoring that the tax rate in London/UK is MUCH lower than Denmark.
Yes, there is a good reason. Mailman is a great mailing list manager that has a long history of stability and ease. While there are benefits to "dog fooding", I'm glad the community is more practical than religious about it.
Link to jobs page: https://intelligence.org/get-involved/#careers (h/t /u/ephrion)
[Link to previous discussion about /u/edwardkmett joining MIRI](https://intelligence.org/2018/11/28/miris-newest-recruit-edward-kmett/)
That is a minor problem compared to Haskell's strict lexical namespace separation between the lowercase values and capitalized types and constructors. Most westerners are unaware of it, but most human writing systems don't have cases. A Haskell identifier can't be written in Chinese. 
The rate listed in the posting is about the average starting salary for someone just out of university in Denmark. Regardless of whether or not it is possible to survive on it in London, it's a low offer, even in Denmark. 
You may want to read /u/snoyberg's [Catching All Exceptions](https://www.schoolofhaskell.com/user/snoyberg/general-haskell/exceptions/catching-all-exceptions).
Requires relocation to the Bay Area (Berkeley), unfortunately.
Eh. Monad comprehensions with a simple type annotation become equivalent to list comprehensions. The same cannot he said for implicits. So I still very much disagree. 
Or [awesome Haskell](https://github.com/krispo/awesome-haskell) or [State of the Haskell ecosystem](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md)
I personally am absolutely not ok with O(n) access time for records, we can easily do much better with proper baked in support for rows/records/variants. It‚Äôs not bad in the mean time and I‚Äôm glad it exists, but it is not IMO an acceptable permanent solution. Arrays of Any is basically exactly what I was talking about. There is no way to index into an array of Any‚Äôs and get back a non-Any without an unsafe function. I also can‚Äôt imagine it optimizes as well as proper GHC-supported records would.
Is'nt mailman one of the [plain text offenders](http://plaintextoffenders.com/faq/devs)?
I'd get some books on the subjects that interest you and try to follow/implement the examples and projects they present in those books. For example, I'm interested in compilers and assemblers, so I took the techniques and examples presented in [Jack Crenshaw's "Let's Build a Compiler"](https://compilers.iecc.com/crenshaw/) and implemented them in C and Haskell (the articles implement a compiler in Pascal). I'm not really into AI, but from what I can reckon there are a lot of Python-based tutorials out there; maybe pick some that match your skill level and try to re-write that code in Haskell.
Not an answer, but you might find this thread interesting: https://www.reddit.com/r/Idris/comments/9hneta/why_is_fin_a_new_inductive_type_instead_of_a/
Thanks! I'll find something to read! haskell is good for compilers so I think I will branch out into that.
[removed]
What conversion rate are you running? While I have been comparing to the higher end, let's take the lower-end for arguments sake. 70,000¬£ = 598,807 DKK, which gives you 598,807 / 12 = 49,900DKK/month. That is a high salary in general. A Ms.c. (engineer), which are some of the better payed, out of university[0] will get an average of 38,000 DKK/month. In general I see people starting at around 36,500 DKK/month. E.g. Deloittes starting salary for a fresh graduate is 36,500. A software dev at e.g. Netcompany, which pays very competitively will be around 40,000 DKK/month. For comparison, most consultant companies start you at 36,500 DKK/month (e.g. Deloitte and others), with a few going a of course going higher though. If you state that 50,000 DKK/month is the starting salary in Denmark for a new graduate, then I can tell with 100% certainty that you have no idea what people are payed in Denmark. [0] https://ida.dk/raad-og-karriere/loen/loenstatistikken Addendum: With a salary of 36,500 DKK/month you'll get payed out ~22,000 DKK after taxes. You can easily rent something for 10k, and often find something below. You can also easily, for two people, eat and live for around 3-4k/month, leaving you with 8k leftover for whatever.
thanks!
The refined library is relevant: http://hackage.haskell.org/package/refined-0.3.0.0/docs/Refined.html
Functional languages in general have properties that are useful for compilers, logic, and artificial intelligence. I think any of those topics would have interesting books and tutorials you could translate to Haskell. Good luck!
So does openFile only throw IOException? What if a later version of base changes that? You can't rely on the fact that openFile doesn't throw. Is it worthwhile to try anyway at the risk of making programmers overconfident? I legitimately don't know.
Wrong link? Think you meant to link to this: https://www.reddit.com/r/haskell/comments/a24hw7/miris_newest_recruit_edward_kmett
Yep, hdevtools has `HdevtoolsType`. ghcmod has `:GhcModType` and `:GhcModTypeInsert`, but last time I used it I had to manually patch and compile it. Intero has `:InteroGenericType`.
I've been dabbling with Haskell for a little while now but there's something I'm unsure of: how would one implement a timer that infinitely counts seconds, and prints how many seconds have passed? Like a stopwatch. Like many things in Haskell, it's probably deceptively simple. Many thanks!
I developed this habit way before Haskell when I lost a term paper in college.
But do you commit after every save?!
Use `ghcid`, it will give a justification to your habit and you'll be happy.
I host this meetup once a month, on every second Saturday of the month, at my workplace in Norcross, GA. If you are in the area we'd love to see you.
I thought it was fairly common knowledge that `IOref` isn't thread safe, and to use `MVar` or `TVar` instead.
You can even use IORefs in a thread safe manner if you're careful and make significant use of `atomicModifyIORef`!
I die a little inside everytime someone says "thread safe". `IORef` is in every sense safe. `modify` is not atomic, which not unsafe, and is even perfectly useful in concurrent code.
 flip fix 0 $ \loop secs -&gt; do print secs threadDelay 1000000 f $ i + 1
In what way that habit correlates to Haskell?
Seconded! This event has a great combination of lightning talks, work time, cooperative spirit, knowledgeable people, and a light dose of accountability. These are some of the most productive hours I spend on Haskell every week. There's plenty of space, too. Join us!
Haskell *to* Scala? [You changed it *to* Latrine?](https://www.youtube.com/watch?v=Rt3gcSyL5vg)
:)
why not `forM_ [1..] $ \i -&gt; threadDelay 1000000 &gt;&gt; print i`
You can actually use `HashMap` to have faster `hashNub` function! You can see different `nub`-like functions in `relude`: * http://hackage.haskell.org/package/relude-0.4.0/docs/Relude-Nub.html
`forM_` over list of sequential integers is very slow. I would recommend to use `loop` package instead: * http://hackage.haskell.org/package/loop-0.3.0/docs/Control-Loop.html
Why is it slow? Would this suffer from the same problem? main :: IO () main = foldMap ((&gt;&gt; pause) . print) [0..] where pause = threadDelay 1000000 
Documentation for `loop` package explains why `forM_` _can be slow_. In short: 1. `(+1)` for `Int` is faster than `succ` (comes from `Enum` instance for `Int`) 2. `forM_` doesn't always fuse automatically
Thanks for mentioning that. Do you have a source describing what's happening with `forM_ [1..]`? How bad is it, are we talking about a linear cumulative buildup of space leaks or just a constant slowdown? Because the latter case could be fine for most applications.
I think /u/nh2_ can answer these questions better cause he is the author of the `loop` package :) I've found the link to the following GHC ticket in the package README: * https://ghc.haskell.org/trac/ghc/ticket/8763
Thanks again! Seems like a constant overhead, so it's probably OK if this is not your innermost loop.
`IOref` itself is thread safe.
Instead of simply dropping branches, it seems that we need a function like &gt; winnow :: Error -&gt; Either APIError Password If the two sum types have their shared branches in the same order, I think a Generics-based solution would be relatively easy to write.
This isn't a fully general solution, but can get you pretty far: \`\`\`\` data Error = APIError APIError | UserPasswordIsNotAwesome Password \`\`\`\` &amp;#x200B; Another approach might be an error GADT indexed by a phantom type that signifies whether the error can appear in public-facing contexts.
I'm sure there's certainly room for improvement on both the GHCJS side of things and the Reflex side of things, but it's actually quite good...likely the fastest and most robust (least likely to leak memory) Haskell FRP library. We used it to build [this web fiddle for the Pact smart contract language](https://pact.kadena.io/) and as you can see if you play around, it is quite snappy. There's also [this cochleagram example](https://reflex-frp.org/getstarted/cochleagram) that does real time audio processing and shows that Reflex can handle more compute-intensive things. It gets even better for native mobile apps (via [obelisk](https://github.com/obsidiansystems/obelisk)) where it uses GHC cross compiling.
A coworker of mine suggested The Haskell School of Expression: Learning Functional Programming through Multimedia by Paul Hudak. Is the book worth it? Any problem with it having been published in 2000? I don't know how much the language has changed since then. Does anyone have any other book/project suggestions after having gone through [http://learnyouahaskell.com/chapters](http://learnyouahaskell.com/chapters). Thanks!
Look at https://github.com/UlfNorell/agda-summer-school/tree/OPLSS/exercises/SECD
I have made some experiments and found out that most of the time ghcjs gives js that is quick enough for the tasks you describe. By quick enough I mean that as a user, I wouldn't notice. Please do note that I'm not very experienced on reflex-dom. On my trials with larger tables (a table consisting of 1000 cells, or 10x100 tbl), there is a bit lagging but not very much. * I haven't applied any optimizations yet * the table is such that you can write text into each cell, after the table the events are shown and what user put into the cell and where * and in this trial there is 15 other smaller tables with varying functionalities so that the page contains a number ui-elements The initial loading time was not very good, though, about 40 seconds (on my home network). But again, I have yet to apply the optimizations (and luckily, there are some things to do). &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
GHCJS can be anywhere from 10x to 50x slower than native GHC. For GHC, Reflex is pretty darn fast, so the main bottleneck is GHCJS. In my experience, it takes a bit of tuning to make it fast for particularly large apps, but it's not monumental, and probably on par with the performance difficulties of typical JS frameworks. For small apps there's very little performance troubles. And note that I'm using "small" and "large" mostly to refer to how interactive the app is; something like github is far less interactive than Facebook, and so is likely much easier to implement with Reflex. The best single piece of advice I can give you would be to watch out for excessive dynamic rendering; i.e. make sure you don't redraw large swaths of the page when little parts will do. This just takes a bit of knowhow with the FRP paradigm, but it's never been a serious impairment IMO. The thing that will likely give you the most technical difficulty is lists / tables / etc with dynamic *numbers of entries*. Having to add and remove entries is something that in principle shouldn't be that hard with FRP, but in practice we haven't quite found an intuitive API for it yet.
I agree that the GADT based approach is very tempting, but it's suffering from type-level complication and `deriving` basically goes out the window. Your nested sum type seems appropriate in this simple example, but as you've acknowledged, this is a circumstantial solution. What happens if API error contains something else that we don't want in Error? Then you can define subsets that are common and different, but now you've complicated your API types for totally unrelated concerns (just an example, your derived instances are weird now.) The most important strength of the flexibility in records is the fact that the types are as simple as possible for the modules that they're used in, and the translation functions are completely non-intrusive. In a sense, you're concerned about how these representations are related to each other only while translating from one to the other, and the complication doesn't infect the whole codebase.
Yeah, a generic based solution would be a good start, but it wouldn't be as flexible as what's going on inside the `fromAPI` function.
Good memory semantics is something I really envy about Rust. That's how it's able to accomplish stuff like this. But I've still yet to see something that's both practical w.r.t. to good memory semantics, and fits a really nice theory the way Haskell fits various lambda calculus theories. True linearity isn't practical enough, and Rust's borrow checker is a very ad-hoc concept without a real calculus behind it.
Thanks! I'll try to work through your comment, but would have to spend a bit more time reading thoughtfully. Meanwhile, is that talk by Paul-Andr√© Melli√®s you've mentioned available somewhere?
By the way, since you've mentioned "inherently non-constructive", I decided to remove the "proving constructively" from my post, since usage of LEM [is actually by definition non-constructive](https://ncatlab.org/nlab/show/constructive+mathematics), will replace with "Curry-Howard"
Have a look at https://blog.acolyer.org/2018/01/18/rustbelt-securing-the-foundations-of-the-rust-programming-language/ &gt; \lambda_{Rust} is the formal version of Rust developed by the authors (see ¬ß3), at a level very close to Rust‚Äôs Mid-level Intermediate Representation (MIR). Programs are represented in a continuation passing style and the memory model supports pointer arithmetic. The operational semantics of \lambda_{Rust} are then given by translation into a core lambda calculus equipped with primitive values, pointer arithmetic, and concurrency.
Sorry, I haven't seen any talk by him on this topic. What I've explained is covered in detail in his habilitation thesis [Une √©tude micrologique de la n√©gation](https://www.irif.fr/~mellies//hdr-mellies.pdf) (specifically pp.135-137), which is in French. Unfortunately, I've found his English articles on this topic to be much less readable, as they skip a lot of context and motivation by instead concentrating on the technical aspects. Feel free to ask questions and I'll do my best to answer them!
My bad, I copied the wrong link.
`fromAPI` is for product types. What would be an analogous example for sum types?
Oh man, the cliffhanger!
&gt; It doesn't make sense for a single field to conflict with itself. That's a very tricky constraint though. ÔøºHaskell's type system isn't powerful enough to express that the elements of that `RealList` are different. Maybe the validator just put the same thing twice, so I wouldn't call that correct by construction.
Looking at the upvotes, I'd say there's a single decade of you.
Do you think a haskell webassembly compiler will produce noticeably faster outputs than GHCJS?
I've been lucky enough to get to hang around with Ed and some of the folks from MIRI recently. So far they all seem like really smart folks and really good people. I didn't really know what to make of the goals of MIRI at first, but they're really good at explaining them (and so many other things besides) if you have the time. (I've since read Rationality : from AI to Zombies and Harry Potter and the Methods of Rationality, and read a bit of Less Wrong. The first of which would probably have significantly reduced the amount of time they needed to spend bringing me up to speed on things. The second was awesome but less directly related. The third I'm still working on.) Now, it seems like really important work with some good people. Even if Ed wasn't there, I'd be suggesting that folks who are solid Haskellers and are either familiar with any of the above materials or have some time and an open mind might consider having a chat with MIRI. Although working with Ed would be an the ultimate cherry on top :) 
I'm using variants: https://docs.haskus.org/variant.html
As of yet unclear but there are reasons to be optimistic: [https://webghc.github.io/2019/01/18/state-of-webghc-january-2019.html](https://webghc.github.io/2019/01/18/state-of-webghc-january-2019.html)
Simpler version with no fancy tricks: countSeconds :: IO () countSeconds = go 0 where go :: Int -&gt; IO () go n = do print n threadDelay 1000000 go (n + 1) 
I know that the definition of a Free structure is that is is the left adjoint of a Forgetful functor which forgets that extra structure. For example, a Free Monoid transforms a type `a` in the category of types into a different type `[a]` + a lawful Monoid instance in the category of Monoids, and the corresponding Forgetful functor takes a type `t` + a lawful Monoid instance, and forgets the lawful instance so we're left with `t`. I never use that definition, because I don't have a good intuition for when a functor is adjoint to another. I do, however, have a pretty sophisticated intuition for Free structures, which led me to the following recipe, which in turn leads me to an alternate definition: a Free structure is one which is isomorphic to the one you get using the recipe. My question is: do these two definitions seem equivalent to you? Here is the recipe. Let's say we want to find what the Free Monoid is. Start by looking at the Monoid typeclass: class Monoid a where mempty :: a mappend :: a -&gt; a -&gt; a Next, do the na√Øve thing and define a datatype with one constructor per method. Plus one constructor to embed the type variable, otherwise we'll only be able to construct expressions which are equivalent to `mempty`, such as `mappend (mappend mempty mempty) mempty`. data MonoidExpr a where Embed :: a -&gt; MonoidExpr MEmpty :: MonoidExpr a MAppend :: MonoidExpr a -&gt; MonoidExpr a -&gt; MonoidExpr a Now, `MonoidExpr a` is not our Free Monoid. It's not even a `Monoid`, because the laws say that e.g. `mappend mempty mempty = mempty` but `MAppend MEmpty MEmpty` is not the same value as `MEmpty`. So we need another step in order to make those equal. The next step is to define equivalence classes. `MAppend MEmpty (Embed x)` isn't equal to `Embed x`, but it is equivalent to it. So their equivalence classes, `‚ü¶ MAppend MEmpty (Embed x) ‚üß` and `‚ü¶ Embed x ‚üß`, are equal. Oh, and if `x` and `x'` are equivalent, and `y` and `y'` are equivalent, I also consider `MAppend x y` to be equivalent to `MAppend x' y'`. Now we can define our Monoid instance quite easily: instance Monoid ‚ü¶ a ‚üß where mempty = ‚ü¶ MEmpty ‚üß mappend ‚ü¶ x ‚üß ‚ü¶ y ‚üß = ‚ü¶ MAppend x y ‚üß That is, to compute the `mappend` of two equivalence classes, pick a representative of each, `MAppend` those, and then return the equivalence class to which that expression belongs. This is well-defined, in the sense that it gives the same answer regardless of the representatives I pick. For example, if `x` and `x'` are two representatives of `‚ü¶ x ‚üß`, then `‚ü¶ MAppend x y ‚üß` and `‚ü¶ MAppend x' y ‚üß` are the same equivalence class, because remember, I said that if `x` and `x'` are equivalent, then so are `MAppend x y` and `MAppend x' y`. All right, so we now have a Monoid, which I claim is a Free Monoid (and I won't prove that it is, because my question is basically whether it is already well known that such a construction does construct a Free Monoid). Of course, it's only a mathematical definition, not a Haskell definition; the hard part is to understand those equivalence classes well enough to be able to define a Haskell datatype with one inhabitant per equivalence class. --- Now, the MonoidExpr GADT above of only syntactically valid because all the methods in this typeclasses return `a`s. So one corollary, if my recipe of correct, is that we can only make Free structures for that kind of typeclasses. Via a similar construction, we can only make Cofree structures for typeclasses whose methods have an `a` as one of their inputs, and if some of the methods have multiple such inputs, then there is more than one Cofree construction for this typeclass. Am I reinventing well-known stuff, discovering something novel, or is my intuition just plain wrong? 
Thanks, good to know.
Idris 2.0 is adding Linear Types, so it might be a promising candidate.
One of the biggest things about the borrow checker is it essentially acts like type inference. When I'm writing rust, I can honestly feel like I'm writing Haskell or python or some other higher level GC'd language. Is it perfect? No, there's still quite a learning curve, but it does a remarkable job at figuring out what's fine and what's not. I can't remember the last time I needed to actually use lifetime annotations. I'm fairly certain linear types or other dependently typed constricts won't catch on in a more mainstream fashion until they're as ergonomic to use. Threading linearity around manually is one thing, but if you're threading around linearity, static properties, stricter types, refinement types, proofs, etc, I can't imagine the type signatures would remain readable... (Which brings me to a related question: is there any research into how to denote and track "multiple things" that might be separate in a nice and lightweight manner that scales?)
Always great to hear about `ST`. I wrote [a similar blog post](https://vaibhavsagar.com/blog/2017/05/29/imperative-haskell/) a while ago.
I've sometimes wondered whether memory management by functional scope would work for Haskell, or Ada's Storage Pools perhaps. 
If we ignore laws, `MonoidExpr` is a free monad: data MonoidF x where MemptyF :: MonoidF x MappendF :: x -&gt; x -&gt; MonoidF x type MonoidExpr = Free MonoidF :: * -&gt; * So the general idea is that if you have a class of F-algebras for some functor F (`MonoidF` here): type MagmaF = MonoidF -- "pointed" magmas: binary operation + a special element class Magma a where magma :: MagmaF a -&gt; a -- (MagmaF a -&gt; a) is short for { mempty :: a ; mappend :: a -&gt; a -&gt; a } Then you can construct an instance for the free monad of F: instance Magma (Free MagmaF a) where magma :: MagmaF (Free MagmaF a) -&gt; Free MagmaF a magma = wrap -- https://hackage.haskell.org/package/free-5.1/docs/Control-Monad-Free.html#v:wrap My own intuition of "free things" is that they satisfy a "univeral property": for any function `f :: a -&gt; m`, there is a unique magma/F-algebra morphism `foldMagma f` such that `foldMagma f . embed = f`. It takes some practice to come up with universal properties, but IMO it's quite feasible without knowing anything about category theory. embed :: a -&gt; Free MagmaF a embed = pure foldMagma :: forall a m. Magma m =&gt; (a -&gt; m) -&gt; Free MagmaF a -&gt; m foldMagma = ... -- exercise for the reader I'm also not too familiar with the definition of freeness as adjunctions, but my understanding is that it describes exactly the properties you want out of "free things", only without saying how to construct them. --- In a language with *higher inductive types* you can still follow the same construction if the structure has laws like monoids, without understanding the structure of equivalence classes. Such a language needs to at least express equalities between terms so the class of monoids can be written more accurately: class Monoid a where mempty :: a (&lt;&gt;) :: a -&gt; a -&gt; a leftId :: forall x. mempty &lt;&gt; x = x rightId :: forall x. x &lt;&gt; mempty = x assoc :: forall x y z. (x &lt;&gt; y) &lt;&gt; z = x &lt;&gt; (y &lt;&gt; z) And you can declare the following *higher inductive type*, which is a "data type with equations", its definition is entirely derivable from the `Monoid` class: data FreeMonoid a where Embed :: a -&gt; FreeMonoid a Mempty :: FreeMonoid a (:&lt;&gt;) :: FreeMonoid a -&gt; FreeMonoid a -&gt; FreeMonoid a LeftId :: forall x. Mempty :&lt;&gt; x = x RightId :: forall x. x :&lt;&gt; Mempty = x Assoc :: forall x y z. (x :&lt;&gt; y) :&lt;&gt; z = x :&lt;&gt; (y :&lt;&gt; z) instance Monoid (FreeMonoid a) where mempty = Mempty ... leftId = LeftId ... Although you can construct `(x :&lt;&gt; y) :&lt;&gt; z` and `x :&lt;&gt; (y :&lt;&gt; z)` and pattern-match on those, the language prevents you to produce a result that depends on the associativity, so that `FreeMonoid` is lawful extensionally. An exhaustive pattern-match requires you to not only match on the three data constructors, but also the equality constructors that are part of the type definition. f :: FreeMonoid a -&gt; foo f (Embed a) = ... :: foo f Mempty = ... :: foo f (x :&lt;&gt; y) = ... :: foo -- Proofs that f preserves FreeMonoid equations f (LeftId x) = ... :: f (Mempty :&lt;&gt; x) = f x f (RightId x) = ... :: f (x :&lt;&gt; Mempty) = f x f (Assoc x y z) = ... :: f ((x :&lt;&gt; y) :&lt;&gt; z) = f (x :&lt;&gt; (y :&lt;&gt; z)) You now need to write proofs, which may or may not be a desirable feature of the language.
Tangential. I'm seeing this text banner: &gt; Which among Rust and Haskell programming language is more preferable from a safety point of view? Read the blog to find out how fpco please copy edit this. for example: &gt; Between Rust and Haskell, which programming language offers better safety features? Read the blog to learn more
1. Actually ATS has (nearly?) all of the features you've mentioned and the type signatures do get pretty big (see https://youtu.be/zt0OQb1DBko at 28:09). 2. "How to track and denote multiple things" -- Maybe separation logic is what you're looking for? https://cacm.acm.org/magazines/2019/2/234356-separation-logic/fulltext
Blodwin currently correctly infers linearity for all parameters, you really only need to think about the "correct" linearity when you are designing an API without yet having a practical implementation.
Nice! Is this always possible to do or is it one of those "works most of the time" things?
Good question! At the very least, I'm hoping that if &amp; when Richard is able to get support for proper "pi"-binders into GHC, it will make it significantly easier to think about a closer integration with refinement types. (The absence of those binders is a major stumbling block for refinements...) &amp;#x200B;
You can always make your own monadic typeclass like MonadPrint, which later gets interpreted with MonadIO or your monad stack immediately. Maybe there is a better solution.
I don't know of a library akin to what you're describing, but there are some scrap your boilerplate libraries out there which make it much simpler to write these passes by providing combinators for defining traversals. Examples would be [`syb`](http://hackage.haskell.org/package/syb) or the newer [`uniplate`](http://hackage.haskell.org/package/uniplate) and [`recursion-schemes`](http://hackage.haskell.org/package/recursion-schemes). The one I use most in the compiler project I work on is the `Plated` module in [`lens`](http://hackage.haskell.org/package/lens)
Uggh, what a gross way to introduce this posting (this was the first tweet in the thread that this links into): &amp;#x200B; \&gt; Most people cannot learn to code. Most people cannot learn to code. It isn‚Äôt fair, but‚Äî Maybe some other Earth found a better way to teach, but‚Äî The just-world hypothesis says it should only take diligence, but‚Äî It‚Äôs happier not to think so, but‚Äî Most people cannot learn to code. &amp;#x200B; I certainly would not want to have that person as a boss/coworker. The idea of a "geek gene" is a \_myth\_ (it ended up as #1 on this list at CACM: [https://cacm.acm.org/blogs/blog-cacm/189498-top-10-myths-about-teaching-computer-science/fulltext](https://cacm.acm.org/blogs/blog-cacm/189498-top-10-myths-about-teaching-computer-science/fulltext)), and it's telling that this sentimentality seems to often come from white men who want a "good culture fit" at their companies (the only other thing they said they wanted, aside from Haskell, when asked). What we also know is that people often use this myth as an excuse to dismiss people (usually people who do not fit the "geek" stereotype -- young, white, male, hyper-focused on technology -- i.e., the "culture" they want), discourage women and people of color from learning, etc. 
You can use lenses, more specifically the makeFields equivalent for prisms. [generic-lens](https://hackage.haskell.org/package/generic-lens-1.1.0.0/docs/Data-Generics-Sum-Typed.html) has the AsType class for this and [this blog post](https://www.parsonsmatt.org/2018/11/03/trouble_with_typed_errors.html) has some explanations why this approach might be nice. I am not entirely convinced that I love this approach but it's the nicest one I know of. 
Why don't you put a Logging transformer in AppT, then switch your application functions to use constraints? So: ``` import Control.Monad.Logger (MonadLogger, LoggingT) type AppT = ReaderT Config (LoggingT Handler) getUsers :: (MonadLogger m, SomeOtherConstraint m) =&gt; m [User] ``` 
I don't think there's a proof anywhere, so it's probably just "works most of the time". I can't think of a particular counter-example, but if you find one it might be worth adding to the test suite.
HKD might help: data Error implerr = APIError APIError | ImplError implerr toAPIErrorF :: (implerr -&gt; APIError) -&gt; Error impleerr -&gt; APIError toAPIErrorF _ (APIError e) = e toAPIErrorF f (ImplError x) = f x -- alternate name exposeWith toAPIError :: Error Void -&gt; APIError toAPIError = toAPIErrorF absurd -- alternate name safeExpose fromAPIError :: APIError -&gt; Error implerr fromAPIError = APIError fromImplError :: impleerr -&gt; Error implerr fromImplError = ImplError
Maybe you could make a merge request that add the functions for more parameters?
I tried this and nothing happened, so I entered input mode and got weird highlighted `^S`s in my text. Was this the desired effect? My program doesn't compile anymore.
Metro in London is 400 Euro per months perhaps. Higher taxes means better things in Copenhagen. 
Entry position at google is 120k Euro in the EU
I think your formatting may have broken a little in the second one.
With the javascript ffi you will need to create a wrapper to pack the arguments and pass them on in a single argument. With JSaddle you can do (fun $ \ _ _ [a1, a1, a3, a4] -&gt; liftIO (print (a1, a2, a3, a4))) http://hackage.haskell.org/package/jsaddle-0.9.5.0/docs/Language-Javascript-JSaddle-Object.html#v:fun
Actually `print` will output something uninteresting. This would work better as a test... (fun $ \ _ _ args -&gt; forM_ args $ \a -&gt; valToText a &gt;&gt;= (liftIO . putStrLn . T.unpack))
As the author of the post that /u/ncl__ linked, yes that's my hope :) Performance measurement of the current backend is my current WebGHC task, and will be followed by switching to the LLVM backend to improve the performance.
I looked at \`fun\`. It is just fun :: JSCallAsFunction -&gt; JSCallAsFunction fun = id How do I pass JSM as a callback to javascript?
I suggest that https://github.com/ghcjs/ghcjs/blob/master/doc/foreign-function-interface.md#interruptible is the easiest way to deal with functions that expect callbacks with arbitrarily many arguments.
Just few months early for me. Hopefully, you guys will have a similar position by the time I'm ready. 
I just ran the numbers for `Data.Text.Text` (commented out discrimination) and I get - * /u/gspia's nub is fastest for size &gt;= 100. * For sizes &lt;= 20, the mutation based ones are slower by 3-4x, others are pretty close to each other. * In between, they're all somewhat close. &gt; doesn't require me to write Hashable instances I don't think that this is a valid complaint given that the compiler can derive these for you. The only annoying bit is that you need to import `Hashable` manually (unlike `Ord`) unless you're using an alternate prelude.
FYI using jsaddle rather than JS FFI is good because it will work with non-GHCJS backends. Important for mobile hybrid apps and eventually for WebGHC.
The biggest problem while getting two garbage collected languages to talk to each other is lifetime management. Python's runtime doesn't know anything about Haskell and ditto for Haskell, so anything traversing that boundary has to be a copy. So it's almost like inter-process communication or calling an API. Though if you're willing to write some C code talking to the runtimes directly, you might tighten the integration.
[removed]
Thanks for making the blog about this and for the extensive tests. I started to make similar too, after I tried the discrimination based version on a real world setting in a frequency counting lib, and saw that the results were a bit disappointing. At the same time I realized that the instance for Text by using hashing can couse trouble if used carelessly and probably the unpacking is the way to go, if using discrimination package. (Or add something like in the ST-array -solution.) With `[Int]` inputs the results are similar to your findings. How the cost of comparing elements affects things? I tried with arbIntVectListsOf ‚à∑ Int ‚Üí IO [[Int]] arbIntVectListsOf n = do rnd ‚Üê getStdGen pure $ replicate (10^n) $ take 10 (randomRs ( 1,5::Int) rnd) and with hard to believe results. Maybe the laziness hit's my tests. Does it matter if `nf` is used instead of `whnf`? &amp;#x200B; And one tiny comment about your blog: there is `Data.Containers.ListUtils.nubInt` in module you mentioned. (I found it out when tried to use the nubOrd, my nixos wasn't able to compile the ghc-8.6.2 so I crabbed the source of the module and used it. But this is totally another story...) And the title of nubOrd use missed Containers. In the text the path is ok. &amp;#x200B; &amp;#x200B;
* Will WebGHC not have its own FFI? * Will WebGHC become independent of nix in the future? * It's hard to figure out how to use jsaddle in GHCJS.
Beginner Question While implementing `map` using `foldr`, (as an exercise in the Haskell Book), I came up with the following solution myMap :: (a -&gt; b) -&gt; [a] -&gt; [b] myMap f = foldr (\a b -&gt; f a : b) [] However, I found this point-free solution online myMap f = foldr ( (:) . f ) [] Can anyone kindly explain how the function `(:) . f` (lets call is `foldingFunc`) is able to work with `foldr`? AFAIK, the function we give to `foldr` has to have the signature `a -&gt; b -&gt; b`, so it takes two arguments, but `foldingFunc` is a compose of `f` which takes one argument, and `(:)` which takes two. So by placing `f` at the front of the compose, we are effectively ignoring the second argument. We are sort of piping `a` (each element of the list) through `f`, and then pass its result to `(:)`, so how does `(:)` know to cons it with `b` (the starting value or the accumulator). Another way of I could think about this is `(:)` is partially applied, so it will take its first argument from the output of `f`, and the second argument would be `b`, however, `f` does not take two arguments, it just takes one, which will be `a`, hence we are effectively ignoring `b` at the front of the compose. I am thoroughly confused.
We have no plans to support the JavaScriptFFI extension at this time. Maybe in the future, but jsaddle makes it unnecessary, and it‚Äôs difficult to do with the way webghc‚Äôs runtime works. WebGHC technically already is usable without nix. Just requires some know how on setting it up. I do want to automate this eventually, but right now mix is the best such automation.
Does remote applications from EU accepted?
&gt;So by placing `f` at the front of the compose, we are effectively ignoring the second argument. No, we are not "ignoring" it, we just don't "care about it yet"... This implementation extensively uses [currying](https://en.wikipedia.org/wiki/Currying), which allows us to treat multi-parameter functions as functions returning functions. If you ever thought about Haskell's syntax for function types, there's actually reason behind it - e.g. you can think of type of map function `(a -&gt; b) -&gt; [a] -&gt; [b]` not only as function of two arguments, but as `(a -&gt; b) -&gt; ([a] -&gt; [b])` too - function that takes function on one value and returns function on list of values (`(-&gt;)` operator associates to the right as you can see). And Haskell really treats them like this - try looking up type of `map (\x -&gt; x + 1)` in ghci. Now, let's go back to the original problem. We know that type of `(.)` (composition) is `(b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c` (which is equivalent to `(b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)`), and we are providing `f` (`a -&gt; b`) and `(:)` (`a -&gt; [a] -&gt; [a]`) to it. But type of `(:)` (cons) is actually equivalent to `a -&gt; ([a] -&gt; [a])` (function that takes value and returns function from list to list), and so it fits `b -&gt; c` in `(.)` without any problems. Putting those types together (`(b -&gt; ([b] -&gt; [b])) -&gt; (a -&gt; b) -&gt; (a -&gt; ([b] -&gt; [b]))`), we see that return type of our expression is `a -&gt; ([b] -&gt; [b])` \- and that's really just a function of two arguments, which even fits type of function being provided to `foldr` (`a -&gt; accumulator -&gt; accumulator`)! Quoting my first sentence this time: &gt;No, we are not "ignoring" it, we just don't "care about it yet"... If you think about context of folding function in implementation of `foldr` like this - `((:) . f) value accum` \- then when Haskell evaluates this expression, we could say it "feeds `value` into our function and uses result as function on `accum`" - so `((:) . f) value accum` \~= `((:) (f value)) accum` \- see? We "don't care" about it, because we haven't came to it yet... It probably feels super confusing, but don't really care about it - after some time, you are going to get used to it. And for now, you don't have to use it - Haskell technically works just as well with explicit application through anonymous lambdas - at the end, syntax should be there to make things easier to understand, not the opposite...
**Currying** In mathematics and computer science, currying is the technique of translating the evaluation of a function that takes multiple arguments into evaluating a sequence of functions, each with a single argument. For example, a function that takes two arguments, one from X and one from Y, and produces outputs in Z, by currying is translated into a function that takes a single argument from X and produces as outputs functions from Y to Z. Currying is related to, but not the same as, partial application. Currying is useful in both practical and theoretical settings. In functional programming languages, and many others, it provides a way of automatically managing how arguments are passed to functions and exceptions. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
While I agree with your point about the geek gene myth, I wonder if you're being too quick to judge this person. A few tweets isn't much to go on.
&gt; I don't think that this is a valid complaint given that the compiler can derive these for you. The only annoying bit is that you need to import Hashable manually (unlike Ord) unless you're using an alternate prelude. That's true. I'm somewhat biased as most Haskell I hacking I do is on GHC which has it's own set of circumstances which sometimes makes that approach troublesome. 
&gt; Maybe the laziness hit's my tests. You can use [env](https://hackage.haskell.org/package/criterion-1.5.3.0/docs/Criterion.html#v:env) so the arguments to your functions will be fully evaluated. &gt; Does it matter if `nf` is used instead of `whnf` ? Yes it does. `nf` adds constant overhead to all of the results. Especially for deep lists this is non-trivial. It's usually not a huge deal but it does tend to skew the numbers a bit. The difference becomes obvious when you consider that nub can deal with certain infinite data structures `length . nub $ [1:[1..], 2:[1..]] == 2` but if you try to benchmark that with nf it will loop/run out of memory. Good points on nubInt and the path. I will try to update that in the next days. 
I think needing hashable is a complaint if you need to nub data types you don't own. If they aren't hashable you have to newtype them. Worse, hashable doesn't give you a way to derive *that* generically, because there's no ghash :: Generic a =&gt; Int -&gt; a
The lack of an exposed `Generic a =&gt; Int -&gt; a` has always been a minor head scratcher and annoyance. Is there any valid reason why it couldn't be exported?
Nope, I just think it hasn't been done.
You probably want to implement WebAssembly module FFI which will be compatible with EcmaScript 2015 modules. https://github.com/WebAssembly/proposals/issues/12 tracks the issue.
For what it's worth, I (Haskeller for 12 years, professional Haskeller for 2) think the point-free version is unreadable and your version is great.
The problem is with the runtime system in WebGHC. We‚Äôre trying to rewrite as little as possible so that it can be upstreamed into GHC. So we‚Äôre trying to make the runtime use blocking like it does natively, which requires running the program in a WebWorker. Thus any JS FFI will only have access to the worker context, not the main thread. Not so valuable. Worth noting, the majority of what you‚Äôd need a JS FFI for is large IO driven tasks, where the overhead of jsaddle is negligible in comparison. Now, it is possible to write nonblocking Haskell code, which will be able to run in the main thread. So having a JS FFI is still somewhat valuable. We may also customize the RTS to avoid blocking. So I think it‚Äôs likely we‚Äôll add it eventually, but it‚Äôs just not very important when jsaddle does the job so well. Also, the C FFI does work, and this is an easy way to basically get a JS FFI with a little manual work. You‚Äôd just import a C function and tell the linked that the symbol is allowed to be undefined; the linker will then import it via the wasm module. And you can export a C function from Haskell, and instruct the linker to export the symbol in the wasm module. But again, we‚Äôre running in a worker by default, so unless you do no blocking or we customize the rts for non-blocking, this FFI will not be particularly useful.
Hmm, I didn't think of that. I suppose you could glue a bunch of things together to make do (define a `defaultHashWithSalt` by copying the default definition from the library)...
I suspect Data.Discrimination would start looking good if you had long strings with long common prefixes ... (If someone wanted to test that?) Also I'm not sure that benchmarking on Strings is very useful :X
I'm sorry -- I think I must have clicked on [this link](https://functional.works-hub.com/jobs/HaskellEngineer-Mar-2017-feaf3) elsewhere in this thread and gotten that rate in my head. This is around the starting salary that I had in my mind; between ~35-40k/month, though most often closer to 35 than 40 in my experience.
Salary lol?
Right? Why do companies asking for senior people refuse to include this, even though only the most junior devs are the only folks who don't care?
Great blog-post, really enjoyed reading it! I also like the example, it's a bit absurd if you think of the effort put in. This makes it even more fun üôÇ
I think refinement types are conceptually easier to work with. What I really don't like about dependently-typed languages like Idris is their reliance on data-types instead of constraints (like [https://www.reddit.com/r/Idris/comments/9hneta/why\_is\_fin\_a\_new\_inductive\_type\_instead\_of\_a/](https://www.reddit.com/r/Idris/comments/9hneta/why_is_fin_a_new_inductive_type_instead_of_a/), what lubieowoce mentioned). When I think, I think in refinement types and constraints. Datatypes make everything complicated and non-obvious, in my opinion.
Ahh yes I think I get it now... `foldr` takes `a -&gt; b -&gt; b` as an argument `f` is `a -&gt; b` `(:)` is `b -&gt; [b] -&gt; [b]` The output of `f` (`b`) gets passed to `(:)`, which returns the function `[b] -&gt; [b]`, so it awaits a list as an argument. (so effectively the type of `(:)` here is `b -&gt; ([b] -&gt; [b])`.) Hence, the type of the overall compose function `(:) . f` is `a -&gt; [b] -&gt; [b]`, that is, it takes `a` and returns a function that will take `[b]` (both arguments will be supplied by the `foldr`) and return `[b]`. Thanks for the great explanation! 
Type classes and constraints? If I understand what you‚Äôre asking then I suppose that would be the best way to decouple code
I'm not clear at what level you want to communicate events but you may want to take a look at the following: pipes, zeromq-haskell, hw-kafka-client. 
They look like cool people with no clue about what to do and a LOT of money to expend that comes form somewhere.
Thanks for the suggestion, is it the same as the section Regain purity in this post [https://www.fpcomplete.com/blog/2017/06/readert-design-pattern](https://www.fpcomplete.com/blog/2017/06/readert-design-pattern) ? I think it work but that mean i need to go all in with mtl and the signature will become getUsers :: (MonadIO m, MonadApp m, MonadPrint m ) =&gt; m \[User\]
Oh some how i forgot i can do that. Will give it a try
Yeah, is pure supremacism in plain sight. But it is not the white one.... Disclaimer: I'm not partisan on this. I'm a man of color (green)
Hyper-focus on the problem domain is absolutely a key indicator of a good engineer. You want someone who actually finds it interesting. That's not to say that there aren't people who are smart or dilligent enough to do the job without really caring about it, but that's not particularly common. What specifically about wanting people passionate about the problem domain is racist?
&gt;The thing that will likely give you the most technical difficulty is lists / tables / etc with dynamic numbers of entries. Having to add and remove entries is something that in principle shouldn't be that hard with FRP, but in practice we haven't quite found an intuitive API for it yet. Are you referring to the fact that something like a table backed by an Signal has to be reconstructed every time the Signal changes?
What do you mean by decoupling event sender and receiver? 
Maybe "loose coupling" is the wrong name for my question. For example: In games are different subsystems like input, graphics, audio, physics, networks and whatever else. If I press a key, for example mouse1 to fire a weapon, I would release an event which gets caught by different subsystems. The graphics subsystem animates my weapon-fire, audio plays a sound, and network sends an event to the server that the user wants to fire the weapon. How can I decouple these subsystems from each other? In OOP one would use something like a Message-Bus.
I'm glad it helped :)
When it comes to data flow in general, laziness gives you a lot of decoupling for free. For example, you can produce an infinite list, or update an entire gigantic data structure at once, and the consumer of that data will determine when that happens. Laziness can of course lead to space leaks and other difficulties, but it's a powerful tool when controlled properly. Reactive programming similarly produces decoupling in that and consumers see a potentially infinite stream of events without having to explicitly control the stream, and producers need not care if there even is a consumer. Any flow control between producer and consumer is handled by the reactive framework plumbing, rather than a consumer having to explicitly poll for events, nor having to fire off handlers on the producer end. That is of course just one kind of decoupling. Between modules, you get decoupling by targeting a generic interface, for example accepting any Monoid of Widgets rather than hardwiring a list of widgets. Or accepting a MonadIO typeclass instead of the concrete IO monad. This sort of thing isn't really unique to Haskell -- any OO language with interfaces can do it -- but Haskell does take generic interfaces to a deeper level than most.
Whenever Ihad that problem I've reached ofor the broadcast-chan package for in process communication and rabbitmq when I need to distribute messages across servers.
Right. VDOM is one way to solve this, but it's pretty inefficient compared to just "don't do that," which is usually not too hard with reflex because we have behaviors/dynamics (unlike elm). The VDOM algorithm is much slower than just doing the right thing yourself. VDOM is also an extremely error prone approach; it's usually solvable, but if the implementation isn't extremely careful with a lot of knowledge of how to precisely control every kind of element, it can break pretty basic UI elements. Places like Facebook may have the time to solve this, but the dev team behind reflex isn't Facebook. Finally, by not using a VDOM, you can get much lower level control of the DOM when you want to with Reflex. This isn't often necessary, but it's critical when Reflex hasn't considered your particular use case. It's generally far more difficult to work around the absence of a feature in VDOM.
You've provided a number of examples of decoupling techniques that are not similar to each other and then asked for a general solution to all of them. I don't think this is a productive approach. Designing to an interface does not provide the same kind of decoupling as a message bus and you shouldn't expect one FP technique to cover both any more than you expect that of OOP techniques. (As an aside, a message bus is not specifically an OOP concept.) I think this question as asked is difficult to answer because of this ambiguity. I think you would benefit from examining more carefully what sorts of decoupling each of these techniques provide so that you can look for their equivalents in FP.
You are absolutely right. I edited my question to be more specific.
Another AI fever. The last AI fever that I remember was when the first multicore CPU's appeared. There was one before when P2P music and video sharing made disk and RAM memory prices plummeted. This is the cloud computing AI fever. There is something that does not change in al these AI fevers: In all it was said that the machines would dominate in a few years.
&gt; Also I'm not sure that benchmarking on Strings is very useful :X It's useful enough. If you care about a particular data type obviously benchmark that one. But I don't expect a huge difference between String, [Int] or [MyShallowDataType].
I heard that he is working in a language with 600 classes to start working with fields. I don't know If working on that is worth it in exchange for having a luxury apartment in the Bay Area.
A project I've been working on involves a system of stateful components in a hierarchy that respond to and generate events. Based on my experience, this is how I would suggest implementing such a system in Haskell. Each component has its own module, and each event it responds to has a function for handling that event. The functions use a monad that handles the state (typically a record) and context. Output events are wrapped up in a datatype and returned in a list. All of this is wrapped in the RWS monad: reader for context, writer for output events, and state for state. I would provide a type wrapper for the particular instantiation of RWS with a runner for it. A sketchy example of a Foo component (that uses a subcomponent called Bar): data FooContext = FooContext { limit :: Int } data FooState = FooState { counter :: Int, bar :: BarState } data FooEvent = SendMessage Message | ProtocolComplete newtype Foo a = Foo { runFoo' :: RWS FooContext (Endo [FooOutputEvent]) FooState a } deriving (Functor, Applicative, Monad) runFoo :: Foo a -&gt; FooContext -&gt; FooState -&gt; (a, FooState, [FooEvent]) runFoo a c s = runRWS (runFoo' a) c s &amp; _3 %~ (\(Endo f) -&gt; f []) notifyMessage :: Message -&gt; Foo () notifyMessage m = Foo $ do FooContext{..} &lt;- ask FooState{..} &lt;- get (n, bar', barEvents) &lt;- runBar (Bar.notifyMessage m) () bar forM_ barEvents handleBarEvent when (counter + n == limit) $ tell (Endo (ProtocolComplete :)) put FooState{counter = counter + n, bar = bar'} where handleBarEvent (Bar.SendMessage m') = tell (Endo (SendMessage m' :)) If you want something other than just sending messages back -- i.e. if Foo needs to make callbacks -- then you could define a custom typeclass for the monad with these functions in. The parent component would then implement the monad interface itself to handle the callbacks. This is what I did at first, but I realised that I only really needed to produce a list of output events, so using the writer monad was cleaner.
Let's try to rewrite the pointfree version step by step. &amp;#x200B; (:) . f = \a -&gt; (:) (f a) = \a b -&gt; (:) (f a) b = \a b -&gt; f a : b
The two standard answers are entity component systems and doing something OOP-y. By OOP-y I mean encapsulating behavior and data behind some interface, this might involve type classes/vtables and existential types. 
Also very welcome to add `nub` benchmarks to https://github.com/nh2/haskell-ordnub !
The closest thing I've found that compares to developing against side-effecting interfaces in other languages is a record of functions. So if you have: interface UserRepository { User getUser(int id); } in Haskell it would be: data UserRepository m = UserRepository { getUser :: Int -&gt; m User } Using records of functions allows you to use multiple `UserRepository m`s in the same monad `m`, which is more similar than typeclasses to what you can do with the `UserRepository` interface in Java.
Ah ok. But thats the way that is closest to oop. I would like to know if the same problem / situation can be handled better in a more functional way :)
so, this is a very old article, I know i have to consider the fact that Monad will soon be MonadFail. is it obsolete in other ways? and what is the next step after this?
The problem with jsaddle is not performance overhead but conceptual overhead. If haskell modules were just WebAssemblu modules, you wouldn't need to export haskell modules. If you could just import WebAssembly modules as haskell modules, you wouldn't need FFI. Is seamless integration with WebAssembly module possible?
This made me think of this blog post I read a while ago about stubbing out database code for testing: https://functor.tokyo/blog/2015-11-20-testing-db-access Note he mentions that record of functions and typeclasses are very similar! (maybe you could achieve your multiple instances in one monad with a phantom parameter?)
I've submitted a pull request to update for \`MonadFail\`: [https://github.com/mgrabmueller/TransformersStepByStep/pull/4](https://github.com/mgrabmueller/TransformersStepByStep/pull/4)
Having polymorphism is just a generally great thing - just because OOP does the same with ‚Äúpass any class that inherits from this class‚Äù versus Haskell‚Äôs ‚Äúpass any data type that implements this typeclass‚Äù doesn‚Äôt make it any ‚Äúless FP‚Äù
N.B. The strategy of sleeping for 1 second between print outs will drift since the print time will make the total loop longer than one second. This usually doesn't matter, but when it does the result is frustrating. &amp;#x200B; For this reason one of my earliest packages (now rotted but not badly) was to make an event system that was based on absolute times instead of relative delays.
Jsaddle is low level on the tech stack. Think of it more like a graphics driver than a graphics library. Libraries for simplifying jsaddle usage can be created. Also, as I said before, interacting with the wasm module system is *already* easy with the C FFI. It's just of little use due to the worker / main thread boundary. You can run Haskell code that doesn't block on the main thread though, which would make it easy to call the browser APIs from Haskell without jsaddle. But a haskell implementation without blocking loses a lot of the nice control structures we have.
Are you saying WebGHC already has constructs for importing and exporting EcmaScript 2015 modules without limitations?
I'm saying you can declare imports and exports with the C FFI and use the WebAssembly API go resolve them. It's not automatically done with ecmascript modules because wasm doesn't have that feature yet.
https://developers.google.com/web/updates/2018/10/wasm-threads says wasm threads are different from WebWorker. WebAssembly is a fast moving target.
It would be nice to update the readme to reflect the addition of nubbing functions to `containers` et al.
Note that a hash-based algo doesn't have to be unstable -- just like in `nubOrd` you can use the hashset as a reference for pruning a list rather than just re-emitting a list.
I've just made an initial release of libtelnet, a close binding of the C [libtelnet](https://github.com/seanmiddleditch/libtelnet). It allows you to write telnet clients/servers in Haskell, if for some bizarre reason that's something you still want to do in 2019. 
This goes back to the difference between parallelism and concurrency. Control.Parallel.Strategies is purely for deterministic parallelism therefore you can't parallelize IO with it. Control.Async is an amazing library for concurrent IO: import Control.Async multiCWD'' = mapConcurrently constWithDelay [1..5]
In `multiCWD'`, the type of `a`, `b`, ... is `Eval (IO ())`. That is, you are sparking the generation of the `IO` action, but not its execution. Its execution is being done at the moment you use `sequence`, which does one at a time.
I‚Äôm trying to get rid of this habit. It‚Äôs useless if you use an editor that can restore a session (like Vim), and I think it‚Äôs good to free yourself from useless mechanical stuff.
Is this something to do with rpar only forcing to weak head normal form?
As said by /u/Tarmen, `parallel` is not well suited for `IO`, and the difficulty to make this work is proof that the `API` tries to stop you from doing this. Try the `async` library instead, it will certainly do the job/
Yep.
You can imagine IO as a function: constWithDelay :: Int -&gt; Token -&gt; (Token, Int) The token doesn't have a runtime representation and it isn't the only way to implement this. But it packages something the compiler doesn't understand (implicit ordering of IO actions) in something it does (data dependencies). Anyway, the important takeaway is that `IO a` is a function so it's in normal form. (well, usually). You could avoid this with a function like unsafePerformIO :: IO a -&gt; a But you probably shouldn't do this without understanding the implementation details and tradeoffs of what happens there. Notably unsafePerformIO has to be thread safe which leads to drastic performance penalties.
Another misconception is that lists aren't free monoids, DList are since they are lazy in both arguments of concatenation. Lists are free in the class of left strict monoids,e.g. ones that satisfy the equation `undefined &lt;&gt; X = undefined`
SPJ's talk on lenses is where I finally understood them. https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation 
We actually have a smaller client that uses telnet across all nodes on their network. So we have been using https://github.com/andrewthad/teleshell. I'll take a look at the bindings you wrote and see if it simplifies anything.
I think you'll find my bindings and teleshell operate at different levels. Teleshell appears to be doing scripted interactions with a server, while libtelnet is handling telnet bytestreams precisely, including option negotiations, compression and so on.
There's (unfortunately) many years old issue to decouple (servant-)server routing code from `wai`. (Similarly as `servant-client-core` is independent of HTTP lib). That would made the problem a lot simpler. Would be nice if someone could work on decoupling.
Wow, there‚Äôs a lot of weird operators in this library. I find that a significant weakness of Haskell personally. I have no idea what things like &gt;\\ or &gt;+&gt; do without looking it up. 
It's just using the pipes library. If you use an API enough then you can remember, otherwise I agree you do have to look them up.
&gt; What specifically about wanting people passionate about the problem domain is racist? It is at this point that you've started talking past each other. This is not what dbpatterson was saying, and you're clearly not trying to comprehend their argument at all if this strawman is your takeaway.
Do you have a link to the issue?
I have a few mixed feelings about all of this. Agree that "geek gene" is a myth. Regarding "good culture fit" at companies, this is just standard buzzwordery at most tech companies these days; it's hard to tell what people actually mean when they say it. I'm hesitant to single out "white men" as abusing the term, without any actual numbers to back it up. I'm also hesitant to view "looking for good culture fit" as as a red flag, since, well, pretty much everybody says it. On the other hand, I do agree that "culture fit" is a problematic concept, due to being so ill-defined. It's the hiring-stage version of at-will employment: candidates can be dismissed for any reason under the umbrella of "not a good culture fit". This clearly has the potential to enable bigots to discriminate against protected classes, which would be illegal, but all you have to do is say "it wasn't a good culture fit" rather than "we don't hire people of your race/gender/whatever" and bam, it's legal.
&gt; I also buy Ranjit Jhala's and Liquid Haskell's belief that refinement types are much easier for people to use in the common case. One thing that makes Liquid Haskell nice is that the proofs are generally constructed passed around implicitly. Dependent types can be less ergonomic when you have to create proof objects yourself and pass them around explicitly.
I'm not sure there's a specific issue as in "GitHub issue", but issue exists as e.g. servant-snap is a copy&amp;paste of servant-server, i,e doesn't share the routing code (which they should)
Unrelatedly: id like to share monad-ste for those who want to use st / prim monad computations while also having a nice error/ abort model https://hackage.haskell.org/package/monad-ste
It's not? Because it seems like the point of his post was to accuse this org of racist hiring practices without any real evidence of it. Saying that coding isn't for everyone isn't inherently racist either. There are tons of things I'm not good at. If someone said "not everyone can play a musical instrument", I would readily agree and identity myself as such a person. It's not for lack of trying, either -- I'm inherently bad at it. There's real injustice in the world. There's no need to engage in witch hunts to get your outrage fix for the day. If you really suspect he's trying to avoid hiring women and people of color, then find real evidence of it. Twisting a neutral comment into something offensive is a lazy way to discredit someone. Do better.
Thanks a lot for the reply. Those are some good points. I shall look into this function.
It's possible that this is a dumb question but is this a "deep" context switching function? I guess what I mean by that is that my experience with other languages is that each and every function that has blocking IO needs to be decorated with async/await syntax. If I call a function with map concurrently that calls another function blocking function, will it know to switch contexts or does it continue with the async strategy until the original function is finalized?
I agree, this does seem possible.
Bad bot
This is something I barely got started on many months ago, but for which I haven't found both the time and motivation simultaneously, having been quite busy at work and in my personal life. If anyone's interested, let's create an issue about this, in which I can throw some thoughts and the very little code that I have.
So am I understanding you correctly that you cannot do any IO with multiple cores? I'm working on learning Haskell to build a performant web crawler, and my hope was to be able to leverage both parallelism and concurrency.
Turns out there's a thing called [Lambda cube](https://en.wikipedia.org/wiki/Lambda_cube)...
I actually disagree, it depends on how they're used. Streaming libraries are a bit frustrating but in general there's a common language underlying how operators are chosen. Example: `($)` is function application, right? So `f $ x = f x`. Now `(&lt;$&gt;)` is an alias for `fmap`: `f &lt;$&gt; x = fmap f x`. It's lifting `f` through the functor. Suppose I told you that `(&amp;)` is backwards function application: `x &amp; f = f x`. What would you expect `(&lt;&amp;&gt;)` to do? Have a guess, [and then check](https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Functor.html#v:-60--38--62-). This idea of operators as a visual language underpins the design of lens, making it very easy to tell what an operator does once you understand the pattern. One of the linear algebra libraries uses a similar principle, where the symbol on each side of an operator shows whether it expects a scalar or vector argument.
You can't write a blocking function call in haskell. mapConcurrently creates a bunch of green threads with a bit of bookkeeping to propagate exceptions. These lightweight threads will be non-blocking. If the work were actually long running computations and other threads were starved they'd be moved to other thread queues which'd cause parallelism. Haskell threads are lightweight because they use a 10kb stack instead of the 2mb one for c-style languages. Control.Parallel.Strategies doesn't use threads whether green or otherwise. Instead it puts the thunks (aka closures) on a work stealing queue which makes for very low overhead parallelism. 
By the way, an advantage of having the \`Result\` type is for returning from a web API call - it can be directly encoded to JSON. Whereas you can't pass exceptions over an arbitrary API call. So there's some sense to having a top-level handler for an API call that catches exceptions, and \_also\_ accumulates log messages, to be put onto a \`Result\` object, to be sent across the API. However they wouldn't be log messages of the detail that you'd log internally. They'd be filtered, and stripped of most detail and be presented in a way that made sense to the caller of the API.
&gt; Why do the two `cfunctionInClass` share the same name? How do we tell them apart? You will need to show uniques (e.g. don't use `-dsuppress-uniques`) to distinguish these two bindings. GHC isn't terribly careful about avoiding (apparent) naming collisions in Core. &gt; What does cast do exactly? `cast` is how coercions are applied to expressions. You can think of it as a function of type `cast :: Expr a -&gt; Coercion a b -&gt; Expr b`. You may want to have a look at the System FC paper if this looks unfamiliar. 3. Is there anything related to typeclass/instance outside of `mg_binds ModGuts`? Typeclass resolution happens entirely during typechecking which occurs on the HsSyn representation and not Core. Typeclasses are entirely lowered to explicitly-passed dictionaries by the time we get to Core.
Good point, done.
I would highly recommend that you read Simon Marlow's [Parallel and Concurrent Programming in Haskell](https://www.oreilly.com/library/view/parallel-and-concurrent/9781449335939/), which will introduce you to the differences between parallelism and concurrency in Haskell. Haskell is probably one of, if not _the_, best languages for writing concurrent programs in; GHC provides an extremely efficient runtime for concurrency which allows you to write code in a threaded style which uses events in the background without you having to worry about all the nonsense other languages require of you (looking at you Node). Many languages struggle with tens of threads, Haskell is fine with tens of thousands. Read the book, at least the first half (it's all *free* online) and you'll be an expert in how to efficiently write concurrent programs, and the problems which can be encountered.
Lol I appreciate the advice, you might not have noticed that my second paragraph starts by saying I started it and that why I'm fiddling with rpar! I'm very much looking forward to continuing to work through it!
Thank you! A follow-up question if you don't mind, where is the `functionInClass` binding `valueA` and `valueB` call?
It... should be there in the core output. How are you dumping core? I just tried with your code, and -ddump-simpl includes a definition: functionInClass = \ (@ a_ase) (v_B1 :: FunClass a_ase) -&gt; v_B1 `cast` (Main.N:FunClass[0] &lt;a_ase&gt;_N :: (FunClass a_ase :: Constraint) ~R# (a_ase -&gt; () :: *)) however, I noticed that the code you gave above didn't have any specialisation applied, so I also gave -ddump-ds a try and it oddly left out the definition. Possibly a quirk in the way that dump flag works / what parts of the program it applies to.
`FromJSON` is in essence a detouring approach to defining parsers, in which instead of just defining the parser, you define a datatype only to be used to auto-generate a "schema parser", the results of which you'll again have to parse into your actual model. I find it a very upside-down abstraction and there's multiple limitations that stem out of that fact. You've just hit one of them. More so, this abstraction is pointless, because it relieves you from no implementation details, making you implicitly describe something that you could have just said explicitly. Instead of working around these issues, I suggest you to just define the parser of JSON AST into your actual model. You still can use the "aeson" parser API for that, however it's not very handy. There's an alternative DSL for parsing Aeson values: ["aeson-value-parser"](http://hackage.haskell.org/package/aeson-value-parser). Disclaimer: I'm the author of it.
`FromJSON` is in essence a detouring approach to defining parsers, in which instead of just defining the parser, you define a datatype only to be used to auto-generate a "schema parser", the results of which you'll again have to parse into your actual model. It is a very upside-down abstraction and there's multiple limitations that stem out of that fact. You've just hit one of them. More so, this abstraction is pointless, because it relieves you from no implementation details, making you obfuscate your original intent by implicitly describing something that you could have just explicitly specified. Instead of working around these issues, I suggest you to just define the parser of JSON AST into your actual model. You still can use the "aeson" parser API for that, however it's not very handy. There's an alternative DSL for parsing Aeson values: ["aeson-value-parser"](http://hackage.haskell.org/package/aeson-value-parser). Disclaimer: I'm the author of it.
I am trying to show the core programmatically like this import GHC import GHC.Paths (libdir) import DynFlags import Outputable import HscTypes import CorePrep import CoreToStg import CoreSyn import Control.Monad import Control.Monad.Trans main = pp &gt;&gt; return () pp :: IO () pp = do core &lt;- runGhc (Just libdir) $ do env &lt;- getSession dflags &lt;- getSessionDynFlags setSessionDynFlags $ dopt_set (dflags { hscTarget = HscInterpreted }) Opt_D_dump_simpl target &lt;- guessTarget "Example.hs" Nothing setTargets [target] load LoadAllTargets modSum &lt;- getModSummary $ mkModuleName "Example" pmod &lt;- parseModule modSum -- ModuleSummary tmod &lt;- typecheckModule pmod -- TypecheckedSource dmod &lt;- desugarModule tmod -- DesugaredModule return $ coreModule dmod -- CoreModule let cb = mg_binds core -- [CoreBind] putStrLn $ showPpr unsafeGlobalDynFlags cb return () 
FTI "on one of the dates between March 17th and 19th" Unfortunately can't make it, but will try to meet you guys if I'm in Israel in early April.
Naive question on dependent types: I've seen the typical vector and list index functions examples for dependent types, but I'm not clear what else they can be used for. From what I understand they can bring the values of our terms into types (as done in the classic examples) - however are there limitations on this? Can I create a type which is only natural numbers? Or a type which is a 12 digits only positive integer with leading zeros being valid) - ie a numeric id of a sort which I can then use the type system to prevent storage in a database rather than validators which I have to reuse every time I wish to guarantee I have that type? (probably not the best example - just trying to understand dependent types better)
When you get core via `desugarModule`, you're getting the first form of Core which is incomplete and not directly usable. There's a pipeline used by GHC and some functions like ["prep"](https://downloads.haskell.org/~ghc/8.0.2/docs/html/libraries/ghc-8.0.2/src/CorePrep.html) insert class methods among other implicit definitions to make the code actually work. That's /u/cgibbard is seeing the actual bindings because using the GHC program invokes these extra steps. Regarding the dictionaries for the classes, if your class has one method then it's inlined to basically a newtype. If your class has more than one method then the dictionary will be visible as an actual record that the method picks apart to find the right slot. I'm working on an interpreter for GHC Core so I've discovered all these things while working through it.
As the author of the above blogpost, I feel I should defend my lack of numbers - my intention was not to produce the most efficient ord implementation but to discuss ST and its uses. But I'm glad it's generated so much discussion :D
Dependent types can express arbitrary functional [1] properties of types, with very few exceptions. For your example, you could define a property `IsNonNegative :: Int -&gt; Type`, then form the so-called sigma type `Natural := Sigma Int IsNonNegative`. Inhabitants of `Natural` are pairs of an integer `n` and a proof of `IsNonNegative n`, so only the natural numbers. The same technique would also work for the 12-digit IDs. That's the good part about dependent types: They allow you to encode (almost) any property you want. The bad part is that they typically come with significant engineering challenges. Some properties are so easily encoded that you have to do essentially no additional work (as in the common vector example), but most require lots of effort to prove that your programs do, in fact, respect them. Btw, both of your examples can also be encoded without dependent types, and in fact these alternative definitions would probably be easier to use (even in a language with dependent types). [1] By 'functional', I mean properties which describe the result of a computation. Properties which describe other aspects of a computation, like time/space complexity, cannot be expressed in mainstream dependent type theories.
Sorry fort he horrible title, something went wrong.... 
I did think so! But the discussion turned towards efficient nub implementations pretty fast so I wanted to improve the discussion slightly.
Thanks - you mentioned that both examples can be encoded without dependent types - do you mean it's possible to encode them as a type rather than just as a function that validates the input? 
You might be looking for the [foreign-library component type](http://qnikst.github.io/posts/2018-05-02-cabal-foreign-library.html).
Hi @ElvishJerricco, thanks for answering. Yes, it is indeed what I am using for building the shared library. My question is more - how do I assist the developer using my package to help set this up? Should I create a feature, where the developer can write "simulationunitpackage init" and then it creates a basic project with a correct cabal file, that he can start to extend upon - or is there an alternative? 
Oh. Hm. Probably not a super helpful idea to you at this time, but maybe `cabal init` should add foreign library as an option vs executable and library.
Yes, that would be helpful indeed!
Beginner's ecosystem question: Is there any solution to the [JavaScript Problem](https://wiki.haskell.org/The_JavaScript_Problem) that is particularly well-suited to data visualization using the d3.js library? And, if there's no good way to use d3.js, is there some good alternative library out there that would work better? Context: I've only just begun learning Haskell (I'm barely halfway through the Haskell Book) but I'm interested, for at least my own edification, in trying to eventually build a simple data visualization web app using Haskell (or a Haskell-like language) that transpiles to JavaScript for the front end. I'd ideally like to use d3.js in building this, but am not sure how feasible this is a priori, or whether there is a particular pure-functional option that would be best suited to using d3. (I've found d3 bindings for PureScript and Elm on GitHub, but they do not seem to have been updated recently.) 
Yes, we can define types that do not allow any values outside the desired range. Here are the (Peano) naturals: data Nat = Zero | Succ Nat zero :: Nat zero = Zero one :: Nat one = Succ zero etc. `Zero` is the natural 0, and `Succ n` is the natural `n + 1`. We can convert from/to ints: fromInt :: Int -&gt; Maybe Nat fromInt n | n &lt; 0 = Nothing fromInt 0 = Just zero fromInt n = Succ &lt;$&gt; fromInt (n - 1) toInt :: Nat -&gt; Int toInt Zero = 0 toInt (Succ n) = toInt n + 1 (`toInt` is technically partial due to overflow.) We can also compute with naturals directly: plus :: Nat -&gt; Nat -&gt; Nat plus Zero m = m plus (Succ n) m = Succ (plus n m) However, note that this representation is very inefficient, in both space and time. For the 12-digit ID, we could do something like data Digit = D0 | D1 | D2 | D3 | D4 | D5 | D6 | D7 | D8 | D9 type ID = (Digit, ..., Digit) -- 12 times Obviously, this gets a bit ridiculous, so you might want type ID = Vec Digit 12 instead, where `Vec` is a vector type.
That's not a correct understanding. What you're stumbling on is the difference between GHC's support for automatic parallelism of \*pure\* computation and normal, manually concurrent programming. In most programming languages there is no such thing as automatic parallelism of pure computations, so the distinction cannot exist. But in Haskell it does. When you're writing effectful code (like a webcrawler would be) you must use IO to perform actions that touch the "real world". These can be split into threads very easily with the \`async\` package as others have mentioned. This is probably the tool you're looking for. \`rpar\` on the other hand might be useful when you actually \*have\* the result of a page and you want to parallelize the \*pure\* function that is parsing/analyzing it. In this case there is no IO going on, but GHC might be able to find a way to squeeze more juice out of your CPUs anyway.
Wow, thanks a lot! I was following the dive into ghc articles, I guess it's a little outdated! Is your project open sourced?
So does the Async package utilize multiple cores? It's my understanding that concurrency (ie node) is single threaded and switches contexts. 
The `async` package is a nice wrapper around GHC's `forkIO` primitive which spawns a new *green* thread. If you compile your program with `-threaded` (in `ghc-options`) and give your program multiple cores (with `+RTS -N`) then all your green threads will be multiplexed over multiple cores. Green threads are sort of like the "concurrency" you get with JS in nodejs but GHC will actually use all the cores you give it instead of only multiplexing on one core. For IO bound tasks (like fetching pages from the web) multiple cores doesn't help *that* much. But for CPU bound things it helps a ton.
I think the db layer is not good enough in Haskell 
If you have a few idle minutes it would be greatly appreciated if you could take some time to peruse GHC's [Trac-to-GitLab migration](https://gitlab.staging.haskell.org/ghc/ghc/issues) looking for errors. I believe we have already sorted the vast majority of issues but, with over 16,000 tickets, the more eyes the better
That's a strong statement to make without backing it up. In what way is it not good enough?
I must be understanding wrong then, because if async can leverage multiple cores, isn't that parallelism as well as concurrency?
Haskell makes a stark contrast between the two. Parallelism is a way to make a computation go faster (and nothing more). Concurrency is part of your application design, needed to achieve certain requirements (i.e. UI should not hang while something is happening so UI code should be different thread than processing code). In Haskell this *usually* means that parallelism is for pure computation and concurrency is for impure.
&gt; I have been through quite a lot of blogs &amp; discussion where they discuss on the pros and cons for both. I think it might be easier for you to start out with Flow/Typescript or some other JS-like language, while trying out some small but non-trivial project on the side with Rust/Haskell. Reading blogs and discussions is no substitute for experience. Introducing a language at your workplace with you (and likely others) not having familiarity seems to be asking for trouble.
- If you know both languages well, you should be able to answer the question yourself - if you don't know either, but with to learn some, pick Haskell (because you are asking on /r/Haskell) - if you already know either one, but not both: it's probably a good idea to stick to what you know already., and deepen your knowledge.
Most books on Haskell are really great. The link you give is my top haskell book. Haskell School of Expression is really great. Real world haskell is good (but a bit outdated). There is a super book on parallel haskell programming too. Just read a few and learn. You need a lot of experimentation and you break even after learning Haskell (+ all the math that it comes with it) after 15 years of using haskell.
seems like it would work well as an [awesome list](https://github.com/sindresorhus/awesome/blob/master/awesome.md)
The search bar in gitlab is excellent, and a casual perusal of my trac tickets reveals no glaring problems. I cannot however, figure out how to search for issues authored by me. Is my name `andrewthad` or `trac-andrewthad`? Neither of the two are autocompleted when I search by author, and neither of them return any results.
was that 50 tickets missing meant to be 500, or are there holes in the numbering for some reason?
Some comments seem to have been lost; in particular comments by David Feuer seem to be consistently missing. Compare for instance https://ghc.haskell.org/trac/ghc/ticket/14135 and https://gitlab.staging.haskell.org/ghc/ghc/issues/14135.
Thank you for this valuable comment. And thank you for authoring `aeson-value-parser` as well. Your comment doesn't seem to address the `ToJSON` and `ToSchema` aspects though. And that's just an example, you can also imagine using that type to derive `FromNamedRecord` and many other data-reprepresentation-related type classes. I know that using Haskell types to solve this problem is disappointing at some level. And that's why I've said: &gt;I've learned to stop worrying and love redundant types But it's really the easiest one for many use cases. It's very common that you have a server that does something very interesting, and you have some stupid CRUD aspect to it, that you'd rather not spend much time and thought for. Just defining a few API types and spraying `deriving` clauses can let you get back to interesting stuff fast. On the hand, I really think the problem we're trying to solve here is very hard as well. We want to have a single place in the code that: 1) Defines a serializer 2) Defines a deserializer 3) Makes sure that the serializer includes all the relevant information 4) Makes sure that the deserializer covers all the cases Just look at the symmetric printing-parsing libraries. The abstractions they have to define are far more complicated than our familiar `Monad`s and `Applicative`s. It's clear that this particular problem lives in a more complicated category than `Hask`. And I've just been talking about `To/From JSON`, and there are other things you might want to define in a consistent-by-construction manner (like `ToSchema`). At the end of the day, the subset of Haskell that's used to define new types is the best DSL for defining new Haskell types, and `Generics` based approaches has the unfair advantage of working with representations in that language directly.
Unfortunately I can't include specifics but we offer competitive salaries for a venture backed company in the DC and Bay areas. 
https://gitlab.staging.haskell.org/ghc/ghc/issues/?scope=all&amp;utf8=%E2%9C%93&amp;state=opened&amp;author_username=andrewthad For some reason you don't autocomplete.
We are unable to offer VISA sponsorship for these positions so applicants need to be qualified to work in the US already. 
What is the advantage of using parallelism if concurrency can use multiple cores as well as context switch? If concurrency can use both paradigms, wouldn't it make sense to just always use that?
Conceptually they're distinct. One can have concurrency with only one core (using cooperative multitasking) for example. &amp;#x200B; Purely parallel tasks which don't need to manage effects also are subject to much more extensive ability to reorder scheduling, etc. So there are super convenient API's available for parallelism-sans-concurrency which are much more lightweight, and also don't run the risk of, e.g. deadlocks or other sticky situations where shared mutable state is involved.
Welcome! My project is on GitHub [here](https://github.com/chrisdone/prana) but I'm still patiently laying the groundwork to support a clean implementation. I'll announce it properly when I've implemented more of [these](https://github.com/chrisdone/prana/blob/master/src/Prana/Types.hs#L54-L67).
Haha good thinking! And I'm honoured (maybe) that my name has now been attached to \`nubSpence\` :D
Gotcha, so if I'm understanding correctly this time, parallelism should be preferred whenever it's deterministic because the runtime is able to optimize more effectively?
Sorry if this has been discussed earlier elsewhere, but‚Ä¶ 1. [Here](https://gitlab.staging.haskell.org/ghc/ghc/issues/15189), I have my nickname as a value for "Operating System" and "Architecture" fields in the Trac metadata. 2. At the same ticket, Trac metadata has "Differential" field empty whereas it is not [on Trac](https://ghc.haskell.org/trac/ghc/ticket/15189). 3. I miss "Resolution" bit so much... I find many times it is not enough to know only that an issue has been closed. It is also vital to know the reason (fixed / invalid / ‚Ä¶). Also, the presence of Diff/MR and merge'ess fields somewhere at the top would also be useful.
◊†◊õ◊†◊° ◊ô◊ô◊ü ◊ô◊¶◊ê ◊°◊ï◊ì
Personally I'd pick Haskell, but I'm also far more familiar. Rust is a really fantastic language, but whenever I use it I still feel the tax of it being a "lower level" language. Most web apps are about dealing with business logic, but their performance is often derived from the DB interaction. Performance you'd gain with Rust is going to be trounced by any mistakes you make at the DB layer. I'd rather work at a higher level of abstraction in my business logic and spend my "low level" energy on DB design and fighting the query planner.
There are multiple approaches, but my recommendation is to use Stack, which can automatically install the appropriate GHC version. There are instructions available at: https://haskell-lang.org/get-started/linux I'd also recommend checking out the "next steps" section for how to get started using Stack. Also, there's a Gitter channel where you can ask follow up questions at: https://gitter.im/commercialhaskell/stack
Not even mistakes, but a simple latency will be the biggest factor. Rust is not worth using to make web pages. 
He probably refers to the lack of strong support for commercial databases (Ms Sql, Oracle, DB2) The best thing we have so far is odbc library. It works for me, but it is very basic and still has bugs and only 1 maintainer and is not promoted anywhere. 
My recommendation is not to use Stack at this point for my personal work. I try to use it every year once. At this point most packages I use are not in stack nightly so I stay with the old way. Also, I cannot at any packages that are not in stack lts in a stack script directly, I have to use another config file, which is more complicated than using cabal stcripts. Stack did a very big improvement to how it was 1 year a go, and 2 years ago, so this is a very positive development!
FWIW I believe Beam is the best db access library I‚Äôve used in any language. Extreme type safety, composable, no magic, very good db specific feature support (I added full pg range support in one evening). It still has some rough edges (migration support is not production ready last I checked, type errors take some getting used to) but if you‚Äôre using non-trivial sql in Haskell (joins/aggregates/windows/fancier sql types) it is without a doubt the most fully featured and extensible option. 
In my dream world, the output of `-ddump-core` would start with `{-# LANGUAGE Core #-}` and I could feed it back to GHC. 
Hmm, it looks like the auto-completion list only includes users who are members of the project. I added andrewthad as a Reporter and he now appears in the list. I'll need to discuss this with upstream.
This issue is really about the git repository migration and not the trac migration, but I figure I'll share it here. For non-project-members, the way gitlab configures submodules when you fork the repo is broken. I keep ending up with stuff like `git@gitlab.haskell.org:andrewthad/libffi-tarballs.git` as the submodule remotes, and I have to clean it up by hand.
Matthew mentioned the other day on ghc-devs the workaround: clone from the main repo, not from the fork, and then add (set) fork as a (origin) remote.
Lots of trouble with Arrays. Implementing Floyd-Warshall algorithm in Haskell. I am taking a language agnostic computer theory class. I decided to do my work in that class in Haskell to force myself to continue learning it more in depth. One of the assignments had a component specifically requiring we use the Floyd-Warshall algorithm to solve. I was unable to translate it to a recursive equivalent so I decided to just use arrays. Unfortunately arrays are a bit difficult and I didn't finish the assignment. BUT I still feel it is vital that I understand how to use them for future reference. I decided to use an edge list where each edge eg (0, 3) is an index and each association is a Bool. The original data structure is \[\[Bool\]\] so that was a pretty easy path to take. However there is something wrong and I just can't understand what. I get a type error, but I don't understand why so I must be fundamentally confused on something. [Please halp!](https://pastebin.com/raw/4vfdCCvS)
Hey, it is the first time I see this site and just wanted to congratulate you on an amazing project and implementation. &lt;3 Greetings from Mexico.
&gt; My recommendation is not to use Stack at this point I think it's better to take the advice from the author of http://haskellbook.com who has a lot more experience with helping newcomers get into Haskell and knows what works best. Also Stack's issue tracker is [very active](https://github.com/commercialhaskell/stack/pulse/monthly) compared to [cabal's issue tracker](https://github.com/haskell/cabal/pulse) so instead of spreading FUD about Stack you might consider filing an issue so whatever your problem is can be resolved. Then everybody wins.
Don't call someone's personal experience FUD? You just make yourself look like a dingus??
&gt;My recommendation is not to use Stack at this point for ***my personal work***. I used haskell since 2003 in research. &amp;#x200B;
Do I? Quite frankly I don't think those "personal experiences" are representative if they're even genuine. My personal experience is that Stack works great. On those few occasions when it doesn't it's almost always due to external factors not in Stack's control.
Stack supports Nix too
Indeed, although this isn't a new behavior. The workaround is described in the [developer documentation](https://ghc.haskell.org/trac/ghc/wiki/Building/GettingTheSources). /u/tdammers is currently busy rewriting our developer documentation for the GitLab workflow (although this particular hack is rather unchanged).
Indeed, this is another option.
&gt; Here, I have my nickname as a value for "Operating System" and "Architecture" fields in the Trac metadata. Fascinating. Being tracked as [#29](https://github.com/bgamari/trac-to-remarkup/issues/29). &gt; 3. I miss "Resolution" bit so much.. Yes, this is a fair point. I've added the resolution field to the metadata table. &gt; At the same ticket, Trac metadata has "Differential" field empty whereas it is not on Trac. Hmm, yes, this is a fair point. Currently the Trac metadata table shows the metadata at the time the ticket was created, with later tables showing changes. However, there is a good argument for making the top metadata table show the current metadata (this is what Trac does, afterall). &gt; Also, the presence of Diff/MR and merge'—Çess fields somewhere at the top would also be useful. Worst case all these might go into labels. Hmm, I wonder how best to handle this.
Are there any tools for Haskell that automatically imports unqualified functions you've used? I'm thinking on making something like that, but I don't want to waste my time if it already exists. Would you guys use it if such a tool existed?
The search is not useable on mobile Forefox, it keeps repeating the words you type if you try to go back. Therefore the whole site is not usable
Did you manage to solve your problem by looking at the bug? It seemed like just installing `libnuma` and its headers via your system's package manager would do it.
So true, I've never met a query builder with this level of safety ! The types are pretty hard to grasp (even without errors) but that's just because SQL is inherently complicated...
Please post the type error. Specifically is `toEdge m` a `[(Int, Int)]`, I think that's what your first `zip` needs to type check.
A few issues I noticed: 1. https://ghc.haskell.org/trac/ghc/ticket/15753#comment:7 is missing from https://gitlab.staging.haskell.org//ghc/ghc/issues/15753. 2. https://ghc.haskell.org/trac/ghc/ticket/14684#comment:12 is missing from https://gitlab.staging.haskell.org//ghc/ghc/issues/14684. 3. When I'm the "owner" of https://ghc.haskell.org/trac/ghc/ticket/14920, should I be "assigned" to https://gitlab.staging.haskell.org//ghc/ghc/issues/14920 too? (I guess I can't be assigned as I am not a member of the `ghc/ghc` project) 4. https://ghc.haskell.org/trac/ghc/ticket/16206#comment:2 is missing from https://gitlab.staging.haskell.org//ghc/ghc/issues/16206. 5. The URL contained in https://ghc.haskell.org/trac/ghc/ticket/14874#comment:1 is slightly mangled in https://gitlab.staging.haskell.org//ghc/ghc/issues/14874#note_297374.
The error is on `return (ST.freeze t)` Error: Couldn't match type ‚Äòm0 (b0 (Integer, Integer) Bool)‚Äô with ‚ÄòBool‚Äô Expected type: UArray (Int, Int) Bool Actual type: UArray (Int, Int) (m0 (b0 (Integer, Integer) Bool) &amp;#x200B; Relevant info: toEdge :: Matrix -&gt; [(Integer, Integer)] type Table = [[Bool]] data Matrix = Matrix Integer Table | Empty &amp;#x200B;
&gt; So by placing f at the front of the compose, we are effectively ignoring the second argument. Nope. That's your misunderstanding. * `(:) :: b -&gt; [b] -&gt; [b]` == `(:) :: b -&gt; ([b] -&gt; [b])` * `f :: a -&gt; b` * `(:) . f :: a -&gt; ([b] -&gt; [b])` == `(:) . f :: a -&gt; [b] -&gt; [b]` * `foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b` * `[] :: [b]` * `foldr ( (:) . f ) :: [b] -&gt; [a] -&gt; [b]` * `foldr ( (:) . f ) [] :: [a] -&gt; [b]`. Your instinct about `(:)` being partially applied is very correct, follow that.
Oh, that looks like you did a `let var = act` instead of `var &lt;- act` or `let var = val`.
Ah, I think you need a `runSTArray` or maybe `runST` somewhere there. Your `do` block is an `ST a something`, but you've go it where an `UArray (Int, Int) Bool` is. (You might also be seeing some error about no Monad instance for `UArray (Int, Int)`.
Have you seen the book (Parallel and Concurrent Programming in Haskell)\[[https://simonmar.github.io/pages/pcph.html\]](https://simonmar.github.io/pages/pcph.html])? Simon Marlow implements the Floyd-Warshall algorithm in at least three different ways, and with and without parallelization. The book can be purchased or downloaded for free, and there's source code in a repo as well. One solution uses a sparse graph built with IntMap. One uses arrays from Repa, the other uses arrays from Accelerate (for GPU computation).
&gt; Currently the Trac metadata table shows the metadata at the time the ticket was created, with later tables showing changes Oh, I didn't notice that! This is better than. I'd suggest to have the final values for the fields posted in the top message. Otherwise you're doomed to gather the data scattered over the page to get an idea of the actual state of the ticket. I think adding labels for wontfix / invalid / duplicate is a general practice. Otherwise, the current information (e.g. autolinking for the corresponding MR / closing commit) is sufficient.
This is funky: [https://gitlab.staging.haskell.org/ghc/ghc/issues/16092](https://gitlab.staging.haskell.org/ghc/ghc/issues/16092) &amp;#x200B;
I learned Haskell from the Haskell School of Expression myself, back in 2001. Great book. You may have trouble getting the graphics libraries to work at this point. &amp;#x200B; I've heard good things about Get Programming in Haskell by Will Kurt. It's a new book, just came out last year. Haven't read it myself, though.
&gt; http://hackage.haskell.org/package/loop-0.3.0/docs/Control-Loop.html It probably doesn't matter if this is slow - it's got a whole second between each tick.
You should use async any time you need to manually spawn new threads and synchronize data between them (using the various methods for this like traditional locks, MVars, STM, or even IORefs). If you have a pure computation that you want to parallelize, use par. Both of these are able to use multiple cores.
This is a solid answer, but there is a lot of information specific to the actual tool-chains involved that not everyone familiar with Haskell might know about. For example, I have no experience whatsoever with GHCJS, and would not be able to speak to the production readiness of any of the Haskell -&gt; WebAssembly efforts. Even if I'd much prefer to write in Haskell, I may decide that the front-end ecosystem seems like a confusing morass of rube-goldberg hackarounds, and decide to opt for a less problematic ecosystem. Or not, as the case may be, just saying, there is more to choosing a language for a project than just the code itself.
sudo apt-get install libnuma-dev Added that to the question.
No, but I will give it a look!
Not to mention which, stack does _nothing_ to resolve the above issue, since to my knowledge it won't `apt-get install libnuma-dev` for you. The knee-jerk "try stack" response to issues that have absolutely nothing to do with either stack or cabal or anything else is frustrating, and it can only stand to frustrate new users who are not one step closer to understanding how to fix their original problem, but have meanwhile wasted a lot of time and bandwidth.
There are also times when you're trying to _really_ tune parallel scale-up that explicit thread management can outperform spark-based parallelism. I wouldn't say this is the common case, but I've certainly seen it occur.
Refactoring in Haskell especially is just a hell of a lot less painful than it is in Java or C#. In Java, you might worry about 'god methods' that contain a ton of logic to invoke four or 5 methods asynchronously over one piece of input, and the maintenance of that code could become really scary. In Haskell that's like 3 lines of code and most of the behaviors you'd write unit tests to guarantee are enforced by the type system, so there is no reason not to just implement it the 'stupid simple' way and just use `Async` or something to simply call however many methods you want to on a given piece of input, and have them all execute concurrently. If you want to implement what you're talking about where, message passing as architecture, the simplest version of this is just a bunch of IO tuples containing each containing one or more `Tchan`s and an `IORef` that spit sum types back and forth, updating state in the IORefs as necessary. You'd need a pretty complex application before that method started to pay dividends - This is well past most weekend warrior projects. Not that it's hard to do, there is just no good reason to do it unless you've got a pretty intensely complicated chain of interrelated actions.
&gt; In OOP languages you can send notifications or messages across different parts of an application by using the observer pattern or a message bus. What is a practical way to notify subscribers in functional languages? There are many channel abstractions in Haskell, starting with `Chan` and `TChan` in the base and stm libraries, and moving up to `UnagiChan` as well as other, richer broadcast abstractions with more control over them and more functions. If you don't want multithreading with channels for communication, you can also thread through pure queues, using something like `Data.Sequence`.
&gt; toInt is technically partial due to overflow toInt :: Nat -&gt; Maybe Int toInt Zero = 0 toInt (Succ n) = do p &lt;- toInt n if p &lt; maxBound then return $ succ p else fail "Overflow" \^ If you hate partiality too much.
This is a grotesque misrepresentation. The above user linked to _one week_ of activity on the cabal tracker, and compared it to _one month_ of activity in the stack tracker. I think it is high time we recognized this sort of nonsense as pure trolling.
Yes, there is an open ticket about it https://github.com/haskell/ghcup/issues/58 Ultimately this should be fixed upstream by recompiling the older bindists. We should open a ticket about it, because it is separate from #15444 imo. At some point, we might also just roll our own tarballs. As a temporary solution I added this to the README https://github.com/haskell/ghcup#libnuma-required
The best thing to recommend is to use the package manager, since neither stack nor ghcup have any useful security features baked in. However, I generally don't cross-recommend stack or ghcup, because these solve different problems with fundamentally different approaches and recommending ghcup to a stack user somehow feels counter productive to me.
You're talking about a backend service right? Haskell is great for this kind of thing, provided there are not specialized libraries you need that aren't available (i.e. do your research). I don't know what benefits rust would give you over Haskell in any domain where you could reasonably choose either.
Now I'm really confused. I just tested a fresh Ubuntu 18.04 image with Stack and GHC 8.6.3, and it was able to build the `digest` package correctly with a `stack build digest`. I had to install the following Ubuntu packages: xz-utils build-essential libgmp-dev zlib1g-dev Those would be installed by the get-stack.sh script typically (and, presumably, by ghcup as well). But numa is conspicuously absent. I'm very curious as to what's causing this difference in behavior. Perhaps it's a deb8 vs deb9 problem?
Thanks for sharing that! Great work!
The strategy with waiting for 1 second via `threadDelay` is not perfect anyways. See comment by /u/tom-md.
Where I work, we've been taking the GADT approach more and more lately, though moreso alongside DMap to get extensible records with all the operations you could ever hope for (especially efficient unions and differences). Usually if you can do a really good job of factoring and designing your types, it's not really necessary, but interacting with certain real-world problems / customer demands can make it pretty attractive. We had one circumstance where someone wanted to be able to take piece of data that might need to be collected in a mortgage application process and generate UI for it based on its type, as well as store it in a database, recall it again, and run a bunch of different rules against the fields that had been collected. Given that there are literally thousands of such potential fields, just enumerating them all is a pain. GADT-based keys with DMap/DSum helped a lot to encode what we knew about the types while allowing our records to have arbitrary subsets of the fields, and any individual field could be communicated along with its value in a DSum. In the process of working with GADTs more and more, I've needed to solve a bunch of challenges. Here's a library with some TH code to generate aeson instances for GADTs, so they can be sent over the wire -- it even works with nested GADTs and ones which have additional type parameters before the index. (There needs to be a single index which comes last for this to work, otherwise we'd have to invent new existential wrappers -- but that has covered every case I've actually run into thus far.) http://hackage.haskell.org/package/aeson-gadt-th Another thing you frequently run into when dealing with GADTs in general, and with DMap especially, is the desire to be able to say things like "Given a value of type G a, we know that there's an instance of C a", where G is your GADT and C is some type class. C (F a) also comes up, where F is some other type function. For that, we came up with http://hackage.haskell.org/package/constraints-extras which lets you write things like: {-# LANGUAGE TypeApplications #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE UndecidableInstances #-} import Data.Dependent.Map (DMap, GCompare) import qualified Data.Dependent.Map as DMap import Data.Constraint.Extras instance forall f g. (Has' Semigroup f g, GCompare f) =&gt; Semigroup (DMap f g) where m &lt;&gt; m' = DMap.unionWithKey (\(k :: f a) x y -&gt; has' @Semigroup @g k (x &lt;&gt; y)) m m' ...which will then proceed to conflict with the stupid instance of Semigroup that DMap already has (which just does overwriting rather than actually combining the aligned elements), but let's pretend that didn't happen. The Has' Semigroup f g says "given any value k of type f a, I can deliver you an instance of Semigroup (g a)", and then on the term level, has' is used to unpack the required instance so we can combine the corresponding values in the DMap. Thus we have a funny sort of constraint-level pi-types. In the case of any particular GADT f, the Has' Semigroup f g constraint will unfold into something like (Semigroup (g Text), Semigroup (g [Integer]),...), enumerating all the types that the GADT index for f can take on. So yeah, `deriving` goes out the window, but you can still actually express a lot of what you need to in a succinct and general fashion. Pile enough of these things together and DMap starts to look a bit like the extensible record system everyone always wanted, with DSum giving the corresponding variant types. It's maybe not quite where we really want everything to be yet, but it keeps creeping its way forward. There are lots of messy database APIs in the real world designed by people who don't think much about their types, and these sorts of GADT-indexed collections can help a lot to make sense of things there too.
True, although there's little we can do in this case. The same ticket on Trac renders almost as poorly.
Markup is different: strike-through is missing https://gitlab.staging.haskell.org/ghc/ghc/issues/13615#note_285772 https://ghc.haskell.org/trac/ghc/ticket/13615#comment:4
formatting is broken https://gitlab.staging.haskell.org/ghc/ghc/issues/13615#note_285782 https://ghc.haskell.org/trac/ghc/ticket/13615#comment:8
More formatting near the end: "a. ..., b. ..." got changed from a list into a line https://gitlab.staging.haskell.org/ghc/ghc/issues/13615#note_285809 https://ghc.haskell.org/trac/ghc/ticket/13615#comment:12
comment is missing https://ghc.haskell.org/trac/ghc/ticket/13615#comment:27
Links in comments are pointing to different places https://gitlab.staging.haskell.org/ghc/ghc/issues/13615#note_288117 https://ghc.haskell.org/trac/ghc/ticket/13615#comment:49
Not at all what he's talking about. New-build is the Nix style builds. Stack JS similar reliability but a different model
Frankly, this is precisely why I don't use stack. Stack really is great, but it has amuch larger surface area of ways to let *other tools* break it than nix and cabal-install do
I'm generally a fan of using a system package manager when possible. Unfortunately they don't always package the right GHC versions. I'd love for someone to create a Nix archive of all GHC versions.
Interesting. Is stack using the same bindists as ghcup?
I think it's the same ones, I intended to check but didn't have a chance before running out of the house. My best guess right now is that Stack is using the deb8 build and ghcup is using deb9.
OP doesn't specify, but linked thread implies that question is about backends. In that area Haskell doesn't have tooling neither library support problems. Rust: I don't know. FWIW, I don't know non-problematic front-end ecosystem. I cannot compare Haskell with Rust, as I don't know them both well. Yet, I haven't experienced tooling problems with GHCJS, the only concern is the output bundle size (which after Closure Compiler is good enough for me). There are no "Haskell webdev" book though, so one need to scrap information from all over the place.
[removed]
However Postgres, MySQL and a number of "NoSQL" DBs are covered very extensively in Haskell libraries. `odbc` is great I agree and should get more press!
Hi Facebook Summer Internships Get the information through the link: https://www.biginternships.com/facebook-summer-internships/ #students #internship #university # college 
I would fail a code review for something that's doing implicit imports on my watch. &amp;#x200B; Automatic import boilerplate generations via HIE+hsimport is fine though.
How to remove nodes AST with `recursion-schemes`? I use a TTG encoding with extension type families. One of the steps has to remove extensions with `Void` and I'm a bit stuck trying to find a correct scheme and/or functors for that. data ExprF pass f = LitF Int | OpF f f | ExtF !(XExprF pass) type family XExprF where XExprF 1 = Text XExprF 2 = Void step = cata f where f :: ExprF 1 (Fix (ExprF 2)) -&gt; Fix (ExprF 2) f = \case LitF i -&gt; Fix (LitF i) OpF a b -&gt; Fix (OpF a b) ExtF _t -&gt; Fix (ExtF (error "Don't want this here." :: Void)) 
Sorry, there are no intern opportunities in that team at the moment.
Thanks. That looks like it will do the trick.
That's `hoist` hoist :: (Recursive s, Corecursive t) =&gt; (forall a. Base s a -&gt; Base t a) -&gt; s -&gt; t hoist :: (Functor f, Functor g) =&gt; (f ~&gt; g) -&gt; Fix f -&gt; Fix g step = hoist \case LitF i -&gt; LitF i OpF a b -&gt; OpF a b ExtF _t -&gt; ExtF (error "..." :: Void) (Somehow it took me a while to realize TTG means "Trees that grow".)
&gt; In OOP languages you can send notifications or messages across different parts of an application by using the observer pattern or a message bus. The same sort of things you can do in Haskell too. Language isn't a limiting factor here. Haskell has many options and even some advantages. Depending on what you have in mind by "a message bus", of course, but one could use `TChan` as a way to communicate between parts of an application. Fanout subscribing then could be as simple as `dupTChan` (see http://hackage.haskell.org/package/stm-2.5.0.0/docs/Control-Concurrent-STM-TChan.html). Or `Control.Concurrent.Chan`, or `Control.Concurrent.BoundedChan`, depending on what you need. Or sometimes packages like `Conduit` or `Pipes` are sufficient to model your application as a set of Producers and Consumers. Or even just `MVar`s (I've put on my helmet so readers of /r/haskell can now hit me on the head for bringing this option ;))... Even if you want a fully synchronous push-based approach, like many "OOP" applications use and which looks something like: interface IMessageBus { void Publish&lt;T&gt;(T message); void Subscribe&lt;T&gt;(Action&lt;T&gt; func); } which would keep a record of subscribers internally and then calls all the subscribers that are interested in a given message. I don't think of any obstacles to implementing it in Haskell, technically. Again, Haskell even has advantages here because of its excellent support for concurrency and because of STM, although I don't think it'd be default (or even a preferred) solution in an FP mindset. I think this discussion could be more helpful if you'd provide some common scenarios to discuss. Like "I have this problem, and in OOP I model it by having one publisher and two observers, how can I model it in FP so that it satisfies my requirements?". The requirements could be like "loose [ temporal | spatial | efferent | ... ] coupling", or anything you want to achieve. This way we may suggest an FP solution that could be different (or not) than the one you'd do in OOP and be more idiomatic to FP/Haskell while still meeting your goals.
An array is stored as a chunk of adjacent memory cells, so accessing any element is O(1). If I tell you a linked list is stored as single cells containing the value of the element and the address of the next element, can you guess the worst complexity of accessing a random element?
https://www.yesodweb.com/book-1.6/persistent#persistent_something_besides_sqlite
Take a look on the yesod templates list in stack, you have quite a diversity there
The full list is here: https://github.com/yesodweb/stack-templates Most likely OP is looking for: stack new projname yesodweb/simple
 prog = between scn eof (sepEndBy equation scn) and [`sepEndBy`](https://hackage.haskell.org/package/parser-combinators-1.0.1/docs/Control-Applicative-Combinators.html#v:sepEndBy) says: &gt; sepEndBy p sep parses zero or more occurrences of p, separated and optionally ended by sep. Returns a list of values returned by p. So you can parse `equantion` zero times, between spaces and eof, which can correspond to an empty line.
If you have a concurrent program you can make it run in parallel. This can make your program 3-3.5 faster on a four core machine which is nice. However this doesn't make this parallelism free - we just already paid the cost of writing a concurrent program. Concurrent programs are hard to write and test - they can deadlock, they are innately nondeterministic, they make other problems like exceptions more complex. If you have a pure program it's nice to get the parallelism speedups without having to pay for concurrency.
YES!! Does this mean that HIE no longer needs to be compiled against a bazillion different GHCs?
&gt; While ghc-lib provides the full GHC API, it doesn't contain a runtime system, nor does it create a package database. That means you can't run code produced by ghc-lib (no runtime), and compiling off-the-shelf code is very hard (no package database containing the base library). 
Numpy "broadcasts" array components so it can operate with arrays that have different dimensions : https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html#general-broadcasting-rules 
Small aside: instead of `-XPackageImports`, one can also use the `mixins:` section of a Cabal file to resolve package collisions: ```mixins: ghc-lib (GHC as GHC8)```
Do I understand correctly that this is essentially a strict subset of the official GHC source code? E.g., you took the GHC repository and then progressively deleted a bunch of stuff (with the appropriate tweaks to other things that you need to retain)?
&gt; I'm honestly not sure how to describe algorithmic complexity It's when you say thing like "the worst case performance of QuickSort is O(n^2) but its average case performance is O(n log n)". If you haven't seen that kind of thing before, then you're missing a prerequisite for your course.
We didn't really delete anything that goes into the GHC API. We did merge things in libraries they depend on (e.g. template-haskell) and generated a .cabal. &amp;#x200B; Yes, this is the mysterious plan :)
Thank you for your explanation. This is exactly what I looked for. It would be cool if I had a more FP-like solution. I provided an example about game engines and their subsystems in the comments.
&gt;Small aside: instead of -XPackageImports, one can also use the mixins: section of a Cabal file to resolve module name collisions: Would you have to rename every GHC package you want? e.g. Bag as Bag8? Can you rehierarchy the whole thing under GhcLib.GHC? If people are likely to find this method easy we can document it.
Thank you! As it turns out, it was a badly designed practical where they covered the required skills in the lecture *after* the practical... These explanations are really helpful though! 
Would the worst case complexity be O(n) where n is the length of the linked list? 
Correct, very good.
This definitely makes it much easier to write Haskell IDE tooling
Thanks very much for your answer! 
I'm not OP. Although there is a remark in the comment about "specializing". This led me to wonder if there was a compilation (or even better a book) of best practices for performances. The wiki.haskell page is somewhat dry. Thanks!
&gt;Does anyone have any recommended practices for combining logging with exception handling? To me, this is a choice that should happen right at the "top" of an application, so very close to `main`. Nothing else should be worrying about this. What you really want is to provide an implementation of `throw` that is essentially `\e -&gt; logFatalError e &gt;&gt; primitiveThrow e`. This kind of stuff works nicely with almost any type of effects systems. My current favourite is [simple-effects](https://hackage.haskell.org/package/simple-effects) \- but the approach should work with almost anything. First, I would write my logic using an effect that says that exceptions can be thrown: data Throws m = Throws { _throw :: SomeException -&gt; m Void } throw :: (MonadEffect Throws m, Exception e) =&gt; e -&gt; m a throw = fmap absurd . _throw effect . toException foo :: MonadEffect Throws m =&gt; ... -&gt; m () Now when I compose my application in `main`, I simply provide an interpreter that would log the exception first. main :: IO () main = foo &amp; implement Throws { _throw = \e -&gt; logFatalError (displayException e) &gt;&gt; liftIO (throwIO e) } But what is `logFatalError`? It's just another effect! data Log m = Log { _logFatalError :: String -&gt; m () } logFatalError :: MonadEffect Log m =&gt; String -&gt; m () Log logFatalError = effect But `IO` doesn't have an instance, so we also need to provide an interpreter in `main`: main :: IO () main = foo &amp; implement Throws { _throw = \e -&gt; logFatalError (displayException e) &gt;&gt; liftIO (throwIO e) } &amp; implement Log { _logFatalError = putStrLn } &amp;#x200B;