Oops. We've fixed the blog post title.
Hmm, interesting. So what is the definition of a "context"? It's hard for me to see `K[ ] = let x = 5 in _` as a context explicity *because* it involves binding a variable. Additionally, I seem to be misunderstanding something about `x` being `undefined`. I agree it is not defined at the top level, but I don't agree that it is *equal to* `undefined`.
As long as ghc can get the code into a normal loop, LLVM can do the loop unrolling.
I disagree. This change is very unlikely to break anyone's code while possibly fixing already broken code. Making e.g. `head` total is guaranteed to break people's working code and cause lots of work and years of CPP #ifdefs for a very small win. If you want safe versions, add them under a new name.
My compilers course (loosely) followed the book you mention. I've been reimplementing the practicals in Haskell. The approach uses alex/happy in order to closely mirror the design of the course but I'm planning to get to a parsec version as well. I'm still learning haskell so it might not be idiomatic in places but you check out the code [here](https://github.com/mpickering/compilers) if you like. lab2 is very basic, lab3 has a type checker and lab4 introduces functions.
Now that would be depressing :-)
Just add -fhaskell-version=
looks interesting.. did anyone have a refactored version of source code for GHC 7.8?
`hoogle data` actually seems to crash on me quite often (I suspect from running out of memory). Anyone else get the same?
the thing is, the spec isn't wrong. Strict foldl makes sense from a performance perspective, but is a total mess from an equational reasoning perspective. 
Thanks. I still don't understand whether in the Criterion documentation for `nf`, they mean HNF or NF ? The link you provided says "head normal form" does not apply in Haskell, so is that a typo in the Criterion documentation? It's also unclear to me why I get from 30 ns for `whnf (scanl1 (+))` to 5 ms for `nf (scanl1 (+)) rand` to 35 ms for `nf (last . scanl1 (+)) rand`. 
This was just a quick hack, but I thought I'd mention it anyway for comparison. I've been using a typeclass like this one to help disambiguate some of my JSON: import Text.AnimalCase (toCamelCase, toSnakeCase) class Disambiguate a where demarcate :: a -&gt; (Text, [Text]) demarcate _ = (T.empty, []) disambiguate :: (Disambiguate a) =&gt; a -&gt; Text -&gt; Text disambiguate x f = toCamelCase $ let (prefix,fields) = demarcate x in if f `elem` fields then prefix `T.append` ('_' `T.cons` f) else f reambiguate :: (Disambiguate a) =&gt; a -&gt; Text -&gt; Text reambiguate x f = let (prefix,fields) = demarcate x (prefix', f') = T.splitAt (T.length prefix + 1) $ toSnakeCase f in if prefix' == (prefix `T.snoc` '_') &amp;&amp; f' `elem` fields then f' else f where `demarcate` supplies a prefix and a list of ambiguous keys. Then if you want `aeson` to convert from/to json for you it should be possible to define your own ToJSON/FromJSON (still working on this myself): class (Disambiguate a, Generic a) =&gt; DemarcatedToJSON where -- ... class (Disambiguate a, Generic a) =&gt; DemarcatedFromJSON where -- ... Also helps with simple data types, e.g. data MetadataDataType = TypeString | TypeInteger | TypeDecimal instance Disambiguate MetadataDataType where demarcate _ = ("Type", ["String", "Integer", "Decimal"]) Finally, to get default values for many of the JSON fields, the `data-default-class` package helps a lot. This is a great deal more manual of course, but I'm already namespacing quite heavily and I only had a tiny number of name clashes... I didn't feel much like adding TH for that.
I did ghc bench.hs ./bench I was running ghc 7.4.1, just updated it to 7.6.3 and `sum` goes faster alright now. That part using `last . scanl1 (+)` is still quite slow compared to the others though.
Go skips libc and calls the system directly on windows, FreeBSD, Linux for different CPU types. This basically means they have to carry around their own layer and port it (runtime). Some systems have stronger guarantees that you can bypass libc than others without being broken by a change. I agree it is a double edged sword, but the advantage Go gets is cross compilation works amazingly well and is VERY easy to set up. I cross compile raspberry pi Linux binaries from Mac OS X and all I need is 3 environment variables and just the normal Go source tree (no additional tools). It's almost as easy as deploying a portable binary (like erlang beam or java class files)
My impression of Pure is that given the same input it will always return the same value, so related to referential integrity. This would include divergent and exceptions, if that is always the result of the input. A Total function is a pure function that is not divergent and does not have exceptions. Applied to a language, this requires otherwise non-pure functions, like IO, to be wrapped in something like the IO Monad which means that by definition the input is not the same.
I've been working in the glibc source on and off for the last year and a half and I've come to really disdain it. Not only does libc (as a specification) have serious issues, but the implementations can be quite problematic as well. I'd like to see a cleaner implementation, such as musl, gain adoption with Linux distros.
Does Go support dynamically linked executables on Linux? I'm wondering because on Linux almost everything uses glibc's elf interpreter. If you run readelf -a on Go executables, does it list an elf interpreter? If so, is it the system wide one? I would be surprised if they wrote their own elf interpreter.
Hmm, I just learned how to code helloworld last week. I think I'm going to sign up
Go currently is static linking only. I think if you make foreign calls to C some other stuff happens but I try to avoid that. Go binaries are the same as statically linked ELF programs on Linux. This means you've got all your dependencies with you, except the kernel :-)
That is a separate proposal. I try to keep proposals small to avoid bikeshedding.
No you'd have to use `-fllvm`
GHC has it as a foldr, with the recursion explicit.
It's a little unfair to benchmark with optimisations turned off.
Will it ever become the default? 
I can't comment on the code yet, but I upvote for doing it for fun. ;) Implementing games is my favourite excercise for learning new languages.
That is one clunky workflow. Thank you for having the patience to figure it out and the courtesy to document it for the rest of us. In my dreams, this would serve as impetus for someone in the community to do something less janky that we could roll into Emacs, vim, and whatnot.
I'd love some comments on the code. I'm newish to haskell and I am still wrapping my head around everything. I'm sure there are more efficient ways to handle what I'm doing.
ideone.com cannot handle more than soe 40000 (http://ideone.com/Z8HX9p). Isn't it due to absent -funfolding-use-threshold1000, do you know? Anyway, C version can compute original example in 2 seconds
Only languages that don't propagate the problem. In languages that do the user has to deal with it ;) It would be nice if we could purge the mistakes of the past, but in the name of backwards compatibility we're sort of stuck with them.
Some people complain about GHC statically linking binaries. I wonder how it will turn out for Go.
Fine for their users I think. I think most big web companies uses static linking to push binaries to their servers, as it greatly simplifies deployment.
Some people complain. Many don't want it changed because the reasons people think shared libraries matter are actually not very real anymore. What I think I would like to see is a dynamic loader for go where I can load modules.
It negatively affects compilation time (which is already slow), and has no noticeable effect on a large body of code, so not for the foreseeable future.
A nitpick on game logic: the standard 2048 adds to the score the double of each merged cell, while you just sum up everything on board. The original approach is a bit more complicated in Haskell, as it begs for a Writer monad in table updating. You can look at my own (admittedly code golfing and unsightly) [2048](https://gist.github.com/AndrasKovacs/9597994) for some tricks. Using `lens` is the single greatest increase of convenience here, I think. More specifically, you can just create a traversal for all the zero cells in the table: zeros :: Traversal' [[Int]] Int zeros = each . each . filtered (==0) And then it acts as a sort of virtual container inside the table. `lengthOf zeros table` gives you the number of zeros in the table. `elementOf zeros n` acts as a functional reference to the nth zero in the table, allowing you to read and modify easily, for example `set (elementOf zeros 3) 2 table` returns a table in which the fourth occurrence of zero is replaced by 2 (if there are less than four zeros then an unchanged table is returned). 
Well, since the scoring complexity must go *somewhere* I don't think it's worth it to keep around the history. A prime benefit of monads is that we can keep the core logic unchanged, at the price of sprinkling around some applicative/monadic syntax, while the alternative can be very ugly and easy to get wrong (think of hand-rolled stateful code). Since updating the table updates the score, it is most straightforward to do both in the same place.
ASLR is an orthogonal issue to dynamic and static linking.
This sounds like an interesting blog post *hint* *hint*
I agree about erring on the side of correctness. But... because programmers do so wish time was simple, they believe it can be done efficiently. Which in turn means that if the `time` library is slow, in addition to refuting the overly simplistic APIs of other libraries, programmers will try to make faster alternatives, thinking this is easy to do— but because it's not, pain abounds. Thus, I think the `time` library should be made as fast as possible/feasible without sacrificing the correctness. For, only by doing so can we stop programmers from needlessly trying to roll their own. Of course, we also need good documentation of both the common idioms and the pitfalls for the unwary. A whole lot of work went into the `time` API; we should explain that to the users! (not just tell them that it's so!!)
You'd only have to recompile if the security fix was for a function you actually used.
As far as RT goes, the main intuition here is that we are always referring to "things" and never to the *names* of things. Once we allow ourselves to refer to the names of things, then we run into issues: if A and B are both names of the thing T, when are A and B "the same"? In the case of Quine, Frege, etc, this comes up in such classic scenarios as proclaiming that "the morning star is the evening star". Given that both names refer to the planet Venus, if we only ever care about things then it is meaningless to make this proclamation because *clearly* Venus is Venus, just as X is X for any X! And yet, paradoxically, when we make proclamations like these, we do so in a way that does indeed carry information. The listener may not have known that the abstract thing referred to by "the morning star" is the same as the abstract thing referred to by "the evening star"; thus, by saying "the morning star is the evening star" we have conveyed that knowledge, even if they do not know that either name refers, concretely, to the planet Venus. Notably, however, this is a piece of knowledge about *names*, not about *things*. In the case of programming languages, RT shows up once we start working on macro systems or implementing compilers. The major issue in both cases is that, when manipulating the abstract syntax for variables, we want to do so in a way that is parametric over the actual variable name used. That is, when manipulating an AST containing "x", we shouldn't treat it any differently than the same AST with "y" in place of "x". RT fails in impure languages because the act of referring, the act of using a name to denote a thing, is context-sensitive and can have side-effects. For example, given a mutable variable, we can't simply replace the variable by the value it points to (cf., replace the name by the thing it refers to) because that value can vary over time and therefore the time at which we perform the substitution will change the meaning of the program. Notably, this failure of RT is a fundamental aspect of the nature of languages, both natural languages and programming languages. The question is only: at what point does RT fail, and how does that affect my use of this language? For example, GHC Haskell fails RT because of the mutation involved behind the scenes in implementing call-by-name. The failure mode shows up when we look at the running time and memory usage of our programs; controlling these failure modes is why we have strictness annotations, and why we must occasionally restructure our code to force sharing or non-sharing. However, this is an extremely low level at which RT fails; the only time we have to care about it is when we have to care about the performance of our programs. This is very different than the high-level RT failures with mutable variables, where we have to worry about it just to know what mathematical function our program is computing! (let alone the performance characteristics of implementing that function) People like to pretend that RT is an all-or-nothing deal, but it isn't. Purity, similarly
Well done!
That Haskell author was probably me, and I'm a "she". IIRC, that was the time Lennart Augustsson made a delightful comment about how he didn't want to program in a language so referentially transparent that he couldn't benchmark things anymore. (Because either all the alternative implementations have the same meaning —including the performance characteristics—, in which case benchmarking is silly; or else they don't, in which case why are we comparing the speed of programs that do different things?) The reason I bring these issues up is not for splitting hairs, however fun that is. The reason is that people get this notion in their heads that things like RT or purity are absolutes, and we either have them or not. Whereas, in reality these things are ideals. It is important to have ideals because they give us something to aspire to, but it is important to realize that they can never be achieved. But it is also the fact that it cannot be achieved which makes it worth aspiring to. We cannot build a perfect language, but we can always make a better one. And in order to make a better one, we must be able to recognize the failings of our current ones.
I don't know anything about Timber, but my understanding of OHaskell is that it's just an embedded DSL, not Haskell with extensions. I think it hasn't caught on because OO is just not that big of a deal. As far I know, OCaml is a popular Caml implementation because it's a relatively good implementation, not because it has objects. 
Would you mind sharing what are the most important use-cases/operations/etc that you care about? I'd like to collect as many of these as possible for the time when (if ever!) I decide to jump into writing a better fast generic time library. My use-cases are mostly analyzing and plotting various time series. So, I'd guess that we care about similar stuff, but probably not exactly.
As soon as a revertible property is combined with a non-revertible property, you get a non-revertible property. If you try to revert it, it'll fail to type check. :) I have not implemented combining revertible properties with other revertible properties yet. That will probably combine them so that when reverting, the steps are run in reverse order. Once that's available, it'll be possible to build up larger revertible properties out of smaller ones. Of course there's no free lunch and it has to be revertible all the way down. I suppose you could be asking if it's a good idea to allow non-revertible properties to be expressed at all..
Actually, it is possible to use non-revertable properties as primitives inside revertable properties. You just have to make sure to pair them with other properties in the revert side appropriately. For example, the Docker.docker property does this. \(Container image containerprops) -&gt; let setup = provisionContainer cid `requires` runningContainer cid image containerprops `requires` installed teardown = Property ("undocked " ++ fromContainerId cid) $ -- elided for space in RevertableProperty setup teardown provisionContainer, runningContainer, etc are all non-revertable properties, which get combined into a "setup" property which is used in the left hand side of the RevertableProperty.
Actually, I need to push the revertableness down futher; runningContainer can certianly be reverted easily for example. ;)
For some reason I thought the elf interpreter positioned the sections for ASLR, but it seems I was mistaken. Thanks for pointing that out.
That's interesting, can you say more specifically about why you think that?
One of Yaron Minsky's early papers about OCaml explained that at Jane Street they really didn't like the O. I can't find the paper now though.
No. It is more complicated, and yet fairly simple in the end. All binary executables on a typical modern Linux system are in the same format: ELF. Same for shared libraries: ELF. Same for object files: ELF. You can think of a static library as a zip file containing ELF object files. An ELF file has code and data, and rules for how to load the code and data into memory. Code/data can be "relocatable", which means that you can load it at any address, but you might have to modify it. It can be "position-independent", which means that you can load it at any address without modifying it. It can also be "non-relocatable", which means that it has to be loaded at a specific address. There are performance benefits to non-relocatable code, which is why executables are non-relocatable unless you want the extra hardening. There are also performance benefits to sharing memory, which is why shared libraries are position-independent unless you are doing something crazy. An ELF file can also specify symbols, which map names to memory addresses. These are used to specify the entry point for executables and the exported objects for shared libraries. There is nothing stopping you from creating an ELF file that is both an executable and a shared library at the same time, and people have done this. 
I think you're looking for [the expression problem](http://homepages.inf.ed.ac.uk/wadler/papers/expression/expression.txt) But maybe they mean open classes like ruby or perl have, where you can add methods at runtime.
PR
I don't think I am looking for that. I'm looking for a formal model in the same sense as products/coproducts.
Well of course you will not have type gotchas in a language like Haskell, but at the end of the day does it matter? These are runtime errors that may or may not appear depending on the conditions. When the program goes down in flames, everything stops working and that's that. Finding out why some variable is a list of lists of ints instead of a flat list of ints can be just as difficult as finding a space leak, except you have no memory profiler to help.
Here's how I define `:hoogle` in my `~/.ghci` file so I can pass it various options: :{ {------------------------------------------------------------------------------- Create a create a GHCi command that runs the `hoogle` command line tool. By default, the top three results are output, and the output is colorized. Using +package, +Module.Name, -package or -Module.Name as part of the search string is supported. The long-form options to hoogle search are supported, eg: --color=0 Disable color output --count=int Maximum number of results to print, defaults to showing all results --start=int 1-based index of first result to print, defaults to 1 --info Show extended information for the first results --verbose Loud verbosity --quiet Quiet verbosity Adapted from http://www.haskell.org/haskellwiki/Hoogle#Command_Line_Search_Flags -------------------------------------------------------------------------------} let createHoogleSearchGHCiCommand x = -- separate the options from the search string let (vs, ws) = Data.List.partition (Data.List.isPrefixOf "--") $ words x search = show $ unwords ws -- default to showing color opts = "--color" : "--count=3" : vs in return . unwords $ ":!hoogle " : search : opts :} :def hoogle createHoogleSearchGHCiCommand Then I can use it just like this (you'll have to imagine color): λ :hoogle id Searching for: id Prelude id :: a -&gt; a Data.Function id :: a -&gt; a Control.Category id :: Category cat =&gt; cat a a λ :hoogle id :: a -&gt; a --verbose = ANSWERS = Searching for: id :: a -&gt; a Prelude id :: a -&gt; a -- Exact Data.Function id :: a -&gt; a -- Exact Data.Functor.Identity Identity :: a -&gt; Identity a -- Rebox+PrefixCI λ :hoogle id :: a -&gt; a --info Searching for: id :: a -&gt; a Prelude id :: a -&gt; a Identity function. From package base id :: a -&gt; a 
you might also enjoy the following presentation by tim sweeney of epic games: [the next mainstream programming language: a game developer's perspective](https://www.st.cs.uni-saarland.de/edu/seminare/2005/advanced-fp/docs/sweeny.pdf)
Obviously not caprica, but: I've programmed one non-trivial from-scratch UI object hierarchy with several dozen distinct classes and a good 10 or so quite non-trivial ones, where the UI objects rendered down to HTML but had some extra juice for merging data from multiple sources simultaneously, optionally (so it had to work both "merged" and in a more traditional single-source setting). It models with inheritance quite beautifully. And you know, I say this as someone with experience in enough other paradigms, and with enough general disdain for inheritance, that that means something. It's not like it's all I know and therefore I can't see the flaws. I've also extended existing UI frameworks (GTK and QT) using inheritance to create my own widgets, and it's probably the nicest "extending of a library" I've ever done. It works. I'm not saying it's the only choice for UI widgets either, and I have my eye on the research being done in the Haskell community and think there's room for improvement. But it does work. That said, I'm unable to name any _other_ examples where inheritance is the obvious best choice. (I like "the template pattern", but that's trivial to do just with a function that takes some closures, or any of a few other equivalent approaches, depending on the local language landscape.) I fear it's an example where our entire industry got taken down the garden path because some people had this one idea, and it worked _great_ for this one use case, and so trampled the herd behind them without asking very many questions. (On a smaller scale, the recent craze for "event-based programming" is much the same way; yeah, it works _fine_ to write a simple socket proxy, and yeah, it's better than your old threading world, but that does little to prove it's a good idea _in general_... we really ought to demand more than one example before we bend entire industries to fit....)
&gt; OO is just not that big of a deal i'm struggling to understand in what context this statement is true. do you mean that in regards to haskell, OO isn't that big of a deal?
I believe his comment is referring to the fact that OO isn't as useful as some would believe given better tools for structuring and writing code.
I mean that OO doesn't really bring as much to the table as its proponents often claim. Most of the features of OO that are often cited as evidence that OO is a good thing could be provided independently of the rest of the OO baggage anyway.
When I started looking into "modern" functional programing, i.e. not Lisp, the ML branch seemed to be ending at OCaml. SML/NJ is still alive but does not appear to have as much interest. I never got the impression it was the Object part of OCaml but the library.
Well certain Simulations (of which GUIs are an example because they simulate mostly real world objects) are most easily thought of as a bunch of independent agents that pass messages to each other and maintain some internal state, perhaps synchronized by an event loop. If you do not want to decide in advance what kinds of agents (e.g. widgets in the case of UIs) you want to support, what messages they can understand and what their internal state is, this becomes hard to implement without something like inheritance. This is not a formal statement, just something I have learned from experience. It certainly can be emulated in Haskell (see for example Snaplets) but it is clumsy in comparison. Part of the reason is that Haskells typesystem does not support subtyping.
Entertaining lecture. I'm teaching myself Haskell and I have an interest in going into data journalism so this is relevant to my interests. This has inspired me to try out something fun with conduits and tweets.
I don't think it's not about runtime extension. For example, template Haskell is entirely compile-time - you can't generate more code at run-time. I'd be surprised if there's a formal model of open sums and products, though I'm no expert. I think that's really an analogy here - the relevant formal models are about typeclasses, not sums and products. By the time you link and run the code the sets are closed anyway, so I assume if you want specifically-sum-and-product-related formal models, the same ones apply as for closed sums and products. 
I used online ideone.com compiler. And second version is really 7x faster, you can try that yourself online.
I was just thinking of multiple inheritance and multiple classes inheriting from the same class, type theoretically this corresponds to something I dont know the name for and subtyping. Intuitively speaking in one case you have arrows (is subtype of) in in the category going from multiple objects (in the category theoretical sense) to one in the other case multiple arrows from the super class to all the inheriting classes, ideally you would want those diagrams to be limits and colimits respectively I think. Formally they would probably be called fibre and cofiber products. They are "open" in a practical sense, you don't need to decide in advance which classes will inherit from a given superclass, which is especially important for something like a GUI library, because you want to write custom widgets while integrating into foreign code. All this really means is that you can form multiple (co)fibre products in a suitable category and that they are all compatible with each other. I am not an expert, one of the formalisations of object oriented programming is in [http://userpages.uni-koblenz.de/~laemmel/expression/long.pdf]. I would welcome suggestions for further reading.
I am by no means the most knowledgeable person on this subject, but I don't see why you couldn't just use a type synonym in the case of, say, long types, for example. I can't really see any other uses, as it will just be an expression that get substituted during the type checking stage, I assume. type Foo = Maybe (LongType a b c d e f g) foo :: Test Foo 
This is really no different to using a named function as an abbreviation. If you're not going to re-use that function elsewhere, it makes sense to use `where` to limit the scope of the name. That way, you can e.g. use `x` as your short name without worrying about all the other uses of `x`. So yes, you can use a type synonym - I just think a `where` clause would sometimes work better. 
I was going to mention snaplets as well. Or Yesod widgets. Or Angular directives. There's always some kind of ... *composition* going on in UIs. I can't articulate what's missing in Haskell, but there *is* something. Part of the problem with "OO" is that it is a nebulous term encapsulating many different ideas, all ad hoc and designed by art rather than science. But there are things in there that can, and should, be teased out and properly defined. Maybe. I guess.
Congratulations on the baby!
I like the idea of where, but I also think you should still use type synonyms inside it. However it creates some problems. If you want to see signature of Test(say :info) what do you show? Expanded synonyms? Because putting them into where clause effectively hides them from the user.
Not speaking of OCaml but objects in general are nice as value parameterized modules.
Actually, just your example shows a potential problem. You say data Test = Test x where x = Maybe x Would you expect the type checker to expand this to data Test = Test (Maybe (Maybe (Maybe (Maybe (Maybe (Maybe ... ?
&gt; The O in OCaml is not used pretty much at all It's used in LablGTK (and therefore a lot of OCaml programs needing a GUI) and major OCaml programs like Matita for logging and pretty printing.
If recursive types like this are useful then conceptually yes, but actually no. Think of it a bit like infinite lists. If it's useful (which I'm not claiming because I don't know) then the point is *not* to try to expand the whole thing, but only expand as much as is needed. So for the natural number representation thing you have... 0 Nothing 1 Just Nothing 2 Just Just Nothing ... The type checker doesn't need to expand the infinite nesting of `Maybe` - it only needs to check as far as the `Nothing`. The type is the "least fixed point" in a similar sense to the fixed points of recursive functions. If I understand correctly, this is already possible (with a different spelling) and at least someone found a use for it - see [here](http://www.reddit.com/r/haskell/comments/211j2a/recursion_schemes_and_functors/). The relevant bit from the linked article is this... data Mu f = In (f (Mu f)) And that type (I think) *is* the fix function translated to work in the type system. Anyway, if this isn't useful and shouldn't be allowed, it could be checked for and rejected. 
I don't have first hand experience. I'm just repeating what Yaron and three of my other friends who work in OCaml professionally said.
What about: show . (id :: Double -&gt; Double) . read or: (show :: Double -&gt; String) . read or: show . (read :: String -&gt; Double) It needs the ScopedTypeVariables extension, but that's a fairly benign one.
That would make the language entirely unusable for beginners and cause a proliferation of mutually incompatible preludes. Look at Scheme, it's still impossible to write real programs with it, you have to use a specific dialect.
How old is that cheat-sheet (as it suggests the anachronistic Hugs interpreter only; moreover, the link to visual Haskell is dead)?
I dislike putting type annotations on `read` and `show` because they both mention a type (`String`) which is irrelevant for constraining the middle type.
I don't know of anywhere near as many instances of FP proponents making unreasonable claims as OOP proponents making unreasonable claims. I certainly don't know of any baggage that comes with FP; functional programming is a much simpler idea than objects. Do you have anything specific in mind?
There's an existing proposal for partial type signatures, IIRC. I must admit though, `show . (:: Double) . read` looks very nice.
Very neat and easy to listen presentation. Thank you.
Can you ask more specific questions?
Really cool talk! I just wish the question section at the end wasn't cut off. That's a non-issue after 30 minutes of "Haskell gets the job done, quickly and well" though.
perhaps you should have started on it sooner then the day before your assignment is due?
I like the proposal. I'm very much in favour of the (slightly) more complicated desugaring of `(:: T) ==&gt; (id :: T -&gt; T)` because of the principle of least surprise: the detail "cannot use quantified types" is not and should not be relevant to the end user. I'm assuming the `(:: T) ==&gt; (\(x :: T) -&gt; x)` was not considered because of the dependency on ScopedTypeVariables? (It would be equivalent to the `id` desugaring anyway)
The initial explanation of `(:: T)` does indeed treat `(:: forall a . a)` as a 'coercion' `(forall a . a) -&gt; (forall a . a)`. All I was trying to say in the last section is that since such a 'coercion' is useless we might want to have a different translation. But I don't really think it's worth having a different translation.
Regarding the hypothetical further extension: "In general, we have to keep quantifiers and qualifiers at the top, i.e., `(:: forall a . a)` turns into `(id :: forall a . a -&gt; a)`." As written, this feels completely unintuitive to me. `(:: forall a. a)` should be, if *anything*, something that forces its argument to be itself quantified. While `(id :: forall a . a -&gt; a)` is of course just `id` without restrictions.
Exactly my issue as well.
+1 This is such an elegant syntax for something I have needed so many times.
The extension that allows polymorphic types is indeed needed. For example, I would use this all the time: (:: Either SomeException a) . try So it is indeed worth extra effort to get that in. But not if bikeshedding about the exact semantics and implementation would significantly delay the feature as a whole.
What? Are they referring to pattern matching, or...? Really, every language is different. Refusing to teach a feature because it's "weird" and not used in other languages is just stupid. Besides, if it *is* pattern matching, there are other languages that use it - the ML family of languages, for example. 
I think `PartialTypeSignatures` would help: (:: Either SomeException _) . try 
Yup. Pattern matching. Something like fib 0 = 0 fib 1 = 1 fib n = n + fib (n-1) is apparently bad and wrong, despite being idiomatic Haskell that every Haskell programmer under the sun understands perfectly well. Here's the pdf: http://cs.anu.edu.au/student/comp1100/Assignments/Assignment%201.pdf
Partial type signatures are a feature I use a lot in F#. I don't know what the proposal looks like, but by copying the F# syntax it would look like this in Haskell: (show :: Double -&gt; _) . read
Yes: http://cs.anu.edu.au/student/comp1100/Assignments/Assignment%201.pdf Someone must have insulted the author's mother with a carefully crafted function or something.
One idea I've semi-seriously suggested before is that `where` should be a namespace mechanism and always introduce a named scope (that may or may not export any bindings, depending on how it's used). So a type synonym A attached to the declaration of a data type Bar in a module Foo would be displayed as Foo.Bar.A just like if Foo.Bar was a separate module.
Just use the `@faq` facility on `#haskell`.
But the worker function is in this case itself recursive, or am I misunderstanding something?
That's what it will look like in Haskell too. Partial type signatures are indeed very useful. 
Yeah, sometimes `case` is better - for example if you want to define a helper-function in a `where`, but you want to use that function for several patterns. Most of the time, though, plain pattern matching gets the job done beautifully.
This is the problem that the `zoom` function from `lens-family-core` solves. It's type is: zoom :: Monad m =&gt; Lens' a b -&gt; StateT b m r -&gt; StateT a m r The idea is that the lens you pass to `zoom` points to a substate (of type `b`) of your global state (of type `a`). The second argument (a stateful computation that operates on `b`) is then automatically converted to a stateful computation that operates on the global state of type `a`. So let's say that you have two stateful computations that operate on states of types `S1` and `S2`: computation1 :: StateT S1 IO () computation2 :: StateT S2 IO () You can combine these into a single stateful block using the `_1` and `_2` lenses: both :: StateT (S1, S2) IO () both = do zoom _1 computation1 zoom _2 computation2 This is an example of the [functor design pattern](http://www.haskellforall.com/2012/09/the-functor-design-pattern.html), specifically the monad morphism pattern. You have two computations in different kleisli categories (specifically, the `(StateT S1 IO)` and `(StateT S2 IO)` kleisli categories) and you want to unify them to agree on a common kleisli category (the (`StateT (S1, S2) IO)` kleisli category). `(zoom _1 .)` defines a functor from `(StateT S1 IO)` to `StateT (S1, S2) IO` and obeys the following functor laws: zoom _1 . (f &gt;=&gt; g) = (zoom_1 . f) &gt;=&gt; (zoom _1 . g) zoom _1 . return = return Similarly, `(zoom _2.)` defines a functor from `(StateT S2 IO)` to `(StateT (S1, S2) IO)` and obeys the same functor laws: zoom _2 . (f &gt;=&gt; g) = (zoom_2 . f) &gt;=&gt; (zoom _2 . g) zoom _2 . return = return When you write these functor laws in terms of `do` notation you get the monad morphism laws: zoom lens $ do x &lt;- m = do x &lt;- zoom lens m f x zoom lens (f x) zoom lens (return r) = return r In other words, these monad morphism laws say that `zoom lens` "distributes" over `do` notation. To learn more about using `zoom` to isolate state you may want to read this other post I wrote on [how to architect imperative-like programs using lenses](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html).
It doesn't look particularly ugly to me. If I'd written it I would have done fOfObjects = fmap fst . mfilter ((&lt; maxDistance) . snd) . minimumByMay (comparing snd) . mapMaybe (\o -&gt; (o,) &lt;$&gt; rayObject ray o) f = fOfObjects objects but that's not a big deal.
A word of general advice. In programming communities people tend to really dislike it when students post "do my homework" ads with titles like "URGENT" "HELP ME!!" or basically anything with ALL CAPS and a lot of !!!1!s. We like to pretend that we're professional[-ish] here, so if you wouldn't put it on a class forum or similar, you probably shouldn't do it here. We also don't like doing your homework for you. It's not that we're not willing to help, but we're not your TA's; there are people who get paid to help you with this sorta stuff. We don't like feeling like your yelling at us to do your work :) More over, most profs I know have figured out how to use google and copying someones work from online is usually a one way ticket to failing. Finally, as far as I know, you are unable to download /r/haskell to your brain for your midterm/final! It's a heck of a lot better to tank on a homework and learn now vs tank on your final.
Yep, and I sure hope it gets implemented. It'll make pair programming with the typechecker a lot more enjoyable :-) https://ghc.haskell.org/trac/ghc/wiki/PartialTypeSignatures This isn't so bad either: show . (read :: _ -&gt; Double)
That signature issue could, I think, could be resolved if it was worthwhile. The signature you display would have to have a `where` clause too, based on the original. There's probably serious problems with that, though - I haven't thought it through, though types inferred in complex ways would seem an obvious source of problems. Ban infinite types and the problem probably gets a lot easier. 
I think rust needs to careful about its complexity budget if they want a meaningful number of people to adopt it. The memory stuff is already a bit tricky. If they want it to be a drop in for C++ for game programmers, anything beyond the safe memory is a bad idea for adoption.
love this proposal. as an avid user of ClassyPrelude, this addresses one of the larger warts of its polymorphism.
I used to read those. Now i do not see the point. Basically those news turned into a rehash of reddit. /r/haskell is my Haskell Daily News :) 
Too bad there doesn't seem to be any free online version of that. I'd love to be able to link people to chapters of that book like I can with [LYAH](http://learnyouahaskell.com/chapters), [RWH](http://book.realworldhaskell.org/read/), and [PaCPiH](http://chimera.labs.oreilly.com/books/1230000000929)...
My favorite compelling example is you can write fixpoints as if it where lambda calculus fun f -&gt; (fun x -&gt; f (x x)) (fun x -&gt; f (x x)) of course in a strict language ⊥ is a fixpoint so OCaml will just loop forever on this :) Also yes, you can always simulate -rec-types with something like data Mu f = Roll {unRoll :: f (Mu f)} These are called "iso-recursive" types since a recursive type is isomorphic to its unrolling (via `unRoll` and `Roll`). So it's a syntactic pain but not a conceptual limitation.
&gt;While the memory stuff is tricky, it's also absolutely neccesary to write safe C++ code; the compiler can just help you manage it. Right, and I think you could convince most developers who write C++ primarily that the extra complexity is clearly worth it. &gt;I personally would like to see HKT in Rust for the same reason I enjoy them in Haskell: even if I don't actually write anything using HKT, I want library authors to be able to. Option in Rust is so much less convenient than Maybe. You like them, and I do to, but what's the goal of Rust? To be the "best" language, or to be a legitimate challenger to C++? I feel like how Rust is now, every feature could be easily justified to a C++ dev. memory safety, results instead of exceptions or return codes, etc. are all clear wins. But, if I've never worked with Haskell, or some other language with HKT, the benefits aren't nearly as clear. My biggest suggestion is to make sure you have the input of AAA game programmers before you add a feature like that, if rust is still trying to go after that market segment.
Is that "equality witness" a run-time overhead? I seem to remember something mentioned in a talk video (SPJ, University of Oregon Summer School) but I really need to watch that again (and take better notes). Something about a needing to handle two different kinds of type-equality so that a type alias in between can't allow two different types to unify, though I may be thinking about an entirely different issue and getting it completely wrong. I don't think I'm likely to care about a small overhead - at least not normally - but I'm a little surprised there could be a run-time impact for that. 
PLEASE stop calling it the "Functor design pattern". It is not a design pattern. Also, pretty sure the OP is asking a more general question.
C++ already has template template parameters, so HKT needn't scare away too many users.
Well, the issues that arise are more subtle. e.g. it can prevent you from using a `newtype`, or the zero-width witness might somehow cause you to have to use a different calling convention that doesn't fall into the nice fast set, down under the hood.
Your reply sounds snooty. Why isn't it a design pattern, in your opinion?
A [link](http://source.mozillaopennews.org/en-US/articles/model-analysis/) to a write up about the fashion week project
Hi, author here; I have asked parts of this questions at various times and I have never been able to ask the "whole" question, so I decided to put it up as a write-up in case anybody has the time to read it over and maybe offer some tips, or in case anybody can learn from it. I have been careful to journal my thought processes during the entire thing and highlight what parts about the interface I find the most attractive. Of course if someone has already done this but better, I'd love to check it out too :)
Hm. But the cost is now dominated by the traversal, instead of the computation of the function itself, right?
I really mislike the asymmetry in treating `(:: forall a. a)` as `(id :: forall a . a -&gt; a)`. We need some way of handling quantification, but I don't think the proposed suggestion is the right way to go, as it complicates the mental model too much. In most cases, using scoped type variables or partial type signatures can handle whatever polymorphism we want. The corner they don't cover is when we need to introduce a new type variable and yet also use that variable more than once in the `T` of `(:: T)`.
I totally agree with you.
Small typo, the type signature for zoom is: zoom :: Monad m =&gt; Lens' a b -&gt; StateT b m c -&gt; StateT a m c Otherwise I couldn't understand how the above could type check :)
Notably, recursive types (of any sort) only cover a subset of the infinite types. Some infinite types have non-recursive definitions (e.g., the type `f1 (f2 (f3 (f4 (...))))` cannot be defined via recursion).
Don't forget the #haskell quotes! 
For anyone wondering about the infinite case, I tried this... {-# LANGUAGE GADTs #-} module Main (main) where --data Test = Test x where x = Maybe x data Test where Test :: s ~ Maybe s =&gt; s -&gt; Test test :: Test test = Test $ Just $ Just $ Nothing main :: IO () main = putStrLn "Hello" and got the following error... [1 of 1] Compiling Main ( test01.hs, test01.o ) test01.hs:12:8: Occurs check: cannot construct the infinite type: a0 = Maybe a0 Expected type: Maybe (Maybe (Maybe (Maybe a0))) Actual type: Maybe (Maybe (Maybe a0)) In the expression: Test In the expression: Test $ Just $ Just $ Nothing In an equation for `test': test = Test $ Just $ Just $ Nothing "Occurs check", I assume, refers to a self-reference check. However, the error was for the attempt to construct a value, not for the GADT itself - it looks like you can define the infinitely-typed component, but can't have values of that type. Constructing `test = Test undefined` also fails with the occurs check (it's really the component type that's rejected, not the value), though `test = undefined` compiles, so it looks like at least that one (finite) value is still legal and I'm guessing any other constructors for the same type that don't have the self-referencing infinite type issue are OK, though I haven't tried that. I have no idea why the expected and actual types are shown differently, though they look equivalent anyway, and as it's not legal it doesn't matter much. 
The thing which allows value-level fixpoints is a bit more complicated than that. The unitype of untyped lambda calculus is `(fix x. x -&gt; x)`. The thing which allows value-level fixpoints, and hence non-termination, is the fact that `x` occurs in a negative position in the constructors of `(fix x. x -&gt; x)`. If you disallow non-strictly-positive use of type variables, then you can't get non-termination out of the remaining recursive types. Of course, some non-strictly-positive recursive types are perfectly fine; but it's tricky to specify which ones are okay and which are not.
Well, we don't allow the LHS to occur on the RHS in `type` definitions (i.e., `type Foo = Bar`). So if that's the only reason, then we could use that same syntax to eliminate the confusion, since `where` clauses are just giving local type alias definitions: data Foo = Bar where type Zot = Quux Rather, I think the syntactic problem has more to do with the fact that "`where`" already has a different sort of meaning at the type level; e.g., `class...where`, `instance...where`, and indeed `data...where` with GADTs. So we could easily introduce type-level `let`-binding, but introducing type-level `where`-clauses would be a lot trickier
There is a syntactic issue though: "`where`" already has a different sort of meaning at the type level; e.g., `class...where`, `instance...where`, and indeed `data...where` for GADTs. So we could easily introduce type-level `let`-binding, but introducing type-level `where`-clauses would be a lot trickier.
The curated stackoverflow content is my favorite part. I don't know why stackoverflow doesn't provide a similar feature automatically.
So in the ML family (unless you add explicit laziness?) the least and greatest fixed point are different things? My knowledge of the theory is *extremely* limited. It had occurred to me that if there's a least fixed point there must be other fixed points, but I assumed there was no significance. There's always so much more to learn! 
Thanks! I totally missed that.
Some kind of `where` or `let` in type expressions makes perfect sense. But your particular example would still have illegal recursion.
Yes, I forgot about that. There's also some semantic difference between `let` and `where` I think - I'll have to check what it is again. 
hoogle data shouldn't crash, but hoogle data all can easily run out of memory. Does it give an error when it crashes? Perhaps raise a ticket on the Hoogle github: https://github.com/ndmitchell/hoogle/issues?direction=desc&amp;sort=updated&amp;state=open
&gt; I was reading a lot of comments on Reddit about how FP is the future or the One True Way or Haskell proves correctness. Any links? People say that FP advocates say these things all the time, but I rarely see it. I'm in a lot of FP circles, so your point that there are fewer of them doesn't really apply to me. Edit: To be clear, this isn't just a challenge to your claim. If you do give me recent links to such occurrences, I will consider replying to set them straight and prevent more misinformation. &gt; We've all made ridiculous claims at one point or another; let's not pretend that *we're* the sane ones. You're right, but we're not talking about the existence of ridiculous claims but the prevalence among proponents. Everything you say about baggage is about languages. I'm not talking about languages. I'm talking about functional programming. I don't think anything you said has contradicted me at all. Just because languages have baggage doesn't mean every feature of those languages have the same amount of baggage. My claim is the functional programming doesn't have anywhere near the amount of baggage that object-oriented programming does, and I don't think it's even a remotely controversial thing for me to say. Functional programming is little more than working with values. The various incarnations of objects are a smörgåsbord of features.
ML family languages tend not to offer (direct) support for greatest fixed points. But you can simulate laziness in the usual way, by using the type `() -&gt; x` for thunks containing values of type `x`. Thus, if we consider lists we get: -- a la Haskell: data LFP_List a = LFP_Nil | LFP_Cons a !(LFP_List a) data GFP_List a = LFP_Nil | LFP_Cons a (LFP_List a) -- a la ML: datatype 'a LFP_List = LFP_Nil | LFP_Cons of 'a * 'a lfp_list datatype 'a GFP_List = GFP_Nil | GFP_Cons of 'a * (() -&gt; 'a lfp_list) I assume that last line works, but if not then the idea still works (assuming there are no side effects going on in that function). The "least-ness" and "greatest-ness" can be thought of in terms of taking the fixedpoints of `f x = 1 + a*x` interpreted as a function on sets (i.e., `1` is a fixed singleton set, `a` some fixed arbitrary set, `(+)` is disjoint union, `(*)` is cartesian product). The smallest set `y` such that `y = f y` is going to include all the finite length lists, and that's it. Whereas the largest `y` such that `y = f y` is also going to include all the infinitely long lists. In both cases we have that `a*y` is already a subset of `y`, which is why applying `f` again doesn't change anything. Notably, the above is just an example. It assumes that types denote sets, which need not be the case. In Haskell, we often think of types as denoting CPOs, since that gives us a way to capture the structure of having infinite values etc. In dependent type theory, we often think of types as denoting ∞-groupoids. Etc. But in all cases it comes down to looking at functions like `f x = 1 + a*x`, interpreting them under the appropriate denotation, and then figuring out what the least and greatest `y` such that `y = f y` (where `y` is a valid denotation of a type, and we have some notion of ordering on the denotation of types).
I don't think there are semantic differences, it's just that they have different scopes and belong to different syntactic categories
Well, many projects create a giant monad transformer stack containing all the global state used by their entire application, and use it throughout their project. It's a common technique, but I don't like it because it decreases reusability. I prefer to code each of my modules as if they were a library, which would need to work in any program. Even when my modules use a monad stack, I don't expect my users to write their code *in* that monad stack. Instead, my modules tend to focus on a particular datatype, which may or may not contain monadic computations. My module provides primitives and combinators allowing my users to construct the value of that datatype which describes the behaviour they need from me. Then, I usually provide a `run` function, which takes that complex behaviour-describing value and evaluates it in the monad of their choice. During that evaluation, I might temporarily add a few layers of state, if needed, but the caller will never see it. Thus, my users are free to use the monad of their choice, not a monad I impose them. Similarly, if I am writing a module which depends on another module, I can again use the monad of my choice and not a monad chosen by that other module. I guess the main difference from an OO design is that I don't expect users to call methods from my module at many different points in their program, so I don't need to maintain state between those calls. Instead, I help my users to construct a large value containing everything I will ever need to interact with, and then they only make a single final call.
I am open to having no standard prelude. Educators can use their own beginning preludes, where compatibility doesn't matter. Experienced users might use no prelude whatsoever, or import some custom prelude explicitly. Also, Custom preludes would be imported explicitly, mitigating comparability. Agreeing on rest of base, especially the standard class hierarchy, is still very important. Then alternative preludes should draw from the same imported definitions, not dream up new stuff.
Wouldn't it be simpler to use Cont rather than codensity for this? Then liftProb just has type `Ord r =&gt; ProbData a -&gt; ContT r ProbData a` doesn't it? EDIT: Also I think you meant newtype RCodensity c m a = Cod { runCod :: forall r. c r =&gt; (a -&gt; m r) -&gt; m r }
Not everyone reads reddit!
Yeah, I didn't like the `r`s littering my type signatures though. I guess you could encapsulate that in the definition of `Prob` though. Also, for some reason ContT has a Monad constraint on its `m` parameter in its `Monad` instance. I don't know why. And yes, you're right about the constraint parameter. It's correct in the linked source but wrong in my writeup. I'll fix it.
Yes of course. I'm merely commenting on the effect reddit has on my internet browsing habbits. HWN is not the only digest i stopped reading because of reddit.
Well, the r you would have is just the same as the c you have for the restricted codensity you defined really isn't it? They're both a restriction on the result type, and both an extra type parameter on your monad/transformer. So personally I'd be inclined to use `ContT` in this case.
Application: I wanted to know how unlucky it was that I died on the last turn in Hearthstone to Avenging Wrath from my opponent with 3 hp and 5 creatures on the board: minionHp :: [Integer] minionHp = [5,3,1,1,1] myHp :: Integer myHp = 3 -- pew is the sound a laser makes :) pew :: [Integer] -&gt; Prob [Integer] pew hps | all (== 0) hps = return hps | otherwise = do idx &lt;- uniform $ map fst $ filter (\(_,hp) -&gt; hp &gt; 0) $ zip [0..] hps let curHp = hps !! idx let rHp = curHp - 1 return (take idx hps ++ [rHp] ++ drop (idx+1) hps) pewpew :: Integer -&gt; [Integer] -&gt; Prob [Integer] pewpew n hps | n &lt;= 0 = return hps | otherwise = do hps' &lt;- optimize $ pewpew (n-1) hps pew hps' avengingWrath :: Prob Integer avengingWrath = do (me:_) &lt;- pewpew 8 (myHp : minionHp) return me -- find the probability that I end up with 0 hp after Avenging Wrath resolves badBeat :: Double badBeat = maybe 0 fromRational . M.lookup 0 . runProbD . runProb $ avengingWrath -- *Prob&gt; badBeat -- 0.26670003485082305 So my opponent got a bit lucky, but not as lucky as I thought! 
doc properties: created/modified Oct 20 2007
can you give an example of a weird manual CPS transform? :)
Yep :) That's why in total function languages like Agda and Coq disallow things which aren't strictly positive. However this isn't the case in Haskell nor OCaml as far as I know.
For anyone curious, Roman's comment seems to suggest that the issue is with Wine, not Windows, though I'm not sure if he tried both the simple case, or the full example. Should I update the title to prevent people from clicking on what is now a misleading title, or should I just leave it be? 
There's a paper by Neil Sculthorpe in ICFP last year about what's going on here [http://www.ittc.ku.edu/csdl/fpg/papers/Sculthorpe-13-ConstrainedMonad.html](http://www.ittc.ku.edu/csdl/fpg/papers/Sculthorpe-13-ConstrainedMonad.html)
...just to ensnare people who have never done anything but Object Oriented Programming and are scared of Functional Programming.
+1 Love the idea.
There are two possibilities. One is to newtype Int with smart constructors that ensure evenness; the other is to observe event integers is isomorphic to integers and represent them that way. It depends on what you want to do...
I literally asked about this exact thing a couple of minutes ago on #haskell. Quite a surprise to see something detailing it here! Thanks!
Perhaps a naive suggestion.. But let's consider some rules, an even number times an even number is an even number, right? Same with add and subtract? What if we newtype or data an Int, and when we "unwrap" it, we just multiply it by 2?
I've done this before. The problem is Codensity reassociates all of your (&gt;&gt;=)'s to the right, so if you have any sort of fanout in your probability tree or Set monad, etc. this leads to an explosion fast. No events collapse back down until the end, so you wind up having to thrash in and out of the monad or wait for the heat death of the universe. It is useful for short jaunts, but not long calculations. My covector monad works this same way.
I believe the point of "makeClassy" and friends is that the "programming in Prolog" (that makes it all work) is already done for you. The true issues, then, are: * trusting that it has been done for you correctly * understanding how to use it (and how to read related possibly-cryptic error messages when you use it improperly) * incurring the cost of Template Haskell
There haven't been any Monad constraints on the `m` in `ContT r m` since 1.x or so: MonadState s m =&gt; MonadState s (ContT r m) MonadReader r' m =&gt; MonadReader r' (ContT r m) MonadTrans (ContT r) Monad (ContT r m) Functor (ContT r m) Applicative (ContT r m) MonadIO m =&gt; MonadIO (ContT r m) MonadCont (ContT r m) No constraints except where needed these days. What verson of `mtl` are you looking at?
Also, note you can simplify down to just needing a class constraint on `r`, sans `m`. newtype Free p a = Free { runFree :: forall r. p r =&gt; (a -&gt; r) -&gt; r } Now you can play with `Free Monoid`, `Free Default`, `Free Semigroup`... 
I haven't seen his slides, but I've heard him talk about it; from what I can gleam, it's an Applicative interface? But I should probably look into the implementation and see if he conquers similar problems.
Simple. module Even (Even, fromEven, toEven) where newtype Even = Even { fromEven :: Integer } toEven :: Integer -&gt; Maybe Even toEven x | x `mod` 2 == 0 = Just $ Even x | otherwise = Nothing 
You're wrong.
I'd explain why but you didn't even take the time to explain why you thought you had a point.
Isomorphic as... an additive group? 
Yes, that's the definition of programming in Prolog: trusting the constraint solver to do everything for you correctly.
Added a 4-point tl;dr, after realizing that the problem statement was way too verbose :) 1. Compose parallelizable computations using expressive proc notation. 2. Consolidate and join forks to maintain maximum parallelization. 3. All data dependencies implicit; allows for nice succinct direct translations of normal functions. 4. All “parallelizable” functions can also trivially be typechecked and run as normal functions, due to arrow polymorphism. The main problem: * Consider `ParArrow a c`, `ParArrow b d`, `ParArrow (c,d) (e,f)`, `ParArrow e g`, and `ParArrow f h`. We execute the first two in parallel, apply the third, and execute the second two in parallel. Basically, we want two independent `ParArrow a g` and `ParArrow c h` that we can fork. And this is possible, as long as the "middle" arrow does not "cross-talk" --- that is, it can't be something like `arr (\(x,y) -&gt; (y,x))`.
Urgh, I think google led me astray into an old version of the library. I just checked in ghci and you're right, no constraint.
Arguably, the unitype of the untyped LC can anything, not just `fix x. x -&gt; x`, since there's no inherent connection between types and terms in the untyped LC like in the STLC.
http://api.stackexchange.com/docs/questions#pagesize=20&amp;fromdate=2014-03-01&amp;todate=2014-03-31&amp;order=desc&amp;sort=votes&amp;tagged=haskell&amp;filter=default&amp;site=stackoverflow&amp;run=true Someone needs to write a nice UI around it.
Because then doing this: fmap fromEven (toEven n) == Just n would be false for all non-zero values of n. It might be a bit confusing if the inverse of wrapping the value produced a completely different value that was wrapped. 
Minor, but for what it's worth (Even a) * (Even b) = Even (a*b) Is `2*a*b`, I think.
The representation here represents 2 as 2, not as 1.
I knew they had an api, I'm just shocked they don't offer it in an easy web interface.
Hi Andy, is there a free pdf of this paper? There's also [How to restrict a monad without breaking it](http://okmij.org/ftp/Haskell/set-monad.html) by Oleg.
You're right, I didn't even notice! May as well finish the remaining items up (as I imagined them): (Even a) * (Even b) = Even (2*a*b) signum (Even a) = Even (signum a) -- Likewise, law-breaking fromInteger i = Even (i `div` 2)
every integer 2*n is represented as n?
I don't understand what Arrows are in the first place to try and understand the originial post. But in that case, I'd like to point out that if f is very slow to execute, you get higher utilization (and thus parallelism) the longer the list is (when there's more work than workers). Of course as long as you're not running many "workers" on each CPU simultaneously -- cache trashing effects and so on.
Well, the mutable state must be explicit. I think the most common way is to declare a ReaderT monad with the readable state being a data structured comprised entirely of 'IORef's or 'MVar's. You can use modules of the lens package to ease the pain of writing functions that update these values. module App.Main where import App.DB import Control.Applicative import Control.Concurrent import Control.Monad.Reader import Data.Monoid data GlobalState = G { gDatabase :: DB, gSocket :: Socket, ... gFiles :: MVar (Map FilePath Handle) } initGlobalState :: IO GlobalState initGlobalState = return G &lt;*&gt; App.DB.newDBHandle &lt;*&gt; socket AF_UNIX Raw defaultProtocol ... &lt;*&gt; newMVar mempty newtype GlobalM a = GlobalM { globalMReader :: ReaderT GlobalState IO a } deriving (Functor, Applicative, Monad) instance MonadReader GlobalState GlobalM where ask = GlobalM ask local f (GlobalM reader) = GlobalM (local f reader) instance MonadIO GlobalM where { liftIO = GlobalM . liftIO } You application's main module would look like this: module App.Main where main :: GlobalM () main = do -- do something with the database handle: db &lt;- asks gDatabase &gt;&gt;= liftIO . App.DB.startDBQuery ... -- do something with the socket: sock &lt;- asks gSocket ... -- do something with the map of file paths to handles: asks gFiles &gt;&gt;= liftIO . flip modifyMVar (\m -&gt; openFile config ReadMode &gt;&gt;= return . insert config) Your main program would look like this. module Main where import App.Main main :: IO () main = initGlobalState &gt;&gt;= runReaderT (globalMReader App.Main.main) If you would prefer a pure state, you could use the (StateT GlobalState IO a) inside of the GlobalM newtype instead of a ReaderT, and you could use "get" instead of "ask", and "modify" to modify the state, and you would not need 'IORef's or 'MVar's as long as you did not share the state between threads. Apart from that, the above would work the same. 
The only `Monad` issue I see here is that `Maybe` happens to instance the `Monad` class, but that doesn't seem relevant here. Which approaches do you mean? 
Possibly it's because you forgot to mention that the wrapping divides by two. Also, one slight awkwardness with that - it's either an incomplete function, or it returns a `Maybe`, or it silently turns odd numbers into even ones. That's just what happens when you translate from a representation that can represent illegal values to one that can't, of course. Anyway, what you're describing is the bijection/isomorphism that ezyang mentioned. 
I don't know if this will help, but I can share my experienced from writing applications: Don't try to anticipate what abstractions(lenses, typeclasses, transformer stacks) you need in the beginning. Start by using "nothing", just pure haskell functions. As your application grows you will see patterns appear, stuff that you have to type again and again, and realize that "If I had X(state monad,error monad.. etc), this would be much easier". Then you introduce that abstraction in your code. This is one of the strengths of haskell, it's easy to refactor your code this way, the compiler will help you. You continue in this way and let your application "grow", taking the role of gardener rather than architect. This advice comes from the many times when I believed I knew the right architecture from the beginning and it turned out to be wrong/unnecessary, my code as a result became needlessly complex. I believe that it is easier to add abstractions to your code than to remove them.
It's probably scope that I was thinking of - it seems a bit obvious and trivial, but I often half-remember that there was something but dismiss the actual thing as too obvious and trivial. It's really annoying when your memory starts wearing out. And tomorrow I suddenly get another year older - sigh. Maybe I'll forget how old I am. 
Whenever I land on hackage via a Google search, I always hit up ‘Contents’ and then select the latest (or relevant) version.
If 'toEven' were defined like this: toEven x = Even (div x 2) then what you say is true. But I defined it to store an even number closest to the original stored number.
It's a mix of a monad and applicative. Applicatives can be statically analysed so you batch on applicative and block on monads. I actually think your arrow idea was really cool. Shame you couldn't get it working. The other lib for explicit data parallelism is the Par lib and it's monad instance. Probably also worth looking at if you haven't already.
Lookup the "Hackage-Fu" plugin for chrome. That reduces the pain a lot by changing the links in the upper right to include a link to the latest version and lets you know if you're looking at old docs.
I agree that Scheme is "unusable" in that way. However, I'm not sure if this is a bootstrap problem, or an eternal problem. It could well be that once a library environment has been established, the language can let go of specifying a library. I am sure that for Java, it was a huge benefit in having a large standard library. Maybe it would have been an advantage for Haskell to have such a vast initial library too. I think Java, with huge organizations backing the implementation and quality of the base library, must be considered to be the "best case" for this kind of design. However, when for example the Apache Foundation and Google started building their own libraries, there was still profileration of incompatible libraries, specifically around concurrency and network support. This happene because Sun could not move the standard libraries forward fast enough. It was also partially the result of bad API decisions early on, similar to what happened with Haskell. I think for a mature language, a mechanism like the Haskell Platform is more suitable to specify what is considered "stable". In the Haskell Platform, potentially multiple preludes could be blessed, it can have releases that are much more aligned with what the community needs, and it can scale to handle everything from the prelude to networking to GUI libraries etc.
Maybe SrPeixinho is thinking a monad is involved due to `Even { fromEven :: Integer }`, which looks like things like `ST { unST :: IO a }`, not realising that monads and record syntax are orthogonal?
Yes, once all the `f`s are running this is true. However since a list is a linear datastructure it takes O(n) time to start all the `f`s. If instead the arguments were stored in a balanced tree it would like O(log n) time to start all the `f`s.
They are not turned off by default, they're just -O, not -O2. -O and -O2 weren't so different a few years back, but now switching from the former to the latter can make a significant difference.
Why does it have to be inside an applicative? Can you not just have: whenMaybe _ False = Nothing whenMaybe a True = Just a In which case something like ` \x -&gt; (guard x &gt;&gt;) . Just` works nicely.
This would have tremendous practical value both during development and in production. +1 for adding it to GHC!
After which OP's function is defined as: whenMaybeF :: Functor f =&gt; f a -&gt; Bool -&gt; f (Maybe a) whenMaybeF fa b = flip whenMaybe b &lt;$&gt; fa I think the need to flip here supports /u/Rhymoid's claim that the boolean argument should come first. Then we'd just define it as: whenMaybeF = fmap . whenMaybe
This looks great. Another alternative for now, is ugly Template Haskell hacks ala the FileLocation package.
&gt; People say that FP advocates say these things all the time, but I rarely see it. I'm in a lot of FP circles, so your point that there are fewer of them doesn't really apply to me. Maybe my frequency hypothesis doesn't hold water, but I hang around a lot of OOP (or at least non-FP) circles, and they don't make claims about OOP at all. They talk about using it; that OOP has benefits is assumed and no one really discusses the details. I think this happens more when people of different circles try to communicate to each other, and especially in articles and research papers. &gt;. Any links? No. Honestly, up to a month ago I had a full year's hiatus on programming, and only the past week got back into reddit. Maybe the culture *has* changed. Back in 2012, I would have expected my post above to have a -6 score. &gt; My claim is the functional programming doesn't have anywhere near the amount of baggage that object-oriented programming does... Which is neither provable nor disprovable because we don't have a common definition of baggage and it is difficult to construct a concrete example where this holds. I did not mean to contradict you, but state a difference of opinion.
see also here: http://www.haskell.org/pipermail/haskell-cafe/2013-February/106617.html and the followup here: http://www.haskell.org/pipermail/haskell-cafe/2013-December/111616.html
Yeah I kinda figured it seemed to easy, partly why I wrote this up. Are you willling to share to code from your post in its entirety in a gist or something?
Haskell doesn't even have dependent types per se. What is your suggestion? Do you actually know what monads or dependent types are?
Not really. The isomorphism way is pretty straightforward, and using smart constructors to enforce invariants on types is standard.
From all the good ideas of the thread, some code: newtype Even a = Even a deriving (Eq, Show) toEven :: Integral a =&gt; a -&gt; Maybe (Even a) toEven n = case (divMod n 2) of (n',0) -&gt; Just (Even n') _ -&gt; Nothing fromEven :: Num a =&gt; Even a -&gt; a fromEven (Even n) = n*2 Don't inspect or construct `Even`s with pattern matching or with the `Even` constructor: `toEven` is what is called a "smart constructor", that is, it ensures that all the values created with it are correct. If you made a module, you could avoid exporting them and thus hide the implementation. You would also have to implement `Num`, `Real` and `Integer`: that wouldn't be hard, but I think you would have to pay extra attention by division. Also, I used a parameter to make even types out of *any* integral type. You can use `Int`, `Integer`, `WordN`s... `fromEven` has just a `Num` constraint just in case, but it could have `Integral` too.
Will this be in ghc 7.8? 7.10?
Yes I'm pretty sure. In my case, if the 'last will and testament' flag isn't set, then I don't want to read the corresponding data off the network (because the flag says there isn't any) and I'll end up with a Nothing.
That makes sense to me and I had that originally. I only changed it when I saw it made my maybeGetText function simpler. Code below. maybeGetText :: Bool -&gt; Get (Maybe Text) maybeGetText = whenMaybe getUTF getUTF :: Get Text getUTF = do stringByteCount &lt;- getWord16be T.decodeUtf8 &lt;$&gt; getByteString (fromIntegral stringByteCount) 
The bool function is nice, thanks. I didn't think of the Alternative instance for Maybe. Is there an existing function that's equivalent to your bool (const empty) pure expression?
This seems good except I only want to run my action if the flag is True. &gt; whenMaybe True &lt;$&gt; getLine hello Just "hello" &gt; whenMaybe False &lt;$&gt; getLine hello Nothing What do I need to do to fix that?
I think the "giant transformer stack" design arises because, eventually, that's what your application really is. At the top level an application is `IO` but can be better specified as a stack. The real trick is then knowing how to decompose a stack. Each subcomponent of your program should be type-enforced to see as small a component of that stack as possible. This is why I love `mtl` since it lets you be explicit about what each subcomponent needs and leaves the composition aspect up to the typeclass system.
Well, in spherical-cow theory if parallelization is free and you have n processors then with judicious splitting each thread only has to descend the height(tree) steps. So depending on how the parallelization is kicked off you might get some simultaneous tree descent. With a linked list that's not possible. At least one thread will have to traverse n steps in order to run the last task.
False. No one who wants them currently has the time to add em. Don't make up claims. I think many of the core contributors want HKT, but none currently physically have the time to sort it out. Many many idioms in rust will be greatly improved by hkt. The Main thing has been a decision to not add them till after 1.0 to keep the 1.0 complexity budget manageable. 
This is impossible without monads, because applicatives and functors always run their effects.
Is there any clever way to use IsEven from GHC.TypeLits to make something like type Even = (n ** IsEven n) I imagine there's some clever way to use ExistentialQuantification and singletons to get the dependent pair, but I don't know it.
I'd focus on converting between `Bool` and `Maybe a` first. It's often a good idea to focus on the core data types of what you are trying to do, because in Haskell that's how you achieve clean design. The way I expect this to work is something along the lines of fromBool :: Bool -&gt; (a -&gt; Maybe a) so `fromBool` takes a boolean and gives you one of two functions back: * If you pass in `False`, you get a function that just swallows a value and returns `Nothing`. * If you pass in `True`, you get a function that wraps a value in `Just`. Implementing this is straightforward. fromBool :: Bool -&gt; a -&gt; Maybe a fromBool p = if p then Just else const Nothing Now to achieve the behaviour you want, we employ a little Traversable magic. What we have in the way of types is fromBool True getLine :: Maybe (IO String) but what you want is `IO (Maybe String)`. Fortunately, the `sequenceA` function in `Data.Traversable` does exactly that conversion. So in the end, you have λ&gt; sequenceA (fromBool True getLine) magic Just "magic" λ&gt; sequenceA (fromBool False getLine) Nothing which only executes the side-effects when the boolean was `True`, according to your spec.
Wish I knew! SPJ's response to that last message seemed to indicate it wasn't necessarily slated for a merge. I think perhaps someone could take up the cause and push it through, however. (not volunteering :-P) Also important to note that the ability to generate unique source locations is innately useful for other reasons as well as debugging, not least having to do with various notions of runtime instrumenting built internally to some DSL.
Sounds awesome!
On GHC 7.8 you could make: newtype Even = forall (n :: Nat). Even (Sing (n*2))
x86 CPUs have a decode buffer for small loops, saving time parsing the instructions. See http://www.agner.org/optimize/microarchitecture.pdf. It doesn't work anymore if the loop gets bigger than 64 bytes. Pure loop unrolling doesn't to improve the contents of the loop; you can reassociate integer but not FP math, so there's still the same long dependency chain.
Won't that reduce opportunities for optimalization? E.g. if I do if condition then someValue else someValue It's illegal to optimize it to just *someValue*, isn't it?
Sorry, I wasn't trying to be insulting. All I mean is that there have been no concrete proposals, for whatever reason. Hopefully we'll get kow soon.
I have never found myself needing this kind of thing. It is just not the way that I program in haskell.
I disagree that OO offers state encapsulation, in fact that has always been it's biggest problem for me. State encapsulation is provided by the ST monad in Haskell for example: pure on the outside, stateful on the inside. 
Ah, but a balanced tree allows you to do fire-offs in parallel! So the total time taken to do n fire-offs is O(log n).
You better not use this in production code. Also, `condition` might throw an error, so you can't just optimize that away, either.
I know it's weird and I should just make a custom type if I actually need this behavior. The type should be `(Num n) =&gt; a -&gt; n -&gt; a`, with some way of restricting `a` to a list, whether it's just a list, a list of lists, etc. In the above implementation the input could be a single int; if it makes it easier it would be fine if it only took a list (maybe "of lists") as input. Is it accurate, though, that this is a case in which Haskell's type system makes things more difficult rather than easier?
nah its fine. Honestly theres been several internal proposals in progress, its just that whole objecty self notation hack actually makes it trickier to have a good syntax. You wind up needing to allow Self to possibly have higher kinds like *-&gt;*, which is *right* and kinda cool, but needs some thought to understand the implications fully, and people haven't had the bandwidth to work through the full implications. HKT needs to happen. Rust will not have proper support for writing decently generic code otherwise. 
The problem is that you want something like doll :: a -&gt; 0 -&gt; a doll :: a -&gt; 1 -&gt; [a] doll :: a -&gt; 2 -&gt; [[a]] doll :: a -&gt; 3 -&gt; [[[a]]] .... and the standard type system of Haskell is simply not powerful enough to express that sort of thing. In a dependently typed language such as Idris you might be able to, and there might be some extensions to GHC that allow you to do that sort of thing, but those are not part of standard Haskell.
Ghc really needs something like this.
What you'd like to have here is [dependent typing](http://en.wikipedia.org/wiki/Dependent_type), and Haskell doesn't have it for various reasons. A dependent function's output type can depend on the *value* of an argument, like in your `doll` function, where the amount of nesting depends on the value of `n`. This is rarely a significant limitation in practical Haskell code. In this case, if you'd like to have varying depths of nesting then you should just use a data type that is already variable-depth: data Tree a = Leaf a | Node [Tree a] With this you can easily create your `doll` function (simple iteration will do). The fixed structure of the plain old list is a good thing that comes bundled with strong typing in general. To be honest it is actually possible to do your original `doll` function, but it's rather deeply in the "magic" category for beginner Haskellers, as it relies on a couple of GHC extensions that enable a limited form of dependent typing. [This is what it looks like.](http://pastebin.com/0Rr9VyuD) It's [somewhat nicer](http://pastebin.com/p8B5RDWC) in Idris, an experimental full-dependent langauge. 
think we could convince /u/ezyang that this is relevant to his research? :-P
That is true when you talk about wall-clock time. _Not_ when you're talking about asymptotic complexity. You can think of it this way (at least I do tend to think of it this way): if the linked-list and tree you mention are infinitely large, they will both take the same amount of wall-clock time to traverse and fire-off f at every element. Regardless of how *n* elements are laid out, the *amount of computation* needed is proportional to *n*. Even for a parallel algorithm. The total amount of computation in all threads is equal to the total amount of computation in the sequential one.
I'd recommend, http://research.microsoft.com/en-us/um/people/simonpj/Papers/pj-lester-book/ and: http://research.microsoft.com/en-us/um/people/simonpj/papers/slpj-book-1987/ yes -- they have almost the same title, but it is two different books.
y...you mean its not the stork that brings them?
As the author of the package in question, I don't see how TH is uglier than special variables and pragmas. But you can't achieve LOCATIONTRANSPARENT with Template Haskell. I mostly use monad-logger now instead of file-location, which is similar, but instead of a bunch of imitation functions it is a nice logging interface. It works great when you are already operating in a monad stack.
Generally you want one README file to rule them all, and that can always link to individual package READMEs. yesod &amp; persistent are "mega repos", and use travis. You just need a script to build and test all packages. We have had to make some adaptations for mega repos, but it has definitely been a better approach.
Not that I know of, no.
What?
The same `bool` is available at `Control.Error.bool` today, exported by the `errors &gt;= 1.4.6` package.
If you like, you can make it even weaker with for_ :: (Foldable t, Applicative f) =&gt; t a -&gt; (a -&gt; f b) -&gt; f ()
You might be interested in the type data Bush a = Leaves [a] | Trunk (Bush [a]) This is subtly different than the type proposed by /u/Puttamalac. I leave it as an exercise to figure out how it is different, and what the point of the difference is.
It's pretty straight forward. `fromBool True = Just; fromBool _ = const Nothing` I guess there are simpler functions that are in libraries, but why bother?
I personally avoid lens because it takes fucking forever to install into a sandbox.
I posted [a similar idea](http://www.reddit.com/r/haskell/comments/1rssfo/getting_source_location_info/cdqlmmw) last time source locations were brought up here. I thought it could also be used for more things, possibly even full stack traces if you go wild. Although people brought up some problems with exactly where it was expanded if a function is used in a higher-order fashion.
Check the recent paper of Tom Schrijvers [Partial Type Signatures for Haskell](http://users.ugent.be/~tschrijv/portfolio/padl2014.html)
This is why I said `lens-family-core` and not `lens`. You can find the `lens-family-core` package [here](http://hackage.haskell.org/package/lens-family-core). It only depends on `containers` and `transformers`. Enjoy!
Looks incredibly helpful! LOCATIONTRANSPARENT is a bit hard to read though, IMO. Don't other multi-word pragmas contain underscores?
Those words scared me, but it's about a laziness *annotation*. That's kind of neat.
I should clarify that I totally agree with you. lens-family-core is awesome.
I put this together out of curiosity, and thought some people might enjoy the program or would like to play around with the raw graph data (provided in `out/out.dot`); the same directory also contains a high resolution rendering of the picture seen in the readme, including package names. Due to the high interdependency of Hackage, it's unfortunately very difficult to get an interesting graph out of it. Local islands begin to form as the popular packages are hidden from the drawing (e.g. mtl, ByteString), but then it's not really a representation of Hackage anymore. Because of this, I think it's mostly an interesting starting point to fire up some software yourself (I used [Gephi](https://gephi.org/), which is quite nice when it's not crashing), and colour certain nodes and their neighbours. Light up `pipes` and friends, or go on a safari through the forgotten packages at the edge! :-) PS: I could not find nice API functions in `Cabal` to do the work for me. If I overlooked something, I'd love to hear about how `MakeGraph.hs` is a one-liner.
Just a minor followup point. The LFP and GFP coincide because of limitations in where we can put strictness annotations. That is, if we look at the fixed points of *the same* functor, then each functor has only one fixed point. That is, the `LFP_List` type above is both the LFP and GFP of the following functor: data Pre_LFP_List a b = Pre_LFP_Nil | Pre_LFP_Cons a !b whereas `GFP_List` is both the LFP and GFP of the following functor: data Pre_GFP_List a b = Pre_GFP_Nil | Pre_GFP_Cons a b We could distinguish the LFP and GFP of non-strict functors if only we could define: -- We can't say this one: newtype Mu f = Mu { unMu :: f !(Mu f) } newtype Nu f = Nu { unNu :: f (Nu f) } In which case, `LFP_List a ~ Mu (Pre_GFP_List a)` and `GFP_List a ~ Nu (Pre_GFP_List a)`. Though, notably, with these definitions the LFP and GFP of strict functors (e.g., `Pre_LFP_List`) still coincide. To force them apart for strict functors, we'd want to use a definition like: -- Add an extra bottom. data Lazy a = Lazy { runLazy :: a} -- Use Lazy to override the strictness in f. -- N.B., if f is non-strict, then this adds too many bottoms... newtype Nu f = Nu { unNu :: f (Lazy (Nu f)) }
I've not fiddled with the source yet, but would it be easy to add a label to the dots? Specially, it would be interesting to see at a glace where my package fits.
So, there's an automated theorem prover that ships with the type-checker? Edit: may have misread, but that would be nice to have...
Automated theorem prover? I don't think it's possible in general for dependent types. You can get close, using more limited logics like SMT, or using tactics which work most of the time but aren't guaranteed.
That's basically the only part I ever read. Unless a question I answered is on the StackOverflow list. In which case I try to find it. O, vanity :).
Sure, it's only semi-decidable, but by fixing some arbitrary limit (on depth of search, say) you could still go a long way for most cases! 
Not the most efficient representation, but it’ll do in a pinch.
Who's roconnor, and what does tying the knot mean?
Since the law is pretty much the only definition of `signum`'s behaviour, I think it might be better to consider it a nonsensical operation for this type, and leave it undefined/as an error.
It might be interesting to have this for other language library ecosystems too and make some comparisons. My gut feeling is that Hackage has higher code reuse than most others due to the small, well thought out libraries that have a low barrier of entry. This might be a good way to visualize this if it is true.
&gt; tying the knot [An explanation here](http://stackoverflow.com/questions/357956/explanation-of-tying-the-knot). This works in OCaml too, even though it is strict (but it doesn't seem useful there IMO).
I'm not sure of the details of the web frameworks, but GHC comes with an interactive haskell shell called ghci that can dynamically load and reload source files. Development modes typically use ghci to run the application, rather than compiling it. This means fast, dynamic reloads and the ability to query the loaded programme.
GHC is also offered as a library that you can embed into your aplication. See [here](http://www.haskell.org/haskellwiki/GHC/As_a_library).
Does this mean that you can run GHCi programatically?
Have a look at [yesod-bin](https://github.com/yesodweb/yesod/blob/master/yesod-bin/main.hs#L156) that offers you a development server with `yesod devel` [that rebuilds when files change](https://github.com/yesodweb/yesod/blob/master/yesod-bin/Devel.hs#L240) (among other things). It [uses GHC as a library](https://github.com/yesodweb/yesod/blob/master/yesod-bin/GhcBuild.hs) as [abac321 mentioned](http://www.reddit.com/r/haskell/comments/229e6k/how_does_automatic_code_reloading_in_haskell_web/cgkm2qw).
As /u/ozgurakgun mentions, the larger version includes labels. It isn't really possible to make them readable in the small version; consider this a thumbnail.
I've written a commented and simpler version of /u/Puttamalac use of type families and data kinds, available in https://gist.github.com/serras/9990086 If you want to know more about type-level programming (working with types in ways as you mention in your question) or dependent types, you can have a look at [my book "Beginning Haskell"](http://books.google.nl/books/about/Beginning_Haskell.html?id=IZsQAwAAQBAJ) where I describe these things in detail.
Sure - it's https://github.com/ocharles/blog/blob/master/code/2014-03-24-loops.hs It's a bit of a mess - I just use a file like this when I write my blog posts to make sure everything generally type checks (who cares about terms?!)
Some of us don't have the time to go through reddit every day, but if you don't it's not very comfortable to catch up on all important stuff that happened on reddit. reddit is a site that has to be viewed daily for any sufficiently active community. That's just the way it is conceptualized. New content *all the time*. Now, granted, I could just click on "top" and "last week," but I find the way HWN presents it nicer. The drawback is that you can't participate in the discussions anymore. And the SO content is nice, too :-) SO is even more difficult to navigate than reddit.
I don't know anything about other frameworks, but Snap uses hint: http://hackage.haskell.org/package/hint Then according to the flag you pass during compilation of your project (e..g -fdevelopment) there is a ifdef in the cabal manifest which pulls out the correct package. In case of -fdevelopment it will compile your webapp using snap-loader-dyamic: http://hackage.haskell.org/package/snap-loader-dynamic Which, as you can see, depends from hint. Reading the documentation is, I guess, a good starting point to understand how the "magic" is implemented: http://hackage.haskell.org/package/snap-loader-dynamic-0.10.0.1/docs/Snap-Loader-Dynamic.html HTH. 
it would be great to have a look at haxl. it's still not open source afaik.
It would be really interesting to have this as a **directed** graph, in dependency order. I really want to see those 'high' packages, as well as the ones which seem to be at the base of everything.
If you make libraries, then you probably test them pretty well and can make pretty good guesses about where errors come from. If you make programs that run and then exit, then you probably test them pretty well and can drill in on where errors comes from without too much extra effort. If you make servers that run for very long periods of time, then you probably test them the best you can, but there is just so much state and context and surface area involved that it's hard to cover every case, and it's also very hard to reproduce certain kinds of errors. These are the cases where reporting the location an error comes from are the most useful.
I didn't mean to say your package is ugly :) Template Haskell uses ugly invocation syntax, and this proposal at least makes the use syntax nice.
Perhaps this feature could be split into two interfaces. The one suggested here as a very impure one. However, a slightly more pure interface could additionally be made: errorLoc :: (Pos -&gt; String) -&gt; a Since imprecise exceptions can only be diagnosed in IO, at least the pure part can remain fully pure, despite the addition of location information. 
I am pretty sure some of the "features" mentioned are mutually exclusive at a pretty fundamental level.
Thanks for the help!
My favourite quote is around minute 40 when he says "[Haskell] is an impressive language".
This may be a dumb and irrellevant question but using a functor to translate one category into another is like creating a (perfect?) analogy? Also, this isn't what Functors do in haskell, is it? It's an entertaining talk, thanks.
I think it depends on the purpose. In my project, I was using IORefs for single-threaded mutable state, but then I switched to StateT with lens-like accessors and I would say it works pretty well. The code is much cleaner because I don't have constantly be using "liftIO." But for multi-threaded your only options are MVar or one of it's cousins from Control.Concurrent.
So, Coq's "eauto", basically.
you might enjoy the following paper. [category theory: an abstract setting for analogy and comparison](http://pages.bangor.ac.uk/~mas010/pdffiles/Analogy-and-Comparison.pdf)
Functors in Haskell are Endofunctors in the category of Haskell types. They translate from Haskell types to Haskell types.
it's not cool to downvote someone asking a very valid question. the haskell community is somewhat cliquish, so preserving that cliquish-ness is to conducive to adding new members.
just an observation: that seems justification enough why their name needs no other qualifier since that seems to be the most general functor in the context of haskell that you'd want to define.
yea, me neither. although, there's this: /u/roconnor. but it still doesn't answer your question in the original context.
Don't think so; `Bool` has the wrong kind for the relevant machinery in the base package. Maybe with `lens` or `mono-traversable` there's a generic way to express it? I'm thinking something like a monomorphic fold of `Bool` into `Maybe ()` and then substitute the unit with `&lt;$`, but even so it's still not one single existing function.
Yeah, I can also see that he has a Haskell-oriented GitHub repo, but it still doesn't tell me much.
It is not a true dynamic reloading like you used to in ruby/python/php. Basically all of the haskell web frameworks employ some sort of external watcher program (like yesod devel) that monitors files in folders and when a file is changhed it recompiles entire project. For small applications it is instant, but once the application grows and becomes more complicated you will see substantial delays between saving a file and reloading the application. There's also ghci that supposed to dynamically load code, but i'm not aware of any haskell web framewokr that uses it. 
Is tying the knot a matter of intrinsic ability to represent circular data structures in the underlying type theory? Or does it have connotations of the physical layout in memory, as well? I don't see the issue representing circular data types. However, I could easily imagine the issue representing them in finite memory.
With constraint types you can define a more general set of functors; specifically, functors between subcategories of Hask.
/u/roconnor is http://r6.ca/ and a frequent contributor to the Haskell community. His [blog](http://r6.ca/blog/) is worth reading. I really like [A Very General Method of Computing Shortest Paths](http://r6.ca/blog/20110808T035622Z.html). He's also the author of the paper [Functor is to Lens as Applicative is to Biplate](http://arxiv.org/abs/1103.2841) which was, for me, an very illustrative paper for understanding lenses.
I get that fmap :: (a -&gt; b) -&gt; f a -&gt; f b. I get that fmap can translate from Maybe Int to Maybe Bool. But fmap doesn't for example translate from Maybe a to Either a a, does it? Isn't that what the Functors from the video can do?
A functor has two parts: 1. It maps objects to objects (where objects in our case are types). This is why the `Functor` class requires its argument to be of kind `* -&gt; *`. For example `Maybe` maps `a` to `Maybe a`. 2. It maps morphisms to morphisms (where morphisms in our case are functions). This mapping is defined by `fmap`. It becomes more clear if you write its type as `(a -&gt; b) -&gt; (f a -&gt; f b)`.
The described workflow is also supported via `-fdefer-type-errors`. (Still, throwing errors with location is certainly also very useful.)
I think the benefits that HKT bring are worth the complexity. We just don't need to display them as prominently as in purely functional languages like Haskell. A good tutrorial on `Option` would suffice for most beginners, then as they progress they can look more deeply into how it is actually put together.
Comments by other people below say the opposite, namely that Snap and Yesod indeed *do* use ghc-as-a-library (i.e., the ghci API) for dynamic code reloading. 
&gt;For example Maybe maps a to Maybe a. Yes, but what maps Maybe a to Either a b? Is it a functor? I don't think it is in haskell. fmap doesn't change the Maybe part. &gt;(a -&gt; b) -&gt; (f a -&gt; f b) Yes, Maybe a is (possibly) a different type than Maybe b... but there seems to be the limitation of not being able to change the Maybe part of the type. The functors from the video don't seem to have that limitation.
Then something like newtype Even = forall (n :: Nat) . Even (Sing n) (IsEven n) can be left transparent and is the type of all even numbers since you can't build it without a proof of evenness.
Dunno, I preferred the bits on category theory as a linguistic phenomenon providing a strong means for mathematical cross-pollination, but I tend to prefer quotes that teach me stuff I don't already know.
Thanks, the analogy to analogies is pretty helpful, both directly and at a higher level so to speak.
What you are talking about is referred to as a "natural transformation" actually, and it varies from Functor pair to Functor pair
&gt; Functors in Haskell are Endofunctors in the category of Haskell types. That is almost as classic as the monoid quote by Mac Lane!
What's up with all the `-n` commits?
They don't recompile the entire project - just what changed, and what depends on what changed. Occasionally that does end up being most or all of the project, but usually the reload is quite fast.
Assuming the graph is acyclic, you might want to spatially order the nodes. For example, you could have root nodes on the left, going down to leafs on the right. I think this would make the graph structure more apparent.
Thanks! Your comment at the end about not being able to write functions whose types depend on values, in the context of `fromIntegral`, is really needed closer to the beginning. The "Problem!" that we cannot obtain `n` from the context immediately suggests an obvious simpler solution dolls :: Nat -&gt; a -&gt; NextedList n a which is not possible for that reason.
I think it's intended to be useful right from the start, before you've learned about functors.
`fmap` is a functor from `Hask` to some subcategory of `Hask` where the objects are expressions of type `f a` for some Functor `f`, for some a, and the morphisms are expressions of type `f a -&gt; f b` for some a, b. So, `Functor` as a Type isn't actually a Functor in category theory, but the function `fmap` is.
You might make a function that is specific to Lists (or whatever is in scope) and not Functors.
It probably existed before the Functor type class. There are probably also some optimizations that can be done on map but not fmap (at least not until you know the type in question is a list).
It's not because we have a more general solution to a problem that one should employ it, because there is a cost attached to it. In map/fmap case, the cost atributed is ease of explanation. It's easier for a newcomer to relate with 'map' than the abstract Idea of functors.
I don't see [how that could be a problem](http://coliru.stacked-crooked.com/a/5922d142c7eae403).
i am not yet familiar with constraint types, but what i meant by "most general functor…you'd *want* to define" was the most practical (i.e., useful) general functor available or at least the most obvious. is that more accurate? (honest question, as i'm still new to haskell)
[see my reply here](http://www.reddit.com/r/haskell/comments/229scs/category_theory_by_tom_lagatta/cgl0qpy)
A few more examples [here](http://stackoverflow.com/questions/18761302/why-were-haskell-98s-standard-classes-made-inferior-to-haskell-1-3s).
putStrLn actually doesn't have anything to do with monads and if Haskell didn't have monads at all, putStrLn would work just fine as it already is without any changes. Also, type signatures do matter, as a learning tool. I can show someone the type signature of map before I ever showed them what it did, and a day 1 haskeller would probably be able to tell you what it does. Type signatures offer much insight on their own, alone. 
fmap does map objects to objects to objects though, so it is only part of the functor.
The main motivation, I think, is in the error messages. Even if we were able to explain what fmap did and specialize the type signature when explaining, imagine using it wrong the first day of learning haskell and getting a long error about Functors or no instances of Functor. Then imagine just seeing an error "not a list of a's." (As a minor possible point, it might also just be from tradition/precedent/historical reasons, like cons and head) 
I think that would just be another giant blob, but this time shaped like a pyramid. Using cabal-db, here's what Pandoc's dependency tree looks like (rendered using GraphViz): http://stupidname.org/files/temp/temp_2014_04/pandoc-dep.svg As you can see, it's already a little chaotic. Sure, the layout is far from compact, but I doubt that would be the main issue. (I don't know a graph visualization program that can handle hierarchical graphs of that size anyway. Suggestions? I might give it a shot anyway.)
It does not matter. In a big project even a linking step takes quite a long time even if only one file was recompiled. I know because i develop yesod applications and often i have to wait 10-15 seconds before application reloads.
It does not matter. The effect is still not the same as with dynamic languages. There's no instant reload. In fact the bigger the project grows the slower it reloads on every change. Which is different from dynamic languages where the size of the project does not really affect speed of reloading one changed module. 
Whew. An hour and a half long. 
Yes, but map as a function you expect to see and use in FP has been around for over 50 years...and keeping some historical perspective is something that CS really should do more of!
Is that really true? My understanding was that a functor takes objects in one category to objects in another (not necessarily distinct) category, and likewise with morphisms. In that since there is really one category for `fmap` - `Hask`. Now the `Functor` type class maps objects of `Hask` - types - to (different) objects of `Hask`: `a` becomes `f a`. Likewise, we map the morphims `a -&gt; b` into morphisms `f a -&gt; f b`. I've never seen it mentioned that the category is `a -&gt; b`.
You don't need putStrLn or anything in the IO monad to learn haskell - I started by doing everything in ghci using pure functions. 
&gt; This looks like a Free functor to me And to a beginner this looks like madness :)
I agree with you. This is why I say: &gt; For the specific case of fmap ... 
I imagine that the problem is that trying to unify `Maybe (Maybe (Maybe (Maybe a0)))` with `Maybe (Maybe (Maybe a0))` results in the type checker inferring `a0 = Maybe a0`, which is illegal in Haskell. Because this is where the error occurred during type checking, this is the unification step it shows. If the expected types and actual types were shown as the same, that would imply to me that it would type check so that shouldn't ever happen (although, there was a bug in GHC where that *did* happen under certain conditions).
Also maybe [ologs](http://arxiv.org/abs/1102.1889).
Hey, don't shoot the messenger. jefdaj's explanation is the same as I've heard: `map` supplies useful type errors to beginners. I do wish the functions had been called `map` and `lmap` instead of `fmap` and `map` but that's just what we ended up with.
To interpret copumpkin's remark, context clues are all that is necessary. He implies that roconner is a person with thoughts about Haskell and other languages, and implies slightly more weakly that he is a person whose thoughts about such languages are worth taking seriously. You may _want_ more information than his remark, but to accurately parse and understand his remark, you don't _need_ more.
http://www.haskell.org/haskellwiki/Haskell_in_industry http://www.haskell.org/haskellwiki/Haskell_Communities_and_Activities_Report
I know, but I'm curious now :)
It's obvious that he comes from the analysis camp. He downgraded associativity from the absolutely most important axiom to a mere side note that can be looked up on Wikipedia if you really care. Please do not make the same mistake when explaining category theory. Associativity is the one axiom that makes categories structure-aware and -preserving!
To be honest I couldn't care less about the performance on Ideone. I'm not even sure what that website is, and I prefer to benchmark on an actual machine that sits in front of me. =)
That might actually be true, but it seems to have the same effect if you look at the generated assembly code.
No, Lambdabot would talk about Haskell, not GHC. ;)
There are a lot of reasons to talk about things other than Hask endofunctors is all I meant to indicate. It's definitely the case that if someone just says "Functor" we may as well assume that they mean, in the context of these kinds of discussions anyway, "Hask endofunctor". But that certainly doesn't mean it's never valuable to be specific. It's highly meaningful to talk about categories which arise in embedded language or subcategories like "Functions on types instantiating `Ord`". They're not often implemented in code, but are certainly valuable reasoning tools.
Basically a free functor is just a codification for an arbitrary number of f (f (f (f (f ... x)...))). So Free [] means it's a or [a] or [[a]] or [[[a]]] etc. It's also the free monad in the same way that list is a free monoid, in that join just splices the f's from two frees under one, like a list just sticks the two lists end to end.
ahh, okay. thanks for the elaboration.
I'm using it at my startup presently and have used it for contract web development in the past (with great success). My startups product consumes soft real time energy usage data being pushed every second from our customers homes. The daemon is written in Haskell, it buffers into a memory backed acid state store. If there are any subscribers to the channel it will also publish using redis pubsub on a channel for that device. This is used for a web socket implementation to display that data to the user as it's updated. The websocket server that listens to redis pub sub is written in erlang so I could leverage Sockjs (which doesn't have a mature Haskell implementation yet). Every 60 seconds a reaper also written in Haskell, reaps the data from acid state, computes change over time and a few other things we need then pushes that data to tempodb (the api wrapper I also wrote in hs). We use Haskell else where too but it's trivial. This piece, combined with supervisord, has been rock solid. I dislike programming in python now and wish I could re write our entire web app with hs but that's too big for just me at the moment...
But `a0 = Maybe a0` is exactly the illegal type I declared at the start. Refusing to unify two things that were both inferred from the same original type, and which are both mathematically equivalent, still seems slightly strange. To me, claiming that two types don't unify means you're claiming they're different. Failing to typecheck is perfectly valid if a type in the program is illegal. There's no need to invent a type mismatch as an excuse. I'm not about to report it as a bug, but it's still slightly odd. Personally, I'd drop the expected and actual type lines from the error. To translate to a different context, if some integer-only language compiler reported `0.1 is illegal because only integers are supported` that would be fine. For it to then say `and because 2.1+1 is not equal to 3.1` would be weird. 
Well, Functor is not a type in Haskell, it's a type class. Instances of Functor are functors. And fmap itself is not quite a functor, as it only maps morphisms; you also need to map objects. That's what the type constructor that is an instance of Functor does for you. The type constructor and the fmap implementation form the instance of Functor, and define a functor. So there is a `Functor` instance for `Maybe` that defines an associated `fmap`. This makes `Maybe` together with its associated `fmap` implementation a functor. Neither without the other is sufficient.
We're building a haskell socket.io implementation if that's any use to you?
It makes it easier for people to understand code. When you see `map`, you know it's operating on a list. When you see `fmap` you must examine more of the code to work out whether it's operating on a list, or part of a pair, or a function result, or the `Functor` mentioned in the type signature, or whatever. To put it another way, humans have less working memory than computers and more easily get confused while inferring types. In short, `map . fmap . second` is clearer than `fmap . fmap . fmap`, at least if the more restrictive type is what you want.
Not 100% either, but it might be because there are a bunch of links on the side of the sub that answer the question? Even as an observer it really frustrates me when people down vote without giving some feedback about about why.
At [Silk](http://www.silk.co) we've been using Haskell for all our backend (non-browser) code for over four years now. We have an irregularly updated [engineering blog](http://engineering.silk.co).
In Picus, we built our network security assessment and monitoring product using pure Haskell.
Well not *too* much historical perspective, or else we'd be using `car` and `cdr` instead of `head` and `tail`.
At work we are using it for a few of the smaller projects, mostly parsing-related ones since Parsec and Attoparsec are a lot easier to maintain than Regular Expressions of the same complexity and also more powerful and flexible.
`lmap` would clash with the bifunctor/profunctor functions, and `map` specialized for lists already has that name on other languages. I think it's fine like it is now.
I'm not sure I subscribe with the content of the blog post (my personal 2 cents obviously). I think there is nothing to do with the use of `liftM` but more with its _improper use_. After all, `liftM` is just: liftM :: Monad m =&gt; (a1 -&gt; r) -&gt; m a1 -&gt; m r So "r" is unbound and can be a "pure" value as well as an effectful computation, so it will obviously type check. In the case of the blogged example, it will yield an effectful computation wrapped in the IO monad. Personally I don't find anything so strange in IO(IO ()), it read naturally as "an effectful computation than, when execute, will yield an effectful computation". On the other hand, the blog seems to have been written more in a sort of "mental note" from someone which is learning Haskell along the way (I beg you a pardon if this is not the case :) ), so is perfectly fine to share the insight which inevitably have tripped a lot of people up when learning about Haskell and lazyness. So a quick ":t" in ghci would have revealed what was going on :) Just to wrap up, I think that the content of the post is not very interesting per se to a wider audience which is not the pool of beginners (which I think is still very relevant though!). I don't subscribe with the conclusions, but this, obviously, entirely subjective and variety is the spicy of life :P
why don't you base it upon/use shake?
I didn't try anything complicated, but this simple example works fine: alternates : Stream Int alternates = x where mutual x : Stream Int x = 0 :: y y : Stream Int y = 1 :: x main : IO () main = print (take 10 alternates)
This is why I strongly believe main should be constrained to IO () and always write an explicit type signature for it.
I'm interested in hearing more about that.
Thanks guys. I found a blog post titled along the lines of "Why Haskell and Python make you a worse programmer", and really is kind of a satirical title because each 'reason' he gives is really a compliment to the languages such as they demotivate him to program in the language he is employed to program in because the Haskell code is usually cleaner and simpler than the code in C#, etc... I'm currently learning C++ in school, and thinking about learning Haskell in my free time. Thanks again for your responses. 
Yes, the data is messy, but this is still quite useful. It clearly shows how 'deep' pandoc is. And how some weird things are still in there: why so many dependencies on old-locale? Unfortunately, I can't help with good graph layout programs for hierarchies, I am looking for that as well!
Shake is open source. Buildsome is proprietary code or closed source in the public view. The license file simply says "all rights reserved."
There's a quite limited Coq-style tactic-based theorem proving feature, but not anything fancier yet. One of my current todo-list items is using the type providers feature to let SMT solvers introduce new postulates for things that they check. That will be fun! It loses some of the nice features of real proofs in the type theory, but for certain use cases it would be convenient.
There was a great wave of monomorphization in Haskell 98, because compilers at the time had terrible error messages. This gave the pedagogical argument a lot of "oomph" at the time. One fix was to simplify some types. We've gotten MUCH better at error messages since then, but we're still stuck with the consequences of that time. With 7.10 we're looking to fix many of the worst of these, mapM, etc. I don't think we've decided in the particular case of map if base should switch for consistency with generalizing foldMap, mapM, foldr, and the like or leave it because fmap exists and it'd just be an alias anyways. There are very reasonable arguments to be had on both sides.
[Link for the curious](http://lukeplant.me.uk/blog/posts/why-learning-haskell-python-makes-you-a-worse-programmer/)
I think there is a subtly I'm not explaining properly. You can substitute the general for the specific, as you did, but not the other way. Think about your function, it can only be used on lists of ints. But the implementation can be used for any type that Functor to any Num goes to.
It will be open sourced, we just haven't figured out the details yet and its not ready yet anyway :-)
Shake has a different design. In buildsome, the main idea is the file system hooks and speculative parallelism. Both of these don't exist in shake. I did consider basing it on shake, but it seemed easier to start something new that I can easily take in any direction.
It's a snaplet at the moment, but I suppose we could look at making it standalone if it's more generally useful: https://github.com/ocharles/snaplet-socketio 
&gt; Is tying the knot a matter of intrinsic ability to represent circular data structures in the underlying type theory? Tying the knot is observationally equivalent to recomputing repeated parts of a knotted structure over and over again; so my understanding is that knot tying is more of an operational feature rather than strictly a denotational feature.
Gotcha. Thanks for the reply.
Have you looked at [this](http://hackage.haskell.org/package/socketio)?
C++ was my first language, and for me it really complements Haskell well. You'll come to appreciate the strengths of each language more, and some experience in Haskell really helps you grok template metaprogramming.
I'm using it at my job for a system to organize work in a robotic installation, for a daemon that get statistics from POS cash register machines, for a small program that converts a very specific subset of PCL into the equivalent postscript, and many small programs.
What is the runtime semantics of this? Is it a circular structure or does it allocate new nodes repeatedly?
Fair enough. The glimmer of hope from 1.4 was fairly short lived. ;) 
pandoc is awesome. I wish the syntax highlighting was more expressive. Kate highlighting has so few tags to mark up with.
Signed up!
That seems complementary to what Shake has already. Did you reach out to Neil Mitchell? It's usually easier to start something new but it hurts everyone when we have to choose between many tools rather than making a few better. :)
Me too, looks like it'll be great!
I contacted him, yeah. It might make sense to cooperate eventually. But schedules are pressing, so gotta get something going. Improving Shake would put a lot of extra uncertainty into the project and be significantly more difficult to start up. Merging later is more probable.
Can't you always say that [] maps into the subcategory of Hask of list types? There doesn't seem to be anything went with that idea so long as we implicitly embed that subcategory back into Hask before writing fold. It's some extra ceremony but has the advantage of highlighting that [] as a Functor is injective but not surjective.
Are there any plans to record some of these talks? They look awesome but I can't attend :'(
When Haskell was designed functors where not really known in the CS community, so `map` was a function operating on lists. Just like it had been since the dawn of time. After monads&amp;co entered the picture it was natural to generalize `map` to any functor, and this was done in Haskell 1.4. There was a backlash against this from people teaching introductory courses, mainly from John Hughes, because the error messages became confusing even for simple list processing. So in Haskell 98 we got `map` and `fmap`. The error message problem is still not solved, but personally I think this should be solved by giving beginners a simplified prelude rather than crippling the language for everyone else.
That could very well be an example of what doesn't work in Haskell. The identity type is trixy.
Good stuff! I enabled this for both sqlite-simple and snaplet-sqlite-simple and added some more tests to bring both above 90%. Improving tests against a metric is fun - almost like playing a game. :) It's good for spotting dead code and other illogicalities too.
I think Brent Yorgey has a guide.
Prof that fmap is the [buffalo](http://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo) of functors.
&gt;It does matter because linking is not necessary for hot code reload. We are talking about non trivial applications that consist of more than one module. I'm not even taking into account libraries. &gt;So it is not compilation that is slow. I'm sorry but that was not the question. No one cares what exactly is slow as long as something is slow. As of now, all haskell web frameworks reload any nontrivial web application with substantial delays as opposed to dynamic languages. I develop and maintain several yesod app servers. I deal with this every day. 
[Catsters guide](http://byorgey.wordpress.com/catsters-guide-2/)
Yeah, true, but if you think of subcategories of Hask as "the only categories," they're basically the same. 
Much better than the last article.
I really hope they do. All of these classes need to be put online.
i'd like to point out that some of this would be subsumed by having stack traces right?
If anyone has advice on how to record I am more than happy to do the leg work.
Can you explain what you mean by "analysis camp"?
Signed up! Looking forward to it!
My favourite part of the video at 58:47: "I'm, uh, using a *couple* of extensions in this code, but that shouldn't be too frightening".
Or equivalently, in an even more point-free style: whenMaybe :: MonadPlus m =&gt; a -&gt; Bool -&gt; m a whenMaybe = (flip((.)(&gt;&gt;)guard).return) But that is 27 characters long (without spaces) where 'whenMaybe' is only 9. 
Everything that does time formatting ends up depending on old-locale.
There's graph simplification and edge bundling that I can think of. For instance: http://www.cs.umd.edu/~cdunne/hcil/pubs/Dunne13Motifsimplification_improving.pdf And: http://www.win.tue.nl/~dholten/papers/forcebundles_eurovis.pdf Edge bundling can be performed in modern graphviz, I believe. 
Very entertaining. Best quote: "I feel like Christian Bale."
Are you in munich?
Love these short and sweet articles, keep em coming!
Great list of speakers!
This is only possible if you wrap the type 'a' in a data type like Tree. So 'doll' could be defined as: doll :: Num n =&gt; a -&gt; n -&gt; Tree n doll a n = loop (toInteger n) where loop n = if n&lt;=0 then Leaf a else Node [loop (n-1)] Therefore the meaning of "Tree a" is that it can represent every possible type: a List a List List a List List List a ... for theoretically infinite levels of nesting the List type. Think of (Tree a) as meaning (List* a) where (List*) could mean 0..maximum number of nested List data types. The (List* a) type will only match the (List* a) types, not (List** a) or (List a) as this does not make sense. In the type system, a type can only match itself or be bound to a polymorphic variable. Likewise the (Tree a) type only matches the (Tree a) type. I hope that is not too confusing. For Haskell types, the * operator means something completely different. I am using it here as pseudo-code, and I mean for * to be the Kleene-star operator. 
&gt; let writer = do &gt; modifyIORef' ref (+1) You don't need do in this case.
Don't forget to write about the bad parts too!
I use it to prototype some networking programs (APNs Socks5 etc). Libraries like TLS, Pipes, Cereal and Attoparsec allow me to quickly build a working (and correct!) example to test if the designed workflow is correct. Then I just rewrite them in Scala since my company is a Java shop:) One thing that bothers me it that currently I cannot find a PKCS12 parser in Haskell, which makes using TLS a little bit harder. Maybe I should build it myself..
Something like this http://bentilly.blogspot.co.uk/2010/08/analysis-vs-algebra-predicts-eating.html although I think the distinction is overstated.
Do you have any internships available?
Do you have a lot of 5 year olds at your company?
What's your major?
Finishing my first year of Computer Science.
&gt; is a disruptive technology company that leverages functional paradigms to enable engineered innovation. Oh boy, this makes me really not want to work for you ;) This post makes me want to play buzzword bingo. &gt; We don’t allow Ruby anywhere on our platform so you should be comfortable about that. Why do you have such restrictions when you allow equally shoddy pieces of software like Python or NodeJS? 
Sure I'll pm you a link to my CV. I'm studying at the university of Nottingham but live in Leeds, West-Yorkshire outside of term-times.
I would also be quite interested an internship, if your CTO gives thumbs up to them.
Ping me over a current CV please.
what do we do before we play with a new toy? we put the old one away. why do we do this? because we don't want to be putting away all the toys at bedtime, do we?
where there's a will, there's a way
... by a group of students at U Bonn during last semester.
Very cool. How did they do the Javascript?
By hand. :-( But there is actually extremely little Javascript. Essentially only for the place-ships-by-dragging before you start the game (https://github.com/zrho/afp/blob/master/battleships/static/js/map.js). After that, the game handling itself is plain Haskell. (Well, a few bits in three or four .julius files in https://github.com/zrho/afp/tree/master/battleships/templates to set up clickables etc., or to show warnings if an unsupported browser is used.) 
Nice, well it's very impressive. Congratulations to them!
Great location in a beautiful city! Wish I could attend, if only to walk across the Danube bridges and drink spritzers in the cozy garden bars.
If you're on linux and comfortable with the command line, yes; I don't know otherwise.
No I'm in Austin, TX. Why? 
I had a good chuckle when I realized this is your answer to number 2. Brilliant.
I'm on Windows 7, using Firefox. I already tried using a Firefox addon - with that installed I couldn't view the video at all. That said, I don't mind Linux (commandline included) - if all I need is to boot a live distro for a bit, I'm OK with that. However, it looks a lot like downloading for offline viewing is *deliberately* blocked. Is that true? 
 So many inspiring people giving classes, I'd love to hear what they have to say.
Mentioning node and python in the same breath is a bit harsh!
http://golem.ph.utexas.edu/category/2007/04/the_two_cultures_of_mathematic.html I think, like me, you'll find The Two Cultures' presentation of Theory Building vs Problem Solver more satisfiying than the analyst/algebraist distinction. Personally I am (or was) an analyst with the approach that would be called "algebraic" but was definitely always a theory builder rather than a problem solver. I'm willing to bet you're the former too!
You have two options here: run your application via the GHC interpreter, giving you something a lot like a Ruby environment; or compile modules on-demand and hotswap with dynamic linking. The former is usually done automatically with the hint package, or manually from GHCi (using `:main` and `:reload`) and the latter usually done with the plugins package, usually with filesystem monitoring via the plugins-auto package (this is the typical setup with Happstack). Both options kinda suck because they mean the web toolkits have to replicate Cabal functionality. I wish there was some support for this in Cabal and cabal-install proper; I think there was a failed GSoC to that end some year ago... For now you can benefit from Cabal using the manual GHCi method and `cabal repl`, but yeah, it's manual.
&gt; there are more thread-safe ways to do mutable counters than IORef. Anything you can list for me to google?
I've encountered `IO (IO a)` "in the wild" through STM; `STM (IO a)` becomes `IO (IO a)` through `atomically`.
It's surprising to me that "algebraist vs analyst" resonated more with you. Your description of "I have a tendency to prefer cultivating a few general and reusable principles that work for a very broad range of problems and I tend to dislike circumstantial solutions or proliferation of concepts." sounds exactly like Theory Builder to me, and exactly the opposite of Problem Solver, so I thought you'd prefer that characterisation.
Fixed.
Hmmm, maybe you are right. Either way, I know that I definitely eat corn in rows! :)
Using atomicModifyIORef' would have been no less "threadsafe" than using any of the other concurrency primitives people have mentioned. If you're brave you can use the counter from atomic-primops, whose increment has the best behavior under contention of anything else by far (especially on x86, where a feth-and-add instruction is used)
I know the CTO at sphonic, he's a good guy and the product as he's described it to me does sound genuinely exciting and interesting. He's also mentioned that they've noticed Haskellers being better programmers so I'd say that there's a real chance that they'd end up with some Haskell in production sooner rather than later. I'd be inclined to forgive the buzz words here - which I think is intended as a joke.
My favorite variant is that you get a shot for each un-sunk ship. Perhaps I should learn Haskell.
Good luck for your candidate search! :)
One big problem Haskell faces in developing GUI libraries is mapping them onto the imperative core libraries. That's way easier to do with an OO language. FRP is promising, I hope to see more of that. Also, subtyping and global values make configuration easy and that's missing in Haskell, but for example Threepenny solved this easily and naturally by using CSS (which also avoids recompilation on minor changes!). 
Is "local to Houston" a hard requirement? Would you consider remote work?
So...pretty much my dream job in bioinformatics. Now I just need graduate already.
Is the AI dumb or do I place my ships in an odd way?
The AI is described here: https://github.com/zrho/afp/wiki/Battleships%3A-AI If you have found a strategy to reliably fool it, that would be interesting. What is it?
Actually, previous testers complained that the AI seemed too clever, suspecting it of cheating (abusing knowledge the program has, for rendering purposes, about the player's grid). :-)
I don't think it was "a problem", there shouldn't be some sudden complete randomness. More likely, the AI strategy is simply not doing well against this specific kind of hedging.
http://i.imgur.com/olPSzQS.png I've tried again and made a screenshot. I can see the end-game thing. Well, I've seemed to have fooled the AI by not caring about moving.
Yep, congrats! :-) From a picture like you have there, it could take the AI just a few turns to sink your whole fleet. Once in end game mode, it is often very effective in making many hits in a row without the player ever getting a chance to fire again.
"Individual who can turn Perl and C++ into maintainable Haskell in Houston" seems like a pretty light requirements description. Is there a complete posting for this job?
Dang - bioinformatics and haskell in Texas and I'm stuck away from home in Ohio for another year
cheque's in the post.... :-) But seriously - we do think Haskellers are, in our experience, better at composing programs. The functional mindset is so ingrained that you can't help but create clean, effective code. I have no problem running Haskell in production either. If it's the right choice - then we'll do it. If a man can't write a few buzzwords on his own post - well, why post! :-) (I'm not the CTO btw)
Perl is (or used to be) *the* language for genetics processing (because genetic codes are just a long string, which you can process with regexes). See [here](http://genetics.stanford.edu/gene211/handouts/How_Perl_HGP.html), [here](http://www.perl.com/pub/2003/02/27/review.html), etc. So biologists would learn Perl and write regexes to solve their problems. I'm not sure if Perl is still the dominant language for new programs, but now people are also using Python or other more maintainable languages. I suppose you would need to convert some regex-heavy code, which may not be very readable, into some clean Haskell code. I wonder if Haskell code could benefit from using more general grammars, regular languages seem to be too restrictive. Also more abstraction is a plus. Anyway, there is this site [Biohaskell](http://biohaskell.org/), you could take a look on the genomic libraries at Hackage.
I found a [posting](http://eurekagenomics.com/ws/about/join.html#bio) on the [Eureka Genomics website](http://eurekagenomics.com). It does not mention Haskell, but it contains other information that is likely relevant.
Typically, (and I believe specifically in this case, as well) yes.
Somewhat off topic, but Haskell comments start with `--`, if you remove that third - you'll probably get correct highlighting in your comments.
I've also updated the tutorial on the Haskell wiki so that the information/examples there actually describe the current state of Glome rather than how it worked in the ancient past: http://www.haskell.org/haskellwiki/Glome_tutorial
&gt; Note how trivial it is in a pure functional language to parallelize a function - compare this to the equivalent snippet in Python, C++, etc. It's pretty trivial in Python too (using processes instead of threads due to the GIL): from multiprocessing import Pool pool = Pool(4) results = pool.map(func, inputs)
Remote is becoming more and more common; I don't understand why an employer would hamstring themselves to a location, when recruiting for reasonably niche position like this.
A while ago I made some effort writing the fastest [Haskell Boggle solver](https://gist.github.com/AndrasKovacs/8223339) I could think of. It solves a board in about 60 microseconds, using a directed acyclic word graph generated from a dictionary (using my [packed-dawg](http://hackage.haskell.org/package/packed-dawg) package). 
+1 to beginners' prelude.
[Video](https://www.youtube.com/watch?v=mVCocSSIkRQ)
My understanding is that moderators can only add CSS styling, not scripts.
Doing what? I'm looking for Haskell programmers too in the Austin area (pm me back I don't want to hijack this thread).
Not sure I would have believed it without the video...
The first thing I would try is to use an unboxed tuple, rather than `(Int64, StdGen)`.
`setBit` is likely slower that the bit twiddling stuff you have in C, no?
 System.Random is slow. Mersenne random is faster. 
Seq can get in the way of some rewrites.
Don't use System.Random, as it is both very slow and of poor quality. You should use the `mwc-random` package if you need speed, or `tf-random` if you need that pure splittable API.
I think it was just as fast, last time I checked.
It's faster with seq's. I'm not sure all of them are necessary, but adding them resulted in about 50% speedup, if I remember correctly.
&gt; standard way these days It is actually not vanilla markdown, what you are talking about is "github flavored markdown". But I agree with you it's quite common today. IMO, it's better to change the way all reddit works (it's free, right?), than do some dirty hacks with javascript. Why not to open an issue on the reddit's github about the shifting from plain markdown to github flavored markdown or try to hack it by your own. https://github.com/reddit/reddit
a) StdGen is slow, b) C version uses a static array allocated on the stack, Haskell version uses a linked list
Yeah, it's in pandoc, quite handy to use in hakyll with pygments 
Yeah, pandoc has "markdown" and "markdown_github".
Perhaps for an embarrassingly parallel map, but I doubt you could get Repa-like performance out of Python, at least not in general.
seems unlikely for vanilla reddit. But perhaps with RES?
why can't we have core apis that are useable? why can't System.Random be something one would recommend? wasn't useful, quality apis a goal of the Haskell Platform? if people should be using mwc-random, why not just make that the default?
&gt; b) C version uses a static array allocated on the stack, Haskell version uses a linked list This is going to be a MAJOR drag on your system. [Herb Sutter just presented on this at BUILD](http://channel9.msdn.com/Events/Build/2014/2-661) (Skip to 23:30 to see this part)- In his C++ talk he showed the effect of a contiguous array vs a linked list, when it interacts with the prefetcher found in most modern architectures. The results for Linked Lists were not pretty at all, while the Contiguous Arrays leveled off.
How did we manage to write a slow random number generator?
You can pass an _environment_ consisting of a mapping of names to values through the computation. When evaluating `Let`, you can evaluate `e1` and add its value to the environment: import Control.Applicative evaluate :: [(String, Float)] -&gt; Expr -&gt; Maybe Float evaluate _ (C x) = return x evaluate env (V variable) = lookup variable env evaluate env (Let variable e1 e2) = do val &lt;- evaluate env e1 evaluate ((variable, val) : env) e2 evaluate env (e1 :+ e2) = (+) &lt;$&gt; evaluate env e1 &lt;*&gt; evaluate env e2 Now you have to pass an initial environment to `evaluate`: ghci&gt; evaluate [] (Let "x" (C 5) (V "x" :+ C 5)) Just 10.0
We cannot really fix `System.Random` (splitting cannot be done fast as far as I know and it's part of the API). We should aim to get something reasonable into the platform though. Anyone want to write a proposal for mwc-random?
And unsound from a randomness perspective!
After BayHac on Sunday, Eran Tromer from Tel Aviv University will be presenting on ZeroCash which is an incredibly interesting application to efficient zero knowledge proofs to cryptocurrency. http://www.meetup.com/EthereumSiliconValley/events/175527242/?a=ea1_grp&amp;rv=ea1&amp;_af_eid=175527242&amp;_af=event
Splitting maybe can't be done fast, but the other operations should be able to be done fast on splittable seed values, no? Also, latest `random` source shows seperating the splittability out into a seperate class.
I rewrote the code using `mwc-random` and being a bit more straight-forward in the implementation: [code here](http://lpaste.net/102436). I got a speedup of around a factor of 4 (1.07s vs. 4.36s). I also tried using unboxed vectors instead of lists, but it actually resulted in a slight performance loss. Still a sizable gap, but closer at least.
&gt; There is no vanilla markdown, either, if you want to split hairs. :-) I meant the original one http://daringfireball.net/projects/markdown As about hacking the reddit, I agree that its quite hard to push something to the upstream, but it seems like a better solution, plus I don't think that superadmins would allow adding scripts.
Switching to just 'create' will use a seed that never changes so you can test the algorithm. Similarly you could use 'initialize' and a known seed.
Using GHC as a library sounds like an improvement over calling the ghc executable. One is strongly typed, the other stringly typed! However my main worry is duplication of build information; what is defined in the `.cabal` files. If I have `hs-source-dirs: src` I don't want to say `-isrc` anywhere else. DRY and all that. This shows up in many more places besides code reloading, though, such as editor integration. My vague thinking is that Cabal and cabal-install should be more than a build tool and/or a package manager, but also be something like a Haskell IDE-as-a-library-and-CLI. Currently `cabal repl` comes closest to this, and in fact it serves no purpose for a build tool or package manager alone. It would be interesting and useful to extend and build on that to support tools like ghc-mod and plugins-auto. I have no idea what this would look like in practice, though. Chris Done did some experimenting with "GHCi as a service" ie. RPC which might be an idea, and then we make it use `cabal repl` instead and extend it with new commands to support ghc-mod functionality like sub-expression type inspection etc, and a command to run a reloadable `:main` in the background that doesn't block the service. But my ideas here are all about development. Hacking on code, integrating into editors. If I recall correctly, your goals are more about, or also about, dynamically loadable plugins for applications, where the end user is the target audience rather than [just] the developer? That's also a worthy goal, and an interesting question whether or not it should be part of the same "cabal server" infrastructure. I'm thinking not, because the server is perhaps best run interpreted, and application plugins best compiled. Cf. "hint" vs "plugins". :-) edit: But I do think "plugins" should be part of *Cabal* and cabal-install, even if not part of this "server" idea. It's obviously relevant to the building and packaging sides of Cabal.
The strange thing is the diference in speedup. I would think that speedup factor would be the same, no matter the machine, and yet on my machine the speedup factor is about 15. 
Didn't make the team?
Actually, `markdown` ([Pandoc’s markdown](http://johnmacfarlane.net/pandoc/README.html#pandocs-markdown)), `markdown_github` and `markdown_strict` (and others (`markdown_mmd`, `markdown_phpextra`)).
Yes, be excited! (I will note as a passive reminder: it's not released until I send an email containing the signed hashes of the binaries. But it's here, pretty much.)
Might be a difference in the randomness sourcing. Since I'm on Windows the source is the system clock, while a *nix box would be pulling from `/dev/urandom`.
With that said, linked lists are terrible from a performance perspective however you cut it. They sure are better in Haskell than in C++, but an array of some sort would still win.
The time has come! Update your packages! Or I will find you ^and ^^probably ^^^submit ^^^^a ^^^^^fix
I'd love to record my class, but I have no idea how! If anyone wants to help, please get in touch with me!
pandoc and most things (aside from Snap) have been working on 7.8 for a few months! This is large part thanks to HVR making the awesome multi ghc travis scripts available https://github.com/hvr/multi-ghc-travis
You're welcome! I have started wriring a book on this subject so that this sort of material is less scattered.
I think Michael is responding to complaints. PVP has very little to say about how to make a reproducible executable, and that is fine. 
wah wah
Preliminary google searching turned up these results: (mac focused) http://speaking.io/prep/recording-your-talk/ (old) http://blog.parleys.com/2009/12/18/how-to-record-your-talks/ (android app of unknown quality) https://play.google.com/store/apps/details?id=name.markus.droesser.tapeatalk (powerpoint specific) https://office.microsoft.com/en-us/powerpoint-help/record-and-add-narration-and-timings-to-a-slide-show-HA010338313.aspx
Speaking of binaries, any chance we could get an official Debian/Ubuntu build against the current libgmp, rather than the ancient libgmp.so.3?
FYI I tried this and it didn't help. Choosing the correct random number generator seems to be the most important factor.
Upper bounds never cause bitrot. If the package really does build with a later version of the dependency but the developer hasn't bothered to update the upper bound yet, cabal now allows you to tell it to ignore the upper bound and the package will build just fine. And if the package doesn't build with the later version - well, then it's not the upper bound that caused the bitrot. It is the *lack* of upper bounds that guarantees bitrot. That makes it much harder to use the package even a short time later, when the state of Hackage has changed, because it's much more complicated to find a working build plan. Leaving off upper bounds makes a package very unstable, and not very suitable for use in products that need to be supported over time.
The ultimate purpose of dependency bounds is to provide very important semantic information: the developer's best estimate of what versions of the dependent libraries are reasonably expected to work. So rule #1 should definitely be: lower and upper bounds should almost never be omitted. If that causes various kinds of build problems, that is a tool problem, not a PVP problem. Don't tell people to omit extremely important and unrecoverable information from their cabal files - fix the tools. And in fact, the cabal team is doing a great job of that. For example, upper bounds in a cabal file are no longer any cause for a package not to build; if you don't like them, you can just tell cabal to ignore them. And the new cabal.config/constraints facility makes it much, much easier to recover - in a reproducible way - from many of the most common kinds of build problems caused by dependency issues, even for packages with huge numbers of dependencies like yesod.. Etc. I do agree with leaving off bounds for libraries distributed with GHC, and for libraries that are included in the Haskell Platform. The bounds add no useful information in that case, and cabal does the right thing by default. One reason that the semantic information of dependency version bounds is so important is indeed for creating reproducible builds. It's not sufficient - as Michael says, you also need freezes, and cabal now provides that. But it's necessary, We don't need to drive developers crazy by requiring them to spend a lot time figuring out dependency version bounds. In fact, that is one of the important services of the PVP. For dependency bounds, just use what you can deduce about each package from the PVP, together with whatever you happen to have learned in practice while developing the package. Weakening the PVP would make it much harder to provide good semantic information in dependency bounds.
It must be that; it's slightly faster than the C program on my mac (0m0.234s vs 0m0.245s). 
In related news, https://www.youtube.com/watch?v=UDGsh-Hrxho :P
How long should it take until binaries are released? I'm on a fresh OS X Mavericks system and kind of excited ...
The final release notes aren't up yet, but here are the ones from 7.8.1-rc2, which should be representative, http://www.haskell.org/ghc/docs/7.8.1-rc2/html/users_guide/release-7-8-1.html
I think you are confused as to what is meant here by a reproduible build. Reproducible means building an application the same way (for this discussion that means with the exact same package set). While the PVP can aid in that effort, it can only be achieved with the help of tools outside of the scope of the PVP such as the soon to be released cabal freeze. As a side note, I find including smiley faces while slinging digs at people to be a poor way to communicate.
A youtube video is oddly appropriate for this specific message...
Sorry about that, I'll try to come up with a better title *before* I hit submit next time.
They are less terrible when used as codata, at least. They are also perfectly reasonable purely functional stacks. They just aren't arrays and shouldn't be used like arrays.
What is the purpose of listing the names of the supporters of the proposal? If it's purely about informing, then it would also be appropriate to list a few names of people who disagree (with their blessing of course).
Intro to FP using Haskell (also by Bird) will make those problems go away. Remember to do all of the exercises :)
I think there should be a curated set of allowed exceptions to the PVP rule that arise very commonly. The most prominent examples I can think of are depending on the `ByteString` or `Text` types (the type only) from their respective packages. It's 99.99% likely that you can safely leave off the bounds completely from those dependencies.
I've never worked in the field, personally, but I can see how e.g. [attoparsec](http://hackage.haskell.org/package/attoparsec‎) could be a clean replacement for an existing regex-heavy code base. Still, I stand by my initial point. Software, as an industry, is heavily reliant on well-specified requirements &amp; etc. Add to that a niche incorporating only a very small subset of the community (Haskell programmers who also read/want to work with Perl and C++), and it will be a small miracle if you can attract candidates with this level of job description. Even assuming this isn't a total troll post, I wouldn't want to work for an organization that puts so little effort into job listings.
I also saw that, but, since it doesn't mention Haskell and specifies one open slot (vs. the "few" in the OP), I went on the assumption that they were unrelated. The fact remains that, as I mentioned to protestor, there's little to prove the validity of this post. The email address could easily have been scraped, and this is the OP's only post.
Well, if anything the odd language here is Haskell. If you want to work with bioinformatics you should be willing to stomach Perl (at least read it). Perhaps the ideal candidate to them would also have experience with bioinformatics but that would be even hard to find. Since they didn't mention it they might be willing to train their hire in this aspect. But training people in Perl would be harder. Some years ago people would say that Perl 6 had some intersection with the Haskell community (I'm not sure the extent of that). Well Perl 6 ended up not very much used, but I suppose that it wouldn't be hard to find Haskell programmers who is familiar with Perl.
I am considering writing a blogpost about them, but I just need to find the time to do it. 
New to the Haskell community, what exactly does this post title mean?
The Glasgow Haskell Compiler, which is the most popular Haskell compiler, is about to have a new release with loads of exciting new stuff that people have waited a long time for.
&gt; I do agree with leaving off bounds for libraries distributed with GHC, and for libraries that are included in the Haskell Platform. The bounds add no useful information in that case, and cabal does the right thing by default. As far as I understood, Michael does mostly argue to leave off bounds for *non-upgradable* packages. The HP packages (and also most of the GHC libraries) are most certainly upgradable.
You're probably right; Simply stated, I considered "reproducible build" to mean that if there was a package on Hackage that I could `cabal install foobar` with a given GHC version at some point in time, I would be able to do that for each later point in time (e.g. 1 year later) using the very same GHC version (at least). Isn't that was the PVP was created to accomplish in the first place?
This would mean that any single Hackage package upload would bear the risk of breaking the builds of many people until the maintainer(s) (NB the author of the breaking package, and the author of the broken package are usually not the same person) of the package(s) depending on that package have had time to react to either fix the package or add upper-bounds retro-actively to all(!) past versions which now are known to break with the new package upload. This sounds like a really fragile situation for Hackage, where the more interdependent packages exist on Hackage, the more likely it becomes at any given time that such an unfixed breakage exists.
The purpose seems to be a variant (the specific name I can't remember right now) of [Appeal To Authority](http://www.nizkor.org/features/fallacies/appeal-to-authority.html)
/r/netsec appear to have syntax highlighting, not sure how though. 
&gt; It is the lack of upper bounds that guarantees bitrot. This!!! Basically the only reason I ever get bug reports for old code of mine not building is because some library "foo" I depended on asked for "bar &gt; 1.2", and "bar-1.7" doesn't fork with "foo-0.8" anymore. What you are saying semantically when you write "bar &gt; 1.2" is that you assert that your library will build with any version of bar, created now or at any time in the future. Obviously you can't hold up your end of the bargain and now cabal **cannot** possibly have enough information to give you a viable build plan.
&gt; I didn't say remove upper bounds as a feature completely, just to stop excluding versions that don't exist yet. [...] When it comes out and breaks things, that's when we should put the bounds there. Do you realize that by that you're basically saying the PVP can be thrown out of the window altogether? The workflow you're suggesting requires only a very simple versioning scheme which is basically just a simple scalar which gets increased monotonically. I.e. just use natural numbers (or something isomorphic to that) as the package version, use lower bounds, and set upper bounds after-the-fact. The PVP goes to lengths about declaring a semantic major/minor/patchlevel versioning scheme together with rules how to write your code **for the sole purpose of being able to declare upper version bounds excluding future package versions**.
Sounds like more of a record industry term, rather than a programming one.
eegreg is correct in his explanation: reproducible builds are about reproducing the exact same build that existed previously, not about being able to build the software again with newer versions. It's incredibly important to use reproducible builds in production software, as even a minor version bump could introduce runtime bugs. It's unfair to imply any kind of ill intention on my part by the usage of the term clarify. I was told directly by an author of the PVP that the PVP was never about reproducible builds, which is why I used the term clarify.
Actually there were build breakages due to API changes for the base that ships with 7.8. We (e.g. bos) fixed lots of breakages for you so you wouldn't be the wiser. 
&gt; I do agree with leaving off bounds for libraries distributed with GHC, and for libraries that are included in the Haskell Platform. The bounds add no useful information in that case, and cabal does the right thing by default. That's far broader than my proposal actually, I definitely am *not* proposing removing upper bounds on all Haskell Platform packages. I was very narrow in what I proposed: base, template-haskell, etc. Cases where cabal is unable to install a different version. &gt; It's not sufficient - as Michael says, you also need freezes, and cabal now provides that. But it's necessary, That's not true. Freezing is *all* that's necessary for reproducible builds. If all version bounds were removed from all packages on Hackage (**not what I'm proposing**) reproducible builds would still be possible.
I discussed it offline with some people, those four gave support to the proposal, no one said they opposed it. This isn't an appeal to authority, it's just stating who was part of the proposal making process.
Not completely on topic, but regarding that cabal feature: it's wonderful, and I'm glad it's being added, but it also isn't a complete solution. The issue is that you end up throwing out the baby with the bathwater. Imagine a package with the dependencies: base &gt;= 4 &amp;&amp; &lt; 4.7, aeson &gt;= 0.6 &amp;&amp; &lt; 0.7 If I'm testing against GHC 7.8, I'd like to drop the base upper bound, but keep the aeson upper bound, as it has a higher likelihood of being necessary for a build to succeed. I don't really see a tooling way around this... unless perhaps cabal had a flag to "ignore upper bounds on this set of libraries." Anyway, --allow-newer is definitely a huge step in the right direction, but even so, I'd still like to drop upper bounds on base and template-haskell.
it's slightly faster than C code! 
I really don't like the chapters in the style of a conversation between teacher and students. All the important results are labeled Anne, Mary, Theo etc and I can never remember what each said or who they are.
I would be very interested in this too.
&gt; By default, GHC will now unbox all "small" strict fields in a data type. Excellent!
&gt; That's not true. Freezing is all that's necessary for reproducible builds. If all version bounds were removed from all packages on Hackage (not what I'm proposing) reproducible builds would still be possible. But you have to admit, that without any version bounds you might need some time to find a buildable/working combination of package versions which you could then freeze. Bounds don't guarantee reproducible builds, but they might help you to get faster in the right direction. I think that's the whole point of bounds and upper bounds, until they sometimes stand in your way, but every solution has its negative aspects. 
you can still use `--constraint 'aeson &lt;=0.7'`. but yes, a `--allow-newer base` would be nice.
Why not try monadloc? : https://github.com/pepeiborra/monadloc
I also think bos fixed a bunch of packages several months ago.
You cannot fix the bounds by uploading a new version, as Cabal with happily pick the old version without the bounds. You could have to mutate the old packages, which brings with it a whole slew of headaches.
Ok, and after all this discussion about who gets it and who doesn't, could you please explain *what does it mean exactly*? You know, for those who don't get it... Does it simply mean a release? Does it have any additional meaning? In other contexts, say drugs, "cutting" is not exactly a positive thing... The content of the link didn't help much either.
&gt; This would mean that any single Hackage package upload would bear the risk of breaking the builds of many people That's more-or-less true now anyway. I will be the angry and probably senseless minority here, but imho PVP causes more pain and problems than it solves. And anybody who needs reliable builds will need much much stronger tools than PVP anyway. &gt; This sounds like a really fragile situation for Hackage Hackage is *extremely* fragile already. 80% of complex packages (with large dependency graphs) always fail to build. That may go down to 50% with a completely fresh platform install.
&gt; 80% of complex packages (with large dependency graphs) always fail to build. While that could be true (how did you come up with that number?) you're leaving out an important detail: You seem to be expecting to upgrade to a new GHC version, thus **forcing** new package versions with major versions bumps to be used, and still expect old packages which depend on those to work? Sorry, but you're expecting something that isn't supposed to work this way. Otoh, there will be an older version of such a package which will work (if there was any such combination in the past), and Cabal will find it, unless somebody was lazy (or simply chose to violate the PVP for other reasons) and didn't set proper upper bounds.
Is there any downside at all to this automatic unpacking? By the way, I like your slides on the representation of Haskell data structures. I read them the other day and they were very helpful.
Ah yes. In which case I find it somewhat suprising that it's a net win to automatically unpack. I suppose that's the magic of the GHC optimizer at work!
Has anyone figured out how to check out the cut? I tried the instructions here: https://ghc.haskell.org/trac/ghc/wiki/Building/GettingTheSources#Gettingabranch But I got the following error: $ ./sync-all get ghc-7.8 == running git clone http://git.haskell.org/libffi-tarballs.git libffi-tarballs ghc-7.8 Too many arguments. 
Well it's not a light read that's for sure. It's more a book you study. It's good advice to read while having ghci open to run some of the code. Some chapters are way harder than others, and they are not arranged in order of difficulty. Although the first chapter is probably the easiest one to understand, and gives a good introduction to the approach used in the book. Some of the algorithms are pretty complex. But that is the point of the book. Most of the solutions presented could not have been found easily by trial and error. Most of them were derived from a simple but inefficient initial implementation (which could be regarded as an executable specification). The techniques used to derive each final and more efficient algorithm is what makes the pearls.
Indeed. In real programs most e.g. `Int`s are used strictly as there's no real way to use them non-strictly without using them at all. There are of course exceptions but I tested this change on nofib and a bunch of benchmark suites in popular libraries. This change will work better and better as people (hopefully) stop having non-strict scalar fields in their data types. In my experience that's almost always a performance bug.
So exciting!! I don't think I've ever been this excited for the release of a compiler update before...
At first I was a bit thrown off like you. But the saddleback search chapter is well worth studying. It made me download "Concrete Mathematics" by Knuth to learn the techniques to transform a recursive formula to a closed form.
Thanks you to everyone who worked on this release! It looks extremely exciting. Does anyone know what the extent of the SIMD support in 7.8.1 is? The announcement says support is "preliminary", and the release notes say &gt; The LLVM backend now supports 128- and 256-bit SIMD operations. I realise these features are new, and thus should not be treated as bulletproof, but are there any practical implications of this for casual users like me who are interested in the SIMD features (of libraries like `vector`)? EDIT: Also, does anyone have a rough idea of when various libraries will be updated to utilize these features? I know there was a fair bit of talk about `vector` and the SIMD intrinsics a while ago, but I never how much actually made it into the library.
I was especially confused when I saw "GHC 7.8.1 Released" immediately followed by "GHC 7.8.1 release has been cut" on the front page. I thought it had been released and then immediately withdrawn for some reason...
Will pattern synonyms eventually allow to inject functions in pattern matching, and get the features of Scala's extractors? I mean: data Seq a = ... pattern ViewL leftmostElem restOfSeq = {- equivalent of what viewL does now -} If so, I think Seq, of course, and libraries like operational or usage of free monads may benefit from it, by abstracting out the conversion from the full AST (Program in operational's parlance) to the view of one cell of the AST.
no, that is not a reproducible build and not what the PVP is designed to accomplish. There are several known scenarios where the PVP can be followed by everyone and your build will break in the future. In the context of installing dependencies for an application this is poor software engineering practice because you have no guarantees that you will have the same behavior in your application even if things keep building. A reproducible build means building the exact same code that worked before, guaranteeing both that the build will succeed and the application behavior is the same.
If you haven't found out yet, [they are released](http://www.haskell.org/ghc/).
That's probably the etymology (where "cutting a record" meant actually cutting the negative master with a lathe), but people say "cutting a release" for software all the time: https://www.google.com/search?q="cut+a+release" If you're expecting English to make sense, you may not be a native speaker :)
The obvious question: when will a haskell platform be released with ghc 7.8.1 ?
&gt; In GHC 7.10, Applicative will become a superclass of Monad I can die a happy man.
Put a dependency on ```lens``` in your next/current project. You'll get the Haskell platform.
Agreed. And having bounds in place is a much better default. Small constant overhead, but way better asymptotic complexity. 
I see. It's still not clear to me what the haskel platform is for exactly. Is it just a list of versions of packages that are know to work well with each other ? Is it really so nightmarish to just install the packages directly through cabal using whatever version they have currently in hackage ?
on windows, compiling some package requires using something like msys or cygwin. For ex network. The packages are reviewed. I see this as the batteries in a python distribution. With the platform, someone can write scripts/ small apps without cabal, and without caring about versions. 
You shouldn't use it yet unless you like pain, low-level primitive operations, and general instability. For one, there are no libraries taking advantage of this. Geoff Mainland, GHC hacker extraordinare (he implemented the New Template Haskell and SIMD operations) is now the maintainer of `vector`, so it's likely `vector` will eventually see his improvements to incorporate SIMD. These changes were originally in a branch when he did the work which is now out of date. So it needs to be reintegrated. But there's no high level API as we speak. Second, these operations are only supported with the LLVM backend. You might think that's not a big deal, but in practice it is a *huge* deal. It means you literally need to compile everything with LLVM that might use it. This is because of the way GHC does cross-module optimization. If you use this LLVM only primop in a module (say a client library of `vector`), it's possible for that primop to be inlined at the use-site of another, completely separate module, even in another library. That means that module - the use site - must *also* be compiled with LLVM. Otherwise the ordinary code generator will freak out because it doesn't know what to do with these unsupported primops. Previously, the regular code generator and LLVM were 'indistinguishable' - they used the same calling conventions and could interoperate. This breaks that, because now LLVM-only things can 'leak into' a regular module compiled by the NCG, and explode things. Terrible. That said if you're adventurous, feel free to take a dive using the primops. But it's going to be rough going, it's likely to change, and it's a relatively experimental (and potentially buggy) API at the moment.
The ghc-7.8.1-i386-unknown-mingw32.tar.xz tar ball is incomplete. There are no dyn libs. Are they coming, or is this a deliberate regression? (I ask, since we payed for them. :) )
I mean that base-4.7 broke a bunch of packages, not because of their upper bounds but because that API actually changed.
It's on the roadmap for 7.8.2, dynamic libs were pulled for Windows right now. There are some rather unfortunate complications here that made supporting dynamic libs rather difficult and required constant fixing (notably symbol limits on Windows.) :( I have something of an idea of how to fix it properly. But it's a bit of work no matter how you slice it. I also know of an acceptable fix for 7.8.2 as well. Actually, with all the bugs we fixed, it may just be possible to re-enable the dylibs right now and get them working most of the way (there's otherwise one big blocker that I know how to work around). I'll look into it today if I can and see what I can find out.
The HP implies a certain quality and stability of the libraries included in there*. * Some libraries were grandfathered in for boring historical reasons, so not all packages are of the same quality, but all new packages should be.
Congratulations on the release, and thank you for all the work you've put in! Huge fan, etc., etc. Why is this an advertised feature if it comes with such severe breakage?
I really, really wanted to put Kazu's Mighttpd on a ridiculous server (~120GB of RAM + 32 cores) and point about 30 other servers at it to benchmark it, to see the kind of utilization we can get. Unfortunately I didn't get around to it and ran out of time. Maybe I can do a follow up on the blog sometime later this week...
Well, it's not like we don't tend to advertise rather experimental features in every release anyway and let users break them. :) Hence the 'experimental' point in the release notes (I'll also note there aren't docs for this, since it's not something most people will touch or care about. More of just a point of note, than anything) That said, some people are surely OK with this tradeoff and more than willing to get their hands dirty. It *does* work, but it's definitely not consumer ready yet, and so most people will want to wait. But adventurous friends need not wait.
Thanks for all the work that has gone into this release!
Two more books that use this style are Conceptual Mathematics by Lawvere and Schanuel and Proofs and Refutations by Lakatos. Lakatos is the more egregious because of his Alpha and Beta and Gamma and Delta and Epsilon and .... Yes, he has a cast of 20 characters and all of 'em wear Greek alphabets! What's surprising is that Lakatos imo is closest to breathing life and individuality into his people. And it's such a damn good book in resetting and healing my perceptions of math, restoring dialogual and contingent truth to pride of place, that I'm willing to put up with 10x more cruft. But they all write for an older time when you couldn't call yourself educated without acquaintance with arts and literature. They all knew their Pirandello and Beckett and suffer strivings of the inner thespian. 
Really cool to see the type-level numbers are getting more and more usable. And the addition of closed TFs makes me think: does somebody has already starting developing a lib easing the use of type-level literals and collections? I see scattered implementations, for instance, vinyl defines a [type-level ++ function](http://hackage.haskell.org/package/vinyl-0.2/docs/src/Data-Vinyl-Rec.html#%2B%2B). Plus I think now we can define (thanks to closed TF) membership test functions that were tricky before: import GHC.TypeLits type family (:&amp;&amp;) (a :: Bool) (b :: Bool) :: Bool where True :&amp;&amp; True = True a :&amp;&amp; b = False type family (:||) (a :: Bool) (b :: Bool) :: Bool where False :|| False = False a :|| b = True type family Contains xs x where -- actually kind inference works! Contains (x ': _xs) x = True Contains (_y ': xs) x = Contains xs x Contains '[] _x = False (it requires -XTypeFamilies, -XTypeOperators, -XDataKinds, -XKindSignatures and -XPolyKinds) This also makes me think we could benefit from a unification of the kinds Bool and Constraint, i.e. (1) to be able to use Bool-kinded type expressions where class constraints or type equalities ("a ~ b") are expected and (2) to be able to use &amp;&amp; and || on type level Bools. Here my ```Contains list x``` wants to be a Constraint, but I don't know a Constraint equivalent to False, nor a way to defined &amp;&amp; and || type operators for Constraints.
This would be awesome. I've been wanting to see the performance numbers for multicore web servers ever since reading the new new IO manager paper. The authors measured 700,000 requests/second for a simple network server on a 20 core machine, and even uncovered a rare Linux bug (which was [fixed](https://patchwork.kernel.org/patch/1970231/) :-) ). I'd love to see the scalability numbers for real-world Haskell web server with 7.8
So what you are also requiring by putting an explicit number in the pattern match is that each element of your list is able to make comparisons to that number to see if it matches. That's what it's talking about when it says "add (Eq a) to the context ...." So the correct type for that function is actually: (Eq t, Num t) =&gt; [t] -&gt; Bool 
To determine whether `(5:xs)` matches or not, haskell needs to check whether the first element is equal to 5 or not. Pattern matching on numbers and chars requires `Eq`. So you need both the `Eq` and `Num` constraints. This is specific to pattern matching on numbers: any other kind of pattern matching doesn't require this.
iOS support sounds really exciting. Is there a guide for this?
Makes sense now. so startsWith5 (Eq a, Num a) =&gt; [a] Bool Is the way to do this. THANKS!
Thank you - very helpful!
The easiest way to get a functor's fixpoint is just `data Fix f = Fix (f (Fix f))`. This is simpler and prior to free monads.
Luke hasn't uploaded the binaries. In the mean time the same steps and binaries are available for RC2: http://www.haskell.org/ghc/dist/7.8.1-rc2/README.ios.html Soon there will be binaries, and you can follow the same setup.
The platform also includes cabal (and happy, etc). Installing cabal when you don't have cabal installed is not as easy as typing `cabal install cabal`!
`xs` can be `_` in both of your cases, btw :) 
The main reason why to your type signature fails us because you're saying "hey y'all, this function should work for *any* Num a." But then later on you compare it, so ghc is like "ummm this won't work for all Num a's... cause what about Nums that you can't test for equality? Your type signature is waaaaaay too loose man, and is not even true, wtf. You can't possibly tell me that you can use this on *any* Num. You're a dirty liar." Then you can change it to say "this function works for all things that are *both* Nums *and* Eq/comparable for equality". And ghc has no problem with that. 
On Windows, it's also ease of installation. Also on Mac OS X to a certain degree, if you prefer a regular installer dmg to something like homebrew.
I've just started working through this book, but speaking on your second point that's a pearl right in the preface. It's analogous to Knuth's "premature optimization is the root of all evil" but speaks to me more for some reason. &gt; I think the editors had asked me because I was interested in the specific task of __taking a clear but inefficient functional program, a program that acted as a specification of the problem in hand, and using equational reasoning to calculate a more efficient one.__
Yes, that's true. But when we do not specify any bounds, cabal will use `installed` by default for those packages. So if we make the following reasonable assumptions: 1. People always either have HP installed, or know what they are doing. 2. The HP version or versions that we expect the package to be able to work with can be deduced from the dependency set as a whole. (Not necessarily so in theory, but almost always the case in practice.) Then omitting bounds for those package does not lose any information for the human reader, and will also work in practice with cabal.
It's also nice for library writers to know "okay, this dependency is really no problem since haskell-platform users will have it"
&gt; There is a new extension, NullaryTypeClasses, which allows you to declare a type class without any parameters. What.
If you have things installed with 7.6, can you just replace ghc with 7.8 and everything will work out fine?
I am not aware of any special testing that HP benefits from.
&gt; cause what about Nums that you can't test for equality Could you help me figure out what "Nums" cannot be tested for equality? Even Complex numbers can be tested. I just tried the following for example print (1.0 :+ 1 == 5) It prints out False Is there a type that I am missing that can represent irrational numbers that does not offer the Eq maybe? 
Pugs (the original Perl 6 interpreter) was written in Haskell, and a few of the features in Haskell rubbed off.
If you're talking about libraries, then no: packages are not binary or ABI compatible in any way. You'll need to reinstall the new packages with GHC 7.8 if that's what you want. Tools shouldn't be a problem in general (e.g. you don't need to rebuild alex, happy, cabal, etc). You can, however, have multiple GHC installations coexist peacefully and without problem.
Technically, that's *always* existed (see `hint`, `hs-plugins` or the GHC API). The difference is now that when you call `unloadObj` in the GHC API, it actually does what it says and unloads the object :P
I have a library in early stages ([shapely-data](hackage.haskell.org/package/shapely-data)) that I think explores some of what you're talking about. And I've been looking forward to refactoring it with closed type families (and still very eagerly waiting for nice type inference properties for closed type families to catch up). One of the goals is to be able to compose data, and support doing algebraic operations on your ADTs. Definitely would love any comments you have if it looks like your thing. I don't understand the last bit of your comment. Couldn't you have: -- :: Constraint type ContainsConstraint xs x = Contains xs x ~ False
You're right that the actual freeze itself has nothing to do with PVP, and that I was referring only to the process of achieving a build worth freezing. It was an answer to a post here, not an answer to your original post. I disagree that PVP can ever bifurcate Hackage. Developers should never be told to withhold information they have at their fingertips about what dependency versions their package can be expected to work with. If providing good information could ever cause a Hackage bifurcation, that is a bug in cabal, not in PVP. The clear and consistent semantics of PVP make it much easier to express that information. PVP is not unique; it is similar to other such policies that are widespread in the industry.
You mean fail to build on the first try, before you tweak constraints, etc.? So what? 100% of packages that ever built can be gotten to build again. The fact that it takes some work to build complex packages has nothing to do with fragility of Hackage. It has to do with tool support. Solving build plan puzzles for complex packages happens to be an inherently very difficult problem. Our cabal team has been doing an amazing job, but there will always be improvements that can be made. If you think omitting upper bounds would make it easier, then go ahead, tell cabal to ignore upper bounds. (Yes, you can do that now.) I wish you the best of luck.
This has been under discussion. I hope it makes it in.
Upper bounds do express useful and important information about future versions too. I might be using only a single function from a library, and I might know that the function's API has been historically extremely stable over many major version bumps. On the other hand, I might be using a library extensively in a way that makes it very likely that even a minor version bump will break my package. Upper bounds express the difference between various such cases, and it is the PVP that makes it possible.
&gt; You mean fail to build on the first try, before you tweak constraints, etc.? That's also a problem, but no, I mean that I most often than not have a set of packages whose dependencies are not compatible with each other, and Cabal cannot really handle multiple versions (or sometimes even a single version...). &gt; So what? 100% of packages that ever built can be gotten to build again. Yes, maybe, by the previous incarnation of the author, on the author's old computer which is gathering dust in the toilet, *then maybe*. Ok, in practice it's not that bad, but from a user's point of view, basically that's the situation. &gt; The fact that it takes some work to build complex packages has nothing to do with fragility of Hackage. Hackage is fragile because people change the packages frequently, and often don't care about backward compatibility. Plus, we have these version bounds, which I'm not exactly sure really help. Lower bounds can also cause problems (one package requires a new ByteString or Text, ok, I install that, then everything else breaks and I have to delete and reinstall everything...) &gt; It has to do with tool support. Indeed, at least partly. Cabal is fragile because it cannot do what people expect from it, and also we have no real package manager (Cabal is a build tool, not a package manager, it pretty much sucks when we force it to be a package manager). &gt; Solving build plan puzzles for complex packages happens to be an inherently very difficult problem. I agree, however I think that maybe we should work on that instead of making unenforceable and very limited policies. PVP is *so fucking far* from a solution that I have no words for it. I understand that the idea is that it is simple and it helps a little bit, *except that I'm not sure it helps even a little bit.* &gt; If you think omitting upper bounds would make it easier, then go ahead, tell cabal to ignore upper bounds. (Yes, you can do that now.) Which version of Cabal can ignore bounds, and what is the flag? Is there an up-to-date Cabal documentation somewhere? Because http://www.haskell.org/cabal/users-guide seems to miss a lot of stuff... (May be I should use sandboxes, but 1) that's just treating the symptoms 2) when I tried to use them once, I did not succeed, and gave up pretty quickly with no intent to return...)
 class AxiomOfChoice where {} class RiemannHypothesis where {} foo :: AxiomOfChoice =&gt; ... bar :: RiemannHypothesis =&gt; ... Also, if MultiParamTypeClasses are allowed, then nullary type classes being disallowed is kind of weird.
Tibbe, you made such an excellent summary of good packages a couple of years ago on your blog. Can't you make an updated version?
Well, there is a [review](http://trac.haskell.org/haskell-platform/wiki/AddingPackages) process.
That's true, although it will mean that we'll have to start to compile an ever growing list of known to be broken packages somewhere that everyone can copy from (selectively) every time they create a new project. :/
I will try to find some time. Hopefully one day the HP will just contain those packages and the list will be unnecessary. :)
 instance Num a =&gt; Num (r -&gt; a) where f + g = \x -&gt; f x + g x f * g = \x -&gt; f x * g x negate f = negate . f fromIntegral = const abs f = abs . f signum f = signum . f The sense of such an instance is to write things like `product * sum` to mean "take a list, calculate sum and product and multiply those". If you did it for `Fractional`, you could also say `average = sum / genericLength`, which reads very naturally. (This is just a special case of what applicative syntax does on functions, but I don't know if you know about applicatives yet). But you can't have computable equality on functions. You could also define a type for arbitrary precision real numbers, whose equality would be impossible to compute(they would be represented by having infinite digits to the left, so you can't check them all). So yes, there are useful instances of `Num` that don't offer `Eq`. That's why `Eq` isn't a superclass of `Num`.
Thanks DR6 - that now makes sense and explains that not every Num offers Eq! 
You're welcome!
right on! I would love to fix a bunch of buggy unsafe code by rewriting in Haskell.
Uhm.. It [cabal *, including repl] just works™ unless i'm missing something. Yesod-bin even complains [correctly] on base &lt; 4.7.
Indeed — they'll be ready later today! I'll be giving a (hopefully recorded) workshop on GHC iOS at BayHac '14 along with a demo of http://tree.is which is built entirely on top of it.
I like the idea of using `NullaryTypeClasses` to track the use of partial functions: {-# LANGUAGE NullaryTypeClasses #-} module Main where class Partial partialHead :: (Partial) =&gt; [a] -&gt; a partialHead (a : _) = a main = print (partialHead [1, 2, 3]) which, without explicitly opting into Partiality with `instance Partial`, results in No instance for Partial arising from a use of ‘partialHead’
I'm surely did - when 7.6 came out.
instead of changing your PATH you can cabal configure a packages with those flags if you want to test out the new compiler. It achieves the same thing, but you don't have to muck with your environment.
&gt; I disagree that PVP can ever bifurcate Hackage. I didn't state that as a matter of opinion, it's a matter of fact. The situations I've described elsewhere (some packages depending on &lt; X, others on &gt;= X) not only can happen because of the PVP, but in fact happen regularly. &gt; Developers should never be told to withhold information they have at their fingertips about what dependency versions their package can be expected to work with. There's no magical information that the developer has at his fingertips, what you're asking for is information that cabal has direct access to: what version of package X was available when package Y was uploaded to Hackage. That's it. In fact, put that way, I think we can get all of the benefits of preemptive upper bounds with a simple extension to cabal: detect the version of all dependencies at the time that the packages using them were uploaded, and then avoid using the following major version.
Thank you very much for the great explanation. Looks like I'll be waiting a release or two then.
I'm very sceptical of that review process (esp. having witnessed how it goes in practice). But even that is irrelevant. Even assuming that packages that are in the HP are somehow of better quality than those which aren't, there's nothing to be gained from installing them via HP (or installing their HP versions) as opposed to simply pulling them from hackage.
As a matter of fact, there are! data FakeNum = FakeNum instance Num FakeNum where _ + _ = FakeNum _ * _ = FakeNum abs _ = FakeNum negate _ = FakeNum signum _ = FakeNum fromInteger _ = FakeNum and voila, a `Num` with no Eq instance :) This is a perfectly well defined, well behaved and law abiding Num instance, and will work exactly as expected in all functions that expect Num... like `sum`. However, it does not have an Eq instance, so it can't be thrown into your function. 
I have a couple patches for a popular drawing library, which broke when base changed from 4.6 to 4.7.
GHC ticket: https://ghc.haskell.org/trac/ghc/ticket/7642
It is as easy as downloading the tarball and running bootstrap.sh though (well, not sure about Windows but nothing useful to developers on non-MS languages is ever easy there).
Sometimes it can end up as "Okay, this dependency is a problem since nobody but people who use the outdated HP use that version anymore" though.
I'm starting to think that all the PVP arguments are futile because they all stem from problems that should be solved with better automated tools. The current PVP puts too much of a burden on human intervention and suggested PVP changes suggest that humans should still play a big role in these maintenance issues. Proponents of the current PVP suggest that it helps software build far into the future. Those who want to revise the PVP rightly say that the PVP doesn't guarantee this. A better way to fix this is to have automated tools that record the dependency graph of a successful build, along with a way to install that dependency graph easily. That should make everybody happier. I'm glad to see progress on things like --allow-newer and cabal freeze. I hope that after awhile these and other tools evolve to the point where people no longer argue about the PVP because our computers are doing what they should: eliminating tedium and uncertainty by using automated processes. When the cabal file can write itself or when automatic tools can easily update the cabal file, we will all be better off.
Would that choice not infect all modules importing from wherever `instance Partial` is defined, though? Perhaps I'm misunderstanding...
Because it's harder to know if someone is right for the job, harder to keep them there, harder from a business standpoint, and there's always the possibility that they run off with the codebase.
cabal install --global cabal-install did the trick for me I think - I had no idea what was going on for awhile though
&gt; You could also define a type for arbitrary precision real numbers, whose equality would be impossible to compute (they would be represented by having infinite digits to the left, so you can't check them all). Arbitrary precision doesn't mean infinite precision. Just that the number of digits is not a priori bounded.
&gt;Really cool to see the type-level numbers are getting more and more usable. Could not deduce (n1 ~ (n - 1)) from the context (n ~ (1 + n1)) Apparently there is still a way to go.
But is lens the best option compared to all the competing libraries? That might be controversial..
It should really just be part of `-XMultiParamTypeClasses` without any further extensions in retrospect. I believe Shachaf Ben-Kiki proposed this initially (we were just talking on IRC), but apparently we just totally didn't hear him. I'm personally receptive to the idea of folding them together and just getting rid of `-XNullaryTypeClasses` and deprecating it in HEAD. It'd probably be a 5 line change or so - patches welcome from somebody :)
`System.Random` does not claim to be cryptographically secure, it's for things like random numbers for games or sorting algorithms. `crypto-api` is the right place to get CPRNG
The text on the linked site doesn't show up for me.
Euh, I think that's a bug with the web fonts. I'll temporarily fall back to normal fonts until I can fix the issue.
It's a big benefit to new users. If you're used to the Haskell ecosystem that doesn't matter, but having the canonical library for most needs already installed on your system (with the assurance it'll work properly) is huge for people new to Haskell. There's a reason people like the "batteries-included" aspect of Python, or Go's comprehensive standard library.
&gt; Num (r -&gt; a) This example is great. On the other hand, according to [Haskell 2010](http://www.haskell.org/onlinereport/haskell2010/haskellch6.html#x13-1350006.4): **6.4 Numbers** ... The class Num of numeric types is a subclass of Eq, since all numbers may be compared for equality... class (Eq a, Show a) =&gt; Num a where ... And also in this code for [exact real arithmetic](https://github.com/andrejbauer/marshall/blob/master/etc/haskell/Reals.hs): -- | It is a bad idea to use Haskell-style inequality @/=@ on reals because it either returns @True@ -- or it diverges. Similarly, using Haskell equality @==@ is bad. Nevertheless, we define @==@ and @/=@ -- because Haskell wants them for numeric types. instance IntervalDomain q =&gt; Eq (RealNum q) where x /= y = force $ x `apart` y Has this specification changed since Haskell 2010?
Ah, good catch, thanks!
The article seems really great and I will definitely have to try fgl at some point but blue on blue on blue is quite hard to read.
I think the biggest problem is that the middle column, which the black text is on, is too dark so it's annoying to make out the text itself. I think the same will be true of the code boxes - the background of them is just too similar to their text, plus all the syntax highlighting is blue too! Is there a 'standard' set of colours that's used for Haskell code? It's a shame that this issue distracts from a interesting and educational article, does your blog have an RSS feed?
Not yet. It's certainly on my list of things to add, along with comments. This is actually the very first article, so things are still in a state of flux.
GHC has removed those two prerequisites for Num.
That's a good point. I suppose I should mention this is in the context of an individual, i.e. only me doing the benchmarks. I'm not going to be dropping $20k or even $1k on a server any time soon for such casual benchmarks. By all means in this case, that server is adequately ridiculous for such a benchmark (and plus just about the right size to test the scalability of the I/O improvements anyway, which top out close to 40ish cores for I/O intense workloads).
yup what austin said
Is that really a code sample from the book? That looks like it's not only somewhat inefficient but not even correct in the general case. Something like `deleteNth` isn't going to exist because it's not that common and it's easy to build with existing primitives. For example: λ&gt; let deleteNth n xs = let (a, b) = splitAt n xs in a ++ tail b λ&gt; deleteNth 0 "asdfasdf" "sdfasdf" λ&gt; deleteNth 4 "asdfasdf" "asdfsdf"
It might be worth noting that if you are using `!!`, or treating the list as an indexed container with random access...you mostly likely have the wrong data structure. If you want something you can index, you should use an array or a vector or something. Anything involving lists that does not involve 'stream processing'/control flow/forward iteration of some sort should make you reconsider using them; Haskell has many great random access data structures. Things like delete, etc. -- not for these linked lists, typically. Also, I'm pretty sure that `delete (stuff !! n) stuff` does not actually delete the nth element (every time)...it actually just deletes the first element in the list that matches the nth element. So...that is hard to do without traversing it least the first `n` steps initially. For a `deleteNth`, you could consider something like `splitAt`, `second (drop 1)`, and `uncurry (++)`. Spoilers: deleteNth n = uncurry (++) . second (drop 1) . splitAt n For fun times, consider how many times the half of the list is duplicated, assuming full evaluation of the final list.
You've just solved one of those problems I've always been too apathetic to look up. Thanks
you already have two answers using elegant style and existing functions. While you should always use those you might get more inside from a naive recursive function like this deleteN :: Int -&gt; [a] -&gt; [a] deleteN _ [] = [] deleteN i (a:as) | i == 0 = as | otherwise = a : deleteN (i-1) as
Well, if you can any precision at any time, it's the same thing as having infinite precision in the haskell sense. That's what I meant. For granted there are more ways to represent that than the one I used.
It's normal for people to express support/opposition in response to proposals, so it also saves a bit of bandwidth to list initial supporters in the body.
I guess this makes sense for Go, since its primary use case is for writing servers.
Maybe it'd be simpler to try and compile and test against all versions and see which ones work. 
I think the short answer is dependency management is in more than just the signatures, and the compiler is always going to check if a given set of deps are interface compatible when the compiler runs.
The inductive graph representation reminds me of a zipper for a tree.
It really *is* horribly nightmarish (or even quasi-impossible) to install packages with external dependencies (eg. http, opengl) through cabal, at least on non-linux platforms. For example HTTP I *never* managed to build myself. Plus, the platform is guaranteed to have compatible versions, so you don't get sucked into the cabal hell within 5 minutes of using Haskell (this saves you a few weeks of pain...) So yes, the platform is crucial, at least in the present situation. (edit: it's probably network, not http which is the problem, but it doesn't really matter) 
But also just as inefficient as the version OP posted. Hint: why is `xs ++ [x]` an anti-pattern?
For Haskell programmers, parallelism and concurrency are naturally two totally separate concepts. It's interesting that for others the distinction is not obvious, and that some even have trouble understanding the difference.
./configure --prefix=... error while loading shared libraries: libgmp.so.3: cannot open shared object file: No such file or directory That is some old shizazz, modern Linux distros are well past libgmp.so.3 days. Only solution I've seen on the net is to symlink so.3 to so.10, you gotta be kidding me, anyone have a better solution? Fedora 18 here. Thanks.
That would probably involve too much compilation work. But I'd like the approach: Gather all possible version combinations newest ones first. Start compile/test cycle. Stop as soon as the first working combination was found. This would probably motivate more people to write more tests :)
Yes but here you cannot stop the recursion (i.e. return pure values). Is it still useful? I mean: what have you used Fix (not Free) for?
Decorate, filter, undecorate: deleteNth n = map snd . filter (\(ix,_) -&gt; ix /= n) . zip [0..]
&gt; assuming that packages that are in the HP are somehow of better quality than those which aren't That's not what we said. For instance I believe vector is still not on the platform. The goal is to provide beginners some "batteries included" feeling, because this has been a recurring criticism against Haskell. Seasoned Haskell users may just pull libs from Hackage as they need them.
&gt; I don't understand the last bit of your comment. Couldn't you have: &gt; -- :: Constraint &gt; type ContainsConstraint xs x = Contains xs x ~ False Didn't you mean: type ContainsConstraint xs x = Contains xs x ~ True rather? Yes, but how do you go the other way around? You can't use logical operators on arbitrary Constraints, you'd need to convert them to Bool before. Constraints were a good match before type level bools, now that we have them we also need logical operators (bools are not very useful without them). Thanks for the link! I'll have a look.
The OP did, and I've both read it on various open source project's sites and heard it from coworkers, so it can't be "nobody".
That [Succs](http://www.haskell.org/haskellwiki/Type_arithmetic).
Right, from [GHC 7.4.1 release](https://www.haskell.org/ghc/docs/7.4.1/html/users_guide/release-7-4-1.html): * The Num class no longer has Eq or Show superclasses. A number of other classes and functions have therefore gained explicit Eq or Show constraints, rather than relying on a Num constraint to provide them. 
Not the easiest to understand but clearly the most featureful. Actually once you stop reading the real types and start reading the shortened types of the documentation along with the big [UML diagram](http://hackage.haskell.org/package/lens) that links those types together it becomes manageable. YMMV, but for instance I found easier to understand ```lens``` than ```conduit```s (and *way* easier than FRP). It's also by far the one which is the most used (which is normally not a token of quality), but that means you can easily get help. I just hope it gains enough traction so that it may be used in the next wave of Haskell books &amp; tutorials. If not, well, one of the assets of ```lens``` is that you may quite easily develop lenses, isos, traversal &amp; prisms around existing datatypes for libraries not providing a lens-y interface.
Before I continue to argue with you, let me make it clear that I really liked your post. It's common sense. I also highly appreciate your work toward improving the Haskell build ecosystem, such as the fantastic Stackage project. I do think we should think carefully about how to define when upper bounds can be omitted, and not just remove the requirement from PVP. Having a clearly defined semantics for version numbers is valuable. In all of your examples of cases where upper bounds can be omitted, the upper bound is not adding any information. &gt; (some packages depending on &lt; X, others on &gt;= X) not only can happen because of the PVP, but in fact happen regularly. I don't call that a bifurcation. If the packages really can't build with each other, then PVP has done its job. If they can, you can override that and still do the build. But then the build is outside the scope of what the package authors recommend and support, so it makes sense for cabal not to do that unless you specify that you want to. For those who are always on the bleeding edge, it is already possible to make ignoring upper bounds the default on your computer. Even without globally ignoring upper bounds, it should be easy to override conflicts, and for the whole build plan, not just for each conflict. It used to be quite difficult, but recent versions of cabal have vastly improved it. &gt; There's no magical information that the developer has at his fingertips, what you're asking for is information that cabal has direct access to: what version of package X was available when package Y was uploaded to Hackage. That's it. Not true. The developer often knows much more than that, as has been discussed elsewhere in this thread. Such as: The nature and extent of the use of each dependency. The stability of the parts of the API being used, as compared with stability of the dependency as a whole. The method that the developer of this dependency appears to have used in the past for choosing version numbers, with emphasis on how that relates to the particular part of the API we are using. And much more. All of that influences the choices a human being makes when estimating bounds if that human takes just a few seconds to think about it rather than omitting the bounds altogether. I don't think that is an unreasonable burden to place on developers.
I can see no reason to explicity forbid these, but are they actually useful for anything?
The full [quote](http://www.haskell.org/ghc/docs/7.8.1/html/users_guide/release-7-8-1.html#idp5784032): * In GHC 7.10, Applicative will become a superclass of Monad, potentially breaking a lot of user code. To ease this transition, GHC now generates warnings when definitions conflict with the Applicative-Monad Proposal (AMP). A warning is emitted if a type is an instance of Monad but not of Applicative, MonadPlus but not Alternative, and when a local function named join, &lt;*&gt; or pure is defined. 
I don't think this will scale well with the number of transitive build-deps. Your total search space would be the product of all versions for each package, and I don't know how well you can reduce the complexity by cutting down the solution-space traversal.
Some of the other utilities (can't remember which ones) require earlier versions of themselves to be installed in order to build. Having prebuilt binaries as part of the platform is useful.
So, what you're saying is that I shouldn't write data Foo = Foo { foo :: Int } but instead data Foo = Foo { foo :: !Int } in (pretty much?) all cases?
Because these two are just low level implementation details and very often border between them is blurred. Compiler algorithms should make a decision when to apply one of them or both - this is the same level of abstraction as garbage collection when you no longer need to manually allocate/free memory because in a sufficiently large software you will almost always do it wrong. Haskell programmers distinguish between them because they are able to deal with them easily, but still do it manually, in contrast to e.g. Prolog or SQL where an algorithm decides what to use. In this light Haskell is a low level language. Others do not bother or don't care because they've been warned and/or their languages make concurrency so hard that it is avoided at all costs.
I want to be as clear as I can: when I'm talking about upper bounds, I'm only talking about the preemptive upper bounds for cases where *no one knows if it will break*. If version 1.2 of foo exists, and you know your code doesn't support it, I think every sane person agrees that `&lt; 1.2` is an absolute requirement. (Though I try to implore everyone to just make their code work with the newer version anyway, to avoid bifurcation.) So the information I'm talking about here is absolutely something that a computer could generate for us. I'm actually surprised that no one has mentioned this previously. It's completely possible, for example, for Hackage to provide an alternate 00-index.tar.gz file that adds in all preemptive upper bounds automatically, and thereby make PVP advocates happy without putting any burden on package maintainers. So my views on versions bounds: * Bounds on any non-upgradeable package are basically useless. They can theoretically save users some time and give nicer error messages, but the benefit is IMO far shadowed by the costs of extra version bounds causing bifurcation (e.g. [this issue](https://github.com/fpco/stackage/issues/128) took six months to close). * Lower bounds: always make sense to include, unless by some magic your package works with *every* historic version of a dependency. * Upper bounds: * If your code definitely doesn't work with a released version of a package, definitely include the upper bound. * If your code might not work with a package to be released in the future, we need more common-sense rules than what the PVP outlines right now to avoid unnecessary bifurcation. I'm trying to propose baby steps towards that. * Unpublished applications: if your code isn't going to Hackage, then you should be using version freezing instead of version bounds. Version bounds can help you get an initial build plan, but after that they only work to hide the possible problems of having inconsistent build environments among team members.
List comprehensions cover the non-generic uses of map and filter pretty well. Using those for lists and generic ones otherwise seems reasonable.
with list comprehensions: deleteNth n xs = [ x | (ix, x) &lt;- zip [0..] xs, ix /= n ]
I think that's a safer default, yes.
&gt; That's not what we said. What do you mean? You said: &gt; You gain from the testing that's been performed My response was: &gt; I am not aware of any special testing that HP benefits from.
This is one of the services Snoyman's stackage provides us with regards to the most recent versions of each package in it. It gives me a great deal of assurance to know that `lens`, `diagrams`, `pipes` and `yesod` and hundreds of other packages all build together many times a day off the latest versions for continuous integration. Before stackage the platform was _critical_ to know we had eventual consistency. Post-stackage? We get it more or less nightly, with explicit bounds captured when there is a known integration issue. It may be exceedingly badly named as it isn't a 'stable hackage', rather more of a continuous integration server, but it has really changed the discussion about how critical everything being in the platform is among those who use it.
And the talk is now available: https://skillsmatter.com/skillscasts/5049-fun-and-profit-with-stongly-typed-data-schemas
Here's 7.8.1 iOS: https://github.com/ghc-ios/ghc-ios-scripts/releases/download/7.8.1-device/ghc-7.8.1-arm-apple-ios.tar.bz2 https://github.com/ghc-ios/ghc-ios-scripts/releases/download/7.8.1/ghc-7.8.1-i386-apple-ios.tar.bz2 https://github.com/ghc-ios/ghc-ios-scripts/blob/master/README.md
Indeed. Types also don't necessarily identify bug fixes or performance enhancements. For that, you need to have a measure of how good an implementation is, and so far, that can only be given by humans.
Cool! Does anyone know of library that would allow me to play them back on Mac OS?
That's what my [cabal-rangefinder](https://github.com/gelisam/cabal-rangefinder) tool does! Given a cabal file listing the package names on which the current project depends, but not their version ranges, cabal-rangefinder finds those ranges by binary-searching through the available versions to find the lower bounds. It assumes that dependencies are independent from each other, and that if your project compiles with a particular version of a package, it also compiles with all subsequent versions.
Sneek Peek: I'm working on it. https://bitbucket.org/robertmassaioli/wavy
I disagree on two of those points: &gt; * Upper bounds ... If your code might not work with a package to be released in the future I strongly disagree with your implication that PVP has serious problems in this case. For me, this is probably the most important service that PVP provides. The lion's share of my Hackage build problems are caused by package authors who fail to provide upper bounds that estimate what *future* versions of a dependency are likely to work with their package. What you call "bifurcation" is a problem that can be solved with better build tool support. (It might also help to *increase* the information provided by upper bounds by providing syntax to distinguish between estimated bounds and actual known compatible or incompatible versions.) Whereas build failures due to missing dependency upper bounds often means debugging the source code of other people's packages. That's not to say that PVP can't be improved. But it should be improved with better and richer semantics, not by dumbing it down. The other point I disagree with (mildly) is: &gt; * Unpublished applications In a proprietary shop, you can use whatever build tools you want, and there are many available. But it is not true that cabal's dependency resolution is always useless. It depends on how your different teams working on different packages interact with each other, your interaction with Hackage, how your testing and release cycle works, etc. If what you mean is for release builds, then yes, once you release a build, you need a version freeze of that build; that is the only way to be able to reproduce it reliably for support purposes.
Right, [found this post](http://lambda.jstolarek.com/2013/02/dont-panic-its-only-an-upgrade/) on upgrading GHC where the author ran into the same issue. The distro supported approach is nice in that you can easily install/uninstall. If I build from source I'd like to avoid having GHC bits strewn about my system; i.e. one target directory so I can blow it away as need be is the goal ;-)
Use the `centos65` tarball. - http://www.haskell.org/ghc/dist/7.8.1/ghc-7.8.1-i386-unknown-linux-centos65.tar.xz - http://www.haskell.org/ghc/dist/7.8.1/ghc-7.8.1-x86_64-unknown-linux-centos65.tar.xz
Oh, awesome! I'll keep an eye on it, thanks!
Generating ADTs. It is also the canonical fixpoint of a functor. Free, while related, is something else.
This traverses the list (deconstructs every `(:)`) twice, once for `take` and then again for `drop`.
Ok, and your functor handles the stop by itself, that's it? Like in: newtype BinTreeCell v a = BTC a v a | EmptyC deriving (Functor) data Fix f = Fix (f (Fix f)) type BinTree v = Fix (BinTreeCell v) ??
Downloading ghc binaries and cabal-install is sufficient to install anything that does not require a non-Haskell library or tool (and those too provided you do install those non-Haskell dependencies). At worst you have to build it twice, once without full documentation (e.g. no coloured source) and once with it.
Ah, so you'd get e.g. an "is Monad" type-level predicate automatically in some way just by having Monad. If I'm understanding you correctly, that's something I'd also love to have. In shapely-data I do that pretty extensively (see e.g. https://github.com/jberryman/shapely-data/blob/master/src/Data/Shapely/Normal/Massageable.hs), and your proposal would simplify things I think. I wonder what sorts of problems that would create. And do you think what you're suggesting would be useful with classes intended to be open?
The a part is traversed twice (because of how ++ works).
To sum up, I think the ideal would be to remove Constraint and use Bool-kinded expressions. (So this way, ```Monad IO``` would reduce to ```True```, and the compiler would accept ```f :: (a, b, c) =&gt; x -&gt; y -&gt; z``` only if a, b &amp; c all reduce to ```True```) There are certainly problems with that \^\^: for instance Constraint is in a certain way an inhabited kind (dictionaries of functions, corresponding to the class instances), whereas the Bool kind is not. There is certainly a reason why you can't use "or" on class constraints. For now, I think the easiest is to stick to Bool kind, use type families and operators like :&amp;&amp; and :||, and convert it to a Constraint at the very last moment by ```f :: MySuperBoolExpression ~ True =&gt; .....```. It's a bit ugly, but not unpractical. The nicest solution would be to lift functions (or just the simplest functions, I'm not saying Haskell should become Agda) along with data when -XDataKinds is used (because as said, booleans are not of much use when you can't use logical operators on them, and the same goes for every datatype deprived of its associated functions), that means tons of problems ahead, of course, but closed type families already *are* type-level functions.
Actually you can cut the search space quite easily by doing only what developers would have to do anyways to test... All that you need are hints =&gt; this dependency worked with version X And then go checking the nearest versions around X, up and down. And stop wherever it stop working... Not much different than most devs would do. 
Would this be more or less efficient than the index-and-delete method? How can I learn to profile and answer such questions myself?
Congrats on a really very solid release! Almost hard to believe this wasn't 7.9 :-)
Just like only part of the list is traversed twice with (!!) and delete. They do almost the exact same thing.
I'm completely open to using another data structure... like I said, this is a code sample from the book. The author probably just didn't want to complicate things. On the other hand, in languages like C and C++, a linked list is exactly the data structure to use when you want to delete something from the middle of a sequence (since you only have to rewrite a few pointers to do so). I know that Haskell makes a new copy anyway; does this mean that a linked list is no longer efficient for this task?
Input and Output, in the Todo program. http://learnyouahaskell.com/input-and-output
I've long thought that Haskell has the raw materials to do a much better job than pretty much any other language out there (excluding the small number of dependently type languages of course) at solving the the dependency version problem. What you describe is essentially an automated system for setting all of a package's dependency version bounds. The interesting thing about it is that when you think about the problem this way it becomes apparent that reexports should really be treated exactly the same way that all your other exported symbols are treated. This highlights an interesting situation: when package bar re-exports some symbols from package foo, but also has very wide bounds on package foo spanning several major foo releases over which some of the re-exported symbols changed their type signature. In this situation, the version number of bar is almost meaningless--it's the version of foo that discriminates bar's API. But there is a good chance that its users don't know this. I wonder if there's a way to handle this better without jumping to the unreasonable burden of specifying a bound on all transitive dependencies.
Not sure if you're correcting me or amending/clarifying, because I did realize that. It didn't seem relevant when answering quchen though.
 deleteNth n = map snd . filter fst . zip (replicate n True ++ False : repeat True)
`deleteN` is a better name. Everyone else (including me) is following OP and using the name `deleteNth`. But the function actually deletes the element *after* the nth one.
&gt; The nicest solution would be to lift functions (or just the simplest functions, I'm not saying Haskell should become Agda) along with data when -XDataKinds is used (because as said, booleans are not of much use when you can't use logical operators on them, and the same goes for every datatype deprived of its associated functions) Right, I want that (or something like it) too. Right now I do with TypeFamilies + classes (which I can't properly close) + OverlappingInstances + (often) UndecidableInstances, etc. what seems basically like a lifted function/type.
errrm, that's exactly what I've been using o_O CentOS is stone age material relative to Fedora, Ubuntu, et al, not surprised re: the libgmp.so.3 issue. Anyway, I'll either build from source (nice intro to Haskell ;-)), or upgrade to Fedora 20 and go with distro supported 7.6.1/2 Have 7.4.1 running right now just to hack around in GHCi, but would prefer to try out the latest and greatest of GHC with EclipseFP. Assume incremental builds are not far off, in my dream world I'd have a terminal open with an incremental build running and EclipseFP providing IDE support. This works really well in Scala land right now, so am hoping Haskell is not far off from this functionality (as most everything else on the Haskell side of the fence is quite appealing). 
It's indeed strange that LYAH would choose this particular implementation.
See, for example, [the `lens-family-core` package] (http://hackage.haskell.org/package/lens-family-core).
And then again for (++).
For what it's worth, LYAH makes less-than-ideal decisions in a few places, like almost any introduction to FP.
&gt; cabal-rangefinder Please upload to Hackage so it gets more visibility!
One day! Polishing cabal-rangefinder and uploading it to hackage is somewhere on a long todo list, and your comment just bumped it way up.
&gt; Prolog or SQL where an algorithm decides what to use. In this light Haskell is a low level language Haskell exposes the high-level parallelism stuff as well. It also exposes low-level non-deterministic concurrency primitives, because it is general-purpose. It is on a wider spectrum between low-level and high-level, rather than lower-level.
Er, maybe! Do you have an example where that assumption could lead to trouble? I'm not a dependency expert, I just wanted a quick tool to automate the steps I would have done manually in order to find proper dependency ranges for my packages. Maybe my manual steps were wrong! Here is how I use the assumption. When cabal-rangefinder is binary-searching, it only puts a restriction on a single package at a time. If I find that your package compiles fine with `foo == 1.0` and no other restrictions, and it also compiles fine with `bar == 1.0` and no other restrictions, then I will output `foo &gt;= 1.0` and `bar &gt;= 1.0` as the final lower bounds for your project, even though I have never explicitly tested version-forcing `foo == 1.0` and `bar == 1.0` at the same time. In particular, maybe `foo-1.0` only works with `bar-0.5`, and only later versions of `foo` support `bar-1.0`. Then the specific combination `foo == 1.0, bar == 1.0` will not work. But if your project's cabal file specifies `foo &gt;= 1.0, bar &gt;= 1.0`, as cabal-rangefinder will output, and if `foo`'s cabal file correctly states that it needs `bar == 0.5`, then I think cabal-install will do the right thing; it will find versions of `foo` and `bar` within your range limits such that the dependencies of your dependencies are also satisfied. Won't it?
That doesn't say too much about separating the concepts - concurrency will be usable for parallelism unless it's crippled with something like a global interpreter lock. The question interesting question is whether you were forced to use forkIO. Were you doing something obviously deterministic that no parallel library supports? monad-par is pretty new to me, but seems to support most things you might otherwise do with threads and MVars (except for allowing a shared cache between threads). Of course, I think there will always be concurrent computations which are deterministic for reasons beyond the reach of any particular parallelism framework.
Often, these things end up being useful for base cases of some sort. I could also see it useful for typeclass that just "tag" types without requiring any particular structure. More generally, having special cases is bad and this removes one. I figure it's like empty data declarations, but for typeclasses.
Most of the code at our startup is in haskell. One component is a proxy server duplicating a subset of the functionality of LogMeIn. Another is a routing manager that routes building control protocols around inside zigbee radio packets in a wireless radio mesh. Haskell really, really shines for large, complex, concurrent programs. The more you learn how to encode your intentions into types the more it feels like you're pair programming with an AI. :)
&gt; To sum up, I think the ideal would be to remove Constraint and use Bool-kinded expressions. Don't do that. At least don't do that without thinking about it a ton. Constraints are *constructive* logic. Constraints give you the openworld assumption. Not being able to prove `C a` does not imply that you can prove `Not (C a)` and this is a feature not a bug. &gt; For now, I think the easiest is to stick to Bool kind, use type families and operators like :&amp;&amp; and :||, and convert it to a Constraint at the very last moment by f :: MySuperBoolExpression ~ True =&gt; ...... It's a bit ugly, but not unpractical. The solution to that is a "sumbool"
I've played around with [Tidal](http://yaxu.org/tidal/) a bit and it's really cool. I think this kind of thing is amazing; music is so inherently mathematical (algorithmic?) that the medium sort of fits, one way or another. Awesome stuff. :)
Great article! It is indeed the ecosystem that makes or breaks a language. p.s. "function fame" lol.
Cool video, awful title. Should be "Haskell without IO is useless. " He is just saying programs need io to be useful. Edit: io not monads
Haskell without monads isn't useless either. The fact that IO is a monad is irrelevant.
Sorry, I should have clarified; the linked list great okay for "delete the first matching element", but not necessarily "delete the nth element". And actually, I'll have to get back to you on that :)
 deleteN !i (a:as)
There's something called *versioned symbols* that solves a lot of these issues. The main problem, however, remains: People are apparently too stupid to make their changes in a backwards-compatible way. We can engineer countless solutions, but all of them won't solve the problem that people don't expand their API's in a compatible way.
What happens when you need to set the lower bound because of i.e. a bug in the package?
I hope to be able to put out a Platform sometime in May. Hold on to your hats, it's going to be a rapid ride!
Why isn't the latest cabal delivered with the latest ghc? 
The platform is attempting to make it *very* easy to decide to use Haskell. One download, one install, you're done for a large range of development tasks. We've a ways to go still, as we don't have the breadth of libraries that Python, PHP, or Java come with, but we can get there. Also, if we *ever* want to get to the point where distros will just decide to include Haskell, we have to have a common definition of what constitutes the base Haskell one should have. Imagine Python if each distro had a different set of the 50 or so base packages!
are you and the author aware of the massive amount of typos the book has?
&gt; I know that Haskell makes a new copy anyway it would make copies of the first n-1 elements the rest of the list would be a pointer to the original lists n'th tail http://en.wikipedia.org/wiki/Persistent_data_structure#Linked_lists &gt; linked list is exactly the data structure to use when you want to delete something from the middle of a sequence if you're doing a lot of insert/delete from the middle you might want a tree look into Map
This is really cool, I also used the game of life as one of my first adventures in Haskell. The bulk of your performance problem is the result of using the definition: data Board = Board [[Cell]] deriving Show The default list implementation in Haskell is not an array, it is a linked list. So your access times are going to be very slow. You might consider switching from the use of Haskell lists to the use of vectors. There are two really good libraries for doing this: * http://hackage.haskell.org/package/vector * http://hackage.haskell.org/package/repa I would recommend using REPA for this project, it already provides a way to represent 2D space in vectors. I wrote a paper on the performance implications of the game of life back in the day. It is available here: http://blog.headcrab.info/haskell-optimization-and-the-game-of-life/. There is also a reddit discussion about it which has some gems of knowledge: http://www.reddit.com/r/haskell/comments/1ozvpl/haskell_optimization_and_the_game_of_life/ Welcome to the wonder that is Haskell!
That's what I always want.
Technically you don't need to think of as haskell `Functor` as a functor to a subcategory of Hask, just as a functor from Hask to Hask, that _happens_ to only target objects/arrows of that form. That distinction is important so you can continue to compose them.
The type constructor `f` defines the mapping from `a` to `f a`. No morphism from `a -&gt; f a` is implied by the definition of a `Functor`. objects are just places to hang your arrows.
Makes sense. I've gotten a few good responses about this, thanks for another. :)
&gt; For instance I believe vector is still not on the platform vector is very much in the HP. http://www.haskell.org/platform/doc/2013.2.0.0/packages/vector-0.10.0.1/doc/html/Data-Vector.html
Haskell will copy only the cons cells of the first n-1 elements, It will not copy the elements themselves. In this way we preserve immutability (other threads/reducers cannot see the changes we make to our list) yet share almost all the data and most of the container structure (reducing memory overhead).
Is there some sort of list that says: "Libraries that would make the Haskell ecosystem that much better." or "A wishlist of the libraries that Haskell should have to compete"? I was wondering what Haskell should do in order to match other languages in terms of functionality. If a large number of companies start to adopt Haskell then we win right? We get to write Haskell for our days jobs, the world gets software that is more robust and everybody goes home happy.
If you are writing chains of more than ~3 functions together there is a good chance that it should be refactored out into individual functions, and testing the functions individually should be easy.
The difference is that haskell doesn't have pointers. If you want a simple list data structure with a cursor that supports efficient next, previous, and delete (sort of like a Java iterator) you can use something like: data Zipper a = Zipper {left :: [a], cursor :: a , right :: [a]} I don't know if that's technically a zipper, and that particular definition only works for nonempty lists (you could fix that by making the cursor a maybe), but it's kind of a neat thing to know. If the implementations for next, previous, and delete aren't clear, try them out. (Hint next, previous, and delete are all Zipper a -&gt; Zipper a)
You should definitely factor out long chains... but making chains work on sides of tuples are actually pretty easy, with some helpers from Control.Arrow. You have `first`, which takes any function chain and transforms it into a function chain that only operates on the first part of a tuple if you have f . g . h, then first (f . g . h) is the same thing, but only on one side of a tuple. then there's ***, which takes two function chains and turns them into parallel function chains, with each working on different sides of a tuple. combine it with something like `\(x, y) -&gt; (x, x)`... and you Zhou have no problem. the main key is working with "function transformers", because functions are first class objects. you can also use proc notation, which allows you to label every step of your function composition, and then arbitrarily pull out intermediate steps and access or return them at the end. I'll put up an example when I get on a computer with a keyboard. 
If you are testing the chain with quickcheck (and if not, why not), generate test cases where the input is a single-element list - then there is only a single element that could have resulted in the wrong output. Remember you can use `printTestCase` to report the input/output data when the property fails. Consider writing your complex chain as `mapMaybe f`, where mapMaybe is from Data.Maybe and f is a complex function that transforms a single value, and testing f instead.
Real World Haskell has a section on profiling, also StackOverflow is a great resource with lots of active haskell users. Check out all the great stuff over there `------------------------------------&gt;`
let's say you have chain = f . g . h . i . j And you want to make a chain where you observe the output of `i` and `g`; you can do chain = proc x -&gt; do y &lt;- i . j -&lt; x z &lt;- g . h -&lt; y r &lt;- f -&lt; z returnA -&lt; (r,(y,z)) so that'll basically make a new function that takes a value, and returns the result of your full, plus the intermediate results after applying `i` and applying `g`. But this isn't too different from chain x = (r, (y,z)) where y = (i . j) x z = (g . h) y r = f z except that it's anonymous, so you can do things like foo = x &amp; proc x -&gt; do ...
I want to expand on the edit, because it points to two bugs in the code, with one hiding the other. ---- createRandomBoard :: Int -&gt; IO Board The result here has type `IO Board` instead of `Board` because it is supposed to be random. That is, if you have do let create = createRandomBoard 10 x &lt;- create y &lt;- create printLn (x == y) then the chances that this will print `False` should be overwhelming. But with your code (after you add an `Eq` instance for `Board`), this will print `True`. What's wrong? createRandomBoard x = do g &lt;- getStdGen return (Board (chunksOf x (take (x*x) (randoms g :: [Bool])))) The last line is pure: it has no side-effects, and it doesn't depend directly on any global state. And yet it contains `randoms g`, which generates an infinitely long `[Bool]`! If you inserted the line printLn (take 100 (randoms g :: [Bool]) == take 100 (randoms g :: Bool)) it would print `True` every time. In a sense, the only random value in `createRandomBoard` is `g`: everything else depends deterministically on `g`. `g` is the result of executing `getStdGen`; [`getStdGen`](http://hackage.haskell.org/package/random-1.0.1.1/docs/System-Random.html#v:getStdGen) “gets the global random number generator”. Do you see the problem? What would this print? do g1 &lt;- getStdGen g2 &lt;- getStdGen printLn (take 100 (randoms g1 :: [Bool]) == take 100 (randoms g2 :: Bool)) `True`, every time. `g1` is the same as `g2`. `getStdGen` gets the global random number generator, but nothing is causing the global random number generator to be updated. Instead, use [`newStdGen`](http://hackage.haskell.org/package/random-1.0.1.1/docs/System-Random.html#v:newStdGen), which updates the global random number generator. ---- So you replace `getStdGen` with `newStdGen` and suddenly your program doesn't work: it stores a bunch of boards on disk, sure, but instead of them being successive generations from the game of life as before, they're now unrelated to each other. What gives? writeBoard :: Int -&gt; Int -&gt; FilePath -&gt; IO Board -&gt; IO () writeBoard gen x name b = do res &lt;- liftM (htmlSvgBoard gen x) b writeFile name res `b` is not a `Board`. `b` is an `IO Board`. `b` is not a simple `Board` value: it is a computation (that may have side-effects) which will eventually result in a simple `Board` value. And that value could depend on anything the computation accesses. In principle that could be the clock, files, the network, etc, but this particular computation accesses the global random number generator. So we're generating a different random starting board for each output file. Almost every function that refers to `IO Board` or `IO [Board]` should instead refer to `Board` or `[Board]`: evolution :: Int -&gt; Board -&gt; [Board] writeBoard :: Int -&gt; Int -&gt; FilePath -&gt; Board -&gt; IO () But this is right: createRandomBoard :: Int -&gt; IO Board And you need to change `main`: main = do board &lt;- createRandomBoard 10 writeEvolution 1 10 (evolution 10 board) See? We now ensure only one `Board` value is created randomly, and that all the other boards are created deterministically from that one board. This also addresses part of your performance issue. Instead of * generate a random board → write it to disk * generate a random board → step it forwards a generation → write it to disk * generate a random board → step it forwards two generations → write it to disk * etc the program now does * generate a random board → write it to disk * step it forwards a generation → write it to disk * step it forwards a generation → write it to disk * etc 
The only currently maintained alternative to GHC that I know of is [UHC](http://www.cs.uu.nl/wiki/UHC/WebHome). I don't know anyone that uses UHC, but some people must because it's still being updated. One reason might be that it can compile to Javascript. Another recent "alternative" is [Intel's Haskell Research Compiler](http://www.leafpetersen.com/leaf/publications/hs2013/hrc-paper.pdf), but that uses GHC as the frontend, which suggests that GHC is basically a defacto standard. Also, quoting from [http://www.haskell.org/haskellwiki/Implementations](http://www.haskell.org/haskellwiki/Implementations): &gt; GHC is the de facto standard compiler if you want fast code.
yes thanks - but maybe we should explain this a bit to the OP - using ! (see [BangPatterns](http://www.haskell.org/ghc/docs/7.4.1/html/users_guide/bang-patterns.html)) - makes the function strict (well technically it reduces it to WHNF ... but no we are going to shave some yaks here) in this argument
Some more minor points. ---- More of an issue than all the concatenations is what you're concatenating: a `String` is literally `[Char]`, and not very efficient. This was fine when Haskell was solely a research language, but less so now. Instead, use `Text`. Add `{-# LANGUAGE OverloadedStrings #-}` to the top of your source file (before the imports). Add import Data.Text (Text) import qualified Data.Text as T import qualified Data.Text.IO as T import Data.Monoid to your import block. Then change e.g. svgRect :: Int -&gt; Int -&gt; Int -&gt; Int -&gt; Cell -&gt; String svgRect x y w h cell = "&lt;rect x=\"" ++ show x ++ "\" y=\"" ++ show y ++ "\" width=\"" ++ show w ++ "\" height=\"" ++ show h ++ "\" fill=\"" ++ (fillColor cell) ++ "\" /&gt;" where fillColor cell | cell = "black" | otherwise = "white" to svgRect :: Int -&gt; Int -&gt; Int -&gt; Int -&gt; Cell -&gt; Text svgRect x y w h cell = "&lt;rect x=\"" &lt;&gt; T.pack (show x) &lt;&gt; "\" y=\"" &lt;&gt; T.pack (show y) &lt;&gt; "\" width=\"" &lt;&gt; T.pack (show w) &lt;&gt; "\" height=\"" &lt;&gt; T.pack (show h) &lt;&gt; "\" fill=\"" &lt;&gt; fillColor cell &lt;&gt; "\" /&gt;" where fillColor cell | cell = "black" | otherwise = "white" or, better, svgRect :: Int -&gt; Int -&gt; Int -&gt; Int -&gt; Cell -&gt; Text svgRect x y w h cell = mconcat ["&lt;rect x=\"", T.pack (show x), "\" y=\"", T.pack (show y), "\" width=\"", T.pack (show w), "\" height=\"", T.pack (show h), "\" fill=\"", fillColor cell, "\" /&gt;"] where fillColor cell | cell = "black" | otherwise = "white" (building a list of text segments to concatenate, and concatenating them in one fell swoop) (and elsewhere replace `writeFile` with `T.writeFile`.) (Really you should probably use a svg+html library to do this, but I don't know of one off the top of my head.) ---- evolution :: Int -&gt; IO Board -&gt; [IO Board] evolution x b | x == 0 = [] | otherwise = b : evolution (x-1) (liftM step b) As I said in my other comment, the type should be different here. evolution :: Int -&gt; Board -&gt; [Board] evolution x b | x == 0 = [] | otherwise = b : evolution (x-1) (step b) Pattern matching on 0 is clearer than comparing to 0. evolution :: Int -&gt; Board -&gt; [Board] evolution 0 b = [] evolution x b = b : evolution (x-1) (step b) Explicit recursion can be unclear. What does this function give us? It gives a list of length `x`, the first element of which is `b`, and subsequent elements are `step` of the previous element. evolution :: Int -&gt; Board -&gt; [Board] evolution x b = take x (iterate step b) (`take` and `iterate` are both in the prelude.) ---- writeEvolution gen x (z:[]) = do writeBoard gen x ((show gen) ++ ".html") z writeEvolution gen x (z:xs) = do writeBoard gen x ((show gen) ++ ".html") z writeEvolution (gen+1) x xs The `z:xs` in the parameter list is confusing, particularly with the `x` parameter before. The `x:xs` convention plays on `s` indicating plural in English. But you have `xs` as being the plural of `z`, and `x` is unrelated. writeEvolution gen x (board:[]) = do writeBoard gen x ((show gen) ++ ".html") board writeEvolution gen x (board:boards) = do writeBoard gen x ((show gen) ++ ".html") board writeEvolution (gen+1) x boards The `writeBoard` line is duplicated unnecessarily. Also, the function crashes if given an empty list of boards. writeEvolution gen x [] = return () writeEvolution gen x (board:boards) = do writeBoard gen x ((show gen) ++ ".html") board writeEvolution (gen+1) x boards This is boilerplate iteration down the list of boards. If each iteration differed only in the board it was processing, we could replace it with `mapM_`, but we also bump `gen` each time through, so we must replace it with `zipWithM_` instead. (Both are from Control.Monad.) writeEvolution gen0 x boards = zipWithM_ step (iterate (+1) gen0) boards where step gen board = writeBoard gen x (show gen ++ ".html") board ---- step :: Board -&gt; Board step b = let dim = getDimensions b in Board $ chunksOf (snd dim) (map (\x -&gt; calculateCell x b) [(a,c) | a &lt;- [0..((fst dim)-1)], c &lt;- [0..((snd dim)-1)]]) Pattern matching can be clearer than explicit `fst` and `snd`. step :: Board -&gt; Board step b = let (xDim, yDim) = getDimensions b in Board $ chunksOf yDim (map (\x -&gt; calculateCell x b) [(a,c) | a &lt;- [0..(xDim-1)], c &lt;- [0..(yDim-1)]]) ---- calculateCell :: (Int,Int) -&gt; Board -&gt; Cell calculateCell x b | countLive x b == 3 = True | countLive x b == 2 &amp;&amp; isLive x b = True | otherwise = False where countLive x b = length (filter (\x -&gt; x == True) (getNeighbours x b)) isLive x b = (getCellAt x b) == True /u/kqr has already pointed out a couple of improvements to this function. calculateCell :: (Int,Int) -&gt; Board -&gt; Cell calculateCell x b | countLive x b == 3 = True | countLive x b == 2 &amp;&amp; getCellAt x b = True | otherwise = False where countLive x b = length (filter id (getNeighbours x b)) If you only ever call an inner function with arguments the outer function receives... why not let the inner function access them directly? calculateCell :: (Int,Int) -&gt; Board -&gt; Cell calculateCell x b | countLive == 3 = True | countLive == 2 &amp;&amp; getCellAt x b = True | otherwise = False where countLive = length (filter id (getNeighbours x b)) Sometimes case...of is clearer than multiple guards. calculateCell :: (Int,Int) -&gt; Board -&gt; Cell calculateCell x b = case countLive of 3 -&gt; True 2 -&gt; getCellAt x b _ -&gt; False where countLive = length (filter id (getNeighbours x b)) 
Yes. Some people still program in Haskell 98 in specific circumstances (a bit like some people actually do use old versions of Fortran), but when you read or hear "Haskell" you should assume the language being described is "GHC Haskell". This Haskell is a single-implementation language. Of course, a particularity of GHC Haskell is that there are so many boxes to tick to use or not use language features that nobody programs in the exact same subset. I think it's fair to assume that most people actually consider that they have the whole language at their disposal (all the options), only they prefer to enable features as they're used instead of by default. There are some conventions as to which features are best avoided or at least not used togethers (GADTs are fine, multi-parameter classes need consideration), but those conventions evolve with the language, the understanding and the practices of the community.
Does someone more knowledgeable than me know about [the AJHC compiler](http://ajhc.metasepi.org/)? It is a fork of the JHC compiler, boils down to a small C API that is the Haskelll runtime. This fork seems current (at least updated in Github as of February 2014) and it is part of an interesting project, Metasepi, trying to rewrite NetBSD drivers, and later other parts of the BSD kernel, purely in Haskell. I found it by accident. Seems pretty cool.
Quickcheck has you covered here. It will automagically try to *shrink* `[good, good, bad]` into `[bad]`. I am not sure if that succeeds always. But for these simple cases it should work.
If you want to be on the safe side, use the standardized Haskell. This is Haskell-98 + some extensions. Things are added to the standard slowly. And, yes, some people use other Haskell compilers extensively.
I would just use `Debug.Trace` to look at intermediate values. import Data.Trace foo = traceShowId . f . traceShowId . g . traceShowId . h . traceShowId (You may want to attach a little more info to each trace to be able to distinguish them.)
If you have prop_myProperty testData = property $ tautologous testData then yes. If you have prop_myProperty = do testData &lt;- myGenerator property $ tautologous testData then I don't think it does. Unless this has changed recently?
I do trust you on that but would you like to highlight with some examples?
Whoops, that's what I meant! Thanks.
It was so clear to me what you meant to say, it took me a while to realise you hadn't actually said it :/
You should try to order your parameters more carefully. take getCellAt. It is used in `map (\x -&gt; getCellAt x b)` - you could rewrite this expression as `map (flip getCellAt)` - it is a sign of wrongly ordered parameters. you should reverse them, because you are more likely to map it over a list of position than a list of boards. `getCellAt b` is a function which provide access to b's content Same for calculateCell 
Welcome :-) Hope you have a great time there. I did!
I just started reading your article, it seems like you went a little bit deeper down the rabbit hole ;)
I don't really understand how the "automatic adding of needed arguments works". Could you explain the thinking behind going from islive x b = getCellAt x b to islive = getCellAt ?
Would like to write IOKit (Darwin/OS X) drivers in some functional language, not in C++ to which apple ties us.
&gt; (GADTs are fine, multi-parameter classes need consideration) I would say that multi-parameter typeclasses are way more widely used than GADTs, if only because `mtl` uses them. 
Well, Lennart himself there has his own dialect Mu, and his older `hbc` compiler, which at one point polymorphed into [Bluespec](http://www.bluespec.com/). There is also some work going into [ajhc](http://ajhc.metasepi.org/) in the embedded space. 
You don't want Haskell98, you want Haskell2010. 98 doesn't even have hierarchical module names... though most people just ignored that fact, as all implementations just supported them silently.
&gt; New revisions of the language are expected once per year. The first revision ("Haskell 2010") was released in late 2009. Certain revisions will be denoted "major versions", which are intended to be supported for longer periods. The latest major revision is still Haskell 98.
A lot of Haskellers like to live on the bleeding edge, and don't just assume GHC but *newish* versions of GHC. I am extremely conservative with the extensions I enable, because I believe it makes my code easier to understand (as well as portable), especially for less-advanced users, but that position is unpopular.
&gt; I am extremely conservative with the extensions I enable, ... but that position is unpopular. Is it? My general sense is that the community generally considers reducing the need for extensions is generally a good thing. You may sometimes need the extensions, but you should give at least some though to using a minimal set.
Haskell-98 + some extensions was supposed to refer to the standard. I didn't want to call it Haskell-2010 since it supposed to evolve every year.
I was pretty interested in this at first but in fact very few libraries are available for it and the maintainer has moved on to ATS for his kernel project.
That's one thing that *really* bothers me about the Skills Matter: it is expensive (in my point of view) and I just don't see (maybe I'm the problem) the tube videos of their courses. Are there videos available (InfoQ, YouTube, Vimeo, etc...) of their courses?
Answering the "how can I learn" portion: there's a lot of resources to learn how to profile Haskell, you can find with Google pretty easily, but I wanted to speak to the *intuition* of functional programming. Many of the people answering your post or talking about the efficiency or time-complexity of your algorithm and data structure (or theirs even) have an intuition for it. Particularly in Haskell, the native list implementation is generally not the right fit for most applications unless (someone already said it better than I) you're doing something that takes particular advantage of Haskell's lazy evaluation of the list or streaming stuff. Knowing that Haskell lists are really lazy means you should by-default look for other data structures that are typically more efficient and have the built in functions you are used to from other languages. https://hackage.haskell.org/package/containers-0.5.5.1 Look through that and you'll see what I'm talking about, I use this package **a lot**. Map, Set, and Sequence I've used many times and the docs all have time-complexity notes on the functions. So understanding the evaluation model of Haskell is the first and most important part of developing that type of intuition and will also help you be skeptical of your program's strict or lazy intentions. That's where profiling gets handy *after* you've picked the data structures that are appropriate for what you're doing. The second piece to building an intuition is to understand functional style, in general. What is Weak Head Normal Form? What is recursion? Tail recursion? What is cons (if you never did anything with LISP)? Those are important things to understand when writing functions that could potentially hurt you without knowing. The difference between these two functions and the good one Koenig posted for you (I've seen these in the wild, more in other functional languages, less in HS) is subtle to someone new with FP but important (ignoring for a minute that a native Haskell list is not a "well fitting" data structure to use): badDeleteN :: Int -&gt; [a] -&gt; [a] badDeleteN _ [] = [] badDeleteN i (a:as) | i == 0 = as | otherwise = reverse $ (badDeleteN (i-1) as) ++ [a] badDeleteN' :: Int -&gt; [a] -&gt; [a] badDeleteN' _ [] = [] badDeleteN' i (a:as) | i == 0 = as | otherwise = [a] ++ (badDeleteN' (i-1) as) **CKoenig's version:** goodDeleteN :: Int -&gt; [a] -&gt; [a] goodDeleteN _ [] = [] goodDeleteN i (a:as) | i == 0 = as | otherwise = a : goodDeleteN (i-1) as Here you have two bad functions that many people new to functional programming think are entirely okay (some won't use the a:as pattern matching notation but something like take or head instead, but that's not important to my point) but have atrocious implications for the running time of your program. `++` with lists is "usually" yucky because you're traversing the whole list and when you do it in the case of the first bad example (someone please correct me if I say this the wrong way) your function is now *not tail recursive* so it cannot take advantage of tail call optimization. Plus you `reverse` it once it's done which is another list traversal. The second bad example is "better" because it's tail recursive but you're still building a list every single time with that `a` element in order to concatenate it (then traversing it when concatenating). The good version has a "good intuition" of functional style that is both tail recursive and cons'ing elements onto the front of the list which eliminates the hairy ball of issues you bring up when wanting to concatenate. Sorry for the length, and also sorry if you know most of this already; I had to learn this stuff the hard way and not many people write about the sillier noobier stuff. Most of it I learned from Erlang before Haskell.
I can understand not wanting Fundeps or type families as part of the standard, but MPTCs *alone* have been around so long it's incredibly surprising to me they aren't in there.
Well, I'm glad from that list we at least have JSON now :)
When most people say Haskell 98 they mean Haskell 98 + the two addenda to add FFI and hierarchical modules, but yes, Haskell2010 is a much nicer standard if you have to pick a standard. ;)
This is pretty much the reason looking at the Haskell' page: https://ghc.haskell.org/trac/haskell-prime/wiki/MultiParamTypeClasses There are also some other things I didn't think about, namely the implications of possibly needing to drag along other extensions too (like `FlexibleInstances`)
I think it's a popular opinion among people who develop Haskell professionally. I tend to stay away from features that are less than 10 years old, unless it's something trivial like TupleSections.
C and Assembly, lacking GC, parametric polymorphism and other features, are quite constrained in how high-level their abstractions can be. Haskell is powerful enough to have SQL or Prolog as EDSL's, whereas C cannot. It's difficult to say that SQL or Prolog are higher-level than Haskell, if you can be encoded into Haskell. Preemptive anti-confusion statement: Of course you can *implement* SQL or Prolog in C or Haskell, and that doesn't mean much -- but you can *embed* SQL or Prolog in Haskell, but not in C.
&gt; I don't think I agree that it's an example of actual non-termination It is in case of IO: module Forever where open import Coinduction open import Data.Unit open import IO forever : IO ⊤ → IO ⊤ forever m = ♯ m &gt;&gt; ♯ forever m main = run (forever (return tt))
We live in the age of sufficiently smart compilers. The times when the language standard was deliberately made small so a single programmer could create a compiler are long gone. Here's the excerpt from Wirth's book about designing pascal: &gt;WIRTH: In hindsight, true. You have to recall that I had not only to make language design decisions but also to implement them. Under these circumstances it is a question of whether to include more features, or to get ahead first with those needed to implement the compiler. In principle, you are right: Dynamic arrays should have been included. (Note: Recursion was excluded in a preliminary version only.) Today we cannot progress forward tying ourselves up to the limitations and capabilities of a single compiler developer, or even a small team of compiler implementors. Just like we cannot reach Mars working from a garage anymore. That means that modern languages for our age are so big and complex that most of the time there cannot be more than one implementation, simply because there's no other team big and capable enough to re-implement such huge and complex body of work. Even though there exists haskell standards like haskell-98 and haskell-2010, i think it is safe to assume that the only "haskell" we have today is GHC. 
I'd have no problem voting +1 for this on libraries.
Be aware, though, that libraries tend to vote against functions that are simple compositions like that. (Especially in prelude.) Personally, I would vote +1 on this addition. Edit: I actually have a patch for this lying around, but I never submitted it after my nand/nor and nany/nall patches were declined. I could easily be persuaded to submitting it. I just need someone who'd vote +1, so I don't get all minuses like the last patches I sent. :-P
Worth considering that “per se” in English (as it were, exactly) is used a bit differently than “per se” in Latin (by/in itself) and might warrant a respelling. Also, *pedantic, unless you were just trolling. ;)
&gt; that is not a reproducible build and not what the PVP is designed to accomplish. Regardless of what terms we use for it, the PVP was absolutely designed for what /u/hagda said. The fact that there are corner cases where it breaks down doesn't negate this. It's not an all-or-nothing proposition.
Huh. TIL. 
Nitpick: You should get into the habit of using `fmap` and `(&gt;&gt;=)` instead of `fromJust` or `let Just a = ...`. Even if you already know how to do this it will greatly benefit beginners reading your blog who might not realize that carrying around `Maybe`s is not hard in Haskell.
&gt; Your “final computation” returns () in the end This is actually a sign that you don't want a monad of any kind :)
Fyi, that specific implementation doesn't satisfy that the result will always be `&gt;= mn` and `&lt;= mx`. A proper definition would have the type-signature clamp :: (Ord a) =&gt; a -&gt; a -&gt; a -&gt; Maybe a
Mistyping, not trolling, so a trifle ironic in context :/
Woops, sorry, really out of date.
&gt; Constraints give you the openworld assumption. Not being able to prove C a does not imply that you can prove Not (C a) and this is a feature not a bug. You could also say that False, used as a constraint, means that you don't have the proof of ```C a``` _so far_ (it's already what GHC does when a constraint cannot be satisfied). But you're right, I expected there were _tons_ of implications I couldn't see.
Suggest it to libraries@haskell.org! I have that function in my util library and I use it all the time. I also very frequently use 'scale' and 'normalize', where 'normalize 0 10 5 == 0.5' and scale is the inverse. I wonder if there's a standard name and location for those? Also, while I'm on the subject, fmod is pretty hard to find in Data.Fixed.mod', maybe it should be moved to Numeric.
I'm a bit stuck of the Failure!/Success! section. 1. Is it just a property of Huffman codes that they can't encode a single letter alphabet without workarounds? The change to decodeAll doesn't doesn't address the issue that the string "aaaaa" can't be encoded. 2. You note that decodeAll is partial. That's the math class definition, meaning that every input only ever corresponds to a single output, correct? I don't know if you want to add something to clarify so it's not confused with "partial application" (or at least so I wouldn't have had to go look that up in Wikipedia). I'm a little disappointed to see that the actual encoding/decoding happens next time. When I was half way through that was the part I was really looking forward to.
http://hackage.haskell.org/package/ListZipper uses just left/right, so safeCursor :: Zipper a -&gt; Maybe a safeCursor (Zip _ rs) = listToMaybe rs -- Just . head or Nothing
Just another example of [Skitt’s Law](http://knowyourmeme.com/memes/skitts-law).
Also, expect an email on the pipes mailing list where I confess that I am a complete outsider to established pipes idioms inviting people to tear apart my code :) 
&gt; Now, `IncoherentInstances` will always pick an arbitrary matching instance, if multiple ones exist. Emergence driven programming, it's gonna be a thing!
wat? Care to elaborate?
Please don't, you can be fairly sure it will be rejected.
What's the reasoning behind this policy? I always feel mildly frustrated when I find that simple functions that I feel *should* exist don't -- such as clamp. (Not necessarily disagreeing; just curious.)
You're confused. Compiling or interpreting another Haskell program at runtime is a simple matter of a Haskell library call. Just like Lisp has "eval" in a library, Haskell has "eval" in libraries (see "hint"). 
One particular problem I noticed is that the base library has become a lot more GHC centric over the years. It used to be the case that all Haskell compilers could at least share the base library, but I don't know how much that is still true now. The base library is also rather bloated, IMHO. A one line main=print "hello world" will bring out a lot of code due to dependencies. Just to name a few: locale and encoding, extensible exceptions, typeable and MD5 finger-print etc. 
Really? I'd figure you wouldn't really need this. Is there something in particular you want to learn?
Could you expand on this? Is it Applicative-related? 
Nice and clear. Will there be a Monad instance for Set by default now?
Another ad for dependent types. These people are shameless :-)
While I'm comfortable using these concepts, at lot of that learning is adhoc. Maybe that's ok, but I'm a bit a classical learner and like to make sure I've done some rigorous studying around this stuff to make sure I haven't overlooked anything. Learning more about higher-rank polymorphism and kind polymorphism is very interesting to me. Infact, I've never used kind polymorphism, so it would be interesting to learn about that - maybe I've just overlooked applications. Likewise, type equality and constraint kinds I am aware of, but have never really found a use for them. The system FC stuff sounds like a nice bonus :)
Oh, I'm being mostly sarcastic, although the change to this language extension probably does make sense as the old behavior was even more odd. And I'm only mostly and not completely sarcastic because I imagine it could be fun to experiment with emergent behavior in computer programs through arbitrary instance resolution. For fun, probably not for profit.
Why not just put it in a useful library on Hackage? What value does it add to core? After all, you could create an entire library based on this sort of code. And with cabal it will be very easy to pull your library in.
I think the biggest problem with it from a libraries standardization standpoint is that the behavior when the bounds are not given in ascending order isn't canonical, so you need then to care about the implementation details. The result when the interval is ill-formed may be the upper bound or the lower bound (depending on the order of the composition you gave), an error if you want to rule out such behavior, or the original answer, since the interval is ill defined, depending on how the author chose to implement the function and their desired semantics -- and each of these has appropriate uses. Case in point: Here are three _different_ versions of this very function in a package of mine: http://hackage.haskell.org/package/intervals-0.7/docs/src/Numeric-Interval-Kaucher.html#clamp http://hackage.haskell.org/package/intervals-0.7/docs/Numeric-Interval-Internal.html#v:clamp http://hackage.haskell.org/package/intervals-0.7/docs/src/Numeric-Interval-NonEmpty-Internal.html#clamp The things we tend to like to standardize are precisely the things that don't explode into this many different possible implementations, that are big enough to be worth writing once and remembering the name for the composition, and canonical enough to only need writing once.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Robert Recorde**](https://en.wikipedia.org/wiki/Robert%20Recorde): [](#sfw) --- &gt; &gt;__Robert Recorde__ (ca. 1512–1558) was a [Welsh](https://en.wikipedia.org/wiki/Welsh_people) [physician](https://en.wikipedia.org/wiki/Physician) and [mathematician](https://en.wikipedia.org/wiki/Mathematician). He invented the ["equals" sign (=)](https://en.wikipedia.org/wiki/Equals_sign) and also introduced the pre-existing ["plus" sign (+)](https://en.wikipedia.org/wiki/Plus_and_minus_signs) to English speakers in 1557. &gt;==== &gt;[**Image**](https://i.imgur.com/LBXuiHo.jpg) [^(i)](https://commons.wikimedia.org/wiki/File:Robert_recorde.jpg) --- ^Interesting: [^The ^Whetstone ^of ^Witte](https://en.wikipedia.org/wiki/The_Whetstone_of_Witte) ^| [^Equals ^sign](https://en.wikipedia.org/wiki/Equals_sign) ^| [^The ^Ground ^of ^Arts](https://en.wikipedia.org/wiki/The_Ground_of_Arts) ^| [^Zenzizenzizenzic](https://en.wikipedia.org/wiki/Zenzizenzizenzic) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cgq9oqw) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cgq9oqw)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I'm not run in a rush. I integrate knowledge comparatively. 'Tis partly why I like category theory.
 System.IO.Unsafe debugit :: Show a =&gt; a -&gt; a debugit x = unsafePerformIO (print x &gt;&gt; return x) Type safety is for production code, not debugging code :P Edit: Aaand of course there is a library function that does it safely that I didn't know about.
Can you clarify your references for this definition of purity? I dug into Filinski's thesis and found him defining strict and total in these ways, but purity hasn't entered the scene yet. I'm getting progressively more confident with the paper so perhaps I will find this idea in time, but I wondered if there were other companion references to read as well. Thanks!
What about `clamp :: (Ord a) =&gt; a -&gt; a -&gt; Maybe (a -&gt; a)`?
yes to all of this
A Writer w result is a Monad if w is a Monoid. If you have a *Writer w result* but you always return (), you're never using the result type, that's like having a Foo w. Which is just a wrapper over a Monoid. You can rewrite your functions using only Monoid methods. (Maybe singpolyma's point is more general though)
It's more correct to call the latest standard "Haskell 2010". Some things got removed (notably library modules), and there were lots of edits to the report relative to Haskell 98.
This can be easily done using STM and its `retry` (or `check`) action.
for those wondering if they read it before. you might be right. it's from 2013.
Of course.
Considering that Applicative is going to be superclass of Monad only now, I wouldn't count on during this decade. 
that might be even better, yes
Made some changes addressing these things explicitly. Thank you for taking the time to read through and note how I can improve :) I really appreciate it! And I apologize for cutting the article off too soon. I guess that means you'll just have to wait until next time :) The main motivation was that I wanted to devote an entire post on the `pipes` library alone.
Please don't create yet another spin-off of [`MissingH`](http://hackage.haskell.org/package/MissingH), as there are plenty of such mini packages on Hackage already providing a few functions, but ultimately bitrotting as the original author loses interest after some time.
Defenitly drop by at one of the CS study associations and say hi! We've got a few haskell enthousiasts here :-) P.s. i always get the feeling a lot of Utrecht students are lurking here. 
Side-channel attacks are irrelevant in the case of Heartbleed, because the bug was not in one of the crypto primitives, but in the infrastructure around them. Worrying about side-channel attacks for primitives is just silly when your application code happily leaks your secrets anyway. If you can't even get a ping right, then you will never be able to defend against fancy stuff like side channel attacks anyway.
The impact of heartbleed is crippling, but a good timing attack will leak you secret key, which is also pretty bad.
There could be commercial compilers though, some people can still afford a big enough team.
that's just nonsense. a successful timing attack gives you information disclosure about the keys. that's not as much as heartbleed, but it's very close: you'll have to record connections as well. openssl has no stellar record and there is much to say about it, but it tries to prevent timing attacks. a naively written haskell implementation will be wide open to _easy_ side-channel attacks that cannot be fixed easily. there is `securemem`, but that will only eliminate the simple comparisons. i am very eager to get to know if (and what) `tls` does to eliminate side-channel attacks. i am pretty sure that the (practical with attacker on same host machine in case of virtualization) branching-attacks can't be eliminated with haskell w/o ffi. i hope i am wrong though. btw, that's one of the things why `cryptol` by gallois is interesting. one spec will generate a haskell implementation as well as a c (and fpga) implementation. i am sure that _can_ be done sensibly without introducing side-channels. i have absolutely no idea whether that is the case though. **edit:** oh, i see. vincent answered on the [hackernews discussion](https://news.ycombinator.com/item?id=7559981)
Pointing a gun to the head of the server administrator is also a very good way to get secret keys. The logical conclusion is that we should only use programming languages whose type systems includes an AK-57. Again, if you can't defend against Heartbleed, then you don't *deserve* to be protected against timing attacks.
Glad to help. The article's length was fine, and it was a good point to stop.
&gt; tls does not use SecureMem. it does, indirectly via `cipher-aes`, `crypto-cipher-types`, etc. i have not auditioned the code to see whether that's sufficient though.
On the other hand, perhaps something like ocaml would be superior to both haskell and c for implementing tls.
To answer the question succinctly: **YES YES YES YES**
I really don't understand what you're trying to say here. What is being said is simple: Haskell does fix one problem. Haskell does introduce another. It is not trivial to see that this tradeoff is worth it. That heartbleed does not deal with sidechannels is irrelevant - what is being said is that Haskell implementations would suffer from this type of bug. Sidechannels are also much more easily attacked than your exaggeration of a gun to someone's head. And I totally miss what your point is about who does and who doesn't "deserve" to be protected against something, sorry.
&gt; If I want to implement a histogram: map (head &amp;&amp;&amp; length) . group . sort is quite more usable than C (or Smalltalk, by the way!). It is just a matter of boilerplate. Your code is a proof. I could write this in APL and would be 3-5x shorter, but this does not change the fact that we are still at a very low level. You are not able to change 'group' to 'newGroup' at run-time without explicitly coding it. 
Apparently there are still issues leftover. Notably the fact that you can't partially apply type synonyms prevents you from doing: type family Foldr (f :: a -&gt; b -&gt; b) (z :: b) (xs :: [a]) :: b where Foldr _f z '[] = z Foldr f z (x ': xs) = f x (Foldr f z xs) let x = Proxy :: Proxy (Foldr (+) 0 '[1,2,3]) Because + cannot be partially applied.
Oh, I see. Thanks for the elaboration. I only read the type signature, not the implementation! My patch, IIRC, was clamp x mn mx = max mn . min mx $ x -- or something to that effect.
It's not just about triviality: library functions should be reasonably universal and free of arbitrary choices, too. Something like `clamp` seems simple at first glance, but as soon as you start thinking about how the edge cases should be handled, it turns into half a dozen variations, each arguably useful. It doesn't make sense to arbitrarily pick one, or add all of them. By contrast, functions like `minimum`, `maximum`, and `comparing` don't admit much variation: their universality makes them good candidates for the library.
I see! I really like that definition of purity as the categorical backing makes it really clear. Effectively if your category models effects then `trivial . f` cannot be `trivial` unless `f` does nothing but return a value. I feel like people are often confused by the difference between RT and purity and even the definition (myself mostly included), so I've been trying to track down some firm references. Do you know of papers or books which further explore this idea?
Why?
Like others have mentioned, you want to learn about `STM`, and there is an extensive [STM tutorial on School of Haskell](https://www.fpcomplete.com/school/advanced-haskell/beautiful-concurrency). I just wanted to mention that my `pipes-concurrency` library is an even higher level abstraction built on top of that that you can use to avoid concurrency deadlocks. For example, here is a program that has three writers to and readers from a shared variable: import Control.Concurrent.Async import Control.Monad (forM) import Pipes import Pipes.Concurrent import qualified Pipes.Prelude as Pipes debugRead :: Int -&gt; Pipe Int Int IO r debugRead i = Pipes.chain $ \n -&gt; putStrLn $ "Read" ++ show i ++ " reads " ++ show n debugWrite :: Int -&gt; Pipe Int Int IO r debugWrite i = Pipes.chain $ \n -&gt; putStrLn $ "Write" ++ show i ++ " writes " ++ show n main = do (output, input) &lt;- spawn Single as &lt;- forM [1..3] $ \i -&gt; async $ runEffect $ each [0..100] &gt;-&gt; debugWrite i &gt;-&gt; toOutput output bs &lt;- forM [1..3] $ \i -&gt; async $ runEffect $ fromInput input &gt;-&gt; debugRead i &gt;-&gt; Pipes.drain mapM_ wait (as ++ bs) `pipes-concurrency` detects when readers or writers are about to deadlock and gracefully shuts them down without raising an exception. You can learn more about how to use the library by reading the [`pipes-concurrency` tutorial](http://hackage.haskell.org/package/pipes-concurrency-2.0.2/docs/Pipes-Concurrent-Tutorial.html).
Well when the OpenSSL team decided to ignore proven infrastructure for performance gains, I don't really think that comes down to a language choice. This event has really just demonstrated how many people know so little about C and C++, or programming in general. It's been a huge, disgusting language circlejerk. Knowing language X doesn't fix stupid, and this thread proves it.
most notably, conduit for https..
I have no idea if anyone else has looked at it, sorry.
Is it possible to reduce timing attacks by making *all* of the SSL code strict? 
I am seeing unconfirmed reports of segfaults with hlint and ghc 7.8: https://github.com/ndmitchell/hlint/issues/39
I've been trying to get a better and better handle on this for some time now. In particular, I'd like to understand whether or not it's valid to say something like "purity is just a choice of which effects you care about". That idea seems fairly reasonable from a definitional point of view and suggests that Haskell is impure in exactly the mechanics of CBN going on behind the scenes. But there's some stickiness as I'm not sure what exactly the choices are which impute this concept. My best stab at it so far is that we lose purity by ignoring reduction effects in our category, we lose RT by not considering the differences between "degree of evaluation" as a difference of reference.
While I agree with you, your example has me a little confused. It looks to me that you're referring to Numeric.Interval.NonEmpty.Internal.clamp twice. And the difference between Numeric.Interval.NonEmpty.Internal.clamp and Numeric.Interval.Kaucher.clamp is only a matter of whitespace. Edit: I take that back. They work on different Interval data types with the same constructor.
Will the homebrew formula be updated?
Thanks to the GHC devs for fast response!
Thanks for the report. I commented on the ticket.
If someone maintains it, sure. I never really maintained the ghc formula in the past when I did use Homebrew anyway - but if someone would step up as a dedicated and responsive maintainer, that would be nice (and we could mention it on the download page or something).
unless someone in the core community puts the time into babysitting the brew formula on a regular basis, i strongly advise against using brew to install ghc (and please never use their haskell platform formula... its deeply wrong)
Changing things at runtime has nothing to do with high level vs low level and changing it in Haskell too is possible even if current implementations don't make it very nice. Your idea of high vs low level, and what similarity to c is, is very confused.
I meant to link to Numeric.Interval.Internal.clamp in the second one. (fixed). 
What's the recommended way to install Haskell on OS X then?
- Download the GHC binary - Run cabal's `bootstrap.sh` - `cabal install lens`
I'm looking at the subject from a practical point of view. For me high level means flexible and generic. For you high level means static and complicated. If something is complicated does not mean it is high level. In some areas PHP is higher level than Haskell. The rest is hype. Haskell is on a hype wave now. It is neither good or flexible, just popular in some circles. Thats all. For me Haskell compared to C does not bring anything special to the table. Actually what is it good at? Do you want to know when I will call Haskell high level enough? When this language gets decent GUI/media libraries built in. When printStrLn will spit out formatted HTML in a nice window instead of useless binary text at terminal which nobody cares about except some old neck-beards. Programming language is not only a bureaucratic formalism (syntax, boilerplate) this is the leas interesting part of it. The most important thing in a language is its flexibility, because your tools depend on it. No flexibility = no tools. Clojure came in 2007 and look at its tools. They are brilliant and some of them are mind blowing. What Haskell has got? I could replace Haskell with C in this answer and it would not change its meaning. This won't work with Smalltalk. Smalltalk as a pure OO language is still tenths of years ahead of Haskell in terms of ease of use and expressiveness. For God's sake, Smalltalk has a GUI built in since 70's! Haskell has a putStrLn... 
&gt; I really don't understand what you're trying to say here. What is being said is simple: What I'm trying to say is that the Heartbleed bug and side-channel attacks are two distinct classes of vulnerabilities, and one class is much worse than the other. Consider the source code of a server that implements TLS. Let's call the subsystem that deals with cryptographic primitives "A", the subsystem that stores secrets "B" and the subsystem that implements the network protocols "C". 1. Heartbleed is a situation where a vulnerability in system C makes system B insecure. This is terrible, because you cannot prove the correctness of the whole system by looking at part B alone. Security of the system is no longer a compositional property. 2. Side-channel attacks are an attack against system A. If you implement system A in a way that mitigates this attack, then the whole system will be secure -- it's compositional. It is true that a staightforward Haskell implementation of system A might be vulnerable to side channel attacks. We definitely can't *prove* that it's invulnerable, so we have to consider it vulnerable, indeed. But thanks to compositionality, we can actually isolate this part and implement it differently, for instance by designing a small DSL that outputs assembly while statically guaranteeing that execution time does not depend on a secret. Or use C if you must, but of course, this language gives little static tools to protect against problems of type 1. I don't see you can ever prove/be confident that a system is secure if you have vulnerabilities that are not compositional, i.e. are of the first type. In contrast, we can be confident that the whole system is secure against side channel attacks by showing that subsystem A mitigates this attack. 
Doesn't the GHC binary require XCode?
&gt; `cabal install lens` While this works, it's also silly. Why isn't there a `platform` package on hackage?
&gt; For you high level means static and complicated Heh, no. It means that the program embeds as much information as possible about the *intent* and *meaning*, and as little as possible about implementation details. &gt; In some areas PHP is higher level than Haskell If you define "high level" as "dynamically typed", sure. But that's a silly definition. &gt; For me Haskell compared to C does not bring anything special to the table. Programs that are much smaller, easier to write, understand, maintain, and have far stronger safety guarantees? &gt; Actually what is it good at? Anything that doesn't require the kind of performance C or C++ allow? Which is virtually all applications. Having assurances from the type system that require far less testing and far safer/easier changing/refactoring down the road. &gt; When this language gets decent GUI/media libraries built in. Like Gtk, OpenGL, OpenAL, etc, which Haskell has? Again, your definition of "high level" makes no sense. This is not what "high level" means in any other context. &gt; When printStrLn will spit out formatted HTML in a nice window instead of useless binary text at terminal which nobody cares about except some old neck-beards. So you don't like command line REPL's. Have you seen [IHaskell](http://gibiansky.github.io/IHaskell/)? &gt; For God's sake, Smalltalk has a GUI built in since 70's! Haskell has a putStrLn Again, it seems like you're trolling, but [here you go](http://hackage.haskell.org/packages/search?terms=graphics).
You also miss a few, eg, graphics libraries from the platform
&gt;Programs that are much smaller, easier to write, understand, maintain, and have far stronger safety guarantees? If i want safety guarantees I write tests. If I want small, easy to write and understand program i write it in [Rebol/Red](http://www.red-lang.org/p/contributions.html). Compare this to Haskell : http://www.rebol.com/oneliners.html - this is what I call high level. &gt;Like Gtk, OpenGL, OpenAL, etc, which Haskell has? Do not make me laugh. How is this different from C? Example code from SDL2 : createWindow :: CString -&gt; CInt -&gt; CInt -&gt; CInt -&gt; CInt -&gt; Word32 -&gt; IO Window Wtf is this? Where is the width, height, starting point, what the hell is this Word32? The one who is trolling here is clearly you. Everyone and their cat clearly sees that Haskell is no different than C. 
One of the many libraries having a crappy API function proves something about the Haskell language? Yes, that API sucks. An imperative, low-level function like "createWindow" is not going to be *very* different in Haskell or C. There is of course the difference that C requires documenting and carefully tracking the memory allocations involved, which is safely ignored here, even in this crappy API. The nice high-level stuff Haskell adds here is for example [FRP GUI libraries](http://www.haskell.org/haskellwiki/Reactive-banana).
the point is, that a segfaulted program cannot do anything harmful. if something like that (buffer overflow) happens, it's better to terminate.
then what is the point of the platform?
Your link to FRP shows examples built using GPL-ed libraries. GPL is a show-stopper here. The rest is wheel reinvention and full of boilerplate just like ... you guessed it: C. Also I find Component Entity System pattern easier to use and faster than FRP. Simpler is better. Why Haskell cannot have its own GUI DSL? Is it so hard? There are tons of BSD/MIT licensed libs (poor SDL2...). Why anyone would write tons of boilerplate for WX using this FRP instead spending his time for a SDL GUI?
Though the graphics libraries don't build under clang, yet, as it is (though I think they'll build with gcc &gt;= 4.8).
See also: https://github.com/idris-hackers/idris-crypto
It is not safer than the function you showed. Behind the scenes it is also implemented with unsafePerformIO.
Imagine getting started with Haskell on Windows without the HP. The Haskell Platform is great for getting people off the ground, but probably not what you want once you are an established Haskeller.
that first point is somewhat invalid as most package makers don't care if their packages work on windows or not. but i see what you mean to a certain extent, as i guess software designers want tighter control over their packages. it's not clear to me what all the issues are, but things should be made clean and easy whether you're a beginner or not. if the haskell platform is doing things that make certain things harder, it seems it should be fixed rather than avoided.
I disagree. I try to stick to developing against the Haskell Platform because it increases the likelihood that my packages will build for other people. Scrapping the Haskell Platform only makes sense if you have no intention of sharing your libraries with others.
My understanding is that mitls is too slow to be accepted for high performance use. IMO, still worth it.
The point is that web frameworks have been using openssl for TLS, and that ruined them.
As far as I know, this is what the tls package tries to do. There's a lot of clueless hand waving about lazy evaluation in the thread on HN, which is a good first- order filter for "I have heard about Haskell but don't know what I am talking about".
Handling TLS via a proxy is a good approach, as the consequences of a successful attack if the TLS stack and the app share an address space are dramatically worse.
Possibly. I am having trouble getting that to work on my system. (Actually I cannot get aplay working in interactive mode, for whatever reason.) But if you have such on your system you could change the `hPutChar` to just `putChar` and pipe the output to `/dev/snd`.
I don't actually install `lens` outside of a sandbox, but it's a pretty good bootstrap to tug on if you just want to get hacking. I try to keep things pretty bare outside of sandboxes to minimize (painful) surprises down the line.
I'm new to Haskell. What is the point of installing lens here?
It's by far the most popular, and there is a fair bit of hackage that won't compile in other implementation. However, as a language, Haskell isn't just defined as "whatever GHC does", ala Perl or Ruby. Instead the the Haskell report serves to define the langauge. GHC *mostly* follows that specification and then has a *bunch* of extensions that you can opt-in to if you want. The Num/Eq split is a good example of divergence, and the Functor/Applicative/Monad join will be another, but I'd expect those divergences to go away in the next report. When I think of alternative Haskell implementations I always think of JHC, because the Metasepi project was using it and I thought that project was pretty neat. IIRC, Metasepi has moved toward ATS instead of Haskell, but JHC might be able to get your Haskell code on some embedded platform that GHC doesn't target yet. If you've got a C (cross) compiler for it, JHC should be able to give you some C99 to compile. Others has mentioned UHC and it was also brought up that the HIW last year around ICFP, so I'm sure it's still active, but I don't think it supports all the the latest Haskell report, yet.
In my mind Haskell2010 code is better than GHC-specific code, but there's some really *nice* extensions, so I end up using them willy-nilly as I'm not (currently) concerned with any platform that is not GHC-supported. Short answer to both of your questions: No. That said, if someone asks about GADTs or Type Families "in Haskell", I go with the flow and mentally substitute "in GHC" with "in Haskell". If they ask how to do something "in Haskell" that's only possible / easy through a GHC extension, I try to make it clear what techinques are only applicable to GHC vs. which as generally availble.
ISTR this being mentioned at the HIW near ICFP last year. The maintainers of base and most of the packages in the platform seemed open to supporting other compilers, but they've been getting too few patches / bug reports about incompatibility and too little help from compiler maintainers other than the GHC team. There's already plenty of existing #ifdef code to originally provide supportto both Hugs and GHC, I imagine much the same could and would be done for LHC, UHC, JHC, or nHC (ninegua's haskell compiler; you are writing one, right? :P).
While a proxy would be safer due to being simpler, this kind of attack is technically more powerful against a proxy machine. It only deals with proxying so will have a higher density of interesting data available.
`lens` is, loosely speaking, a batteries-included library for data structure access and manipulation. Beyond being useful in itself, it happens to pull most of the platform packages as depencies.
[Broadcast TChan](http://hackage.haskell.org/package/stm-2.4.3/docs/Control-Concurrent-STM-TChan.html#v:newBroadcastTChan)
My favorite maze generation algorithm was based on union-find. Initially, you have a set of independent cells, each considered a maze in its own right. For each possible edge in the maze/graph, in random order, check whether the to- and from- cells are in different trees (using union-find, determine if the cells are in different equivalence classes). If they are, join the two trees via that edge and union the two cell-sets in the union-find structure. It's probably equivalent to [Kruskal's algorithm](http://en.wikipedia.org/wiki/Kruskal%27s_algorithm), but without the (explicit) edge weights. Instead of choosing edges in order of randomly-assigned weight, you choose them in random order. This approach is awkward in purely functional code because the standard algorithm for an optimal union-find relies on mutation of pointers. It's certainly possible to implement the standard union-find in Haskell using `IORef` or similar, but it's also possible to have a pure functional union-find. Here's [one interesting link](http://jng.imagine27.com/index.php/2012-08-22-144618_purely-functional-data-structures-algorithms-union-find-haskell.html). 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Kruskal's algorithm**](https://en.wikipedia.org/wiki/Kruskal%27s%20algorithm): [](#sfw) --- &gt; &gt;__Kruskal's algorithm__ is a [greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm) in [graph theory](https://en.wikipedia.org/wiki/Graph_theory) that finds a [minimum spanning tree](https://en.wikipedia.org/wiki/Minimum_spanning_tree) for a [connected](https://en.wikipedia.org/wiki/Connectivity_(graph_theory\)) [weighted graph](https://en.wikipedia.org/wiki/Glossary_of_graph_theory#Weighted_graphs_and_networks). This means it finds a subset of the [edges](https://en.wikipedia.org/wiki/Edge_(graph_theory\)) that forms a tree that includes every [vertex](https://en.wikipedia.org/wiki/Vertex_(graph_theory\)), where the total weight of all the edges in the tree is minimized. If the graph is not connected, then it finds a *minimum spanning forest* (a minimum spanning tree for each [connected component](https://en.wikipedia.org/wiki/Connected_component_(graph_theory\))). &gt;This algorithm first appeared in *[Proceedings of the American Mathematical Society](https://en.wikipedia.org/wiki/Proceedings_of_the_American_Mathematical_Society)*, pp. 48–50 in 1956, and was written by [Joseph Kruskal](https://en.wikipedia.org/wiki/Joseph_Kruskal). &gt;Other algorithms for this problem include [Prim's algorithm](https://en.wikipedia.org/wiki/Prim%27s_algorithm), [Reverse-Delete algorithm](https://en.wikipedia.org/wiki/Reverse-Delete_algorithm), and [Borůvka's algorithm](https://en.wikipedia.org/wiki/Bor%C5%AFvka%27s_algorithm). &gt;==== &gt;[**Image**](https://i.imgur.com/WoRdj9m.gif) [^(i)](https://commons.wikimedia.org/wiki/File:MST_kruskal_en.gif) - *Visualization of Kruskal's algorithm* --- ^Interesting: [^Prim's ^algorithm](https://en.wikipedia.org/wiki/Prim%27s_algorithm) ^| [^Minimum ^spanning ^tree](https://en.wikipedia.org/wiki/Minimum_spanning_tree) ^| [^Joseph ^Kruskal](https://en.wikipedia.org/wiki/Joseph_Kruskal) ^| [^Borůvka's ^algorithm](https://en.wikipedia.org/wiki/Bor%C5%AFvka%27s_algorithm) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cgr5um6) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cgr5um6)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I wonder if there are any types of attacks such an abstraction could introduce, which could mean you'd be vulnerable with *either* library? But presumably that's unlikely, if it just redirects calls in pure Haskell to the other libraries. I like the idea in theory. One cool use case is swapping out OpenSSL for tls until the latest vulnerability is patched, under the presumption that everyone will be trying to attack the latest OpenSSL vulnerability but no one knows the vulnerabilities of tls. It's not *quite* security through obscurity, I don't think. ;-)
Ok, thanks for the clarification. Btw, when you say "observing a secret by timing" does this refer to an attacker observing unix processes (by getting acess to the server) or outside using just the network? 
Events and behaviours are mathematically dual to each other, as seen by how they *behave* when pairing (sum vs product). Which is pretty funny because the implementation is usually the same :)
Both. Having access to the server makes it easier, of course, but apparently, it's also [possible from the network](http://en.wikipedia.org/wiki/Timing_attack#Examples).
The third root is equivalent to a power of 1/3 for the real numbers. Does that help?
well it kinda doesent work i think...i want to have the radius from a Sphere by giving the volume. formel from my point of view: (v^1/3 [= 3 sqrt v ??])/(4/3*PI)
 V = 4/3 * pi * r³ divide both sides by `4/3 * pi` to get 3V/4/pi = r³ and then take the third root of both sides to get r = (3V/4/pi)^(1/3) I think you take the root too early. You need to divide first!
This seems very interesting, but how comes that the linked papers are unreacheable and there has been no change since 2012?
Interesting, thanks. I'd say that Wadler et al's implementation both more closely models LINQ syntax and has less syntactic Haskell noise (although still work to be done in the latter to strip down to [hoped for] M$ LINQ style concision). Forgot [the paper](http://homepages.inf.ed.ac.uk/slindley/papers/practical-theory-of-linq.pdf), easier to view example queries than waiting for Waddler to get to the point(s) in the video presentation ;-) 
 (.&gt;.) = flip (.&gt;.) What is this I don't even.
If you believe that Haskell is "far apart" from C, you haven't seen the implementation of Haskell arrays or Haskell IO. You also should read this brilliant post by Conal Elliott: http://conal.net/blog/posts/the-c-language-is-purely-functional
This seems like the obvious choice if you're going to use Yesod. Given that Esqueleto adds join capabilities to persistent, is there something else missing from it?
There's nothing yesod-specific in either library. They are general purpose abstraction libraries. Another choice is [acid-state](http://hackage.haskell.org/package/acid-state). What do you find missing from these, and what do you find less elegant?
Great question. I've been hoping for the same thing.
Esqueleto isn't anywhere near typesafe.
Use the [Haskell Platform](http://www.haskell.org/platform/) installer. It won't give you this new version of GHC yet - that will be in the next version of HP, in a month or two. But it's a regular Mac OS X installer, very well thought out to integrate well with both the Mac OS X and Haskell ecosystems, and quite stable and usable.
No, just xcode command line tools. Those can be downloaded separately if you don't have XCode installed. They are free, though it does require signing up for an Apple ID and for [Apple Developer](https://developer.apple.com).
Good catch! Serves me right for not checking the types.
Agreed, except I wouldn't tie it to time. Some useful extensions become well-known, widely used, and known not to have any serious issues. As an example - if your new type sections proposal gets implemented, I have a feeling I'd be using it in real code with a short time.
In my mind the biggest benefit is that it's an easy way to bootstrap a cabal-install binary. Also, we test against several past platforms in our build bot, so in that sense it provides a nice way at getting to historical sets of consistent packages, which would otherwise be hard to do without it. But for actual development I avoid it because sooner or later you're going to want to upgrade a package that came in the platform. Then you'll probably end up having two versions of that package installed on your system which in my experience very quickly leads to build problems.
Yes. More specifically, because it's taking a long time to develop any consensus about which of those two companion extensions to use. Recently, I think a consensus is starting to converge - that we need both. Type families are simpler and usually best when they work out well, and fundeps are more powerful but harder to reason about.
&gt; Thanks for directing me to the missing linq. :)
What do you mean, tomejaguar? Esqueleto will prevent you from querying columns from one table against another table, for example, nor can a query for one table be used to return a datatype associated with another table.
Here's one issue I found that leads to runtime invalid SQL https://github.com/meteficha/esqueleto/issues/41 and here's another which is arguably not a violation of typesafety, but is certainly very odd semantics in my opinion https://github.com/meteficha/esqueleto/issues/40
Well, for starters [the documentation](https://github.com/meteficha/esqueleto) is a bit like the sound of one hand clapping, so it's hard to say at first glance ;-) But if we [dig through the tests](https://github.com/meteficha/esqueleto/blob/master/test/Test.hs) then some examples can be found. Here's a basic join between 3 tables: select $ from $ \(follower, follows, followed) -&gt; do where_ $ follower ^. PersonId ==. follows ^. FollowFollower &amp;&amp;. followed ^. PersonId ==. follows ^. FollowFollowed orderBy [ asc (follower ^. PersonName) , asc (followed ^. PersonName) ] return (follower, follows, followed) which is type safe but fairly busy syntax-wise, particularly the orderBy bit, reminds me of Scala's flagship SQL DSL (Slick), and the hideous hoops one has to jump through with groupBy/orderBy and tuple _._3, _._god-knows-which-database-column-this-number-refers-to madness o_O I guess I'm just assuming given Haskell's DSL power that LINQ is not only possible, but possible char for char in terms of concision (or very close to) 
It's not the obvious choice because it's yesod-specific, but because some of the yesod ecosystem is persistent+esqueleto-specific. 
&gt;when can one expect a production ready LINQ in Haskell land? When 800 pound financial gorillas (MS, IBM, Oracle, Google) become interested in haskell. Seriously though, it is obvious that whatever library appears in whatever language it is only because people have a vested interest in developing those libraries. Unfortunately haskell is not used nearly enough in the field of interfacing with commercial databases (Oracle, MS Sql, DB2) to warrant anyone spending their money on developing complex libraries. I'm working with MS Sql server from haskell app servers on linux. And the state of hdbc libraries is abysmal. Neither persistent no esqueleto support working with Ms Sql server. And hdbc-odbc driver is buggy, not even being able to handle memo fields (text fields larger than 4096 bytes.) I myself do not have expertise to fix it. So i have to resort to ugly hacks. They work for my case. But of course cannot be offered to general public as a solution. So to answer your question. LINQ analog will appear in haskell when number of haskell developers who work with commercial databases (NOT Postgres and NOT MySql) will be comparable to that of java or dotnet developers. 
What's wrong with the `orderBy` bit? I'm not a persistent fan, but the `orderBy` looks to be about as close as you can get to SQL syntax. What does it look like in ScalaQuery?
&gt; I'm just assuming given Haskell's DSL power that LINQ is not only possible Given that LINQ is not typesafe nor first class this seems unlikely.
I don't know how "order by [ (ascending) this column, (ascending) that other column ]" could be more clear or less ambiguous. The only objection to anything in that clause that makes any sort of sense is that the field names get messy and redundant (especially when you have a long table name, though that doesn't really show up here).
Basically it ended with George getting a Job outside of university. After that I think most of that research groups' focus was non-Haskell related. I think I tried contacting George about those paper links, but he doesn't seem to have responded… *Edit:* Just sent a mail to George and Prof. Grust. Let's hope they respond this time.
Another LINQ player in the Haskell market, good to see ;-) Being able to compose query snippets is an incredibly powerful feature. In my current type safe DSL of choice, ScalaQuery, you can compose like so: val userBase = for{ ur &lt;- UserRole u &lt;- ur.user // fk join, same as, `User if ur.userId is u.id` r &lt;- ur.role } val forLogin = for{ email~pass &lt;- Params[String, String] (u,ur,r) &lt;- userBase if u.email =~ email &amp; u.password =~ pass } yield (ur,u,r) val forStatus = for{ userID~active &lt;- Params[Int,Boolean] (u,ur,r) &lt;- userBase if ur.userID =~ userID &amp; ur.active =~ active } yield (ur,r) SQL DSL Nirvana for me would combine the ability to compose snippets _with_ parameterization -- i.e. being able to build up queries where parameter values are passed in at any point in the composition chain. In ScalaQuery, Slick, and every other composable SQL DSL I've run across, you have to delay parameterization until the final composed query. Anyway, hope that LtU (LINQ the Ultimate ;-)) arises in Haskell soon, treading water on the Scala side of the fence, does the trick, but looking for a spark.
That syntax seems to correspond pretty closely to the arrow syntax that my library (Opaleye offers). Can you say more precisely what you mean about "being able to build up queries where parameter values are passed in at any point in the composition chain". From that description it seems easy to do it in Opaleye, but I'd like to be sure I understand exactly what you mean before making such a bold claim!
If you want to see the type safety holes that I found you can see them in the sibling thread! http://www.reddit.com/r/haskell/comments/22x2zy/haskell_wheres_the_linq/cgrbekl
Hi there, apologies for the broken links. The hosting website has been remodelled a few days ago. We'll update DSH's hackage as soon as possible. Until then, here are the corrected links: - http://db.inf.uni-tuebingen.de/staticfiles/publications/ddfp2013.pdf - http://db.inf.uni-tuebingen.de/staticfiles/publications/haskell2011.pdf - http://db.inf.uni-tuebingen.de/staticfiles/publications/ferryhaskell.pdf Work on DSH has *not* ended, by the way. In fact, its innards have been completely rewritten and are more stable now. A hackage release is planned already. 
&gt; In summary it's like HaskellDB without the dependency on an adhoc record system, and with a lot of the bugs designed out. &gt; It's not publically available yet Why do you do this to us :( Now I will have to endure a pain similar to waiting for a GoT episode 