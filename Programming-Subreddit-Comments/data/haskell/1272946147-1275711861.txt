&gt; The patterns in GoF are solutions to old problems in a context (some old, some still relevant). I have met some people that think that "design patterns" is a general framework for talking about software development problems. I do not agree with them. The "context" you mention of GoF is too dependent on OO. I don't see this kind of posts as a "check list for language features" but more as a critic to what "design patterns" really are.
Iteratees ain't iterators. Enumerators are more like iterators, but the latter aren't (usually) composable. And to top that, the whole iteratee pattern is actually just an elaborate left fold.
When you say it is an "elaborate left fold", do you mean there is a simpler/better alternative? 
Visitor combines both the Pattern-matching (an ADT implemented as a subclass hierarchy) and the folding (inversion of control). It is indeed not elegant that Visitor does both, and I find it is a testament to the uncomposability of OO notions.
Well, you can just use foldl/concatMap (or even plain old open recursion) and lazy bytestrings provide chunking, but if you need to interface with a strict Monad, Iteratees are the way to go. Their CPS capabilities come in handy, too. Generally speaking, if you have foldl f x0 xs , then f is the iteratee and foldl, x0 and xs are the enumerator. Oleg's code gives more expressibility to f by CPS'ing it, allowing it to drive further iteratees, itself, so that the type of an enumerator is `Monad m =&gt; Iteratee a m b -&gt; m (Iteratee a m b)`, which makes `&gt;=&gt;` enumerator composition (concatenation). It's scary how easy it is to e.g. parse the concatenation of two files with iteratees, the difference to parsing a single file is a couple of keystrokes, the boundary of the files being transparent to the iteratee (The only thing that's not likely to happen is that the iteratee will receive a chunk that spans two files, but even that is possible if you re-chunk your data for whatever reason) What's still missing from Iteratees is a proper Alternative instance (there are parsers but they all either lookahead or backtrack, which you usually don't want to do when using Iteratees (e.g. because you don't want to hold on to data you read over a socket, that is, have a sensible guarantee that the fold uses O(1) space)), but I'm going to get around finishing that, some time or the other.
The proposal was to add `swap (a,b) = (b,a)` to the tuple library, but because we couldn't decide if the default swap function should be lazy or strict the proposal died.
Or did it die because of lack of interest? Personally I've never had use for a function like that, probably because the first and second item in a tuple are usually semantically distinct (even if not different actual types). A tuple is usually an impromptu record, not an impromptu list. 
Lack of will, by the looks of it.
People seemed to find the idea of swap = uncurry $ flip (,) hilarious in that thread. Why is it so obviously a bad way to do it? I don't get it.
I don't think this is right. If Haskell were a strict language there would be no issue and it would have been accepted right away because there would have been no minor issues. So I don't think it is a lack of will, it is simply a matter that it impossible to build a consensus on *all* minor issues.
That's to be expected. The headline doesn't suggest that.
Please explain. I don't see how my headline is inaccurate.
"Impossible to build consensus around trivial patch with two equally valid implementations". Your headline points to systemic failure -- is that what you're suggesting?
Because it takes so much longer to read than the straightforward equivalent `swap ~(a,b) = (b,a)`.
That is what I'm suggesting: The system of requiring consensus to accept library patches is broken. I propose having some active maintainer of these libraries actively resolve these bikeshedding issues by making a decree in order move forward. Or perhaps a system where proposals can move forward if there are no objections to a proposal (I hope no one would object to either solution to this swap definition), though I worry that this would involve politics too heavily. Still, perhaps someone can still convince me that current is still the best system.
Gratuitous SHE-ism: (|snd, fst|) for lazy swap. Maybe I'm naive, but what would go wrong if patterns (a,b) were always irrefutable, and translated in terms of projection? Wouldn't that buy us a law that x == (fst x, snd x)?
Is that SHE's notation for `(&amp;&amp;&amp;)` ?
It's not clear to me that this is a bad thing. Proposals for little utility functions in the core libs *should* have a high utility threshold to get over.
It's got a slightly high cognitive load. But there are other simpler alternatives without making up a new name: blah swap blah blah (\\(a,b)-&gt;(b,a)) blah blah (\\~(a,b)-&gt;(b,a)) blah Only, slightly longer, says what it does and tells you if it's strict or lazy. [Why does reddit's markup add a big space after a '\' character?]
Clearly the right thing is a template Haskell expression that covers tuples of any arity, mapping each slot to its successor, mod n. 
I don't think this proposal is a good example for illustrating your point. This was a trivial proposal that added little. It was not obviously the case that selecting one option or the other was much better than not selecting any option at all (indeed I'm quite content that neither option was picked).
What does ~ do? Is it the opposite of ! ?
I agree with Duncan. In general there has been too many proposal on the form "Y Haskell programmers use X (a lot/frequently/at least once) and therefore we should add it to the standard libraries". I don't think that's good enough reason to add things to the standard libraries as it increases their API surface area without increasing their expressive power. Stick it in MyUtils.hs
In effect, yes. It's just SHE idiom-bracket notation. (|snd, fst|) desugars to pure (,) &lt;\*&gt; snd &lt;\*&gt; fst which is lazy swap in the (-&gt;) environment idiom. Given that every Arrow has an environment idiom, it's (&amp;&amp;&amp;). 
 blah (snd &amp;&amp;&amp; fst) blah
It is an irrefutable pattern match. As long as you never inspect the result, `swap` doesn't actually care to what it's applied (EDIT: as long as it is of the correct type). const "OK" $ swap $ error "Not a tuple" -- "OK" EDIT: I originally had &gt; const "OK" (swap "Not a tuple") -- "OK" but, as [doliorules](http://www.reddit.com/r/haskell/comments/bzwcn/consensus_failure_bikeshedding_kills_another/c0pfseh) points out, that doesn't type-check.
For those like me who found SHE a little hard to google: [the website](http://personal.cis.strath.ac.uk/~conor/pub/she/).
Not quite: ghci&gt; const "OK" (swap "Not a tuple") &lt;interactive&gt;:1:17: Couldn't match expected type `(t, t1)' against inferred type `[Char]' In the first argument of `swap', namely `"Not a tuple"' In the second argument of `const', namely `(swap "Not a tuple")' In the expression: const "OK" (swap "Not a tuple") It makes it lazy: ghci&gt; let swap ~(x, y) = (y, x) in swap undefined `seq` "OK" "OK" ghci&gt; let swap' (x, y) = (y, x) in swap' undefined `seq` "OK" "*** Exception: Prelude.undefined it is equivalent to swap xy = let (x, y) = xy in (y, x) 
It is used for example in implementation of second in terms of first class Category a =&gt; Arrow a where -- ... second f = swap . first f . swap However I guess that more popular may be (at least in arrows): rol ~(x, ~(y, z)) = ((x, y), z) ror ~(~(x, y), z) = (x, (y, z)) 
Thanks. You at least gave me a term to search for. I had a hard time trying to get google to accept ~ literally, but with "irrefutable pattern match" I eventually found this: [http://en.wikibooks.org/wiki/Haskell/Laziness#Lazy_pattern_matching](http://en.wikibooks.org/wiki/Haskell/Laziness#Lazy_pattern_matching).
but second f = swap . first f . swap is way more clear to me than second f = (\\(a, b) -&gt; (b, a)) . first f . (\\(a, b) -&gt; (b, a)) Edit: I think I should add that having a concrete name for the swap function makes is quite useful when reasoning about program transformation (i.e. the program calculus).
Because it is mental masturbation trying to make something that is straight forward into something obscure.
I think you are missing the point. The proposal **wasn't** dropped because it was a little function, it was dropped because of bikeshed issues. If there weren't two plausible implementations it **would** have been added. So you (and all those upvoting you) must admit the process in broken in one way or another or both.
I remember now seeing a recent post about *cabal init* and I think that will be a huge help. I remember running into a problem where I didn't declare my dependencies correctly. Perhaps cabal check will help me with that? But in all I would say my main problem was probably sub-par documentation and lack of cabal init. Looking forward to trying again! (A little bit delayed response sorry)
So... do we have Prelude.Strict yet? (there's a haskell-cafe thread about that linked in the comments that seems bikeshedded too)
&gt; The proposal wasn't dropped because it was a little function, it was dropped because of bikeshed issues. It's true that the few people participating in the [mailing list discussion](http://osdir.com/ml/libraries@haskell.org/2009-06/threads.html#00144) discussed mainly whether it should be strict or lazy rather than whether we should have it at all. So maybe the process is broken because we didn't reject the proposal for the right reason. We had the people participating who tried to work out a sensible design but nobody was motivated enough to join in and say that we already have perfectly good syntax for this problem and that there is no need to add a new name. I still think it's fair to say the proposal failed for lack of interest. Re-reading the discussion, people don't sound exactly enthusiastic, except to make sure the wrong decision is not made. If it had been important to make some decision rather than no decision there would have been a little more enthusiasm. I acknowledge that sometimes some decision is better than no decision. I don't accept this was one of those occasions.
Perhaps the proposal should be resubmitted but to add these (and perhaps the rotate utilities mpiechotka suggests) to the Data.Arrow module if that is the context where they are useful and frequently used. The restricted context wold probably also resolve the lazy vs strict choice.
Actually, much of it is irrelevant to OO languages other than C++ or Java, too.
BTW, for reference, [here is the consensus protocol](http://trac.haskell.org/haskell-platform/wiki/AddingPackages#Consensus) we agreed to use when adding packages to the Haskell Platform.
I needed it just yesterday -- wanted to get the right element of the tuple into the first position so I could call M.toList . M.fromListWith -- of course I could have had the tuple elements swapped all along, but that would have changed a bunch of existing code.
Big upvote for FiniteRecursiveTypes
Nice! Now I have to update my stolen copy of IxSet with the shiny new improvements.
I agree that this is handy; I fake it like so: partialSig :: (Float, a) -&gt; (Float, a) partialSig = id foo = partialSig (3, (3.2, "foo")) &gt; :t foo foo :: (Float, (Double, [Char]))
What about inventing a naming convention for strict vs lazy? For example, swap would be the lazy version, and swap' is the strict? As much as I dislike naming that contains types or any other meta-info, that would fix this problem.
But MyUtils.hs is only mine. We can't have every programmer run around with their own util file, now can we? Well, we can, but only for small, personal hacks. I have my Brelude.hs, which contains utility functions that do everything from swapping values in a tuple to swapping them back again. I would never use it on a project anyone else were hacking on, or even on any of my own personal projects that have their own .cabal file. So what do we do? We need that function somehow, so we write it in some other way, e.g. (\(x,y)-&gt;(y,x)), or uncurry $ flip (,) if you're feeling pointless, or uncurry.flip.curry for the higher-order version, or case foo of ... Or you create a MyProject.Utils module and you copy or rewrite every little utility function you need and have ever needed, and all the other developers are doing the same, and the end result is a module full of neat little functions that act as glue pretty much everywhere in the rest of the project, all of which have been copied from somewhere else, nobody has any control over what's in there and there's probably lots of duplicate functionality (swap, flipPair, flipTuple, flipTup, pilf, exchange, xchange, swap2...) Yes, it doesn't quite belong in the standard library. I like modules that are small and add very little superfluous stuff. "This is a list. Here is it's language; 'nil', 'cons' and 'foldr'. This is a map, here's how you can break one open and peek inside, and here's how you join them. From these you can define insert, lookup and remove yourself." That's not really enough though. Libraries don't just have to provide what programmers need, but also what they want or what they think they need, or they'll spend eternity writing the same functions over and over and over and propagate them everywhere into every corner of every module under slightly different names, and nobody can remember what the function's called in each project so they just go "oh screw it" and write it again. Just because it's short and simple to write doesn't mean it's not useful to put it in a library. Character wise swap is only a bit shorter than the lambda form, but from a little more abstract syntactical vantage point the lambda is hueg like xbox in comparison to swap, which is just one element on every level, and even though that extra syntax is tiny in comparison to most other expressions we write it wastes precious braincycles and brainstack. I don't have to lecture you on this. We all know that reuse is good, and we all know abstraction is good even in simple cases. The issue is really just where we should put all these small abstraction aka. glue or utility functions, because putting every little utility function in the base library just wouldn't work for all of them. Not putting them in libraries won't help either, it would just lead to annoyance at having to type in the same function for the nth time, lots of duplicate functionality and internal APIs (both project and module level) that are impossible to get a decent overview of. I don't know the answer. I would suggest someone put those utility functions in a library and upload it to hackage, and indeed someone has already. Some are just MyUtils.hs uploaded to hackage and are only really used in that person's projects (some marks for at least doing that instead of copypasting everything), some are a bit more professional, and some try to be a Proper Library in themselves (I personally think these utility libraries have a different nature than Proper Libraries and that they shouldn't try to pretend otherwise.) Those libraries, however, are islands. I think we could try to group them up a bit more, make sure people know they're just extension libraries, give them their own mailing list, give them their own top-level namespace like Utils or Hoist. Conventions would be established, such as e.g. modules always pulling in the base library as well, always both strict and lazy versions of functions as long as that makes sense, and the strict version always ends in a '. That's what I'd like to see anyway, but I'll let people vote with their package uploads. I can see problems with this as well, but I feel like it would be detrimental to my rhetoric and dishonest to my dishonesty to list them.
I'm interested in both (got a math degree, wrote some math libraries, and doing "AI" at work). Though I have a bit less than zero free time... 
I think you are right.
Publish (and maintain) a Prelude.Berengal on hackage!
Wow, the meaning of those diagrams was horribly underspecified.
Thanks for pointing out what happens when I try to execute Haskell in my head. :-) Can I ask you to clarify why it is that applying `const "OK"`, which shouldn't care about its argument, to `swap "Not a tuple"` runs into trouble; but applying `` `seq` ``, which is explicitly *supposed* to evaluate its left-hand argument, to `swap "Not a tuple"` and `"OK"` works fine? (This is probably just an indication of the fact that, like many, I've never really understood what `` `seq` `` is doing.)
fibs = fibs
These are [Hasse diagrams](http://en.wikipedia.org/wiki/Hasse_diagram); I added a sentence of explanation.
Wow, this proposal was dropped as a direct result of me adding myself to the CC list. The exact opposite of my intention. :-( I guess that swap just goes onto the long list of functions that I have to continually reinvent... Edit: Wow, 3 of the 5 issues I CCed myself on yesterday were closed as "won't fix". I guess I'll remember to not add myself to proposals that I'm interested in or else the 'bump' might kill it...
Indeed that was the proposal made.
No. It died specifically because of my interest in it. I cc'ed myself on 5 proposals yesterday (effectively bumping them). Of those, 3 were killed. Seems a bit perverse... Swap makes for a quite useful combinator in my experience, and I end up reinventing it time after time. A simple example: listLengthPair :: [a] -&gt; ([a], Int) listLengthPair xs = (xs, length xs) foo = curry replicate $ swap $ listLengthPair "test" This example is a bit contrived, but in the course of my programming, many times I end up with with tuples that contain a list with some statistic about the list. The natural way to represent this is (String, Int), but most Haskell functions that can operate that are of the form bar :: Int -&gt; String -&gt; a. Opposite of the storage format.
This is a horrible idea. Have you already forgotten the awfulness of `missingh`?
But quite obvious if you've studied domain theory.
Don't worry about it. The proposal was zombied anyways. As I recall there were about equal numbers of people for the lazy version and for the strict version. As the proposer, I guess I would be in charge of moving it forward. Although I prefer the lazy version for being more like the Haskell style, I didn't see any really objective criterion to judge which version was better, so I stalled.
The problem is that your program has to type check, even in the parts that may not be evaluated. swap :: (a,b) -&gt; (b,a) and `"Not a tuple"` has type `String`, so it is not okay to pass it to `swap`. `seq (swap "not a tuple") 5` won't get past the type checker, either.
I suppose I have. What's awful about it?
Thanks! edit: Though I would say a few of them are still pretty ill-specified... for example, you have some infinite chains. The first one has one node above the dots and three below, which apparently denotes \omega distinct values; the second one has three nodes above and two below, which denotes \omega + 2; and the third has three nodes above and one below, which denotes \omega + 1. Is that right? If so, what's the pattern...? Can we square that away with the infinite chain with one extra value off to the side? Sorry if I'm being unusually dense.
I find the pointed version much easier to comprehend: foo' = let (lst, len) = listLengthPair "test" in replicate len lst In your point-free version I have to know the types and of all the participants involved and mentally decode the pipeline. The pointed version is only slightly longer and the names of the variables remind me what the meaning of the result is. Point-free style has its uses, but Haskellers tend to overuse it quite a lot, IMO.
Can't you find a better, more widely understood word than "bikeshedding"?
 fswap = uncurry.flip.curry now, fswap replicate $ listLengthPair "test" Isn't that far nicer? (edit: I realise this has been proposed further down the thread.)
Proposals always come with a deadline. You adding yourself to the CC does not change that, it just meant someone noticed the proposal was past its deadline.
Ah. I guess I would do well to acquaint myself with the (logical) processes involved.
On a related note using Σ for summing would be nice, but of course it's upper-case so not an identifier.
What about making the distinction by type? How about (,) for always lazy products, and a one-constructor datatype for the strict version? How often are irrefutable patterns important for *sums*?
Yes, agreed. Fswap would make a more useful library function than swap. mpiechotka's arrow proposal might still be reasonable though.
I'd say the difference is which elements have [covers](http://en.wikipedia.org/wiki/Covering_relation). So in the first infinite chain there, the top element (fix S) is larger than all the naturals, but doesn't _cover_ any other element (no largest natural). So the difference between these chains is how many cover relations there are at the top and bottom. Looks like you can spot when there's a cover relation by the half-line coming out of the nodes towards the dots. 
Oh, bikeshedding is a classic term. When I first saw it used, I was thrilled to learn its history. I didn't complain to the user! This isn't a particularly good example of bikeshedding, though. Hardly a debate at all, the proposal just fizzled after a mere fraction of the comments here. True bikeshedding would make the U.S. Senate blush.
Oh, bikeshedding is a classic term. When I first saw it used, I was thrilled to learn its history. I didn't complain to the user! This isn't a particularly good example of bikeshedding, though. Hardly a debate at all, the proposal just fizzled after a mere fraction of the comments here. True bikeshedding would make the U.S. Senate blush.
The recently published Design Concepts in Programming Languages has some great Hasse diagrams in the Fixed Points chapter on domain theory. For example, it shows that you can get the diagram for the product of two domains by sweeping one domain's diagram along the other's. Smash sums are relegated to an exercise and smash products are unfortunately not mentioned at all.
Say you want to use [snd3](http://hackage.haskell.org/packages/archive/MissingH/1.1.0.3/doc/html/Data-Tuple-Utils.html#v%3Asnd3) from the library. Now your library depends on MissingH, which implies your library now depends on array, base (4.*), containers, directory, filepath, haskell98, hslogger, HUnit, mtl, network, old-locale, old-time, parsec, process, random, and regex-compat
Point-free code tends to be easier to transform and refactor using the algebra of programs.
It has real advantages, you can compose it directly: myFunc 10 . uncurry . flip (,) You can decompose it into: flip (,) and uncurry Whereas the pattern-matching one can't be composed before being named swap first. I don't think it's a big deal either way -- but I've come to prefer the points-free version of things over time as: * It became readable with practice * The types tell the story too -- in case the code is harder to read * It is more composable/decomposable when I want to refactor things around * It is easier to generalize this form as it generally exposes the underlying structure better. * It is much shorter 
http://blog.tmorris.net/20-intermediate-haskell-exercises/ and I'm still waiting for advanced level!
I don't see why you have to reinvent it: Stick it in a module and reuse that module. If you want to share your module with more people create a package and release it on Hackage. Sticking new functions in base is not the only way to achieve reuse of code.
Seconded. What's worse than missingh would be to add missingh's functions to base.
Except it isn't shorter in this case.
I think you touch on an important point: abstraction versus code reuse. Many of the proposals for adding new functions to the base package only offer the latter. They name simple function compositions, point-free or not, that don't allow the programmer to reason at a higher level or write something he/she couldn't write using the old API. That's why functions like 'map' make sense in a base library but 'concatMap' doesn't. So functions that introduce abstraction are good candidates for inclusion. Another class of functions that warrant inclusion in an API is the class of functions that cannot be implemented efficiently using the current API. 
It looks like the documentation isn't available from the Hackage page anymore, but you can still get it from: http://happstack.com/docs/0.5.0/index.html
Though it has to be said that concatMap is a bit special. It's arguably just as much a primitive as map and concat are. It's the &gt;&gt;= for list afterall.
Well, even help on organizing / thinking about that project would be much appreciated. Don't hesitate to share some thoughts, drop by the IRC channel and so on.
Why do you need to add `map` to the base library? `map` is just `flip foldr [].((:).)` and we can just write that.
Suppose you want to use [snd3](http://hackage.haskell.org/packages/archive/MissingH/1.1.0.3/doc/html/Data-Tuple-Utils.html#v%3Asnd3) from the MissingH package. Now your library not only depends on MissingH, but also depends on array, base (4.*), containers, directory, filepath, haskell98, hslogger, HUnit, mtl, network, old-locale, old-time, parsec, process, random, and regex-compat.
Suppose you want to use Data.Tuple.Utils.snd3 from MissingH. Now your project not only depends on MissingH, but also depends on array, base (4.*), containers, directory, filepath, haskell98, hslogger, HUnit, mtl, network, old-locale, old-time, parsec, process, random, and regex-compat.
I suspect that falls under the "abstracts things away" header, not the "cannot be implemented efficiently using already existing functions" and that's why it should be included.
Is tibbe arguing that `swap` doesn't abstract things away, and if so what is the difference?
I have a response, but reddit won't let me post it. :( Maybe I can mail it to you.
Many of these small utility functions *do* offer abstraction though. `swap` is more abstract than `(\(x,y)-&gt;(y,x))`, it's not just syntactically shorter. The problem is that it offers so *little* abstraction (and reuse) that the added API complexity, however small, might not be worth it. I argue that it is worth it, or at least worth to try to manage that complexity. My suggestion was to separate the small utilities from the core library... -ish. Put concat and map in the core library, and put concatMap in the utility add-on module. Or the other way around. In the end you hopefully have a fully function and utilized library with a small and clear core. Of course, this separation adds complexity as well...
Another common case is that visitor methods for a node can decide whether to recursively visit their children by calling or not calling visit() on them. When the state threading is explicitly expressed with lazy evaluation over a monoid, this is easily done by simply substituting the identity element for the folded values of the children to be skipped.
What about SYB-style generic traversals? Visitors are commonly used by inheriting default visitor methods for each class that recursively visit children. That way you only have to implement visitor methods for the classes you specifically care about when making a custom visitor.
Yes, it is annoying that packages that provide a wide range of convenience functions also have a wide range of dependencies. But maybe that's not so bad after all; especially in this case, I think it's pretty reasonable to expect things like containers, mtl, base, array, directory, process, parsec, and network to be installed already, and grabbing a few more libraries is a small price to pay to avoid code duplication!
Actually, I don't think that swap is more abstract at all. Indeed, swap means *precisely* what you wrote (except that the strict versus lazy issue tells you that it's an ambiguous name). Frankly, I wouldn't dignify it with a utility function, because that would mean someone reading my code may have to stop, and go look elsewhere to see whether I really meant swap, or if I were hiding something else behind the name. Writing the lambda is clear, doesn't lose you any abstraction, and makes life easier on the person reading your code. I accept that there might be situations where this isn't true, where having a name for the operation would make sense because it's done commonly enough to justify forcing a reader to remember the name. They are, however, not the norm, in my view.
HP is not appropriate for all operating systems. Just because libraries such as network are in the HP isn't reason enough to make libraries that could be crossplatform into libraries that are not crossplatform.
&gt; I don't think this proposal is a good example for illustrating your point. This was a trivial proposal that added little. Alright, how about http://hackage.haskell.org/trac/ghc/ticket/3292 - adding Control.Monad.void? A useful convenience function used all over the place, easily understood from its type signature alone, and which has been asked for for years. Yet, from when I first started asking whether it was a good idea and why wasn't it in base yet to when a patch was actually applied, was somewhere over half a year, and even that was only after 10 nagging emails and 7 or 8 tracker comments &amp; patches. If I hadn't expected the bikeshedding (something like 5 suggested names or type sigs, with all the possible combos), and made use of calendars and reminders to periodically bug people about it, when do you think we would have gotten Control.Monad.void? Ever?
&gt; Or perhaps a system where proposals can move forward if there are no objections to a proposal (I hope no one would object to either solution to this swap definition), though I worry that this would involve politics too heavily. FWIW, I've been doing something similar with XMonadContrib. We had a real problem with dozens of patches outstanding and never being applied because few people commented on them and recommended them. I began issuing ultimatums and applying patches if no one objected to them. Now we're down to [one outstanding patch](http://darcswatch.nomeata.de/repo_http:__code.haskell.org_XMonadContrib.html). You can see a recent graph by aavogt where he charts how long a patch was outstanding before it was applied: http://code.haskell.org/~aavogt/darcsVersions/applyDelays.png (As far as I can figure, I was given the XMC commit-bit somewhere around -500.)
Producing infinite output is not the same thing as looping. Granted, though, it's hard to imagine finding the Show instance very useful.
I'd argue that `map` names a higher level notion than its implementation (in terms of `foldr`) while `swap` doesn't. The same way sum is an abstraction of `foldl' (+) 0`. Another example of something that I wouldn't consider an abstraction is `fooAndBar = bar . foo`, another pattern that we sometimes see in function proposals.
Isn't `swap` an abstraction of `snd &amp;&amp;&amp; fst` as much as `sum` is an abstraction of `foldl' (+) 0` ?
I've said it before and I'll say it again second f = swap . first f . swap vs second f = (\\(a, b) -&gt; (b, a)) . first f . (\\(a, b) -&gt; (b, a)) Tell me again that swap isn't an abstraction.
Being able to write something in point-free style doesn't make it an abstraction!
But we already have `\(a,b) -&gt; (b,a)` to say `snd &amp;&amp;&amp; fst`.
he needs to see this, let's summon him! jdh30. jdh30. jdh30.
flights that don't type check don't leave the ground
What about flights with the value bottom?
Is the tower crew rather strict or lazy?
you mean charter flights?
Depends, is the controller strict or lazy?
Sadly, the site has been crushed by reddit.
Anti-success principle?
The haddocks are here: http://hackage.haskell.org/package/Holumbus-Distribution New stuff is under Distribution, and it looks pretty awesome.
 let stopWhen p f x | p x = Nothing | otherwise = Just (f x) in unfoldr (stopWhen (==0) $ swap . flip divMod 10) 13249582
I've been playing with this idea myself, resulting in a prototype extension tentatively called [ContextConstraints](http://github.com/softmechanics/context_constraints). Here's how I would implement your example above with ContextConstraints: [http://gist.github.com/391600](http://gist.github.com/391600) 
-XDefaultImplicitParams Would provide optional named parameters for functions. Proposed syntax: &gt; inc :: (?arg :: Int) =&gt; Int -&gt; Int &gt; inc **(?arg = 1)** = (+ ?arg) Then inc could be called in two ways: &gt; inc 3 or optionally, &gt;let ?arg = 2 in &gt; inc 3 or (even better): &gt; inc **?arg=2** 3 NOTE: **Bold** denotes required syntax extensions.
Unqualified open imports must die! Once everything is used qualified, or explicitly listed in () when unqualified, I could finally know where symbols are coming from when reading code, and could also export a new name without fear that that would somehow break any module that imports my module. Modules without export lists and unqualified open imports should, IMO, be banned from hackage.
THE POWER OF TYPES COMPELS YOU
I'm really excited by all this cool distributed Holumbus stuff - distributed computations, file stores and a mapreduce implementation! However, before the wiki went down I noticed that there was only a MapReduce tutorial - but the distributed storage and computation systems are very interesting in their own right. Who's up for writing some examples for the wiki once it comes back up? :) (serious question btw) It would be nice if it didn't come down to guessing at realistic API usage, because many of these modules seem pretty undocumented as to use - at least with static typing this becomes much more easy to do reliably!
Could be awkward, but I suppose the existence of partial flights is preferable to all flights being unlifted?
This seems really awesome.
this talk is a really nice explanation of what's going on, the paper was a bit terse.
Last week I wrote a Holumbus-Distribution tutorial on my own site. It is out of date now (ignore the "Installing" section, cabal should just work) but it might be enough to get you started. http://richardfergie.com/holumbus-distributed
Nice, though be aware that this doesn't tie the knot like the standard definition of fix does: fix f = let x = f x in x
In my extensional pocket universe, I can't tell the difference. :)
We made the site work again. Some internal stuff for the Hayoo! search engine was changed and obviously something went wrong. The static stuff (blog and wiki) should be up again, Hayoo! may still be down from time to time but we are working on it.
-XJustWorkDamnYou
&gt; all you need is a single transfinite value to get general recursion This immediately strikes me as wrong, I thought transfinite ordinals were well founded - as I recall from studying logic that's why transfinite induction is a valid proof technique. So what's the discrepancy here? omega is defined by guarded corecursion but iterateN is defined by structural recursion. Is it really a surprise we can cause an infinite loop in that case, (if we think a little about Curry-Howard infinite looping should be something to do with having an inconsistency i.e. a proof of false). Structural recursion terminates when done on a well founded order, but omega is not well founded. Is omega an ordinal? In the topology of data types it is a limit (the reason a solution to omega = S omega exists is because N is defined by the greatest fixed point). In set theory omega contains every natural number but this omega does not contain any natural numbers - the comparison is a bit of a stretch. The way to define an analogue of some of these small ordinals is data O = Zo | So O | Lim (N -&gt; O), using this we then have omega = Lim id. 
Maybe you could create a mailing list? I think thematic haskell mailing lists work ok, they are pretty low traffic so even busy people can keep up
A similar observation was made in [Getting a fix from the right fold](http://www.haskell.org/sitewiki/images/1/14/TMR-Issue6.pdf): fix f = foldr (const f) undefined (repeat undefined)
I corrected ellipsis on diagrams; I hope it's comprehensible now. It is placed just above or below an infinite chain (\omega or \omega*); previously its placement was unclear. The first one is infinite ascending chain and then a top element - \omega + 1. The second one is two nodes and then infinite descending chain - 2 + \omega\*. The third one is one node and then infinite descending chain - 1 + \omega\*.
This is a good talk if you are interested in scientific computing with Haskell. It covers dph which looks like it should be good for sparse matrix-vector calculations on a multicore machine. Clusters and graphic processors not covered yet. Also see the library repa, which should be good for dense-only problems. I am looking forward to trying them as soon as I get a chance to upgrade my ghc version. 
Awesome talk!
It's frustrating to look at this code. I can kind of grok what any individual line is saying but I know I am totally not understanding the Big Point it's trying to make.
Sounds like an interesting place. :-) I would be impressed if they could find any decent candidates at all offering only 150 GBP per day for a contract job, though. The going rate for a competent (not stellar) contract programmer outside London is probably about twice that today, and that's before you factor in needing a good working knowledge with a relatively unusual programming language. Good luck, though, maybe somebody keen will snap it up because it's interesting work.
&gt; If you have inductive data types with structural recursion then all you need is a single transfinite value to get general recursion. Yep. That's why adding inductive datatypes to a language introduces (the possibility of) nontermination. Though technically you need coinductive datatypes, or some other means of adding laziness (e.g., functions from unit) in order to make the recursive value well-formed.
&gt; if we think a little about Curry-Howard infinite looping should be something to do with having an inconsistency It's "something" to do with inconsistency, though it's subtler than proving falsity. For instance, do you consider infinite proofs to be valid? Many people consider them invalid, though IMO this is based on a knee-jerk fear of infinity. E.g., how can we hope to (generally) provide proof of equality on streams? Sure, the proof is infinite, but we only ever need a finite amount of it and it should be well-cofounded and thus amenable to coinduction. But many people shy away from coinduction for the same knee-jerk reasons... Of course, mixing induction and coinduction in the wrong ways can easily lead to inconsistency. It's worth noting that the least and greatest fixedpoints coincide in Haskell's datatypes, so there's no real distinction between data and codata, and thus it's easy to mix induction and coinduction inappropriately. But then, Haskell is trivially inconsistent since bottom inhabits all types.
I should note that we achieved significantly above that rate last year and we shared it fairly with our contractors. It is simply that as a very small company with an irregular flow of work, it is hard for us to make guarantees. So we split it into base level + profit-share bonus.
You say living in England is not required, but what (if any) limitations do you have on location otherwise? UK-only, Europe, elsewhere? Some skill with English is presumably necessary, but what degree of fluency? I imagine you don't care too much personally, but as a small company there are bound to be awkward practical considerations. I'm horribly unqualified myself, regardless, but there might be more competent folks reading this wondering the same thing...
I read that as about 57000 dollars a year at current (1.48018 USD/GBP) exchange rates (assuming 260 work days a year). Sans benefits like insurance (it's a contractor job, right?), it'd might make a decent second job. Sounds fascinating, of course. Good luck with your hiring.
&gt; if we think a little about Curry-Howard infinite looping should be something to do with having an inconsistency i.e. a proof of false &gt; let fix f = f (fix f) &gt; :t fix id fix id :: t There you go--a proof of anything you wish, false or otherwise! Roughly speaking, via C-H, inconsistency is equivalent to unrestricted recursion, which is similarly equivalent to Turing-completeness. In fact, the λ-calculus was actually intended as a foundation for mathematical logic, and it was the construction of an infinite loop (the Kleene–Rosser paradox) that proved it inconsistent. It was afterward that Church stripped out the logical elements to get the untyped λ-calculus as a formal model of computation. Interestingly, Curry himself took the position that such "paradoxes" are only meaningless within the rules and interpretation of a system, not when viewed from outside, and suggested that allowing such terms is desirable in order to understand the paradox instead of merely avoiding it. From a logic standpoint I'm not sure what to make of that, but when viewing paradox as computational non-termination, there's something to be said for a bit of tolerance (e.g., \_|_).
That strikes me as overcomplicated... why use three languages (Haskell, your DSL and shell scripting) when two will do just fine (Haskell and your DSL)?
Is there any reason to use such EOF mark ?
it would be often extremely painful to write out all the imports. About exports, there your position is more justified, but still i find it a bit extreme.
The pain you have when writing an import is 1 divided by the amount of modules you import of the pain the reader of your code experiences when he tries to decipher where a name is coming from. It is much less painful for the writer than the reader. And, of course, you have ghc -ddump-minimal-imports
To play the devil's advocate, there are also tools for looking up names :) Also, I don't read that much code (maybe I should, I don't know). I agree that it's a problem that packages are getting broken because of open imports, but my ad-hoc estimation is that this causes only a few percent of all the broken packages. 
Tools for looking up names are much harder to use than "-ddump-minimal-imports" or even explicitly typing out every import. I explicitly type out every import, and the pain is really *not that bad*, compared with other pains Haskell has (The boilerplate surrounding each newtype, for instance).
Literate Haskell, anyone? #!/bin/sh runghc \\begin{code} -- haskell here \\end{code} eidt: Sigh. Markdown insists on putting a space after the backslash.
At the slide "Overview of compilation" Matthias Felleisen was commenting about something. PDG? BDG? DDG? I couldn't quite make it out. Anyone know what that was referring to?
If you are wondering, the code that creates this is in the github repository at http://github.com/jlouis/combinatorrent/tree/master/tools/ It is a fairly crude combination of shell-scripting, Haskell and R to produce the graphs. By the way: Firefox won't show the SVG images correctly since the SVG-support of FF is laughable: https://bugzilla.mozilla.org/show_bug.cgi?id=276431 - that bug report is from 2004 :) Opera and Chrome has no problems. 
Some of the names are a bit unconventional (i.e. not the same as used by standard lists, bytestring, text, and vector). * `unit` -&gt; `singleton` * `index` -&gt; `!` * `:!` -&gt; `unsafeIndex` * `+:+` -&gt; `++` (I'm not sure why there's both `append` and `+:+`) * `fold` -&gt; `foldl` (or is this some other kind of fold? What about a strict version.) Furthermore I don't know if `toScalar` is necessary as it seems to simply take the element at index zero. (If it's not the perhaps the comment could explain why/when one would use it). Finally, the QuickCheck related functions seem not to belong in a public API (perhaps put them in an internal module). 
isn't this because haskell is the only language without "built-in" collections?
&gt; the only language Isn't what because? Naming conventions? Lists are in the Prelude for 20 years -- they're "built in" and should be used as the source for names (map/ fold/ filter). 
there are two big extensions made for collection class, and still no such class exists. Naming conventions? using those we would still use one monad per file/ E.bind and S.return. and don't foget .+ for adding fp numbers! edit: or (+.)
Paczesiowa has a point. Since there's no common type class for collections, sequences, etc naming conventions aren't enforced and thus we see needless variation in function names.
Ah, a collection *class* -- that would have clarified your original point. Yes indeed, cobbling together Functor/Traversable/Monad/Monoid isn't ideal. But no one has put together the effort for a single class -- and [lessons from other languages](http://drops.dagstuhl.de/opus/frontdoor.php?source_opus=2338) are somewhat scary.
This is fantastic. SPJ gives running Haskell that feels like pseudocode it fits so well; this can't be far from the Hundred-Year Language
Have you seen Yi? It once could run with a Cocoa UI, but it's badly in need of fixing.
It's not even written in Haskell! Shame!
How does Repa compare to DPH? To Vector? Which pieces of DPH are still missing -- is it usable at all? If it is usable, why would I want to use Repa? I'm trying to track DPH progress, but it is difficult.
[smultron](http://smultron.sourceforge.net/) is nice, even though it's unmaintained now. Don't how well it fares with literate haskell, though.
To build it, open the xcode project file and build. You do of course need Xcode. My apologies, dangerous assumptions on my part.
It isn't written in Haskell because I wanted something that was 'very native', and I am not good enough in Haskell to be able to do something with the partial cocoa bindings. I think this would be a right tool for the job perspective.
I have one question about Yi: Is it basically something like Vi, but written in Haskell? If this is the case, it isn't what I am going for. I wanted an editor that feels natural on the mac. Note I cannot build it because I have ghc 6.12.
If this grows into something like [TeXShop](http://www.uoregon.edu/~koch/texshop/) for Haskell, that would be awesome. Feature requests: * syntax highlighting * tab turns into 4 spaces * increase/decrease indentation with Apple+] and Apple+[ I have more features in mind ("candy", aligning equations) for the Haskell code displayed. I wouldn't mind contribute them myself, but I'll only write Haskell code. The main problem is to have a reasonable representation of the contents for such things; you can steal from Yi there, I think. Heck, even a plain `String` with cursor position and selection will do for the start. 
Good news - indentation already works (forgot to mention that...), the tab key is already 4 spaces, but I don't convert tabs on loading. Syntax highlighting is something I am waiting to implement, because it is a major undertaking - but I do have some code for it. My main goal in the short term is get some building up and running for GUI apps. But a terminal can still be good for that. I do bet it is possible to make a sort of haskell integration, but have to be careful because during real time editing it can take too much time if you are not careful.
&gt; I do bet it is possible to make a sort of haskell integration, but have to be careful because during real time editing it can take too much time if you are not careful. True that. But I think you can defer thinking about real time things to later if you keep your data type abstract, so you only have to change the implementation later. Haskell integration is definitely worthwhile because most potential contributors will be Haskell programmers, after all. :-)
It's not configured properly. * The Code Signing Identity needs to be removed. [Pic of the Info Box](http://imgur.com/57PaE) * You need to install the InterfaceBuilder plugin from [BWTools](http://www.brandonwalkin.com/blog/2008/11/13/introducing-bwtoolkit/). I've copied the download to a new folder `~/Documents/Xcode/` and then double-clicked the `BWToolkit.ibplugin` file. * A file `Hicon.icns` is missing. I've simply duplicated the `LambdaIcon.icns` and renamed the duplicate to get something up and running.
Bird track Apple+. doesn't work when the cursor is at the beginning of the line.
One of my goals was to "ignore Haskell". I want people to do things in a unixy way. So: #!/usr/bin/mylanguage statements -- like you do for pure haskell with runhaskell, or perl, or python or whatever. So then there needs to be a shebang interpreter - that can take that script and cause it to run. It happened to be that I hacked together a 'snaprotate' 'interpreter' as a shell script. I could have implemented it in haskell too. The main thing that stopped me doing that was lack of an in-my-head way of running fragments of haskell code in the environment that I want (i.e. doing what the shell script does, but written in haskell).
&gt; tab turns into 4 spaces Please be civilized. It's 3 spaces. :)
Yi is a very flexible and customizable editor in its own right. You could write a mac frontend for it.
Repa and vector are parts of DPH, in a way. * vector: flat, fast arrays * repa: flat, fast, parallel arrays DPH: umbrella project for fast parallel nested arrays of arbitrary Haskell types.
So, from the responses so far it looks like...nobody. From my end, I am very interested but need to get multiple versions of ghc living together on my machine in order to run repa and dph. This makes me nervous. Just upgrading cabal-install breaks my current installation. OK I'm a wimp. But I will get around to it at some point because I really want to do scientific (matrix-vector) programming in haskell Real Soon Now. 
Sorry for the late response. &gt; I will ask about getting the "how to pull" GHC repository documentation updated. Looks like they the GHC folks have updated their documentation to point to the new instructions. &gt; There are already bug reports for the move/delete-an-open-file issues. One was was reported by SPJ himself, IIRC. That's how I found out that Darcs' developers' plans to fix the move/delete-an-open-file issues involve trying to get Unix filesystem semantics working on Windows. Thanks for bubbling this back up to our attention. Your details have allowed me to track the issue down to our [issue1675](http://bugs.darcs.net/issue1675). We had talked about this earlier in the team and the actual conclusion we came to (2009-11) was that the way to deal with this problem was to move deleted files somewhere else and delete it on exit. I'm sorry if our issue1452 gave you the wrong idea. &gt; The problems with building using GHC 6.12 on Windows are caused by a change in GHC's internal representation of filepaths on Windows; they were narrow strings previously and they are wide strings presently Thanks. We've received some reports about this in the community and addressed it in our [Darcs 2.4.3 release](http://wiki.darcs.net/Releases/2.4). &gt; I do have a MTA installed--Microsoft Outlook 2010 beta. To be honest, I don't know if Outlook or Darcs is to blame here. I will try again with Outlook 2010 RTM soon, if GHC is sticking with Darcs. I think you may be interested in our [drop MAPI support](http://bugs.darcs.net/issue1688) ticket. The plan on Windows is to ship MSMTP with Darcs instead. &gt; I appreciate your calm response to my blunt comment that Darcs is horrible. But, I stand by my statement. On Windows it has regularly been unusable when I've tried to use it on multi-person projects, over the course of several years. Even git on Windows seems to be more reliable now. I don't see Windows support improving without a much stronger committment to Windows support than Darcs has currently. Blunt is good. Better is blunt engagement. Windows is indeed under-represented in the (relatively small) Darcs team. We do take Windows support seriously and we try to compensate for this by dedicating one of our developers to Windows-specific issues, but we could always use help. It's thanks to our Windows Czar that we were able to knock out all the filehandle leaks that were making Darcs life so miserable on Windows before Darcs 2.3. (which is not to say that delete-and-open-file issue isn't still present, just that we've been making a lot of progress). We now have official binaries for Windows and in Darcs 2.5, we will provide a proper installer. These are things that we provide specifically for Windows because we realise that Windows users can have somewhat different needs. So things are creaking forward, but we need your help. Windows-specific patches or even complaints on darcs-users could go a long way. I hope that you will bear with us and in time see why despite all of these problems Darcs is so appealing to us. We are definitely very keen to solve a lot of the day-to-day problems that users like you are dealing with. Given that you're particularly conscious of what the specific problems are, you engaging with us on a more regular basis could have a really positive effect. 
Yi has both Vi-like and Emacs-like interfaces available. You pick one in your configuration file. There's a third default configuration available, but I don't remember what it is.
Doing work you enjoy, using tools you love, alongside smart people: priceless.
Better suited on [the xmonad reddit](http://xmonad.reddit.com).
[Fraise](http://www.macupdate.com/info.php/id/33751/fraise) is a fork of Smultron that *is* maintained. The more you know.
I thought I was the only one!
and now we're 2! (even numbers are gross)
Incidentally, has anyone implemented manual parallel arrays with Data.Vector and/or Data.Vector.Unboxed, similar to parMap for lists? It doesn't seem like it ought to be hard, but all my attempts so far have resulted in code that is at best twice as slow as the serial version, and sometimes ten times slower. Perhaps I should be using repa or dph, but I don't really want to move to a newer ghc until it's supported by the platform, and using a different API just to get parallel arrays is, for me at least, a bit of a hard sell. Which isn't to say I don't think it's a worthwhile project, it's just that I'm not yet ready to use it yet.
&gt; Manual parallel arrays with Data.Vector That's kind of what Repa is for -- taking the shortest path to a decent parallel API for vector. I think people should be using Repa for parallel arrays, and vector for non-parallel ones, for now.
There shall be an option of course :-)
Good to know for sure.
I appreciate your information here, I have hopefully just fixed all of these issues. My apologies.
All the issues should be fixed now. Could you tell me if you have any issues building it from a fresh download?
This should be fixed now in the head.
What is the benefit of this sugar? The quadratic formula example you give doesn't seem to be enough motivation. How is it better than what we can do now? Perfectly willing to be persuaded but I'm not getting it yet ...
I fail to see how extensions to the type system (new kinds) is merely syntactic sugar. Your example seems to be of negligible benefit. Perhaps explain how this is any better than, say: a +- b = [a+b, a-b] infixl 6 +- quadraticFormula a b c = (/( 2*a)) &lt;$&gt; (-b) +- (sqrt (b*b - 4*a*c)) -- even better, using applicative idiom brackets quadraticFormula a b c = [| ((-b) +- sqrt (b*b - 4*a*c)) / 2*a |] main :: IO () main = do input &lt;- getLine let (a,b,c) = read input let solutions = quadraticFormula a b c print solutions (disclaimer: I haven't tested this). It seems like most of your syntactic conveniences can be easily expressed just by having applicative idiom brackets, rather than this enormous, unlikely to be approved extension. 
You're right, it's inaccurate to call it a sugar. The primary advantage is that it makes everything monadic. `sequence` becomes redundant and `mapM` becomes `map`. In addition, all sorts of non-monadic functions can be used as though they were monadic. For example: typeAliases2 &lt;- liftM Map.fromList $ sequence [do decl2 &lt;- qualifyNamesInDecl (moduleScopes ! mName) decl return (Qual mName dName, decl2) | (Qual mName dName, decl) &lt;- Map.toList typeAliases] becomes typeAliases2 = Map.mapWithKey (\\(Qual mName _) -&gt; qualifyNamesInDecl (moduleScopes ! mName)) typeAliases while still preserving whatever special monadic behavior `qualifyNamesInDecl` is taking advantage of (in this case, error handling). Also, applicative functors are less powerful than monads. Edit: I think it also makes monad transformers redundant (if you apply the desugaring to the definition of `(&gt;&gt;=)` and `return`), but I'm still looking into this.
I've edited the post to give one motivating example. I'm working on more.
No hard restriction on location. Written English is important.
I am not sure I fully understand what you suggest, but "everything becomes Monadic" does not sound like a good thing. Applicatives being less powerful than Monads means we should use them for everything we can -- because using the least powerful construct that can handle the problem means our code will be able to work with the widest variety of types, optimizations, and libraries.
Applicative functors are less powerful than monads, but all monads are applicative functors. You gain nothing by promoting them.
&gt; You gain nothing by promoting them. I'm not sure why you say that. You can do things with monads that can't be done with applicative functors, so a syntactic sugar for monads can do things that can't be done with a syntactic sugar for applicative functors... right?
So I decided to just sit down and do something, and here's the preliminary result. A library for talking to Second Life (or OpenSim-based servers). It's incomplete, but the basics are done, and it's now just a matter of wrapping more of the messages in a more convenient API. Connecting this to your favorite FRP implementation is left as an exercise for the reader.
http://hackage.haskell.org/packages/archive/WashNGo/2.12.0.1/doc/html/WASH-Utility-RFC2279.html Seems to have the wrong types, and be redundant to bytestring-utf8?
Awesome. To integrate it with Haskell, you could maybe look at Tim Scheffler's work: http://tscheff.blogspot.com/ It would be cool to have a minimal Cocoa project and then provide a lot of hooks for Haskell code. Or make it extensible in XMonad-style, but that's a lot more work.
1 Consider a function: func f = lower f What is its type? (a -&gt; m a)? But then func id /= lower id which is a list strange. Also there is no way of checking if a generic function (i.e. one which was supplied as of type a, (a -&gt; b) etc.) have 1, 2, 3... arguments. Therefore: lower f May have dramatically different meaning depending on the general type of function. In Haskell so far: -- Intraferred type: (Monam m, Class m a b) =&gt; (a -&gt; b) -&gt; a -&gt; m a b (something about flavor) doSomething f x = &lt;some code&gt; worked if we just specialize it: instance Class IO Z (T Z Z -&gt; U) doSomething :: (Z -&gt; T Z Z -&gt; U) -&gt; Z -&gt; IO (T Z (T Z Z -&gt; U)) (something about flavor) doSomething f x = &lt;some code&gt; However with your syntax it may not sompile as lower f means something different. Also does a -&gt; b -&gt; c means something else then a -&gt; (b -&gt; c) 2 It seems to be much more applicative-oriented then monad-oriented 3 lower seems to be a pure/fmap/liftA2/liftA3/... with broken free theorem 4 I don't think (f)map&lt;-&gt;sequence+mapM can be avoided. With (f)map you have only fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b If you combine it with monadic function you'll get (b = m b'): fmap :: Functor f =&gt; (a -&gt; m b) -&gt; f a -&gt; f (m b) However you need to have something to swap f and m. Well - that's sequence for you ;) In general - 
Oh, neglected to mention that to log in, you first need to set up stunnel, so that localhost:8001 is proxied via SSL to login.agni.lindenlab.com:443. The stunnel config looks like this: client = yes verify = 0 [my-mvclient] accept = 8001 connect = login.agni.lindenlab.com:443 
I will say that I am trying to keep this simple, but I have sent an email to Tim about how stable and applicable this would be to my project. It is a very good idea. UPDATE: Tim replied, and it is not ready for real world use. But that said - it wouldn't stop me from looking at it eventually.
Kind of a bummer that all these great new array libraries cast off `Ix` in favor just using `Int`s. Not that I think indexing should begin anywhere other than 0, nor would I ever use anything but a machine word to index an array, but being able to parameterize your array not just by its contents but also its (`newtype`d) index type is crucial for maintaining safety (and sanity) in large struct-of-array style programs. E.g., consider a 3D triangulated mesh. You've got: vertices, edges (directed and undirected), faces, objects (connected components), each with a distinct id. If you're doing mesh processing you might have clusters of vertices, edges (polylines) or faces (patches) and each vertex/edge/face maybe needs to store it's cluster id. Etc, etc. All of these ids are just `Int`s. If you stick with H98 arrays, you can do `newtype Vertex = Vertex Int deriving (Ix,...)` and be sure that you'll never accidentally index an edge property with a vertex. Not so with any of the new libraries. It really makes a difference when you're writing array-indexing expressions like a[b[c[i]]]. It also matters for overloading, as you can't overload indexing functions if they all take ints, whereas if you have an operation common to all index types but semantically different for each (say, sending to OpenGL) then you can write a class which abstracts that and define instances for each of your `newtype`d index types. I can understand why people are not interested in the baggage of `Ix`, and much bikeshedding could come of trying to find a "better" array index class, but there is already a class which is parameterized over types which map to integers: `Enum`. You can bolt this on as a safe wrapper type which is parameterized by both the element and index type, but this kind of afterthought-approach to type safety never works. IMO a Haskell array facility that only accepts `Int`s as indexes is no more useful than Fortran or Matlab. You'll never beat Fortran on performance, and Matlab is a monstrosity but if you're just going to bang out some quick numeric code without real care for safety or correctness then Matlab is tough to beat. Chasing these Dinosaurs is not what (I think) Haskell should be doing.
What about the fishr operator &gt;&lt;&gt;?
Which paper is that?
[Type inference in context](http://personal.cis.strath.ac.uk/~adam/type-inference/)(draft) Edit: was meant to be a reply to davidwaern
What about: &lt;&gt;⥽ ⥼&lt;&gt; There is also the fish eye symbol (◉), but it's a bit big for such a small operator. 
&gt; But then &gt; &gt; func id /= lower id &gt; &gt; which is a list strange. Also there is no way of checking if a generic function (i.e. one which was supplied as of type a, (a -&gt; b) etc.) have 1, 2, 3... arguments. Therefore: &gt; &gt; lower f &gt; &gt; May have dramatically different meaning depending on the general type of function. I think you're right. (Crap.) That might make the whole idea useless. Thanks for pointing that out. A possible solution is to say that yes, `lower f` does have a dramatically different meaning depending on the type of the function. Since `lower` is a keyword, it can hypothetically do whatever magic it wants, including having different behavior at runtime depending on what type is passing through it. However, I would have to spend a lot more time thinking about it. So for now I'm declaring this idea broken, and I'll consider posting again if I come up with something to fix it. &gt; I don't think (f)map&lt;-&gt;sequence+mapM can be avoided. Consider an example: sequence [Just 1, Just 2, Nothing] With the sugar, this would be: lower [1, 2, lift Nothing] Which simplifies to the same thing: do x &lt;- Nothing return [1,2,x] (It took me a while to perform the desugaring and compute what it would become; if you like I'll post the intermediate steps to show that it works.)
Well, it typechecks :-)
haskell lends itself to all kinds of fun operators like the reproduce operator 8==)
I propose also a 'bird' operator: &lt;°^°&gt;
&lt;o_0&gt; , the face operator, you know in case you want to do some computations of simplicial sets... 
For the more advanced programmers... o o o o &lt;*)))&gt;{
Nice. But one comment: newNVar :: NCContext -&gt; String -&gt; IO () putNVar :: Binary a =&gt; NCContext -&gt; String -&gt; a -&gt; IO () takeNVar :: Binary a =&gt; NCContext -&gt; String -&gt; IO a It can be made more consistent with MVar's API if it was: data NVar a = NVar NCContext String (a -&gt; ByteString) (ByteString -&gt; a) newNVar :: Binary a =&gt; NCContext -&gt; String -&gt; IO (NVar a) putNVar :: NVar a -&gt; a -&gt; IO () takeNVar :: NVar a -&gt; IO a This would also be more type-safe and less bug-prone. It would enforce that the NVar value would always be parsed to the same type. Can also add a Typeable constraint in newNVar to verify on master and slaves that it is made of the same type.
I wish interesting preprints like this were linked not only when they contained funny ascii operators. :-)
Sadly 8 is not an operator character in Haskell.
&gt; A possible solution is to say that yes, lower f does have a dramatically different meaning depending on the type of the function. Since lower is a keyword, it can hypothetically do whatever magic it wants, including having different behavior at runtime depending on what type is passing through it. However, I would have to spend a lot more time thinking about it. No - as I explained you don't know the type of parameter until the linking time. I know it is strange but *THERE IS NO MULTI-PARAMETER FUNCTIONS IN HASKELL* (sorry for shouting - that is very important IMHO). f :: a -&gt; b -&gt; c. Is not a 2 parameter functions - it is a one parameter function (a -&gt; d) which happens to return a function (d = b -&gt; c). Due to free theorem it is impossible to distinguish if c in above example is a function or 'final' value. Hence is something like that valid f :: a -&gt; m a f x = lower x Well - maybe. It depends if the x is a function or not - i.e. if a = b -&gt; c for some b or c or not. But WE DON'T KNOW IT AT THE COMPILE TIME. If the function is exported from library it is not known what a be instancized into. Hence it is not know what x is so it is not know it type is legar or should it be: f :: (b -&gt; c) -&gt; m b -&gt; m c Also please note that it may be unsure which value you should choose: class MyInt i where getZero :: i addOne :: i -&gt; i toIO :: i -&gt; IO Int instance MyInt Int where getZero = 0 addOne = (+1) toIO = return instance MyInt (IO Int) where getZero = print 0 &gt;&gt; return 0 addOne v = v &gt;&gt;= \v' -&gt; print v' &gt;&gt; return v' toIO = id doSomething :: Int -&gt; IO () doSomething v = print f = lower (doSomething getZero) What should f print? Should it inform about undeciable type - both lower (doSomething (getZero :: Int)) [unary function] and lower (doSomething getZero :: IO Int) are correct. Of course similar problems occures in Haskell - but here doSomething getZero is correct expression WITHOUT THE lower KEYWORD. &gt; (It took me a while to perform the desugaring and compute what it would become; if you like I'll post the intermediate steps to show that it works.) Ok. Please derive function: sequence :: (Functor f, Monad m) =&gt; f (m a) -&gt; m (f a) If you want something simpler: data Tree a = Leaf a | Node (Tree a) a (Tree a) instance Functor Tree where f `fmap` (Leaf v) = Leaf (f v) f `fmap` (Node l v r) = Node (f &lt;$&gt; l) (f v) (f &lt;$&gt; r) sequence :: Monad m =&gt; Tree (m a) -&gt; m (Tree a) sequence (Leaf v) = v &gt;&gt;= Leaf sequence (Node l v r) = liftM3 Node (sequence l) v (sequence r) -- Pre-order -- Or should it derive sequence :: Monad m =&gt; Tree (m a) -&gt; m (Tree a) sequence (Leaf v) = v &gt;&gt;= Leaf sequence (Node l v r) = do v' &lt;- vl' &lt;- sequence l -- In-order r' &lt;- sequence r return $! Node l' v' r' -- Or maybe it should traverse post-order or level-order? You need to specify the [folds](http://www.haskell.org/ghc/docs/6.12.1/html/libraries/base/Data-Foldable.html) - but you still need to know about the internal structure to rebuild the tree. OK - with lists you have a relativly know structure but even for them you have several ways of implementing sequence - one normal and one reverse. Also what should someting like: lower (flip (:) [lift (print "123")] (lift (print "456"))) be compiled into? Sorry if I'm harsh but: 1. The syntax have very little benefits as pointed out 2. It changes a few fundamental basics of the Haskell (free theorem, distinguish between the pure/impure functions, relativly minimal syntax sugar). It is as if you didn't liked the strict distinguish between pure and im-pure computation in Haskell. It is fine - noone forces you to like them but it is like Haskell works. 3. It does not have well-defined evaluation order. Since monad's are structure you use when evaluation order usually matters (IO, CHP, [], StateT, ...) it is a big problem (I guess that Maybe and Identity are one of the few in which is does not matter).
It seems worth mentioning that perl's general ordering comparison operator, equivalent to Haskell's `compare` on the `Ord` type class, is written `&lt;=&gt;` and, since this is perl, is referred to by *official documentation* as the "spaceship operator". Given Haskell's liberal support for free-form infix operators I was actually kind of disappointed to find that it uses the rather dreary `compare`.
These names are used because they're the same as the ones in the DPH internals. I don't mind changing the names to look like vector, but then you'll be typing R.++ instead of +:+, just saying. Repa fold could be implemented as a parallel tree fold, so is neither left nor right. I should probably add a note to the docs. 
Your shell script can be simplified a bit using your shell's built-in temp-file creation. Untested code follows: #!/bin/bash LIBDIR=`dirname "$0"` FILE="$1" shift runhaskell -i$LIBDIR &lt;( echo import SnapRotate grep --invert-match '^#!' "$FILE" echo main = runLevels policy ) $@ This will also correctly clean up the temp file after runhaskell exits and allow them to have their own imports (arguably bugs in your script ;-).
&gt; No - as I explained you don't know the type of parameter until the linking time. I know it is strange but THERE IS NO MULTI-PARAMETER FUNCTIONS IN HASKELL (sorry for shouting - that is very important IMHO). f :: a -&gt; b -&gt; c. Is not a 2 parameter functions - it is a one parameter function (a -&gt; d) which happens to return a function (d = b -&gt; c). Due to free theorem it is impossible to distinguish if c in above example is a function or 'final' value. I was aware of that. &gt; Well - maybe. It depends if the x is a function or not - i.e. if a = b -&gt; c for some b or c or not. But WE DON'T KNOW IT AT THE COMPILE TIME. If the function is exported from library it is not known what a be instancized into. Hence it is not know what x is so it is not know it type is legar or should it be: That's why I said "different behavior *at runtime*". If it worked (and like I said, I'm not sure it would), then it would be similar to how the behavior of a class method is chosen depending on what type it is being instantiated to. &gt; What should f print? Should it inform about undeciable type - both lower (doSomething (getZero :: Int)) [unary function] and lower (doSomething getZero :: IO Int) are correct. Of course similar problems occures in Haskell - but here doSomething getZero is correct expression WITHOUT THE lower KEYWORD. That's another good point, and I would have to work that out before my idea was viable. Like I said before, your first objection (`func f = lower f`) is a major one, and I'm declaring my idea broken for the moment. It might be fix-able, but I'm not sure how. &gt; Ok. Please derive function: &gt; &gt; sequence :: (Functor f, Monad m) =&gt; f (m a) -&gt; m (f a) That's not what I said. I said I could show how `lower [1, 2, lift Nothing]` could become `do { x &lt;- Nothing; return [1,2,x] }`. I've included that derivation at the bottom of this post, because it's long. &gt; You need to specify the folds - but you still need to know about the internal structure to rebuild the tree. OK - with lists you have a relativly know structure but even for them you have several ways of implementing sequence - one normal and one reverse. The behavior would depend on exactly the way the expression to be desugared was worded; two expressions that were equivalent in pure Haskell might turn into different things after being `lower`ed. &gt; Also what should someting like: &gt; &gt; lower (flip (:) [lift (print "123")] (lift (print "456"))) &gt; &gt; be compiled into? By applying the rules that I listed in my original post, plus editing to make it more readable, I get this: do x &lt;- print "123" y &lt;- print "456" return [x,y] &gt; Sorry if I'm harsh Don't be sorry. The reason I posted my idea on reddit was so that if it was a bad idea (and apparently it is) it would get shot down before I spent too much effort on it. &gt; 1. The syntax have very little benefits as pointed out I disagree with that. I think that if it worked out, the benefits could be tremendous. This post is already quite long, so I'll just refer you to the "Edit:" that I added at the end of my original post. &gt; \2. It changes a few fundamental basics of the Haskell (free theorem, distinguish between the pure/impure functions, relativly minimal syntax sugar). It is as if you didn't liked the strict distinguish between pure and im-pure computation in Haskell. It is fine - noone forces you to like them but it is like Haskell works. You're right -- this is an attempt to get rid of the pure/impure division and make it as easy to write impure code as it is to write pure code. However, the rules that I have outlined would (if they worked) prevent confusion, because the compiler would keep track of which monads each piece of code was operating under. &gt; \3. It does not have well-defined evaluation order. Since monad's are structure you use when evaluation order usually matters (IO, CHP, [], StateT, ...) it is a big problem (I guess that Maybe and Identity are one of the few in which is does not matter). It does have a well-defined evaluation order. The rules that I listed on my original post provide a clear and unambiguous evaluation order. Again, I would like to thank you for pointing out the problems in my idea. Here is the desugaring process from `lower [1, 2, lift Nothing]` to `do { x &lt;- Nothing; return [1,2,x] }` as promised: lower ((:) 1 ((:) 2 ((:) (lift Nothing) []))) (do f &lt;- lower ((:) 1) x &lt;- lower ((:) 2 ((:) (lift Nothing) [])) f x) (do f1 &lt;- lower (:) x1 &lt;- lower 1 f &lt;- f1 x1 f2 &lt;- lower ((:) 2) x2 &lt;- lower ((:) (lift Nothing) []) x &lt;- f2 x2 f x) (do let f = \\y -&gt; return (1:y) let f2 = \\y -&gt; return (2:y) x2 &lt;- do f3 &lt;- lower ((:) (lift Nothing)) x3 &lt;- lower [] f3 x3 x &lt;- f2 x2 f x) (do let f = \\y -&gt; return (1:y) let f2 = \\y -&gt; return (2:y) x2 &lt;- do f3 &lt;- (do f4 &lt;- lower (:) x4 &lt;- lower (lift Nothing) f4 x4) x3 &lt;- lower [] f3 x3 x &lt;- f2 x2 f x) (do let f = \\y -&gt; return (1:y) let f2 = \\y -&gt; return (2:y) x2 &lt;- do f3 &lt;- (do x4 &lt;- lower (lift Nothing) return (\\y -&gt; return (x4:y))) x3 &lt;- lower [] f3 x3 x &lt;- f2 x2 f x) (do let f = \\y -&gt; return (1:y) let f2 = \\y -&gt; return (2:y) x2 &lt;- do f3 &lt;- (do x4 &lt;- Nothing return (\\y -&gt; return (x4:y))) x3 &lt;- lower [] f3 x3 x &lt;- f2 x2 f x) (do let f = \\y -&gt; return (1:y) let f2 = \\y -&gt; return (2:y) x4 &lt;- Nothing f3 &lt;- return (\\y -&gt; return (x4:y)) x3 &lt;- lower [] x2 &lt;- f3 x3 x &lt;- f2 x2 f x) (do let f = \\y -&gt; return (1:y) let f2 = \\y -&gt; return (2:y) x4 &lt;- Nothing let f3 = \\y -&gt; return (x4:y) let x3 = [] x2 &lt;- f3 x3 x &lt;- f2 x2 f x) (do x4 &lt;- Nothing x2 &lt;- return (x4:[]) x &lt;- return (2:x2) return (1:x)) (do x4 &lt;- Nothing let x2 = (x4:[]) let x = (2:x2) return (1:x)) (do x &lt;- Nothing return (1:(2:(x:[]))))
Oops, I forgot about that Reddit. My bad. :o)
Even if it was you need to start with the (.)(.) operator.
[Choose your poison](http://hackage.haskell.org/package/data-aviary) ;)
The main reason I posted this was because i wanted to remark that I really wish that these examples were split up into functions. Large, monolithic code like this feels very un-haskellish to me.
&gt; need to get multiple versions of ghc living together on my machine Try [nix](http://nixos.org/nix).
Main&gt; let (&gt;&lt;&gt;) = ((flip ($)) (foldr ((:) . succ) [] "hs&amp;r\\US`\\USehrg") $ (.) const const) Main&gt; "what is this" &gt;&lt;&gt; "i don't know" "it's a fish" 
Great so not only are our folds confused but so are our fish?
I prefer the Tie Fighter operator |o|.
 sortBy (&lt;=&gt; `on` size) -- Hmm...
 sortBy (comparing size) -- Already taken care of 
Yes, but it is applicable to fewer structures.
Anybody using wash for "real" web development, or is this just one of those phd type projects? Stand up and be counted :) What are the nicest demos? (Are there demos?) How does wash compare with happstack/yesod?
Sorry about that. The lack of a registration link was due to an upgrade I did to the bug tracking software that went awry. I've fixed it now, so you can create a login. Thanks!
http://www.reddit.com/r/haskell/comments/c262b/the_fish_operator/c0ppz6s
Link to their site busted. Sounds like a cool job, though. I'd love to work on something like that, but I'm too intimidated :] I'd really like to see a real GUI solution for Haskell. I understand why GTK is the most popular at the moment for creating GUIs in Haskell, but it's such a pain to build and distribute applications with it. Not to mention that the actual experience of coding the UI glue is only one or two steps above regular C. You get the benefits of monads and tying into your other Haskell code, but it's still mid/low-level in nature. I've played with some FRP stuff and much of it is elegant and nice, but I haven't yet seen anything polished enough to build full applications with. The rate with which FRP projects are created and destroyed also leaves you a little wary when looking to choose a toolkit. Something that's a little higher up than "do this, then do that" but not as experimental as FRP would be nice. What I'd really like to see is something that just throws out the toolkits from other projects and operating systems and builds one from scratch in OpenGL. It wouldn't much look like the widgets of whatever OS you're running it in, but it would perfectly portable between them. Also, my personal opinion is that the most interesting applications tend to make little use of the standard buttons/sliders/spinners that come in widget toolkits, and instead have controls and representations that are tied more closely to the task at hand. I think it's kind of nuts that there's not a good way to do GUI in Haskell (or most other non-C languages.) I can't even do a VTY app, since it won't work in Windows. Ok, rant over :)
are you sure it's fisheye? (◉)(◉)
Interesting, you can navigate to: http://tsurucapital.com/en/ But they only seem to accept the address posted in haskell-cafe if it is referred from their address.
I intended that imports would be allowed in the version that I provided - what makes them not work? But using &lt;() seems much nicer, yes... 
All imports have to be at the top of the file, so the definition of `main` can't go in before their file.
I like.
I'd be worried about the notion of working for a Japanese company. I've heard a lot of bad things about their corporate culture in general - long hours, high stress, etc. Perhaps someone who has actually tried it could disabuse me of this notion?
There is wxHaskell which is cabalized; I usually had no problems installing it on OS X. But in general, creating and maintaining a GUI binding is a thankless task. I mean, people will be really happy about it, but the (work / interesting work) ratio is very small and there is little intrinsic motivation to do it. Put differently: why don't *you* write a GUI binding? ;-)
&gt;The team is based in Tokyo but there are projects that could be done remotely so feel free to apply for this type of work also.
Tulmult's asking for something way worse than another binding... tulmult's asking for a _GUI_, full stop. I've seen these efforts in a couple of other languages. Results are... not so good. GUIs are hard in a dimension that merely being written in Haskell isn't going to help much, because the problem is fundamentally hard. Haskell used properly can brush away accidental complexity, but nothing can remove essential complexity.
I absolutely love the comics on that blog. So much.
Yeah, but what is merely sort of bad style in Javascript is _really_ bad style in Haskell. In the original [Russion Doll article](http://javascriptweblog.wordpress.com/2010/04/27/the-russian-doll-principle-re-writing%C2%A0functions%C2%A0at%C2%A0runtime/), three possible uses are mentioned. The second and third are both better handled by memoization, and the first is better handled by something that, you know, _actually_ shows the logic in action. (And that _specific_ action used as an example, not running the shutdown process again while shutdown is occurring, is probably ripe for a combinator so you end up with something like `shutdown_process = ...; shutdown = run_only_once shutdown_process`.) There's nothing wrong with the article per se, it's just that anytime someone shows a cute trick, it is utterly inevitable that someone will see it and end up thinking it's a good idea. If you are that someone, go figure out how to memoize.
Right. I added a disclaimer to the bottom of the article, lest people get the idea that this code is anything more than a cute trick.
They're an international company who happen to live in tokyo.
May be embedded webservers and the browser as the gui is the way to go forward ?
This doesn't look like avoiding success! Scala has more questions than we do, so I guess we're okay for now. Seemed to be ahead of most other functional languages, though. Sigh. On the other hand, maybe it's just evidence that Haskell is more confusing than the others...
Well, 5x the rate of OCaml questions, so that's good trend for the 2010s
Righty-o, and thanks!
and only 34 questions tagged "haskell" and "monads". the rate of them is much lower than rate of tutorials about monads :)
and how would you implement run_only_once?
Download it now! Includes free entry for Bulwer-Lytton fiction contest!
*\*shudders\**
&gt; Nothing can remove essential complexity, ... ... except powerful abstractions. ;-) I'm certain that some form of reactive programming can make GUI programming simple. Designing good GUIs for human beings is still hard, but coding the functionality should not be.
Totally depends on the "thing" you're trying to "run" only once. A simple MVar added as a parameter to run\_only\_once might work in this case (with the MVar only used for "full" or "empty", the value wouldn't matter). If I were using CHP, I might have a designated "once-ifying" process. All kinds of possibilities, but I can't see how to write a generic one for all cases without making the big assumption that I've got direct access to IO, and also without knowing the _exact_ concurrency guarantee(/exact definition of "run_once") you're looking for. (For instance, the MVar solution has the problem where you may have the following sequence: 1. Start shutdown. Shutdown nears the end. 2. Another shutdown process begins, starts checking for other running shutdowns. 3. First shutdown stops. 4. Second shutdown gets around to checking if a shutdown is running, it isn't, it commences shutdown number 2. But if we're just talking about shutdown, this may never be possible for other reasons and you may not care. Also, depending on the exact meaning of "gets around to checking" you may not care, or it may even be exactly what you want, for instance if instead of 'shutdown' we have 'wants to own a socket' that sequence works fine as long as the process is checking and claiming ownership before it does any socket operations.)
By Fred Brooks' definition of "essential", _nothing_ can remove it. If you removed a bit of complexity with powerful abstractions, by definition it was accidental. It may certainly be easier to write a GUI in Haskell than other languages, but no matter what you do, if you really want a _good_ GUI it's _hard_. Sit down and make a list of everything single thing that GTK does, and just coming up with the _list_ is pretty hard. Heck, let me spot you just coming up with everything that, say, the text widget does. Still pretty hard. Even some sort of purely declarative specification is still left with tens of events to respond to, tens of default behaviors (with ability to override), numerous graphical states, several layers of abstraction doing rather complicated things like "layout" (which may be easier to write in Haskell but still _has to be written_), and so on and so on. It would be neat, it might even be fun, but to be a competitive widget toolkit is _hard_. (To be a non-competitive toolkit is much less hard, but there's that nasty "non-competitive" adjective in there.) (Meanwhile, what is our "noncompetitive toolkit"? Probably one with no resolution independence, one hardcoded encoding with poor glyph handling, crappy layout managers if not actually manual layout, quirky scrollbars if not actively buggy scrollbars, poor performance, and frankly I could go on. Like I said, I've seen these in other languages. And one that, if anything, is _harder_ to use than GTK because it has fewer sane defaults, as many of the "sane defaults" are actually quite complicated behaviors themselves. It's hard.) I don't want to discourage anybody, but I don't want anybody to go into a task like this thinking it's easy, either.
I only meant single thread scenario. if you hide a mvar/ioref in funcion's closure (usually one function creates that ref, then defines and returns the resulting function which first checks the value of reference, and updates it sometimes), it still works more or less like redefining functions at runtime, because its behavior depends on mutable state that is only accessible from the function itself. that said, I still have no idea how to create a gtk button that prints if it was clicked once or more times, without such tricks.
Single-threaded run\_only\_once is basically a no-op. Either two shutdowns can't interfere with each other because they're pure (somehow...), or they're already sequenced, probably in IO. (Or you've _really_ contorted yourself around into some sort of cooperative-timesharing continuation-based thing, in which case, well, why did you do that...?) &gt; I still have no idea how to create a gtk button that prints if it was clicked once or more times, without such tricks. Errr, then you've got way bigger problems than anything this cute trick can solve. Your GUI app is going to need a great deal more than one bit's worth of state. This is on the level of a "hello world" tutorial, not deep magic. Maybe there's a hypothetical state-free GUI toolkit that theoretically exists out there in Platonic space, but for now, if you're using GTK, you're using state anyways.
Link is broken - there is a space at the end right before the g. Working link: http://www.galois.com/blog/2010/05/12/tech-talk-developing-good-habits-for-bare-metal-programming/
I'm really interested in seeing this. Someone needs to record it and put it online.
I registered just to say, I too am interested in this talk. Someone please make this available to those of us who can't attend.
any one know anything about this? 
I'm curious what the ultimate aim is in doing something like this, I think visual programming languages are wrong because visualising what a program does should be something you internalise not externalise by expressing it in some form of a picture language.
I found [this](http://hasp.cs.pdx.edu/)
Here's an outline of a possible alternative solution that recruits familiar mathematical tools: A stream corresponds to a formal power series `f(x)` with coefficients in the stream's underlying domain. The head is `f(0)`, the tail is `(f(x) - f(0)) / x`, and `a` consed onto `f(x)` is `a + x f(x)`. With this translational dictionary, you can write a stream equation as an equivalent formal power series equation; the `x` denominators are cleared by multiplying through by them on both sides. 
If I had a monad for every time I said that...
* This is the ultimate debugging tool (not sifflet specifically but its concept can be made the ultimate debugging tool) * Some people may find this form of presentation more intuitive. It's best to allow for different forms of representation. (i.e to give the option to view textual style code too) 
I think it will be most useful on mobile devices, there it isn't very comfortable to write the code down using the keyboard
If you can post these videos it would be great since these talks are public anyway.
Can you explain why you think that?
isn't it intuitive to form an internal picture when learning or understanding something rather than rely on an external representation? I don't seem to think in terms of clumsy trees or graphs, some details are near imperceptible or given less attention than is often found in a visual programming language. My comment isn't particularly directed at Sifflet, I think I was just more interested to see if there was an accompanying paper and what the thesis of that paper was.
Sifflet means whistle in French. 
...I imagine you'd have a lot of monads. And one monad to bring them all and in the darkness `&gt;&gt;=` them.
So you're saying that when you visualize program behavior, the behavior is not as detailed/well-defined as when it is laid out in program source code, right? But I don't see why visual expression necessarily has to be less precise. It's certainly possible to describe trees and graphs visually with as much precision as in code. Perhaps we have different ideas of what visual programming languages could entail.
I get what you are saying and disagree with the person who modded you down (as I write this). But, counterargument: In Haskell the graph is actually _not_ deceptive, due to imperceptible details, which you mentioned, or implicit state flow, which you did not, on average because there's usually no because the details are all there and there's (usually) no implicit state flow. I still tend to agree with you about it not being all that great a mental model, but I'm prepared to be proved wrong.
Edit: Removed bitching about wide divs
Hit "Ctrl+-" several times.
how do these problems translate to haskells STM? One problem about STM (and optimistic locking) I've always wondered about: For example we may have 2 processes both accessing a TVar. The first one does reading only and the second one reads and modifies it. So what will happen, if the later one reads the variable and is stopped before storing the modified value into the TVar while process 1 just reads the TVar and returns the (now invalid value) to it's caller?
Heh. I didn't even think of that. I'm used to using Readability on poorly sized articles, but I never resize my pages by hand. Wow, I feel stupid.
&gt; If you removed a bit of complexity with powerful abstractions, by definition it was accidental. Quite so, but this depends on time: the abstraction may not have been discovered yet. An example would be Haskell IO before and after monads. Actually, Haskell itself is an example, compared to languages like C. What is essentially complex today may become dead simple tomorrow. But yes, GUI kits are not easy to implement, there's lots of stuff to be taken care of. I don't think the many options and variations are a problem for the user, though, they can be understood incrementally. The real problem, and that's where I think an FRP abstraction may shine, is to find a clean way to specify the functionality / interactions between different GUI elements and the data they represent.
The main problem cited here doesn't exist in Haskell. You can't accidentally introduce side effects into your transactions, so no chaos is possible. Separatley, contention issues are the main thing to consider when using STM -- how much contention will there be?
I'm a little saddened to see this: I may only develop relatively small-scale applications most of the time, but I have never had any difficulty finding examples where replacing locks with transactional logic would simplify my code and remove scope for errors. (Edit: Almost all of these examples are to do with having a nicely structured domain model, rather than dealing with large chunks of homogeneous data and embarrassingly parallel computations.) Something like STM as part of a grown up type and effect system would be top of my wish list for features in a mainstream development language that are not widely available at present.
I can: atomically (return (unsafePerformIO (putStr "hello" &gt;&gt; return "hello")))
That's one thing I was always wondering about: Will multi-threading tip the dynamic-vs-static-typing debate in the direction of statically-typed languages (with a type-system that knows whether a function has side-effects)?
Additional note: [don't install GHC 6.12.2](http://www.haskell.org/pipermail/haskell/2010-May/022066.html)
If the caller wants the value to be current for the entire computation then it must be inside the atomic block. What you just asked is equivalent to saying you lock, read a varaible, unlock and want that variable to remain correct.
Of course you can -- but you can't **accidentally** do it -- which is the whole point: you have a clear proof obligation when you introduce unsafePerformIO.
1) unsafePerformIO isn't part of Haskell. 2) You didn't introduce any side effect in the STM monad - you made it return a routine that has a side effect and stripped away the IO tag. I.E. you didn't execute the putStr routine in that code. 3) Even in Glasgow Haskell you have to do something blatantly dumb, as you just tried to do.
unsafePerformIO should be renamed to iAmNoLongerUsingHaskell.
STM isn't in the Haskell standard either. unsafePerformIO is simply the easiest example---you could do the same thing with unintentional use of shared buffers, FFI'd code, etc.
All of which must be explicitly enabled. That's the point. They're *off* by default. As you know.
Nothing about the type signature of md5sum, e.g., tells me anything about whether it's implemented using pure Haskell, unsafe operations, the FFI, some combination, or none of the above.
If it doesn't say anything then you're allowed to assume it's pure. Any internal impurities must be "verified" safe by the person making the explicit choice to circumvent the type system. If you don't trust the person, then you shouldn't be running his without verifying it for yourself anyway. At least you know that *if* it's using impure code, then the person writing it must've explicitly chosen to take on the proof obligation of making sure it's actually pure. It's not just a free-for-all where there is no static distinction between pure (actually pure, or merely "promised" to be pure by a programmer explicitly taking on the obligation to ensure it) and other code. 
Only if the static type checkers get a lot smarter. Right now .NET's compiler can't even deal with nullable reference variables.
Yup!
It may seem specious for someone to say to you, after you point out that an extension can be used to break STM, that this extension is not properly part of Haskell. What does that mean in practice, where `unsafePerformIO` can be anywhere? At one level or another, we have to bootstrap a language on some little programs about which we offer certain assumptions. The implementers provide us some side-effecting operations that are notionally pure -- like laziness itself -- and these are the little programs and assumptions we start with. The type checker assures that we took reasonable steps in assembling these little programs to make bigger programs. If our assumptions are correct, our programs are correct. This is as much "fault isolation" as "program verification" -- we know a lot of places the problem isn't. The Haskell compiler can not prove our assumptions; and in fact, our assumptions are provisional anyways. We might say, machine addition is externally pure given that this or that assembly instruction does not launch the missiles. But why can't I wire up an Intel chip such that addition launches the missiles? No reason I can't do that -- but it doesn't mean we have gained nothing. We can cleave the world in two by stating our assumption. Just make sure you're on the right half of the world. If the programs are many and the assumptions are few, fault isolation has gained us a great deal. With `unsafePerformIO`, end users are able to introduce new assumptions. That is good and necessary. It is possible to introduce new assumptions all over the place but that is generally not done. In fact, a new assumption about something that's pure generally gets a whole branch of the module hierarchy to itself. What we're striving for with typed functional programming is machine aided program verification. The machine can't verify everything -- we've known that for a long time -- and sometimes we turn the verifier off. Just the same, we get a lot of aid -- far more than is presently available for imperative programming. 
You can find powerful scrolls of wisdom about the One Monad in the first entry of this newly recovered [spellbook 15](http://themonadreader.wordpress.com/2010/01/26/issue-15/).
It's worth pointing out that there is perhaps a use case for an option to make these assumptions even more strongly verified, and there's a proposal to do so in GHC. http://hackage.haskell.org/trac/ghc/ticket/1380
Um, don't all Haskell Monads have to have a 'fail' function that gives mzero? Or, what would be mzero, if MonadZero were its own class? This would be very useful to some, being able to split out MonadZero (and thus the 'fail' function) from Monad. 
I'm unsurprised to see this. It's always been obvious to me that the way they were trying to do STM -- grafting arbitrary code into transactions -- was going to fail. I suppose one could have hoped that the obvious thing was incorrect here, but I didn't have a lot of hope for it. It's worth pointing out that Haskell's STM solves basically all of the problems that they point out there, with the exception of one. That one is the lack of a compelling use case. There's promising evidence about scalability, but still to the best of my knowledge no really significant concurrent application that is obviously correct with STM and would have been prohibitively hard to write with MVar. That might just be my lack of experience, but it jives with what the article says, too. Still, having a working technology that is showing promise and looking for more uses is a good space to be in. My personal suspicion is that to really get into more widespread use, STM is going to have to make a case in terms of making the programming model more consistent with external transactional data systems. There is a HUGE problem with programmers screwing up database transactions because the application code all runs, in essence, outside the transaction. That's a definite issue that STM can immediately solve; all that's needed is to shoehorn the database two-level commit stuff into the STM infrastructure. Then programmers will have more flexibility to move certain things like caching into the application, while leaving them transaction-consistent.
&gt; would have been prohibitively hard to write with MVar that's the key. We use STM at Galois, but it is only marginally easier to develop than an MVar solution, and the STM is usually slower.
&gt; Something like STM as part of a grown up type and effect system would be top of my wish list for features in a mainstream development language that are not widely available at present. The simplest solution is for Haskell or a Haskell dialect to become a mainstream development language.
&gt; shared buffers, FFI'd code, etc. All of which involves `IO` actions, not `STM` actions, unless you cheat as you did before.
I don't know if this is compelling, but it's a fact. A while back, I had a crack at concurrent unification, where variables are represented mutably, and the like-constructors case splits into a bunch of concurrent subproblems. The MVar version was irredeemably banjaxed because the occur-check was not atomically combined with the actual instantiation of variables: one could unify (Node (Var x) (Var y)) (Node (Var y) (Node (Var x) (Var x)) by solving x:=(Var y) and y:=Node (Var x) (Var x), both of which would pass the occur-check, then disastrously commit. STM solves this problem at a stroke, by making the occur-check-and-solve combo atomic. Most of the time, there's no interference between problems, so no big deal. Should I have made a louder noise at the time? The old old code is here, if anyone cares. http://www.e-pig.org/idle/ctm/STMUnify.lhs I must confess I didn't do anything convincing like measuring how fast things went. I just thought the STM abstraction made my brain hurt less.
Ah, type checking! Didn't Sulzmann write a paper on his use of STM in the Chameleon compiler?? Here: http://www.cs.mu.oz.au/~sulzmann/publications/chr-stm.ps 
Well after my efforts. Oh well. Another one down the pan.
But there are many things you can't do with MVars, like block on two at the same time, or block until the contents satisfy a predicate.
This is very promising. Although I don't think it is ready for release yet, and I'm not sure whether they support LLVM code or only use LLVM.
In the meantime there's always that haskell -&gt; JS compiler.
What is it with people trying to put everything and the kitchen sink in the browser? If I want programming languages, video players, games, CAS, maps, and whatever, I'll download dedicated applications. Everyone quit shitting up the web! This is an order!
HTML &amp; CSS is one of the worst domain specific languages for specifying layout I've ever seen...
Running native code in the browser. What could possible go wrong?
I was talking with somebody who actually uses HJScript the other day, and it's not a Haskell -&gt; JS compiler, it's a Haskell EDSL to generate JavaScript.
Google disagrees.
MSIE + ActiveX worked out so well that Google wants to repeat the experience.
They might have been referring to the YHC js backend: http://www.haskell.org/haskellwiki/Yhc/Javascript
They sandbox it and enforce three rules on the resulting assembly code which they have proven in a paper to be secure. I forgot what the rules were exactly, but one of them was something like: no deferencing of pointers outside a certain scope.
Well, downloading/installing a desktop app and alter uninstalling it is too much of a hassle if you could just fire up the browser use it right away.
You're kidding, right!? Did you just thaw out of a glacier? Sometimes it's easier to understand something by solving the problem yourself than by staring at someone else's solution. So here's the problem: You're Google, and you want to supplant Microsoft by moving the OS to the cloud. How do you do this? For bonus points, notice Apple's market cap moving past Microsoft as you worry about this, figure out why and respond.
I think there was one about not injuring humans, or allowing them to come to harm through inaction.
Unless there is a newer paper I am unaware of, "proven" is far too strong a word. "claimed" it to be secure is much more like it - I see no semantics for x86 ASM, no formal model of the relevant operating systems, and no proofs.
You are probably right, proven is the wrong word. Here is there [paper](https://docs.google.com/viewer?url=http://nativeclient.googlecode.com/svn/data/docs_tarball/nacl/googleclient/native_client/documentation/nacl_paper.pdf), for reference. * C1 Once loaded into the memory, the binary is not writable, enforced by OS-level protection mechanisms during execu- tion. * C2 The binary is statically linked at a start address of zero, with the ﬁrst byte of text at 64K. * C3 All indirect control transfers use a nacljmp pseudo- instruction (deﬁned below). * C4 The binary is padded up to the nearest page with at least one hlt instruction (0xf4). * C5 The binary contains no instructions or pseudo-instructions overlapping a 32-byte boundary. * C6 All valid instruction addresses are reachable by a fall- through disassembly that starts at the load (base) address. * C7 All direct control transfers target valid instructions. 
I think it looks much nicer when the lines are not aligned at characters, so I generated HTML+CSS to center them after rendering: import Data.List import Data.Ord import Language.Haskell.Extension import Text.XHtml main = print $ (style &lt;&lt; css :) $ map (p&lt;&lt;) $ map (\s -&gt; "{-# LANGUAGE " +++ s +++ " #-}") $ map (thespan &lt;&lt;) $ sortBy (comparing length) $ map show knownExtensions css = [ "body { color: #DDD; text-align: center; font-family: monospace; }" , "p { margin-left: auto; margin-right: auto; width: 50em; }" , "span { color: black; }" ] Sadly, this program doesn't need any of these extensions itself. Maybe that would be a nice exercise for an introductory Haskell tutorial: "Write a program that outputs a list of the names of all known language extensions sorted by length, using all of them". ;)
Also, does the GHC LLVM backend spit out a big self-contained ball of LLVM goodness, or does it still need to link against a Haskell run-time library written in C?
I really like the DFA example.
It seems from the article that Google has added native code bindings, but the LLVM support is just a feature request. Am I right?
They never sandboxed those, that was the stupidity. I'm more worried about high processor load and stuff like that.
Could you elaborate? what's better? Even Visual Studio has implemented HTML-like layout options you can use instead of the default.
Interesting, if not for the link to an interesting paper. As an aside, I tried to prove the lemma "10 * 0.999... - 9 = 0.999..." with a short Haskell program, but (not being a Haskeller) I stumbled upon the fact that sharing is not purely observable (I found this obvious after the fact, but I felt disappointed nonetheless). 
here's formalized x86 semantics (part of x86, no fp): http://proofos.sourceforge.net/doc/ 
so all you need is one-click system to download,install,run and uninstall apps. absolutely impossible!
&gt; I see no semantics for x86 ASM See [TALx86](http://www.cs.cornell.edu/talc/releases.html). x86 asm was formalized over 10 years ago.
&gt; sharing is not purely observable Maybe see http://www.ittc.ku.edu/~andygill/data-reify.php (which tries to say when it is safe). &gt; Andy Gill, Type-Safe Observable Sharing in Haskell, Proceedings of the 2009 ACM SIGPLAN Haskell Symposium, Sep 2009.
I am thinking of the way in which [Lout](http://en.wikipedia.org/wiki/Lout_%28software%29) specifies layout. (Not descriped on the wikipedia page, follow the references.) It's like TeX' box model, but simpler and cleaner. Also, HTML &amp; CSS being bad does not depend on the existence of a good alternative. ;-)
Example: if I want a quick Google Earth view of something, I go to google.maps.com and have it right in the browser (granted, plugin needed, but this would be made obsolete then). Much better than installing it, launching it, and then uninstalling it, even if each of those take only one click (which is highly unprobable anyway).
That's the paper linked to in the comic.
Oh, oops :-)
&gt; You're Google, and you want to dominate the operating system market by taking control over everyone's data. How do you do this? FTFY
Even if the run-time is written in C, you can run it through llvm-gcc or Clang and get a big self-contained ball of LLVM goodness.
when you go to google.maps.com you download code from that domain, you run that code and then the code is removed when you close tab/browser. when you avoid using the word "install" it sounds much better doesn't it?
Well, if it depends so much on the browser like you say. than this is basically the same as native code in the browser.
Can someone explain?
Wow, a comic strip about Haskell coding. It's like xkcd on crack.
The point is that the Native Client paper failed to ascribe to any formalization and prove their work or verify it though other non ad hoc means. I didn't claim there is no formalization of x86 in existence.
The explanation that you comprehend is not the true explanation. &gt; Once [Zhuangzi dreamt he was a butterfly](http://en.wikipedia.org/wiki/Zhuangzi#The_butterfly_dream), a butterfly flitting and fluttering around, happy with himself and doing as he pleased. He didn't know he was Zhuangzi. Suddenly he woke up and there he was, solid and unmistakable Zhuangzi. But he didn't know if he was Zhuangzi who had dreamt he was a butterfly, or a butterfly dreaming he was Zhuangzi. Between Zhuangzi and a butterfly there must be some distinction! This is called the Transformation of Things. data Dreamer = Zhuangzi | TheButterfly deriving (Show) data Dream = DreamsOf Dreamer Dream instance Show Dream where show (n `DreamsOf` d) = show n ++ " dreams he is " ++ show d dream z b = let d = z `DreamsOf` (b `DreamsOf` d) in d Who is the dreamer, and who is the dream? &gt; let cycle = dream Zhuangzi TheButterfly &gt; show cycle "Zhuangzi dreams he is TheButterfly dreams he is Zhuangzi dreams he is TheButterfly dreams he is Zhuangzi dreams he is TheButterfly dreams he is Zhuangzi dreams he is TheButterfly dreams he is Zhuangzi dreams he is TheButterfly dreams he is Zhuangzi dreams (...) To perceive the true nature of things, one must look in from beyond the dreaming. import GHC.Vacuum &gt; nameGraph $ vacuum cycle [("DreamsOf|0",["Zhuangzi|1","DreamsOf|2"]), ("Zhuangzi|1",[]), ("DreamsOf|2",["TheButterfly|3","DreamsOf|0"]), ("TheButterfly|3",[])] The naive student asks, "Is not the cycle the true nature of all things? Surely, all that is must eventually return whence it came." But it is not so! For that night, the student dreamt of butterflies... data Dreamer = AStudent | AButterfly deriving (Show) nightmare s b = s `DreamsOf` (b `DreamsOf` nightmare s b) &gt; let cycle = nightmare AStudent AButterfly &gt; nameGraph $ vacuum cycle ...but never again awoke. ^CInterrupted
Ahh, I forgot about that. Good call.
Personally, I'm having some problems regarding networking and Windows. My code, which works fine in linux, takes a socket and converts it to a handle with line buffering (socketToHandle, hSetBuffering). Then it loops around hReady to see if there is incoming data while possibly sending data out onto the port. This apparently is blocking under Windows. I've tried putting "GHC-Options: -threaded" in my cabal file and running with "+RTS -N2 -RTS" to no avail. This link is the closest thing I've seen to a solution. Isn't there a better [cross-platform] way? 
&gt; "The poet Hoha once dreamed he was a butterfly, and then he awoke and said, 'Am I a man who dreamed he was a butterfly, or am I a butterfly dreaming he is a man?'" said Lobsang, trying to join in. &gt; &gt; "Really?" said Susan briskly." And which was he?" &gt; &gt; "What? Well... who knows?" &gt; &gt; "How did he write his poems?" said Susan. &gt; &gt; "With a brush, of course." &gt; &gt; "He didn't flap around making information-rich patterns in the air or laying eggs on cabbage leaves?" &gt; &gt; "No one ever mentioned it." &gt; &gt; "Then he was probably a man," said Susan. &gt; &gt; -Terry Pratchett, in his novel "Thief of Time"
This should be entirely portable. Sounds like a library or runtime bug. Can you file a report?
Sure. I hesitated because I wasn't sure if I was just making a mistake of my own. Edit: [submitted](http://hackage.haskell.org/trac/ghc/ticket/4078).
No, in that this is actually nerdy and obscure. xkcd is a pop-culture imitation of nerdiness. It uses Internet memes everyone knows as a stand-in for actual subcultural in-jokes. It strokes the egos of programmers and scientists and mocks everyone else -- this from a cartoonist who sells T-shirts for a living and doesn't do the coding for his own website. It's basically the *The Big Bang Theory* of webcomics. 
&gt; So ...1111111 has many of the properties we expect of -1. Added to 1 we get zero and squaring it gives 1. It is -1 in the 2-adic integers. This gives us a new insight into twos complement arithmetic. The negative twos-complements are the truncated last n digits of the 2-adic representations of the negative integers. We should properly be thinking of twos-complement numbers as extending out to infinity on the left. This way of thinking is essential if you want to support the familiar bitwise operations on arbitrary-precision integers. You can verify in GHC that `testBit (-1 :: Integer) n` is `True` for all values of `n` you care to try. Taking this approach means that all the usual bit-based identities of two's complement arithmetic like `-x == 1 + ~x` continue to hold. That particular identity is useful for finding the bits of the two's complement representation of a negative integer in sign-magnitude form, as in `Data.Bits.testBit` for `Integer`: To test whether bit `n` of `-x` is set, you test whether bit `n` of `x - 1` is _not_ set. That is readily answerable for the usual multi-word representation. When you subtract 1 from a positive integer, the least set bit becomes unset and all the lower unset bits become set. You can do this efficiently by first finding the least nonzero word, decrementing that using machine arithmetic, and setting all the lower words to a saturated bit pattern (i.e. -1 to word precision).
NaCL grew out of MIT's [vx32](http://pdos.csail.mit.edu/~baford/vm/) project IIRC. Don't recall the degree of formalization they sought or achieved.
Has integer multiplication really gotten that fast? I would have expected movl 4(%esp), %eax subl 8(%esp), %eax movl %eax, %ebx lshl 3, %eax subl %ebx, %eax ...or perhaps `gcc` has decided it just can't afford the extra register?
&gt; Has integer multiplication really gotten that fast? Yes. On most i386/amd64 CPUs, you can issue at least one multiplication every other cycle. It's certainly faster than a long dependency chain of other instructions (as in your example).
The other distinguishing feature between the two is that one is humorous and well-drawn (and the other, less so).
That code multiplies by 7, not divides. 
While there's some interesting math there, it's hardly necessary to understand why multiplying a 32-bit integer by -1227133513 is the same as dividing it by 7. It's pretty much elementary number theory with the extended Euclidean algorithm to show that a has a multiplicative inverse in Z/(n) iff gcd(a,n) = 1, and the Euclidean algorithm gives it to you. In particular, in this case, it gives 3067833783, which is the same as -1227133513 when treated as an unsigned integer. inverse :: Integral a =&gt; a -&gt; a -&gt; a inverse q 1 = 1 inverse q p = (n * q + 1) `div` p where n = p - inverse p (q `mod` p) And then &gt; fromIntegral (inverse (2^32) 7) :: Int32 -1227133513 That handles dividing by any odd number. To divide by an even number, you could first divide out the 2s (e.g., with a bit shift), and then divide by the remaining odd number.
Issue rate is only an indicator of speed when you have excellent ILP. IMUL latency on Core 2 is 3 cycles, so if the result is consumed by a long serial chain with no ILP, what he wrote could be faster in terms of total chain latency by one cycle, I think. But that depends on the mov/shl/sub triple being decoded in a single 4-instruction phase.
Not directly -- but you can still build that on top of them, no?
Is the novel completely free from inconsistencies? If it were not, would the characters be capable of perceiving them?
There are a number of technical issues in the way: * http://news.ycombinator.com/item?id=1251408 * This looks promising: http://google-opensource.blogspot.com/2009/06/introducing-android-scripting.html * Another idea: http://marblemice.blogspot.com/2010/04/android-java-and-c-well-haskell.html Can we run native code apps? If so, we're in business. GHC on ARM/Mips/x86 should all work.
ekmett: you coming to hacphi again? would love a short talk on how this works.
Okay, fair enough. Just statements like "I don't know how gcc generates its approximate 2-adic reciprocals" strike me as odd. GCC isn't generating approximate 2-adic rationals. It's generating completely precise multiplicative inverses in Z/(2^32). I suppose it's possible (though extremely unlikely) that it uses a convergent sequence in the 2-adic numbers to get there... but the difference still matters: they'd then need to prove they get an appropriate error bound to guarantee that the result is the same as the actual multiplicative inverse in Z/(2^32), since that is what they really wanted.
That's what I'm missing then. You said "It's not like working with the reals where approximate results are just, well, approximate results." But it sure looks like that to me! You are just (with a bit of handwaving over how to know when to stop) continuing your approximation to the point where you can be sure it will give the right result when truncated to an integer in Z/(2^32). That's very similar to how I *could* compute fibonacci numbers by a formula involving the golden ratio, and checking that I have enough precision that I get the right integer answer after rounding.
Definitely!
My hope is that there will soon be an Android / NaCl fusion. This would allow us to build apps for Android in the most wonderful way possible. 
&gt; So here's the problem: You're Google, and you want to supplant Microsoft by moving the OS to the cloud. How do you do this? Ask Netscape + Sun how well things worked out with applets and the whole network is the computer thing. Less snarkily, what do you think are the key differences with Google's approach? Was the sucktitude of applets the core reason people did not take to running apps in the browser? Will Google's tools allow for more robust behavior? Are there fundamental issues of trust? Right now (and really, this is not new) you can write desktop apps that can interact locally but ultimately save off everything to the cloud. You could do these things in Java and make it cross-platform (the big win for browser-based apps). But I this has yet to become a popular mainstream idea. If it does, count on Microsoft ensuring that it gets baked into their apps, all talking to BizCloud or Cloud.Net or something. 
&gt; The native code generator was started later to avoid these problems. It is around 2-3x quicker than the C backend and generally reduces the runtime of a Haskell program by around 5%. In my experience, the C backend consistently generates faster code than the NCG. Based on the standard advise to use "-O2 -fvia-C -optc-O3", I suspect others have seen the same improvement by compiling with GCC. Even the article's numbers indicate that the C backend is much faster than NCG. &gt; hoping to **depreciate** the C backend Man, *seriously*!?
Sort of and yes, respectively. If time shatters, and some of the characters try to reassemble it but know they have failed to completely smooth over the inconsistencies, well, you'll have to decide for yourself what the answers to your questions are. However, if you're asking about the traditional "time travel" inconsistencies, no time travel (of significance) occurs in that particular book. Another book does happen to involve a time travel event as a result of the events of Thief of Time, though. (However, don't get too excited about either that or the description above; time's shattering is in the past from the POV of the reader, and the time travel event is just a set up for the rest of the story. Get excited because these are really good books, and are otherwise fairly full of the sort of hacker-esque humor that is reasonably well correlated with an interest in Haskell.)
When LLVM will be in GHC? I mean - 6.12.x, 6.14.x, "as soon we consider it done"...?
6.14.x
I'm not sure it is - try writing a function that takes either of two MVars, but not both, and if interrupted with an exception takes neither (not even temporarily, because then another putMVar could intervene).
Well, I certainly haven't experienced that. I long ago took -fvia-C out of the .cabal for (u)vector-algorithms because it made no discernible difference. And that's entirely messing around with imperative loops. Does GCC only generate better code for pure stuff?
Well, individual experience isn't statistically significant but when I benchmarked my libraries its dead even, and as the article cites nofib shows a slight win for NCG. And yes, they have been planning on depreciating the C backend for a while to save on maintenance. Makes sense. EDIT: Anticipating any comments wrt the nofib suite, comment-time would probably be best spent submitting more benchmarks that reveal performance gaps. The article also mentioned this is happening partly as a response to the planned depreciation.
It's signed "david", is his full name anywhere there? Dude, make your name more emphasized and take some credit! :-) Thanks for the great work.
Nothing so deep (and I have resolved not to read any of Discworld after the first, on the theory that it can't be healthy to laugh so much). Just the usual defects in characterization, characters taking too long or not long enough to go from one place to another. The sort of thing that editors are there to catch.
Interesting. How can we help these people write more Haskell?
NCG was definitely faster in the binary-trees benchmark, too, fwiw.
http://www.cse.unsw.edu.au/~davidt/
Well, I'm trying, but no-one was keen on the Hubris session, apparently :/ I guess that gives you the subset of the people interested in both...
Good point.
Do this mean there is more of a chance to get SIMD support in GHC? Things like what data parallel array/vector libraries could use, utilize SIMD instructions.
What's up with all you unsw people? It makes me wish I had gone to a better uni.
To be honest, they get less "funny" pretty fast and become genuinely strong books, strong characters, strong stories, strong themes, etc. I almost stopped after the first few because I wasn't really looking for that sort of thing, but they rapidly turn into solid fiction.
Can you be more specific about your problems? Here's how I set myself up on most of my Ubuntu machines: first, I install GHC through the Ubuntu package manager. Then I download a recent GHC (e.g. 6.12.1 at the moment), do ./configure &amp;&amp; make &amp;&amp; make install. That gives me a recent GHC (this step is not always necessary, but I like to use the latest compiler and keep it upgraded myself). After that, I grab the latest cabal-install from Hackage; there is a script in the tar file called bootstrap.sh which I run. That downloads and installs everything necessary for cabal-install, and finally installs cabal-install itself. At that point I'm nearly done; I check that ~/.cabal/config has user-install: True (I think it's the default anyway) and library-profiling: True (saves a world of pain later on if you do this immediately), and thereafter I use "cabal install foo" for any and every package that I want.
The word is "deprecate", _to express disapproval, to urge against_. While "depreciate" might have similar connotations in _to belittle_ it's really do to with a reduction in value. This is what the OP was getting at. :-)
I'd like to be more specific, but I usually dive in when I have the time and generally take few notes. Here is the output right now: cabal install gitit Resolving dependencies... [1 of 1] Compiling Main ( /tmp/HUnit-1.2.2.15810/HUnit 1.2.2.1/Setup.hs, /tmp/HUnit-1.2.2.15810/HUnit-1.2.2.1/dist/setup/Main.o ) Linking /tmp/HUnit-1.2.2.15810/HUnit-1.2.2.1/dist/setup/setup ... [1 of 1] Compiling Main ( /tmp/HaXml-1.13.35810/HaXml-1.13.3/Setup.hs, /tmp/HaXml-1.13.35810/HaXml-1.13.3/dist/setup/Main.o ) Linking /tmp/HaXml-1.13.35810/HaXml-1.13.3/dist/setup/setup ... [1 of 1] Compiling Main ( /tmp/cautious-file-0.1.55810/cautious-file-0.1.5/Setup.lhs, /tmp/cautious-file-0.1.55810/cautious-file-0.1.5/dist/setup/Main.o ) Linking /tmp/cautious-file-0.1.55810/cautious-file-0.1.5/dist/setup/setup ... [1 of 1] Compiling Main ( /tmp/filestore-0.3.4.15810/filestore-0.3.4.1/Setup.lhs, /tmp/filestore-0.3.4.15810/filestore-0.3.4.1/dist/setup/Main.o ) Linking /tmp/filestore-0.3.4.15810/filestore-0.3.4.1/dist/setup/setup ... [1 of 1] Compiling Main ( /tmp/ghc-paths-0.1.0.65810/ghc-paths-0.1.0.6/Setup.hs, /tmp/ghc-paths-0.1.0.65810/ghc-paths-0.1.0.6/dist/setup/Main.o ) Linking /tmp/ghc-paths-0.1.0.65810/ghc-paths-0.1.0.6/dist/setup/setup ... [1 of 1] Compiling Main ( /tmp/pandoc-1.5.1.15810/pandoc-1.5.1.1/Setup.hs, /tmp/pandoc-1.5.1.15810/pandoc-1.5.1.1/dist/setup/Main.o ) Linking /tmp/pandoc-1.5.1.15810/pandoc-1.5.1.1/dist/setup/setup ... [1 of 1] Compiling Main ( /tmp/recaptcha-0.15810/recaptcha-0.1/Setup.lhs, /tmp/recaptcha-0.15810/recaptcha-0.1/dist/setup/Main.o ) Linking /tmp/recaptcha-0.15810/recaptcha-0.1/dist/setup/setup ... Configuring regex-pcre-builtin-0.94.2.1.7.7... Preprocessing library regex-pcre-builtin-0.94.2.1.7.7... running dist/build/Text/Regex/PCRE/Wrap_hsc_make failed command was: dist/build/Text/Regex/PCRE/Wrap_hsc_make &gt;dist/build/Text/Regex/PCRE/Wrap.hs Configuring sendfile-0.6.1... Preprocessing library sendfile-0.6.1... running dist/build/Network/Socket/SendFile/Linux_hsc_make failed command was: dist/build/Network/Socket/SendFile/Linux_hsc_make &gt;dist/build/Network/Socket/SendFile/Linux.hs Configuring unix-compat-0.1.2.1... Preprocessing library unix-compat-0.1.2.1... running dist/build/System/PosixCompat/Extensions_hsc_make failed command was: dist/build/System/PosixCompat/Extensions_hsc_make &gt;dist/build/System/PosixCompat/Extensions.hs cabal: Error: some packages failed to install: ConfigFile-1.0.6 depends on HUnit-1.2.2.1 which failed to install. HUnit-1.2.2.1 failed during the configure step. The exception was: ExitFailure 127 HaXml-1.13.3 failed during the configure step. The exception was: ExitFailure 127 MissingH-1.1.0.3 depends on HUnit-1.2.2.1 which failed to install. cautious-file-0.1.5 failed during the configure step. The exception was: ExitFailure 127 filestore-0.3.4.1 failed during the configure step. The exception was: ExitFailure 127 ghc-paths-0.1.0.6 failed during the configure step. The exception was: ExitFailure 127 gitit-0.7.3.6 depends on unix-compat-0.1.2.1 which failed to install. happstack-data-0.5.0 depends on unix-compat-0.1.2.1 which failed to install. happstack-server-0.5.0 depends on unix-compat-0.1.2.1 which failed to install. happstack-util-0.5.0.1 depends on unix-compat-0.1.2.1 which failed to install. highlighting-kate-0.2.6.2 depends on regex-pcre-builtin-0.94.2.1.7.7 which failed to install. pandoc-1.5.1.1 failed during the configure step. The exception was: ExitFailure 127 recaptcha-0.1 failed during the configure step. The exception was: ExitFailure 127 regex-pcre-builtin-0.94.2.1.7.7 failed during the building phase. The exception was: ExitFailure 1 sendfile-0.6.1 failed during the building phase. The exception was: ExitFailure 1 unix-compat-0.1.2.1 failed during the building phase. The exception was: ExitFailure 1 One of issues popping up is the unix-compat; that seems to be an issue for all the programs I've attempted to install
I fear we've lost that fight. I'm now shocked when I hear or see someone use the term "deprecate" _correctly_. That's usually a sign that the language shift has passed the point of inevitability.
If you dont mind the learning curve, Arch Linux seems to be the best Linux distro for haskell. My haskell OS experience, in order of wonderful to windows listed below: OSX &amp; Arch - tie. The reason that OSX doesn't win hands down is because of what a clusterfck the lack of 64-bit support in 6.10 caused in Snow Leopard. With 6.12, they still don't support 64-bit on OSX but at least the tool chain is fixed so everything works fine. Arch is also good, I still can't get any of the AUR tools working right though so I mainly use cabal for packages. ---- Fedora 11 - Needed to use it because thats the only distro that supports the piece of sh1t video card on my piece of sh1t netbook (GMA 500 poulsbo). Downloaded the prebuilt 6.12 binary (since RPMs for the HP with 6.12 only exist for F13) then built the HP from source. No problems there. ---- And, in dead last place - Windows anything. Never get anything to work right. libcurl - I'm looking at you in particular. Avoid at all costs. Sorry NDM, but at the moment its true. 
yes, we can, with NDK
It's not just my individual experience -- for example, [Real World Haskell's chapter on profiling and optimization says that using the C backend can generate better assembly than the NCG](http://book.realworldhaskell.org/read/profiling-and-optimization.html#id679553).
Looking at your logs and taking a pretty blind guess, I'd suspect that you're missing some standard pieces of the dev toolchain -- header files for external libraries (such as pcre) in particular, but also perhaps lots of other stuff. In general, you need autoconf, gcc, make, etc. etc. Do you have all that up and running?
Why not? You already have a cross platform UI rendering engine (the browser) and a sort of DSL (javascript) to program it, for free. Why not make use of it? With good haskell abstraction libraries to generate the html and javascript, it might be a very easy way to get together GUI's. It will not be as handsome as a hand-crafted GTK2 application. But most applications don't need to be that rich.
leaving coffee thermos in class seems to work
Wouldn't that be ironic?
You may be interested in running the `haskell` script in http://bitbucket.org/danderson/builders/src . It's the script I use to bootstrap a Haskell development environment on all my machines, which currently means an ubuntu hardy, an ubuntu karmic and an ubuntu lucid. It: * installs necessary ubuntu packages, * installs the appropriate GHC 6.12 binary distribution for your CPU architecture, * configures Cabal to build all libraries with profiling variants (future-proofing for development) and to generate documentation for all packages you install, * install enough packages "by hand" to get cabal-install built and running, * installs the Haskell Platform packages via cabal-install, Everything is installed to ~/software/install . You'll want to `source paths.sh` in your shell before running the builder (and thereafter to use it). That script adds the stuff under ~/software/install to relevant environment variables, so that software installed there will take precedence over any system stuff. Unless you find a bug I haven't hit yet (this stuff scratches my own itch, nothing more), you'll end up with a working GHC 6.12 Haskell Platform, all configured and ready to go. Also of potential interest is the xmonad builder, although it just runs `cabal install xmonad xmonad-contrib`, relying on the haskell builder to provide a working Haskell universe. Hope this helps a bit, if not to automate it all away, then to show you the steps required to get a Haskell Platform running on Ubuntu systems.
Nope. Programming the web browser is much harder than programming something in, say, wxHaskell. It certainly isn't easier, but in addition, you get all the problems of keeping session state etc. It's not more useable either. [Sage](http://www.sagemath.org/) has a browser GUI. It's horrible. I don't see why I have to start a web server, log in at my own computer, unable to work with an ordinary document and always having to fear that my browser will crash.
did you start by installing the haskell platform? are you using the most current version of cabal-install? try apt-get install dev versionf of libpcre and zlib when doing cabal install do cabal install -v3 to get verbose info on where it fails are you lucid lynx? I am, and gitit is definitely installable there there is also an ec2 image with gitit installed, see patch-tag blog.
If I'm shocked when I see it used correctly now, do you really think this is the only instance of this I've seen? Or _heard_? I'm not picking on anyone. Part of being descriptive and not proscriptive is that you have to go with the flow on things like this, even if it sort of bothers you.
you've obviously never tried doing anything in Graphviz. christ almighty it's terrible...
- just the haskell that was required by xmonad, i downloaded and installed 6.12.2 earlier - 0.8.2 cabal-install - dev versions installed - on lucid - v3 returns a lot more information: it seems to flake when it gets to zlib: Preprocessing library zlib-0.5.2.0... Creating dist/build/Codec/Compression/Zlib (and its parents) ("/usr/local/bin/hsc2hs",["--cc=/usr/bin/gcc","--ld=/usr/bin/gcc","--cflag=-D__GLASGOW_HASKELL__=612","--lflag=-lz","--cflag=-I/usr/local/lib/ghc-6.12.2/bytestring-0.9.1.6/include","--cflag=-I/usr/local/lib/ghc-6.12.2/base-4.2.0.1/include","--cflag=-I/usr/local/lib/ghc-6.12.2/include","--cflag=-I/usr/local/lib/ghc-6.12.2/include","--lflag=-L/usr/local/lib/ghc-6.12.2/bytestring-0.9.1.6","--lflag=-Wl,-R,/usr/local/lib/ghc-6.12.2/bytestring-0.9.1.6","--lflag=-L/usr/local/lib/ghc-6.12.2/base-4.2.0.1","--lflag=-Wl,-R,/usr/local/lib/ghc-6.12.2/base-4.2.0.1","--lflag=-L/usr/local/lib/ghc-6.12.2/integer-gmp-0.2.0.1","--lflag=-Wl,-R,/usr/local/lib/ghc-6.12.2/integer-gmp-0.2.0.1","--lflag=-lgmp","--lflag=-L/usr/local/lib/ghc-6.12.2/ghc-prim-0.2.0.0","--lflag=-Wl,-R,/usr/local/lib/ghc-6.12.2/ghc-prim-0.2.0.0","--lflag=-L/usr/local/lib/ghc-6.12.2","--lflag=-Wl,-R,/usr/local/lib/ghc-6.12.2","--lflag=-lm","--lflag=-lrt","--lflag=-ldl","--lflag=-L/usr/local/lib/ghc-6.12.2","--lflag=-Wl,-R,/usr/local/lib/ghc-6.12.2","-o","dist/build/Codec/Compression/Zlib/Stream.hs","Codec/Compression/Zlib/Stream.hsc"]) running dist/build/Codec/Compression/Zlib/Stream_hsc_make failed command was: dist/build/Codec/Compression/Zlib/Stream_hsc_make &gt;dist/build/Codec/Compression/Zlib/Stream.hs /usr/local/bin/hsc2hs returned ExitFailure 1 cabal: Error: some packages failed to install: something there I need to look at, hmm. Thank you.
If you take over maintenance of the C backend I'm sure it will survive. Without infinite resources you have to prioritize, and I think they made the right decision. But the great thing about open source is that if you don't agree you can make your own decision. 
I'm talking about people using "depreciate" instead of "deprecate". I, personally, do not particularly care at all how my Haskell gets to assembler; I am grateful it can do so at all. I offer as further evidence in my "the fight is over" case the number of people here who didn't catch the spelling difference even when bolded. That little "i" just gets lost.
I just read it as "deprecate," (didn't see the 'i') because that's the right word, and assumed the point of bold was intended to emphasize that an important back end was being phased out. I'm not sure what fight you've lost, though. I still spell deprecate correctly. If the battle was over people's brains unconsciously correcting spelling mistakes when reading things quickly, and knowing from context which word is correct, that battle was probably lost thousands of years ago.
Leksah's looking better and better. Might even be enough to drag me out of emacs, at least for a while. There's experimenting to be done at least.
Enjoying the progress bar when you start it up. I still find the workspace/package system a little confusing. How do you add an existing cabalized project to Leksah for example?
First make a workspace (Workspace -&gt; New Workspace), then right click in the workspace pane and choose "Add Package" and select the .cabal file you want to add.
I was not aware of that. Anyway, I'd rahter have 32-bit 6.12 than 64 bit 6.10
Why have I never seen this advertised?
hmmm $ runhaskell &lt;(echo "main = print 5") *** Exception: /dev/fd/63: hFileSize: inappropriate type (not a regular file) so I suppose it wants a real file (this is ghc 6.10.1) 
Probably because I never advertised it. As I said, it's a set of scripts that scratch my own itch. It's most visible in the fact that it doesn't support OS X or Windows, since I don't use either system. That said, if you find them useful, then I'm happy. Patches are also welcome, though I can't promise I'll accept them :). I've also just noticed that a license is missing from that repo, I'll add one momentarily. For the record, you may use that code under the terms of 3-clause BSD, same as my Haskell code.
Thanks to RubyOnRails many people are familiar with Ruby already, so i'd expect the interest in a Ruby introduction to be low.
Well, what would be the appeal of a new tool to you? Perl got popular because it made text processing really really easy. PHP got popular because it made dynamic webpages programming really really easy. Ruby got popular because it made dynamic webpages and database programming even more easy. Haskell will get popular because it makes ______ really really easy. I really love Haskell but i don't see what its killer feature is - besides being purely functional with static-typing. Neither XML, database, webpages, GUIs or regular expressions are easy to get started with or are a breeze to use in Haskell, yet. Maybe Haskell's killer feature are EDSL and one should market that.
You just made me jealous of him.
Hmm, [here](http://developer.android.com/sdk/ndk/index.html#overview) it says: "Please note that the NDK does **not** enable you to develop native-only applications. Android's primary runtime remains the Dalvik virtual machine." 
&gt; makes ______ really really easy. Secure? Fast? Parallel?
And Ruby is much more similar to Perl, Python and other well known languages than Haskell.
* "Fast": Haskell code (at least without LLVM) does seem to be slower than C/C++ most of the time. * "Parallel": you need to annotate your code in a similar way, as you would annotate your code with OpenMP which would be faster than the Haskell version. Don't get me wrong: i love referential transperancy and side-effect free code, but i get less and less impressed with the fast/parallel hype. If you look at http://www.haskell.org/haskellwiki/Haskell_in_industry it seems that in the commercial world Haskell is mostly used to generate stuff or verify stuff. So it's rather used as a development tool than the original programming language to implement stuff. Why do you think that is?
&gt; "Fast": Haskell code (at least without LLVM) does seem to be slower than C/C++ most of the time. But an order or two of magnitude faster than Ruby. Haskell is more mature as a compiler design language, hence the compiler/language projects. Newer projects are aimed at performance/concurrency/multicore. See e.g. &gt; Starling Software are developing a commercial automated options trading system in Haskell, and are migrating other parts of their software suite to Haskell. Starling Software's experience building real time trading systems in Haskell http://www.starling-software.com/misc/icfp-2009-cjs.pdf The multicore stuff is ~5 years old. Pattern matching is 25 years old.
We're recording video today, and will make it available.
Haskell may not make the fastest programs (on uniprocessor), or have the shortest code (first-order approximation for productivity), but it is unique in combining excellence in most important categories. Haskell gives you the productivity from the Python/Ruby world, with the speed (nearly) from the C/C++ world, with reliability/robustness much higher than either. 
Does it still take excessive time to startup given a large dev directory? I tried it back when I had a flat ~/dev directory that included several kernels - it would have taken way too long to startup so I made a sub ~/dev/Haskell directory just because of Leksah.
Order of magnitude faster than ruby is probably an understatement. But "faster than ruby" is like "safer than php" or "less verbose than java".
Well the metadata implementation has changed completely. You don't really need to add any directory any more. It still takes some time when you start Leksah for the first time, depending on how many packages you have installed and how quick your machine and your internet connection is. To speed up the process, Leksah can now download prebuild metadata. If you are lucky its just 5 minutes at the first start.
Correct?
NDK basically lets you embed native parts in your SDK apps. The SDK app is still what gets started when you run an app, and I think the NDK parts even need to interface with the SDK for certain system calls (it's pretty backwards. not done it myself but a friend explained it to me). So I bet you could make an SDK app that served as a generic interface between a Haskell NDK app and the OS.
I wonder if you could make a Dalvik backend to llvm? Google doesn't seem to have much on the subject. Could solve a bunch of problems, though, for languages who want in on Android. EDIT:typeo
This is hot shit! I would like to see a justification of compile-on-keystroke though.
do you have all these packages? thartman@ubuntu:~&gt;dpkg -l | grep -i zlib ii zlib1g 1:1.2.3.3.dfsg-15ubuntu1 compression library - runtime ii zlib1g-dev 1:1.2.3.3.dfsg-15ubuntu1 compression library - development ii zlibc 0.9k-4.1 An on-fly auto-uncompressing C library I'm on 6.12.1 fwiw (not .1, not .2). You may need to do export LD_LIBRARY_PATH=/path/to/needed/dir/where/so/files/are:$LD_LIBRARY_PATH before running your cabal instal, if the needed .so library is installed but for some reason cabal install isn't picking it up. I had to do this for hdbc-odbc. 
...oh. =/
There were two people at the Haskell session you said they had signed up for the Hubris talk. Of the other folks, I don't think any were doing Ruby. Some Java, some Scala, maybe some Python. Meanwhile, I tell Ruby people that Haskell has many of the features that makes Ruby so Rubyish, and in some cases does it better. But while there is interest, there is a perception that getting as productive in Haskell as they are in Ruby would take, well, forever. Or at least more effort than they wish to apply. There are still assorted barriers to entry that makes people reluctant to just hack around with haskell they way they could when they heard about Ruby (or Python, or Go, or whatever).
It was empty! And I left it in (I think) a JQueryUI session. :)
It sometimes underlines errors before you would otherwise have noticed them. This can often highlight things you need to change when refactoring. It can sometimes be faster (since it may already have built some of your modules when you come to make your last change). If it slows your machine too much you can switch on GHCi mode on the toolbar (it uses :reload instead of compiling) or just switch of background building on the toolbar (or in the preferences).
But should I really hear about errors every keystroke? What was wrong with waiting until I was ready to compile? Also thanks.
&gt; I really love Haskell but i don't see what its killer feature is ... When I first got into Ruby, and would espouse its virtues to others, I got the same reaction. "It's nice, but [per|python|php] yada yada yada ..." Yet to those who take a liking to Ruby, it's qualitatively different from those languages. Thing is, you won't experience that unless you spend a bit of time with it. Now, Ruby ended up with as its big advertisement, enticing people to learn enough Ruby to build Rails apps. Is there, or will there be, a killer app for Haskell? &gt; - besides being purely functional with static-typing. *Cool* static typing. Not-java static typing. "Less work than the unit tests you need to replace it" static typing. I tried to convey this in my Haskell session. I don't see the type system as a flat-out replacement for unit tests, but I do see it as replacing *some* tests, and on the balance the effort needed to handle types n Haskell s arguably less then what is needed to create some equivalence in tests. I have zero data to back up that claim; it's a gut feeling. BTW, at the 2nd Ruby conference there was some discussion about what would be Ruby's killer app. As ideas were tossed about, there was a general consensus that it wouldn't be Web development because PHP had that locked up. So, one never knows how things will play out.
I'm sorry I didn't ask the attendees why they were in the class. I think that while Rails has raised awareness of Ruby, it doesn't make everyone run out and learn it. I'm guessing these people had heard about Ruby or Rails and decided to see what the language was about. About two years ago I gave an intro Ruby session at Desert Code Camp to a room of about 100 people. Perhaps in the intervening time Ruby/Rails saturation has set in. The Ruby community in Phoenix is pretty small, and has been so for the last 4 years or so that I've been running the user group. It seems to be mainly a Java/.Net/PHP town. Those people are *everywhere*. :) (All the more surprise at the interest in Haskell.)
Nothing. If that is what you want click on the toggle button on the toggle button on the toolbar (white square with a cog in the middle) and use the &lt;Ctrl&gt; B shortcut.
Make it snake-simple to get started. An installation should be easy to get, fool-proof to install, Just Work right away, and include tutorials and help files or references. In my session I advocated using code generators to jump start projects because I kept forgetting the syntax for a basic `main` function "Hello world" app. So I used a Ruby tool to kick off boilerplate code. One less thing I had to think about. After seeing that code a number of times I've gotten to the point where the generator isn't needed. But I'm lazy so I use it to save some typing. Real World Haskell is an important step forward, too. Prior to that I found most Haskell books too mathy and academic. When I have a hard time with something (git commands, for example :) ) I tend to collect simple examples for things I want to do but often cannot recall the right syntax or commands. Having lots of example Haskell programs for assorted common tasks might help people get over the speed bumps. What frustrates me when trying to do something is getting stuck on a side task that I have to puzzle out before I can move on to doing what I really want. I realize all these things are part of a learning process, but having a few things to smooth the path makes one more likely to stick it out. 
Seems like an awful lot of things are using type-level Peano numbers for type indexing. Since the idea of actually pulling data (including but perhaps not limited to ints) up into types is pretty obvious, I assume there must be some killer reason why that's a bad idea (in Haskell). I'm going to take a stab and guess a rapid descent into undecidability? Should I be googling "dependent types"? (Not sure if that's what this is, honest question.) Even if that's true, seems like maybe there's a feasible middle ground worth exploring. It seems very silly to be sitting there and literally defining arithmetic when we have a (for lack of a better word) "real" arithmetic sitting right here. And repeatedly doing so, no less.
http://hackage.haskell.org/platform/ ? and 'cabal init' to initialize a project.
Yeah, undecidable instances show up very quickly, mostly because type-level data often brings along with it some vacuous classes that are used to encode type-level functions. Type-level programming (including type level-arithmetic) is really handy with combinator libraries, though. In the case of the EDSL described in the blog, using type-level Peanos makes it possible to statically check that operands are of the proper size. By tracking this data at the level of types, Haskell's type inference is able to propagate the assumptions automatically; if these assumptions were implemented in the data-level, extra code would need to be written to accomplish this. I think other users of type-level arithmetic have similar concerns in mind.
Dependent types means having functions that bind terms as arguments and evaluate to types. Imagine something like this pseudo-Haskell: data Cons h t = h :*: t nList :: (n &gt; 0) =&gt; Int n -&gt; * -&gt; * nList 1 t = Cons t () nList n t = Cons t $ nList (n - 1) t So that'd be a function (which you can call at run time) that takes an integer greater than zero and a type of kind * and evaluates to the *type* of a fixed-length list of the given length. Yes, that makes type checking awkward. The axes on Barendregt's [lambda cube](http://en.wikipedia.org/wiki/Lambda_cube) are a rough taxonomy of features for typed λ-calculi. Haskell 98 is, I think, roughly some subset of System F; GHC with all extensions is probably some crazy subset of System Fω with lots of sharp corners. Adding dependent types to Haskell gets you basically everything, and thus something like the [Calculus of Constructions](http://en.wikipedia.org/wiki/Calculus_of_constructions), at which point type checking is nearly hopeless and you have to provide correctness proofs. Might as well just use Coq or Agda or something at that point... That said, much of the crazy pseudo-dependent typing people seem to do doesn't actually need full dependent types! When all values are known at compile-time, arbitrary code could be quietly translated to the type level and used to construct the types for the actual program. Also, dependent types alone added to the simply-typed λ-calculus aren't too bad, so perhaps some heavy restrictions on where dependent types could be used to keep them isolated from other features would be tractable. I suspect it's the potential for run-time cycles of types -&gt; terms -&gt; types ad nauseam that makes things tricky, which is avoided in both of the above. A lot of the pain of type-level metaprogramming is not actually lack of dependent types, it's GHC's type system being a terrible programming language with very limited expressive power short of enabling UndecidableInstances, which lets you (painfully) write almost anything you like. What I'd actually like would be an explicit type-level implementation of something like H-M (kind polymorphism with decidable inference) with some degree of automatic lifting of values known at compile-time to the type level.
Haskell 98 alone requires features of Fω, because the kind language has (-&gt;), not just *. Also, type checking the calculus of constructions is perfectly feasible (although if you add general recursion, you can write loops in the type system). Inference won't work in general, but inferring higher-rank types in F isn't possible either.
&gt; and 'cabal init' to initialize a project. I wish that were true. "cabal init" has way too many steps for such little gain. It doesn't even create a useful bare-bones app you can build and run. Where's Main.hs? Where's the README that tells me what I need to do next? I'll stick to Rhesus. Also, last I checked, Haskell Platform was not built to create shared objects. I've been trying to get Rubyists to play with Hubris, but it seems you have to build Haskell yourself to get it to do the shared object thing. (And I don't know if that's even possible on Snow Leopard.) And there are these comments (none of which has the posting date): http://davidsiegel.org/haskell-platform-in-karmic-koala/ I realize much progress is being made, and there is serious commitment to making things easier, but it's not as easy as, say, popping out a Rails app or a Ruby gem.
This isn't specified anywhere but usually for monads fmap = liftM pure = return (&lt;*&gt;) = ap For example [a] Applicative have only concat Applicative - not Zip.
hunh. I knew the first two, but not the third. thanks!
&gt; 1) unsafePerformIO isn't part of Haskell. No true Scotsman performs unsafe I/O!
I am not sure that comparing Haskell to another glue language (Ruby) is helpful. Thanks for the Starling paper, i've read it before but couldn't get many details from it. Most investment banks (Credit Suisse, Barcap) indeed fall into the code-generation-category though. My advice to your original question *"How can we help these people write more Haskell?"* would be: **Help them to get things done**: provide rich, easy to understand and use libraries that work out of the box.
&gt; August 6th, 2009 The current platform is [a lot easier](http://packages.debian.org/sid/haskell-platform).
I'll do that. Who said there's no support on FOSS eh? Have an appreciatory upboat.
doesn't the typeclassopedia mention that?
This is more showcasing multithreaded code in Haskell than STM specifically, is it not? There's nothing inherently transactional about what's going on. Instead you could perfectly well use regular MVars. withForking :: IO GroceryStore withForking = do a &lt;- forkJoin getTomatoesCountFromDB b &lt;- forkJoin haveFreshBerries c &lt;- forkJoin getNameOfCurrentStore GroceryStore &lt;$&gt; takeMVar a &lt;*&gt; takeMVar b &lt;*&gt; takeMVar c forkJoin :: IO a -&gt; IO (MVar a) forkJoin m = do var &lt;- newEmptyMVar forkIO (m &gt;&gt;= putMVar var) return var Don't get me wrong, it's a nice article, but I don't feel it's about STM specifically. It's more about how nice multithreaded code is in Haskell compared to the old synchronization primitives found in other languages. Now the problem with STM is really finding a use for it. When it comes to shared mutable memory I really do think it's the best alternative, but in Haskell shared mutable memory is a rare thing. It's much more common have immutable data shared through message passing channels (MVars are basically reuseable one-shot channels). I have encountered situations where shared mutable memory was the right thing to do, but not very often. In my experience it has always been as a cheap way to avoid overburdening one channel; every thread was responsible for doing it's own work and getting their own resources, and STM was used to facilitate taking multiple locks atomically.
sigfpe has lots of great stuff. I periodically trawl through the entire blog and I keep picking up new ideas and getting a fresh look on old ones. Wouldn't this monad be even more Trivial? data T a = T instance Monad T where return = const T (&gt;&gt;=) = const $ const T I wonder if there's any use for this one, though...
Type-level numbers are going to be the next big additional to general purpose languages IMO. See the [Habit](http://hasp.cs.pdx.edu/) language, a low-level Haskell dialect, to see how useful type-level numbers can be.
It seems like a lot of the Haskell "marketing" has been focused on the amazing things the language can do in regards to concurrency/parallelism. If you look at our steadily increasing core counts and how awkward some of these tasks can be in Java/.Net it makes sense. I'd suspect most of the people coming for that reason are hoping to learn something that will make their work life easier. Well, that or they're trying to make heads or tails of Linq and have heard it uses "monads", which according to the internet came from Haskell.
I don't know if I'd call that trivial so much as degenerate.
That is what I usually call the trivial monad.
I was talking about this with quicksilver this morning. It turns out that sigfpe's `W` (Identity) is initial in the category of monads over Hask, while `T` (which might be called Zero) is terminal.
The [docs for the Applicative class](http://haskell.org/ghc/docs/6.12.1/html/libraries/base-4.2.0.0/Control-Applicative.html#t%3AApplicative) do as well.
such is the nature of decomposition as a way to attack problem solving. A lot of time the best code I write is when I have the time to decompose a big problem into smaller composable chunks, and then go back and compose them. The value of this is not always well understood by others, or even myself until I find one of those famous "drifting requirements situations" where my boss says "Hey can you make it do X?". If the pieces are small enough, and self-contained (orthogonal), you can often answer "yes, and it's not very difficult". If you've hard coded all your routines, you often say "It'll be about 2 weeks!" Note this is true in all programming languages I've worked with. 
&gt; It's like zero. Well, it happens to be the monad for the constant functor, whereas yours is the monad for the identity functor, presenting an obvious analogy to the K and I combinators, which also happen to be the Church encodings for the numbers zero and one, respectively. So in a sense, it actually *is* zero. I'm not sure how well that analogy will hold up to any kind of rigor, though.
That's a good point. I think that STM would be useful for writing something like a connection pool, if you had to write one from scratch.
http://www.wolframalpha.com/input/?i=microsoft+net+worth+%2F+apple+net+worth Not why, but interesting
I'm working on Mozilla Firefox for Android ("Fennec"), which is an NDK app. (No, it's not written in Haskell - that's what I use for my hobby projects.) Unlike typical NDK apps, Fennec is almost entirely written in C++ (and JavaScript, using Mozilla's own Tracemonkey VM which in turn is written in C++). It uses a thin Android wrapper (about 1300 lines of Java code) that sets up an OpenGL surface for the C++ app to draw on, and passes user input between the app and the Android API. Something similar could work for OpenGL apps written in Haskell. If you wanted to write Haskell code that looked and felt more like "real" Android apps then you'd also need bindings to the Android API. My friend Koush has done this for C#/Mono using the Android NDK (which of course is a lot closer to Java than Haskell is): http://www.koushikdutta.com/2010/05/mono-on-android-state-of-union.html
I think an even greater benefit, is that these small, self-contained units are easy to understand, more likely to be correct, and more testable.
I don't know what happens with monads, but something mildly amusing happens with the category of applicative functors and their homomorphisms, where you have the similar situation that I, the identity functor, is initial and K (), the constant unit functor, is terminal. By initiality and/or terminality, there's a unique homomorphism from I to K (). Nobody faints with amazement. Except that anytime two applicative functors F and G have a homomorphism between them, h : F a -&gt; G a, they produce a baby applicative functor (F :+: G) with pure = Inl . pure Inl f &lt;*&gt; Inl a = Inl (f &lt;*&gt; a) Inl f &lt;*&gt; Inr a = Inr (h f &lt;*&gt; a) Inr f &lt;*&gt; Inl a = Inr (f &lt;*&gt; h a) Inr f &lt;*&gt; Inr a = Inr (f &lt;*&gt; a) That is, F is somehow 'purer' than G, and the plan is to stick with nice F computations until we encounter a nasty G computation and fall from grace along h. So, exploiting the hom we didn't pay for between I and K (), we get that I :+: K () (also known as Maybe) is applicative. Of course, hiding in that definition is the fact that Inl is a hom from I to I :+: K (), so you can go again... Trivial cases are like bunnies.
This monad can demonstrate that you can't take the value out of a general monad.
Doesn't C++ already allow this with integer template parameters? I looked over the Habit report and didn't really see anything you couldn't do in C++, especially with C++0x and constexpr. Unless there is some way to do meaningful numeric type-level program _without ever instantiating a specific number-indexed type_. I'm not quite sure what that would look like.
Habit permits all sorts of type-level arithmetic in a *convenient way*. C++'s templates are Turing-complete, so theoretically they could do anything any other type system could do, with the exception of proving their own termination.
Template parameters in C++ do allow for numeric encoding, but as I recall, the compiler will build separate code for each integer that is actually used, as the idea is that the code that gets generated is allowed to depend on the template value. In type-level programming, one is more interested in performing symbolic calculations as a way of introducing additional things that need to be checked before a value can be used. Frequently this is done polymorphically, so that the code which gets generated doesn't depend on the type-level values. C++ templates (to my knowledge) lack this symbolic aspect.
I'd be interested in that, too. Awhile back, I made a proposal to that effect over in haskell_proposals: http://www.reddit.com/r/haskell_proposals/comments/7jyfn/library_to_provide_access_to_sse_instructions/
Hi :) Haskell newb here. I have had a quick look at your code and I wanted to see if I could adapt it to achieve the following: 1. make a channel 2. spawn a thread to read once from that channel, print the result (block until it reads) 3. spawn a thread to write once from that channel 4. exit My code blocks indefinitely, and I can't figure out why. I guess what is happening is the the read thread is created, but somehow the channel is never read? If I switch the two forkJoin calls it runs as expected. Any help would be greatly appreciated. module Main where import Control.Concurrent main :: IO () main = do ch &lt;- newChan takeMVar =&lt;&lt; forkJoin (readChan ch &gt;&gt;= putStrLn) takeMVar =&lt;&lt; forkJoin (writeChan ch "test") return () forkJoin :: IO a -&gt; IO (MVar a) forkJoin task = do mv &lt;- newEmptyMVar forkIO ( task &gt;&gt;= putMVar mv ) return mv
Completely OT, dons, but you've kinda-sorta destroyed the subtlety of the OP's title. You really can't "lay the foundations for the Foundation".
Sign-up is broken (again)?
Looking into it. Though the form (http://nfa.imn.htwk-leipzig.de/hal5/) shows up for me. What is broken?
Have a look at any of the bindings on hackage for examples - or the source to gtk2hs too, probably!
I didn't even notice the "Sie sind angemeldet.Bestätigung anzeigen." appearing through JavaScript. Clicking it however yields "keine Anmeldung mit Code [...]"
If you write with your real name to Johannes Waldmann, then he can tell you whether or not you are indeed registered successfully.
I've done a bit of FFI work, I'll assume you already know how to do basic FFI imports. if you want to map a C POD-type structure to Haskell basically just mirror the structure using a record type as closely as possible, make it an instance of [Foreign.Storable.Storable class](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Foreign-Storable.html). To implement poke/peek You need to know the byte offset of members. You need be careful with alignment and byte padding (in C you have *offsetof* standard library macro). Look at the [Foreign.Marshal.Utils](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Foreign-Marshal-Utils.html) in particular look at the function [with](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Foreign-Marshal-Utils.html#v%3Awith) which is quite useful when implementing higher-level functions over low-level FFI imports. As you can tell when it comes to things like byte offsets this becomes quite ugly and platform/compiler specific so you should look at using tools like c2hs and hsc2hs (you probably want to use both). They can help you write more portable Storable instances. Have you seen the [RWH chapter on FFI](http://book.realworldhaskell.org/read/interfacing-with-c-the-ffi.html)? Here is some examples with c2hs from [my code](http://github.com/snkkid/HDirectX/blob/master/src/HDirectX/Direct3D9/Structs.chs).
A new version of gtk2hs is going to be released some time soon. I think gtk2hs is great. The old version 0.10.1 works fine on Linux and Windows.
I haven't looked at RWH properly yet. I actually bought it but haven't got around to reading it. Thanks for the tips.
Have you tried [WxHaskell](http://haskell.org/haskellwiki/WxHaskell) instead?
I've repeatedly tried compiling it on Linux, Windows and Mac OS X and have gotten nowhere. It always blows up in a series of compiler errors.
Also have a look at: [bindings-DSL](http://hackage.haskell.org/package/bindings-DSL).
You should at least read the table of contents. I came in here to answer your question by pointing at the RWH chapter.
`readChan` blocks until there is something to read. Your code spawns a thread to read from the channel, then waits for its completion (`takeMVar` blocks until some thread did a `putMVar`) before spawning a thread to write into that channel. *Edit*: try something like this: main :: IO () main = do ch &lt;- newChan v1 &lt;- forkJoin (readChan ch &gt;&gt;= putStrLn) v2 &lt;- forkJoin (writeChan ch "test") takeMVar v1 takeMVar v2 return () 
I belive that taking look at qthaskell (http://qthaskell.berlios.de/) might be better experience... 
Note that a new cabal installable wx-0.12.6 has just been [released](http://sourceforge.net/mailarchive/message.php?msg_name=1274450248.2954.1376226825%40webmail.messagingengine.com)
&gt; As I understand it since GUI programming is inherently stateful this shouldn't be too big a loss. GTK, Qt, Cocoa, etc. may all be stateful, but I personally do not believe that GUI programming is necessarily so.
Ahhh, doh! Thanks!
From darcs or the release? The current release doesn't work on GHC 6.12. Compiling the new cabalized version is much easier.
qthaskell is licensed under the GPL, which could be a problem. I know it is for me :)
Suppose you want to allow your EDSL---call it dsl---to be able to run either with a script as an argument, or from a shebangable script. How to do that? Note that running a dsl script (i.e., one with a dsl shebang line) causes dsl to be run and passed the path of the script as its first argument (see [here](http://homepages.cwi.nl/~aeb/std/hashexclam-1.html)). So your interpreter needs to accept a script path as its first argument, without requiring a -f or any other option.
If anyone (else?) es having problems, please write to waldmann at imn dot htwk minus leipzig dot de.
This goes in a relatively different direction - but have you though about using one of the Haskell web frameworks and doing an AJAX-style GUI? Unless you need something relatively specific, Javascript (with something like prototype/jQuery/ExtJS in the background) is quite a cool way to get the interaction going, and the only thing you need to do then is figure out how to shovel the data back and forth using JSON/RJSON.
code or it didn't happen
Unfortunately, the code you wish for me to introduce does not yet exist, and it would take some time to create a new GUI toolkit. Edit: Downvoted? Seriously? For not wanting to do something that would take years to get right?
Nice link. I've been wanting to write a library for embedding inference rules into Haskell for a while for animating structured operational semantics. This paper should give me a few ideas!
so it really didn't happen:) and it's not about another gui toolkit, users want qt (or gtk), to blend with the rest of the desktop.
Then you are changing the definition of "GUI programming" to "GTK or Qt programming," so you are making demands not relevant to my point.
I toyed with the idea of doing this about a year and a half ago. Glad I didn't waste my time. This looks amazing.
A small list of ugly denglicisms and errors on that page: "domainspezifische Sprache", "domainenspezifische Sprache", "sicher stellen lassen". Organizers: please remove that in order to not sound silly. :)
Via [Lamba: the Ultimate](http://lambda-the-ultimate.org/node/3957)
What about using it as the backend of the ncg? Metacircularity is fun.
The darcs version of gtk2hs works on ghc 6.12.1 on Linux. It worked for me about a week ago. It's getting near to being released, so the chances of it working are quite high. You need to 'cabal install' in the tools directory first, then type ./bootstrap.sh. The old version 0.10.1 works fine on Linux on GHC 6.10.4. It does *NOT* work on ghc 6.12.x. The binary release, which requires GHC 6.10.3 works fine on Windows Vista. Also I successfully built it on Windows with GHC 6.10.4 but I needed to patch it a little. In both cases you obviously need your -dev packages for GTK installed on your OS, but it sounds like you know all about that bit. If you've got time to spend on UI bindings, I urge you to contribute it to an existing project.
I have a sudden urge to write GHC's garbage collector in typed assembly language.
This looks fantastic. This can be our Rails. Great work.
Oddly enough I've been working on something just like this for a while now for my machines, though it's a bit higher level. I'd like to be able to write commands like "surface(some curve)" or extract geometry from dxfs or whatever, and have some higher level language with which to control my machines. Right now I'm working on cutter compensation and pruning, as well as trying to figure out how I'm going to simulate the final cut. Computers don't seem to enjoy boolean operations on solid bodies too much... as well with weird arbitrary swept volumes... and finding those swept volumes is a whole nother can of worms. There are methods of ray tracing... voxel representations, some stuff kinda in between... really interesting application of computational geometry, really. The nice thing about having a haskell DSL is that you can do funny things like writing subprograms that will generate involutes for you... so a part might import CNC.Involute or something.
[I agree](http://www.reddit.com/r/programming/comments/ajztk/followup_to_functional_programming_doesnt_work/c0i06jk) (there's some stuff on FRP near the end of that wall of text). Some of Conal's work shows some options for non-stateful GUIs, but there's still much work to be done. I think anyone saying that anything is "inherently" anything should have to provide a damn convincing argument. Not being able to imagine an alternative to the conventional approach is not a proof that no such alternative exists, or that it isn't better. Oh, and "code or it didn't happen" is precisely the kind of close-minded programmer attitude that pisses me off. It's implicitly saying that if it hasn't already been written, it's not worthwhile to even think about. Or that innovation can't occur. Grr!
This is way more awesome than the title implies. I'd go for: "Assembly with compile time guarantees!" 
Nice site and nice benchmark (I wonder why the headline doesn't mention it's fast...)! The quickstart guide could be linked from the main page, not just from the main text of the download page.
 site = route [ ("" , ifTop (writeBS "hello world")) , ("echo/:s" , echoHandler) ] &lt;|&gt; fileServe "." echoHandler = do req &lt;- getRequest writeBS $ maybe "" id (getParam "s" req) Indexing parameters via _strings_ at _runtime_? Also, sendfile/splice support is missing, but it's arguably the iteratee package that doesn't deliver.
So about this 'fast'. Does that mean [these benchmarks](http://snapframework.com/benchmarks) are good when the bar is higher? (which I doubt since php would be leading otherwise). Don't get me wrong, 'fast' is not a bad goal too have but apparently it's already a feature. I just can't find a link to any info on how 'fast' it is.
Don't get me wrong, I don't exactly believe the benchmarks at this point in time, but PHP's reputation for being "fast" is largely undeserved.
I don't understand what those benchmarks are showing at all. Help, anyone?
Those are ByteStrings. We're using the OverloadedStrings extension. I'm not sure what you mean by "sendfile/splice" support. fileServe uses sendfile under the hood.
 site = route [ ("" , ifTop (writeBS "hello world")) , ("echo/:s" , echoHandler) ] &lt;|&gt; fileServe "." echoHandler = do req &lt;- getRequest writeBS $ maybe "" id (getParam "s" req) How is it with Haskell programmers, is it easy for you to read code like this? Even for such a trivial example, there is quite a lot of thinking required (for me). 
looks like requests per second. so higher bars are better.
Your pasted version is a little tougher due to screwing up the indentation, but overall I don't think this is too difficult, personally. Of course I don't *know* what every function means, but I can make a reasonable guess that the left side of `&lt;|&gt;` sets up some routing information and the right side is a fall-back of serving files out of the "." directory. (The `&lt;|&gt;` operator, by the way, is a standard overloadable operation in the `Alternative` type class, which might give you a strong hint about its function.) The routing itself has two handlers, one for the top level and one for "/echo/&lt;something&gt;". The top level just writes "hello world," and the other stuff defers to `echoHandler`. `echoHandler` itself just writes out information it scrapes out of the request. The length of my description is probably a bigger testament for the *density* of Haskell code than for its readability, actually.
&gt; Indexing parameters via *strings* at *runtime*? I'm pretty sure that parameter is indexed that way so they can use the printf style templates in the routing.
It's plausible to me, under the circumstances they lay out ("we don't know how to optimize these frameworks"). Bear in mind PHP is pretty optimized for both the cases being examined here, neither of which involve intensive computation. Apache could probably get a few more percent eked out though I doubt it would materially change much. RoR seems oddly small, though I suppose the answer there would be to serve static files through something like Apache. This benchmark may overstate the difference between RoR and Snap, since this is in a way the worst case for RoR; you're paying all the indirection and overhead and "meta" cost for RoR, then serving a static string. On the other hand, Haskell is going to be a much better language at taking all that indirection and meta and etc. and rewriting it all at compile time to be one straight chunk of code with minimal branching that all fits in even relatively modest code caches, so it may still be fair. (I'm interesting in the performance possibilities of Haskell web frameworks for that reason, and think that performance is probably a good goal to aim directly for; to get something as high level as Rail or Django on the development side, but that ends up with the kind of performance you can usually only get by bashing together strings in C(++), is a very interesting and feasible possibility. And possibility even better performance than the straightforward C(++), which will still involve a lot of bouncing through pointers and the resulting cache trashing....)
No problem, and I'm a shitty Haskell programmer.
I'm not sure I agree. PHP is end-to-end fast enough that it can be run as a CGI, which means programmers don't have to deal with the headache that is FastCGI or a persistent server process; empirically, you just can't do that with python or other languages, the setup time is too much. (This doesn't say anything of the performance of the language itself, of course).
Hey! great work. I'll try to play with it tomorrow. I just have one (probably dumb) question. Anybody knows what happens when you want to do IO by your own? Let's say I want to open a file by myself, or create a db connection.
...I should've looked at the source instead of inferring from the api. splice is how linux implements legacy sendfile under the hood, it supports zero-copy not only from file to socket but also socket to file, file to file, socket to socket and possibly more, all via a kernel pipe buffer. It could be used to speed up both large PUT requests as well as forwarding requests to another server.
Anyone who cares about running a real-world website would setup PHP as a SAPI module anyway. Most shared/dedicated websites already have it setup for you that way anyway. It's not terribly hard to set up either Apache modules or FastCGI; you spend an hour or two writing some config files and then you're done. The benchmark that matters is how these perform under "real world" server loads, i.e. a website with a few dozen routes, making a couple calls to backends or databases and then filling a template to output 50ish-K of data. I don't see that benchmark in these performance tests, but I'd like to.
I wasn't going to bind them. I was going to write the GUI purely in C or C++ and then call out to Haskell code that does the actual work.
The [Snap](http://hackage.haskell.org/packages/archive/snap-core/0.1.2/doc/html/Snap-Types.html#t%3ASnap) monad has an instance for MonadIO which enables you to lift any IO action into the Snap monad.
I didn't know you could _also_ write the instance of MonadIO. (Instead of using a stack of monad transformers over IO) I learned something new today. Thanks! 
This looks promising.
Yes, that pretty much summarizes one of my complaints about the current benchmarks. Certainly, these benchmarks aren't a bad thing, it's just that I don't know how meaningful they really are.
I just happened to be trying to learn happstack, and found it to be more of a collection of helper functions that you can use to make a webapp, rather than a framework for one. It's a bit complicated for me to pick up and use. At first glance snap seems a lot simpler than happstack to learn. So I'm hoping it'll be fast not just in terms of performance, but in terms of developing time as well. Will be trying this out!
Even so, it seems like a way to add some incorrectness to the program without a lot of benefit. Is TH sugar not practical here? Plus: &gt; If more than one value was entered for the given parameter name, getParam gloms the values together with: intercalate " " is kind of surprising I guess.
Also, the Snap monad is a stack of monad transformers over IO, which likely makes the MonadIO instance easier to write.
Also, dcoutts started a [thread](http://www.haskell.org/pipermail/libraries/2010-May/013659.html) suggesting changes to the data types in Data.Time to improve memory usage. That thread branched off into a discussion about the need for enhanced documentation for Data.Time.
That looks like blogspam. It's the same text as David's post on the LLVM project blog.
It has been created. More info on : https://patch-tag.com/r/alpmestan/hasklab/wiki/
Err, and the link?
Functional fun for fold fans :-) New paper, to appear in the JFP special issue on generic programming.
" empirically, you just can't do that with python or other languages, the setup time is too much. " Setup time for what? Starting the interpreter and executing a script? I've run plain CGI Ruby sites with little problem. I've not done benchmarks against PP CGI, and if you (or anyone) knows of any, I'd love to see them. But my empirical evidence says you can Ruby (and I'd bet Python and Perl) CGI apps just fine. Now, if your app needs to make DB connections or populate a slew of objects on each call, sure, that's am issue. But I don't see that it's any more or less worse for PHP or Ruby or whatever. As others have noted, though, few people need to run any of these as CGI since setting up an Apache module is pretty simple and common. But if you just want to toss up a plain CGI app any number of languages will do.
Perhaps could you sum up the basic idea in a few words? What is achievable using it? (the abstract didn't answer my question and it would be helpful for other readers of this thread, in any case)
http://workerwrapper.com might help
Great article, its incremental style makes it very easy to follow! Also, bolding the modifications in the code snipplets is a really nice touch. I would just add one thing, reading the article it is not really clear what composing two iteratees means (that is, upon completion of the first iteratee, the second one performs its calculation on the remaining elements of the stream). Keep up the good work!
It's about a special case of fold fusion that is simple but widely applicable -- the paper gives three examples to illustrate the utility of the idea, culminating in a proof of correctness of an efficient method of implementing monadic substitution.
I strongly disagree with the general direction of throwing the TH/quasiquoting/preprocessing/etc. tank at the simple problem of dispatching incoming HTTP requests. If it were actually a significant problem that was awkward to solve in Haskell, that would be one thing... but it's not a significant problem, and other Haskell web application frameworks like Happstack and Ella provide perfectly reasonable solutions using normal old Haskell terms. I mention Ella because it strikes me as unusually elegant and usable. Happstack's approach is perfectly adequate as well. Basically, this is a simple problem, elegantly solvable in Haskell, and there's no need to throw new languages at the problem. (That comment also applies to Snap. I still intend to read more about it, but I lost a lot of interest when I discovered that dispatch of requests uses this weird thing with assigning semantic meaning to special syntax inside character strings.)
I looked up Ella, and I agree that its approach would be perfectly sufficient. A TH mini-language would probably be overkill for the task, but I think that we agree that a run-time mini-language is underkill.
As you do more and more "real work" the overhead attributable to the web framework becomes less and less significant. Relative speed of Haskell vs. other languages is already well-known (alioth shootout, etc). By benching trivial tasks we are trying to measure the upper-limit ceiling of what each web framework is capable of. Not saying this rationale is ultimately correct (and the benchmarks are definitely not the most scientific) but this is what we were aiming for.
&gt; Also, sendfile/splice support is missing We do sendfile() actually.
I actually nearly put splice() support in last week. It's on the radar. But since vmsplice() doesn't do zerocopy anymore it's less attractive than it might have been.
This would be more interesting if they included release notes.
&gt; I strongly disagree with the general direction of throwing the TH/quasiquoting/preprocessing/etc. tank at the simple problem of dispatching incoming HTTP requests. +1 from me on this (obviously!). Re: the other poster's comment about "indexing parameters via *strings* at *runtime*??" (oh the horror), I'm sorry but I don't want to have to deal with a whole bunch of over-complicated template haskell/bondage &amp; discipline datatype bullshit just to dispatch an HTTP request. Call me old-fashioned. I'm sure you could decide to use something like web-routes if you wanted to. The other approaches I've seen are superficially appealing ("your url dispatches are statically checked, isn't that great!") except you have to do 10x as much typing and futzing around as just checking that the damned parameter isn't "Nothing". This is a matter of taste and we'll have to agree to disagree on it. &gt; (That comment also applies to Snap. I still intend to read more about it, but I lost a lot of interest when I discovered that dispatch of requests uses this weird thing with assigning semantic meaning to special syntax inside character strings.) This is really not that weird at all, it is pretty similar to what django does. And in my opinion it's more convenient than the alternatives I've seen. Also "getParam" is a convenience function, the intercalate behaviour is intended (obvs) and if you want a list instead there is a way to do that (liftM (rqParam "foo") getRequest).
I agree. The raw version of the story is online, though: [ghc trac 6.12.3](http://hackage.haskell.org/trac/ghc/milestone/6.12.3)
Err, your premise that Haskell has no advantage for "stateful" code is pretty flawed. Features like first-class functions and actions, polymorphism, and powerful, lightweight static typing are also useful for imperative, stateful code. If you look through the modules that come with [Haskell Platform](http://hackage.haskell.org/platform/) you'll see that a good deal of it has to do with stateful programming. To answer your question, it's been said before, but [the RWH chapter on FFI](http://book.realworldhaskell.org/read/interfacing-with-c-the-ffi.html) is excellent.
This is a really nice introduction... I am slightly bothered by the use of (Maybe String) and such to indicate errors... There's probably a nicer way to do that. Also, the chunking-for-performance sounds to me like it belongs in a higher layer. The lowest Iteratee layer should be more parameterized, and then Iteratees can yield a "rest-of-chunk" to each other, in addition to their values.
Bugfix release? &gt; Ticket status by bug 37 / 235 feature request 0 / 73 task 0 / 27
Don't worry, Ian has been writing release notes.
The title of the paper, in conjunction with the fact that the file name is f5.pdf, made me laugh. :)
Well now we know who dons' favorite Haskell hacker is.
It's true, but stock PHP and CGI can go amazingly far without it. FastCGI becomes a headache in distributed and multi-user environments; for example, many older implementations of FastCGI fail to commit seppuku when they notice the relevant source files have changed, and as a result users can no longer "edit a file; see changes instantly!" that they might have had with PHP cgi.
hahaha, well thought out... even better is if instead of faces it was different shaped lambdas talking to each other! :)
Any chance of bugs [3961](http://hackage.haskell.org/trac/ghc/ticket/3961) and [3983](http://hackage.haskell.org/trac/ghc/ticket/3983) being addressed before release? I find it a little worrisome when HUnit can't pass it's self-tests.
Is it Adam Wick?
Yes, I was thinking more of Haskell, Mercury, or ML, rather than .NET, Java, or C.
I don't understand the bit about 10 times as much typing. I pointed out Ella... it's very usable, and okay, you might save a couple keystrokes with Snap's string-based mini-language, but we're talking on the order about may 5 characters per handler, to close the quote and type a 3-4 character Haskell operator? Plus, in your handler, you get the accept the data as a parameter instead of an explicit call to getParam, so you get the keystrokes back. Honestly, I don't care about URL dispatch being statically checked. I mainly care whether I have to learn a new mini-language of colon-escaped terms in Haskell strings, or if I can use Haskell operators.
I seriously doubt languages like Haskell will ever become popular. They just don't have the right mix of syntax and features. ML evolved into F#, which is even worse than C# or Java with it comes to dealing with nulls. (It has at least three different types of null and no compiler checking for using them correctly.)
For a site selling a (pretty neat) web framework, the web design looks like it's from 1995. Snap seems to have set the standard for Haskell web design. *Edit*: I take back my comments. Must learn to refresh my browser cache, I was viewing an ancient stylesheet.
It was pretty clear to me that `return x` will never throw an exception.
There is also a similar project in http://code.google.com/p/phaskell/wiki/index edit: formatting
The snap site is great. So is the yesod framework.
Great, I'll look at how they do currying and other features my code's missing. Thanks for the link.
To mix it with "normal" C++, you can always just include the resulting .h file and use the template-level functions from there. Or am I missing something?
|Let's see how many Haskell projects will improve their sites. Otoh, Yesod looks quite nice to me providing many features out-of-the-box. Of course, we're curios to see Snap's higher-level stuff as well. ;) 
Well, that's the gist of what I'm wondering about: Will "extreme" multi-threading force us in the direction of Haskell? Multi-threading is hard, with shared mutable state by default, and no control of side-effects, there's much abuse and head-in-the-sand going on. We'll see...
&gt; Will "extreme" multi-threading force us in the direction of Haskell? I would say no because the Erlang model looks like it is going to win. In .NET, Java, C++, and ObjectiveC I see what looks like Erlang's message passing and "process" model. (I call it the Erlang model because that is the most popular example. Not all of these were directly inspired by Erlang.) 
This is a pretty common gotcha though, so glad to see it written up again.
Right, and ultimately you need both micro and macro benchmarks, but I would still like to see the macrobenchmarks. A lot of "real world" performance losses happen in things like template and database abstraction libraries and big-O factors in those and the route dispatching - these aren't covered by the language speed itself, and aren't covered by "hello world" framework microbenchmarks.
In my case, it's a little less obvious when you fmap a pure computation onto a monadic value. :^)
Correct me if I'm wrong but I think that the author was actually try to point out the difference between this: safeCall = do return alwaysFails `catch` errorHandler and this: safeCall = do evaluate alwaysFails `catch` errorHandler and that without either 'evaluate' or 'return' ghc assumes you mean 'evaluate'. 
Ah the dreaded server process space leak. It happens to the best of us, and also it happens to me :-)
Indeed. But the yesod site is not so great.
Adam is the roxors.
Well, I literally wrote: find . -name '*.cabal' -exec less {} \; and wrote down the names of non-core packages I saw.
What's wrong with it?
Default ugly times font and styles. Pale green background on black text looks rather ugly.
I get this error: Downloading glib-0.11.0... /tmp/glib-0.11.024194/glib-0.11.0/Gtk2HsSetup.hs:25:0: warning: #warning Setup.hs is guessing the version of Cabal. If compilation of Setup.hs fails use -DCABAL_VERSION_MINOR=x for Cabal version 1.x.0 when building (prefixed by --ghc-option= when using the 'cabal' command) [1 of 2] Compiling Gtk2HsSetup ( /tmp/glib-0.11.024194/glib-0.11.0/Gtk2HsSetup.hs, /tmp/glib-0.11.024194/glib-0.11.0/dist/setup/Gtk2HsSetup.o ) [2 of 2] Compiling Main ( /tmp/glib-0.11.024194/glib-0.11.0/Setup.hs, /tmp/glib-0.11.024194/glib-0.11.0/dist/setup/Main.o ) Linking /tmp/glib-0.11.024194/glib-0.11.0/dist/setup/setup ... Configuring glib-0.11.0... setup: gtk2hsC2hs is required but it could not be found.
I think you need to run cabal install gtk2hs-buildtools and you also need ~/.cabal/bin/ in your path (export PATH=$PATH:/home/username/.cabal/bin), as detailed in the [INSTALL](http://code.haskell.org/gtk2hs/INSTALL) file.
Thanks. The command: cabal install gtk2hs-buildtools &amp;&amp; cabal install gtk turned out to be all I needed. I should have read the detailed instructions, rather than believing the blog post that "cabal install gtk" was all that was needed!
I like it too. Makes me think to Andrew Loomis' Fun with a Pencil.
"Statefulness" and "Imperative" are very different things. I find the functional approach better for stateful code. Additionally, I don't see what you mean by "GUIs being inherently stateful". Take a look at Phooey or Fudgets...
What about Phooey or Fudgets, as counter-examples P.O.C's?
I was most alarmed to discover that dons hadn't already submitted this.
For the first problem, I think the solution is not to statically link the C lib, it's just not a good idea. The GNU C lib (which includes iconv) has a stable ABI and it explicitly says it does not recommend static linking. For the second problem, about lazy decoding and binary, yes we switched it back to being strict. The lazy decoding did not work out well. They were ad-hoc extensions that did not fit with the rest of the lib and made performance worse. The solution is to do the lazyness more explicitly. If the file is basically a big seqeuence of records then decode them one by one lazily. That is, run the strict record decoder `runGetState`, grab the tail of the input and lazily decode that. You can do it as an `unfoldr`.
It seems to require cpp on Windows.
GPL for a library?
However it should be pointed out that Haskell advocates lack of the stacktraces. There should be no partial functions which are easy to avoid (no implicit null values, so no NullPointerException) etc. There is also no mutable values or dynamic casting (well - ok. they are but they are also avoided.
It's wrong in the .cabal file. It's actually LGPL-2.1.
are you saying that stack traces are less useful in Haskell than in other languages? In my experience they are just as useful, because in real world Haskell: * there are mutable values * there are partial functions * there are plenty of other sources of programming errors
A "stack trace" is not a good tool in a functional language, because function calls play a fundamentally different role than in an imperative language. But what the author wants is a way to ask "where did this thing come from?" Perhaps it would be nice to have a debugging mode where you could ask an object at what line in your code its constructor was called.
Do other languages represent evaluation strategies in their types?
I spent a month or so hacking up an EDSL assembler in Haskell, but then Harpy came along and it seemed somewhat redundant, despite Harpy's limitations. This is neat stuff!
He sent me the screenshot, and it *was* ugly. It was an old cached stylesheet; apparently nearlyfreespeech doesn't do a very good job of setting those headers sensibly. I'll use a version parameter next time to avoid this.
&gt; A "stack trace" is not a good tool in a functional language, because function calls play a fundamentally different role than in an imperative language. In what sense do function calls in Haskell differ from function calls in Java which is relevant to debugging programming errors ? &gt; what the author wants is a way to ask "where did this thing come from?" isn't that what stack traces provide ?
I can't get it to build: $ cabal install gtk Resolving dependencies... /tmp/cairo-0.11.012968/cairo-0.11.0/Gtk2HsSetup.hs:25:0: warning: #warning Setup.hs is guessing the version of Cabal. If compilation of Setup.hs fails use -DCABAL_VERSION_MINOR=x for Cabal version 1.x.0 when building (prefixed by --ghc-option= when using the 'cabal' command) [1 of 2] Compiling Gtk2HsSetup ( /tmp/cairo-0.11.012968/cairo-0.11.0/Gtk2HsSetup.hs, /tmp/cairo-0.11.012968/cairo-0.11.0/dist/setup/Gtk2HsSetup.o ) [2 of 2] Compiling Main ( /tmp/cairo-0.11.012968/cairo-0.11.0/Setup.hs, /tmp/cairo-0.11.012968/cairo-0.11.0/dist/setup/Main.o ) Linking /tmp/cairo-0.11.012968/cairo-0.11.0/dist/setup/setup ... Configuring cairo-0.11.0... Preprocessing library cairo-0.11.0... Building cairo-0.11.0... Graphics/Rendering/Cairo.hs:8:0: error: cairo-version.h: No such file or directory /tmp/glib-0.11.012968/glib-0.11.0/Gtk2HsSetup.hs:25:0: warning: #warning Setup.hs is guessing the version of Cabal. If compilation of Setup.hs fails use -DCABAL_VERSION_MINOR=x for Cabal version 1.x.0 when building (prefixed by --ghc-option= when using the 'cabal' command) [1 of 2] Compiling Gtk2HsSetup ( /tmp/glib-0.11.012968/glib-0.11.0/Gtk2HsSetup.hs, /tmp/glib-0.11.012968/glib-0.11.0/dist/setup/Gtk2HsSetup.o ) [2 of 2] Compiling Main ( /tmp/glib-0.11.012968/glib-0.11.0/Setup.hs, /tmp/glib-0.11.012968/glib-0.11.0/dist/setup/Main.o ) Linking /tmp/glib-0.11.012968/glib-0.11.0/dist/setup/setup ... Configuring glib-0.11.0... Preprocessing library glib-0.11.0... gtk2hsC2hs: Errors during expansion of binding hooks: ./System/Glib/GObject.chs:107: (column 22) [ERROR] &gt;&gt;&gt; Unknown identifier! Cannot find a definition for `g_object_get_type' in the header file. cabal: Error: some packages failed to install: cairo-0.11.0 failed during the building phase. The exception was: ExitFailure 1 gio-0.11.0 depends on glib-0.11.0 which failed to install. glib-0.11.0 failed during the building phase. The exception was: ExitFailure 1 gtk-0.11.0 depends on glib-0.11.0 which failed to install. pango-0.11.0 depends on glib-0.11.0 which failed to install. On Ubuntu 8.04 on x86_64. I have installed e.g. libcairo2-dev and libglib2.0-dev
"... Haskell advocates lack of the stacktraces." I don't understand that. I am assuming Haskell does not have stack traces because it requires implementation effort and nobody has stepped forward to do it yet. Obviously you would want to have it switchable because it would slow things down and take up space. Are people against it because some functions that call themselves can currently run in constant space, but with a stack trace they would need space per call? 
&gt; The newtype definition for `N` is semantically equivalent to `D2` (the strict [sic] This is not really true. When you pattern match the constructor of `D2`, the `Int` is forced. Not so in the case of `N`. I've never really felt a need for stack traces in Haskell, so I can't sympathize with those complaints. I do think it might be nice to be able to express strictness and laziness in the types, though.
&gt; Are people against it because some functions that call themselves can currently run in constant space, but with a stack trace they would need space per call? That's one aspect of it. But theoretically, the idea that the "stack trace" is the thing you're looking for is not quite correct, it's just that you're used to that being the case in other languages. The nature of the stack in functional languages in general tends to be different. You might be interested to read [A call stack is not a history](http://calculist.blogspot.com/2006/03/call-stack-is-not-history.html), which gets at an aspect of this. 
"real world" haskell really can and should avoid partial functions most of the time. Use ndm's Safe library, and if you want additional power to set useful preconditions, use assert. Between the two, it helps quite a bit.
ARGH. *Must write clearer papers in future*. Elision in the Finding The Needle paper is mis-understood by the author. (Assuming, of course I've understood the example!) So, to clarify, given this: module Main where foo0, foo1, foo2, foo3, foo4 :: Bool -&gt; () foo0 x = foo1 x foo1 x = foo2 x foo2 x = foo3 x foo3 x = foo4 x foo4 False = foo0 True foo4 True = error' "Ha!" main :: IO () main = print $ foo0 False (Skipping a few details (like error' and an import) to make the experimental prototype work). &gt;./ghc -fexplicit-call-stack-all -fds-simple -fforce-recomp --make /tmp/Foo [1 of 1] Compiling Main ( /tmp/Foo.hs, /tmp/Foo.o ) Linking /tmp/Foo ... ╰16:36:23tora@colorado:~/msr-ghc/ghc-Stack/ghc/stage2-inplace♠╯ &gt;/tmp/Foo Foo: Ha! in error', /tmp/Foo.hs:19,14 in foo4, /tmp/Foo.hs:13,14 in foo3, /tmp/Foo.hs:11,10 in foo2, /tmp/Foo.hs:10,10 in foo1, /tmp/Foo.hs:9,10 in foo0, /tmp/Foo.hs:8,10 in foo4, /tmp/Foo.hs:12,14 ... in main, /tmp/Foo.hs:16,16 in main, /tmp/Foo.hs:16,1 Yes this trace doesn't show the first invocation of foo0...fooN, but only the ones nearest the error. However a property of the elision technique is that any function that would be mentioned in a full one, is mentioned in the elided one too.
Apologies. On re-reading, I mis-understood the example. Re-running a corrected version now. [edit:] module Main where foo0, foo1, foo2, foo3, foo4 :: Bool -&gt; () foo0 x = foo1 x foo1 x = foo2 x foo2 x = foo3 x foo3 x = foo4 x foo4 False = foo0 True `seq` error' "Ha!" foo4 True = () main :: IO () main = print $ foo0 False Gives: &gt;./ghc -fexplicit-call-stack-all -fds-simple -fforce-recomp --make /tmp/Foo [1 of 1] Compiling Main ( /tmp/Foo.hs, /tmp/Foo.o ) Linking /tmp/Foo ... ╰16:51:04tora@colorado:~/msr-ghc/ghc-Stack/ghc/stage2-inplace♠╯ &gt;/tmp/Foo Foo: Ha! in error', /tmp/Foo.hs:19,18 in foo4, /tmp/Foo.hs:12,34 in foo3, /tmp/Foo.hs:11,14 in foo2, /tmp/Foo.hs:10,14 in foo1, /tmp/Foo.hs:9,14 in foo0, /tmp/Foo.hs:8,14 in main, /tmp/Foo.hs:16,20 in main, /tmp/Foo.hs:16,5 
Sounds like your cairo package is too old. I have cairo-1.8.10 which includes /usr/include/cairo/cairo-version.h.
I really really wanted stack traces in numerous occasions... I think this is an area where Haskell really makes life much more difficult than it should be
Yeah, pretty much. Strict languages represent "strictness" in all their types, and if you want a lazy "a", you use something like () -&gt; a.
Well, calls in IO are very similar -- and it would be real nice to have an IO stack trace. Additionally, for the pure side, "where did it come from" could be really nice too.
I think "usually" is "always", at least semantically. A more optimized definition may look differently, but it must adhere to those semantics.
Yes, that's interesting about loops and function calls not giving the same things on the stack. I suppose, for the purpose of the stack trace, we could reasonably omit the function calls that have been zapped by TCO because they are effectively loops. Maybe that would be sensible also because it would not increase the space requirements. 
I believe I built the non-Cabalized version of gtk2hs on this machine. Is the higher cairo version only required by the Cabalized build?
It's not quite that simple. TCO can apply to any tail call (depending on the language and implementation), so it doesn't necessarily involve a loop. Tail calls to other functions are effectively jumps, and they leave no runtime trace unless the implementation makes a special effort, like the continuation marks that PLT Scheme uses for this purpose.
Nice work Noam! Interesting piece of trivia: Noam's first Haskell toying was hacking on GHC to warn about order-significant patterns! While the warning turns out to be impractical, it's still pretty impressive to have as your very first Haskelling...
Very nice that gtk2hs is now, apparently, completely cabalized and available from hackage! One note: The "haskell98" dependency on some of the packages looks very wrong. "haskell98" is a compatibility package for when you, say, import Maybe instead of Data.Maybe.
&gt; In what sense do function calls in Haskell differ from function &gt; calls in Java Because function calls are the mechanism for iteration, as well as almost every other kind of "flow control". Even the tightest loops are ultimately implemented by a function call per iteration. So a "stack trace" in a functional language is more akin to "step-by-step execution trace" in an imperative language. Which is much less useful due to information overload. &gt;&gt; what the author wants is a way to ask &gt;&gt; "where did this thing come from?" &gt; isn't that what stack traces provide ? In an imperative language, yes, exactly. So what we want is something analogous for a functional language. tora's paper takes the approach of taking that "step-by-step execution trace" and somehow trying to pick out of it the distinguished steps that correspond to the conceptual leap represented by an imperative function call. I'm suggesting a different approach - trace back the constructor calls that lead to the creation of a particular data object that represents the problem you are trying to solve. By focusing in on different parts of a complex object and tracing back their construction, you may be able to get answers to "where did it happen?" in a way that is somehow analogous to the imperative stack trace.
Useful advice, I am a big fan of Safe myself. But I have the impression that you are basing your argument in that programmers "should" avoid errors. That approach rarely works.
&gt; Well, calls in IO are very similar Not in principle. But I suppose that since we try very hard not to do very much real program logic in IO, that stack trace might sometimes be a bit more useful. But not always. You still find plenty of iteration in IO implemented by function calls. 
&gt; Because function calls are the mechanism for iteration, as well as almost every other kind of "flow control". Even the tightest loops are ultimately implemented by a function call per iteration. So a "stack trace" in a functional language is more akin to "step-by-step execution trace" in an imperative language. Which is much less useful due to information overload. But the pervasive use of recursion isn't specific to lazy evaluation or Haskell. Are we not in the same boat as Scheme, Ocaml, and other languages with tail recursion ? I might be wrong, but isn't it elision (as in tora's paper) the approach that they take there? &gt; 'm suggesting a different approach - trace back the constructor calls that lead to the creation of a particular data object that represents the problem you are trying to solve. By focusing in on different parts of a complex object and tracing back their construction, you may be able to get answers to "where did it happen?" in a way that is somehow analogous to the imperative stack trace. Sounds interesting, but I'm not sure I understand how this differs from a kind of stack trace. An example would be helpful.
My argument is that every time I've called error I've regretted it, eventually. In fact, I'm in the middle of a refactor to turn a particular set of functions total right now. So its not what programmers "should" do -- its rather what enough painful experience hammers into programmers.
Whenever I've wanted a stack trace, the stack I have in mind has not been the function call stack, but something else depending on the domain I'm working on, and it may not even be a stack. The most prominent example I have is parsers, where I often want a call tree that could explain why a given branch was taken. I cannot think of any occation where a function-level call stack would've been useful to me in debugging.
Thanks for the feedback, I made some changes that should fix it. Can you confirm that this is correct? Also, which site are you looking at precisely, the blog or the documentation site?
It's even less simple than that in Haskell, because non-tail calls don't necessarily use stack, either. Stack is not intrinsically tied to function calls, it is tied to evaluation in a context (which strict languages happen to tie together with function calls). f 0 = [] f (n+1) = n : f n can be used in a program without using any stack. Also, the tail recursive: f acc 0 = acc f acc (n+1) = f (n+acc) n may well use stack (unless the strictness analyzer is smart enough), because when you go to evaluate `f z k`, you will be evaluating a thunk that looks like `(k-1) + ((k-2) + ...)`, where `+`, a primop, does require evaluating each of its arguments in a context that must be returned to. Similarly, if you used `z = error "blah"`, the "stack trace" when you got to it would look like: (0+) (1+) (2+) ... (k-1 +) Because that's what's on the nested-evaluation stack when the error is reached. If you want information on the sequence of function calls that produced that nested thunk, you're going to have to require that GHC remember things that it currently doesn't (because it doesn't use a traditional call stack at all).
You can't tie the knot with `() -&gt; a` can you? For the last 2 or 3 years laziness has meant to me the ability to tie the knot.
`() -&gt; a` will get you call-by-name if you're pure. It will possibly do call-by-need if you're impure, but it may well be tricky to get right.
Scheme among others put a bit of thought into this: http://docs.plt-scheme.org/reference/Delayed_Evaluation.html
&gt; Also, the tail recursive `f` may well use stack (unless the strictness analyzer is smart enough), because when you go to evaluate f z k, you will be evaluating a thunk that looks like (k-1) + ((k-2) + ...) Oy, this bit me so hard last year that I was on the verge of tears. Down to the last few minutes before the assignment is due, and the whole thing starts crashing on large inputs... WHYYYY?
I have to disagree. Anyone who has had to debug a significantly complex application has asked themselves "how did I get here?". Stack traces help you answer this question. This isn't to say that they should follow the same control flow as in imperative languages, but they should at least allow you to see the callers and arguments. Ideally, there might be a debug mode that would capture stacks when lazy values are first resolved too (rather like saving stack traces with exception objects).
Arch packages available for: * haskell-vte-0.11.0 * haskell-svgcairo-0.11.0 * haskell-soegtk-0.11.0 * haskell-gtksourceview2-0.11.0 * haskell-gtkglext-0.11.0 * haskell-gstreamer-0.11.0 * haskell-gnomevfs-0.11.0 * haskell-glade-0.11.0 * haskell-gconf-0.11.0 * haskell-gtk-0.11.0 * haskell-pango-0.11.0 * haskell-cairo-0.11.0 * haskell-gio-0.11.0 * haskell-glib-0.11.0 * gtk2hs-buildtools-0.9 
I think c2hs will require that, yes.
Great! I hadn't reinstalled gtk2hs since the move to 6.12.1, this is good timing and worked fine for me. Thank you!
&gt; But the pervasive use of recursion isn't specific to lazy evaluation or Haskell. Are we not in the same boat as Scheme, Ocaml, and other languages with tail recursion ? Yes indeed. But yitz was responding to how Haskell's function calls differ from Java's, not OCaml's or Scheme's. The same response would be appropriate for any traditional functional language vs any traditional imperative language, but all the same--- functions play a different role in functional languages than they do in imperative languages.
&gt; Default ugly times font and styles. Pale green background on black text looks rather ugly. That's just design of the site. Let's better discuss what kind of sites one can create with Yesod... To me, it looks that, considering that Turbinado failed, Yesod is quite decent framework bringing many features out-of-the-box and by bringing persistance layer, it will become very complete. 
The machine tries to murder its master at the end.
↑ don't use them. [future proof link](http://www.haskell.org/pipermail/haskell-cafe/2010-May/078005.html)
Brilliant! I've wanted the time to do this exact thing for years.
FRP for transportation - this is a very promising trend
" We believe in giving back to the Haskell community, so we've open-sourced our ghc-iphone project, which allows GHC to produce binaries for the iPhone." Sounds cool but isn't this the exact thing that that clause in the new iPhone dev. agreement that everyone is bitching about outlaws?
Yes. But they had open sourced it before then, I believe.
That doesn't really matter if Apple just points to that clause and decides that this is a clear and obvious violation of the "you didn't ORIGINALLY write this in one of Apple's sanctioned languages for iPhone"
I wonder what method they are developing with. Any examples of games like the type they mention?
Ohh - it's about the theory of programming language semantics. There are *so* many different things [out there](http://www.google.com/search?q=rule+systems) called "Rule Systems", and by now Haskell has probably been applied to almost all of them. I couldn't tell what this was about until I started reading the paper.
Does this version fix using gtk2hs with -threaded?
Which doesn't matter if Apple just points out that other clause which reads "we may reject any App for any reason or no reason at all". If Apple wants to kill their own platform by being smug bastards, I say let them. The ghc-iphone code is still very likely to be useful for any other ARM port.
Well, they have their website up: http://ipwnstudios.com/bloodknight/
I am saying that disadvantages from the lack of stacktraces are offset by other Haskell features. I.e. writing a error-free Haskell program is easier but once the error exists it may be difficult to track.
mmm... the technical side seems ok but I am bit worried about the art/game design. 
This has long been possible. You must use `unsafeInitGUIForThreadedRTS` instead of `initGUI`, and then use `postGUIAsync` if you want to do any GTK actions off the main GTK thread. You might like to have a read through the documentation for this module: http://www.haskell.org/gtk2hs/docs/current/Graphics-UI-Gtk-General-General.html
While I'm sure the behavior you show above is actually how your tool behaves, that doesn't seem to agree with the paper. Please explain why the foo0..foo4 calls have not been "elided". The last bullet point on page 5 of your paper indicates that they should have been elided, because all of those locations were later encountered again (in the course of evaluating "foo0 True"). Perhaps your implementation does not actually have bounded space usage?
Also 'happy' and 'alex' are required for gtk and not automatically pulled in through cabal either.
Seems like the most interesting part of this link. Android would be more interesting.
Hi there, according to the Haskell 98 report, "In particular, (N _|_) is equivalent to _|_" http://www.haskell.org/onlinereport/decls.html#sect4.2.3
Yes, that's what I meant. I'll add a note to the page that "stack trace" as I use it doesn't literally mean silicon-level call stack. More of a "how did I get here". In the case of imperative languages they're the same thing; I could easily be convinced that a different notion is appropriate for lazy languages. But, regardless, this information ought to be available and is not.
Hi, I think you're taking my use of the term "stack trace" too literally. Any explanation -- stated in terms of the programmer's own source code -- of "what computations led up to this error" is acceptable. In the case of imperative languages a stack trace provides that; in a lazy language there might be other alternatives. But the important point is that none of those alternatives are collected "by default" the way stack traces are gathered "by default" in the JVM.
Due to Jobs, no jobs in Haskell iPhone development.
It is *if you force it*, but that's not what I said. Pattern matching the constructor of a newtype doesn't force it. [Full explanation](http://www.haskell.org/haskellwiki/Newtype)
They don't have much to say about Haskell, though: http://www.google.com/search?q=site:ipwnstudios.com+haskell Click on the links and all mentions of Haskell are removed.
Isn't this dead in the water because only C/C++/Obj C dev is allowed on the iphone?
In fact, iPwn studios has never heard of Haskell. It has always been the case that iPwn Studios does not use Haskell, or other ungood languages. Steve Jobs is watching you.
Say what?
Say what?
iPhone isn't the only [mobile platform](http://www.android.com/).
Great new look guys, a few minor comments: * I would make the banner at the top shorter. You guys have some great content on your homepage; let the user see it! * I personally would put the "simple app" towards the top of the page, and maybe rename it to something like "demo" (I don't know what exactly). Also, why not drop the "module Main where" line; you can say that Happstack has a 2-line hello world! * A logo would probably be a nice addition. * For state, templating and type-safe URLs, it would be cool to have a link to the side that could show an overlay with a code sample. Seeing a MACID code sample right on the homepage would probably be enticing to users who have never experienced it before.
I think it's really naive to believe that there's enough pissed off developers to kill the iPhone because of this change. Sure they might drive folks to Android by doing this stuff, but they've always got an edge over Android in that they control the software and the devices, and thusly when they do their packaging correctly, seem to come out with successful products.
But isn't this about them open-sourcing their ghc-iphone project?
Ok two parts to this. The first is why are these things not elided and the second is bounded space use. You can think of the stack data type being a purely functional stack (it's not quite implemented that way to make point 2 work, but I'll explain that in a min). So after re-writing, foo4 will look akin to: foo4' stack False = foo0' (push HERE-LHS stack) True `seq` error' (push HERE-RHS stack) "Ha!" foo4' stack True = () So you can imagine that (push HERE-LHS stack) builds a new stack with the elisions happening, but will not (it's side-effect free!) affect anything in 'stack', therefore no information is lost when it returns after the seq and is used in error'. But now there are lots of stacks floating about, and I claim bounded space usage! Hmm, I need some statements / assertions: * There are a statically fixed number of program locations that could be elements in a stack trace (e.g. foo4, Foo.hs:12,34) * Because of the elision technique each of those program locations could occur at most once in a stack trace, possibly with ...'s inbetween. * This means that there is a statically bounded max size of any one stack (2 x number of program locations for pathalogical programs) * AND a statically bounded maximum possible number of stack configurations So now if we make sure we only ever have one of each stack configuration (through use of unsafePerformIO and hash-tables and pointers, noting that all stacks have a root at 'Empty') and some optimisation (tail sharing) we can keep space usage bounded by factor of a static property of the program. [edit:formatting, ptrs] [edit:] That last sentence was a bit terse. The stack's basically are implemented as a chain of HashTables mapping the next location you could push to a pointer to the stack that you end up with when you push it. The push operator works out if the next stack has been created yet and if so finds its pointer, otherwise it creates it in a way that means other push calls can find it later if they need to produce a stack like it. The paper has lots more details and pictures/diagrams of how these pointers mutate (sections 4.3, 4.4, 4.5)
you've ruined it... we had a perfectly good community, everyone was busy writing throw-away lambda calculus interpreters, but no! it wasn't good enough for you! you actually want to write something useful, better looking than other things, and faster than everything else. and that pretending to be nice and asking the community questions, seriously, how dare you? anyway, we should be grateful, that our community has this kind of people. but we can't forget about other, less fortunate communities (less pure but more eager), that have to pay for their success with in-your-face attitude.
how do you render this .lhs file? pandoc? what about those nice boxes for code snippets?
I render the file using [Hakyll](http://jaspervdj.be/hakyll), which is a pandoc wrapper to create static websites. If you're interested, here is [the source code](http://github.com/jaspervdj/jaspervdj) of the website. The boxes are basically just some custom CSS.
Probably doesn't matter a whole lot, but when I went on this site with my phone's browser all I could see was the header (`Happstack Haskell Application Server Stack Home/Download/Docs/Community`).
How about this syntax for attributes: link &lt; rel := "stylesheet" type := "text/css" href := "screen.css" &gt; doing so should be possible when (:=) and (&lt;) returns a continuation which on the next keyword will decide what to do and (&gt;) is some kind of stop sign or use some nice type class hackery. I think in the yi source code something similar (but much simpler) was used to create snippets. 
Oh, Snap! Happstack's feeling the pressure.
Without the (:=) I could believe it, but with the (:=) that almost certainly parses as rel := ("stylesheet" type) := ("text/css" href) := "screen.css" ...which could work for literal strings, I suppose, depending on just how overloaded you wanted your strings to be... =P
Indeed. Everyone knows that the only languages you use for writing double plus good Smartphone applications are C, C++ and Objective C. Using any other language is downright silly. I'm glad I'm a beta. 
A tiny gripe -- the haddock index squeezes the package names into tiny cells which take up two lines on my browser -- this forces an effective double spacing between module names. It would be nice to clean that up.
Excellent post and a promising library. I have two comments. Q7.1: I like the syntax. I don't like that you break a monad law. I worry this will interact badly when BlazeHTML is combined with some other monadic action. Q7.2: How compositional is the representation? Can I write functions to combine HTML snippets easily? That use case will be very common when creating a web application that has any amount of shared code. Finally, can HTML only be generated in Haskell source files using BlazeHTML? How would a programmer and designer (HTML person) interact when using BlazeHTML, if at all? I suspect this is not a design goal; in that case, can BlazeHTML be used to build an application that can combine static HTML w/ generated HTML? Hope this helps! Excellent start!
&gt; Q7.1: I like the syntax. I don't like that you break a monad law. I worry this will interact badly when BlazeHTML is combined with some other monadic action. Right. I'm going to further investigate this. &gt; Q7.2: How compositional is the representation? Can I write functions to combine HTML snippets easily? That use case will be very common when creating a web application that has any amount of shared code. Composability is a design goal. Our `Html` type is no different than any other Haskell type, so it can be easily passed as an argument, returned from a function i.e. everything you can do with a Haskell value. &gt; Finally, can HTML only be generated in Haskell source files using BlazeHTML? How would a programmer and designer (HTML person) interact when using BlazeHTML, if at all? I suspect this is not a design goal; in that case, can BlazeHTML be used to build an application that can combine static HTML w/ generated HTML? Ah, good question. We've heard that in most cases, once a page gets sufficiently dynamic, a designer is usually not able to write it. On the other hand, we try to make the syntax as easy as possible, and it should not be harder to write our DSL than to write HTML.
jasper - first of all i wanted to say that i have been watching your progress on github, reading the log there etc. i am very excited about blaze that said, i think focusing on html5 now instead of html4 would be a better idea anyone adopting blaze is going to be an early-adopter type who is probably already thinking about html5. i have already moved on to it myself and i am not going back. html4 is the conservative choice, but i would offer that your target audience is not conservative in its technical decisions (if so, they would probably not even be bothering with haskell for html generation). the work you are doing now is likely the largest concentrated amount of work that blaze will ever have in one shot. i think using this dedicated time to support html5 is important
I would love to apply. But TBH getting a Haskell job, competing with so many bright people, it's something near to impossible. 
Ugh I hate the function name clashes. Is there a way to just not import prelude at all rather than hiding 50 functions? I was trying to write a bytestring parser the other day and had nothing but problems with both bytestring.char8 and bytestring, attoparsec, and prelude having 20 functions in common with each other. I ended up qualifying them all and hiding half a dozen prelude functions, but it looked like hell when I was done.
I'm in it for the chicks.
You can always import Prelude () **edit**: What dcoutts said.
You're supposed to import ByteString quaified. That way you don't need to hide prelude stuff and the code is usually fairly readable.
i wish more of these projects would focus on intergrating with apache as well as their own servers i understand that with iteratees etc, some of these projects can deliver crazy performance, but i need rewrite, ssl, and all the other features provided by apache too. as far as i can tell, most of the servers provided by the haskell stacks lack these features
_**Q4**: What do you think of this approach for chosing combinator names?_ How about providing three modules: one with trailing underscores for all combinators; one without trailing underscores, leaving out those which would conflict with Haskell keywords; and one for convenience which reexports a mix of the first two to match what you've got there currently. The second one would just import the first and have a bunch of definitions like `a = a_`. That way, users can choose the consistent names with underscores, or mix and match between the first two as they like, or use the convenience module, or use qualified imports if they prefer that. Personally I would probably find that not having to worry about name clashes is worth typing an extra character per HTML combinator.
use mod_proxy?
Yep, HTML 5 is on my list!
By the way, do you plan to enforce some degree of well-formedness of the generated HTML through the type system? That would be rather interesting just because writing such a library which is fast _and_ safe seems to be more or less impossible in most languages.
mod_proxy is neat but it seems sort of hacky to have a long-term server setup that stitches together two http servers. i suppose cgi/fastcgi could also been seen as hacky, but it feels less so
thanks! i intend to use blaze as soon as you release it, and i think its great how you are documenting and logging your design decisions. thanks for all of your hard work!!
i really like the fact that the html attributes just chain together with "!" (rather than the two alternative methods that place the attributes in an array). the less syntax, the better.
We've always been at war with Eastasia
(+1) namespace collisions are _mad_ annoying.
Using mod_proxy is in fact *less* hacky than fastcgi -- fastcgi is a kludge, why bother translating http requests to/from some arbitrary boutique protocol when you can just speak http to your application servers?
This is obviously not going to be a designer's first choice. there's going to be a learning curve. but it's not a big deal. if they can learn something like HAML, they can learn BlazeHTML. and if they can't... well, perhaps you need a new designer.
yeah, when i think about it, you're right, enjoy an upvote. maybe i am overestimating the performance issues here. 
The best information I've seen is the paper at the top of the linked page. The nested array model has very interesting parallels to the DPH work as well, regarding index transforms and the like especially. I'm excited about playing with it on release.
It's really about ghc generating ARM code, which is used on just about everything mobile. It's currently in a mach-o object file, which is mac/iphone-specific, but a bit more work could fix that. LLVM will also provide another option for this.
Can't hurt to try!
I don't see anything in the Haskell 98 report about "only if you force it". The text there is pretty black-and-white, and I copied my example directly from the report. I'm not saying you're wrong. Maybe the Haskell 98 report is wrong. Who knows. I just repeated what's written in the report. Like I said, I'm convinced that the amount of overcomplication introduced by this whole phenomenon, while not superfluous, does not bring enough benefit to justify it.
The difference between newtypes and strict fields is that with a strict datatype: case undefined of -- or case N undefined of N _ -&gt; 5 is bottom, while with a newtype, it is `5`. Newtype matches are irrefutable, while with a datatype with a strict field, you must change the match to: case undefined of ~(N _) -&gt; 5 The types are isomorphic, and may be considered the same under certain semantics, but they do not behave identically in all situations.
Honestly this seems a bit daft. It is far more expensive in terms of resources to load all the files into memory at once than it is to just open all the files but not immediately load their contents [ok, unless all the files were really small]. If resources were a problem then the other solution would have been to reorganised the code so that it processes one file at a time rather than opening all the files and presumably subsequently processing them one at a time. In that case there would still be no problem with using ordinary readFile. The only problem is certain operating systems (i.e. OSX) that, by default, have a ridiculously low limit on the number of file handles. File handles are not some magic resource, they're just memory allocated in the kernel on behalf of the process. Processes are allowed to use masses of user space memory and yet only allowed a measly number of file handles.
The files are really small, and after being loaded they're all combined into a single list for analysis.
Has the video been uploaded anywhere?
HTML version: http://haskell.org/communities/05-2010/html/report.html
&gt; A brief introduction to CnC using this module can be found at http://software.intel.com/foobar. Wait, what?
Can you guys post recordings or slides of these talks?
Thanks Janis for all your hard work in preparing this.
Here's his papers on the subject (warning: super dense): http://www.uoregon.edu/~dspivak/cs/ Edit: oh, and a previous LtU discussion -- http://lambda-the-ultimate.org/node/3761
Blog post about this. http://software.intel.com/en-us/blogs/2010/05/27/announcing-intel-concurrent-collections-for-haskell-01/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed:+IntelSoftwareNetworkBlog+(Intel+Software+Network+Blog)
Yeah, I keep seeing mention that these are being recorded and stuff, but nothing is ever put up. :(
I have to say I'm surprised it worked that smoothly. Getting the Gtk+ C libs on Windows is still a bit problematic, but apparently the [Gtk2Hs INSTALL file instructions](http://code.haskell.org/gtk2hs/INSTALL) work ok.
We're recording them. Videos going through post-processing now. Up soon.
Awesome!
that's more helpful. Enough so, that I'm reposting in proggit.
I wonder two things: - from a performance point of view, can a step function be as simple as an addition, building bigger operations as graph; or can nodes be just Int, or other simple types? - is there a mean to run again a graph knowing that only one or just a few input(s) has/have changed (with execution optimized because of a previous execution)?
"The types are isomorphic" -- yeah, that's what I meant. Thanks for confirming!
Thanks for this. I was having a hell of a time with Gtk2hs not even a week ago. Even better, my Notepad++ has been giving me the business lately and I was looking for something proper to use. I didn't even know this Leksah existed. 
I enjoy hearing about new talks, but when will videos be posted for the previous ones :)
Is readFile unsafe? (I know it says its safe but mmap usually isnt) Are changes to the rope reflected back to disk? (would be extremely useful) 
What do you mean by safe? Some may argue that lazy I/O is never safe. If the file is changing behind the scenes, the result will be unpredictable. If it's not, the resource usage will still be unpredictable. But if you mean could it cause the program to seg fault? I don't see how.
There is also the rope library which has better asymptotics and support for utf-8 decoding, though perhaps I am somewhat biased as the author. http://hackage.haskell.org/packages/archive/rope/0.6.1/doc/html/Data-Rope.html Though, I suppose I could stand to document that fact better. All of the O(log(n))'s turn into O(1)'s except for append and split.
These going to SIGMOD 2010 and interested in SciDB- check out "691 Overview of SciDB: Large Scale Array Storage, Processing and Analysis" - or read the paper once it is published.
&gt; has better asymptotics do you have numbers, besides?
Another reason to learn Haskell: protection against raptors.
Just asymptotics. I haven't benchmarked the two against each other as I'd mostly been comparing against Seq and Bytestring for benchmarking purposes and hadn't really noticed this library. The one thing the Data-Rope version does support that I don't is the notion that you can have rope fragments that know that they come from a given filehandle at a given offset. The main difference between the two libraries is that they store a tree of bounded balance by size that holds a bytestring equivalent, while I'm storing a 2-3 fingertree of bytestrings. Both try to grow the leaves up until some reasonable chunk size is reached, but the fingertree gives me much faster access to the end points. It is very hard to provide a fair benchmark between the two because with the large difference in asymptotics, I can just make the problem larger until they lose. ;)
Another vote for HTML5! HTML5 has two serialization formats; `html` and `xml`. I would suggest focusing on the HTML one. The only reason to use XHTML (4 or 5) is if you have are in an XML environment (you have whole pipeline of XSLTs or something). In that case I don't think someone would be using Blaze, but rather some dedicated XML library. So I think HTML5 is the best future-proof format, and it is supposed to be backward compatible, so it'll work with older browsers as well.
This problem was my motivation for learning calculus and differential equations.
&gt; (I'll only be calling it for non-negative arguments, but alas, Haskell has no natural number type.) Sure it does: newtype Nat = [()]
http://hackage.haskell.org/package/nat
awesome! can't wait for the README / tutorial. glanced at the code... i'm still new to haskell, but curious, how common is it for function names and data types to be word-separated with underscores? from what i'd seen before i assumed camelcase was a bit more standard.
This module needs more documentation. Especially examples.
So Haskell is copying Factor, that is interesting.
Factor did not invent combinators, although it does help make them popular. 
Well... someone embedded some Factor idioms into a DSL in Haskell and put the library on Hackage.
This sounds really cool! But from the LtU reference, it appears that the title of this link is backwards. Spivak is not saying that categories are databases. He's saying databases form a category.
It must be the [raptors](http://www.reddit.com/r/haskell/comments/c9bcz/solved_xkcds_raptor_problem/)
Haskell protects better against raptors when you use *iterate*, rather than *reverse . iterate*, which is what the poster's *iterateUntil* is essentially doing.
&gt; In this talk I’ll show how to turn any database into a category _and any category into a database._
Yeah, I know that now :)
This is a cool post. But can anyone figure out what it has to do with windshield wipers? (see trackbacks)
Oh, OK. I have a feeling that the other direction is the more interesting one, though. :)
How does this perform compared to regular python implementations?
i must only emphasize the wat.
What is the coverage? 100%??
Is translating an impure imperative language to a pure declarative language that easy?
By the power of monads: StateT EvalState (ContT Object IO) a Yes.
Here's the status: http://wiki.github.com/bjpop/berp/developing-berp
The emphasis here should be on factor-**like**. For example, 'bi' in factor has the following semantics (assuming the stack is represented as nested tuples): \(f, (g, (x, s))) -&gt; f (x, g (x, s)) The Haskell library, however, has the following: \f g c x -&gt; c (f x) (g x) These are really very different functions. Unlike Haskell, Factor does not have curried functions. All functions operate on a stack, and this is what allows the (mostly) effortless concatenation of programs. There's really no good way to add this to Haskell without rewriting the standard library and adding row polymorphism (for typing "the rest of the stack").
 Python 3.1.2 (r312:79147, May 13 2010, 00:43:41)... SyntaxError: 'continue' not supported inside 'finally' clause
[John Meacham's lazy naturals](http://www.haskell.org/pipermail/haskell-cafe/2007-October/033213.html) I wish John would put these up on hackage already. 
Nice! I've been wanting to do this for a while myself now.
This is a more general question: why isn't any language other than C, C++ and Java (to a first approximation) popular in industry? Nonetheless, we're [not doing too badly](http://haskell.org/haskellwiki/Haskell_in_industry#Haskell_in_Industry) compared to our language brethren.
Not to do down C/C++/Java/C# et al's market share, but other languages do have a fair foothold? Scripting languages (particularly Lua) are very popular in games, for instance. Python is also huge and used to glue christ knows what together. Not to mention Perl, PHP, Javascript... I think it is interesting that anything performance related usually falls back to C/C++ though (with fortran hanging on in there). It is the nut to try and crack.
Solved.
What's the plan for implementing eval?
callCC is already supported.
Try it and let us know.
Arrows are useful too for chaining things together.
John has some ... problems with the design of Cabal, so is extremely unlikely to do that. I'm pretty sure he wouldn't mind someone else maintaining such a package. If you're willing to volunteer, ask him.
I must respectfully disagree with the idea that other languages can just swipe ideas from Haskell. I doubt that they can copy monads to the tune that Haskell does. Once you learn to use them, you can't live without them. Lots of "hard" things become easy when you treat imperative routines as first class values. It's not just parallelism. My second favorite example involves these new DBs that don't guarantee consistency. Having first-class imperative routines makes it much less of a pain to implement retry features for these DBs, and it makes it much easier to chain up routines without losing robustness. As for the list itself, most companies are run by managers, not engineers. All you needed then was #5 and #9. And that cannot be blamed. Most customers don't know what the #@*# they want, so neither does management. Why invest in code you'll probably throw away anyway?
It is nut a nut to crack. Assembly is popular for tight inner-loops at places. C enjoys the same kind of specialty. But C has already lost its elegance for anything but that, like Assembly. This is the endgame for C. The further endgame hoists on: Multicore becoming common (MIMD or SIMD). It may not happen if phones and tablets rip out the market.
Yeah, camelcase is supposed to be good practice.
Which DB's are you referring to?
Not quite the same thing, is it?
To comment only on a single sentence of the article: numeric-prelude, at least in its present form, is basically a wasted effort: It calls every type T and every class C, and thus never will be widely used (I hope). As far as I know, there is exactly one person in the Haskell community using this rather questionable convention, which makes the Haddock documentation totally unusable, among other issues...
That is unfortunate. Other than that though, it's exactly what we need to do to the numeric hierarchy.
No but it demonstrates that tricky constructs are supported. Edit: Honestly who really cares about eval() anyway? Come freakin' on.
Is there any support for a sort of Haskell 3000 Prelude rewrite? It seems like a lot of people have complaints about Prelude, and they all seem to be very similar... it's the sort of hassle you get used to after a while but the cognitive load never really goes away, you just stop noticing the drain.
I think the Prelude as it stands is mostly better suited for newbies, though.
I believe the expectation is that qualified names would be used for everything, e.g., `Monoid.C`, `Ring.C`, etc. It's an, errr... interesting approach, and I can see some benefits to making modules the basic unit of program structure. Perhaps it was inspired by some other language with a more powerful module system? I'm not sure. But as it stands, it's not idiomatic for Haskell and nothing else does things that way, which is why tools like Haddock produce useless output (and will likely continue to do so).
&gt; a lot of people have complaints about Prelude, and they all seem to be very similar... Yes, but *how* similar? I'm suspicious it's one of those cases where everyone strongly agrees that a bikeshed needs to be built, but little else.
Of course that's the intended usage. However, as you say, it's not exactly idiomatic Haskell... Also, I actually tried to interface with code written in that style, and it was a not a nice experience.
it is expected that in year 3000, a consensus will be reached, hence Haskell 3000 Prelude :)
Awesome. I'm really looking forward to seeing this completed.
Clearly the right thing to do is a "SimplePrelude" or "LearningPrelude".
There's a wide consensus that the categorical hierarchy should go Monad, Applicative, Pointed, Functor. The only debate here is whether we should bother with having a Pointed class separate from Applicative. And the only thing preventing this change is the fear of breaking legacy code. Generalizing (.) to be the Category method for composing morphisms also has a fairly wide consensus, though it's less of a hot-button issue and many people simply don't care one way or the other. The Foldable and Traversable changes are somewhat more controversial since there are reasons for wanting monomorphism without giving type signatures, namely efficiency+brevity (though there are other ways of solving this). Similarly for renaming `fmap` to `map`. Personally I'm a fan of both changes, though the `fmap` renaming will break a whole lot of code and is only a minor wart. Everyone agrees that the numeric hierarchy needs fixing, but ---as you say--- noone can agree on what the end result should look like. The one thing that is clear is that the only way to make everyone happy would require major enhancements to the typeclass system in order to reduce the boilerplate involved with doing things correctly. But noone can agree on what those enhancements should look like either. I think the need for a listless Prelude has finally become a big enough issue that people may be willing to switch to Prelude.Listless, though this is more of a common annoyance than a hot-button issue. The one thing we'd want to do here is to have an implicit `import Data.List` to go along with the implicit `import Prelude` in order to make the change invisible to users who don't need to distinguish lists from other list-like interfaces. The ListLike class is not a good idea as a solution because the dictionary is too large, which is part of why noone uses it. The lack of generalized zipping and unzipping is a wart, but not one that's actually significant. If you need to go even as high as `zip7` then chances are you're doing it wrong. There are solutions out there (including my own), but they're more about proving that we *can* make it general than they are about trying to improve the Prelude. The only thing that might need improvement here is adding [`zipWithBy` and `zipBy`](http://hackage.haskell.org/packages/archive/list-extras/0.3.0/doc/html/Data-List-Extras-Pair.html) in order to fill out the interface. (N.B. `zipBy` has the wrong type in that version, compare against `pairBy`. Corrected in version 0.4.0) The IO sin-bin is much like the numeric hierarchy: everyone wants something better; noone's sure what "better" looks like. The only other general issues I can think of are the problems with Float and Double. Namely that they do not form a total order and therefore should not be instances of Ord. The [PartialOrd](http://hackage.haskell.org/packages/archive/logfloat/0.12.1/doc/html/Data-Number-PartialOrd.html) class could fix that and is valid Haskell98. Similarly, though controversially, Float and Double should not be instances of Eq or Num either since equality is unstable and they do not obey the arithmetic laws. Unfortunately, there is no clear way of figuring out what should replace those instances, so this will never change (for better or worse). Similarly, it would be nice to add the [Transfinite](http://hackage.haskell.org/packages/archive/logfloat/0.12.1/doc/html/Data-Number-Transfinite.html) class (which may soon be renamed to Limited) to the Prelude in order to allow a correct and general implementation of `log`.
Not quite. There's no support for semirings because of the failure to distinguish addiction from subtraction. Semirings are extremely important in certain mathematical fields like, e.g., computational linguistics and natural language processing. Like most attempts to fix the numeric hierarchy, numeric-prelude starts from rings and fields and quickly runs off in the direction of vector spaces. That's great if you swing that way, but it completely ignores the tradition of abstract and universal algebra which focus on simpler and more general structures (semirings, pseudorings, nonassociative rings, Lie rings,...). Discrete mathematics is just as important as continuous mathematics, but completely ignored in this project.
Great summary -- you forgot one more issue though, which is partial functions. I'd be strongly in favor of a total prelude, removing head, tail, and co. by default. Although this overlaps with the "listless" issue, of course.
Amusing, but not strictly accurate; `destroyUniverse` is actually a pure function operating in the extended quantum nondeterminism monad (import `Control.Monad.Quantum.MWI` from package `decoherence-extras`). Specifically, `destroyUniverse` is mostly equivalent to `guard` for `MonadPlus`, except that instead of filtering states out of the superposition, it filters potential observers out of undesired states. The "quantum bogosort" algorithm is the simplest application of this, as it works by generating all possible results with equal probability, then filtering observers out of all states but the desired one. From a Haskell perspective, in fact, `MWI` actually has a lot of advantages over other instances of `MonadQuantum`! Computations in the `MWI` monad are straightforwardly pure, referentially transparent, and (looking in "from the outside") fully deterministic. Other implementations rely on destructive updates or a source of random numbers. Most people don't realize this, but Einstein's complaints about "spooky action at a distance" were in reference to destructive updates of global state; Einstein avoided `IO` as much as possible in all the Haskell programs he wrote.
&gt; Similarly, though controversially, Float and Double should not be instances of Eq or Num either since equality is unstable and they do not obey the arithmetic laws. Unfortunately, there is no clear way of figuring out what should replace those instances, so this will never change (for better or worse). I'll cop to not having as much practical experience with strong types as many people around here, but it has struck me that _strong_ but _inaccurate_ types are actually worse than weak types. Strong, inaccurate types actually _mandate_ wrong behavior and _forbid_ correct behavior; at least with weak types you can write something correct with sufficient effort. (The extent of mandating and forbidding vary from error to error and degenerates to nothing in one clause in some cases, but I think pretty much by definition every type error must do at least one of those things.) This actually applies to a few of your paragraphs, but I thought I'd just take one. I guess to me Prelude is more than just practical problems; it is an odd locus of incorrect types in a language otherwise very focused on correct types, and I find it an odd concession to impurity (not the technical sense of the term, the broader sense). And it's right there at the core of the language, where it can't be ignored.
&gt; Other implementations rely on destructive updates [...] Quite destructive indeed to retain the desired asymptotics. ;)
Yea, I saw that too. That whole thread is kind of sad.
i would like to subscribe to your newsletter
make cabal like cpan, thats what there is left to do
&gt; Other implementations rely on destructive updates or a source of random numbers. Everyone knows that the GHC implementation of `MWI` really uses `unsafeHasturHasturHastur :: RealWorld -&gt; Void` under the hood.
Great analysis, and also the type of discussion I was attempting to prompt by my post. I have some thoughts on this of my own, but they aren't particularly collected so I will sit on them a little longer.
I'm a little ambivalent about this idea, since 1. hackers object to being given the "basic version" and 2. I think there's a space for concrete type examples of general signatures to be made.
winterkoninkje, do you think it's possible for discrete and continuous mathematics to coexist together in the type class framework we have now?
"MyFirstPrelude"
rank them? by popularity, possibly reviews with comments on the library.
Is that why there's no new messages on haskell-cafe ?
Would it be possible to use statistical methods to rank them automatically? Things that might be interesting are: How long has the package existed? What's the frequency of new releases? When was the last release? (difficult, if it was a long time since the last release, does that mean that the package is very stable or that it's orphaned?)
On #haskell, BMeph implied that the Yale server serves the "@haskell.org" mailing lists, including cafe.
There are a lot of libraries out there, some of them have like...a couple of downloads. I still can't tell if they are libraries worth using or not.
[Ranked by download popularity in Q1 2010](http://www.galois.com/~dons/hackage/april-2010/popularity.csv).
Yes, I agree, it's very frustrating and makes Haskell look bad. There is work going on to move the servers though, possibly to sparky, which is be hosted at Chalmers. 
The problem IMHO is to rely on university departments to provide the hosting. That is inherently flaky as they are usually run by small teams on a tight budget. Can't we raise the money for proper hosting?
I am worried that sparky will have a similar situation since it is also hosted by a university. Can someone reboot sparky on the Swedish Memorial Day?
haskell.org is hosted at Yale. Yale was informed on Saturday. it is a long weekend in the US, and the admins aren't in till June 1. The solution is to have haskell.org constituted properly, and direct funding to a dedicated, high bandwidth server.
I don't know the reasons for why haskell.org (and indeed code.haskell.org) has been offline so much, but I would think that if setup properly a server wouldn't go offline all the time. Also, sparky is running in a LDom, not on the bare metal. As I understand it, this means that any admin would be able to reboot any of the VMs inside the LDom if so required.
The somali are running their operations with haskell?
&gt; The solution is to have haskell.org constituted properly, and direct funding to a dedicated, high bandwidth server. Maybe [the Industrial Haskell Group](http://industry.haskell.org/) can serve as an organizing point: &gt; Associate and academic members contribute to a separate fund which is used for maintenance and development work that benefits the members and community in general.
On which machine is code.haskell.org hosted?
Keep the reports flowing! I like to know how our money is spent :)
I would like to crowdsource data gathering on library quality. Some sort of reddit-like voting mechanism and the ability to comment on a libraries hackage page could eventually be a huge leap forward in terms of making it simpler for developers to choose which libraries to use. (reposted from my comment on donsbot)
If the image was set up correctly you could just distribute that machine (not necessarily sparky) to anyone with enough hardware. The nodes could synchronize and distribute load and data.
It's currently hosted on a commercial VPS. That does provide good 24x7 tech support, but also limited resources. That VPS is also beginning to show its age, another reason why it is not performing as it should. We are in the process of moving it - probably to sparky, which is an enterprise quality server owned by the Haskell community.
That is true, but here we are not talking about a stock PC. This is a very highly performant enterprise-level machine. They are known to be very reliable in practice - they often run literally for decades without need for a reboot. But of course, problems can occasionally occur even with this level of server, so we will also have backup facilities to deal with those scenarios.
It would be nice to have a cutting-edge distributed cluster. If you have the hardware and manpower to set it up - please let us know. As it is, this work is being done on a volunteer basis, and already it will take a huge amount of work to complete the current conventional upgrade in a way that meets professional standards.
Correlation does not equal causation. Then again, this shit looks pretty legit. 
Clearly, Somali pirates are kidnapping sailors and then forcing them to write Haskell. Torture!
It's the number of *successful* hijackings. Obviously the pirates have rewritten their piracy software in Haskell and are reaping the benefits in reliability, hence more successful hijackings.
desperation feeds innovation
Could you please explain this weird syntax: main = do (schedule, _, _, _, _) String -&gt; String -&gt; String varInit t var val = cType t ++ " " ++ var ++ " = " ++ val ++ ";" 
It should be no problem at all to get the donations necessary. I know I'd pay for a lot more than I use - Haskell is a very big part of my life.
I'd like some big shiny badges (up front and center) next to each library indicating: * builds on windows * builds on linux And a way of telling cabal install to ignore anything without the badge for your OS. I think delegating non-portable libraries to the naughty corner by promoting the good citizens is a good way to fix the situation for haskell libraries on windows (i.e. they very often fail to build because the build script uses a linux shell even though they could in principle work just fine on any platform).
My bet is with nostrademons: every successful hijack means more haskell packages; I doubt than every new package on Hackage make the pirates more effective regardless of the improvements made on the existing packages.
I will publish a report per week so don't worry for that heh.
&gt; It means that developing highly rich and featureful software that needs to be very flexible and performant, is a whole lot more complicated than necessary. This has definitely not been my experience. Haskell is not perfect, but I've found it much better than anything in the mainstream for writing rich and flexible software - even featureful software. As for performance, it doesn't beat C++ or Java but it beats the hell out of any of these flexible dynamic languages you're talking about.
How about a distributed, key-signed torrent approach to hackage ;)
We have to consider three options for causation here: A. more haskell packages causes more hijackings. Haskell is aiding the pirates in doing their work more effectively B. more hijackings causes more haskell packages to be written. Somali pirates celebrate successful hijackings by hacking all weekend. C. There is a common root that results in both more successful somali hijackings and more haskell packages being written. In this case, it is clear that programmers are pirates. Over time there are more programmers, and some decide to write haskell libraries, others decide to join a life of crime on the open seas. In conclusion, hijacking and haskell both start with 'H'... coincidence??????
In some ways, but it could also make the zombie package problem worse, unless measures are taken to dissuade people from using a once-popular zombie. 
Or... D.) More and more Haskellers are making a killing in the financial services sector, taking their relatively unarmed yachts on joy rides around the world, and sometimes happen to pass too close to Somalia.
I don't know how workable this is, but cabalizing performance tests could be interesting. Ideally, people making packages in similar functional areas would implement the same performance suite, so that apples-to-apples comparisons are possible. How this centrality would be established and enforced is an exercise for the reader. The GHC team would have an easy way to spot performance regressions in real code on various optimization settings, package maintainers would have a productive way to help keep their package from being regressed, and new packages in a functional area would have clear targets for performance and features (and an incentive to stick to an established API).
Piracy in Haskell is impossible. It is mutation.
[Ranked by the number of reverse dependencies](http://bifunctor.homelinux.net/~roel/hackage/packages/archive/revdeps-list.html) (note that you can sort those columns by clicking on them)
&gt; Also, Haskell really isn't that fast unless you write (or rewrite) your code in an un-idiomatic imperative style that is a far cry from the elegant looking idomatic functional code. Yeah. In Haskell you might have to write the time-critical inner loop of your program in clunky low-level imperative style. In C++ you have to write your entire program in that style. And C++ is enormously popular. &gt; Most programmers don't appreciate the assurances of static types Because they only know C++ and Java, whose type systems are cruel jokes. &gt; in practice, most domains can't really take much advantage of them anyway. Citation needed. I can name dozens of counterexamples, backed up by both peer-reviewed academic papers and real-world projects.
yargh
`eval` can be useful. GHC-Haskell has `eval` via [the hint library](http://hackage.haskell.org/package/hint). Presumably, `berp`-compiled code could link against `hint` and use it to call the `berp` compiler. This actually should be pretty simple to implement.
Yes. Haskell is an imperative language; it supports imperative code as a first-class type. And all purity means is that two concepts that are muddled together in most languages (functions and actions) are separated in Haskell. Both are still present. You just translate each Python function to a Haskell function-returning-an-action.
Hmm................................
the automated testing sounds great. though in general, as a professional ruby developer / haskell wannabe, i don't see the package growth as much of a problem. as haskell becomes more popular, we'll end up with the haskell equivalents of "railscasts", "ruby inside", and "ruby flow". the community will gravitate towards certain packages naturally as more and more blog about them. though before ever adding any new functionality to hackage, i would have instead given it a facelift. the site looks straight out of 1999 :-) also, the biggest problem with libraries on hackage is the lack of helpful homepages for the libraries. one of the best things about the ruby community is that most people that release a package go to the trouble of putting their code on github, creating a README with a basic how-to on how to use the library. 
&gt; hackers object to being given the "basic version" True -- I mean, it'd be bad to be condescending about it. But in my experience, beginners in `#haskell` are very humble. They recognize that there's a steep learning curve and that they won't understand everything at once, and they're willing to take advice on what to defer. The flip side is that the gurus usually are willing to explain "the hard way" when specifically asked. A good way to do this would include a website with side-by-side comparisons between the beginner's prelude and advanced prelude. 
It depends on the industry.
A lot of these are available in other libraries already or if they aren't are available in slightly flipped form. Now if you want real (or at least parenthesis free) concatenative combinators, it is fun to rederive the ones Chris Okasaki mentions here: *http://www.eecs.usma.edu/webs/people/okasaki/jfp03.ps [Postscript] *http://www.eecs.usma.edu/webs/people/okasaki/hw02.ps [Postscript] *http://www.eecs.usma.edu/webs/people/okasaki/Hw02code.zip [Zip]
Too big. inf = () : inf infinity = Nat inf shouldn't be a Natural ;) You can of course, get there with a strict list however. ;)
Delete some of them at random?
This reminds me of Beautiful Concurrency : &gt; For example, consider &gt; &gt; atomically (do { x &lt;- readTVar xv &gt; ; y &lt;- readTVar yv &gt; ; if x&gt;y then launchMissiles &gt; else return () }) &gt; where launchMissiles :: IO () causes serious international side-effects. 
&gt; As it is, this work is being done on a volunteer basis, and already it will take a huge amount of work to complete the current conventional upgrade in a way that meets professional standards. What can we do to help?
Haskell, of course, often elides the difference between data and codata. Point taken though.
While Eq for floating point is usually not a good idea, there are situations when you really want it (together with Ord): for example when you want to use them (or more generally, ADTs containing Floats or Doubles) as keys in a Map. Similarly, PartialOrd would break for example sorting a list of Doubles, which is like, extremely common. 
http://github.com/jlouis/combinatorrent get to work
Does this mean there's hope for [global warming](http://www.venganza.org/images/spreadword/pchart1.jpg) ?
Nice video, it gave me something to learn while eating a grapefruit ;). I thought the pace was a little slow, but I've seen GADTs before; don't take my opinion as binding. What tool did you use for making the screencast?
Nice video! Here's the example implemented with TypeFamilies instead of GADTs: {-# LANGUAGE TypeFamilies #-} data ExprInt = I Int | Add ExprInt ExprInt | Mul ExprInt ExprInt data ExprBool = B Bool | Eq ExprInt ExprInt class Eval e where type Res e eval :: e -&gt; Res e instance Eval ExprInt where type Res ExprInt = Int eval (I x) = x eval (Add x y) = eval x + eval y eval (Mul x y) = eval x * eval y instance Eval ExprBool where type Res ExprBool = Bool eval (B x) = x eval (Eq x y) = eval x == eval y I wonder: Is it always the case that one could use type families instead of GADTs? 
I really like the format of this video! As to the content: I don't think the step into phantom types and smart constructors is that useful, especially since it is a dead end (w.r.t. pattern matching). I think you could just jump from the extended language to the GADTs. &gt; I have also written an equivalent explanation in text form. Which one is better to learn from, which one do you prefer and why? For me personally, I like the video better. I'm dyslectic and just don't want to read anything over 140 characters, but I'm happy to spent an hour watching a video on an interesting topic. But of course there are also people that prefer (non-linear) reading over a watching video. So I think it's great that you have both, it increases the size of the audience. By creating the content in two formats, I think you increase the quality of both. If you're planning on doing more of these: please do!
I really enjoyed that. Liked the gentle progression from very simple language, to extension of that language, to using phantom types and finally to GADTs. The pace was just fine, in my opinion. The format of lecture and blackboard worked really well here, particularly with the subtle detail of typing the Haskell and writing your annotations. Mmm, more inventive Apfelmus teaching :-)
thank you for making this video. it was a great introduction! (feel free to make more! ;)
 #7 (libraries) is the biggest reason i don't use haskell for real projects and for most companies it's a show stopper. this could be fixed with a stable jvm ffi. java excels in stable libraries that communicate with the outside world and run on any platform. it also doesn't have a preprocessor which makes a lot of c libraries a pain in the ass to write ffi wrappers for.
&gt; I wonder: Is it always the case that one could use type families instead of GADTs? Possibly (I'm not sure either way), but if so, probably only in a loose sense. For instance, look at the type of your eval: eval :: (Eval e) =&gt; e -&gt; Res e It's rather different from: eval :: Expr t -&gt; t And with good reason, because GADTs and type families are rather different constructions. GADTs are similar to inductive families in Agda. You are defining a family of types (in the Haskell special case, T :: * -&gt; *) by specifying how the family as a whole is closed under some constructors. Then, you can write functions polymorphic over the index of the family, while only handling the restricted indices of each constructor. When pattern matching, this makes a match against a constructor (potentially) refine types. By contrast, type families (and type classes) treat the universe of types as if it were inductively defined, enabling you to define types and functions by case analysis on other types. This ends up meaning the flow of the information goes the opposite way: doing case analysis on a type informs you of what constructors you're allowed to match on. You can do analogues of both in some dependently typed languages. For instance, we can define sized vectors in two ways: -- Inductive family data Vector a :: Nat -&gt; * where [] :: Vector a 0 (:) :: a -&gt; Vector a n -&gt; Vector a (1+n) -- Case analysis Vector' :: * -&gt; Nat -&gt; * Vector' a 0 = () Vector' a (1+n) = (a, Vector' a n) but they turn out to be different to use in practice. When you write a function: foo :: Vector n a -&gt; ... You (can) match on [] and (:) immediately. By contrast, writing the same: bar :: Vector' n a -&gt; ... you must first match on the `n` parameter, to determine whether you have an empty vector. The latter becomes inconvenient for a lot of things. For something a little fancier, what about this: data Expr a where Inj :: a -&gt; Expr a Add :: Expr Int -&gt; Expr Int -&gt; Expr Int Mul :: Expr Int -&gt; Expr Int -&gt; Expr Int Eq :: Expr Int -&gt; Expr Int -&gt; Expr Bool Case :: Expr Bool -&gt; Expr a -&gt; Expr a -&gt; Expr a eval :: Expr a -&gt; a eval (Inj x) = x eval (Eq x y) = eval x == eval y eval (Add x y) = eval x + eval y eval (Mul x y) = eval x * eval y eval (Case b x y) = if eval b then eval x else eval y It's probably still doable, but you'll end up with a bunch of redundancy. Or simply an identity type: data Id t u where Refl :: Id t t That can be encoded otherwise, but not in a way that's as convenient to use.
&gt; What can we do to help? Thanks for the offer! The admin team was recently significantly expanded, so I'm not sure if we can take on more people for the move itself. Still, if you have experience as a Unix admin and want to put in some time, give us a buzz at **support** at **community** dot **haskell** dot **org**. Besides that, there is always plenty to be done. Fix some [Cabal bugs](http://hackage.haskell.org/trac/hackage/report). Code up a mirroring scheme for [Hackage](http://hackage.haskell.org/trac/hackage/wiki/HackageDB). Use your imagination! Or help Baughn organize more computers for us!
If you can get people together and organize for us one or more additional machines for the community, that would certainly be helpful. The more mirrors and backups we have, the better service will be.
Below's a type-families version of your example with "Case". It did end up longer: class Eval e where type Res e eval :: e -&gt; Res e data Id a = Id a instance Eval (Id a) where type Res (Id a) = a eval (Id x) = x data Add a b = Add a b instance (Eval a, Eval b, Int ~ Res a, Int ~ Res b) =&gt; Eval (Add a b) where type Res (Add a b) = Int eval (Add x y) = eval x + eval y data Equ a b = Equ a b instance (Eval a, Eval b, Int ~ Res a, Int ~ Res b) =&gt; Eval (Equ a b) where type Res (Equ a b) = Bool eval (Equ x y) = eval x == eval y data Case a b c = Case a b c instance (Eval a, Eval b, Eval c, Bool ~ Res a, Res b ~ Res c) =&gt; Eval (Case a b c) where type Res (Case a b c) = Res b eval (Case cond ifTrue ifFalse) = if eval cond then eval ifTrue else eval ifFalse 
So Res e is a type which is associated with the type parameter e?
yes. Res is a type constructor
Nice video! There is a small error though: In the phantom type example, if you define the `Eq` constructor as | Eq (Expr a) (Expr a) then your definition of `eq` eq :: Expr Int -&gt; Expr Int -&gt; Expr Bool eq = Eq won't compile. You have to define `Eq` as | Eq (Expr Int) (Expr Int) 
I always thought of that as a type name, I didn't realize type constructor was the same thing. 
Well, that's one option, I suppose. One problem with that is that the actual concrete types are absurd, and if you add multiplication, separately from addition, you can end up with something like: Case (Eq x y) (Add x y) (Mul x y) :: Case (Eq (Id Int) (Id Int)) (Add (Id Int) (Id Int)) (Mul (Id Int) (Id Int)) (Edit: Oh, and I just realized another caveat with this part, I can write: Case (Id 5) (Eq (Id 6) (Id 'c')) (Add (Id "s") (Id 5.5)) :: Case (Id Int) (Eq (Id Integer) (Id Char)) (Add (Id String) (Id Float)) and not get a type error. The error only comes when I try to `eval` that expression, and there's no instance of `Eval` for that type (hopefully). So, the types that make up the building blocks of your syntax tree don't enforce any kind of correctness invariant on their own. The only (or easiest) way to get a type whose only inhabitants are well-typed syntax trees is to reify the `Eval` instances and all their associated equality constraints into a type using existential quantification, which is essentially a roundabout way of making a GADT, as described below.) The other problem comes if you try to rectify that by writing some sort of existential quantification around the `Eval` class. You have a lot of equality constraints floating around there, and identity types like that are primordial GADTs (so at that point, you may be implementing a GADT using the primitives the way GHC does). In fact, you can (I believe) make any inductive family using only identity types and existential-like quantification: data T :: * -&gt; * where C :: ... -&gt; T E -&gt; ... -&gt; T R becomes data T :: * -&gt; * where C :: forall a b. a ~ E -&gt; b ~ R -&gt; ... -&gt; T a -&gt; ... -&gt; T b So once you start throwing equality constraints around, you're straying toward GADT territory. GHC takes the equality constraints as primitive, and uses them all over the place (like newtypes, even), though, including in type families, presumably, so it's hard to tell them apart.
&gt; I'm [dyslexic](http://www.merriam-webster.com/dictionary/dyslexic)
It seems to me that this is a variant of the tagless final representation, essentially with the transformation of a tagged structure into a tagless representation fused with a particular evaluation of the tagless representation. http://okmij.org/ftp/tagless-final/course/
huh? Most of that article seemed like barely coherent gibberish...
&gt;I don't think the step into phantom types and smart constructors is that useful, especially since it is a dead end (w.r.t. pattern matching). I think you could just jump from the extended language to the GADTs. The step into phantom types is the most important part. It is the part that makes you understand the thing.
`vim` spell check says they're both fine.
I can recommend eating applesauce while watching this video. ;) I've used [ScreenFlow](http://www.telestream.net/screen-flow/overview.htm) (Mac) to create the screencast. The nice thing about this program is that it's actually a general video editor.
I like ScreenFlow for screencasts, it allows for nice post production. What is the canvas your drawing on? What tool is being recorded?
Dang, I totally missed that! And now I discover that it's much more difficult to correct errors in a video than in a text. :-) 
The graphic tablet I'm drawing on is a Bamboo Fun Pen &amp; Touch S . The software I'm using to draw is Adobe Photoshop Elements 6, which was bundled with the tablet. (Which is fortunate, because the pen moves are really jerky when I'm trying to capture them with GIMP on my Mac). I created a custom color palette and a document with a black background.
what happened to thread-ring?
one little comment: you could've tried changing the type of constructor Eq, and explaining what happens then: Eq :: Eq a =&gt; Expr a -&gt; Expr a -&gt; Expr Bool You can compare two bools as well!
there is also another small error at 03:00: eval (Mul e1 e2) = eval e2 * eval e2
Curious why they chose LGPL instead of BSD3.
IIRC, it mostly came down to what Ryan could get past corporate.
I specified the grapefruit because I usually can't eat a grapefruit and get Haskell done at the same time (it's too messy).
Oops, thanks!
&gt; I really like the format of this video! Thanks. :-) &gt; So I think it's great that you have both, it increases the size of the audience. By creating the content in two formats, I think you increase the quality of both. Originally, I made the video also because a presentation somehow takes less effort to create than a well-formulated written text. But after the sixth take, I figured that my words would work just as well in print. I think that [Graham Hutton's book](http://www.cs.nott.ac.uk/~gmh/book.html) is so crystal clear exactly because it's a distillation of his lectures. &gt; If you're planning on doing more of these: please do! Stay tuned. ;-)
Thanks. :-) &gt; The format of lecture and blackboard worked really well here, particularly with the subtle detail of typing the Haskell and writing your annotations. Indeed, and the ability to draw pictures and annotate source code with red squiggles offers considerably more freedom than an ordinary text.
I like Graham's book, but to be honest, I don't think I've read all of it. But I have watched all the videos in Erik Meijer's [lecture series](http://channel9.msdn.com/tags/C9+Lectures/) based on the book, so again the multiple format setup really works! &gt; Stay tuned. ;-) Exciting!
This is an excellent form of feedback for package authors.
I'm going to blame Merriam Webster. When I looked it up (I've never seen that variant), I thought they said the word [doesn't exist](http://www.merriam-webster.com/dictionary/dyslectic), but I didn't read carefully. It's just not in their free version. Stop using expensive words! I can't be subscribing to premium dictionaries just to understand you. ;)
I could have sworn I tried this last night and it wouldn't compile. It was because I didn't have the (Eq a =&gt;) part I bet.
Would this subject make a nice little book, Don?
This is a feature I've been wanting for awhile.
I don't intend to post something here for every meeting, but just in case someone out there is in this area, we're just getting this started.
Nice. Frequently asked question (including by me), "How much Haskell is written with the STM?" [Partial answer](http://bifunctor.homelinux.net/~roel/cgi-bin/hackage-scripts/revdeps/stm-2.1.2.0). (I say partial because one would assume that even with that list, STM is something that tends to be used in apps, not libraries.) In another recent discussion dons characterized stm as essentially solved and something Haskell users can "just use" without drama; I'd say that backs that claim up nicely.
(Note that you can also sort the columns by clicking on them.)
There is also a patch against hackage-scripts which adds the reverse dependencies functionality. [Relevant ticket](http://hackage.haskell.org/trac/hackage/ticket/576). But the live demo has some minor changes which are not in the patch (like sorting columns with a little Javascript). 
what about --clean like in [rubber](http://www.pps.jussieu.fr/~beffara/soft/rubber/) instead ?
&gt; Now one needs to start working with globs or utilities like 'find'... Oh no! Not _globs_! EDIT: I'd much rather have GHC interact nicely with build systems than be a build system. Given the module system, the latter is probably easier unfortunately.
http://www.pps.jussieu.fr/~beffara/soft/rubber/doc_2.html#SEC2 That sounds like it does nothing; but we want ghc to leave behind the xmonad file which we're about to run.
Yes, but sorting a list of Doubles (as it is currently done) is wrong. This is fundamentally the problem of Float and Double: do you want do be able to do something cleanly and wrong, or not do it at all? Having worked on a number of projects that use floating point numbers in an essential manner, I can't say that I've ever desired to use them as keys in a map. Sorting is frequently desirable, but it's not too hard to do that correctly. Besides, whenever you're sorting a list of fps you generally want to filter or handle NANs, since they truly are an exceptional value which typically indicates programming errors. *edit:* I could imagine wanting to use them as keys in a priority--search queue however.
Oh yes, I forgot about those. (I've been too busy implementing length-indexed lists in Coq, which forbids unsafe uses of `head`, `tail`, etc.) I'm strongly in favor of removing the partial functions, or at least giving them ungainly names like `unsafeHead`, `unsafeTail`, etc to signal to the developer that they need to be proving correctness outside of the type system. Of course, part of the problem here is that Haskell's type system doesn't have a good clean way to express "non-empty list" or "a Maybe constructed by `Just`". In the past I've proposed adding simple refinement types in a way that doesn't involve the problems of adding full dependent types, but there didn't seem to be enough support for the idea to make it worth doing the necessary research/implementation. 
The Num instance (or lack thereof) is, I think, more controversial than the Eq instance. When teaching people about typeclasses, one of the big selling points is that we don't need to distinguish all these different mathematical operations like they do in Ocaml with the horrible duplication of `(+)`, `(+.)`, etc. When typeclasses started out they really were all about ad-hoc polymorphism. Since then they've grown into a whole area of sophisticated research, but the Prelude still enshrines these humble beginnings. What sorts of laws and transformations are valid on Float/Double depends a lot on what you're doing with them. One common use case is when you want fractional numbers which are all about the same size. People aren't usually familiar with fixed-point numbers (or don't want to specify the precision, perhaps because they don't know it beforehand), so they reach for the floating-point. Since the numbers are all about the same size, the usual laws of arithmetic will still hold for the sorts of computation used here; and so we'd like to have all the same optimizations available. However, there are other use cases for floating-point such as encoding probabilities in log-space for natural language processing. The sorts of laws and assumptions which hold in this scenario are quite different, and we don't want the compiler to perform invalid program transformations because it makes the wrong assumptions. I think the take home message here is that Haskell should have a robust library for fixed-point arithmetic where the precision is encoded at the type level and thus easily altered on a program-wide basis by using newtype wrappers or type aliases. This would give efficiency for one of the common use cases, without sacrificing correctness. Good fixed-point numbers are one of the reasons Cobol is still alive afterall.
You mean in the Prelude's class hierarchy? Not really, because the Prelude's hierarchy isn't good for advanced mathematics of any sort :) If you mean the numeric-prelude (or similar projects), I see no particular reason why not. If we take "numbers (whatever that means)", as the starting point of mathematics, the discrete/algebraic/logical side of mathematics is all about drilling down to simpler structures in order to understand the minimal requirements that make concepts work, whereas the continuous/vector-space/physics side of mathematics is all about building up complex structures in order to try to capture how things in the world work. While these goals and directions are very different, I view them as complementary rather than as interfering with one another. There are only two problems I see with trying to do both in a single system, and they're both the same problem. The first problem is the sheer complexity of encoding all of mathematics into a single framework. Even if the project is feasible, whether the result will be usable is a separate issue. Even if bureaucracy doesn't kill usability, there's the problem of figuring out which abstraction correctly encodes everything you need and nothing else. Ultimately, this is the very project of mathematics itself, so we shouldn't expect to solve it perfectly. The second problem is whether Haskell's typeclass system is really up to the challenge. One of the big limitations of Haskell's typeclasses is that they offer no mechanism for giving the laws that instances must obey, or of enforcing that instances actually do obey them. A similar issue is being able to automatically derive instances which are a natural conclusion of other instances. For instance, if I prove that some type is a Kleisli triple by giving a mapping from types to types, a definition of `return`, and a definition of `(&gt;&gt;=)`, then we can derive that this type function is also a monad, an applicative functor, a pointed functor, and a functor. Perhaps less well-known, if I can define the group operations on some type then we can derive the quasigroup and loop operations. Or if I make a ring into a boolean ring, then it follows that it's commutative and that it's in isomorphism with some Kleene algebra. Ultimately, sorting out all these laws and the things that follow from them is also an issue of complexity and also an issue of trying to solve the very project of mathematics itself. So I don't expect that we'll be able to do everything perfectly in the near future, but I also think that shouldn't really be considered the goal of projects like numeric-prelude (or, analogously, category-extras). I do think it's a worthwhile project and that we could make enough progress to give a usable interface for people to write code with a bit more mathematics than they're using already. Certainly, nothing precludes doing both discrete and continuous styles of mathematics in a single framework.
But is it fair to say that Haskell is controlling the Arduino if Haskell is used to host an EDSL that generates a program that later runs without the Haskell runtime? I haven't used Atom, but I'm curious about what limits it imposes in terms of expressivity, there must be some drawbacks compared to actually running Haskell code.
Automated derivation based on existing instances sounds like it would be something useful to do. You can approximate this right now if you allow IncoherentInstances.
True, though IncoherentInstances do raise their own problems. We can also fake it a little bit by defining `liftM`, `fmapDefault`, `foldMapDefault`, etc for people to use when writing boilerplate instances... but that doesn't relieve them from having to write the boilerplate in the first place. This is one of the big issues for fixing up the numeric hierarchy: noone wants to write instances for seven different singleton classes just to define a field.
How about something like -pipe in gcc?
My build script includes the ghc options -hidir "$builddir" -odir "$builddir" which has always handled this issue just fine for me. This has the added virtue of working with e.g. Mercurial or Unison (ignore this directory, and don't copy it cross-platform).
The second GHC adds some other intermediate file you're going to have to update your build system (or at least the overly bloated makefile for it written in some half assed unnecessarily turing complete language). I'd love for GHC to be a build system, since all existing build systems suck so badly.
This is a great way to see what libraries are considered "standard". And a great way to find real world examples on how to use libraries.
Congratulations: you've just added another build system that sucks really badly to the world. Just like all the other single language ones, it falls down badly on any multi-language project, of which there are many. Heck, it doesn't even handle generated Haskell code very well. Of course, if GHC could nicely tell the build system about dependencies and temporary files... And if we had something better than .hi-boot files for explicit interfaces...
If you're generating haskell code, you're doing it wrong, or haskell is doing it wrong.
I can see useful things to do with Haskell code generation, mostly to do with FFI.
Well, I'm using them as keys right now. Imagine that you have a large recursive data type describing some kind of process or transformation, and this description includes floating point numbers. Now you have a set of these, and you want to associate more values with them. There you are. This a realistic example, but I'm sure there are much simpler use cases too (eg. a priority search tree/queue as you write) Also, maybe sorting floats is wrong, but you still want to do it pretty often. We want Haskell to make hard things (relatively) easy, not the opposite. So while I agree with you that many things we do with floats is not right, I still want to be able to do them if I explicitly choose to, and do them without much hassle. If there is a solution which incorporates both viewpoints, then we should go on and adopt it.
There was an issue with GC finalization and a big warning on the gtk2hs website about it.
Oh, yup, there was a problem with GHC 6.12.1 (I think that version) that was fixed with GHC 6.12.2.
Hi. Was the video ever made available online?
What is it that generating Haskell code will let you do in this case, that you could not accomplish more easily and succinctly by writing sufficiently general code manually, and why isn't that considered a fault of Haskell?
I can barely wait till it works on ghc 6.12!
&gt; Also, maybe sorting floats is wrong, but you still want to do it pretty often. Yes, you do want to arrange a collection of floating point numbers in sequential order; I never claimed otherwise. But doing it the way it is currently done is incorrect because floating point numbers do not form a total order. Since they form a very specific sort of partial order, there is still a useful definition of sorting for that order. I am merely advocating that we use the correct definition of sorting for floating point numbers. My point is not that you don't want to do it, but that *when* you do want to do it, you also want to handle the any NANs in the list. If NANs are showing up (which are what breaks the Ord instance), then you have something exceptional going on. Silently percolating those errors is wrong, IEEE be damned. In particular, why *not* handle NANs at the point of sorting a list of floats? Why should we impose the computational cost of performing a post-hoc filter/map to find and handle NANs, when the sorting algorithm will already have determined which values are NAN? Why not use the type `sortFloat :: [Float] -&gt; ([PureFloat],[NAN])` which allows easy access to do whatever the user wants with the NAN values?
&gt; In another recent discussion dons characterized stm as essentially solved and something Haskell users can "just use" without drama; I'd say that backs that claim up nicely. [Source](http://www.reddit.com/r/programming/comments/c4367/microsofts_experiments_with_software/c0q08s9?context=1)
Have you heard of [hsc2hs](http://www.haskell.org/ghc/docs/6.12-latest/html/users_guide/hsc2hs.html)? This tool allows your code to be more portable and use values derived from local C header files. For example [Network/BSD.hsc](http://code.haskell.org/network/Network/BSD.hsc) becomes [Network/BSD.hs](http://hackage.haskell.org/packages/archive/network/2.2.1.7/doc/html/src/Network-BSD.html).
I'm changing my view to edwardkmett's idea: &gt; More and more Haskellers are making a killing in the financial services sector, taking their relatively unarmed yachts on joy rides around the world, and sometimes happen to pass too close to Somalia.
he will soon give you a tour of his monads
We are made to `bind` eachother.
i am constantly reloading lyah as i write this!
Don't Haskell the Hoff
monads or gtfo!
I'm wondering if it's for Credit Suisse. They were hiring some days ago.
One would have imagined that if it were CS, there would be F# mentioned.
Well, yeah, floating point is not easy... Still, for keys you want a total ordering, and you don't especially care about what exactly that total ordering is. Maybe some kind of IEEE module which handles all the corner cases (including sorting) would be useful, though designing the interface is probably quite nontrivial.
Jane Street also looks for people in that field.
And Barclays, Standard Chartered, some Japanese banks, and some private hedge funds.
of course, David Haskellhoff [understands recursion](http://www.theinternetpics.com/old/71739057_l.gif)!
It's not Credit Suisse, but we are definitely hiring, if anyone's interested.
What section of "Learn you a Haskell" is this part of?
The [logfloat](http://hackage.haskell.org/package/logfloat) package aims to do a bunch of that, though I'm not sure that I know of every corner case. I can add a version of sorting to the next release.
What does "compromised" mean in this context? Hacked?
I was about to ask this. Considering the Yale servers went down for 24 hours just in the past week, this is bad news, especially if the servers have been hacked by someone.
Wow, I'm glad I could download ghc at the office this afternoon
Maybe it's a sign that some of those high-brow Haskell Braniacs should hurry up and get a working web server, already... ;) 
http://snapframework.com/
Earlier this week, after about 48 hours of www.haskell.org downtime, [yitz discussed how the community can help with the server situation](http://www.reddit.com/r/haskell/comments/ca4ls/wwwhaskellorg_has_been_down_for_about_24_hours/c0r7mja). I emailed the [Industrial Haskell Group](http://industry.haskell.org/) to ask if they would help organize the effort. The reply urged caution, noting that "haskell.org outages are rare, and commercial hosting might be a lot more expensive." This was all before dons posted this reddit thread noting the security breach. I'm not suggesting, of course, that fixing the "nobody at Yale can reboot on holidays" problem would fix the security problem. For those wanting to help, [yitz explained that the community.haskell.org server is ailing, too](http://www.reddit.com/r/haskell/comments/ca4ls/wwwhaskellorg_has_been_down_for_about_24_hours/c0r67zf). Anyway, thanks to everyone who works to keep things up and running.
We're working on the IHG aspect. But this is bigger than the IHG -- it needs Haskell.org to exist and look after the open source infrastructure.
&gt; We're working on the IHG aspect. Great! Whether the IHG gets involved or not, please don't hesitate to let the community know how we can help improve the infrastructure.
Haskell is like scientology... you only get to the really weird parts after you're in too deep to get out. Beware!
That recursion is not well-founded! David Haskellhoff evaluates to bottom. :(
Well, I received a message for a haskell.org mailing list ~2 hours ago, so that must be up at least.
http://hackage.haskell.org/platform is fine.
CT has nothing to do with software engineering in Haskell, just FYI.
if functors, applicatives, monads, and monoids are common patterns used in haskell, then doesn't category theory have some influence on software engineering in haskell?
Personally, I feel like CT represents some fundamental understandings of software, period.
It does have some influence. But these are patterns that were found in real code and subsequently abstracted out. The CT connection provides some extra intuition for some. But to many (most?) Haskell programmers, it's just a source of funny names for concepts that are otherwise pretty straightforward and concrete. 
Did you [read](http://book.realworldhaskell.org/read/) *Real World Haskell*? It has some engineering topics like library design, profiling, and testing.
still working through it. currently i'm on the monad chapter. i've also read the typeclassopedia. and i've read thompson's "haskell: craft of fp" and much of hudak's "learning haskell through multimedia" book.
I think it's more just that Haskell occasionally beats up category theorists and steals their l̶u̶n̶c̶h̶ ̶m̶o̶n̶e̶y̶ useful terminology.
I'm also learning Haskell at the moment and I really wish there was some sort of mentoring program in place. There are a lot of great books and blogs about Haskell (I'm currently reading Real World Haskell as well) but there are certain things that go beyond learning the syntax of the language. When I watched [Graham Hutton's video on the countdown problem](http://channel9.msdn.com/shows/Going+Deep/C9-Lectures-Dr-Graham-Hutton-Functional-Programming-Fundamentals-Chapter-11-of-13/) I was impressed by one of the idioms he used to make his code more simple and elegant. Those are some of things that are hard to learn from a book but a mentor could teach you. Someone to read your code and say "you're missing a trick here."
They're not evil. But it looks like the hand optimizations from 8 years ago are broken. Want to submit a revised version? The [original](http://shootout.alioth.debian.org/u32/program.php?test=knucleotide&amp;lang=ghc&amp;id=2) was about the same as the C code. Profile the current one, fix whatever broke, resubmit.
Exactly.