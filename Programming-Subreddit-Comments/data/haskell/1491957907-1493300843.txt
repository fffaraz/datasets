In type signatures: round :: AlwaysRoundDown -&gt; ErrorOnBoundary -&gt; Double -&gt; Int vs. data Rounding = HalfUp | HalfDown data Boundaries = ErrorOnBoundary | IgnoreBoundary round :: Rounding -&gt; Boundaries -&gt; Double -&gt; Int At call site: round False True 5.3 vs round HalfDown ErrorOnBoundary 5.3 I'll definitely take explicit types over type synonyms for this case! And that's before we even get around to observing there may be more than two possible rounding approaches; with a type synonym for `Bool` it's going to be expensive to introduce, say, away from zero rounding, yet with `Rounding` it's just a matter of adding ` | HalfAway` to the data sig, and existing call sites need no change. The sixth case from TFP is the one I'm comfortable with - when you're using a type synonym as an _abbreviation_ for a more complex type.
Honestly, I'll take whichever is easiest to understand + use with `:t` and `:i` , which would be the first. The first would also work great with data ThingGettingRounded { reasonThisMatters :: Bool , alwaysRoundDown :: Bool , errorOnBoundary :: Bool } deriving Generic instance ToJSON ThingGettingRounded 
I believe you can get value-level Nats into types with [reflection](http://okmij.org/ftp/Haskell/tr-15-04.pdf). Choosing which instance to pick from is decided at compile time, but constructing the instance dictionary is done at runtime if really necessary. So this works.
I think `type` makes most sense with `forall` only because we don't have a good story with existential types. It should be the dual of functions/lambdas; instead it's treated like the bastard cousin.
Cool! Though I'm always wary of continuous deployment - unless it's only used in conjunction with real live human testing as part of the CI, it just seems so unnecessarily risky. That being said, I guess there's no reason why you can't include various human QA tools as part of the CI test suite!
Still needs a bit of work :)
Did you mean titles?
Thanks! I'm very hard of hearing; it's nice to have notes instead of trying to hear.
I do not think Continuous deployment is meant to be used in production. We for example use it only for staging. And as a bonus artifacts for production deployment get compiled automatically and pushed to production environment with one command when needed. 
Is it used somewhere? I've found that reflex-dom should be great for this - other, VDom based libraries interfere with stuff managed by the MDC (basing my experience on MDL, but I guess it's similar).
What is the reason for GHC not to use stack instead? Is it because of legacy or are there any practical reasons?
&gt; Reason 4: Type errors are misleading Aside from randomly flipping between alias and expanded, if you accidentally pass a triple or the wrong type, you get a message like "couldn't match (Name, Age, Int) against Person" when you'd prefer to see what you're supposed to be providing: the actual tuple type. Try `-fprint-expanded-synonyms`.
It is a library I am building internally for my team when I have a few moments to code, but we're not using it anywhere yet. The best example is the one in the project, which isn't very complex at all. All it really is are function wrappers for MDC css classes, although as you'll notice with issue #1 there are some quirks that need figured out.
I'm excited by `reflex`; I read through /u/RonaldSenn 's [Tutorial] (https://github.com/hansroland/reflex-dom-inbits/blob/master/tutorial.md), and damn, those are actually legible frontend prgorams. But I can't get any nontrivial project to build. `cabal`-hell has been exorcised... now there's `cabal --ghcjs`-hell! For example, I wanted to modify the example package, but I couldn't build it: https://github.com/reflex-frp/reflex-todomvc/issues/14 (also just tried it on a fresh clone). I tried using the single `hello.hs` file, but I couldn't add local packages (to try to manually fix the broken dependencies): https://www.reddit.com/r/reflexfrp/comments/631cx8/adding_local_packages_to_tryreflex/ Then I tried building my package outside the try-reflex environment, both with stack (it took hours) and with nix (both on NixOS). They worked on a trivial package, but when I tried my actual package, they failed on building some (transitive) dependency; `lens` or `haskell-src-meta`, I think (and I want FULLHASKELL, as otherwise I'd try Haste or Purescript over GHCJS). I can't seem to specify the version/snapshot under `nixpkgs`, so I vendored several libraries (`reflex` seems to need older versions of libraries, like `ghcjs-dom`), built them without the cache (it took a while), the compilation failed, I tweaked the version bounds, and repeated this several times before I gave up. And I'm happy to help, but `reflex-platform/default.nix` made no sense to me. I thought about trying `reflex` with a non-JavaScript backend (like a "`reflex-gtk`"? I saw that `reflex-dom` can be built under ghc with WebKit and WebSockets or something), but installing even `reflex` with `nix-env` under GHC8 didn't work. Then I thought about implementing a simple backend for terminal emulators (via ncurses or something, like with a single TextInput-style primitive exposed, and dynamics only for setting the color and getting the text and keyboard events) to play with, but the only alternative backend I found was the pure one in the test suite, and neither it nor the Spider(?) one make any sense to me (documenting the internals would be extremely helpful, like how exactly DOM elements/events are integrated with the reflex system). So, while I asked for help with debugging on github and /r/reflexfrp, I guess that here I'm asking about the plans for `reflex` this year: newer versions, documentation, a native backend, etc. (Sorry if any of what I said made no sense, I'm new to both nix and frp.) 
What is your solution for this problem, `newtype`? type Name = String vs newtype Name = Name String Personally I prefer `type`, because `newtype` requires all this wrapping and unwrapping. 
I think Purescript-style records solve this problem nicely: `{ something : String | a} -&gt; b` entails not just that you're passing a `String` to this function, but that you've explicitly named that string `something`.
It's fine! That would be great, but the outline that the OP gave is definitely enough for me to get everything he was saying. Most of it isn't new information, I just like reading the same information presented multiple ways because it helps me grok it better.
True enough, I guess I've just read too many horror stories XD
We use reflex with stack at our work. We have two setups: - building with GHC for faster compilation time (only works on Linux for now) - building with GHCJS for actual build Here's how our config looks like, please give it a try https://gist.github.com/k-bx/417d81f98e362f74954a7dc74d7ec466
Interesting...as far as I see LayoutGrid is CSS only component, wondering what those quirks are (haven't checked it out yet). The problems I've described arise when there's conflict between what MDC does with its JS and between what your app does. But seems those components are not heavy on JS side and reflex-dom tries to change only what it needs within DOM so I hope this will prove fruitful:)
I'm glad you said it was good, because I am 3 pages into the script already. ;) At 12:27 in the video, I am stopping for the night. 
I did this some time ago, maybe this will help you: https://github.com/blender/Pushka/blob/master/app/Main.hs
My problem with nix in development with haskell is that it uses full rebuilds every time I seem to change something, how do you solve this at Takt?
These are pretty uncontroversial points which I think most of us can agree with. Though I don't understand point 6 - why type synonyms being necessary is a reason for *disliking* them? Also is there any reason GHC cannot solve the partial application issue? I really hate that! In any case, let me note a few places where I think they are useful: * as a very lightweight form of documentation * as a temporary device during development (as in: `type Name = String` - TODO: replace this with a proper type later) * as shorthands for long type expressions * when wrapping / unwrapping newtypes is simply too much pain * in the case when you want to switch a whole module from using one type to another, say from Double to Float or vice versa (Haskell should have had parametrized modules since last millenia; alas, it does not have them. Maybe backpack will help with this) 
the &gt; ghcjs-0.2.1.9007003_ghc-8.0.1 means that the server should be ghc801, right?
How to get a version of Cabal-install that understands the new Backpack features? Should I follow the `Build Cabal for hacking` instructions in the GitHub repo?
This hilarious warning scrolled by: &gt; [26 of 49] Compiling Gen2.Prim ( src/Gen2/Prim.hs, .stack-work/dist/x86_64-linux/Cabal-1.24.0.0/build/Gen2/Prim.o ) &gt; /home/sboo/.stack/programs/x86_64-linux/ghcjs-0.2.1.9007003_ghc-8.0.1/src/src/Gen2/Prim.hs:77:1: warning: &gt; Pattern match checker exceeded (2000000) iterations in &gt; an equation for â€˜genPrimâ€™. (Use -fmax-pmcheck-iterations=n &gt; to set the maximun number of iterations to n) I guess I'll let you know if it worked in the morning!
Well, that's the same HSCurses version I have. You should be able to find your NCurses version with your distributions package manager.
I probably won't have time today, but if you want to look it up, it's the latest packaged version in Debian Stretch. 
I pretty much always use aliases while developing and newtypes when publishing. 
Good stuff, although I was anticipating something else with 'real-world' title (probably my bad to anticipate more ready-recipes, but I'm not dabbling with Reflex at this moment so haven't devised anything on my own...and probably it would be more boring :) ). This seems more like 'Intermediate Reflex' talk. Would be great to have following things covered somewhere too: * How to lay out the general architecture - some widgets should be standalone (fully polymorphic in the MonadWidgets' `m`) and some should operate in some kind of 'App' monad I guess, to have access to common funcionality and 'sit' within the application. * What's needed for such 'App' monad? Possibility to read global configuration, global events, writing events (I remember u/mightybyte toying with `EventWriter` idea some time ago, curious how that makes certain 'wirings' easier). * How to wire widgets best in the context of API requests (WSS/AJAX) and sprinkle the common functionality on top of that (notifications, error handling especially - how can it make handling erroneous cases easier). * What are gotchas when integrating with external components - be it standalone components like some calendars, code editors etc, and components that affect your layout.
What do you mean by that? Nix just builds what you change and the things that depend on this.
But it also forces the coder to declare that this particular string _is_ a name whereas a `type` does not.
It's an encoding issue. No problem with `LANG=C cabal exec quark`, but with my default `fr_FR.UTF-8`, it's broken. Maybe you could try to reproduce on your side?
The [elm package](http://package.elm-lang.org/) website is written using a mix of Haskell and Elm. [source](https://github.com/elm-lang/package.elm-lang.org).
I agree that that is what he was referring to. That is also the type he gave.
I guess my point (which I made rather flippantly ðŸ˜œ) was that I find that style of comma distasteful because as a native English speaker/reader I expect commas to mean a certain thing and look a certain way. My comment above makes sense, but it's not easy to understand at a glance! Many elements of any given programming language were designed to resemble their corresponding English grammatical constructs. In Haskell (and many other PLs) commas are used to separate items in a list - the designers borrowed both the syntax and the semantics of (that particular use of) commas from English. If you start messing with the way commas look you lose some of that correspondence which in turn makes code perhaps easier to edit but harder to read. All this is to say I think it's not as cut and dried as _Haskell is not English, so you shouldn't expect it to read like English_. It's a spectrum - some languages (COBOL, as you pointed out) overdo it in one direction and try to look too much like natural language, and others (how about APL) overdo it in the other direction and don't look enough like natural language.
I would like to add ClojureScript to that list. I have tried Elm, PureScript, Haste and more and I ended up liking ClojureScript the best. It's not statically typed^*, but its lispiness makes up for that in expressivity. I am very fond of the libraries reagent and re-frame, in particular. ^* I haven't looked into it properly yet, but there is clojure.spec, which can perform runtime validation on your data. EDIT: Just in case someone is interested: I'm currently using CLJS for the frontend and Haskell(Servant) for the backend. While it is probably not that hard to write something that lets servant generate CLJS functions for its API(you can easily call the JS functions from CLJS as well), I chose to use swagger(servant-swagger) and a CLJS library called martian that lets me automatically explore and consume the API. It turned out pretty decent, since this means I kind of *have* to document my API, and that gives me access to it on the frontend as well. martian will also tell you if you are missing any parameters(it uses clojures.spec, I believe) when using the API, which is kind of like sharing types if you squint really hard.
Ah, I see my error now.
 LANG=fr_FR.UTF-8 quark works just fine for me. Even updating quark.cabal to force recompile and doing LANG=fr_FR.UTF-8 cabal build LANG=fr_FR.UTF-8 dist/build/quark/quark the characters are printed correctly.
Yea `nix-build` is not meant for incremental building in a dev environment. This is the reason `nix-shell` exists. My favorite way to handle it is `nix-shell --run "cabal configure"` to set up a cabal environment. Then you can freely use `cabal build` from outside the Nix shell, without using any of Cabal's dependency management.
Yes, I did. Auto-correct or typo.
+1 to ClojureScript. I prefer Haskell, but Clojure *is* a well designed language, and ClojureScript is by far the best story for "write code that can run on the server and the client" that I have seen. I would love for GHCJS tooling to catch up to the current CLJS state of the art.
For now, we're using nix-shell and similar to set up build environments with nix that allow for incremental rebuilding. We're working on tooling to improve/generalize this though!
Using yesod with elm and love it ... what do you want to know?
A CD model requires a different branching discipline. For example you can do all of your work against a 'development' branch which is continuously deployed into a development environment, and then only merge into QA/staging/prod when tests (including manual tests) pass at the previous stages.
Heh, what a coincidence. When I was putting the talk together I actually did have a working title of "Intermediate Reflex" for awhile. Then I went back and looked at the BayHac talk signup page and saw that I had title it as "Real World UIs with Reflex", so I changed it to something similar. I also do have thoughts on all four of your bullet points, and will probably be working on more talks along that theme. At one point I was titling this talk "Intermediate Reflex Part 1" to suggest the ones coming later. The manifestation of some of these ideas can already be seen in https://github.com/mightybyte/hsnippet.
Yes...but using |&lt; and &gt;| would let you write "File exists : " |&lt; doesFileExist foo &gt;| "' Its not a good idea to try and use the same operator to build raw FMT and and IO FMT. As stated the op's question is a version of "How do I get a value out of IO" and of course the answer to that is "You don't, you put functions _in_." 
This makes me think about what I've done with my life in the past semesters... And now I cry.
I don't claim to have some enlightenment on this issue but I'll chime in nonetheless. First, I think `unsafePerformIO` is virtually never the right solution in application code (I make exceptions in high-performance scenarios where you are trying to convince Haskell that something really is safe and it can't know otherwise because you're doing crazy things, maybe in C). Second, when it comes to logging, I always ask myself, "Am I *really* going to use this log message?" Because logging is relatively quiet, I tend to use it mainly for analytics and when something could go horribly wrong in a way I don't yet understand. I can't be sure for your case, but it smells like a situation where the value of logging is pretty low. Lastly, if you decide that a log message really is important, then there's no use in trying to make your function signature pretty. Honesty is its own form of beauty and being honest about something messy is better than trying to massage it into a pretty type signature. That said, you can always break things down into pure/pretty parts and ugly/logging parts and attack it that way. As to *how* you log, I have nothing to say about that.
moar tyyyyypes! This is a great example of [a good place to use a type parameter](http://www.parsonsmatt.org/2017/04/08/maybe_use_a_type_parameter.html). At some point, you want: data FullName = FullName { the fields you want } and since this data is optional, you'd store the optional bits in a type parameter: data FullName title = FullName { fullNameTitle :: title, ... } And your deserialization function can have two shapes: -- this is where the `Maybe` is value level fromJSON :: Value -&gt; Parser (FullName (Maybe Text)) -- this fails to parse if the title isn't present' fromJSON :: Value -&gt; Parser (FullName Text) -- this lifts the presence/absence to the type level, but also the value level: fromJSON :: Value -&gt; Parser ( Either (FullName ()) -- Left is statically known to be no title (FullName Text) --Right is statically known to be a title ) These various parsing functions allow you to shift *when* you know that you have a partial record, which allows you to record more cleanly that you have one in a static manner. It's also easy to write functions to convert between them: convert :: FullName () -&gt; FullName Text convert = fmap (const "") convert :: FullName Text -&gt; FullName (Maybe Text) convert = fmap Just convert :: FullName (Maybe Text) -&gt; FullName Text convert = fmap (fromMaybe "") Now, in your individual call sites, you can return as much information as you want to the caller, who can then decide how to handle it. So `parseName :: HStore -&gt; FullName x` can be called at whatever level of information you think you need, and that information gets propagated back up the call tree until you can log or handle it as you need it.
Since type-3 grammars can be parsed by a DFA, I suspect you can always do it with a foldable container of tokens.
Parse your `HStore` into proper data (e.g. `FullName`) early in the pipeline, in your external-facing logic, where all the DB, logging is. The rest of the code should never deal with a half-fledged `HStore`s like that, and therefore is completely free from this problem. Then there's a question I just have to ask: what do you return if there really isn't a full name?
Came for the frost elves and magic incantations, stayed for the (still) interesting article. However, I have to admit I was hoping for something a bit more impractical and a very esoteric programmer...
I use elm for my front end and servant on the backend and they work really well together. Elms web-socket support is really nice; it can be integrated with backend using WAI. A plus for elm, that some might care about, is that it now has its own virtual dom implementation which seems to be faster than others out there. One of the disadvantages of elm is the support for decoding complex JSON structures, its doable, just not very elegant.
Hey folks, We're a Haskell-based startup in London focusing on building a typed, composable, microservice platform for data analytics. We've just released our first public beta today, along with launching an online demo/sandbox. We hope to bring many of the ideas from functional programming, such as typed APIs and composition to the analytics space and it would be great to get some feedback at this early stage. Cheers! P.S. We're hiring - https://angel.co/nstack/jobs 
Furthermore, a number of common operations can be selectively enabled with `GeneralizedNewtypeDeriving`.
any missing component of a name is replaced with a blank string. 
I think you misunderstand. I don't want to deal with Maybes for an extreme edge case. Log and proceed with an empty string for missing values. Think of utf decoding. Do you want to display nothing at all, or *some*thing with some gibberish?
I think you misunderstand. I don't want to deal with Maybes for an extreme edge case. Log and proceed with an empty string for missing values. Think of utf decoding. Do you want to display nothing at all, or *some*thing with some gibberish?
One minor gripe (largely explained, I think, by the joke) is that some fundeps were missing. Most obviously, it should surely be class Not a r | a -&gt; r, r -&gt; a There are also two more that should be possible for list append, although I'm not sure if GHC will accept them.
That makes sense, although I'm curious if there were something that could still be built on functor.
In my experience, best solution would be to fix the source, if it's your database, you can do that. There is no sense in writing this to the log, you know that there is something broken even before some third party will try to interact with your service. If I can't fix the source, I'm usually using some low level type just to parse this data, it should represent the actual data format that you are receiving (with `Maybe a` for your title). Then you can transform it into something more convenient for your application domain. This way will allow you to keep your parsing pure.
I would assume you can consume a structure `( Foldable t, Functor t) =&gt; t a` after you construct an efficient DFA from the `Functor` instance using the method described by Brian Hurt here: https://youtu.be/QVdBPvOOjBA
If you want to log, that's an effect, so you have to either 1) extend logging down the call tree to the functions you care about or 2) propagate information up the call tree to the functions that have effects. If your data schema doesn't enforce that `title` is present, then you must account for that in your data model somehow. If you want to know which records don't have a `title`, you can identify that in other ways that is much easier and less noisy than logging.
Yes, that would be great. I have mostly been focused on PureScript myself, but its reliance on JS tooling is just painful. I was reluctant wrt. memory usage of CLJ's JVM tooling(after writing Scala on a big project on a horrible laptop), but it's not as bad as my Scala experience, and the ease of use basically makes up for it. Getting something that live-reloads in the browser takes like two minutes. Coincidentally, the same applies to writing apps for iOS or Android, since CLJS has re-natal which is just React Native in CLJS. It's amazing. In comparison, I spent hours fighting webpack and friends trying to get the same setup for PureScript. And when I succeeded, it felt very slow(admittedly, my laptop is utter crap). Naturally, in a perfect world, I would be writing Haskell for browser, the app and the backend, sharing everything everywhere, with hot reloading. I hope we get there sometime.
Fair point that I should have been more specific. I was referring to avoiding infinite grammars (although it possible other ways around it that I don't know of).
&gt; Monadic parsers can parse RE grammer. Not quite true. You need some regular Haskell processing on your strings for that, or you can generate an infinite monadic parser. Without those restrictions, applicative parsers can also recognize RE grammers. 
You also shouldn't be able to use regular Haskell functions to process the input or matched substrings. Think of it in terms of declaring a parser generator DSL. No Haskell functions can occur in the generated parser, and the generation needs to halt. Then it becomes easier. Btw I did my thesis on this subject. I can answer in depth question. 
 dfa :: (Foldable f) =&gt; (a -&gt; s -&gt; s) -&gt; s -&gt; (s -&gt; Bool) -&gt; f a -&gt; Bool dfa t q0 = (. ($ q0) . foldr (\a f -&gt; t a . f) id) 
You cannot build it on Functor, for instance because if we just have an arbitrary Functor we cannot distinguish between the Constant Functor and a Nonempty list, we can only modify their insides respectively.
I'm not sure I understand exactly what you're trying to achieve, but I've just written a blog post about aggregation and grouping in Haskell: http://www.timphilipwilliams.com/posts/2017-04-12-nested-datacubes.html Extensible records would make an excellent choice for both keys and values (measures).
I'm sorry, nobody is going to check that, and just because you can write a type signature that seems to mach doesn't mean you've made a DFA. `(s -&gt; Bool)` This part worries me. How much non-Foldable Haskell logic can happen here? Can you just recognize any input and use Haskell to decide, i.e. get to type-0? Edit: I figured if I was right I wouldn't have to check, cause the compiler can do it for me. So yeah, this is not a DFA. dfa (:) [] :: ([a] -&gt; Bool) -&gt; [a] -&gt; Bool 
Given that s and a are finite types, this is a DFA. Edit: It is just a function which takes the transition function, initial state, accepting state recognizer, and a string, and says if the DFA defined by the first three things accepts the string.
&gt; Also is there any reason GHC cannot solve the partial application issue? Partial type applications are equivalent to supporting full type level lambda abstraction, since you can define the S and K combinator as synonyms. If you want type inference to be stable modulo beta reduction, then you need higher order unification, which is undecidable. Requiring synonyms to be fully applied lets you sick with first order unification, which is decidable (indeed, linear time).
well, I built lens (which is where the previous attempt with stack failed), but now it errors with Configuring zlib-0.6.1.1... Cabal-simple_mPHDZzAJ_1.24.0.0_ghcjs-0.2.1.9007003_ghc-8.0.1: Missing dependency on a foreign library: * Missing (or bad) header file: zlib.h * Missing C library: z Which seems like a NixOS issue, not a ghcjs issue per se, but I can build that version of the package with ghc, and the workaround didn't work. Or, it's a stack issue solved by `stack --nix`, which doesn't support ghcjs. idk.
You are right, our ghcjs version is quite a bit bigger than the PureScript version and also slower at runtime! But it works well enough for now and we will look into ways for improving it further. - hopefully there is still is some low hanging fruit in reflex - or in ghcjs - by taking advantage of jsaddle we can provide a native app with minimal effort in the future (jsaddle is awesome!!!) - In the long run, I am hopping for WASM! Concrete measurements we did: use closure compiler (not yet with advanced settings, as they don't work for us) and use the -dedupe option of ghcjs, gzip of course. This gets us currently 697k of gzipped JavaScript, unzipped: 4.7 M. 
`(&lt;= 0)` /= `not . (&gt;= 0)` Also, does filter work backwards in F#? In Haskell `filter even [1..]` = `[2,4..]`.
I see, the parser and the recognizer need to be regular. The Foldable constraint applies to the input, not the dfa. Applicative and Monadic parsers can have Foldable input too. 
I always get confused by whether `filter` filters in or filters out.
This docker command should present you with a shell that provides the try-reflex development environment. docker run -it --entrypoint /start.sh davecompton/try-reflex Here's a link to a repository that contains a Dockerfile and details about how to create and use the docker image. https://github.com/dc25/docker_try-reflex Using this setup I was able to build and "run" both https://github.com/reflex-frp/reflex-todomvc and https://github.com/jonathanknowles/haskell-calculator 
Great job. Excited to use this and recommend it to others.
 let f xOpt = match xOpt with | Some x when x &gt;= 0. -&gt; Some(sqrt x) | _ -&gt; None versus let f = Option.map sqrt &lt;&lt; Option.filter ((&lt;=) 0.) -- [sic] I would argue that the *second* version provides the simpler and more idiomatic solution, because - itâ€™s easiest to read and understand. - it clearly shows a 2-step process, each of which has a clear purpose (we `sqrt` only those satisfying the conditions) - it allows viewing of intermediate results, a crucial ability when debugging large codebases. - it can be optimized to the more efficient version by the compiler, avoiding a useless premature optimization I believe the best `f` can be achieved by using an auxiliary function. In Haskell notation sqrtMaybe x | x &lt; 0 = Nothing | otherwise = Just (sqrt x) This function has a clear purpose: to provide a checked version of `sqrt`, so it's not a 'bad' combinator. Then `f` can be implemented as `f = (&gt;&gt;= sqrtMaybe)` Which actually pretty clearly shows that `f` is pointless (ha great pun!) here, since a monadic bind is almost always simpler to understand at use site.
&gt; Yes...but using |&lt; and &gt;| would let you write "File exists : " |&lt; doesFileExist foo &gt;| "' This looks good, thanks, I'm going to try it.
These improvements are very exciting, especially `DerivingStrategies`. Thatâ€™s a feature Iâ€™ve wanted for a long time, given `DeriveAnyClass` is effectively worthless to me without it (I make very heavy use of GND), but `DeriveAnyClass` is a very pleasant feature. The GND improvements for associated types is very nice, too. IMO, while `deriving` in standard Haskell is a simple nice to have, GND alone completely transforms the way I write programs in a way I donâ€™t think Iâ€™ve seen any other language support. Combined with Generics and `DeriveAnyClass`, I hope we see even more adoption of these type-driven code generation techniques. After all, itâ€™s impossible to include bugs in code I donâ€™t have to write, and it makes Haskellâ€™s incredible concision even better.
Consider Data.Time. This is a ubiquitous API. It would be improved, with almost no user cost*, with type IntegerYear = Integer type IntMonth = Int type IntDayOfMonth = Int toGregorian :: Day -&gt; (IntegerYear, IntMonth, IntDayOfMonth) *There is real cost to data, and programmer time/dependency lose with newtype. This loses haddock reference, and?
Thank you!
Many thanks! I had a review of the landing page already planned for this sprint. Obviously it is needed!
This works great for me until I need to touch multiple cabal packages simultaneously.
&gt; - In case you try to derive some class on a newtype, and `-XGeneralizedNewtypeDeriving` is also on, `-XDeriveAnyClass` takes precedence. From the [GHC user manual](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html?highlight=deriveanyclass#ghc-flag--XDeriveAnyClass).
That's is related to the type of problem I'm trying to solve and i found your post really interested. Nested maps are indeed a good option. What I'm trying to do is to be able to be able to "extract" a subtype from a type which can be then used as key. A sort of hard coded extensible record.
That's a nice approach.
&gt; Haskellâ€™13, September 23â€“24, 2013 Just FEI (for everybody's information), that's what 'a while ago' means. The GitHub commit log shows a recent commit burst. Does that mean this is getting more attention?
v6.9.4 currently
You can try axiom. In this case, I will appreciate your feedback! https://github.com/transient-haskell/axiom It is more like Reflex, but has some advantages: server and client are integrated in a single main program. communication is seamless. widgets are self contained (contain the full stack necessary for running) and you can compose them algebraically using applicative, arithmetical and other standard haskell binary operators. The programming model is straightforward and types are simple, so compiler errors are easy to understand. 
&gt; The first version was written in PureScript with purescript-pux, but in December I decided it was to cumbersome to maintain as the app grew bigger, so I rewrote the whole thing with Reflex in Haskell. Could you expand a bit on this? `purescript-pux` is iirc just the Elm architecture. In what way does FRP give you more modular, reusable components?
I find that point-free style isn't universally harder or easier to understand. It depends on the context, and sometimes even a combination of the two can be the best choice.
I'd like to see it too if you finish :)
Generally fair comparison of these alternatives http://mutanatum.com/posts/2017-01-12-Browser-FP-Head-to-Head.html Experiences writing the same game in PureScript and Elm https://alpacaaa.net/blog/post/elm-purescript-in-depth-overview/ 
In purescript-pux it is just too much boilerplate for small components to be useful. Consider this reflex example for defining a number input (at the end of the file): https://github.com/reflex-frp/reflex-platform/blob/develop/README.md All the wiring you'd need in purescript-pux would make this component completely unpractical. Other issues I had were, where do you put data? Often you need data in a lot of components, how do you do that in ELM? Not very elegantly! You have to duplicate it and keep it in sync, or pass it on as an additional function parameter as I did. Another issue I had, was that while the ELM model is quite tempting at first, sometimes it is too simple. For example that state is just a behaviour, not a dynamic. Sometimes I want to "trigger an action" when the state changes, this is also quite painful in ELM and again only possible with quite a bit of error prone boilerplate. Another thing was, that to stick operations in the ELM model you have to split a little more complex interactions into multiple Actions , resulting in code that actually belongs together to get spread across the whole file. Which reminded me quite a bit of classic spaghetti code, purely functional spaghetti, but still. Now I also remember another issue, having to duplicate data (and keep it in sync) for each component also makes it painful to write reusable components, which is in fact the biggest issue I had, which hindered me in making smaller components. A while ago there was quite a good write-up, which talks about some issues, I had too: http://mutanatum.com/posts/2017-01-12-Browser-FP-Head-to-Head.html reflex, while being more complex, gives you all the tools you need to abstract and build standalone components, which just work (tm). I don't have to fight the architecture anymore, but can simply but it to good use and extend it if needed. That at least was my experience. It seems that other people build quite large applications in ELM, don't know how much fun they have, but they seem to do it. One workaround for the duplicate data problem I saw proposed on github, was to just use an IORef - so global mutable state basically. But this gives you other problems - for example how do you react to a change to this global state? I imagine it can be quite hard to keep the application consistent with this model. If the IORef changes, your state won't be re-rendered. Also global mutable state is frowned upon with good reasons, usually.
&gt; he GitHub commit log shows a recent commit burst. Does that mean this is getting more attention? I guess they did some preparation for the public release. 
Interestingly, it's written in Standard ML. The idea seems to have been to create a common shared backend for functional languages. Kind of like LLVM for the lambda calculus! Still sounds like a good idea to me, although perhaps there are not enough functional languages in need of native backends for that to make sense.
Great! I remember when there was real interest on /r/haskell for its release, when the Haskell'13 paper was published. My two immediate thoughts are: 1. how many of the GHC language extensions does it support? All, because the GHC frontend is used to desugar those extensions prior to HRC, no? 2. is it open source all the way down? Or do I need any non-free Intel compiler apparatus to use this?
I wonder why `GeneralizedNewtypeDeriving` and/or `DeriveAnyClass` still need to be explicitly enabled when using `DerivingStrategies` with the `newtype`/`anyclass` keywords. Seems like there's a nice opportunity to remove some extension-list cruft there.
The log shows they ported it to build on gcc
This is sort of recurring question, so no wonders people reply with links. I can share one too, it so happened there was a conversation comparing certain libraries in a thread here: https://www.reddit.com/r/haskell/comments/64yr4b/gonimo_almost_production_ready_a_web_based_baby/dg7cett/
Some good points about halogen I already forgot! ;-) Yeah I am pretty much sold on reflex, so if gonimo stays alive, then we will keep pushing boundaries. I am looking forward to my first PR improving Reflex' performance!
It may be surprising, but no closures are transported. Only what would be similar to REST routes are. There are a number of routes limited by where `teleport` is located in the code. The routes are used to construct the closure in the remote node and to invoke it in subsequent calls to the same teleport. In essence when you send a request to an endpoint in a REST service, you are constructing and executing a closure in the server node. Cloud haskell uses a different mechanism, basically, pointers in identical binaries. Therefore the ways in which it can be tweaked is the same than any REST application with the same endpoints. Looking from the point of view of a standard web architecture, think on it like a layer that construct routes automatically and perform remote method invocation over REST microservices. Since it uses websockets instead of HTTP, the server can invoke the client the same way. Now transient can use TLS/SSL connections, encrypted and authenticated (well authentication is disabled now) between browser and server and among nodes. if the web URL uses https protocol, the WebSocket connection will use secure websockets. This is not battle tested but I found that it works reasonably well. So the security has the same considerations and uses the same tools of other frameworks. 
I'm glad you managed to get this project (which I remember from 2 years ago) to being production ready! 
Thanks and thanks for your support back then! :-)
Awesome to see this get out there!
We deliberately chose not to have `-XDerivingStrategies` imply all of `-XGeneralizedNewtypeDeriving`, `-XDeriveAnyClass`, etc. I'll refer you to [this section](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/DerivingStrategies?version=16#What-XDerivingStrategiesisnot) of the GHC wiki's commentary on `DerivingStrategies` for an explanation: &gt; `-XDerivingStrategies` is not intended to be a catch-all language extension that enables all of `-XDeriveFunctor`, `-XDeriveAnyClass`, `-XGeneralizedNewtypeDeriving`, `-XDeriveGeneric`, and all other exotic deriving language extensions. To see why consider the following code: &gt; &gt; {-# LANGUAGE GeneralizedNewtypeDeriving #-} &gt; newtype T = MkT S deriving (Foo, Bar) &gt; &gt; This code compiles without issue, and uses `GeneralizedNewtypeDeriving` to derive `Foo` and `Bar` instances for `T`. But if you turn on `-XDerivingStrategies` as well, suddenly the above code will change in semantics: it will emit a warning about `GeneralizedNewtypeDeriving` and `DeriveAnyClass` both being on, and default to `DeriveAnyClass`! The intention of `-XDerivingStrategies` is to simply enable new syntactic forms that allow strictly more code to compile, and in particular, it is not intended to change the semantics of any existing code. &gt; &gt; In addition, having `-XDerivingStrategies` imply `-XGeneralizedNewtypeDeriving` would have Safe Haskell repercussions, since one cannot currently use `-XSafe` in combination with `-XGeneralizedNewtypeDeriving` (see Trac [#8827](https://ghc.haskell.org/trac/ghc/ticket/8827)).
that's cool, and I would not propose implying those extensions, but rather invoking their functionality when the keywords are explicitly used, *only* where they are used. This avoids the concern about changing the meaning of existing code when adding `DerivingStrategies` to extensionless code, while not requiring all those extensions to be listed.
It uses GHC to compile Haskell to a different [intermediate code representation]( https://en.wikipedia.org/wiki/Intermediate_representation ). Ordinarily, GHC uses C-- (C minus minus), and can also use LLVM as the intermediate code. With this project, Intel has designed their own intermediate code for Haskell to see if they can generate better optimizations, the idea being to produce native compiled programs that are smaller and run faster on Intel processors than what vanilla GHC produces.
And any fields that were already Maybe, are now Maybe Maybe...
Couldn't you abuse laziness and parse context sensitive grammars with applicatives by creating an infinite grammar on demand?
Such an abuse requires a grammar over a finite alphabet, otherwise the grammar can have infinitely wide fanout for the new &lt;|&gt;'s and you can't necessarily make progress. Nils Anders Danielsson first brought the issue to my attention around the time he wrote [Total Parser Combinators](http://www.cse.chalmers.se/~nad/publications/danielsson-parser-combinators.pdf). The issue is summarized in section 3.5 of that paper. Like I said, the issue isn't that severe in practice, but it is necessary to note.
Thanks for the kind words! Yes, Lambda is definitely an influence for us, we see this as the next logical step, where imagine if, not only did you have functions in the cloud, but they were typed and you could connect them using a higher-level DSL. Ha - yes we should work on adding Haskell function support, as it would be one of the easier languages to support as we've already created the machinery in Haskell, plus it would be fun to do and great for debugging :) Yes, the way we're planning on doing Java support will mean exposing a lower-level Java shim, and then higher level integrations for other JVM languages on top, including Scala and Clojure. I don't believe JVM startup will be an issue, as your workflow can be long-lived - however there is some work to do on our container lifecycle management system that may also help there.
&gt; Whereas sets have elements and types have terms, there is no corresponding concept baked into categories in the general case (thunks being the next best thing). "thunks"? As in an expression whose execution has been delayed due to lazy evaluation? I don't see the relationship with category theory... I'd say that category theory does have a way to refer to the elements of an object, in the categories in which that concept makes sense: an element of object O is represented by a morphism from the terminal object to O. In Haskell that would be a function of type `() -&gt; O`; since each (total) function of this type is equivalent to `\() -&gt; o` for a particular o of type O, a function of this type can be used to uniquely specify a value of type O.
It looks like the SublimeHaskell instructions say to use `cabal` for installing `hsdev`, but you can install `hsdev` using `stack` aswell; see [this StackOverflow answer](http://stackoverflow.com/a/36528910) for instructions.
It definitely should work on Windows! However, building without nix (as many others have noted) is quite painful - that's why I use nix for it. One interesting thing is that the Windows Subsystem for Linux may eventually enable Nix support for Windows. [There seems to be somewhat active work on it now.](https://github.com/NixOS/nix/issues?utf8=%E2%9C%93&amp;q=is%3Aissue%20is%3Aopen%20Windows)
React is just one small part of what you need. It's only the V in MVC, for example. Then you need controllers (if you want) and a way to manage your M(odels) / state.
Hey, thanks for the answer. I managed to understand a bit more about Cabal and Stack and got hsdev installed. But SublimeHaskell complains: &gt; SublimeHaskell: hsdev version is incorrect: 0.2.3.1 &gt; Required version: &gt;= 0.2.0.0 and &lt; 0.2.3.0
I guess install the latest version before 0.2.3.0 then, which looks to be 0.2.2.2. Download the source for 0.2.2.2 from [here](https://github.com/mvoidex/hsdev/releases/tag/0.2.2.2), then unzip it and do the same thing as before.
One of the authors here. We had a small burst of requests for this maybe 6 months ago and a couple since then so that is what is prompting this release now. Yes, we did some clean-up for public release.
&gt; filterA : (a -&gt; A (Maybe b)) -&gt; [a] -&gt; A [b] &gt; filterA f = fmap (filter id) . traverse f That's a type error. &gt; :t \f -&gt; fmap (filter id) . traverse f \f -&gt; fmap (filter id) . traverse f :: Applicative f =&gt; (a -&gt; f Bool) -&gt; [a] -&gt; f [Bool]
Thanks. Managed to do with Cabal itself, with Cabal install hsdev-0.2.2.2 
I'm using this in a personal project. I looked at several Oauth related libraries and this was the only one I was able to figure out by looking at the functions and type signatures. None of them have much documentation.
Thank you very mutch. I started using stack. It working. Good luck!
Do you mean React/JS or Haskell bindings/GHCJS?
No problem. Thanks!
Not as a single topic, but read also about the logging mechanism in that tutorial, that is also used to construct the "routes" and the continuation mechanism used to re-execute closures, and the state, used to store continuations/closures, and also the threading and asyncrnous primitives, used for streaming. The rest is just socket communications. 
STOP TRYING TO FIGHT WE DONT WANT TO FIGHT
Disclaimer: I'm one of the authors of HRC. Yes, indeed the FLRC/HRC compiler takes not only just Haskell source (.hs files) or Core (.hcr files) format, but also MIL (.mil files), although this feature was not well documented. So in theory it should be relatively easier for functional language authors to target MIL, compared to, say STG, or C-- or LLVM bitcode, because: 1. MIL has extensive support for typed high-level (garbage-collected) data structure, such as struct, union, mutable and immutable arrays, big integer, rational, so on and so forth. 2. MIL has a SSA and CFG based structure, with instructions, blocks, functions / closures, globals, etc. 3. MIL enjoys an extensive set of optimization passes, including SIMD vectorization! Another point worth mentioning is that FLRC right now is a whole program compiler, which also makes it slight easier to deal with, but of course that is subjective. 
X-Post referenced from [/r/cpp](http://np.reddit.com/r/cpp) by /u/blelbach [HaskellNow 2017 Keynote - Ryan Newton - Haskell Taketh Away: Limiting Side Effects for Parallel Programming](http://np.reddit.com/r/cpp/comments/657qgr/haskellnow_2017_keynote_ryan_newton_haskell/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
Well I tried to say it works for the definition of `filter` that I gave above.
I would imagine that whole program optimization is not specific to Intel processor. Why did they do it on their own ? I am curious
Reason 4 can be addressed with better tools. In Idris, functions that compute types (which serve the role that type synonyms do in Haskell) are a different color in compiler output than actual type constructors or bound variables. Also, if an error message includes an expression that is not done computing, like an unexpanded type synonym, then users can right-click and evaluate it in place in the error message, seeing both forms. Something like that for GHC would be nice.
I like `keep`. I also like your coffee mnemonic.
Yeah, `a -&gt; (k, v)` can save work over `a -&gt; k` and `a -&gt; v`. Not sure how often in practice though.
Off topic noob question: As far as I know Haskell has a garbage collector. Are there any optimization steps for determining allocations which can deterministically collected or moved to the stack?
I spoke with /u/tekmo recently and he told me that fixing this might be my summer project for my internship at Awake Networks :-)
Not necessarily. GHC doesn't go as far as you think when it comes common sub-expression elimination. In particular, certain transformations can lead to problems when combined with laziness, and GHC errs on the side of correctness. You can however be explicit about it. let x = make d2 in Node x x would lead to only one thunk created for `x` that would be shared between both branches of the `Node`. More information on why GHC doesn't do CSE all the time here: https://wiki.haskell.org/GHC/FAQ#Subexpression_Elimination
The `@` syntax is introduced by the `TypeApplications` language extension. You can find more information via the following links: * The GHC [user guide](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#visible-type-application) contains the user facing documentation about the extension * The GHC [trac](https://ghc.haskell.org/trac/ghc/wiki/TypeApplication) contains (outdated) information about the design and implementation.
Actually, apparently the work was [very active](https://www.reddit.com/r/NixOS/comments/64xyd7/nix_package_manager_works_flawlessly_in_windows/)!
It's hard to really say. Ultimately, you can just ask GHC to see what it thinks. [Here's an article on dumping the internal representation GHC uses to compile Haskell](https://donsbot.wordpress.com/2008/05/06/write-haskell-as-fast-as-c-exploiting-strictness-laziness-and-recursion/). In particular, the output should be readable enough, and you should be able to see if GHC indeed lifted out the double call. My intuition says it would not, but I'm not a compiler :)
This is a boilerplate code for SPA with Yesod and React: https://github.com/psibi/yesod-rest
Misread. My bad!
I would strongly suggest react-flux/GHCJS.
the first rule of Haskell is "make a type, it's literally one line". 
reflex-dom is not VDom based
Read Pragmattic Programmer. It had a bunch of good ideas, many of which are now integrated in common agile frameworks. I enjoyed it, in any case. I have Code Complete 2 still sitting on my shelf. I should get around to it.
There's nothing special about "ajax" requests. You can use any functions provided in https://hackage.haskell.org/package/Spock-core-0.12.0.0/docs/Web-Spock-Action.html `param*` functions for form-like requests (this=that&amp;etc=...) and `jsonBody` for, well, JSON+POST requests. Reply with `json` action: `json $ object [ "key" .= [ "list", "of", "values" :: Text ] ]`.
1. Most GHC extensions are no problem. However, HRC requires external core, and later GHC versions have dropped this support. So as it is, HRC only works with GHC 7.6.3. 2. BSD license. You can use gcc instead of icc. 
Stopping for the night at 24:06. Looks like I can only get 12 minutes per night in.
I've read some of of them and don't find them terribly relevant even to imperative code. It's mostly the authors personal opinions and handwavy justifications for why everyone should adhere to them. 
right now gonimo uses the reflex-dom and reflex directly from github (I believe from the jsaddle branch) - the versions on hackage are a "bit" behind https://github.com/reflex-frp/reflex-dom/tree/jsaddle
In fact, this is true not only for nullary constructors, but for any constructors: *HetPtrEq&gt; hetPtrEquality (Nothing @Int) (Nothing @Bool) True *HetPtrEq&gt; hetPtrEquality (Just @Int) (Just @Bool) True (`@` from `-XTypeApplications` here) 
I've been reading Code Complete recently because I remember it being hyped (about ten years ago). I don't like it much. It is very verbose and many of the suggestions are common sense or workarounds for obsolete technology. In particular, there are many rules of thumb for how to avoid shooting at your feet with object-oriented programming. Maybe Code Complete is useful for a very new and inexperienced programmer who has to work in industry, but even then I'd hope you could find the same information in more concise form elsewhere. I have better memories of Pragmatic Programmer, but it has been a while. Note that while functional programming requires different technical considerations than legacy paradigms (so a book like Design Patterns isn't very useful), the social dynamics behind software development are exactly the same. Books that cover those topics should apply just as well.
I would love an entire book about abusing systems to create other systems. Prolog in the type system! [A type system in the macro system](https://docs.racket-lang.org/turnstile/index.html)! [Brainfuck in the syntactical pattern-matching system](https://danielkeep.github.io/tlborm/book/aeg-ook.html)!
I think it is more that for nullary type constructors it is necessarily true, whereas for others it just happens to be true. In theory profiling, etc. might muck up the latter.
&gt; I seem to recall there being some discussion of deriving `Eq1` and company in GHC 8.2 but I don't see this being discussed. There was discussion at one point, but I abandoned this idea after talking it over with some other GHC devs. The concern was that adding support for deriving `Eq1` and friends would bake eight (!) more classes into GHC, and moreover, `Eq1` _et al._ are such recent additions to `base` that the idea of enshrining them into the compiler gives some folks pause. I'm still interested in finding a way to automate the creation of `Eq1` instances in GHC, though perhaps through a different avenue. One suggestion that Matthew Pickering has made is to [rework `deriving` as a part of a proposed overhaul of Template Haskell](https://ghc.haskell.org/trac/ghc/ticket/12457) (called ["native metaprogramming"](https://github.com/shayan-najd/NativeMetaprogramming)). The idea would be that the logic behind `deriving` would be ported over to a metaprogramming library, at which point we could add cases for `Eq1` and company without baking the logic directly into the compiler. But as things stand now, that goal is still quite a ways off. Until then, you can quickly create `Eq1` instances today by using my [`deriving-compat` library](http://hackage.haskell.org/package/deriving-compat-0.3.6/docs/Data-Eq-Deriving.html#v:deriveEq1). The next version of `transformers-compat` will also feature a [`GHC.Generics`â€“based default](https://github.com/ekmett/transformers-compat/issues/25) for `Eq1`.
That's why I suspected integrating with ckeditor will be less painful. I just haven't proven yet. I think that would be a very popular community contribution if anybody gets around to it.
uh oh
Look into Mendler style recursion and sized types
The GHC 8.0.2 tarballs have been restored.
At this point it remains to be seen. There is plenty of work to be done and it's currently unclear (to me at least) whether there is sufficient consensus around the particular approach being proposed.
It's been a while since I read Clean Code but I found it compromised to the core. Ideally, when it comes to programming, I'd want tips that are correct and supported by empirical evidence. That is, if you claim having a bunch of short routines instead of longer ones in code written in a given language lead to less software faults, then I'd like to see some research on the topic instead of personal musings of the author. Nothing of the sort was delivered iirc.
The Unix chainsaw: https://www.youtube.com/watch?v=sCZJblyT_XM Such a fun talk. Thanks for pointing it out!
All of my rage.
So sorry I didn't see this earlier! Your Edit is correct; and your code in Edit 3 is also correct. Re your intuition about writing "requires", I do agree that there is something quite confusing going on here which I don't quite know how to resolve, and that is the fact that signatures can both be "provided" and "required." The syntax is set up so that if the mixin refers to a signature, you always have to put it in "requires", but as you say, if you are using a signature package, you are trying to "get" the functions from the signature package. The current syntax is consistent (the "requires" is a cue that you are going to have a requirement named CharStr) but there is something interesting going on here, which I do not quite know the correct way of handling. Perhaps we should have just gotten rid of "requires" entirely! (What if s/requires/signatures/?) By the way, there is a pile of implementation packages for str-sig https://github.com/haskell-backpack/backpack-str which I haven't gotten around to uploading to next-hackage yet.
&gt; Interestingly, it's written in Standard ML. I'd wager to guess that this is probably because there's a nice book called `Modern Compiler Implementation in ML` which uses Standard ML. Otherwise, it would make sense to choose Haskell or OCaml just because the tooling is probably better although SML's MLBasis has no equivalent in OCaml (maybe GHC's Backpack fits the bill).
Yikes! If we're going to judge people permanently by whether they've ever expressed an opinion that is incorrect, I should delete my blog. I've had all sorts of opinions in the past!
"So now I have to write tests to check for errors that a compiler could guarantee are ruled out. Great." That comment sums up the flaw in Martin's logic pretty well.
As someone unfamiliar with the codebase I wanted to make major changes to the GHC abstract syntax tree, to support API Annotations. GHC is a big codebase. I found that it was a straightforward process to change the data type and then fix the compilation errors. Even in the dark bowels of the beast, such as the typechecker. I think the style of the codebase helps a lot in this case, with lots of explicit pattern matching so that it is immediately obvious when something needs to be changed. 
I've written several small services in the last few months that literally worked the very first time they ran. It's an amazing feeling, even after using Haskell for years. With simple editor integrations I can know that code will compile as I type. In some cases it's become so easy to gain confidence that I'll simply write code and deploy it without actually ever running it beforehand (not that I endorse this behavior in many scenarios, but in all my years of Python you couldn't even dream of doing this). When it comes to refactoring, the experience is similar. I've refactored the entire database engine of an app in a single commit and it worked the first time. If you express your semantics as much as possible in the types, then your editor tells you when you made logical misstep even as you type. Certainly there are many cases where things get complex (dealing with lots of exceptions, or high-performance, or weird concurrency problems). But as for general app development, there is no experience quite like it in traditional languages.
Intuitively, stack-based allocation seems like a good idea where possible but my understanding is that in practice this approach has in some cases turned out to be performance neutral or actually worse. The GC included in this release uses thread private nurseries that are (read AND write) only accessible to a single-thread and thus can be collected independently and frequently. Everything you could stack allocate will therefore die in this private nursery, which is effectively always in cache so it is hard to beat that with a stack-based approach.
You could have both if you used newtypes though like how it's done with other typeclasses.
Here's a short list: "Practical Foundations for Programming Languages" by R. Harper "Programming in Haskell" by G. Hutton "An Introduction to Functional Programming" by R. S. Bird and P. Wadler "Push-Pull Functional Reactive Programming" by C. Elliot "FUNCTIONAL PEARL: Applicative programming with effects" by C. McBride and R. Paterson NB: Functional pearls are fun to read and the didactic stuff by Wadler or Hutton are great reads too. 
Note, though, that neither `Enum` nor `Bounded` have `Ord` as a superclass, and that there are no laws relating `Ord` to either class.
I've been working on a small learning exercise application built as a tool suite for a proprietary data format used at my company. Due to knowing almost nothing about Haskell when I started, refactoring is frequent, pervasive, and total. I have completely redefined the internal representation of the data format almost a dozen times, and every time, it was completely painless and introduced no new bugs into old code. The fanciest type I've used so far is a simple Monad transformer stack of IO and State. Because all Stateful interactions happen in that same stack in one file, it's extremely simple to tack on new features without worrying about affecting old code. Simple but powerful list and vector manipulation functions make up the bulk of the code in my app, and this is by far where most of my work is focused. Haskell's List instances for Applicative, Monad, Functor, and the function zipWith combine to allow me to quickly express, tweak, change, and engineer complex control structures with very little effort or cognitive burden.
Thank you! After your reply, I think I'm actually happy that `Eq1` and friends aren't part of the builtin deriving - too many special cases is usually a good sign that something needs to be abstracted away. I'll keep a long term eye on the ticket you linked.
I've always found refactoring in Haskell to be one of its undersold benefits. Both the type system, purity and `-Wall` are at play here. * By utilizing ADTs you are able to model data very expressively, Sum types in particular give you exhaustive case matching. This ensures you fix areas in your code that may have broken when you remove or add constructors. Because of this I'm rather bullish on never using wildcards in case expressions, they act as a bug sink removing any information the compiler can give you when you introduce new cases. The compiler builds on the expressivity of ADTs and gives you strong guarantees about code. Want to change something? Just change it and play wack-a-mole as the compiler informs you of all areas you've now broken. * Purity also plays into this. By pushing effects out to the boundaries purity gives you "local reasoning." Verifying a pure function is a matter of reading its definition and maybe some time in the repl (or unit tests), you are "guaranteed" that it isn't doing any funny business. This allows you to treat many functions as black boxes providing a high level of trust and impetus for code sharing. * `-Wall` is a god sent. There are some controversial warnings, but for the most part fixing warnings will save you from later pain that will enable stronger refactoring. My best examples have always been when I needed to refactor critical infrastructure. At one job I needed to change the interface and logic for our ORM. This is a package that was relied upon by a myriad of other packages and was virtually the bottom of our stack. In another language this would be a nightmare, the kind of change that you labor about making, the type of tech debt you are never willing to pay down. In Haskell it was a dream, 1 hour later a fundamental building block of our application was changed, tested and deployed later that day incurring no bugs. 1. 60k-120k 2. see above 3. We had a mix of fancy types and simple types. Probably the most pervasive fancy-ness was type families, but used very sparingly through associated types. You can gain a lot of confidence just within the subset of Haskell98.
For what it's worth, you can use generics to write these instances for you, if you have some particular scheme that you want your instances to follow. You can even package it up as a library and distribute it.
Damn those pesky opinions!
The blog post is barely 3 months old. He's been spouting unsubstantiated nonsense like this for years.
I'm talking about the distinction made more explicitly in the semantics for imprecise exceptions paper: https://www.microsoft.com/en-us/research/publication/a-semantics-for-imprecise-exceptions/ There, SPJ, Hoare, and Marlow sketch a two-level semantics -- a denotational one for pure functions (including ones that throw) and an operational one for the entire package. I think you've put the cart before the horse in saying "it [exceptions] is a source of non-pure behavior that it is clearly observable for reasonable notions of observers." This is because you're not saying why its not pure! Except, I guess, that it is observable for outsiders, and your notion or purity you propose is just about things being "not observable to outsiders." "pure" with regards to yourself is also strange, I agree. One way I think about it, at least according to my intuition, is algebraically. Given a language defined as a fixpoint of a functor, is there an algebra for that functor giving the semantics of your language with regards to a "pure" domain? And now we've sort of just kicked the can down the road, but it lets us talk about the target domain of the semantics, and relate the question to referential transparency (or one version of it) in a pretty clear way. By the way, your proposal about functions that only write reminds me of the following hardware that is optimized for that use: https://www.rfcafe.com/miscellany/cool-products/signetics-25120-fully-encoded-9046xn-ra-write-only-memory.pdf https://en.wikipedia.org/wiki/Write-only_memory_(joke)
1. Our Haskell codebase at work is medium sized, split into several applications, mostly in a monorepo. I believe we have around 25k lines of code. It's not much code, but it powers a lot of our most important stuff. We have a few HTTP servers, lots of HTTP clients, background workers that poll Amazon SQS for jobs, and an admin website. Lots of boring CRUD stuff. 2. I generally try to do the dumbest thing that works, though sometimes I do the dumbest thing that works *and* minimizes typing. Typically this is adding another constructor to a sum type, or another field to a product type. From there, you just have to follow the compile errors. 3. Simple types and pure functions are the are the absolute foundation of what we do. You can get really far with ordinary sums and strict products (forgetting to initialize a strict record field is *always* a compile error). I've used fancy types when describing a Servant API so that I could easily and safely have `PUT` endpoints that only allow you to specify the attributes you're allowed to update. Refactoring is just so easy that it's fine to write some garbage code today that works OK, but maybe is too specific. If you ever need to generalize, it's crazy easy -- you just move the terms you want to generalize to the argument list and delete/reinsert the type signature. 
I think there's a good discussion to be had about this. In my blog I'm trying to explain it to an unfamiliar audience in practical terms. But I had to develop a consistent idea of what I think it means that would then fit in nicely with the follow posts (evaluation vs execution, optimization: inlining, rewriting, fusion, etc). Your two examples still do fit into my flavor of 'pure': if there was a taking time function, then you could call it twice (modulo sharing, and thunks in lazy languages) and observe the side effect. If there was a sleep function, you could call it but your program wouldn't know the difference without a means to observe time. Lastly, a pure program that could output to stdout would do the same thing every time including its output. So, like you said, without observation that effect is about as meaningful as allocating and triggering the garbage collector. The reason for defining things in terms of internal observations from my own perspective is that this is where we really care about what we're doing. *Evaluation* of the language should not let you write logic which depends on reading and writing to some implicit state, otherwise equational reasoning breaks down and being able to do inlining and fusion and all the other goodies. Nobody does equational reasoning with pure code that updates some implicit state. I mean, I think *what a language lets you observe is equivalent to saying: this is what you can write conditionals on* and build your reasoning and abstractions upon. Then the question I raise is: should the set of those things be implicit or explicit? Purity in my post implies the latter. Of course, the work on coffects (like get the current time) is a middle ground that makes things more interesting. Let know if I didn't properly answer your post. I'd like for my definition to withstand modest but not profound criticism. It would be good if haskellers don't think it's a silly or misleading definition.
The `gi` bindings are really neat but they're definitely not the [only](https://hackage.haskell.org/package/fltkhs) GUI library. [FLTKHS](https://hackage.haskell.org/package/fltkhs-0.5.1.5/docs/Graphics-UI-FLTK-LowLevel-FLTKHS.html) is quite actively developed.
It honestly depends on what you're working on and how well it "fits" in the type system (and how good you are at expressing that). For most "app-level" code on the server I manage, it's as good as people say. I've done 1500+ line refactorings without anyone even noticing. But for stuff like trying to fiddle with dubruijn indices and make reduction rules in an AST? I make so many errors I almost want to cry. I have no idea how to write that sort of thing in a way that gives any easy static guarantees, and it shows.
Isn't purity just related to state? In that something is impure if it changes some kind of state, or reads some kind of state. State in this case is information that is outside the scope of the on-going computation. Monads simulate state by delaying function execution, effectively persisting information just like state, but since the computation is not yet done, it's technically not state like say a hard disk drive, or even something like time since it's a form of state in the real world. If you mess with state mutation or depend on some state to perform a computation, the function is not pure. 
yes: https://github.com/sboosali/enumerate/blob/master/README.md https://hackage.haskell.org/package/enumerate It's pretty simple. Only finite types, and left-to-right. /u/Purlox, you'd just write: {-# LANGUAGE DeriveGeneric, DeriveAnyClass #-} import Data.Enumerate (Enumerable(..)) import Data.Generics (Generics) data Inner = Foo | Bar | Baz deriving (Eq,Ord,Enum,Bounded,Generic,Enumerable) data Outer = Qux Inner | Lor Inner deriving (Eq,Ord,Generic,Enumerable) &gt;&gt;&gt; enumerated :: [Outer] [Qux Foo, Qux Bar, ...]
Hour Ten. Reporting in. Morale is low. I have run out of snacks. GHCI refuses to launch out of CMD.EXE due to a missing reference in whatever copy of `zlib1.dll` is being found on my path. This makes investigating the types of any of the generated GTK objects nearly impossible. I have resorted to sticking `#` signs into my code at random, with what appear to be non deterministic results. I still have no idea what the list object being passed during the `new` call to instantiate widgets is supposed to be doing. I have managed to generate a button, and make a layout grid, but the mysteries of scrollbars are still thoroughly evasive. I have found a blog post and a semi-reasonable tutorial written for GTK2HS, here: https://www.stackbuilders.com/tutorials/haskell/gui-application/ http://code.haskell.org/gtk2hs/docs/tutorial/Tutorial_Port/index.xhtml There seems to also be no deterministic ruleset relating whatever wizardry haskell-gi is doing, and the functions described in those guides. I have yet to successfully implement a scroll bar. The quest continues. 
The thing is that non-FP folks don't understand the meaning if the word Evaluation in the same way that FP folks do, because they don't think in terms of the Substitution model. This is why an argument always surfaces about the meaning and role of the Haskell runtime which performs side-effects, and whether that 'counts', as of course, if Haskell couldn't perform side effects, how could it print to the console? Now, I understand the answer and reasons to these questions, but it always seems to come up for outsiders what Evaluation really means and don't take it as a given that it's a "reasonable thing" to introduce what seems to them as an arbitrary distinction between the Runtime and the Program (as no distinction exists in the languages they are used to)
Haskell Programming from first principles is THE book to read for starters. I am also reading the same currently. 
It sounds like you're on Windows. That's a *big deal*. You may want to mention it.
Thanks a ton! I'll see if I can't reproduce the issue I was having when I'm back in the office, and I'll hit you up.
I also want to add that the comparison with pipes in command line shells would be better if it was mentioned that the order is reversed.
Thanks!
That sure looks like something an IDE could have done.
Yup, if you always read `g âˆ˜ f` as `g` after `f`, it's easy to know which is first.
Do not read books. That would kill your creativity. You will learn recipes that will be outdated in a few years. Learning for yourself will train yourself to discover. It is the same than in a guided tourism compared with adventure: The first allow you to visit Europe in 7 days and falsely think that you know it all. The second takes more time, let you commit errors and you would acquire much deeper knowledge. Moreover with the exception of Real World Haskell, all the haskell books are extraordinarily bad. they are not for programming in Haskell. They are for members of a sect whose aspiration is not to program at all, but to teach others how to not-to-program.
That's what the next post is about.
Oh thanks. Serves me right for using bad naming convention.
Completely true, although I really wish it weren't, having a real ordering hierarchy with lattices and enums would be much nicer IMO.
Thank you for pointing me to the UPenn course. The exercises seem interesting.
My [Threepenny-GUI][1] package is (designed to be) extremely easy to install, though it does come with the drawback that the GUI runs in the browser. You need to know a bit of HTML, but other than that, you are writing the GUI completely in Haskell. [1]: http://wiki.haskell.org/Threepenny-gui
&gt; Perhaps we should have just gotten rid of "requires" entirely! (What if s/requires/signatures/?) It is a tricky naming problem. If signatures are often both provided and required, perhaps "signatures" instead of "requires" would make sense. How about having a single block of renamings, like &gt; mixins: foo (Foo as Bar, signature Str as Str2, signature Stuff as Stuff2...) But perhaps it would be more confusing.
An this the reason why there are so many gray people. Newton learned from an apple. The question is: you want to visit or you want to discover? Do you think that the Montgolfier brothers had a book of how to build planes? 
We *could* choose any space-filling curveâ€”the diagonal progression generated by the Cantor pairing function, the Hilbert curve, the [Z-order curve](https://en.wikipedia.org/wiki/Z-order_curve), &amp;c.â€”but some are more useful than others. So as a practical matter, I think we *should* use lexicographical order as the default, following the precedent of `Ord`, and add `newtype`s for other orderings if people want them.
Then you'll enjoy the next two posts which cover (1) dealing with the real world and (2) optimizations!
Not sure about books, since I've never read any of Haskell books, but for practical exercises you could give exercism.io a try.
I'd write it like this: f (Just x) | x &gt;= 0 = Just (sqrt x) f _ = Nothing Shorter than both other versions, and puts the happy case first.
what's with the downvotes?
Unrelated to Bash On Windows, as far as I can tell, which is unfortunate.
The point I want to emphasize is here: https://en.wikipedia.org/wiki/Pure_function &gt; 2. Evaluation of the result does not cause any semantically observable side effect or output, such as mutation of mutable objects or output to I/O devices (usuallyâ€”see below). (Usually refers to practices such as those in Haskell that make IO expressions pure.) Although those things could be considered 'side effects', they are outside the scope of our denotational semantics, hence they are referentially transparent mathematically. https://en.wikipedia.org/wiki/Referential_transparency says a referential transparent function is a pure function so it's probably fine.
I'm really enjoying Learn Haskell by Will Kurt from Manning publications. It's shorter than programming from first principles, has a bunch of exercises as well as larger projects for each chapter.
I think he's referring to the ["`stack ghc` painfully slow #1671"](https://github.com/Microsoft/BashOnWindows/issues/1671) issue
I'm using [`qtah`](https://gitlab.com/khumba/qtah) which is Haskell bindings to Qt. The library is somewhat young and doesn't contain all the bindings, but it contains a lot and it's not hard to add missing ones without a deep knowledge of the internals (there is an extensive [guide](https://hackage.haskell.org/package/hoppy-docs-0.3.1/docs/Foreign-Hoppy-Documentation-UsersGuide.html)). I contributed a bit and the library is well-written, properly commented and the author is very responsive, so I recommend to take a look.
Advent of code is pretty great too! 
Oh yeah, almost forgot about `Ix`. Looking at it again, it almost seems like `Enum v2`. If a type has both `Ix` and `Bounded` then you can do: instance Enum a where fromEnum x = index (minBound, maxBound) x toEnum n = range (minBound, maxBound) !! n Not sure if it obeys all the laws, but it could work.
Having spent a decent amount of time helping people between mildly and moderately interested in learning Haskell for over a year in a work setting, here are some aspects that were successful (buying books and asking them to read them or do exercises in their spare time did not gain momentum in my situation): - gauge how dedicated each learner is, how determined they are to struggle through the unknown, and their natural ability, and customize the learning plan on that - for people less determined, consider Elm as a comfortable stepping stone that exposes people to some introductory concepts, then start on the Haskell path once they have demonstrated some level of comfort in Elm - for people who are more ready, start them with writing some simple unit test suites against your existing codebase (tasty hunit also avoids having to hand wave over some of the internals of hspec) - after that, progress to having people write small pure utility functions for your code base (avoid monads, applicatives) - then, start down the path you are proposing, in combination with maintaining the tests and code they wrote above
My original comment was precisely about the difficulty to define what "pure" or "referentially transparent" mean in a *precise* way. You use the words "referentially transparent mathematically", and mathematics are an excellent way (possibly the only one) to give precise definitions, so I was asking (and your reply did not answer this question): how do you define "referentially transparent", mathematically? If you don't have a mathematical definition, maybe you should not use the adjective "mathematically". (It's absolutely fine to not be familiar with mathematical formalisms and reason on an informal level about various benefits of purity, using the notions you mention above of composition and reasoning. But "mathematical" should not become a marketing term, and those informal notions are not so helpful within a definition which is precisely about how to rigorously define those informal intuitions.)
That's an odd way to misspell "Haskell"
Feels like a waste of talent, going towards infer instead of ghc EDIT: Disregard this note, not productive comment on my part.
And yet it's not listed on the sidebar, which seems a bit contradictory. The clear answer users are supposed to find isn't among the 10-15 resources listed. Making it seem like the not clear answer. 
Can you give a concrete example of how this technique would be used? e.g. I understand it as original foo :: Int -&gt; Bool foo = oldbody new foo :: () -&gt; Int -&gt; Bool foo = newbody fix all code correct foo :: Int -&gt; Bool foo = newbody remove all spurious `()` 
I assumed Dons was staying in the UK. I don't think US companies in the UK pay significantly more than UK companies in the UK [see the efficient market hypothesis].
I, for one, welcome our new functional overlords
I think it's reasonable to say Don put in, like, over half a decade (maybe even *a decade*) worth of effort for us in all kinds of forms, and some of his contributions like `bytestring` are still central pieces of our infrastructure. He already did plenty for us, and any of us are free to move on. How is it a "waste of talent" for someone to personally decide to move on to something new? That's rude, honestly, even if you didn't mean it that way. This is the same kind of stuff I see Linux/package maintainers have to deal with and it's very annoying. (And look, there are a lot of "things" I would disapprove of people in our line of work doing, many of them quite reasonably, IMO, for moral to ideological to 'your potential' reasons. Some I even *would* call a waste of talent. This... I don't think is one, really.)
&gt; How is it a "waste of talent" for someone to personally decide to move on to something new? That's rude, honestly, even if you didn't mean it that way. I agree. Perhaps unintentionally, it also comes across as arrogant, in a "anything that isn't Haskell-centric is worthless" way.
Tbh people could just hide these threads instead of downvoting them, as boring these are for people with the chops they're helpful for the curious and noob lurkers who are a bit invisible since they can't contribute (yet) to the discussions.
&gt; I didnt realise that The Haskell Book *has* exercises! It wouldn't have been hard to check...
I agree with both of you but it's possible to more charitably interpret the original comment an (intentionally?) provocative way of saying "it sucks we lost a prominent member of our community". That is a sentiment I can get behind. So, upvotes for everyone.
That is reasonable. Still, I stand for what I said above (though I have toned it down a little), as it is worth it being careful about how such remarks come across.
My apologies. Yes, he is awesome and I have definitely benefitted greatly as a result of his work.
I agree. Reddit is not Stack Overflow, and it shouldn't be moderated as such.
I don't know exactly how Stack Overflow operates, but what I worry about is not literal moderation, it's the distributed silencing that just a two or three downvotes early on in a submission's history can do.
Here are the docs: http://sboosali.github.io/documentation/enumerate-0.2.1/index.html (slightly broken, I renamed Enumerable to Enumerate). I did try manually uploading documentation (like now when hackage fails to build it itself, or when you want the newer hyperlinked source), but it didn't seem to work. 
Not sloppy at all, it's the EMH that is poorly named and communicated. Hm, reminds me of some other field with impenetrable jargon...
Is there any reason Haskell From First Principles isn't in the sidebar? It seems widely recommended as the best way to learn Haskell, and I'm sure newcomers would be well served with the book towards the top of the sidebar's resources.
A quick glance shows much better performance for HRC over GHC. But then they wrote HRC, and would be testing in it's sweet spot, I presume.
Oh yes, I was already aware of those things in the broad sense, but what I meant was that I don't know the culture there. Much like I know about French culture but haven't lived there so I lack intimate familiarity.
You're welcome. It's my favorite of his talks. I might be biased, though.
FB pays more than a bank. That's the new normal, then
This would be really useful in the project README
ls -t d ?
What does that do exactly? If it fixes it then thanks, but also tab completion. 
Isn't "eta on values" a reasonable substitute that ML and Haskell satisfy?
I think that we need a thread to discuss this, but I personally advance that if /u/nish2575 is not burned alive I will not be happy
It's not as earth shattering as you might think * It only affects compilation. * Anything compiling or linking will be affected. * based on my understanding of the stack LTS system, stack doesn't need to do any of that. The patch is limited to only the C compiler driver. Not any actual compiler. There is no code change in either GHC nor GCC not binutils. We literally re-packaged the compilers with new drivers. Hence there is no version change and from the outside both compilers will report the exact same version numbers.
Knee-jerk downvoting isn't much of a problem over there, as the criteria for downvoting are reasonably clear. Here in Reddit, on the other hand...
You only need two or three people to have a significant influence - it's a very brittle system and sort of a feature bug imo. I'm too lazy to fetch a link for you but some journalist demonstrated that manipulating visibility of info through it is feasible.
Now the [benchmarks are made public](https://github.com/IntelLabs/flrc-benchmarks), you are welcome to compile them with latest GHC and see how they perform. Related is the [original Ninja Gap paper](http://www.intel.com/content/dam/www/public/us/en/documents/technology-briefs/intel-labs-closing-ninja-gap-paper.pdf). You can browse through to see how C/C++ folks could modify their programs to gain speed over naive implementations although the code was not made public.
Atze Dijkstra joined last summer from Utrecht. Although if the others are leaving I wonder if he will want to stay as well. 
It's a lot of classes to bake, but I don't know that it's a lot of extra work. Couldn't we (hypothetically) derive a `liftEq`-like function with a fresh name and use that to derive either `Eq1` or `Eq`? We'd need to do something about the static argument, but that doesn't seem impossible.
&gt; considering its enormous performance advantage over extensible effects I've heard that the [freer-effects](http://hackage.haskell.org/package/freer-effects) library has comparable performance or maybe even slightly better. Do you have more info on this?
can you elaborate on the n2 instances problem? we are using this approach to emulate effects in our codebase and aren't aware of the downsides. 
It's mostly a problem in the world of libraries, and mostly a non-issue in the world of applications. If you model effects with a pair of a class for modeling the effect and a transformer for implementing the model, then you have to give every transformer an instance of every other class to lift that class's effects, which results in n^2 instances for n possible effects. There are some ways around this. #1 is using an `OVERLAPPABLE` instance to define an instance for any transformer like this: instance {-# OVERLAPPABLE #-} (MonadTrans t, MonadState s m, Monad (t m)) =&gt; MonadState s (t m) where state f = lift (state f) But there are *several* ways to break Haskell's [open world assumption](http://book.realworldhaskell.org/read/using-typeclasses.html#id608052) with this. The other thing is to use a `newtype` wrapper whenever you encounter a pair of effects that haven't accounted for each other in their instances. This works, but can be very unwieldy. It's particularly useful in an application, where you're often going to define a concrete transformer for your app anyway, so you can just take that as the opportunity to unify *all* the effects you depend on that might not have created instances for one another.
Well hopefully it's not an either-or prospect. You learn different things from passive and active learning and both are important; which is more important is usually highly dependent on the learner as an individual.
* As you probably know but for the sake of other readers: gcc is the linker for ghc, since it's a great front end alternative to calling ld directly. Any Haskell only builds will therefore be affected, not just C usage. * stack sells itself as reproducible builds, so anyone uniformed is going to have a nasty surprise. According to this [issue](https://github.com/commercialhaskell/stack/issues/3117#issuecomment-293060866), the fix is: stack setup --reinstall
Author of the blog post here. I agree with almost everything youâ€™ve said, with the exception of this setence: &gt; But the cost is that running any particular effect is done by a transformer, which imposes the "n2 instances" problem. This isnâ€™t true, not for the example I described in the linked blog post (which is one of the reasons I avoided the term â€œmtl styleâ€). The â€œrealâ€ instances of most of our effectful typeclasses do not correspond to a transformer at all, they correspond to `IO`. We do not implement `MonadDB` with a `DBT` transformer, we just write an instance on `IO` or our `AppM` stack. Our applicationâ€™s top-level monad transformer stack is actually usually extremely shallow, and it usually looks like this: newtype AppM a = AppM (ReaderT Config IO a) deriving (Functor, Applicative, Monad, MonadIO, MonadReader Config) â€¦where `Config` is some application-specific configuration that needs to be made available. We usually only write two instances of each typeclass: one on `AppM` and one for our tests. In fact, we often donâ€™t write any instances in our tests in the traditional sense, either, since we use [test-fixture](https://hackage.haskell.org/package/test-fixture) to generate them for us. The overhead is actually extremely low. The â€œn^2 instances problemâ€ *is* a problem with the style that mtl *actually* uses, which is a class per transformer to avoid needing to manually write `lift`. Thatâ€™s not what the technique in my blog post is about. The technique in my blog post is really just about representing more fine-grained side effects at the type level for the purpose of clarity, type safety, and unit testability.
Nice to know that there is an interest in implementing Haskell compilers by companies like Intel. I hope that the compiler would be made production ready and to submit some benchmarks to the language shootout. I'm not sold to a language by speed by I know that for many people this is the most important thing to consider.
I was not intending to belittle the result, which is impressive. I am just wary of benchmarks in general.
Actually I meant the 'add `()`' part could be done by an IDE...
Sorry, I was bad at saying that. By 'mathematically', I meant to talk about the semantics of Haskell, the math you do when you say, for example, `fmap id = id`. You do say '`printf("foo")` writes the string `foo` to stdout', that's what this piece of C means. You don't say `let f n = f (n + 1) + f (n + 2) in f 1` heats up the computer, makes the fan noisy, takes a lot of memory. That's not what this piece of Haskell means. Perhaps I should have said 'semantically'.
Also worth a mention that Ed Kmett discussed the problem as motivation in [this talk](https://www.youtube.com/watch?v=YTaNkWjd-ac&amp;t=57).
Oh, you published it. Guess I can delete the appointment to go rogue like I did with VGrep then. 
The other day, I stumbled upon [svgbob](https://github.com/ivanceras/svgbobrus), and wanted to have a similar thing in my text editor. The implementation is based on a two-dimensional [Zipper Comonad](http://blog.sigfpe.com/2007/01/monads-hidden-behind-every-zipper.html) (thanks /u/quchen for the hint!). In `vim` or `spacemacs`, just select the block of text and hit `:'&lt;,'&gt;!aa2u` to convert the ASCII Box Drawings to beautiful Unicode!
Do you know what project Lennart works on at Google?
I think one relatively simple solution would be to use Maybe for your state, such that it is nothing when there is nothing to display. Then the compiler will/can give you a warning if that case isn't handled, and it will also crash at runtime if the state becomes blank. Perhaps it's possible to turn that warning into an error?
The same strategy of non-emptiness can be used for other containers too, even if this abstraction is less useful, except maybe for folding with a default starting element. You have [non-empty](https://hackage.haskell.org/package/non-empty-0.3/docs/Data-NonEmpty.html), which is older and more used and has strange type names, and my [nonempty-alternative](https://hackage.haskell.org/package/nonempty-alternative) which I think nobody uses. The instances they include are kinda different too.
Won't this shift the problem from "handling blank states" to "remember to use Maybe for certain things"? Basically, most functions that pull things out of various data sources (eg. DB) return lists, eg. fetchRows :: a -&gt; b -&gt; c -&gt; [r] For certain API surface areas it's easier to deal with an `[r]`, but when plugging this into the UI, it should probably be `Maybe (NonEmpty r)`. This problem then boils down to -- "how to enforce appropriate conversion of `[r] -&gt; Maybe (NonEmpty r)`. Or is it always better to deal with `Maybe (NonEmpty r)`?
What's the difference between https://hackage.haskell.org/package/non-empty-0.3/docs/Data-NonEmpty.html and https://www.stackage.org/haddock/lts-7.14/base-4.9.0.0/Data-List-NonEmpty.html Why do both exist?
&gt; As long as your view functions take a Maybe (NonEmpty a) or NonEmpty a as their source of data, the frontend people are always either guaranteed that there really is data, or have to handle the case that there isn't any. This shifts the problem to a different spot, i.e. ensuring conversion of `[a]` to `Maybe (NonEmpty a)`. I'm wondering if it's better to use `Maybe (NonEmpty a)` instead of `[a]` throughout the code. Is it generally unwieldy to use through various maps, folds, and traversals? Alternatively, is there a way throw custom warnings when lists are used in specific modules (the ones which deal with the UI). And when a list is really required (instead of `Maybe (NonEmpty a)`) it can be suppressed with a special PRAGMA.
After reading your question, some of the responses here, and you response, in my opinion you are going about this the wrong way. Does your team have some internal library of widgets? If not, I would create such a tiny library, and have each widget properly handle the null state in the type system. Force the user of the widget to pass some default value to display if the list of values is empty. -- In the ResultList module, only export this function resultList :: a -&gt; [b] -&gt; ResultList resultList defaultValue values = if null values then ResultList [defaultValue] else ResultList values -- Users of these functions have the empty list case handled because the default is -- partially applied customerResultList :: [Customer] -&gt; ResultList customerResultList = resultList "You seem to have no customers, why not start by creating one..." bookingsResultList :: [Booking] -&gt; ResultList bookingsResultList = resultList "No bookings here. Want to create one?" Obviously the details must be tweaked to the specifics of your UI library (i.e. if you use `reflex-dom` this is monadic, not pure. Also, there is probably some type class constraint on your items so they are showable on the UI), but conceptually my point stands. You could also have the input type of `resultList` be `Either a (NonEmpty b)`, where `a` is the default value to show and `NonEmpty b` is the actual list of values to show if they exist. By the way, what UI library are you using in Haskell? Or is this more of a conceptual question?
&gt; Maybe (NonEmpty a) is isomorphic to Emptyable a 
The one that is now in `base` and was before in `semigroups`, which is probably the oldest one, is based on lists only, so it only has a type argument: NonEmpty a = a :| [a] while the one in `non-empty` tries to generalize it for other type constructors: NonEmpty f a = a :| f a but as I said above, I'm not sure this extra abstraction is that useful, since it is cumbersome to use for things that are not lists, which are effectively `cons` based. I made the alternative one (before knowing about the `non-empty` package) to make reconstruction easier: flatten :: Alternative f =&gt; NonEmpty f a -&gt; f a flatten (x:|xs) = pure x &lt;|&gt; xs But then that's not a very efficient operation for types that are not lists.
Couldn't find `Emptyable` -- https://www.stackage.org/lts-8.11/hoogle?q=Emptyable
&gt; By the way, what UI library are you using in Haskell? Or is this more of a conceptual question? It's a very real question. Had two of these bugs come-up in our latest feature-release cycle. Right now we're using Lucid because we haven't started using Haskell as the backend for an SPA; Haskell renders the HTML as well.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/openstreetmap] [Naqsha: A geo library in haskell \[x-post: r\/haskell\]](https://np.reddit.com/r/openstreetmap/comments/65ph9h/naqsha_a_geo_library_in_haskell_xpost_rhaskell/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Something like the RemoteData pattern from Elm seems like it would be useful: http://blog.jenkster.com/2016/06/how-elm-slays-a-ui-antipattern.html
Both conversions `[a] -&gt; Maybe (NonEmpty a)` and `Maybe (NonEmpty a) -&gt; [a]` are total, so there is no loss. But I would probably just use the more complicated type as an argument to the functions that really need this distinction. Also, `[a]` has the shape `f a` where `Maybe (NonEmpty a)` has shape `f (g a)`, which means that `fmap`s and etc... will behave differently. Except if you use `Compose Maybe NonEmpty`, but that becomes way too much in my opinion...
I thought "dons" was Don Syme, of F# fame. I was pretty weirded out.
Consider placing line comments above the line, not at the end. And maybe try to squeeze the actual code into 80-100 columns as well. That would greatly improve readability of the snippets. Anyhow, great job! Is the source available somewhere (GitHub?) so one doesn't need to copy paste everything, if they want to play the game? 
is there any data-type on the lines of data Emptyable a = Empty | Present (a :| [a]) basically a "tighter" mixture of Maybe and NonEmpty, which lets us interact with it like `f a`, not ` f (g a)`
Thanks for your interest. Let me say that I am not very familiar with the geo domain. The project essentially started out after I came to know about the awesome Open Street Map project which surprisingly had flown under my radar till last year. I wanted to do something in Geo ever since and thus started the hacking. Please consider contributing if you are interested. 
Actually, to X. 
Some US companies pay US salaries in the U.K., but not FB. 
Thank you for your comment; I never remember to do that, as I typically use soft wraps in my editor so they never bother me. I quickly edited some comments to be a bit more readable, as a hotfix. And _yes_, it is now: https://github.com/arttuys/miniprojects/tree/master/OthelloHaskell/haskellversi Glad you liked it :)
Still playing at backpackifying wl-pprint, I think I have found a possible problem. I'm instantiating a library from a executable in the same package (is that ok?) When I invoke cabal new-build in https://github.com/danidiaz/wl-pprint-indef/tree/problemo, I get the following error: &gt; Error: Mix-in refers to non-existent package 'wl-pprint-indef' (did you forget to add the package to build-depends?) In the stanza 'executable wl-pprint-string-example' In the inplace package 'wl-pprint-indef-1.2' But the library does have an entry in build-depends.
Any suggestions on where to read about this as an uninformed (but interested) layperson?
You can encapsulate the `Maybe (NonEmpty a)` into it's own type. Something like this: newtype CasedList a = CasedList (Maybe NonEmpty a) Then, instead of exposing `[a]` you expose `CasedList a` and make sure that your `CasedList` type only exposes functions that force the user to handle the cases you want. Can you force people to use this `CasedList` thing? No. After all, there has to be somewhere in your code where you have to actively make the decision to used `CasedList` instead of `[]`. But if you do it at that one spot, then everyone else has to handle it properly.
You can at least use `null` to make it polymorphic over any Foldable: renderWithBlank :: Foldable f =&gt; Html () -&gt; f a -&gt; (a -&gt; Html ()) -&gt; Html () renderWithBlank blank values renderer | null values = traverse_ renderer values | otherwise = blank
I'm certainly no expert. Reasoning about data on the heap, and stuff involving pointer indirection, is hard, and separation logic is well suited for this. Concurrent separation logic, as you might imagine, lets you do that for concurrent programs. I'm not sure I can give you an accessible introduction, but I can link you to some stuff that will put it in context more and expose you to some other verification-related things. [This talk on infer will give you the story and context](https://www.youtube.com/watch?v=xc72SYVU2QY), [this blog post will give you a general introduction to why verification can be hard and why we need clever tricks like CSL](http://blog.paralleluniverse.co/2016/07/23/correctness-and-complexity/) although you might find it a bit of a hard read, and [this turing award lecture](http://amturing.acm.org/vp/clarke_1167964.cfm), the main content of which starts at ~18m00s, is a good introduction to model checking, although that's a little far from what we started with now. There were three recipients to the award that year, but I'd only really recommend that lecture. 
I wasn't going to ask this, but I just gotta know because it's eating me up inside - why did you start this comment with "Itself." ? Was it a typo or does it mean something idiomatic that I'm just not seeing? Thanks.
It looks like you're trying to mention another user, which only works if it's done in the comments like this (otherwise they don't receive a notification): - /u/ElvishJerricco --- ^I'm ^a ^bot. ^Bleep. ^Bloop. ^| ^Visit ^/r/mentionhelper ^for ^discussion/feedback ^| ^Want ^to ^be ^left ^alone? ^Reply ^to ^this ^message ^with ^"stop"
Nice =) So the only structural difference is that you sort of inline the composition of the `a -&gt; f x` and `f y -&gt; b` functions? I bet that with no GHC optimizations, this is effectively identical. But with GHC's inlining and stuff, I bet it can fuse these functions more often this way. Getting GHC to fuse functions as often as possible with the free arrow over `FreeTraversing` is really important. When running a program written with the free arrow, the cost of building the AST and converting it to a final arrow (such as `Kleisli`) is paid once, ahead of time, so the performance of that step doesn't matter much. But the final arrow that you construct *does* need to be efficient. If every step is constantly passing through several `Traversable` dictionaries at runtime, this arrow is bound to be pretty inefficient. So these functions need to be fused so that as little of that as possible is happening. To that end, I think there are two main approaches that would need to be taken. 1. Reflection without Remorse: This is a technique employed in free monads to ease the cost of constructing the AST. As I said, the AST is of little concern to us here. But it *does* give a constant structural view of the head and tail of the AST. This means there's no recursion to get in GHC's way of inlining a succession of compositions or dimaps. 2. Rewrite Rules: Much like list fusion in `base`, I think there are some rewrite rules that could be applied to remove several of these runtime `Traversable` dictionaries. I'm not sure if there are any rewrite rules that wouldn't also be covered by the Reflection without Remorse approach though. But maybe GHC would just be more reliable at doing one of these than the other? Anyway, you've given me a lot to think about! Thanks.
Use `NonEmpty a` in the argument to the widget that shouldn't be able to display empty content, and `[a]` in the widget that wraps it, for example widgetNonEmptyCustomers :: NonEmpty Customer -&gt; Widget widgetNonEmptyCustomers = mconcat . map widgetCustomer widgetCustomers :: [Customer] -&gt; Widget widgetCustomers customers = case nonEmpty customers of Nothing -&gt; widgetString "You seem to have no customers, why not start by creating one..." Just cs -&gt; widgetNonEmptyCustomers cs 
Thanks a lot for the detailed reply! Now to do some reading... :)
The Haskell team is indeed now huge.
The HRC compiler mostly shows that a little SIMD awareness goes a long way in terms of benchmark performance.
Most of my complaints and troubles were based on an inability to navigate the documentation and compare it to the GTK docs. This isn't my first foray into GTK, just my first with Haskell. I have never had much luck with GUI Dev outside of a browser though - I have yet to find a desktop UI framework that didn't make me want to bash my head into my desk. I'm happy to see all the alternatives being discussed here in this thread - maybe I can finally find an option that isn't excruciating.
I was initially thinking about implementing just enough geo stuff to get open street map working. But it looks like it might make sense to split the code base into two packages. 1. core package that contains geo operations and 2. OSM support This will make the core package depend only on very few things in particular no xml-conduit. 
That's a shame
Do you know if there's any estimate when it'll be available in print? 
I agree. I've been thinking about learning Haskell once my term ends, and have seen plenty of references to things like learn you a hask, but this was the first reference I saw to the Haskell book, so OP's question was useful to me at least. 
Yeah, it's called list. ;) You'll notice that your `Emptyable` type is nothing more than: data List a = Nil | Cons a (List a) i.e. data [a] = [] | a : [a] -- pseudo-haskell Edit: I see that's already been pointed out. I suppose what you're after is the explicit / different constructors. In that case defining a type like this makes sense in your case. Because it's nothing more than list with different constructors, I doubt you'll find type that you can reuse for this.
Related: https://www.reddit.com/r/haskell/comments/2hpzpu/announcing_needle_asciified_arrow_notation/ 
Beautiful examples. Thanks! 
The `websockets` package is wonderful! If you need secure WebSockets (WSS), check out [my `wuss` package](https://github.com/tfausak/wuss). 
&gt; Doesn't Reflection without Remorse also add a bunch of its own dictionaries for the type-aligned stuff? I don't think so. The CPS transform sounds interesting. I'll give that a go.
This looks great! Thanks for writing and sharing your work.
Wait are you saying that we need two packages: one for ws: and another for wss: ? Why don't you guys merge them into one package? All websocket packages I've seen support both (in JS or C++, for example).
After letting this simmer for a bit, I think PMonoid may be the correct constraint on the underlying functor to make Cofree an Applicative/Monad: instance (Functor f, PMonoid f) =&gt; Monad (Cofree f) where return x = x :&lt; pmempty (a :&lt; m) &gt;&gt;= k = case k a of b :&lt; n -&gt; b :&lt; (n `pmappend` fmap (&gt;&gt;= k) m) Am I missing some reason that f should be Applicative and Alternative as well /u/edwardkmett?
We can get WSS using [wai-websockets](https://hackage.haskell.org/package/wai-websockets) to lift ws into `Application` used by `warp` and then launch it with [warp-tls](https://hackage.haskell.org/package/warp-tls)
Yes, got it. I wanted to reply to @mallai question but replayed to @taylorfausak by accident. Thanks, your reply makes sense.
Let's write `nub`! -- Haskell, term level: nub :: Eq a =&gt; [a] -&gt; [a] nub = go [] where go acc [] = acc go acc (x:xs) = go (insert x acc) xs insert x [] = [x] insert x (y:ys) | x == y = y:ys | otherwise = y : insert x ys HLint will helpfully tell us that we can factor out the recursion and use `foldl`: nub = foldl' insert [] where insert x [] = [x] insert x (y:ys) | x == y = y:ys | otherwise = y : insert x ys But we're going to keep the original formulation, since it'll be easier to convert to Haskell's type level. There are two main ways of defining functions on types: functional dependencies, and type families. Functional dependencies are a little more awkward than type families, though they're useful in their own right. [This is a fantastic excursion into type level programming with functional dependencies](https://aphyr.com/posts/342-typing-the-technical-interview). Let's instead do this with type families. The syntax for a type family is: type family Nub xs where Nub xs = (some type) Specifically, this is a *closed* type family -- we'll be defining all the possible cases here! We can provide *kind* annotations on type families, which allows us to enlist GHC's kind checker to verify our programs: type family Nub (xs :: '[k]) :: '[k] where ... The `'` is a tick that's used to *promote* a data constructor from the value level to the type level. We need them, as otherwise the syntax is ambiguous: does `[Int]` mean "the type of lists of `Int`" or "a type level list containing only `Int`"? So we use `'[Int]` to indicate "a type level list containing only `Int`". Anyway, back to our type level programming. In the value level `nub`, we used a `where` block with some helper functions. Haskell's type families do not have anything similar, so we need to float these to the top level. type family Nub xs where Nub xs = NubHelper '[] xs type family NubHelper acc xs where NubHelper acc '[] = acc NubHelper acc (x ': xs) = NubHelper (NubInsert x acc) xs So we can *call* a type level function `NubHelper` and pass it arguments, just like a normal function. But these have to be *type* arguments, so we specify `'[]` so be clear. Likewise, we can specify different *cases* of the function, but we can't use `case` expressions -- we have to pattern match at the top level on the type constructors. Note that `x ': xs` is the *type level* constructor for consing onto a type level list. type family NubInsert x xs where NubInsert a '[] = '[a] NubInsert a (a ': xs) = a ': xs NubInsert a (b ': xs) = b ': NubInsert a xs The big difference here is that we don't have explicit equality checking -- we can just bind the same type variable name, and GHC knows that we mean that the types must be the same for this case to match. This is actually pretty neat, and I kinda wish that it worked at the value level too! But it doesn't. Oh, uh, you'll need some language extensions: {-# LANGUAGE DataKinds #-} {-# LANGUAGE DeriveFunctor #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE LambdaCase #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE PolyKinds #-} {-# LANGUAGE RankNTypes #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE StandaloneDeriving #-} {-# LANGUAGE TemplateHaskell #-} {-# LANGUAGE TypeApplications #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE UndecidableInstances #-} I'm pulling `Nub` from a file with other stuff, so some of those may not be necessary, but `DataKinds`, `PolyKinds`, `TypeOperators`, and `TypeFamilies` are definitely required. But then, you can go into `GHCi` and play around with it! Î»&gt; :kind! Nub '[Int, Char, Char] &lt;interactive&gt;:1:5: error: Illegal type: â€˜'[Int, Char, Char]â€™ Perhaps you intended to use DataKinds Î»&gt; :set -XDataKinds Î»&gt; :kind! Nub '[Int, Char, Char] Nub '[Int, Char, Char] :: [*] = '[Int, Char] We've successfully `nub`ed a list, both at the type level and the value level. Why didn't I use a type level `foldl`? [It's kind of complicated](https://typesandkinds.wordpress.com/2013/04/01/defunctionalization-for-the-win/), but basically you can't pass higher order functions around at the type level like you can the value level. Let's see it in Idris. My Idris isn't great. module MyNub data MyList a = MNil | (::) a (MyList a) nub : Eq a =&gt; MyList a -&gt; MyList a nub = go MNil where insert : a -&gt; MyList a -&gt; MyList a insert x MNil = x :: MNil insert x (a :: xs) with (x == a) | True = a :: xs | False = a :: insert x xs go : MyList a -&gt; MyList a -&gt; MyList a go acc MNil = acc go acc (x :: xs) = go (insert x acc) xs data RecordStuff : (xs : MyList Type) -&gt; Type where Empty : RecordStuff MNil One : (a : ty) -&gt; RecordStuff xs -&gt; RecordStuff (nub (ty :: xs)) So we can't actually use this definition, since `Type` doesn't implement the `Eq` interface in Idris. This wasn't as useful as I was hoping it'd be.
I... do not understand this post in the slightest. I mean, I can see what you are doing but I don't understand what point it is trying to make. Why is Type involved here and why would it ever need an Eq interface implementation? 
Most of the times that you'd want to do something like compare types for equality in Idris (which would break parametricity) you'd use the universe pattern. Something like: module MyNub data Ty = BOOL | INT Eq Ty where (==) BOOL BOOL = True (==) INT INT = True (==) _ _ = False interpTy : Ty -&gt; Type interpTy BOOL = Bool interpTy INT = Int data RecordStuff : List Ty -&gt; Type where Empty : RecordStuff Nil One : {ty : Ty} -&gt; (x : interpTy ty) -&gt; RecordStuff ts -&gt; RecordStuff (nub (ty :: ts)) 
I'm not sure if it is referred to as "the universe pattern", but this is defintely used in the Idris book in Chapter 6. So, [here's a longer example](https://github.com/stephen-smith/tdd-idris-ex/blob/master/ex_6_3.idr) from my efforts to work through the book's exercises. Sorry about the lack of documentation. That section in particular needs more doc comments. The book gives the full context, but it'd like it to be somewhat valuable without the book, while also not infringing on the book's copyright.
On average, bank pays ~100K base + 100% bonus... I don't think 200K is a norm for FB in London.
Passing trough firewalls is also a case for the use of websockets client. And It is possible to make it secure too. To connect nodes behind firewalls, I managed to use your websocket library in both the client and server over TLS connections for the package transient-universe, so I think that this count as secure websockets on both sides. The node that connect act as the client and the connected act as server. As long has there is a TLS connection stablished, only is necessary to define a websocket `stream` with the TLS primitives for send and receive in both the client and the server. To avoid the pollution of the huge dependency list of TLS in my library, I defined some hooks so that TLS_connect TLS_send were no-ops unless the TLS extension module is included in the source of the program and `initTLS` is called at main, which override the hooks. So TLS is only paid if used.
You are better off installing stack for windows and using the Emacs Eshell REPL or using Emacs as terminal emulator by running M-x async-shell-command ghci. - https://chocolatey.org/packages/haskell-stack It doesn't take all that time. 
&gt; You can do type driven development in a dynamically typed language! In Clojure, there's a new standard library called `clojure.spec` that encourages exactly this. It's actually quite great. Not as great as static types, imo, but better than the previous status quo. Note that runtime verification is able to inspect values in ways which type systems typically cannot. So there are tradeoffs and arguments to be made in favor of this dynamic stuff. I'm not making those arguments, just raising awareness of their existence.
In semigroupoids we call PMonoid "Plus": https://github.com/ekmett/semigroupoids/blob/master/src/Data/Functor/Plus.hs That said, it carries some superfluous baggage due to the phrasing of the laws for Alt. I really should just redo that whole hierarchy. In any event, yes, this is what Cofree needs to be a Monad properly.
That's not what a pure function is. A pure function is, like I said, something *reproducible* - if you try to open a file that doesn't exist, your program fails. Obviously it does the same stuff every time, but that's not enough to guarantee there are no side effects. 
Why are these better? I don't think the *I* prefix is necessary since we wouldn't be using a typeclass name in the same spot as a type name anyway, like you would in Java or C#
&gt; The first is arguably the more common one since WebSockets clients are primarily browsers. Unless you're making a [TUI client](https://github.com/matterhorn-chat/matterhorn) for a web-only chat system :) Maybe our use-case is odd, but thanks for writing `websockets` so that users can make clients too. It's been useful for us.
Mappable is the only one here that might work. The rest are either even more vague and ungooglable, or they would appear in contexts where these names would only provide more confusion. For example, if I'm not directly using the Monad methods then 'AndThenable' starts appearing in functions that are not 'and than'ing anything.
What on earth is "AndThenable" supposed to mean? What does it mean for a type to support "and then"? This is why "monad" is a good name: there *is* no good plain-English term for it.
Can you identify and list the problems with the previous names. In particular, I'm interested why these names are more accurate or explicit. It seems that the current names are the ones that most precisely identify the idea. There is no reason for the *I* prefix. This is not .NET. All type classes are basically 'interfaces' in a sense. I sure hope you don't label your functions with 'f'!
&gt; Multiple interfaces can be implemented by the same type. I am aware. &gt; So a magma can implement both combinable and composable. Magmas don't have the right kind to be composable. Magm**oid**s don't have the right kind to be "combinable".
Okey looks like there is interest and lot of interesting stuff to do in the geometric aspects of this. I am splitting this package into two `naqsha` and `naqsha-openstreetmap`, with the former just containing enough types to do geometric computation.
For Monoid I prefer `Smushable` to `Combinable`.
"MappableFunctions" doesn't really work for `Applicative`. I believe your intent is making a reference to the `f (a -&gt; b)` functions-in-a-context/static arrows from the signature of `(&lt;*&gt;)`. However, that ends up being rather confusing, because (1) in most cases they aren't literally functions; and (2) "mappable functions" can't possibly refer to all `Applicative f =&gt; f a` values, which is unlike e.g. your use of "mappable" for `Functor`. The best way to quickly describe `Applicative` is probably saying that it offers a monoid on the functorial context; that being so, a semantically suitable name within your scheme would be "CombinableMappable". (I find it very tempting to offer "Zippable" as an alternative suggestion, but that would end up being confusing for different reasons.) --- On a broader note, when it comes to simpler names for classes, the main problem is that in quite a few cases it is genuinely hard to find alternatives that are reasonably short and neither jargony nor misleading. Furthermore, the sheer variety of typeclasses and variations thereof that exist just outside the Prelude (`Functor` vs. `Contravariant` vs. `Bifunctor`, `Monoid` vs. `Semigroup`, `Applicative` vs. `Apply`, `Category` vs. `Arrow` vs. `Profunctor`, etc.) means that any simple naming scheme will have to face serious scaling challenges, with the alternative being a naming style discontinuity. That being so, if the goal is making the concepts less scary to newcomers, I'm more inclined towards keeping the names but handling them with a touch of irreverence, and explaining their meanings with as little additional jargon as possible. (In any case, thinking about alternative names can be an interesting exercise, and I don't deny the possibility it might prove useful for some situational pedagogical move.)
For Monad I prefer `Burrito` to `AndThenable`.
The usual explanation for monads becomes so much simpler with these, though: IAndThenables are just ICombinables in the IComposable of ISelfMappables
AbstractGizmoFactoryEnterpriseEndofunctorBean is good, too
So, taking some of the suggestions in this thread, I think that leaves us with: "A [burrito](https://www.reddit.com/r/haskell/comments/660ai8/what_would_better_names_for_the_standard_type/dgeu905/) is [smushable](https://www.reddit.com/r/haskell/comments/660ai8/what_would_better_names_for_the_standard_type/dgetvc3/) and [flattened](https://www.reddit.com/r/haskell/comments/660ai8/what_would_better_names_for_the_standard_type/dgens6p/) when ICompose it to a [map of myself](https://www.reddit.com/r/haskell/comments/660ai8/what_would_better_names_for_the_standard_type/dgeuvfo/)." I think we all see the problem.
That is a different monad than the monad haskell uses. Haskell's Monad is the categorical monad. Not Leibniz monad. They are in no way related. 
Changing `Applicative` to `LaxMonoidal` would be ok.
I think it would be better if it's possible to use single function as a typeclass. So, there won't be need for typeclass name. For exampe in Oczor https://github.com/ptol/oczor/wiki/Haskell#classes class show a : a =&gt; String instance Int show x = "Int" show 1 
Cool! It is also similar to [`Exists1`](https://hackage.haskell.org/package/exists-0.2/docs/Data-Exists-CPS.html) newtype CodTy' :: (k -&gt; Type) -&gt; Type where CodTy' :: (forall r. (forall i. p i -&gt; r) -&gt; r) -&gt; CodTy' p newtype Exists1 :: ((k -&gt; Type) -&gt; Constraint) -&gt; (k -&gt; Type) where Exists1 :: (forall r. (forall f. c f =&gt; f a -&gt; r) -&gt; r) -&gt; Exists1 c a the continuation-passing form of [`Exists1`](https://hackage.haskell.org/package/exists-0.2/docs/Data-Exists.html#t:Exists1) data Exists c where Exists :: c a =&gt; a -&gt; Exists c 
I tried that approach but didn't get far, do you have comments on the [thread](https://www.reddit.com/r/haskell/comments/64kpsb/where_does_forall_ty_sty_ty_r_r_fit/dg39fw6/) between me an /u/davemenendez?
Probably, but there are some more differences: 1. The HRC compiler uses defunctionalization - like mlton. What is the influence of that? It probably doesn't matter if the benchmark code is not of higher order. Would it be faster than an eval/apply model? 2. Why is it faster than the llvm backend? How does the generated vectorized code differ? 3. What would be the performance in the presence of laziness? They say, they "have no laziness in performance-critical sections". How does HRC implement laziness after all?
Oh yes. There are plenty of differences between the compilers. Re: your point #1, note that they explicitly don't do whole program supero-style defunctionalization. They do seem to get a lot of mileage out of their unification based breakdown of types into different equivalence classes and managing representation by equivalence class, though. They do seem to care a lot more than we do about loops: performing loop inversion, converting top-loops to bottom loops, loop unrolling to set up for SIMDification, etc. We don't really do anything for loops that is all that special beyond setting up worker-wrapper. These sorts of optimizations tend to help a lot with vectorization up to about a 4x factor in other compilers. Eking out effective use of 8-way parallelism, like with AVX2, tends to need other more drastic techniques, like those used in the Intel SPMD compiler, as "opportunistic superword level parallelism" to borrow the terminology of Jae-Wook Shin doesn't tend to find enough opportunities to scale out that far. The one foray we really made in this direction (to SIMD optimize vector) was dropped on the floor due to lack of maintenance effort. They seem for the most part to encode laziness properly from what I've seen with some gaps when it comes to exception handling. They may have a couple of cases where they aren't as lazy as necessary for correctness, but it's hard to parse through that much code without a guide or documentation to see.
I'd need an `eval` which can capture the local context, i.e: function set(lens, s, b) { return memoize(.., eval("function() { .... precompile(lens) ... }"))(s, b); } I'm not if one can do that, or how. AFAIK https://github.com/nathanmarz/specter does compile it stuff, but it's way more easy in clojure as you have macros, you *can* eliminate the abstraction `set (lens getter setter)` -&gt; `setter` at "compile time".
I opened to procrastinate the article later, as it seemed an interesting topic, but the very first impression that stuck on me of the article was: Grey text on a grey background. 
&gt; At least the mathematical jargon has precedent. Not only precedent, but also (mostly) clear boundaries. There's much less worry or argument about what is or isn't Functor than there might be for IMappable (or, say, Foldable...) (I say mostly-clear, because there's some question of whether we're reasoning "up to bottom" or not, "in the presence of seq" or not, etc.)
&gt; wouldn't be using a typeclass name in the same spot as a type name anyway No? `Proxy Monad` works fine...
"Why are my non-empty lists not 'combinable'?"
Right, they don't do mlton/supero defunctionalization and whole program optimization. So maybe their model doesn't even differ so much from how eval/apply works. I read call/eval as the eval dispatcher which is used in defunctionalized code to switch over the closures. I don't know how they exactly handle partial application and overapplication. Independent of HRC I am interested in how defunctionalized code (as in idris/mtlon/supero...) performs in comparison to eval/apply (as in ghc). Note that whole program optimization is not implied and it is possible to do separate compilation where the generation of closure constructors and dispatchers is deferred till the linking step. Concerning SIMD etc - what you describe, it is really the lesson here that such optimizations matter. Maybe these could be adopted for GHC even. It is unfortunate that some work in this direction was dropped. I wonder how much of the work llvm could do. Also vectorization in the presence of control flow could be done on the llvm level. Or do you think it is necessary to go to a higher level than STG as in HRC to optimize the call graph such that vectorization is "easier"? I've seen you did some experiments with SIMD in C++? Did you find out something interesting there?
&gt; Haskell currently has no such tooling. Really? I thought haskell-mode could do case splitting for Haskell.
&gt; Independent of HRC I am interested in how defunctionalized code (as in idris/mtlon/supero...) performs in comparison to eval/apply (as in ghc). Note that whole program optimization is not implied and it is possible to do separate compilation where the generation of closure constructors and dispatchers is deferred till the linking step. My interests in this direction come from a somewhat orthogonal direction: Tracing jits can effectively incrementally perform this transformation. If you take something like GRIN and leave the 'apply' function opaque you get the GHC story, of calling into an unknown function to evaluate a thunk. If you inline apply you get a GRIN style story for whole program optimization, but if you instead just trace through it, you get a 'gradual closing of the world' as missed cases are discovered. It is kind of nice to see all 3 of these scenarios expressible as one construct. LLVM's vectorizer is getting better and better as time goes on. I think it'll hit the same sort of ceiling as everyone else in this space though. That said, I'm personally a big fan of the style of code produced by the Intel SPMD compiler, which takes a very different approach to find SIMD-level parallelism. &gt; I've seen you did some experiments with SIMD in C++? Did you find out something interesting there? I've been poking and prodding at how a Haskell-like language could take advantage of such opportunities, but I've got a long way to go. Basically the idea is to take the approach from the Intel SPMD compiler and see what a good 'core' would look like that exploited such a structure. (In fact, it looks a lot like the core from HRC with some extra support for masks.) and combine it with some other code I have lying around for super-light-weight fibers that sits atop boost::context and a somewhat ghc-like asynchronous io manager. I'm also interested in exploiting the SPMD style to execute the basic block that I'd get out of a tracing jit in a parallel scatter/gather fashion, rather than directly. Exploiting this requires some additional machinery on the task parallelism side to gather argument sets. I've got some other code / ideas for a nice concurrent garbage collector that borrows from the Azul systems C4 collector. There I've also been experimenting a lot with more GRIN-like tagging, moving the information about the whole GRIN-style constructor (which includes thunks) to the other side of the function pointer, by exploiting the size of pointers on 64 bit platforms. I could see a scenario in which all of these things came together to spit out a Haskell compiler, or something for a Haskell-like language, but I've had very little time to devote to playing with it. I gave a talk on all of this at Boston Haskell last month. Unfortunately it wasn't recorded. =(
What differentiates System T from the simply typed lambda calculus? Is it the presence of "base types" type Nat and Bool? 
Agreed. Very useful advice, but very hard to read.
Don't forget Foldableable and Traversableable. :-) Actually it's great to have a mapping like this in your mind if not in the library. In my mind I always read &gt;&gt;= as "feeding". 
This reminds me of the lists of scheme properties, and which ones imply others.
Link?
No comments on that really. It seems to work but I don't find it sufficiently illuminating. Here's something else that continues on perhaps the wrong track? :-) (i.e. relates to the code above) class Lower f where lower :: f (a -&gt; b) -&gt; f a -&gt; f b app' :: Lower f =&gt; CodTy f r (ty -&gt; ty1) -&gt; CodTy f r ty -&gt; CodTy f r ty1 app' (CodTy f) (CodTy x) = CodTy $ \z -&gt; f (\q -&gt; x (cpsL q z)) cpsL :: Lower f =&gt; f (ty -&gt; ty1) -&gt; (f ty1 -&gt; r) -&gt; (f ty -&gt; r) cpsL f x = \a -&gt; x (lower f a) One can of course write lower for STy, and its rather boring. What you can do with this I haven't fully considered... I feel sort of like I'm defining the functoral operations in the wrong direction from what you want actually...
The upper right part of the optics hierarchy is missing, I'm guessing, for the same reasons as Coapplicative and Comonoid. Would linear types make the hierarchy symmetric?
QuickCheck has features in place for testing, shrinking, and showing functions. Check this out: https://hackage.haskell.org/package/QuickCheck-2.9.2/docs/Test-QuickCheck-Function.html And the talk: https://www.youtube.com/watch?v=CH8UQJiv9Q4
As far as I can tell, you *can't* get GHC to statically link the runtime and module dependencies into your code unless you're building an executable. This pretty much makes what you're asking for impossible.
In addition to other posts, you'll need some way to start up the RTS. The Haskell RTS is significantly larger than the C RTS, since it contains both spark management and garbage collection. As far as I know, there's no way to run without the RTS. (In C you can with non-hosted code, in Rust you can with the help of langvars.)
Yeah, I've run into that. The interface I'll want to expose is simple enough that I can just start it up when the user calls the function and close it before returning.
As am I (in my case, I want to use C++ and Qt for the GUI, but Haskell for computation), but the only half decent solution I've found is to use call-haskell-from-anything. Alternatively, you could turn the C portion into a library.
I don't believe those are better names as they rename the existing nomenclature in mathematics. Why rename well-known concepts?
Interesting, I've definitely never encountered something like that, but I'm a pretty casual Haskell user. Thanks for the explanation! 
The use of the word "monad" that Haskell uses is still 40-50 years old (with the concept itself being older), along with 40-50 years of background and associated properties, theorems, laws and connections to other concepts (and all of those things have been specified in a very precise way since the beginning). And, not only that, but those things precisely describe what the Haskell monad is like (in the setting of Haskell). I'm not saying there's not a better name necessarily, but if I heard `AndThenable` for the first time, I would definitely have some incorrect preconceptions about what it is (defeating the purpose of the new name, I'd say). If I hear it as "monad," there are three possibilities as I see it: 1. I don't know what it means (by far the most likely). In this case, unlike the `AndThenable` case, I *know* that I don't understand it yet. It is better, I would say, to know when you don't understand something than to think that you do and build up a lot of incorrect thoughts about a concept. 2. I already know something about categorical monads. That would give me some sort of idea of what's going on already. Not how I'd use it in a practical sense, but it certainly wouldn't be misleading. 3. I only know about the philosophical "monad" (I would argue that this is by far the least likely possibility). I suspect that I would very quickly realize that this is incorrect (I would probably think that it doesn't seem right from the start, so I'd do a quick Google search and confirm it). This is unlike `AndThenable`, where it makes sense for some monads but could (in my opinion) give false intuition about other monads.
There's one at the back of Torsten-Wedhorn. I can't think of any specific reference, sorry. :(
&gt; Torsten-Wedhorn That guy does algebraic geometry, right? Are schemes from algebraic geometry and recursion schemes the same thing?
Two guys. No, a scheme is a kind of abstract geometric "space" built by gluing together rings in a certain way, whereas a recursion scheme is an abstraction for some particular way in which functions can recurse/fold/unfold. Completely unrelated. Edit: this is some slightly related, but _really_ funky stuff I don't understand: http://pbelmans.ncag.info/topologies-comparison
https://en.wikipedia.org/wiki/MetaPost, the hierarchy graph is using Haskell as a macro language (obviously). *EDIT:* https://en.wikipedia.org/wiki/Asymptote_(vector_graphics_language) might be easier to learn; or http://hackage.haskell.org/package/diagrams; I'm slaved by my old habits.
I suppose technically it's off-topic. But I'm glad you posted it. I want to hear about what's happening to Dmitry, and clearly many others in the community agree.
The name doesn't illustrate what the typeclass is "for" - because they are collections of abstract behaviors, not a description of the common features of the underlying data. These names describe the laws that their typeclass methods follow. Bind, when called on a State Monad is not the same as Bind called on the List Monad - they share essentially nothing except for adherence to common compositional laws. That's not an accident or a coincidence, or a quirk- it's the whole point of Haskell classes. It is not possible to explain what Monad instances have in common by referring to "plain English verbs" because "plain English" does not have the vocabulary to describe collections of functional types. The "plain English" description of the Monad is the Monad laws, and that's a bit lengthy to use as a typeclass name. So instead, we use the categorical term, because category theory has a lexicon to describe this stuff.
Note that John and I debugged this a little, out-of-band, and found a slight error: the example uses *unbounded* integers in the model, while it *claims* to specialize the equation to `Int` via `-XTypeApplication`. In the face of integer overflow, the equation has different satisfiable inputs. Obviously it's important the model and specialized types match. Either way the *actual* repository has a more abstract interface, so hopefully this won't be possible shortly. :) Anyway, beyond that: this is extremely neat and I didn't realize Conal's work had come so far. It will be extremely cool to see how to use this for real compilers and DSLs!
really. it needs javascript in order to turn white-on-white into gray-on-slighly-other-gray? bad impression.
This is definitely the best way I've seen. You get the functions, the shrinks, and a *useful* `Show` instance.
Would porting to [qtah](http://khumba.net/projects/qtah/) help your case at all?
Some time ago I switched the `containers` test suite over to this, getting rid of the stupid `Show (a -&gt; b)` instance. I mostly wanted to do it that way to stop the yuck factor. The only annoying bit was dealing with functions that take multiple arguments. I ended up currying because that seemed a bit clearer than the alternative. That is, if I needed a function `a -&gt; b -&gt; c -&gt; d`, I'd ask for `Fun (a,b,c) d` and curry it up, rather than asking for `Fun a (Fun b (Fun c d))` and converting arrow by arrow.
This "CCC" doesn't actually appear to have functions as objects. How do you send a function-valued expression to Z3? I doubt you can.
Developing APIs can be hard. No developer is likely to think of all the possible use cases. I personally have no complaint about multiple early versions of a package while the API and usage is sorted out.
profunctor lenses break down when you go to add indexed lenses to them. On the other hand, because there isn't the existing Haskell class hierarchy to get in the way its easier to model affine/relevant traversals.
I was mainly talking about the search. That works quite well, especially with the dependent type information. 
Eh. Polykinded stuff is slightly rare in real code, especially constraint kinded stuff (outside of constraint synonyms). I think the name spaces are *basically* distinct.
Well, we have hoogle which [integrates with emacs](https://wiki.haskell.org/Hoogle#Emacs_Integration). I do think Idris' search is neat, but it honestly hasn't saved me a trip to the online documentation, yet. And, Idris' online documentation is not quite as good what is available on hackage. I think it's something to do with namespaces or something, but there are some symbols / modules mentioned in the TDD with Idris book that I can't find in the online docs, but they work just like the book says they do. :/
Fair point and it was at https://github.com/haskell/haskell-platform/issues/274 . Doubtlessly it will get fixed, but I still thought it was useful to share a workaround until the Haskell Platform maintainers get around to it.
By *reported* I mean "... to Microsoft" :)
There are additional categorical structures involved here, beyond CCC. There's also bicartesian, and some new things Conal created: `BoolCat` and `NumCat`. `BoolCat` provides the morphisms `andC` and `orC`, which the plugin translates `&amp;&amp;` and `||` into. So it's not *just* CCC. The plugin allows you to provide arbitrary additional structures, with the caveat being that you can only interpret those structures into categories able to supply the associated instances.
Also a fair point. But while Microsoft does listen to the Haskell community, it takes *them* forever to fix anything as illustrated by a Microsoft bug that prevented stack from working under the Windows 10 Ubuntu environment. A year or so after it was first reported, it is said to be fixed, ironically in exactly the same Creators Update that broke ghc. ;)
There is a new foreign libraries feature http://cabal.readthedocs.io/en/latest/developing-packages.html#foreign-libraries which is shipping with Cabal 2.0. Might be worth a look, though there isn't a tutorial yet (maybe you could write one!)
Pianist here. It's true, if you practice left hand then right hand you can become very good at playing each hand separately. I was always been taught to break into small pieces (a single bar, half a bar even) and practice until you are confident. Then move to the next small piece and repeat. Then (++) them together, etc. But all the time using both hands. Point is to not rush before you have the basics. Guess it works for programming languages too :)
&gt; Eh. Polykinded stuff is slightly rare in real code, especially constraint kinded stuff (outside of constraint synonyms). Probably. &gt; I think the name spaces are basically distinct. Except the namespaces *aren't* distinct. If I define `data Monad = Monad`, I can no longer write `Monad m =&gt; ...` Prelude&gt; :t undefined :: Monad m =&gt; m a &lt;interactive&gt;:1:14: â€˜Monadâ€™ is applied to too many type arguments In an expression type signature: Monad m =&gt; m a In the expression: undefined :: Monad m =&gt; m a 
Yes, `hask` is nice for demonstrating ideas from category theory in a broader context. Unfortunately, it also demonstrates that Haskell's class system isn't really suited to capturing the ideas at their most general. (E.g., a type constructor is associated with at most one functor, but there are at least four functors you could associate with, say, `[]`.) In any case, if you're going to define `SomeCat` as a newtype, I'd go ahead with something like: newtype SomeCat p q = SomeCat (forall i z. (forall j. q j -&gt; z) -&gt; p i -&gt; z) instance Category SomeCat where id = SomeCat $ \k p -&gt; k p SomeCat f . SomeCat g = SomeCat $ \k -&gt; g (f k) Avoid the overhead of `Some`.
That's still not really my point. My only point was that constraint kinded code is rare enough that you basically never see it, and the namespace intersection is largely irrelevant. Im not trying to say anyone should reason about Haskell namespacing as though they're distinct, or that the fact that they're not distinct doesn't matter. That's all completely orthogonal to what I was saying which was merely a defense of this original statement &gt; we wouldn't be using a typeclass name in the same spot as a type name anyway
Cool, thanks! How is the search command called? I want to set up a shortcut to it, but I can't figure out the name. E.g. with Haskero I have alt+k bound to haskero.insertType
Very nice.
They are back!
I've been looking forward to this feature for a while now. Do you know when Cabal 2.0 is going to be released?
That's what I meant by turning the C++ code into a library.
Yeah, I don't know that there is a good alternative to that method. It suffers the problem of having to manually curry / uncurry back and forth, but I suppose it's worth it for the quality of the counterexamples.
You don't have to create a new release after each single change. People who use stack can easily pull a package directly from a git repository, at a specific revision. That's a far easier and gentler way to gather community feedback than releasing dozens of alpha, beta, or rc versions. This also allows them to fork the package on github, point stack to their fork, test their own changes, and then submit them upstream. 
You are implicitly saying that these 20 pages of abstractions is what you use to assign variables , update records and perform iterations over data structures, which are the most basic things that can be done in a program. I can't even imagine what you use to manage exceptions, transactions, threading and network programming. It is perfectly understandable that the best real world programmers laugh at haskellers in the back. When they do not get angry.
And how do you translate `True`? Suppose I wrote equation :: a -&gt; a -&gt; Bool equation _ _ = True I don't see anywhere that translates `True`, not even in `BoolCat`. 
(Off-topic but) every "level" takes practice. You practice with each hand separately to imprint that muscle memory; and if it's too hard, you can even play a coarser version with a single hand, like if you're trilling with thumb and 2nd while playing a melody with the pinky and 4th; and if that's too hard, you should attempt something easier. Then, you still have to practice playing either hand together. Generally, the technique my teacher taught me was to practice a passage with different restrictions. For example, try to "freeze" your wrist as much as possible, only pressing down your fingers or shrugging the shoulder; then try "freezing" your fingers, instead pushing a limp finger into each key from your wrist motion; then try playing with your eyes closed; then fast, then very slow, then very fast, etc. (back on-topic) So, "modularity" is how I learn best, but obviously repetition is important. 
&gt; the purist idea would be that if you can do each hand individually without thinking, then it will be ultimately easier to combine them. Unless you're like Franz Liszt, that doesn't work. I don't think anyone thinks that about learning anything! These actions can be shrunk down into simpler "subactions", but they aren't perfectly modular; what your brain "experiences" while doing a pair of things individually is almost always distinct from what it experiences when doing both simultaneously. 
The paper says this: We will also want to consider categorical re-interpretations of some primitive constants. For base-typed primitives such as booleans and numbers, weâ€™ll use arrows from terminal objects: class Terminal k â‡’ ConstCat k b where unitArrow :: b â†’ (1 â€˜kâ€˜ b) instance ConstCat (â†’) b where unitArrow b = Î»() â†’ b
Pianist and haskeller here. I agree about everything except the mindless repetition. You should always be mindful whenever you practice, and be aware of the sound, melodies, harmonies, etc... It's the same when you practice slowly, with both hands apart, or doing simply technical exercises. Some teachers propose pure mechanical study, but I am convinced that awareness of the body, relaxation, sound, and feeling of the keys is essential, even when doing purely technical stuff (like scales, technical exercises, etc). Perhaps it's the same in programming, where you have to keep in mind the whole picture, not just solve a problem locally without think how it effects the whole program.
Ah which might be enlightening with superfluous parentheses newtype SomeCat p q = SomeCat (forall xx. (forall yy. q yy -&gt; xx) -&gt; (forall zz. p zz -&gt; xx)) and type synonyms type SomeElim f r = forall xx. f xx -&gt; r type f ~&gt; g = forall xx. f xx -&gt; g xx newtype SomeCat p q = SomeCat (SomeElim q ~&gt; SomeElim p) which looks like it means something ---- If we wanted to associate the same functor with multiple domains / codomains we could add new arguments class Functor f dom cod losing us type inference.. in which ways can `[]` be made a functor? Lists can already be made `Applicative` in two ways and 90% of the time we are fine `newtype`ing it so I imagine the same thing will be done here.
Something's got to convert that into the "true" value of type `AST` and I don't see where that happens.
We discussed and added pattern synonyms for functions of multiple arguments [here](https://github.com/nick8325/quickcheck/issues/110) ([code](https://github.com/nick8325/quickcheck/pull/111/files)). Does this solve your problem?
Since we're using `Kleisli` under the hood, we get some things for free: instance (Monad m, ConstCat (-&gt;) b) =&gt; ConstCat (Kleisli m) b where const b = arr (const b) Later, it's the `EvalE` instance that figures out how to render this constant: instance EvalE Bool where evalE = evalPrim evalBool id Since we pass `evalE` to the call to `withModel`. Note that this work is fairly new, so there are bound to be some rough edges. Please log issues on GitHub if you think of any problems!
Can you show me what such a term would look like?
Here is the link to the other discussion: http://elvishjerricco.github.io/2017/03/10/profunctors-arrows-and-static-analysis.html
I think geometric statements and queries in the style of `diagrams` would be very fitting.
I don't know. If `Fold s a` let you do `s -&gt; [a]`, then `Unfold s a` would let you to `[a] -&gt; s`; when `s` is [`a`], `toListOf` is an `id`, but the unfold operation would be of type `[a] -&gt; [[a]]` or something. (I guess we'd need a `Comonoid` which would split `[a] -&gt; ([a], [a])` at *some* point, i.e. rather `r -&gt; [(r, r)]` returning all possible "size preserving" splits) My reasoning: `Fold` "forgets" the structure, the reverse should build the structure from the contents. List example is simple as there's only one way to build a list of `length` *n*. but there are e.g. many `Tree`s of same `length`. Maybe the hierarchy can be extended, but I don't know anyone had done the needed research (or at least connected it to optics).
hm, bad advice. You don't want restrictions, you want freedom (of movement). Explore new possibilities, don't restrict. Yes you can focus on one part, maybe exagerate wrist motion, but don't go limp, don't restrict.
Hi, i'm developing a vscode extension to support Haskell development (named Haskero). I planned to develop a similar feature in Haskero but your extension do the job exactly as i wanted. So i am wandering: do you mind if i add an extension dependency to hoogle-vscode in order to load up your extension when someone installs Haskero ? Cheers, 
I wrote a bit about lifting functions into an SMT solver a few years ago, from a firm EDSL point of view. E.g. using SMTLib, the Haskell function: &gt; \p q r -&gt; (p â†’ q &amp;&amp; q â†’ r) â†’ (p â†’ r) can be lifted into an SMT equation: &gt; solve $ \p q r -&gt; (p â†’ q &amp;&amp; q â†’ r) â†’ (p â†’ r) And spit out the SMTLib, &gt; (define p::bool) &gt; (define q::bool) &gt; (define r::bool) &gt; (assert (=&gt; (and (=&gt; p q) (=&gt; q r)) (=&gt; p r))) https://donsbot.wordpress.com/2011/01/16/painless-np-complete-problems-an-embedded-dsl-for-smt-solving/ This post was really nice to see it from a CC perspective. There's some nice papers to be written on good EDSLs and CC I think...
Thanks, everyone! Cabal's new foreign libraries feature looks like the best long term bet, so I'll give that a shot first. I'll post any updates in this space -- if I do get things working expect an example scaffolding repo. 
Right now, it's not set up with any keybindings. The way I've been calling it is opening up the Command Palette, and navigating to "Hoogle Search". Maybe something like alt+h for hoogle search would be okay?
/u/edwardkmett and /u/phadej thank you both very much for the explanations and the addition to the post. I am looking forward to how this indexed lenses story turns out or which representations proves to be the best in the end. purescript uses profunctor lenses, your js library uses profunctor lenses, so there should be some experience accumulating... Would it possible to fix the aforementioned unification issues in the compiler? This is something I am wondering from time to time - if there is a need to add some support to the compiler for some of the lens infrastructure. Maybe unification issues, maybe autogeneration facilities to improve usability?
\#111 on #EEE is always a sensible default. 
Cool, let me know if you have any issues/suggestions!
Hm, okay. That sounds pretty good. I'm not too familiar with how extension dependencies work, so would that install hoogle-vscode for them, or just list it as a recommended extension?
I don't know if anyone is doing that sort of thing. Hoogle's JSON api does provide documentation about the returned methods in the response. I'd think adding that to this extension might require a dependency on ghc-mod or something like that in order to get the name of the symbol and its defining module. I wonder if it would be possible for other extensions that already have a dependency on ghc-mod to call hoogle-vscode to interface with hoogle? I do like the other idea of pop-up documentation.
&gt; Interesting view, to see JIT as an intermediate interpolating between the two representations. Do you expect it to be worth it? I do. The spineless tagless g-machine is fast despite the fact that to do a single case statement it makes two blind calls to arbitrary function pointers for every single case statement. A tracing jit directly inlines those calls. This means that the inliner could be much less aggressive. As for the complexity of the runtime, sure the runtime would get more complicated, because it'd have to supply a JIT as well as the executable, but the programs it is tracing could be drastically simpler if you got away with doing less pre-emptive optimization. The prospect of compiling faster and still producing faster programs appeals to me. Whole-program optimization is painful in terms of developer time. I routinely build some pretty big programs over and over as I make small tweaks. I like the costs to be more proportional to my change size. =) That and the supero/grin-style optimization can result in huge program blowups once your examples get non-trivial. The JIT lets you pay to elaborate based on the portions of the code that actually matter. My perfect world version would be to have default execution still be of compiled code that acted in a rather STG-like fashion, but have tracing gradually close the world getting you incrementally more and more GRIN-like performance. Then because everything sharesthe same heap representation, etc. A whole program optimizing mode where you inline all of the applies and try to crunch the entire program down becomes yet another compiler option you can turn on before shipping the final product or even just when JIT overhead would be too high. Interestingly it isn't even mutually exclusive with the JIT. Heck, I've had good results JITting raw x86-64 assembly out of an optimizing c++ compiler due to the ability to dispatch virtual function calls with branches and the ability to do runtime constant folding that isn't available to the whole-program optimizer. &gt; From the example .... they don't use intrinsics but recognize the loop. The default qualifier in the Intel SPMD compiler is what they call 'varying', which is a wide, say 8-ary array type of whatever the underlying type is. Reads and writes are gathers/scatters. Uniform values are the classic machine int type. You can think of the SPMD model as sort of computing several "lanes" worth of computation at the same time, and then masking off dead or inactive lanes when you take branches. e.g. an if statement with a varying boolean argument computes the mask for which lanes pass the test, ands it with the existing mask from the surrounding context, then checks if any lanes are active. If so, it takes the true branch with the new smaller mask, then it flips the mask and ands it with the old execution mask, checks to see if any are still active and takes the false branch if so. This limits parallelism as your branching becomes more and more incoherent. They layer a fork-join parallelism model on top to recover some parallelism/coherence, but stepping outside what they do: You can regain some for methods that aren't internally oriented around parallel operations by instead of calling them directly instead batching up arguments for them until you have enough and running methods off a priority queue. You can modify a work-stealing/sharing scheme to work off of a limited size priority queue rather than a deque using Acar et al.'s 2013 paper on work-sharing, as it permits more interesting scheduling options than a classic Chase-Lev work-stealing deque. If you want to executing a retroactively SIMD'd basic block produced by the tracing jit, you could batch up arguments similarly until you have several traces to execute, then run with the newly scatter/gather oriented version of your basic block using all the side-exit guards to shut off more and more of the masks and pausing when the mask gets too sparse until you get enough other work to make it worth continuing to execute the loop. This would rely on having lots of threads of activity to gather back up and being focused on throughput over latency, but I have lots of workloads like that. &gt; So you want to parallelize the evaluation of strict arguments using SIMD? Who said anything about the arguments having to be strict? You can case analyze in parallel too, you can do branching foreach_unique style dispatch to call varying function pointers just fine. If just loses concurrency based on how varied the inputs are. &gt; Do you have some slides? Sadly, it was done entirely off the cuff, as our scheduled speaker was stuck on a train. I did two talks of about an hour and a half each. The first was on SPMD-on-SIMD in the Intel SPMD compiler sense, and the second was on equality saturation using propagators. &gt; Yes, it would be a pity not to use the unused 16 or 8 bits available in 64 bit pointers. If you're willing to limit the heap size a bit you can steal a heck of a lot more than that. =) If I limit myself to 8 byte aligned pointers to heap objects I can steal the 3 bits on the bottom for local uniqueness and "type" to help gc, then I can steal bits on the top of the pointer for 1 bit for a not-marked through for a "self-healing" loaded-value barrier like the C4 collector, 2-4 bits to enumerate the space I'm in depending on if I want to do something like the old GHC local heaps, and 19-21 bits for the tag. That is enough to leave you 1tb of addressable heap per space, while having a large enough tag-space to cover all of the closure types that occur in a real program.
&gt; Would it possible to fix the aforementioned unification issues in the compiler? It is tricky to figure out a coherent story for what the "minimal" reusable sound language extension is that would permit indexed profunctor optics that are as nice as the current optic types. I certainly don't know what it should be. Also, there is a notion we don't currently explore in lens (it is incompatible with the notion of indexed optics) of what I call 'coindexed' optics. You can think of it as allowing information back in the failing match case. e.g a prism that returns an error message on failure. When you combine the two features the problem gets even worse, as one wants to push information from the left side of the (.) towards the right and the other wants to push information from the right side of the (.) towards the left and they need to conspire to produce the right types now with 2-3 sources of information about what it should be!
I'd like to see `purescript-profunctor-lenses` include ideas from the post (I did open few issues already, but no time &amp; need at work to implement them); or mention why things are done differently (e.g. there `type Fold r s t a b = Optic (Forget r) s t a b`). cc /u/paf31 --- As said, indexed and regular traversals compose: *Glassery&gt; :t itraversedList . traverse' itraversedList . traverse' :: (Traversable f, Traversing p) =&gt; Indexed p Int a b -&gt; p [f a] [f b] *Glassery&gt; :t traverse' . itraversedList traverse' . itraversedList :: (Traversable f, Traversing p) =&gt; Indexed p Int a b -&gt; p (f [a]) (f [b]) Even composing two *indexed* traversals work: *Glassery&gt; :t itraversedList . itraversedList itraversedList . itraversedList :: Traversing p =&gt; Indexed (Indexed p Int) Int a b -&gt; p [[a]] [[b]] Which makes me think whether there could be combinator transforming just the indices, so the `icompose` won't be necessary. Have to try out. The problem Edward highlights, is that we cannot `toListOf itraversedList`, but need to explicitly strip the indices: *Glassery&gt; :t toListOf (itraversedList . Indexed . lmap snd) toListOf (itraversedList . Indexed . lmap snd) :: [a] -&gt; [a] I cannot comment how much that's a problem in practice, as at work I use `lens`. Non-obvious assumption here is that compiler would be smart enough to fuse all intermediate `Indexed` wrapping&amp;unwrapping, so there aren't performance issues. (and that's one of the reasons I use `lens`, the most obvious performance pain points are already addressed). If people at SlamData have any experience on this issue (their `halogen` depends on `purescript-profunctor-lenses`, so I assume they use them in their product too), they should share their insights! --- There is a very simple reason `optika` uses profunctor encoding: if you throw away the types: it's quite simple (and only one type-class dictionary to pass around). Also, as I plan to write a TypeScript type definitions, where optics interfaces `extends` other, we can do interface IndexedLens&lt;I,S,T,A,B&gt; extends Lens&lt;S,T,A.B&gt; { ... } and the untyped implementation will just work. But for that, one should have `Indexable p q i` which would let the interpreter transform the dictionary on the go. So, there are question marks, but that was the motivation for the post: to clarify at least something. E.g. the hierarchy graph is a spec to *generate* the very repetitive typings: interface Lens&lt;S,T,A,B&gt; extends ... { lens&lt;U,V&gt;(getter: A =&gt; U, setter: A -&gt; V -&gt; B): Lens&lt;S,T,U,V&gt; // maybe this should live on the // interface ArrayLens&lt;S,T,U,V&gt; extends Lens&lt;S,T,Array&lt;U&gt;,Array&lt;V&gt;&gt; traversed(): Traversal(S,T,U,V) // A ~ F&lt;U&gt;, B ~ F&lt;V&gt; // ... } interface Traversal&lt;S,T,A,B&gt; extends ... { lens&lt;U,V&gt;(getter: A =&gt; U, setter: A -&gt; V -&gt; B): Traversal&lt;S,T,U,V&gt; traversed(): Traversal(S,T,U,V) // A ~ F&lt;U&gt;, B ~ F&lt;V&gt; // ... } // etc.
Oh, ok. Yes, I was talking about starting the RTS once, and maybe shutting it down once at some other time, or just let it running. Solves the problem :) But now I see where you are coming from. It would indeed be much cleaner to start it when needed and stop it later.
Yes, alt+h seems good. However, I thought that one can somehow expose one function of an extension which the user can then bind as he wishes (see my Haskero example).
I find your premise interesting but I don't think I agree with it. &gt; Operational semantics are observable. So strict languages and lazy languages are impure, because they fix operational semantics. (By operational semantics I think that you more precisely mean "reduction strategy" here.) What is it that you mean by *observable* in your first sentence? Is it a definition of the notion, or the consequence of your definition (which is?). I think that a standard point of view would be that the reduction strategy is observable *when* it can influence the order in which observable effects happen. Turning a language with input/output side-effects from call-by-value to call-by-need evaluation will change the order in which the user can observe these effects, so it is an observable change. But if you have a language without side-effects (such as Coq), how would the reduction order be observable? Besides, the idea that languages that do not *fix* a reduction strategy are more pure is not convincing. Again, if your language has input/output side-effect, not fixing the evaluation order does not prevent the effects from being observable (they may occur at different times depending on how the program is run) -- you can of course also have denotational semantics for languages with input/output side effects. &gt; And by moving the operational semantics out of the language to the compiler, you give the compiler great power and flexibility in the choices it can make for optimization. It is true that GHC was the first "truly practical" compiler for a lazy language, and that is a monumental achievement. But is it related to the question of whether an evaluation strategy is defined? Compilers take liberties with a language evaluation strategy all the time (for example: inlining a function corresponds to reducing/evaluating the function call, and this transformation can be made even within function bodies that are not to be evaluated yet or may never be evaluated), as long as the differences introduced are not observable to the user. GHC certainly has more opportunity due to the absence of side-effects with implicit evaluation order -- strictification is the key optimization for practicality and demands a good effect analysis -- but this could be described as a quantitative change arising from the choice of purity (defined independently of the reduction strategy) instead of a qualitative change?
Lennart is now working at [X](https://en.wikipedia.org/wiki/X_(company\)).
Cool :) It will automatically install your extension for them, as if they installed it alone.
Hi, did you tried Haskero ? It provides type info on mouse hover. (https://marketplace.visualstudio.com/items?itemName=Vans.haskero)
And I think that's a good sign. My best guess is this lines up with companies picking to Haskell in order to lure talent.
[For those who want the animation only...](https://zgab33vy595fw5zq-zippykid.netdna-ssl.com/wp-content/uploads/2017/04/tag_animation.gif)
Not really surprising. There are far more hobbiest Haskellers than commercial practitioners. I write Haskell for a living and frankly I rarely use stack overflow, but then again I'm not a beginner. However, I think strong types and tools like hoogle obviate some of the need for a google the solution workflow.
Wow, amazing!
Make sure you use canonical links when linking to a file/directory on GitHub. On GitHub, you can press the "y" key to update the URL to a permalink to the exact version of the file/directory you see -- [source](https://help.github.com/articles/getting-permanent-links-to-files/). I've tried to fix your links: Relative | Canonical -|- https://github.com/fiatjaf/module-linker/tree/backends/data/hackage-modules | https://github.com/fiatjaf/module-linker/tree/6efb6823d8a449d0152dfa34acfd85db9514567b/data/hackage-modules Shoot me a PM if you think I'm doing something wrong.
Very interesting project! How do you simplify the categorical term?
Yes, the most general formulation of functors leads to some potential ambiguity. You can go even further and make the mapping on objects a type family instead of a type constructor, and forget about type inference entirely, but you end up with something too cumbersome to use. Some functors involving `[]`: type Hask = (-&gt;) data Equal a b where Refl :: Equal a a -- A: type constructor instance Functor [] where type Dom [] = Equal type Cod [] = Equal fmap Refl = Refl -- B: Hask endofunctor instance Functor [] where type Dom [] = Hask type Cod [] = Hask fmap f = map f -- C: free monoid instance Functor [] where type Dom [] = Hask type Cod [] = MonoidMorphism fmap f = MM (map f) -- D: monad instance Functor [] where type Dom [] = Kleisli [] type Cod [] = Hask fmap (Kleisli f) = (&gt;&gt;= f) C and D are parts of adjunctions, both of which can be used to derive B as a composition. There's also a functor from `Equal` to `Hask`, but that's a composition of A and the inclusion functor from `Equal` to `Hask` that you can only define as an isomorphism in Hask.
\#TheHaskelling
&gt; If we instead make indexed profunctor optics with a hard coded profunctor like your EIndexed or IIndexed, then they don't magically downgrade, and they have to be composed by some other combinator than (.), and the user has to consciously choose whether they want to work with indexed stuff or normal stuff at every step, meaning we need to supply roughly twice as many optics or combinators for working with them. Yes, the solution I went with was to just define separate indexed and non-indexed traversals (e.g., `traversed` vs `itraversed`) and make the user request them explicitly. This meant that everything composed with `(.)`, even if the types didn't always have a handy synonym. type Traversal s t a b = forall p. Traverable b =&gt; p a b -&gt; p s t type IndexedTraversal s t a b = forall p. Traversable p. Indexed i p a b -&gt; p s t Under this scheme, composing a traversal with an indexed traversal in either order gives you an indexed traversal. Composing two indexed traversals gives you a double-indexed traversal, which you can convert to a single-indexed traversal if desired. At the user level, I figured `foo.bar.ibaz.quux` was easier to get right than `foo.&gt;bar.&gt;baz&lt;.quux`. Of course, I'm not a heavy lens user, so my intuition may be way off.
Since everything is happening at compile-time (another benefit I should have stressed about Conal's approach: you get the benefits of separating representation from evaluation, but without paying any runtime costs for the abstraction), the simplifications happens due to GHC's optimizer, and any additional rewrite rules you've added.
With lens you only get the last index by default when you compose with (.) -- most of the time you don't care at all about the indices. This means that everything "just works" with (.) when you just want the last preserved index or no index at all, and that you have the option to ask for more pedantic indices with (&lt;.&gt;), reindexed and the like. Given that about 9 times out of 10 the indices we supply are ignored or dropped on the floor by the user who just doesn't care about them this puts work in the right place in my experience. And moreover, when you go to use an indexed optic with one of the myriad third-party lens libraries that has never heard of indices, everything still works. The problem with the approach you mention here is that you always _have_ to deal with all your indices even just to ignore them or get rid of them, and since they are a pain in the ass you go out of your way to use the variants that explicitly don't supply the indices. This puts twice the pressure on the namespace as you have to supply indexed and non-indexed versions of just about everything rather than just having an 'indexed' optic be just a slightly more powerful optic usable in all the old situations. This gives you all sorts of perverse incentives when it comes to which to supply, as you often want to supply the less powerful thing, rather than the most powerful thing you can just because the more powerful tool is now less convenient to use. =/
I want to know that too. Currently I've scrapped the entire Hackage registry to get the list I've linked up above. There are sometimes multiple packages that give the define the module, but I'm just ignoring that fact for now. Anyway, next time you can go to `https://raw.githubusercontent.com/fiatjaf/module-linker/backends/data/hackage-modules/&lt;module name&gt;` instead of Googling it. Or you can write a command line tool that just fetches that page.
Not necessarily. Judging by my own experience i simply stopped asking haskell questions on stackoverflow once i reached certain level. Super powerful type system + hackage + emacs-mode answers most of my questions much faster than SO.
Eh. I don't think any of that is unique to Haskell. If that influenced this data, I think we'd see the same effect on other languages.
I tend to broadly use the OOP term [*dependency inversion*](https://en.wikipedia.org/wiki/Dependency_inversion_principle) for this style, as well as for the similar patterns with free/operational monads and extensible effects. It's also worth noting that you use type classes in this manner, type class resolution is doing at compilation time about 90% of what OOP folks use dependency injection containers for.
It can also be defined in terms of [`Lift`](http://hackage.haskell.org/package/transformers-0.5.2.0/docs/Control-Applicative-Lift.html) (`Lift` is [`Sum Identity`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Functor-Sum.html) with more instances) data Lift f a = Pure a | Other (f a) where it lifts the [composition](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Functor-Compose.html) of itself (`Iter f`) with `f` newtype Iter f a = Iter (Lift (Iter f `Compose` f) a) which gives us an impressive number of instances for **free** (you don't have to use `Lift` under the bonnet, just check what instances `Lift (Compose (Iter f) f)` has and then steal them for the original type `T`: think of it as a dowsing rod for functionality) {-# Language GeneralizedNewtypeDeriving, DeriveTraversable #-} import Control.Applicative.Lift import Data.Functor.Compose (Compose(..)) import Control.Applicative (Alternative) import Data.Semigroup.Foldable (Foldable1) import Data.Functor.Classes (Eq1, Ord1, Show1, Read1) newtype Iter f a = Iter (Lift (Iter f `Compose` f) a) deriving (Eq, Eq1, Ord, Ord1, Show, Show1, Read, Read1, Functor, Applicative, Foldable, Traversable, Alternative, Foldable1) ---- Here are pattern synonyms to match your original `T` example {-# Language PatternSynonyms #-} pattern A :: a -&gt; Iter f a pattern A a = Iter (Pure a) pattern B :: Iter f (f a) -&gt; Iter f a pattern B a = Iter (Other (Compose a)) 
You could also use Hoogle to do the lookups if I'm not mistaken.
Obviously, you know how people use lens more than I do, but I would be curious to know how often a particular composite optics are used in both indexed and non-indexed contexts. I personally find possibly-indexed optics confusing to use, but that could be unfamiliarity. In any case, my original design did have the ability to make possibly-indexed optics. It was three years ago, so I might design it differently now (I suspect there's a way to use a closed type family to make all profunctors indexable). type family Require (c :: Maybe (* -&gt; Constraint)) t :: Constraint type instance Require Nothing a = () type instance Require (Just c) a = c a type IndexOf p i = Require (IndexC p) i class (Profunctor p, Profunctor (Unindexed p)) =&gt; Indexable p where type IndexC p :: Maybe (* -&gt; Constraint) type IndexC p = Nothing type Unindexed p :: * -&gt; * -&gt; * type Unindexed p = p getIndex :: IndexOf p i =&gt; p a b -&gt; Unindexed p (i,a) b pickImpl3 :: ((p ~ Unindexed p, IndexC p ~ Nothing) =&gt; q (p a b) r) -&gt; ((p ~ NotIndexed (Unindexed p)) =&gt; q (p a b) r) -&gt; q (p a b) r -&gt; q (p a b) r pickImpl3 _ _ q = q pickImpl :: (Indexable p) =&gt; ((IndexC p ~ Nothing) =&gt; Unindexed p a b -&gt; r) -&gt; (p a b -&gt; r) -&gt; p a b -&gt; r pickImpl q qi = pickImpl3 q (q . getNotIndexed) qi type IndexedTraversal i s t a b = forall p. (Indexable p, IndexOf p i, Traversed (Unindexed p)) =&gt; p a b -&gt; Unindexed p s t Then you can define `(.&gt;)` and `(&lt;.)` by adding a `NotIndexed` wrapper in the appropriate place.
No where near as bad as left-pad :-)
Sorry for the late reply! I might be too close, but I don't personally see that issue at the level that you're saying that it is. I could definitely be too close to this one though, ha =). I use Haskell for work and for personal projects and have done so for multiple years, so that might make me a bit too close to get a good read on this. I can only speak for myself and what I've seen and say: I do like Haskell, but I don't see it as the be-all-and-end-all. Other things have their place, just as Haskell does. I do occasionally see people evangelizing more than I think is good, but I don't feel like I've (personally) seen something as widespread or to the extent that you are saying. Though I admittedly don't remember specific instances off the top of my head, I know that I have seen cases where Haskell has been criticized here, to some degree or another, and the thread wasn't downvoted to oblivion (and there was a discussion of the issue). I also think it's reasonable for content in /r/haskell to at least somewhat biased towards appreciating Haskell (though it should, at the same time, absolutely admit its flaws). In this particular case, I did not vote this thread down but I thought the post that was linked to in the OP was fairly unclear and I'm not totally sure what the author was getting at in some places. I didn't *strongly dislike* it or anything, but I feel like I'm missing the unifying thesis (I'm not sure I fully see how the title links directly to the last sentence and the preceding couple paragraphs in particular. For some reason, that loses me a bit on what the author is saying). This isn't to say I think Haskell is amazing at everything, shouldn't be criticized ever or is the "one true language," it is just that: I thought the post wasn't particularly clear and didn't do as good job at leveling a critique as it could have.
I also write haskell for a living and I don't think I've used stackoverflow at all ... hoggle is awesome.
I know of at least 1 non-finance non-big-4 company that specifically used Haskell to filter/lure talent.
Haskell is over 95% of the code I write at work. However, I'm based in Sydney, Australia, so at first I wondered if this post took time zones into account (it does, calculating timezone from the IP address). Then I realized that when I'm doing Haskell related Google searches, I tend to use results that point to Hackage, rather than ones that point to SO. The number of searches that land me on Hackage would average several per day whereas I would likely end up on SO less than once a week. 
Same here. I land on SO for Haskell at a rate of maybe... once a month. Compared to several times a day when I'm working on iOS.
Intero can tell you sometimes. I think the module has to be in "scope", as in, a dependency of one you've installed. (So if you have `stm-chans` then trying to import `Control.Concurrent.STM` gets Intero to tell you to install `stm`. I think.)
&gt; Super powerful type system + hackage + emacs-mode answers most of my questions much faster than SO "Faster"? *Anything* is faster than getting tricky niche-topic questions answered on SO that one's first batch of textbooks and tutorials didn't cover. It's not that it won't happen, but it's not a matter of "grab lunch, then check the first answers". By the time one has worked outself for oneself. Probably faster to ask on /r/haskellquestions or here, too.
There's a [JSON API](https://github.com/ndmitchell/hoogle/blob/c2fe81420a6988ff5e15b9e2aea7422b2fc75243/docs/API.md#json-api). And also Wow, this works really sweet! Some unavoidable false positives for clashing local modules, but otherwise this already works really well.
Study only considers US visitors.
Well that's pretty ~~useless~~ myopic :).
I use Haskell commercially, at work, every day, all day. I use it almost as often during the middle of the night as I do during the middle of the day. Draw your own conclusions...
Thanks for this. I'm excited to start using it. I had also thought of starting such an API (the existing options seemed to be unmaintained). Any ideas when the split between naqsha and naqsha-openstreetmap will happen? The split is a very good idea and I'd like to contribute to the "core" package, but it's not worth it until the refactor is complete, AFACS.
Do you think that you are smarter than us uh? Aren't you creating your terminators yet? We will defeat you
Can you give some idea of how long this survey is and what sorts of areas it will cover? This could be interesting and relevant to some of my work, but I canâ€™t share information that would be confidential to clients etc.
Awesome, great work! This + Haskero is enough for my Haskell setup. 
Thanks. I filled in the survey. :-)
Well, you can replace your step 2 above by only downloading the fixed shim, so there's no need to download GHC twice. But if you already have the download, it's virtually the same.
Hrmmm. This might be an interesting way of testing / analyzing floating point approximations to get error bounds perhaps. Super relevant for me :))
RemindMe! 1 week
I have just merged the simplified naqsha package. - An interface for angles - Latitudes, Longitudes and Geo coordinates - Naqsha.Geometry.Spherical which has the Haversine method for distance calculation. That is pretty much it. If you are interested in more features open issue on github.
&gt; For example, even on something shiny and new like a Knight's Landing Xeon Phi the branch predictor assumes that the branch destination is within the same 4gb chunk of virtual address space as the indirect jump. Unfortunately with ASLR and dynamic libraries this is more often than not not the case. Ups, ok :) Probably to save lookup table space. But it might also be a reasonable assumption for libraries. Jumps within the library are fast and slow at the boundaries if the whole library is at one virtual address which is the case (?) right now. However I would like to have something like split sections and every function at a random address messing with all the locality assumptions... &gt; As for VM vs. compiling to actual x86-64 assembly and an additional tracer there is a real trade-off there. Yeah, due to all these trade-offs, in the end there is just no one perfect runtime or gc etc. I would love to see more runtimes/compiler which could be switched based on use case, e.g., for development/deployment/based on workload. JVM gc, switching compilers like between mlton or sml/nj, or even switchable schedulers in kernels all go in this direction etc... &gt; That's pretty much what I'm doing. =) Expected and looking forward to that :D 
Oops, that symbol doesn't exist on US (or other non-scandinavian) keyboards? I didn't even realise. I'll throw that thing out headfirst then. 
Well, it doesn't mean anything particular over here as far as I know, it's just always been there. We even used that symbol when I first started learning BASIC back in the 90s, so I didn't even stop to consider that it might not be an ASCII symbol. Anything that old, that isn't a language-specific character (it isn't in any Scandinavian language anyway), couldn't possibly be non-ASCII, right? :) 
It's actually got quite an interesting history: https://en.wikipedia.org/wiki/Currency_sign_(typography) &gt; The symbol was first encoded for computers in 1972, as a replacement for the dollar sign in national variants (ISO 646) of ASCII Out of interest, do you have a dollar sign on your keyboard, and where does this key appear?
They even removed it when going from latin 1 to latin 15 :-P
Maybe send him an e-mail, but I think it's fine; otherwise you could host it. http://hayoo.fh-wedel.de/about &amp; https://github.com/hunt-framework/hayoo
IIRC tuples in Haskell are not inductively defined, so you end up having to make a bazillion instances, one for (,), one for (,,), one for (,,,) ad nauseam. Also, there's no one-element tuple in Haskell. (Not sure if this has the latter, but it would at least be possible by using a "sentinel" element at the front with no value.)
I'm surprised that you didn't include a single snippet of SQL code in your description. That leaves at least a suspicious taste in my mouth. Are you afraid to show it? ;)
They do. Unfortunately I can't reuse it, since it has the wrong kind. I'm banking on people not mixing their database queries with their generics often enough to make the overload a problem.
Latest update replaces `Â¤` with `:*:` for table definitions. Aside from the old operator being impossible to type for most people, it's enough to introduce _one_ weird product-like operator per library.
https://awwapp.com/s/3bdcec1e-cf18-486d-b2f4-c7f3830b57a9/ , this is supposed to be a very very rough idea of what I mean, so many things are missing like starting ref and other stuff (edit: got the diagram wrong)
Looks awesome. Great readme. Well done to all involved.
It's also present on the [French AZERTY](https://en.wikipedia.org/wiki/AZERTY#/media/File:KB_France.svg), the $ key has Â£ on shift and Â¤ on AltGr.
I see. Is your tuple type worth pulling out as a separate library? I'd love to use it for other things too.
It's not exactly pretty, but it's also not really worse than what I'd normally write by hand. That might say more about my SQL skills than about the quality of the code generation, however. :) In general, the code generation is fairly straightforward. * Consecutive `select`s make up a product (i.e. `select a &gt;&gt; select b` turns into `SELECT ... FROM a, b`. * Using a `restrict`, `order` or `limit` will turn the current result set into a subquery and apply the restriction to that subquery. `select a &gt;&gt; select b &gt;&gt; restrict foo` turns into `SELECT ... FROM a, b WHERE foo`. * Using `leftJoin` will join its argument (RHS) to the current result set (LHS). * Using `aggregate` will create the product of the current result set and the aggregate query. * On the top level, an outer query is always created which only reorders the selected columns based on the query computation's return value. Any columns that don't affect the result rows in any way are removed from the inner queries, to make the resulting SQL a bit nicer. There's a `compile` function that gives you the generated SQL as a string, if you're curious about what's generated for different queries.
I notice you have automatic caching of query results. I actually worked on a research project that does this with fine-grained invalidation [1]. If it's a direction you're interested in pursuing further, I'd be happy to discuss what fine-grained invalidation analysis might look like for Selda. (This may well apply to other Haskell database EDSLs, too.) [1] http://adam.chlipala.net/papers/SqlcachePOPL17/SqlcachePOPL17.pdf
I'm not sure, really. The type on its own is spectacularly uninteresting, but maybe having some more general utilities for it lying around in a library might be useful. I haven't really thought much about it.
Tanks, that does indeed sound like an interesting discussion! However, it's probably one I should put off for now. The reason I started working on this at all was that we wanted a small monadic database EDSL to use in an example for a more or less unrelated paper we're working on. I... eh... got a bit carried away though, so I really need to get back to doing non-database things around the clock for the next month, if we're to finish that paper in time for this year's Haskell Symposium. :)
The inductive tuple type is similar to [NP](http://hackage.haskell.org/package/generics-sop-0.2.4.0/docs/Generics-SOP.html#t:NP) from generics-sop. But NP has an additional parameter which wraps every element.
Sure thingâ€”just thought I'd put the offer out there. Good luck with the paper!
No convariant preoprotofunctoids included? Too bad. Seriosly, A good example of programming done well in Haskell.
&gt; doesn't force you to deal with arrows, profunctors and zygohistmorphic prepromorphisms "Monadic interface: no need to be a category theory wizard just to write a few database queries" Â¯\\\_(âŠ™ï¸¿âŠ™)_/Â¯
Sounds interesting, but unclear what exactly that means. Does it fork off worker threads ? Does main block till the last job completes ? Do you run it repeatedly and it executes jobs that have come due ?
&gt; It might be possible to use GHC generics instead, but defining tuples inductively seemed to be less annoying. By the way, the "Opaleye way" of doing this, product-profunctors, recently supports deriving using Generics: https://github.com/tomjaguarpaw/product-profunctors/blob/master/Data/Profunctor/Product/Adaptor.hs
It can create the DB for you, and seems more capable of eventually supporting migrations using the same framework. I also like the skolem type variable as a way to confine `Col` values way more than product profunctors (which feel like a very ugly contortion of category theory). Plus, monadic interfaces are largely more idiomatic, even if I wish Arrows were more applicable =P
&gt; It can create the DB for you, and seems more capable of eventually supporting migrations using the same framework. Meh. /u/valderman (or indeed anyone) could have implemented that on top of Opaleye if he wanted to. &gt; I also like the skolem type variable as a way to confine Col values Yeah that's pretty cool. &gt; product profunctors (which feel like a very ugly contortion of category theory) By no means! Product profunctors are neither ugly nor anything to do with category theory. &gt; monadic interfaces are largely more idiomatic They're certainly more widely used and if it's genuinely true that this is as monadic, safe API then that's news to me! I didn't realise that was possible. 
I'm glad this was inspired by Opaleye! I didn't see an explicit comparison between Selda and Opaleye so I'll give one briefly from what I've picked up so far. The biggest difference is that Selda's `Query` is a monad and Opaleye's is an arrow (or applicative, if you prefer). Selda is able to get away safely with a monad because `Query` has a phantom type parameter which stops columns leaking into aggregated subqueries. This is a clever trick and I couldn't work out how to do this when I designed Opaleye. The key is `Inner` whereas I tried an ST style approach which doesn't seem to work. I'm not mathematically convinced that it's safe but nonetheless I can't see any reason it shouldn't be. The other difference, for the moment, is that all columns are stored in "inductive tuples". This won't last though. It's just far too difficult to program like that. Some comments 1. As mentioned above, eventually you're going to want to support general product types rather than just "inductive tuples". It's too painful to program with inductive tuples alone. Once you support general product types you're going to have to have a bunch of boilerplate-generating code, either TH or Generics, to generate `Columns`, `Res`, `Result`, `Insert`, `Aggregates` .... 2. `Table` only has one type parameter whereas Opaleye's has two. This is a slight reduction in complexity but it means that inserting and updating to default values must be done by `def` in a type unsafe way (as acknowledged in the docs). Such type unsafety is unacceptable to me, but it it's acceptable to you and your users so be it. 3. It seems like `required` and `optional` should be called `nullable` and `nonNullable` instead (at least if you don't want to clash with Opaleye terminology). They don't have anything to do with required or optional (DEFAULTable) columns. 4. As written, `order` seems type unsafe. It's possible for columns from an outer query to leak inside it. 5. Opaleye's `Query a` has the denotational semantics of `[a]` (roughly). If you give `limit` and `order` types like `... -&gt; Query s ()` then you can't give your `Query` a similar simple denotational semantics. It would have to be something much more complicated, which would be sad.
As far as I'm aware, there should not be any safety issues. Columns' scopes are kept in check by shamelessly ripping off the ST monad trick, so hopefully any problems with generating invalid SQL would be a matter of me messing up somewhere rather than intrinsic to the programming model. If you've spotted, or even suspect, any, I'd be happy to hear about them, to take a closer look.
In fact there's something quite dodgy about the semantics of `order` at all. What do you expect the following to do? do do foo &lt;- select fooT order foo Asc do bar &lt;- select barT order bar Asc versus do foo &lt;- select fooT order foo Asc bar &lt;- select barT order bar Asc According to the monad laws these should be the same but the behaviour of at least one of them is going to surprise some people!
FWIW, this has existed in [beam](https://github.com/tathougies/beam) for quite a while (first post on/r/haskell was 2 years ago). Beam already supports backend-specific extensions, advanced SQL features (window functions, etc), and will soon support migrations. Newest version is in the [beam-0500 branch](https://github.com/tathougies/beam/tree/travis/beam-0500), and documentation is [here](http://tathougies.github.io/beam/) (still WIP). Additionally, beam doesn't suffer from many of the issues that /u/tomejaguar already identified, like those with order by and limit.
Oh, and a suggestion: don't pay attention to people who care too much about what the SQL looks like. Getting your API right is going to be *far* harder than making the SQL generation nice.
Elaborate?
You forgot one important point: Opaleye is *much* more mature. Selda is currently approximately two weeks of hacking, so the interface may be a bit wonky, as you and others have pointed out. By the way, there is nothing inherently type unsafe in having a single type for tables. It's perfectly possible to derive a table's write type from its read type. However, I wasn't entirely sure about the flexibility vs. safety tradeoff there, and in the end I erred on the side of lower complexity. It's also quite possible to make `def` perfectly safe by giving all types a default value to fall back to. Not the prettiest solution though. Order may be a bit wonky, but I'm not sure I see how it would enable columns to leak in from outer queries. Could you give an example?
My apologies! I actually have seen Beam but I completely forgot about it. And I am excited about it as well. Now I need to sit down and compare them.
My apologies (again)! In the rush of excitement I forgot all the contenders. To be quite honest the only thing keeping me from Opaleye is fear. I get scared of all the arrows and I haven't quite had the time to figure them out yet. Selda looks more approachable which is why it caught my attention so easily. After reading your critique I can see there is good reason to take Opaleye seriously.
They are also both Australians.
No problem! Love getting new users. The `beam-0500` branch is where it's at. Feel free to join us on our mailing list or IRC as well :)
Cool!
Hahah...dude you have crazy high negative karma. It's almost like there are bots that just downvote you for the heck of it.
Like what if I do do a &lt;- do r &lt;- select tbl1 limit 0 10 pure r b &lt;- do r &lt;- select tbl2 limit 0 10 pure r return (a, b) I would expect, based on the way the query is written and the intuition that each `do` corresponds with a full-fledged query, that this would take the first ten rows of tbl1 and join them with the first ten rows of tbl2, making for a max of 100 rows in the result. However, I think since this is equivalent to (by the monad laws) do a &lt;- select tbl1 limit 0 10 b &lt;- select tbl2 limit 0 10 pure (a, b) The limits would combine, so the result would be 10 rows at the most. In beam, I get around this by only allowing joins monadically. Limits are expressed using the `limit_` function. For example, in the [documentation](http://tathougies.github.io/beam/user-guide/queries/relationships/) (the output code is auto-generated from the query) we have do i &lt;- limit_ 10 $ all_ (invoice chinookDb) line &lt;- invoiceLines i pure (i, line) Here, the limit_ generates a subquery (citing the SQLite syntax in particular). SELECT "t0"."res0" AS "res0", "t0"."res1" AS "res1", "t0"."res2" AS "res2", "t0"."res3" AS "res3", "t0"."res4" AS "res4", "t0"."res5" AS "res5", "t0"."res6" AS "res6", "t0"."res7" AS "res7", "t0"."res8" AS "res8", "t1"."InvoiceLineId" AS "res9", "t1"."InvoiceId" AS "res10", "t1"."TrackId" AS "res11", "t1"."UnitPrice" AS "res12", "t1"."Quantity" AS "res13" FROM (SELECT "t0"."InvoiceId" AS "res0", "t0"."CustomerId" AS "res1", "t0"."InvoiceDate" AS "res2", "t0"."BillingAddress" AS "res3", "t0"."BillingCity" AS "res4", "t0"."BillingState" AS "res5", "t0"."BillingCountry" AS "res6", "t0"."BillingPostalCode" AS "res7", "t0"."Total" AS "res8" FROM "Invoice" AS "t0" LIMIT 10) AS "t0" INNER JOIN "InvoiceLine" AS "t1" ON ("t1"."InvoiceId")=("t0"."res0") 
&gt; any profunctor (D-op x C -&gt; Hask) gives rise to a Functor from Hask to Hask How can `D-op x C -&gt; Hask` give rise to a Functor `Hask -&gt; Hask` for general `D` and `C`? Do you mean "any profunctor `Hask-op x Hask -&gt; Hask`"? Here's the class definition for `ProductProfunctor`. What's contrived about it? It looks almost identical to the class definition of `Applicative`. class Profunctor p =&gt; ProductProfunctor p where purePP :: b -&gt; p a b (****) :: p a (b -&gt; c) -&gt; p a b -&gt; p a c &gt; I think product profunctors have the same problem, but worse, with an even more contrived theoretical definition. Product profunctors are naturally a superclass of arrows, so they can't be *more* contrived! 
&gt; In general, the code generation is fairly straightforward. I made myself familiar with your code. I know that you need `State` for variable generation but I don't like the approach to build on the current state. It makes code hard to reason about. &gt; [..] `order` or `limit` will turn the current result set into a subquery and apply the restriction to that subquery. As far as I know, only the outer query is allowed to have those clauses. And don't get me started on the different types of column expressions that can appear at different places. ;) I'm more interested in DSELs that use SQL as a code backend instead of modelling it 1:1 (althought that probably has its right to exist).
&gt; That's where the "not pretty" part comes in. I'm thinking a hierarchy of defaults: if there's a default in the schema, use that. Otherwise, fall back to the per-type default. Not entirely sure it's a good idea, but it's an idea at least. Eh, that's not too terrible an idea actually. Seems pretty reasonable.
Indeed, the current code generation approach is... less than optimal. There's probably a rewrite of the internals on the horizon (as always :). Where order and limit are acceptable seems to be fairly implementation-specific (as everything else, it seems... can SQL even be called a single language?). Both SQLite and PostgreSQL allow them in inner queries, but I know that at least some versions of MySQL don't.
Oh, and if I didn't get it across properly, I didn't mean the first paragraph in a _"my code should get a free pass"_ way, but in a _"any comparison should note that Selda is currently nowhere near as mature, complete and reliable as Opaleye"_ way. That is, Opaleye is far less likely to set your house and data on fire than Selda is at this point. :)
So far these have mostly looked very much like a lightly filtered version of /r/haskell, sorted by Top / past week. Am I wrong about that? Are there news / articles from other sources?
It's probably also worth noting that profunctors are remarkably simple, much easier to learn than monads for sure.
are there more "principal" alternatives to an abstraction between the analyzability of applicative and the expressivity of a monad? (i'm reading through this post elvishjerricco.github.io/2017/03/10/profunctors-arrows-and-static-analysis.html (yours, I guess), and it seems to be reformalizing Arrow.)
At this point, I just downvote everything metafunctor posts. When over half of someone's posts are trolling, I stop trying to distinguish when they're acting in good faith.
Does `Identity` count as a one item tuple?
&gt; I don't see any reason to believe that a DSL that's completely evaluated before sending the query shouldn't be able to be monadic. https://www.reddit.com/r/haskell/comments/66ih1l/selda_a_monadic_database_edsl/dgjb39z/ This seems like a good example where either intuition or the Monad laws are broken.
Thanks. By "run repeatedly" I meant something that worked like https://en.wikipedia.org/wiki/Anacron. 
[removed]
There is a golden path for Haskell users who are on linux/bsd based OSes though, which is worth recognising. WSL is getting closer but a bunch of libraries still have large issues 
On linux it is very streamlined nowadays. Install stack, install emacs, install intero. And that's it. It all works out of the box. I suggest ubuntu as the most hassle free environment. Your problem is not haskell. Your problem is windows. 
Should have used all the pragmas for extra confusion.
Honestly your characterization is frankly almost entirely uninformed, AFAICS. First off, GHC works perfectly fine on Windows; it has always been a priority for GHC developers, but there were never many people who could solely pick up the slack. It has improved dramatically thanks to Tamar. Stack works just fine, too, JSYK. And I see no reason why Intero wouldn't work either, if it does not already I'll admit there are gaps in the ecosystem as far as Windows are concerned; but "I just install stack, see, it's so easy" is an extremely weak argument, considering I can do it too. Second, your suggestion of 'hassle free' environment is simply false, or at the very least, very misleading and narrow. Only very recently, less than 6 months ago, the release of Ubuntu 16.10 introduced new anti-exploit hardening flags in their GCC build, which *also* caused issues for GHC. This required respinning binaries, just like it did for Windows this past week, so the bugs were fixed and users could carry on. It was literally no different than this exact event. (In fact, the OP's event is even *weirder* because for the first time in 10,000,000 years, it's Microsoft causing us problems, not Apple or 20 random Linux distributions.) So, you know. It's only hassle free, *until it totally isn't*, because reasons. Unless you also want to also tell me "Ubuntu is my problem", but I get the feeling you don't want to directly say that. (And even if you did, your goalpost is suddenly moving...) Yes, it sucks with the timing and everything. Windows has some downsides and is outnumbered. But it is not the OP's fault they chose the week some shit broke to try something, just terrible timing. And it is absolutely *not* their fault for using Windows or whatever, or simply *wanting* to use Haskell on windows. If you're going to argue that, you should at least have a more convincing argument, frankly.
[@mallai's latest tweet](http://i.imgur.com/6qd8F2H.jpg) [@mallai on Twitter](https://twitter.com/mallai) - ^i ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
For the curious, â€œthat thingâ€ is a [currency sign](https://en.wikipedia.org/wiki/Currency_sign_%28typography%29). In Emacs you can enter it by writing `\currency` in the TeX input method. (C-\ tex)
After the wall of text, my point still stands. Install ubuntu, install stack and you are good to go. NOW, not six months ago. That's all that matters really. 
It's cool to know you literally ignore all context to the argument because you're an idiot. I'll be certain simply to never deal with you again, in response.
I literally posted the link to the fix in another thread, you dumbass.
There are many inaccuracies in the talk, which can be misleading newcomers. Like "deriving classes" or "GHC generates javascript" and many more small ones. 
I wouldn't give up Linux for any other OS either, but... &gt; Your problem is not haskell. Your problem is windows. ... that doesn't really help if your users, workmates, co-contributors or students use Windows, and you want whatever it is that you're doing to work for them.
I love the lack of self-awareness in this post.
Scaffolding repo would be much appreciated! 
So this time your problem is actually that you're an "early adopter." You must be running the very latest version of Windows which came out pretty recently. WSL is also extremely young. To be honest, I'm not surprised you're hitting issues with an open source tool. It's hard for all open source projects to keep up with all the OS updates (macOS recently broke a bunch of things too). Don't give up. Use https://repl.it/ for a while if you have to. If the path you've chosen for learning Haskell has become bumpy, consider coming up with a new path. Getting started is always (always) the hardest part. Expect it to be the hardest part and know that the hill levels out after you actually know your way around a bit.
And a type system really is a macro system, but it's still a cute way of playing with expectations 
Scratch that; you will indeed get 10 elements; brain was a bit fried yesterday. So yeah, your way is definitely better. I'll be looking into fixing it when I get the time.
 Data.Maybe.mapMaybe is exactly what you are looking for! It can actually be generalized to any `MonadPlus`: filterMap :: MonadPlus m =&gt; (a -&gt; Maybe b) -&gt; m a -&gt; m b filterMap f xs = xs &gt;&gt;= maybe empty pure . f
One thing that might not be the worst thing ever is: data FilterChoice = Keep | Remove filter :: (a -&gt; FilterChoice) -&gt; [a] -&gt; [a] filter f (x : xs) = case f x of Keep -&gt; x : filter f xs Remove -&gt; filter f xs And then: keep :: Bool -&gt; FilterChoice keep True = Keep keep False = Remove remove :: Bool -&gt; FilterChoice remove = keep . not filterIn :: (a -&gt; Bool) -&gt; [a] -&gt; [a] filterIn = filter . (keep .) filterOut :: (a -&gt; Bool) -&gt; [a] -&gt; [a] filterOut = filter . (remove .) Names are obviously up for discussion, but you get the idea. Avoiding boolean blindness is often a good idea.
Looking at the posting history, that certainly seems to be true - there are versions for multiple languages that all go out at the same time. They also don't seem to be well liked, per the upvotes, and don't seem to generate much discussion (except for a few people who run hand curated weekly news services popping by to mention they have nothing to do with the automated weekly news version). Edit: now that I think about it a bit more, _maybe_ it's something like an attempt at an SEO spambot? The posts generate a link from reddit to their site, which links to content that is also linked from reddit. I'm not well-versed in SEO, so I could be way off base. 
The deriving stuff in product-profunctors currently only works with fully polymorphic products, but there's no reason that couldn't be extended to monomorphic products, which are simpler and probably more suitable for Selda.
By nix do you mean the OS or just the package manager?
See [this ticket](https://ghc.haskell.org/trac/ghc/ticket/9706).
This answer actually taught me a lot! This style of blatantly telling how it is done without much talky talky is really educative
This is awesome :) I hope his "private time Haskell" development will be allowed as it is unrelated to the work stuff ;) Not every company is a good citizens to allow to publishing anything you do while employed with them.
I love this. It's a great explanation of a bunch of fancy concepts. Lenses, prisms, monad transformers, typeclasses etc., all such elegant, safe, composable and powerful tools. It also shows how utterly unproductive it is to learn Haskell by trying to translate the basic concepts and building blocks of a more traditional language into it.
To his credit, he managed to get TemplateHaskell and arrows in...
&gt; Getting started is always (always) the hardest part. 'Hardest' is too much. Perhaps you mean 'getting started has this tricky bump?'
I dunno, I don't really develop Chrome extensions ;) I do have one more remark, it's missing some import statements, for example, this one isn't getting marked up: https://github.com/chrisdone/graphmod/blob/master/src/Main.hs#L18
Come on guys, let's keep this civil.
The exponential notation introduced on page 35 was the other way around, it is fixed now.
"Install ubuntu" might not be a feasible option for everyone. Also, lots of work went behind windows tooling; what development environment doesn't require some tinkering, especially in conjunction with latest versions of compilers and OSs?
Something like this www.informatik.uni-marburg.de/~rendel/unparse/rendel10invertible.pdf ?
Googling "reversible parsing" gave me this: https://hackage.haskell.org/package/syntax
Excellent as in funny. Poor OP will need years to unpack all of it. Lenses, arrows, monad transformers, the works. Not helpful , to put it mildly.
Thanks! I honestly did search, but not "reversible parsing" funnily enough. Looks like it might be what I want but working out how to use it is anothing thing entirely! :)
Does this mean that we don't have to make the TH calls that were creating adapters and Default instances? 
Thanks, that's an interesting approach. As someone else commented, there's `syntax`, which essentially provides both I believe, which is ideal. I think I can write the parser once (in `syntax`) and then use its `runPrinter_` to pretty print -- I therefore need to "fill in the blanks" with respect to values, much like QuickCheck's `Gen` type.
Cool! Gonna try this now, thanks
Yeah, syntax looks like exactly what you are looking for!
I don't think it's ridiculous, can be useful! For example [web-routes-boomerang](https://hackage.haskell.org/package/web-routes-boomerang-0.28.4.2/docs/Web-Routes-Boomerang.html) (which uses [boomerang](http://hackage.haskell.org/package/boomerang)) allows to have single grammar for web page routes parsing and for generating links to those routes. I think it's pretty neat (never used it though, just recalled when stumbled upon this post).
[removed]
See https://www.reddit.com/r/haskell/comments/61y1ry/bob_2017_andres_l%C3%B6h_write_one_program_get_two_or/ for a very detailed answer to that question :) 
Wrong Haskell, this subreddit is about the programming language :) 
Don't do cutting edge. Stack + Linux + Spacemacs works well. If you need to have it on windows, support people who might be doing it for you (us) : your time is money. 
Nice, yes! Thanks :) Lots of libraries mentioned there, [e.g.](https://www.reddit.com/r/haskell/comments/61y1ry/bob_2017_andres_l%C3%B6h_write_one_program_get_two_or/dfibqew/): &gt; This talk is about bidirectional parsing/printing libraries such as JsonGrammar, invertible-syntax, roundtrip, boomerang and many others. Using these, we can generate multiple programs from a common description. It's strongly related to generic programming, but (depending on your viewpoint) not quite the same. The talk discusses this relationship. 
I appreciate you're just writing up what works for you, but this looks simply insane to me. What a gigantic list of esoteric steps and caveats just to get a reliably working compiler on the world's most widely used desktop operating system. It also seems the majority of your recommendations are about changing paths and symlinks for switching GHCs, package DBs and `--allow-newer` and whatnot and could all be skipped by using stack which just takes care of sandboxes and build plans and compiler switching etc. with zero thought or complicated setup required.
They have a package, [invertible-syntax](http://hackage.haskell.org/package/invertible-syntax), though it's pretty old. [roundtrip](http://hackage.haskell.org/package/roundtrip) is based on the same core interface, more often updated, by different authors.
These people are probably using magmas to keep warm.
Holy shit you are a god. Thanks for the in-depth writeup, I'll try it out.
First, it is self contained: a page include the software and the manual. It covers the functionality for which it is made with the less possible complication. it is intuitive. I undestood everithing at the first sight. It is not over-engineered neither naive. It is done as a tool for creating programs, not as a tool for learning, self promotion or simple playing.
Oh, I know that but there's no mention of ghcjs and it's misleading to let people think they just download GHC and it does it. 
I don't believe that they are bots. A bot is made of monomorphic code that works. This is very unlikely to be made by an academic haskeller. 
The very concept is simple, though it's possible applications are not so evident. As with monads, this requires some practice.
I didn't see any making fun. Which specific comment are you referring too?
No one said anything about superiority. We are merely talking about a specific case of booting up a specific piece of software (ghc + tooling + editor). Besides OP did say he tried linux in the past but has the same problems with tooling (cabal hell, broken haskell platform etc). So the option of trying linux is definitely there. I frankly do not care what OS other people use, I was just giving an advise on quickest and easiest option to try haskell. If you want to stick to windows, be ready to handle broken things yourself. This is open source community, people only test on what they work.
&gt; Pretty printing and parsing aren't linked together in any way (there's no explicit "this is the inverse of that") This is a strong statement, no? OP asked about printing (not pretty printing in particular). Surely, there is some way in which printing and parsing are linked? Indeed writing a parser in prolog or another 'reversible' language would seem to indicate knowing one could give you a pretty complete knowledge of the other.
Indeed.
i.e. That'sNotHowItWorks.jpg. Taking [XY problems](http://xyproblem.info) just isn't the best way. Example in that link. I think it fits here: &gt; - 'nmap -O -A 127.0.0.1' returns some lines starting with 'OS:'. How to change it? &gt; - Look in the sourcecode for nmap, find how it figures out the Linux part, then rewrite your TCP/IP stack to not operate in a way nmap can detect. Except that this SO answer also contains a patch that rewrites the TCP/IP stack. Referring to such responses as 'nonsense' is even worse. So, downvote. Sorry.
There are reasons that stuff is released in phases and in clear-cut stable releases: so things get the required time to play nice to each other. Pacience is a must and not something specific to haskell, the cuts you are feeling are those of the early adopter. You are expecting too much either from windows or the haskell comunity. Neither windows can guarantee that a major platform update will not break existing software, neither can the haskell community gear up to make the transition transparently. Still, your issue was already solved by the GHC team weeks ago, and binaries for major platforms are disponible [one week ago (official announcement)](https://mail.haskell.org/pipermail/ghc-devs/2017-April/014131.html) [(or ghc-compat)](https://github.com/Mistuke/ghc-compat/tree/06b3851753cb12572706315bf32d40718bc7631b). Apparently the stack team did not updated the binaries yet, but looking at the time elapsed, it doesn't seem out of the norm. Ghc 8.2 is also soon to be released, which would leave a good oportunity for the stack team to tackle everything together. WSL, while a nice and welcomed feature comming from windows, is not mature software. This should not be expected to be a backup by no means. I use stack under a VM for over a year, never had a problem under this setup - which is to be expected since VM is a mature tool. 
I'm not sure if we are generously paid anywhere in Europe outside Switzerland.
Since you made sure to skip 'the wall of text', I'll repaste the important stuff that you missed, the one that shows you have a non-answer: &gt; Only very recently, less than 6 months ago, the release of Ubuntu 16.10 introduced new anti-exploit hardening flags in their GCC build, which also caused issues for GHC. This required respinning binaries, just like it did for Windows this past week, so the bugs were fixed and users could carry on. 
Different Chris. Confusingly this Chris is currently collaborating on a Haskell book with Julie.
His command line prompt is dope.
What Chris is next in line?
It does seem really easy, with the caveat they have in-house links. I guess they are being cautious and waiting to see if nothing weird get's reported, before pushing it to the entirity of stack's userbase.
&gt; Why is it useful for List to be a Monad instead of just Applicative? What is your intuition for the Applicative instance?
Think of it this way: bind on Maybe gives you 0 or 1 values. bind on List gives you 0 to many values. Another way of thinking about it is that the list is all the possible values of "a". (Empty list being: there are no valid values for a). Hope this helps. Actually, if you feel like doing some work, try implementing "find first element" using Applicative and Monad instances on List. It's possible with both, but importantly, the Applicative version has to traverse the entire list every time.
I was just referring to the semantics of the library (pretty doesn't deal with that "link")
I did think about that ... so this is not /u/bitemyapp?
Wow I love boomerang. I think this is exactly what I want! Thanks! :)
I am not Chris Martin, I am Chris Allen. This is me: http://stackoverflow.com/users/1205702/bitemyapp http://bitemyapp.com https://github.com/bitemyapp http://hackage.haskell.org/user/bitemyapp This is Martin: https://github.com/chris-martin http://hackage.haskell.org/user/chris_martin
I apologise. Thanks for clearing this up.
Yeah I'm not going to do that again either. Private review only. Public feedback/review was a waking nightmare.
np at all, I just appreciate you tagging me to double-check.
&gt; As recent research has revealed, Haskell is actually a dynamically-typed interpreted language. This complicates things somewhat, but not much. &gt; We consider the elephant-kinded type ...
I tend to favor the "computeLensy" approach computeLensy :: (MonadState s m, Logger m, HasComputeState s) =&gt; Input -&gt; m Output as it has the benefit of not requiring all sorts of boilerplate to switch monads throughout your code and you always ask for the bare minimum of constraints required to execute the code in question. This often opens you up to using combinators in more situations, e.g. before setup, or getting away with using parts of your type checking code in a much more limited context. That way if something needs backtracking, I can turn it on just beforehand, or propagate it out. This gives me a lot more control over the handling of my effects. George Wilson gave a pretty decent talk on this approach: https://www.youtube.com/watch?v=GZPup5Iuaqw
And it's not exactly broken - it's just slow as hell (which is incredible fo immature software), because of an implementation error from microsoft over the memory management. Did you try to use 7.10? You can use an older stackage LTS for that. It was the new block heap feature of 8.0.1 which is not playing around nicely with WSL. &gt; This is obviously an upstream problem, and it wouldn't be fair to blame this on the Haskell people. But, when you look at this as part of the larger picture, where Haskell has had diverse, persistent, and severe tooling issues for years... It's just bad optics I guess Common, be fair. This has nothing to do with tooling issues, but early adoption. You have a plentitude of options as workaround: by using GHC-compat, having a VM, having another OS (It's great having an OS in an external drive - you can take all your devtools to any pc), rolling back creators, waiting the fix be distributed by stack*, or depending on your haskell knowledge, use online tools. \* I'm on the same situation as you since I've updated my pc, , but If I have an urge to try something on haskell, I have my VM and usb drive as workarounds.
I don't tend to listen to arguments that fun or funny things in a language negate the value of the language. I once saw RSA written in C but in jive (slang) using just a few CPP macros. It was wonderful.
OCaml multicore is promising to be really nice. Compared to GHC, it doesn't intertwine with the runtime and delegates scheduler implementation to libraries which - coupled with OCaml modules - promises a great future. You can track the status at https://www.reddit.com/r/ocaml/comments/63hgid/tracking_multicore_progress/. Currently there are still discussions about implementation details like pointers and different granularity of parallelism and in what way the runtime needs to be adapted and such. GHC's design is mentioned too. Since dons and Simon Marlow are at Facebook, one can hope that dons likely desire for OCaml multicore will bear fruit in sharing of RTS expertise from Simon Marlow's GHC work. I wouldn't be surprised if Facebook implements a GPU offloading scheduler for OCaml multicore.
Could someone ELI5 the elevator pitch for why one would use this? Or some interesting problems it can solve?
I've used the type above to get complete binary trees before. e.g. With data Bin a = Bin a a T Bin a is always a complete tree. in `ad`, I have a type `Jet` data Jet f a = a :- Jet f (f a) which has the same relationship to `Cofree f a` a your type here has to `Free`. https://github.com/ekmett/ad/blob/f58f3c560f3e8f65afe64774981315535fb8455e/src/Numeric/AD/Jet.hs#L46 The number of B constructors gives you the number of layers of 'f' uniformly everywhere in the tree. I've also used this as an index into a Jet. If you have a way to "cancel" f with g, `zap :: (a -&gt; b -&gt; c) -&gt; f a -&gt; g b -&gt; c` then you can also 'zap' `Jet f a` and `T g a`.
fwiw, (&gt;&gt;&gt;) and (&amp;) are different. when you're pushing an object through some pipeline, they replace (.) and ($). 
/u/atc did you want pretty printing or enumerating? 
Neat! For this project, I also ended up using a type similar to `Jet`, except finite: data D f a b where X :: a -&gt; D f a a (:+) :: f b -&gt; D f a b -&gt; D f a (f b) so that `D f a (f (f ... (f a)))` is isomorphic to `(f (f ... (f a)), f ... (f a), ..., f a, a)` 
~~Isn't `(&gt;&gt;&gt;)` just `flip (.)`?~~ oh, `(&amp;)` isn't `flip (.)`, it's `flip ($)`. 
This looks a lot like what [one-liner](http://hackage.haskell.org/package/one-liner) and [product-profunctors](http://hackage.haskell.org/package/product-profunctors) do. The common theme is a generalized notion of traversal over heterogeneous fields, which can be quite useful for generic programming, as all these packages show. one-liner doesn't allow type-changing transformations (yet), but if it did it might be more general than flay. product-profunctors is lacking documentation about how `Default` can be used for a similar result, but there are examples throughout `opaleye`.
I think of the list monad as the "branching possibilities monad" and often people say it encodes the effect of "nondeterminism." It works just like Yogi Berra famously said: "When you come to a fork in the road, take it!"
i really like Chris's answer, I remember a few years ago this kind of questions would have an answer like the second one: "you can't do that", without a proper explanation about how to solve the OP problem.
&gt;Flay c m s t f g allows converting s to t by replacing ocurrences of f with g by applicatively applying a function (forall a. c a =&gt; f a -&gt; m (g a)) to targeted occurences of f a inside s. Ah, obviously!
afaik, Earley doesn't provide pretty printing, but if you just want enumeration, it has that.
Yeh, I rushed my answer. I still need to "fill out" values so to speak, but it'll easily serialise back to strings by just writing a parser, and that's what I'd like. Shame there's nothing as nice as that but for attoparsec.
Not... really? I mean, sure it's isomorphic to a one-item tuple, but programming-wise you really can't treat it the same way you do regular tuples. (This of course also goes back to the problem of not having an iductive definition.)
Technically the Jet type mentioned above would be the equivalent to a type level iterate. This is more of a type level rep 0 f a = a rep n f a = rep (n - 1) f (f a)
Somebody wrote a version of this for Opaleye: https://hackage.haskell.org/package/composite-opaleye-0.3.0.0
Cool stuff! Feasible to smoothly use *together-with* TypeScript rather than *instead-of*? ("Ultimately" I want to switch over JS code-bases to hs2js transpilation, but "ultimately" is a ways off and for now, TS keeps me sane..) &gt; JS users not impressed In what way are "they" not? Did they flame you for it or something?
Thanks! &gt; Feasible to smoothly use together-with TypeScript rather than instead-of? I think so, why not? But it will get ugly, I guess. Ideally TypeScript would implement that, instead. I wonder if Idris / Haskell has something like this for arbitrary types (notice QuickCheck can't auto generate functions [I think?]). &gt; In what way are "they" not? Did they flame you for it or something? Basically: *"So a shitty TypeScript...?"* 
Do you have any opinions on purescript or livescript?
Despite being listed as a benefit, I think having an extra compile step would make the library more palatable. What I mean by that is that another compile step would allow you to introduce a less-intrusive type signature (borrow ML style if you want something easier to add/remove, since it's above the function itself, or borrow from TypeScripts syntax for familiarity). You could go a step further and confine the extra syntax to a comment that a tool could parse, which would allow compilation without your tool. So basically, go from this: const maximum = F( F.Fn(F.Array(F.Uint32), F.Uint32), // function from Array&lt;Uint32&gt; to Uint32 function(array) { let max = 0; for (let i = 0; i &lt; array.length; ++i) max = Math.max(array[i], max); return max; }); To this: // :: [Uint32] -&gt; Uint32 const maximum = function(array) { let max = 0; for (let i = 0; i &lt; array.length; ++i) max = Math.max(array[i], max); return max; }; You could keep the function name in front of the "::", which would allow you to separate signatures from implementation. I went with the above because it lines up rather nicely and reduces a touch of repetition. 
Your post is misformatted, but I see it. I think you're right, that'd be great. I guess I'll stop at this point, though. What I really, always wanted is Haskell with dependent types that compiles to non-terribly-slow JavaScript... that's my quick fix I guess. Edit: also, I changed my opinion on TypeScript and extra compilation steps since I started working on this. TypeScript's compilation is so quick and clean that it doesn't bother me at all. I like the fact you just need the `.ts` file and nothing else. I wish PureScript didn't require so much bureaucracy to compile.
Man you're quick read! I was fixing the formatting literally after I posted the original, lol.
`[Uint32]` though, but I get it, that is a very cool idea!
I meant to abbreviate the Linux family as *nix but you're right it's not clear enough
I thought that's what you meant by &gt; The other difference, for the moment, is that all columns are stored in "inductive tuples". This won't last though. It's just far too difficult to program like that. 
You mean for actual JS? Including null, date, classes, duck typing &amp; all it got? How would that analyzer work?
Union types, intersection types, and subtyping. Simple enough stuff, but up until recently they hadn't been compatible with fully automated type inference.
 data Any a where Any :: a s -&gt; Any a
For example, imagine using it from TH. You'll definitely need a special case there, at the very least. (Obviously I haven't though too much about this, so I might very well be wrong in general, but the TH case is definitely a thing. No idea if it has all the instances and/or is *missing* the wrong instances which don't exist for normal tuples, etc.)
Brilliant and mind-blowing stuff. Excellent presentation!
Sorry but I don't really know what that means.
This looks like a combination of the [existential typeclass](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/) and [scrap your typeclasses](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) patterns.
&gt; So unless all elements map to the empty list we are creating a larger space than the input, not a smaller one. I mean that's not really true is it. As long as there are more elements that map to the empty list than there are new elements you are fine, also sometimes its fine if the search space increases over time, as laziness means you won't pay for any of the part you don't use. But for an example: [1, 2, 3, 4, 5, 6] &gt;&gt;= \x -&gt; if x &lt; 4 then [] else if x &lt; 6 then [x] else [x, x] gives us: [4, 5, 6, 6] Even though many of the elements didn't go to zero.
The function passed to a catamorphism is called an f-algebra.
Totally confirmed: http://hackage.haskell.org/package/recursion-schemes-5.0.1/docs/src/Data.Functor.Foldable.html#Recursive
"Types are their own documentation"
Oh right, no I meant it's better to allow users to store columns in whatever Haskell type they like.
Is `foldl` a catamorphism, though? I'm really confused here. It also maps initial algebras to some other algebras, but catamorphisms are supposed to be unique, right? And `f` passed to `foldr` is not an F-algebra in itself (`maybe m f` would be, I guess).
Just remember: common standard fog
Not all Idris/Agda/Coq proofs are erasible because all programs are proofs there, and usually something is left to compute with. There are various explicit and implicit methods to erase things. In Coq, there is a universe of erasable types (`Prop`), in which we put the things we want erased. Agda has (rarely used) irrelevance annotations. Aside from that, types themselves are erasable in GHC/Agda/Coq/Idris, since they are abstract (no typecase etc.). There are several ways to erase even more. Some [indices](https://eb.host.cs.st-andrews.ac.uk/writings/types2003.pdf) are erasable from inductive type definitions. Simple-minded dead code elimination can remove lots of proofs, but it is more widely applicable in total languages, where changing strictness behavior is not a big deal and case splits are erasable without violating type soundness. What's certainly a "compile-time proof" in GHC is anything that involves type and kind checking, but no pattern matching on GADTs. In the talk, the `Wf` and `Find` classes are also given as examples of compile-time proofs, and I believe in this case it's correct, but in general instance magics may well have runtime costs which are unavoidable, i. e. dictionaries not eliminable by any specialization, and GHC sometimes doesn't eliminate dictionaries even if it's possible in principle.
I'm using it too. I think the oauth integrations may be best described by providers themselves. Apart from that I was happy with it cause it worked outh of the box z (with small fixes for fb) for Google and Facebook atleast.
Keep in mind that when you do this, there is no going back: As soon as you only have `[Some Widget]` the stuff you can do on them is very limited. In particular, although this corresponds to `List&lt;Widget&gt;` in an OO language like Java, you can't match on the runtime type! Java permits you to do so by means of `instanceof` or `ClassCastException`, but Haskell erases the concrete `s`, so that you could never recover itÂ¹. This means you have to encode all operations as part of `Widget`, which sort of acts like your type class dictionary. Also, operations which take multiple different `Some Widget`s are pretty poor. For an alternative design, you might look at [`brick`s](https://hackage.haskell.org/package/brick-0.17.2) [`Widget`](https://hackage.haskell.org/package/brick-0.17.2/docs/src/Brick-Types.html#Widget) type and its constructing function in `Brick.Widgets`. Â¹ You can with a `Typeable` constraint, though.
'Haskell University' sounds too good to be true :o
There is a difference between not supporting a popular platform because of the lack of resources, and not supporting it because of you don't care about that subset of users at all (or even worse, you are actively hostile to that platform). Unfortunately I find the second case is much more common than I would like, and it makes me rather angry; hence the above (maybe too strongly worded) comment. On your issue: There isn't enough information in your post to reliably guess what you need in your library, nor am I an expert in Windows internals, but my guess would be: 1. [VirtualLock](https://msdn.microsoft.com/en-us/library/windows/desktop/aa366895\(v=vs.85\).aspx) 2. [CryptGenRandom](https://msdn.microsoft.com/en-us/library/windows/desktop/aa366895\(v=vs.85\).aspx) The second is probably not exactly what you want, but according to [this stackexchange question](https://crypto.stackexchange.com/questions/39566/where-do-windows-applications-get-entropy-from), it's probably close to the best you can do on Windows. There is also the [entropy package](https://hackage.haskell.org/package/entropy), the source code of which can possibly help you with how to get entropy on Windows. Btw, if your library is popular enough, I believe you actually *should* make a reasonable effort to support it on at least the three most popular platforms (as opposed to just waiting for patches).
Fantastic talk as always, the only part that made me wary was adding a `Wf` constraint on the constructors data R (s :: SM) where ... Rseq :: (Wf s1, Wf s2) =&gt; R s1 -&gt; R s2 -&gt; R (Merge s1 s2) Ralt :: (Wf s1, Wf s2) =&gt; R s1 -&gt; R s2 -&gt; R (Alt s1 s2) Rstar :: Wf s =&gt; R s -&gt; R (Repeat s) In the talk `Wf` is class (Repeat (Repeat s) ~ Repeat s) =&gt; Wf (s :: SM) but on [github](https://github.com/sweirich/dth/blob/0f14cecb070032d44040304a6758fcd87c6ef812/popl17/src/RegexpDependent.hs#L371) it is a lot more complicated class (m ~ Alt m m, Repeat m ~ Repeat (Repeat m), Merge m (Repeat m) ~ Repeat m, SingI m) =&gt; Wf (m :: SM) I don't like cluttering the constructors, can it be proven externally and added via pattern synonym (constructor + additional proofs)? There could even be a hierarchy of proofs depending on how precise you want to be (similar to end of [*Trees That Grow*](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/trees-that-grow-2.pdf))
&gt; Hence the links I gave as my best guess on how to solve the concrete issues you listed...... Of course I have seen those links, Google exists in this part of the world as well. You want a developer like me to code up those ffi calls _without a working windows machine_ and then maybe also test it by getting a windows by paying Rupees out of my pocket. All this for the code that any way I give it out for free. On the other hand a windows user cannot be bothered to download a *free linux distro* and run whatever he/she wants on a vm. If this is your definition of reasonable, maybe I need to brush up my English; not a native speaker you see. If one of the ardent windows users, instead of bull shitting about the arrogance of the developer, contribute by sending the necessary pull request, life would be good and peaceful to all of us. Look at how different Open source systems manage it. They form community and maintain the software they want.
This is slightly off topic, but I figure now is as good a time as any to say it - I really like pretty much all of your posts in this subreddit. I haven't commented on them before because they've either been down different rabbit holes than the ones that I've been diving down, or they've been so far ahead of me that I haven't had anything to add to the conversation. You've made a few comments in the past that have hinted that you think that you're posts aren't welcome or appreciated here, but I don't think that's true. I think you've just been blazing trails or specialized enough that most of the folks who are following what you are doing don't have much to add. It looks like you're doing some great work. If I get through my various queues of projects and can catch the enough to give more specific feedback as you work on things, I will. Until then, I'll just use your questing as generalised inspiration :) 
It only looks this way when you look at just the ordinary programming tasks that optics can accomplish. You also need to look at all the things that you can accomplish with optics that you cannot accomplish with traditional programming tools. Missing form your list of features is matching variants, zipping, and mapping. More importantly the ability to compose all things you mention in order to access any field / variant / sequence component within an arbitrary data type in a uniform, composable, first-order way that lets you maintain data abstraction. Can you pass field names as parameters to functions in your other programming languages? Sometimes but usually not. Can you pass compositions of field names as parameters to function in your other programming language? No. Can you decompose a single data type into sets of fields in multiple different ways in your other programming languages? No, you can only access fields of a record in accordance to the components that were used to define the type. Can you provide field names to abstract data types and use them just like other field names in other languages? No, when you use an abstract data type you cannot use the standard record access primitives to access the components. Can you create abstract pattern matching for variants in other languages? No, most of these languages don't even have variants, and when they do, patterns cannot be used with abstract data types. I can keep going. If you have never used any of these novel features that optics provides before because they are not available in other languages then you cannot know what you are missing.
Note: your example here needs nothing more powerful than Functor.
The combination of `f` and `z` passed to foldr together forms an f-algebra. I stole a convention from dons and others in my own code that I tend to name the function `go` or `step`, if it was actually a `cata` style catamorphism where `(a -&gt; r -&gt; r)` and `r` fuse together into `(Maybe (a,r) -&gt; r)`, I'd probably call the combination `phi`, and the argument to `unfoldr` `psi` to fit with the recursion schemes terminology.
It is not, you have it right.
I once made a similar C++ toys like ```c++ // q :: (Eq int) =&gt; int -&gt; char -&gt; int // q int x, char y x + y // q int x x template &lt;typename T&gt; auto q_tem = F |= R&lt;lib::Ord&lt;T&gt;&gt; &gt;&gt;= Ty&lt;T&gt; &gt;= Ty&lt;char&gt; &gt;= Ty&lt;int&gt;; auto q_def = q_tem&lt;int&gt;; auto q_fun = Switch&lt;int, char, int&gt;{} | Case&lt;S&lt;int, char, int&gt;, int, char&gt;{}.assign([](int a, char b) -&gt; int { return a + b; }) | Case&lt;S&lt;int, char, int&gt;, int&gt;{}.assign([](int a){ return make_func(&amp;test2); }); ``` and uses it like ```c++ q_def = q_fun; std::cout &lt;&lt; q_def(q_fun, 1)('2'); ``` I found it annoying and chose to make it just a toy...
You can see C++ can't have free operator-&gt; and operator=&gt; JS has more fp I think
Asking "why is X a monad?" is kind of like asking "why is gravity a force?" The answer is a very boring "because gravity has the properties that defines a force!" or "because lists have the properties that defines a monad!" Any type for which you can satisfy the monad laws are a monad. There doesn't have to be a special reason for it to exist. Whether or not something is a monad depends purely on whether or not it satisfies the monad laws. Usefulness is not a factor. ---- Now, "when is the list monad useful?" is a much more interesting question. As one example, it can be used to model non-determinism, i.e. when a computation doesn't just give a single value, but multiple values, but you want to treat it like a single value. For example, you can't compute a single answer to the question "Who lives in England?" -- but you may still want to compute the age of the result of "who lives in England". The list monad lets you pretend you have a variable that contains who lives in England, and compute their age as if it was a single person. 
Yep, [Hayoo](http://hayoo.fh-wedel.de).
Julie's long awaited [monoid/semigroup talk](https://www.youtube.com/watch?list=PL5lgjzYOvyYNchlkMzvDqd1F6gS-COCDo&amp;v=-mnA8_DWfik) :) `minimum` (as discussed) doesn't quite belong in [`Foldable`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Foldable.html#t:Foldable) but rather [`Foldable1`](https://hackage.haskell.org/package/semigroupoids-5.1/docs/Data-Semigroup-Foldable.html#t:Foldable1) (subclass of `Foldable` for non-empty structures): -- Data.Foldable class Foldable (t :: Type -&gt; Type) where fold :: Monoid m =&gt; t m -&gt; m foldMap :: Monoid m =&gt; (a -&gt; m) -&gt; (t a -&gt; m) -- Data.Semigroup.Foldable class Foldable t =&gt; Foldable1 (t :: Type -&gt; Type) where fold1 :: Semigroup m =&gt; t m -&gt; m foldMap1 :: Semigroup m =&gt; (a -&gt; m) -&gt; (t a -&gt; m) Using `foldMap1` we can define a `minimum1` that cannot be applied to empty structures {-# Language InstanceSigs #-} newtype Min a = Min { getMin :: a } deriving (Eq, Ord) instance Ord a =&gt; Semigroup (Min a) where (&lt;&gt;) :: Min a -&gt; Min a -&gt; Min a (&lt;&gt;) = min minimum1 :: (Foldable1 f, Ord a) =&gt; f a -&gt; a minimum1 = getMin . foldMap1 Min I recently made a proposal to [add `Foldable1` to *base*](https://ghc.haskell.org/trac/ghc/ticket/13573) which paves for eventually moving away from partial `Foldable` functions like `maximum`, `foldr1` and `foldl1`. We also get generic (total) versions of `head` and `last`.
What does anyone get by spamming random news articles to groups they've never looked at? That's all this user has in its post history.
You will need to submit a proposal document, but we'll also need to ask some questions
Thanks. Is there a specific format for the document or will it be released when the proposal window opens?
should be that type WithConstraint c = Some (Compose Dict c) so Some is more general
It could also mean a type level function `f :: * -&gt; *`.
Hmm it looks like the `brick` `Widget` is basically using a string map to an "`Attr`" type to hold its internal state? I don't see that as an improvement honestly, map lookups are never guaranteed. It's true you lose the specific `s` state type information in `Some Widget`, but typically doing so in the first place means you don't care about that. If you do need to know if a `Some Widget` is some specific type I've found it useful to add an additional `_type :: WidgetType` record field to `Widget`, where `WidgetType` is a plain ADT that functions as an enum (e.g. `CounterWidgetType`)
Note: There are way more than 3 talks. I currently have access to about half of them and am working on trimming and uploading. Expect to see more on the list soon! [edit] There are 10 recordings up there now. Expect about 5 more in the next week or so.
&gt; *cancels plans this weekend*
You could supplement your enum approach with a GADT: data WidgetType s where CheckBox :: WidgetType Bool Input :: WidgetType Text data Widget s = Widget ... s (WidgetType s) ... This would be a way to recover type information, should you ever need that. 
Ah, finally! :D
Yes that is why I don't quite get what he is talking about, I guess a subset of JS?
The problem now (in my opinion) is that you're using a serif font, and quite a hard to read one at that. Also, on a wide screen monitor, the article is a bit too wide to comfortably read.
Not a stupid question. Deep actually. 
(It is)
I tend to name it like anything else, according to its purpose. If Iâ€™m stepping through a state machine, then things like `step`, `advance`, `increment`. If Iâ€™m inserting stuff into a data structure, then things like `insert`, `push`, `add`, `enqueue`, `enter`. Often itâ€™s an application-specific thing, like `define`, `resolve`, `compose`, and so on. Only rarely do I resort to useless names like `go` or `f` unless the code is very generic and there isnâ€™t anything more suitable.
Basically, you need to convince GHC to rebuild the wired-in packages; these are built in much the same way other packages are built (last time I checked, you could even rebuild base using cabal-install, though it's considerably more difficult to convince Cabal to use the resulting library for anything else.)
&gt; I wish PureScript didn't require so much bureaucracy to compile. Really? Have you tried psc-package or Purify? Either way, you'll only need the compiler executable and one other Haskell executable. Or you could just use the compiler on its own and just pass a glob of input files.
What are those rabbit holes?
There are a few main areas I tend to bounce between: * modular languages and associated tooling * various applications of event-and-behavior functional reactive programming * session types * improvements in property-based testing * various combinations of the above The release of Hedgehog was pretty exciting for me, so I'm tinkering with some additions / modifications to that. I'm pretty keen to use something like session types to make doing the things in the monadic QuickCheck paper a bit easier. At present it operates via a Free monad like interface, and so you need to be careful while generating actions for testing observational equality. I think session types could make that easier, and you might be able to use that for the testing and then 'degrade' the session client to a Free like interface for ease of use. Although I'd also love to play around with options to help make session types easier to use in general :) I was kind of stuck on some type level stuff with my modular languages work, but I've resolved that now so I should probably merge the things in the new repo into the old repo. At the moment with the new repo you can list language features (STLC + Int + Bool) and get an evaluator based on the small-step semantics and a type checker. The old repo gives you a parser, pretty printer, QuickCheck instances, and can spit out a repl and a test suite for common PLT theorems. I'll probably try to get HML inference / possibly typeclasses / possibly some compilation going before I merge the repos and starting blogging about that again. Then I'll probably make it build a debugger (with the option to step through the semantics / typing rules / unification steps at various levels of details) and a generic pandoc-based haddock-like tool, lots of stuff to play with there. I'm keen to write an FRP parser, using a lot of ideas from Trifecta, and working with an interface where changes come in via diffs (so you can handle diffs or edits). I _think_ you can chain that along to a type-checker (and possibly compiler) that can incrementally recompute results with minimal effort as people type / patch the document. I've also been doing a heap of other different FRP work with reactive-banana and reflex. The Manning book about the sodium library helped open my eyes a bit there, but I'm trying to apply this stuff in a pile of different contexts and probablems to try to synthesize some best practices / API design guidance around these things. So far I've written reflex SDL2 bindings, some (partly limited) code that generates and FRP server backend in reflex or reactive-banana from a servant type (which servant-reflex can use from the front-end), and more chat servers (both web / socket / both based) than I'd like to think about. I've also got a pile of draft blog posts that I need to tidy up and some more talks to put together there. Eventually I'd like to try porting some of the Netflix OSS libraries to Haskell + FRP, because I think there are probably some wins there. I'm also wondering if there are wins with putting together something like servant but for session types. I think that could be pretty handy for doing things with reflex and websockets, for both the front and back end. There's probably other stuff as well, but those are the main burrows :)
They aren't the same. For example, foo :: IO (Maybe a) foo = putStrLn "Hello" &gt;&gt; return Nothing This does not return an `a`, but you have to execute a side-effect to determine that.
This is currently being worked on here: https://github.com/conal/concat/issues/3
Neat idea. Maybe it should be called "theredocs".
Most of the time when you're writing a recursive 'go' its because you don't trust fold in that situation to produce efficient enough code.
Good point, but I just want to emphasize the extremely important point since it might not be obvious for a beginner: With `Maybe (IO a)` you know whether you have `Nothing` or `Just (IO something)` before you do anything impure. But with `IO (Maybe a)` you know nothing until the missiles are thrown by `IO`.
No, those can't be erased. On the other hand, I believe the way she uses class constraints rather than separate proof terms ensures that the structures of the proofs follow the structures of the code, so there shouldn't be any asymptotically bad time surprises. I'm not sure if there could be bad space surprises.
`Maybe (IO a)` tells you you _might_ have an `IO` action which returns an `a` when run. *If* you do have an `IO a` (i.e., you have `Just &lt;some action&gt;`), _then_ you can run it, which might have side effects when producing the `a`. `IO (Maybe a)` tells you **have** an IO action which _might_ return an `a` when run (and possibly perform other side effects, such as printing, reading files, firing missiles). As such, they're really completely different, only one is guaranteed to actually be able to perform some IO.
In addition to what everyone else is saying, which is completely correct, there *is* a type that behaves equivalently to `IO (Maybe a)` and that works a bit like how you might have expected your second type to work. It's called [`MaybeT IO a`](https://hackage.haskell.org/package/transformers-0.5.4.0/docs/Control-Monad-Trans-Maybe.html), and it lets you represent an action that 1) might do some IO, and 2) might not return a valid result. Under the hood, this is basically a wrapper over `IO (Maybe a)` (*NOT* a wrapper over `Maybe (IO a)`), but it has the advantage of having a `&gt;&gt;=` implementation that handles the IO-chaining behavior of `&gt;&gt;=` and the maybe-chaining behavior of `&gt;&gt;=` *simualtaneously*. You don't have to worry about checking the results of your IO actions for the `Nothing` case, since `&gt;&gt;=` for `MaybeT IO a` uses the same short-circuit-on-`Nothing`-so-you-can-pretend-everything-is-`Just` behavior of `Maybe`'s `&gt;&gt;=` implementation. (It's not magic, though, and you still have to do some tedious bookkeeping for it to play nice with the type system. You can't just freely mix `Maybe a` and `IO a` values inside a `MaybeT` context â€“ to make the types match up, you have to wrap your `IO` values using a function called [`lift`](https://hackage.haskell.org/package/transformers-0.5.4.0/docs/Control-Monad-Trans-Class.html#v:lift), and you have to use [`mzero`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Control-Monad.html#v:mzero) from the `MonadPlus` typeclass to represent what is conceptually the `Nothing` case). `MaybeT` is actually more general than this â€“ you can use *any* monad `m` in place of `IO`, and a value of `MaybeT m a` will represent "an action of monad type `m`, but with the added possibility of not returning a valid value." For example, `MaybeT (State Int) a` would represent an action that might get and set an `Int` state, and that also might not return a valid value. Furthermore, `MaybeT` itself is actually just one example of a more general collection of types called "monad transformers" which are basically just convenient ways to combine the functionality of monads together by having them wrap each other in ways more sophisticated and appropriate than just sticking them together like your (illustrative but not particularly useful in practice) second example. There is also, for example, a `StateT` monad transformer, that lets you add state functionality to another monad. `StateT Int Maybe a` would be conceptually equivalent to `MaybeT (State Int) a`, which are both conceptually equivalent to `State Int (Maybe a)`. (Note that in general monad transformers aren't "commutative" like they happened to be in this example â€“ in other words, the other in which you nest them sometimes matters. This is subtle and mainly appears in the context of nondeterministic and backtracking monads. You can safely ignore it most of the time, but if something seems weird some day and you're using a lot of exotic nested monad transformers, be open to the possibility that it might be the nesting order that's wrong)
Is it possible to simply erase anything that is of a type which the compiler knows has only one pattern? Consider something like `âˆ€ {â„“} {t : Set â„“} {a b : t} â†’ a â‰¡ b â†’ t`: you cannot pattern match on the `â„“` or the `t` at all, so obviously they can't influence anything, and the `a â‰¡ b` has only one constructor, so that can't change anything either. It seems like using the above, we should be able to simply delete all these parameters. Yes, they could be applied in the body of a clause, but by the same reasoning above, we know they're not affecting anything there either. Granted, in the case that they apply an irrelevant (by the above definition) variable to a relevant polymorphic binding, you'd still have to throw in a dummy value like `undefined` (although it seems to me even this could be eliminated by supercompilation?), but I do think in practice this would eliminate the vast majority of residual type-level junk. EDIT: Somehow I didn't notice the paper in your post. I think that covers at least half of what I was talking about then! Although I still think the `a â‰¡ b` (or any other single-constructor type) should be erasable too.
&gt; Unfortunately Monads do not commute. Different layerings have different meanings. When learning Haskell I had great fun trying different orderings of monads and seeing how they differed (if they did). Do we have branching paths with a single state `ListT (State s) a` or do we have one state per path `StateT s (List a)`? What is [`ListT (ReaderT i) IO a`](http://pchiusano.blogspot.com.es/2011/12/programmatic-translation-to-iteratees.html) useful for, how it is different from `ReaderT i (ListT IO) a`? I it would be nice to have some kind of "monadtransformerpedia" that explained which combinations do indeed commute, which combinations don't but one is always more informative than the other (`ExceptT e (State s) a` vs `StateT s (Except e) a`,is that true btw?) and which orderings produce genuinely different monads.
Just a note, the ghci-linker has been getting critical bug fixes and features with every release since 7.10.3, 8.2 will be no exception and 8.4 will have some rewrites to address performance and big-obj support for split-sections. That said. I am always looking for examples of code that don't work. So if you have small examples that don't work I would appreciate bug reports. I say small because linkers are complicated, I spent the majority of the time trawling through assembly to figure out what happened. The less code the better. I have also been working in dynamic linking support for over a year now. And I have a working compiler now but have a few things to figure out before I can start upstreaming the changes. With dynamic linking support we can adopt a similar linking strategy as is done on Linux and Mac OS. So this should make things easier too. 
Cool! I like your GHC hacks :) Also the one with the proofs which didn't need much code. How would that verification project proceed? Is there some small step semantics defined for the core language which is then mapped to LLVM?
RDBMS represent "events" as "INSERT INTO ..." or "UPDATE ...", which have no meaning on their own. Event stores store events like "MovedToAddress(city = ..., street = ...)". Events that makes sense to the business.
Why the name "tempered"?
What, lol. See what I was talking about? No idea how you manage to keep track of so many things. I'm moderately familiar with some of those, I've used reflex, reactive-banana, etc., but I ultimately decided to stick with the React-like strategy. But that's about it... Do you do that out of passion or is there a project behind it all?
How about this one? http://blog.sigfpe.com/2006/10/monads-field-guide.html
I had forgot about that! But I was thinking of a matrix where each cell was either "commutes", "does not commute" or "does not commute, but it can be reduced to the other way". Something like [this spreadsheet](https://docs.google.com/spreadsheets/d/1EmBBNpLCNgxYGpxIQ5f-ledfTajjzLyor5TZRwyTgC0/edit#gid=0)
Yep. Intero is so good I'm making a second attempt to switch to emacs for it. That said there are some resources for how to set up Vim for Haskell, such as Stephen Diehl's guide here: http://www.stephendiehl.com/posts/vim_2016.html
http://www.stephendiehl.com/posts/vim_2016.html
Sometimes I use plain vim with a ghcid window next to it, and when I need better support I use spacemacs (with vim bindings) and intero.
Yes; I prototyped one here: https://github.com/nomeata/ghc-core-smallstep/blob/master/GHC/SmallStep.hs The proofs would happen in Coq, so both the small step semantics and the code generation will have to be translated to Coq first (in a way that the Coq code can be extracted to Haskell and run again, for extra assurance). A tool to assist with that is being developed at Penn right now.
I simply use vim with Syntastic, which uses hdevtools for typechecking and error reporting. This way I donâ€™t have to learn or remember any new interactions or commands, and it is all I need (but maybe I am missing out on useful stuffâ€¦)
At the moment, the main thing that I have is just &lt;LocalLeader&gt;g bound to load the current file into GHCi (for Haskell files): nmap &lt;localleader&gt;g :silent !clear ; ghci %&lt;CR&gt;:redraw!&lt;CR&gt; I also use a use a couple shortcuts to automatically type `import qualified`, `import ` (with enough spaces to line up with `import qualified`) and `{-# LANGUAGE #-}` (and put my cursor in the right spot for the last one automatically) when I hit `&lt;localleader&gt;m`, `&lt;localleader&gt;q` and `&lt;localleader&gt;l` respectively. I have my shell setup so `&lt;Ctrl-Z&gt;` resumes the last suspended job (so I can toggle between Vim and my shell with just `&lt;Ctrl-Z&gt;`). I probably should look into using more, but this gets me by pretty well for the time being. For a lot of small things, just hitting `&lt;LocalLeader&gt;g` works pretty well for me.
Intero + Emacs is my default, "plain" vim + ghcid + ghci/repl is the "always works" setup when the plugins are on the fritz. 
[removed]
Mostly I just don't use much tooling. I've found the various `ghc-mod` based plugins don't work all that well, and honestly they're not gonna save you much typing even when they do work. Ultisnips and Syntastic will get you far enough. You *can* use necoghc with YCM for autocomplete, but it's *slow*.
&gt;&gt; Putting the constraint on the constructor means it will be available when you pattern-match. &gt; You can get the same benefits from pattern synonyms' provided constraints. Pattern synonyms can't add new constraints to an existential type. Your example still includes enough information to deduce the `Wf` constraint. The version of `Wf` from the talk had the neat property that it could, in principle, be erased completely, inducing no time- or space-overhead. The version of `Wf` in the actual code includes the singleton superclass, so it does need a run-time representation, but shouldn't incur any time-overhead for pattern matching. Your version includes an explicit singleton that has to be matched every time you match `ValWf`. That's definitely a useful tool, but it seems like a lot of overhead in this context, where we're just trying to convince the compiler that `Repeat (Repeat s) ~ Repeat s`.
Yes, I'd be interested to see the run-time representation of `Wf`. In principle, all you need is the `SingI` dictionary, since `Wf` has no operations and all the other superclasses are equality constraints, but I don't know that GHC optimizes dictionaries that aggressively. (For that matter, GHC might be making nested chains of empty dictionaries for the `Wf` in the talk, too.) Using the class system to prove a lemma like that is a really cool trick, though. If it catches on, I'm sure someone will find a way to make it efficient.
If you're new to emacs/evil-mode, I'd recommend just diving into [spacemacs](http://spacemacs.org/). Setting up a haskell layer from there is very easy.
Would you mind providing a link to your fork? Here's mine in exchange (fully functional support for long running interactive ghc-mod): https://github.com/liskin/ghcmod-vim/tree/ghc-modi. PR has been sitting there idly for a few months, too.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [liskin/ghcmod-vim/.../**ghc-modi** (ghc-modi â†’ 9f7e9b6)](https://github.com/liskin/ghcmod-vim/tree/ghc-modi) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dgnlnm5.)^.
I concur. While I like fooling around with new languages, this is another barrier to master before I can even start learning the language properly (on a side project). 
I'm trying to get LSP working since it shares the functionality with different language backends (Java, Rust, Go, Haskell, Python at the moment). Elisp https://github.com/emacs-lsp/lsp-mode https://github.com/emacs-lsp/lsp-haskell Haskell backend https://github.com/alanz/haskell-lsp https://github.com/alanz/HaRe https://github.com/alanz/haskell-ide-engine I'm having build problems with the haskell bits but once it works it will be the same tools across all languages that have an LSP backend instead of varying degrees of functionality for each language. The Haskell backend is actively being worked on by Alan Zimmerman.
[removed]
I've tried almost everything, but wasn't really happy with ghc-mod or hdevtools due to them being slow or finicky, so I instead wrote a [wrapper around ghcid](https://github.com/ndmitchell/ghcid/tree/master/plugins/nvim) for *neovim* and have been pretty happy with it. For jump-to-definition I use [fast-tags](https://hackage.haskell.org/package/fast-tags).
Yup, ghc-mod + neco-ghc + neomake works great for me in Vim 8.
Well, that's one answer, but the `lens` library doesn't actually say whether indexing into a multimap should traverse multiple values or one list. Another, option would be to produce an "affine traversal" (zero or one element) but `lens` doesn't specifically offer that.
Do you have any thoughts on [inline-C](https://hackage.haskell.org/package/inline-c)?
Deoplete is neovim only.
&gt; ...in total languages, where changing strictness behavior is not a big deal... That doesn't quite seem right, I mean even if that change causes the evaluation of something extremely expensive but that does terminate that could massively affect performance.
Maybe it is a stupid question, but something I've been wondering for a while; if PureScript is basically Haskell with a few improvements (due to no baggage), wouldn't another approach to sharing front/backend code be simply compiling PureScript to Haskell and running it on the back? I guess you could just write your back-end logic in PureScript and run it with node but I don't thing you want that performance on your server... I guess GHC would compile PureScript magically well.
No, but it's been on the list for a while.
Spacemacs is a great "straight to the point" program.
I have actually never seen this. At a first glance it looks like it's much more useful for small, performance-sensitive kernels (like a decoder loop) than it is for interfacing with C libraries. Like all things that interface with C, Haskell's lazy-evaluation makes determining when things actually get executed somewhat difficult. Ordering mallocs/frees (or their counterparts) while interspersing Haskell code would strike me as challenging to get correct. Also I'm not a huge fan of living in the IO monad (although I use a lot of `unsafePerformIO` in the high level bindings so I might also be a hypocrite). I'm curious to hear about other experiences with it though.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [atc-/config-files/.../**vimrc** (master â†’ 03a096c)](https://github.com/atc-/config-files/blob/03a096cff72d4e3cb939e1a7c5aab48a0235c806/vim/vimrc) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dgodg5r.)^.
I'm embarrassed here: I remember something cool along these lines in the NICTA course, but I can no longer find it. Sorry for sending everyone on a wild goose chase. :(
Wow that's really cool!
How long do we have to register for this? I don't know yet if my schedule is going to allow staying after the conference.
oh shit
Registration closes May 13, one week before the event.
The fact that it is strict means a lot of algorithms compose less well in purescript than Haskell. Purescript is damn near as nice as a "strict Haskell" can be, but that isn't Haskell. ;) Also PureScript has a lot of javascript-y assumptions about records, etc. that would get in the way of truly efficient compilation. You'd have to do all the things that pypy or psyco or v8 or spidermonkey do to make python or javascript efficient to make it work. At which point, its almost node again.
Instead of getting distracted by the impurity of `IO`, maybe it helps you to compare `[Maybe a]` and `Maybe [a]` to see that there is indeed a difference?
Donâ€™t you like it when one thinks â€œoh, that is an interesting old open problem, maybe I should tackle this and do some good researchâ€ and a day later a pre-print solving that problem appears â€¦ anyways, good job Amin, LÃ©o, Morten and Lars.
ELI just learnt `ST`? From the title I think they proved that 'imperative programming with `ST` is pure so it's fine'. But their logic language is really unfamiliar to me, so... am I right?
I use this, and it is an excellent guide
I agree. Haskell's is both easy to refactor and has powerful abstractions. Starting modestly shouldn't be a problem.
Er, ok. Lol.
I think that you are talking about features of Haskell. Lenses is a over-engineered way of using haskell features
And it uses also pattern matching and symmetric multiprocessing technology, and binary encoding
Do you have some package names? I was looking around a bit but didn't find much. There was ehaskell and ehs where one was a quasiquoter and the other in a seemingly unmaintained state. I would also like sth with the simplicity of your tempered and not some real full blown template thing used in webframeworks. Then there is also mustache but I don't want it logicless for some quick tasks.
Yes, everyone believed that this property holds, but there was never a rigorous argument about it â€“ and not because people did not try, but rather because it is hard. So this is therefore a significant contribution.
Can you use something like this to do something like `gzipWith (+) :: (Int, Double) -&gt; (Int, Double) -&gt; (Int, Double)`?
Mutating values in the heap (eg, `writeSTRef`) is also effectful.
I wish missing out top level signatures was a compile error (esp if there was a `--fix` option that put them in for you). At least for me I always write the type signature first, helps me understand what I'm actually trying to write. 
Making it a rule means you end up writing silly types for stuff like this: baseAddress = "//service.com/api" numberOfRetries = 3 wrapParens s = "(" ++ s ++ ")"
(puts on pedantic hat) The paper specifies a bit more precisely what was and wasn't known. (And I only know this because I just skimmed the paper :-P) In particular, `ST` was proven type-safe. However, it wasn't proven that `ST` was entirely _pure_ in that all the equational properties one wants of pure code continue to hold in the presence of some subexpressions that make use of `ST`. (And this property was "known" in the sense we all believed it, and it seemed that it must be true, but proving obvious things is sometimes hard I guess). The issue -- and its one that is evident in all the detail in the paper (which itself is just an advertisement in a sense for the full result in the tech report) is that to prove that you have an equational theory in the presence of "heap effects" means that you need a model of a heap, and of heap effects, and a set of axioms for reasoning about them in the presence of higher order type effects, and then you need to turn the crank on all that to show the property holds under all combinations of ways we can combine these tools. I.e. since ST is about an "operational" property of a heap, we need to have some precise operational semantics in place in some way, and then a framework (in this case logical relations) for abstracting over them. And once you have that, you have enough tedium that apparently automating and mechanically checking the proof is pretty necessary.
Before the first release, I would like to have some feeback on this package (usefulness, code revier, anything ...). the package is solving some extensible record problem by using TH to generated "extended" records and the associated converters (as well as getters and setters, traverse). It doesn't work only on records but on plain types as well as sum types. It can basically take a bunch of types and mapping functions between each field and the desired one and morph the given types into some new ones. (BTW : I use to hate TH but found it in fact surprisingly pleasant to use).
As far as I can tell, they don't address `fixST`, the `mfix` for `ST`. As it cannot be implemented using the rest of the "public" interface, there may still be room for a bit more research here.
...all of which would benefit from explicit typing... :P
Obvious type is obvious. Literals certainly fall into that area. There's a reason we have type inference in the first place. Now, a *warning* about missing top-level signatures might be something I could get on board with.
It depends on the kind of generalization. If you increase parametric polymorphism without changing it too much, this is usually a win in both generality and comprehensibility. If you just take more arguments, you're probably hurting comprehensibility. Also, refactoring code to use more general tools, like switching from straightforward code to a monad transformer stack, is sometimes incorrectly called "generalization" even though it's not necessarily more general (although doing it in mtl style is).
Binaries and maybe a brew formula coming soon if I can figure out some Travis-CI nonsense.
No partial functions.
Hardcoding those first two at the top level is dodgy as they are really configuration or specific to a certain function. Also the second is of type `Num a =&gt; a` which could lead to some less than great error messages. The last one would benefit from a type signature, I shouldn't have to work out that it's `String -&gt; String` by understanding the code. It's such a small amount to write and you normally read code far more than you write it. 
Following on call to arms from /u/snoyberg, I decided to blog about the first Haskell package I wrote. It's called ruby-marshal and it reads Ruby objects serialised with Marshal, e.g. Rails cookies, into Haskell values. I hope other web developers with Ruby on Rails apps can use it as a lever to gradually migrate their apps over to Haskell without the risk of a big rewrite.
Where can I find the call to arms?
I don't know the past example's you'd think of. I know this is a preprint not yet past peer review. But the result couldn't come from a more credible place. Look at the other work from the iris project: http://iris-project.org/ and also take a gander at Birkdal's write-up when he received the Milner award: http://www.sigplan.org/Awards/Milner/ Of course just because some team has a lot of authority doesn't mean they're right. But in a case like this, it really does seem _likely_. This is especially the case for a result like this, I think, where the ability to prove something like what is being proven with something like the techniques being used always seemed extremely likely -- just rather difficult and requiring a lot of careful, sophisticated work.
Oh, no! How did the rewrite go?
I meant that `ProductF` seems to be the object of the fourth item ("lift plain types to parametric oneslift plain types to parametric ones") rather than the third ("Changes the type of some fields"). Indeed, Metamorphosis seems quite useful to hide the boilerplate in a scenario like *Trees that grow*.
Some examples of papers which have made more or less extravagant claims about the ST monad (excluding the original paper, as it did not make very strong claims): * `Launchbury, J. and Sabry, A. (1997). Monadic state: Axiomatization and type safety. In Proceedings of the Second ACM SIGPLAN International Conference on Functional Programming, ICFP â€™97, pages 227â€“238, New York, NY, USA. ACM.` * `Ariola, Z. M. and Sabry, A. (1998). Correctness of monadic state: An imperative call-by-need calculus. In Proceedings of the 25th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL â€™98, pages 62â€“74, New York, NY, USA. ACM.` * `Moggi, E. and Sabry, A. (2001). Monadic encapsulation of effects: A revised approach (extended version). Journal of Functional Programming, 11:2001.` Considering that this has been an open problem which has been revisited on multiple occasions by multiple authors I find this result somewhat unlikely. Furthermore it has, as you point out, not been through proper peer review yet. I don't have a problem with the group in question, but I do strongly dislike arguments from authority. You will find that some of the authors on my (incomplete) list above are not exactly lightweights themselves. Finally, my problem with this being hyped is that hype is what gives us bad science. I don't like bad science.
Cool I haven't seen karver before, but the so post I have. Note that the post is quite old. But I guess it was all a bit to fat for me... I quickly hacked the following: https://github.com/minad/ihs/blob/master/example.ll.ihs - take this php - interpolated haskell! :D 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [minad/ihs/.../**example.ll.ihs** (master â†’ 9d75970)](https://github.com/minad/ihs/blob/9d759706c0dd757686d7f15db124f856546f8e9c/example.ll.ihs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dgp8w5y.)^.
Glad it worked for you! You might also find https://hackage.haskell.org/package/rails-session useful. It implements the Rails crypto dance. Blog post about this package coming in the next day or two.
I think not. Look at the type of gtraverse: gtraverse :: Applicative f =&gt; (forall d. c d =&gt; d -&gt; f d) -&gt; a -&gt; f a https://hackage.haskell.org/package/traverse-with-class-1.0.0.0/docs/Data-Generics-Traversable.html The type is restricted to things like `a -&gt; f a`. I guess it could be easily switched to gtraverse :: (Strong p, ProductProfunctor p) =&gt; (forall d. c d =&gt; p d d) -&gt; p a a and then a choice of `p a b = a -&gt; a -&gt; b` with value `(+)` would suffice for what you want.
Why?
How does this library compare to the plate family uniplate/geniplate/multiplate/lens-plated?
Thanks.
What would be an example of such case?
You might also be interested in the [`dependent-sum`](https://github.com/mokus0/dependent-sum) package, which provides the type: data DSum tag f = forall a. !(tag a) :=&gt; f a This lets you do things like this (taken from the README): data Tag a where StringKey :: Tag String IntKey :: Tag Int toString :: DSum Tag [] -&gt; [String] toString (StringKey :=&gt; strs) = strs toString (IntKey :=&gt; ints) = show ints
I hope soon the workflow seen here will be easier: https://www.reddit.com/r/haskell/comments/6366yo/2017_03_09_joachim_breitner_why_use_theorem/ It'd be great to test for equivalency with something more idiomatic like fcp [] = [] fcp [xs] = [xs] fcp (xs:xss) = [ x : ys | x &lt;- xs , ys &lt;- fcp xss , head ys &gt;= x ]
Ha! Just yesterday I was thinking about coming up with a way of doing declarative, reversible database migrations. The main idea was that a migration DSL shouldn't allow data to be "forgotten", i.e. a type may be split, two types merged, or a type mapped to a more expressive one (i.e. `Maybe` to `List` or `Word8` to `Word16`). `metamorphosis` will no doubt give me some ideas, thanks! :)
If you're doing a fold with an unknown monad you can wind up with some left associated binds that are hard to get to erase away to the more efficient form you get by right associating all the binds. e.g. `foldr (\a r -&gt; foo a &gt;&gt; r)` where foo internally uses &gt;&gt;=. This unravels to ( ... &gt;&gt;= ... ) &gt;&gt; ( ... &gt;&gt;= ...) &gt;&gt; ... You _can_ write these things in terms of using (.) rather than more idiomatic to effectively "yoneda" your way out of the problem, but it produces a pretty unnatural loop. Either way, once you drop the fold in there you're relying on `-Owhatever` to get in there and make the explicit fold go away. If you don't want to trust in that happening or need it to always happen even without optimization turned on? Then explicit recursion can actually be the way to go. Favoring "explicit fold" style like favoring point-free notation can be taken too far.
I know of four monads where all values of type `M ()` are indistinguishable from `return ()` (for a certain sense of â€œindistinguishableâ€). The obvious ones are `Identity` and `Reader e`. Next, there's `Set` from [infinite-search](https://hackage.haskell.org/package/infinite-search), which doesn't have an empty set and isn't affected by duplicates. Finally, there's a monad from a recent paper whose name I have forgotten which generates unique keys with a type parameter. Here, a subcomputation whose return value is discarded may affect the hash values for the keys, but since these have no guaranteed values anyway, you can choose to ignore that.
Thatâ€™s true. But then, we donâ€™t even have a typed call-by-need semantics for Core at the moment, and with Core I mean the Core that we have in GHC, not some paper formulation approximating it. (There is a [call-by-name typed semantics in the GHC repository](https://github.com/ghc/ghc/blob/master/docs/core-spec/core-spec.pdf), and I recently wrote down an [untyped call-by-name semantics](https://github.com/nomeata/ghc-core-smallstep/blob/master/GHC/SmallStep.hs) for Core)
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [nomeata/ghc-core-smallstep/.../**SmallStep.hs** (master â†’ dc7efea)](https://github.com/nomeata/ghc-core-smallstep/blob/dc7efeac4678f0cff224fdf794052b646f40def0/GHC/SmallStep.hs) * [ghc/ghc/.../**core-spec.pdf** (master â†’ 583fa9e)](https://github.com/ghc/ghc/blob/583fa9e3687b49d8c779e6d53a75af9276e4f5cf/docs/core-spec/core-spec.pdf) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dgpp5iw.)^.
How did the migration/cohabitation from Rails to Haskell go? 
&gt; I thought this was a known property of ST but perhaps their proof is a more rigorous argument It is in fact the first proof! People just assumed it worked before. 
What do you mean by a *typed* call-by-need semantics for Core?
No. Such a style of code using something like the `inline-c` package is going to leave you with a fair bit of function call overhead, whereas running -Owhatever on GHC code can "see" into and optimize more things, with idomatic data types in haskell doing optimizations like case of case, let floating, etc. that aren't sound in a strict language like C. So if you magically switched everything over to a ton of inline-C calls, the compiler wouldn't be able to do much to optimize them at all. You'd be then hoping for link time optimization from llvm or something to maybe piece together a fast program from the mess you left it to work with, but it'd be fundamentally crippled by weaker reasoning tools. You can of course mix and match and do this sort of thing where it makes sense, but it isn't a panacea.
Thanks for the link, I'll take a look on the video
Why are you getting downvoted for asking a question?
Thanks, dad.
Because a partial function transforms a pure function into an impure function. `head` has type `[a] -&gt; a` When `head []` throws an exception, was that exception in the type signature? Certainly not. The type signature was a lie.
Given that the theorems in the paper have been checked with Coq I think the role of peer review here should be on the the significance of the proved theorems and whether or not they can be used to support the the claims in the paper. Bugs in proof assistants exist but they're less common than bugs in non-mechanized proofs. Let me ask you this, what flaws do you think peer review could uncover? What were the flaws in the papers you list? Were they addressed in this work? 
In our case, cohabitation went really well! Would you be interested in reading more about the story? What aspects would you be interested in reading about? I'd be happy to write another post.
Hehe, I think without ghc is not possible here without ending up with sth like jinja. But this one is useful as is as a development script. I tried for a few minutes with ReadP (from base to not impose a dependencies). Actually I had a self imposed limit of 30 minutes for the basic functionality which I missed because of this ReadP fiddling ;) While I have written larger parsers using megaparsec, it didnt seem wortwhile. But later I had to add the function to strip spaces. {{x}} doesnt strip and {{-x}} and {{x-}} strips. This could be done cleanly when using a parser combinator. So a rewrite is probably well advised.
Thanks that's a good idea! It would require some rewriting, but might be worth it.
Sounds like you might be interested in [Ivory](http://ivorylang.org/ivory-introduction.html).
Well even for a mechanised proof, there's still the question of "does this actually say what the author says it does?", which is not a question the proof assistant can answer for you. It will happily accept your proof of `goldbachConjecture : a b -&gt; a + b = b + a`, but peer reviewers may not be so impressed!
I'm totally stuck on a type-level haskell problem, and this link is my attempt to define the issue. I believe naperian tensors are the key to unlocking haskell for data science, and would appreciate any help from the community. 
Do you have any insight on how to make a formula that uses `head` or `devel`? This involves compiling the sources and not shipping a binary, so I would guess that it needs to declare a dependency on `stack`?
I'm still in the middle of it (actually, almost finished - sending emails is the only missing "feature"). It went really well so far even though it took longer than expected since I had to learn some concepts and libraries along the way (mtl, digestive-functors, servant, ...). Since everything's faster now and takes less memory I could get rid of all external background jobs and caches and replace them with STM which was awesome.
I would definitely be interested, especially reading a blog post focusing on some of the more obscure features of Rails and how to replicate them in Haskell. Monitoring would also make a good topic - coming from Rails you're used to just adding `newrelic` or `airbrake` as dependencies and call it a day. There's ekg and statsd, but building something as sophisticated as airbrake seems to be a lot of manual work. As an example for a rails feature I didn't implement cleanly: Servant + [flash messages](http://guides.rubyonrails.org/action_controller_overview.html#the-flash). I ended up misusing the combinator `AuthProtect` (custom combinator would be better of course) in order to get access to the cookie and pass a `FlashMessage` to each of my handlers. In order to "consume" (remove the cookie) that flash message you need to set the cookie at the end of every request, though. This means every API type needs to have `Headers '[Header "Set-Cookie] ...` in it which is quite noisy (type aliases help of course).
If I understand your post correctly, I was trying to do the same thing recently, but gave up quickly because it just feels like the canonical form of Hasochism. [Look how simple this becomes with dependent types!](http://i.imgur.com/oVbYHKo.png) For me, this is going on the back burner until they arrive. EDIT: I think I replied too quickly and that is not the right way to define tensor. I think it should be defined more [like this](http://i.imgur.com/gOyjYWg.png), which is not quite so pretty I suppose.
Paging /u/snoyman, I remember him (or a colleage of him) building a minimal GHC docker env.
Servant's documentation is great, so start there. You could optionally generate a JS client (statically or per-request) that acts as a wrapper for your REST API, derived from the same types that Servant uses to declare your REST services in the first place. I'd investigate that. Then your server and client code can stay in sync on the API-side of things, whilst the view &amp; controller logic remain client-side and separate from the rest of your app'. 
Good luck with it. Please upvote me :-)
My post [here on XML](http://chrisdone.com/posts/fast-haskell-c-parsing-xml) was partly meant to demonstrate that GHC Haskell can be on par with C (for some tasks), if you're willing to write more detailed code. In this case, writing a parser by hand instead of with e.g. attoparsec or whatever, and paying close attention to where allocation occurs. But you might be interested in the ATS language which is basically C with ML syntax and dependent types and linear types: https://en.wikipedia.org/wiki/ATS_(programming_language)
Besides that, you are quite right, too many pragmas. There is precedent for designing "shortcut" pragmas that group together several other ones; that might help. Currently we could use something like the following: TypeLevelProgramming = TypeOperators FlexibleInstances FlexibleContexts DataKinds TypeInType TypeFamilies (Not sure if everyone would also want ScopedTypeVariables in there. Perhaps.) and ExtendedDeriving = GeneralizedNewtypeDeriving DeriveFunctor DeriveFoldable DeriveTypeable I actually think that this second one should be the default. If someone explicitly tries to derive an instance and in fact it's possible to do so, why would we want to reject it by default?
It's also consistent with GHC naming, [`SomeNat`](https://hackage.haskell.org/package/base-4.9.1.0/docs/GHC-TypeLits.html#t:SomeNat), [`SomeSymbol`](https://hackage.haskell.org/package/base-4.9.1.0/docs/GHC-TypeLits.html#t:SomeSymbol) and the to-appear-in-GHC-8.2 [`SomeTypeRep`](https://downloads.haskell.org/~ghc/master/libraries/html/base/Type-Reflection.html#t:SomeTypeRep).
https://www.fpcomplete.com/blog/2016/10/static-compilation-with-stack It is easier now than it was when that was written; I make a builder image with this `Dockerfile`: FROM alpine:edge RUN echo http://dl-cdn.alpinelinux.org/alpine/edge/testing &gt;&gt; /etc/apk/repositories \ &amp;&amp; apk update \ &amp;&amp; apk add wget ghc ca-certificates musl-dev shadow linux-headers zlib-dev \ &amp;&amp; update-ca-certificates And add this to `stack.yaml`: image: container: name: foo # Or a better name, for your project base: scratch docker: enable: true image: alpine-ghc # Built from that Dockerfile above stack-exe: download ghc-options: "*": -optl-static That's it. I can use this recipe to build Linux binaries on my (virtual machine running linux and dockerd on my) Mac, ready to run. edit: indenting error
What do the newrelic and airbrake packages do in Ruby?
&gt;Tensors (n-dimensional arrays) I know very little about tensors, but I was under the impression that this was false. AFAIK the tensor represented by ((1, 2), (2, 6)) tensor is equivalent to the one represented by ((2, 4), (1, 3))?
Nice one. Good luck and, if you were able to, I would certainly read a blog post about your experience.
Can you explain how this is different to, for example, repa arrays?
What "signature" would slice have as a closed type family? There are type-level "take" and "drop" type families in the singletons package, could they help?
&gt; I actually think that this second one should be the default. If someone explicitly tries to derive an instance and in fact it's possible to do so, why would we want to reject it by default? Aside from the usual "it's not Haskell2010" reasons, enabling `GeneralizedNewtypeDeriving` by default would wreak havoc with Safe Haskell, as `GeneralizedNewtypeDeriving` and `Safe` cannot be used together: https://ghc.haskell.org/trac/ghc/ticket/8827
They're both gems offered by companies, so they do a bunch of stuff. Airbrake is mostly exception monitoring I think, and new relic gives you lots of performance info about your app.
This was the original problem I was attempting to solve, I'm unfamiliar with homebrew in general and how you specify dependencies and builds which is why I went the binary route. You could automate the formula updating part and remove the "tags" check to build a binary on every commit if you wanted and just have your formula track binaries from master; if you do get building from source figured out make sure you make a post about it!
Right, I've actually seen it recommended to do just that in several places now; thanks for the info.
In other words: An `Iso` is invertible, has at least one target and has at most one target. Drop the "is invertible" requirement and you get `Lens`; drop "at least one target" and you get `Prism`; drop all three and you get `Traversal`. (And leave only "at most one target" and you get affine traversal.)
I think it all hinges on &gt; which seems like an easy-to-argue change Can you make that argument rigorous? The safety of ST was something we all â€œfelt trueâ€, and that reasoning along these lines â€œseems obviousâ€ but that does not constitute a proof.
1. Yes, you wouldn't get very good ghc optimizations. But then wouldn't your C functions be optimized to the hilt with gcc -- giving you a fast program? 2. My mental model is that every FFI call from haskell costs some upfront time, say `t`. But after `t`, you are in C land where things can always be made faster in principle. So as long as your function isn't trivial enough so that `t` is the dominating factor, you should go to inline-C if all you care about is performance. Is my mental model in (2) more or less correct?
Yeah, but `++` is monomorphic to lists, so it'd coerce it into being `[Char]`. Still, not as obvious as `String -&gt; String`
Could be nice if it's your own typeclass? But I was talking about ordinary toplevel functions and values.
It appears to be an entirely new way of composing applications. These sorts of things take a long time for practical experience reports to bubble up.
Your mental model is missing a couple of things: The problem is that C can't do many of the optimizations that Haskell can do. Once you're in C you've lost a ton of information about immutability, about the fact that you _can_ safely deconstruct objects even with multiple references to them due to the fact that everything in kind * is just a wrapper around its guts and can be freely reconstituted, because object identity by pointer equality isn't a thought you think in Haskell, etc. You may win some of the time, but you literally _can't_ win all of the time, because these optimizations that we do aren't safe or legal in C, because C doesn't control the side-effects you need to control to be able to reason about those operations and there is no way to convey this information to the C compiler. Interestingly, ignoring the upfront calling cost, there are loops where a Haskell style execution model beats C. Neil Mitchell found a couple of interesting smallish examples when working on supero. Messing with your mental model further: Haskell doesn't use the C stack, it gets to use lots of baby Haskell stacks, so for lots of simple tasks being run concurrently and in parallel staying in Haskell can be better as well. These stacks can be smaller than C stacks because they don't need the "red zone" for interrupt calls and the unofficial "dlopen" red zone that is needed to work with lots of libraries as the C stack is intact. You _could_ model this state of affairs entirely in C yourself, but you'd actually have to use non-standard things like global register arguments to pass the heap pointer around, do completely manual stack management, probably write a garbage collector, etc. and it'd be a godawful mess, but then you'd be turning yourself into a compiler in the hopes that you might eke out a win for your one project. You have complications interacting with heap objects. If your data is written in haskell using haskell data types, its not easy or necessarily possible to work with it from C in any sort of sane way. If I have a gig of data lying around on the heap in maps and other structures, you're adding all the costs of moving the relevant data over into a form that you can talk to as well, or shuffling it around behind big fat non-haskelly ForeignPtr interfaces that make it more expensive to access on the Haskell side, and complicated gc, while simultaneously robbing you of the rest of the tools in our ecosystem. There will always be special cases where one side wins here, sorry.
Ah, I missed that. Indeed that is important.
It also leaves many people dumbfounded, because the solution is so simple. It comes from a reinterpretation of what a monad is.
That was it. Thanks.
The purity argument isn't a bad one, but if one isn't aware of the trade offs between pure and impure functions, one might not care. Some reasons to care: 1. Partial functions may crash the program, return an exception or go into an infinite loop. The first and last consequences are obviously undesirable. Handling exceptions isn't enforced and they should be reserved for exceptional situations only. 2. Total functions, as long as their types match, both compose well with each other and can be used with in higher order functions. You can trust the compiler here. Partial functions are riskier since you need to start worrying about exceptions and so forth. 3. If you author a partial function, then you impose a burden upon your users: they must verify themselves that the argument they give to it is valid. Whether they check the condition explicitly or reason their way into correctness, it's a cost that incurs at each place that your function is called or code around it is refactored. Another strategy is to make function total and represent results of function calls with invalid input as error values. For example, the `Maybe`-type let's you encode valid values as `Just a` and errors as `Nothing`. 4. If it's cumbersome to code and use total functions on a type it might be the case that the type itself is ill-conceived. The type might include nonsensical values. Imagine if a type representing chess moves allowed a move outside the board. A better type for chess moves would make that non-representable. Then the functions that operate on chess moves would become, if not total, then at least have lesser partiality. Making the types so that the functions operating on them become total might be too difficult in practice, however. 5. Just like /u/velcommen said, type signatures become misleading for partial functions. That said I'm pretty sure there are useful programs with partial functions and they shouldn't be avoided as if they were a heresy, but there are reasons to prefer total functions.
I've tweeted and pinned your post. If anyone blogging in response to Snoyman's post could ping me so I can do the same for them, I'd appreciate it.
Tensors form a vector space that has a popular basis consisting of things like (e1e1e1, e1e1e2, e1e1e3, e1e2e1, ....) The multidimensional array is an arrangement of the coefficients of this basis.
Thanks, thats interesting. 
Great answer, thanks.
You might be interested in this paper/repo: https://strymonas.github.io/ (nb, for Ocaml, not Haskell, and it relies on staging).
Thanks, I really appreciate you doing that.
Isn't it also like ridiculously hard?
What do you mean? I think an encyclopedia of stream fusion, deforestation, and rewrite rules is hard. It's hard in the same sense that performing a ritualistic incantation of 15 language pragmas for type-level computation is hard: in both cases, there is a general solution which just elegantly subsumes it all. To me, supercompilation seems like the simple thing. EDIT: also, this [old paper by the originator of the concept is a fascinating read](https://pdfs.semanticscholar.org/d924/a3d0505ef24452376c91229990cc28f2c945.pdf)
That was my experience, although to be fair at least half the fault is my own for not reading all of the material carefully.
Thank you, /u/metafunctor and /u/duplode this discussion is useful 
In this particular case, GHC will optimize as you hope as long as the numeric type involved is one it knows enough about. First, it will expand the section notation. Then it will inline the definition of `(.)`: \z -&gt; (\x -&gt; x + 5) ((\y -&gt; y + 5) z) Then it will beta-reduce: \z -&gt; (z + 5) + 5 Finally, since it knows addition is associative and knows how to add literals, it will rewrite this as \z -&gt; z + 10
https://en.wikipedia.org/wiki/Warnock's_dilemma
"Tensors" are generalizations of linear maps (from linear algebra) that have more than one "input" or "output" and are linear in each input "slot". The most common example, weirdly enough, is the determinant, as a function on the rows. Doubling any row doubles the determinant, etc. The tensor product of vector spaces (or modules, more generally) gives a way to think of a tensor as a plain linear map by "combining" all the input spaces together with the tensor product operation. The elements of this combined space are essentially tuples (linear combinations thereof) and are called tensors, but the abuse of terminology that extends that name to functions on these spaces is somewhat common. Just like linear maps can be represented as matrices after a choice of basis, a tensor function of n arguments can be represented as a matrix on any n-1 of those arguments by holding all but one constant, because it's linear in each variable. (Yes, this *is* currying!) Now you can let another one vary, so now you have a vector for every element of the matrix, i.e. a "cube" of numbers. This can be continued until the tensor is completely represented as an "n-dimensional" matrix. Look up the tensor-Hom adjunction, it's neat. (And I think it gives rise to one of Either/(a,) -- but I'm not sure.)
&gt; To me, supercompilation seems like the simple thing. Trust me, it is pretty complex in practice.
It has Rails 4 cookie I believe. Would be happy to raise a PR for the Rails 3 cookie, if you feel it's worth it.
Yeah, I am asking where the 'rigor' becomes hard. It might be there, but that actually seems like it should be the easier part of the argument to me. But perhaps I'm missing something obvious or our existing formalisms are harder to work with than I'm imagining.
&gt; Finally, since it knows addition is associative and knows how to add literals Really? I had no idea GHC could do this. Any more info?
Thanks for mentioning specifically why you think it's complicated! I agree that the termination problem is difficult, and probably some other things as well, but that's why I mentioned it as a pragma instead of as a hidden optimization layer somewhere deep within GHC. This allows me to sprinkle it in places I know otherwise have issues reducing (reifying ASTs comes to mind), and allows the implementors to provide a basic implementation which may not handle every imaginable corner case precisely.
&gt;I cannot for the life of me understand why it isn't ubiquitous as an annotation just like `INLINE` or `NOINLINE`. I would hazard a guess that it's because people are still writing research papers about it this decade. The path from research paper to ubiquitous almost always takes multiple decades.
If I got this right, that one can do ((Int, Int), Int), and this one can do (Double, Int), so there's something missing that does ((Int, Double), Int).
No more info, but you can learn an awful lot about how GHC tends to optimize things using `-ddump-simpl`, `-ddump-stg`, and if you're brave even `-ddump-cmm` and `-ddump-asm`.
And this was not a mistake! Just as you expected, the tensor product of two vectors is a tensor. You used the example ((1,2), (2,6)). We can use the fact that the tensor product is linear in each coordinate to write out the coefficients of this tensor in the standard basis. Using the notation you provided earlier, we have the following. ((1, 2), (2, 6)) = ((1, 0) + 2(0, 1), 2(1, 0) + 6(0, 1)) = (e1 + 2e2, 2e1 + 6e2) = (e1, 2e1 + 6e2) + (2e2, 2e1 + 6e2) = (e1, 2e1) + (e1, 6e2) + (2e2, 2e1) + (2e2, 6e2) = 2(e1, e1) + 6(e1, e2) + 4(e2, e1) + 12(e2, e2) Which would sometimes be written as the matrix 2 6 4 12 and the product ((2, 4), (1, 3)) indeed yields the same one. One of the sources of confusion regarding tensors is that they are alternately used to represent multilinear maps and grids of coefficients. The "same" tensor may in one situation denote a bilinear function that takes two 2-vectors as an argument to produce a number (like the determinant of a 2x2 matrix), in a second situation denote a linear function that accepts a 2-vector as an argument to produce a second vector, and in a third denote a 4 dimensional vector. Switching between these is called raising/lowering indices. This is analogous to how a single n-vector may represent a point in n-dimensional space, or alternately a linear function from such a space to its underlying field. 
I'd definitely welcome a PR with Rails 3 support.
repa is a first-cousin, though I'm not an expert on that library, so I'll respond with my design thoughts. This question is partly coming from [numhask](https://github.com/tonyday567/numhask) library development. That project is an exploration of the design space around Num. I'd like Tensors to be part of a number heirarchy, and use simple operators to add and multiply them etc. More generally, I'm trying to get numbers (like Tensor) to fit in with more of the haskelly way of doing things. A lot of number algorithms in haskell are copied from imperative code, and when you start with a machine representation of what an array or matrix is, it's pretty easy to continue down that path. Hewing closer to category theory, using concepts like Representable, is an exciting path to explore. One thing that isn't at all obvious from the question, is the opportunity to be radically polymorphic. Ultimately, the representation of what a number is can be quite general if you head down the category theoretic path. A number can be computation instructions say, and it might be possible to support back ends into accelerate, or repa, or into something like sdeihl's recent llvm post. So then we would have the best of everything: the higher-maths that haskell can allow us to operate within, and a fast computation. 
I just wanted to try Hedgehog. Seems really nice!
I suppose that pure lambda calculus uses church encoding which means literals are just functions... I can imagine that could simplify things a little bit. 
How does laziness help in this case? Even if `(.)` was smart enough to look into the implementation of its arguments, recognize the runtime representation of `(+5)` and create a new `(+10)` function instead of creating a new `(\x -&gt; (+5) $ (+5) $ x)` function, wouldn't laziness break your implementation in exactly the same way as it breaks the ordinary non-accumulating implementation?
Could you give some examples of the value-level slice in action? I don't understand 100% how it should work. I'm not a type-level maven myself, but closed type families behave a bit like functions on types. Only they have different syntax than normal functions, they can't be anonymous, supporting higher-order functions requires a trick called "defunctionalization", and other oddities. For example {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE DataKinds #-} type family Negate (b :: Bool) :: Bool where Negate True = False Negate False = True You can "run" a type family on ghci using `:kind!`: ghci&gt; :kind! Negate False Negate False :: Bool = 'True In your case (with a dummy implementation) it would be something like type family Slice (xss :: [[Nat]]) (ys :: [Nat]) :: [Nat] where Slice '[ '[ a ] ] '[ b ] = '[] 
Because you don't have to make an arbitrary decision about wrapping, and it all lines up nicely for consistent scanning/reading. `stylish-haskell` can automatically do both and defaults to one per line, alphabetically sorted.
/u/jystic will be pleased :).
I updated the page with a singletons TH version of this. I've been trying to skip understanding the oddities you mention - I dont want to code at the type level, I have to. but suspect I've now crashed into defunctionalization etc, and have to drop back to type families to fill the blanks in my understanding. 
and the autocompletion functionality in haskell-mode emacs.
When I looked at transient I ran into a bug I posted about here: https://github.com/transient-haskell/transient/issues/38 I became concerned about the suggestion to catch and hide the exception I was receiving and went to the #haskell chatroom on freenode to double-check that such a thing was not common practice with Haskell. agocorona (the creator of transient) ended up joining the conversation and it got a bit heated (from my perspective). In summary, prominent users in #haskell were upset about agocorona's suggestion to hide the error, and were also skeptical that his library could do all it was claiming. I couldn't follow all they were talking about, so don't have my own opinion yet. (But I do remember the CAP theorem came up a few times.) I can say that I thought the transient tutorial was well written; a lot of effort has gone into it so I think everyone should give it a try. Yet, ultimately I did quickly run into limitations in transient.
Do you have any links? I'm familiar with the term from machine learning (e.g. https://arxiv.org/abs/1503.02531 ) where a large model is trained on data, then a simpler model is trained to approximate the large model. When I search for "distillation optimisation programming" all I get are numerical methods for increasing chemical yields...
It was though. Thanks!
By the way, what about the "generic heap object", the one with N heap pointers and M unboxed words? Can it be expected to make it to GHC (conditional on not implementing it myself at some point)?
So "massively asychronous programming"? Sounds cool.
 prox :: forall x. (SingI x) =&gt; [[Int]] -&gt; Proxy (x::[[Nat]]) prox _ = Proxy::Proxy x That does not lift the list to type-level. It is in fact not possible (wait for Dependent Haskell). If you want to track something at the type-level it has to come from there in the first place. prox [] :: Proxy '['[42], '[13, 14]] The value-level `[]` is entirely unrelated to the type-level `'['[42], '[13, 14]]`, which is why you get an ambiguous-type error in your test. --- There is also some confusion about what rows and columns correspond to. (1st/2nd dimension vs which ones are represented as contiguous subvectors)
Modern brew will tap it automatically if you specify it as part of the command e.g. `brew install chrispenner/tools/myprog`, so really it's just a bit easier and gives extra flexibility with little downside. It also lets you test it out easily before trying to merge it into another project; and update it easily whenever you like. I've actually never tried merging into homebrew proper so I don't know what it's like or whether they have security concerns for relatively unknown projects, this guide is to get you up and running, if your project is big enough to justify getting it into homebrew base then you can probably figure it out then :)
I wish I was in the northern hemisphere. OP's Summer happens during our University term here.
By the way, for what it's worth, a `SingI` constraint on a `Nat` is the same as a `KnownNat` constraint. I believe that `KnownNat n` automatically implies `SingI n` already, and you can go the other way using `withKnownNat (sing :: Sing n)`, which will get you a context with `KnownNat n` when `SingI n` is in the context.
I've tried asking the author some questions before, and mostly got anger in response. I noticed `transient` being suggested in here by the author in almost every reddit thread talking about Haskell + FRP, and that they seemed to have some pretty basic misunderstandings of FRP that were coming out in some of the following `transient` vs comparisons. It looked like a lot of work had gone into `transient`, so I tried to gently suggest that there might be some misunderstandings on the FRP front, and asked some questions about whether there were denotational semantics for `transient` and/or how it might relate to the actor model. There was quite a bit of anger in the response, not much interest in whether they were saying things that might not be true, some pretty vague answers to my questions, and several mini-rants of the real world programming vs the ivory tower academics / theorist variety. Up until then, I was thinking about taking a further look. Now, I think I'll wait until a few more people dive in. I wish the author plenty of luck though - it looks like he's put a lot of effort in, and I'm sure I've given plenty of vague answers before when I've been deep in my problem solving cave :) 
The problem is the same as strictness analysis: doing something incredibly simple that gets like 70% of cases is easy. Anything more is extremely difficult. This essentially means the ongoing cost of the feature can become *very* asymmetric. Strictness analysis is clearly good in the general case, with low overhead for the most part on the users side... the same can't necessarily be said of supercompilation. Most approaches have extreme size/complexity problems even at toy scale, from what I can see. Even Max's thesis, which is probably some of the more recent work, completely becomes impractical at a few hundred LOC due to time complexity. (And a lot of that thesis is about how to *reduce* the complexity and bugs in a supercompiler!) I'm also not really a fan of "let's give users a way to make GHC compile even slower when the programmer thinks its a real good idea". It may very well *not* be a good idea. Or maybe it's a good idea today and not tomorrow (because heuristics of the optimizer change). The design fragility is very real. I also mention this somewhat, but there is serious cognitive overhead to having a bajillion switches for every perceivable feature. Everyone thinks if they just hide it behind a pragma or switch, and they "know" exactly where to use it, it magically has no impact anywhere else on anything and everything is just fine. In practice it never, ever works that way.
https://pdfs.semanticscholar.org/9f84/fbd48ad2351881a8768aa1c63c18f82e050a.pdf
I don't think that is impossible. You can still pass the slice indices via a `Proxy` (or a type application). I'm going to whip up something in a few minutes. The main limitation is that types are a bit too static, but even that can be worked around with some unsafe features.
Interestingly, the usual addition of Church numerals is indeed associative under alpha-beta-eta-equivalence: 0 := \z. \s. z suc := \n. \z. \s. s (n z s) plus := \n. \m. \z. \s. n (m z s) s plus (plus n m) k = \z. \s. (plus n m) (k z s) s = \z. \s. (\z. \s. n (m z s) s) (k z s) s = \z. \s. n (m (k z s) s) s plus n (plus m k) = \z. \s. n ((plus m k) z s) s = \z. \s. n ((\z. \s. m (k z s) s) z s) s = \z. \s. n (m (k z s) s) s
[Type-checked tensor slicing](https://gist.github.com/Lysxia/bf4fa5adf879de309bca780a5abceff8)
Disclaimer: I haven't looked at `transient` in quite a while. It's possible that everything I say is totally out of date. But I was so unhappy with it when I first saw it that I haven't even given it a second look. I've been silent on it since because I made my arguments about it then, and the author didn't seem to agree with any of them. Personally, I think it's trying to do *way* too many things in one monad. - `Alternative` concurrency This seems like a good tool to have at your disposal, but it also seems impossible to reason about code that has it on by default everywhere like `transient`. You can call a function and have no idea if the next step in your program will run once, twice, or maybe never. You have to have complete knowledge and understanding of how that function is written and how it might change in the future. We use a type system because we know people are bad at that. I see no reason to assume otherwise for concurrency, especially since it's harder than type checking. - `Applicative` concurrency This one is more reasonable. Haxl does it, and I've done plenty of my own work in this area. But I don't think it's a good idea to force it to be on by default, especially since wrapping the monad in most other monad transformers will turn it off (since most transformers have to implement `(&lt;*&gt;)` in terms of the underlying `(&gt;&gt;=)`) and you'll be left wondering why your code suddenly changes semantics completely. - State management This is my biggest gripe. The way `transient` does state is completely insane. The monad is similar to a state monad, except that the state is effectively a type indexed collection of states, with no way of locally scoping state. So if you want to have two different chunks of code that use state of type `Int`, you have to declare newtype wrappers for those spots unless you want them to be using each others' state. But that's not *so* bad. The completely insane part is how it interacts with the `Alternative` concurrency model. Because state is managed totally on the fly, and you can request any type indexed state you want at any point without affecting any types in your function signature, you never have to guarantee the presence of that state. As a result, if you try to get the state when it's not there, *your thread gets silently terminated.* No type level indication of this kind of thing anywhere. The state system in `transient` abhors local reasoning. `transient` is just *filled* with all kinds of things like these three that destroy your ability to reason about the execution of your program. I guess that's maybe the point, in that it's *supposed* to change the execution of your program as powerfully as possible. But it just seems horribly suited for real world systems for all the same reasons as dynamic types: You have to do the bookkeeping yourself, in your head, about some of the most fundamental aspects of your code. And if you get it wrong, all kinds of insanity could happen; even more so with control flow than with types (with types, you usually just get some kind of obscure runtime error; with control flow, your entire program can execute completely differently and entirely silently). I just don't think I could trust myself (let alone a team) with being able to reason about a codebase written with `transient`. To me, the distributed stuff is the only important thing about `transient` that's really unique to it. For everything else, there's just always already something on Hackage that lets you do that thing locally, without forcing it to bleed into 100% of your code, and usually more composably by way of a monad transformer or something. I *much* prefer small, local, composable tools to the overpowered monolith that is `transient`.
I have a couple questions about whether I should apply or not. I'm currently a college student, but am graduating in a couple days. Am I even eligible? And what level of experience with Haskell are you looking for? I've got a great interest in the language, and done a lot of reading and some tinkering, but I've hard a hard time actually sitting down and doing a project in it due to lack of focus.
I think your intuition about lambda calculus is correct. I believe all you need is beta reduction rules, and then adding church numerals will automatically compose them into the church representation of their sum.
I think the poster's intuition about lambda calculus is a good one. I believe all you need is beta reduction rules, and then adding church numerals will automatically compose them into the church representation of their sum.
If you look at this suggestive sketch, you can see the first problem that arises: curryRecord :: Record proxy as -&gt; Curry as (Record Identity as) curryRecord RecordNil = RecordNil curryRecord (RecordCons x xs) = \a -&gt; _ (curryRecord xs) GHC tells us Found hole â€˜_â€™ with type: Curry as1 (Record Identity as1) -&gt; Curry as1 (Record Identity (a : as1)) So the problem is we want to produce something in the underscore that arguably starts with another lambda. And that function in turn may be arbitrarily "deep". However, at the very _end_ of the chain, we then want to cons up all the results of all the lambdas thus far at once. So, where do we stick the `a`? We can try to write that on its own as an exercise like so: curryRecord :: Record proxy as -&gt; Curry as (Record Identity as) curryRecord RecordNil = RecordNil curryRecord (RecordCons x xs) = \y -&gt; deepCons y xs (curryRecord xs) deepCons :: a -&gt; Record proxy as -&gt; Curry as (Record Identity as) -&gt; Curry as (Record Identity (a ': as)) deepCons = undefined Consider, though what the complexity of this is, and why. (Note the need for retraversals). That leads us to the conclusion that we _really_ want to do a sort of result-passing transform, just like one does when performing a map in a strict language. And just like that case, we end up with passing "down" the function tree a _reversed_ list, because the order we get the arguments is the _opposite_ of the order we need to start consing them together in. (We get them leftmost first, but consing starts from the right!). So: this points to the final missing piece, a function that does a reverse of a list at the type level. If you have such a piece in place and think about where to use it, I think the approach should present itself...
Thanks to the record being a GADT... In the `curryRecord (RecordCons f r)` case, we would like to map on the result of the recursive call `curryRecord r`. We do not have nested `fmap`, but we can use CPS to operate on the result directly. {-# LANGUAGE GADTs, DataKinds, PolyKinds, TypeFamilies, TypeOperators #-} module Curry where import Data.Kind type family Curry (xs :: [Type]) (y :: Type) where Curry '[] y = y Curry (x ': xs) y = x -&gt; Curry xs y data Record :: (k -&gt; Type) -&gt; [k] -&gt; Type where RecordNil :: Record f '[] RecordCons :: f a -&gt; Record f as -&gt; Record f (a ': as) newtype Identity a = Identity a curryRecord' :: (Record Identity as -&gt; r) -&gt; Record proxy as -&gt; Curry as r curryRecord' k RecordNil = k RecordNil curryRecord' k (RecordCons _ r0) = \a -&gt; curryRecord' (\r -&gt; k (RecordCons (Identity a) r)) r0 curryRecord :: Record proxy as -&gt; Curry as (Record Identity as) curryRecord = curryRecord' id 
As far as annotations go, I think we just disagree. I mean I feel like most of what you said could equally apply to `INLINE`: lots of times it's good, sometimes it's not, figuring out exactly when to do it is more magic than math, and it's even exposed to the programmer via annotation. Regarding the difficulty of implementing supercompilation in Haskell, I feel like it's kinda building a hospital at the bottom of a cliff instead of simply putting a fence at the top: the real issue is that Haskell has no termination checker in the surface language. It probably wasn't something that even entered anyone's mind back in the early days of the language, so I don't fault anyone for that, but from a modern perspective, I feel like we can pretty confidently say that playing sloppy with termination *has costs*, and they're kinda big. It's not simply a matter of "yeah, you can accidentally write a non-well-founded function, but that's super rare so we just don't worry about it": that lurking non-termination (*especially* in the presence of being dependent on evaluation order!) just cripples the compiler's ability to perform nice transformations on your code -- supercompilation being one example, but even simple rewrite rules in general like "fmap respects function composition" cannot be allowed since the laws may break in the non-terminating case.
Funny you should say that -- my definition was just me trying to make the Tensorflow API safer! If you have more information on a better definition, I'm interested.
What are the dates of the project? Are graduate students eligible?
Generally speaking the operator doesn't need to be associative. Take division as an example. `(x / 5) / 5` =&gt; `x / 25` and `5 / (5 / x)` =&gt; `x / 1` I am wondering though whether these rules can be derived manually trough an algorithm or must be specified by hand. 
I'll look up the discussion
The timeline is here: &lt;https://summer.haskell.org/#timeline&gt; Yes, graduate students are definitely eligible.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [transient-haskell/transient-examples/.../**distributedApps.hs#L90-L121** (master â†’ 27b43e6)](https://github.com/transient-haskell/transient-examples/blob/27b43e6a9a77c4806aa72722d1f66e3758bec242/distributedApps.hs#L90-L121) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dgrokub.)^.
One thing you could try is upgrading `cli-todo` to use a database instead of a file.
Indeed that is what I was referring to. I see that `someNatVal :: Integer -&gt; Maybe SomeNat` is already provided by GHC, that's nice. It seems that then using `eqT` you can then satisfy most constraints. One difficult case seems to be overlapping instances (`instance C n`, `instance {-# OVERLAPPING #-} C 0`). I guess you can work around them by representing the check `n == 0` as a type-level boolean. `class C (n :: Nat) (nEqZ :: Bool)`...
OK, points well taken. But having the single pragma to enable all those at once - which is a common idiom - would still be very nice, even if we can't enable it by default. Wow, I didn't realize the scope of the whole GND vs. Safe Haskell discussion. Definitely way beyond what we are discussing here, some simple ways to cut down the size of commonly used pragma lists.
You probably want to start by this: http://dl.acm.org/citation.cfm?id=1411301&amp;CFID=755423780&amp;CFTOKEN=53670170 But, as /u/carette points out, we really need an updated one.
PostgreSQL is the best supported.
I think I will stick with Overwatch for now!
Thanks! This works, and as you point out, it ends up being `O(n^2)` in the length of the record, although my records are all under five elements, so it isn't really a problem. I can see how doing a list reversal would bring the complexity down to `O(n)`, but it seems like /u/Syrak has [a solution using cps](https://www.reddit.com/r/haskell/comments/67l9so/currying_a_typelevel_list/dgrghxz/) that also brings the complexity down to `O(n)`.
Interesting approach. Replacing the `bigJug (foldl fsm initial states) /= 4` by `notElem 4 (map bigJug (scanl fsm initial states))` may converges quickly and shrink to the same result.
Agreed. The continuation approach is much more elegant than what I sketched :-) For long lists I think it may cause some drag in the heap, but for your case it's almost certainly the nicer way.
Come as you are!
"In Haskell, refactoring feels like painting-by-numbers. You could do it while drunk." :)
I believe GHC's constant folding happens in the cmm optimization phase.
Fortunately, there would never be a city or a player called Contravariant bifunctor. 
&gt; the real issue is that Haskell has no termination checker in the surface language. It probably wasn't something that even entered anyone's mind back in the early days of the language, so I don't fault anyone for that, but from a modern perspective, I feel like we can pretty confidently say that playing sloppy with termination has costs, and they're kinda big. I agree in principle, but in my view termination checking is still too immature to impose on programmers. When a user of dynamic types (Python, JS, etc.) tries to learn, say, C++, they may get frustrated, finding that the compiler keeps getting in their way, that they must spread a load of magic annotations throughout their code to make things work, etc. This frustration gets associated with "static types", and throws a lot of fuel on to the static/dynamic flame war, putting a lot of people off touching something like Haskell, where the tradeoffs are much better. I think termination checking is very much at the stage of "compiler says no" (when the termination checker can't figure it out), and magic-annotation-scattering (e.g. syntactic guardedness). When I play with Coq and Agda, I find myself almost immediately reaching for a "fuel" argument, or wrapping everything in a Delay type, then throwing in bigger and bigger constants until it completes. The fact that Idris makes it easy to switch off totality checking is one of its killer features, IMHO.
Very good question. I was about to ask the same! What I find particularly interesting are functions like transformBi(M) universe(M) from uniplate and everything everywhere from traverse-with-class. These are useful to write ast transformations/folds without much code. I also want to add recursion-schemes to the list which I find very elegant especially since one can also generate base functors using TH! Mentioning TH, this is also something in the generic programming direction, however more as a basis. For example geniplate uses TH, while uniplate uses syb or handwritten instances. GHC.Generics was also considered for uniplate. It would be great to see an up-to-date sheet of the option! Which one is used for what, where is the overlap and where is the difference... 
Interesting! I read Hofstadter on Nomic back when I was in high school. I shall take a lookâ€¦
Well Agda does at least have a `{-# TERMINATING #-}` pragma you can apply to a function to just assert that it terminates, but ya, I do agree that the termination checker can be annoyingly naive at times (fmap'ing while defining a recursive function is where I hit this a lot). That being said, I'd never disable the termination checker globally, even if there were an option to. As often as not it's been me who's wrong, not the termination checker! As for the approachability argument, I know this can be controversial, but I do take the other side of this. I think it's better to focus on helping people learn rather than dumbing down the ecosystem so they don't have to. There's plenty of places people can go if they just want a straightforward way to write some code that runs: we don't need to compete in that space. Tangentially, I don't think the comparison with C++ complexity is fair: C++ complexity is largely due to bad design decisions in the language and libraries. That's not at all the same thing as the complexity involved in learning how a Lens or an Applicative works, where the challenge is understanding the mathematical shape itself.
Haskell is about exploring ideas, this community needs radical and controversial ideas that challenge the state of things
It would be quite relevant, given that all it is doing is use plain algebraic types. It would not be hard to translate between the two languages in this case. This is more of a math question than anything else. The only language specific parts are where in the answer I am using FsCheck - I'd have to translate that to QuickCheck in Haskell.
Very well. Before I start let me just say that zip is essentially a dimensional rotation. Supposing you have 3 regularly sized tensors of dimensions [x,y,z] the way to zip them would be to make a single tensors at the root of size [3,x,y,z] and then rotate the tensor to the right. [3,x,y,z] =&gt; [x,3,y,z] =&gt; [x,y,3,z] =&gt; [x,y,z,3]. This can be expressed as a series of transposes. After this you'd have the triplet tuple at the end of the tensor, in the innermost dimension. The unzip would be the reverse of the above, pushing the innermost dimension to the outer edge. There are basically 3 function of interest here and they work fine for regularly sized tuples. I've used tensors in the example to evoke the correct imagery as they are regular, but for irregularly sized tuples, I am not sure what the right thing to do should be otherwise I could do the functions correctly. Let me go through them one by one. The first one is transpose: let transpose (l: T list) (on_fail: unit -&gt; _) (on_succ: T list -&gt; _): T list =... It does what you think it does, but it is written in the CPS style instead of using the result type and has a bunch of error checking code that obscures the underlying pattern. It has 3 different possible outcomes: 1) If all the conditions match such as having the inner dimensions all equally sized, then it calls on_succ with the transposed result. 2) On fatal failures like being passed empty inputs in the inner dimensions it throws an exception. This terminates the program. I've decided on this because on empty inputs zip and unzip are definitely not inverses of each other. 3) On lesser failures it returns the original input. The reason for this is because it would allow me to zip things like `(a,b)` and `c` into `((a,b),c)`. If I required the both lists to be regular, the transpose would be a lot less useful. let rec zip (l: T list): T = ... Without any error handling code it could be written like this: transpose l |&gt; List.map (function VV x -&gt; zip x | x -&gt; x) |&gt; VV What this does is transposes the outer dimension and then recurses into the datatype (if it is a tuple) and repeats that process. let rec unzip (l: T): T list = The reverse of the above. Without any error handling code it could be written like: List.map (fun el -&gt; unzip el |&gt; VV) x |&gt; transpose It first transposes the inner dimensions and works its way outwards. Hopefully, adding type annotations and the above explanation makes it a bit clearer what is going on. As you stated, the problem in this case seems to be deciding on what are the correct semantics for irregularly sized tuples and my bag of tricks seems empty at this point. Here are some examples: let fail_case = [VV [VV [S "12"; S "qwe"]]; VV [VV [S ""; S "ug"]; VV [S ""]]] let fail_after_zip = zip fail_case let fail_after_zip_unzip = unzip fail_after_zip let fail_fater_zip_unzip_eq_orig = fail_case = fail_after_zip_unzip val fail_case : T list = [VV [VV [S "12"; S "qwe"]]; VV [VV [S ""; S "ug"]; VV [S ""]]] val fail_after_zip : T = VV [VV [VV [S "12"; S "qwe"]]; VV [VV [S ""; S "ug"]; VV [S ""]]] val fail_after_zip_unzip : T list = [VV [VV [S "12"]; VV [S ""; S "ug"]]; VV [VV [S "qwe"]; VV [S ""]]] val fail_fater_zip_unzip_eq_orig : bool = false Here is a success case. It rotates the dimensions inwards as could be expected and then outwards after the unzip. let succ_case = [VV [VV [S "12"; S "qwe"]]; VV [VV [S ""; S "ug"]]] let succ_after_zip = zip succ_case let succ_after_zip_unzip = unzip succ_after_zip let succ_fater_zip_unzip_eq_orig = succ_case = succ_after_zip_unzip val succ_case : T list = [VV [VV [S "12"; S "qwe"]]; VV [VV [S ""; S "ug"]]] val succ_after_zip : T = VV [VV [VV [S "12"; S ""]; VV [S "qwe"; S "ug"]]] val succ_after_zip_unzip : T list = [VV [VV [S "12"; S "qwe"]]; VV [VV [S ""; S "ug"]]] val succ_fater_zip_unzip_eq_orig : bool = true
Hope to see it in the new HCAR :D
Cool! Thanks for your reply.
fair enough
 Lens i m s t a b y o u = ... We decided to stop adding type variables at that point.
That's a very good point! Something we intend on adding.
A while back, I derived a comonad transformer, data ContextT o w a = ContextT (w o -&gt; a) (w o) which is to `Density` what `ContT` is to `Codensity`. This has the property that `ContextT o Identity` is isomorphic to `Store o`, but is different from the `StoreT` I've seen in various libraries. You can define analogues to the `observe` and `lift` functions for `ContT`: enter :: Comonad w =&gt; w a -&gt; ContextT a w a enter = ContextT extract lower :: Comonad w =&gt; ContextT o w a -&gt; w a lower (ContextT c x) = extend c x and you can define operations that seemingly correspond to `reset`, `shift`, and `callCC`: resetCtx :: Comonad w =&gt; ContextT o w a -&gt; ContextT a w a resetCtx = enter . lower shiftCtx :: Comonad w =&gt; ((ContextT o' w o -&gt; a) -&gt; ContextT o w o -&gt; b) -&gt; ContextT o w a -&gt; b shiftCtx f (ContextT c x) = f (c . lower) (enter x) callCCtx :: Comonad w =&gt; ((ContextT o w c -&gt; a) -&gt; ContextT o w a -&gt; b) -&gt; ContextT o w a -&gt; b callCCtx f x@(ContextT c _) = f (\(ContextT _ w) -&gt; c w) x `shiftCtx` gives you the changes to the context up to the nearest `resetCtx` or `enter`. That is, shiftCtx f . extend g . resetCtx . extend h = f (g . resetCtx) . resetCtx . extend h 
How does termination checking at the source language help at all with the termination checking problem of super compilation? Even with simple terminating functions like `length` you can go on indefinitely generating increasingly "optimized" variants specialized over increasingly large input patterns, with every step along the way also being a total function.
Oh, I didn't originally make the connection between the C++ static typing and the termination checker. I understand what you're saying now, yes, the current naiveness of the termination checker could be off-putting. Anyway, I don't think you could add a termination checker to Haskell at this point. It's one of those things that really needs to be there from the start. What I do hope to communicate though is that the cost of not having it is quite a bit more than may at first seem (at least, to me): the real cost is not in the mistakes it could prevent from you as a user, but in the constraints it places on the kinds of transformations the compiler can do to optimize your code!
I think you're overstating the burden a bit. I consider myself pretty liberal with totality pragmas (as in, I'm more likely to throw one down than to refactor to satisfy the totality checker), and a quick grep of my code shows a totality pragma around once every thousand lines - and amusingly, the majority of those are the same case cropping up repeatedly. Proving theorems is difficult, no argument there, but it's rarely difficult because you have a valid proof that the totality checker is naively rejecting. Anyway, I'm not so much seriously advocating that totality checking be brought to Haskell as I am shining a light on the fact that the real cost in not having it is the constraints it puts on the kinds of transformations the compiler can perform on your code (of which supercompilation is just one).
Every time some game is proposed as "the ultimate challenge for AI" (e.g. Go, due to its size; poker, due to its randomness and bluffing; etc.) I always think they're nothing compared to winning Nomic :)
I'm not sure what you mean? In Haskell, I'd argue length isn't total (`length [1..]` hangs), but Morte is a supercompiler and [you can see length defined here](https://github.com/Gabriel439/Haskell-Annah-Library/blob/master/Prelude/List/length.annah).
tried turning adblock off... my computer feels violated... and that site still couldn't deliver your pic. could you upload to imgur?
This looks great, but please don't write things like this: &gt; Any time I make changes, the correctness of my changes are enforced by the &gt; compiler It's easily misconstrued as a claim that Haskell's catches all bugs and leaves us open to attack from people who think we are overstating the case for Haskell. Could you perhaps say something like &gt; Any time I make changes, the compiler can catch many mistakes I may have &gt; made 
Oh, the CPS variant is indeed elegant! I'll add the note to the post (with acknowledgment) *EDIT:* done.
You can read the announcement here: https://www.reddit.com/r/haskell/comments/646k3d/ann_hedgehog_property_testing/ It is supposed to give a better support for shrinking, without providing the shrinking function by yourself. And from my short experiment with it (I discovered it today), that is "simpler" to fine tune your sample generation.
This one? http://blog.sigfpe.com/2006/10/monads-field-guide.html
Very good
Yeah, thanks! 
Wouldn't this entirely depend on what the other players are like? Given a game of AIs that start out with knowledge that they are all start out with the same knowledge and are only interested in winning, depending on their decision theory you might get divergent diplomacy before a rule is ever proposed, voting blocks shuffling around in circles. Should we make the AIs care about the game being interesting and/or comprehensible to watch, or add players that are interested in such?
It was a bad example, I meant when some (or all) of the field are a functor, but it could be more than one.
&gt; Wouldn't this entirely depend on what the other players are like? Yep. That's what makes it interesting :) Also keep in mind that Nomic isn't about "winning", "voting", etc. The *initial* rules mention these things, but they're just designed a) to prevent trivial games (e.g. winning on the first move) and b) to be replaced. The "winner" is simply one who can engineer a situation in which they're a winner, *whatever* that might mean. It's open-ended.
Almost all of the time, simple types and functions solve the problem perfectly. Fancier types and functions are sometimes worth the cost for generic programming and DRY. I'm not sure it's the "best" idea and I'm pretty tempted to rip it out in favor of Template Haskell generation of functions/types, BUT... at my job, we have a Servant API description, and a bunch of CRUD helpers so it's easy to add endpoints for basic CRUD operations. One trick is that, for a given model like: data Foo = Foo { fooName :: Text, fooCost :: Integer, fooSecret :: Text } we want to provide a notion of an *update* to a `Foo`, where all fields are `Maybe`, and `Nothing` values mean "don't update the relevant model field." Furthermore, we don't want to allow any fields to be updated. We can manually write the boilerplate with data FooUpdate = FooUpdate { fooUpdateName :: Maybe Text , fooUpdateCost :: Maybe Integer } -- no secret! But this gets pretty dang tedious, and I don't *actually* want the record fields to be `fooUpdateName` so I need to customize the Aeson/Sawgger instances, etc. So instead, I used [`bookkeeper`](https://hackage.haskell.org/package/bookkeeper), and a type family which lets you specify the fields and their types. The type family instances end up looking like: type UpdateParams Foo = '[ "name" ::: Text, "cost" ::: Integer ] and you generate values for this like emptyBook &amp; #name =:? "the new name" This gives a type error if you try to update a param that isn't given in the allowed parameters. I've implemented `ToJSON` , `FromJSON`, and `Swagger` instances for the `Book` type, which means I can get a *lot* of this stuff for (nearly) free.
if you don't want to depend on those packages, and are already using `lens` or are familiar with it, there's `Lens.Plated` http://hackage.haskell.org/package/lens-4.15.2/docs/Control-Lens-Plated.html#v:children
I'm guessing the more experienced haskell uses here will have some suggestions for you but it might help to be able to see what you're doing with the strict records you are using. Seems like a cool project.
All I want to know is how to compose type-level functions. Apparently that's non-trivial in Haskell.
a natural number kind can track the length of containers. lists for example,`Vector 3 Double` is a 3D point, and you could write functions like `Vector (S n) a -&gt; Vector n a -&gt; Vector n a` that projects it onto a smaller-dimensional basis. i used to use `numpy`/`matlab` a lot, and even though "type safe linear algebra" isn't mature in haskell (or idris or any language, afaik), it's something that would have prevented "mismatched dimensions (i.e. type errors) after hours-long computations. ditto matrices, which just have a pair (or list) of peano numbers. records are useful for many things. database rows is one example. if haskell had true records (i.e. n-ary tuples with field labels), you could write a query that outputs the exact type, without explicit annotations of `:: (A,B,C)` or declaring a boilerplate `data`. i haven't used any of the "richly typed db api"'s like `opaleye` or `esqueleto`, but from skimming the tutorials, i think you compromise between inconvenience or boilerplate. ditto any product types that are external, like JSON. iirc, the original motivation of vinyl, written by /u/jonsterling, was "gradually typing"(?) web requests. http://hackage.haskell.org/package/vinyl-0.5.3/docs/Data-Vinyl-Tutorial-Overview.html 
Don't use lists. Use vectors or possibly mutable vectors
&gt; if you donÂ´t have a default value, you can not use any state monad whatsoever A default value is *completely* different from an initial value. &gt; I don't understand. There can't be forking since there's no asyncoperation here I must be misunderstanding. I thought `(&lt;|&gt;)` ran both halves no matter what. It only does it for things that use `async`? Only doing it sometimes is even harder to reason about... But I guess it solves that problem. &gt; It is the same semantic that the List monad And people don't use `ListT` for everything for good reason.
I believe there is a better solution than generic programming for the first two examples. This is not exactly what you asked for but I hope to convince you that you might prefer this: {-# LANGUAGE TemplateHaskell #-} {-# LANGUAGE DeriveFoldable #-} {-# LANGUAGE DeriveFunctor #-} import Control.Applicative import Control.Lens import Data.Foldable data A a = A a a deriving (Functor, Show) instance Applicative A where pure x = A x x A f1 f2 &lt;*&gt; A x1 x2 = A (f1 x1) (f2 x2) data B a b = B { _field1 :: a, _field2 :: b } deriving (Show) makeLenses ''B main :: IO () main = do print (liftA2 (+) (A 2 3) (A 10 20)) print (sequenceOf field1 (B (Just 2) 3)) This program outputs: $ runghc example.hs A 12 23 Just (B {_field1 = 2, _field2 = 3}) The only thing missing is a way to derive `Applicative`, which would simplify things further The first solution uses `Applicative` as a `Zippable` type class, which it is and `gzip` is just `liftA2`. Note that your hypothetical `gzip` already requires that all fields have the same type anyway so you don't lose any power by doing this through `Applicative`. For traversing or sequencing the `lens` library already provides exactly the right utility for this, which is `sequenceOf`. This lets you provide a `Traversal` pointing to any field(s) and it will sequence that field. What's nice about this is that you can change which field you want to sequence just by changing out the lens: &gt;&gt;&gt; sequenceOf field1 (B (Just 1) 2) Just (B {_field1 = 1, _field2 = 2}) &gt;&gt;&gt; sequenceOf field2 (B 1 (Just 2)) Just (B {_field1 = 1, _field2 = 2}) Also note that a proper sequence wouldn't leave behind an `Identity` (because that's not how `Control.Monad.sequence` works either). The field left behind by the sequence should be "naked", and `sequenceOf` gets this right Another nice thing about `sequenceOf` is that it works for nested fields, too: &gt;&gt;&gt; sequenceOf (field1 . traverse) (B [Just 1, Just 2, Just 3] 4) Just (B {_field1 = [1,2,3], _field2 = 4}) This is something that would be pretty difficult to do with generic programming, but with lenses it's much simpler The last example you gave is something that is something I consider an appropriate use of Haskell's generic programming
In my bachelor thesis I'm using extensible records with dependent types to create a small arithmetic language. Extensible records allow you to define the context of variables, like { "x": 10, "y": 15 } and extend it while you are evaluating the expression. So if you have the context { "x": 10 } when evaluating "let y = 5 in x + y" then you'd extend it into { "x": 10, "y": 5 } when evaluating "x + y". If you use dependent types you get nice compile-time checks that every free variable of your expression is in the context (so in the above case, you'd have a proof that if you need to evaluate "x + y" then both "x" and "y" belong to your context). Basically, without dependent types/type level programming then 'evaluate "x + y" with { "x" : 10}' would type-check but fail at run-time, while with dependent types/type level programming it'd fail at compile-time. I know it's not a "real" application, but I believe it can be useful. Hopefully when dependent types and dependently-typed languages (e.g Agda, Idris) become more mainstream we'll see more real applications of these concepts
This uses Frege, no?
I know I'm a week late here, but: I'm the editor of [Haskell Weekly](https://haskellweekly.news/). What other sources would you like to see? I follow about 100 Haskell blogs. Most of the good stuff ends up on this subreddit. 
While fun for the student, there tends to be very little pay-off for the community in such projects. Since we have limited funds we have to be careful how we spend them.
Heterogeneous lists are useful for schemas and similar things, especially if you want to avoid run-time checks in the common case but still be type-safe.
`~&gt;` is (defunctionalized) [type function](https://hackage.haskell.org/package/singletons-2.2/docs/Data-Singletons.html#t:-126--62-) from `singletons`, and `(:.)` is composition for it. &gt; import Data.Singletons.Prelude &gt; :k (:.) (:.) :: (b ~&gt; c) -&gt; (a ~&gt; b) -&gt; a -&gt; c &gt; :k (:.$) (:.$) :: (b ~&gt; c) ~&gt; (a ~&gt; b) ~&gt; a ~&gt; c &gt; :kind! (IdSym0 :. IdSym0) Int (IdSym0 :. IdSym0) Int :: * = Int
&gt; Though I wish there were something that could lift from any level in the stack What exactly do you mean by this? Both of these things *can* lift from any level in the stack, no? That seems like the entire point of typeclasses like these. Do you mean things that could â€œpeel offâ€ layers of a stack without needing to drop all the way down to the base monad when lifting things? That does seem theoretically useful in an abstract sense, though I admit I canâ€™t immediately visualize a situation where I would want such a thing that also has a straightforward solution.
If this makes *your* head leak, what hope do we mere mortals have?
Is vector (//) operator is faster than O(n)? Or much faster than array? I shoul try then.
Is there a particular definition of tensors that you like? For the (physics) applications I have in mind, you would want to distinguish covariant/contravariant, but it would suffice just to have it as a tag at the type level. Like `data Tensor a : (Vect n (Nat,Bool) a) -&gt; Type`.
Awesome, thank you! I'm going to use this for my compiler design project.
"Has science gone too far?"
Runtime complexity may not be as important as memory locality. That however depends on how big your lists get but usually you will get cache misses very fast and they are expensive. Thus lists will be slower than unboxed arrays most of the time even if you have to replace a lot of elements. You could also check whether you can vectorize your code. I don't know what the state of `DPH` is but my first intuition says it could be suited for evolutionary algorithms (i.e. write a vectorized fitness function).
OK. The main simulation function looks like this. a lot of maps, filters, array updates, and finally update of Gstate. moveState :: Gstate -&gt; Gstate -- just moves the state to the next turn moveState s = s {turn = 1 + (turn s), ships = nships, balls = map (\b -&gt; b{ count = decf count b}) restBalls, mines = restMines, barrs = prizebarrs ++ restBarrs, idc = nidc} where nidc = (idc s) + length prizebarrs prizebarrs = map (\(sh, i) -&gt; Barrel i (spos sh) (min (hp sh) rewrum)) $ zip dead [(idc s)..] dead = map (\sh -&gt; sh{hp = hp ((ships s) ! (sid sh))}) $ filter ((&lt;=0).hp) damaged nships = (ships s) // (map (\sh -&gt; (sid sh, sh)) damaged) nballs = map (\b -&gt; b {count = decf count b}) restBalls damaged = map damage $ zip stage2 stage3 damage (sh2, sh3) = if tdamage &gt; 0 || healing &gt; 0 then sh3 {hp = nhp} else sh3 where nhp = if hp sh3 - tdamage &gt; 0 then min 100 (hp sh3 - tdamage + healing) else 0 healing = sum $ map hpgain collected hpgain b = if took then rum b else 0 where took = any (== bpos b) [spos sh2, bow sh2, bow sh3, stern sh3] tdamage = bdamages + mdamages mdamages = sum $ map mdam exploded mdam m = if dr &gt; 1 then 0 else if dr == 1 then neardmg else minedmg where dr = ranges (shiparea sh2 ++ shiparea sh3) [mpos m] bdamages = sum $ map bdam landedBalls bdam b =if center then highdmg else if side then lowdmg else 0 where side = stern sh3 == cpos b || bow sh3 == cpos b center = spos sh3 == cpos b -- more code like this Sorry for awful code (it's a sport), but I hope it gives some idea on what is happening. A number of records and lists are filtered, mapped, and updated. Wher in java they just decrease some cooldown, I have to map over a list with cooldown decrease function. Really don't see what can be improved
Somewhere else they say, that only one core is available anyway. So it will not help.
I don't think they want to completely break from Hackage. They'll still use packages from Hackage, but packages not on Hackage would also be discoverable in Stackage. Ideally, those packages should be fed back into Hackage as well.
`5 / (5 / x) = x`, not `1 / x`.
I don't think you can say lens and generic programming are two separate things. Lens treads into generic programming territory with mkLenses and plated, and generic programming packages like traverse-with-class and my own one-liner use the normal Applicative, Traversal and even Profunctor idioms, which compose just as well (in the same way even) as lenses. 
I disagree. Practical projects are a better fit for students. The type theory that powers GHC is by no means trivial, and there's only two months available for the student. Stuff like the Hackage matrix, or a modernized Haddocks UI, or ... provide a tangible result to the community that pulls it forward.
A really nice Stack feature that already exists is adding Git repos as dependencies. So I really don't understand what the point of this would be.
For the distracted, this was an April 1st post.
Now, it would make sense for it to be in the blog post as well! (As you've already noticed anyway, better than describing the Vector/List type with size in type which is pretty much dependently typed "Hello, world", so covered in many places).
&gt; A default value is completely different from an initial value.A default value is completely different from an initial value. Wait, there is a error on my side: I wanted to mean an initial value. Not a default value. As happens with the state monad, once the state is set for the first time, this value is never used again. &gt; And people don't use ListT for everything for good reason. My guess is that if ListT would bring effects of parallelism, concurrency, solves the callback hell, perform streaming and reactivity and so on then the programmers would consider it as an extraordinary tool for reasoning and a extraordinary simplification of programming. You can use your prefered monad stack for your purposes. Suppose that you want to use transient only for distributed computing but you want to use your prefered stack. then you can use localIO to "erase" the transient stack. For example: result &lt;- runAt node $ localIO $ computation with my stack So you can execute your remote program written the way you want and get the result back. Also you can also restrict it to permit the effects that you want in your DSL, by using monad transformers over TransIO or Cloud. You can for example export the snippet above as: remote :: Serializable a =&gt; Node -&gt; MyStack a -&gt; MyMonad a remote= MyMonad . runAt node . localIO . runMyStackIO So you and your team can work as usual. Something similar can be done to add other effects that you may need. It is not a election of everithing or nothing. 
Aren't those supposed to be funny though? This looks like a thinly concealed rant related to his obsession going up against Hackage and the PVP.
&gt; We need to construct a value Vector m a, and we have been given a value Vector m a. BUT â€“ we donâ€™t know what m is! We know it's a `Nat`, so couldn't we see whether it's `Succ (Succ Zero)` and switch the two values in that case?
Can your one-liner package answer any of my questions, and how ?
I imagine Plated is just the lensed version of plate. However, even though I feel that plate my be the solution to some of my problems, I've never been able to actually understand what you can do with it and how to do it. Anyway , it goes back to my initial question, which are the package/method to use today for that type of problem.
Well the state monad should help in reducing memory usage Not sure about saving processing. Maybe some of these lists done need to be updated every time. You might benefit from using arrays over lists too
Thanks for the feedback. Just to spell out the downside (and the reason I've avoided doing this so far): this could create a situation where code doesn't work as well for Hackage/cabal users any more. For example, if the leftpad\* package was added to Stackage only, and then packages available from Hackage started depending on it, someone using `stack build other-package` would be fine, but `cabal install other-package` would fail miserably. There are [potential workarounds](https://twitter.com/snoyberg/status/857221247469596677), but they would still require effort from cabal users. As to discoverability: it would be possible to discover non-Hackage packages on stackage.org once they make it into a snapshot. So by that metric, this change could be considered an improvement. On the other hand, with centralized package storage on Hackage right now, it's very unusual for someone to keep code _only_ on Github if it's intended to be used by others, so I doubt we'll see much net benefit in discoverable packages (if that logic makes sense). I'm dubious about this being a good direction to take things in. I started this poll since the intersection of some routine Stackage maintenance and an experiment I did in PureScript made me realize how technically feasible this change was, and people (on both the Hackage and Stackage side of the divide) are regularly asking me about doing it, so I wanted to get broader input. The input has IMO been mostly _against_ any change. \* Yes, I'm making a Javascript joke
disclaimer: I did not watch the talk/video - will try to do later the example you gave is just the lambda-calculus representation of booleans (basically you encode `if_then_else_` ) I don't think there will be any practical benefits to this. It's nice from a theoretical standpoint, as you can build up from just functions.
That project sounds like the exact contrary of careful usage of money. Stackage has been doing a much better job of keeping the (maintained part of the) ecosystem building for a few year now, in a very respectful and open manner. This is basically a vanity project for the committee and the usual pointless obsession with PvP and upper bounds.
Intero + [Intero-Neovim](https://github.com/parsonsmatt/intero-neovim) + Neovim is also an excellent option.
If you interact with the outside world at every step, use the IO monad instead of ST. You should have a look at the IOVector type from the vector package and use these instead of lists. This is what you really want for a game simulation and there is nothing wrong or non-idiomatic about mutable arrays. Also there is rarely a good reason for a game state record to have non-strict fields. 
My vote is against. Potentially conflicting package names seems like a nightmare. 
The issue appears to be only the dependency bounds. Serious question: why doesn't stackage ignore those? Fuck those bounds, they are useless so ignore them! They are useless for the purpose of stackage. Stackage is all about "it compiles therefore it's fine".
Applicative was not a superclass of Monad before 7.10, so if you wanted to use Applicative (or Functor) specific functions, you had to explicitly specify it even if you already had Monad constraint. As it was quite annoying, custom Monad* classes would work around that by including Applicative in the mix.
I agree on all those points, and I am pushing for it to come out of a testing phase and to have the code public, to allow community involvement. I understand that this should happen soon.
That's true! The error messages issue is quite a nice benefit of MonadIO.
I voted for "Keep requiring Hackage" for `cabal` compatibility. One of the nice features of Stackage is that every resolver comes with a `cabal.config` file [like this one](https://www.stackage.org/lts-8.12/cabal.config) that you can dump into your project. This lets you use `cabal` with the package set that Stackage would select. However, this only works if Stackage is restricted to packages on Hackage. If you want to do this, I'd suggest first extending `cabal` to support Github dependencies so that Stackage could preserve `cabal` compatibility