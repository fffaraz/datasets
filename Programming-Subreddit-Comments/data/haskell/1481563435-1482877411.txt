The Author recognises this: &gt; ... the typical workaround for this problem in Haskell is to define a newtype, which lets you turn off the default shrinking for your types and possibly define your own If one allows constraints on the Generator it makes sense allow them to apply to the shrinker, ideally without writing a custom one. Not sure if there is already a way to do that in QuickCheck though. I have only used QuickCheck for fairly trivial cases so far.
Even without defining a new type, there is `(==&gt;)` for rejecting inputs that don't pass a runtime check without failing the test. &gt; quickCheck (\x -&gt; even x ==&gt; x &lt; 4) *** Failed! Falsifiable (after 14 tests and 1 shrink): 4 You will never see it print an odd number, even after shrinking.
My objection is not to the content, but to the rhetoric. When you come up with a new idea that's good, I can understand the impulse to say "old bad, new good" but it's also really alienating. It's more like "old was pretty good, new is _even better_". Encourage the switch -- but there's no reason to be ashamed that we didn't have the new idea earlier. Progress happens a little at a time.
Is the problem that people are not heeding QuickCheck's [own advice](https://hackage.haskell.org/package/QuickCheck-2.9.2/docs/Test-QuickCheck-Arbitrary.html)? &gt; A final gotcha: we cannot define shrink as simply shrink x = Nil:genericShrink x as this shrinks Nil to Nil, and shrinking will go into an infinite loop. &gt; &gt; If all this leaves you bewildered, you might try shrink = genericShrink to begin with, after deriving Generic for your type. However, if your data type has any special invariants, you will need to check that genericShrink can't break those invariants.
This is also discussed in the QuickCheck repository: https://github.com/nick8325/quickcheck/issues/130 --- Specifying generators using `RoseTree`is tempting, as you get shrinking function for free: you don't need to write them! On the other hand, as Nick points out, that approaches doesn't work (well) for monadic generation, only for Applicative one. Fortunately, you can write a library so your test would look like propEvenSmallerThanFour :: Property propEvenSmallerThanFour = rForAll r $ \x -&gt; x &lt; 4 where r :: R Int r = fmap (*2) rarb rForAll :: ... =&gt; R a -&gt; (a -&gt; prop) -&gt; Property rForAll r = forAllShrink (rGen r) (rShrink r) (Not sure if `disorder-jack` could be adopted for this) Even so, I haven't experienced a huge need for non-structural invariants, so generic shrink (and generic generators for non-recursive data) were enough. Yet, I have written simple web apps mostly :) BTW, soon, with `recursion-schemes` providing Template Haskell deriving of base functors, and `QuickCheck` getting `Arbitrary1` class, one can write decent `gRecursiveGen` without too much manual boilerplate! 
Oh whoops, how can I take steps to get it on stackage then?
It's already on stackage: https://www.stackage.org/package/distributed-process
Maintainer here. Thanks for the reminder. I'll look into these pending changes and work towards getting them released!
This is really neat, though I get the sense that many of their difficulties go away in the presence of -fdefer-type-errors.
I see this a lot in C# code and I think it is because programmers are loathed to create a new type when there is one you can use off the shelf. It is unidiomatic to create an alias for a Boolean. Even more so in C#/Java etc. where you can't create an alias but need to wrap something in a new class! But I like the idea of creating explicit types even for the most simplest of things, event to replace a boolean. I think it helps with the 'make illegal state unrepresentable' paradigm - you can't pass in the 'wrong bool' if the argument doesn't take in a bool!
Thanks! (For posterity: https://two-wrongs.com/haskell-time-library-tutorial )
right, it's a ghc7.10 release on stackage though
Thanks!
Underwhelmed by the MBP release announcements I decided to embrace the dark side and picked up a W10 laptop... I planned on booting Linux, but so far so good with Haskell under MinGW, so here's an appreciative user!
&gt; Perhaps someone more famous having the same opinion I do makes it more valid? Nope.
Yes, exactly! `(Void, a)` has no values, so we always know what it is, namely a non-representable value. `((), a)` is the same as `a`, so we don’t make anything less clear in terms of what values can be represented by needlessly adding a `()`.
&gt; How do you say at a call-site which version you want used? You don't. There's only one version. I must not be explaining myself very well. I'm suggesting that you could write foo :: (a -&gt; Int) -&gt; (a -&gt; Int) foo :: (Int -&gt; a) -&gt; (Int -&gt; a) foo = id and `foo :: (Int -&gt; Int) -&gt; (Int -&gt; Int)` would be inferred because that is the most general type compatible with its implementation and all given type signatures. That would be its only type. Does that make sense?
The problem with all these attempts at records is that type inference really doesn't work very well. I can't see any alternative than actually having record types in the language.
I'm curious about the performance difference between the C parser and writing the same parser in Haskell. It seems like a perfect opportunity to study writing performant code in Haskell, with C being the benchmark.
While I do believe that it's fast - how much faster is it than the alternatives? I could not find any benchmarks.
/u/ndm uses conduit last I heard, so if anything, you'd be more likely to get a conduit interface. Another idea would be to use [streaming-commons](http://hackage.haskell.org/package/streaming-commons). Then it's trivial to add thin wrappers that provide the original list-based interface, plus interfaces for conduit, pipes, and any of the other streaming libraries.
Good point. Also, note that /u/ndm uses Windows. So the common problem of portability of C libraries to Windows is not likely to be a concern in this case.
You don't even need to depend on streaming-commons for that, you can easily implement a streaming-agnostic API by encoding the [state machine as an ADT](http://hackage.haskell.org/package/zlib-0.6.1.1/docs/Codec-Compression-Zlib-Internal.html#g:2), or see [this example](https://hackage.haskell.org/package/lzma-0.0.0.3/docs/Codec-Compression-Lzma.html#g:3) which allows to add thin wrappers for pipes, io-streams, conduit etc quite easily.
That's exactly the point of streaming-commons - so that you don't need to re-invent that wheel in every library. 
My main issue has been reliable profiling information. For instance, for the character table I was originally using an Array of a union type, and it took me a while to switch to an UArray Bool. The CCS profiler didn't help me realize how slow the Array reads were, since -prof completely changes the performance characteristics of the code. 
Is it just me or is this making Haskell look like Perl?
Rawr is my favorite package for records like this. Unfortunately it only allows records with [up to 8 fields](https://github.com/PkmX/rawr/issues/2). That's probably easy to fix with Template Haskell, though.
I haven't tried it out but it looks promising - in fact it looks exactly like what I wanted. Although I did say in the prompt that I'd prefer that it uses standard Haskell and no external libraries because I'd have to try and build the dependencies as well. Having a lot of dependencies means a larger apk which is bad. But I'll try it soon - this week is finals week. 
This makes more sense. I did not expect that "Unit is not very blind" would imply a blindness of 1 for `()` (but rather 0 or a very small number).
Not quite, -fdefer-type-errors essentially replaces the ill-typed term with undefined. While this does allow you to run the program (which is great!) it doesn't help explain the type error in terms of a runtime error, which was our goal in the paper.
There's also mine :3 http://hackage.haskell.org/package/labels There's a fair bit of documentation about it in the README and linked pages https://github.com/chrisdone/labels/blob/master/README.md `rawr` is similar to `labels`, but has the `R` wrapper and some other types, `labels` is just plain old tuples.
labels supports up to 24. Also achieved with template-haskell.
In PureScript you write `record.field` to access a field of a record.
I think it's fine so long as it's something you have to specify. In the language I'm designing. type Ex = [name :String] -- foo : Ex -&gt; String let foo = { r -&gt; r.name } Here foo infers a concrete type (of course if there were two records with a name field you'd have to give the type.) let foo : [ name : String | _ ] -&gt; String = { r -&gt; r.name } Since I have specified the type, this should be allowed.
It's just not you. Even though extensible records is one of the few thing I really need and Haskell is lacking, I still can't resign myself to use any of those packages. They just don't feel right (a bit like TH ...) 
Not being funny, do you actually use labels (intensively I mean) or is just an fun exercise ?
https://github.com/pepeiborra/bytestring-xml/blob/master/Bench.hs#L32 I think what you've created there is a real buffet of tricky benchmarking issues: let suite = [ env ((,) &lt;$&gt; BS.readFile p &lt;*&gt; readFile p) $ \ ~(bs,s) -&gt; bgroup p [ bench "hexml" $ whnf (process . Hexml.parse) bs , bench "bytestring-xml" $ whnf (process . parse) bs , bench "xml" $ nf parseXML s , bench "xml-conduit" $ whnfIO $ XML.readFile def p ] | p &lt;- paths ] 1. `BS.readFile` is strict, so you'll have a full vector in memory before you start parsing. 2. `readFile` is lazy, so the first time you parse will cause IO to happen, possibly introducing outliers and causing that 823ms standard deviation. 3. The `nf parseXML s` correctly evaluates the full structure, whereas `whnfIO XML.readFile`and `whnf (process . Hexml.parse)` only force the head, not the full structure. We don't know whether it's fully forced inside or not. 4. "xml-conduit" reads the whole file every single iteration, all the others only read it once. I think the only way to make meaningful benchmarks is to separate IO from pure code, and to fully force all input data structures before-hand, and to fully force all output data structures afterwards. Alternatively, you benchmark reading the whole file in every single time for each library, but that's likely to introduce more variation in the results.
Thanks Chris. About 1. and 2. I believe that Criterion's env forces the whole environment to nf before starting the benchmarking, at least that's what the documentation and the NFData constraint suggest. The deviation is more likely to come from the 5w laptop CPU, seems to mostly go away when I run the benchmark in the desk. Re the other points you are right. Fully forcing the DOM tree is unlikely to reflect real world usage, but any option that forces only partially can be constructed to favour one library or another. And indeed, the benchmark for xml-conduit is completely wrong, I will disable it for now.
What is the actual use case of extensible records. I am asking because, I use to think I needed them and realize now, that a bit like big tuples, it's an anti-pattern and there is usually a better way. My best way at the moment it to create a big record with all field I need and "turn off" the one I don't need with type families. Obviously my use case might not be the same as everyone, so I'm really interesting in knowing why you really need them.
A good extensible record system would make binding to OO code a zillion times easier (and nicer to use). I looked at a bunch of packages and ended up rolling my own totally specialized for my needs because the error messages were terrible or they restricted the record size.
I would love to see this for Haskell too! The two challenges for Haskell are laziness and type classes. Laziness means that an error could be hiding in a thunk under some constructor, so you would need to either `deepseq` the final value, or search for a context in which the value is fully demanded. The issue with type classes is that you can end up at a point where you're applying a class method to a hole, and now we can't know which instance to pick. I think full symbolic execution (as opposed to the greedy instantiation we do for OCaml) would solve the problem here, because you'll eventually learn what type the hole has, or you'll quantify over it and can pick any instance.
The pattern match of binds `s` such that its type has a `"foo"` tagged member, I believe.
I see, so `GotThat s -&gt; print (get #bar s)` won't type check.
&gt; you just add a column for each subterm and reuse the previous column in the next one. You're just describing an ordinary pipeline. I don't see the case for record extension specifically. Certainly you can use record extension to solve these problems, but they seem to have other equally maintainable solutions.
You can change compiler user the `--compiler` flag. Eg. `--compiler ghc-8.0.1`.
Maybe TH and libraries like vinyl don't *feel* right, but there sure are helpful for a lot of real world tasks.
I use Frames/vinyl for a 150 field monstrosity.
Querying a database with 200 columns. Writing functions that with with any row that contains certain columns rather than duplicating logic.
QuickCheck isn't the same thing as AFL, although it can "fuzz" in the sense of "throw a ton of random inputs at it and see if shit gets bad". Fuzzers like AFL are path-guided: they watch the program and see what paths it takes, on a given input, so they can then mutate the input to try and get to new paths. This forms a feedback loop, and it's why tools like AFL are incredibly effective: because they try to find inputs which allow them to take a path like "Copy this input into this buffer", even if there may be an `if` statement guarding the copy. There is [QuickFuzz](http://quickfuzz.org/), which is written in Haskell, and a real fuzzer. But it's not meant for Haskell programs, it's meant to be a fuzzer written in Haskell for other things. You'd use QuickFuzz much like AFL, for example, to fuzz this library by generating XML inputs. I believe QuickFuzz does do path-monitoring in order to determine how to mutate the input. I don't know how you would apply AFL-style path guidance to regular Haskell executables, because self-mutating code due to thunks, and the compilation model, means that the CFG the monitor has to watch is incredibly complex, so the state space of possible inputs explodes dramatically. That's my intuition, at least. It would be an interesting research topic to try and make something like AFL work on GHC-compiled code.
I would use memory-mapped IO to parse the XML without allocating strings. Tokenization would just be allocating offsets into this buffer. This package looks interesting for accomplishing this : https://hackage.haskell.org/package/mmap-0.5.9
I keep wanting to use vinyl but compared to many of the other libraries, it *really* needs a facelift. Being able to use type-level lists to describe your record type is *really nice*. But all the examples I've found of vinyl show a much less beautiful syntax with tons of boilerplate.
Just curious, from the docs... -- Disregard upper bounds involving the dependencies on -- packages bar, baz and quux allow-newer: bar, baz, quux -- Disregard all upper bounds when dependency solving allow-newer: all What happens if someone puts a package named `all` on hackage?
ContT, [The Continuation Monad](http://www.haskellforall.com/2012/12/the-continuation-monad.html), might help. 
If frames didn't use TH to generate these I probably would have been more reticent to use it.
That's impressively lasting impact for a tutorial, apologies! The `Frames` equivalent is, ``` type Foo = Record '["a" :-&gt; Int, "b" :-&gt; Bool] ``` The Vinyl equivalent is, ``` type Foo = FieldRec '[ '("a",Int), '("b",Bool) ] ```
...You may be on to something. I think I have just re-created `anyWord8` and `take` from `Data.Attoparsec.ByteString`... I should not be programming this late. Thanks.
Well that's much more palatable! Is there an updated tutorial I can use which makes this kind of usage more front and center? Might I suggest updating the tutorial on Hackage?
`type Splitting s a = StateT s Maybe a`, no?
That's really cool, and it's great work. Making `POSIXTime` faster is good. But the real problem is `NominalDiffTime` and `DiffTime`. These, especially the former, are very common in time calculations. Not only are they using `Integer`, they are using `Data.Fixed`, which introduces many extra `Integer` multiplications and divisions in places you might not expect. Wouldn't it be nice if we could actually select between some different options for these types? Say, using a type parameter. We could select a 64 bit representation, giving a range of tens of thousands of years at 10 ns resolution, good enough for almost every real-life application, and super fast. Or a 128 bit representation like the one being worked on for `POSIXTime`, which covers the rest. Or the current infinite precision when correctness proofs are needed.
I can help with that tutorial, if you need it. (@sboosali on github). Also, I played around with a few IsLabel instances when GHC8 came out, for a "vinyl frontend" specialized to ElField. https://github.com/sboosali/eight/blob/master/sources/Eight.hs But I think something like the following (untested) might be the best wrt convenience and inference: instance (KnownSymbol s) =&gt; IsLabel s (Proxy s) where fromLabel _ = Proxy type F s a = ElField '(s,a) type P = Proxy type (s ::: a) = '(s,a) (-:) :: forall (s::Symbol) (a::Type). (KnownSymbol s) =&gt; Proxy s -&gt; a -&gt; ElField '(s,a) _ -: a = Field a -- concrete proxy to resolve islabel instance infix 8 -: -- more tightly than :&amp; (&amp;) :: F s a -&gt; Record fields -&gt; Record ((s ::: a) ': fields) (&amp;) = (:&amp;) infixr 7 # -- as tightly as :&amp; -- "empty" e :: forall (f :: k -&gt; *). Record'[] e = RNil r = #a -: 0 &amp; #b -: True &amp; #c -: 'c' &amp; e -- (Inferred):: (Num a) =&gt; Record '[ "a" ::: a, "b" ::: Boolean, "c" ::: Char ]
Yeah, that scared me away too. But then I read through the package (it's pretty small, but yeah having to read the source code is lame). You don't need a type-level enum to make a new "universe" of keys, you can just use type level strings as keys like every other records library. e.g. type LifeForm = ["Name" ::: String, "Age" ::: Int, "Sleeping" ::: Bool] -- type level association list type (:::) key val = '(key,val) -- type level pair 
Do you mean the description of the update monad [for parsing](https://www.schoolofhaskell.com/user/edwardk/heap-of-successes) /u/edwardkmett wrote about a year or two ago? (Which, to _vastly_ oversimplify things, is like a `WriterT/ReaderT` combination where the reader data is modified by the monoidal value accumulated in the writer).
I just don't think its very natural. I get what you're sort of doing -- for every result in g, "forward" that to the result of looking it up in `f`. But of course many things may not forward properly since they might not be in the keyset of `f`. So you end up with a lot of `Nothing`s. It just doesn't feel nicely algebraic to me without some further wrapping, etc. That said, I think you can write this much more efficiently (and concisely) as `fmap (f!) g` (or, perhaps `(f!) &lt;$&gt; g`
You can use map (g !) f EDIT: I got it backward. It should be map (f !) g.
Under the constraint (Ord a), Map a b is isomorphic to a -&amp;gt; Maybe b, which are Kleisli arrows for the Maybe monad and compose just fine. Sounds pretty algebraic to me :-) EDIT: On mobile, sorry for the lack of formatting...
Here's one very specific use case: web app middleware. It's really useful to keep a 'context' of all the data attached to a single request as it passes through layers of middleware and handlers. Many languages are either dynamically typed (so there's no way of knowing for sure what's in the context at any given moment) or encourage you to use dynamically-typed capabilities (Go's `Context`). I've always wanted to be able to, for example, write middleware that specifies in its type "adds a `SignedInUser` to the context", and write a handler that says "I need a `SignedInUser` in the context". You _could_ do this with a non-extensible record full of `Maybe x`s, which is certainly more type-safe than 'throw it all in a `map[interface{}]interface{}`', but doesn't capture all the statically-available information it could. Spock actually [does this](https://www.spock.li/2015/08/23/taking_authentication_to_the_next_level.html), but in a way I don't find as clear as it could be. (I'm sure Servant can do it, too, but I haven't looked into it.) Elm used to have extensible records that would be perfect for this use case, but a) it's not a server language and b) the record extension syntax was removed IIRC. EDIT: actually it seems you can [update records](http://elm-lang.org/docs/records#updating-records) in Elm, which includes adding fields or changing their type. EDIT: I wrote &gt; dynamically typed (so there's no way of knowing for sure what's in the context at any given moment) But it's more accurate to say "there's no way of knowing statically what's in the context".
[E-reader friendly version](https://github.com/beerendlauwers/haskell-papers-ereader/blob/05c3bc023072a6fc5b1a09e11b5f59298fa8cc2b/papers/Dynamic%20Witnesses%20for%20Static%20Type%20Errors.pdf). Figure 4 is a little weird, but the rest looks fine.
That's pretty neat indeed. However it looks like it is exactly what implicit parameters are for. Why not use implicit parameters then (as it also have the advantage that it can work outside the context monad)?
This is unsafe. It will crash if some k maps to a value that isn't in the key set of g.
This isn't exactly what you asked, but since we're on the topic of maps and syntax there are a couple pieces of related syntax that I know of. The first is the [`map-syntax` package](http://hackage.haskell.org/package/map-syntax) that leverages do notation to define maps more concisely than the list of tuples approach. The second is [the `(=:)` function](http://hackage.haskell.org/package/reflex-dom-0.3/docs/Reflex-Dom-Class.html#v:-61-:) that lets you define maps like this: `(keyA =: valA &lt;&gt; keyB =: valB)`.
&gt; Our system is based on the C++ Thrift server, which provides a fixed set of worker threads that pull requests from a queue and execute them. Possibly a naive question - I assume Sigma presents itself as a Thrift RPC endpoint to other FB services. If that's the case why is that interface not also written in Haskell? Wouldn't that avoid the whole C++ to Haskell scheduling overhead and just let you use lightweight threads directly?
That's what OP asked for. He didn't ask for the resulting values to be Maybe.
The values don't have to be maybe for the composition not to crash...
Thanks! It'd be nice to have similar functionality available in Haskell, even if only a FFI example of what you describe above.
Let's go to the stuff that matters: How Facebook can be defined in terms of Category Theory? 
In any case, his expression and mine compute the same thing. I'm not trying to fix his expression to not crash. EDIT: A non-crashing version would be: mapMaybe (`lookup` g) f
Aaah, I didn't know of mapMaybe in Map. Yes, that would do and is much better than what I proposed.
As a challenge: https://github.com/nick8325/quickcheck/pull/136 See the test, generating simple lambda calculus expression. The invariant is that they are well typed. This example is still trivial to convert to GADTs, so indeed `Exp` would be well scoped and typed, but how about much more complicated languages? And the use case is to QuickCheck the optimiser passes: when you start with well typed expression, optimised one is well typed (same type), and reduces to the same normal form (assuming we generated total expressions only)
I have benchmarked it, but on a corpus of documents matching one particular use case (which is what I really want to use it for), and my benchmark both parses and extracts some relevant data. They aren't good benchmarks that would convince anyone apart from me. 
yes and no. SMT solvers have been tried to do that, and they kind of work. But: - controlling the quality of the solution is hard: which plan is better, how to encode that in a SMT language. - SMT (at least was) much quicker to say no when there aren't any solution, but then it also failed to say why it's so (e.g. conflict set) IIRC /u/kosmikus has plugged Z3 into Cabal, and also explored other solver formalism(s) for the problem.
Very cool. It's always good to read tales from the largest scale Haskell shop in the world.
Simon, if it's not too much trouble could you please add an RSS feed to your blog? Here are the GH docs: https://help.github.com/articles/atom-rss-feeds-for-github-pages/ Thanks!
This is definitely a simpler variation compose :: Ord b =&gt; Map b c -&gt; Map a b -&gt; Map a c compose a b = Map.mapMaybe (flip Map.lookup a) b
Is this the right thing? http://simonmar.github.io/atom.xml
A use for the `witherable` package!
The proof is not by Russ Cox; it has been known since at least 2005 as he notes in the article.
Yes, except the entry summary tags are empty. Ideally they will contain the full text of each post (not an excerpt). Thanks! Also the feed could be added to Planet Haskell.
I'll happily accept a pull request: https://github.com/simonmar/blog
I've always had terrible experiences with parallel garbage collection - making programs go 10x slower without any difficulty at all. I usually turn it off. Does FB have better experiences than that?
Looks like a "parser" monad to me.
What's your motivation for trolling /r/haskell? Did `unsafePerformIO` kill your first born child or something?
Hmm, perhaps I don't understand specifically what you're after then. If you're just mapping a single input to output, that's just function application. If you're building a large structure incrementally, ie. by assembling it from pieces, then you don't actually want to extend it a field at a time because that has terrible memory behaviour due to constantly creating intermediates that are immediately thrown away. So you want an abstraction that captures each extension and its dependencies, and then initializes the final result all at once (unless you want to rely on fusion to eliminate the intermediates).
The details of particular package managers vary, but the assumptions 1-4 made in the article are pretty standard, e.g. see this from 2007 https://cseweb.ucsd.edu/~lerner/papers/opium.pdf
To get good reports, you'll need to fully set up AFL and write some code to make the fuzzer fast. I'll post an issue on the bug tracker with the exact setup I used... &gt; The memcpy and memchr is exactly what I'd expect in performance sensitive code. Well, it's more *the way* they're hanging around that makes me suspicious... I can't really intuitively explain it very well, other than by saying "I've been attacking C programs for a while" and it stood out :)
How to turn parallel GC off? by build/run with ``+RTS -N1``?
I thought that at first, but I changed my mind after looking at the author's history on the haskell subreddit.
`a` is a `Map b c`? and `b` is a `Map a b`? I don't think my coding style and your coding style would get along.
`+RTS -qg`
Russ Cox claims that his proof is new in that it applies to a broader class of package managers than previous proofs. I did not check his claim but currently don't see a reason to doubt it. The new proof is supposedly more general because, unlike previous proofs, it only uses very basic standard assumptions on package managers. 
It did find a consistent pattern that I had got wrong, which was the cause of about 5 potential overflows (although in truth, a 1 byte trailing read, so not the worlds worst issue). None were near the memcpy or memchr - but we'll have to see what else it finds.
You need a profiler that pauses and walks the stack/PC every N ticks if you want even remotely accurate profile information for something like this. I profiled my C in Visual Studio which has that, and it worked nicely.
Oh yea I would never commit that code lol. Needs better names.
Wow. It's always best when the tool builders get to apply their own tools in practice. Many people were afraid about loosing Simon to Facebook but the results are amazing! 
You should doubt that it's a new proof, because (a) it follows basically immediately from the assumptions, and (b) the given assumptions are both old and standard. For example, a few minutes of Googling turns up [a version of the same proof](http://stackoverflow.com/questions/28099683/algorithm-for-dependency-resolution) someone posted on Stackoverflow two years ago. (Sometimes people really do overlook something obvious for years on end, but just not in this case.) 
Any good paper or intro to anonymous records?
Thanks to help from /u/aseipp I ran AFL a lot. I found a few issues where I could walk over the end of a character buffer by a few characters, all of which have been fixed in v0.2. It's now survived 15M+ executions through AFL without error, so it seems at least plausibly not terrible.
Does anyone have a link to the recipe how the stack fully static binary is built. It talks about avoid libgmp and distro problems so I'm wondering if it's using musl for the rest of it and if this also means that the stacks'a ghc binaries are also fully static musl variants that run on any Linux 2.6+ kernel.
&gt;For the next stack release after this one, we are planning changes to our Linux releases, including dropping our Ubuntu, Debian, CentOS, and Fedora package repositories and switching to statically linked binaries. See #2534. Note that upgrading without a package manager has gotten easier with new binary upgrade support in stack upgrade (see the Major Changes section below for more information). In addition, the get.haskellstack.org script no longer installs from Ubuntu, Debian, CentOS, or Fedora package repositories. Instead it places a generic binary in /usr/local/bin. I'm concerned that while this may reduce maintenance overhead, this is the wrong move for Haskell adoption. Most (all?) popular language frameworks / runtimes are packaged for standard package managers. If I put myself in the shoes of a newcomer to Haskell, I would be wondering why I need to install some binaries that aren't managed by my package manager, and this could make-or-break my decision to try it out. Can we put this up for more community feedback?
https://www.fpcomplete.com/blog/2016/10/static-compilation-with-stack
Not sure if we're on the same page. I'm talking from the perspective of a linux user who might not be a fan of installing random binaries from the internet but prefers to install software using their package manager.
[removed]
I agree that being able to use the package manager is prefreable. It's not as bad as it could be though because stack is just one file to place anywhere on the PATH.
I'd assume that the servers are dedicating the majority of their time to computation after decoding the request, not 'request-handling'.
Thats using musl libc not glibc, the difference in a fully static stack is 44.4MiB vs 44.0MiB dynamic. And yes ghc could use some slimming down in code size.
Great work. Not an expert on GC, but it seems to me that rather than parallel GC the problem of haskell is the stop-the-world schema that degrades latency and makes haskell ill suited for some applications where a predictable small response time is important. Other languages have no stop-the-world strategy for this reason. Throughput is important as is the case of this particular application, but latency is increasingly important, and the point is that a lazy language like Haskell is theoretically perfect for low latency. But it seems that the stop-the-world schema is the, sort to say, eager guy that destroy the theoretical advantage. I read something about some alternatives going on for the haskell GC but I'm not sure.
There isn't enough information in that sentence to infer anything about Haskell: it all depends on what a typical "request" involves. 
I feel like as it is currently, "a newcomer to Haskell" will probably apt-get ghc, and not even know to look for stack. Whether or not that is a problem is probably the discussion to be had first.
That *is* nice, but it only helps a little bit. As far as I know, allowing different versions of the same dep only avoids the NP-complete issue if *every* instance can be different. That is if there are *no* constraints that two deps have to be the same version, not just that not all of them have to be. And this is not something nix can do because those constraints are a property of the underlying languages/packages and not of nix. For example, in general C programs are just not going to work if you link each different C lib used by a prog with a different instance of glibc. That's not due to a limitation of nix, nix could almost certainly do it. But it's not going to work. It's not due to a limitation of the C linker, that could be made to work. It's due (amongst other things to) the fact that C libraries provide APIs where they malloc() and expect the caller to free(). Thus they expect the caller is using the same instance of the libc allocator. The same kind of issue crops up all over the place. In Haskell we can link multiple instances of the same package together, but that doesn't mean we can always make them compile. We very often have packages that provide types that are then shared in the public interface of other packages (e.g. ByteString, Text). These then have to be the same type (provided by the same package instance) when shared across package boundaries, and so we get directly to the diamond dependency constraint mentioned in the article. The summary is, the diamond dependency constraint is fundamental to the way we construct and use libraries in Haskell.
LTO doesn't just magically work, even with LLVM. It does require some toolchain integration. As an obvious case: you must generally aggregate the bitcode files at some point, especially if you support separate compilation. LLVM does not magically aggregate `.bc` files together -- it has tools to do so, but you must use them. GHC just uses LLVM as an assembler: it gives it bitcode and it gets assembly. It doesn't aggregate or defer code generation in any way. So, GHC has no form of automatic LTO, LLVM or not. Also, even if you're using the LLVM backend, the RTS does not have any sort of bitcode build/link time capacity. And, the LLVM backend is designed to perfectly interoperate with the non-LLVM backend. It's not possible to optimize across boundaries where you use the native code generator, or talk to the runtime. In practice, it would be extremely limited unless we just went all in on LLVM and redesigned the way linking works in GHC.
I am not sure in which context you're working exactly, but MTL-style typeclasses normally give you such benefits. Kind-of like lifted-base frees you from the burden of calling liftIO yourself. With this style programming, the lift is done once in the implementation of the function you are calling, thus decluttering your own codebase.
&gt; Can you point at a specific library you are using? I think just Base and Free, see my reply below for some more details.
https://www.reddit.com/r/haskell/comments/4kp6zg/summer_of_haskell_2016_accepted_projects/ says: &gt; Due to a conflict with a student also being enrolled in the Google Summer of Code, "Jupyter for GHCJS" is taking the place of "CodeWorld mobile" and will be mentored by Andrew Gibiansky. But both projects are listed in the linked mailing post? How can this happen?
This is also what I was refering to in my comment. P.S. You probably meant `Free m` instead of just `m` there.
Note that we do already have a relatively sophisticated (and these days rather fast) solver. It's not based on SAT/SMT, but it is based on standard published constraint solving techniques (with a nice functional lazy modular implementation).
Also, although politically unwise, maybe just look at the download stats from the repositories. I'm guessing that Ubuntu dwarfs all the others combined. Just keeping Ubuntu might make this more manageable. Also, keeping repositories for all up to date with static binaries might be easier, so maybe there's no need to drop them anyways. What the package managers do nicely is handle keys. The "add key and then repo" is well understood. These `curl https://somesite.com/ | sh` is not curated wrt security, so you don't really know what you're getting into. `Go` and `Node` might get away with it, but `snuffle`, the new whitespace-oriented functional reactive framework just launched might raise some eyebrows if they ask for the same trust.
&gt; Mm, ya I was afraid of that. See I already use these things elsewhere, so it's not a matter of just adjusting the old definition, because then that would break all the other places which only need access to just one of the pieces. No it won't, because the new one is strictly more general, and thus could be used as before elsewhere. There *is* a problem of ambiguous types introduced by double-lifting. (What's the type in the middle if I lift something twice?). But you did ask about removing all those `lift`'s right? If so it's not a problem.
I think it's really good that you're solving this setup. I can see other situations where this applies. For example some Java framework, say some SOAP relic with some WS-Security awfulness that the open source community can't stomach to reimplement in Haskell.
&gt; No it won't, because the new one is strictly more general Oh, yes I see what you mean. For some reason I misread your post as you hardcoding `Database U HTTP` in for the generic part, even though that's not what you said.
Huh? The only mention of Haskell in the linked document is for IHaskell, the Haskell kernel for Jupyter notebooks which is no news, as far as I am concerned. OTOH, I don't know anything about this Pynq board…
You can go the Haskell route by compiling to the FPGA using [CλaSH](http://clash-lang.org).
&gt;on a fresh ubuntu 14.04 system. For almost all of the software i wanted to install (git, emacs, vim, etc) the only available versions were ancient. That makes sense! Since 14.04 dates to April 2014, I'd guess all the software you were trying to install was also dated from that period.
Because what they add is a fixed hash of a key. This key is fixed and documented in multiple places. This is unlike the alternative where a breach of the serving infrastructure is catastrophic. The signing key for a repo can be closely held unlike the alternative which is guaranteed to be broken if left to itself without regular patching. So the repo is clearly safer. But it's also more common and recognizable.
Looks like this has been up for feedback for some time: https://github.com/commercialhaskell/stack/issues/2534 Let's move any formal discussion over there.
HLS might work for you when a) you already have a working C++ kernel b) you know the unholy mess of pragmas to add to turn a serial program back to a parallel one again. I have written Clash from scratch and it is smooth sailing.
Modern hardware design is adopting formal verification and construction: https://shemesh.larc.nasa.gov/NFM2010/talks/harrison.pdf. It may not be widespread but it seems to be making inroads into companies such as Intel: http://is.muni.cz/el/1433/jaro2010/IA159/um/intel.pdf 
Ah great to hear. I've had similar conversations but minus the jest. It can be pretty frustrating in that context. Glad you're not in that boat exactly.
Which is why I distinguished between theory and practice.
We've been thinking about this in the context of Cabal for some years. We've often called it private packages or package encapsulation. A dllthomas says, it does avoid NP-completeness but it does increase the space of solutions. So one still relies on a sophisticated constraint solver and accurate version constraints. It's also not uncontroversial as an idea. Many people (including distros) want there to be exactly one version of each package (for reasons other than technical limitations of tools). The concern I've seen expressed is essentially that if we make it easy to have solutions with multiple versions then we'll never again be able to get back to having only one version (which is seen as desirable in some cases). Backpack is certainly moving in the direction of multiple different instances, and we'll see how that goes. Then it also turns out that private deps are not as easy as they look. The tricky details are all about how we know what is public/private or encapsulated and whether things can leak, or which package is responsible for ensuring encapsulation. The details are sufficiently tricky that we don't yet know how to do it generally.
Fair point, must have missed that.
I think it's well past "inroads". Intel has been in the verification game for over a decade at this point, at least. After the FDIV bug, they were already rolling out formal methods as early as the late 1990s/early 2000s, from what I gather. John Harrison, the author of your first link of slides -- actually works at Intel on floating-point verification, funnily enough. And he's also the author of what's probably the [bible of automated theorem proving](http://www.cl.cam.ac.uk/~jrh13/atp/). From what I understand, Intel is arguably at the forefront of hardware verification and has been for a long time, and is well ahead of its competitors in this space.
As an FPGA engineer, I completely agree. I have played with Clash for side projects and it is wonderful. If I could use it at work, my productivity would quadruple. Despite what many of those Hacker News commenters say, Clash does address many of the real issues faced by hardware designers. It is great for designing sequential logic. Having all of Haskell's testing infrastructure available to you to test your design is vastly superior to anything you could do in System Verilog (providing you're willing to trust the Clash compiler). And, being able to drop into GHCI and simulate parts of your design with a few keystrokes is a huge productivity booster. 
Dpkg has features like `dpkg --simulate -i package.deb` you can use to inspect a package before installing for real
Will have to check it out
Yes, the static binaries use musl. They're built on Alpine 3.4 using `--split-objs --ghc-options=-optc-Os -optl-static -fPIC`. The GHC bindists that Stack installs are still the official dynamically linked ones. I'd love to have statically linked GHC bindists because that would simplify things a lot, but GHC is a complex beast.
So I guess in practise it'd look something like this? put "/blog/:post" $ do user &lt;- userFromCookie post &lt;- blogPostFromRoute edits &lt;- blogEditsFromRequestBody handler Or, under "/admin" $ do user &lt;- userFromCookie put "/update" $ do stuff &lt;- something handler2 where `handler2` could have `?user` and `?stuff` as parameters. And you can substitute the routing with whatever more typesafe solution you like.
&gt; GC is not a perfectly parallelisable task, it depends on the shape of the data in the heap). Would it be interesting to investigate whether you could do automatic transformations of data structures by the compiler if you were going to use parallel GC?
I think I found a solution: https://github.com/huseyinyilmaz/spotprices/blob/master/src/Types.hs#L17-L25 
I can only speculate about your use case, but if you would care to sketch a (like) 3-stage pipelined scenario with how the BRAMS are supposed to be fed, here or over at [github](https://github.com/clash-lang/clash-compiler), then /u/darchon and the other Clash folks would be very happy. They are genuinely interested in (figuring out) the spatial/temporal partitioning of computations (possibly guided by the types) that result in nice solutions to the problems you mention.
I have no background in this field, but I just wanted to say that comments like this are why I read r/haskell most days and rarely go to Hacker News any more. The levels of constructive dialog here are an unfailing source of cheer to me :)
Afaik (and this may be wrong) GHC doesn't build via C by default, but C can be output with a command line switch. As for targeting C it can never hurt but at the same time the LLVM IR may be a bit more fit for purpose for what you want.
OP is referring to 'C minus minus', https://ghc.haskell.org/trac/ghc/wiki/ImprovedLLVMBackend
If it's just a fun project, why bother with the popularity of C--? The only reason I can see that it could be beneficial to learn LLVM which is probably a more useful skill than learning C--. 
John Harrison is himself principal engineer at Intel. He has written a "Handbook of Practical Logic and Automated Reasoning", which I happen to have by my bedside. I warmly recommend it to anyone interested in those topics and preferring a "hands-on" approach.
I would learn use LLVM instead. It'll have much more documentation, still has an interesting type system etc and will be useful to understand how it works going forward. I'm certainly glad I chose LLVM rather than Cmm when I was evaluating them for my university compiler project and have used the knowledge since them numerous times. That said, it's a personal project so if you're interested in Cmm, go for it.
Can you explain?
I don't know C--, but LLVM has a good language specification, great document, compilers to many mainstream platforms(x86, arm, etc.), and an JIT interpreter IIRC.
It seems kinda strange to construct proxy types via type applications. It would be less boiler plate if servant moved away from proxys.
This is indeed exactly the reason why servant has not moved away from Proxy yet, and I agree that explicit type application makes things slightly nicer.
Almost, Implicit parameter start with `?user` but for some reason you can't use it with `&lt;-` do binding so you'll have to do under "/admin" $ do user &lt;- userFromCookie let ?user = user put "/update" $ do stuff &lt;- something let ?stuff = stuff handler2 
This is a very long post, but shows how the ideas from the pure world are percolating (albeit slowly) into other ecosystems, in this case C#. Some interesting snippets: &gt;This meant that a lambda conforming to the PureFunc interface could only close over immutable state. &gt;Notice how powerful this has suddenly become! This PureFunc is precisely what we would want for a parallel task. &gt;All side-effects need to be exposed to the type system ... This obviously covered I/O operations – all of which were asynchronous RPC in our system – but also even – somewhat radically – meant that even just getting the current time, or generating a random number, required a capability object. 
PartialTypeSignatures, nice! What are the drawbacks?
Just target LLVM :)
A lot of these are pretty subjective, or make far reaching claims that aren't actually universally agreed upon. * It really doesn't matter what source control you use, or where you host it, as long as it's not complicated to get. * Your code should build with both stack and cabal. * What even is hpack? Who's claiming YAML is a good format? * I do actually prefer kebab case, but it's perfectly reasonable to use a name that matches the file naming standards enforced by cabal and stack. * Semantic versioning breaks down all the time precisely because people *don't* think about version numbers the way it does. Better to include an actual explanation of your versioning policy in the project. * Vague statements like "avoid heavy dependencies' just lead to people re-implementing shit everywhere. Determining if your dependencies are too heavy is in the actual case non-trivial. * There isn't one good answer to what your module hierarchy should be, most people would agree that it should include your package name to avoid conflicts. * Oh my god shut up about hindent. * Don't tell people to use CI without linking to a good explanation of how to do that. There are a million shitty tutorials people will find on their own instead of the one or two good ones.
Very interesting read, thanks for sharing
Agreed. This is hardly an objective checklist, and more of a list of preferences. `hindent` in particular bothered me. I love `hindent` in principle, but I just can't actually use it. It's way too common for it to completely ruin formatting (especially comments), and you can't reconfigure it for your project anymore. If you use a lot of dollars, lambdas, and `do` blocks, it just obliterates lines and breaks them up into weirdly indented, very short lines that look sorta like this x = do f $ \i -&gt; do print $ longerExpressionsHere
I totally disagree about SemVer v. PVP Versioning. PVP Versioning is awesome! The problem with PVP versioning is that with two major versions, authors are tempted to stay at 0.X major version because they're afraid of the implied commitment they see with a 1.X major version. This dilutes the meaning of what version 0.X means, but really isn't a big deal. The problems with SemVer are much more serious. Firstly, SemVer has only one major version. This tempts library authors to pretend breaking changes aren't actually breaking, because they see having a major version of 52 or whatever as reflecting on the stability of the library. It also reduces what can be communicated by the version number. Secondly, and much more seriously, SemVer makes an actual meaningful distinction between major versions 0 and 1. While on major version 0 you can make a breaking change but only bump the minor version, after 1 you have to bump the major version. If you think we have problems now with people staying on 0 too long it's nothing compared to what SemVer encourages. Another classy point behind PVP versioning: if you don't want to bother making a distinction between two major version numbers you don't have to, just keep the second one at 0. Doing this you end up with foo-52.0.1.1 or whatever, and it's totally allowed by PVP Versioning. In summary we have a better versioning policy than most other languages, and frankly deserve to brag some about it. PS: I really like the idea of opinionated guides to different parts of Haskell, and I look forward to seeing this one develop.
CircleCI is also fine choice for doing CI things.
Yes, these are subjective. They spell out how I think Haskell packages should be written. I don't expect everyone to agree. &gt; It really doesn't matter what source control you use, or where you host it, as long as it's not complicated to get. That's true. On the other hand, I'm much more likely to submit a patch to a package on GitHub compared to darcs hub. &gt; Your code should build with both stack and cabal. Does building with Stack preclude something from building with Cabal? I think Stack provides a nicer developer experience so I prefer to build with it. But I'm reasonably sure I can take any one of my packages and `cabal build` them. &gt; What even is hpack? Who's claiming YAML is a good format? [hpack](https://github.com/sol/hpack) is "an alternative format for Haskell packages". I don't think YAML is a good format, but I think it's better than the Cabal package format. Using YAML lets you write scripts to do rote stuff without getting Cabal (the library) involved. For example, how hard is it to write a script that increments the version number? &gt; it's perfectly reasonable to use a name that matches the file naming standards There are no standards. In fact, Hackage might make package names case-insensitive: &lt;https://github.com/haskell/hackage-server/issues/522&gt;. &gt; Semantic versioning breaks down all the time precisely because people don't think about version numbers the way it does. Better to include an actual explanation of your versioning policy in the project. Are you suggesting the PVP does not break down in the same way as SemVer? Also, my change logs usually include a statement like: "This package uses Semantic Versioning." I probably should have put that in the post. &gt; Vague statements like "avoid heavy dependencies' just lead to people re-implementing shit everywhere. Determining if your dependencies are too heavy is in the actual case non-trivial. Sure, but does that make it a bad guideline? For instance, does the sdl2 library really need to pull in linear (and therefore lens)? https://github.com/haskell-game/sdl2/pull/123 &gt; There isn't one good answer to what your module hierarchy should be, most people would agree that it should include your package name to avoid conflicts. In my opinion the "standard" hierarchy is both useless and confusing. Why is it `Data.Function` but `Control.Monad`? And who would prefer to write `import Text.ParserCombinators.Parsec` over `import Parsec`? &gt; Don't tell people to use CI without linking to a good explanation of how to do that. Fair enough. I provided my own package as an example, but there's also [the Stack documentation about Travis CI](https://docs.haskellstack.org/en/stable/travis_ci/) and [Michael Snoyman's post about AppVeyor](http://www.snoyman.com/blog/2016/08/appveyor-haskell-windows-ci). 
&gt; Use Semantic Versioning. This is bad advice IMO. The PVP, for better or worse, is a pretty accepted standard with well-defined meanings, and many tools (including stack) and workflows depend on it.
OP here. After the recent extensible records thread, I found this work-in-progress (?) paper interesting. It is about extending ADTs (without field labels) with extra branches, and also extra components in particular branches. One thing which seemed odd to me is the use of the Void type-with-no-values to signal "unextended datatype". My intuition says that () should be used instead! Perhaps the extensible ADT should have two type parameters, one for extending each particular branch, and another for adding additional branches?
This is a great checklist. Sure, it's opinionated and simplified and we won't agree on every item. But it includes useful ideas and pointers for haskell package maintainers, and I like to see things aimed at raising our community's quality standards. Thanks for posting!
&gt; Put Haskell files in source/. I agree on *In other words, separate your package metadata from actual source files.*, but virtually all packages I have seen use `src/`
As is Appveyor.
&gt; Avoid heavy dependencies. This is very dividing opinion. Should one depend on `contravariant` because one type in the package could be made `Contravariant`? You'd write that instance if the class was in `base`. OTOH it's not in `base` partially because there are no much use: chicken-egg problem. &gt; Think about how long it would take to install your package starting from scratch. using `cabal new-build` or installing from Stackage snapshot with `stack`: near to zero. This is not a good reason anymore.
I have to disagree about SemVer. Most of the ecosystem is currently built around the PVP. Using a different scheme makes it MUCH harder for people to understand what version numbers are intended to mean. There is a pretty detailed discussion of the PVP as it compares to SemVer [here](http://pvp.haskell.org/faq/#semver). I also have to disagree on using hindent. It literally crashed on the first file I tried to use it on. The idea is great, but it's really important for tools like this to be rock solid. If it works for you, great. But it's not good enough to be suitable as an across-the-board recommendation IMO. I've found [stylish-haskell](http://hackage.haskell.org/package/stylish-haskell) to be more robust.
The warp executable from [wai-app-static](http://hackage.haskell.org/package/wai-app-static) does this out of the box, a useful Haskell alternative to `python -m SimpleHTTPServer`.
Nice! This point never occurred to me.
But they are! ^(...if you select the right sample over Hackage...)
&gt; They are presented as prescriptive though. As I said in another comment, everything on my blog is subjective. It feels like a waste of time to say so. &gt; you're unlikely to end up with a .cabal file that accurately represents your version bounds Why so? I recommended using `--pvp-bounds=both`, which automatically adds the correct bounds. &gt; a lot of developers don't actually know what constitutes a breaking change So the solution is to add another field for encoding breaking changes? I don't see how it helps. &gt; Both cabal and stack require file/folder names to match the module names Oh, you were talking about module names. I thought you meant package names. I think we agree here? "While the package name uses `kebab-case`, module names should use `CamelCase`." &gt; It's the wrong guideline I'd say it's a good default to avoid pulling in dependencies, especially if they have a lot of transitive dependencies. For example, consider [Data.Either.Combinators.isLeft](https://www.stackage.org/haddock/lts-7.13/either-4.4.1.1/Data-Either-Combinators.html#v:isLeft). It pulls in 35 dependencies! https://imgur.com/NW9lrzF
This. For the love of all that is pure and functional, please use PVP. It's just semver with the first two segments being the major version rather than one. It's what the vast vast majority of hackage is using.
&gt; I've found stylish-haskell to be more robust. I'm with you on hindent being kinda bad, but stylish-haskell doesn't really do the same thing. It pretty much just formats language pragmas an imports. Hardly "robust."
Just as comparison, with clean GHC 8.0.1: - lens pulls in 26 new packages (in addition to what comes with ghc) - http-client-tls needs 38 And these sets overlap. I keep wondering how "lens is heavy" myth is born. It has many *direct dependencies*, but they are very light themselves.
Maybe `http-client-tls` is heavy too.
Heavy dependencies are much less of a problem with the nix-style builds (tech preview in `cabal` as `new-build`). I think stack will get this feature at some point to as ezyang offered to implement it for them. The thing about dependencies is that you should use them when they make sense from a software engineering point of view. If it helps you write your code more maintainably with fewer bugs then use it. At least for me, correctness is more important than the build time when we're talking about &lt; 1 hour per build. And for longer builds I'm probably still going to dramatically prefer good use of dependencies.
Every time I start with `microlens` I end up requiring some of the missing things such as Prisms, and then I'm back again requiring `lens`. It's as if `microlens` was nothing more than a gateway drug to `lens`...
You still pay the price of building those dependencies. You may pay it less often with Nix-style builds or Stackage snapshots, but the cost is there. And nothing can save you from rebuilding when a new version comes out. I agree that using dependencies to write more maintainable code is good. I question the utility of bringing in [35 dependencies](https://www.reddit.com/r/haskell/comments/5iok3l/haskell_package_checklist/db9xloe/) for a function that does `Either a b -&gt; Bool`. 
Yeah, I'd recommend "maximize dependencies" to haskellers. Reusability is so high, so we should reuse what we can. However, I do try to fragment my packages, and I do prune away some dependencies. More work for me, but then I can split out (for example) a low-level ffi package which has ghc-only dependencies from a higher-level package with ekmett-based abstractions like lens, which then saves me work. 
On phone, I would think type instance XExp UD = Void And the rest of the `UD` instances be `()` but I haven't given it any thought
&gt; Format code with hindent. This would be great advice if hindent actually worked. Unfortunately this is not the case: - https://github.com/chrisdone/hindent/issues/348 - https://github.com/chrisdone/hindent/issues/310 - https://github.com/chrisdone/hindent/issues/296 And it doesn't look like anything is being done to solve these issues.
I think it is ultimately a people problem. Norman Ramsey was the probably the main voice behind C-- and he worked on it when he was at Harvard, but from what I understand eventually they denied him tenure, and he landed over at Tufts and had to start pumping out papers again. In the meantime LLVM came along.
Yep. AppVeyor also has the advantage you can install a 32-bit copy of GHC and test your projects that way, too (Travis only allows 64-bit builds). The 32-bit build caught a few bugs in `binary-serialise-cbor` for instance, aside from just being a good smoke test.
Looks like `hpack` was just what I was looking for. Just the fact that it handles the Cabal `other-modules`-stuff is enough for me to switch. I mean really, Cabal... why do you insist on failing with linker errors unless I manually copy-paste, into my cabal file, the list of `other-modules` that you print out? Come one now. $ cabal build [...] FYI, everything will break mysteriously unless you manually copy/paste the above list of modules into your cabal file
The most important thing is to remove the social aspect of meat popsicles from the equation. Versioning should be handled entirely by the type checker and platform. Once the monkeys are disallowed from fudging the numbers for social signaling, everything becomes much simpler.
&gt; I'd recommend "maximize dependencies" to haskellers. Reusability is so high, so we should reuse what we can. [Never forget 3/22/2016](https://pbs.twimg.com/media/Cxpdl35WIAQ-7S2.jpg)
Just pick any webframework instead...
If you had to pick just one, I don't know why Appveyor wouldn't be a reasonable alternative to Travis. It's more usable IMHO, and it'll flush out your Windows incompatibilities.
Yeah, that seems correct to me.
?
I raise you the tux kernel module. No executable needed.
Caused by mutability! Not applicable in /r/Haskell
IMO the main advantage of formatting tools like `hindent` is not that the result ends up looking great or the entire codebase has a consistent style. It's that I don't have to spend any thoughts on how to format my own code.
Hackage is more mutable than you might think. Packages can't be deleted, but their Cabal files can be changed in place. You can, for example, change the version bounds of your dependencies without changing your version number. 
That's literally true of every formatting tool, like, ever. I already do this with tools like `clang-format` all the time. I hit "save", and my file is formatted immediately. I never think about it. It would be (and is) true of most Haskell formatters, too. Now, the secret to why I use things like `clang-format`, but not `hindent`, is quite clear once that's out of the way: `clang-format` doesn't force me to use a style I hate, out of some misguided attempt to make the whole C++ community fall in line.
If you want a prerelease version that you can change rapidly, then use 0.\*, and bump the * with every breaking change. When you're ready to leave that style, bump to 1.\*.
When reading your post it seemed it was the standard way to do a package. Maybe you should add a disclaimer at the beginning explaining that it is a personal point of view not necessarily shared by the community.
What does he mean by &gt; The complexity and difficulty of hardware design is not in the combinational part, it's in the sequential (i.e., state-carrying) part of the circuit. ? What is it about Clash that is inherently "combinatorial", and thus not state-carrying? Is it difficult to translate stateful Haskell code into hardware? 
My rough guideline is: in packages like GTK3 and SDL2, the digit added to the name is what goes in the first PVP position (I like to call it the "series number," though I have no idea if that makes sense outside my head). It represents a deep architectural change, and the potential need to maintain the old series for legacy users is arguably the primary reason for separating them to begin with. Granted, Python managed this with only three digits. :)
What would be the "modern" alternatives to Proxy for passing around type information?
The point is that in SemVer, it's a special case on 0.x. In PVP, it's just the default.
 &gt; :set -XTypeApplications &gt; :t fmap @[] (a -&gt; b) -&gt; ([a] -&gt; [b]) 
Word. I feel bad for OP. Lots of people in this thread are triggered by his "authoritative" and "objective" tone. And he's just taking it! Like, &gt; As written, if a newcomer saw your post, they'd have no idea that these are your opinion, and different from standard practices. --- &gt; Fair point. No, that's not a fair point, that's a terrible point. A newcomer wouldn't have any idea that this guy's blog contains his opinions? Sheesh. 
Fair point =P Though I'm curious what code you had that crashed hindent. I've never managed to achieve that.
Ah. Well at least that doesn't crash anymore. It just exits with a clear error message =P And it does support `mdo`.
So... its the wording in the specification you have issues with? "By default semver bumps from 0.x to 0.(x+1) are breaking changes" ... For me, as a user of a package, the effects are the same. And the implications are well understood in both communities.
No the issue is that with SemVer, there is a semantic difference (ironically) between 0 and 1. This is unnecessary, and has lead to problems in the past. PVP is consistent and does not have this issue. As a consequence, PVP supports the desire to have a fast changing 0.x release series with breaking changes, without requiring any special behavior in the version semantics.
Interesting questions. My comment is a informal sketch with a incomplete description. two remarks in the alternative point: the way I modelize &lt;|&gt; is leaving it sequential as you say, and adding some primitives like `async` that make a term asynchronous, such that it return `empty` to the current thread and some value to he new thread created, which executes the continuation. In this way &lt;|&gt; behave sequentially when the terms are non asynchronous. In the other side, why, if there is a monad, `&lt;*&gt;` should be sequential? . I have a monad (transient) that has a concurrent `&lt;*&gt;` (Well it is optional concurrent, if the terms use async primitives). 
As a simple example, consider the `Concurrently` applicative. newtype Concurrently a = Concurrently { runConcurrently :: IO a } deriving Functor concurrently :: IO a -&gt; IO b -&gt; IO (a, b) instance Applicative Concurrently where pure = Concurrently . pure Concurrently f &lt;*&gt; Concurrently a = Concurrently $ fmap (\(f', a') -&gt; f' a') $ concurrently f a And imagine we gave it the only possible monad instance. instance Monad Concurrently where Concurrently a &gt;&gt;= k = Concurrently $ a &gt;&gt;= (runConcurrently . k) Finally, consider the following program x :: Concurrently a x = Concurrently $ do threadDelay 1000000 throwIO FirstException y :: Concurrently a y = Concurrently $ throwIO SecondException z :: Concurrently a z = x &lt;*&gt; y z' :: Concurrently a z' = x `ap` y With the above exception-ful program, `z` and `z'` will behave differently. `z` will throw `SecondException`, and `z'` will throw `FirstException`. And as long as there is any observable difference between `ap` and `(&lt;*&gt;)`, besides performance I guess, the monad laws are broken. It's quite rare for the observable differences to be negligible enough that this law can be broken.
Why not write a `microlens-prisms`library that adds the missing functionality without the weight of lens?
Don't even get started on alternate preludes... (Disclaimer: I'm partial to [basic-prelude](https://hackage.haskell.org/package/basic-prelude-0.6.1/docs/BasicPrelude.html).)
You misunderstood the part about paths, and that probably made you think the book is less formal than it actually is. Most of it can be faithfully formalized in [Agda](https://github.com/HoTT/HoTT-Agda) or [Coq](https://github.com/HoTT/HoTT). I find the level of formalism in the book entirely adequate, as full formality is overly verbose for the format, and it's available anyway in the above links. Regarding paths: there's absolutely no reference to set theory. The book never builds on set theory or other theories (which is obviously necessary if HoTT wants to be fundamental). Graphical illustrations and set-theory flavored explanations only serve to connect back to familiar concepts and definitions. Up to Chapter 4 the book only uses minimal intensional Martin-Löf type theory extended with univalence as axiom (and optionally classical axioms), which system could probably fit on one page when described in full formality. All the topological notions like "path" "space" or "continuity" arise from that system alone. The only formal gap in the book I'm aware of is the metatheory and general formulation of valid higher inductive definitions (Chapter 6). 
Yeah. But that is not a problem. it means for the programmer that one or the other has executed first. This kind of non determinism is unavoidable in multithreaded programs where the threading system is not controlled by the programmer (by means of his own monadic effect). This is produced by the fact that there are threads. For a programmer that program multiple threads, he knows that inherently the parallel program may differ in behaviour from a serial one, since threading is an impure effect. In fact it should behave different in order to run concurrently. Even a single tread and a single program with impure effects may differ in behaviour from a run to another. IMHO the rules in the context of impure effects must be understood in terms of identical semantic behaviour, not in terms of identical behaviour, since no two runs are guaranteed to be equal for every expression where the rule applies. So if the concurrent definition of &lt;*&gt; is different from the serial definition is no surprise, since there is an impure effect more to consider. EDIT: The advantage is that instead of a non composable primitive, like `forkIO`, we now have an *effect* within a monad-applicative-alternative that is *composable* in such ways. That is a HUGE advantage.
While your statement about `()` is certainly correct, usually this fact is just ignored and we pretend that bottom does not exist. In this case I find it especially confusing since `Void` is used as the unit type in some places (i.e. annotations of branches other than `X_{Exp}` and the uninhabited type in the case of `X_{Exp}`. I feel like these should really be different.
Well, it starts to matter a lot more once you realize that there are countless functions written that work on any ole monad, and these functions expect identical semantics for `ap` and `(&lt;*&gt;)`. But besides that, sure you can ask the programmer to remember that the semantics differ between `ap` and `(&lt;*&gt;)`, but that still means the semantics are law breaking. It's ok up to a point, as long as the observable semantics are negligibly different. But when trivial things like IO exceptions break them, that's unacceptable and law-breaking. It's not hard to just wrap things up in law abiding newtypes.
Alright looks like I might just end up doing that, I am a bit disappointed though as C-- seems like it would have been more fun. What is the best way to generate LLVM from Haskell, should I just output a String or is there a better way?
https://hackage.haskell.org/package/llvm-general This might be a decent candidate. 
The same using transient, serving the default folder `static/out.jsexe ` #!/usr/bin/env stack -- stack --resolver lts-7 --install-ghc runghc --package wai-app-static --package transient-universe import Transient.Move.Utils main= simpleWebApp 8080 $ return () or main= initNode $ return() host/port is asked from the command line or console input. 
In any case, you are right: the serial behaviour is necessary in some cases, and a monad with parallel applicative by default, like the case of your example above, is not a good idea. There are applications, like parsers, that consume a single input, and in such cases `&lt;*&gt;` is inherently serial (but parallel `&lt;|&gt;` may be very interesting). As I said, my approach is serial by default unless an async modifier is used. So `ap == &lt;*&gt;` unless the programmer want concurrency. async :: IO a -&gt; TransIO a a &lt;*&gt; b -- serial (if a and b have no async inside) threads 0 $ a &lt;*&gt; b -- always serial, even if a or b use async primitives a &lt;*&gt; async b -- concurrent, b run in a new thread async a &lt;*&gt; b -- concurrent async a &lt;*&gt; async b -- concurrent That happens also with `&lt;|&gt;` but in that case, if the expression has asyncronous statements, the created threads continue executing the monad.
&gt; How would I gather feedback from early adopters then? Hackage lets you upload [package candidates](http://hackage.haskell.org/packages/candidates/): &gt; Package candidates are a way to preview the package page, view any warnings or possible errors you might encounter, and let others install it before publishing it to the main index. (Note: you can view these warnings with 'cabal check'.) You can have multiple candidates for each package at the same time so long as they each have different versions. Finally, you can publish a candidate to the main index if it's not there already.
&gt; It really doesn't matter what source control you use, or where you host it, as long as it's not complicated to get. I'm not going to spend hours learning some niche VCS and I'm not particularly enthusiastic about creating an account on whatever weird bugtracking system might be installed on your webserver. Basically everybody knows Git and has a GitHub account and can immediately get your code, submit PRs, bug reports, contribute to the Wiki etc. Using anything else is massive barrier to participation that'll make a huge percentage of potential contributors give up. For the vast majority of projects it makes absolutely no sense to use anything else, especially given that GitHub &amp; Git are both free and pretty damn good.
* [hindent#361](https://github.com/chrisdone/hindent/pull/361) * [hindent#362](https://github.com/chrisdone/hindent/pull/362) * [hindent#363](https://github.com/chrisdone/hindent/pull/363) No guarantees they'll get accepted and merged, though :)
&gt; Host on GitHub Yes! I love clicking the "Star" button, let me star your project :) &gt; Define with hpack I'm not sure it's worth it. I like YAML but I've never had *problems* with Cabal's format. &gt; Put Haskell files in source/. In other words, separate your package metadata from actual source files. This makes it easy to write scripts that work on every Haskell file, like formatting or counting lines of code. Eh… I don't like extra folders. We're not in Java land, we don't need `src/main/haskell/com/…` or even `src/library/`, just `library/` and `executable/` is fine. I don't think scripts are a good reason — I can just glob for `**/*.hs`! &gt; Travis CI will need your Hackage credentials, so be sure not to leak those into the build log. Yeah, and my GPG key :D Nah, manual releases are fine. &gt; Test with Tasty using Hspec Why not just Hspec standalone? It has its own QuickCheck integration, automatic spec discovery, parallelism, beautiful output formatters… (Quick plug: I made [a fork of hspec-expectations](https://github.com/myfreeweb/hspec-expectations-pretty-diff) that pretty-prints and colors data structures in the output.)
Quoting the [readme](https://github.com/aelve/microlens#design-decisions) of *microlens*: &gt;`Prism` and `Iso` aren't included, as their definitions depend on `Profunctor` and I don't want to depend on *profunctors*.
At that point, though, I believe you'd be bound to most of the *lens* dependency graph, and so you might as well just use *lens*.
Bottom is a really special value because you can't inspect it. If you say `()` has two values, then I expect to have boolean operations on it, so you can start by writing a negation function `not :: () -&gt; ()` that takes `()` to bottom and bottom to `()`. This is, of course, impossible... unless you're willing to do some dirty tricks with `catch` and `unsafePerformIO`. And at this point you have not one bottom, but infinitely many bottoms, because `catch` can distinguish between `error "A" :: a` and `error "B" :: a`. All of this means that `Void` is a really poor substitute for the unit type: you either ignore bottoms (and then `Void` is empty) or you embrace them (and then `Void` has infinitely many inhabitants).
lens also pulls in filepath (why?), kan-extensions, adjunctions, free, prelude-extras, and semigroupoids, and also takes [significantly longer to compile by itself](https://github.com/aelve/microlens). Agreed you're probably running into diminishing returns at that point, though. I admit I am a bit biased to microlens, simply because I find its [documentation on Hackage](https://www.stackage.org/haddock/lts-7.13/microlens-0.4.7.0/Lens-Micro.html) to be the most accessible of the lens libraries - each section includes a definition and summary.
Agreed. The article says &gt; This post covers everything you need to know about developing a Haskell package. A simple change could have avoided this whole argument: &gt; This post covers everything I follow when developing a Haskell package. 
Err, is this supposed to be sarcastic? (Hard to tell via Reddit). If not it is laughably ironic, since you're giving an opinion as if it's fact.
yep we make progress by people just makin shit up
Can HoTT serve well as a first book on type theory, or I should familiarize myself with something like "Types and Programming Languages" first?
When was this paper drafted? I can't find a publish/draft date within it.
It's been [answered on Stack Overflow](https://stackoverflow.com/questions/41199217/taming-parallelism-in-haskell-ghc/41199787). I should have been using `barBuffer` instead of `parMap`, as `parMap` eagerly processes the whole list. A working version is: md5sequenceS = withStrategy (parBuffer 100 rdeepseq) $ map (makeMd5) [0..] where makeMd5 i = stretch $ getHash (salt ++ show i) stretch h0 = foldr (\_ h -&gt; getHash h) h0 [1..2016]
Naming the post "My Haskell Package Checklist" would be sufficient.
I also tend to think anonymous records + overloaded labels is getting pretty close to a more language-integrated notion of imports. Why is the `Queue` field (and Dependent Haskell) needed? I.e., why not have: signature Queue q = Queue { push :: a -&gt; q a -&gt; q a , pop :: q a -&gt; Maybe (q a, a) , length :: q a -&gt; Int } You'll end up with more polymorphic signatures, but I don't immediately see that as being bad. It seems to me like this could suffice to capture nuances such as the difference between: id' :: (?queue :: Queue q) =&gt; q a -&gt; q a id' = id convertQueue (?queue1 :: Queue q, ?queue2 :: Queue q') =&gt; q a -&gt; q' a convertQueue = ... (Using `ImplicitParams` since in this context that seems natural.)
Are you a first year philosophy student? &gt; See how weak these tone-policing arguments are? It's not *tone* it's basic English. Naming the article "My Haskell Package Checklist" *unambiguously* indicates that the list is personal. The author's use of "you" within the article does weaken that by indicating that they *do* think other people should be doing this, but not as much as *not* having that prefix in the title.
&gt; "For Id_A, they define it as a path space of all continuous maps on interval, providing no definition for continuous maps or intervals" You're reading the definition backwards. They're equipping the formal definition of Id in type theoretic terms with some additional vocabulary to describe it, which "coincidentally" happens to use terms familiar from concrete homotopy theory. Which is to say, they're defining an axiomatic homotopy theory from scratch. Part I is just a series of definitions, rules, and theorems about them for the most part. I'd recommend Part II as its actually a lot of fun, and lets you try to work through some proofs in this system -- as formally as you'd like. One thing you might enjoy is the first section of Brunerie's dissertation. The dissertation as a whole uses the tools of HoTT to construct a quite nontrivial proof (sort of a high-water-mark of synthetic HoTT to this point). As the dissertation needs to be semi- self-contained, the first section discusses the setting in which subsequent chapters take place in a way that is pretty succinct but also sufficient. And because its just trying to set-up the real meat of his work, it doesn't go into a lot of the additional informal explanation that may be throwing you off: https://arxiv.org/pdf/1606.05916v1.pdf (by the way, though its low traffic, questions about HoTT would be welcomed on the more suitable and specialized /r/dependent_types/ as well.) 
That looks pretty good. I'm not sure Dependent Haskell is necessary. I was just thinking in terms of imports and exports where types and values are treated on the same level. I guess I like the idea of a minimum of core concepts inspired by elegance.
Haskell users would benefit from library developers following Rich Hickey's advice. [Lack of backward compatibility is the root cause of the PVP debate.](https://www.reddit.com/r/haskell/comments/3mto7o/the_true_root_of_the_pvp_debate/cvip3nn/)
Well, we are trying to get to the bottom of this problem in a constructive, non-conflict way [here](https://github.com/haskell/ecosystem-proposals/pull/1)
Loosening upper-bounds is a big improvement, but the real win would come from accepting the value of backward compatibility. Then upper bounds wouldn't be an issue at all.
so you disagree with the assessment that "backwards compatibility has lead to harmful APIs sticking around far longer than they should" (in Java, HTML, etc..)?
&gt; accepting the value of backward compatibility i.e.? 
I don't disagree with the assessment, but the time scale is much longer. It's OK with me to deprecate a function and remove it *years* later, with warnings in the meantime. Backward compatibility isn't an absolute requirement when other issues make a *good* case for breakage. To enable backward compatibility, APIs must be designed to allow for upgrades. Coding practices that depend on the size of certain data structures must be avoided. Perhaps functions and data structures should be accessed through virtual tables that are initialized at runtime to allow for additional functions and additional data items in future versions without breaking compatibility. [Najd and Peyton Jones's paper](https://www.reddit.com/r/haskell/comments/5iovx9/trees_that_grow_interesting_paper_about_extending/) looks applicable.
That's the trap, believing thinking is either formalism or bullshit. If you limit the tools you are willing to use, solving problems will take longer, and they will be harder to explain. Logic is beautiful, but it's often the victim of a confused ideal.
Just making sure that people don't solve the same problems again and again goes a long way For example if the tools would upload results then we would only need to find a problem once and everyone would benefit from this information Not discussed much is the number of people affected by a given issue. This number can be decreased by several orders of magnitude by having the tools communicate with each other.
&gt; Wow again, but isn't this approach takes us back in time? It took centuries to get math on the analytic rails and formalize it (with a lot of benefits), but now we are going backwards? This really isn't the case. The return to axiomatic theories has been one of the key elements of the latter half of 20th century math. Axiomatic homology theory was one of the most important developments of Ellenberg and MacLane, and it opened the way for a pervasive transformation in approaches to mathematical structures. Then homotopy theory was really axiomatized by Quillen who gave us model structures -- very simple equipments on a category which suffice to perform lots of homotopical constructions. HoTT in a very loose sense can be among other things "a (higher) logical syntax for model categorical constructions)." The beauty of these axiomatic theories -- and others -- is that constructions could be expressed in much greater generality across a range of different set-ups. This matters in particular in settings like homotopy theory and homology because the "spaces" in which we work can be built in such a variety of ways. And it really starts to pay off when one wants to consider spaces with various notions of differential constructions, etc. which is a cornerstone of modern physics. A very nice article on a related point of view was written by the Bourbaki group: http://courses.daiict.ac.in/pluginfile.php/12495/mod_resource/content/0/References/Bourbaki_architecture_of_mathematics_amm.pdf The real danger is letting something like the "f word" (i.e. foundation) throw you. Too many people have used it to mean too many different things -- so its often misleading to bring too many expectations to one or another group's use of it. In this case, my feeling as to why the "f word" is being invoked is given by the second half of the book -- using these very elementary notions, one can build up a shocking amount of modern mathematics, and in a fashion much more "natural" and direct than from e.g. a set theoretic collection of axioms. (And hence better suited to mechanized formalization).
I would like a course that teaches things after the basics and monads. We did 2 modules in Haskell at university (Introduction and Advanced) but I still have no idea what is going on when I look at code posted here.
I mean that by default, if both operation are not asynchronous, `&lt;*&gt;` is sequential, like `ap` , so both behave the same way. BUT if some of the terms is asynchronous, they differ. The programmer can choose. In your case both differ, since there are asynchronous operands. `ap` is ever sequential. ap (async a) (async b)=== do x &lt;- async a y &lt;- async b return $ x y x and y are executed in different threads, but in sequence. the first async run a in a new thread and finalizes the current thread (return empty). when "a" finalizes, executes the continuation, which executes `async b` in the same way. Then the continuation of b executes x y.
Whatever you ordain, comrade commissar
Ah, didn't see that. Still, 28 results is far from 43,000 =P
No, it is neither sarcastic nor ironic, it is, in fact, quite subtle. You do not mind if opinions are presented as facts, so it should be fine to you that I present it as a fact.
Well to quote Harry Potter, let me know when it's available it sounds very interesting.
&gt; In fact, there's quite a bit of work in scientific computing going on right now, and I look forward to seeing it come to fruition. It's just not materialized yet, as it's quite recent for Haskell. Yes I know. they are very busy defining the concept of number. Thrilling 
It was written as a large-scale collaboration -- this is quite different than being written by committee.
Is experimental stability a big problem or is that usually an acceptable stability level?
I don't think it needs to be an all-or-nothing approach - simply having the code for constructing prisms and those which are present in `base`(i.e. everything exported by [this module](http://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Prism.html)) would be a pretty decent improvement. Splitting lens into one module containing definitions and another for each set of lens, etc. might be overkill though - it makes sense for aeson, filepath and exceptions, but I suspect it wouldn't reduce the no. of transitive dependencies / compile-time that much though. (Particularly since stack will cache the binary for a given GHC version, so it's almost a moot point these days.)
&gt; Don’t make your own recursion ah, yes, this is pretty sensible! &gt; Don’t use higher order functions (too much) huh
Violating distributivity can be useful for recursive definitions, like parsing left-recursive grammars. e.g. https://github.com/sboosali/commands-core/blob/master/sources/Commands/RHS/Types.hs But you want the Alternative instance for other laws (like annihilation, pseudo-distributivity given an interpretation), as well as consistency / reusing `(Alternative f) =&gt; ...` functions. 
Hm. These things appear to be Free variants. There is a [free alternative with distributivity](https://lirias.kuleuven.be/bitstream/123456789/499951/1/main.pdf). I wonder if chart parsing can be modeled under this Free variant.
I find myself using pipes a lot for things that might just be function composition or loops in other languages, basically for the reasons mentioned in the pipes tutorial - effects + composability + streaming. In other words I end up using it as a workaround to strictly compose effectful computations without much complexity, and not necessarily because I really have a particularly large dataset/streaming need. Does anyone find themselves reaching for pipes a lot for routine things? is there a risk pipe abuse becomes an antipattern?
I used to use pipes sometimes, but I find myself using [streaming](http://hackage.haskell.org/package/streaming) more and more, which I like using more unless I need bidirectionality.
&gt; If it is really true, and HoTT is based on undefined notions of path and continuous functions right from the beginning, then I think I will need a little bit of therapy to accept it as a foundation. Well, first of all, it is true. The basic undefined notions *are* types, the identity type of a type, function types, etc., which a topologist would prefer to think of as spaces, spaces of continuous paths, spaces of continuous functions, etc. I think your reaction to having these notions there from the beginning is a perfectly valid philosophical position: you want the foundations to be sort of low level so that believing them is not a big ontological commitment. Sets are pretty hard to disagree with, you probably have no qualms about using them for foundations.. But spaces seem much more complicated, surely they should be defined in terms of more basic things in out foundations? I think I understand the appeal of that line of thought. HoTT also makes sense as foundations but from a practical point of view. You could say, "I want my foundations to be relatively close to the level I'll be doing math at because then there is some hope that I can formalize my arguments with out a ridiculous explosion in length". (Again I offer a programming analogy, slightly different from the previous one: say informal mathematics are like pseudocode, and your foundations are an actual programming language. You might want the language to be low level so that you have confidence you fully understand how the language it works, the price you pay is that translating your pseudocode to a working program (formalizing a piece of math, in this analogy) is difficult and the resulting programs are really long. On the other hand, you might prefer to use a high level language, so that the translation process from pseudocode is relatively straightforward.) I'm also not convinced HoTT is an ideal foundation for mathematics, but for different reasons than you. I do like the idea of high level foundations so that formal proofs are actually relatively readable. And I think that HoTT is pretty close to achieving this for homotopy theory. What I don't like about it is that it doesn't really seem to have this sort of advantage over earlier foundations for any other field of math! Stuff like algebra or analysis, for example, can be developed in HoTT, but as far as I know it's done basically in traditional set theoretic manner (with what HoTT calls "0-types" or "types with h-level 2" in place of sets). So this great advantage of "easily" formalizing arguments in HoTT seems to only be available for homotopy theory! I'd love practical foundations for all of math not just one field (even if that one field is my favorite :)).
`map` is one of the absolutely fundamental parts of functional programming, along with `fold` and such. So I'm a bit skeptical of this blog post. 
&gt; Don’t stack your Monads (too much) &gt; &gt; I think most can agree with this, but not with the proposed solution of "Instead, it’s better to put everything in one State monad". Especially when there's an indexed monad that has basically everything you could need. &gt;Higher order functions (like map) are confusing, they are harder to reason with than first order functions/parameters. &gt;I think this depends. Higher order functions with more general types are much easier to reason about when you start doing more type level programming for instance. Especially when like... it's just `map`? A pretty damn essential function and incredibly useful in e.g. Accelerate or Repa. Also I really like monadic parser combinators - hard to understand at first but definitely worth it. 
Lots of good points. I find Haskell is easier to refactor than many languages, so getting it working in an ugly way is fine. If necessary, tidy it up just a bit. Not too much or you'll spend all day studying theory just to make the perfect model.
&gt; higher order /= complex Seriously, hell even `$` is higher order.
The downside to a Types module is that it's going to be imported by *everything*, so any change there is going to require rebuilding your whole project.
Right: they should be different (as monoidal zero of sums (A+0=A=0+A) is fundamentally different from monoidal zero of products (A*1=A=1*A)). As I said, we really want `()#` and `Void#`. With special treatment of levities (which is work-in-progress), we *may* be able to get that... until then, we either "pretend bottom does not exist" and go with `()` and `Void` (as I personally did in my HiW development), or use `Void` and modules and synonyms. Either case, we were ourselves impressed of how far (e.g., handling GADTs) we could go with this simple *programming idiom*, and in the draft we focused on explaining that. We are now looking into performance-related issues, and that may include dealing with the levities.
"Turing completeness is overrated"
As the project gets bigger you just split the types into Types.PackageName and Types.Build etc like in Stack. This preserves incrementality and decoupling.
Most languages are object oriented which inherently couples data with behavior which in my opinion is the wrong approach and in Haskell we decouple them, which is proper. A data type may have different consumers, producers and combinations. Does listToMaybe and maybeToList belong in Data.Maybe or Data.List? There is no Correct answer. Should all the Vector sorting algorithms on Hackage go in the Data.Vector module? In a typical language you are forced to choose. In Haskell we decouple and so behavior can go anywhere without solving silly hierarchy puzzles. Encapsulation is also overrated. 
I changed slightly the sentences, `map` was a bad example. What I'm saying it that structuring you program with a lot of callbacks is a bit confusing, why not using a more direct style?
&gt; Don’t use higher order functions (too much) &gt; &gt; Higher order functions (like map) are confusing, they are harder to reason with than first order functions/parameters. And here I thought Higher order functions were easier to reason about than own code trying to emulate them, because they are generally pretty simple and come with their own invariants/laws.
I assume you're talking about [this](https://github.com/commercialhaskell/stack/tree/master/src/Stack/Types)? This exhibits exactly the kind of issues I mentioned elsewhere that I want to avoid: you're forced to have instance code right in your types file. This puts you in a bad position when you have a function that's useful in more contexts than just instance code. I see there is even an hs-boot file, is this used to resolve circular dependancies (glanced through it but I'm not familiar with hs-boot beyond that they can be used to resolve circular deps)? EDIT: On second thought, I guess the reason you're splitting into e.g. Types.Build is so that Types.Build can depend on e.g. Make or whatever without circular dependancies. Which means it's equivalent to what I came up with, just a different naming convention. I'll have to give some thought to decide if Stack's naming convention is more self-describing than mine. Mine is probably more recognisable from other languages but maybe Stack's is closer to what a Haskeller would expect.
This all sounds fine, but I'd recommend against using type synonyms for anything but... abbreviations. If you're using type synonyms to convey semantics, then consider using a newtype instead. Unlike newtypes, type synonyms don't help you check that you haven't inadvertently swapped a GameName for a PlayerName. Unlike newtypes, type synonyms don't enforce abstraction boundaries. IME they just obscure what GameName really is, i.e. a String (newtypes do too, but in that case at least I know I'm not supposed to worry what a GameName is under the hood - it's an abstract datatype).
Will it include advanced type wizardry? I'd love to see something comprehensive about things like singletons and just how far you can push GHC in encoding invariants in types.
&gt; If it ain’t broke, don’t fix it This is monumentally bad advice. It's along the lines of "If its stupid, but it works, it's not stupid." and it's exactly what leads people to kill musquito's with cannons or invent things like [electron](http://electron.atom.io/) 
You always have instance code in your type's file, that's the law of no-orphan-instances. I don't think Types modules change the situation. We've achieved better decoupling, but not 100% decoupling. I do think that circular modules and hsboot (which I didn't add to Stack) is not the right direction, programming itself has enough infinite loops without introducing them in our build system.
Interesting discussion. Thanks for pinging me! If I understand the free alternative formulation that you linked to correctly it gives you distributivity by always factoring to the RHS of the equation: f &lt;*&gt; (a &lt;|&gt; b) = (f &lt;*&gt; a) &lt;|&gt; (f &lt;*&gt; b) As you say, the LHS is better for performance, so we prefer that in Earley. We _could_ always use the RHS but it would duplicate the work of `f` when parsing while the LHS doesn't. As far as I know it's not possible, in general, to always factor to the left in the equation; basically you'd have to be able to decide equality between `f` and `g` in `(f &lt;*&gt; a) &lt;|&gt; (g &lt;*&gt; b)`, which we can't because `f` and `g` may contain functions. In Earley the `Prod` representation thus violates distributivity (for performance reasons). However, distributivity is still "morally true" if we only care about the parse results and not the internal representation of `Prod`.
Do people also just not know about mtl? Who actually works with concrete transformer stacks?
I as well. It has the advantage that the functions mimic standard list functions more closely, doesn't declare new operators, and there is a single producer abstraction, instead of producers / transformers / consumers.
Sometimes they do! https://github.com/quchen/articles/blob/master/loeb-moeb.md
Some of these sections still sound very interesting though, like the one on FRP or the ones on parallel and distributed programming. How long is one section going to be approximately? Unfortunately I don't know of one single comprehensive resource that goes over all the nifty things you can do with the type system these days. It's one area where I think documentation is severely lacking at the moment, and the only way to explore it is through trial and error and reading the GHC manual a lot.
looks "responsive" to me
what you don't like using 5 browsers to run a few apps? also wow I just went to the electron site and saw gitkracken is in electron, phew glad i didn't buy that ... when I was thinking it would be nice to have a gui client for git every now an then
Each section consists of about 5 videos of about 15 minutes each. I was supposed to aim for 5 minutes each, but I have a lot to say :)
yes elm certainly counts as a high impact project. one reason it's not mentioned more is that a big reason to have high impact project is to be able to cite it to senior managers "look, language X powers [some well-known product]". elm doesn't work for that since the product is for other developers.
`*` [isn't wildcard search in github?](https://github.com/search?utf8=%E2%9C%93&amp;q=*) I feel like every day I find a new way they've screwed it up.
I didn't need to write a ghc wrapper script to use GHC on Rpi 3, I edited my GHC settings file `$prefix/lib/ghc-8.0.1/settings` ` ("C compiler flags", " -marm -fno-stack-protector -mcpu=cortex-a7"),`
Thanks I will add this if you don't mind
FYI: blog looks terrible on mobile! 
Exactly, my problem with higher order functions "in the large" is that it obscures somewhat the control flow.
Thanks for the tip! I'll look at it (even if I have no idea how to fix).
You're welcome! Can't help you that much with that either... Some responsive grid CSS might do the job... But from where I am at the moment I can't tell how to integrate it into the blog. 
Looks like things are a lot better now that there is a stack binary, removed 90% of the trouble ;-) What's the issue with `-rtsopts -with-rtsopts=-N`? That's kind of important. Does it reject the option simply because the threaded runtime is not enabled?
I cannot say why it's rejected because I don't know enough. It simply states that -N (use all cores) is not a recognised option.
I currently have only `-threaded`. I had to remove the following part `-rtsopts -with-rtsopts=-N` otherwise when I run the binary an error pops up. ``` $ hello-exe hello-exe: unknown RTS option: -N ... ``` At compile time no complains.
symDiff xs ys = [ x | x &lt;- xs, not (elem x ys)] ++ [ y | y &lt;- ys, not (elem y xs) ] 
What's so stupid about Electron?
I personally think that humanity needs a very lightweight, minimalistic, cross-platform browser with a good Javascript engine. It should be possible to bundle it with a web application and ship to customers. That way it would be easy to distribute cross-platform programs using for example Haste.
Allen _and Moronuki's_ book. Please don't omit one of the two authors!
That is true. I just thought of `fold` because it translates nicely with Accelerate and such. 
In addition to what others have said, if you don't write your own recursion or use higher-order functions you cannot have general "looping" behavior... You no longer have Turing completeness (and, at the same time, you don't get many of the benefits that some non-Turing complete languages have over Turing complete languages like provably terminating recursive/traversal behavior). I would say those two bullet points are in pretty direct conflict with each other.
Haha you're right, but thats not as quick to type or as easy to spell
I tend to make a src/lib folder for most of the code and then a src/binaries, src/tests and src/benchmarks (if in use) with subfolders for each binary, testsuite or benchmark. If you do want to test your code most of it should really be in a library you can import from your binaries and your testsuites and benchmarks.
I understand and agree we want easy good things as developers, just sayin that on the user end of the spectrum it's not all rose bushes; My laptop can only run so many of these electron apps at once. I suppose at this point it's our best choice for pretty xplatform UI's though..
&gt;You always have instance code in your type's file, that's the law of no-orphan-instances. And I'm fine with the law. It was just the result of the law that drove me away from the Types strategy. The instances alone wouldn't be a big deal but any common auxiliary function they use, etc., etc. Though in the end it looks like I came up with a similar solution to what you all have, just a different naming convention. With circular modules, I would tend to agree. I can only recall ever having the issue in C#, and we solved it by having a "common" assembly (analogous to the Types solution).
can it? or does it currently? 
Isn't it probably because the infrastructure to make STM look and feel right already existed in Haskell, but would have been hard work in the C# environment? Or is the whole thing closer to the metal and should have been portable?
I don't think it'd be hard for someone to write an Intero plugin for Atom. But I don't think Atom currently has anything as good as Intero.
I'll just keep trucking along then
I answered with `Foo/Bar/Mu.hs` only because of convention. In reality, I think I might actually prefer `Foo.Bar.Mu.hs`, or maybe even a mixture. But as it stands, convention is more important to me.
The fact `Foo.Bar.Mu.hs` is accepted is now my favorite Haskell feature. 
Following the `Foo.Bar.Mu.hs` scheme seems like it would get out of hand quickly for any reasonably sized project. 
I have 46 modules in a 5,600 line project at work. having them in separate directories is pretty useful for organization imo
I find directories *really useful* for organizing larger projects—they make the structure immediately clear and easier to navigate. Also, as I pointed out in [another comment](https://www.reddit.com/r/haskell/comments/5j2hpx/how_should_haskell_filenames_be_structured_poll/dbcy5d4/), existing tools can reclaim the benefits of a flat structure from nested directories, but not vice-versa. So to me, dropping the directory style is absolutely not an option, even if we were ignoring backwards compatibility. The question is then whether we want to support the `.` style *in addition*, on which I'm pretty ambivalent.
I tried keeping just -rtsopts and not baking -N and it runs fine. This is the output of --info http://pastebin.com/q36Ef7y4 With +RTS -N I still get unrecognised option. Here is the full output. I don't see any mention of cores or threads: http://pastebin.com/sKjfkRtq Thanks for helping on the issue! 
Why is it needed ?
Paging /u/chrisdoner, I believe you can answer this better than anyone?
Is Raspbian running as AArch64 native or is it still 32-bit? FreeBSD can run as native, now [even with SMP](https://lists.freebsd.org/pipermail/freebsd-arm/2016-November/015092.html) :) But Haskell support for FreeBSD/aarch64 (or FreeBSD/arm for that matter) is… not there I guess? Does ghc support AArch64 at all?
The main feature is that it works and works reliably. That's a priority over other features. Ghc-mod is well-known for not working or being hard to use.
I guess the complexity is in being kind polymorphic - I also struggled with that, but it's also quite useful in some situations. I guess a basic usage first would be good?
The limited number of operators certainly creates that look - maybe ad-hoc overloading like Idris does really is necessary to prevent too many crazy operator strings?
You sound like a vim user, so I suggest giving [spacemacs](https://github.com/syl20bnr/spacemacs) a shot if you want to get bootstrapped quickly. Emacs + vim bindings, and quick and easy haskell env setup with layers. This has been my editor for choice for the last year and a half now. Oddly enough I tried out the Atom Haskell stuff last week so that I could confidently have a quick and easy suggestion for people that wanted to get started quickly without having to learn some crazy editor. It was surprisingly pleasant, although I agree with others that intero would be a nice step up over ghc-mod.
Why? You have `unfoldr` and `iterate` don't you
Those are both higher-order functions.
I think that you should have to pick one style or the other. If you mixed both styles, you could create conflicts if both `Foo/Bar.Mu.hs` and `Foo/Bar/Mu.hs` existed. I believe mixing styles would also make it harder for someone to understand the structure of your project. Consistency is preferable, in my opinion.
Also, atom is known for publishing your usage data for google analytics by default. You have to opt out this feature manually. Which sucks. This made me very reluctant on trying out Atom.
Yea, but it's easy enough to disable, and informs you of it on the welcome screen. I don't see why it should make you reluctant to try it, especially since you already know about it.
I know it's easy to disable , but it's really off putting for me. Of all the things in the internet, now even a text editor wants to track you.
Playing devil's advocate: They don't want to track you, they want to improve their product, which means knowing how it's being used.
It's kind of terrible in my experience. Many standard vim controls are either buggy or non-existent. It also feels slower, to me. Vim's plugin ecosystem is much better to me, as well.
I personally think both should be supported, even though I personally would pretty much always use `Foo/Bar/Mu.hs` because I could see `Foo.Bar.Mu.hs` being useful for very small projects. One other situation I could see the `.` style being useful is in situations like `Foo/Bar.Mu.hs`. Having `Foo/MyType.hs` and `Foo/MyType/Internals.hs` (for smaller abstract data types where you don't need any more than just those 2 files) or something like that is pretty annoying, as it means `Foo/MyType` is a directory with just one file. So I would love to have `Foo/MyType.hs` and `Foo/MyType.Internals.hs`. I am hoping that it was implied that "both" included such an option. On a side note one thing that annoys me is that often `ls` essentially shows every module twice, once for the file and once for the directory. Is there any chance we will see something that is equivalent to Python's `__init__.py` in future? The specific naming scheme of such a file I don't have much preference on, but something of that style.
If we take into account the number of users, atom has already surpassed emacs (At least according to the developer survey in stackoverflow. But is this representative of the total population of emacs/atom users? I do not know). Since Atom is basically a browser configured to be an editor, it'll never be as fast/snappy and memory efficient (i can open as many files or processes I want and emacs never chokes) as emacs/vim
You've never been exposed enough to basic material. Unless you're able to cite it word for word. Your plan sounds great. Read and practice. Theory reinforces practice, practice reinforces theory. Do both. Also go back and from from more advanced to more basic stuff to get exposure. Basically, take the ideal path in the book, and add noise.
I believe the question was more about: Why would you need a directory for each style? Most people would rather have one or the other; not both.
Only time will tell, Atom has a lot of potential. Till then, I'm really happy with what emacs offers.
I would be very happy to do so but I'm not sure what I should file. I just run `stack setup` to get ghc :) Maybe something like "-threaded present but no -N option unrecognised on ARM"? Maybe you can provide some guidance on this? You are very helpful. As mentioned in the post while doing so I got an error while sanity checking but everything installed just fine. Here is the error http://pastebin.com/8GA8ndFu This is 100% because ghc is not having ``("C compiler flags", " -marm -fno-stack-protector -mcpu=cortex-a7")`` while compiling stack's sanity checks. I don't think it's necessarily related.
In some sense it's higher-order because the dictionary for `Monoid m` contains `mappend`, so after some desugaring it's basically this: ``` fold :: Foldable f =&gt; (m, m -&gt; m -&gt; m) -&gt; f m -&gt; m ```
I understand both to be mutually exclusive, not to allow mixes. 
Maybe, however I've been asked my opinion , I'm just saying that I would have voted "mixed" were it available.
That seems very unnecessarily restrictive, I can't really see any benefit in not allowing mixes.
That's true. 
I mean why do you need a flat listing ? Within the command line `tree` or even `find` list everything flat. Within a text editor, that doesn't really make a difference doesn't it ? 
Same here. The convention of organizing by directory is deeply ingrained in the ecosystem, so it's hard to predict what will happen in practice. I would suggest trying it for a package or two and let's see how it goes. If after a while it becomes clear that no particular problems arise, then great, I change my vote to allow both. If problems arise, well, let's see what they are and take it from there. EDIT: See post of /u/hvr_ elsewhere in this thread.
I hope this is correct: https://ghc.haskell.org/trac/ghc/ticket/13007#ticket
Looks great, really curious if there's an actual regression or just some user/install error. The fact that nobody else here has chimed in with a 'duh' type response makes me think this might be an actual bug ;-)
For the use cases it was intended for it seems fine to me, pretty light even. Where I imagine it becoming cumbersome is when it starts to be creatively misused. Similar to how templates in C++ had very acceptable syntax for polymorphic containers but became burdensome when [SFINAE](https://en.m.wikibooks.org/wiki/More_C%2B%2B_Idioms/SFINAE) was (ab)used to turn templates into a type level programming language. Those burdens are being addressed in be and upcoming iterations of the C++ standard. However, not knowing anything more about backpack other than watching this video, I don't know whether it would be possible to do anything strange and wonderful like package level programming using it. Edit - added link 
I liked that the level was right to to the point of clipping, it meant I could hear it on my crappy phone speakers over the sound of doing the washing up.
It may be "cumberstone". But too much boilerplate to write? We have full duplication of source code in unix, with duplicated bugs and not duplicated maintanence patches. I don't see how this can be more boilerplate than duplicating the whole packages to change the string Rep. Actually it may remove much of the boilerplate haskell code for (bytestring,text,string) hanging around with some "cumberstone" signatures.
I've been using llvm-general to write a compiler for a simple imperative language as part of my bachelors thesis, and so far it's been quite stable. Take a look at Stephen Diehl's tutorial to get started: http://www.stephendiehl.com/llvm/
Thanks
Have you ever heard anybody complaining that ML Functors are too cumbersome and too much boilerplate to be practical? No, everybody says they are a great and distinguishing feature of the language and they allow doing amazing things like, for example, MirageOS people do (I refer to how incredibly easy it is to develop an application on host machine and then keep swapping leyers to gradually migrate the application to become a unikernel). So, it's been demonstrated that such capabilities are very much needed and Haskell was lagging in that respect. Now, with Backpack, Haskell not only catches up to ML but, if I understand correctly, it actually exceeds what's possible in ML. I am eager to see how it'll be adopted and what people will be able to do with it. Also, it seems like our only realistic shot at solving the "string problem".
I'm on my crappy phone in a noisy environment, your comment said exactly what I needed — thanks!!
OK, quick reply from GHCHQ, looks like an actual bug. Duplicate, though. I googled a bit for this before suggesting to file one, didn't find any existing bug. Ok! But this tells you a lot about the state of Haskell on ARM. A major feature, actual multithreading, is broken in the current release of GHC and it takes &gt;6 months for somebody to even notice ;-)
have you tried vim plus?
I agree. If one finds it annoying to, say, click multiple times on GitHub, that sounds like an issue with GitHub rather than Haskell. Likewise with editor tabs, commandline screen space, etc. In these sorts of situations, I'd tend to favour clear improvements (e.g. adding Emacs-style disambiguation to other IDEs) rather than regressing to towards the lowest common denominator. In the worst case these sorts of compromises give us things like `MYMODU~1.HSK`, `MYMODU~2.HSK`, ... I think it's backwards to treat `Foo.Bar.hs` as a "feature" for nesting module hierarchies without requiring directories. Rather, I'd say that it's the principle of least surprise: if a user doesn't know or care about module hierarchies, then the hierarchy support should try to stay out of their way. If the user wants to call a module `Amazon.ComScraper` in a file called `Amazon.ComScraper.hs` then that doesn't really *harm* anyone, the filename and module name match, so the tooling shouldn't complain. The converse situation is easier to handle: few, if any, filesystems allow the `/` character to appear in filenames, so it's reasonable for tooling to treat paths like `Amazon/ComScraper.hs` as hierarchical module structures. So really it's a question for each package about whether it wants a hierarchy, and hence use directories; or whether to have no hierarchy, and hence call the files whatever they like, regardless of whether they contain `.` characters or not. I'd ask those who are tempted to eschew directories but still pretend they're using hierarchies, e.g. manipulating them with scripts, etc.: Are you just "reinventing the square wheel"? If someone volunteered to implement your scripts as a FUSE filesystem, would you jump at the chance? In which case, you're probably just reinventing directories.
I think it was just called vim mode. I'm willing to give another one a shot, though. It still lacks the plugins I want (like nerdtree) though unfortunately.
Text, Map, etc. are slow-moving targets: the domain they're modelling won't really change. There may be optimisation advantages of using a different representation (e.g. adding a `Skip` list constructor to allow fusion of `filter`) but they're not found very often, and usually appear in a new package. On the other hand, libraries like haskell-src-exts and those of Template Haskell are modelling a moving target, so they break more frequently.
On the bright side it will be fixed with ghc 8.0.2 . Kudos for the hue dashboard btw.
Directories is how practically all other languages do it, no? I'd rather conform to the de facto standard on this one.
From what I can gather it works with more than every third or so ghc release(?)
isn't too difficult to get arch running, had no problems with ghc (iirc) just do `pacman -S stack` and you're good to go Generally on the rpi just make sure not to compile too much stuff in parallel, ghc is using a fair chunk of memory
Do *all* exercises, that's what makes the difference.
&gt; I use some heterogeneous lists in Nomyx (maybe an anti-pattern) I'm inclined to say yes, heterogeneous lists in Haskell are usually an anti-pattern.
There is nothing magical about lenses, so yes, you can of course export them. Lenses created with TemplateHaskell are just plain old functions, however if you have an explicit module export list, you have to include the individual lenses explicitly as well, just like any other function. E.g.: {-#LANGUAGE TemplateHaskell #-} module Foobar ( Foobar (..) -- this exports the Foobar constructor and the raw _foobar , foo -- this is needed to export the actual lens ) where data Foobar = Foobar { _foo :: Int } -- This generates the 'foo' lens; it does not appear explicitly in the -- code, but other than that, it's as if we had written: -- foo :: Lens Foobar Foobar Int Int -- foo = ... makeLenses ''Foobar 
Take a look at [coerce](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Coerce.html).
The thing I like about `hpack` is that you can factor out common dependencies, extensions, etc... For example, if both your library and executable depend on `text`, you can just factor out `text`, writing it only once and `hpack` will create a cabal file where it is in the correct places.
Last time I tried it wasn't possible because even if you create a 'Target' with 'targetContents' set to 'Just (yourcode, date)', GHC tries to find the module file to check if your version is more recent or not (using the date you provide), then it warns because the module file doesn't exist...
The library uses `vector` internally, and the modification function is based on [modify](https://hackage.haskell.org/package/vector-0.11.0.0/docs/Data-Vector.html). From the documentation, it will be O(1) it is safe to do so, otherwise, it will copy the vector/matrix, so maybe the documentation is not 100% accurate.
By construing the types, I suspect you are supposed to set the `targetContents` field of the `Target` type to use in-memory code. It expects a `StringBuffer` which you could acquire from your `ByteString` by `StringBuffer.stringToStringBuffer . Data.ByteString.Char8.unpack`. Please note that there are the packages [hint](https://hackage.haskell.org/package/hint) and [plugins](https://hackage.haskell.org/package/plugins) specifically targeting your use case.
Thanks! Also thanks for writing this up. I ask a lot of dumb questions but I try to give some back by condensing what I learned into a writeup, a gist or some piece of code. GHC on anything but Linux x86-64 can be a bit difficult. I develop Hue Dashboard on a Mac and deploy it on an RPi3 and there's always something broken on either Mac or ARM, and it can be quite frustrating when there's basically no version of GHC available that is working on both ;-)
Why is `Pointed` controversial? `pure` being a natural transformation between `Identity` and `f` is not enough?
`coerce` is a much stronger relationship than isomorphism. `coerce` implies that the types have identical runtime representation. Isomorphism just means you can convert losslessly between them.
I'm not familiar with the debate, but I would guess there are few useful `Pointed`s that aren't also `Applicative`s. After a bit of searching, it also seems like there are some pedagogic etc. concerns: https://wiki.haskell.org/Why_not_Pointed%3F
We could always use sysadmin support if you have experience and some availability! Anyone who wants to do so should reach out to me over direct message and we can talk about your skills and availability, and from there to the sort of projects you might be able to help out with.
Maybe you've solved it since then, but here goes. You call `go` recursively before modifying the state. So when you are compiling a jump instruction, the previous nodes of your program have not been added to the map yet (they will be after the current call to `go` returns). You can use `mfix` to tie the knot here, by inserting the current node in the map before calling `go`, with the node still depending on the result of that call.
&gt; I get an exception from (!.IM) [sic] that the desired Int [...] is not in the dictionary. [...] My gut tells me there is some hidden strictness somewhere in the code forcing the evaluation of the IntMap prematurely, but I am not sure. That doesn't sound right. Laziness has an affect performance and on the propagation of bottoms, but unless you're using something like `unsafePerformIO`, it cannot affect whether a particular key is in a Map or not! &gt; I am using a lazy IntMap as the layer of indirection. That's probably the source of the problem. I'm not sure what you're expecting laziness to do for you here, but it's not doing what you think it's doing. I recommend ignoring laziness and to write your solution as if Haskell was strict.
Isn't []-based parsing pretty exponential in the worst case?
I've worked on perf/latency sensitive Haskell services used in ad tech, among other things and never once had space leak in production. I know others have, but knowing when to use a streaming library, strict-in-the-leaves/lazy-in-the-spine and some other guidelines have spared me the grief. I've only needed to diagnose a space leak when poking at a library I didn't write. (Once) Partial functions are not a big deal for us as we avoid partial functions and don't use the regular Prelude in our work projects. They're primarily a problem for new people.
Python is very easy to reason about, has an easy and powerful debugger, and simply much more libraries and industry support. Yes, it doesn't make any promises of correctness, but in return you get much greater flexibility, and no promises is really better than a strong but false promise. I know that Python can fail and I will test the hell out of it, and it has plenty of libraries to enable that. I expect that if Haskell compiles, then it is correct, but in reality its base components are just as fauly as Python's. In the end it's simply impractical: Haskell is objectively more complex and less industry-ready than Python or Java. It supposedly makes up for that with strong correctness and optimization promises, on the level that no other (non-functional) language can deliver. If those promises are fake, what's the profit of using it? Frankly, I'm baffled that the main reference for the language is 98's standard. 18 years have passed, people! Even C++ had several standard changes in the meantime! Meanwhile Haskell looks like it didn't evolve, at least not in the usability realm. Everybody hates partial functions and many strict libraries are on Hackage, but the base distribution is the same as 20 years ago.
Just a thing, it is possible to use `yesod` with 8.0.1 one just need to `import` some missing constructor in the module which fails with **impossible had happend**... 
&gt; The answer is: Yes! ~~Haskell~~ A type system can do that. One possibility would be to start from something like [call by push value](http://www.cs.bham.ac.uk/~pbl/cbpv.html), like [Frank](http://homepages.inf.ed.ac.uk/slindley/papers/frankly-draft-march2014.pdf) does (among other things). 
Looks like a little trolling... are you a regular haskell user? You cannot reason about Python. Say you have a function "parse_xml(x)". What's supposed to be input? Output? Is it thread-safe? I was just designing a db access library; I was actually surprised how flexible haskell is. People flock from Java/C* to python because these languages are inflexible (it's getting better. slowly). I couldn't have done these things in Java; I could have done it in python with no static guarantees. I can build it in Haskell. With static guarantees. Additionally, I can use type inference; the compiler essentially writes code for me. I can use TemplateHaskell; it's not that pretty, but does the job. I can hack symbol tables in python to the same effect. Java still has its Null. Haskell is basically GHC and GHC has changed a lot (see e.g. the AMP). We might drop the partial functions from Prelude, but they seem quite good for beginners. Maybe we should teach beginners not to use them anyway.
&gt; Have you ever heard anybody complaining that ML Functors are too cumbersome and too much boilerplate to be practical? Yes.
Luckily, the partial functions in Prelude are very obviously partial, so they're easy to avoid. Space leaks are also easy to avoid if you follow a few rules of thumb, like the ones /u/bitemyapp mentioned.
For context, the backend of our product is at about ~500 Haskell modules now and I've yet to see a single space leak in the past few years. And I'm not exactly a super refined Haskell developer either, so that's not it.
I'm not familiar with purescript, but I can imagine that if the lack of the Partial constraint entitles the compiler to assume a function is total, it can get you quite a bit in terms of strictness analysis and moving computations to compile time rather than run time.
Mostly? I just got bored and never finished it.
Relax, it's not so bad. If you write some large programs you'll get a feel for what's a real problem and what's a theoretical problem.
&gt; Haskell is objectively more complex and less industry-ready than Python or Java. Huh? I don't think Python is used in industry that widely. Things like "objectively more complex" fall apart since most people learn imperative first. &gt;Python is very easy to reason about, has an easy and powerful debugger, and simply much more libraries Which is "easier" totally subjective though. Haskell has `hlint` and `ghc-mod` which do work quite well as debuggers. "More libraries" isn't really a good thing when so many python libraries are just not that great. Haskell has *multiple* libraries in multiple domains that make the language - e.g. `yesod`, `lens`, or `aeson`. &gt;n return you get much greater flexibility, and no promises is really better than a strong but false promise. I know that Python can fail and I will test the hell out of it, and it has plenty of libraries to enable that. That sounds like "re-implementing typechecking" to me. Increased flexibility is good in many domains and bad in many domains. &gt;Frankly, I'm baffled that the main reference for the language is 98's standard. 18 years have passed That is not the case. Haskell2010 exists as a reference, and since GHC is the only compiler really used, you can use language extensions essentially without too much fear of incompatibility. &gt;Everybody hates partial functions and many strict libraries are on Hackage, but the base distribution is the same as 20 years ago. Aside from lazy IO, the strict libraries fix most of the issues with laziness. 
Arguably, it buys you a lack of overhead, and it lets you write "idiomatic" rather than monadic-looking code if you have a delicate section that _should_ never fail but relies on some algorithmic invariants being maintained correctly. I don't feel strongly about this going in base -- honestly just giving a total prelude sans-constraints seems the easier solution for now, if we gotta disrupt stuff anyway. Just pointing out some potential arguments for such an idiom in general... (and indeed, compiler support for some totality checking, etc also seems like it might benefit here).
Looking at the [Haddocks for `read`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#v:read), I don't find it so very obvious that `read` is partial. The haddocks for the other partial functions listed in the post are more explicit about what inputs are expected though.
[`Text.Read.readMaybe`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Text-Read.html#v:readMaybe) (and [`readEither`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Text-Read.html#v:readEither)) should be more prominent, a lot of people seem unaware of them
It's not because of type classes. It's because of currying. In the same way that `foo :: a -&gt; b -&gt; c -&gt; Bool` is not a function of three arguments returning `Bool` but is instead a function taking a single argument returning a function (etc...), `Either` is not a type constructor taking two parameters, but a type constructor that, when applied to a type, returns a type constructor taking a single argument. Eg, `Either :: * -&gt; (* -&gt; *)`. `Either a b`'s monad instance is "Either short circut with an `a` or evaluate to an `b`." This can be error *or* success, depending on what you want out of it.
That's a nice thought, dunno about the physics metaphor though just in that the structural symmetry remains, it's just the notational symmetry breaking, though this could've just been what you meant.
How about creating a temp .hs file with [temporary](https://hackage.haskell.org/package/temporary)?
&gt; and don't use the regular Prelude in our work projects Wait what? Is that a thing? What do you use in place? Rolled your own of much in Prelude? (Semi-newbie asking)
I think a simpler way that you can think about what u/edwardkmett said (though you'd be surprised how simple the code/math is that describes the things he said), is that RankNTypes can help you constrain the behavior of your functions in a much cheaper way than hiding your implementations and other classic tricks. The best example of this is using types like this: type NumFn = forall n. Num n =&gt; n -&gt; n If something is of type NumFn, you can almost imagine it's "closed" in some sense under the things you can do with numbers. It seems like it would make your function super general or something, as it is universally quantified, but the whole point of that is actually to constrain the implementation space, which allows you to transport your logic between all of the domains which support the necessary abstractions. One non-simple but incredibly practical uses of RankNTypes is the Codensity Monad, which has the following type: newtype C f a = C { unC :: forall b. (a -&gt; f b) -&gt; f b } which is a Functor via fmap f c = C (\g -&gt; unC c (g . f)) and a Monad via return x = C ($ x) a &gt;&gt;= f = C (\g -&gt; unC a (\b -&gt; unC (f b) g)) and it can automatically right associate the monadic operations you perform in the "host" f, which for the case of the list monad for example can actually yield speedups of simple stuff like concatenation, which can yield O(n^2) behavior if you have nested applications, but can be reduced to O(n) if you use Codensity. It's pretty cool in my opinion, and though it's not simple necessarily in understanding it is in terms of application. That being said, it's also quite simple in understanding if you give it a little bit of thought and trace some code. This was probably more of the same as above, but I love using RankNTypes and I especially love seeing how others use it, plenty of which I can thank u/edwardkmett for. It's a unique style of programming that you're able to do when you have that feature on your team.
Haskell is easier for me to reason about than Go is. The top reason being type assertion nonsense/interface{} and lack of generics.
&gt; Use classy-prelude which is as far as I can tell the most popular alternative prelude. That's odd. According to your link, `basic-prelude` is way more popular in terms of [reverse deps](http://packdeps.haskellers.com/reverse/base-prelude). 
You may like to read [this](http://publications.lib.chalmers.se/records/fulltext/232550/local_232550.pdf) experience report (talk [here](https://www.youtube.com/watch?v=zi0rHwfiX1Q)). Additionally, [this](http://www.haskellforall.com/2013/11/test-stream-programming-using-haskells.html) blog post might be relevant. I can summarize as follows: Make a model, then test that your model accurately represents your system by generating a trace of operations and comparing the real system with the model.
Ok but you don't use function randomly either. When you use read and are trying to convert a string to an int, its pretty obvious than the string can be not well formatted. So you should think what happend in that case. There is only three options, crash return a default value (probably 0) or Nothing. As Nothing is not an option that leaves only 2. Option 1 is the only "reasonable". It's the same for head, it's obvious that it is partial especially if you come from an imperative background when you are supposed to check that type of things all the time. Having said I wish readMaybe was in the prelude. I don't need headMaybe because never use it but pattern match instead. 
So, how about this for an initial proposal: * Add `readMaybe` to Prelude, possibly `readEither` too * Make the Haddocks for `read` a bit more cautionary Does this sound right? What's the proper way to propose such a change, write an email to the Libraries mailing list?
So, basically this is the Elm/React+Redux architecture for the server? &gt; Now, you can make assertions about the results of running an arbitrary set of actions against your domain. **How** does one write this without essentially going back to the regular way of testing? "Regular way" = if the sequence of actions is X =&gt; Y =&gt; Z then the resultant state of the system should be P. 
Yeah the type lambda thing confuses me a lot because I'd imagine that combined with the ability to define a fixed point of a type, you'd essentially have general recursion in the type system... Confused how they dodge this in Scala, though for all I know subtyping puts them right outside of decidability anyways in regard to type inference and the like. By the way, been following the backpack story for a while, really excited to play with it after my finals are over! I love ML modules but hate having to leave Haskell with its various features of higher order logic to utilize them, real excited to see what people make with this new toy you've given us :D
Sure it is! That endofunctor is just a bifunctor =P
That's basically what I need indeed. Thank you very much. However, I'm surprised that it's not already part of a package (.i.e convertible). I was hoping it could be achieve without any code using things like [eot](https://hackage.haskell.org/package/generics-eot-0.2.1.1) or equivalent. 
&gt; Or even Rust, with a good static type checker. These languages are lightyears ahead of Haskell in their ease of use and learning curve. Trolling, yes.
I used to use `vinyl-plus` for open sum types, but it seems to be unmaintained now (/u/andrewthad?), and you can't use the version on Hackage in GHC 8. It's also a linear-time/space solution, which is fine if you're striving for purity. But a `Typeable` approach would be much more performant, right? A good open sum type would mostly solve the problem on its own. If we steal PureScript's syntax, we'd be able to do something like this: foo :: MonadError (http :: HttpException | e) m =&gt; m () bar :: MonadError (some :: SomeException | e) m =&gt; m () baz :: MonadError (http :: HttpException, some :: SomeException | e) m =&gt; m () baz = foo &gt;&gt; bar
Traversable is an interface for finite containers that support internal iterators. The effects for each iteration step are combined using Applicative. Think of it as containers that support "forEach".
You need RankNTypes to use church-encoded natural numbers in Haskell.
&gt; Stack for FreeBSD 64 bits obviously it's amd64 not aarch64
&gt; Haskell, say you have a function "return x". That's not a function..... do you mean "f :: IO a"? That never returns. Or "f :: a -&gt; IO a"? That either returns it's input or never returns. You can get quite far even with IO. Is it thread-safe? Potentially no. When will it execute? It's in IO monad, that's actually well-defined. Lazy IO - did get burnt recently with one library, nobody uses it anymore, I'd say lazy IO is a bug. Anyway, where did we get from "you can know a lot about many haskell functions and pretty much nothing about most python functions" to "you don't know everything, therefore it's no use"? You don't know if a function isn't a bottom in many languages yet we still can know a lot about the function. &gt; Is Haskell much more easier than them to "reason" about, all things considered? I don't know much about Rust - it's a modern language designed with strong typing in mind. Comparing haskell to Go seems to me strange. You cannot do a lot of things in Go. Yes, haskell is easier to reason about, all things considered. &gt; Sure in Haskell, you can look at a signature and see it does some IO by looking at the signature. But does the community have a notion of how valuable this kind of capability is and what the price users are paying for that? Quite valuable - e.g. you cannot do STM without it, free monads would be quite impossible as well. If you do unit testing, you should structure the code as pure/unpure anyway, so the price is very low. Nobody forces you to do so - you can do everything in IO, but why would you do that?
&gt; Lazy IO - Please stop bad-mouthing lazy IO. It's by far the simplest IO approach, and it works great for simple situations with no more risk than any other more complex tool. But yes, if you need to do something complex you need a more complex tool, life is like that. Lazy IO is a bug. I just got a nice bug in my code with HDBC (probably the only one prominent lazy-IO library still there) and I was quite suprised that these 2 things produced different results: res &lt;- withDb $ query somthing print res withDb $ do res &lt;- query something liftIO (print res) 
False positives are impossible. It would render pointer equality completely unusable, and certainly in that case it would not be used pervasively in [`unordered-containers`](https://github.com/tibbe/unordered-containers/blob/master/Data/HashMap/Base.hs#L1243), for instance. Pointer equality is uninterruptible by GHC RTS. Interrupts can only occur at heap and stack allocations, and nowhere else. (Sometimes this can be problematic, for example fusion may eliminate all allocations for some relatively long-running list functions, thereby making them non-responsive to timeouts and other async interrupts) I don't know how exactly false negatives could happen, but in my own benchmarking I never managed to find one, so I assume it's not very common. In short, pointer equality is a completely fine optimization tool.
Is there any law that says these two need to be equal? It's not obvious to me at all.
 force :: NFdata a =&gt; a -&gt; a res &lt;- force &lt;$&gt; (withDb $ query something) res &lt;- withDb $ force &lt;$&gt; query something I'd say yes, it is. `Force` isn't supposed to change the result (NFData law?), withDb isn't supposed to change the result (it's `m a -&gt; IO a`). To avoid NFData - the result of query is a list, running `seq` on elemnts of lists does the same and `seq` documentation says: &gt; The value of `seq a b` is bottom if `a` is bottom, and otherwise equal to `b`. We can eliminate the `force` (up to bottom) and the results should be equal.
&gt; Python is very easy to reason about [...] Yes, it doesn't make any promises of correctness wat
I am hoping to make it public over the holidays. I need to write it up for a broader audience for it to have a chance to be adopted in other languages.
I'm also pleased to see the warning off abandonware. With my sysadmin hat on, I know to vet the source of non- or semi-curated packages before relying on them; but this isn't always obvious for everyone.
 &gt; these are the absolute essentials in every friggin introductory book, course, tutorial etc. Now all of a sudden "don't use them".. Lol, something to dig into over x-mas. I don't think people are saying to not use essentials, but safe version of thoses. You'll be forced to deal with `Nothing`, as you note, but that's just exception handling. If you don't code for that, whatever language it is, you are lying by omission. AFAIU you can't really do much better on partial functions. But still, Haskell helps dealing with the sea of maybe: just by chaining computations in a maybe monad, you don't have to unwrap data at every computation. 
Quick question, why is `GConvertible` needed. Couldn't we reuse `Convertible` for the generic classes ?
yeah, i'm not convinced it's worth the conceptual overhead. Unpacking Maybes has never been the bottleneck in my programs. (Agreed on total prelude.)
I think that you should use whatever you want to use, but I also think that Haskell is a great tool for writing compilers. Compilers are essentially about transforming trees. Haskell is very nice for doing that, and has a good deal of libraries that help with such things. Additionally, there are Alex, a lexical analyzer that generates Haskell code, and Happy, a parser generator that also generates Haskell code. They work very similarly to the classic lex/yacc combo. They can also work almost seamlessly together. You basically end up taking your grammar and converting it(nearly directly) to sum types, defining your tokens in the lexer, writing your grammar for Happy and plugging together the datatypes from before, and then you can plug the lexer from alex directly into happy and get a function that lexes and parses the code. All in all, it makes for a pretty pleasant experience. I can't comment on rust very much, as I have only barely tried it. But I'm sure the people over at /r/rust will be happy to oblige. 
&gt;That's not a function..... Oh yea? "parse_xml(x)" is a function but "return x" is not? Are you being pedantic just to ridicule me? &gt;When will it execute? It's in IO monad, that's actually well-defined. If it is actually well defined then why the issues with lazy IO? &gt;you can know a lot about many haskell functions and pretty much nothing about most python functions No. It is more like "You can know a lot about many haskell functions, but that info is mostly useless to the day to day programmer. And you can know all there is to know about a python function pretty easily, even though it take a bit more than a look at the type signature.".
You know, this "trolling" accusation is a symptom of communities that are not really sure of themselves. Do you know where you can see it frequently. /r/php. Don't be like that.
Certainly workable. In fact that's exactly the direction we're headed in our first-stage prototype for adding linear types in GHC. Point is, one minor and one major downside to using an external heap are: a) you have to allocate/free explicitly, b) manual memory management is hard to get right for any non-trivial mutable data structure, so you may well segfault. So modulo these caveats you can do this today with just Haskell's existing FFI. What we (Tweag I/O) are doing today in our first-stage implementation is to focus on making it possible to assign safe types to the FFI bindings, which will guarantee the absence of segfaults. i.e. solving (b). Later, we'll try to solve (a) too. But that's a more invasive change. As an aside - the original use case at Pusher is a great use case. So thanks for sharing your challenges! We'll make sure to use it to illustrate the significance of this line of work.
I would hardly call Lazy IO the simplest approach to IO when it requires unsafe functions just to define. Compare the strict IO functions in Text.IO which are much easier to understand.
There's a Purescript that compiles to javascript and is written in haskell. You can check its code to learn.
With regards to 3: my understanding is that static instance resolution was one of the primary characteristics of type classes. Is this only true for Haskell '98 classes? If so, do you know what feature exactly forces us to give up on static resolution? 
Also realize that a lot of people who are relying on an alt prelude are people writing applications rather than libraries, likely closed sourced apps at that. In that sense I'd imagine total downloads would be a more useful scale to look at than reverse deps, but a survey of industrial Haskell folks would perhaps be the most useful. I personally don't use an alternate prelude and I regret it. I definitely will in the next major project.
Yes, it can be statically resolved. That doesn't mean GHC will always use that information to prevent dynamic dispatch. To do so, it has to properly specialize all functions in the call stack. And as that article describes, specializing isn't always going to happen.
You could use vectors of storable elements to allocate on the C heap: module Main where import qualified Data.Vector.Storable as Storable ( Vector, replicate) import qualified Data.Vector.Mutable as Mutable ( IOVector, new, write) import Data.Word ( Word8) import Data.Foldable ( for_) import Control.Exception ( evaluate) windowSize = 200000 msgCount = 1000000 type Message = Storable.Vector Word8 type Buffer = Mutable.IOVector Message mkMessage :: Int -&gt; Message mkMessage n = Storable.replicate 1024 (fromIntegral n) pushMsg :: Buffer -&gt; Int -&gt; IO () pushMsg b highID = do m &lt;- evaluate (mkMessage highID) Mutable.write b (highID `mod` windowSize) m main :: IO () main = do b &lt;- Mutable.new windowSize for_ [0 .. msgCount - 1] (\i -&gt; do pushMsg b i) This reduces max GC pause time from 0.0855s to 0.0275s and bytes copied during GC from 4,183,209,208 to 149,888,472 when compared to using unboxed vectors for messages.
If you use a polymorphic function from some library and you give it some value of your own type, how do you avoid dynamic dispatch if you're the one providing the implementations for your type, but the function is the one using them? The answer is that you specialize the body of that function, producing a new function that automatically uses the implementation specific to your type. In order to do this, however, you need the code to that function available.
That's just a fancy fish.
Whoa, I just saw that `string-conv` was removed from Protolude back in July. That was the main thing holding me back from using it (while I think `string-conv` is a good idea, I also don't want a way from going from `ByteString -&gt; Text` without handling errors -- `string-conv` is either partial or uses replacement characters depending on how you use it). Maybe we should all switch to Protolude.
Might be helpful http://www.purescript.org/learn/eff/ Knowing PureScript is definitely worth learning. There's not a lot, but what's there is all quite lovely.
Ah yeah that's a common enough one that I wonder if `modifyIORef` has a legit purpose.
That sounds good, yeah send it to the Libraries mailing list. The documentation for `read` should mention `readMaybe`, `readEither`.
I thought it was fairly strongly implied that bottom should only be optimized out when there is an unambiguously correct value that it could be optimized out to. I will think about how to formalize that now, but basically something on the lines of what it would theoretically return if it were given infinite time and could sum an infinite structure or similar. 
I'm assuming that the primary outcome you're looking for is a learning experience. Using Rust will teach you about performance, memory usage, and concurrency above all; using Haskell will teach you about laziness. I've generally found Rust to be an acceptable language for getting shit done, in contrast to Haskell where I get bogged down in analysis paralysis. However, not everyone is like me, and if you're able to get past this particular hurdle you might find that Haskell "gets out of your way" better.
I'm familiar with Ocaml and I think all ML languages are similar syntax-wise to that. Haskell's syntax is characterized by layout rules and 'where' clauses for me.
Other actively maintained ones: [Clean](http://clean.cs.ru.nl/Clean), [Curry](http://www-ps.informatik.uni-kiel.de/currywiki/).
If you feel like it, try running it with `+RTS -K1K`, which thanks to Neil Mitchell is becoming the go-to method of finding a large class of space leaks. He has [a blog post about it](http://neilmitchell.blogspot.de/2015/09/detecting-space-leaks.html), and also [gave a talk at this year’s Haskell Exchange](https://skillsmatter.com/skillscasts/8724-plugging-space-leaks-improving-performance).
We do, it's `F` itself. The `Functor` typeclass in haskell is limited to functors between the category `Hask` and itself. `Hask` is the category that has haskell data types as objects, and haskell functions as its morphisms. When reading about category theory, you'll often see discussion of the category `Sets`, which is approximately `Hask`, the objects in question for `Sets` is the *domans* and *codomains* of the functions `Sets` has as its arrows, not the (set theoretic) values those functions act on. It's the *types* that dictate which functions compose. The mapping between arrows, which should map between haskell functions to haskell functions is `fmap :: (a -&gt; b) -&gt; (f a -&gt; f b)`. The mapping between objects is a mapping of types, and thus exists at the type level instead of the term level, thus, `class Functor (f :: * -&gt; *)`.
Have you benchmarked sums based on vinyl? (I haven't but) afaik, for types like the one below, access/update can be constant when enough is known statically. you define the operations against vinyl's rget/rput, which are methods. https://github.com/sboosali/vinyl-sum/blob/master/sources/Vinyl/CoRec.hs
I saw the video and skimmed through the paper. Brilliant stuff! Thanks for sharing. Do you know if complete QuickCheck tests for the circular buffer example have have shared somewhere? I'd like to go through them to understand how it's possible to build the "model" without actually rewriting a circular buffer in the test itself. And as a side remark, the dets story is why I don't give up on RDBMS even with all the marketing hype around the noSQL databases. 
Hm it does appear that `rget` does not require `UndecidableInstances`, so it *should* be possible to perform in constant time. It is still linear space though. Also, there's no guarantee GHC will do the inlining well enough to be constant time. `rget` and `rput` don't have any inline pragmas.
Sounds good, thanks for the tip!
fwiw, (1) I've never had a "fatal" space leak, and (2) I use total functions almost exclusively (for example, from the safe package) and a custom prelude, which also prefixes partial functions like "unsafeHead". Took a day of work, but didn't feel disheartening. Head is the lower bound: it's like you're writing in Python or Java for ~1% of your program (which you do anyways when writing bindings or, more broadly, interfacing with existing systems). &gt; what is left of the Haskell's safety and purity promise in the end? Its safety and purity. IO and Maybe and 1000 other things. 
Yeah, a custom prelude: https://github.com/sboosali/spiros/blob/master/sources/Prelude/Spiros.hs Mine has: no partials, some classes like Generic / Hashable / NFData, some operators like (&gt;&gt;&gt;) for "readable" left-to-right composition, and a few extra utilities (like lstrip/rstrip). So, I didn't roll my own, I just hid some names (e.g. head) and reexported some others (e.g. safeHead). 
Or how you know you've failed at communication...
Ok I see what you mean there. In that case we could perhaps denote some things as a true `_|_` and some things as "won't terminate but semantically should be `x`". Then `V.take 5 $ V.fromList [1 ..]` would have value "won't terminate but semantically should be `V.fromList [1, 2, 3, 4, 5]`". Which I guess is more or less what you are saying. Are you sure that such semantics could not be derived automatically most of the time? I honestly don't really know.
OCaml is impure, which harms refactoring, concurrency, transformation pipelines, etc
As someone who wrote a lot of Python... Haskell promises correctness and keeps it (I rarely if ever get either space leaks or non-termination, the "curse of laziness"), while Python promises that though your code will run slowly it will work if you test it and debug it enough, which it then breaks.
Okay this might be a bit out there, and it's from a different programming paradigm, but I believe Rust has some Haskell similarities. I don't know too much about either though tbh
/u/edwardkmett Perhaps you can clarify on the grandparent's comment as well as whether the extensive use of pointer equally in unordered-containers is safe or if what was said in your original comment about false positives applies? 
After coming across https://wiki.haskell.org/Library_submissions#Guide_to_proposers I have opened https://ghc.haskell.org/trac/ghc/ticket/13013#ticket.
&gt; your guarantees are no worse than Python's Oh, that's good to know.
Yes I'm referring to the default instances needing inline pragmas. As is, I believe they will not inline in any other module.
How about [Scotty](http://hackage.haskell.org/package/scotty)?
Yeah, they should use unsafePerformIO with makeStableName and eqStableName (I think?) https://hackage.haskell.org/package/base-4.9.0.0/docs/System-Mem-StableName.html 
Yes, but I don't see how that has any relevance to your claim &gt; classy-prelude which is as far as I can tell the most popular alternative prelude.
C/C++ benefit greatly from tools that already exist for compilation. I'm not sure such things exist in the Rust world yet. 
Rust definitely has some Haskell in it.
90% guarantees are _worse_ than 0% guarantees, because you don't expect them to fail. You either treat them as 100% guarantees and then get burnt _hard_, or you treat them as 0% guarantees and cover everything in tests, which render guarantees obsolete.
&gt; It seems like this approach is solving all the issues pro-exception folks raise about type-checked error handling. The thing is, "open sum types" are just exceptions by another name. Since the type is "open", any called function can always return a variant that the caller isn't equipped to handle (other than by catching the error and just discarding it - the `ON ERROR RESUME NEXT` solution!). "Open sum types" are precisely dual to "open record types", i.e. arbitrarily-extended "derived objects", and share similar flaws.
How would you optimise it away automatically? Bear in mind that something like 'last (concat (replicate 100000 "foo"))' is actually much faster when "foo" is a list of chars, rather than an array, and that generally compilers aren't allowed to make datatypes more strict without input from the user, because it could easily convert a terminating program into a non-terminating one. I think your impression of Haskell being anything like C++ is wrong, but if you don't like Haskell, by all means don't use it. You don't owe anyone your time or unhappiness.
Write your compiler in PureScript. And then you can run the compiler in the browser!
Oh, wow, I didn't realize anyone still worked on those.
I didn't expect to find curry here, but I'm glad I did. 
They aren't "guarantees", it's just gradually rolling up and reducing sources of failure which is _always_ good. Haskell(ers) are perfectly honest about these limitations.
You've just triggered me to remember [Epigram](https://en.wikipedia.org/wiki/Epigram_%28programming_language%29) which is more Haskell-like.
Eta https://github.com/typelead/eta
Great!
Thanks ! I'm not a huge fan of these operators either, but I have yet to find a better alternative. The idea was to encode: * what we select in the Variant: `.` the head, `..` the tail, `%` mandatory type, `?` optional type * what kind of function we apply to it: `-` pure, `~` monadic * how we combine the result with the left part: `+` concatenate, `|` union, `^` lift one, `^^` lift both, etc. I'm open to new ideas to encode this in a better way.
Python is #4 according to [IEEE spectrum rating](http://spectrum.ieee.org/computing/software/the-2015-top-ten-programming-languages), [TIOBE index](http://www.tiobe.com/tiobe-index/), #6 most wanted language by INDEED job posts and #3 according to Dice ones [(link)](https://blog.newrelic.com/2016/08/18/popular-programming-languages-2016-go/), #4-6 in most used technologies of Full-stack and Back-end developers, #1 for Data Science, and within 10 of most used languages in general ([StackOverflow survey](https://blog.newrelic.com/2016/08/18/popular-programming-languages-2016-go/)). Haskell isn't even on any of these lists. On a total popularity rating it's rated #23, around D and Dart. Same with any other ratings (GitHub repos, tutorials, etc). Haskell doesn't have a proper IDE, until a couple of years ago it didn't have proper package management. For a language which is more than 25 years old we can safely say that it already passed its prime and will only lose popularity. I'm not surprised, with such a sectarian community attitude.
Thanks! In addition I have been working on increasing the chances that this optimization triggers with additional constant folding: https://ghc.haskell.org/trac/ghc/ticket/12984
A typical use of list isn't `last (concat (replicate 100000 "foo"))`, it's something like a loop over it, or over its zip with something, or over its n-element subsequences, or over its reverse, which all boil down to a loop anyway. All these things should be optimized into a loop over array. Your example is contrived purely for the argument, like saying that the halting problem is unsolvable, so making corretness checkers doesn't make sense. Is increasing memory footprint of strings and number arrays _by several times_ not something to bother with?
I have not used lazy IO in haskell in the last 3 years. Could you point me to the the prominent libraries that provide support for lazy IO - that is widely used?
A compiler can afford to be garbage collected, so I think Haskell would be a better choice, only due to ease of use. Rust is also a great tool, however.
Thanks a lot, that's the kind of explanation that mortals can intuitively grasp and go to town with =) "turning traversibles of things into applicatives of things that turn into traversables", yikes, at the end of the day we want to read-and-write bytes from-and-to here-and-there.. 
Thanks for the clarification - I love that paper btw!
Do you use unsafePerformIO for that or what?
&gt; OCaml allows polymorphism by means of parametric polymorphism and ad-hoc polymorphism through modular implicits. mr Diehl seems to have a time machine.
`force` is not `seq`. And the documentation for `seq` is not a law. It is the semantics of the function in the compiler. And no, nothing in that type says the data will be the same. Regardless of the `x`, functions of type `x -&gt; IO a` can do _basically anything_. That's what IO means. And finally, no, I'm not trolling. I'm trying to correct you, because you're very mistaken about many things in Haskell.
Scala.
&gt; For a language which is more than 25 years old we can safely say that it already passed its prime and will only lose popularity No. Haskell's warts are slowly but steadily being fixed and the language's popularity doesn't seem to fall.
Coincidentally, I fixed this in `Vinyl` today!
There is [Miranda](http://miranda.org.uk/) which I think is the first haskell-like language if you view ML-like as a different category. Clean and Haskell are the successors of Miranda (after some licensing/legal issues with Miranda I think?), and from there Haskell has sort of escaped academia and inspired a ton of other languages (Elm, Frege,...). 
There is a difference though: ATS is used literally nowhere while Ur/Web powers several commercial applications.
A common anti-pattern I didn't see mentioned (but which relates to the `foldl` mistake) is lazily suspending small, constant amounts of work. Only suspend work if there's a *reason* to. The best way to make sure your function is as eager as you want is often to use a type that's strict in (at least some of) its contents. Don't be afraid to declare a datatype that you only use to implement a single function. When you need to force things manually, bang patterns, `$!`, and `&lt;$!&gt;` are often nicer than `seq`. On the flip side, you generally shouldn't force a potentially large computation unless you have a reason to do so. `deepseq`, in particular, should pretty much only be used for parallel programming.
[Disciple](http://trac.ouroborus.net/ddc/) is a Haskell-like with strict evaluation and region types. 
Pretty interesting - I suppose there's no hope for doing similar optimization without unsafeCoerce? For example we can make a similar Variant type: data Variant :: [Type] -&gt; Type where L :: x -&gt; Variant (x : xs) R :: Variant xs -&gt; Variant (x : xs) Which can do the same operations but without unsafeCoerce at all. Yet where I'm guessing nested case operations will never flatten out? 
First off, I was talking about the `force` actually in the `deepseq` package, not your homegrown one. Second, the description of `seq` is _not_ a class law. Its documentation of how the primitive function actually built in to the compiler works. You literally can't violate it. You're correct that in the _absence_ of lazy effects you can't tell the difference between having evaluated a thunk or not. So, modulo termination and performance you can't tell the difference between different evaluation strategies. But in the presence of lazy effects all that changes! And that doesn't violate any laws! You haven't cited typeclass laws at all in this -- the law for `seq` isn't a law, and the property of natural transformations you give isn't a "law" that one must abide by either -- its a description of a "free theorem" that comes automatically. Just like the behavior of `seq` it is impossible to escape. But note these are _not_ laws imposed that one must abide by -- they are descriptive facts and documentation about how things actually work. And, they do _not_ hold in the presence of IO, because IO has screwy semantics. All our nice equational reasoning is no longer nice in the presence of IO, but that's the language we actually have, even if you don't like it. You write that a function of type `x -&gt; IO a` "cannot do anything". Of course it can! its in IO. It can print things to the terminal. It can even return a value of the proper type! ioError :: IOError -&gt; IO a We're in IO, everything is possible. You might say "but wait, I didn't mean to include errors and bottoms in this." But we're talking about IO, so we're not in some idealized pure fragment of haskell -- that's the whole point. Now, you might not want to take my word for this, and want some source you feel is more authoritative. Here's Lennart on the topic: https://mail.haskell.org/pipermail/haskell/2009-March/021068.html (and for your own good, if you think he might not know what he's talking about in that thread, I suggest you google him first, and perhaps check the authors on the Haskell Report.)
I think there's more than that. If you have OneOf2 and want to make it OneOf3, that's rather awkward in this setting, but straightforward in the other. Ditto if you just want to collapse down two possibilities, etc.
So, John Hughes' paper has a comment that QuickCheck needs two separate descriptions of the expected behaviour; bugs will be identified in areas where these descriptions don't agree with each other. And the circular queue example also talk about a state model inside the test itself. Does this mean we have to build two systems - the actual one and the test model? Also, how does one pass around the test state (model state)? All examples are talking about some pretty/post checks on the state
Normal order or lazy evaluation already both have the property that if there exists an evaluation order in which an expression can be assigned a value, then they give the expression that value. You could appeal to this principle to justify ignoring strictness annotations, perhaps; but that's not the issue here. It doesn't help in assigning meaning to expressions that naturally yield bottom as their answers even in a lazy language. The issue here is that there is some primitive data type (implemented in C), and it must behave consistently. To reason about it, you need equational rules telling you which substitutions are sound with respect to this type. You cannot just appeal to the reduction rules of the language, because the type isn't implemented in the language. Primitive operations on Vector are ultimately implemented in foreign function calls, which belong to the underlying runtime. So, is there such a system of rules, which is consistent with the reduction rules of the language, but also consistent with the rewrite rule we're discussing? I don't know. But you can't wish the question away with an appeal to alternate evaluation orders, when the problem is tied up in an abstract type with a foreign implementation.
Sorry, I must have explained myself poorly -- no, you need to worry about the readchannels too. (that's what I meant by both sides). You can hide them behind a WeakRef if you actually want the behavior you describe, or you can just explicitly delete them from the clients that use them easily enough (each client could hold them in an mvar, and the map could keep a list of mvars along with the "main" channel, and then on deletion of the main channel also just go thru and wipe the mvars). But it really depends what you want to do. I'd assumed that you wanted a channel (or pair) per user&lt;-&gt;user direct chat. So if either user logs off, you'd want to kill the whole shebang, which is a slightly different control pattern... 
&gt; Normal order or lazy evaluation already both have the property that if there exists an evaluation order in which an expression can be assigned a value, then they give the expression that value Primitive binary operations can evaluate the left argument first or the right argument first. Either one may lead to non-termination when the other one doesn't, depending on the argument expressions. IOW there's no one perfect evaluation order, irrespective of laziness. Appealing to a non-determininstic evaluation avoids that. If an expression can be evaluated, there exists *some* outermost-first-when-possible evaluation order that will evaluate it, but in general, deciding which order has halting problem issues. Essentially, you can't always know which order will work without evaluating the expression anyway. &gt; The issue here is that there is some primitive data type (implemented in C), and it must behave consistently. Absolutely - I clearly understand that or I wouldn't have already mentioned implementation data structure details. But the implementation is not the end-user requirement. Ultimately, we choose implementations with potentially problematic properties rather than perfect ivory-tower mathematical abstractions because we have to. It doesn't mean the application requires those problematic properties. Being able to prove bad things won't happen for good programs is generally enough. We don't generally need to prove that bad things absolutely will happen for bad programs - generally there's some simplified model that concluded "it might" rather than "it can't", but failure to prove the bad things can't happen is generally enough to say "there's a bug". This is fortunate, really, especially when one the salient property is termination - the best-known example of a property that cannot always be decided. I've never yet heard of a customer complaining that their computation failed to non-terminate. Well, OK, actually I have - interactive systems that just suddenly exit out for no sane reason - but that's not really the same thing. Basically, if I owned a potentially-exploding phone, I wouldn't demand proof that my particular phone will explode before returning it. I'd be far more interested in proof that the replacement won't explode. If someone is telling you "But you can't/couldn't absolutely prove that bad thing will happen in this case", that's called a weasel-worded excuse - e.g. maybe that person is employed by the company that sold you an exploding phone. 
bit sad this is no longer ghc on the jvm
You first sentence finally made me think it through, like you said, Functor f itself is of kind (* -&gt; *) which is sufficient in taking an object from the source to the target category. No need for an explicit a -&gt; f a.
But isn't that due to the lack of typeclasses which would cover some of the needs?
Ok. What is a better module system then?
[Spock](https://www.spock.li/) was created to be a simple Haskell web framework. Though if you're doing something simple enough you might prefer using Wai directly as others suggested.
In `unordered-containers` it is used to preserve sharing in a way that isn't observable outside of the library except in terms of the amount of memory used. All of the danger is nicely encapsulated and handled by the library. In GHC false negatives can happen, but false positives actually can't, given that we have a parallel collector but not one that works concurrently with the mutator. If we moved to a fully concurrent parallel collector like [C4](http://www.azulsystems.com/sites/default/files/images/c4_paper_acm.pdf) then false positives would become a concern an `reallyUnsafePointerEquality#` would have to die or be replaced by a more complicated mechanism.
PureScript: OMG is it a chore to write. I love the type safety (I worked with Haskell a few years ago and I do like statically typed languages). But PS makes you write soooo much (boilerplate) code. The developers justify it by the increased type safety. But to be honest, I don't really see it. What I see though is me having problems with my wrists because I write thousands of lines of code each day. Also, PS is strict, which makes certain things which would be relatively easy to do in Haskell a lot more difficult. And I worry a lot more about performance. Have to constantly check the generated code and rewrite if necessary. It's easy to write innocently looking code which performs glacially slow at runtime. Reasons are: multiple levels of abstraction (polymorphic functions add overhead), all functions are curried even if fully applied (gives runtime no chance to optimize the functions, and produces a lot of garbage). I'd much rather work with GHCJS instead. While GHC is constantly moving forward with new language features, the whole Haskell ecosystem appears more focused on solving practical issues than the PS ecosystem.
I've wondered if I really use Haskell's laziness much. What are the things you run into that are made difficult by strictness?
How do you use `pipes-concurrency` for broadcast?
So would something like this: unsafePtrEquality = eqStableName `on` unsafePerformIO . makeStableName Be a slightly better version of `reallyUnsafePtrEquality#`?
Couldn't you just say that `Vector` (and `Sequence`) are semantically lists, just with infinite time complexities for certain operations. So if it would work on a list then it is acceptable for it to work on these data structures, even if you should not rely on such optimizations for termination.
Oh cool, this has a list of partials I can keep in mind.. are you *actually* hiding (&lt;) and (&gt;) comparison operators, er, functions?!
*inb4 4 months late only posting because it's 1:30 and I hate my life* &gt;Also a multi-million dollar bitcoin mine in high school means 1 of two things. Either they grew up in a wealthy enough house they were able to purchase high end coprocessors or you're revaluing the bitcoins at what they're worth at the peak rather than what they were worth when the mine was operational. The dude played at Carnegie Hall when he was like 10. He's got no trouble with money. Yes, that valuation comes from the peak. 
Yes (but I don't think "on" works if the equality is heterogeneous.) I don't know what `reallyUnsafePtrEquality#`'s guarantees are, and given its name, it doesn't sound like it should be used unless you are certain about its behavior. StableName's guarantee is documented as "if they're equal under (==), then they're the same exact object", which is what shortcircuiting equality needs (no false positives, possible false negatives). 
If you don't want people to think you are trolling, try not to mix nonsensical claims (return x is usually a value, not a function, so it has no input, and its type is known. It also has nothing to do with lazy IO) with claims that are at best dubious (Rust *might* be simpler to learn than Haskell, but it really isn't "light years ahead", and it is, IMO, a *lot* harder to use, which isn't a surprise as it gives the programmer a lot more control over the execution of the program he is writing). And in case this isn't pure trolling, here is an answer : yes, Haskellers find Haskell programs much easier to reason about than programs in Go or Python. And yes, the "community" has a pretty good idea of how useful type signatures are, given that they use it all the time.
Haskell's preferred array library is `vector`, and there's `vector-algorithm` implementing functions for sorting arrays.
This one of these cases where actually understanding how sorting works is important. Sorting is not some linear scan type operation, and you can sort an array a lot faster than a list, even if the compiler can fuse away all of the overhead of conversion. Arrays are one of the most the most important data structures as long as computers keep working they way they are, that's true in 50s FORTRAN, in 90s Fortran and in 2020 Haskell.
I think unicode operators are another can of worms, imagine the number operators that'll look almost identical to each other. I think the root of the problem is the rising complexity, the fact that you're running out of ASCII operators is a manifestation of that.
&gt; What I am saying is that a 'return x' in Haskell does not tell much more than parse_xml(x) in python. And what it does not say in Haskell, and can be easily figured out in Python, are often vital informations for the programmer about the behaviour of the function. Now I'm not nitpicking. What is `return x`??? Type it in haskell - you will get an error. What do you mean by that? And what can you easily figure out in python that you cannot in haskell? &gt; In short. "How much memory or how fast will this run" is more important that "Is this thing a functor"? In case you pass a non-functor to a function expecting a functor, you will get a runtime error (in python). So I guess it's more important the program runs fast than if it runs at all.. &gt; I am not sure. Isn't the only way to lie to the compiler using things like undefined and unsafeperformIO? How does opening a file, reading contents and passing contents to another function, lying to the compiler. It type checks, but when you run the program, boom. File is closed before the program ends up reading it. Yes, it is. This is how `hGetContents` is implemented. Do you see the `unsafeInterleaveIO`? https://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.IO.Handle.Text.html#hGetContents &gt; Or you can just write it in python and cover stuff with a vast breadth of unit tests and documentation, which I agree, is an ongoing pain. But the pain is bearable, and you didn't have to spend a decade learning stuff... Also, if you are talking about python, then in recent versions it includes facilities for type annotations and stuff. It's definitely not bearable anymore to me. Yes, many more languages are getting more and more haskell-type stuff. They are about 15 years back - they mostly cannot do what Haskell98 could; at least they are slowly getting there. When I write a program - after a few hours or days of typing, I have about 90% chance it will run. Correctly. Straight away. Without tests. I never get anywhere near this number with other languages.
Curry is still maintained and subject of on-going research and development by the research group at Kiel University. Spoiler alert: next year type (constructor) classes will be available!
Laziness brings some overhead. But it's not that much, and also partially offset by GHC being a better compiler than psc. If you use purescript-lazy then you're actually worse off. GHC can optimize much more because laziness is part of the language rather than a library.
The only reason I created `union` as a separate library (http://hackage.haskell.org/package/union) instead of making a PR to `vinyl` is that defining prisms requires dependencies on `profunctors` and `tagged`. If you're ok with those dependencies, I'd like to go ahead, make a PR and deprecate `union`. Otherwise you don't get prisms and it's better for corecords to live in a separate package, I think.
Did you do any benchmarks? It would be interesting to see a result.
So I wonder is it hidden because (A) it's partial or somehow bad, or (B) because the rare occasions you need it you just wanna use its qualified-import but no particular custom replacement?
Yea. I've really enjoyed the relationship between the Haskell and Rust communities. We've given each other a lot.
GHCJS has all of GHC's optimizations. PureScript does little to no optimization.
I'm curious what specifically about PureScript you consider problematic boilerplate. I can't think of much that Haskell doesn't also have. And it's certainly not much boilerplate at all compared to the likes of Java and Scala.
You can do that. They are basically type classes with MPTCs and associated types, except the first type parameter is syntactically distinguished (implicitly declared and called `Self`), as is the first parameter of methods if it is of type `Self`, `&amp;Self`, or `&amp;mut Self` (written as `&amp;self` instead of `self: &amp;Self`, etc). There is also language-level support for what in Haskell you'd express as `data Object constraint = forall object. constraint object =&gt; Object object` (OO-esque existentials).
Makes sense. Thank you.
One option I guess would be to define my own `Eq` (and probably `Ord` along with it), that did require law abiding instances. And then use rewrite rules / `unsafePtrEquality#` to get some of the optimizations I want. I think some Preludes like SubHask do that kind of thing anyway, so maybe I could ask them to implement my desired Double / Float instance. Optimizations based on structural equality would also be really cool. The only thing with that is that I can think of plenty of data structures where you don't want true structural equality, even if outside the module you do not break `a == b ==&gt; f a == f b`, for example sets. Would it be possible to still optimize based on structural equality but perhaps only do it outside that module? I realize that even outside the module you can use certain functions to view the internal structure, but the results of those shouldn't really be relied on anyway. &gt; Heck, in theory we could require productive stable unordered discrimination for all Eq instances. ;) If you don't mind can you give me a very brief explanation of what that is? 
Without TH you have to write instances manually (foreign, json, ...). Generics is not an option because it produces horribly slow code. In Haskell you can write `newtype Foo = Foo { unFoo :: Text }` and you get the `unFoo` function for free. Same with record accessors. In PS you have to write those manually. In general records are not pleasant to work with at all. If you try to model your domain accurately, you'll end up with lots of small data types (either data or newtype, because you want to attach instances to them), but then also a lot of that boilerplate.
I know how to do that. I know it's technically possible. That's not the issue. But consider the case when you have 50 types (data or newtype, doesn't matter), each with 5-20 fields and now go to manually write accessors for all of them. Or you can say fuck that, I'm not going to write accessors for all the fields and instead going to pattern match in places where I need and you end up with convoluted code like `map (\(Foo x) -&gt; x.someField) xs`. or `let innerFoo = case foo of (Foo x) -&gt; x`. Do you really want to tell me that's more readable, safer and easier to understand than Haskell? PS looks more like lisp with all the parens that you need.
Haskell accessors are total (unless you use a sumtype of records). What are PS's upsides over GHCJS?
Hashing requires you to touch all of the structure. Grouping requires you to touch only enough of the structure to discriminate between them. Let's consider strings. To hash a string you need to walk it all the way to its end. So if you have a bunch of strings S, with total length |S| you have to pay Ω(|S|) work to finish hashing them. Let LCP(S) be the set of the longest common prefixes of S. EVERY correct sort must pay at least Ω(|LCP(S)|) otherwise it will mis-sort some strings. In general any symbol-wise comparison sort such as multikey mergesort or multikey heapsort must pay Θ(|LCP(S)| + n log n). n here are the number of strings to be sorted. |LCP(S)| &lt;= |S| and for fully random strings |LCP(S)| tends towards n log n so hashing is suboptimal. Classic bottom up radix sort on the other hand pays Θ(|S| + n log σ). σ is the number of buckets used at a time, _NOT_ the word size of the machine. It can be any constant you choose &gt;= 2. On the other hand, discrimination is based on top down radix sort, which gets away with Θ(|LCP(S)| + n log σ) where sigma is the word size of the machine, and independent of the size of the structure being manipulated. It uses a stable out-of-place variant on the American flag sort. Unlike with hashing, there is no concern over if the bounds hold only in expectation, which is a weak enough claim to cause problems when trying to work with polynomial numbers of operations, or even w.h.p. bounds (with high probability), where analysis can break down with exponential numbers of operations. This provides the best combination of each of the bounds supplied so far. I'm talking about sorting here, which is the job of the `sorting` combinator, but the improvement for `grouping` is actually stronger. Given mere pairwise equality comparisons with no additional structure the `n log n`s above goes to `n^2`. Pairwise discrimination into equivalence classes with nothing more than Eq is Θ(n^2). Hashing still requires the Θ(|S|) bound instead of |LCP(S)|, but `grouping` gets away with the same O(|LCP(S)| + n log σ) cost as discrimination based sorting which dominates hashing. For sorting the string story is complete, effectively you turn the structure into a string and sort that but for discrimination the bound becomes a bit less tight because LCP(S) is more nebulous, as you can discriminate with any part of a structure and don't have to do so from left to right. There are multiple strategies you could use to interleave the discrimination of either side of a tuple. This leaves some nebulous wiggle room in the Ω half of the bound. No exploration order can dominate all of the others for inputs that are all finite, but in theory it might be better to interleave the discrimination bit by bit from the left and right halves of a tuple to maximize the productivity of discrimination on infinite trees. Finally, IORefs are able to be grouped as described above, but cannot be hashed.
Which video claims strings take x100 more space? perhaps 4x, but 100x is outrageous. Also, lists may be garbage collected, so they can sometimes run in constant space. List allow for 0(1) prepend, sharing, and have different properties than arrays, you cannot simply replace a list with a vector. Pragmas are just tips for the compiler to improve performance, I'd rather have them the way they are, than to clutter the syntax more. An amateur (like you claim to be), wouldn't need to bother about them, since ghc will pick reasonable defaults most of the time. Yes, finding the best performance isn't easy, but it isn't easy in any language. Just follow the general advice of the experts, use existing libraries (vector, streaming, pipes, etc...), and you'll be fine. EDIT: head, tail and friends aren't less safe than in other language, they work just the same as in List/scheme/etc... The haskell standard library is way more safe than the standard library in C/C++, even including partial functions. 
Jaskell favours clean abstractions and safety over memory and speed. It's not that memory isn't important, but it isn't on place one. If you don't care about safety, and need performance, go program C++.
Right, it's sumtypes that I'm talking about with Haskell accessors. PS sumtype accessors are total, it's awesome. PS translates pretty directly to JS, which makes it much easier to trace and debug. It's FFI is slightly nicer. Generated JS is much smaller. There's no runtime overhead (GHCJS introduces laziness, a GC, and a mock threading system, all of which add significant overhead). The compiler is way faster. Some people think strict-by-default is better (I don't). Extensible effects and extensible records are both amazing (you can mock them in Haskell but it's really ugly, the inference sucks, and the interface is bad). The class hierarchies are nicer and more correct. I'm sure there's more, but I think I've made my point. There are as many reasons to choose GHCJS as there are to choose PS. It's just a tradeoff.
You said that we needed to come up with formal semantics, and assumed that by "come up with" you didn't mean, "just give up and stick to the current defined semantics because all the other ones are wrong because they aren't the defined ones". My new formal semantics are that `Vector` is semantically the same as `[]`. If you want me to give you formal semantics that work for all types for now and forever then I think I am going to be held back by the halting problem, as I am pretty sure such a thing is not really possible.
Output is a monoid :-) https://hackage.haskell.org/package/pipes-concurrency-2.0.7/docs/Pipes-Concurrent-Tutorial.html#g:5
I never said that we need to come up with semantics here and now. My position has been that semantics already exist. The formal semantics that work are the ones that currently exist, because they are consistent both with the behavior of the functions, and with the existing semantics for `bottom` (which you can't just decide to change). The only inconsistency is with some rewrite rule that is breaking the semantics. I think this is a bug. I'm not yet convinced that the rule can't be rewritten to preserve performance and fix the bug. But I do consider it a bug.
I don't know :(
You mean the Radboud university in Nijmegen, I suppose. Indeed, clean is still used for research. Education, however, switched to Haskell this year.
A string is a list of chars. This means each element of String takes 3 words of space (for constructor and its two arguments), and Char is a boxed type. I don't recall its exact definition now, but in its simplest form it would be Char Char# with Char# an 8-bit ASCII character (probably more complex if we're doing Unicode). This means each Char takes 1 word + 1 byte. On a 64 bit machine a word is 64bit=8byte, so each string element takes 33 bytes, which is 2 orders more than it should. Even if you unpack Char you would still need 17 bytes per string character, and that's without unicode.
I think the operators are somewhat a diversion. IMHO, the meat of the discussion in checked vs. unchecked exceptions is safety vs. composability. The rest is just syntactic sugar, and therefore has no bearing on your architecture. If one uses nothing but your operators, the resulting architecture (the way pieces fit together, the way components are coupled etc.) should be very similar to using `Either sumOfErrors result` everywhere. So, we haven't gained much in terms of composability. Safety aspects will also probably be very similar. I humbly feel that some magic could come from defining some type classes and instances on top of this library, such that I can specify precisely how I expect my functions to fit. Things like; "If you have a monadic result with a possible set of error values including these, I'm going to handle these two error cases only". Or, "I can only execute actions that can result in at most one of these error conditions.". Maybe then we could attain the holy grail of error handling, that is, interfaces that are both type-checked AND as composable as the problem domain allows.
funnily enough, the first two benchmarks I wrote (the concatenation one above, and a more complex one involving finding all anagrams of a word) had String performing competitively, so it's not as rare as you think. If the strings are used repeatedly and exist in memorly in full, then yes, Text is better. You are wrong about that loop over the array thing, btw. Lazy evaluation means that it frequently takes constant space rather than the linear that you would expect. I recommend reading more, your current intuitions are not a useful guide.
The intention with the article was to restrict it to libraries &amp; functions that you might reasonably expect to be safe and performant, but aren't. We threw out about half our ideas because they were more to do with user strategies &amp; techniques, rather than explicit, concrete mistakes in the libraries. We'll probably write another blog with the other material if we get time.
I'm not talking about the internal implementation. The GHC compiler is built from the ground up to support lazy IO. But you don't need to understand the implementation details to use it in simple cases. And lazy IO is *only* for simple cases.
doesn't have anything to do with the compiler, except that there's an unsafeInterleaveIO primitive that System.IO uses. It's misleading to say GHC is built from the ground up to support it. 
[Pure-lang](http://purelang.bitbucket.org/), not to be confused with Purescript, is dynamic typing functional language and based on term rewriting. Prolog-ish [Mercury](http://mercurylang.org/) has type system comparable to Haskell.
Char in haskell is a unicode char using 31 bits. So each char in a string would take 3 words. That means a string is 3 times the size of an unboxed vector of characters. That's a lot less than 100! Still if you need to handle lots of data, Text (or bytestring for binary data) may be better.
Where's the source mentioning type class? I don't see any news about that.
Just for a quick sampling - base, text, bytestring, network, process, ... And any library that supports streaming via lazy structures - parsec, attoparsec, aeson, zlib, tar, tagsoup, ... Anything that uses conduit also supports lazy IO, because conduit provides an adapter. Probably pipes too, I suppose. Actually, are there any prominent general libraries at all - not specialized ecosystems like web frameworks - which don't support lazy IO? I don't think so. In fact, probably not so many non-prominent ones either, but there are a lot of weird dark corners in the Haskell ecosystem.
The buffer is still allocated on the Haskell heap and has to be copied during GC. If all your messages have the same size you can go one step further and allocate the entire buffer on the C heap: module Main where import qualified Data.Vector.Unboxed as Unboxed ( Vector, replicate, generateM, ifoldM_) import qualified Data.Vector.Storable.Mutable as Storable ( IOVector, new, write) import Data.Word ( Word8) import Data.Foldable ( for_) import Control.Exception ( evaluate) import Foreign.Storable ( Storable(..)) import Foreign.Ptr ( castPtr) windowSize = 200000 msgCount = 1000000 newtype Message = Message (Unboxed.Vector Word8) instance Storable Message where sizeOf _ = 1024 alignment _ = 1 peek pointer = do vector &lt;- Unboxed.generateM 1024 (\i -&gt; peekElemOff (castPtr pointer) i) return (Message vector) poke pointer (Message vector) = do Unboxed.ifoldM_ (\() i b -&gt; pokeElemOff (castPtr pointer) i b) () vector type Buffer = Storable.IOVector Message mkMessage :: Int -&gt; Message mkMessage n = Message (Unboxed.replicate 1024 (fromIntegral n)) pushMsg :: Buffer -&gt; Int -&gt; IO () pushMsg b highID = do m &lt;- evaluate (mkMessage highID) Storable.write b (highID `mod` windowSize) m main :: IO () main = do b &lt;- Storable.new windowSize for_ [0 .. msgCount - 1] (\i -&gt; do pushMsg b i) This brings down max GC pause to 0.0005s and bytes copied during GC to 1,658,152. It is not important, but here we allocate the individual messages on the Haskell heap before we write them to the buffer. This means the GC can just "not copy" them and doesn't have to call `free`.
`sum`, `product`, `minimum`, `maximum` are some of the functions in `base` that use `foldl` instead of `foldl'`. The problem is we can't fix them because [the standard gives their definitions that way](https://www.haskell.org/onlinereport/haskell2010/haskellch9.html).
I think that's the case at the moment, yes.
Suppose you have a snoc-list: `data SL a = Nil | Snoc (SL a) a deriving Foldable` For a snoc-list, the "good" folds are `foldl` and `foldr'`. In a post-FTP world, lists aren't the only things to worry about in the fold department.
We could change the standard in the upcoming Haskell 2020.
The first paragraph of the linked page: &gt; In this chapter the entire Haskell Prelude is given. It constitutes a specification for the Prelude. Many of the definitions are written with clarity rather than efficiency in mind, and **it is not required that the specification be implemented as shown here**. Am I wrong in thinking that this indicates the possibility that we could indeed fix them?
I stand corrected. Still not sure what is meant by "industry-ready" but the lack of an IDE is not really a huge deal like many make it out to be. I already knew it was great for Data Science, I just didn't realize it was used in nonacademic settings. 
&gt; Generics is not an option because it produces horribly slow code. Well `psc` doesn't do much in the way of optimization yet, but I'm hoping to focus on that after 1.0. Others have used code generation to get around any boilerplate issues. &gt; In Haskell you can write newtype Foo = Foo { unFoo :: Text } and you get the unFoo function for free. You can derive `Newtype` instances or use lenses. &gt; In general records are not pleasant to work with at all. What would your ideal record system look like? &gt; Also, PS is strict, which makes certain things which would be relatively easy to do in Haskell a lot more difficult. Well yes, it's strict for different reasons. If you try to use PureScript like Haskell without regard for the differences in evaluation order then you're going to have a bad time. Generally, please remember that PS doesn't have the benefit of 20 years of research behind it. I think we're making good progress, but the end goal was never parity with GHC. And ultimately, if people find it doesn't make the right sorts of trade-offs for them, they can and should use GHCJS or Haste, but there are plenty of use cases where it is a good fit.
Hoogle is best for searching by type signature, not by title. If you don't know about `Vector a` already it will be less useful. 
I'm waiting till Idris JVM and ETA have stable versions but they look equally promising. I prefer Frege because it compiles to Java - it's easier to inspect the source code and build great FFI. By way of JVM languages in general I've worked with Scala and Kotlin. Both are great for their use cases. Scala + Spark is a great duo so is Kotlin + Android. Haven't done anything in clojure because I don't like dynamic typing and I don't know what exactly to use it for. 
My limited experience with doing type and effects analysis for a typed lambda calculus-like language in Haskell, and working on a program verifier for a very simple imperative language in Rust: * Rust is a lot noisier than Haskell. For instance, if you define recursive data structures for your ASTs, then you need to explicitly use pointers (`Box`) in Rust, otherwise the size of the type would be infinite. If you care about performance then this level of control is important, but it makes it harder to see what is going on on a high level due to explicit dereferencing etc. * Haskell is more succinct in a number of ways. For example, when you want to pretty-print your ASTs, you can define a `Show` instance that is basically a list of one-liner string combinations. In Rust you would implement `fmt::Display`, but then you have to deal with a formatter and use `write!` macros. It is not a lot worse, but again, it is extra noise, and for something as simple as formatting, the noise can be more than the actual code. As a different example, in Rust you often have to materialise things into a collection, or convert between vectors, iterators, and slices. If you care about performance then you want to think about these things, but it does distract from the actual algorithm. * Working in a pure language can be limiting. Although compilation is ostensibly a pure function from the source language to the target language, in practice many parts are more naturally written in an imperative way. For instance, one of the things that often comes up is inventing fresh variables. In a language with object identity, you just allocate something, and the address of that is guaranteed to be distinct from all your current variables. In a pure language with value identity, you need to keep some kind of global counter. Now you need to thread this counter everywhere and think about how to update it correctly. You can use monads to hide most of that, but that is "infectuous" in the sense that all of your code now uses monads everywhere. As a different example, during program analysis, you sometimes "learn" things about e.g. variables as you go. If all your data is immutable, you cannot update existing references with new information. You can add a layer of indirection, but that is basically simulating pointers into a mutable heap. * In neither of these projects I wrote the parser myself, so I cannot comment on that. * Both languages have great package managers / build tools. * Both languages have great support for testing. I haven’t finished the Rust project yet, but based on my current experience I would choose Haskell for such a project. Disclaimer: I am not a very experienced Haskell programmer.
&gt; tring takes almost x100 times more space in Haskell, and similarly for most other lists. For one thing, laziness optimizes this away in many cases. I'm quite confused as to when the x100 factor comes into play but I assure you it's quite rare. &gt;the fact stands that the most obvious ways to do your job which the standard library provides are the ways to shoot yourself in the head, like no other language allows. It's more limited than you imply there. Most things beginners do will not cause issues with `String` vs. `Text` thankfully. &gt;when a language makes pragmas, macros and various other out-of-syntax hacks mandatory, it is a sign of a deep design flaw and a strong reason to avoid it. The pragmas are in a few libraries so that they don't have to be everywhere. &gt;C++ has massive legacy which keeps it alive, Haskell doesn't. Haskell is very much alive and growing. 
Plus, in some cases, as pointed out before, the laziness makes `String` more efficient, e.g. `head . (map toUpper)` or something.
&gt; Well crikey me, I'll save that for another day.. or life ("I just wanna code Applicative functors are standard in most tutorials, thankfully. Anyways, the abstractions you learn are well worth your time. If you just want to code, the languages/paradigms you already know will obviously be easier. 
&gt;return a default value Which is sort of ruled out by the `Read` typeclass not requiring an instance of `default`
&gt; What would your ideal record system look like? I started writing PureScript at work a day ago and it's a great language. I like its record system, but it still can be improved: (1) Lens-based access. The lenses should be generated by the compiler and be syntactically lightweight, e.g. `.foo` could be equivalent to foo :: forall b r a t s. Lens {foo :: a | r} {foo :: b | r} a b foo = lens (\o -&gt; o.foo) (\o x -&gt; o{foo = x}) (2) Arbitrary kinds for field labels. I described the motivation behind them in https://github.com/ghc-proposals/ghc-proposals/pull/6
Ahh nice, that's cool. Slight typo, you capitalized Constraint in the RHS of the declaration.
STG is untyped.
fixeded
[Disciple is a dialect of Haskell that uses strict evaluation as the default and supports destructive update of arbitrary data structures. Disciple includes region, effect and closure typing, and this extra information provides a handle on the operational behaviour of code that isn't available in other languages.](https://wiki.haskell.org/DDC)
I don't believe there is a way to solve this measurement problem. You'd have to take a reasonably sized random sample and manually check them for PVP vs SemVer to get a likely distribution, and apply that distribution to the 3-component packages to get a better estimate.
That's a big jump in dependency weight. There really is no way to break off the `profunctors` bit without introducing orphans?
The definition of a prism is type Prism s t a b = forall p f. (Choice p, Applicative f) =&gt; p a (f b) -&gt; p s (f t) with `Choice` from the `profunctors` package. If the aim is to be compatible with the `lens` ecosystem, this class should be used.
That means the strongest thing I can say about this data is: "Up to 6,004 packages may not use the PVP."
I think he's saying that you can pass a String (or whatever other lazy type) to Parsec that comes from lazy IO, so it's true but vacuous. No extra work has been done in Parsec to support it.
Unfortunately that doesn't work either. Even though SemVer mandates exactly 3 parts, some people release tiny patches using a fourth part. In my experience this means the previous release was broken somehow. For example, version 1.2.3 might have a show stopping bug that was hot fixed with version 1.2.3.1. That being said, it would be interesting to see how many packages have ever used some number of version parts. 
Not only that -- it specifies them: "A package version number SHOULD have the form A.B.C, and MAY optionally have any number of additional components" All my packages use the pvp, and all use 3 components. This is very common.
&gt; I suppouse the more List-specific version is used when using lists. Digging a little further, we find, at `Data.Foldable`: instance Foldable [] where elem = List.elem foldl = List.foldl foldl' = List.foldl' foldl1 = List.foldl1 foldr = List.foldr foldr1 = List.foldr1 length = List.length maximum = List.maximum minimum = List.minimum null = List.null product = List.product sum = List.sum toList = id At `GHC.List`: -- We write foldl as a non-recursive thing, so that it -- can be inlined, and then (often) strictness-analysed, -- and hence the classic space leak on foldl (+) 0 xs foldl :: forall a b. (b -&gt; a -&gt; b) -&gt; b -&gt; [a] -&gt; b {-# INLINE foldl #-} foldl k z0 xs = foldr (\(v::a) (fn::b-&gt;b) -&gt; oneShot (\(z::b) -&gt; fn (k z v))) (id :: b -&gt; b) xs z0 -- See Note [Left folds via right fold] -- etc. sum :: (Num a) =&gt; [a] -&gt; a {-# INLINE sum #-} sum = foldl (+) 0 At `GHC.Base`: foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b -- foldr _ z [] = z -- foldr f z (x:xs) = f x (foldr f z xs) {-# INLINE [0] foldr #-} -- Inline only in the final stage, after the foldr/cons rule has had a chance -- Also note that we inline it when it has *two* parameters, which are the -- ones we are keen about specialising! foldr k z = go where go [] = z go (y:ys) = y `k` go ys See also [AndrasKovacs' comment](https://www.reddit.com/r/haskell/comments/5jjzoq/foldl_vs_foldl/dbguv2f/) to this post.
I'm not really qualified to answer that since I'm not advertising the position, but I will say that in my opinion enthusiasm and general technical ability are more important than specific skills for an internship. They can probably find a project to match your skill level, it's (generally) *supposed* to be a learning experience. The important thing for you is to demonstrate is a basic level of competence, and it sounds like you probably have that (Haskell and research experience as a Freshman is not common!) So you should definitely apply, the worst that can happen is they say no. That's happened to me a few times, and it certainly sucks, but the world keeps spinning, and they'll usually, sincerely suggest that you apply again next year. 
That's not SemVer though. Following SemVer a hotfix release for bugs in 1.2.3 would have the version 1.2.4.
As the other reply here says, yes, I've bumped the third component on bugfixes typically. Note also that often people don't release new versions with every bugfix, but will accumulate a few different patches between cutting releases. I know that in some circles that's not considered the "right thing" these days, but its a very longstanding pattern that I don't see going away anytime soon.
&gt; I rarely see packages mention which versioning policy they use Yes, because everyone knows that the standard is the pvp. by the way, `free` does not use SemVer. All of edwardk's packages use the pvp.
Prove they aren’t!
No. you're not left to guess. Edward has said many times that he uses the pvp. There are pvp related discussions in lens tickets. Packages don't register which versioning scheme they use if its the standard scheme. Because it is the _standard scheme_.
Your methodology is flawed, and sending out tweets like these doesn't help your cause at all. If you want meaningful metrics, what you need to do is sample actual package contents, derive the exported APIs from the source code, diff them against those of previous versions, and check whether the corresponding version number changes would fit with either SemVer or PVP. The most likely outcome, I believe, would be that of the 10,642 Haskell packages on Hackage, over 9,000 do not accurately follow either version policy. Which wouldn't really make a good argument either way, except that the tool you wrote deserves a spot in the Haskell packaging toolchain.
&gt; I think Haskell has generally done very well by making as much as possible of the language "first-class" Arguably, typeclasses work by making method dictionaries not "first class".
The `free` package has bug-fix version bumps (the fourth number) on at least half a dozen releases. Additionally, as /u/sclv mentioned, Edward's packages always use pvp.
Regarding bounded channels. If I'm running a Servant server using Warp, do I need to add a queue in front of it, perhaps tagging incoming requests with a time stamp, I order to have some sense of when my app is overloaded (either too long a queue or too much latency)? I haven't thought about this before, but it makes sense.
Have you tried using cabal? I suspect stack may be pinning too much here... There's also apparently a ticket on the ghc-mod tracker with some suggestions... https://github.com/DanielG/ghc-mod/issues/855
The way I think about it is that it's like factory_girl combined with QuickCheck, where the types define the relationships between the different entities and tell the test scenario generator when to generate dependencies for row row you insert into a table in your integration test.
We've had generics for a while now, but we recently added a new implementation in the style of GHC.Generics, which allows us to derive more things, like Semigroup and Monoid instances.
Choosing to say something that the methodology doesn't back up because of a bias you wish to support can very easily be seen as intellectual dishonesty. It wouldn't be hard to believe that you'd purposely left out the ambiguity with the intent of supporting your conclusion. Is that really how you want to be seen?
What do you think the run time of that is? (hint: that's not really quicksort) I wouldn't use that if I were you. You can find 'sort' in Data.List of 'base' if you want to sort a list. It'll serve you far better.
I am really interested in applying, but I could really use some more practical information. For example, "paid competitively" is not enough information for me to determine how hard it would be to get by after paying rent and all that stuff, and it would be nice to know before applying. ~~BTW: In the description in the link, it says "starting any time in the fall months is acceptable", while the link where one is supposed to apply says "starting any time in the summer months is acceptable". Which one is it? I'm thinking it's probably gonna be the latter, considering the application deadline.~~ EDIT: Never mind -- it says may to august. But now you know there's a typo. 
There's a reason why this was in a tweet rather than a blog post. I started with the assumption that A.B.C means SemVer and A.B.C.D means PVP. I am happy that I now know those assumptions are worthless! I did not know that before. 
&gt; the tool you wrote deserves a spot in the Haskell packaging toolchain. Yes! This tool is needed badly. [hackage-diff](https://github.com/blitzcode/hackage-diff) is a step in the right direction.
Which is why one should state their methodology when making a claim. Which is a good reason never to use twitter for an actual discussion.
*Here's the Urban Dictionary definition of* [***spicy memes***](http://www.urbandictionary.com/define.php?term=spicy%20memes) : --- &gt;Memes that are more than dank. They are the dankest of the dankest. They are so good that the burn your toungue. Its spicy. A spicy meme --- _When you are about to have a foursome but you cant stop making them spicy memes._ --- [^(about)](http://www.reddit.com/r/autourbanbot/wiki/index) ^| [^(flag for glitch)](http://www.reddit.com/message/compose?to=/r/autourbanbot&amp;subject=bot%20glitch&amp;message=%0Acontext:https://www.reddit.com/r/haskell/comments/5jlgxe/of_the_10642_haskell_packages_on_hackage_4638_use/dbhg69k) ^| ^(**Summon**: urbanbot, what is something?)
A simpler but less elegant way would be to make a website that gives guidelines for the identification of the versioning scheme and allows the community to report the scheme of individual packages.
&gt; The microlens package defines some prisms without using Choice It doesn't, it exports them as traversals. From documentation: "However, it's not possible for microlens to export prisms, because their type depends on Choice from profunctors. So, all prisms included here are traversals instead (and you can't reverse them)." &gt; and I'm still not sure that the prism definitions need to be in the same package as the corecord definition The lens are defined with records, by symmetry and consistency prisms should be defined with corecords.
Yaay, another item to entertain beginners: don't use Prelude.readFile to read a file!
I definitely don't think the advice in the blog post is meant to be interpreted as saying you should write your own decoding/encoding routines -- just that you should call those routines yourself.
Rather than use other packages, one can open the handle and then use `hSetEncoding` to set the encoding directly: http://hackage.haskell.org/package/base-4.9.0.0/docs/System-IO.html#v:hSetEncoding One can also just call `setLocaleEncoding` (http://hackage.haskell.org/package/base-4.9.0.0/docs/GHC-IO-Encoding.html#v:setLocaleEncoding) to set it once and forall for the duration of a program without having to do it per-handle. That said, a `readFileWithEncoding` would make sense to add to base as well as `Text` since reading in a specified encoding is so common. (i.e. encoding-io looks nice and it seems worthwhile to pull those functions back into the libraries they augument). On the other hand, reading in a system-dependent encoding is pretty idiomatically unixy, and matters more than we may realize. After all, that's why there _is_ a system locale encoding. Just as a basic example -- mac and windows differ in the high ascii character sets. And historically there are lots of character sets that are used that _aren't_ unicode even for non-ascii stuff. One example that I think still holds is chinese, where the unicode representation is still not preferred because it mashes together characters that people would rather it not. If you're working in a uniform linux server environment, this stuff never comes up. But if you're writing end-user apps that are expected to respect the locale settings the user has, and are working on locale-specific files on the user's system, then the more responsive behavior makes sense. I don't have an opinion if one or another should be the sensible "default" since there are actually legit cases for both, but I thought it was worth adding at least a little context here. (And I know there's a lot more context that I'm omitting and a fair bit more that I don't even know -- encodings are *hard* and even after many years I'm still learning new things about them every now and then).
The article is true.
Yes but fold goes left to right, which makes foldl more efficient here, in fact exactly as efficient as foldr on [] I believe
The line between what people here call partial and total functions is definitely fuzzier than most would admit. For instance, folds over lists are partial, but nobody seems to ever mention this. People only seem to hate eagerly partial functions, those which will actually tell you so, which is understandable because these give "the newbies" errors but sort of ironic as generally these are easier to actually debug, whereas an infinite loop in a very lazy program is just hell on earth. I understand why a lot of people want all the partial functions gone, all I'm saying is be careful what you wish for, because a lot of great abstractions have very useful but partial instances. The reason one uses a partial function is because you, the smarto programmer, understands that you're passing something to it which lies in its domain of definition, or you trust that this won't actually get evaluated if that's not the case. I want to clarify I don't think it sends a good message to have these functions in the Prelude, but I also don't think it actually is a problem if you know they're partial and use that knowledge, like your knowledge that sum shouldn't be used on an infinite list... because its partial. Aside from it's strictness properties, do any of you want to get rid of sum from the Prelude? Gonna get lots of downvotes for this post, I bet. 
Noob question: what's the difference between this approach, and exporting a module with functions that act on a typeclass, eg. `IsString a` rather than `String`?
https://github.com/singpolyma/permute-RankNTypes
Here's my advice: do not start out your Haskell career by memorizing all the things you need to avoid. Just start writing some code. When you run into problems, search for the answer and you will almost certainly find it. Using `fromJust` as a newbie is *fine*. You're exploring Haskell, not writing production software.
Please provide the build error in its entirety. Makes it a lot easier for us to help you. Another thing you can try is to ask this question as stack Github issue, and also including all the logs there.
Too much drama by my opinion, This is routine "Unicode support" thing, in some python3 it would be the same. Note also that in Windows the current encoding is not utf-8 by default, so most Earth population should be aware of the issue.
Well - as an aside, from another relatively new Haskell user, I tend to find that programming was basically set back to square one for me when I started learning. Monads are still totally incomprehensible to me, and I just cannot find a good explanation for them. Maybe I'm just a crap programmer but I can work my way around basically any other language I've tried without much issue (including other functional languages, like elixir). I do think that I need to give it more time, but I've never really encountered the walls of understanding that I find in Haskell anywhere else. I *really* understand that once I comprehend the concepts it's simple as day... It's just getting there is the problem
Even if people do use Prelude.readFile in production, what's the worse that can happen? The program crashes? Weird things happen? If so, you'll be no worse off than users of other programming languages, and people still manage to build great software in those.
I started learning Haskell about a decade ago, in my spare time. I've been using it professionally for three of the past five years (including currently).
This is not sensible thinking. "I grew up in a shabby school and I turned out fine, so these yung'ns will too!" Better is "If a better Prelude had been in place when I was learning, would I have less skill than I do now?" The best argument I know against majorly cleaning up the Prelude is that a lot of existing code relies on it.
The first search result for the error in question gives the solution within the first three comments in that issue: https://www.google.dk/search?q=commitBuffer%3A+invalid+argument+(invalid+character)&amp;rlz=1C9BKJA_enDK633DK633&amp;oq=commitBuffer%3A+invalid+argument+(invalid+character)&amp;aqs=chrome..69i57j69i58.1666j0j4&amp;hl=da&amp;sourceid=chrome-mobile&amp;ie=UTF-8 That being said, I completely agree that Prelude could use a tidying up. I'm just not sure how to go about it. Compability is a big issue, as you mention.
&gt; This behavior makes a lot of sense for the special standard handles of stdin, stdout, and stderr, where we need to interact with the user and make sure the console receives bytes that it can display correctly. I'd argue it doesn't even make a lot of sense then... https://www.reddit.com/r/haskell/comments/5dty4k/stdout_commitandreleasebuffer_invalid_argument/
I started using Haskell in 2008. I had been aware of it for a few years before that, but as a Smug Lisp Weenie, I didn't really see what the fuss was about. I only *really* started learning it for my bachelor's project (making Arduinos programmable in an FRP style with Haskell - an adaptation of Geoffrey Mainland's Flask). After that it has been my primary go-to language for complex data transformation. I mostly use it for [writing a compiler](https://futhark-lang.org) in the context of my work as a PhD student. So, uh, I guess I'm not really using it outside the ivory tower.
IMO calling it `unsafeHead` is a bit extreme, that puts it in the same bucket as `unsafeInterleaveIO` and `unsafePerformIO`, and such. Which can actually violate the semantics of Haskell that we know and love, beyond simple non-termination.
Got a link to a feature description/comparison. Is it more fine-grained and will it require upping open file ulimit maybe?
I honestly kind of agree, now partial functions are definitely not a great thing, but they are sort of necessary for a nice programming environment. A simple arithmetic error is going to be way worse to debug and way more of a big deal than a partial function, because you won't get a handy error message telling you where shit is going wrong. But no one is trying to do much about that as far as I can tell. With that said I think that certain functions like `read` should probably return `Maybe`, and I do think that partiality should be well documented, perhaps in the function name (I wish `read!` was a valid name, because `unsafeRead` is way to scary and nothing like `unsafePerformIO`, maybe `readEx` for exception?). A few examples of functions people don't normally complain about that are very partial: fix many some foldr1 foldl1 foldl1' And like half the functions on Vectors and Maps and such, from indexing to non-maybe lookups.
Yea, as someone aspiring to eventually create his own Haskell-like language, a more comprehensive list of things to avoid would be very nice. That's a good start though.
&gt; Title: Beware of ~~readFile~~ - Michael Snoyman's blog &gt; &gt; Right off the bat, the title of this blog post is ambiguous ... FTFY
Another good idea is to look at the popular libraries and see which problems they try to solve. 
`fromMaybe` is better. However using `fromJust` will not launch nukes, or delete the internet. 
I'm writing Haskell (almost) every day since I joined [Checkpad MED](https://www.checkpad.de/en) back in April 2012, so soon 5 Years. Before that I did PHP and JavaScript :-)
I see. This might be an academic question, but do you know if it's possible to write a non-leaky `foldl'` using only the guarantees in the language spec?
Aren't they monoids in the same category with a difference choice of tensor?
10 years. I've used it for tons of hobby projects, from toy programming languages to games.
7 years.
Does Frege make you absolutely happy?
Sorry for not including the build error, I have updated the original post with the build error. Thanks to sclv I was able to work around the issue by installing ghc 8.0.2 rc2 as the system compiler and building ghc-mod using cabal. So far I have had no issues with using the binary with stack built projects that use the same system compiler. Once GHC 8.0.2 is released and a Stack snapshot based on it is released this problem should be solved, it seems that there is not much hope that Sierra linker fix is going to be back-ported to older GHC versions any time soon.
I don't use it professionally, but all of my personal programming projects have been in Haskell for the past 10 years. I first used it for my masters thesis.
From Germany, maybe moving to Kiel next year, sounds interesting. The title translates as: "Extension of Curry with typeclasses and typeconstructorclasses" ;)
I started learning it in 2008, just for something to do. Since then I used it for some upper-level undergrad course-work, and again during my masters to implement the ideas in my thesis. I still use it for when I need a quick rest server, or need to do some data transformation. Where I work now is mostly a Delphi-and-C++ shop, so I'm not using it in production. But since I'm so familiar with it, I can bang out quick one-off programs really quickly in it. I'm a big proponent of modern C++, but Haskell is still the nicest/cleanest/best language I've ever learned.
That's probably true. Honestly, I probably don't know enough about Frege to be making these claims.
All of those libraries - and many, many more - support lazy structures for streaming, such as lazy lists (including String), lazy bytestring, and lazy text. Where would you normally get one of those? From lazy IO. Yes, you can create them other ways, but if you are already invested in a more complex and general purpose streaming method there is little reason ever to touch those. Except to adapt to APIs that require them. Of course they are used in production. It is the default way to do simple IO in Haskell. The reason not to use conduits et.al is because they are far more complex and difficult to use if all you are doing is reading from one source, processing it with a pure function, and writing it back out. If you are doing anything any more complex than that, though, lazy IO quickly becomes unmanageable.
It's a different monoidal category. 
I totally agree with the read comment, but because it would be more useful that way :P fix is a great example because it's partiality gives Haskell Turing completeness essentially
I first got into it maybe nine years ago when I was writing a roguelike in C, then rewrote it in C++, then got super frustrated because I was missing (what I know now as) composability. Nothing fit together well, and everything was tied intimately to everything else. It took me a few years before I became productive, but now I code all my personal projects in it, and also a lot of work stuff (only utilities for myself).
I'm in the same boat as an ex Java guy. Definitely think I learned a lot while using Java that I wouldn't have learned while using Haskell, but I'm certainly glad that I switched, mostly for the reason that I'm steering towards academia and research, letting my inner math nerd fly free. Thoughts on Eta?
In cases where you need to understand that, you should not be using lazy IO. It is only for simple cases. Like read from a single source, process it with a pure function, and write it back out.
I don't know nearly enough about Eta to give you a proper opinion. But my first impression is that it's GHC on the JVM, and it's a shame that it will supposedly diverge from GHC. I'm just not sure what putting Haskell on the JVM buys you. I need to read more about it.
I started learning Haskell from mid-2016. I haven't written any real application yet. I try to solve Euler project problems. I am planning to completely spend next two month writing some real application.
Yes, the idea of transpiling to Java makes me considerably less than happy. My feelings on Java can be summarized in this comic: http://leftoversalad.com/c/014_javavsjavascript/ 
Ah well you can! It comes with a slightly modified version of cabal which works and you can get anything that works with GHC 7.10 or earlier, which genuinely is a whole lot on there, and I think will continue to be a whole lot for a while. I myself will continue to use GHC because I have no use for the JVM, but I'd imagine Eta is a more effective solution than Frege at this point.
(The secret reason why I like Eta so much is because the maintainer said he'd be open to me proposing a potential Prelude for them as they diverge)
You don't have to work with the translated code at all.
Yeah I think one major reason nobody can find that info is because it doesn't have corporate sponsorship. Really would be surprised if Facebook doesn't claim that role one of these days.
I still love C++, but not for its beauty. It has a nice set of zero-cost abstractions for writing solid code, but it looks like a serrated lisp offspring ;)
This is a linting issue. We need to aggressively deprecate and ensure that warnings are visible to users in the normal workflows, I.e. ghc
well, some folds are. I'd like to get rid of foldr1 &amp; friends - if you know that there's at least one element in the list, you should model it with something like Data.List.NonEmpty. The problem of actual computable totality is much harder, and probably Haskell isn't the place for it to be solved. The post is just about the really avoidable, silly stuff. (fwiw, i think this was a reasonable objection and I hope you don't get downvoted.) 
2 years now. I was enthusiastic about functional programming after learning Standard ML in school. In my last year of undergrad, a prof pushed me towards using Haskell for a project. I kept on learning it, and later I got some work using Haskell.
Pretty cool that you got taught SML in school. We had a brief bit on it in my first programming languages course but that's all. We used OCaml in my second one though.
Not true that you can't write head without invoking error! let x = head [] let y = sum [1..] Type these into GHCi and you won't have any problems doing whatever you want after, as long as you don't force them to be evaluated. Edit: Didn't fully read your comment, what do you mean by leaving off a case or deliberately spinning forever in this case??
I meant the _definition_ of head, not its use. I'm well aware of how laziness works.
Yeah total is probably not possible without restricting to a subset of Haskell, particularly a subset of allowed data declarations, restricting to properly inductive and coinductive definitions.
It was really cool. They've actually started teaching Haskell in that course now instead of SML.
It is very difficult to learn.
I've been using Haskell for well over six years now. I made the decision to learn it at the end of high school (2010), and picked it up properly during my first semester of college—at the same time as I was learning Scheme with SICP. I've stuck to it ever since: Haskell was the first (and so far only) language I actually liked *unreservedly*, although Racket and OCaml do come close. During those six years I used it for my own projects and for part of my undergraduate research, but I've only been using it professionally for about half a year. For various reasons, I ended up doing a bunch of OCaml instead in a couple of internships and my first job out of college, but I still like Haskell a lot more. That's one of the (many) reasons I'm really enjoying my new role :).
What do you want to do? If the type `Foo` has a `Read` instance, then `readMaybe &lt;$&gt; getLine` will return `IO (Maybe Foo)`. If you now want to repeatedly `getLine` until said `Maybe Foo` isn't `Nothing`, try getFoo :: IO Foo getFoo = do x &lt;- readMaybe &lt;$&gt; getLine case x of Nothing -&gt; getFoo Just y -&gt; return y That is, explicitly pattern matching instead of using `fromMaybe`. 
2-3 years at this point. First project I ever wrote in Haskell went straight to production and has been there ever since. A couple of years later and the entire backend and all of the command-line tools are in Haskell. I actually just realized I've never written haskell non-professionally.
I learned Haskell 26 years ago (before monads) for the sake of some ivory tower relief from C++. I've used it pretty regularly for home experiments and applications, and in a couple of cases for supplemental tools on the job. Happily, as C++ grew over the years, Haskell has consistently delivered more that's worth learning. 
&gt; it looks like a serrated lisp offspring ;) With C++14's generic lambdas, it became exactly that. :-)
yes, but it's still a different language. Things that used to be accepted now wouldn't be - most of hackage wouldn't compile. Good idea, different language :)
I think OP wants to know if any research has been put into arbitrary rule matching. Like, if you add arbitrary laws to the lambda calculus (resulting in a different calculus), are there known ways to make use of these arbitrary laws? For example, is it well known how to match arbitrary laws to arbitrary terms, or how to decide which set of reductions based on those laws is optimal, etc..
Small typo (a missing `Ap`, I suppose?) abstract x (Ap f1 f2) = (abstract x f1) (abstract x f2)
Using [Equations](https://www.irif.fr/~sozeau/research/coq/equations.en.html), Coq becomes a lot more similar to Haskell.
If you want to test this release with Stack, use [this `stack.yaml`](https://gist.github.com/tfausak/81efc18c701a32d785f968c0a3639fad). 
Somewhere between 2002 and 2004, as best as I can recall. I have yet to use it for paying work, but I've found it helpful for clarifying my thinking and teaching me new concepts.
The good use case is whenever you read a file, apply a pure function to it, and write the result, with no other IO. And other similarly simple cases, where there is no significant IO logic and ordering doesn't matter. In other words, maybe 60 to 70 percent of programs. It's crazy to force beginners to learn pipes, conduits, or similar before they can write a simple program that reads and writes data. Or to teach them that they must load their terabyte files entirely into memory before they can process them. Lazy IO always was and still is by far the simplest way to do streaming IO in Haskell. And it has massive support across the ecosystem and within GHC. Until someone comes up with something else that works as simply as lazy IO (which I hope will actually happen, but so far has not), lazy IO will remain the default way to do streaming IO in Haskell - for the simple cases where it works.
Hum!! This looks very convenient.
9 or 10 years in total, full-time professionally for the last 7.
About 10 years, I think. I was aware of it for long before that, but finally learned it while in college by reimplementing some of my homework assignments in Haskell. They were superior to my original versions in nearly every way, and the rest is history.
Inching towards deterministic builds 💯💯💯
I'm so glad Haskell has such terrible records. `lens` is a much nicer solution than records for the majority of cases people want a record.
One major downside of pattern synonyms is that there is no exhaustiveness check. There's [some](https://ghc.haskell.org/trac/ghc/ticket/8779) work being done in that regard, but it's not clear when it will land.
Other people have mentioned "term-rewriting". One specific thing to look at within that category is [Maude](https://en.wikipedia.org/wiki/Maude_system). I'd also take a look at the [SF-calculus](http://lambda-the-ultimate.org/node/3993), which is a minimal system (like the [SK-calculus](https://en.wikipedia.org/wiki/SKI_combinator_calculus)) that supports rewrites and has confluence guarantees.
One difference is Inference: https://www.reddit.com/r/haskell/comments/56rkfl/try_backpack_ghc_backpack/d8m8vvy?context=3
&gt; parsing Parsing is one of the great use cases for lazy streaming. The major parsing libraries have been deeply tweaked an optimized for that, using fusion and other techniques. &gt; Except when you miss the detail in the documentation... that the function is lazy... Dude, I think you missed a detail in your very first Haskell tutorial. Haskell is a pure non-strict language. Functions are lazy. In fact, the burden of documentation red flags is on library functions that do crazy strictness stuff for optimization purposes - those can really bite you. &gt; ...typing discipline... unrestricted mutation... Wat. None of that is even remotely related to lazy IO. Lazy IO is principled out of the box, because it meshes naturally with Haskell's underlying philosophy and is supported directly by the compiler and all core libraries. Whereas the newer streaming libraries needed to use some deep theory and complex techniques to be principled (and they did a fantastic job of it).
Strict IO does not work for streaming; you need to wrap it in a higher-level streaming library. For the simple case, the streaming libraries are definitely usable, but there is no comparison with lazy IO. I have been using conduit for years, yet I still find myself double-checking the documentation almost every time I use it, even in simple cases. Whereas lazy IO is natural and fits right in with the Haskell way of thinking.
Okay, I think I understand now. I think, though, that the base `PermParser` type itself should, to use `RankNTypes` better and hopefully be more performant in general, be something along the lines of: newtype PermParser m a = PermParser { unPermParser :: forall x r. (a -&gt; x) -&gt; (forall y. m y -&gt; PermParser m (x -&gt; y) -&gt; r -&gt; r) -&gt; r -&gt; (x -&gt; r) -&gt; r } This unfolding of the `PermParser` type obviates the need for a separate `Branch` type (because having `Branch` be a `RankNType`d Church tuple looks rather awkward), and as an added benefit uses a Yoneda encoding so an `fmap` on it doesn't have to be propagated all the way to every leaf immediately. Maybe I should make this a comment or pull request on the GitHub page itself, though.
Learned it at Uni in 2001, but didn't get back into it until recent i.e. 2014. What got me back into it is remember how much I loved it and that it could express things mathematically. But at the same time I was put off by the learning curve. Nowadays it has been made a lot easier with so many blog posts, and monad tutorials etc.
Unfortunately, this seems to be easily missed. Perhaps because people don’t look for help if they don’t know that they need it and it’s available.
Isn't this related with what Simon Marlow discussed the other day ?
The `ghc-8.0.2` package in [my PPA](https://launchpad.net/~hvr/+archive/ubuntu/ghc) provides builds of the 2nd release candidates configured &amp; compiled specifically for Ubuntu 16.10/Yakkety, 16.04/Xenial(LTS), 14.04/Trusty(LTS), and 12.04/Precise(LTS); and for x86_64 and i386 respectively. The basic instructions (for more details see the PPA description) are sudo apt-add-repository ppa:hvr/ghc sudo apt update sudo apt install ghc-8.0.2 cabal-install-1.24 and then simply adding `/opt/ghc/bin` to your `$PATH` If you install multiple versions, you can control which version the `cabal` and `ghc` symlinks point to (by default they point to the latest stable released version; release candidates are not considered released versions) via sudo update-alternatives --config opt-ghc sudo update-alternatives --config opt-cabal respectively. This scheme allows to easily switch between different ghc versions via cabal's `-w`/`--with-compiler` flag; e.g. you can simply say `cabal install -w ghc-8.0.2` (or `cabal new-build -w ghc-8.0.2`) or place `with-compiler: ghc-8.0.2` into your `cabal.project` file to select a specific GHC version.
&gt; Parsing is one of the great use cases for lazy streaming. The major parsing libraries have been deeply tweaked an optimized for that, using fusion and other techniques. E.g. attoparsec's lazy bytestring seems to do pretty much what conduit would do: https://hackage.haskell.org/package/attoparsec-0.13.1.0/docs/src/Data-Attoparsec-ByteString-Lazy.html#parse There's slightly higher overhead with using conduits, that's about it. I just don't see any reason to use Lazy IO except adding risks to the code, because of IO happening in pure functions. &gt; Dude, I think you missed a detail in your very first Haskell tutorial. Haskell is a pure non-strict language. Yep, in those tutorials they write that IO is basically a State RealWorld# - which is *pure*. So I would expect that given a same RealWorld# and other input the function yields same output. You know what? Once you start working with Lazy IO, this doesn't work anymore. &gt; In fact, the burden of documentation red flags is on library functions that do crazy strictness stuff for optimization purposes - those can really bite you. Oh, really... so this can't bite me? *R&gt; bad_ctx $ \(x,y) -&gt; x == y True *R&gt; bad_ctx $ \(x,y) -&gt; y == x False Or the sample I gave in other response which has bitten me: &gt; withDb $ force &lt;$&gt; query something [Record, Record...] &gt; force &lt;$&gt; withDB (query something) [] So suddently this free theorem doesn't hold: r :: forall a f g. (Functor f, Functor g) =&gt; f a -&gt; g a fmap h . r == r . fmap h No, this really can't bite you. This is something you really shouldn't expect to work in pure language, right? 
&gt; For every complex problem there is an answer that is clear, simple, and wrong. &gt; -- H. L. Mencken Lazy IO is works fine... until it doesn't. The boundary is not clearly marked, which makes it very unsafe. In contrast, strict IO works acceptably in every language. The places where it doesn't work are clear and obvious, and it makes a perfect building block for higher level solutions. I would like to think that "the Haskell way of thinking" involves learning from both theory and practice, and using safe and principled techniques. 
They're more likely to be read than the formatting help link though. 
/ I renamed it to "greaterThan". 
it's a good question. I'm not really sure, tbh - i think perhaps at the moment people just rely on client timeouts? It's hard to see what you could do that would be more sophisticated, unless you want to start autoscaling. You'd also have to be able to distinguish between deliberate longpoll requests and normal ones. wait, i think i misread. Just tracking how long each request takes, forwarding to a logger somewhere, and making interventions manually sounds like an excellent idea.
I didn’t know about the package takeover process, interesting to know!
I didn't know this was possible! Very cool
1. if x :: Int, then return x :: Monad m =&gt; m Int return is a method of an interface, the behavior depends on the implementation. but it's almost certainly thread-safe in every instance (in IO, in Maybe, in State, etc). 2. what does urllib2's post() do? (I had to write a Python shim for my Haskell service). After reading a lot of text, a "file-like object", which isn't hyperlinked. I called list() on it and called it a day. In Haskell, the types are all automatically hyperlinked, and I can always answer "what does the function output?" and "what exactly is that output?". 3. no, it doesn't take a decade, I got a lot done in the first month, and you don't know enough Haskell to be making these claims. As someone who learned Haskell after Python, nothing you've saidrings true to me.
This still leaks memory: http://lpaste.net/350028
I learned it last year (freshman year of CS). The adviced language for the course was [Clean](http://wiki.clean.cs.ru.nl), but Haskell would be "okay" as well, but none of the Student Assistants and other personel really understood Haskell (even though it is not too different from Clean). So when the follow-up course had Clean as a mandatory requirement, I kind of dropped out of that course, (there were rumors at the time that the course would be in Haskell this year, turned out to be true). This year, the course was turned on its head completely, now Haskell is the only language, and I've been using haskell for my own things for a year. The first course has also risen in difficulty massively (with good reason). So after the Christmas Break I'll be attending the second course, not quite sure of the contents yet. (I believe I went slightly off-track, I hope you still liked it?)
It's amazing how many new things are coming for 8.2.1 which was supposed to be only a "consolidation release". :-)
I'm not recovering from an exception in some lazily evaluated pure expression, so "unsafe" sounds right to me.
Yes, you should not have tweeted this at all because your measurements have nothing to do with reality. The hackage front page clearly says (and it has been that way for quite some time) that packages are expected to follow the PVP. Before that, it was commonly understood to be the case. There is nothing to debate about here. A version scheme only works when it is a standard that everyone follows. People already complain about the difficulty of following one standard. Introducing a second one makes that even harder. On top of this, SemVer is a less expressive scheme! There is literally not a single good reason to switch to SemVer. The PVP is the standard. Please don't continue to fracture the community with a debate that has zero potential for positive impact and a significant potential for negative impact.
I look forward to backpack making it possible to generalize these kind of parsing libraries over all the other various parsec libraries (parsec, megaparsec, attoparsec, etc)
Are there any talks?
Yup, it's very annoying. I've gotten in the bad habit of writing foo _ _ _ = error "impossible: broken exhaustiveness check in GHC" at the bottom of all my functions that use pattern synonyms because I love them so much. 
The methodology used for this was flawed. But I think it is possible to get a fairly accurate measurement. For each package: 1. Look at all the versions of that package and find all of the ones that make a breaking change from the previously released version (probably by date, not by version number). Breaking change roughly means that an exported type signature changed in a way other than simple type alias substitution, or an exported type signature was removed. 2. For each of these breaking changes, look at how the version number changed from the previous version. In A.B.C.D, if the package ever bumped B on a breaking change, then the package is most likely using the PVP. If the package only bumps A on a breaking change, then the package is probably using SemVer.
[removed]
Advanced Funtional Programming (a master course) is still taught in Clean. Not sure for how long though. 
Not yet for this paper: it is going to be presented at POPL in Paris; the conference is between January 15 and January 21, and this talk is currently [scheduled](http://popl17.sigplan.org/program/program-POPL-2017) for 14:45 on Thursday January 19. (Feel free to attend if you are around, although if you want to attend more than just a couple talks it is probably better (but costly) to [register](http://popl17.sigplan.org/attending/registration).) There have been talks given in the past on the topic, although they usually presented rather different version of the system, and I don't know if any was video-recorded.
&gt; this was adopted to avoid confusing newbies. And to make things that "look like" constants be evaluated exactly once and only things that "look like" functions be evaluated each time they are referenced. This is done by avoiding a type class constraint (roughly equivalent to an implicit parameter) when there are no explicit parameters, by choosing a monomorphic type rather than a (parametrically) polymorphic noe.
about 2 years. just for my own personal projects so far but I've managed to do a fair bit of development with Haskell (and PureScript). fwiw I'm a self-taught Programmer Joe. 
Since 2008 or 9, I guess. I remember learning from the 98 report, and doing my first non-trivial program for the Google AI Programming Contest in 2010. I wish I had learned it a decade earlier.
Scala's ok. If I had to do FP on the JVM, I'd probably do it in Scala right now. Kotlin doesn't really scratch that itch for me; in my view, Kotlin is to Java what Rust is to C++ in terms of purpose and style. But anyway, I just like Haskell so much more than Scala that I'd rather use it than the JVM. My MC username is the same as my Reddit one (perhaps unsurprisingly =P). But unfortunately, I haven't really been a player in a year or two so it's unlikely that I join you.
Nice to see more work on CBPV (aka focusing/polarity) - and hopefully a more natural treatment of effects, too. These are two of the clearest pain points in Haskell.
This doesn't work great for sophisticated beginners. I have a friend who's a huge Erlang fan and I was trying to sell him on Haskell. Runtime errors due to the lolPrelude cooled his enthusiasm.
I can't say I've yet fully disgested it, but this is another interesting and incredibly well-punned paper :). I love how there are little puns in the text itself: "[...] to be frank [...]" (etc.)
I can't put my finger on it but that line gives me the giggles.
How big a deal is it that this disallows strict functors? {-# LANGUAGE DeriveFunctor #-} {-# LANGUAGE ExistentialQuantification #-} {-# LANGUAGE FlexibleInstances #-} class Seq a instance Seq a data Foo a = Seq a =&gt; Foo !a deriving Functor $ error: $ * Can't make a derived instance of `Functor Foo': $ Constructor `Foo' has existentials or constraints in its type $ Possible fix: use a standalone deriving declaration instead $ * In the data declaration for `Foo'
I mean it makes the whole proposal essentially a non question really, a lot of data structures have a real nice algebraic structure and this prevents us from using our syntactic sugar for it. That being said, could there be another proposal to add constraints to our major higher order abstractions to allow for things like this? I'd be really interested in hearing peoples arguments for and against this. I think SubHask showed us that its strictly more expressive and not a big hit off performance.
Maury cripmass y'all 
Have you tried to add: ``` compiler-check: match-exact ``` to `stack.yaml`
[removed]
That makes sense, thanks!
As a native Spanish speaker, I find it terribly confusing when words other than *map*, *fold*, *functor* or *monad* are used to refer to them. I'm just giving some examples, but of course this applies to every other programming concept as well, unless this concept has a very straightforward translation in everyday life. For example, "node" translates to "nodo", and "tree" to "arbol", and you don't even need to know about programming to know those words. Oh, and this applies to other speaking languages as well. Learning English is part of learning to program. Otherwise how will one read the many books, articles, papers and documentation, and listen to people's talks, which are mostly in English? Those are things one will need to do anyway to learn more about programming.
I've used Haskell for 26.5 years, since August 1990. 
You know about "return x" because return in part of a common interface in Haskell. I don't know what the point you guys are trying to make. Yes, in Haskell functions have type signatures, that can tell a lot about the function. I get it. My point is that, even if you know the type signature of the function, you are not 100 safe. Say, your functions can still throw an exception if you use partial functions (Prelude.head). But the type signature does not tell you about it. The disclosed absence of safety is better than a false promise of safety. And there are a lot of ways Haskell gives these kind of false promises. You see, if you have enough experience in the language, these problems essentialy becomes invisible to you, as it has become second nature to navigate around them. But for someone who is starting with the language, these are big red flags, and gives the impressions that Haskell's hype is more backed up by true believers than good reasoning... My problems is that the type level gimmick sometimes gets too much out of hand, and for no good reason. You take any staticly typed langauge. Go, Rust or even Typescript. These hold a better job at balancing complexity of type system vs the safety it provides IMHO. With Haskell it feels like that the primary purpose of the language is enabling people to play Type golf with it, rather than solve real problems. Might be one of the reasons it is still not popular despite the persistent hype around it for so many years...
Great stuff /u/Axman6! The use of `mmap` is interesting, thanks for sharing your work.
It's actually his DPH example - I should have stated that. https://youtu.be/kZkO3k9g1ps 24 min in he talks about why it's like that. In particular, making the "sorted" intermediate list allows you to target the DPH. I think without making that list, if you replace sort (x:xs) = sorted !! 0 ++ eq ++ sorted !! 1 With sort (x:xs) = sort lt ++ eq ++ sort gt I think our solutions are equivalent, except resorting the equal elements.
I had no idea!
True. Though I think when it comes to stuff like functor laws, we tend to sorta pretend bottom doesn't exist =P So at least we're *able* to create that instance. With this change, it would be actually impossible; not just law breaking.
This sounds useful, thanks for building it! In my daily job I have to transfer hundreds of GB each day from/to S3, so this seems like a nice improvement over the standard "aws-cli" from Amazon. Did you have any chance of comparing the two for performance?
What about strict collections?
I used GHCJS with Reflex, the experience was lovely. I'm not experienced enough with the JS world to be able to compare performance, but it was enough for my simple project.
&gt;Python and Rust are memory safe. Haskell is pure, "effects safe". Idris (let's say) is total, "termination/exception safe". This exactly is the whole issue. This is the fine print in the Haskell hype. Every common imperative programmer lured into learning Haskell, when banging their head to make sense of the abstractions in it, push forward in the hopes of enabling themselves to write programs free of "errors" in general, But very often they won't know that the only thing they are getting is this "effects safety" in return for all that effort and trouble. I think Haskell community should be more upfront about this. 
I usually use collections that are: * spine lazy, element lazy * spine strict, element lazy (where sometimes I might use functions that evaluate them strictly, but the data structure itself still doesn't enforce it) * spine strict, element strict (but in this case it's almost always because I have some other constraint on the elements anyway, such as unboxed or storable or something, so I don't get a functor instance anyway)
Were there any compiler bugs? What packages did you use?
Nothing I could see. Just `reflex`, `reflex-dom`, `servant` and some very common packages. Take a look at /u/mightybyte 's comment below, his posts here were one of the sources I used when deciding.
Elm is not simply backlogged on features. It is actively trying to find the minimal set that yields good productivity. It sounds like you want something more like Purescript. 
Clojurescript
`Data.ByteString.Char8.unpack` maps bytes 0..255 to unicode code points 0..255, which you can think of as using ISO-8859-1. But really it's not using any encoding at all. To specify a character encoding, convert to a `Text` using one of the functions in `Data.Text.Encoding` in the `text` package. If you really do require a `String`, then use `Data.Text.unpack` on the result.
Yes, now I wish I didn't wrote that app in elm... Oh well, you live and learn.
Our front end at SlamData is powered by around 130k LOC of PureScript. 
It's probably worth mentioning that SlamData is open source software as well, with the [code available on GitHub](https://github.com/slamdata/slamdata/), in case anyone is curious to see what real application development in PureScript looks like.
If you aren't wary of using internal functions, then you can get added performance by using [`Data.Text.Internal.Encoding.Fusion`](http://hackage.haskell.org/package/text-1.2.2.1/docs/Data-Text-Internal-Encoding-Fusion.html)'s `streamUtf8` function to produce a stream of characters, and then use `unstreamList` from [`Data.Text.Internal.Fusion.Common`](http://hackage.haskell.org/package/text-1.2.2.1/docs/Data-Text-Internal-Fusion-Common.html) to turn it into a normal `String`. This reduces the encode-decode overhead.
Would really love to hear you elaborate on this! I can't say I agree that PS is better than *Haskell*, but I would agree that it *may* be better than GHCJS. But it hasn't been a problem for me so far, and I'm hopeful that WebAssembly will come to the rescue for GHC Haskell on the browser.
We've been making steady progress on the PureScript language and compiler. I've spent the last month writing about new features [here](https://github.com/paf31/24-days-of-purescript-2016/blob/master/README.md) for anyone interested ("24 Days of PureScript", in the style of 24 Days of Hackage). Here are some highlights from the past year: - Functional dependencies - Type class deriving for various classes including Eq, Ord, Newtype and Generic - Generics in the style of GHC.Generics - Lots of improvements to editor tooling, including a fast rebuild tool like ghcid - An experimental new package manager inspired by Stack(age)
There is also Fay. I have just toyed with it. But really liked it.
Can you elaborate on your experience with Elm?
Strict IO works fine... until it doesn't. Strict IO is unsuitable for streaming, since it quickly fills all memory and crashes your program. So how could anyone ever consider using such an unprincipled and unsafe approach as strict IO, ever? Yet strict IO is in fact principled, usable, and safe, if you take the time to understand its principles and apply it correctly. The same is true for lazy IO - it is safe and principled.
It's hard for me to understand why people become so irrational and emotional these days when you mention lazy IO. Why do you keep harping on some broken DB library, or your own misuse of that library, whichever it is? That has nothing to do with lazy IO itself. Your "example" of `bad_ctx` is totally incomprehensible; it's like saying that lazy IO causes cute kittens to die. Please, calm down, and state your arguments clearly.
Use `readMaybe` instead of `read`: readMaybe :: Read a =&gt; String -&gt; Maybe a `readMaybe` is not in the Prelude, but it is in base, in `Text.Read`.
When we (and the authors of the original post) say that "partial functions" are bad, we don't mean: * Higher-order functions, which are vacuously partial if you feed them a partial function as an argument. * Functions that are partial when you feed them an infinite list or other infinite structure. The problem with `some` and `many` is a problem with the Alternative class. They are not partial for parser-like Alternative instances, and they are completely unusable for other Alternative instances. There has been discussion in the past about dividing the Alternative class into two for this reason, but so far it has not happened.
I was having a great time with Reflex+GHCJS until I upgraded to macOS Sierra and [it all broke](https://github.com/reflex-frp/reflex-platform/issues/79). I still haven't figured out how to fix it, so I^started^using^GopherJS^instead^sorry.
6 years (and 3 years learning it on and off before that) http://joeyh.name/blog/haskell/ I write free software, entirely in Haskell lately, and somehow get funded to continue doing it. Partly funded by a college these days, but I am not an academic myself.
I like the idea of using Nix more than using Bower, but I'd prefer a solution which didn't assume any particular package manager at all. I think the new package sets approach is quite good in that regard, in that users can continue to use Bower if they want, or use psc-package, or even use the same package set data to create Nix derivations.
Ignore this, I forgot that I could uninstall things through the software manager. A silly oversight on my part.
Purescript supports enough type system features that you can write interesting code and can continue to build on top of it by being clever. The resulting code is strict but has a reasonably straightforward desugaring. The compiler could be smarter about the produced code, but we've largely been able to fix that up locally with custom rewriting tools applied to the output javascript. On the other hand I find that with Elm you rapidly hit a ceiling and have no way forward other than writing lots of additional completely first order code. The lack of rank-n types and typeclasses cuts you off at the knees. Now, I'm clearly not the target demographic for Elm, but I find I don't really want anybody I'm training or working with to wind up stuck with only the limited set of tools provided by Elm as the only tools in their functional programming toolbox, so I'm left with no demographic to actually recommend it to. I don't get paid per line of code. You didn't ask about it, but as /u/chrisdoner mentioned, GHCJS produces pretty massive codebases. That said it can run a rather unexpectedly large cross-section of standard Haskell code. When you need to share code between your backend and frontend, and can afford the transfer time, it can still be a huge win.
&gt; or even use the same package set data to create Nix derivations. [Which is exactly what I've done](https://github.com/ElvishJerricco/purescript-packages2nix) =P Well, the reason I think Nix makes a good baseline is that if someone writes a package in PureScript that requires their Nix configuration, that package can no longer be trivially depended on through any other system. For reasons like this, as well as reasons of version management and reproducibility, I think dependency management system inevitably must be chosen. And I think Nix makes the most powerful / safe system.
I'm curious, what did you find Gopher JS offers over Purescript or even Typescript?
My experience seems to show that the concurrent upload manages to saturate our gigabit ethernet connected to our 10 gigabit internet link, which is as good as was-cli manages. I might be able to do some more sensible things if I add better support for how many concurrent uploads occur by splitting the file into `length/n` sized segments, instead of `length/chunkSize` minimally sized parts.
&gt;Haskell v Java is effects safety... Yes. that is what I am telling. A statically typed language is good enough and gets rids of most of the potential bugs already. The massive additional baggage (including all the numerous extensions that you might come across in the wild) of Haskell get you a tiny bit further, bit nothing more. In terms of safety., I mean. In terms of powerful abstractions, probably a lot more, I concur. But people are not really interested in powerful abstractions. But there is this undeniable beautiful warm and fuzzy felling when you string functors and applicative and monads and what ever type gimmick that new package on hackage employs, together and get it to work. But I agree that Haskell has a very important role in advancing the programming world. But it might be best for every one, that it is not advertised as a general purpose language..
The GopherJS tooling is far simpler than TypeScript's, so it's much easier to work with. I attempted the same project with TS, but had trouble getting as far as configuring the project for development and deployment. The package management was shoehorned into npm in a way I couldn't ever really figure out. The GopherJS story is simple: run "gopherjs serve" from anywhere to develop, and "gopherjs build -m" to deploy. Go tooling was already amazing and the GopherJS people somehow tacked on JavaScript compilation without messing any of that up. (Go's package management does suck, but they didn't make it any worse. And it doesn't suck for being incomprehensible, only being too simple. That's easier to work around. :) I haven't tried PureScript, but I did try Elm (are they similar?). The Elm abstractions (commands, events, etc) made it very easy to write small prototypes, but I had trouble modularizing my code because eventually every module wanted all of those features and the language didn't seem to have an easy way to mux them. So the size of an Elm code base that's difficult for me to manage is a lot smaller than I'd like. I'm sure I'll hit the same threshold with GopherJS, but at 1.5kloc I still haven't even glimpsed it. 
I personally just gave up on doing SIMD from Haskell for now and moved to compiling native code or shaders from EDSLs, rather than trying to get GHC to do something clever here. Without a way to use shuffle operations I really don't have any applications that I can accelerate enough to matter. As to your actual question, you might be able to figure out if an LLVM flag is being passed to GHC by using a custom Setup script and looking for it explicitly. You can then pass information about it to the rest of your code by using a file like Paths_foo. I do something vaguely like this to set up my doctests to figure out exact versions of all of the package dependencies to load and to emit where the autobuild dir is so I can find the extra source file: https://github.com/ekmett/lens/blob/master/Setup.lhs#L36 You might get some mileage out of this approach.
it's more than effects, it's about how rich the types are. Java has null (Haskell doesn't, Maybe is explicit) and everything subclasses Object (Haskell types don't, if you write `id :: a -&gt; a` you can't call `.toString()` from the aether, you need to be explicit about the interface you're consuming (i.e. `(Show a) =&gt; ... `). Even if it were "just effects", that's the overwhelming majority of my bugs, not a minority like you claim. I tried to write pure code in Python/Java (before learning Haskell, and before even knowing that "pure" was a thing that the machine could verify) and failed (because it wasn't being verified, and the libraries I has to program against weren't very pure even if mine was). I don't know what you mean by "not general purpose", because you can google "haskell applications" and find xmonad, git-annex, etc; they're completely different, they're all open source, you can read them yourself if you don't trust me that they exist. also, banks use Haskell, if OSS doesn't count as "general purpose" for you. Or, you could give me an example of an arbitrary "general purpose" problem you expect any language to be able to solve, and then you can google its probably already existing solution. 
Curious - can PureScript eventually generate to LLVM and replace GHC itself, given a brand new RTS implementation?
&gt;Java has null (Haskell doesn't, Maybe is explicit) and everything subclasses Object (Haskell types don't, if you write id :: a -&gt; a you can't call .toString() from the aether, you need to be explicit about the interface you're consuming (i.e. (Show a) =&gt; ...). To be clear. I love those parts of Haskell. The part I hate most is the advance type level stuff provided by some language extensions and how far some popular libraries are willing to go with them in the name of type safety, when the kind of type safety provided is not really 100% and is not that critical. It is extreamly exhausting. You think you have at a point where you are comfortable with the langauge, and there comes another library with some weird type signature or something. And you look up the file and sure, there is another extension that you are not familiar with. Now you have to decent again into the rabbit hole to actually wrap your head around it, which often takes as much time and effort as you spent learning the basics of the language in the first place (DataKinds, GADT, PolyKinds, RankNTypes!). I mean, these extensions keep coming and coming and as I told before, it is exhausting... And it feels like you will be treated like an outcast if you are not into the library that uses, latest type level wizardry, because hey, that is what Haskell is for right? &gt;I don't know what you mean by "not general purpose", because you can google "haskell applications" and find xmonad, git-annex, etc; they're completely different, they're all open source.. agree. Haskell is general purpose alright. What I probably meant was that it is a combination of the communitie's fetish with the type level gimmickry and whole phycological battle one has to wage to burst through the whole "magic" surrounding the languages and the abstractions in it, that makes it, in some kind of "meta" way, not a general purpose language... 
May I ask, what UI library? Was checking out some PureScript lately, and the experience was really great from dev-process perspective. However, I wish it could have some FRP library (not in broader sense of this word). I know the JS runtime currently is not up to the task (weak pointers), but I found that, for instance, Reflex is so much nicer than other approaches - feels like it stands firmly on well defined basics, whereas other libraries (not only Purescript ones, generally in JS world too) are in kinda...experimental state ("let's check if having single global state is good idea", "let's check if having some cursor into that state is good idea", "let's check if the UI being modelled as a fold of user actions is good idea" and so on, and so on....)
I've mostly found haste to be flanked on either side by purescript for things that really need to generate cheap javascript, and ghcjs for when I really need existing hackage package support. Then again I don't really know how much haste has improved since the last time I tried it. Glad to hear it has at least one happy customer!
When is `purescript-0.10.3` going to be in Stackage? The latest version in Stackage LTS-7 is `purescript-0.9.3` and when following the instruction to install `purescript-0.10.3` I have to manually unpack the tarball and then it wants to install half of LTS-6 which seems weird given that `purescript-0.9.3` was already in LTS-7. There doesn't appear to be any `purescript` included in the nightlies either.
What's your take on scala.js?
Thank you for your explanation! This is obviously a very important side of typeclasses and I completely forgot about it when thinking about the Elm/Haskell differences. I use functions dependent on typeclasses everyday and I am very glad they exist. I suppose the main reason I forgot about this is that typeclasses would be more useful in writing libraries or in the context of a big project and I have done neither in Elm so far. With your explanations I can understand the tradeoffs better. Code reuse is a very important aspect of good software development and I will try to improve by reusing code as much as I can. &gt; Elm is extraordinarily boilerplate heavy. It is, when writing a JSON encoder/decoder I am very thankful for Haskell's Generic or Template Haskell approaches. On the hand Elm's boilerplate way does have the advantage of being concise and understandable to many people. Accessibility to many people is part of the tradeoffs, and how much better would web development be if JavaScript people started to program in a statically typed functional language? I think Elm has more chances than Haskell with GHCJS to be picked up by a lot of people due to its simple design. This is why I think it is worth trying to improve Elm while keeping simplicity as a design choice makes sense. Something I forgot to mention in my previous comment is just how good the Elm package manager is. It enforces semantic versioning depending on a library's exposed API. This is something other statically typed languages should definitely try to copy! :-) I haven't tried all the JavaScript replacement languages mentioned by OP, I'll checkout PureScript next as soon as I can. 
Sure, though being somewhat compatible, I wonder what subset of current Hackage packages it can already build now considering little or no modifications.
&gt; Why do you keep harping on some broken DB library, or your own misuse of that library, whichever it is? How would you semantically describe a contents of the `x` variable in this code sample: x &lt;- readFromHandle h It's a String. The value is a result of reading from a handle when the readFromHandle was executed. How would you describe the content of the `x` veriable in this snippet of code: x &lt;- hGetContent h It may be String, it may be bottom, it is a result from reading from a handle at some point in time. It's non-deterministic (reading from a handle at different point in time will result in different values). You could say "Lazy IO brings non-determinism, don't expect any laws to work with non-determinism" - in which case I'd argue all these functions should be marked `unsafe`; which is essentially 'don't use it'. Yes, it's a misuse of a library to use `unsafe` functions and expect any laws will hold. E.g. the following 2 snippets provide completely different results: code = withFile "a" $ \f -&gt; do res &lt;- getContents f return res code = withFile "a" $ \f -&gt; do res &lt;- getContents f print res return res Values in haskell are immutable. Not so in this example; the `res` is obviously mutable. No, the `IO` monad by itself doesn't allow mutability of immutable variables. So either Lazy IO brings non-determinism or it brings mutation. I haven't read in the tutorials that haskell is a non-deterministic language. And I have read that String is non-mutable. 
One way out of this is to split the UI from the business logic. For example using `react` for the UI means that UI designers can tweak the layout with instant refresh while business logic takes more time. You want business logic to be tested anyways, so this actually does solve the issue, and it's faster than anything that could possibly be done in the "real" `GHC.
A couple years ago I worked on a project that was using Haste. We found that even its seemingly minor differences from GHC made it so that we weren't able to share much code between the front and back ends. Since then I've heard that Haste may have improved on this front, but my experience there leads me to believe that if we were unable to achieve significant sharing with Haste, you're much less likely to achieve it with PureScript.
If strict IO was unsuitable for streaming, then all languages other than Haskell would have big problems with it. And yet this is not the case. Sure, you can full up memory in, say, Perl but it's pretty straightforward avoid that. And - and this is critical - It's relatively easy to reason about. Not that I'm an advocate of Perl. The streaming libraries are built on strict IO. But I'm pretty sure you know all this. Could you give me an example of code you think works better with lazy IO please?
Yes. The "unpack" you see is from the awful, broken, bad-idea "Char8" module. For unicode encodings there is utf8-string (or the Text package decoders) -- for everything else there is the text-icu and other iconv-based packages.
Bloomberg recently released [Bucklescript](https://github.com/bloomberg/bucklescript), it's Ocaml's JS backend which emits readable code (the existing JSOO doesn't).
&gt; PureScript is better than Haskell As a language that compiles to JS I presume?
Agda's JS back end just got an overhaul I think.
Yes. This is probably one of the most successful languages that cross compiles to JavaScript along with typescript.
GHCJS is just a very slow compiler, even compared to GHC. It's been getting better recently. But it's still a notable problem. And the JS it outputs is quite large, as it includes a runtime with a GC and a model for GHC's green threads, plus it has to shoehorn laziness into every function.
Take a look at hledger.
There are some handy libraries for auto-generating Elm frontend glue code: * [elm-export](https://hackage.haskell.org/package/elm-export) * [elm-export-persistent](https://hackage.haskell.org/package/elm-export-persistent) * [servant-elm](https://hackage.haskell.org/package/servant-elm) With persistent, these libs and Elm, you can achieve full type safety from DB to frontend. Because of this, I've decided to use Elm for now. Purescript is definitely on my radar though.
[Link](http://hledger.org/)
The trick is to not compile to JS and ship the whole output to the browser but use GHCi (ghcid, even), compile on the server, run the business logic on the server and ship just the UI to the browser. Been there, done that. I see sub-second reload times after I change the code. It's on par with a plain JS/Babel/Webpack project where long loader pipelines often take multiple seconds to recompile/reload a project.
I had not thought about using `ghcid`. That's really interesting, I'm sure that if you described this workflow a little more in detail somewhere, the community would appreciate!
It's easiest to just clone the repo and build from source. The binaries are also available on the release page.
Related: there is a C++11 backend for PureScript which can be used to create native binaries: https://github.com/pure11/
This is one reason I don't use the servant-auth stuff and just do the auth directly. You can see my approach http://blog.wuzzeb.org/full-stack-web-haskell/server.html Essentially, I use the Nat which converts from the custom monad to Either ServantError. The nat performs the auth check and then sets the result into the reader monad.
I'm on the latest version of servant. I would say I've pasted all relevant code, only thing I could do besides that is adding a link to servant auth tutorial :D. http://haskell-servant.readthedocs.io/en/stable/tutorial/Authentication.html#generalized-authentication
I usually agree about lawfulness. But in the case of `bottom`, I give a pass. There's just very little that passes the `bottom` test in laws.
&gt; The big output isn't a problem A lot of people would disagree there. Bandwidth is important. Anyway, I'm sure there's a lot of RTS overhead not just in the RTS itself, but in all the ways the code has to be different to account for it, like the GC, threading, and laziness.
awesome software in awesome language, thank you :)
I've had odd speed issues with ghci too. Usually just on the initial load of a module. Have you seen this?
You’d have to think about connectivity. For local applications that’s wonderful, for everything with possibly only a flaky mobile connection in between it’s an *absolute* no-go. Maybe take a look at https://github.com/ghcjs/ghcjs-vdom?
To be clear - I was not claiming that strict IO is unsuitable for streaming. I was applying the *reductio ad absurdum* argument that parent poster's reasoning, which involves applying an IO technique in a naïve, simplistic, and incorrect way, equally shows that strict IO is unsuitable for streaming. Therefore the reasoning is flawed. For your example - I posted it by mistake in reply to /u/ondrap instead of to you, sorry about that.
For comparison, compiling 21kLOC Haskell with GHCJS takes 3 minutes for me.
For the sake of completeness: you can use the Flow type checker and https://github.com/gcanti/flow-static-land to get ADT, HKT, and other nice features in otherwise vanilla JS. If interop with JS is highest priority, this approach might be worth considering. Though PureScript has a good FFI.
Do give PureScript a [Try](http://try.purescript.org) :)
Well, let's assume an average of 4 compliant version numbers for any package. My projects tend to pull in at least 50 dependencies at *least*. That minimal number is 4^50 possibilities to test. That's 1.3e30. At one build a minute, that's 2.4e24 years. Even at 2 versions instead of 4, that's still 2e9 years. Doing that but with with 20 dependencies instead of 50, that's still 2 years.
I once had a similar idea and wrote [sunlight](https://hackage.haskell.org/package/sunlight). But as ElvishJerricco points out it winds up being too many versions to test. So I focused it on two package sets: the minimum versions specified in the Cabal file, and the current versions posted to Hackage. Ultimately I was unwilling to put in the work necessary to make this work reliably; Cabal files have all sorts of features (like conditionals) that make this a harder problem than it first appears to be. In the end Stack and Stackage solved this better than I could hope to. Now I can just specify a Stack resolver and know my package builds with that. Trying to keep something buildable with multiple sets of dependencies gets way too hard way too fast.
I agree. Some of my projects have that few dependencies/options per dependency.
IIRC several banks use Haskell extensively, but that is proprietary stuff
We _may_ have it for all packages. We're _going_ to have it if you want it for your own packages. Just to be clear.
&gt; Because these functions are fully polymorphic, we don’t need to check the definition of these functions to see if they are really equivalent. This is true in case of SKI combinators, but a dangerous statement in general. Consider `a -&gt; a -&gt; a`, which has two different implementations. 
I wrote a blog post a while back about implementing an auth combinator in Servant, and you could pass in a database connection pool to your auth combinator via the Servant Context. https://jerrington.me/posts/2016-06-18-token-authentication-with-servant.html Some of the material probably need to be adjusted since the post used Servant 0.7 if I recall correctly, but the idea is there.
I think this kind of thing is possible (really, with TH, anything's possible...) but as you say unlikely. Maybe find an order that first makes sure every package is included once, then every pair, then every triple... You may be able to safely stop at some point in that progression, but even if not it should scare out issues earlier.
It's possible to use it with stack. [But it's not exactly pretty](https://github.com/ElvishJerricco/nix-cabal-stack-skeleton). EDIT: To elaborate, stack can very nicely use Nix to get GHC. This is known as its Nix integration, and it's very straightforward and nice. To get it to use Nix's Haskell packages, you have to do as I linked
I *think* I ran into the same issue and commented here with my solution (partial function application): https://github.com/haskell-servant/servant/issues/455#issuecomment-247624616 Does this help?
it is very interesting but I dunno whether it is feasible to start with something like this. 
My guess is not... or it already is depending on how you look at things. Stack do global cache as long as you do not play with your versions of pinned packages (nor change flags on them). Otherwise, it is impossible to cache all possible versions.
afaik, GHC isn't deterministic. i.e. recompiling the same source with the same dependencies/flags can output binaries with different symbols. e.g. a compiled program that imports `x` from `A` expects a symbol `x123` but is linked against a separately compiled `A` that provides only `x456`. So caching can fail. But, ghc is working towards deterministic builds. And, I've installed Haskell packages via nix without any corruption. Running `nix-env -iA haskellPackages.lens` and downloading all dependencies pre-installed within a minute is awesome. 
Advent calendars have become popular among Japanese tech communities. I think they originated from the [Perl Advent Calendar](http://www.perladvent.org/archives-Yd.html), and were imported by Japanese Perl community back in 2008. They are similar to 24 days of Hackage but typically written by different authors for each day. Here is a list of past and relevant calendars: * [Haskell Performance Advent Calendar 2016](https://translate.google.com/translate?sl=ja&amp;tl=en&amp;js=y&amp;prev=_t&amp;hl=en&amp;ie=UTF-8&amp;u=http%3A%2F%2Fqiita.com%2Fadvent-calendar%2F2016%2Fhaskell-performance&amp;edit-text=&amp;act=url) * [Haskell Advent Calendar 2015](https://translate.google.com/translate?sl=ja&amp;tl=en&amp;js=y&amp;prev=_t&amp;hl=en&amp;ie=UTF-8&amp;u=http%3A%2F%2Fqiita.com%2Fadvent-calendar%2F2015%2Fhaskell&amp;edit-text=&amp;act=url) * [Haskell Space Leaks Advent Calendar 2015](https://translate.google.com/translate?sl=ja&amp;tl=en&amp;js=y&amp;prev=_t&amp;hl=en&amp;ie=UTF-8&amp;u=http%3A%2F%2Fqiita.com%2Fadvent-calendar%2F2015%2Fhaskell-space-leaks&amp;edit-text=&amp;act=url) * [Haskell Advent Calendar 2014](https://translate.google.com/translate?hl=en&amp;sl=ja&amp;tl=en&amp;u=http%3A%2F%2Fqiita.com%2Fadvent-calendar%2F2014%2Fhaskell) * [Haskell Advent Calendar 2013](https://translate.google.com/translate?hl=en&amp;sl=ja&amp;tl=en&amp;u=http%3A%2F%2Fqiita.com%2Fadvent-calendar%2F2013%2Fhaskell) * [Haskell Advent Calendar 2012](https://translate.google.com/translate?sl=ja&amp;tl=en&amp;js=y&amp;prev=_t&amp;hl=en&amp;ie=UTF-8&amp;u=https%3A%2F%2Fweb.archive.org%2Fweb%2F20150512134951%2Fhttp%3A%2F%2Fpartake.in%2Fevents%2F45a01d39-af5e-42f1-91c7-e8fcc91db244&amp;edit-text=&amp;act=url) (slow because it's from the internet archive) * [Haskell Advent Calendar 2011](https://translate.google.com/translate?sl=ja&amp;tl=en&amp;js=y&amp;prev=_t&amp;hl=en&amp;ie=UTF-8&amp;u=http%3A%2F%2Fweb.archive.org%2Fweb%2F20160116140243%2Fhttp%3A%2F%2Fpartake.in%2Fevents%2Feaea52c2-61ef-46d5-a855-3a2dde459e3a&amp;edit-text=&amp;act=url) (slow for the same reason) * [Haskell Advent Calendar 2010](https://translate.google.com/translate?sl=ja&amp;tl=en&amp;js=y&amp;prev=_t&amp;hl=en&amp;ie=UTF-8&amp;u=https%3A%2F%2Fatnd.org%2Fevents%2F10631&amp;edit-text=&amp;act=url)
Yes, got the same answer on stack overflow. I wanted to hit my head against the wall when I realized that I was so focused on the wrong thing. Still, it bothers me that if I decide to use `State` monad in my stack than I would essentially have to pass two "separate" instances of my state to auth handler and my regular handlers which would basically make it useless.
From the last comments on the nixpkgs issue tracker, I had the impression that they were dropping support for past versions and only following the last LTS. One of the reasons is that 20% of nixpkgs was only Haskell packages, and the maintainer was overwhelmed.
I was just following some conversations on the issue tracker of nixpkgs [here](https://github.com/NixOS/nixpkgs/issues/14897#issuecomment-224564140). By LTS I meant Stackage LTS. It's been some time already, maybe something changed?
It would only really make sense if you have more than one version per dependency (otherwise you would already test things with a normal compile). At 2 versions per dependency I would say the largest feasible number of dependencies (direct + indirect) you could do this with would be 3 (for 8 builds total, at 4 you would have 16 already), that is pretty much a library so simplistic it doesn't need testing at all (Hello World levels of trivial).
&gt; Stack can use nix under the hood Still not for Haskell dependencies though, or did that change? Last I checked, you had to use Nix directly to get the binary cache (which is fine, but it's a shame that Stack then becomes so orthogonal to Nix.)
IMHO the Javascript problem is a subproblem of the Web problem, which is a consequence of: - A particular, underdeveloped platform for the client, that is completely different than the one of the server. Both have almost nothing in common. - An idiosyncratic and primitive communication model, with no state, between the two. - On the fly download of applications in the Browser (navigation) which makes state management a nightmare - Client side neither manage user interfaces neither it is a document visualizer: It is both. User interface must be indexable!?? by search engines. - An event model and a rendering model which is a nightmare to program - Huge number of potential clients in potentially critical applications that may demand huge scalability, reliability and availability in the server side. - An unlimited set of different kinds of applications, from static pages to real-time streaming that supposedly must work in the same platform. - An infinite list of frameworks for every kind of task in the server and client side that grows each day.. that do not solve the above problems 
&gt; afaik, GHC isn't deterministic. i.e. recompiling the same source with the same &gt; dependencies/flags can output binaries with different symbols. Isn't that kind of....*ironic* ??? 
I used Haskell at JPM for a couple years for a few projects. One of them was https://github.com/kadena-io/juno -- a permissioned blockchain infrastructure that we open sourced. Technically, it does bookkeeping... after it does a *lot* of other stuff.
I use reflex and ghcjs all day at work and love it! Being able to grab random packages from Hackage or try the latest language extension is a constant source of joy. Sharing types between the frontend and backend is really great. Sharing _state_ itself is an interesting research area :) I have a library called [servant-reflex](https://github.com/imalsogreg/servant-reflex) that gives you FRP functions for querying your servant-defined API. Someone managed to compile [pandoc](http://pandoc.org/) with ghcjs to get an in-browser markdown previewing tool, [markup.rocks](http://markup.rocks). It's all a lot of fun, and the rough edges (tooling, prototyping speed, js code size) are getting smoother. Hoping that all the projects you mentioned continue to cross-pollinate.
If you use 'cabal new-build', all of your locally compiled packages get stored in a centralized .cabal/store and can be reused across working directories. This is stable even if you change flags/versions/etc, because like Nix, each compiled product is identified by a hash that is supposed to capture every possible input. In principle, these compile products should be transferable across machines but no infrastructure is in place for doing this.
Fantastic post. Loved hearing about the breadth of things happening in the ecosystem, which I don't really keep up with in detail week by week! And for some reason I hadn't noticed that `MonadFail` had landed in 8.0! &gt; There’s some truth to most of these. There’s also a answer or workaround to all of them except documentation. I'm not sure that's _quite_ fair, is it? It is completely true that people perceive each of these as problems. Further I imagine that in the cases of strings and records (at least), having to rely on workarounds _is_ the issue. Changing tack, I hadn't come across [ImplicitCAD](http://www.implicitcad.org/) before. While I was an undergrad doing mech eng subjects, some sort of programmatic replacement for Solidworks would have been of extreme interest to me! I'll be interested to see if using something like this matches the convenience of a UI (for power users, that is). Depending on the complexity of the task, I found Solidworks's UI almost incapable of keeping up (though operator inexperience could also be blamed, if we're being fair). ImplicitCAD obviously has benefits in terms of being able to reuse functionality, as well as benefits to having everything in a textual format where the created object is 'immutable' in a sense, and can be more conveniently version controlled. But whether there is a benefit related to managing complexity in designs isn't as obvious to me.
AFAIK, this is exactly the reason `serveWithContext` was introduced. Allowing one to pass in connection pools to custom combinators. In your case, potentially a custom authentication combinator with a connection pool retrievable from the context. https://hackage.haskell.org/package/servant-server-0.9.1.1/docs/Servant-Server.html#v:serveWithContext
&gt; This would effectively allow a ‘zero-cost abstraction’ (with full type safety) use of the Maybe monad when fully optimized. In my understanding, that would only happen if the strictness of `Maybe` was altered, right?
Isn't it (more or less) what https://wiki.haskell.org/Threepenny-gui is doing?
Char8 intentionally confuses ByteString (which is packed `[Word8]`) with `[Char]`. This is the world's leading source (in every language) of encoding errors and improper text handling. Char8 does not even document the encoding properties (hence this question) and libraries get tricked into using it for text handling because the types work out. It even provides an instance of IsString (when ByteString clearly *not* is string) that (if you have OverloadedStrings especially) causes silent corruption of data.
This is a great summary of what happened in Haskell 2016. I like this format a bit more than the Haskell Communities and Activities Report which is also great. &gt; The innovation of Stack can be put quite simply as making all build commands idempotent and sandboxes as stateless, which was a vast improvement over stateful sandboxes which would quickly become corrupted or inconsistent and needed to be rebuilt constantly. &gt; Stack has since grown a rich set of integrations with tooling such as Nix, GHCjs, TravisCI, CircleCI, Docker, Kubernetes as well as broad set of templates for quickly starting Haskell projects. &gt; The alternative cabal new-build build system is under active development. The new build system uses Nix-style local builds which makes a distinction between local packages and external packages. Local packages, which users edit and recompile and must be built per-project whereas external packages are cached across projects and retrieved from Hackage. To me this sounds like Stack already does everything "cabal new-build" is supposed to provide and more. Can somebody ELI5 what "cabal new-build" offers or will offer over Stack? Also, when will "cabal new-build" be ready? &gt; Javascript unfortunately continues to exist. haha ... seriously though, what's the alternative?
Thank you :) English is my second language, and I don't have a lot of experience writing. I'm glad you liked the tutorial! 
Stellar post! Typo police report (**bold** is superfluous or incorrect, *italic* is lacking): * "Enable -XStrict will automatically performs the equivalent of adding strictness annotations to **the** every argument source" * "It simply **makes a slightly alters** the behavior" * "Development on HalVM out of Galois picked up with some public statements on *the* project." * " The Foundation *is a* bit grander in scope and aims " * " While I remain skeptical of the entire transpiring phenomenon" - Transpiring -&gt; Transpi**l**ing? * "Backpack is Haskell’s partial answer to *the* “module problem”," * "An implementation of this signature then imports the*n* library, and **can then be implement the interface** downstream by using a mixin." * "Earlier this year I commented that compromises in type-safe database libraries consisted of the following **traedoffs**:" * "Doing this in a safe **manor** " - manner. * "by which a table object can load the information_schema and check the **constancy** of the query before evaluation" - Consistency, probably? * "and it would **be** follow naturally to lift this into a type-safe quasiquoter"
A nice summary. I would also add Don's talk to your list of production Haskell talks at the start: https://skillsmatter.com/skillscasts/9098-haskell-in-the-large-the-day-to-day-practice-of-using-haskell-to-write-large-systems
Hehe, I remember Jurriaan Hage mentioning his homebrew anti-plagiarism tool and how it outperformed many of the commercial tools. Great to see it worked out as a Haskell program! [Slides](http://www.ou.nl/documents/987531/d51b6061-36aa-4ef3-a0b3-55dc1d238f7c)
Sure. Let's say in my previous example I want to read from 1000 files instead of just one and write the output in order to a single output file. Well, it still works fine with lazy IO in terms of the data I will read and write. But if I don't want 1000 file handles open at once, then ordering of the IO operations now matters for more than just the data. I must read each file to the end and close its handle before I open the next. So lazy IO is no longer appropriate. There are tricks to get this case right using lazy IO. But it gets complicated and non-intuitive very quickly. That is not surprising, because this kind of ordering does not fit in with the conceptual model of lazy IO. In my opinion, as soon as there is even a tiny bit of IO order dependency, as in this example, stay away from lazy IO. A simple rule is that if you read from more than one data source, don't use lazy IO.
`print x` returns an IO action which prints the value `"x"`. The value is a value, like the mathematical value 42 - it's one thing. It doesn't make sense to talk about mutating it or not mutating it. Internally, the compiler may mutate memory locations to compute x, may compute `x` multiple times in multiple memory locations, etc. Or not. We don't care, because the meaning of what we write only refers to the denotational semantics, not the operational semantics. Of course, sometimes we really do care about operational semantics. Because we need direct control over some aspect of how our program interacts with the world. Or because we need to optimize performance above the usually-quite-good default we get from the compiler. So then we need to think about operational semantics of the IO actions we created denotationally. In that situation, lazy IO is out.
You can check the https://github.com/fpco/stackage commit history. It appears there was some sort of test suite failure in wreq.
Thank you.
That ticket just refers to interface files. The one mentioned as still open refers to object code.
Thank you.
Are these rewrite rules generic? Could they be incorporated into the compiler? 
&gt; print x returns an IO action which prints the value "x". The value is a value, like the mathematical value 42 - it's one thing. It doesn't make sense to talk about mutating it or not mutating it. Well, in this example 'print x' mutates the value; if you omit the `print x`, `y` is bottom. If you add it, `y` is contents of the file. Obviously, `print` mutates `x`. y &lt;- withFile "foo" $ \f -&gt; do x &lt;- hGetContent f print x return x &gt; We don't care, because the meaning of what we write only refers to the denotational semantics, not the operational semantics. Right, so what is the meaning of `x1` and `x2` in this example: (h,_) &lt;- accept sock x1 &lt;- getContents h x2 &lt;- getContents h return $ somePureFunction x1 x2 Is it correct to say, that the meaning of the whole program is dependent on operational semantics - and therefore undefined? 
This fixes the problem: https://github.com/bos/wreq/pull/94 However, the author doesn't seem to respond on the github issues. Just wondering if somebody (I) should ask to get the maintenance of the package, as I have a few packages that depend on this (there doesn't seem to be a reasonable alternative, yet)
This is the issue on wreq from July: https://github.com/bos/wreq/issues/83 And some PR discussions. Nothing merged yet. https://github.com/bos/wreq/pull/89 https://github.com/bos/wreq/pull/94
That's my impression as well. I know there were experiments on Windows, but I don't think the current status is anywhere near usable. So yes, there's that consideration as well.
I've used it several times, it's quite nice
The problem with Stack is that it has no dependency solver. It relies heavily upon central curation, rendering the version bounds specified in Cabal files largely moot. Cabal new-build will let you build using the Cabal file information for package dependency versions for the package. Ideally this will give you more flexibility to use whatever package versions you want rather than the ones specified in Stackage. Yes, you can work this way with Stack, but cabal new-build hopefully will make it easier.
Suppose `type App = ReaderT Config IO`. You can write a function: strip :: App a -&gt; App (IO a) strip action = runReaderT action &lt;$&gt; ask which would let you write your authentication function in `App`, and then strip it out in your main function. 
Not idempotent, unless you weaken isomorphism to something more general (e.g. by dropping the restriction that the functions must be the inverse of each other).
Oh, you're right. `Either a a` caries an extra bit of information.
Thanks, that is pretty clear.
Certain classes of algebraic data types corresponds to lawless algebraic structures in two intimately related ways, depending on the details of the type: If the algebraic data type is a functor `f`, it corresponds to the algebraic structure consisting of an algebra `f a -&gt; a`. We can decompose this algebra to note that each constructor of the algebraic data type corresponds to a different operation. For example, the algebraic data type `Maybe` corresponds to an algebra `Maybe a -&gt; a`, which can be decomposed into two operations, `() -&gt; a` and `a -&gt; a`. This algebra is closely related to the natural numbers. If the algebraic data type is a recursive data type that can be described as the fixed point `µf` of a functor `f`, where `f` is also an algebraic data type, then `µf` corresponds to the initial algebra of the algebraic structure for `f` that I described in the previous paragraph. 'Initial algebra' basically means that you can recursively decompose `µf` using an arbitrary `f`-algebra. An example of this would be the unary natural numbers, i.e.: data Unary = Zero | Successor Unary These are the fixed point of the algebraic structure for `Maybe`, and so form an initial `Maybe`-algebra.
It's not the function definition which should depend only on its input parameters, but the result of the function should be fully determined by the values of the arguments. In this case `subst` is pure, because `substTable` is constant. See Wikipedia for a more precise definition: https://en.wikipedia.org/wiki/Pure_function
There is one. PDF has it after the front-matter. HTML version has a link to index on top of page, gets you to https://www.haskell.org/communities/11-2016/html/index.html
Great talk I appreciate it, I firstly came across what A stands for in ADT in this [lecture](https://www.youtube.com/watch?v=w1WMykh7AxA&amp;list=PLbgaMIhjbmEnaH_LTkxLI7FMa2HsnawM_&amp;index=10&amp;t=24m20s).
It's mostly the same situation as in Nix: it's always safe to delete the cache because new-build will know to rebuild everything in this case, and if you knew what packages were live, you could GC the cache, retaining only the packages that were reachable (although, this code is not written yet).
Thank you, I missed that link. I'd vote to have it inline in the HTML version, like the PDF.
See [this](https://github.com/ghc-proposals/ghc-proposals/pull/30).
You could easily inline `substTable`, without changing the definition. That's what ghc often does.
Generally on Linux, you can Uninstall software through the terminal. Sudo apt purge haskell
How much do you know about algebraic structures? A more specific question could allow more interesting answers. 
I'll keep in mind for when I work on the new pipeline.
Your compress function type is off. It doesn't actually return type either. Change it to return Right y or Left (length ys, head ys).
The talks by Milewski are great, and so are the blog posts.
Cabal new-build is exactly like old cabal, but you will only have to download, compile, and store a package once. Stack has sets of frozen packages that you can choose one, and everything inside it will work well together. Or, in other words, besides managing packages, there isn't much similarity.
They'd be much better in some sort of pre-pass before it spits out javascript, but yes. Right now we use them as a sort of peephole pass to reduce output code size. We also have to convert stuff to es6 modules. We've been working on getting at least the latter stuff incorporated.
Unclear. Definitely not for the upcoming Cabal 2.0 release, I can say for sure!
one bit, ha! Either a a ~ (Bool, a) 
Oh. Yeah, that's obvious once you told me... I somehow expected it would figure out by itself which of the "either" to slot the output into.
it's partial. pack :: [Char] -&gt; [Byte] has to crash, or drop chars, or change them, or something to shrink the bigger type into the smaller one.
no mention of [Haskell Programming: From First Principles](http://haskellbook.com/) :/ 
Thanks. I didn't know this at all.
It'd not a bug because it functions as designed. However, IMO the design of Char8 is misguided.
I thought you could already use the DOM with WebAssembly? You can instantiate a module with JS functions filling the imported functions
Oh yes, sorry. It's cabal freezing that makes builds reproducible. `new-build` just means that those reproducible builds are more useful because the results are cached, and parts may be shared when sections of the dependency graph are unchanged. It's also important for reproducibility because it means you can't get "cabal hell," where two dependencies are built with differing versions of a common dependency.
The existence of a better alternative is not a requirement for legitimate complaints.
To add on to this, here are some more examples of monoids: * Lists! The identity element is the empty list, and the binary operator is `(++)`. So for any type `a`, the type `[a]` is a monoid. * Booleans, with `or` as the binary operator and `True` as the identity element * Booleans, with `and` as the binary operator and `False` as the identity element * For some type `a` which is a semigroup, the type `Maybe a` is a monoid, with `Nothing` as the identity element. That is, we take a type which has some associative binary operator, and add an identity element to it to make a monoid.
To explain that last one: the Haskell code for it would look like this instance Semigroup a =&gt; Semigroup (Maybe a) where (Just a) &lt;&gt; (Just b) = Just $ a &lt;&gt; b (Just a) &lt;&gt; Nothing = Just a Nothing &lt;&gt; (Just a) = Just a instance (Semigroup a) =&gt; Monoid (Maybe a) where mempty = Nothing
&gt; Everyone agrees Prelude sucks. I don't think the prelude sucks. It's not perfect, but it's far from sucky in my opinion.
Someone should definitely make a Prelude with no IO and call it like that.
To what extent does/can backpack fix the string problem?
It might be something I last did before sharing was implemented. The issue remains though, even implementing it in principle is much harder with cabal new-build's approach since there is just a random collection of packages with no indication which ones are in use or more recent.
Backpack doesn't just "fix the string problem." It gives us a long migration plan for it. We can make a version of `base` that is parameterized by a string signature. Then we make `base` be a module that instantiates that version with `String = [Char]`. Once everyone has changed their packages to depend on a parameterized `base`, we can switch `base` and all those modules to a different string type freely.
PureScript does not have let-generalization (ala http://research.microsoft.com/en-us/um/people/simonpj/papers/constraints/let-gen.pdf), which is something I've found somewhat irritating (natural transformations are very common in our codebase). If something can be _checked_ as rank-n in lambda position, why not if you just happen to put it in a let? I had never considered the monomorphic case (due to _inference_) presented in in this blog post. So hey, TIL. I may have to reconsider my opinion.
Very excited about unboxed sum types. If I understand it correctly it has massive potential to cut the cost of monad transformer stacks as it is taken fufrtger in future GHC releases.
If it's sensible, then sure. But not all partial functions (even just those on lists) can be easily or sensibly expressed by a simple pattern match. E.g. what if I want to use `last` or `(!! 100)`? In the first case I would be basically reimplementing the function `last`. In the second case I would fill up the code with 99 pattern matches I don't care about.
Start by adding lints that are on by default. 
&gt; It's nice to use partial functions sometimes too - e.g. if you already know that the list isn't empty (maybe you pattern matched on it earlier) Sounds like a good use case for [`nonEmpty :: [a] -&gt; Maybe (NonEmpty a)`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List-NonEmpty.html#v:nonEmpty)!
Thanks for reminding me that the haskell-issues repo exists!
So it just means if you have a container which can fold over and it's filled with monoid values, you can just add up all the values in the container and extract the result. If you are familiar with a binary tree, imagine that you start at the leaves and for each node just add the two children giving you two leaves. You keep adding the branches until the whole tree is eaten up. The importance of the monoid property is mempty covers the case of an empty tree and &lt;&gt; must be associative so the order in which you add up the branches doesn't matter.
There are also cases like `map head . group` which is always safe. This can be codified by using [`Data.List.NonEmpty.group`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List-NonEmpty.html#v:group) group :: (Foldable f, Eq a) =&gt; f a -&gt; [NonEmpty a]
Ask and you shall receive: https://wiki.haskell.org/Typeclassopedia
https://github.com/mrkkrp/req/pull/4
Right... which to me seems to satisfy "a + a" just fine. As one extra bit is the same as multiplying by 2. 
Among other things, the downside of this plan of course is that the asymptotics for virtually everything you write will go all over the place depending on the choice of string.
&gt; "partial function suck : yes", but what is the alternative ? Maybe everywhere, NonEmpty everywhere ? That's not always a solution. Sure it is. I noticed that I almost always want `Maybe` when using partial functions like `head` so there's no reason why that couldn't be the default. Want to live dangerously? Write `fromMaybe (error "this wasn't supposed to happen. i guess i should've used Maybe after all!") $ head list` Or just something like `unsafeHead`. But partial functions should *not* be the default because they are rarely what you want.
This, a thousand times this. As an "advanced beginner", I've wished many times that the packages I've dealt with had documentation much like the docs /u/Tekmo writes. I think a Tutorial module is a very good idea, and it's something I plan on doing when publishing my own libraries on hackage in the future.
Just because you had a bad experience with documentation once or twice that doesn't make documentation useless or unnecessary. Even if some people will understand your library just by looking at the api it does not mean that documentation wouldn't accelerate their learning time a lot and make them understand it a lot better. I have made a choice of not using libraries that don't have a minimal amount of documentation. If you don't care about the people that will want to use your library, you don't have to write docs. If you do, consider investing the time and effort to write and keep docs up to date. Your users will surely appreciate it.
&gt; Booleans, with or as the binary operator and True as the identity element &gt; &gt; Booleans, with and as the binary operator and False as the identity element If I use the above intuitions on numerics, such as &lt;&gt;=* with id=1 (x*1=x), or &lt;&gt;=+ with id=0 (x+0=x), I can't quite relate or-with-True / and-with-False: x|True is always True, not always x --- y&amp;False is always False, not always y.. how to reconcile from an "intuitional" approach? **Now** swapping the ids works however: x|False=x and y&amp;True=y --- did you get them mixed up or am I missing something more substantial here?
Oh, sorry, it should be Poorlude or course 😃
&gt; Just because you had a bad experience with documentation once or twice that doesn't make documentation useless or unnecessary. I think I stated very clearly that I agree with this and was not trying to suggest otherwise.
&gt; if I have a value of type [a] that I know to always be non-empty then I pass around a `NonEmpty`.
but returning `Maybe` is useful because you can set a default with `fromMaybe`, or use it as a monad.
&gt; &gt; However, if you still want to fuzz test warp's input parsing code, I would suggest that you hack warp's source code to produce program that reads the HTTP requests from stdin like AFL expects. Not sure how easy it is with `afl` + `qemu` solution, but it is often possible to mock the socket functions of libc with `LD_PRELOAD`.
Libdislocator is a replacement malloc that only makes allocations up against a page boundary and doesn't provide a subsequent page - see https://github.com/mirrorer/afl/blob/master/libdislocator/README.dislocator. Even a 1 byte read overflow will segfault. I suspect that's mostly equivalent to address sanitiser - but I'll certainly try adding the flag and let you know. 
I mean if I need to compute a result over the whole tree I'm usually producing it as some kind of catamorphism or fold-like walk. Instead of, say, trying to annotate expressions with their types, or source locations or what have you, I tend to rather produce a well-typed translation into a witness in a strongly, explicitly typed core language. Successfully emitting core is the proof I got it right and contains any things I'd be referencing anyways. I may well have some sort of environment for typing that I'm carrying around as I'm working, but then I turn to things like `bound` to ensure that the environment must contain all the local stuff in, say, a binding group that i'm working on.
Address sanitizer also works for stack allocations which I don't think libdislocator does.
You might very well only know that it is non-empty in the particular case alternative you are in (e.g. because of pattern matching or guards), not globally.
I didn't mean to pick on that library either, but the issue is that quality of documentation has probably more to do with "ripeness" of the library than with the choice of language (Haskell). An unripe library does not make a good case for the overall quality of documenation in the Haskell ecosystem. (Of course, one can wonder about the overall maturity of Haskell libraries, but that's not quite the same as wondering about documentation quality.) 
Wow, just wanted to say that I appreciate such a well-put article. Many, many useful things I've missed from 2016 it seems. Also, will try out &gt; stack new protolude Looks like a very sensible thing to try for toy projects without all the standard Prelude's pain-baggage.
What about Frege with GWT? :P
I mostly agree, but that's not "THAT" simple. For example, head can be easily replaced by a pattern match, but that does not solve anything, and the total `head` function which returns a `Maybe` introduce a lot of boilerplate when you know that your list is not empty. On the other hand, how are we supposed to handle the `div` function ? Should `div` be `:: Integral a =&gt; a -&gt; a -&gt; Maybe a` to make it total ? Should we write code like that: case safeHead l of Just x -&gt; doSomethingWith x Nothing -&gt; error "WTF, this is not supposed to happen" instead of doSomethingWith (unsafeHead l) -- I know what I'm doing.
Monoid is any type that is appendable so that two elements can be combined and create a third of the same type. Don't care about the *"Oh no, because blah blah blah..."* they are trying to mess you and make your learning curve difficult. Because they spent an inordinate amount of time around this elementary concept, they think that you must spend so much time in this shit too. In addition they can not live without the idea that there are kabbalist words and concepts that only they can understand. 
No, I was just half-asleep when I posted, I think - my mistake!
Looks great! I wish all the functionality will be merged into stack. 
I wish there are books that teach people the "modern" practices. It sucks to have all these tutorials teaching beginners the prelude. 
Elixir does docs the right way, they use docstrings as a way to test their functions. It's quite amazing. Haskell should really follow their example.
You can do this using the doctest library in Haskell 
This is probably the most important thing for the community that happened, yes. 
&gt; if you have two buffers that be default end up next to each other in memory Please excuse my ignorance, but what's broken about this? What's the bug as long as there's no overlap somewhere?
Please see my edit. 
I'd also like to point out that if an API involves a lot of type level programming the type signatures are often stunningly unhelpful. They don't communicate intent and can be incredibly hard to understand what values fit.
Too be fair, the setup for doctesting is kind of an annoying ritual in Haskell.
The name would suggest that this is for sparse matrices. Kind of the opposite of what repa does.
Someone asked me to debug `PHP` once (that's my story, and I'm sticking to it) and one thing I've always liked about their documentation system is the comments at the end. Some corners of their API have collected some really useful notes in the comments. 
I think quality documentation should be considered necessary, and if that means fewer new features in order to spend some time on them, it's OK. There's more time in the future to write new stuff. It's rare for people to go back in time and flesh out the docs.
Never mind. I have found the fix. Just add this line under `dotspacemacs/user-config` in the `.spacemacs` file: (setq haskell-process-args-stack-ghci '("--ghci-options=-ferror-spans"))
Pretty much something that has really bothered me since I started learning and using Haskell. I fully understand and subscribe to the idea that you can force people to write code in a particular manner in order to totally incinerate a bunch of bugs. The idea that you can force people to *learn* in a particular way is just wrong, though.
But it's not hard to cover the partial functions in Prelude. `listToMaybe`, `NonEmpty` and `maybeRead` already cover most of it. Truthfully in practice I almost never use partial functions these days, so I'm not sure why everyone is so afraid of it. Well - except for partial numeric things, like division and floating point things. For those, it's really not so clear what the right approach is.
It would be nice to have Rank-2 type inference along with our Rank-N type checking. :) It wouldn't cover everything, but it would cover the `\f -&gt; (f 0, f '\0')` case.
&gt; For example, we can write a function which concatenates an array of values in some monoid by using a fold. The wording here is a bit confusing and misleading. It's perhaps clearer as something like &gt; For example, we can write a function which concatenates an array of monoidal values (i.e. values of some type that implements the monoid typeclass) by using a fold.
This is actually what I'd like to see as a solution. I.e. prefer converting `[a]` to `NonEmpty a` instead of converting `a` to `Maybe a` as a way to get totality.
Other people have already given good answers, but here's the way _I_ think about it: A monoid is anything with a natural default and a natural combination operation. Sometimes there's more than one way to view things as a monoid, so you get things like `Product` and `Sum` `newtype`s without a `Monoid` instance for numbers. Sometimes the author chooses to give a `Monoid` instance for what they think is the most common case (I'm thinking of [`diagrams`](http://projects.haskell.org/diagrams/), where IIRC, the `Monoid` instance overlays two diagrams, but vertical and horizontal juxtaposition could also be monoidal (although there's two variants of each)). Food for thought (and future reading): https://www.reddit.com/r/haskell/comments/1ka4q6/a_great_series_of_posts_on_the_power_of_monoids/ This is an /r/haskell post that discusses a great series of blog posts that describes how to use monoids to solve a difficult problem.
&gt; It's rare for people to go back in time and flesh out the docs. On the other hand it is not that rare to find a repository with documentation but no usable code, which is not exactly what we want either. I think any extreme focus on a single aspect will be bad, there needs to be a balance between code, tests, documentation,...
&gt; I think quality documentation should be considered necessary This sounds great and is a wonderful goal, but isn't very realistic. Not everyone has the same priorities as you. Not everyone has the time to put into that kind of documentation. Sometimes people just write code for themselves to solve their problem, and that's it. Then they throw their code up on github or hackage, and figure they've contributed to the world by doing just that. And you know what, they have! They've just contributed in a way that takes some effort to figure out. Newcomers to Haskell need to understand this. If someone gave you a chocolate pudding for Christmas, you don't complain to them because they didn't also give you a spoon along with it.
You *still* keep mixing up the static denotational semantics of Haskell programs with the operational semantics of effects of IO actions. Remember, do notation is only syntactic syntax for applying a function. We talk about "binding a value to x", but in fact, x is a parameter to a function. Like any other function, the parameters can have different values when the function is applied in different contexts. The value of a function parameter does *not* only depend on the expression itself. That true of every function, it is completely valid for a pure language, and it has nothing to do with mutation. There is nothing different happening in the context of lazy IO. The fact that a Haskell program never depends on the environment is not the same thing as "produces the same output". Output is produced by running an IO action, and it depends on the environment and on the operational semantics of the particular compiler you are using. But those things are not part of the definition of Haskell; the meaning of a Haskell program is its denotational semantics, and that is defined completely by the program itself, not by the environment, and not by any operational semantics.
The `append` functions is implemented differently (see https://github.com/haskell/bytestring/blob/3c97952002593ee3b3d7cc00a9ae32fb12fa8a55/Data/ByteString/Internal.hs#L454-L461 ) and the builder execution looks much more complicated (https://github.com/haskell/bytestring/blob/8904ca83faa668e8fc59224f79fd3e37fc4ddec2/Data/ByteString/Builder/Internal.hs#L1107-L1153) and seems to differ from the simple 'malloc' + 'realloc' to grow strategy. As I mentioned above, the primary goal was not speed but to reduce memory usage. The `Builder` approach (especially with lazy bytestrings as inputs) was too lazy for my use case.
It's *really* important that there's a difference between a "default" and an "identity." It's crucial that `mempty` be an identity element.
I think it is a realistic goal to require documentation for packages uploaded to Hackage. My view is that if a person does not have time to document their package then they don't have time to maintain it
Yeah, I definitely don't disagree with that sentiment. But that would be a pretty big course adjustment. If it happened, there would still be value in having a hackage without that restriction. Some might argue that's what github is for. But github doesn't collect all the Haskell packages in one place very effectively. Having all the undocumented Haskell code in one place is very useful to the more experienced haskellers who are willing to dive into haddocks to figure out how to use a library.
Great addition, but they need to be more obvious. I've accidentally scrolled past those many times. Personally, I think haddock needs a CSS overhaul. 
Maybe not maintain, but who knows, someone more willing might take ownership!
Most of the time (like almost always) you'll want to handle the empty list case anyway. `Maybe` gives you a *nice* interface (`Functor`, `Applicative`, `Alternative`, `Monad`, etc) to both encode the invariant of the an empty result and not handle it everywhere. Consider this code that takes the 3rd element of a list and inserts it instead of the first element (given that `index` and `tail` are total and return `Maybe a`): foo :: [a] -&gt; Maybe [a] foo list = (:) &lt;$&gt; index 3 list &lt;*&gt; tail list How would you write this code if `index` and `tail` were not total? Also, if you think you know what you are doing, you can use `unsafeHead`. That way: - Beginners will know that they are doing something that is `unsafe` - Your coworkers will have every right to blame you for using `unsafe` functions when your code breaks in production (or will have an easier time to spot them during code review) Regrading your comment on `div`, it is an odd case that I don't have a good answer to. I don't think Maybe is the answer, a runtime exception probably does make sense there when lacking a more sophisticated type system. However that is also the expected behaviour of almost any other language I know of and therefore does not make everything else I wrote invalid. btw, [PureScript handles div in a different way](https://pursuit.purescript.org/packages/purescript-prelude/2.1.0/docs/Data.EuclideanRing#v:div). tl;dr: The are a lot of reasons why total functions should be **the default**.
Both, although the names may differ (I think in this case they have the same names though). I'm somewhat familiar with PS but my explanation comes from my knowledge of Haskell.
Well, for one thing, I don't have any other projects where it's working to copy it from...
&gt; Turtle and Shake are probably the best examples to follow, and they are very popular among my Haskell newbie coworkers because of it. I think Edward Kmett said in a talk "I wrote lenses, then I discovered that if you write documentation people actually use your libraries." People *want* to use useful code by default. I think sometimes people forget how much it's in your self-interest to have good documentation. 
Linux and Darwin. People have done proof of concept windows ports but no one seems to have the staying power to support it for the long haul.
I think this relates to monad tutorial fallacy: monad tutorials try to tell 'what is a monad?' when the true question is 'what is a monad good for?'.
&gt; Like any other function, the parameters can have different values when the function is applied in different contexts. I have no idea what you mean here. The `x` as a parameter of a function is an expression. As `1 + 1` is an expresion or `x * x` is an expression. It is a closed expression. Therefore, according to Eliott it does not depend on anything else. Except in case of lazy IO it does. &gt; Remember, do notation is only syntactic syntax for applying a function. We talk about "binding a value to x", but in fact, x is a parameter to a function. Like any other function, the parameters can have different values when the function is applied in different contexts. I have no idea what changes here: getContents h &gt;&gt;= \x -&gt; getSomeInput &gt;&gt;= \y -&gt; write_to_file h y &gt;&gt; print x &gt;&gt; return x The value of `x` should not depend on anything in the environment; except it does depend on the input. &gt; the meaning of a Haskell program is its denotational semantics, and that is defined completely by the program itself, not by the environment, and not by any operational semantics. Except that e.g. this program derives it's meaning from operational semantics, so it is a direct counter-example to your claim that meaning of a program containing lazy io is its denotation semantics: (h,_) &lt;- accept sock c1 &lt;- hGetContent h c2 &lt;- hGetContent h print $ c1 !! 50000 == c2 !! 0 
Does anyone have experience using Protolude for big projects? How did it work out? It looks very tastefully done, and since it's not trying to make big changes I'm considering using it until something like Foundation becomes more developed.
The author of the quote has already clarified their meaning, but I think that it's worth bringing up one place where I do think that partiality buys us something: division. At the moment, we can easily write: microsecondsToSeconds :: Int -&gt; Int -&gt; Int microsecondsToSeconds us = div us 1000000 In this instance, using `div` is safe because the second argument is statically known. If `div` were total, and if we restricted ourselves to only using other total functions from the prelude as well, then it would not be possible to write this function. For things like `head`, `last`, `foldl1`, etc., I don't feel the same way. It's unfortunate that those are all in `base`.
Not that I can think of.
Better example: f x = do y &lt;- someInput writeFile "x" y print x c &lt;- getConent "x" f c As you can see, the value of `x` in the `f` depends on input. Yet, `x` isn't supposed to depend on anything, in the `f` function `x` is not a free variable.
Can't say I agree ... by that argument, what was the point of repa in the first place? blas/lapack is powerful and it's the right tool for a lot of things but it shouldn't be the one and only solution in existence. Some use cases not great fits for blas/lapack bindings are large-scale / parallel computations and the client-side web app computations previously mentioned. 
I fuzzed it with ASAN. I found another bug - https://github.com/ndmitchell/hexml/commit/ff6706580494e46164b8a833b077876b9d79a4f0. I've switched my fuzzing script to use that by default - nice catch. In this case it was a read of a global static variable that isn't allocated by malloc, so libdislocator can't do anything to find it.
Bingo! Yep ASAN is brilliant.
I should say it also makes the performance 4x worse than libdislocator, but that's fixable by just waiting longer.
Don't :)
The `[]` case was omitted for brevity. Probably should have mentioned that, but I thought that would have been obvious. I guess dividing it on cases _after_ calling `nonEmpty`on it is a good solution though.
The word "learn" seems like it's being overloaded here. There's a subtle distinction here between learning to code and learning to read type signatures. I think the distinction is analogous to the old "give a man a fish" vs "teach a man to fish" proverb. Experienced haskellers are trying to communicate to newcomers that there is huge value to be had in learning to read type signatures. How you learn to do that I can't say. But it's massively valuable. Yes, examples/tutorials/how-tos are important. Yes, Haskell needs to have more of them. Yes, having these things will make Haskell more approachable to newcomers. But newcomers still need to learn how to read type signatures. It's difficult and takes time, but it has huge value!
What would it infer f to be?
I think the comment was Gershom Bazerman's
The only "Oh no, ..." is that monoid needs an identity and the operation needs to be associative. Neither of those should be hard concepts or obscure language (both being taught in elementry-school level math). Also, so far as I'm aware, the only kabbalist words in the Haskell ecosystem are "yesod" and "keter"...
There's a broken link. This: * ~~https://github.com/hsyl20/ViperVM/blob/master/src/lib/ViperVM/Utils/Flow.hs~~ Should be this: * https://github.com/hsyl20/haskus-system/blob/master/src/lib/Haskus/Utils/Flow.hs Or use a stable link: * https://github.com/hsyl20/haskus-system/blob/77a1c0e382eaa61b81cd02ffd006d454c76103dc/src/lib/ViperVM/Utils/Flow.hs
I wonder if there's room for some free wins here by exposing (some?) tests as examples?
True, and perhaps I am remiss for not mentioning it. My excuse is that I, personally, think of the laws at a later stage; crucial, but not at the fore. I also find it difficult to imagine a "default" that is not an "identity" in the context of a "combination operation", but that may be the years of using `Monoid` talking.
&gt; The other major idea I wanted to communicate was that non-Haskell people seem to have a quite different world view regarding documentation Yeah. I have heard "Haskellers think type signatures are documentation" which is funny of course, but honestly the fact that you *can* figure out how to use `sum` from the `Data.Array.Accelerate` library from type signatures alone gives at least a partial vindication. 