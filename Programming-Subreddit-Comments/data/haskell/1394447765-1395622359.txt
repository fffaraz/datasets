I think that react maps really nicely to functional programming (e.g. an ocaml implementation: https://github.com/bobatkey/ocamlmvc), whichi is why I got all excited about Om (lack of types not withstanding). I did have a bit pf a poke about with fay to do something similar with Haskell, but I didn't really get very far with it. Perhaps I will have another go with Haste :-)
The main reason is that I find javascript gets in the way of expressing the ideas too much (but that's what we're doing at the moment).
Given that you have four operations (append, delete, copy, and paste) that each take a given amount of time (1, 1, 3, and 2 ticks respectively), find the shortest string length such that for every chain of operations that (1) yields a string of that length and (2) takes a minimal number of ticks, that chain contains a delete. Alternately, you're a lazy but curious typist. It takes you one key press to add or delete a character, three to copy everything, and two to paste. You want to know what's the shortest document length where the fastest way to type it requires you push delete.
Yes without a doubt. Especially when doing web applications. The combination of Haste and Scotty = super productivity.
I have been wanting to do that too, with accelerate, hmatrix and other backends. That's the reason why I started working on [accelerate-blas](http://github.com/alpmestan/accelerate-blas) (because while we're at writing linear algebra stuffs, why not choose the fastest road when we can). I don't think fpnla solves the problem in an ideal way either, but at least that makes for another project in that area, so that brings some more ideas. I think it becomes obvious that we should really resurrect the "haskell-math" task force. I may write a blog post about this today or in the upcoming days. Let's get this ball rolling altogether, so that we'll then be able to focus on *what we want to use the libraries for*, instead of "loosing time" comparing all the libraries we have etc.
&gt; This is why I'm working on a Haskell shell. I'm near to it being my full-time shell, once I've implemented one last thing. What's the last thing remaining? I'm really interested in the idea of a Haskell shell. I use zsh and really like it, so I don't feel an urgency to switch, but I'd really like to be able to write quick-and-dirty scripts in Haskell rather than shell.
Probably because people from languages like PHP are used to a web application consisting of a stack of about 5 different applications. Even though the Haskell way is safer and will perform much better, it's going to feel alien to those who've built up experience doing it the other way.
&gt;Why does including them in a webapp framework seem like a very strange practice? Because they are generally bolted on and have little to do with the rest of the framework. When the framework and everything in it is built on a request-&gt;response flow, websockets doesn't really integrate with anything else. If the web framework was written well, you shouldn't need web socket support in the framework to be able to reuse your application's domain model code. Websockets are basically saying "here's a random TCP handle, have fun". You can't assume the data traveling either direction is HTTP requests or responses, so the framework has basically no involvement anyways. It is like adding support for IRC to your web framework.
I'm waiting for this one too! But I'm not sure how likely it will be given that I haven't found any really good examples, even in Agda/Coq. They all just seem to be studs with a TODO in them.
&gt; Haskell only has functions of one argument. `a -&gt; b -&gt; c` is really `a -&gt; (b -&gt; c)`. That's not exactly true. The two are denotationally equivalent. And you are right that this adds a lot of expressiveness and flexibility to the language. But compilers do still tend to keep track of the arity of functions.
This was pretty awesome. I now have a SAT solver in 8 lines of code taken from this paper, and every time I try to work out how long it takes to solve a problem I feel like my mind is going to explode.
Did you pick a project yet? Mixing the two ideas already mentioned, you might make your PDF resume builder an online thing. You can run Pandoc on a server to go from lots of formats into PDF. And there's mozilla's pdf.js library, which you could either call from javascript, or bind from Fay/haste/ghcjs. I'm personally interested in seeing what can be done with PDFs on the web (like sharing PDF annotations and feedback online), because that would be a killer feature for my own web [project](http://reffit.com). That all sounds too ambitious for 11 weeks at 3-6 hours (and you would be spending valuable on building things outside the typical haskell build environment), but just throwing the idea out there. If you keep me posted I'm interested to hear how your learning goes. I talk mostly to people who already love Haskell, or people who would never touch it (I'm not in the CS field). So it'll be interesting to hear how it goes for you as a student.
I don't agree about using de Bruijn indices. I almost never do. IMO, they are slow and cumbersome.
Interesting! I, too, tried to build the selection monad transformer, but didn't find it very useful. Looking forward to reading the paper.
The version under github does support cabal sandboxes, I just haven't had time yet to release it. Rename and find references should work as long as your project builds. Check in the console output for your project if any errors are reported. 
I'm intentionally avoiding De Bruijn indices. Having tried to use them, they're causing much more hassle than they're worth for me. So I'm trying to find a nice stepping stone in the direction of a library for normal named binding. I don't need it to be fast, and certainly the way I've done it here is *vastly* less cumbersome than the De Bruijn versions I was twiddling with.
Slow because you've got to traverse all the way out to the leaves whenever you bind or reduce. I think this is exactly what [bound](http://hackage.haskell.org/package/bound) is trying to solve.
You seem to be confusing the idea of a 'webapp framework' with a 'HTTP-based app framework'. You don't *need* websocket support in your framework for your domain model code to be reused, but if you want websockets for your application to work over the same ports as your HTTP interactions, your HTTP server has to understand the upgrade process and how to hand off the resulting handle to whatever subsystem handles websockets. And if your server needs to handle the upgrade part anyway, and it simplifies things by having it handle more of the websocket interaction as well, I don't see the problem. Although HTTP itself is highly request/response oriented, arguably the whole point of a 'web app framework' is to provide a richer set of interactions by allowing one to create sessions that span multiple HTTP interactions. Once you do that, adding Websockets is a very natural extension, since they fit the extended session model even better than stateless request/response interactions do. This will make sense for some frameworks, but might not for others. There are good reasons to confine yourself to REST interactions at times, but also good reasons for an application to support a wider range of interactions. Since web browsers now support HTTP and Websockets, supporting both in an integrated manner on the server seems very reasonable to me.
Not that I know if `bound` is exactly fast.
I agree with you. For in many languages it is not even possible to write an editor/IDE. But the fact that the community can extend on its IDE in its favorite language should really be a plus. And I believe that it is ultimately possible to write a really cool editor/IDE in Haskell. &gt; Although Yi is written in Haskell, I'm not sure it's any better in its current state for supporting Haskell development than Emacs or vim with associated Haskell support packages. Again I agree. :) But cool stuff like SHM is written in ~4% Haskell and the rest elisp. Hacking on that particular tool requires one to know elisp, and that is not what I think is supporting "Haskell as nice as Emacs supports elisp".
You are making a lot of unfounded assumptions. &gt;but if you want websockets for your application to work over the same ports as your HTTP interactions There is no reason for that to be the case of course. &gt;your HTTP server has to understand the upgrade process and how to hand off the resulting handle to whatever subsystem handles websockets Which has nothing to do with the framework that sits above that HTTP server needing to be integrated with websockets. &gt;Although HTTP itself is highly request/response oriented, arguably the whole point of a 'web app framework' is to provide a richer set of interactions by allowing one to create sessions that span multiple HTTP interactions. The point of a web app framework is to provide useful abstractions for creating web applications. Websocket applications are not web applications, they are just ordinary network applications. Anything you can do with a standard TCP socket can be done with a websocket. Websockets just adding support for arbitrary network communication to browsers. On the server side, we've always had that capability. &gt;There are good reasons to confine yourself to REST interactions at times That has nothing to do with the subject of discussion. &gt;Since web browsers now support HTTP and Websockets, supporting both in an integrated manner on the server seems very reasonable to me. That is lovely. And I explained why some people feel it isn't reasonable (since websockets imply absolutely nothing about the application, unlike http). Why ask a question if you are going to argue that the answer is wrong because it isn't your opinion?
[Assuming this is what you're talking about?](http://hackage.haskell.org/package/linear-1.8.1/docs/Linear-V.html) It is, among other things, [affine](http://hackage.haskell.org/package/linear-1.8.1/docs/Linear-Affine.html#t:Affine) and [metric](http://hackage.haskell.org/package/linear-1.8.1/docs/Linear-Metric.html#t:Metric). For a full list, look in the first link.
Actually, `a -&gt; b -&gt; c` and `a -&gt; (b -&gt; c)` really are the same. The first is just the same shorthand for the second used for any right-associative operator: λ&gt; :t const :: a -&gt; (b -&gt; a) const :: a -&gt; (b -&gt; a) :: a -&gt; b -&gt; a It's true that the the compiler will track function arity at runtime.
It's asymptotically faster (`O(log n) -&gt; O(1)`) to my understanding... but I have no idea how big an expression tree you need before you start seeing it.
Take a look at the `Functor`, `Applicative` (and `Apply`), `Monad` (and `Bind`), `Traversable`, `Foldable`, and `Ixed` instances, for starters.
I haven't finalized it yet, but I have a meeting with my faculty advisor on Friday when I'll have to submit all the paperwork. I like the idea of a web-based PDF builder, since the web is the easier (IMO) interface to build - plus it's cross platform! So my proposal, overall, was to do something w/ Haskell or some automated analysis of time series data to find cyclic patterns. I'm leaning towards the cyclic patterns project because it's basically a feature for a side project (hopefully I can launch it this year and call it a startup). But, I did suggest implementing the cyclic pattern discovery stuff in Haskell in order to maximize my learning. I can keep you posted on what happens, thanks!
Er.. well, bound is a De Bruijn index based system that traverses on reduction, and partially traverses on binding (it forces traversal when you convert from a scope to an expression).
It's from 2010. Not that the ideas got bad in that time though.
Most of what you need to manipulate that data type is distributed across a bunch of typeclasses. That said, not everything you need for that particular usecase is there! Part of the pain you are experiencing is that type level arithmetic via type nats has just started to work, so talking about the type `V (n+1) a -&gt; V (n+1) (V n a)` only just now started to work in 7.8. You can used the `Ixed` instance to index into and manipulate it. It also supplies many of the standard classes from the haskell ecosystem along with `Representable`, which lets you `tabulate`/`index` to construct one from whole cloth or consume it. Some parts that would make it easier to use are definitely missing though. Partly, because comparisons between TypeNats were _also_ not working until recently, so we couldn't nicely talk about a `Fin` data type. All that said, we could swap the `Ixed` instance to be positional, rather than based on `E` so that `unit (ix 2)` would be able to make a unit vector. On the other hand, there are arguments against that approach.
It would be more funny if I were actually joking. I use ed a lot, and like it. I feel like I should post a confession bear.
The best thing about this is reading in the voice of a twilight zone narrator. I'm serious, try it!
I think you may have misunderstood my intent; I did not mean to make any assumptions about web apps. I am not trying to say that all web app frameworks *ought to* encompass websocket handling. I am only saying that it is reasonable for certain web app frameworks to do so. To suggest that websockets *should* be handled separately seems to impose an overly narrow definition of what a "web app framework" is. I have no problem with people creating frameworks that are opinionated about what a framework ought to encompass, but it seemed odd to me that someone might not even understand why another might want to take a wider view.
This job has come a year too early! I'm doing my PhD in FP and did my Master's in simulating Quantum Dots, I feel bred for this role ;) Best of luck to whomever applies!
I routinely write code in Haskell that I am not smart enough to write correctly in other languages. Given the productivity ratio of 'implemented and shipped' to 'not able to implement at all' is infinite, I feel it is safe to say that Haskell has made me infinitely more productive at certain types of code. &gt; With it I can do most of what Haskell can do, semantically - but I do not have to worry that much about the type system, nor dribbling the lack of IO with monads. Maybe 5% of my uses of monads are for IO. Add non-determinism to that code or switch to a callback style because you need to work with asynchronous requests and you have to restructure the whole codebase around adding that one demand... or you can just switch monads.
What's wrong with the notation I used? Seems straightforward enough re distinguishing types (`U`, `F U`, `Ui`,...) vs terms (`s`, `t`, `p`,...). The only place singleton types would come in is the fact that the `u :&lt;: v` types are all singleton (if `u` and `v` represent open sets) or empty (if either `u` or `v` does not represent an open set). Because of this fact, we might prefer to have `(:&lt;:)` be a type class or type family instead of being a type constructor— that way we don't need to explicitly pass them around. Though, doing so makes it much harder to explain what's going on, since the audience must do more type checking in their heads.
Well, it is a property of `res` in that the reason it follows is because `res` is cofunctorial. Also, because `res` is cofunctorial (i.e., contravariant), you actually have that: res (r1 . r2) = res r2 . res r1
while i agree with some of what you say, there are some intangibles. like, i really trust the authors of warp and wai and if they "bundle" something, it gives me a warm fuzzy feeling, and i presume they have made sure that things "fit" with their other work 
If you give more information about which repo you are talking about, what you did and the error I might be able to help you.
upset was the repl also the gui sometimes freezes up and doesn't not seem to respond the only way to exit is killing emacs
Thank you, I get it now! All because in this strange example, copying and pasting is *not* relative to the length of the current string, but constant.
the repl pops up but doesn't work
No, no it's not constant. The first copy copies all six characters. The first and second pastes each paste six characters, bringing the total to 18. The second copy copies all 18 characters. The third and fourth pastes each paste 18 characters, bringing the total to 54.
You do realize you can still apply for this job, right?
Yes, I realize that nothing is stopping me from applying. However, I have no intention of taking *any* job until I finish my PhD (expected September 2015). What I meant by "a year too soon" was that it was too soon *for me to take*. I feel it would be inconsiderate to apply to a job I can't actually start until 18 months from now. Is that naive of me? (honest question)
I may be able to whip something up quicker in coffeescript. But this is irrelevant, as anyone who has inherited a large dynamically typed application will attest it's incredibly hard to maintain. I am working on a large coffeescript application at the moment and it has changed from being rapidly developed to impossible to refactor because of its size. Also, I feel *far* more confident my haskell is *correct*. I am always suspicious of the dynamic code I write as you often don't know if you've made a basic mistake until runtime, which also slows down development. 
Nope! Never bite off more than you think you can chew.
Oh, the simplicity of holding a job as a full-time programmer (even in such a highly technical field), some days it is so very tempting to go back to that. Research is amazing fun, it's really too bad that it seems to be a rather minor part of one's time as a university professor... How I envy people at MS Research, INRIA, etc.
&gt; And Websockets are upgraded HTTP connections. That's just not true. Websockets use HTTP-looking handshakes to make them easier to co-locate with webservers, but after that the protocol is nothing like HTTP in any way.
If you decide to make do with low-order types, call me.
Even if they hire someone else right now, applying will at least put you on their radar for another role in the future (or perhaps this one, if they don't hire the contractor they go with).
I am beaming -- I remember a time when that article would have been a total whoosh to me. But when I read the intro today, the ListM solution came to me immediately. The generator solution exceeded my mental capacity in Reddit-browsing mode though. Haskell, it sinks in...eventually.
That's not necessarily true. When you have strong momentum behind you, derailment can be fatal.
Currently I use LiveScript, a CoffeeScript derivative with a lot of syntactic sugar borrowed from Haskell, as the main scripting language; it's great for Exploratory programming — i.e. when I have no idea where I'll end up with. For Serious™ programming, usually with a spec or test case showing exactly how the program fits with other pieces, Haskell becomes the most productive tool — the work is 80% done when I finish translating the spec to the type system.
What is a better designed language than Haskell today, honestly? There are some, yes, but those are not widely used yet... 
I know some of these words
I feel the same way. As soon as I saw the code: iterateR (something 2 0) &gt;&gt; iterateR (something 3 10) I thought, "What? Of course that will loop infinitely! Isn't that obvious?" I suppose I would not have thought it to be obvious back when I was first starting out as a Haskell programmer. But learning how the State monad transformer worked was the very first thing that I tackled when learning Haskell. I sat for hours alone in the cafe working through explanations and example code, trying to implement my own state transformer, manually type checking the definitions of (&gt;&gt;=), return, StateT, runStateT, to make sure I absolutely understood it inside and out. It was time well spent.
This isn't really a pitfall of lazy evaluation, it is just a misunderstanding of how the state monad is supposed to work. The act of passing a state to a pure function and returning it in a pair is abstracted away behind a nice and clean monadic interface so you don't have to worry about it. But if you use state transformers you should always keep it in the back of your mind that any function which modifies that state MUST be evaluated before any other function which also modifies that state. The updates to the state may occur lazily, but will always occur in the correct order. So if one loops infinitely, it may do so lazily but it will loop infinitely. If you expand the monads, i.e. replace every use of (&gt;&gt;=) and "return" with the definitions for State's instantiation of Monad, it becomes obvious why this is necessarily so.
[React](https://github.com/dbuenzli/react) is also an OCaml FRP library which has existed for some time. Some people use it with js_of_ocaml. I've been using my own FRP library though to good effect.
Likewise. Happy to see someone getting some mileage out of it as a transformer.
Come (back) to the dark side! =)
Or use [`randomRs`](http://hackage.haskell.org/package/random-1.0.1.1/docs/System-Random.html#v:randomRs).
I suspect the only reason you're being downvoted is that Oleg certainly knows why this happens, so it's probably inaccurate to say it's a misunderstanding of how State works. I think it's just a bad title. I don't see it as a pitfall of lazy evaluation, just a newbish kind of mistake thinking lazy evaluation is magical. 
I totally agree.
&gt; just a newbish kind of mistake The fact that the mistake is hard to spot and leads to a not easily identified bug makes me think it's unfair to minimize it like so. It reminds me of python's compare semantics, which are total and will compare strings to always be greater than ints; it's something that results in a constant stream of beginners with bugs in the python channel which is obvious once you understand it but impossible until you do.
Writing a pototype in perl or whatever, then get funding, and refactor in another language is perfectly reasonable in my book.
This was an interesting overview in general of introductory Haskell.
I think it is roughly similar to having TLS and non-TLS on the same server. Nobody in their right minds would send the same application data both over encrypted and non-encrypted channels, right? Similarly nobody in their righ minds would HTTP request/reply and Websockets for the same type of data, right?
SublimeHaskell with Sublime Text 3 works pretty well for me. 
But the tracking of arity is an implementation concern rather than something tied to the language itself. 
Yes, there a [blogpost](http://blog.ezyang.com/2010/10/ocaml-for-haskellers/) by Edward Z. Yang, which is mentioned in the "Haskell for OCaml programmers".
Jane Street is up there reading this ... somewhere. Yes, I'm looking at you.
&gt;I am only saying that it is reasonable for certain web app frameworks to do so. I know, and I am saying I disagree. There is no misunderstanding, there is a disagreement. Persistently trying to frame a disagreement as "you just don't understand" is rude and not productive. &gt;To suggest that websockets should be handled separately seems to impose an overly narrow definition of what a "web app framework" is It imposes a completely reasonable definition of what a web framework is. A framework for writing web apps. The web runs over HTTP. Again, a websocket is just a regular old socket with an initial http handshake. There is nothing web related about it on the server side. Having a framework for websocket apps (on the server side) is exactly the same as having a framework for TCP apps. Those don't exist for a reason, there is virtually nothing that you can generalize about such a wide range of applications.
It's not about the same type of data, no one is saying that. It's about the same kind of utilities. Nothing that my webframework gives me can be used in the websockets code anyway, because it's a totally different flow and environment. My app code can and should be shared, of course.
I guess slashdot had something going with its suggestion that I learn to RTFA. :P
I'm fascinated that at no point did the author ever say "IO Monad". I think that's actually a really nice choice. The concept of 'monads' kind of carries a lot of awkward "oh god this is so complicated" cultural baggage, and primes people to immediately throw their hands in the air and declare Haskell only fit for mathematicians. (... and to be fair, the entire purpose of the IO Monad isn't really monadic. It just happens to fulfil monad laws so it's a monad, as far as I know?) I do know that trying to teach Haskell to my bf has been difficult since he's kinda scared of monads and maintains that you can't do anything useful (everything has to IO eventually) without understanding monads. 'List of statements' it is. **Edit:** Alright, "monad" does pop up 80% of the way through.
I still have to do some research on if C#'s closures (implemented with clever compile-time code re-writing) are the same as closures in languages based on lambda calculus, or are functionally equivalent (if they are then is it a distinction without a difference?)
&gt; Edit: Alright, "monad" does pop up 80% of the way through. I really wanted to avoid "monad" entirely. But it would be unfair, because once you start wanting to make your own "lists of statements" you end up doing something that's 99% a monad already, and as a Haskell learner I would hate to discover I spent many hours to do 99% of something other people already got right before me with a well-known name. But I agree, you definitely need to starting teaching Haskell and its side-effects without using the word "monad" at all.
Thanks for making this, it's really really helpful. There's no attempt at justification of design goals of either language, which immediately cuts off most language-war gut reactions. That and it's almost a kind of quick reference; ocaml code on side and haskell code on the other. I'm pretty sure I could use this to write basic ocaml just as well as an ocamler could use it for haskell. Definitely gonna link it to my bf. :)
Source code repo link?
You say a "web app" is an app that runs over HTTP. I say a "web app" is an app that runs at least partially in a web browser. You say a "web framework" is for writing "HTTP-based applications" while I say that it's for writing applications that run at least partially in a web browser. You suggest that a "web framework" should only provide functionality that is completely general across all the functionality provided by the http protocol. I say that a "web framework" can also provide specific solutions for any problems that occur in supporting, on the server side, apps that run partially in a web browser. This could reasonably include some framework-specific protocols for doing things over Websockets as well as HTTP requests. In my personal lexicon, I would call what you have described as a "web app framework" a "http server library". I reserve "web app framework" for things that are opinionated about how the way that web browsers and servers communicate beyond the general specifications. Your insistence that your definitions are correct while mine are not is also a bit rude, but you seem set on them, and I have no further interest in trying to change your mind.
My comment was a reply to the Haskell shell author who has since deleted his comment for an unknown reason.
I have witnessed a beginner shouting himself in the foot with unsafePerformIO (used with FFI). He did learn about lazy evaluation though. 
This is actually exactly what I've been looking for - I've been looking into learning Haskell for a while, but a lot of stuff I've found tends to be aimed entirely at beginners (which, obviously, isn't what I want). I'd still love to be able to find a decent explanation of a Monad that doesn't include words I don't understand (like "functor", "monoid", etc) or the word "burrito", since obviously they're important in Haskell - the logo's a Monadic bind, for example. Edit: One criticism - "layout-based code structuring" isn't necessarily a good thing. I quite liked OCaml's choose-your-own-structure style, and it's definitely a preference - indentation as syntax isn't necessarily a good thing.
You will find that this article is quite thorough in its treatment of monads, even though the word itself is avoided.
Ok great!
Is there a copy of the talk or other media for those who aren't going to create an account just for this?
Thank you for the post, I loved it! I believe though that your passage on ST monad is rather misleading. &gt; State s a and ST s a: “recipes” that eventually provides a value of type a, but where each intermediate statement can modify an internal value of type s (a state). The difference between State and ST is that the former is more lazy, and the latter is strict. 's' in 'ST s a' is not a state type but a phantom one. ST is much closer to IO than it is to State, and the key difference between State and ST lays in mutability rather than in strictness (there is a strict version of State monad, after all). Given all that phantom machinery behind ST, I don't think that ST is worth mentioning in the post.
Honestly, I would have been skeptical of that code anyway. It's not really that hard to spot if you are experienced enough to realize that laziness doesn't mean "magically does what you mean instead of what you say."
I applied a couple of years ago. It was the only interview I've had where I thought I did the best I could and didn't make it past the phone screen, for what it's worth.
&gt; The engineer's life would be miserable without [unsafePerformIO]. I don't understand at all why this is the case. If you need to perform IO effects, you do it in the IO monad. If you really need a pure interface that secretly performs IO, that's quite a rare case for most engineers, afaict.
Sorry, yes, this is an upcoming talk on March 26. Perhaps a mod could add "upcoming talk" somewhere in the title? As for recordings, I'm not an organizer so I can't speak to that - this will be the first one I've attended and I just thought I'd spread the word. 
The problem is that lazy evaluation does not generalize well, but `ListM` generalizes to any base monad. Teaching beginners lazy evaluation as a solution to this problem is just setting them up for eventual disappointment.
http://ncatlab.org/nlab/show/HomePage
For some context, here's a paper that describes the [Julia approach](http://arxiv.org/abs/1209.5145) to typing. Seems the approach is strongly based on subtyping and doesn't use unification. I must admit I'm a bit taken aback by the comments by jonsterling, definitely comes off a bit arrogant and condescending even if that wasn't the intent. I think we can do better.
I tried very hard to keep the tone positive, I have no clue if I managed to do so though. Edit: that said, I have very very strong opinions about what good (and modern and performant) numerical tools look like, and I think Julia doesn't accomplish what I want / need in good numerical tools. But thats a whole nother topic. I will give them credit for building a tools thats a better language to build on than Matlab/R/Python, but for how I like to engineer, Haskell and friends are much better for enabling the tools i want to have :) 
It's in a section on print debugging, couched in an example where a programmer just wants to log all the arguments being passed to a function to help solve a transient problem in their code. None of the examples show `unsafePerformIO` being used for IO effects that you would ship in a product. Changing the type of a function---and everything that calls it---to safely perform IO doesn't seem worth it for doing that kind of debugging. In fact, to a programmer used to being able to throw `print_string` anywhere, it seems incredibly backwards. I know a lot of programmers like to mentally infer runtime state and never use run-time debugging tools, but many programmers use them *constantly*, and `unsafePerformIO` could help them stay productive. (I thought the article was great and felt like it was written for me. :)
PHP also has dependent types. What's the type of a zval? It depends ([source](https://twitter.com/old_sound/status/443543705170223104))
?
It seems that about halfway through that, Stefan removed the term "dependent" from the docs... and then the argument carried on anyway.
Do you have a real example of use of `unsafePerformIO` whitout which the engineer's life would be miserable? (There are *interesting* uses of `unsafePerformIO` for mutation-using code that are observationally transparent but cannot be proven as such using the simple discipline of the `ST` monad. At the end of the day I'm rather confident that you can avoid implementing those yourself and instead rely on expert-written libraries that take their responsibilities for what they do; it's possible to become such an expert, but outside the scope of this document.) I think any use of `unsafePerformIO` (or `Obj.magic` for that matter) should come with a paper proof, written in a comment, that it is safe. It's a shame that compilers don't enforce this.
Do you have any example code on github/bitbucket/etc by any chance?
I think you might like it in /r/lolphp.
What's to say really, in this case the point that Julia doesn't have dependent types is true regardless. It's just unfortunate they couldn't have a bit friendlier in pointing that out. I know some people claim that Haskellers seem condescending, which has never been my experience, but I can see where that might come from. I always look at guys like /u/tekmo are always genuinely nice in helping other people understand Haskell ideas and think "lets do more of that".
There is a clear misunderstanding of the recent [deferred type errors](http://research.microsoft.com/en-us/um/people/simonpj/papers/ext-f/icfp12.pdf) feature of GHC Haskell in this discussion. StefanKarpinski first says: &gt; You can run the Glasgow Haskell Compiler (GHC) in a mode where it defers all type checks until run time. If you run a type-correct Haskell program this way, do Haskell's types suddenly cease to be types? Has the meaning of the program changed even though its code and behavior are both identical? Does the existince of this GHC mode make Haskell a unityped language? and nox answers: &gt; Given that all type checks are deferred to runtime, it makes all expressions inhabitants of all types, making Haskell in that mode unityped. Neither of this is correct. In `-fdefer-type-errors`, type checks are *still done at compile-time*, the only difference is how the typing errors are handled. In normal mode, compilation fails with an error message. In deferred mode, the elaborated code uses runtime errors for the dynamic witnesses of type equality. This means that the type failure will notified at runtime (and stop the program) as soon as the part of the code that needs this equality (as decided by the type inference engine) gets forced. But the equality test itself has been done at compile-time. Another way to see it is that the compilation of an ill-typed program with `-fdefer-type-errors` will produce a *correctly typed* Core intermediate representation, not using a single type (as in the "unityped" view of dynamic languages), but using the full Haskell type system. The difference with the elaboration of correct programs is not a collapsing of static types, but the presence of well-typed error terms to witness incoherent type equalities. This is not an "implementation detail". There is a rich and well-defined notion of static type that can be used by users and compilers to reason about code written and compiled using that mode. Edit: for more information, you should [read the paper (PDF)](http://research.microsoft.com/en-us/um/people/simonpj/papers/ext-f/icfp12.pdf): "Equality proofs and deferred type errors; a compiler pearl" (Dimitrios Vytiniotis, Simon Peyton Jones, Jose Pedro Magalhães, 2012). It starts with a very clear informal introduction to how this works.
&gt; you can avoid implementing those yourself and instead rely on expert-written libraries that take their responsibilities for what they do And this kind of remark is exactly what most ML programmers I know who also know some Haskell (myself included) are very frustrated about. There are people in the Haskell community who somehow manage to project an arbitrary separation between "experts" and "non-expert" and the idea that some language features are "reserved" to expert programmers and that non-experts "should" somehow avoid using them. Can you name any other language than Haskell with this type of separation? Can you name any feature of any other language where the community of language users would commonly agree that "newcomers are not worthy to learn it"?
I have a friend working for [Tsuru Capital](http://www.tsurucapital.com/en/). Looks like what you're looking for as far as I can tell.
The real question is not whether Julia has dependent types, but whether it has types at all. If you accept that Julia has types, then clearly you must accept that it has dependent types. The 'dependent' bit is a red herring. There are some static type fundamentalists that insist that types only means static types, and that everything else is "unityped". While this is internally consistent use of terminology, this isn't good terminology. "Statically typed" and "dynamically typed" are much clearer terms than "typed" and "unityped". This is why the rest of the world outside an extremely narrow subset of people, namely the statically typed PLT academics and their disciples, have moved on to those clearer terms.
Haskell programmers in my experience are less condescending than the average programmer. It's a low bar. 
Pretty much for me, if Haskell had better ("first class") matrices and vectors, (and it was a touch easier to make them mutable when needed), and it was very easy to slice and dice them in a matlab-esque notation, it would be perfect. Do let us know when you have some more details about your upcoming numerical library. I'm eager to try it out and see how haskell may be nice to use for my numerical work. I'm trying out Julia for a testing project, and I was constantly frustrated not knowing the return type of functions. I had to dig around in source code to find out what was returned!
OCaml, `Obj.magic`. Java / C(++)11, memory barrier atomics. I think it is a *good thing* that we can define language layers such that it is reasonable to teach a "core subset" of the language without abstraction leaks that force people to actually understand the full thing. The same cannot be said of less fortunate language features/design. Are you suggesting that any Java programmer must be able to implement the library of concurrent collections, instead of happily using it, ignoring the complexity inside?
Type theory is basically a field of math, while developers of dynamic languages come from an engineering background. Terminology has developed on either side of the fence and now people are feuding about who's got it right. On one hand, I don't trust engineers to make the terminology consistent, on the other I don't trust mathematicians to make it intuitive. I still think type theorists have a much better case, especially since typed languages are growing in popularity, and typesystems are gradually growing richer.
You may be right, but I think it's a bit more complicated than that. TL;DR: I think that not all uses of the "type" wording in the dynamic community are interesting, but that Julia definitely qualifies as honest design that really talks about typing. Even if you accept to use the word "type" for the dynamic information, what I'm interested in is the structure of this information, and how it helps to reason about programs. I'm not sure there is anything interesting that can be said about typing for, say, PHP or Javascript. On the contrary, as [the julia paper](http://arxiv.org/abs/1209.5145) linked by htmltyp (thanks!) clearly demonstrates, there was a clear reflection about (dynamic) types, their structure and the reasoning they enable in the design and use of the Julia language. I mean when you read the following: &gt; It is often useful to refer to parameters of argument types inside methods, and to specify &gt; constraints on those parameters for dispatch purposes. Method parameters address these needs. &gt; These parameters behave a bit like arguments, but they are always derived automatically from &gt; method argument types and not specified explicitly by the caller. The following signature &gt; presents a typical example: &gt; &gt; function assign{T&lt;:Integer}(a::Array{T,1}, i, n::T) &gt; &gt; This signature is applicable to 1-dimensional arrays whose element type is some kind of &gt; integer, any type of second argument, and a third argument that is the same type as the &gt; array's element type. Inside the method, T will be bound to the array element type To me it is clear that this is talking about typing. (While it is not when I read hackish not-well-defined notions of `typeof` used by a community that's still not quite sure how it is related to `instanceof`). It is different from typing in the ML tradition, and I suspect it may have design defects because of that rejection of type erasability, but it quacks like types.
Except typed languages are not growing in popularity, the opposite is happening. Couple of years ago almost everybody was doing Java, nowadays you have a lot of people using Python, Ruby, etc. This is all quite secondary, since what makes good terminology is mostly determined by other factors than popularity. Here's why the mainstream terminology is better. First have a look at the dictionary definition of type: &gt; a category of people or things having common characteristics. If you look at a statically typed language you have types like int, string, array, function types, etc. Then you look at Julia which has all those same things: int, string, array, function "types", etc. There is a very clear correspondence here. Now you go to a type theorist and ask him "what types does Julia have?" and he answers "Julia is unityped, it has a single type (namely the union of all Julia values)". This does absolutely nothing to shed light on the clear correspondence between static types like int on the one hand and Julia "types" like int on the other hand. The statement that Julia has a single type is so vacuous that you might as well be talking about assembly language. The type theorist would call what Julia has a "tag system" rather than a type system, which makes it sound like it's something completely unrelated, yet again obscuring the very clear correspondence between those tags in Julia and types in a statically typed language. The alternative is to choose the terms "static types" and "dynamic types". Not only are these terms far more descriptive, they also make it clear that there is in fact a correspondence between static types and dynamic types, and that they are not two completely unrelated concepts. Sure, if you live in a universe where only static types exist it is annoying to have to use that extra word "static". Most people do not live in that universe. For this small subset to try to force the rest of the world into terminology that fits their narrow worldview, while making things unclear for the general world is a bad development.
I like OCaml more than Scala. You should apply to Jane Street. 
Again I think you are missing the forest for the tree. On the one hand I agree with you that it is desirable to teach languages using layers. Some languages are better suited to this teaching styles than others. On the other hand I want to remind you of the audience here: people who can write OCaml code effectively. These programmers are not mere “newcomers.” They know about parametric types, abstraction, currying, partial application, closures and recursion. They know about *types*, FFS! This is a level of expertise where Obj.magic is not a topic "best left to experts" -- you are then already talking to the experts that are very well able to understand when not to use it. To this audience, Java's and C(++)11's memory barriers are certainly a valuable tools that you *must* mention somehow in training materials, lest they won't be able to write efficient but correct concurrent code. 
I still think "tag system" is a more intellectually honest definition. Runtime tags can be altered, ignored, abstracted over, used as data, etc. This is completely at odds with static types. In my opinion, we should not bring attention to the similarities between types and tags, as they are only superficially similar; to the untrained eye, they look the same, but in reality they present very different mechanics and opportunities. You're saying that static types and dynamic types are two different strategies to obtain the same benefit; I think they're two fundamentally different mechanics that happen to be partially complementary.
~~You probably want to use the `Bound` class, and parametrize `TypeScheme` over the type of monotypes.~~ data TypeScheme m a = TS (Scope () (TypeScheme m) a) | T (m a) A type scheme isn't a monad; it doesn't make sense to substitute type schemes for free variables inside a type scheme. But it *does* make sense to substitute *monotypes* for free variables within a type scheme, ~~and this is exactly what the `Bound` class provides~~. ~~(Your original type can be recovered as `TypeScheme (Type χ t) α`.)~~ **Edit**: having attempted to actually write the `Bound` instance I suggested above, this doesn't work. `Scope b t` for `t` not a monad doesn't behave well. Your best bet may be to bind all the type variables at once and have `type TypeScheme χ t α = Scope Int (Type χ t) α`. Then you can use `Scope`'s `Bound` instance for substitution.
It's also worth pointing out that you still see the type errors at compile time. It's just that they are turned into "warnings" and don't interrupt the build.
We lost the word "literally" this way, though. 
Dependent typing does not mean that types can be used as values; it means that types can depend on values, which enriches typechecking without altering program semantics. &gt;&gt; Runtime tags can be altered, ignored &gt;In a memory safe language they cannot. Yes, that's kind of the definition of "memory-safe". Without that constraint however, this is possible in tag systems while fundamentally at odd with static typing.
Oh thanks! The [generic-deriving](http://hackage.haskell.org/package/generic-deriving) library is very nice. In our code base, someone used generics for `Monoid` for a large configuration record type exactly in the way described in this post. But manually, without the generic-deriving library. That code still gives me the weebies. I wrote a long comment to warn other devs about what is going on there. Now I can just get rid of the ugly stuff. Truth is, I would personally be just fine with: instance Monoid EmployeeProfile where mempty = EmployeeProfile mempty mempty mempty mempty mappend (EmployeeProfile a b c d ) (EmployeeProfile a' b' c' d') = EmployeeProfile (a&lt;&gt;a') (b&lt;&gt;b') (c&lt;&gt;c') (d&lt;&gt;d') Yes, and update that whenever the number of components of `EmployeeProfile` changes. Yes, our type has more like twenty components, so it's a bit hairier than that. So what? But anyway.
But they are still a Haskell newbie. Introducing language features (especially `unsafe` ones) in a way that can misrepresent their intentions and valid use-cases is potentially harmful, no matter what your level of experience in another programming language.
&gt; Can you name any feature of any other language where the community of language users would commonly agree that "newcomers are not worthy to learn it"? It's not that newcomers aren't worthy. It's that `unsafePerformIO` is very difficult to use properly if you aren't very familiar with the underlying semantics of Haskell, and even possibly of GHC. It's a very misrepresented feature as well - many discussions of Haskell from an outside perspective play it off as a "cheat", something that's there because "Haskell Style" isn't always good enough - "use it when things get tricky" is not an attitude we want to be putting in newbies' heads. &gt; Can you name any feature of any other language where the community of language users would commonly agree that "newcomers are not worthy to learn it"? Honestly, for most languages people will say "best leave it to the library writers." As /u/gasche says, the average user of Java or C++ is not going to be (and probably shouldn't be) writing concurrent collections libraries. Likewise with using `unsafePerformIO`, as it's so easy to get wrong.
Yeah, literally.
It is not possible to unify these two classes of properties. There are plenty of properties can be enforced statically, but not dynamically: for example, you can't test for termination, or indeed any other liveness property. Conversely, there are properties which can be checked dynamically, but not statically: for example, is the current day of the week Wednesday? (It would be nice if there were a duality, but AFAICT the relationship is actually more complicated than that.) 
&gt; There are some static type fundamentalists that insist that types only means static types, and that everything else is "unityped" ... "Statically typed" and "dynamically typed" are much clearer terms than "typed" and "unityped". This is why the rest of the world outside an extremely narrow subset of people, namely the statically typed PLT academics and their disciples, have moved on to those clearer terms. [Type theory predates PLT as a discipline](https://en.wikipedia.org/wiki/History_of_type_theory). Type systems have existed for nearly twice as long as compilers, and predate computers themselves by 40 years. By the time FORTRAN came around, type theory had already come up with a definition of type system (something like "a tractable syntactic method for classifying expressions by the kinds of values they can produce"). It is similar but different to the one that was comparatively recently developed by early compiler writers (where in dynamic "types are tags associated with values and functions; a type system makes sure a function's arguments have compatible tags"). "Statically typed" and "dynamically typed" aren't really clearer. They basically entail throwing the type theory definition out the window, because that distinction is nonsensical in it. They only make sense if you grant that the only thing that can have a type system is a programming language, because formal systems don't have runtime environments.
Carter, I, personally at least, want to applaud that your comments in that thread really were genuinely positive and informative. I think you did a far better job representing "this" side of the technical argument than some others. I think this is important in terms of representing the "PLT-weenie" community. This thread demonstrates to me that the term "type" certainly has multiple definitions and that if you want to convince someone that there's value in your own definition... you have to show it to them clearly, invitingly, and unambiguously. Being dismissive or curt just reinforces someone's desire to ignore your definition and whatever value it may hold. --- Sadly, I think this is one major point where having most of the documentation for type theory locked away in academic books and journal papers is a very bad thing. Those media have a built-in opacity and prejudice which, I think, is still harmful for encouraging others to see the value of type theory. I think an interesting metric when examining a discussion like this is to count the blog post v. book/white paper reference counts of either side. It's completely obvious that the larger side here tends to cite blog posts more than papers.
Thank you for the excellent information. HalVM piqued my interest as both a battle-hardened SysAdmin and as a Haskell neophyte.
At the risk of sounding like I'm blowing smoke up your ass - yes you and Ed were the primary voices of reason on that thread. Great job representing. The congratulatory note to Stephan for staying calm, that didn't mention you, was pretty unbelievable. Wondering if it would be worth it to pretend I agree that julia's got dependent types though - I would love to get one of those free PFPL copies mailed to me :)
Yes, and that's literally the worst thing ever. (Sorry, couldn't resist.)
&gt;Except typed languages are not growing in popularity, the opposite is happening. Couple of years ago almost everybody was doing Java, nowadays you have a lot of people using Python, Ruby, etc Your perception is not reflective of reality. Almost all of the ruby converts came from PHP. Python picked up people from perl and PHP for most of its existence, and lately is picking up ruby people too. Javascript on the server side is taking users from ruby. Most people who used java, still use java. Java, C#, C++ and C are among the most popular languages. On the other hand scala, haskell, and especially go are pulling in users from untyped languages. Go is getting tons of people from python and ruby.
&gt; `age :: Int -- for legal reasons, you don't store the age of some employees` I guess you mean `age :: Maybe Int`
Since neither of us have exact data, we can take a look at google trends. Ruby, Python rising in popularity, while Java, C#, Haskell &amp; OCaml are all declining. To me this does not look like static types are getting more popular.
Fixed. =)
&gt; In practice that version of EmployeeProfile is much less painful for your users. I somewhat agree, but what I really like on the 'monoid deriving' is that it does not depend on the number of fields. And to me that's a major advantage! &gt; Actually what you have is knowledge that there is some employee who is 25 years old, some employee Of course... that part was a joke on depicable human resources practices :P &gt; Consider that your query of your employee might want the intersection of these features not their union. Do you really want to define a whole new record? That's actually a good point, thanks! I think I'll update the post. _Edit_: post updated 
Yeah. Parametricity is a wonderful tool. You touch on a key point: good matrix code should be doing lots of slicing! And yes a lot of what I've bee doing has been working out good idioms for that. Which winds up being a teeny bit distinct from how matlab does it, though there is a relationship. Assuming I don't get overloaded with flame wars and meetings, trying to get the preview alpha out this week plus some basic examples. 
You can do e.g. the termination monad in dynamically typed languages just fine.
&gt; Dependent typing does not mean that types can be used as values; it means that types can depend on values, which enriches typechecking without altering program semantics. I don't see any distinction here. You can pass around types in a dependently typed language. I think what you mean is dependently typed languages disallow typecase, which is certainly the case, but you can also disallow typecase in a dynamically typed language. &gt; Yes, that's kind of the definition of "memory-safe". Without that constraint however, this is possible in tag systems while fundamentally at odd with static typing. The exact same argument can be made for statically typed languages. e.g. in C, a non memory safe language, you can ignore static types. There is no distinction here either.
History isn't a good way to choose terminology. &gt; They basically entail throwing the type theory definition out the window, because that distinction is nonsensical in it. Yes, as I said, if your universe consists of only static typing, this is what you get. The whole point is that it's nonsensical to view dynamic typing through the lens of static typing, and that a holistic view makes much more sense.
&gt;we can take a look at google trends We can, but that wouldn't make much sense. How many people are searching for X isn't a very good indicator of how many people use X. Even if we did that, you are cherry picking results that suit your agenda. F# is up, scala is up, go is **WAY** up. PHP is down, perl is down, ruby is down (contrary to what you state). We can also take a look at things that are at least slightly reasonable, like the various language popularity ranking sites. They all have haskell, scala, go, and F# moving up. The biggest languages move up and down a few percent (both the javas and the PHPs), but mostly don't show any major movement trends (ruby has largely stabilized since its post-rails falloff).
Well, I suppose it _is_ a way to let them find someone with 10 years of `Idris` experience, so carry on then. ;)
That's different from checking termination of your language, though. The termination monad only works if you (a) equip the object language being interpreted in it with checkpoints and (b) discover proofs that all possible language fragments between checkpoints terminate. The former point cements that the termination monad doesn't even apply at the level of your whole language and the latter point suggests why that's weak—you still need to bring proof of termination of the metalanguage in order to get a complete proof. Compare this to the termination checker in, say, Coq, which ensures that the only kind of recursion allowed in the metalanguage (Coq) is structural. You can embed languages in Coq which may be non-terminating, but then if you build an interpreter for them in Coq that interpreter will include a Termination monad.
Part of that is I don't think anybody saw it. The usual closure indicator for this sort of discussion is when everyone sees the 'Closed' label at the bottom, but because the issue had been pre-emptively closed as a duplicate, that social cue was removed. =)
did you notice one of my comments on that thread was [literally](http://theoatmeal.com/comics/literally) a remark on that?
Precisely :D
Do you have a proposed definition of "type" and "type system" that unifies both definitions, or do you just want to throw type theory (or at least some type-systems-a-la-type-theory) out the window?
&gt; Another way to see it is that the compilation of an ill-typed program with `-fdefer-type-errors` will produce a correctly typed Core How does *that* work? Does the Core just have an `error` buried inside of it? I've never actually played with an `-fdefer-type-errors` compiled artifact. It always stays in GHCi.
Let me see if I understand. A `TypeScheme` is a structure of the form `TS (\a -&gt; TS (\b -&gt; TS (\c -&gt; T t)))`, where `a`, `b` and `c` are `TypeScheme`s and `t` is a `Type`, is that correct? Is there any way to use those `TypeScheme`s to construct a `Type`? **edit**: Wait, no, you clearly want `a`, `b` and `c` to be `Type`s, not `TypeScheme`s. From what I understand of the first page of [bound's documentation](http://hackage.haskell.org/package/bound-1.0/docs/Bound.html), Bound takes care of renaming, while you take care of implementing the substitution. [Monad-based substitution](http://blog.sigfpe.com/2006/11/variable-substitution-gives.html) is convenient when substituting a `Term a` into a `Term a`, because `join` squashes the `Term (Term a)` into a `Term a`, but in your case you want to substitute a `Type a` into a `TypeScheme a`, so `join` won't help you squash the `TypeScheme (Type a)` into a `TypeScheme a`. I don't know if bound supports other kinds of substitutions; in order to discover the answer, you will have to explore further than I have, beyond the first page of the docs.
Sure, you need a terminating language for it to work. If your language already allows unbounded nontermination you're of course not going to get termination out of it, whether it's statically typed or dynamically typed. Look at how static termination checkers work: they basically check that any recursive call is on some smaller data than the original call. This can easily be checked dynamically.
Whenever GHC tries to do typechecking, it has to solve type equality constraints. Insoluable constraints like `Char ~ Int` for example, would normally be immediately rejected and the compiler bails. Instead, GHC uses a principled approach: it leverages the existing support for type equalities in Core to generate witness terms that result in an `error` call, as you said - GHC will then use this 'evidence' of the type equality to build a coercion term which results in the error. In other words: let x = ("Hello", 'a' + (1 :: Int)) in ... turns into something like (handwaving the general case): let c = error "blah blah..." :: Char ~ Int -- bogus equality witness x = ("Hello", (cast 'a' c) + (1 :: Int)) in ... So in this case, it's safe to use `fst x` but not `snd x`, as it will require evaluating the bogus witness. See [the paper](http://research.microsoft.com/en-us/um/people/simonpj/papers/ext-f/icfp12.pdf) for details, but that's the basic idea. **EDIT**: Also see my reply to /u/enolan for another case of the same idea.
Thanks, but my understanding of dependent types is sufficient. I have already read the first chapter of HoTT. Thanks for being so non condescending though.
This is a thoughtful comment and thank you for putting it so well. This is precisely why I am now hesitating between picking ML and HS for a toy project. On the one hand, ML will give me the programming power that I need (controlled impurity together with strict evaluation), but on the other hand HS gives me type classes and a more compact syntax. I just wish strict Haskell was really a thing.
&gt; And what exactly is typecase? Google has nothing on it. With some context Google works fine for me. Search for "typecase programming languages". &gt; Maybe, but you can't alter types at runtime, nor use them as data. You can do that using reflection in certain languages, but that's basically the dynamically "typed" part of the language. No it isn't, it's the reflection part of the language. Scheme is dynamically typed and allows no such thing.
`(+)` has four parameters because Core is a (very suped up) variant of System F, which has explicit type application (and a lot of other stuff). That means any time you have a polymorphic type variable mentioned in some type, it has to be applied *explicitly*. The `@` in the arguments is a type application - it specifies that `(+)` as defined in `Num` takes a type argument, in this case a `[Char]` (or `String`.) This doesn't make sense at first it seems, but read on... The second argument is the dictionary for the actual `(+)` operation - GHC turns typeclasses into explicit dictionary application. In a nutshell, this basically means GHC turns things like this: class C a where foo :: ... f :: C a =&gt; ... into: data CDict a = { cFoo :: a -&gt; ... } f :: CDict a -&gt; ... So the implicit typeclass becomes an explicit dictionary value, which is just a set of the type-class functions for a specific type (you can learn a lot more about this from a [thing I wrote here](https://www.fpcomplete.com/user/thoughtpolice/using-reflection)). If you combine these two, you get something like: f :: forall a. CDict a -&gt; ... -- note 'forall' was implicit before but remember, `a` is a parameter Core and we have to explicitly apply it! So application has to look like f @ SomeType (dictionary :: CDict SomeType) ... Which is what you see above - an application of a type, and a dictionary for the class. This is the second argument to `(+)`, which is the type-class dictionary. In this case, with `-fdefer-type-errors`, the dictionary *itself* is an `error` - because it must be evaluated to actually run the program, these are the precise semantics you want when your program 'runs'. And this is why `$dNum_rn6` has a type `Num String` - it's a bogus dictionary.
&gt; because the essence of dependent typing is compile-time value information getting into the types and being used to reject more worgnly-typed programs If you take that as the definition of dependent types then a dynamically typed language is never going to have dependent types because you have presupposed static checking. If you take a broader view, then dependent types are about types that depend on values. For instance the type Array{T,n} which are n-dimensional arrays containing values of type T. This type depends on the value n, which is an integer. This kind of type is the poster child of dependently typed languages. Julia has these kinds of types as well.
Strict Haskell really is a thing. You just can't have it. :)
&gt; It explains them by why they are needed, instead of why they are useful. I don't read this article in the same way you do. To me this article demonstrates by examples that monads are a good solution to a variety of problems. It does not demonstrate (nor even argues) that monads are the *best* solution to these problems. So in my reading the article "explains why they are useful" rather than "why they are needed". 
Thank I have removed the reference to ST as you suggest.
We must be looking at different google trends then. Scala isn't up, F# isn't either. Go is indeed up, but note that the go programming language does not even appear in the top 10 list. It's all about Go lyrics, &amp; Go the game, so this has nothing to do with the programming language. You are right that whether Ruby is or is not up is debatable, it depends on what time frame you look at. Let's call it more or less a wash. In any case, as I said before, you shouldn't base terminology on what's becoming more popular and what's becoming less popular. Dynamic &amp; static typing are both popular enough to warrant a sensible technical term. The logic that we can call dynamically typed language unityped because statically typed languages are growing in popularity makes no sense whatsoever, even if it were true which it isn't.
This is a common misconception, and it's rather baffling---people seem to think that it's incorrect to use "literally" figuratively, because it *means* "not figuratively". No one objects to "I was beside myself", which is figurative language---and no one even objects to "Honestly, I was really beside myself, no fooling; I mean it!", which is also figurative, and which also includes several words and phrases essentially saying "take me seriously; this isn't *just* me being figurative". Yet if you include the word "literally", people have a fit. It's really [silly](http://heteronomy.wordpress.com/2009/02/17/tuesday-hatred-dismembering-displeasing-and-lurking-too/). Cf. [this informative article](http://unfogged.com/literally.pdf).
This is a false dichotomy. Calling static typing static typing is not throwing type systems out of the window any more than calling dynamic typing dynamic typing throws dynamic typing out of the window. For a definition of type, the dictionary definition does a reasonable job: &gt; a category of things having common characteristics Before you insist that this is too vague, try to think of a definition of 'static type'. It's not so easy either to come up with a concrete definition that encompasses all of the things that are commonly called static types.
That assumes that there was anything we could do to stop it.
You mean `fst` and `snd` `x`, yes? That's really neat. I've never thought too hard about how type equalities would play out when they get all the way down to Core/System F.
&gt;The logic that we can call dynamically typed language unityped because statically typed languages are growing in popularity makes no sense whatsoever Nobody put forth that logic, so perhaps arguing against it doesn't make much sense. &gt;even if it were true which it isn't. Just repeating it doesn't make it so. 
The thing is, we have been able to do run-time checks based on values since the early days of higher-level programming languages. Dynamic array bounds checks are not hard, and they have been around for a long time! The reason this isn't a big deal is that run-time is inhabited by concrete values, and checking arbitrary properties of the values before use is trivial; just insert code that performs the check! This provides safety, but at a run-time cost. The run-time cost of these checks is why they are so rare in typed languages without dependent types (Java being a notable exception). The interesting thing about dependent types is the fact that the same kinds of checks that may otherwise only be checked at run-time (i.e. those that depend on run-time values) can actually be checked ahead of time. This allows one to omit the run-time checks, admitting the safety of the dynamic check without the run-time overhead of performing it. I think there's some middle ground (which Julia may inhabit here; I haven't read the paper about its type system) where some analysis of run-time code could take hints from trace analysis and these run-time value constraints to elide some bounds checking in a code path that it can prove to always meet the value constraints. This would be a sort of JIT-compiled dependent type analysis done in a staged manner, I guess. This is actually very interesting for performance reasons, and I think that if Julia does this, it would warrant a claim of 'staged recompilation with dependent type analysis and recovery' or something like that, though it's still likely to cause confusion with people who are only considering the types of source program terms rather than types of compiled program terms.
You know that Debug.Trace uses unsafePerformIO right? trace :: String -&gt; a -&gt; a is just a trivial wrapper that prints to stderr.
The crime is that so many people have used it ambiguously that the word is useless now, not that it has ever been used figuratively. It's a matter of responsibility more than correctness.
Debug.Trace also just uses unsafePerformIO...
&gt; How does that work? Does the Core just have an error buried inside of it? The best way to answer this question is to [read the paper](http://research.microsoft.com/en-us/um/people/simonpj/papers/ext-f/icfp12.pdf). (As everything SPJ writes) it is extremely well-written, readable, and starts with an introduction without the formal detail -- it's ok if you don't read the formal part. Do yourself a favor and go read it.
Of course, how else could it work? 
&gt; Nobody put forth that logic, so perhaps arguing against it doesn't make much sense. Except, somebody did: &gt; I still think type theorists have a much better case, especially since typed languages are growing in popularity
What is your definition of "type system"? Does it need to be attached to a programming language? If so, you're throwing many type-systems-a-la-type-theory out the window. Is [homotopy type theory](https://en.wikipedia.org/wiki/Homotopy_type_theory) a static type system, under your definition? Could you try to make a [static type system for natural language](http://www-personal.umich.edu/~akao/NLP_Paper.htm#IL)? 
&gt; The run-time cost of these checks is why they are so rare in typed languages without dependent types (Java being a notable exception). Most languages, statically typed or not, have array bounds checking nowadays. This includes Python, Ruby, Java, C#, F#, OCaml, Haskell and more. Only languages like C/C++ do not have bounds checking. I completely agree with the rest.
"I still think the sky is blue, especially since so many people can see that the sky is blue". This statement does not mean "I think the sky is blue *because* people look at it".
TL;DR - with the new mechanisms for [`Coercible`](http://www.haskell.org/ghc/docs/7.8.1-rc2/html/libraries/base-4.7.0.0/Data-Coerce.html#t%3ACoercible) implemented in GHC 7.8, it's now possible to do safe, zero-overhead coercions between (possibly nested) datatypes with the same runtime representation. The classic example (but not in the paper) is: newtype Age = MkAge Int let x = map MkAge [1..10] In the above example, you would expect `newtype` to be free - but it is not. `Age` and `Int` have the same runtime representation, but not the same type! This carries a runtime cost - GHC's intermediate language is *typed* and this is a change in the two types - optimizing it is outside its field of view. So it must build a list just to shuffle it around a little bit, which is an overhead. In other words, `Age` and `Int` are *nominally different* and the compiler can't see past that. Coercible changes this and now depends critically on the new concept of Roles, which also allow features like `GeneralizedNewtypeDeriving` to be used in a safe way. Now we can say: import Data.Coerce newtype Age = MkAge Int let x = (coerce :: [Int] -&gt; [Age]) [1..10] In this case, we can eliminate the runtime overhead - GHC automatically sees instead that `Age` and `Int` are actually *representationally* equivalent now - so zero-overhead casting is completely safe. But you cannot use `coerce` to coerce between two representations which are not representationally equivalent. There have been a lot of subtle points while watching this work being done. In particular, after [Richard implemented Roles in GHC](http://typesandkinds.wordpress.com/2013/08/15/roles-a-new-feature-of-ghc/), the Coercible work really did expose some subtle points about nominal vs representational equality that took time to hash out, I think. There were lots of good discussions on the mailing list. Anyway, now newtypes are less expensive in a safe and principled manner - yay!
You're right; I guess the right way to put it is that for a long time, it was rare for statically typed languages used in industry to *use* run-time bounds checks. Languages that *could* perform run-time bounds checking have been with us since at least Algol 60, but even OCaml lets you turn off bounds checking at runtime for performance purposes.
&gt;No it isn't, it's the reflection part of the language. Scheme is dynamically typed and allows no such thing. Because Scheme is memory-safe...
I don't think I've ever been confused as to how someone meant the word "literally", which is, in any case, tailor-made for the figurative use which is so popular. Seriously, read the paper I linked. It's a completely natural *and correct* use of the word. 
&gt; The criteria you give ... does hold for `f=Same` No it doesn't. Consider `unSame (Same x) = x`. Then `Same 1 == Same 2` but `unSame (Same 1) /= unSame (Same 2)`.
This is not only interesting, but also enlightening about continuations. For other people, it may strike you as odd how `\p -&gt; p True` selects an input for which the function returns True, but it makes sense because there are just 4 functions `Bool -&gt; Bool`, so it just does this: (const False) True = False -- there is no correct solution here (const True) True = True -- anything goes, so True goes id True = True not True = False -- works because not False = True And if `Select` selects an input for a function, the `Cont` it gives rise to finds that input and applies it to the function again. So in the `\p -&gt; p True` case, it returns True when whatever boolean it chose does indeed return True, and False in the `const False` case, thus checking for satisfiability. It's neat. The real magic is in `sequence`, but that's not hard either.
assuming the authors pay attention: there is a `-&gt; a` too much at page 5, showH.
Wouldn't `map coerce` still cause overhead from the `map` traversal? Or is there a rewrite rule that `map (coerce :: a -&gt; b) = coerce :: [a] -&gt; [b]`?
The sets in [Data.Set](http://www.haskell.org/ghc/docs/7.6.2/html/libraries/containers-0.5.0.0/Data-Set.html) require that their elements are in `Ord` which prevents them from being a Functor. The `map` implemented in the library shows this: map :: (Ord a, Ord b) =&gt; (a -&gt; b) -&gt; Set a -&gt; Set b But `fmap` must have the type fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b without any typeclass constrains on `a` or `b`. The `Ord` requirement prevents it from being used in the Functor implementation. Every function producing a Set either requires that the input type be in `Ord` or constructs the value from an existing Set (which must necessarily then possess the Ord constraint). 
I botched that example, sorry - fixed.
If the datatype is appropriately structured and the functions fulfill the monad laws, it is a monad :p
that's `f=unSame`.
Is there any reason to not have rewrite rules like that?
Will Coercible reduce any of the Profunctor.Unsafe usage?
Oh, perhaps I misunderstood what you were saying. Anyway, I think that you have misunderstood what ueberbobo was saying. He/she is saying that a sensible condition for all `Eq` datatypes could be that x == y =&gt; f x == f y for all `f`. `Same x` (the datatype) does not satisfy this condition.
My favourite this about this extension is that it can (I think) always be replaced with `unsafeCoerce` on implementations that don't have it yet.
Would this not work for some reason? {-# RULES "coerce" forall (xs :: Coercible (f a) (f b) =&gt; f a). fmap coerce xs = (coerce xs :: f b) #-} 
Well I saw you on stage at IFL doing a compiler for it, so I guess it is in the realm of do-able things. If you had some pointers about related technologies, I'd be interested.
I believe this is what the role system outlaws.
This looks really helpful. There's a mistake in [this](https://ghc.haskell.org/trac/ghc/wiki/PartialTypeSignatures#named-wildcards) section of the proposal. &gt;-- Inferred type: Show a =&gt; x -&gt; String should be &gt;-- Inferred type: Show a =&gt; a -&gt; String Also &gt;We currently only allow one extra-constraints wildcard, in the outermost set of constraints. Otherwise, we might get ambiguous situations like this: Why can it not work with multiCs :: Show a =&gt; a -&gt; (Enum a, _) =&gt; String
https://ghc.haskell.org/trac/ghc/wiki/Roles, see "Role annotations".
So wouldn't a better approach be to mention that sometimes it is convenient to use unsafePerformIO and give as an example the implementation of trace? 
Thanks! This is exactly the sort of advice I was looking for. &gt; A type scheme isn't a monad This is what made me feel I was "doing it wrong", because I couldn't see how a TypeScheme could form a monad, but I also couldn't think of a way to make use of Bound without making it one. I will try your suggestion and see how that goes.
Thanks for that monad-based substitution link -- I think reading over that a couple of times will help me get an intuition for what libraries like Bound are trying to do and how, rather than just blindly trying to tweak code from the tutorial until it compiles.
Does this give any advantage over just using new type variables? 
Well now I'm curious. 
`fmap f . fmap g = fmap (f . g)` is only free given `fmap id = id` for the structural map by the free theorem. This `map` isn't that one, so you need to prove it from scratch.
Can you explain?
Yes, wildcards mean that the type signature is valid for *some* type, while new variables need to vbe valid for every type. Lets take the example from the linked discussion. You cave a function foo :: Bool -&gt; Bool foo x = not x With partial type sugnatures, you can write foo :: _ -&gt; Bool and the `_` gets inferred to be `Bool` by the typechecker. On the other hand, if you wrote foo :: a -&gt; Bool you would imply that your function is polymorphic on its first argument (meaning that you could instantiate that variable with any type you want, like `Int -&gt; Bool` or `String -&gt; Bool`)
Well, that doesn't work whenever `f` is still parametric. Given how rarely I actually work over a concrete `f`, I personally get very little benefit from things as they stand. ;) Worse, that rule has no guarantee of semantics preservation beyond the laws unless you take away the ability to hand-implement `fmap` from users, because instances aren't guaranteed to not do hinky things to other fields. They are ruled out by the `Functor` laws, but many well meaning packages use non-structural `Functor` instances. At best, this means you can't use such a rule under SafeHaskell. The new role machinery at least means it can't cause segfaults any more, though! To go further, I spent some time exploring the design space. My first attempted fix was to play with an explicit `fmapCoerce :: Coercible a b =&gt; f a -&gt; f b` as a member of `Functor`, with a default signature that defnes it in terms of `fmapCoerce = coerce`, but after you dig into that for while, you'll find that isn't actually strong enough to handle things like `Compose f g` with parametric `f` and `g` or most monad transformers. =( Currently the best fix I have awareness of is to add a member to `Functor` that provides a possible lifting of a coercion `Coercion a b -&gt; Maybe (Coercion (f a) (f b))`. _That_ version can at least be worked up to provide a constant time lifting over law-abiding structural functors, even parametrically, falling back on the user's `fmap` if the coercion won't lift over one of the functors involved. There are still issues, though. e.g. There are questions of the right way to default it or instantiate it. Should it return Nothing by default or should it use the more useful but riskier default signature trick to automatically implement it in terms of Coercible internally? How can the user write such a function for more complicated recursive or worse, polymorphically recursive, cases? etc. Moreover, any such solution raises a bunch of ire from the "it's too complicated!" crowd, even before you get a chance to explore the design space, so I rather despair of finding the "right" solution. As it stands literally all the things I want to exploit `Coercible` for don't fit the constraints of the current implementation. I mostly stopped working on it due to a combination of exasperation and figuring that the release of 7.8 was immanent (this was October), so I figured at the time that I'd punt thinking about it until 7.10. In the meantime I've been absorbed by other things.
Nothing ensures a `Functor` is implemented structurally, so it can change answers. A `map coerce = coerce` rule is safe, but an `fmap coerce = coerce` rule isn't sound in general.
Finally, I offer you this hobgoblin. newtype Trivial a = Trivial a instance Functor Trivial where fmap f = coerce . f . coerce which your rule makes non-terminating.
This is GADT trickery again, yes?
Because `Functor` is defined as: class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b And to support the `Ord` constraint imposed by `Set`, it would need to be defined as something like: {-# LANGUAGE ConstraintKinds, TypeFamilies #-} class Functor f where type FunctorConstraint f :: * -&gt; Constraint fmap :: (FunctorConstraint f a, FunctorConstraint f b) =&gt; (a -&gt; b) -&gt; f a -&gt; f b Then you could happily do: instance Functor (Set a) where type FunctorConstraint Set = Ord fmap = Set.map Same goes for `Applicative` and `Monad`, I believe. Maybe this will be the Functor-Applicative-Monad Fiasco Part 2. 
Basically, because `Data.Set` is not an endofunctor over `Hask`, since its domain is only `Ord` (so it's a functor `Ord -&gt; Hask`), whereas the Haskell `Functor` class is for `Hask -&gt; Hask` endofunctors only. There's a very nice paper about using typeclass-associated constraints to implement proper (as in, non-endo-)functors in Haskell: http://dorchard.wordpress.com/2011/10/18/subcategories-in-haskell-exofunctors/ (the paper itself is at http://www.cl.cam.ac.uk/~dao29/drafts/tfp-structures-orchard12.pdf)
Ah, so the same as for the Monad instance, obvious in retrospect, thanks. Also, it's funny how you can easily find this on google once you know the answer.
"typecase" is a thing in the literature. It isn't a good thing since it can break parametricity
&gt; For instance the type Array{T,n} which are n-dimensional arrays containing values of type T. This type depends on the value n, which is an integer. This kind of type is the poster child of dependently typed languages. Julia has these kinds of types as well Haskell has exactly "dependent types" in this sense as does any modern language with existentials. Haskell does not have Pi or Sigma. It is not dependently typed. Neither is julia for exactly the same reason.
&gt; The whole point is that it's nonsensical to view dynamic typing through the lens of static typing Why? Isn't this exactly what the julia compilation model does? It figures out type tags, and uses those to generate statically typed code that is optimizable. Thinking about dynamically typed languages in these terms makes them easy to design and implement.
&gt; My first attempted fix was to play with an explicit fmapCoerce :: Coercible a b =&gt; f a -&gt; f b as a member of Functor, with a default signature that defnes it in terms of fmapCoerce = coerce, but after you dig into that for while, you'll find that isn't actually strong enough to handle things like Compose f g with parametric f and g or most monad transformers. =( I'm not convinced. Why doesn't a rewrite rule of the form fmap coerce = fmapCoerce fix this? You would define instance (Functor f, Functor g) =&gt; Functor (Compose f g) where {-# INLINE fmapCoerce #-} fmapCoerce (Compose x) = Compose (fmap fmapCoerce x) a call to fmapCoerce would optimize as follows fmapCoerce (Compose x) = Compose (fmap fmapCoerce x) = Compose (fmap coerce x) = Compose (fmapCoerce x) = Compose (coerce x) this only works when the dictionary can be found at compile time, and so might not be much use in some of your libraries, but it would seem to work for "tradition" haskell code. I don't think the lifting idea (with a default as `Nothing`) is a bad one. The "it's too complicated" crowd be damned.
Speaker here. My understanding is that it will be recorded.
What I meant is that if you use the "typed" vs "unityped" distinction, that gives no insight whatsoever into dynamically typed languages. Assembly language is unityped, Julia is unityped, and yet there is a world of difference between them. Hence why "dynamically typed" and "statically typed" are better terms.
You can write a function `constructArray : T -&gt; (n:int) -&gt; Array T n` in Haskell? I don't think so. Now certainly Julia may not have all the constructs of a dependently typed language, but "types can depend on values" or "tags can depend on values" it certainly has.
Can you give an example of a language where this is possible, or is this just hypothetical? This certainly isn't common (or even possible) in any dynamically typed language that I know of.
Making arguments against dynamically typed languages based on non memory safe languages is like making arguments against statically typed languages based on C. It's disingenuous since all modern dynamically typed languages are memory safe.
Again, it is internally consistent to take this view. Is it a useful view for talking about languages like Julia? No.
This analogy makes no sense since what we're talking about here is a question which definition is better, whereas whether the sky is blue is a physical fact.
&gt; Does it need to be attached to a programming language? If so, you're throwing many type-systems-a-la-type-theory out the window. This isn't the case. It is considered a major open problem to create a computational interpretation of HoTT. That it's attached to a programming language is the whole point of type theory as opposed to just logic.
&gt; they basically check that any recursive call is on some smaller data than the original call. That ensures that a recursion over an inductive family is terminating, sure. And while something like that can be checked dynamically, how would you rule out something like: (λf. (λx. f (x x)) (λx.f (x x))) g without types?
According to the paper, no there isn't (on page 11). The thing is that you would need rewrite rules for every map-like function, and you couldn't safely do `fmap coerce = coerce`.
It seems like what we want is for the `Functor f` instance to contain a witness of `f`'s argument's role. `Compose f g` ought to be able to say that its argument is phantom if `f`'s is phantom, representational if both `f` and `g`'s arguments are representational, and nominal otherwise. `Coercion a b -&gt; Maybe (Coercion (f a) (f b))` is one representation of a role witness (though it equates phantom and representational roles). Might there be others that are easier to work with?
Same as with every other FP language - you can find a lot of this [here: why haskell matters](http://www.haskell.org/haskellwiki/Why_Haskell_matters)
Perhaps [this](http://www.haskell.org/haskellwiki/Why_Haskell_matters) ?
Will GHC be smart enough to replace `map MkAge [1..10]` with `(coerce :: Int -&gt; Age) [1..10]` for me automatically?
&gt; We describe in Section 3.2 that we allow coercions to happen even on datatypes for which the constructors are not available, such as Map. However, this violates Safe Haskell’s promise that no abstraction barrier is broken through. To rectify this problem, GHC uses a more stringent check when satisfying a Coercible constraint when compiling in Safe mode: all constructors of all datatypes to be coerced under must be visible. This means, essentially, traversing the entire tree of datatype definitions, making sure all constructors of all datatypes, recursively, are available. With this check in place, we can be sure not to break any abstraction boundaries. The fact that abstraction violations are allowed by default unless one adds a role annotation makes me a bit uneasy. Wouldn't it be more in the spirit of Haskell to make it the other way around?
&gt; TL;DR: I think that not all uses of the "type" wording in the dynamic community are interesting, but that Julia definitely qualifies as honest design that really talks about typing. An honest design - yes. Some subtle indirect relationships between what they call "types" and what we call "types" - yes. Some of the same words are used, but with very different meanings; you can't use that as a proof for anything. The term "dependent types" has not, until now, been commonly used for anything in the world of dynamic types. So if you want to use it and give it a new meaning in that context, based on some loose analogy with static types, then define precisely what you mean. But this usage seemed to be just sloppiness. It doesn't make make me very optimistic about how carefully the author thinks even about dynamic types, let alone static types.
Yes! Absolutely. Here's a real-world example from something I'm working on. collectLoggerT :: forall msg m a. Monad m =&gt; LoggerT msg m a -&gt; m (a, [(Level, msg)]) collectLoggerT = ... appender ... where appender :: Log msg x -&gt; WST (Endo [(Level, msg)]) Level m x ... Because `Log` is a GADT in my code, `appender` needs a type signature. But most of it is just type signature noise that's required to get the type checker to accept this. Ideally, with this proposal, that would all be written down like this: collectLoggerT :: Monad m =&gt; LoggerT msg m a -&gt; m (a, [(Level, msg)]) collectLoggerT = ... appender ... where appender :: _ x -&gt; _ x ... Lots of type signature noise is removed. The only parts that remains are the the top-level signature for documentation and what GHC actually needs to be told about in order to see how to proceed.
That's right. A GADT doesn't have to obey the laws of functorality, so: {-# LANGUAGE GADTs, KindSignatures #-} import Data.Coerce import Data.Functor data Foo :: * -&gt; * where Bar :: Foo () Baz :: Foo a instance Functor Foo where fmap f _ = Baz onoes :: () onoes = case (fmap coerce Bar :: Foo ()) of Bar -&gt; () ogood :: () ogood = case (coerce Bar :: Foo ()) of Bar -&gt; () will give you a runtime exception, when really it never should. If you remove `Bar` then the `Functor` instance is still valid, but can no longer change answers behind your back.
No, the compiler will not do it for you automatically. There's no RULE for `map coerce = coerce` as it stands. 
You have to explicitly mark `T`'s parameter `a` as being nominal using a role annotation - otherwise it is inferred as phantom. If you do this, `coerce` can no longer work as the two types are different. (In general, role inference will always infer the most general role, as it should. See [the wiki](https://ghc.haskell.org/trac/ghc/wiki/Roles) for details, where the exact *opposite* problem pops up with `Ptr a`) {-# LANGUAGE RoleAnnotations #-} module Foo (T, mkT) where import Data.Coerce data T a = T deriving Show type role T nominal mkT :: T Int mkT = T Then: *Foo&gt; let f = coerce :: T Int -&gt; T String &lt;interactive&gt;:2:9: Could not coerce from ‘T Int’ to ‘T String’ because the first type argument of ‘T’ has role Nominal, but the arguments ‘Int’ and ‘String’ differ arising from a use of ‘coerce’ In the expression: coerce :: T Int -&gt; T String In an equation for ‘f’: f = coerce :: T Int -&gt; T String **EDIT**: I'll also mention that I'm not sure how much `coerce` changes this particular example - before we had coerce &amp; roles, I could always just `unsafeCoerce` the phantom argument away anyway if I knew that it *was* phantom (which in your case, it must be - otherwise it would be inferred as nominal and this wouldn't work). I will also note that if you try to compile this with `-XSafe` it will fail with or without the annotation, because it fails one of SH's safety checks (that constructors are in scope for things like this).
Well, it's now considered `-XSafe` compatible because it no longer can break type safety - it's subject to role checking, which will prevent you from doing Bad Things. Is that what you meant?
If only there were some way to get rid of that annoying "Preview" watermark..[.](http://superuser.com/questions/455462/how-to-remove-a-watermark-from-a-pdf-file)
Type classes are one of Haskell's great practical strengths. Because you say that category theory won't help here, then perhaps something practical like serialization to JSON using type classes would be a convincing example for this person.
I'm not arguing about languages, I'm arguing about terminology &gt;_&gt;
This method does seem to make it easier to combine effects than monad transformers. Also, I've often wanted to have something like the ! in Haskell...
Go through the test suite and see which ones could be simplified or eliminated with stronger typing. Remark also that typing not only eliminates errors those tested paths, but all of the untested ones as well. Do something with concurrency where STM is required for safety. If your other options are capable of implementing that as well, try to expose some part of the functionality to an end user and ask what happens when they violate constraints maintained by the STM type. Try to estimate the cost of `null` in your codebase. Beat current code in speed while retaining a high level presentation. Obviously situational, but applies in more situations than may first be believed. Use Aeson to parse some JSON. That is not always a pretty process. Use (Atto)parsec to parse something. That is rarely a pretty process. If you used Attoparsec, try to optimize it and benchmark it against something a parser generator generated. If web is your thing, demonstrate type-safe URLs. Find a typo-based bug somewhere in your code. Estimate how many hours were spent fixing it. (This hits repeatedly in our Rails codebase)
What can you tell us about this person? How much programming experience do they have, what sort of programming languages do they know, which do they like, what do they like doing with them, etc? Can you think of any problems they have that Haskell solves? Finally, if they're happy with what they're got, maybe you shouldn't try to sell them on something just because you like it. Remember the old motto: *Avoid success at all costs*.
Same thing applies.
In the case of `map`, can't the compiler see that `MkAge :: Int -&gt; Age` is equivalent to `coerce :: Int -&gt; Age`, and then apply the `map coerce` rewrite rule?
It is when proving correctness is too expensive, which for most code it is for the foreseeable future. Even for code where it isn't it is nice to write the code first, then run it safely &amp; test it with dynamic checks, and introduce proofs on a case by case basis where it makes sense.
mkAge is the same in core as `coerce :: Int -&gt; Age`. see section 6.4 in the paper.
Yes, according to my understanding of that section, this should work. But [/u/aseipp](http://www.reddit.com/user/aseipp) says that I'll have to write additional `RULES` specifically for `map MkAge`.
if `fmapCoerce = fmap coerce` is the fallback you expect users to use when they have a non-structural recursion that suggested rewrite rule leads to a loop. You can of course break that cycle by making a second `coerce`, etc. fmapCoerce (Compose x) = Compose (fmap fmapCoerce x) can only be resolved when `g` is concrete. That means that the entire code path down to your use of `fmap` has to be inlined or your asymptotics change. That is a very brittle situation. &gt; I don't think the lifting idea (with a default as Nothing) is a bad one. The "it's too complicated" crowd be damned. I'm with you. I just put it on hold until I could have more time to look at it when 7.8 wasn't in "any day now" mode and folks weren't frantically scrabbling to reduce scope. Defaulting to `Nothing` is safer, defaulting to `Just` does the right thing for more users, but either means special casing some things in GHC to define fmapCoerce automagically based on the role of the last argument or makes authors of "evil" Functor instances add a line opting out of coercions. There are a lot of points in the design space, and I've had a hard time getting a calm discussion started on the topic.
Senior enginieering management who has coded in a bunch of imperative langauges (including asm) all the way back to Fortran. He's heard a whole lot of stuff about Haskell and is kicking the tires on it. I'm currently working on an motivating example where Haskell has a really neat composable solution where regular languages like C++ and Java cannot easily provide a solution.
Unfortunately such a design gives you zero benefit if you write reasonably complicated code (complicated enough not to INLINE) that is at all parametric, hence the need for something more.
`=&lt;&lt;` often solves that need for me: print =&lt;&lt; getLine
That is one option in the design space. Another option is to add more structure to the roles, so you can have things like `StateT s m`'s next argument has a as `representational` role if `m`'s argument is `representational`. That serves a related purpose, but makes roles more like classes. Then the witness can permit the lifting of `Coercion a b -&gt; Coercion (f a) (f b)`. A similar stronger class can permit the lifting of phantom args sans coercion, recovering the distinction and as it is 'open' to subsequent addition of such instances follows the open world model of the rest of Haskell more closely.
Nothing requires a type theory to be computational. Take Russell and Whitehead's original theory of types from the Principia Mathematica. They filled 3 volumes with material in a non-constructive type theory and helped formalize the underpinnings of modern mathematics. I know a number of researchers who do a lot of work on classical mathematics in theorem provers by simply postulating the axiom of choice and/or what other logical tools they need. They can no longer run their programs, but they can still write their proofs. Type theory gives you a better vocabulary for many problems than other logical frameworks. The vocabulary is in many ways "the point". The fact that many type theories are computational is a nice side-benefit, but I can get the same computational benefits in the logic vocabulary just by limiting myself to a Heyting algebra or constructive logic or classical linear logic. As a computer scientist I like to work in a constructive setting. This motivates the design of many of the type systems, logical frameworks and categories we computer science types choose to work in, but type theory isn't just about computer science.
One way you can do this is using the diagrams backend for chart. http://hackage.haskell.org/package/Chart-diagrams-1.2/docs/Graphics-Rendering-Chart-Backend-Diagrams.html 
But neither are enough imo. What about if blocks? There none of them are enough. I really do think something like ! would have a place in Haskell. I would rather call it @ or something, as ! is already used.
Oh, I misunderstood. That's what I get for replying early in the morning - I edited my reply*. \* And truthfully I don't know *why* we don't have the `map coerce` rule already, because on inspection of the source we don't. I've casually asked Joachim - perhaps he will answer soon.
&gt; just logic which logic?
I don't disagree with the thrust of what you are saying, but my blurry vision doesn't see such a strong connection between Russell and Per Martin-Löf. They seem to be part of different traditions. I would like to be correct though :)
Yes, "literally" and "honestly" have different meanings. But the fact that "literally" *means* "not figuratively" doesn't mean that it can't be used figuratively, and both "literally" and "honestly" convey something like: "I am not being deceptive, please take the content of my speech at face value". Which is exactly why "literally" has the figurative *use* it has. Cf. "really", "actually", the phrase "I'm not kidding", the phrase "I'm not just joking", etc. They all have the figurative uses they have *in virtue of* their literal meanings. There's really no point to attempting to keep one word safe from figurative uses, and it betrays a deep confusion about language to think that just because a word has a certain meaning, it can't be used in a non-literal way that exploits that meaning.
That's fine as long as you don't mind getting the "effects" from both branches of the conditional. Usually when people talk about having a monadic conditional expression, they mean one where the effects are conditional, not just the values. Note that this is impossible with only `Applicative`.
The path is rather twisted. the typed lambda calculus was originally a russell style theory as i understand it, and somewhat of an afterthought. then for a long period with the "BHK interpretation" we did not have "propositions as types", but we _did_ have "proofs as programs" and the kleene program of "realizability". The PaT interpretation was finally named explicitly by Howard, through a revival of type theory, which hadn't been considered that interesting for a span prior. Howard's work, in turn, was deeply influential on Martin-Löf, etc.
Incidentally, I believe the paper you linked is incorrect about the historical timeframe; there are documented uses of "literally" as a general intensifier for figurative or exaggerated expressions dating back at least a hundred years earlier than what the paper claims. As is often the case with commonly heard "errors" of language use, the supposedly erroneous usage was well-established many generations before the people complaining about it were born. Many words have entirely shifted meaning in more dramatic fashion in that same length of time, so the fact that anyone alive today even *knows* that literally can be used both ways proves that it has not and likely will not become useless.
Dynamic language type signature: `_ -&gt; _`
how is this supposed to work? the whole function where a theoretical `ifThenElse` is used has to indicate that it might potentially use these effects. maybe i am missing something.
I'm not sure what you mean. The way you'd write a monadic version is something like this: ifM :: (Monad m) =&gt; m Bool -&gt; m a -&gt; m a -&gt; m a ifM p t e = do p' &lt;- p if p' then t else e Whereas the `Applicative`-based version is equivalent to this: ifM' p t e = do p' &lt;- p t' &lt;- t e' &lt;- e return $ if p' then t' else e' For those averse to `do` notation, the monadic version could also be written as `\p t e -&gt; bool e t =&lt;&lt; p` to ensure maximal unclarity.
Minor typo on page 10. g' &lt;- g y' x' The paper calls `f`.
As an update: this was actually just a total oversight and we're fixing it now. :) Shame on me for not keeping on top of this particular point (I thought it was already done). :( --------- (We literally were discussing this on IRC earlier after I posted, and separately at the *exact same time* it popped up on the mailing list, with about 10 replies in minutes.)
Jules, I like where you're coming from, but how about: # pointer to integer convert(::Type{Uint}, x::Ptr) = box(Uint, unbox(Ptr,x)) https://github.com/JuliaLang/julia/blob/master/base/pointer.jl
&gt; But no one, that I can find, is using Haskell. http://functionaljobs.com/ has a few offers.
You should send him a pull request: https://github.com/edwinb/eff-tutorial
I'm confused why this topic has 50% downvotes.
You basically prove my point. It's either dynamic languages (ruby, python) with modern sexy syntax and lambdas but lacking strong typing support or haskell. Where will poor java/c# programmer go? 
And for those adverse to bind: ifM b t f = join $ ifThenElse &lt;$&gt; b &lt;*&gt; pure t &lt;*&gt; pure f or with GHC 7.8's `Data.Bool.bool` we get: ifM b t f = join $ bool f t &lt;$&gt; b
I like selling the richness of types. Due to purity and parametric polymorphism, types say quite a lot about functions. This is nice as a selling point, because you can: A) Go over some API (e.g: Data.Map) and see how you can tell quite a lot about each exported name by only looking at the types. B) Think about some operation you want to do. e.g: combine two lists together into tuples, element-wise. Write down the type: `[a] -&gt; [b] -&gt; [(a, b)]` into Hoogle, and get the function you wanted, voila! C) Sell Haskell's concurrency primitives: STM, atomicModifyIORef (and how it facilitates purity &amp; laziness), cheap forkIO, async, etc. For example, people tend to be impressed when I show them that creating a pool of threads that all run some loop uses this code: threadIds &lt;- replicateM 20 $ forkIO $ forever $ do .. each thread iteration code here .. Many more selling points to sell, many mentioned by other comments.
I only found one there for haskell...in Singapore!
This page explains the backends: https://github.com/timbod7/haskell-chart/wiki/How-to-use-backends 
Which logic a type system corresponds to depends on the type system. For instance simply typed lambda calculus corresponds to natural deduction, System F corresponds to second order logic, System F_Omega corresponds to higher order logic, linear types correspond to linear logic, etc. See also http://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence
I think you have the arguments to `bool` backwards there. It should match the order of constructors in the type definition, like `maybe` and `either`.
Don Stewart's blog might be a good place to look then, especially the stuff about fusion allowing clear code with extreme performance. Also Simon Marlow's book on concurrency and parallelism has lots of great examples of how to make elegant, high performance and clear concurrent and parallel programs; these are areas I don't believe C++ or Java are very good with at all. When you can show him that writing a high performance network server just means forking a thread for each connection and being able to handle millions of connections per second. IMO, this is an area where Haskell really shines for practical work. Light weight threads, several different ways to use data structures concurrently (IORefs + atomicModifyIORef, MVars, STM, all assisted by purity and laziness by default).
Here's example1.hs modified to use the Diagrams backend http://lpaste.net/101169 to make an svg file, which looks fine in my browser.
I don't think that's going to convince anyone to be more interested in haskell. People already know that haskell is great for project euler kind of problems, but they don't know how it handles day to day io stuff.
If you want to "sell" haskell (or anything, really) don't forget to talk about some of the drawbacks as well, or you'll just sound like an evangelist and people won't take you seriously. For example, the community is many times smaller than even the ruby community, and the tooling isn't where it could be. Also, if you mean sell, as in use for a business, I don't think Haskell is there yet anyways except for extremely "progressive" companies. There's more than just the language itself when it comes to business concerns, like hiring, support, backwards compatibility, etc. As a company, I know C# and Java are going to be around for at least 20 more years, and I'll easily be able to find people who can work with my code base. What happens if haskell dies off/evolves into something new and cool, but not backwards compatible? Do I want to be dependent on libraries who are more cavalier about breaking backwards compatibility than Java libs? I don't think any of these issues are insurmountable. But for a company with a large, established code base in another language, I don't think it's worth porting existing code to haskell. And if the company has the entire code base on the jvm or clr, I don't think it's worth taking on the ghc and friends dependency just to make your life easier. Because in the future, when you leave the company, someone else will have to support that code. On the other hand, if you're a startup doing greenfield development, go wild. I think haskell is definitely mature enough for that
 from x in new List&lt;int&gt;(){2,5,6,9,2} from y in new List&lt;int&gt;(){6,3,6,7,3} where x + y == 15 select new { x, y} You aren't going to convince any .c# developer that haskell is worth using by showing them list comprehensons. And Java 8 has similar (though less powerful) capabilities.
What's your platform? OS X (for instance) requires a little bit of fiddling to get `cairo` to build properly. If that's the case, you need to pass `--with-gcc=whatever real gcc you have` to `cabal` when installing `gtk2hs-buildtools` and `cairo`.
Ah, my bad. fixed
I won't link my happiness to whatever tool I happen to be using. My joy happens when I tell my computer to do something, and it does. In many situations a tool like haskell can make that happen, but sometimes I want to write a new module for the control board on my friend's eHERMS homebrew setup, or write a script to automate setting up my user environment after I've reinstalled my OS again. Sometimes haskell would be a better tool, but the rest of the tool chain doesn't support it yet, and trying to shoehorn it in is more aggravating then using the existing tools. The projects make me happy, the tools are of secondary importance.
Languages and vocabularies we use form our thought process. Haskell is not "just a tool". And you are severely limiting your growth as a programmer by sticking to tools that pay your mortgage. &gt;Sometimes haskell would be a better tool, but the rest of the tool chain doesn't support it yet I write in haskell web applications, web services, automation tools. I create pdfs, send emails, print files, manipulate jpegs and tiffs, interact with databases and front end applications. I can assure you there's enough toolchain and libraries to use haskell in your everyday job. 
&gt;And i cannot possibly love writing a lot of boilerplate. I think the whole boilerplate thing is overblown. When I'm writing Java, I don't really miss the succinctness of haskell, and actually find its wordiness an advantage sometimes. What I do miss is ADTs, higher kinded types, and having side effects in IO.
You do this by assigning a piece of information to each lambda (lets call it a tag). Then you have some evaluation rule for how this piece of information propagates when applying a function. A very simplistic method would be to tag each lambda with an integer and have a rule that a function can only call other functions with a lower number. A more sophisticated method would be to have information structured exactly like the types in some type system (e.g. simply typed lambda calculus), and dynamically maintain the types of each expression along with evaluation. The preservation theorem of the associated type system induces a way to augment the dynamic semantics to propagate the type information along with evaluation. This is dynamic typing (tagging) with more sophisticated types (tags) than atomic identifiers. This isn't currently common in dynamically typed languages. The only languages I can think of are Julia and Common Lisp (?), which have structured types (tags) like Array[T] rather than just atomic Array tags that you find in other dynamically typed languages. I think this is how dynamically languages will evolve to be closer to statically typed languages, by having tag systems that closely resemble a static types of some type system.
I just played around with [TryJulia](http://forio.com/julia/repl/) a bit, and wanted to mention my findings here, as I found them interesting and didn't see something like them mentioned in this thread. &gt; If you accept that Julia has types, then clearly you must accept that it has dependent types. To me it seems that Julia really does not have something like dependent dynamic types. (A `dynamic type' being that what the Julia documentation describes as types): * Julia does not (seem to have) any other function types except `Function'; typeof((x :: Int64) -&gt; x) ===&gt; Function I also could not find anything in the docs on how to specify a specific function type. So, in particular, Julia does not have dependent function types. * Julia does not (seem to have) dependent records/sigma types. Trying to define type U n::Int64; a::Array{Int64, n} end yields an error. What Julia *does* have is parameterized types/type families (clearly). And the type Array{T, n} used as an example in the discussion is just that; Array is a type (constructor) with *parameters* T and n. In particular, T and n are independent from each other. What Julia also can do is compute types (actually, not so surprising for a dynamic language, IMO): f{T, n}(a1 :: Array{T, n}) = (a2 :: Array{T, n + 1}) -&gt; ... (note that f{T, n}(a1 :: Array{T, n}) (a2 :: Array{T, n + 1}) = ... is not allowed, either) It seems sensible to give f a dependent type like forall T n -&gt; Array{T, n} -&gt; Array(T, n + 1) -&gt; ... (and write it as a type annotation) but, as noted above, Julia's type for f is always `Function'. (Also, what I also did not see explicitly stated anywhere, is that the `type parameters' of functions like f{T,n} are actually (just) implicit arguments... which is a noteworthy feature for a dynamic languages, no?) Edit: formatting 
&gt; Type theory gives you a better vocabulary for many problems than other logical frameworks. The vocabulary is in many ways "the point". Thanks, I had not considered this. I was under the impression that type systems are isomorphic to the corresponding logic, except that type theorists very much like to have a computational interpretation per se, whereas proof normalization on the logic side is just a lemma to prove consistency. Apparently this is not the case, and there are other advantages of being a type system. Could you elaborate on these advantages?
I think it's mostly for the FFI where Haskell *thinks* you're doing IO, but you're really not. Then there are things that "break the abstraction" of Haskell—instead of being normal, high-level code, you have libraries that take on the semantic role of the compiler or RTS. I'm thinking of things like memoization, instrumentation or debugging: it happens completely behind the scenes of the Haskell code and does not (should not) affect its actual *meaning*. This second idea is pretty important and deeply related to the "benign effects" Harper likes to harp on about. I'm not sure I explained it very well though; I really have to sit down and work out a clear way of stating my thoughts on the matter.
The difference is in its *semantics*, not the implementation or effects. At a high level, `Debug.Trace` is clearly for debugging: that's what its name says, that's what its type allows and that's more or less all you could use it for. On the other hand, `unsafePerformIO` is for any sort of IO. Unsafely. It's a more general tool which clearly *looks* more general. This is just like the usual argument about `goto`: `return` and `throw` are just special layers over `goto`, and yet they're much better to use. No need to explain jumps just to introduce exceptions!
Documentation bug report: The code in [these](https://github.com/timbod7/haskell-chart/wiki/example%208) [two](https://github.com/timbod7/haskell-chart/wiki/example%202) examples (and perhaps others) stops abruptly during/after the second line of `price1`.
I think concurrency really hits home with some people. For example, I recently impressed a couple of people with the core of a serial terminal application that uses channels to communicate between threads. The core logic that implements this two-way channel is: incoming &lt;- newChan outgoing &lt;- newChan serial &lt;- openSerial "COM1" serialConfig reader &lt;- forkIO $ forever $ recvLn serial &gt;&gt;= writeChan incoming writer &lt;- forkIO $ forever $ send serial =&lt;&lt; readChan outgoing I like to point out the symmetry, clarity and brevity. And then, to send a message to the serial port from some other thread: writeChan outgoing (B.pack "content")
This example would be better if It wasn't so much in a vacuum. Like a real world example of where you needed to "write a small function that produces all pairs from 2 lists of numbers whose sum equal some specific number"
Thanks for that F# article - it was great to see some quantitative analysis of different codebases.
Senior management – and everyone on this thread comes up with language features and hands-on code examples. That's not the way to go. Even if it makes them like Haskell, they cannot do business decisions based on what they find likeable. Explain why Haskell will be good for *their* project. Maintainability is a huge point. And your motivating example is a good idea.
When we write `FILE_IO r`, we mean that the `FILE_IO` effects are available on the resource `r`. Hence the type of open &gt; [FILE_IO ()] ==&gt; &gt; {ok} [FILE_IO (if ok then OpenFile m else ())] indicates that by performing this action we move from a context `FILE_IO ()` where the trivial resource (unit type) is available to a context `FILE_IO {ok} SomethingComplicated`. `{ok}` binds the returned value of open thus meaning that `SomethingComplicated` depends on it. And rightly so: if we fail to open the file (e.g. because it does not exist), it does not make sense for a handle to it to be available for us to use. Now, by inspecting the returned value of `!(open name Read)`, we can learn whether it succeeded (or not) thus being able to perform `FILE_IO (openFile Read)` (or just `FILE_IO ()`) effects.
Is the inferred role going to be the "safe" default for abstract types (doesn't allow coercion of type parameters)? I'm a little concerned if I have to think about this new GHC feature every time I hide a type's representation.
I think you misunderstood something. 
*Are* there any imperative languages with the same degree of typing as Haskell, or even OCaml?
&gt; AFAIK it is not enforced. Yes, it is enforced. According to the Ministry of Home Affairs, there have been [185 convictions under Section 377A](http://www.mha.gov.sg/news_details.aspx?nid=MTAwOA%3D%3D-6U8KGA8qkeo%3D) in the period from 1997 to 2006, alone. As of 2013, the Singapore High Court is dismissing appeals that challenge the law. It's comforting to wish that bad laws like this will go unenforced, but this is just a wish: no personal opinion (even the personal opinion of Lee Kuan Yew) can trump a law. Until the law is repealed, the government is mandated to enforce it, and even though the police and justice system claim to not "proactively enforce" it, they still deal have to deal with all reports and prosecutions brought to them. How is a committed couple supposed to live their life together if any neighbor, or person that they meet, could potentially choose to bring the hammer of Section 377A down on them? &gt; As they say, you can shoot a Scot with a bow and arrow in York after nightfall. Not all laws make sense today, not all archaic laws have been removed from the legislature either. This is a folk story: no such law exists. (Neither do almost any of the other alleged archaic laws that are spread around in popular stories; if they actually existed, they would have consequences and spark outrage just like Section 377A does.) It does not help anyone to compare the stuff of urban legend to a real, actual law, that is convicting and punishing real people. I know that Section 377A hardly affects everyone, and that the vast majority of LGBTI people in Singapore do manage to get by without facing it, but I'm sure that's little consolation to the people who do get charged and convicted. I can only commend the Singaporeans who are putting their heads on the block to actively fight Section 377A in court: they'll surely prevail, someday, but until then, it's not a problem that can be swept under the rug or disregarded by any prospective LGBTI employee or immigrant.
I guess that is fair. It's true that the "real world" is full of non Haskell code with which one must integrate. Harpers gonna harp. :) Haskell already has a few "benign effects" not reflected in the type system. Particularly, the effects of memory allocation and lazy evaluation.
Not exactly sure what you mean by "typing" in this context, but Rust at least is an imperative language that has a very strong type system. 
I don’t understand that last step. I read the `case !e of True =&gt; …` as do b &lt;- e case b of True =&gt; … Are there no terms with type `{ [FILE_IO ()] ==&gt; {ok} [FILE_IO (if ok then OpenFile m else ())] } Eff e Bool` representing an effectful `False`? What about: t = do open … pure False What happens if I perform `case !t of False =&gt; close; …`, is that alternative well-typed? How did we witness the file having been successfully opened? edit: I don’t think I wrote that right. I’ll sleep on it.
Let's desugar the program and annotate it line by line with the type we expect the following subexpression to have. Notice how b is used in the type of the subexpression right after it is bound &amp; how case analysis on it refines the type by reducing the if expression: dumpFile : String -&gt; { [FILE_IO (), STDIO] } Eff IO () dumpFile name = do { [FILE_IO (), STDIO] } Eff IO () b &lt;- open name Read { [FILE_IO (if b then OpenFile Read else ()), STDIO] } Eff IO () case b of True =&gt; ============================ Begin Branch where b = True { [FILE_IO (OpenFile Read), STDIO] } Eff IO () do putStrLn (show !readFile) { [FILE_IO (OpenFile Read), STDIO] } Eff IO () close Final type of this branch: { [FILE_IO (), STDIO] } Eff IO () ============================ End Branch where b = True False =&gt; ============================ Begin Branch where b = False { [FILE_IO (), STDIO] } Eff IO () putStrLn ("Error!") Final type of this branch: { [FILE_IO (), STDIO] } Eff IO () ============================ End Branch where b = False Final type of the program (both branch agree so the thing is well-typed): { [FILE_IO (), STDIO] } Eff IO () If one were to try and close the file in the `False` branch, it would not typecheck because the available resource is `()` but `close` expects a resource `openFile m`.
`RoleAnnotations` does not use nominal as the default, which while it'd be more conservative, would also break all existing uses of `GeneralizedNewtypeDeriving`, so if you maintain safety by hiding constructors, you will need a role annotation.
Well, in that case rust is probably the best candidate - it'll hopefully get higher kinded types soon, which should put it somewhere near Haskell 98.
"It also seems a bit disingenuous to claim a dependent type system if only a few sorts of dependent type constraints can be checked statically." There is certainly precedent for using the term "dependently typed" for limited implementations. See, for example: "A Dependently Typed Assembly Language" by Hongwei Xi and Robert Harper, ICFP '01 Sept 2001. While the abstract and the body of this text do qualify the implementation as "a restricted form of dependent types", they clearly felt comfortable using a less restrictive tag in the title.
Technically, OCaml is imperative too.
It's a mistake to assume one can't do imperative programming in Haskell. Hence, why wouldn't System F already work for imperative languages?
Sounds like an ad to me. &gt;It’s easy to deploy snap applications on Clever-Cloud, and you can take advantage of all its great features: pay-as-you-go, auto-scalability, and great performance. Do you own/work for this company?
What's your take on an EventSource? http://www.html5rocks.com/en/tutorials/eventsource/basics/
No I do not. Just thought it was interesting.
You can do imperative programming in Haskell no doubt, but I feel your answer misses the point. In GHC much effort is spent compiling Core to STG code and then to C--, it would not make much sense to translate a language like C to Core any more than it would make sense to replace GHC's Core with a something based on a Turing machine just because it's possible. A language which doesn't have garbage collection or which has ubiquitous mutation would have a very different requirements than Haskell. I'm asking for a language specifically designed to fit imperative programming, something that might incorporate arrays, mutation or memory management. I apologize for not making this more clear.
&gt; If you want to "sell" haskell (or anything, really) don't forget to talk about some of the drawbacks as well, or you'll just sound like an evangelist and people won't take you seriously Upvoted just for this.
I don't think it's the interdependency that sets them apart, but rather that types can depend on values rather than only on other types. Take a function like reverse : (a:Type) -&gt; [a] -&gt; [a]. There is an interdependency between the parameters: if a = int, then the other parameter is [int], if a= string, then the other parameter is [string]. What sets dependent types apart is that this is extended to values, rather than only types.
That is interesting to note how snap responds to HAProxy. Do you know if Yesod does too? Also, the "don't log to files, use stdout" and such is suggested in the 12 factor app :-) 
It's not a regression if you don't use RoleAnnotations, correct? You just will have the same GND encapsulation issues as before, correct?
I can see 3 uses for the underscore character: It is used at value level for the wildcard in the argument list and now as type hole in the function body. This is introducing the use of a wildcard in the type signature, I wonder if there is a use for a wildcard in the type parameters?? That would close the loop nicely.
Without taking any stand on proper language use: &gt; I don't think I've ever been confused as to how someone meant the word "literally", Genuine confusion can (and does) occur. My SO once told me that two friends of hers had "slept together -- literally". She could have meant "they slept together, honestly!" or "they slept together, but don't take it the wrong way; they just slept next to one another". Could've gone either way.
&gt; Sure, but this is not part of the Scheme standard and more a way to hack at the internals than something that you regularly do. "In a memory safe language they cannot." "You're wrong; here's an example." "That doesn't count." FWIW, it's easy to do in Python as well (for user-created types anyway).
&gt; For me at least, monad-based substitution is much simpler than Bound! Well, since Dan's post doesn't cover variable capture it can be simple indeed. ;) 
You mean streaming HTTP responses? Those should absolutely be supported.
type-tag-set! isn't memory safe, so the premise of your argument is false.
Before GND would work, now the same GND instances would just break. Overall, it was going to be a pretty painful experience no matter which way the decision was made. On the other hand, I think it closes out the longest standing bug in GHC, so some pain was to be expected.
Are you sure ? ... In dynamic languages all types are inferred. So yes that is a typical signature of a dynamic language function.
"The sort of type system described in the paper seems like one that Julia could definitely benefit from, though" I agree that additional type checking options would be useful. The type-specific methods associated with a generic function in Julia are amenable to some type checks, though such checks are not built in at the moment. @astrienna has done some initial work along these lines at https://github.com/astrieanna/TypeCheck.jl. 
Just a follow up. The instructions on Haskell Platform Installer README might be useful to get started with a new GHC on Windows: https://github.com/23Skidoo/haskell-platform-windows Explains which dependencies and how to install them manually, build cabal-install and the platform.
I tried this with Control.Lens and woke up two weeks later in a dumpster, smelling like garbage, with no recollection of what had happened.
Dynamic languages do not infer types at all. There are no types to infer!
Would that be bad? I would love to see more commercial activity with Haskell.
It's kind of apples and oranges really. We aren't talking about compile time types but at runtime a language like python does have a strong sense of types. You can make any type quack like a duck but python does infer the type from its properties.
Is there a name for a "function" that isn't a function in the mathematical sense? Procedure? Subroutine? Something else?
Python has no type inference. Duck typing is nothing like type inference. Everything in Python just has type Object. Method invocation is a dictionary lookup. That's it. There is no notion of "type" involved. `_ -&gt; _` is not the same as a dynamically typed function. That function still has some type. The signature just constrains it to being a function, and nothing else. It doesn't say the function has to accept some universal type or that it returns some existential type. It doesn't say the function has to accept or return `Dynamic`. It says nothing about the types of the function's input or output at all. They still have types, and they are still inferred, they are still type checked, and they even still statically drive choices of type class dictionaries (impossible in a dynamically typed language like Python). Edit: It occurs to me that I might be coming off as a bit aggressive. I mean well, really.
I've heard both of those used, yes. I use the latter more often myself. 
I guess that I had always associated those terms with something whose sole purpose was to "affect" either the input(s) or some global state, but I think that there isn't a general consensus there... Maybe the real answer was right in front of me, and we can just call these things "impure functions"?
It's already really easy. foreign import ccall unsafe "cos" c_cos :: CDouble -&gt; CDouble foreign import ccall unsafe "sin" c_sin :: CDouble -&gt; CDouble foreign import ccall unsafe "tan" c_tan :: CDouble -&gt; CDouble foo x = c_sin x + c_cos x
afaik, its impossible to change titles on Reddit.
Julia does have dependent record types, you just didn't quite get the syntax right: type U{n} a::Array{Int,n} end This is a type family `U` with a single parameter `n` and a field named `a` which is of type `Array{Int,n}`, where the value of `n` depends on the specific type of `U`. One thing that throws people off is that there's no way to annotate or constrain what kind of value `n` is. We could add such a feature, but it's not really necessary. `U{String}` is a valid type, just one that you can't construct since you can't create an `Array{Int,String}` object to construct it with. We used to have function types, but they turned out not to be terribly useful, so we removed the feature. We'll probably add a more sophisticated form of function types at some point, but the interaction between dynamic multiple dispatch and function types is subtle and a bit complicated. If you dispatch on a function type and the argument that's passed is a generic function with lots of methods, some of which match the expected signature and some of which don't, it's not at all obvious what should happen.
I think the point of the OP's suggestion is that all the *other* non pure methods would be parsed automatically from the header file, so that the foreign import declarations don't need to be changed manually each time the upstream package changes.
I was responding to your counterexample. I wanted to illustrate that there is no (non-trivial) interdependency of type parameters in the type (a:Type) -&gt; [a] -&gt; [a] as it only has a single type parameter, namely "a". The same holds for (n:Int) -&gt; Vector n -&gt; Vector n and "n"; it would still just be a parametric type, if Vector n is parametric (which it would have to be, if it were a Julia type). EDIT: considering StefanKarpinski's response, I think you can define non-parametric types in Julia... by dynamically failing for some type parameters during construction. So my reasoning does not hold. 
Basically. What I'm looking for is something with the following benefits: * If a type changes only client code needs to be updated (no library maintanence) * If something new is added then it is at least useable without having to change anything (IO is usable even if not ideal) * no need to marshall between types (marshalling is mechanical and automatable, so have the compiler do it)
I was actually thinking about something that was dynamic at compile time. I suppose you could have c2hs run as part of your build. I assume c2hs has a naming scheme such that you can figure out what c2hs will create solely by knowing the names of the c function/structure/variable. [Edit] Does c2hs do purity, or rather detect impurity properly?
Ton of upvotes, no comments. I'll bite. As a non-Haskell user this seems somewhat painful. Not only do you have to add some BP to get the compiler to tell you the type, but you also have to wait for the source file to compile, ouch. Would not a full blown Haskell IDE be a great joy? I just hover my mouse over a given type and voila, I know it's type _instantly_, no annotation required. This is the single biggest blocker for me in seriously considering the switch to Haskell. I need to hit the ground running, not groping in the dark.
Fair enough. =)
I don't know anything about those two solutions beyond what I have quoted. But I don't see how any solution could possibly detect purity by looking at C signatures from C header files.
The reason this most likely has so many upvotes is because, traditionally, many Haskell programmers *do* use types to help guide them a lot when programming something. This new feature really does increase the amount of useful information the compiler is giving you when you learn to interactively program with it. It makes writing a program - in some cases - more like writing a question and the compiler helps you formulate the correct answer. But that's a very particular programming style in some ways which is hard to explain until you've written a lot of Haskell code (and I won't bore you with it now). Anyway, this is a good step forward too, because this feature is now integrated into the compiler, so tools can further take use of it. Then, we will be able to do things like you want. The technique in this blog post is *exactly* the kind of principled technique you would use in an IDE since you can use it easily for any expression! Also, the IDE can integrate with holes - so it could also use this information to highlight surrounding terms when you *do* use a hole explicitly. We also have interactive reloading with precise dependency tracking in GHCi, so recompilation time shouldn't be too dire. All the pieces are there - this is just the start to better tools supporting it all. In fact, I believe tools like Eclipse FP and FP Co's IDE both support this already (but using other means). I'm sure Emacs is inevitable. :) In general I agree we're pretty lacking in this area... I'm hoping people will utilize them in interesting ways. The feature in question here - typed holes - are very useful for this kind of work, and they are directly inspired by similar languages such as Agda, which have excellent interactive programming support (quite unlike an IDE in the traditional sense, but very powerful).
Marshalling is bitch though. Primitive functions with integer arguments sure. But how you gonna automatically handle strings, records, pointers to pointers, arrays? 
Then what do you consider a dependent type? It is my understanding that the second type would certainly be considered a dependent type, since the result type depends on the run time value of n.
This looks pretty backwards-compatible. Are there any downsides? Asking differently, why not just do that?
One thing that's irked me about ghc-mod and hdevtools to find type information is that either the program compiles and it works, or there is a compile error and you get *no* type information out. I understand this is a consequence of the way type inference works, but you'd think it should still be able to work out the type of a function that has no dependencies on the "broken" parts of the code. Are there any good ways around this, or to just be careful to always keep your program in a compiling state?
I believe this is pretty much precisely the kind of problem that deferred type errors aim to solve. It will allow your program to 'compile' in the sense you can still run it - but attempting to use parts of your program which do not type check will throw an exception. This will let you test 'correct' parts of your code and continuously load it, even if some module further down the chain has some type error. For something like an IDE, this is very useful - it allows you to still get type information even when certain parts of the program are incomplete. I'm not sure any of these tools are leveraging that work yet, though.
The first claim in the sentence &gt; In the case of parsers, *empty* matches the empty input string, &gt; while &lt;|&gt; is a choice operator between two parsers. is not true of any parser implementation I'm aware of. Generally, *empty* matches **no** input string, rather than the *empty input string*. 
&gt; since you can't create an Array{Int,String} object to construct it with Ah, I think I see your point. Thanks for the clarification.
Something like (n: Int) -&gt; C n -&gt; ... where C is a constraint that can fail/cannot be satisfied for some n. But I see now how you could define such Cs in Julia. Thanks for the discussion
"detect" was probably the wrong way to put it. More properly, what I would be curious about is if there was a way to tell c2hs "anything with these names are pure".
I suspect the biggest reason is simply that `Functor` is part of the core language, while the proposed solution is a non-portable GHC extension extravaganza.
Convention, I would imagine. It's not that hard to imagine marshalling arrays to Data.Array and pointers are usually IORefs (IORef (IORef Int)?) Our strings need marshalling to char*. Structures and records are basically the same thing. I will readily admit that coming up with the convention for arbitrary types might be itself arbitrary. 
Yes, Control.Lens is quite a special case :)
not a static type, but with a dynamic type tag and an existential.
assembly language is not type safe. I don't think describing it as "unityped" is fair to dynamic languages. 
I'm pretty sure the dynamic refinements are in ADA 2012, but mostly just hypothetical.
In a mathematical sense, they are not functions at all, though.
But you won't be reaping all of the benefits of doing so.
Whenever structs are involved, doing direct Haskell FFI is very painful.
Thanks, helpful reply. Haskell seems to have a huge amount of momentum going, thus my interest in making the switch (from Scala). Sounds like the IDE story is evolving (saw FP Complete has a pretty impressive web based IDE).
However, I think it is telling that it took being introduced to Haskell for Steve to become exposed to and learn functional programming. Particularly since the programme he is writing is very mathematical in nature. Steve works with people who are trained in mathematicall thinking, solving very mathematicall problems, and yet their culture does not acknowledge the value of the functional style. It is true that you can write functional code in other languages, but what are the impedimants? At what point are you fighting the language? At what point are you fighting your co-workers? Edward Kmett has been vocal about how working in Scala, a language that ostensibly supports functional programming, makes him feel a lot of frustration when trying to express functional idioms. I whole heartedly agree with this assesment, and find myself cringing every time I turn to mutability and inheritance in Scala to avoid climbing down a rabbit hole. So, maybe Haskell is not the only language you can write functional code in, but the culture, libraries, and language features encourage it in a way you won't find many other places.
Given a machine state as input, then produce a new machine state as output. It's just those pesky implicit arguments.
**WARNING** - This is ~~probably~~ definitely wrong - see the reply. `Data.Set` is an ordered container. Ignoring the implementation, it's basically a sequence of unique values constrained to be in order as defined by the `Ord` instance for the contained items type. However, for efficiency reasons, the implementation isn't a simple list - it's probably some kind of tree. Imagine we have the set `{ 1, 2, 3 }`. Now let's `fmap` that using the function `\x -&gt; mod x 2`. A functor isn't supposed to change the structure in any way except replacing the values in their existing positions, so that gives `{ 1, 0, 1 }`. That's not in order. Even if re-sorting as part of the `fmap` is valid (which perhaps depends on how you interpret the word "structure"), the elements aren't unique (`1` occurs twice). So the result isn't a valid instance of `Data.Set`. Also, since uniqueness in this context means no two values can compare as equal, but distinct values may compare as equal, there's also the issue of how to choose which value to keep. Basically, the promises made by `fmap` conflict with the promises made by `Data.Set`, and by polymorphism not all `fmap` users will even know that they're dealing with `Data.Set` so the interface and promises that `fmap` makes are what they're relying on. Arguably, you could make it the programmers responsibility to only `fmap` using a function that preserves order and uniqueness of the elements, but as I said, some callers won't even know they're dealing with `Data.Set` - and further out from that, the original caller may not know that the code it's passing a `Data.Set` to uses `fmap` internally. If you really want to reorder and rebuild the `Data.Set`, you can use the existing `map` function for `Data.Set` - which reorders the elements and may give a smaller set, but that's OK because it isn't bound by the promises that `fmap` makes. Also, it's possible to wrap `Data.Set` in a `newtype`, instance `Functor` for that, and implement `fmap` as `map`. I'm curious whether that is justifiable or not - whether it's OK to decide that the only relevant structure is that the result is a set containing the correct values, even if there's fewer of them and even if there's arbitrary choice of which value to keep from distinct but equal-according-to-`Eq`-and-`Ord` values. 
&gt;The reference implementation was written in Matlab, and was extremely slow. So slow, in fact, that it ran for a month and produced no output. &gt;During the tail end of 2013, I had been casually studying Haskell, mostly reading about it on the bus and programming in my mind. I took the opportunity to use Haskell to solve this problem. The results were amazing. What would have taken months to solve in Matlab solved in less than 4 minutes in a program written by an intermediate-level Haskell programmer (and, judging myself as intermediate may be generous). That, to me, doesn't suggest that Haskell made the program faster. Rather, all it suggests is that the Matlab implementation was exceptionally slow. Haskell is great, but it's not four orders of magnitude (2.628x10^6 sec -&gt; 2.4x10^2 sec) better than Matlab. A speedup as miraculous as that suggests that it was the act of rewriting the code that produced the speedup, not rewriting the code in Haskell, specifically.
My experience is that this happens because you can prototype new algorithms more quickly because refactoring is cheap (strong types) and data structures are cheap (garbage collection + large library of purely functional data structures). This is why it is important to compare what is possible for an average programmer in practice versus in theory. In practice, Haskell wins on performance for short-lived projects because you move much more quickly.
It should be noted that Matlab code is very hard to refactor and optimize because of how much the standard library relies on side effects. 
Well, then what's the point of a distinction between pure and impure functions? Is it just an attempt to "reclaim" the word function?
If they're fully deterministic, then that would be world state, not machine state. And if they're not fully deterministic (e.g: They use concurrency, or any fancy non-pseudo random generators), then that is not correct. Trying to fit that back into "function" (with some implicits thrown around) will not bode well.
If your job hunt strategy is to apply for fifty jobs and hope that you get one by accident then, yeah, you will be limited to mainstream programming languages.
Use `lens-family-core`: it solves all the common lens use cases with simpler types and much fewer dependencies.
The pieces started falling together as I went to sleep: the tutorial drew our attention to dependent effects not because there’s simply a dependent type in there — after all, in a DT language that’s fairly routine. Dependent effects are interesting because the relation is between the type of the effect and the effectful value itself, so to inspect the latter is to learn about the former. I would blame the small delay in comprehension on the (at first glance) cryptic syntax of effect types, but OTOH I’m sure it’s very convenient and compact on a more pragmatical level. I found the mental gymnastics of shifting the params around and filling in the type-level lambdas a little too much, given that the ‘sugarless’ type is already a challenge to kind correctly. To be fair I had no idea Idris allowed syntax definitions either. Today is going to be a great day!
Thanks ninereeds314, you have good points though some of what you're saying is not really true. A functor is supposed to follow the laws fmap (f.g) = fmap f . fmap g and fmap id = id, no more. This is true for sets as long as the law x == y =&gt; f x == f y. There is no notion of "shape-preserving", for functors. For instance it's reasonable to look at listToMaybe as a natural transformation, even though it throws away all elements but the first one. A Functor instance for Sets is completely reasonable, but it isn't because of type system restrictions. See e.g. www.haskell.org/pipermail/haskell-cafe/2010-July/080978.html or some of the comments in this thread. Essentially we'd like to have something like instance (StructurallyEqual a, Ord a) =&gt; Functor (Set a) where fmap = mapSet though we can't express this in Haskell. The issue of structural equality kinda got lost in this thread though.
It's actually not my strategy, and I've been gainfully employed for the past year writing embedded systems drivers as well as an accompanying IDE/toolchain for use in amateur/academic scientific instrumentation while I finish a double first in math/cs. Doesn't mean I'm not interested to know what is going on in industry around me though, and that's actually the reason I started looking at Haskell in the first place. That one job was for an SDN and network telecom company who ran their systems almost exclusively on haskell. But my main point is that you can be a happy programmer if you step back and see the language as a means to an end, and not an end in itself.
That's a good outlook. I am sorry about my comment.
I actually helped Steven a teeny bit with his code towards the end. ~ 100x of the performance boost was just the way that ghc 7.8 has very very good parallelism with low overhead
How many of those were because of homosexuality? Outrage of Decency is also used to lock up for a night drunk guys who groped a girl in a club (and THAT is very much enforced).
It is worth mentioning that the "BHK interpretation" is generally considered the *intended* meaning of intuitionistic logic/constructivism. Or at least in its most general sense. That is, the use of untyped lambda calculus is a good way of formalizing the basic ideas of "pairs", "tagged unions" and "computable function"
Hello. Post author here. I really appreciate this feedback and I think that I can address it more clearly. Also, this post (and posts 2 through N) has as a target audience people who are primarily mathematicians and operations research people. /u/efrey helped me get started with Haskell and he is absolutely right - by learning Haskell it took away my global variable escape hatch. Thanks for the feedback. I will address that.
No worries, reading back on my comments I sound a bit combative and didn't layout my position well.
Seems like a straw-man. Tools matter. Some tools alienate you from what's really going on; some increase labor by absurd amounts; some decrease one's ability to comprehend the system as a whole; some make maintenance and agility a nightmare. Choosing a row-boat to cross the ocean can rather hamper one's enthusiasm to do so. There are consequences to various narratives engendered by various techniques and technologies (such as the - as I see it - unfortunate implementation of "OOP" in C++/Java, which Alan Kay would probably not approve much of). Ruminating on these considerations doesn't mean that tools are necessarily "an end", here. The "end" remains (ostensibly...) the successful engineering of a solution to a problem or challenge.
That's pretty much how I see it as well. Posts that draw attention to commercial ventures because their use of Haskell is interesting are A-OK by me, even if it basically is an ad. If there's nothing terribly interesting about their use of Haskell and/or it seems like just a pretense for posting an ad, that will be downvoted and possibly removed. Hopefully people will realize that if a post gets a positive response at all, there's absolutely nothing to be gained by hiding their association with it. I wonder if I should add something to the sidebar about this since I've seen similar questions raised in maybe a third or half of the posts that are commercial announcements/job ads/&amp;c.
&gt; How many of those were because of homosexuality? All of them: the statistics are only for Section 377A convictions. &gt; Outrage of Decency is also used to lock up for a night drunk guys who groped a girl in a club (and THAT is very much enforced). This is definitely not the case. Section 377A's _"Outrage of Decency"_ subtitle is not an umbrella term, and has nothing to do with offences other than male homosexuality: it is just the section's shorthand characterization of "gross indecency with another male person". The text is quite specific. (For comparison, look at the codes around it: Section 377 is _"Sexual penetration of a corpse"_, Section 377B is _"Sexual penetration with living animal"_, and so on. These are all far removed from drunken groping (which probably counts as an assault on modesty under Section 354). You can see all the legal codes in context [here](http://statutes.agc.gov.sg/aol/search/display/view.w3p;page=0;query=CompId%3Ac834c73a-3531-48b0-8040-4450d41d1351;rec=0#pr377A-he-.).) 
Creating FFI bindings to C or C++ code the way you describe is probably impossible to do in the general case. In particular: looking at just a forward declaration for a C function, you don't know what the ownership policy of heap-allocated objects is. Whose responsibility is it to free the pointer-to-foo that your C function returns? Another difficulty: are pointer arguments to C functions inputs or outputs, or both? C header files don't codify this.
Most of the way through it. Has been really eye opening. Repetition of recursive vs list comprehension vs higher order function methods of solving problems has been a good supplement to LYAH. Also drives home evaluation by substitution, huge eureka moment there.
I tend to do my bindings by hand too, but I would say there is a problem. How do you know your binding is correct? Can you talk about memory or type safety of your binding? Of course one end is C so type and memory safety are limited by that, but a manually written FFI doesn't even ensure that you've mapped between the languages with the correct types.
Maybe something using libffi and inspecting debug symbols is a start? For example in http://lpaste.net/100052, we get `cspline :: DL -&gt; Int32 -&gt; IOVector Double -&gt; Int32 -&gt; IOVector Double -&gt; IOVector Double -&gt; IO ()`, which might be more convenient if it was `cspline :: DL -&gt; IOVector Double -&gt; IOVector Double -&gt; IOVector Double -&gt; IO ()`.
That sounds good to me. This one seemed egregious to me in that it mentions the company name 15 times in a page or so of text, and the word haskell only 4-5 times.
Great idea!
If by "superior" you mean "more powerful" then the answer is yes. But, there's still a lot of research on what a practical type system for dependent types should look like. Back when Haskell was designed there wasn't even close to being a dependent type system ready for practical use. Also, back then the awareness about dependent types was a lot lower. 
Dependent types allow for a lot more type safety. They're pain in the ass to use though. Type inference is very important and I don't think losing it is an option.
Well, I think that the experience of creating a language counts more than the results. :-) Also, the GHCJS story isn't that bad — I've coded up an Atom.io extension with it a couple weeks ago, with an one-liner as a workable HS-to-Node bridge: https://github.com/audreyt/markdown-preview-ghcjs/blob/master/render.hs#L16 
The general problem is undecidable, though I'm sure it can mostly be tackled through heuristics.
interesting. I'd never heard of Mercury.
Yeah, it's wrong. Thanks!
Something very important to understand IMO (and I was skeptical about limitation of type inference until I saw it in action), is that instead of inferring type, a language with dependent types like IDRIS can infer implementation. Of course you still have type inference but not on top level definition. 
Oh that is a great point that I was not aware of, thank you!
Isn't Haskell open to changes anyway? The whole "avoiding success" thing?
Dependent types are not turing-complete, at least doesn't need to be, e.g. Agda and Coq have decidable type-checking and strong normalization in type reductions, as well as some form of type inference (see http://www.cse.chalmers.se/~ulfn/papers/thesis.pdf, it is not general of course, general type inference is undecidable). There is also some work on "effective" type inference for ~~dependent types~~\^W\^W ~~[dependentish](http://lambda-the-ultimate.org/node/1566#comment-43875)~~\^W languages with "fancy type-level computations" which close to dependent types, e.g. http://adam.chlipala.net/papers/UrPLDI10/UrPLDI10.pdf. Btw, here is a quote: &gt; Type inference for CIC and other such systems is undecidable, seen via fairly straightforward arguments: general mathematical proof search can be reduced to type inference in such rich type systems, with unification variables standing for mathematical proofs encoded syntactically. This is the real reason (aka Entscheidungsproblem) for undecidability of type inference, I think.
&gt; if you want to use it and give it a new meaning in that context, based on some loose analogy with static types, then *define precisely what you mean*. I think this is *precisely* what the community of friends-of-academia should do on the issue: find out the precise semantics of the language feature of Julia, formalize it, and put out a precise, scientific, interesting discussion of its relation to what we already understand well. But of course it's a lot more work than playing circle-jerks on the internet. 
I have a related question. Could we have a full dependent language that features an inferred subset much like the inferred subset of GHC Haskell? After all, I can write a lot of uninferable code in GHC Haskell, from rank-n types and polymorphic recursion to what is essentially dependent typing realized through singletons and GADTs (although much more clumsily than in Idris, for instance). But I can still blithely use inferred code in a lot of my everyday programming. So why not both do type inference and dependent types?
http://stackoverflow.com/a/13241158 &gt; Haskell is, to a small extent, a dependently typed language. As well as new http://ghc.haskell.org/trac/ghc/wiki/TypeNats So it's moving there ;)
With all recent additions to GHC its version of Haskell has become dependently typed, even if there some power still missing. That said, due to early design decisions (e.g., separate name spaces for values and types), the Haskell syntax for dependent types can be rather clumsy. 
Yes, what you're describing here is exactly something that breaks the law x == y =&gt; f x == f y which does indeed cause problems for instance Eq a =&gt; Functor (Set a) ... if we cannot assume this holds for Eq. Since Haskell is often influenced by category theory, it would make sense to have a type class where this would hold, or indeed to be able to assume it for Eq, since the categorical notion of equality is "sameness". There are other relations such as 'isomorphic', 'congruent' that we could use in addition. (That is, to be able to say the values 3 7 are congruent modulo 4, but not equal). Part of my question was asking why there are no such distinctions in the typeclass hierarchy. Equipped with a notion of "observably equal", Set is certainly a functor, and promising that there are indeed no observable differences is no different from e.g. promising that we don't have both x &gt; y and y &gt; x when implementing an Ord instance.
You can do lots of type inference in a dependent language. Just not all of it, if you want convenient syntax. The more annotated your syntax is, the more you can infer. There are techniques for overcoming some bit of inconvenient syntax with conventional solutions, but I think there's a general tendency to want to avoid silent conventions in favor of explicitness.
Type inference isn't even undecidable in total languages. It's just often problematic because of ambiguity. Consider the pair `(true,5)`. What is it's type? Is it `(_ : Bool) * Int` or is it `(b : Bool) * (if b then Int else String)`? There's no answer to this, because the pair is insufficiently annotated. But what about the function `\{X : Type} (x : X) -&gt; x`? What is it's type? Trivial: `{X : Type} -&gt; (x : X) -&gt; X`. Lots of things can be inferred if there's enough annotation around. In fact, *everything* can be inferred if there's enough annotation. Our pair's type can be inferred if we use Cardelli-style pairs: `(b : Bool = true, 5 : if b then Int else String)`. There is a tension between inference and convenience. We can make conventional decisions whenever inference becomes problematic (e.g. let `\x -&gt; x` infer to the above function type, as Idris does) or we can just let the inferences fail, and put the burden on the programmer to compensate. But the fact that inference isn't *always* possible doesn't mean it can't be done *much* of the time, and it doesn't mean that the cases where it *can't* be done are problematic. It's just less convenient, that's all.
Right.
Was the original Matlab code serial and the Haskell version parallel using Repa or something?
something like that. I don't wanna write spoilers for Stephen's blog posts. But yes that + the parallel package on hackage were involved. there was also a cute use case of the reader monad that caught a very subtle bug in the "global config" formulation that it started with.
Many of the same techniques for type inference using in Haskell or Hindley-Milner are applied in DT languages but they no longer return unique values as often or guarantee an answer. Usually when programming in a DT language you're expected to have a dialogue with the compiler where it tries to infer as much as it can and you fill in the blanks to allow it to move forward. The upshot is that typically these "blanks" are legitimately places where you need to better specify your intent. The lack of complete inference drives you to be explicit in a larger, more expressive space.
This is definitely possible, but there are some issues. The biggest one is that dependently-typed languages usually restrict some very basic language features in order to preserve some basic safety/consistency properties of the type system. In particular, in languages like Coq and Agda, you can't write non-terminating code. So these languages don't include Haskell as a sublanguage. There are three basic options: - You can take a language whose theory we understand pretty well (like coq) and design a compiler that does good type inference for some subset of it. Actually this is already mostly the case for current dependently-typed languages. However, because of the various restrictions in these languages, there will be a number of basic features from a language like Haskell that are missing. - You can just extend a language like Haskell with dependent types, keep the type inference that exists for the Haskelly core, and ignore the fact that you're going to lose some metatheoretic properties that we usually want for dependently-typed languages. This is actually the direction Haskell itself seems to be going in. It's slow going though, because dependent types introduce a number of other issues (for example, how to compile dependently-typed code efficiently is still active research). - You can figure out how to have it all - design a new dependently-typed language that has the features they are usually missing, and still somehow has the type-theoretic properties we want. The problem, of course, is that no one really knows how to do this. The theory is very, very hard. There are some proposals, but the programming languages research community hasn't really found something everyone agrees on yet. All existing proposals have various issues. Research in this area is very active.
With singleton types you can connect the type and term levels. I didn't say it was a convenient dependently typed language, but it has much of the same power as a real dependently typed language.
Well the fact that we are good at computationally representing types and rules of turning them into one another means that even if we posit e.g. the axiom of choice or whatever else we desire (even constructively provable things we don't bother to prove) then we have a good mechanism for forwards-and-backwards chaining proof search, unification, etc. Recall we don't just have curry-howard, but curry-howard-lambek, so in theory we could do this all in categories too. But sometimes it turns out thinking computationally is nicer, sometimes categorically, and sometimes logically. Having a variety of notations and tools available, and the ability to transport results between them is very powerful.
Making these storable instances by manually copying each and every struct definition is quite tedious. It could all be automatic. 
As for trouble with using dependent types, perhaps I've had a lot of bad experience with Coq. As to inference being an important thing: try anotating every variable binding in Haskell. That'll be painful. 
Thanks Carter! Those are for some upcoming blog posts :).
Dependent types give you a lot a power, but with great power comes great verbosity. There are lots of type systems you can _check_. Haskell's is notable because of how much it lets you _infer_, while still offering an escape hatch of letting you push types down to check things that you provably _can't_ infer. This "infer what you can, check what you must" model provides an excellent power-to-weight ratio. Just throwing in dependent types doesn't necessarily work while preserving the inference component. There are languages like Agda and Idris that go farther into the territory of dependent types, but programming in those languages requires you do adopt disciplines that are much more verbose, harder to work with, and in my experience more brittle in the face of refactorings than Haskell. I'd rather continue along the slow measured path that we have been following, borrowing insight from those cultures into what works, and doing things that don't compromise our existing functionality.
OK but I've heard a lot from Haskell experts that the rule is writing the types anyway, because it makes code more readable and brings many benefits. So why is it so important to have inference all around right now?
The way I read it, they mean that CI can catch bugs if you write tests. I don't see any mention of 'scanning' for bugs, just 'monitoring'. 
Agda doesn't have inference per se, it's just a fairly mechanical filling in of types that can be determined from an outer type annotation. In particular Agda doesn't let me have unannotated top-level functions (but it's alright there, since we use Agda for proofs, and inferred "dumb" rank-1 top level types are pretty useless for proving). 
What I really like about Haskell's chances at successfully integrating features of dependent typing is that it provides a useful context of usability and performance that new developments must live up to. There are plenty of Haskell packages and extensions out there that have been abandoned because they didn't measure up in terms of what they offer users. This is incredibly valuable data! Haskell has enough users who are sophisticated enough with respect to static typing to try new things, and enough users who care about running code that, at the end of the day, if a particular architecture is seen as unwieldy, you don't have as much worry that it just didn't find its niche. Languages with less adoption make these experiments much harder to run. It is very hard for me to evaluate the application of a design in an experimental language if I'm missing too many supporting libraries, or if the compiler is immature, or if the build system is inadequate. As an example success story, consider the `vector` library. Its implementation is rather complicated, and if all you had to go on was its source code, you might be doubtful that it would hold together in practice. But it has been tested by fire, and shone through as a tremendously valuable addition to the ecosystem. I suspect that TypeNats will prove valuable in its own way, but the best part is that this success or failure will be determined by running code used to accomplish things other than demonstrating TypeNats.
Yay! Thanks for stepping up as organizer, /u/jfischoff. Looking forward to it.
Looks pretty cool! Will have to give it a try. I suppose it's still to early to have binaries? I can install GHC 7.8 from source, but you know...
I think it would make great sense to use _ instead of a in your example!
You probably want to use what is known as a "Free" `Arrow`, which you can think of as a syntactic representation of all the `Arrow` operations. See [here](http://stackoverflow.com/questions/12001350/useful-operations-on-free-arrows) for an example implementation. The idea is that you can use Haskell's `Arrow` operators or `Arrows` language extension to build up a Free `Arrow` which will represent the graph of components. This then produces a syntactic representation of the graph you just built, and you can compile that representation to Verilog.
I think this is somewhat confused with regards to what I was getting at. The BHK interpretation was formulated entirely independently of the tradition of lambda calculus! My point in raising it was to talk about how "proofs as programs" existed in both that sense, and then in the later notion of Kleene's realizability, prior to Curry-Howard, and prior to the revival/reinvention of type theory. Flowing from this work, Lawvere's categorical rendition of model theory was already able to give an account of constructive formulae and their meanings. Hence it was only a small step for Lambek to relate the categorical perspective to type theory once Howard's work began circulating. Furthermore, the BHK intepretation was (I believe) considered by its originators a fruitful interpretation of intuitionistic logic, but not necessarily _the_ interpretation, and from what I've seen, Kolmogorov emphasized that his work also made sense in a classical setting, etc. So the causality you present is a bit backwards. That said, the general formula is indeed BHK + STLC = Curry Howard, at least in a very general sense of the ideas involved, but from there to get Martin-Löf's type theory one had to come up with something more _powerful_ than the STLC to have types sufficient to capture the entirety of intuitionistic logic.
The reason I'm for being a bit stricter on not saying haskell is dependently typed is not about haskell, so much as other languages. There are ongoing debates over whether language x is dependently typed (not talking about julia or the dynamic world even), and the argument tends to be "well since we can do everything Haskell can [nb: sorta, not really] then if Haskell has dependent types so do we" and it tends to encourage people not getting the power of a full-spectrum dependently typed approach. Our go to example for "faking it" has long been type-level nats, but as those have gotten very usable in Haskell, I think its time to move the bar. Perhaps a genuine sigma type for a monoid, that carries a type, its dictionary, _and_ a proof of associativity and identity?
I agree, it does make sense! But the benefit of allowing it would be pretty minor and probably not worth the hassle of e.g. implementing it as a GHC extension.
Yes. I think the last sentence of my previous post gave the wrong idea. I intended to make two claims * BHK/"proof interpertation" is the intended semantics * BHK doesn't have to have anything to do with lambda calculus As to the question of it it was the intended semantics: for Brouwer this is sort of half true. Brouwer was interested in mathematics of the human intelect. It mattered that people could intuit the the construction. And so, BHK semantics is too platonist. The rise of the "BHK interpretation" as *the* interpretation is though pretty evident in the work of Heyting such as "Sur la logique intuitionniste." And I think Kleene. Perhaps though the definitive version of it is in Dummett for whom there can be no doubt that this is what is intended.
I love that whenever someone tries to hit Mark with pedantry (and I have done it), his reply is to cheerfully redirect to issue at hand.
btw, what happened to your IRC account? It seems to have erased itself..
The "witnesses" I am talking about are not type signatures, they are extra proof arguments. For example if I implemented binary search in Haskell, I would use the type binarySearch :: Ord a =&gt; [a] -&gt; a -&gt; Maybe Int And then I would add a comment saying that the input list must be sorted. In Agda, instead of relying on a comment, I can use the type system to enforce the precondition that the list must be sorted: -- simplified type binarySearch : (xs : List a) -&gt; Sorted xs -&gt; a -&gt; Maybe Int But now the user must provide an extra argument of type `Sorted xs`, that is, a value encoding a proof that `xs` is sorted according to some ordering "≤", and probably also a proof that "≤" is a total order. That's a lot to ask for.
its really really easy to write ffi stuff in haskell. I'm actually deliberately doing it in a very very mechanical way in my own libraries so as to make it very very easy to get new people to contribute. FFI engineering thats too clever means you'll never get new contributors!
With all that said, I wish you the best of luck for this project! The world needs more Haskell. :)
I couldn't disagree more. I like Agda, but I think there are fundamental weaknesses is all current dependently typed languages that haskell addressees. We are never going to lose haskell 98--and that is the point. Haskell + dependent types is a very different thing from a dependently typed language that happens to have haskell-ish syntax and features. With type nats you still get type inference for a huge subset of the language. If we have Pi and Sigma, we would still get type inference for massive subset of the language.
then don't do that.
This may be the stupidest bot I've seen...
ATS is an ML dialect. The comparison would be about the same as with Haskell, only with the addition that ATS programs will tend to run more efficiently and reliably.
This is a bug in the github wiki. The code blocks get confused sometimes with the sequence [[ ..... &lt;- . The bug has been reported to github.
Why do everybody think I'm talking about type inference? **edit**: Ah, probably because of the emphasis on inference in [edwardkmett's otherwise excellent answer](http://www.reddit.com/r/haskell/comments/20h12b/is_dependently_typing_generally_superior_to/cg3bwhz). For what it's worth, I think Haskell and Agda both have great type inference systems. To clarify my position, then: I am not worried about GHC implementors adding language features which would reduce its inference powers, I am worried about GHC implementors adding language features which would encourage a programming style in which proof objects are being constructed and passed around explicitly.
Why would I leave MY device un-rooted? It’s not even a computer if it’s un-rooted! Who in the world *likes* living in a prison??
You mean Mercury is to ATS as ML is to Haskell? 
No, Mercury is to ATS as Mercury is to ML, and ML is pretty similar to Haskell. There's not a clear way to make a valuable comparison that's not the same as how Mercury compares to Haskell. ATS is still a functional programming language. It's main defining characteristic is its type system, which is way stronger than Haskell's, but this doesn't make it any closer or farther from Mercury than its peers.
I don't want to be able to choose whether I do that or not. I want one language in which everybody uses very precise types and willingly implements proof witnesses, because users of that language think correctness is worth the extra effort, and I want a second language and community in which correctness is considered important, but not important enough to ask programmers to write down proofs. This way, depending on the needs of the project, I can use one language or the other, and all the libraries and language features will work towards the same goal.
Something like [BlueSpec](http://www.slideshare.net/mansu/bluespec-talk)? It's similar to FRP in that you describe guards and conditional behaviours, and the compiler takes care of scheduling those behavours in an optimal way. It also has a formal verification language built in, which can be thought of like unit tests for your circuit, e.g. Given that I see this input, verify that this output will go high in 2 cycles.
&gt; In particular, in languages like Coq and Agda, you can't write non-terminating code. Agda uses coinduction for non-termination, Partiality and IO monads can (and are, in [agda-stdlib](http://github.com/agda/agda-stdlib)) be implemented that way, so non-terminating and effectful code can be written in those monads, being separated from total code and type-checking process (just like pure partial code and effectful code are separated in Haskell using IO). Coq have something similar too (http://ynot.cs.harvard.edu/).
Indeed - Coq and Agda support coinduction to model non-termination. I was simplifying a fair bit. There are still some substantial differences though. When you model non-terminating code with coinduction, you use an entirely different set of tools and reasoning principles to define and reason about your code. This is very inconvenient in practice. The most common case for programmers is to write code they believe to terminate, but don't feel like proving is total right away. Coinduction is an unfortunate tool for this, since when you actually want to go back and prove that your code does terminate, you have to redefine it in a different way and change all your proofs. Moreover, coinduction really models **productive** nontermination. This is like a web server - it runs forever, but accepts requests and will always produce an answer in a finite amount of time. If you want to write code that you genuinely aren't sure will ever terminate, coinduction isn't really the right tool (for example, think about the GHC interpreter - if you ask it to evaluate a loop, it should really not terminate).
Oh so ATS is more similar to Haskell then? I thought it would be the other way around. I should've asked how those two compare Are there serious downsides to ATS' stronger type system? Judging by the preview available for this question it just seems to be that it's more complicated (harder to learn) than Haskell: http://www.quora.com/Programming-Languages/Why-isnt-the-ATS-programming-language-more-popular
It's one of the **worst myths** of dependent types that type inference is impossible. In most DTLs, you do need to specify the types of your top-level declarations, but once you have that, most everything inside may be inferred. The issue seems worse than it is because both Idris and Agda are less conservative than Haskell in their namespacing policy. In Idris (and I believe in Agda, too), for instance, you may have a type with a constructor of the same name. Unlike in Haskell, there is no phase distinction, so both of these are identifiers living in the same namespace with two different types. These kinds of issues are not by any means intractable. They are simply open problems in an underexplored area of language design. For instance, none of the major DTLs have yet experimented with partial type inference. One could imagine that type inference should be possible for a large subset of the language. At least, it should be for a predicative variant of System F. As you write a snippet of Haskell code, you ought to know what type it will work out to. (The cases in Haskell that you don't are typically a matter of Haskell having no `Type`-valued functions). Dependent types allow you to work backwards, inferring the program from your types instead of the other way around. Even the very primitive Emacs and Vim IDEs available now should convince anyone there is potential in this. It would be silly to suggest that DTLs are ready for any kind of commercial use, and if you're coming from that perspective, you're entirely right. However, it would be silly to confuse immaturity for inadequacy. Haskell has been cherry-picking ideas from the DPL community for a few years now, just as C# has been cherry-picking them from Haskell. Please do not discourage others from taking interest because of their lack of immediate pragmatism :)
It's also undecidable for Haskell with mild type extensions (RankNTypes).
I'd say the worst part about them is the sense of madness that overcomes you when you first learn about them. Once that passes, you can use as much as you need to get the job done the way that makes sense :)
It's a strange kind of madness, I guess. To mean it seemed to be a nice simplification (which it is) that made things infinitely more interesting.
Yeah. Haskell is quite the Kitchen Sink language. But for as many extensions as it has, most are trying to do what DTLs do out of the box. 
I think you read something I didn't say, or at least didn't mean to imply. My statement was in response to a question asking "Isn't Haskell open to changes anyway? The whole "avoiding success" thing?" as if the only reason we're not doing them is because Haskell is tired and old or doesn't want to be successful. I have a great deal of appreciation for these languages. My statements were about the fact that we really as a culture in _Haskell_ are not going to run out and pick up greater rigor about separating data from codata, checking if recursion is well-founded or productive, give up typeclasses for implicit passing, etc. simply because each of those requires us giving _up_ something to get something else, and trades off on that power to weight ratio poorly for _Haskell_. The brittle statement was actually an attempt to capture that Haskell sits at a remarkable sweet spot that lets you ignore many distinctions that lets you define a list type once and use it, rather than dealing with every thing you want to prove over it inductively, basically as a separate data type. Algebraic ornaments and ornamental algebras provide some insight into how he Agda community might reduce the 'brittle' nature of refactorings that I mentioned. That said, given the status quo, Agda code tends to refactor very poorly in terms of retaining any of the proof terms. Coq in that area holds up bit better because tactics, while awful, are more robust in the presence of refactoring. I write code in Haskell rather than Agda for most of these things because it is greatly easier for me to restructure my code and retain the properties I want to retain without all the scaffolding shearing off. Yes, I can prove less, but I also waste less time on administratively boring proofs that are just boilerplate as well. Idris is helping to show it isn't all just about theorem proving, and that there are practical reasons to write programs with dependent types, and yes, I have been known to turn to Agda to help write tricky code, and yes, I'm more than happy to keep stealing good ideas from the dependently typed community! Heck, Stephanie Weirich has been talking about just throwing open the kind system to get a universe tower in GHC for a couple of years now. If they can borrow enough machinery without losing too much of what makes System F_c^whatever make that make sense, it'll probably happen. I didn't mean to imply that dependently typed research isn't valuable. However, I also don't want to give the impression that all we have to do is reach out and pull all that stuff into Haskell, as a lot of it doesn't fit, and what we have has a great deal of value. A little bit of vaseline on the lens can occasionally let you take a much nicer picture. ;) 
Fair enough. &gt; A little bit of vaseline on the **lens** can occasionally let you take a much nicer picture. ;) You of all people would know know about that kind of thing :P
I just made this a restriction in my "randomPokemon" function: randomPokemon :: IO Int randomPokemon = getStdRandom (randomR (1,150)) -- make a json instance for pokemon and return that type! getPokemon :: Int -&gt; IO L.ByteString getPokemon p = simpleHttp $ "http://pokeapi.co/api/v1/pokemon/" ++ show p ++ "/" main = do p &lt;- getPokemon =&lt;&lt; randomPokemon print . fromJust $ (decode p :: Maybe Pokemon) https://github.com/codygman/pokemon-battle/blob/master/src/Main.hs edit: Though, "random Pokemon" should probably use getPokemon inside of it and be named getPokemonData or something.
I'm glad my response was appreciated! I'll answer as best as I can. &gt; what types of problems are better suited to logic programming? This would be a question better suited for a logic programming sub. The main one seems to be /r/prolog. The main use I've personally had is database querying. If you have a database of countries and borders, and you want know what countries border Germany, you can type, roughly, border(Germany,?) and get everything you need. I'm sure someone with more experience could give better examples. &gt;Can that same principle not be applied to Mercury or is it actually counterproductive and less efficient to try to use logic programming as compared to functional? Mercury specifically is a logical-functional language. As a comparison, in Haskell you can write the following to compute fibs; fib :: Int -&gt; Int fib n = if n &lt;= 2 then 1 else (fib (n-1) + fib (n-2)) In Mercury, you can write the following to do the same thing :-func fib(int) = int. fib(N) = (if N =&lt; 2 then 1 else fib(N - 1) + fib(N - 2)). While they use different syntax, neither clearly uses logical programming. Mercury offers a mix between the two, so you can apply your Haskell skills to Mercury programming if you want, and when it's best. Often, especially when dealing with large databases, logic programming is much easier to manage than functional. A mixture gives the best of both worlds. Programming well in Mercury means using both paradigms where they shine. &gt;is there a low level language similar to Mercury I figure I should mention that ATS isn't *really* a low-level language. Neither is C. "low-level language" usually means assembly. ATS and C would be considered high-level languages that are good at low-level programming. Is there a language like Mercury that does this? As far as I know, no. Logic programming implies an interface with some sort of theorem prover, which is necessarily high-level itself. There's no reason this can't do low-level programming, but I personally haven't seen it. There's probably a Prolog library, or something similar, that allows for this, but it likely wouldn't be portable. Hence, you couldn't make an OS with it. &gt;Since I'm mostly interested in web development to start You may want to look at [Elm](http://elm-lang.org/). It's a Haskell dialect for the web. It compiles to Javascript, and it all around pretty neat. &gt;A big part of my interest in this are pipe dreams of some operating system combining all the best ideas from Plan9/Inferno with Phantom OS being written in one of those languages. Jeez, I haven't thought about Inferno in years. I too dream of an amazing OS. One that is fully formally verified so that crashes and memory leaks provably never, ever happen. Unfortunately I know next to nothing about making an OS. I hope you the best in your endeavors!
If you are familiar at all with Scheme (and perhaps even if you are not), a great introduction to logic programming can be found in the book "The Reasoned Schemer". It takes a sort of Socratic approach to building up the concepts involved in logic programming from a very basic level. To a rough approximation, logic-based programming languages like Prolog are most helpful when your problem domain can be expressed as a set of logical propositions (statements of fact), some predicates the define relationships between them, and queries about whether some other facts are supported by the data in the program. You don't have to worry (in general, anyway) about how the program figures out the responses to queries; you just have to worry about modeling the problem in the form of facts and predicates. In practice, efficient logic programs do tend to require some knowledge of how the language will perform its search so that you can provide some hints for it to constrain its search space. Having said all that, it is hopefully apparent that there are some problem spaces where writing a declarative Prolog program is a massively more efficient way to encode the solving of the problem than to manually write out a program that will search the problem space for the solution. I haven't thought much about how logic programming might apply to operating systems, but a great deal of operating systems code is likely not going to benefit at all from a built-in logic solving system, and in that case you might end up with a language that looks a lot more like Erlang (which is actually a distant Prolog descendant) than Mercury. I don't know enough about Mercury to say definitively whether it would be useful for low-level OS programming, but I strongly suspect that it would have around the same level of applicability as Haskell--that is to say, you could certainly do OS-level things with it, but it is probably not suitable as a sole implementation language for a OS kernel due to the requirement for a bunch of runtime support libraries. A promising up-and-coming language that may end up being suitable for writing OS kernels is Mozilla's Rust language, which borrows mostly from ML and Haskell, but is designed with low-level programming in mind and can build programs that do not require a runtime support library at all. It does not have nearly as expressive a type system as ATS, but it's also a much higher-profile language and seems much less likely to disappear when someone completes a Ph.D. ;)
&gt; To a rough approximation, logic-based programming languages like Prolog are most helpful when your problem domain can be expressed as a set of logical propositions (statements of fact), some predicates the define relationships between them, and queries about whether some other facts are supported by the data in the program. You don't have to worry (in general, anyway) about how the program figures out the responses to queries; you just have to worry about modeling the problem in the form of facts and predicates. In practice, efficient logic programs do tend to require some knowledge of how the language will perform its search so that you can provide some hints for it to constrain its search space. That makes sense, but I'm more curious about the second part of my question. Can the same be said about logic languages that are said about haskell that using them when it doesn't *seem* easier can lead to unexpectedly elegant results and a useful new perspective / way of thinking about the problems. 
Should I send a pull request to use the arrow keys instead of QWERTY specific keys? Nobody seems to think about Dvorak users :-) (I likely won't, since I have too many other things to do, but I also intend to make a point,)
There's already [zeroth](http://hackage.haskell.org/package/zeroth); it just needs to be adopted and updated.
It's always a nice feeling when you find a game that "just works" in dvorak. It always makes me want to support the developer :)
Here is another 2048 clone in Haskell: https://github.com/glguy/TwosGame/blob/master/Main.hs
both are colloquialisms. "silicon valley" includes san jose, which is larger than SF by population....
&gt; That makes sense, but I'm more curious about the second part of my question. Can the same be said about logic languages that are said about haskell that using them when it doesn't seem easier can lead to unexpectedly elegant results and a useful new perspective / way of thinking about the problems. Logic programming is one of the few basic paradigms of programming that we know of, and learning it will almost certainly lead you to some new ways to think about problems. For a small taste of it, see the video here, in which Will and Dan live-code a quine-generator, a 'twine'-generator, and a type inferencer for the simply-typed lambda calculus: http://www.infoq.com/presentations/miniKanren
Its nice to see a real world example using machines
&gt; I too dream of an amazing OS. One that is fully formally verified so that crashes and memory leaks provably never, ever happen. seL4 is already this.
&gt; The most common case for programmers is to write code they believe to terminate, but don't feel like proving is total right away. Then {-# NO_TERMINATION_CHECK #-} / --no-termination-check on a per function/module/file/project basis can help. E.g. http://github.com/agda/agda-stdlib/blob/bd6b7c06a996e743b925e00ed0d09ca553402ffa/src/IO.agda#L40. &gt; If you want to write code that you genuinely aren't sure will ever terminate, coinduction isn't really the right tool Hm, I clearly remember that I was able to write such a code using simple coinduction. In case of IO/Partiality what you **produce** is IO/Partiality terms, there is ∞-arguments in its constructors, so there is no forced termination in (co)recursive functions automatically. Here is an example: http://github.com/agda/agda-stdlib/blob/bd6b7c06a996e743b925e00ed0d09ca553402ffa/src/Category/Monad/Partiality.agda#L43. You can write Haskell's forever function that way too, because of ∞-arguments in \_&gt;&gt;=\_ constructor: http://github.com/agda/agda-stdlib/blob/bd6b7c06a996e743b925e00ed0d09ca553402ffa/src/IO.agda#L32.
This was for [PCG](http://codegolf.stackexchange.com/questions/24134/create-a-simple-2048-game-clone), wasn't it? :)
&gt; Then {-# NO_TERMINATION_CHECK #-} / --no-termination-check on a per function/module/file/project basis can help. Sure - this turns off the termination checker. The problem is that, when we use this flag, we immediately give up all safety guarantees about our code. The logic is no longer consistent - we can't be sure our proofs are really proofs. The ideal solution (in my view) is a language that allows non-termination without giving up consistency. Sometimes I want to formalize something and check properties of it, but the termination behavior of the code is complicated. It would be nice if I could ignore its termination behavior and formalize other properties of it without giving up the consistency of my system. &gt; Hm, I clearly remember that I was able to write such a code using simple coinduction. In case of IO/Partiality what you produce is IO/Partiality terms, there is ∞-arguments in its constructors, so there is no forced termination in (co)recursive functions automatically. &gt; Here is an example: &gt; &gt; never : ∀ {a} {A : Set a} → A ⊥ &gt; never = later (♯ never) The example you cited is interesting. I don't think I agree that it's an example of actual non-termination, though. The # operator's meaning is to delay execution of its argument until forced (it's a bit like a thunk). So, this code should terminate, since there is nothing that forces the recursive call. Corecursive calls must be "guarded" by suspensions. 
One thing I think that should be done is create some way to support dynamic csrf tokens. This is needed to prevent replay attacks, although I think it would require the server remembering some state. Edit: Also, another auth library that supports bcrypt for hashing passwords would be nice. 
At Docmnch we use selenium via protractor to test our Haskell applications. There are some advantages to doing this in Javascript (we use TypeScript). We create client-heavy applications and a front-end developer need not know Haskell to develop and test the front end. I know there are Yesod users using selenium via Haskell though and if you are programming your front-end in Haskell it is probably nicer to use the Haskell selenium bindings.
This is the third post about 2048 I've seen in the last couple of days. Have you guys heard of [Threes](http://asherv.com/threes/)? It's a great little puzzle game based on the same mechanic.
So, a linear function A —o B is one where you must use the A argument exactly once. The sort of thing you're talking about (a file's type changing as you do things to it, for instance) doesn't involve linear types, but rather dependent types. Idris uses this sort of thing to great effect. It is probably possible to combine the two, but I don't know much about that…
No, linear types are orthogonal to dependent types. AFAIK, you cannot encode linearity with dependent types. If you look at the Curry-Howard isomorphism then linear type systems correspond to those where sub-structural logic rules (eg weakening and contraction) are reflected in the types. 
You can encode linearity the same way you can encode anything else, with a typed inner language syntax that encodes your typing rules. It's a PITA, of course, but Idris has some syntactic sugar that makes that sort of thing less painful. Edwin has a loosely related example using an indexed monad to track resource usage.
Yes, if course, but implementing a new language with the feature you want isn't what I call encoding. By that reasoning it can be encoded in C as well. 
Oh cool, I've been working on an [OpenGL version](https://github.com/egonSchiele/ones) of the same! I might steal some of your logic, it looks more elegant than mine :)
LYAH is a very good book, but my book is a little more practical. For instance, I explain about creating real-life project, about cabal and Hackage. Also my book is shorter and, according to opinions of my readers, it's simpler to understand.
TIL: Ordinary programmers speak Russian.
I speak a bit of Russian and am actively learning. I actually really enjoy reading about Haskell in Russian, because the pedagogical repetition of material (as opposed to a novel or news article), and also juxtaposing examples of Haskell code, which I can understand, with explanations in Russian, which might be more difficult, works really well pedagogically.
If you're going to go to that extreme then you can encode it even without dependent types: http://okmij.org/ftp/tagless-final/course/#linear
Well, my point isn't to be extreme. Idris's embedded DSL functionality makes it reasonably pleasant to work with. But yes, if you have a reasonably powerful type system you can do all sorts of fun things with it.
Now if we could compile this to javascript and make it look like the original, Haskell will take over the planet. 
No, but extraordinary ones do.
It always freaks me out that by knowing one Slavic language you can read and understand all of them without too much trouble. But I'm not sure how topical this is for an English speaking website. Have you thought of a translation?
Talk to the workman users, it is even more so.
Oleg.
Along the same lines, but somewhat different and fancier: https://www.fpcomplete.com/user/mutjida/typed-tagless-final-linear-lambda-calculus
[Scheme specifications meticulously uses the term "procedure"](https://pay.reddit.com/r/compsci/comments/20bad0/julia_has_no_dependent_types/#cg24zdq)
Thanks for this! I'm just starting to learn haskell so I'll give this a read. So far it looks well written!
`$dNum_rn6` is the dictionary for `instance Num String` (which doesn't exist, thus the call to `runtimeError`). Your program is well typed in the presence of that instance (as opposed to if the definition of `foo` was `"bat" + (12 :: Int)`, which is never well typed. So the core says: foo = (+) -- :: forall T. NumDict T -&gt; T -&gt; T -&gt; T @String -- type application, for the 'forall' $dNum_rn6 -- :: NumDict String, the dictionary missing error "bat" (fromInteger -- forall T. NumDict T -&gt; Integer -&gt; T @String $dNum_rn6 -- again, dictionary missing 12) -- the actual integer 
Speaking of web testing, it would be nice to have a Haskell alternative to [JMeter](https://jmeter.apache.org/) the Java load testing tool. &gt; Your hardware's capabilities will limit the number of threads you can effectively run with JMeter. It will also depend on how fast your server is (a faster server makes JMeter work harder since it returns request quicker). The more JMeter works, the less accurate its timing information may become. The more work JMeter does, the more each thread has to wait to get access to the CPU, the more inflated the timing information gets. If you need large-scale load testing, consider running multiple non-GUI JMeter instances on multiple machines. When simulating high loads, JMeter is often limited by the number of threads that it must create. A Haskell program would (theoretically) be able to generate a lot more traffic from a single machine.
You're right, _many_ Russian speakers read this reddit.
I'm excited to see it as well Carter, a friend and I have been hacking on some distributed matrices as a prototype (to fulfill a course project as well). I would be interested to talk about it and contribute when you are ready. I love the comment about Cydia's creator he is one of my good friends and he won't even tell any of his friends what he is working on until it's ready enough.
Thanks! Do you think the results from trying to use logic programming are generally as equally or more elegant and efficient as functional programming if you can get the hang of it? 
I don't understand how come persistent needs its own "drivers" and doesn't rely on HDBC. If HDBC is not powerful enough surely it could be improved or forked to become good enough. I think in java it's accepted that JDBC is the base and the drivers are at the JDBC level. All the ORMs then build on top of JDBC and then need only little extra support for any database (mostly testing and some quirks I guess). Maybe persistent developers feel they can get a little extra performance by bypassing HDBC and going low-level on their own? But I don't know enough on the topic, I'm mostly wondering out loud.
If you're open to the idea, we can start a community translation project. My Russian's a little rusty, but I skimmed your book and it seems like a good bridge for programmers coming from C style languages. EDIT: Interest in a translation seems to exist, so I've opened a github repo: https://github.com/jhenahan/ohaskell-translations
Agreed. And while Google's auto-translated version has some...*interesting* word choices in it, it's still pretty easy to get the gist of what the author means. Fingers crossed for a potential English translation at some point in the future, but for now: Спасибо, Денис!
From what I understand, Haste wins on code size and completeness (in the Haskell-&gt;JS direction), but GHCJS is improving all the time, so that may not be the case in the long term. I don't have any testcases or profiling results, though.
What kind of machine was this running on? On most of today's machines you can sadly only get a 6x speedup at best with parallelism.
&gt; I am thinking of a type system where the type of a thing can change depending on what happened to it in other statements/expressions. Actually, that's what's called a typestate system. For example, see [Aldrich et al. 2009](http://www.cs.cmu.edu/~aldrich/papers/onward2009-state.pdf). In linear type systems statements don't have type-changing side effects like they do in typestate systems. Rather, in linear type systems you have the requirement that everything is used exactly once (unless you use "exponentials" which allow you to escape the requirement of using things exactly once). Because you can only use things once in linear type systems, you can make use of linearity to fake certain kinds of typestate. For example, file handles can only be used once. Naively this should make them kinda useless since you could only read/write once. To get around that, the read/write functions will take in a file handle (thereby using it up) but will also give you back a "new" file handle (which you have to use later). Whereas the function for closing a file wouldn't give you back a new file handle— which is sort of like changing the type from open to closed. If you prefer something more immediately like typesate, then you can give back a closed file handle (where the closed-file-handle and open-file-handle types are different). However, linear types only let you capture that sort of typestate: threading things around à la the `State` monad. We can make our code look pretty by variable name shadowing, as in `case m s of (x,s) -&gt; ...s...` but this is just a nicety for humans. The old variable `s` and the new one are actually two different names, two different variables, which only look the same in our textual representation of the source code. In a true typestate system, the type-changing effects of statements happen *in place*— it's the *exact same* variable which suddenly has a new type after doing stuff. This means that the typestate changes can happen at a distance or in other threads. You might be able to code something up in a linear type system to be able to mimic that (e.g., store everything in `IORef`s so that changes can be seen from afar) but it's certainly not something you get for free.
&gt; I love the comment about Cydia's creator he is one of my good friends and he won't even tell any of his friends what he is working on until it's ready enough. That's pretty awesome! I spent a few minutes looking for that thread where he talked about it, but the google machine failed my 10 minute search. I definitely notice the phenomenon and I've met 1-2 very productive people who share similar viewpoints. 
I don't see how dependent types comes into it. Seems more like a typestate thing to me. (Though, of course, you might like your states to be defined via term-level values...)
You can definitely do it in non-dependently-typed languages (probably possible even in C); it's just most natural with DTs.
There are a lot of options besides JMeter for HTTP load testing. Have you tried used pronk?
I'm using haste with no problems. Since GHC 7.8 has not been officially released yet, I imagine that is contributing to your problems with Haste. Try installing haste with GHC 7.6.3.
Well, we're very likely to wind up with a nice universe tower in the internal theory at least. I'm not sure about if we'll get to the rest. JHC showed having a little bit of a dependently typed internal language can be a big win.
I wrote an even more minimal one: &gt; import Control.Lens &gt; import Data.List &gt; import Data.Maybe &gt; import Text.Printf &gt; import System.Random The board is a stack of rows of cells, each possibly containing values. &gt; type Board = [[Maybe Int]] `shift` shifts cells leftward and `f` crushes them together if they match up. &gt; shift :: [Maybe Int] -&gt; [Maybe Int] &gt; shift = take 4 . (++ repeat Nothing) . concatMap f . group . filter isJust &gt; where f (x : y : xs) | x == y = (fmap (*2) x : xs); f r = r Using `transpose` and `reverse`, we can `shift` the rows and columns of the board around in both directions. Note that `move` isn't total, and invalid input just crashes the program. &gt; move :: String -&gt; Board -&gt; Board &gt; move "h" = map shift &gt; move "k" = transpose . map shift . transpose &gt; move "l" = map (reverse . shift . reverse) &gt; move "j" = transpose . map (reverse . shift . reverse) . transpose Simple formatting. `s` displays a single cell. &gt; showBoard :: Board -&gt; String &gt; showBoard = unlines . map (concatMap s) &gt; where s Nothing = " |"; s (Just x) = printf "%5d |" x Given a board, find a random spot on it (using `randomIO`) and place a 2 there, or return `Nothing` if none could be placed. &gt; place :: Board -&gt; IO (Maybe Board) &gt; place b = do &gt; let ps = concat $ zipWith (zip . repeat) [0..] $ map (elemIndices Nothing) b &gt; if null ps then return Nothing &gt; else do (y,x) &lt;- (ps !!) `fmap` randomRIO (0, length ps - 1) &gt; return $ Just $ b &amp; (ix y . ix x) .~ Just 2 Game loop: given a board, try to place a tile and game over if the board is full, otherwise print the board and let the user make a move. &gt; game :: Board -&gt; IO () &gt; game board = do &gt; board' &lt;- place board &gt; case board' of Nothing -&gt; do putStr "Game over! Score: " &gt; print $ (sum . catMaybes . concat) board &gt; Just b -&gt; do putStr $ showBoard b &gt; getLine &gt;&gt;= game . flip move b Start the game on an empty board. &gt; main = game $ replicate 4 $ replicate 4 Nothing 
I've been bouncing back and forth with xplat over the idea of a language where instead of simply being able to add a few intro axioms (and their corresponding automatically generated elims) via data declarations, you could instead define the whole type system. So that, in essence, you would do `open import DependentTypes` and you'd have the axioms of dependent type theory available, or you'd `open import LinearTypes` and have linear logic types available, etc. Forget functional programming, forget dependently typed programming, this is *type system* programming! Oh if only I could get off my butt and actually make it happen. :)
There are forks of zeroth that do compile: &lt;https://github.com/aavogt/zeroth&gt; for example. Zeroth only gets rid of DecsQ splices, so code that uses TH splices in other places (types, expressions) is out of luck.
Please do not suffer in silence. Hop on IRC channel #ghcjs or log an issue on github.
Well, I have not followed logic programming much lately, so I am probably not the best one to give a solid answer for that. I think it's worth learning about and gaining some facility with, as it can be *very* elegant for some things, and in a sufficiently powerful host language you can embed a logic language fairly cheaply to express those things it's a particularly nice fit for. I can't say anything about efficiency, though--it won't be very efficient if implemented naively, but I have no idea what the state of the art is like now.
Very cool, especially using `Lens` to modify the board. It does leave out two important things though: it doesn't keep score (points are accumulated when tiles crush together), and you can't win (when a tile with value 2048 is created, the game ends in a win. yours always ends on a lose). The randomly placed tile is also supposed to have a value of 4 1/10th of the time.
Hey, thanks for taking the time to look at this! I had a shot at adapting your idea to my code this morning, and it seems to work quite well; though over the course of debugging I think I uncovered a problem with my `Monad` instance for `Type`, so I'll have to work through that before I proceed any further. The type variables are a little excessive, I know! The rough plan is to have another module which has type synonyms for versions of `Exp`, `Type` etc with all the identifier types (α, χ, x) set to Text, but for now I'm keeping them separate just to keep that differentiation clear in my head. Anyway, thanks -- your proof of concept is very helpful :-)
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**CHIP-8**](http://en.wikipedia.org/wiki/CHIP-8): [](#sfw) --- &gt; &gt;__CHIP-8__ is an [interpreted](http://en.wikipedia.org/wiki/Interpreter_(computing\)) [programming language](http://en.wikipedia.org/wiki/Programming_language), developed by Joseph Weisbecker. It was initially used on the [COSMAC VIP](http://en.wikipedia.org/wiki/COSMAC_VIP) and [Telmac 1800](http://en.wikipedia.org/wiki/Telmac_1800) [8-bit](http://en.wikipedia.org/wiki/8-bit) [microcomputers](http://en.wikipedia.org/wiki/Microcomputer) in the mid-1970s. CHIP-8 [programs](http://en.wikipedia.org/wiki/Computer_program) are run on a CHIP-8 [virtual machine](http://en.wikipedia.org/wiki/Virtual_machine). It was made to allow [video games](http://en.wikipedia.org/wiki/Video_game) to be more easily programmed for said computers. &gt;Roughly twenty years after CHIP-8 was introduced, derived interpreters appeared for some models of [graphing calculators](http://en.wikipedia.org/wiki/Graphing_calculator) (from the late 1980s onward, these handheld devices in many ways have more computing power than most mid-1970s microcomputers for hobbyists). &gt;An active community of users and developers existed in the late 1970s, beginning with ARESCO's "VIPer" newsletter whose first three issues revealed the machine code behind the CHIP-8 interpreter. &gt; --- ^Interesting: [^COSMAC ^VIP](http://en.wikipedia.org/wiki/COSMAC_VIP) ^| [^Chiptune](http://en.wikipedia.org/wiki/Chiptune) ^| [^Telmac ^1800](http://en.wikipedia.org/wiki/Telmac_1800) ^| [^Sobriety ^coin](http://en.wikipedia.org/wiki/Sobriety_coin) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cg4f21k) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cg4f21k)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Oh and I should add, you can't even not use it, since if you use arrow notation, it will be compiled into `arr` calls all over the place.
No, it does not require linear types. I can make such an API in Haskell, or Standard ML for instance.
Being a native Russian speaker I am ready to help with translation.
Haste works fine for me (Ubuntu 13.10 - Default GHC). And (by the way) I am very happy with Haste!
Unfortunately not (commercial/proprietary code).
Agree. Haskell makes me think more up front, resulting in much cleaner and maintainable code. The more important and long term the code is, the more important a solid language like Haskell becomes.
Chiming in; I'm a native Russian speaker (though somewhat rusty and not technical). I'd be up for contributing to the translation effort.
I think GHCJS wins on completeness. 
I mentioned that. seL4 is only a microkernel, though. Things like drivers, the file system, standard programs, etc. aren't formally verified. I'm talking about a full BSD-level OS.
I mentioned that. seL4 is only a microkernel, though. Things like drivers, the file system, standard programs, etc. aren't formally verified. I'm talking about a full BSD-level OS.
Installing 7.8 RC2 and following the installation instructions on [github](https://github.com/ghcjs/ghcjs) should work well now. If you have any trouble please let us know.
Ah, I haven't tried installing it since the vagrant box was a thing. 
`shift` is the most elegant implementation of that logic I've seen so far.
Sweat. I've got some reading to do. Thanks.
I'm wondering how you can do the following without linear types? The below is ATS code. var i : int // uninitialized, [i : int?] val () = i := 5 // [i : int] now Note that variable [i] is being destructively modified which also changes its type. How do you do this in Haskell or SML? EDIT: formatting
Oh, the problem is that you require the type of an actual variable to change scope-to-scope. That's difficult to achieve and almost certainly requires some substructural trickiness like linear types. I'm talking about something which is much less difficult: imagine you have a resource modality `FileProgram s` where `s in { Open, Closed }`; it's quite easy to make it so that you could run `open` only on `FileProgram Closed` and `close` only on `FileProgram Open`, etc. The reason your way is "hard" is that it requires all the nonsense that accompanies using references for everything. References are tricky; linear types are often necessary to get the right. Achieving something similar in a pure fashion often requires a less advanced type system.
Lately I've used clojurescript, (clojure to js) and found it to be pleasing in many ways (not code size (not even minified)) Perhaps Haskell might be able to capture intent better. Funscript, the F# analogue, has also captured my interest, has anyone else got any comments on this?
Wow! I couldn't imagine that my book will be so interesting for my English-speaking colleagues! Of course, I'll help with translation. Thank you very much!
This is a funny place to ask that question, as persistent-odbc *is* using HDBC. Also, persistent in general may not be the right place to ask that question: a better question would be "if HDBC-mysql and HDBC-postgresql existed already, why were mysql-simple and postgresql-simple written?" And there are other people who are far better equipped to answer that question than I. I *can* speak for what I've done with persistent. I wrote two of the backends: sqlite and postgresql. For sqlite, I adapted the direct-sqlite C binding for the task. I actually had a lot of experience using the sqlite3 C API, and felt more comfortable with using it. For postgresql, I *did* initially use HDBC, until I was sent a pull request to switch over to postgresql-simple. I just realized you may be asking a slightly different question: why do we need to have separate -postgresql, -sqlite, and -mysql packages at all? The reason for *that* is that the majority of the code in each of those packages is actually all about database migrations, which are very much backend-specific. Creating a persistent-hdbc without migration support should be *mostly* trivial, though there are still some tricky backend-specific bits relating to getting the autogenerated ID of a newly inserted row.
Also, in addition to overhead, is lack of laziness a real problem? i've read that the one major difference is that haskell is lazy by default but mercury is not
Is there a good reason none of the examples seem to have types written? I feel very lost without them
The original reason was to show to people used to Verilog that you can have a hardware description language with type safety, but not necessarily with the verbosity of VHDL. I concur that the examples, especially the Calculator examples, could use more annotations. I've updated the examples now, thanks for the suggestion.
I'm curious about how Haskell is perceived among Russian-speaking programmers. Would you say that Haskell is becoming more popular among them than for English-speaking programmers? I always get the sense that there is so much software innovation going on in the Russian-speaking software community that is practically hidden for the rest of the world.
Something like this seems perfect for interactively exploring data from a repl while defining helper functions in a source file. This is something that has always felt a little painful for someone leaving behind Lisp/MATLAB/Ruby/etc, so its great to see this use case served by ghci.
`type Endo A = A -&gt; A` is not allowed, `type Endo a = a -&gt; a` is. type family F a type family F Int = Char type family F Char = Int has a copy/pase error (should read type instance) in the two latter lines. Also typo: "principle types" -&gt; "principal types"
Thanks for the close reading, fixed :)
you missed the last line (sorry my late edit)
oops, fixed too
Yeah, I think I've seen Shen. Part of my intention was really to find nice little places where you can define efficient aspects of the type system so that a fast type checker and evaluator could be extracted. The goal being a platform for playing around with alternative designs, an easily hacked type system to let you see what it's like to use such a language and gauge their strengths and weaknesses, not a system for real practical purposes.
&gt; Would you say that Haskell is becoming more popular among them than for English-speaking programmers? I don't think so. My impression is that the Russian-speaking community lags behind. There is some amount of people who are passively interested in Haskell, but very few work with it seriously. Only in the last few years was there some growth of Russian names on haskell mailing lists and hackage. And no, there are no Russian mailing lists of the same level as haskell-cafe (and there's no Russian hackage either). &gt; I always get the sense that there is so much software innovation going on in the Russian-speaking software community that is practically hidden for the rest of the world. Not really, at least if we're talking about open source software. Perhaps the only really successful open source product that came from the Russian-speaking world I can think of immediately is nginx.
I use Matlab daily at work. You can write mostly pure functional Matlab code (I mean, of course I use mutation all the time, but the functions themselves are usually pure). Also, Matlab can be pretty fast. Sometimes the standard library code is low quality and can be slow, but Matlab itself is not that slow. I guess in this case the problem domain was not well-suited for Matlab, and/or the Matlab code was bad. That said, Haskell is of course a much much better language, but Matlab, even with all its idiosyncrasies, can be a convenient environment especially for prototyping.
&gt; Perhaps the only really successful open source product that came from the Russian-speaking world I can think of immediately is nginx. 7-zip is well worth mentioning too. =) 
It'd be a pity to skip this, so I also wrote my [own version](https://gist.github.com/AndrasKovacs/9597994) which has the same mechanics as the "original" as far as I can tell. 
Regarding the footnote, I spend all my time in GHCi and hardly ever compile/run a binary. 
&gt; Snap has currently two issues: it logs to files You can pass "-" or "stderr" to "--error-log" and friends to get the logs written to stdout/stderr.
I am about to take Functional Programming module in university and was not sure, what to start with/how to prepare. Looks like a great book! Спасибо! I am pretty sure, that it will be quite popular after the English version will be created. Good luck with that!
Cool, I didn't know 7-zip was written by a Russian. Somehow archivers are popular here. There are also FreeArc (partly written in Haskell, but not very popular/widely known) and WinRAR (famously shareware).
AmaZon and other hosting providers have machines with 32+ physical cores. And you can buy such machines yourself if you have the hardware budget. (I know an org where every engineer is provisioned such hardware)
There's a few folks who hope to work out a garrow patch for ghc at hac NYC start of April. Mind you, that still means no garrow for 7.8 :-)
I'd suggest both [Pierce's *Software Foundations*](http://www.cis.upenn.edu/~bcpierce/sf/) and [Chlipala's *Certified Programming with Dependent Types*](http://adam.chlipala.net/cpdt/‎). Both should be pretty approachable by an intermediate Haskell programmer.
If I coread this, will I understand q?
Even the exponentials must be used linearly, in a sense. It's just that if the polarity is right one of the ways they can be used is by duplicating them. If the polarity is the other way around, using an exponential means handling any number of uses, rather than choosing some number of uses.
Very classy.
I must say this is really cool, a killer app on Emacs platform.
Gergö, I have 2 minor commits here, feel free to pull them: https://github.com/ggreif/kansas-lava/commits/master (Sadly ku-fpg does not react to pull requests.)
This is great, it also might come in handy for music live-coding with GHCi!
Of course it's not actual non-termination, since the whole point is to satisfy a termination checker. :] On the other hand, any potentially non-terminating expression can be transformed into a similar thunky corecursion. What this really amounts to is a sort of cooperative multithreading, where each layer of the delay type represents the "thread" yielding. This obviously only serves to punt on the issue of termination, presuming that the "thread" will be run periodically by some other code. that other code may itself be pseudo-partial and you can keep recursively delaying (ha, ha) a resolution to the issue until you reach a layer that's either truly corecursive (and can interleave evaluating the pseudo-partial term(s)) or recursive by way of an iteration limit. An example in Haskell would be trying to find the first element of a list satisfying some predicate; if no such element exists this won't terminate. So instead you can `map` over the list with a function that replaces non-matching elements with `Nothing` and wraps matching elements in `Just`. If the function that wants the value is suitably corecursive, it can keep pulling elements from the "filtered" list until it finds a match; otherwise it can `take` a finite prefix of the "filtered" list, use `catMaybes`, then pattern match on the (possibly empty) result list. This should work for any possibly-non-terminating expression, but in most cases there are probably better solutions than that sort of mechanical translation.
... wait, so Coq has no free monads?
I use Fay with Yesod; a nicely integrated solution that work for me.
Only if you are a coHaskell programmer.
Only if you cofree you mind. 
Using a type parameter for recursion isn't what makes something codata. That just makes it a functor. Codata is defined by its eliminators. For example: functions are codata since they are anything that supports the application operation. Products are codata since they are defined by the projections `fst` and `snd.` The mu fixed point combinator here is not even the corecursive one. It just behaves like it because in Haskell the least fixpoints and greatest fixpoints are conflated.
ejabberd is written by russian Alexey Shchepin in erlang. And it is one of the most used jabber servers out there. 
Another native russian speaker here reporting for duties :) 
Now i need to find a marker to write russian letters on my keyboard :) 
GHCi is my main debugger. 
where can I donwload atom ?
I realize this is an old post now, but I've been reading about `Coercible` today, and I have a question: I understand that `Coercible` instances are somewhat special in that they are generated by GHC. Are there plans to expose, through the preprocessor or some other way, the existence of specific `Coercible` instances at compile-time? I can imagine it would be useful to have the preprocessor swap an expensive type conversion for a free coercion if it just so happens that the types in question form a `Coercible` instance on the particular GHC version on the particular platform. As an example, I might want to ask (at compile-time): "Is there a `Coercible CInt Int32` instance for *this specific* version on GHC on *this specific* platform?". If the answer is "yes", I might want to convert between the two by coercion, otherwise I use (the non-free) `fromIntegral`. This sounds really useful for staying platform independent while enjoying the performance benefits of treating types with the same representation as the same. **Edit:** Or, alternatively, would a function such as coerceOrConvert :: (a -&gt; b) -&gt; a -&gt; b where GHC behind the scenes makes sure `coerceOrConvert f` is `coerce` if there is a `Coercible a b` instance, and `f` otherwise?
And on Thursday, April 3, a talk on Homotopy Type Theory as foundations: Homotopy type theory as a foundational framework: some philosophical remarks Jean-Pierre Marquis, Université de Montréal Thursday April 3, 2014, 7:00 - 8:00 PM., CUNY Graduate Center, Room 8405 365 Fifth Ave. (between 34th and 35th St.) Homotopy type theory, a new field at the crossroad of homotopy theory and type theory, is being vigorously investigated and is being presented as a new candidate for the foundations of mathematics. Too many logicians and philosophers, the latter suggestion seems dubitable, to say the least, if only because homotopy types are understood as being too abstract and complicated to provide a convincing foundational framework. In this talk, I will briefly discuss the various desiderata associated with a foundational framework and try to sketch a view that provides a justification for this new approach. 
It's one of the things that GHCi lacks compared to Erlang. This and process/state-inspection tools.
What happens if, besides adding functions, you change the definition of a datatype?
A nice read. You write `flatten (Constant i) = Constant i` where you most probably meant to write `flatten (Literal i) = Literal i` (the same hiccup is found in `applyExpr`). Also, `(f &gt;&gt;&gt; g) x` computes `g(f(x))` not `f(g(x))`. 
Fixed. Much appreciated.
Right! He also wrote tkabber, a jabber client.
Definitely appears to be broken now.
I wouldn't be surprised, since it was a massive backdoor :)
Does `Shape` need to be open to extension from third parties? If so, make it a typeclass. If not, have an ADT for `Shape`. I would make the ADT contain the actual parameters of the object, and make a separate `render :: Shape -&gt; Transforms -&gt; IO () ` function.
You can think of the data type as an enumeration of the possabilities of the shape, with a common `render` implementation, while the typeclass is completely polymorphic and extendable, but you'd have to overload the `render` function for each instance.
&gt; With the caveat that I have not tested this code. While this last version seems cleaner and just as extensible, somehow I feel it leaves you with less room to manipulate the shapes besides rendering, or maybe I'm still looking at it from the OOP perspective. No, in fact the datatype version lets you do exactly the same you could do with the existentially qualified version, only easier. Because as the EQ version was existentially quantified, there are only two things you could do with a `Shape`: construct one from an instance of `Renderable` and render a `Shape`, as anything else would be a type error(because the only thing you know for sure about the content is that it's an instance of `Renderable`). In the datatype version, the only thing you can do is make one from an `IO ()` and take the `IO ()` out to render it: thus it does exactly the same thing, but it's more straightforward(this is exactly why most uses of EQ are antipatterns: because you would think that they give you more power, but they make everything clunkier without winning you anything). And more or less the same applies for objects actually. At the point at which you have many shapes of different types in the same list, what on earth could you do besides rendering them? Because for it to make sense it must be something that you can do to *all* shapes: if it only applies to different shapes, you should do it before making the list.
When you've got something like class X a where foo :: ... bar :: ... baz :: ... data AnX = forall a . X a =&gt; AnX a instance X A where ... you've eliminated information about what the type inside of `AnX` was excepting that it implements `X`. In many circumstances this is exactly the same as data XDict = XDict { foo :: ... , bar :: ... , baz :: ... } forget :: A -&gt; XDict -- like `instance X A` excepting that in this latter situation you don't have to worry about weird type errors occurring due to escaping, forgotten type variables. You can see `forget` as expressing the idea that `A` is a supertype of `XDict` and thus we can `forget` (cast) though the relationship may be less obvious than a typical subclass-driven subtyping relationship would be.
Neither does Agda, but there are workarounds. https://github.com/jwiegley/notes/blob/master/agda-free-monad-trick.md
Are you sure in even in the OOP world such a "taxonomical" decomposition is common/standard? E.g Luxrenders scene graph has a few basic shapes, but mostly the "shapes" are primitives (meshes, nurbs ...) that you can build things with. [ http://www.luxrender.net/wiki/Scene_file_format_1.0#Shape ]. The same goes for Open Inventors shape hierarchy.
(Small note that the "Haskell syntax" link is broken due to a dangling parenthesis)
Try for instance emacs with haskell-mode. This lets you edit a source-file (or many) and Ctrl-C Ctrl-L to get it into GHCi with the edited file loaded. You do lose the definitions that you put into GHCi though unfortunately, so you'd have to go putting permanent things of interest in the source, but it should work ok for your use case. This would make it even better.
Don't drink the OOP-koolaid about not being mainly used "taxonomically", in reality it is the vast majority of the code. 
You mean something like `Shape = Circle Double | Square Double | ...` and then use pattern matching on the `render` function? Ideally it would be extensible. Is there a way to extende ADTs, or are they "closed" in this sense?
Your existential quantification version can be simplified by storing a list of `IO ()` actions that render each object directly and then running through it with `sequence_`. That's the problem with existential quantification being used in this way: you've already lost all useful type information, so you can't do anything but run `render` on it anyway. May as well remove the extra step and store the actions directly: class Renderable a where render :: a -&gt; IO () -- Data types, instances here renderList :: [Circle] -&gt; [Square] -&gt; IO () renderList cs ss = sequence_ ((map render cs) ++ (map render ss))
Hmmm thanks, I see your point, it does make everything a lot cleaner. Is that bit about EQ being equivalent a theorem or does it just happen to be in this particular use cas?
ADTs are closed on the types, open on the set of operations that can use them. Records of functions (or type-classes) are open on the types, but closed on the set of operations that can use them. I would not suggest a type-class here, because then you'd need to use existential types to have a hetero list of objects. A record of functions is much simpler and more versatile, and what existential type-classes reify to.
Thanks for the update!
Word on the street is that everything is back online now.
This is the right solution. Alternately just closing the world, and making constructors for the cases you need to handle, and then writing an interpreter that spits out the `foo`, `bar`, and `baz` cases, if you need more structure. The trade-off is between being able to do _anything you want_, but give only a few ways to inspect it or manipulate it, chosen a priori, or handle a few cases on a case-by-case basis. In general life is good if you can either close the set of things or you close the set of stuff you want to be able to do to those things. Otherwise, the expression problem rears its ugly head and you have to turn to more complicated solutions, e.g. class-based prisms describing a sub-set of the cases, etc.
&gt; However, this seems to be considered an antipattern (although I don't quite understand why). Neither do I. I wrote the Glome raytracer [1] using existential types for the primitives, and it works just fine, despite a few vocal naysayers who seem convinced that I'm doing it wrong. You can view the source on hackage [2]. Take a look at the Solid typeclass and the SolidItem Existential type if you want to see what I did. [1] http://www.haskell.org/haskellwiki/Glome [2] http://hackage.haskell.org/package/GlomeTrace-0.2/docs/src/Data-Glome-Solid.html
Use your heterogeneous list idea, so that you can recover lost information with unsafeCoerce. (disclaimer: don't) 
It's equivalent because it is more or less literally how typeclasses actually are implemented in ghc. 
Well, I've been accused of being a Haskell coprogrammer.
I don't believe there's a way to disallow it entirely, but you can just use braces and semicolons: do { line &lt;- getLine; putStrLn line; } This is not considered idiomatic unless you're resolving some kind of ambiguity.
&gt; Is there a way to turn off whitespace parsing? Yes: always put an open brace after `let`, `where`, `do`, and `of`. &gt; Also, I usually use tabs that consume less screen space. Most people don't use hard tabs in Haskell.
I don't currently use tabs when programming in haskell, but I'd prefer to. One issue I'm having is with `let` expressions. For instance, I'd like to write: let x = case ... of Just _ -&gt; ... Nothing -&gt; ... I can't do this though, and have to indent each line more like: let x = case ... of Just _ -&gt; ... Nothing -&gt; ... 
I normally use tabs, but I'm in the minority. 
With your `Shape` type, a list of renderables looks like this: [Shape (Circle 4), Shape (Square 5), Shape (Square 6)] However, it might as well just be this: [render (Circle 4), render (Square 5), render (Square 6)] No power is gained by using `Shape` instead, since all you can do with one is apply `render` to it anyway.
&gt; Yes: always put an open brace after let, where, do, and of. Interesting. I'll keep this in mind. &gt; Most people don't use hard tabs in Haskell I currently use spaces in my Haskell code, but I just don't really like them (which I understand is just my preference). My personal opinion is that spacing shouldn't be in a language's grammar, and your IDE can take care of displaying things nicely. 
Well, one possibility is brackets like haxly suggested. Also, I almost always use where statements instead of let statements for large expressions. Why would you prefer to use tabs? That can cause a lot of problems in Haskell and I'm not sure I see what the benefit would be.
I don't know many languages where hard tabs are still common.
Make functions or make variants: the eternal initial/final encoding struggle.
What does "closing the world" mean? Also, is there a book/article/dissertation or two that I could read to get a basic idea of this kind of terminology? I am incredibly interested in category theory and passionate about Haskell, but I am finding it difficult to "learn the language" of category theory. Thanks!
 interface Shape { void render(); } Is this really a good design, in the first place? I'm reminded of [Luke Palmer's blog entry on Denotational Design](http://lukepalmer.wordpress.com/2008/07/18/semantic-design/); it's not really about this topic, but it does make an off-hand argument that this may be a poor semantic domain for shapes or images anyway. In other words: don't think of the taxonomies, think of what all shapes have in common, and the operations you can perform on shapes—with emphasis on the *composable* operations, the ones that take shapes to shapes. The `render()`-based design means that the only such operation you get is this: /** * A monoid over Shape. */ public class Shapes implements Shape { private final ImmutableList&lt;? extends Shape&gt; shapes; public Shapes(List&lt;? extends Shape&gt; shapes) { this.shapes = ImmutableList.copyOf(shapes); } /** * Render the shapes in sequence. */ public void render() { for (Shape shape : shapes) { shape.render(); } } } The other point to consider is this: what does it really gain you to have the `Circle`, `Rectangle` and other such classes? Unless you're using downcasts from `Shape` to the implementation classes, nothing at all. If your code truly is generic over the `Shape` type, then you never need to reify `Circle` or `Rectangle` as types; all you need is functions that construct `Shape`s that *behave* like circles or rectangles. If your code isn't truly generic in that way, then arguably you ought'n't have the supertype; you ought to have a closed sum type of some sort that enumerates all the alternatives (or otherwise your program can't guarantee it's handling all subcases correctly).
I believe he's referring to the fixed sum type method mentioned elsewhere in this thread. It let's you write new functions over your data type in finite work but means you cannot easily extend the variants in your sum type without breaking all of the functions you've written in the past.
Okie dokie, that should be enough to go on. Thanks!
Losing pointers to the data you are working with in the repl is actually exactly the use-case I miss. ghci+emacs is otherwise very nice, though, I agree.
Was all excited because I got into the beta but then I realized it was currently os x only. Sigh.
As the other commenter said, it's more or less how typeclasses are implemented in ghc, but that's not it since in *also* would be like that in other compilers with EQ. The real reason is something called parametricity in polymorphims: basically, that if you have a function `forall a. Show a =&gt; a -&gt; String`, you know that the only thing that function can do to its input is to use `show` on it, since it must be something that works for *all* `Show a`, and the only thing they have in common is the `Show` typeclass. I hope it makes sense.
Filed https://github.com/valderman/haste-compiler/issues/144 (this all isn't needed in some future perfect world where cabal is supporting haste) 
Here's a question you might want to ask yourself. Pretend your OOP language has first-class function. Why would you have a `Shape` (when you don't have a more specific type) instead of just having its rendering function?
Oh this is much easier to understand, thanks :)
No, not really. "Layout" is not relevant everywhere though, and where it is (`let`, `where`, `do`, `of`), you can use explicit curly braces and semicolons. It's not considered idiomatic though, and you'll have a hard time settling in the Haskell ecosystem if you insist on this. I agree that it's one of Haskell's less fortunate design decisions (albeit less of a nuisance than in, say, Python), but the way things are, you're better off getting used to it. On the tabs vs. spaces thing: I'm totally a tab person normally, but for Haskell, spaces just work better. Layout syntax does specify how to handle tabs, and you can use them, but using spaces is explicit and avoids confusion. Just man up and configure your editor to use spaces for Haskell.
Nope. `(f &gt;&gt;&gt; g) x` is `g (f x)`. But you're right. The original article has a parenthesization issue. 
There is [fay-text](http://hackage.haskell.org/package/fay-text), which uses normal JS strings, which should be fast enough for most uses.
that might be a poor example. The following does not look nice at all. f :: Monad m =&gt; m a f = do let a = case ... of Just _ -&gt; ... Nothing -&gt; ... b = ... ...
how should a indentation-aware grammer look like when _not_ specifying spacing?
You can use a phonetic keyboard layout. I use the one for dvorak and feel myself comfortable with it.
&gt; Otherwise, the expression problem rears its ugly head and you have to turn to more complicated solutions, e.g. class-based prisms describing a sub-set of the cases, etc. Are there any examples around that illustrate the approach you mention? And how does it compare to Data Types a la Carte (which is the solution I would think of first)? 
Yes, when I used it, everything was in flux, so I rolled my own.
But as far as I understand, the dynamic dispatch table will be shared for all instances of a given class. OTOH, with the datatype version, each instance will copy the dispatch table. This is similar to the difference between "C"-style OOP and C++ vtable-based dispatch. In the vtable approach, the dispatch tables are shared. If the number of methods in the class is large, I expect that the EQ version will use less memory.
Can't we get a GSoC that converts the hackage package archive into a list of mirrors + sha2 of the package? Hackage is a security and availability nightmare.
Thank you for the notice. How hard would it be to improve the availability of the hackage database without compromising reliability and security ? 
Thanks, what does cabal not have that haste needs?
As other people have pointed out "procedure" is good in general. In Haskell though, we really have *multiple* kinds of procedures: `IO` and `ST` as well as more specialized ones like `Random` or whatever. I personally prefer to call these "actions", mostly with an adjective like "IO actions". So `getLine` is an "IO action" where in another language it would be a "procedure".
Ah, that's true. Good point.
Благодарю за отзыв! Я искренне считаю, что функциональное программирование не заслужило ярлыка "супер-сложное и для супер-умных". Почему-то языковые инструменты, присутствующие в Haskell, многими воспринимаются чуть ли не как сакральные тайны. "О, ты понял АФ? Ну ты крут! А ты что, проглотил монады?? Ну ваще!" А на самом деле речь идёт о красивых и полезных инструментах, которые нужно лишь объяснить по-человечески.
It's already a problem with Generic Newtype Deriving. import Data.Map class BoolMapLike k where fromBoolMap :: Map Bool a -&gt; Map k a instance BoolMapLike Bool where fromBoolMap = id newtype BackwardsBool = BackwardsBool Bool deriving (Eq, BoolMapLike, Show) instance Ord BackwardsBool where BackwardsBool a &lt;= BackwardsBool b = b &lt;= a badMap = fromBoolMap (fromList [(False, 3), (True, 1)]) :: Map BackwardsBool Integer `badMap` is an invalid map, and will not behave properly.
Can you give an example of these "weird type errors occurring due to escaping, forgotten type variables"? Personally, what this looks like is that you're maintaining the vtable yourself instead of letting the compiler do it for you. At first sight, it also seems to depend on currying the "object" into the "methods" (in order that the type of the functions doesn't need to know the type of the objects as an argument type) so you need a different "vtable" for every object. That suggests that if you're modelling a mutable object, you need a complete set of new methods (with the new object value curried in) replacing the vtable for every mutation. With an existential type, you just need the new value. And it's hard to compose a sequence of simple state changes into a complex transition function if a fixed start state has already been curried into all available state-change functions. I'm guessing that the argument against this value-is-locked-in claim is that the `forget` function can be passed in - using a higher rank argument type so the caller gets to decide which type to call it for when requesting the "vtable" / `XDict` for a particular object. Also, that `forget` function could be composed into your methods too - in effect, the `XDict` values become proxies for the underlying values. So you could e.g. thread an `XDict` state through the `State` monad. How you get back to the underlying state at the end is a bit of a puzzle, but you get that with an existential too. In OOP that's not so much an issue as you tend to mutate the object - references that see it's original unwrapped type see the value changes, so you don't need to unwrap to see the final state. Something similar happens in Haskell 98 even with replacement as the polymorphism doesn't allow the callee to choose the type. All you can really do with this `XDict` (or existentials) is have functions that let you query for details with known types. Anyway, passing around the `forget` function or currying it into the method functions doesn't sound like a hardship, but it looks like a potential efficiency issue - objects, thunks and some closures being produced to be immediately thrown away, with the `forget` function being called across module boundaries so it's less likely to be inlined and optimized. OK, premature optimization, but I want to understand what the disadvantages in the alternatives are. For the higher rank version, it's mainly passing that `forget` function around - an extra argument to whatever functions need it. For the existential version I don't understand what the problem is - I know it's considered an anti-pattern but I don't know why. 
I use intellindent.info -- which makes heavy use of tabs
I've seen a lot of people say the "tabs don't work well in Haskell" thing, but I've never had an issue.
&gt;`&lt;&lt;&gt;~` is first and foremost in the library because in a language that has robot monkey operators `(:[])` and Kleisli fish `&gt;=&gt;`, a rat operator must not be missing. Thanks for the laugh; also excellent document. This will be really nice next to the [Operators cheat sheet](https://github.com/ekmett/lens/wiki/Operators)!
&gt; ??: Obscure way to flip arguments. 
Actually ‘canonical’ (as you you ask) FP solution is data Shape = Circle Int | Square Int render :: Shape -&gt; WhatEver Notice that we cannot add Shape without modifying code. And in your OOP you cannot add new operations. Completely orthogonal. This solution is not ok every time obviously. 
That's great! Would it be possible to include a link to the documentation for each operators?
I think it would be useful it it worked on arbitrarily many arguments (similar to Mathematica's `#` syntax), but right now it's easily replaced by flip or infix sections, and has arguably surprising functionality when applied to non-function Functors.
*Weird type errors* Existential types provide a way of creating a "forgotten" type that the compiler ensures cannot escape from contexts where it is bound—you're no longer allowed to create values of that type, only continuations. Particular example where this goes wrong is &gt; ghci Prelude&gt; :set -XExistentialQuantification Prelude&gt; data X = forall a . X { unX :: a } Prelude&gt; :t unX &lt;interactive&gt;:1:1: Cannot use record selector `unX' as a function due to escaped type variables Probable fix: use pattern-matching syntax instead In the expression: unX So we've lost the ability to destruct `X`es in a positive fashion, instead we must CPS to control the scope where the escaped type variable can exist Prelude&gt; :set -XRankNTypes Prelude&gt; let { f :: (forall a . a -&gt; r) -&gt; X -&gt; r; f g (X a) = g a } Prelude&gt; :t f f :: (forall a. a -&gt; r) -&gt; X -&gt; r (As an aside, note that the only functions `(forall a . a -&gt; r)` for fixed `r` are `const`—another fun thing that shows up with existentials. It really just means that we "defined the class `AnX` without any methods", but it's a little unobvious what's going on in this presentation.) --- *Stateful vtables* I am not certain that the performance issues are terrifically high with passing around "stateful vtables" like this, but you're right to say that sometimes performance does drive a different choice. A good example is the `folds` or `foldl` library where types like the following are defined data Fold a b = forall x . Fold (a -&gt; x -&gt; x) x (x -&gt; b) which is equivalent to the non-existentially quantified data Fold a b = Fold (a -&gt; Fold a b) b but the former is better optimized by GHC, I believe, and thus ends up being preferable to the latter. It may also be the fact that it's relatively easy to write a function f :: (X -&gt; A -&gt; X) -&gt; X -&gt; (X -&gt; B) -&gt; R which has no dependency on a `Fold` type (or even any GHC extension requirements), then in your `foldl` library write purely :: (forall x. (x -&gt; a -&gt; x) -&gt; x -&gt; (x -&gt; b) -&gt; r) -&gt; Fold a b -&gt; r which runs a `Fold` using `f` in exactly the CPS style mentioned previously. purely f :: Fold A B -&gt; R This compartmentalization is harder to achieve without the existential context. --- *Getting the state back out* Consider a slight variation on this method with the following example data XDict a = XDict { stat :: Int, this :: a } deriving Functor forget :: String -&gt; XDict String forget s = XDict (length s) s We can see `XDict String` as a biased view of a `String` now, where `stat` has been pre-(lazily)-computed. So long as we only ever use `XDict` in homogenous contexts then we can also get the original "state" back out with `this`. If we want to use `XDict` in a heterogenous context then we are already giving up the ability to distinguish between different instantiations (supertypes) of `XDict`, so we can define coerce :: Functor f =&gt; f a -&gt; f () -- a.k.a. `shape` coerce = fmap (const ()) forgetInt :: Int -&gt; XDict Int forgetInt i = XDict i i xDicts :: [XDict ()] xDicts = [coerce $ forget "foobar", coerce $ forgetInt 3] using `coerce` simply deletes local, type-specialized information. This is basically unavoidable unless we dynamically inspect and case off of type information at a later stage. For this we need the `Typeable` machinery. I'll return to existential definitions to demonstrate that class X a where stat :: a -&gt; Int data AnX = forall a . (X a, Typeable a) =&gt; X a deriving Typeable instance X Int where stat = id instance X String where stat = length xes :: [AnX] xes = [X (3 :: Int), X "foobar"] xStrings :: [AnX] -&gt; [Maybe String] xStrings = map (\(X a) -&gt; cast a) &gt; xStrings xes [Nothing, Just "foobar"] --- *Antipattern* And this finally drives home the point of the antipattern. If you're going as far as to using runtime type reflection to pass a set of dynamic types through a heterogenous passage and then bring back some specific set of them into your type-strict machinery then you're working quite hard without necessarily reaping that much benefit. Much better is to try to find a way to pick one side or the other—either write a closed-world sum type and live on the "functional"/"initial"/easy-to-define-new-functions side or write from the typeclass/vdict/OO/existential type/easy-to-define-new-variants side and constrain yourself to having a particular set of destructors from which you must build all future functionality. This is the so-called expression problem and whenever you try to have your cake and eat it too things get really dicey. In the example above, I basically fall back to dynamic typing to achieve each side of the expression problem at different times. In the method that Edward mentioned before, you use a bunch of machinery to effectively allow you to have very flexible definitions of destructors of your types (using Prisms). There's also the Data Types a la Carte method where you leave both the initial and final definitions of your type open and flexible, smashing them together explicitly only at the last moment. Ultimately any of these methods work, but they're really heavy. The existential method gets banged up a lot because it's often chosen by default and without care to the strength-to-weight ratio simply because it lets you model OO in Haskell. It's not, in my opinion, necessarily much worse than other, comparable options, though.
I've always imagined it to be a shorthand for `Applicative`s &gt; (+) &lt;$&gt; readLn ?? 3 5 8 but it doesn't have the right fixity to work in the middle of such a chain &gt; let f a b c d = a + b + c + d &gt; f &lt;$&gt; readLn &lt;*&gt; readLn ?? 2 &lt;*&gt; readLn &lt;boom!&gt; &gt; (f &lt;$&gt; readLn &lt;*&gt; readLn ?? 2) &lt;*&gt; readLn 1 2 3 8 So I'm not sure if there's a better place to put it.
So does everyone here. I don't think we're as big a minority as it seems, since we all go out of our way to convert to spaces when sending pull requests to open source haskell projects.
So just use tabs. All our code is written like you want, and works just fine. No need for braces or semicolons, and no need to worry about stupid alignment problems. Just normal indentation.
I think it is because most people have never tried it, hear "haskell recommends spaces" so they don't realize tabs work just fine.
&gt; So we've lost the ability to destruct Xes in a positive fashion, instead we must CPS to control the scope where the escaped type variable can exist I don't see that as a weird error - I see that as getting what you ask for. If you've asked to hide/forget (the type of) what's inside your object, you shouldn't be surprised that it's hidden, and that you then have to live by the consequences of that. Basically, what I said about getting the state back out shouldn't be taken as meaning you *should* be able to easily get the full state back out - at least not complete and in its unknown type. Actually, I've realised I was being a bit of an idiot in my question. Rather than currying the object into all the functions in the `XDict`, you can have an `XDict` full of polymorphic method functions. That way, you can share the `XDict` value - you don't need another one for each object/state of the same type. With that, you *can* get the final state back out at the end even when you don't know the internals of the callee, providing your `XDict` uses functions of type `forall a. a -&gt; a`, the callee doesn't need to know `a`, but it automatically knows that the result has the same type `a` as the argument. So that is a case of managing the vtable manually, but getting an advantage - you know you can get the state out because you know it's the same type as you put in, even though it's not the same value. The restriction is that the caller has to decide the type and the callee has to respect that choice - ie again, you get what you ask for and so you'd better be ready to live with it. I said [here](http://www.reddit.com/r/haskell/comments/20nxjo/what_is_the_canonical_haskell_solution_to_the/cg5jx4q) that it's possible to reinvent dynamically-typed references - in this version, the plugin knows the details of the shape but the application only sees the interface - e.g. how to serialize. That seems a lot like existential types. I need some time to properly consider your answer, but I think I'm getting somewhere. 
Yes, that looks like a perfect example of an existential that accomplishes nothing other than making the code more complicated. Why do you think that's a good design? What problem does the class and existential wrapper solve that a record of functions can't?
I mean "weird" not in that it doesn't make sense but simply in that it doesn't show up too often. The intuition isn't hard to grasp, though. To be clear, I don't really mind existentials. There are some very close connections between dictionary passing, existential types, typeclasses. If you look at them hard enough you end up seeing them as all doing roughly the same thing. The real comparison point is versus sum types and dynamic type dispatch. [This paper](http://www.reddit.com/r/haskell/comments/1vlr84/jeremy_gibbons_unfolding_abstract_datatypes/) was for me one of the most cogent explorations of these topics.
It's not a big deal, really. Just some stuff that can be a bit counter-intuitive.
I've not seen that paper before - thanks. On "all doing roughly the same thing" - yes, that's why I usually end up talking about vtables - a reasonable assumption (but an assumption nonetheless) about compiler implementation that helps good intuitions. A lot comes down to who provides the vtable and how you know you're using the right one. 
I see a lot of people doing weird alignment things in Haskell, like: do thing thing2 Whereas I always write: do thing thing2 or: where x = 1 y = 2 Again, I always write: where x = 1 y = 2
A fun mixture of the solutions: class CanRender a where render :: a -&gt; Render newtype Render = Render (IO ()) deriving (Monad) listOfRenderable :: [Render] listOfRenderable = [render Circle, render Square] -- laziness is magic, no rendering work is done here renderList :: IO () renderList xs = let Render io = sequence xs in io
Yeah, the weird align-everything style some Haskell code has can be hard to read. If you treat the contstructs as needing indentation, it just works out.
AFAIK, none of these are necessarily the case. Haskell and specially GHC do too much optimization to allow you to make such statements.
Type classes are preferable when you are distributing the code as a separate library and it is not practical to have your users change the definition of Cipher if the want to add their own novel Cipher type. In general if a function can solve the problem just as well as type class method, use a function.
Yeah well, of course it works fine. It's just that it's equivalent to: data SolitItem = SolidItem { rayint :: Ray -&gt; Flt -&gt; Texture -&gt; Rayint rayint_debug :: Ray -&gt; Flt -&gt; Texture -&gt; (Rayint, Int) packetint :: Ray -&gt; Ray -&gt; Ray -&gt; Ray -&gt; Flt -&gt; Texture -&gt; PacketResult shadow :: Ray -&gt; Flt -&gt; Bool inside :: Vec -&gt; Bool bound :: Bbox tolist :: [SolidItem] transform :: [Xfm] -&gt; SolidItem transform_leaf :: [Xfm] -&gt; SolidItem flatten_transform :: [SolidItem] primcount :: Pcount } And this one is more idiomatic, it's haskell98, and allows better manipulation.
One OO-like compromise would to do something like data Circle = Circle Double deriving Typeable data Shape = { render :: IO () , object :: Dynamic } circleShape :: Circle -&gt; Shape circleShape c@(Circle r) = Shape { render = ... , object = toDyn c } which lets you dynamically cast back to the original type if you want to make changes, like, for example, double the radius of all circles.
Not just linux, most C code uses tabs.
The relevant announcement comes half-way down the page: &gt; Any engineer in the exact sciences should be familiar with the concept of using mathematical functions to model the world (through linear algebra, calculus or differential equations etc.). Around 1930, the mathematician Alonzo Church developed lambda calculus in an attempt to formalise the fundamentals of mathematics using functions. In doing so, he laid the foundations for understanding calculable functions or, more specifically, the use of mathematical functions in computer programming. &gt; Through one of the very first programming languages, Lisp, lambda calculus has since made major advances in the computer sciences. It now supports all modern programming languages (e.g. JavaScript, Scala, Visual Basic, PHP, C++, Mathematica and, recently, even Java), all of which consist of higher-order functions in combination with the imperative style of programming based on the Turing machine. In this MOOC, we will use the purely functional language Haskell to prepare participants for the modern practice of programming with mathematical functions.
I've been meaning to write it up. We use this approach a lot in the `ermine` compiler. It is easier to explain for state than the dual case here. You can work with `(HasFoo s, HasBar s, MonadState s m) =&gt; ...` using the `HasFoo` classes generated by `makeClassy` from `lens` very easily and composably. Unlike the data types a la carte approach it doesn't need overlapping instances, so type inference works, not just checking. I try to stay to the inferable fragment of Haskell as much as possible. The price is you need to make a couple of instances for the composite structure yourself.
For convenience, it's nice to write the type signature as f :: proxy t -&gt; B t -&gt; C t as now you can use whatever kind of "proxy" you have lying around, not just `Proxy` itself, incurring that dependency. f (Nothing :: Maybe Whatever) x y Although, obviously `Proxy` is easier to read.
Relevant [tweet](https://twitter.com/headinthebox/status/439425470388396032) by Erik Meijer. &gt; Haskell and C9 fans rejoice, I can confirm I will do a MOOC version of "Introduction To Functional Programming" on #EdX this fall. #TUDelft. 
That was fantastic. When's the next one? 
Mostly I just avoid `let` statements as there really is no good way to indent them that feels right, swapping them for `where` to avoid this situation.
The operator you're thinking of is `^.`: * `^.`: view a single value, as in `(a,b) ^. _1 == a`
&gt; forget :: A -&gt; XDict -- like `instance X A` I'm curious, how would you generalize this such that you can also encode `instance X B`? 
You're missing the operators in System.Filepath.Lens ((&lt;/&gt;) and (&lt;.&gt;) plus variants)
Nothing more complex than forgetB :: B -&gt; XDict If this becomes unbearable then you'd want to reach for a typeclass to get polymorphism, but at that point maybe you should just move to the typeclass-based method entirely. I'm not necessarily arguing one is the better than the other, just that all of these tools exist and should be traded off between.
Looking forward to it :-)
Very cool!
One of the things that bums me out is that not all lens operators can be composed. For example [(1,"Foo")] ^? at 1.non "" is a map lookup with default "" if the key is not found But if you try to do the same thing with indexing: ["Foo", "Baz"] ^? ix 3.non "" In this case trying to get the value at specific position in list, you cannot combine it with "non" anymore. You have to write it like this: fromMaybe "" $ ["Foo", "Baz"] ^? ix 3 
OK, thanks for clarifying. :)
I actually use Int for cells, and I find there's no need to group: shift2 :: [Int] -&gt; [Int] shift2 = take 4 . (++ repeat 0) . mg . filter (/=0) where mg (x:y:xs) = if x == y then (x+y):mg xs else x:mg (y:xs) mg r = r Therefore for me the shift function above can be further shorten to: shift :: [Maybe Int] -&gt; [Maybe Int] shift = take 4 . (++ repeat Nothing) . map return . f . catMaybes where f (x:y:xs) = if x == y then x+y : f xs else x : f (y:xs) f r = r Update: we can collect score like this: shiftAndScore :: [Int] -&gt; ([Int], Int) shiftAndScore = filter (/=0) &gt;&gt;&gt; mg &gt;&gt;&gt; first (take 4 . (++ repeat 0)) where mg :: [Int] -&gt; ([Int], Int) mg (x:y:xs) = if x == y then let (xs', sc') = mg xs in ((x+y) : xs', x+y+sc') else let (xs', sc') = mg (y:xs) in (x:xs', sc') mg r = (r, 0) 
This (minus the name) is what I prefer for most languages, but I've never tried it in Haskell (instead using spaces only). Have you hit any downsides?
While this is basically my dream job, I don't have nearly the differential equations skills necessary, nor the linear algebra for that matter. All I have is some experience with scientific computing in Python, a passion for higher math, and a love for haskell. If you're absolutely desperate then feel free to ask me, I just wish I had the full skillset you're looking for. 
The EQ approach gives a guarantee that the other approach does not. It relies on a "sufficiently smart compiler".
Same, hah. I'd say I'm waiting for the day a posting comes up for something less scientifically-oriented, but I know there'd be so many applicants that I'd never make it.
I don't think I've seen the "record of functions" idiom before, but my mind is blown. Thanks a ton.
Is it basically a judgment call based on whether you anticipate needing more flexibility for modifying either the datatype or the operations on said datatype in the future?
&gt; typeclasses when the set of operations is fixed and the set of types isn't I would prefer to suggest using a record of functions in this case, and only using a typeclass if both the associated plumbing becomes too tedious to stand *and* the typeclass abstracts a well-defined universal concept, not merely an interface.
Those monads don't seem very free, given how much structure you have to define first :) 
&gt; I've been meaning to write it up. I'm looking forward to it :)
Based on this, here is a version with arbitrary Monoids: You now play 2048 with Product or [42] :-) https://github.com/lordi/2048.hs
Ah, thanks.
I guess what I want is an option to modify the grammar such that it is not indentation aware. 
Hmm ok I guess I should give that a try. I think I was doing this when I started programming in Haskell, but ran into some problems. I don't remember what they were though so maybe they weren't that big of an issue. 
Huh. I honestly don't think I would have thought of using records like this. In my case, it should work perfectly. So I could have a function for each cipher (like genSubstitution) that returns a Cipher?
If you don't need the selection you could always also just do rot13 :: Cipher caeser :: Cipher aes :: Cipher usage: &gt; let x = encrypt aes "hello world" &gt; decrypt aes x "hello world" (due to "abusing" record syntax)
Yes that's nice. I don't even think that's an abuse. It's pretty much what record syntax was invented for.
You could have something like genSubstitution :: [(Char, Char)] -&gt; Cipher genSubstitution = ... where the argument specifies how to substitute characters (you have to make sure it's bijective!), and use it like rot13 :: Cipher rot13 = genSub [('a', 'n'), ('b', 'o'), ('c', 'p'), ...] -- Explicit for clarity. In practice you'd do it programatically! &gt; encode rot13 "hello" "urryb" 
This does sound close to a dream job too, hah. Only, after I finish this doctorate program. 
Thank you! I'm working on a follow-up :)
We sometimes hire a person who has other senior skills (say, a brilliant C++ and compiler person, or a brilliant devadmin) but only basic-level Haskell, and then let them learn Haskell on the job. Not this particular position, but it has certainly happened with other positions and will likely happen again.
Typeclasses are social. 
Possibly related: [ADTs vs. Typeclasses — Canonical use](http://stackoverflow.com/questions/13610164/adts-vs-typeclasses-canonical-use) (StackOverflow) Full disclosure: I asked the question.
The code in question: forkFinally action and_then = mask $ \restore -&gt; forkIO $ try (restore action) &gt;&gt;= and_then The `action` is run inside `try`, which converts exceptions to `Either SomeException`; the `mask` call is there to protect everything else. Also, the `restore` argument of `mask` doesn’t *necessarily* unmask async exceptions; it just sets the mask back to what it was outside the `mask` call.
github for the wiki? and gh-pages for all the packages on hackage?
I have a BA in physics and used to know all of that...and then I decided I like computers better than the real world and all of it is now gone :(
Ah, indeed it is late. Thanks!
Use { after certain keywords and there is no indentation sensitivity. 
Right, I was just pointing out that the exponentials change the rules from the basic "use exactly once" idea
That would confuse me so terribly if I didn't know about it.
That's equivalent in the sense that it provides the same semantics, but from a memory-use point of view it's definitely not equivalent. Those eleven closures require memory to store; in most cases, a more than the actual geometry data. Let's say you have a sphere, which is defined by the x, y, and z coordinates of its origin and a radius. In the ideal world, you would represent the sphere as something like four doubles. To use the sphere in a polymorphic context, you can't use the base representation directly, you need some kind of table of methods. The problem with the example you give, though, is that that each of those closures has free variables that are specific to the particular instance, so you can't share that table across all instances of Sphere. What you really want is a method table where the methods take the geometry as an explicit argument: data Sphere = Sphere Vec Double data SphereMethods = SphereMethods { rayint :: Sphere -&gt; Ray -&gt; Flt -&gt; Texture -&gt; Rayint ... } This way, you only ever need one method table in memory; it can be shared by all instance. That breaks polymorphism, though, because now the method table is specific to the type. We'd like to be able to just bundle up a primitive's geometry data along with a pointer to a method table shared by all instances. My understanding of existential types is that this is pretty much what they allow you to do; you can share that method table across all the instances, and only pay the memory costs for the actual data plus a pointer. (If the implementation doesn't currently do that, it clearly could.) There may be a solution that is memory efficient and doesn't require existential quantification, but I can't think of one off the top of my head. I'm willing to accept the argument that existential types aren't officially part of the language, and so if you care about writing standards compliant code that compiles with all compilers, they might be off the table. However, there are reasons to use existential types that aren't necessarily obvious.
See my other reply, but the tl;dr is: those records of closures take a lot of space. Existential types (in theory) give you a way to share the method tables across all instances of a type, thus saving a lot of memory that would otherwise be taken up by a lot of unnecessary pointers to closures and free variables. There may be a way to have a compact representation of the data without using existential types, but I don't know how.
This post is great timing for me, because I just finished RWH. Now I've got something to take its place for the next few weeks!
It's a shame he didn't examine the first section, because some of the libraries covered (Repa, to a lesser extent Accelerate) let you write very haskelly code and get decent parallel performance almost without trying. There is still some tweaking required, but I haven't seen it easier in any other programming language.
I've read this book cover to cover and I really like it! (I actually preordered it) I like the style and the approach to the subject Marlow uses. If you want a overview of packages and techniques in the haskell world for concurrent or parallel programming then this is the book to go to. If you want very detailed examples of how to use Repa under an old legacy codebase with older version of ghc/hp your out of luck though. (Repa usage under a brand new project with top of the line hp is a breeze though) If you've been using / playing with cuda then chapter of Accelerate should be of interest.
Nice review Bryan! BTW if anyone wants his book signed, come to [ZuriHac 2014](http://www.haskell.org/haskellwiki/ZuriHac2014) this summer where Simon Marlow and also Edward Kmett will be giving talks.
&gt;"Real World Haskell Of Six Years Ago Which We Should Have Edited A Little More Carefully" Anyone familiar enough with RWH to comment on the shortcomings he's talking about? I'm about to read it, will the errata not be enough?
Whilst this is a tempting objective, I would caution against re-inventing for Haskell (actually ghci) that which is already well catered for by Python etc. (with all the libraries that are in use for analysis: pandas, numpy, pytables, rpy, scipy, sympy, etc. etc.), but rather think about horses for courses. An interpreted "glue" language like Python, R is perfect for this kind of exploratory data analysis. But what is Haskell good for? Well my interest in it, (and DS/ML is the day job), is around stuff like pipes, lenses, and hlearn, llvm etc. -- that is to say algebraic approaches to high order composition of algorithms and transformations to data that can be efficiently implemented with fusion and clever compilation. I would therefore look at, say, designing a DSL for data analysis that leverages all that goodness and move the state of the art on in another direction. Take a look at HLearn for an example -- not as a ML tool box - there are plenty of those: but as an approach to using algebraic methods in the domain of ML. 
Well now's your chance to review the first half of the book. Between us, we can construct a Voltron-like giant mecha-review that will defeat its weaker review enemies!
You can write comments below each paragraph in the online version. There are some parts with a lot of useful feedback. The book is still incredibly useful for anyone who wants to use haskell and does not have access to a mentor. Some stuff are no longer valid (regex library, concurrency management etc) but it is not really a problem once you know enough to use the other online resources.
More details below. When you can both modify the types (add constructors to the ADT) and modify the functions over the types, you don't really need typeclasses. Because you have all the code in your hands, both your ADT and your set of functions are open to changes. Typeclasses become very useful when you want to build a library around some type properties and base functions, but at the time of writing the library you don't know yet what types will use it. The user will have the freedom to use his own types, and provide the instance for your typeclass. This is why typeclasses are social, they enable different people of the community to agree on type properties and interface, and have all this code work together. 
I have tried it in Haskell, and [have some comments](http://dmwit.com/tabs/) about how to do it well.
Simon's book really does deserve all the praise it's receiving. It's an excellent book!
Every time I read these blogs or comments about how the existential approach is an antipattern, I just cannot understand why. First I find it odd that people refer to the class based approach as object oriented and not the record based one. Based on [On understanding data abstraction](www.cs.utexas.edu/~wcook/Drafts/2009/essay.pdf) it seems to me it should be the other way around: the record based approach is OO (because it is based on procedural abstraction) while the class-with-existential approach is ADT based. That aside, here is (yet) some arguments for the so called antipattern approach. Suppose we want a library of shapes. The first approach may look like this: data Rectangle { width :: Double, height :: Double } data Circle { radius :: Double } We want to be able to render and get the circumference for every possible shape, so here is a type class capturing this: class Shape s where render :: s -&gt; IO () circumference :: s -&gt; Double instance Shape Rectangle where render = ... circumference = ... instance Shape Circle where ... If we, for some reason, want to store all shapes in a list for later rendering (or something) we could wrap them in an existential "AnyShape" type. All well. ----- The alternative, record based approach would look like this: data Shape { render :: IO () circumference :: Double } rectangle width height = Shape { ... } circle radius = Shape { ... } I am less in favor of this design because now we lump together state and behaviour just as we do with OO! We cannot create a rectangle and observe its state like we could with the Rectangle type above. We cannot derive instances like Show for these shapes. We can only observe their behaviour. Also I find updating their state awkward and error prone, because the state is not "spelled out" anymore -- you have to redefine all affected behaviours (functions) instead. And, yes, we could keep the Rectangle and Circle types and convert to a Shape type for rendering, but what's the point of this conversion when we could just make our shapes instances of a shape interface and call these functions directly? Edit: I don't understand how to make the link work. This markdown language seems to add some characters to it by itself :-( Edit: Link works (but ulgy). Additional comment Edit: text bugs
Anyone else feel like that lens exports way too many operators? I think it would be painful to try read a codebase full of these, especially the esoteric ones like !?
I've eschewed using `Lens` in my own Haskell because it seems like a bottomless toolkit^1. I find that my code is more verbose, but I feel like I would need to paste in a primer above any function that dealt with lenses over a `HashMap (a,b) (c,d -&gt; e,Map f a) ` or some such. What are other people's thoughts/experiences with `Lens`? [1] - (No type theoretic pun intended.)
`!?` and the pseudo-composition by appending `?` I get - it's something that I wish other languages had. But even some of the examples posted here I do not get. For example, one of the operators is defined in terms of `review`. What is review? Well, I looked it up: &gt; This can be used to turn an `Iso` or `Prism` around and view a value (or the current environment) through it the other way. &gt; &gt; review ≡ view . re &gt; review . unto ≡ id &gt; &gt; &gt;&gt;&gt; review _Left "mustard" &gt; Left "mustard" &gt; &gt; &gt;&gt;&gt; review (unto succ) 5 &gt; 6 &gt; &gt; Usually review is used in the `(-&gt;) Monad` with a `Prism` or `Iso`, in which case it may be useful to think of it as having one of these more restricted type signatures: &gt; &gt; review :: Iso' s a -&gt; a -&gt; s &gt; review :: Prism' s a -&gt; a -&gt; s &gt; &gt; However, when working with a Monad transformer stack, it is sometimes useful to be able to review the current environment, in which case one of these more slightly more liberal type signatures may be beneficial to think of it as having: &gt; &gt; review :: MonadReader a m =&gt; Iso' s a -&gt; m s &gt; review :: MonadReader a m =&gt; Prism' s a -&gt; m s Well, that didn't help.
Yes, the dependency on an external server is quite horrible, and I'd be quite happy if I could put all of Haste's needs on Hackage as a normal package. Unfortunately, Haste needs to install low-level packages like ghc-prim and base into its package database, which is not possible using cabal. While I guess it would be possible to just put this up as packages, they would be uninstallable using normal methods and Hackage would just act as a file host from which haste-boot would fetch tarballs to rip open and do its own thing on - this feels to me like gravely abusing Hackage. However, as of the latest release (0.2.99), these external libraries are instead hosted on GitHub pages, which should at least be less fragile than my hosting provider turned out to be.
Yeah, I haven't looked at the Prism stuff at all. I'm sure it's very cool, but all I care about for my needs is the nicer way of getting/setting through nested records.
Oh god, regex. I was trying to get a few simple regexes working, and all I could think was, well, now I have 2 problems. There are far too many regex libraries on hackage and no clear tutorials or guides that are current to the state of what to use in 2014. Even though RWH is a touch out of date, many of the tips still lead me to the correct solution. (the polymorphic on return type was key -- despite the documentation being extremely sparse...)
&gt; While I guess it would be possible to just put this up as packages, they would be uninstallable using normal methods and Hackage would just act as a file host from which haste-boot would fetch tarballs to rip open and do its own thing on - this feels to me like gravely abusing Hackage. On one hand, it does feel really hacky. But on the other hand, if it was on hackage, you can be sure that any mirror of Hackage will have everything haste needs, and it's (hypothetically) nice to run a private mirror for company needs. Github is a lot better, but it goes down relatively often as well. 
Examples added.
Personally, I like my code to be as weird-ass as possible, so I love them.
This is exciting! I just started skimming but in section 4.1 they mention this approach is not haste-specific, and I found some nice (but maybe a little dated, as this all seems to be moving very fast!) discussion of the various hs -&gt; js compilation solutions here: http://www.reddit.com/r/haskell/comments/1ldqav/thoughts_on_uhc_vs_haste_vs_fay/
I had the same experience that you did. RWH, while dated, was helpful in explaining how to use tdfa with capture groups. There's an overview of all of the regex libraries [here](http://www.haskell.org/haskellwiki/Regular_expressions) but you're right it looks old. The page talks about GHC 6! It's surprising how many C regex libraries out there have [problems](http://www.haskell.org/haskellwiki/Regex_Posix).
Not sure if he's aware, but Helium also refers to a Haskell dialect https://en.wikipedia.org/wiki/Helium_(Haskell)
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Helium (Haskell)**](http://en.wikipedia.org/wiki/Helium%20(Haskell\)): [](#sfw) --- &gt; &gt;__Helium__ is a [compiler](http://en.wikipedia.org/wiki/Compiler) and a dialect of the [functional programming language](http://en.wikipedia.org/wiki/Functional_programming) [Haskell](http://en.wikipedia.org/wiki/Haskell_(programming_language\)). It has been designed to make learning Haskell easier by giving clearer error messages. It is being developed at [Utrecht University](http://en.wikipedia.org/wiki/Utrecht_University), [Netherlands](http://en.wikipedia.org/wiki/Netherlands), primarily by Arjan van IJzendoorn, Daan Leijen, Bastiaan Heeren and Rijk-Jan van Haaften. &gt;Certain language features of Haskell have not been included to create more specific error messages. For this reason, it (currently) lacks [type classes](http://en.wikipedia.org/wiki/Type_class), rendering it incompatible with many Haskell programs. &gt;It also includes Hint, an [interpreter](http://en.wikipedia.org/wiki/Interpreter) written in [Java](http://en.wikipedia.org/wiki/Java_(programming_language\)) with a [graphical user interface](http://en.wikipedia.org/wiki/Graphical_user_interface). &gt;==== &gt;[**Image**](http://i.imgur.com/DUJZ7WW.png) [^(i)](http://commons.wikimedia.org/wiki/File:HintScreenshot.png) --- ^Interesting: [^Haskell ^\(programming ^language)](http://en.wikipedia.org/wiki/Haskell_\(programming_language\)) ^| [^List ^of ^open-source ^programming ^languages](http://en.wikipedia.org/wiki/List_of_open-source_programming_languages) ^| [^List ^of ^educational ^programming ^languages](http://en.wikipedia.org/wiki/List_of_educational_programming_languages) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cg75j0k) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cg75j0k)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
&gt; These are called dependent types in my circle of friends, or, more specifically, refinement types. I don't think that dependent and refinement types are so related. The author should probably also look at LiquidHaskell. Also, from my discussions with some well known PLs people, Whiley doesn't seem so revolutionary. Seems like yet another verifying imperative language based on an automated theorem prover. People have been making those for at least 10 years now.
Very cool. I've been doing this with Fay till now but I'll take a look at Haste!
cabal file from hackage: http://hackage.haskell.org/package/Cabal-1.18.1.3/Cabal.cabal If you look in the library section you will find "default-language: Haskell98". Edit: I misunderstood the question originally. Here is a relevant issue report as to why cabal uses Haskell98 by default: https://github.com/haskell/cabal/issues/1158
Does this relate to the [tagless final](http://okmij.org/ftp/tagless-final/course/index.html) approach (if that's not what you're referring to from the get go)? 
This is an extension of what is possible with Fay, however. With Fay you can use the same types on the client and the server, which is much, much better than the usual web way of doing everything with strings, but there's really nothing to prevent you from using them in the wrong way. With Haste, you actually get a static guarantee that your entire program is type safe since both client, server and network communication itself are type checked as a single unit.
Interesting! Thanks for pointing that out. Any idea why cabal does not use a more recent version of the language by default? _edit: Ah, your edit answers my question much more directly. Thanks again!_
Can you clarify what you mean by "using them in the wrong way?" Could you give an example of a problem that could arise with the Fay approach?
it's not cabal's default. this is set in the source of the library. 
Using Fay and shared types, you get type safety for the client side and for the server side. However, there are no guarantees that the types used on the client and the server actually match up. In the example given in [1] for instance, nothing prevents an overeager intern from refactoring the server to use two types NewCommand and NewResponse while the client still uses the old Command and Response. The method used by Haste prevents this by type checking the entire application - client and server - as a single program. [1] http://hackage.haskell.org/package/happstack-fay-0.2.0/docs/Happstack-Fay.html
Presumably that would break packages released before Haskell2010.
Surely that's the option for building cabal itself, not building other packages using cabal.
That sounds like a build/deployment system issue, not a library issue. What's preventing someone with a Haste application from accidentally deploying a new server-side app without updating the generated Javascript?
Oops. I misunderstood the question. Fixed my reply slightly, thanks.
I believe our difference of opinion here is about point of view; with Haste.App (yes, the name is silly) your web application is seen as one single program, not two different programs that just happen to talk to each other. If you look at it this way, it becomes obvious that the issue is not about deployment but about type safety. If, on the other hand, you take the view that your client and server parts are distinct applications unto themselves, then it is equally obvious that this is a deployment issue. I believe both views have their merits and are suitable for different kinds of applications. While type safe communication seems to be working out pretty nicely so far, it's completely optional. If it doesn't fit a particular application (and I can definitely imagine a few applications where it won't fit very well), then the developer is free to just use Haste in the "normal" way. The point about misdeployment is very valid in both views, however. It'd probably be a good idea to somehow ensure that mismatching client/server deployments bomb out in a highly visible way immediately upon launch, or even to bake the client part into the server binary.
Looking at the website, the two would seem to be quite similar. One difference might be that the Haste solution is implemented as a library with no compiler support needed (except, obviously, being able to compile to both native code and JS), whereas Eliom seems to require you to use their stack for everything.
"Wierd alignment". People take inspiration about style from sources they use. I and others, having read LYAH we took its style in our code. http://learnyouahaskell.com/syntax-in-functions#where
Nice! Is there some nice FFI to external JS, so one can use, say angular and such?
There is the standard GHC FFI, as well as a more flexible Haste-specific interface: http://ekblad.cc/haste-doc/Haste-Foreign.html
YES! I have been waiting for someone to bring coherency between frontend and backend web development for all of my miserable web-developer life. THANK you! I can't wait to get into this!
I also started in LYAH, and yes, some of the stylistic choices there are what I'm talking about.
You make a very good point. Thanks for explaining!
There is a good reason why there exists no implementation of a parallel graph reduction algorithm on GPUs. GPUs are very good at a certain kind of parallel task: Working intensively on homogeneus data that is layed out continuous in memory. Random access slows down the performance considerably and reduction on graphs requires this kind of access pattern. The abilities of a multicore CPU is much more suited for this task. Although if you find a reduction algorithm that works faster on the GPU than on the CPU let me know. Best Sven
I think I understand what you're saying. Thank you for taking the time to explain the approach, I really appreciate it.
x consists of an infinite list of different nodes, each of which points to the next one. Conceptually: x = 1 : x1 x1 = 1 : x2 x2 = 1 : x3 ... As you iterate through the (lazy) list, it creates more and more nodes and so RAM goes up. In contrast, y consists of a *single* node that points to *itself*. Equivalently (due to laziness, this is a valid definition): let y = 1 : y So when when you "iterate" through that list you're always looking at the same node (y), and RAM usage is constant.
Why though? It's probably not required behavior is it? My guess would be that it comes from not optimizing: I would guess that [a,b..] is defined to have elements a, a+(b-a), a+(b-a)+(b-a), ..., with no special case for a==b, so that in the [1,1..], ghci doesn't actually "know" that the elements are 1 until you ask for a particular element and it computes (previous 1)+(1-1)=1.
Cool! Question: since you're using OverlappingInstances anyway, do closed families buy you anything over open families?
What cabal does do is that if you specify that your file as a minimum cabal version `&gt;= 1.10` which is needed to use some later features, it requires you to supply a `default-language:` for each library, executable, test-suite or benchmark.
Right. In fact, from: http://hackage.haskell.org/package/base-4.6.0.1/docs/src/GHC-Enum.html If you look for "instance Enum ()" in that file, you'll see that enumFromThen *is* defined in the space-efficient way. But for Integer, it doesn't check whether the "from" and "then" are equal. I think this is an oversight (or a decision to make the code simpler) rather than a requirement from the language definition.
It makes the types clearer as we can reason about the output type of a function: type family Output f where Output (a -&gt; b) = Output b Output x = x I don't think this would be possible without closed type families.
Wow! thank you very much.
[Here's my stab at it](https://gist.github.com/mikeplus64/9664188). I can't decide whether this is more or less obtuse. edit: I should have added `withHaskell :: (a -&gt; IO b) -&gt; Haskell a -&gt; IO b`, so we can use `withCString`, and not have any spooky inaccessible `CString` hanging around afterwards. edit 2: ok, did that to the gist above
That should be more efficient since it doesn't have to de/construct the heterogenous list, but it does rely on some deep IO magic. :P EDIT: Does anyone know if it's possible to eliminate the list from mine with rules? EDIT 2: Another difference is that my one doesn't require the function to marshal to be in IO at all. I know this isn't too useful for foreign imports but the marshalling rules can specify their requirements on the result type, e.g.: instance (Marshal b, MarshalOut b ~ IO a) =&gt; Marshal (CString -&gt; b) where marshal' f (s :. as) = withCString s $ \cs -&gt; marshal' (f cs) as
My understanding of this is pretty limited so forgive me if I'm wrong, but doesn't this plumb the `RealWorld` from the point of marshalling into all future calls of the marshalled function? marshals :: Marshals f =&gt; f -&gt; IO (MarshalEach f) marshals f = do w &lt;- getWorld marshals' (return ()) w f
You're right. But, perhaps luckily, the whole "Real" and "World" aspect of `RealWorld` seems to be exaggerated, so it doesn't matter.
Ah, interesting. I've never thought about how to do more selective IO management than pure `unsafePerformIO`.
Yeah. You might be able to write something like class Marshal f g | f -&gt; g where marshal' :: f -&gt; g instance MarshalOut b ~ IO a =&gt; Marshal (CString -&gt; b) (String -&gt; b) where {...} instance Marshal (CInt -&gt; b) (Int -&gt; b) where {...}
Looks more like a marketing site for the blogger and as such holds as much content.
&gt; I don't think that dependent and refinement types are so related. Dependent types and refinement types look quite different, but are surprisingly closely related. See Stefan Monnier and David Haguenauer's paper [*Singleton types here, Singleton types there, Singleton types everywhere*](http://www.iro.umontreal.ca/~monnier/comp-deptypes.pdf), which shows how to translate a dependently-typed language (the Calculus of Constructions) into a language only supporting refinement types. The thing that really blows my mind is that the translation they give is essentially a version of the parametricity translation that Jean-Phillipe Bernardy, Patrik Jannson and Ross Patterson describe in [*Parametricity and dependent types*](http://www.soi.city.ac.uk/~ross/papers/pts.pdf). Something really subtle is going on here, and I don't think we understand it yet. 
Largely different goals. A "finally tagless" (sic) approach basically abuses the fact that typeclasses are internally a limited form of rank-n types to avoid using GADTs to describe the syntax tree. On the plus side, such an approach can get many of the same composability benefits, but it is a different emphasis. Here I'm gaining constraints on the state or the error type or the environment, not the monad or syntax tree itself.
Apparently Bryan O'Sullivan ( /u/bos ) runs this project, for those who didn't know already.
yup, closed type familes are need for that that sort of awesome :) 
More context. http://www.wired.com/wiredenterprise/2014/03/facebook-hack/
Source code: https://github.com/facebook/hhvm
Nice work, but I have one gripe. I really, really wish people would stop using the quicksort example to show off how concise Haskell is. You are comparing the efficient, in-place quicksort in C to an inefficient, not-in-place Haskell implementation. The in-place quicksort in Haskell looks like [this](http://www.haskell.org/haskellwiki/Introduction/Direct_Translation) or [this](http://stackoverflow.com/questions/5268156/how-do-you-do-an-in-place-quicksort-in-haskell). When non-Haskell people notice this and point it out, it makes the entire Haskell community a laughing stock, and reinforces the impression that Haskell is not a serious language. In general, though, great work. I really liked the section on IO at the end.
Thanks :) Of the audience, no one had heard about Haskell before :( although they are quite technically versed.
I know... I like the quicksort example because it's quite visual (the pivot goes in the center, the lesser and greater elements on each side...). You can read it even without knowing Haskell. 
It appears the Hack itself is written in ocaml https://github.com/facebook/hhvm/tree/master/hphp/hack/src
I am working on Haskell-to-hardware as well in my job at [Tabula](http://tabula.com/). The project is described in a few recent posts on [my blog](http://conal.net/blog). It's an ambitious project, and I would *love* to find collaborators, so if you're interested, please let me know.
You may be interested in http://dmwit.com/papers/201107EL.pdf . A general "transformer compiler" C : (S -&gt; S) -&gt; (T -&gt; T) that can take ANY function (S -&gt; S) is probably way too much to ask (unless perhaps the type S is suitably restricted, or the function has to be parametric, e.g. (forall a. S a -&gt; S a), etc.). Another approach is to have a concrete data type of "edits" E which can be interpreted as S -&gt; S functions (using some applyEdit :: E -&gt; (S -&gt; S) function) but which can also be compiled into corresponding edits to T (this is the approach taken by the paper I linked above).
In other news, Twitter releases Cobol with dependent types.
Great. It would be interesting to see this approach integrated with some more safe html and css tamplates, like hamlet. Also, on-request implicit loading html templates, like angularjs approach with haste. There might be some cool framework lying here. I will surely try it :)
I was wondering why ocaml instead Haskell given that /u/bos is involved.
Facebook's pretty inventive. PHP is terrible, but they have a large PHP codebase... so they improve PHP!
I see, that's really neat! Is there a paper or other text describing this typed injection in more detail? The way Haste currently does things is a bit more like "traditional" application development I guess; this would be a nice way of going a bit more declarative.
I agree. And especially being able to check at compile-time the names of CSS classes, DOM elements, etc.
I wish facebook would invest in making good languages better rather than making a bad language tolerable.
The eliom documentation try to explain this, but lack precisions sometime. You can see several document [here](http://www.pps.univ-paris-diderot.fr/~balat/publi.php). I'm slowly working on a paper giving a formal semantic of eliom's syntax extension, but it's not really ready yet. There is my Master internship report, but it's in french...
As someone who works at a company that uses PHP and Haskell I welcome this. The more understandable the code is, the easier it is to rewrite, interoperate with, extend etc, and types help you understand code. 
It's 2014. Why can't we let PHP die already...
I feel the same about C, but not everything we wish for does happen.
Last year at ICFP 2013 I presented Unifying Structured Recursion Schemes, which was joint work with Ralf Hinze and Jeremy Gibbons. A few people asked me if I would be able to make the presentation online, so here it is on youtube. You can find a revised version of the PDF here: https://www.cs.ox.ac.uk/people/nicolas.wu/papers/URS.pdf
Is this what Simon Marlow has been involved with? They've been dealing with some spam filter that is being rewritten in Haskell.
In a book review for Simon's parallel Haskell book, Bryan mentions that he is Simon's manager at Facebook so there's a pretty good chance that he is at least giving input to it.
Nope.
What does CAF mean to you?
Cut the ageist crap sonny.
Your presentation style here is brilliant. (The content isn't half bad either!) =)
Missed this @ icfp. Wonderful presentation, great result. I wonder what the next big idea will be now that such a simple (but abstract) subsumes so much of what we know.
PHP isn't very old though. In fact, I'm pretty sure COBOL's inventor would resent the comparison if she were here to comment.
&gt;I'm the manager of the team that developed Hack, and I'm sitting here with some of the language designers. Happy to answer your questions. http://www.reddit.com/r/programming/comments/20wuuo/facebook_introduces_hack_a_new_programming/cg7h0vf
Interesting! Even if that is the case I'd be interested to know how much worse it is. Sadly seems like nobody took the mission yet (:
I remember you were waiting on fixing those major bugs before releasing it under the GPL or other free software license. Would you consider doing that now or soon? 
It seems there are a few things being asked here. To answer the question posed in the subject: as far as I am aware, no. Auditing a runtime system is quite a tricky task as there are many moving parts. It's not something you can just run Coverity on. To answer your second question: no, the degree to which GHC optimizes does not open up buffer overflow vulnerabilities. That is not to say that Haskell code can't suffer from buffer overflows, however. Like most high-level languages, Haskell's `vector` library provides both [bounds-checked](http://hackage.haskell.org/package/vector-0.10.9.1/docs/Data-Vector-Unboxed.html#v:-33-) and [-unchecked](http://hackage.haskell.org/package/vector-0.10.9.1/docs/Data-Vector-Unboxed.html#v:unsafeIndex) variants of accessors. A responsible library developer will ensure that all accesses are somehow checked, either by using bounds-checked accesses or by designing the library interface in such a way to preclude out-of-bounds access. In short, Haskell is in principle neither more nor less vulnerable to these sorts of exploits vulnerabilities. In practice, the Haskell community has build up a set of tools which strongly push authors to write safe code. An excellent example of this is `vector`'s `unsafeWrite`, et al. which make the user quite aware of the loss of safety that their decisions carry. This is in stark contrast to, e.g., C++'s STL, where accessors are unsafe by default (`operator[]`). 
Twitter uses Scala...
SPJ would say "it's reached the threshold of immortality, it cannot be allowed to die". 
not sure why people are so invested in this one way or another. think of it as an internal fb tool that is coincidentally documented and on github. but still, at its heart, it is a tool to solve a problem facebook has. do you know anyone outside of facebook that uses previous extensions like XHP? nope, me neither. stock php goes to great lengths to retain backwards compatibility...for good reason...most people using it have no interest in new features or breaking changes. this same audience is not interested in a fork of php either. what will be interesting is to see how the php core team reacts once if it starts to believe hhvm+hack represents hijack of their tool by facebook...
but for that to be true, you need to get all of your old php converted into hack....i wonder if FB plans on automating this or merely scheduling old-style php for conversion by hand....i can't see any benefit to merely adding a new language into the mix
That was a nice little easter egg.
Contant Applicative Form, My intution is that any top level binding which has got its type fully resolved(i.e. no type variables, either because of explicitely writing its type signature or because of MR) and looks non function like(doesnt take any args explicitely i.e. doesnt bind args, it might take args pointfree style), then it'll be cached to avoid recalculation, Am I thinking right?
Because it's a useful scripting language. 
Thanks!
Allow me: ~~Unifying Structured Recursion Schemes~~ Edit: removed link, see OP's link above.
Thanks, Brent. That's exactly what I was looking for!
That's basically it, but replace "file" with "program". You can split your web app into modules however you like, the important thing is that it is type checked as one unit, not two. 
Thanks!
Thanks! That version has a couple typos in it, which have been fixed in the link I provide.
You should probably update the link [here](http://www.cs.ox.ac.uk/people/nicolas.wu/) (it is the first google link for your name).
Actually you're touching on material that we worked on after this paper: adjoint folds don't cover hylomorphisms (although you might argue that this is a fold after an unfold), but they certainly don't cover dynamorphisms, where a recursive coalgebra sits in the place of $\inv{in}$. We cover this in our sequel paper, tentatively titled: Conjugate Hylomorphisms: The mother of all structured recursion schemes. Conjugate hylomorphisms generalize recursive coalgebras from comonads in the same way that adjoint folds generalize recursion schemes from comonads.
I agree, I am sad to see talents and time spent in this sort of project. Also I don't really see "lots of haskell inspiration".
It's not bugs that stop me from open sourcing. I need to restructure code. Now I'm just using a single repo for the whole project with hardcoded configuration values, private API keys, TODO (with some sensitive information), configs, SSL keys and so on. It's less convenient to use two repos (main factor) and some time is needed to split privacy/security related information out of main sources and to write some documentation about how to setup everything. Since only few people have asked about open sourcing I don't plan to work on it in the near future. Maybe I'll return to it later this year but at the moment I'm focused on adding new major features that many users have requested me to add.
What do you mean by a "unit", I still don't get that? I so far only understand it from the point of "can fit in the same file".
I experienced how getting Fay up and running can be hard at first; when finding out others had similar issues I wrote a piece on it. Since it is one the company's blog, I added a little introduction to Haskell, Yesod and Fay. Hopefully it may be of help to some..
http://lpaste.net/1548986767120531456 I get the same observations if I cut down the application to something simpler as above.
Well, what's the free theorem?
This line of reasoning does not work in large deployments. There is no way you can switch deployment atomically. This is really fundamental limitations in distributed computing. You need to do distributed consensus in order to to atomic upgrades. You do not want the overhead of distributed consensus for normal web apps. For small web sites it can work. For larger deployments it does not. It is a little similar to how you would handle data stored in localStorage. How do you ensure that it is forwards compatible when you upgrade, and backwards compatible when you have to roll back. I think there is a great future for type safety in distributed deployments, but it is impossible to solve this problem by just looking at a program. The deployment must be modeled and made available to the compiler in some way.
So "unit" is program. Thanks. &gt; While each is type safe in isolation, there are no guarantees that the communication between them is well typed. Using the "shared" types this is possible with Fay I believe. I just posted a blog in which I setup a simple example app with Fay. http://www.hoppinger.com/blog/haskell-in-the-browser-setting-up-yesod-and-fay In this example app the communication seems completely type-safe and boilerplate free to me.
Hm, do you have any other information that may be of use? Did the program bomb out with an error immediately upon launch or did it wait until you responded to the message box? Which compilation flags did you pass to Haste and GHC, which Firefox version are you using, and is there any other information about the server that may be useful, etc.? I copied your pasted example verbatim, only changing the host URL, and it works as expected on my end. What happens if you try http://ekblad.cc/hasteapp/test.html? The output goes to the server obviously, but do you get the same browser error?
We've already got good languages. I think it is actually more important to start dragging the bottom of the barrel up.
I do get the same browser errors yes, both in chrome 33.0 (Uncaught WebSockets connection died for some reason!) and Firefox 28.0. I just used "hastec ./test.hs" to compile test.js, and then ghc --make ./test.hs -o testsrv to compile the server binary on an extremely run-of-the-mill arclinux droplet on DigitalOcean, but I guess that can't really be the source of the problem if I reproduce the exact same problems. Thank you though, it looks like the problem really is with my computer or internet connection somehow if you're experiencing no issues with that same web application. I'm going to buy a VPN right now and try again (in case maybe it's my internet connection?) and see if I still suffer problems.
Nice!
Yeah, it sucks to be old--and Facebook is old and crusty.
You might want to read this [post I wrote on default empty values](http://www.haskellforall.com/2013/04/defaults.html). I can summarize it by saying that you can't rigorously define what "empty" means unless you define a combining operator (i.e. `mappend`) that it is empty with respect to.
So, the case I'm thinking about when you do a put before any get, and the inital state value is never actually used. In that case, we don't need an id element even, just an arbitrary element, correct?
...which is maybe related to the web sockets server (localhost) and web server (remote) being on different hosts? Maybe Chrome doesn't like that? Sorry, let me try putting a local web server together right now and try again.
Well a church numeral has the type type Nat n = (n -&gt; n) {- succ -} -&gt; n {- zero -} -&gt; n Our free theorem is that any term occupying `forall n. Nat n` is going to be built purely out of `succ` and `zero`. Or, stated formally, define `comp :: Int -&gt; (a -&gt; a) -&gt; (a -&gt; a)` so that comp (n+ 1) f ~ comp n . f comp 0 f ~ id Our theorem is forall c : (forall a. Nat a). exists n. c ~ comp n This theorem essentially states that any natural number is equivalent to `0 + 1 .. +1 + 1`. Which isn't terribly interesting.
There is a package data-default, which supplies such a class, but I concur with others here that it is generally ill defined.
Nah, Chrome doesn't behave even if I start a local web server as well : ( Either way I can confirm that "contention" and "fragmentation" are two very appropriate words for my internet connection.
Yeah, that's definitely true, but I've had some code where it was clearer to do the extra put. I don't have an example on hand though.
No, there is not. The type error on slide 50 was mentioned during the lecture and is probably due to copy-and-paste of the `parfib` implementation on slide 54.
That's not really any better than just putting `undefined` there, though, since if you do mistakenly end up `get`ting it before `put`ting it, you will find yourself in an infinite loop.
&gt; data-default Or data-default-generics, which may be helpful for more complex structures.
Im a newbie concerning haskell and all i managed was write a function that returns a list of lists of all naturals along with a list of two adders for each respective number. How do you propose i do the first step ? Edit : If we find a way to get a list of lists of the adders of a number , it would help a lot. Any thoughts on that ?
I did a simple proof-of-concept of using Angular in particular with Haste; code [here](http://github.com/zopa/aghast). (The Haste stuff is in app/hask). So it can be done---but I don't really recommend it. The Haste FFI is great for calling snippets of JavaScript from Haste. It's possible, though not nearly as natural, to write Haskell functions and export them to be called from JavaScript. With Angular, you're exporting Haskell functions that, internally, call out to JavaScript (because they need to use Angular services, or else what's the point?). It works! But it's messy. If you can, it's better to keep control in Haste-land. 
/u/damn_dats_racist, congratulations! People like you have taught me how to think functionally. :)
Ohh, I see! Thanks for the clarification. Maybe this could be a GSoC project next year? That way you could work with someone on it
Could you perhaps give me an example on what do you mean cause i really cant follow your question. English is my second language after all. Thanks for helping me though ! If you mean something like this [[1],[2],[3],...,[n-1]] then i would insert on the head of this list, the list with the number 1 ([1]) and then for all the other members of the list i would increase their value by one. But im pretty certain that you dont mean something like this cause i can get the list of lists of all naturals with this: [[x] | x&lt;-[1..]]
[subsequences](http://hackage.haskell.org/package/base-4.6.0.1/docs/Data-List.html#v:subsequences) [1..] Edit: Markdown Syntax 
Happstack and Yesod are libraries for making web servers. Haste.App is a library for building a web sockets server and its corresponding javascript client. Happstack and Yesod serve web pages, but Haste.App can't. On one hand this does make say... Yesod and Haste.App very simple to combine together (you just run the web sockets server separately and throw the javascript into your Yesod project), although I think that the resulting fragmentation of code would miss the whole point that the project was designed to address. From what I've read of the paper though, and someone please correct me if I'm wrong, it does seem like this is just a limitation of this particular implementation, and the same principles could (in principle) be used to make a server-heavy, AJAX-driven web development solution that provides type-safe JavaScript integration and serves web pages. It just hasn't been developed yet! (although probably not with Haste in the immediate future as I understand that it currently chokes on Template Haskell, which Yesod and acid-state use with great gusto) I hope that one of the major players gets inspired by this though. How amazing would it be if someone came up with a full-featured web framework that was capable of something like this?
I think it's not that big for a GSoC. And probably I'll spend more time describing how to change code than to change it myself. Hope I'll get to it later this year.
How do the Par monad and pipes-concurrency differ from the user's point of view? It looks to me like they provide similar models for dataflow parallelism with stateful (monadic) nodes. I haven't used Par, so apologies if this question is ill-informed. 
&gt; `&amp;`: Like ordinary `$`, but flipped. What is this used for? I can't image any time where I would want this.
I think that skips [1,3].
I was expecting something more like n f . f = f . n f which doesn't hold for the concrete n :: (Int -&gt; Int) -&gt; Int -&gt; Int n f x = f x + 1 -- *Main&gt; n (*2) . (*2) $ 2 -- 9 -- *Main&gt; (*2) . n (*2) $ 2 -- 10 but would hold for a polymorphic Nat at that type, no? But I'm not sure how this game is played; if we could refer to other occupants of the polymorphic type we could say that n f . m f = m f . n f which looks a little more like arithmetic. 
Heh, that's a really fun problem. (Though, I'd feel bad for spoiling it...)
If you wanted to be rigorous about it, then you could always use `State (Maybe A)` with the initial value `Nothing`— of course, then all your `put`s have to be changed to `(put . Just)`... I.e., using `State (Maybe a)` doesn't guarantee the monotonicity property you want
judging from the names one is for parallelism and the other is for concurrency, so they would be very different. slide 5 shows parallelism vs. concurrency
Well, your `n` isn't a good natural number because it doesn't just use the arguments `f` and `x`, it also does this `(+1)` business. If we forced `n` to be universally polymorphic in the carrier type, then you couldn't do that since `(+1)` doesn't work for *all* types.
Hey, are you guys a Dutch internet agency using Haskell?
Oh, whoops. Misread the documentation.
Nic Wu is an awesome guy. He taught me at university for practicals once, glad to see his research is now a paper. Edit: Oh wait you are Nic. 
[Here](http://yow.eventer.com/yow-2013-1080/moving-faster-with-php-at-facebook-by-julien-verlaguet-1373) is a video of Julien Verlaguet talking about Hack at the YOW conference. It's an awesome talk. Of particular interest is that Hack can be run in two modes - there's an optionally typed mode and a strict mode. Between the features of Hack and what I presume are the PHP coding standards of Facebook, they were able to get their codebase across to the optionally-typed mode of Hack by changing "&lt;?php" to "&lt;?hh" (more or less). The talk also mentioned how much of the code (most of it) had been converted to strict mode and the rate of progress on the rest of it. There's plenty more in that video - highly recommended - and if you get a chance to char with Julien or anyone else on the team at a conference, I highly recommend taking it - friendly, practical, smart folks.
Right, that was my point, though -- that my n fails to satisfy my little equational 'theorem' though any polymorphically definable one would have to. I was just trying to get something more like Wadler's little equations, since jozefg's remark, which is presumably the only possible one, doesn't fit the picture you get from the paper -- the examples are all simple equations involving composition, e.g. id . f = f . id . Dut I don't know how you would formulate 'the' single unique free theorem for this type. (Does the doctrine of the paper entail that there is 'the' free theorem, by the way?)
Notice that this falls out of this since `comp n f . f ~ comp (n + 1) f ~ f . comp n f` by the associativity of `.`
A set of all subsets is called a powerset. What you are looking for is the powerset on ℕ (the set of natural numbers). That may help you in your googling. EDIT: Aaaaaaand I can't read. As /u/damn_dats_racist said, you're looking for something that is the powerset of an (uncountably?) infinite multiset.
No, that's not what he is looking for. Look at his list. He wants something like power-multiset.
Doesn't this skip [1,1]?
can you prove that? It seems non-obvious to me how this comes from the basic free theorem. The naive free theorem is that given: f :: forall a. (a -&gt; a) -&gt; a -&gt; a g :: a -&gt; a h :: b -&gt; b a1 :: a b1 :: b R is a subset of a * b (a1,b1) in R forall (x,y) in R, (g x,h y) in R we can deduce (f g a1,f h b1) in R which is easy to verify. I don't see how to get from that to your claim about naturals: a claim which I can prove syntacitcally, but for which I don't know how it is a consequence of the free theorem.
I know you wanted a hint, but maybe sharing my solution would be just as helpful. The approach I took in coming up with this solution is as follows: 0. Understand the properties of the list. Example: Given lists of the same sum, shorter list come first. Given lists of the same length, they should be sorted in ascending order 1. List down the first few elements based on the expected output, which is your sample. 2. Group similar elements together (I wrote a table down as shown below. They are grouped by their sum and length.) [1] [2] [1,1] [3] [1,2] [2,1] [1,1,1] [4] [1,3] [2,2] [3,1] [1,1,2] [1,2,1] [2,1,1] [1,1,1,1] 3. Find the most trivial cases. (In this case elements of length 1 were the easiest to solve.) 4. Find common patterns to derive an element from a previous element. (This is rather difficult for me to explain, but I think you'll eventually see it. If not, my code below shouldn't be too hard to understand.) 5. Code. 6. Fix compile errors. 7. The code compiled. QED. This is probably far from the most most efficient solution, but it does get the job done. (*WARNING* the code below is by someone with no actual haskell coding experience (only read about it, quite often) and therefore unidiomatic!) I had fun coming up and coding this though, thanks! ans = [] : concatMap lol [1 ..] -- list of list of numbers that sum up to `n` lol n = concatMap (lol' n) [1 .. n] -- list of list of numbers that sum up to `n` of length `len` -- n must be positive AND n &gt;= len lol' n 1 = [[n]] lol' n len = concatMap (\x -&gt; map (x:) $ lol' (n-x) (len-1)) [1 .. n-len+1] main = putStrLn . show $ take 50 ans
Rotterdam based web shop, currently evaluating Haskell. Mainly how hard it is for our team to get productive and for (potential) customers to be confident in choosing it.
Mainly Ruby+Rails for apps and PHP+WP/Drupal stuff that represents a large chunk of our business. As said, just evaluating Haskell currently. Interesting to us are things like: compiles to JS, HM-type-safety, speed, less moving parts, higher code quality, less code needed. We want to achieve: better for less, while cheaper to maintain or extend upon.
I did it without recursion. Hint: `foldr (.) id`.
This [[x] | x&lt;-[1..]] gives you the list of lists that have length 1 and that they keep a natural number (This list : [[1],[2],[3],...] ). This whole problem confuses me and i can't give a definition of it in order to help me code in Haskell.
Please do spoil it ! Or at least your thought process or even your definition of the problem.
An endofunctor is actually a functor from a category to itself, not the thing in that article. Your endofunctor seems to actually be a monoid endomorphism on the monoid of endofunctions over `a -&gt; a`, which is Fancy for saying that you have a functor that essentially only works with one type.
Thank you. Open sourcing is better to be done by me (there are some private information like API keys that I don't like anyone to see). I think that after open sourcing you could try to help (don't know how yet). There is also a problem that all comments are in Russian. Not sure yet that I will translate them. And if you like to work on real project you could create one by yourself (like I did). Just find something you don't like and want to improve and start coding.
leftBarNode :: Bar -&gt; Bar leftBarNode (Node l r) = Node (leftBarNode l) (Leaf 0) leftBarNode other = other Wasn't the whole point of this to separate traversal and mutation? Why do you recursively call leftBarNode on the left child here?
I studied your code and although lamda calculus gave me a bit of a hard time i finally got it. Thanks a lot for your effort and help!
So, UNSW PLS, please submit to this workshop? :)
It seems to me that there's a deeper issue here. Haskell supports only "planned" functoriality. If you want to use abstractions such as `fmap` and `traverse`, you have to make sure that the substructures you intend to act on are uniformly typed and abstracted out as type parameters at the time you declare the type of the superstructure. We pick out the `x` in `[x]` when we declare it: it is much harder to define a type containing substructures of type `Salary` and then give everyone a pay rise. The uniformity is no problem once you shift to indexed families of sets, because you can have a different type of substructure for each different index. The real nuisance is the need to pick out the relevant notions of substructure-to-act-on once and for all at the outset. It would be great if we could take some monolithic type `T` and notice *anytime we want to* that it is representationally equivalent to some type `F S`where `F` is (by construction, we hope) a `Functor` and `S` is the notion of substructure to hand. It's exactly this unplanned functoriality that thingyplate libraries exist to reconstruct, some more sensibly than others. I'd much prefer to be able to point out the functorial structure directly. One of the joyous things about "universe encodings" of datatype systems in dependently typed languages is that the codes which describe types have a substitution structure: variables in codes identify the places where substructures sit in the data, inducing functoriality. Any decomposition of a code as some `T[S/x]`allows us to deliver a `T[S'/x]`, given some function in `S -&gt; S'`: we just need to find where `x` occurs in `T` to know where to act with the function. First-class datatype descriptions are a liberation! Back in Haskell land, I'm left wondering whether the all-new newtype "representational equivalence" and "role" technology gives us a facility to capture "discovered" functoriality with minimal evil. I'm certainly pleased to see that pattern synonyms now allow us to give datatypes cooked up generically as fixpoints of sums of products the readable appearance of bespoke types. The question then is whether we can expose the substitution structure of the generic components. Accept no substitute for being able to say what is actually happening.
 leftBarNode :: Bar -&gt; Bar leftBarNode (Node l r) = Node (leftBarNode l) (Leaf 0) leftBarNode other = other
Cool! I think a lot of people (myself included) would be very interested to know how you fare. 'The other' Dutch Haskell company, Silk, also uses Fay as far as I know. Are you in touch with them by any chance? I could see an interesting Haskell meetup emerging from all of this. :)
I met them at a Haskell meet-up in Utrecht.
At last, somebody who knows how to compute free theorems! We can now attempt to answer the OP's question. Here is what I got: First, I proceed as usual and instantiate `R(x,y) = (f x == y)` for an arbitrary `f`. I get: if f . s1 = s2 . f then f . n s1 = n s2 . f Then, in an attempt to obtain a numeric interpretation, I instantiate all the successors functions with `(1+)`: if forall x. f (1 + x) = 1 + f x then forall x. f (n + x) = n + f x Which is quite obviously true, but not very useful! Maybe we can instantiate `R` in a more unusual way and obtain a deeper truth about natural numbers?
I've gotten through the paper and peered through some of the source code on github. Uncompilable Haskell code isn't nearly as problematic a barrier as you might think to developing a really solid web framework based off of Haste and the principles espoused in this publication! Haste.App is able to work thanks to this basic preprocessor directive which differentiates server and client-side interpretations of source code: \#ifdef \_\_HASTE\_\_ \#endif Furthermore it's apparent that specifying the LANGUAGE TemplateHaskell pragma doesn't itself cause Haste to choke and die. So what do we do with existing web framework libraries that aren't directly supported by Haste? We just hide them from Haste completely! \#ifdef \_\_HASTE\_\_ import Happstack.Server \#endif That lets us use pretty much any existing libraries we like for the server side. What remains to be done is simply provide a Haste-safe wrapper around these libraries so that they can be compiled comfortably on both sides without the end-user having to worry about ugly CPP preprocessor directives. It's so simple! And most of the legwork has already been done for us.
I hadn't considered that use for pattern synonyms. That's actually fantastic, because it does answer the `In` noise objection in the article. Now we just need a way to derive pattern synonyms automatically. It probably can be done with TH, but that's never a 100% satisfactory solution.
Of course there is STILL a lot of work to be done - routing is a massive obstacle to overcome, as you still just have a /single/ output javascript file which must suffice for every route on your website. That's not an insurmountable obstacle - it just needs a clever abstraction! Edit: Thoughts: You can make the outputted javascript attentive to the displayed route to squeeze the logic for every page into one file, but as the route can be tampered with from JS is this possibly not an optimal solution? Maybe it's the wrong approach to the problem. Is there a way to construct source code so that from the GHC perspective it's one, coherent application, but viewed from the Haste perspective is many programs corresponding to each individual route? (without exposing CPP preprocessor directives)
&gt; Does the doctrine of the paper entail that there is 'the' free theorem, by the way? Yes and no! The paper does entail that there is a single free theorem for each polymorphic type, but that theorem doesn't look like `id . f = f . id`. Instead, the single free theorem for `id :: a -&gt; a` states that for any binary relationship `R :: a -&gt; a -&gt; Bool`, if `R(x, y)`, then `R(id x, id y)`. To obtain theorems of the form `id . f = f . id`, you need to instantiate `R` to `R(x, y) = (f x == y)`: R(x, y) -&gt; R(id x, id y) (f x == y) -&gt; (f (id x) == id y) f (id x) == id (f x) f . id == id . f You could of course instantiate `R` with other relationships in order to obtain other theorems, but they probably wouldn't simplify to that pretty form. Now, the paper doesn't mention this, but I think free theorems could be generalized even more, for example we could use a ternary relationship `R :: a -&gt; a -&gt; a -&gt; Bool`, and I'm pretty sure that `R(x, y, z)` implies `R(id x, id y, id z)`. Well for `id` it is obvious anyway :) I once wrote [an argument](http://gelisam.blogspot.ca/2009/09/samuels-really-straightforward-proof-of.html) (but not a proof) explaining how `R` can be generalized in this and many other ways. But don't click that link, it's totally unreadable :)
huh, i should check this out :) 
Categories are everywhere. Why can't Bars be a category too?
Just in case GK &amp; MC didn't know about it... :-)
The dependent types stuff goes completely over my head, but with regard to "unplanned" functoriality, that's exactly what [`Control.Lens.Setter`](http://hackage.haskell.org/package/lens-3.9.1/docs/Control-Lens-Setter.html) is for. I'm not sure if that's much weaker than the dependent types approach you are talking about, though.
I was wondering how long it would take someone to say "lens". Yes, lenses are a rather good way to deal with unplanned functoriality in Haskell as presently constituted. Lenses promote particular access patterns for substructures whilst hiding the detail of the superstructures which contain them: these `(a -&gt; b) -&gt; s -&gt; t` types don't make explicit the common structural relationship by which `s` contains `a` and `t` contains `b`. In the dependently typed setting, that structural relationship can be represented by a first class lump of syntax containing (gasp!) a free variable. Ultimately, I'd prefer to be systematically precise about what things *are* (and thus able to compute what can be done with them as I see fit) than to be systematically abstract about a particular kind of *doing*.
I'll let them know about it.
You can view functions of the form `Bar a -&gt; Bar b` as forming a sub-category of Hask. This viewpoint however, doesn't lead to a useful understanding of `Monad`, or other things that require `f` to be an endofunctor.
I'm aware of the distinction between the two concepts, but the actual interfaces exposed by the Par monad and pipes-concurrency seem to largely intersect. E.g., in both you construct stateful dataflow graphs (a digraph with monadic nodes and edges representing data dependencies). Such graphs can model parallelism _and_ concurrency.
No it wasnt the answer i was looking for but i wanted some hints to manipulate a list of lists and describe the patterns i was looking for. Thanks though for the trouble :) I have found the answer to my problem and you guys helped me a lot. So , thanks again!
I agree with this. The "default" item really needs more context. Should 0 be the "default" Integer, or should 1 be the default integer? If you are talking about multiplicative operators, 1 should be the default, but for additive operators, 0 should be the default. There are all kinds of different default values, it depends on the context you are talking about. In my own project, I defined a "class HasNullValue" which has two functions, "hasNull" which evaluates to the default value, and "testNull" which evaluates to True if the object evaluates to null. It isn't mathematical or fit into any type theoretic framework. I just needed it so I made it, and I only use it in that project. I don't need it anywhere else because it doesn't make sense anywhere else. If I did find another place to use it, I would just define a new type class for that situation.
There's an idea that `singpolyma` contributed that I really liked: truthy :: (Eq a, Monoid a) =&gt; a -&gt; Bool truthy a = a /= mempty
Thinking about it more, I do think the free theorem implies jozefgs answer. We wish to show that for any `f :: forall a. (a -&gt; a) -&gt; a -&gt; a` there exists a natural `n` such that for any `g :: a -&gt; a` and `x :: a` `f g x = comp n g x`. So we can get out a witness for the natural in the following way: instantiate R with a subset of `Church * Church` defined as `{ (f,g) | exists n, f = comp n /\ g = comp n}` For the main result though, we take R to be a subset of `Church * a` given by `{ (comp n,comp n g x) | for some natural n}`. This tells us that `f g x = (f succ 0_church) g x` and that `f succ 0_church = comp n` for some `n`.
ok, awesome! Now i just need to figure out if i can travel then! 
&gt; * New genericShrink function provides generic shrinking with GHC. This one seems great. If I'm reading it right, should greatly simplify some tests.
That was quite compact! Please let me double-check my understanding by expanding and slightly simplifying your proof. We start with a polymorphic function of type `(a -&gt; a) -&gt; a -&gt; a`, for example three :: (a -&gt; a) -&gt; a -&gt; a three s z = s (s (s z)) For any function of that type, there is a free theorem stating that if forall x y. R(x, y) -&gt; R(s1 x, s2 y) then forall z1 z2. R(z1, z2) -&gt; R(three s1 z1, three s2 z2) In the case of `three`, the proof of that free theorem is that we can go from `R(z1, z2)` to `R(three s1 z1, three s2 z2)` by applying the hypothesis `R(x, y) -&gt; R(s1 x, s2 y)` three times: R(z1, z2) R(s1 z1, s2 z2) R(s1 (s1 z1), s2 (s2 z2)) R(s1 (s1 (s1 z1)), s2 (s2 (s2 z2))) R(three s1 z1, three s2 z2) The point of the theorems being free is of course that we don't have to write down the above proof: just by looking at the type `(a -&gt; a) -&gt; a -&gt; a`, we already know that such a proof exists. Anyway. The theorem is valid for any `R`, as long as we can fulfil the two hypotheses: `R(x, y) -&gt; R(s1 x, s2 y)`, and `R(z1, z2)`. That looks a lot like an induction step and a base case, doesn't it? The fact that `R` is a binary relation looks like a red herring to me. To turn it into an arbitrary proposition about `three s z` instead of a relationship between `three s1 z1` and `three s2 z2`, we simply have to ignore the second argument of `R`: R(x, _) = exists n. (x == comp n z) For which we now need to fulfil the two hypotheses. For the inductive step, we need to prove that if `R(x, _)`, then `R(s x, _)`. That is, if `x == (s . s . ... . s) z` for a certain number of occurrences of `s`, then `s x` can also be expressed as such. Indeed: s x s ((s . s . ... . s) z) (s . (s . s . ... . s)) z (s . s . s . ... . s) z For the base case, we need to show that `R(z, _)` is true. Is there a number of occurrences of `s` such that `z == (s . s . ... . s) z`? Yes: zero occurrences. Now that we have proved the two hypotheses, we can apply the free theorem and conclude that `R(three s z, _)` is true, that is, that there exists a number of occurrences of `s` for which `three s z == (s . s . ... . s) z`. In the case of `three`, we know that this number is 3. But the interesting part is that nothing in our proof depends on the fact that `three` has the implementation we gave at the beginning! We have only relied on `three`'s polymorphic type, and the corresponding free theorem for that type. Therefore, the conclusion is true for all functions with that polymorphic type: for any church numeral foo :: (a -&gt; a) -&gt; a -&gt; a there exists a number of occurrences of s such that foo s z = (s . s . ... . s) z QED.
...and this is precisely the `Reader` monad, for the record.
Thanks for this. I'd always know that `fix` was written with `let` for efficiency reasons, but I didn't really intuitively understand it until just now.
The fact that the Reader monad is the same as a function seems irrelevant to me. 
This is a dream job! Too bad I don't have GPU experience (and not enough Haskell experience....yet!) It seems to me that functional languages, particularly modern ones with stellar features like Haskell, are the future of software development.
Very very nice! I wonder how Snap can be combined with Haste.App. Specifically, what's the relationship between splices in Snap/Heist and exporting values from the server in Haste.App? Seems that in Snap/Heist the splices are inserted into a template during rendering and then a page is served to the client. In Haste.App, I could export some constant functions that would serve the values and JS on the client would retrieve them. Any other ways of somehow combining Haste.App and Snap? Haste.App could probably use Snap for serving pages and routing and itself stay focused on the interactive part of the application.
I personally don't care much for FB as a product but man do they create some great software. For example: The [HipHop virtual machine](http://en.wikipedia.org/wiki/HipHop_for_PHP) is GREAT gift to the free software community.
Sorry, did you find a solution to your problem? Don't worry about generating that list that I gave. Once you can create a new list for n given the list for (n-1), you can recursively generate it for all n given that the list of naturals that sum to 0 is [[]].
Took me about 2 or 3 hours to get through all the steps, then this: &gt; yesod devel Yesod devel server. Press ENTER to quit yesod: my-yesod-20140322.cabal: 1: user error (Parse of field 'name' failed.) What now? 
If you write something or your talk gets recorded, I would really love to see it!
maybe your cabal file is messed up? I'd open it and check the `name` field to see if it doesnt have any weird characters or something like that.
That's a really good point! However, this article is for novice Haskell programmers, not novice programmers. It may just be hard to tell the difference sometimes. :)
I agree with you! I just think it's pessimistic to think that someone would get &gt; "see a case where IO is painful? Just get rid of it with unsafePerformIO!" from a section titled "Print debugging" that begins: &gt; The functional engineer's nightmare: “what actual argument values is this function really called with at run-time?” and then shows three examples of how to print argument values at run-time. I'm pretty sure we agree on every point but that. :)
A common use for `def` from data-default is as a default value for a configuration type. That's not the same as "empty", though in some cases the two concepts can overlap. In any case, that does still satisfy the requirement of proving that a type is inhabited.
&gt; It’s also my experience that I don’t care about GHCi’s debugging support if I have to manually set breakpoints myself and step through things manually. Who wants to bother doing that? You want some tool to decide for you where the breakpoints will be? I want complete control over where my breakpoints go. And I don't want more that one or two of them, at least to start with; I add more as I drill down. Setting a breakpoint is only a few keystrokes. As for "stepping through things manually" - well, isn't that what breakpoints are for? If you don't want to step manually, use a breakpoint. If you do want to step manually, you have that option too. Where is the bother?
Sure. I actually use `data-default` despite not liking the concept very much. ;)
In a multiset, order isn't significant. /u/Aerathir wants a list of every sequence.
The demo video doesn't play for me, it says "this video is not available in your country" o.O I'm UK based.
Pure speculation, but there was some music in the background. Perhaps Youtube algorithms decided there's no right to replay it in the UK.
Now is working though, wondering what was the cause.
Yep i did ! That was more or less the aproach i followed. Thanks again for your suggestions
&gt; breaking when a variable is updated You'd have to define what that means in the context of Haskell, considering that there is no such thing as a "variable" being "updated" in the core language. And a breakpoint on something like an `IORef` being updated is equivalent to a code breakpoint on the appropriate library function. &gt; conditional breakpoints... when a counter reaches a specific value. for instance Yes that would be nice. Also, being able to set break points on functions defined in where/let bindings. EDIT: Or even anywhere in the middle of an expression. That's probably trickier to define and implement though.
Another point is that `vector` and other libraries like it are used in far less a percentage of Haskell code than, say, C arrays in C. This issue is just one facet of why the underlying reason for that makes sense. Libraries that wrap underlying non-functional operations like direct memory access are harder to reason about than purely functional libraries. This is especially true for libraries like `vector` where the non-functional "smell" of the underlying operations leaks through a bit into the externally visible API. That's why these kinds of libraries would generally be used only where the optimizations they provide are really needed and not premature.
I would too! :) 
how does it leak in vector? The pure vector api is quite pure, observationally
It seems you're essentially saying that debugging is only for mutable/imperative code. I disagree: I want to debug pure code a lot. And purity *should* have made debugging (of correctness, not performance) a breeze. Just "dive" down the wrong subexpression until you find the source of the error. Unfortunately, we don't have such debug tools, and adding prints is much more cumbersome than what this could have been. In C, I fire up the emacs-gdb integration, which is quite OK (visual debugger). A crash in C code: I can get the full stack trace and all of the frames' information. A crash in Haskell code (e.g: "head of empty list" or "Stack overflow") and my best bet is -Xc which is somewhat painful due to the rebuilds necessary, and far worse than the crash debugging tools in C. I agree that with Haskell's tools you need to debug less often -- but it is a huge PITA, and it really shouldn't be.
The moral judgment is now merely implied.
&gt; Javascript &gt; Posted in /r/Haskell
&gt; &gt; Lazy evaluation is known to make debugging tricky. &gt; When you're "debugging" laziness He's not saying he's debugging a laziness issue, he says laziness makes debugging (in general) tricky. 
With a lot of influence from haskell packages. &gt; A (still experimental) port of Haskell's Control.Monad package, which provides common combinators and operations to work with monads in a higher level.
Well, for one thing there is the unsafe interface, which /u/bgamari already discussed. But even the "pure" part of the vector API is influenced by the fact that it's impure under the hood. It emphasizes operations which are "`O(1)`" - but really are at best `O(log n)`, except that they behave as if they were `O(1)` because of their use of parallel hardware. So the API is skewed to emphasize those operations, and away from operations that turn out poorly given the underlying hardware implementation. You would never write a vector API that way if were implemented in a purely functional way. So the skewed API somehow "leaks" the underlying impurity in a very weak sense. Intuitively, one would expect a library designed around a specific hardware implementation to have more of a chance of leading to portability problems, or other kinds of problems related to the underlying platform. Even if the external API is pure. It's a small issue, but enough of an issue that I consider using `vector` an optimization, not a default. I don't hesitate to use it when needed, but only after I have verified that using a purely functional library really is a noticeable performance bottleneck in my particular application.
I've given that consideration, and all I can say is... I don't really get head-of-empty-list problems. I like my functions to be total, so if there's lists with an expected form a list must take, usually my functions will return a Maybe or whatever. That said, I understand there are costs for defensive coding, and there's a limit to how defensive you can be. With stack overflow - that's a rare thing in Haskell - a correctness issue that isn't controlled by the type system, yet isn't referentially transparent. The only times it has happened to me, I deserved it for writing silly code. But still, I see the point. So maybe my impressions are naive - I still don't have experience dealing with large projects in Haskell. 
Looking up Pattern Synonyms it looks like you mean make a `pattern Foo a b c ... = In (Foo_ a b c ...)` for each data constructor `Foo_` It seems there isn't Pattern Synonym support in TH though :(
I definitely agree - purity should be a strength rather than a weakness for debugging. How often do you really need to know about evaluation order? Probably only if you're dealing with unsafePerformIO. We're all familiar with the wisdom that the trickiest bugs are found in code with side effects. This is why it's a good idea to try to restrict the amount of code that's in the IO monad to just what's required. If a debugger were able to only step along the imperative bits of a program, then this design principle can provide even further value, as there's less code to step through. Debugging imperative code is special, though, because once you've executed an action, you can't go back to exactly how things were before the step. In the past, I've thought of this idea as a "monadic debugger" - a debugger that pauses on binds, and maybe lets you inspect the values of all the local variables. However, this is unsatisfying, because what's so special about monads? Sure, stepwise debugging makes sense for monads in general, but so does "diving" down into sub-expressions. More recently, I've been idly thinking along the lines of having the debug mode execution yield a lazy structure representing the evaluation details of the program. It seems like this would be a great interface for implementing different debugging strategies - it turns the task of writing a debugger into just interactively visualizing a datatype! The downside of such an interface is that it would likely yield from execution way too much - code might run /really/ slow if it's constantly yielding execution to the debugger. Coming back to the "monadic debugger" idea, one possibility is to have a debugger where execution only yields to it before evaluating an IO action (e.g. stuff that main is assigned to, or is immediately on the right hand side of &gt;&gt;= or &gt;&gt;). The info yielded to the debugger would include everything relevant to executing that action. This might be all of the in scope locals (this seems more useful), or just the ones it depends on. If mixed with the above strategy, these locals might be provided along with their lazy debug info structure. What I like about this idea is that the debugger would have the option of saving the environments used to execute IO actions. This could potentially allow a potent form of reversible debugging: not only could you look at past states of the program, you could resume execution from a given location in the program, possibly providing different values than were used before. Mixed with chris's [ghci-reload](http://chrisdone.com/posts/ghci-reload), it might also be possible to pause the program, modify, and resume! One last idea to throw in: it'd be very nice to be able to ask the question "Where did this value come from? Who created it?". It might also be interesting to know which action caused the value to be created, and which action demanded its value (if any).
Still no generic arbitrary? :(
&gt; Took me about 2 or 3 hours to get through all the steps Wow.. May I ask what took so long? I might have an advantage, since my system was already a *-buntu, and it's quite performant in the compiling phase (10mins for all deps). &gt; yesod: my-yesod-20140322.cabal: It seems you have chosen a different name then I did in the example. Possibly numerics are not allowed at some point. Could you paste your .cabal file somewhere? (like http://lpaste.net/) Glad you gave it a go.. I promise, once running this web app flies! I get 5ms in production when the db is not hit. Good luck!
15,000 on Mar 23, 2014 EDIT: 16,000 on June 10, 2014 EDIT: 17,000 on August 20, 2014 EDIT: 18,000 on November 10, 2014 EDIT: 19,000 on February 8, 2015 EDIT: 20,000 on April 25, 2015
&gt; I like my functions to be total So do I, and you won't find lots of partial functions in my code. However, I use libraries from hackage, and sometimes they do. &gt; The only times it has happened to me, I deserved it for writing silly code We all write silly code sometimes. Forgetting a strictness annotation on an accumulator somewhere is not far-fetched. See this example: http://neilmitchell.blogspot.co.il/2013/02/chasing-space-leak-in-shake.html
The one thing that I want when I'm trying to figure out why something is going wrong is to test functions that I've written in a where clause. It's a pain to lift them out to toplevel so I can call them in ghci, but being able to do foo ctx xs ys = map bar xs where bar x = ... --Using ctx and ys ghci&gt; let bar = $extractWhere foo 7 [...] [...] ghci&gt; bar 6 &gt; 8 Or something like that. It's possible to do this now with some TH. Maybe I should write that TH and put it in a library...
This is fairly close to correct. I'd say my primary disagreements orient around points (4) and (5). In particular (4) This is *not* a requirement of a monad, it simply shows up a whole lot. These functions are often called "run" functions and can take all shapes and forms (or even fail to exist). They relate more to the design of the "context" of a monad than the general structure "monad" itself. (5) You can't exactly (immediately) compose `f` and `g`. In particular, as both `f` and `g` take an unwrapped value and return a wrapped value you'd need to have a layer of "unwrapping" between them. This is easier to state in types, so forgive me for writing a little Haskell instance Monad M x :: A f :: A -&gt; M B g :: B -&gt; M C -- g (f x) doesn't typecheck; it ends up making two "layers" of `M` But this is exactly what `bind` does, really, it lets us compose functions which wrap values in the monad. bind :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b bind (f x) g -- this does typecheck! So, modulo those small details, the general intuition of (5) is correct—bind lets us compose wrapping functions (also known, somewhat frighteningly, as Kleisli arrows) and it doesn't matter in what order we do it. --- I should also mention that there's a bit more to monads beyond what you've outlined in these points, though they do form a very functional intuition for working with them. In particular, there's a lot more to be said about what it means for a monad to "wrap" a value and how people go about creating "wrapped" values. For instance, it's quite possible that a monad that is said to be "wrapping" some value... does not contain any such value at all! They can be entirely virtual, or not yet there, or the result of some yet-to-be-known process.
Re (2): yes, it must return the same monad.
Firstly, thanks for responding to such a lengthy post. &gt; (4)This is *not* a requirement of a monad, it simply shows up a whole lot If you don't have to have a 'run' method, though, how do you get the final result of the computation? I suppose you could have a computation where you don't actually get the wrapped value, but surely you'd need some kind of interface to do some kind of assertion on it, right? &gt; (5) ... both return a wrapped value I think this is something I missed; I didn't realize that the functions you bound to the monad had to return monads themselves; I assumed that the 'first' monad would do that itself. I guess this behaviour is for the sake of flexibility of composition? &gt; For instance, it's quite possible that a monad that is said to be "wrapping" some value... does not contain any such value at all! They can be entirely virtual, or not yet there, or the result of some yet-to-be-known process. I think I get this idea; it reminds me a lot of promises in JavaScript, where an object represents the not-yet-returned value of async operation (like an AJAX call), and I can chain operations that will only be 'fully' evaluated in a late manner. I'm going to do some reading on particular types of monads to try and really get my head around what it actually 'means' to be a monad. Thanks a lot! 
I`m no expert but i think this is related http://homotopytypetheory.org/
i've struggled to find introductory materials to category theory that 'work' for me; much of the time they've basically felt like the "A Simple Category-Theoretic Understanding of Category-Theoretic Diagrams" article from [the SIGBOVIK 2014 proceedings](https://dl.dropboxusercontent.com/u/2363336/sigbovik-proceedings-2014.pdf) [PDF]. A significant exception to this has been Steve Easterbrook's "[An introduction to Category Theory for Software Engineers](http://www.cs.toronto.edu/~sme/presentations/cat101.pdf)" [PDF].